<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 31]
- [cs.AI](#cs.AI) [Total: 23]
- [cs.LG](#cs.LG) [Total: 46]
- [cs.CV](#cs.CV) [Total: 29]
- [cs.DC](#cs.DC) [Total: 1]
- [eess.IV](#eess.IV) [Total: 5]
- [cs.IT](#cs.IT) [Total: 1]
- [math.OC](#math.OC) [Total: 3]
- [cs.AR](#cs.AR) [Total: 1]
- [cs.NE](#cs.NE) [Total: 3]
- [cs.SD](#cs.SD) [Total: 3]
- [cs.CY](#cs.CY) [Total: 3]
- [cs.IR](#cs.IR) [Total: 5]
- [cs.SE](#cs.SE) [Total: 6]
- [cs.RO](#cs.RO) [Total: 3]
- [cs.NI](#cs.NI) [Total: 1]
- [eess.SY](#eess.SY) [Total: 1]
- [cs.DB](#cs.DB) [Total: 1]
- [cs.PL](#cs.PL) [Total: 1]
- [cs.SI](#cs.SI) [Total: 1]
- [physics.med-ph](#physics.med-ph) [Total: 1]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 1]
- [stat.ML](#stat.ML) [Total: 7]
- [hep-ph](#hep-ph) [Total: 1]
- [cs.CR](#cs.CR) [Total: 4]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [SFT or RL? An Early Investigation into Training R1-Like Reasoning Large Vision-Language Models](https://arxiv.org/abs/2504.11468)
*Hardy Chen,Haoqin Tu,Fali Wang,Hui Liu,Xianfeng Tang,Xinya Du,Yuyin Zhou,Cihang Xie*

Main category: cs.CL

TL;DR: 研究发现，监督微调（SFT）会通过模仿专家模型的伪推理路径阻碍后续强化学习（RL）。提出新数据集VLAA-Thinking和RL方法GRPO，模型VLAA-Thinker在4B规模LVLMs中表现最佳。


<details>
  <summary>Details</summary>
Motivation: 揭示SFT对RL的负面影响，并探索更有效的训练方法以提升大型视觉语言模型的推理能力。

Method: 构建VLAA-Thinking数据集，结合SFT和RL实验，提出GRPO强化学习框架。

Result: SFT导致模型陷入模仿性推理模式，而GRPO方法显著提升性能，VLAA-Thinker在4B规模模型中领先1.8%。

Conclusion: SFT可能阻碍RL学习，GRPO方法更有效，为未来研究提供新方向。

Abstract: This work revisits the dominant supervised fine-tuning (SFT) then
reinforcement learning (RL) paradigm for training Large Vision-Language Models
(LVLMs), and reveals a key finding: SFT can significantly undermine subsequent
RL by inducing ``pseudo reasoning paths'' imitated from expert models. While
these paths may resemble the native reasoning paths of RL models, they often
involve prolonged, hesitant, less informative steps, and incorrect reasoning.
To systematically study this effect, we introduce VLAA-Thinking, a new
multimodal dataset designed to support reasoning in LVLMs. Constructed via a
six-step pipeline involving captioning, reasoning distillation, answer rewrite
and verification, VLAA-Thinking comprises high-quality, step-by-step visual
reasoning traces for SFT, along with a more challenging RL split from the same
data source. Using this dataset, we conduct extensive experiments comparing
SFT, RL and their combinations. Results show that while SFT helps models learn
reasoning formats, it often locks aligned models into imitative, rigid
reasoning modes that impede further learning. In contrast, building on the
Group Relative Policy Optimization (GRPO) with a novel mixed reward module
integrating both perception and cognition signals, our RL approach fosters more
genuine, adaptive reasoning behavior. Notably, our model VLAA-Thinker, based on
Qwen2.5VL 3B, achieves top-1 performance on Open LMM Reasoning Leaderboard
(https://huggingface.co/spaces/opencompass/Open_LMM_Reasoning_Leaderboard)
among 4B scale LVLMs, surpassing the previous state-of-the-art by 1.8%. We hope
our findings provide valuable insights in developing reasoning-capable LVLMs
and can inform future research in this area.

</details>


### [2] [ReTool: Reinforcement Learning for Strategic Tool Use in LLMs](https://arxiv.org/abs/2504.11536)
*Jiazhan Feng,Shijue Huang,Xingwei Qu,Ge Zhang,Yujia Qin,Baoquan Zhong,Chengquan Jiang,Jinxin Chi,Wanjun Zhong*

Main category: cs.CL

TL;DR: ReTool通过动态代码执行和自动化强化学习，提升模型在结构化问题解决中的性能，显著优于纯文本基线。


<details>
  <summary>Details</summary>
Motivation: 解决推理模型在结构化问题（如几何推理、复杂方程求解）中的不足，利用计算工具的优势。

Method: 结合实时代码执行与自然语言推理，通过自动化强化学习优化工具调用策略。

Result: 在AIME基准测试中，ReTool-32B达到67%准确率，优于基线（40%），扩展设置下提升至72.5%。

Conclusion: ReTool展示了结果驱动的工具集成在复杂数学推理中的潜力，为混合神经符号系统提供新见解。

Abstract: While reasoning models (e.g., DeepSeek R1) trained with reinforcement
learning (RL), excel in textual reasoning, they struggle in scenarios requiring
structured problem-solving, such as geometric reasoning, concise computation,
or complex equation solving-areas where computational tools like code
interpreters (CI) demonstrate distinct advantages. To bridge this gap, we
propose ReTool, which enhances long-form reasoning with tool-integrated
learning, including two key features: (1) dynamic interleaving of real-time
code execution within natural language reasoning processes, and (2) an
automated RL paradigm that allows policy rollouts with multi-turn real-time
code execution and teaches the model in learning when and how to invoke tools
based on outcome feedback. ReTool employs a systematic training framework,
beginning with synthetic cold-start data generation to produce code-augmented
long-form reasoning traces for fine-tuning base models. Subsequent RL training
leverages task outcomes as rewards to iteratively refine the model's tool use
strategy, enabling autonomous discovery of optimal tool invocation patterns
without human priors. Experiments on the challenging MATH Olympiad benchmark
AIME demonstrate ReTool's superiority: Our 32B model achieves 67% accuracy with
400 training steps, outperforming text-based RL baseline (40% accuracy, 1080
steps) in efficiency and performance. Remarkably, ReTool-32B attains 72.5%
accuracy in extended settings, surpassing OpenAI's o1-preview by 27.9%. Further
analysis reveals emergent behaviors such as code self-correction, signaling an
''aha moment'' in which the model autonomously masters adaptive tool use. These
findings highlight the promise of outcome-driven tool integration for advancing
complex mathematical reasoning and offer new insights into hybrid
neuro-symbolic systems.

</details>


### [3] [AskQE: Question Answering as Automatic Evaluation for Machine Translation](https://arxiv.org/abs/2504.11582)
*Dayeon Ki,Kevin Duh,Marine Carpuat*

Main category: cs.CL

TL;DR: AskQE是一个通过生成和回答问题来检测机器翻译错误并提供反馈的框架，帮助不懂目标语言的用户评估翻译质量。


<details>
  <summary>Details</summary>
Motivation: 现有机器翻译错误检测和质量评估技术未解决不懂目标语言的用户评估翻译质量的实用场景。

Method: 基于ContraTICO数据集和LLaMA-3 70B模型，设计AskQE框架，利用生成的问题和答案检测关键错误。

Result: 在BioMQM数据集上，AskQE的Kendall's Tau相关性和决策准确性优于其他质量评估指标。

Conclusion: AskQE为不懂目标语言的用户提供了一种有效的机器翻译质量评估方法。

Abstract: How can a monolingual English speaker determine whether an automatic
translation in French is good enough to be shared? Existing MT error detection
and quality estimation (QE) techniques do not address this practical scenario.
We introduce AskQE, a question generation and answering framework designed to
detect critical MT errors and provide actionable feedback, helping users decide
whether to accept or reject MT outputs even without the knowledge of the target
language. Using ContraTICO, a dataset of contrastive synthetic MT errors in the
COVID-19 domain, we explore design choices for AskQE and develop an optimized
version relying on LLaMA-3 70B and entailed facts to guide question generation.
We evaluate the resulting system on the BioMQM dataset of naturally occurring
MT errors, where AskQE has higher Kendall's Tau correlation and decision
accuracy with human ratings compared to other QE metrics.

</details>


### [4] [Improving Instruct Models for Free: A Study on Partial Adaptation](https://arxiv.org/abs/2504.11626)
*Ozan İrsoy,Pengxiang Cheng,Jennifer L. Chen,Daniel Preoţiuc-Pietro,Shiyue Zhang,Duccio Pappadopulo*

Main category: cs.CL

TL;DR: 研究发现，指令调优虽然提升模型指令跟随能力，但可能导致预训练知识遗忘或过度冗长，影响少样本学习性能。通过部分适应方法降低调优强度，可改善少样本学习表现，但会牺牲部分指令跟随能力。


<details>
  <summary>Details</summary>
Motivation: 探讨指令调优对模型性能的影响，尤其是少样本学习与指令跟随能力之间的潜在权衡。

Method: 采用部分适应方法，逐步降低指令调优的强度，并在不同模型家族和规模上验证效果。

Result: 降低指令调优强度显著提升少样本学习性能，但削弱指令跟随能力（以AlpacaEval衡量）。

Conclusion: 实践中需权衡少样本学习与指令跟随能力，部分适应方法提供了一种可行的优化路径。

Abstract: Instruct models, obtained from various instruction tuning or post-training
steps, are commonly deemed superior and more usable than their base
counterpart. While the model gains instruction following ability, instruction
tuning may lead to forgetting the knowledge from pre-training or it may
encourage the model being overly conversational or verbose. This, in turn, can
lead to degradation of in-context few-shot learning performance. In this work,
we study the performance trajectory between base and instruct models by scaling
down the strength of instruction-tuning via the partial adaption method. We
show that, across several model families and model sizes, reducing the strength
of instruction-tuning results in material improvement on a few-shot in-context
learning benchmark covering a variety of classic natural language tasks. This
comes at the cost of losing some degree of instruction following ability as
measured by AlpacaEval. Our study shines light on the potential trade-off
between in-context learning and instruction following abilities that is worth
considering in practice.

</details>


### [5] [Higher-Order Binding of Language Model Virtual Personas: a Study on Approximating Political Partisan Misperceptions](https://arxiv.org/abs/2504.11673)
*Minwoo Kang,Suhong Moon,Seung Hyeong Lee,Ayush Raj,Joseph Suh,David M. Chan*

Main category: cs.CL

TL;DR: 论文提出了一种新方法，通过生成详细的虚拟人物背景故事，使大语言模型能更准确地模拟人类对群体的感知和评价，适用于政治科学研究。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决大语言模型在模拟人类对群体感知和评价时的不足，以支持政治科学中的极化动态、群体冲突等研究。

Method: 提出了一种基于多轮访谈转录生成详细虚拟人物背景故事的方法，确保背景故事更长、细节丰富且一致。

Result: 实验表明，该方法显著提升了模型对人类响应分布的模拟能力（Wasserstein Distance改进达87%），效果接近原始研究。

Conclusion: 该方法扩展了大语言模型的应用范围，使其不仅能模拟个体观点，还能用于更广泛的人类行为研究。

Abstract: Large language models (LLMs) are increasingly capable of simulating human
behavior, offering cost-effective ways to estimate user responses during the
early phases of survey design. While previous studies have examined whether
models can reflect individual opinions or attitudes, we argue that a
\emph{higher-order} binding of virtual personas requires successfully
approximating not only the opinions of a user as an identified member of a
group, but also the nuanced ways in which that user perceives and evaluates
those outside the group. In particular, faithfully simulating how humans
perceive different social groups is critical for applying LLMs to various
political science studies, including timely topics on polarization dynamics,
inter-group conflict, and democratic backsliding. To this end, we propose a
novel methodology for constructing virtual personas with synthetic user
``backstories" generated as extended, multi-turn interview transcripts. Our
generated backstories are longer, rich in detail, and consistent in
authentically describing a singular individual, compared to previous methods.
We show that virtual personas conditioned on our backstories closely replicate
human response distributions (up to an 87\% improvement as measured by
Wasserstein Distance) and produce effect sizes that closely match those
observed in the original studies. Altogether, our work extends the
applicability of LLMs beyond estimating individual self-opinions, enabling
their use in a broader range of human studies.

</details>


### [6] [Unsupervised Classification of English Words Based on Phonological Information: Discovery of Germanic and Latinate Clusters](https://arxiv.org/abs/2504.11770)
*Takashi Morita,Timothy J. O'Donnell*

Main category: cs.CL

TL;DR: 研究表明，英语中的日耳曼语和拉丁语词汇区别可通过音位信息学习，无需依赖词源知识。


<details>
  <summary>Details</summary>
Motivation: 探讨语言学习者如何在不了解词源的情况下，区分日耳曼语和拉丁语词汇。

Method: 使用无监督聚类方法对语料库提取的词汇进行分析。

Result: 聚类结果与词源分类高度一致，并发现新的语言学特征。

Conclusion: 音位信息可用于区分词源类别，为未来实验研究提供新假设。

Abstract: Cross-linguistically, native words and loanwords follow different
phonological rules. In English, for example, words of Germanic and Latinate
origin exhibit different stress patterns, and a certain syntactic structure is
exclusive to Germanic verbs. When seeing them as a cognitive model, however,
such etymology-based generalizations face challenges in terms of learnability,
since the historical origins of words are presumably inaccessible information
for general language learners. In this study, we present computational evidence
indicating that the Germanic-Latinate distinction in the English lexicon is
learnable from the phonotactic information of individual words. Specifically,
we performed an unsupervised clustering on corpus-extracted words, and the
resulting word clusters largely aligned with the etymological distinction. The
model-discovered clusters also recovered various linguistic generalizations
documented in the previous literature regarding the corresponding etymological
classes. Moreover, our findings also uncovered previously unrecognized features
of the quasi-etymological clusters, offering novel hypotheses for future
experimental studies.

</details>


### [7] [Enhancing Web Agents with Explicit Rollback Mechanisms](https://arxiv.org/abs/2504.11788)
*Zhisong Zhang,Tianqing Fang,Kaixin Ma,Wenhao Yu,Hongming Zhang,Haitao Mi,Dong Yu*

Main category: cs.CL

TL;DR: 论文提出了一种显式回滚机制，增强网络代理在复杂动态环境中的导航能力，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有网络代理在复杂动态环境中因单向搜索策略难以从错误状态恢复，需改进。

Method: 引入显式回滚机制，允许代理在导航轨迹中回退至先前状态，灵活控制搜索过程。

Result: 在两个实时网络导航基准测试中，零样本和微调设置下均表现有效。

Conclusion: 显式回滚机制显著提升了网络代理的导航效率和效果。

Abstract: With recent advancements in large language models, web agents have been
greatly improved. However, dealing with complex and dynamic web environments
requires more advanced planning and search abilities. Previous studies usually
adopt a greedy one-way search strategy, which may struggle to recover from
erroneous states. In this work, we enhance web agents with an explicit rollback
mechanism, enabling the agent to revert back to a previous state in its
navigation trajectory. This mechanism gives the model the flexibility to
directly control the search process, leading to an effective and efficient web
navigation method. We conduct experiments on two live web navigation benchmarks
with zero-shot and fine-tuning settings. The results demonstrate the
effectiveness of our proposed approach.

</details>


### [8] [Selective Attention Federated Learning: Improving Privacy and Efficiency for Clinical Text Classification](https://arxiv.org/abs/2504.11793)
*Yue Li,Lihong Zhang*

Main category: cs.CL

TL;DR: SAFL是一种新型联邦学习方法，通过动态微调关键注意力层，显著降低通信开销并增强隐私保护。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习在大型语言模型训练中的通信开销和隐私问题，特别是在医疗领域。

Method: 利用注意力模式识别关键层，仅动态微调这些层。

Result: 在临床NLP基准测试中表现优异，通信效率和隐私保护显著提升。

Conclusion: SAFL在保持性能的同时，有效解决了联邦学习的通信和隐私挑战。

Abstract: Federated Learning (FL) faces major challenges regarding communication
overhead and model privacy when training large language models (LLMs),
especially in healthcare applications. To address these, we introduce Selective
Attention Federated Learning (SAFL), a novel approach that dynamically
fine-tunes only those transformer layers identified as attention-critical. By
employing attention patterns to determine layer importance, SAFL significantly
reduces communication bandwidth and enhances differential privacy resilience.
Evaluations on clinical NLP benchmarks (i2b2 Clinical Concept Extraction and
MIMIC-III discharge summaries) demonstrate that SAFL achieves competitive
performance with centralized models while substantially improving communication
efficiency and privacy preservation.

</details>


### [9] [Efficient and Adaptive Simultaneous Speech Translation with Fully Unidirectional Architecture](https://arxiv.org/abs/2504.11809)
*Biao Fu,Donglei Yu,Minpeng Liao,Chengxi Li,Yidong Chen,Kai Fan,Xiaodong Shi*

Main category: cs.CL

TL;DR: EASiST提出了一种高效自适应的同步语音翻译方法，通过单向架构和多阶段训练策略优化了延迟与质量的平衡。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的同步语音翻译方法存在计算开销大或策略固定的问题，限制了效率和性能。

Method: EASiST采用单向架构，包括多延迟数据生成策略、轻量级策略头和分阶段训练。

Result: 在MuST-C数据集上，EASiST在延迟与翻译质量上优于基线方法。

Conclusion: EASiST通过创新架构和训练策略，显著提升了同步语音翻译的性能。

Abstract: Simultaneous speech translation (SimulST) produces translations incrementally
while processing partial speech input. Although large language models (LLMs)
have showcased strong capabilities in offline translation tasks, applying them
to SimulST poses notable challenges. Existing LLM-based SimulST approaches
either incur significant computational overhead due to repeated encoding of
bidirectional speech encoder, or they depend on a fixed read/write policy,
limiting the efficiency and performance. In this work, we introduce Efficient
and Adaptive Simultaneous Speech Translation (EASiST) with fully unidirectional
architecture, including both speech encoder and LLM. EASiST includes a
multi-latency data curation strategy to generate semantically aligned SimulST
training samples and redefines SimulST as an interleaved generation task with
explicit read/write tokens. To facilitate adaptive inference, we incorporate a
lightweight policy head that dynamically predicts read/write actions.
Additionally, we employ a multi-stage training strategy to align speech-text
modalities and optimize both translation and policy behavior. Experiments on
the MuST-C En$\rightarrow$De and En$\rightarrow$Es datasets demonstrate that
EASiST offers superior latency-quality trade-offs compared to several strong
baselines.

</details>


### [10] [ARWI: Arabic Write and Improve](https://arxiv.org/abs/2504.11814)
*Kirill Chirkunov,Bashar Alhafni,Chatrine Qwaider,Nizar Habash,Ted Briscoe*

Main category: cs.CL

TL;DR: ARWI是一个针对现代标准阿拉伯语的写作助手，提供语法纠错、自动评分等功能，填补了阿拉伯语写作辅助工具的空白。


<details>
  <summary>Details</summary>
Motivation: 尽管阿拉伯语使用者众多，但高级写作辅助工具稀缺，ARWI旨在填补这一空白。

Method: ARWI整合了提示数据库、文本编辑器、语法纠错和自动评分功能，并支持构建标注语料库。

Result: 初步用户研究表明，ARWI能提供有效反馈，帮助学习者识别语法问题并提升语言能力。

Conclusion: ARWI为阿拉伯语学习者和研究者提供了实用工具，并有望推动相关研究发展。

Abstract: Although Arabic is spoken by over 400 million people, advanced Arabic writing
assistance tools remain limited. To address this gap, we present ARWI, a new
writing assistant that helps learners improve essay writing in Modern Standard
Arabic. ARWI is the first publicly available Arabic writing assistant to
include a prompt database for different proficiency levels, an Arabic text
editor, state-of-the-art grammatical error detection and correction, and
automated essay scoring aligned with the Common European Framework of Reference
standards for language attainment. Moreover, ARWI can be used to gather a
growing auto-annotated corpus, facilitating further research on Arabic grammar
correction and essay scoring, as well as profiling patterns of errors made by
native speakers and non-native learners. A preliminary user study shows that
ARWI provides actionable feedback, helping learners identify grammatical gaps,
assess language proficiency, and guide improvement.

</details>


### [11] [Déjà Vu: Multilingual LLM Evaluation through the Lens of Machine Translation Evaluation](https://arxiv.org/abs/2504.11829)
*Julia Kreutzer,Eleftheria Briakou,Sweta Agrawal,Marzieh Fadaee,Kocmi Tom*

Main category: cs.CL

TL;DR: 论文探讨了多语言大语言模型（mLLMs）生成能力评估的不足，并借鉴机器翻译（MT）评估的经验，提出了改进建议。


<details>
  <summary>Details</summary>
Motivation: 当前mLLMs生成能力的评估缺乏全面性和科学性，影响了其发展潜力。

Method: 通过实验借鉴MT评估的最佳实践，分析模型质量差异，并提出元评估的关键组件。

Result: 展示了MT评估方法如何提升mLLMs评估的透明度与可靠性。

Conclusion: 总结了一套可操作的mLLM研发建议清单。

Abstract: Generation capabilities and language coverage of multilingual large language
models (mLLMs) are advancing rapidly. However, evaluation practices for
generative abilities of mLLMs are still lacking comprehensiveness, scientific
rigor, and consistent adoption across research labs, which undermines their
potential to meaningfully guide mLLM development. We draw parallels with
machine translation (MT) evaluation, a field that faced similar challenges and
has, over decades, developed transparent reporting standards and reliable
evaluations for multilingual generative models. Through targeted experiments
across key stages of the generative evaluation pipeline, we demonstrate how
best practices from MT evaluation can deepen the understanding of quality
differences between models. Additionally, we identify essential components for
robust meta-evaluation of mLLMs, ensuring the evaluation methods themselves are
rigorously assessed. We distill these insights into a checklist of actionable
recommendations for mLLM research and development.

</details>


### [12] [Could Thinking Multilingually Empower LLM Reasoning?](https://arxiv.org/abs/2504.11833)
*Changjiang Gao,Xu Huang,Wenhao Zhu,Shujian Huang,Lei Li,Fei Yuan*

Main category: cs.CL

TL;DR: 研究发现多语言推理在大型语言模型中的表现上限显著高于仅用英语，但现有方法难以达到这一上限。


<details>
  <summary>Details</summary>
Motivation: 探索多语言推理在大型语言模型中的潜力，尤其是其表现上限高于英语的现象。

Method: 分析多语言推理的上限及其原因，并评估现有答案选择方法的局限性。

Result: 多语言推理的表现上限比英语高近10个Acc@$k$点，且对翻译质量和语言选择具有鲁棒性。

Conclusion: 多语言推理潜力巨大，但需改进方法以充分利用其优势。

Abstract: Previous work indicates that large language models exhibit a significant
"English bias", i.e. they often perform better when tasks are presented in
English. Interestingly, we have observed that using certain other languages in
reasoning tasks can yield better performance than English. However, this
phenomenon remains under-explored. In this paper, we explore the upper bound of
harnessing multilingualism in reasoning tasks, suggesting that multilingual
reasoning promises significantly (by nearly 10 Acc@$k$ points) and robustly
(tolerance for variations in translation quality and language choice) higher
upper bounds than English-only reasoning. Besides analyzing the reason behind
the upper bound and challenges in reaching it, we also find that common answer
selection methods cannot achieve this upper bound, due to their limitations and
biases. These insights could pave the way for future research aimed at fully
harnessing the potential of multilingual reasoning in LLMs.

</details>


### [13] [FiSMiness: A Finite State Machine Based Paradigm for Emotional Support Conversations](https://arxiv.org/abs/2504.11837)
*Yue Zhao,Qingqing Gu,Xiaoyu Wang,Teng Chen,Zhonglin Jiang,Yong Chen,Luo Ji*

Main category: cs.CL

TL;DR: 论文提出了一种基于有限状态机（FSM）的框架FiSMiness，用于提升情感支持对话（ESC）的效果，通过单一大语言模型（LLM）实现对话规划与自我推理。


<details>
  <summary>Details</summary>
Motivation: 现有研究在ESC中未从状态模型角度定义对话流程，导致长期满意度不足。

Method: 利用FSM结合LLM，提出FiSMiness框架，支持单模型在对话中规划、推理情绪与策略。

Result: 实验表明FiSMiness优于多种基线方法，包括直接推理、自优化、思维链等。

Conclusion: FiSMiness通过状态机模型显著提升了ESC的长期效果，优于参数更多的模型。

Abstract: Emotional support conversation (ESC) aims to alleviate the emotional distress
of individuals through effective conversations. Although large language models
(LLMs) have obtained remarkable progress on ESC, most of these studies might
not define the diagram from the state model perspective, therefore providing a
suboptimal solution for long-term satisfaction. To address such an issue, we
leverage the Finite State Machine (FSM) on LLMs, and propose a framework called
FiSMiness. Our framework allows a single LLM to bootstrap the planning during
ESC, and self-reason the seeker's emotion, support strategy and the final
response upon each conversational turn. Substantial experiments on ESC datasets
suggest that FiSMiness outperforms many baselines, including direct inference,
self-refine, chain of thought, finetuning, and external-assisted methods, even
those with many more parameters.

</details>


### [14] [Finding Flawed Fictions: Evaluating Complex Reasoning in Language Models via Plot Hole Detection](https://arxiv.org/abs/2504.11900)
*Kabir Ahuja,Melanie Sclar,Yulia Tsvetkov*

Main category: cs.CL

TL;DR: 该论文提出了一种通过检测故事中的情节漏洞（plot holes）来评估大型语言模型（LLMs）的语言理解和推理能力的方法，并构建了一个名为FlawedFictions的基准测试。


<details>
  <summary>Details</summary>
Motivation: 故事是人类体验的核心，而检测情节漏洞需要复杂的推理能力。随着LLMs在文本生成和理解中的应用增加，评估其深层次语言理解能力变得至关重要。

Method: 研究者开发了FlawedFictionsMaker算法，用于在人类编写的故事中可控地合成情节漏洞，并构建了FlawedFictions基准测试。

Result: 研究发现，即使是先进的LLMs在情节漏洞检测任务中表现不佳，且随着故事长度增加性能显著下降。此外，LLM生成的故事和摘要更容易引入情节漏洞。

Conclusion: 情节漏洞检测可作为评估LLMs语言理解和推理能力的有效代理任务，揭示了LLMs在深层次叙事理解上的局限性。

Abstract: Stories are a fundamental aspect of human experience. Engaging deeply with
stories and spotting plot holes -- inconsistencies in a storyline that break
the internal logic or rules of a story's world -- requires nuanced reasoning
skills, including tracking entities and events and their interplay, abstract
thinking, pragmatic narrative understanding, commonsense and social reasoning,
and theory of mind. As Large Language Models (LLMs) increasingly generate,
interpret, and modify text, rigorously assessing their narrative consistency
and deeper language understanding becomes critical. However, existing
benchmarks focus mainly on surface-level comprehension. In this work, we
propose plot hole detection in stories as a proxy to evaluate language
understanding and reasoning in LLMs. We introduce FlawedFictionsMaker, a novel
algorithm to controllably and carefully synthesize plot holes in human-written
stories. Using this algorithm, we construct a benchmark to evaluate LLMs' plot
hole detection abilities in stories -- FlawedFictions -- , which is robust to
contamination, with human filtering ensuring high quality. We find that
state-of-the-art LLMs struggle in accurately solving FlawedFictions regardless
of the reasoning effort allowed, with performance significantly degrading as
story length increases. Finally, we show that LLM-based story summarization and
story generation are prone to introducing plot holes, with more than 50% and
100% increases in plot hole detection rates with respect to human-written
originals.

</details>


### [15] [An LLM-as-a-judge Approach for Scalable Gender-Neutral Translation Evaluation](https://arxiv.org/abs/2504.11934)
*Andrea Piergentili,Beatrice Savoldi,Matteo Negri,Luisa Bentivogli*

Main category: cs.CL

TL;DR: 论文探讨了使用大型语言模型（LLMs）评估性别中立翻译（GNT）的方法，提出了两种提示策略，并通过实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前GNT评估方法局限于单语分类器，未考虑源句子且难以扩展至新语言，因此研究LLMs作为评估工具的潜力。

Method: 采用两种提示策略：直接生成句子级评估和先进行短语级注释再生成句子级评估（类似思维链方法）。

Result: 实验表明，LLMs可作为GNT评估工具，且先进行短语级注释能显著提升所有模型的准确性。

Conclusion: LLMs为GNT评估提供了更优且可扩展的解决方案，短语级注释策略效果更佳。

Abstract: Gender-neutral translation (GNT) aims to avoid expressing the gender of human
referents when the source text lacks explicit cues about the gender of those
referents. Evaluating GNT automatically is particularly challenging, with
current solutions being limited to monolingual classifiers. Such solutions are
not ideal because they do not factor in the source sentence and require
dedicated data and fine-tuning to scale to new languages. In this work, we
address such limitations by investigating the use of large language models
(LLMs) as evaluators of GNT. Specifically, we explore two prompting approaches:
one in which LLMs generate sentence-level assessments only, and another, akin
to a chain-of-thought approach, where they first produce detailed phrase-level
annotations before a sentence-level judgment. Through extensive experiments on
multiple languages with five models, both open and proprietary, we show that
LLMs can serve as evaluators of GNT. Moreover, we find that prompting for
phrase-level annotations before sentence-level assessments consistently
improves the accuracy of all models, providing a better and more scalable
alternative to current solutions.

</details>


### [16] [Robust and Fine-Grained Detection of AI Generated Texts](https://arxiv.org/abs/2504.11952)
*Ram Mohan Rao Kadiyala,Siddartha Pullakhandam,Kanwal Mehreen,Drishti Sharma,Siddhant Gupta,Jebish Purbey,Ashay Srivastava,Subhasya TippaReddy,Arvind Reddy Bobbili,Suraj Telugara Chandrashekhar,Modabbir Adeeb,Srinadh Vura,Hamza Farooq*

Main category: cs.CL

TL;DR: 论文提出了一种用于检测机器生成内容的系统，特别关注人类与LLM共同创作的文本，并引入了一个新的数据集和一系列模型。


<details>
  <summary>Details</summary>
Motivation: 现有系统在短文本和部分人类-LLM共同创作的文本上表现不佳，需要更通用的检测方法。

Method: 使用基于标记分类的模型，训练于大规模人类-机器共同创作的文本数据集。

Result: 模型在未见过的领域、生成器、非母语文本及对抗性输入上表现良好，并提供了详细的性能分析。

Conclusion: 提出的方法在多样化场景下具有鲁棒性，为机器生成内容检测提供了新方向。

Abstract: An ideal detection system for machine generated content is supposed to work
well on any generator as many more advanced LLMs come into existence day by
day. Existing systems often struggle with accurately identifying AI-generated
content over shorter texts. Further, not all texts might be entirely authored
by a human or LLM, hence we focused more over partial cases i.e human-LLM
co-authored texts. Our paper introduces a set of models built for the task of
token classification which are trained on an extensive collection of
human-machine co-authored texts, which performed well over texts of unseen
domains, unseen generators, texts by non-native speakers and those with
adversarial inputs. We also introduce a new dataset of over 2.4M such texts
mostly co-authored by several popular proprietary LLMs over 23 languages. We
also present findings of our models' performance over each texts of each domain
and generator. Additional findings include comparison of performance against
each adversarial method, length of input texts and characteristics of generated
texts compared to the original human authored texts.

</details>


### [17] [LLM-as-a-Judge: Reassessing the Performance of LLMs in Extractive QA](https://arxiv.org/abs/2504.11972)
*Xanh Ho,Jiahao Huang,Florian Boudin,Akiko Aizawa*

Main category: cs.CL

TL;DR: 论文提出使用LLM作为评估者（LLM-as-a-judge）重新评估QA模型的性能，发现其与人类判断高度相关，优于传统EM/F1指标。


<details>
  <summary>Details</summary>
Motivation: 传统EM和F1指标未能全面捕捉QA模型性能，LLM的成功应用为评估提供了新思路。

Method: 在四个阅读理解QA数据集上，使用不同家族的LLM和多种答案类型，评估LLM-as-a-judge的效果。

Result: LLM-as-a-judge与人类判断的相关系数从EM的0.17和F1的0.36提升至0.85，显著优于传统指标。

Conclusion: LLM-as-a-judge可替代EM/F1指标，尽管对复杂答案类型仍有不足，但无偏见问题。

Abstract: Extractive reading comprehension question answering (QA) datasets are
typically evaluated using Exact Match (EM) and F1-score, but these metrics
often fail to fully capture model performance. With the success of large
language models (LLMs), they have been employed in various tasks, including
serving as judges (LLM-as-a-judge). In this paper, we reassess the performance
of QA models using LLM-as-a-judge across four reading comprehension QA
datasets. We examine different families of LLMs and various answer types to
evaluate the effectiveness of LLM-as-a-judge in these tasks. Our results show
that LLM-as-a-judge is highly correlated with human judgments and can replace
traditional EM/F1 metrics. By using LLM-as-a-judge, the correlation with human
judgments improves significantly, from 0.17 (EM) and 0.36 (F1-score) to 0.85.
These findings confirm that EM and F1 metrics underestimate the true
performance of the QA models. While LLM-as-a-judge is not perfect for more
difficult answer types (e.g., job), it still outperforms EM/F1, and we observe
no bias issues, such as self-preference, when the same model is used for both
the QA and judgment tasks.

</details>


### [18] [SemEval-2025 Task 3: Mu-SHROOM, the Multilingual Shared Task on Hallucinations and Related Observable Overgeneration Mistakes](https://arxiv.org/abs/2504.11975)
*Raúl Vázquez,Timothee Mickus,Elaine Zosa,Teemu Vahtola,Jörg Tiedemann,Aman Sinha,Vincent Segonne,Fernando Sánchez-Vega,Alessandro Raganato,Jindřich Libovický,Jussi Karlgren,Shaoxiong Ji,Jindřich Helcl,Liane Guillou,Ona de Gibert,Jaione Bengoetxea,Joseph Attieh,Marianna Apidianaki*

Main category: cs.CL

TL;DR: Mu-SHROOM共享任务旨在检测多语言LLM输出中的幻觉和过度生成错误，吸引了大量社区参与，并分析了影响性能的关键因素。


<details>
  <summary>Details</summary>
Motivation: 解决多语言LLM输出中的幻觉问题，提升模型可靠性。

Method: 将幻觉检测任务定义为跨度标注任务，收集了来自43个团队的2618份提交。

Result: 展示了参与系统的结果，并分析了影响性能的关键因素，如语言间的幻觉差异和标注不一致性。

Conclusion: 强调了当前挑战，包括语言间的幻觉差异和标注不一致性，为未来研究提供了方向。

Abstract: We present the Mu-SHROOM shared task which is focused on detecting
hallucinations and other overgeneration mistakes in the output of
instruction-tuned large language models (LLMs). Mu-SHROOM addresses
general-purpose LLMs in 14 languages, and frames the hallucination detection
problem as a span-labeling task. We received 2,618 submissions from 43
participating teams employing diverse methodologies. The large number of
submissions underscores the interest of the community in hallucination
detection. We present the results of the participating systems and conduct an
empirical analysis to identify key factors contributing to strong performance
in this task. We also emphasize relevant current challenges, notably the
varying degree of hallucinations across languages and the high annotator
disagreement when labeling hallucination spans.

</details>


### [19] [Language Models as Quasi-Crystalline Thought: Structure, Constraint, and Emergence in Generative Systems](https://arxiv.org/abs/2504.11986)
*Jose Manuel Guevara-Vela*

Main category: cs.CL

TL;DR: 论文提出将大语言模型（LLMs）与准晶体类比，强调其通过局部约束生成全局连贯但非周期性重复的语言模式。


<details>
  <summary>Details</summary>
Motivation: 传统评估LLMs的标准（如预测准确性、事实性或对齐性）未能捕捉其最显著的特征，即生成内部共振的语言模式。通过准晶体的类比，重新定义LLMs的行为。

Method: 将LLMs视为准结构化语言的生成器，提出以约束传播和形式连贯性为核心的新评估与设计路径。

Result: LLMs的输出应关注其约束与连贯性的组织模式，而非仅内容本身。生成语言被视为一种涌现模式的空间。

Conclusion: LLMs既非完全随机也非严格规则驱动，而是由约束、共振和结构深度的逻辑定义。

Abstract: This essay proposes an analogy between large language models (LLMs) and
quasicrystals: systems that exhibit global coherence without periodic
repetition and that are generated through local constraints. While LLMs are
often evaluated in terms of predictive accuracy, factuality, or alignment, this
structural perspective suggests that their most characteristic behavior is the
production of internally resonant linguistic patterns. Just as quasicrystals
forced a redefinition of order in physical systems, viewing LLMs as generators
of quasi-structured language opens new paths for evaluation and design:
privileging propagation of constraint over token-level accuracy, and coherence
of form over fixed meaning. LLM outputs should be read not only for what they
say, but for the patterns of constraint and coherence that organize them. This
shift reframes generative language as a space of emergent patterning: LLMs are
neither fully random nor strictly rule-based, but defined by a logic of
constraint, resonance, and structural depth.

</details>


### [20] [Bayesian dynamic borrowing considering semantic similarity between outcomes for disproportionality analysis in FAERS](https://arxiv.org/abs/2504.12052)
*François Haguinet,Jeffery L Painter,Gregory E Powell,Andrea Callegaro,Andrew Bate*

Main category: cs.CL

TL;DR: 提出了一种基于贝叶斯动态借用（BDB）的方法，通过语义相似性度量（SSM）增强自发报告系统（SRS）中不良事件（AE）的定量识别。


<details>
  <summary>Details</summary>
Motivation: 解决当前不成比例分析（DPA）中刚性分层分组的局限性，通过语义相似性实现更灵活的信息共享。

Method: 将稳健的元分析预测（MAP）先验嵌入贝叶斯层次模型，并结合SSM对临床相似的MedDRA首选术语（PT）进行加权信息共享。

Result: IC SSM方法在灵敏度上优于传统IC和HLGT借用方法，能更早检测到信号，尽管F1分数和Youden指数略有下降。

Conclusion: SSM-informed贝叶斯借用是一种可扩展且上下文感知的DPA增强方法，未来需在其他数据集中验证并探索更多相似性度量。

Abstract: We present a Bayesian dynamic borrowing (BDB) approach to enhance the
quantitative identification of adverse events (AEs) in spontaneous reporting
systems (SRSs). The method embeds a robust meta-analytic predictive (MAP) prior
within a Bayesian hierarchical model and incorporates semantic similarity
measures (SSMs) to enable weighted information sharing from MedDRA Preferred
Terms (PTs) that are clinical similar to the target PT. This continuous
similarity-based borrowing addresses limitation of rigid hierarchical grouping
in current disproportionality analysis (DPA).
  Using data from the FDA Adverse Event Reporting System (FAERS) between 2015
and 2019, we evalute this approach - termed IC SSM - against standard
Information Component (IC) analysis and IC with borrowing at the MedDRA
high-level group term (HLGT) level. A novel references set (PVLens), derived
from FDA product label updates, enabled prospective evaluation of method
performance in identifying AEs prior to official labeling.
  The IC SSM approach demonstrated improved sensitivity compared to both
traditional IC and HLGT-based borrowing, with minor trade-offs in F1 scores and
Youden's index. IC SSM consistently identified more true positives and detected
signals over 5 months sooner than traditional IC. Despite a marginally lower
aggregate Youden's index, IC SSM showed higher performance in the early
post-marketing period, providing more stable and relevant estimates than
HLGT-based borrowing and traditional IC.
  These findings support the use of SSM-informed Bayesian borrowing as a
scalable and context-aware enhancement to traditional DPA methods. Future
research should validate this approach across other datasets and explore
additional similarity metrics and Bayesian inference strategies using
case-level data.

</details>


### [21] [Selective Demonstration Retrieval for Improved Implicit Hate Speech Detection](https://arxiv.org/abs/2504.12082)
*Yumin Kim,Hwanhee Lee*

Main category: cs.CL

TL;DR: 提出了一种无需微调的新方法，通过上下文学习和自适应检索演示来改进隐式仇恨言论检测，优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 隐式仇恨言论检测因依赖上下文和文化细微差别而具有挑战性，现有模型易受偏见和误分类影响，需改进检测精度和鲁棒性。

Method: 利用上下文学习，自适应检索相似群体或高相似度得分的演示，增强上下文理解。

Result: 实验表明该方法优于当前最先进技术。

Conclusion: 该方法有效提升了隐式仇恨言论检测的精度和鲁棒性，无需模型微调。

Abstract: Hate speech detection is a crucial area of research in natural language
processing, essential for ensuring online community safety. However, detecting
implicit hate speech, where harmful intent is conveyed in subtle or indirect
ways, remains a major challenge. Unlike explicit hate speech, implicit
expressions often depend on context, cultural subtleties, and hidden biases,
making them more challenging to identify consistently. Additionally, the
interpretation of such speech is influenced by external knowledge and
demographic biases, resulting in varied detection results across different
language models. Furthermore, Large Language Models often show heightened
sensitivity to toxic language and references to vulnerable groups, which can
lead to misclassifications. This over-sensitivity results in false positives
(incorrectly identifying harmless statements as hateful) and false negatives
(failing to detect genuinely harmful content). Addressing these issues requires
methods that not only improve detection precision but also reduce model biases
and enhance robustness. To address these challenges, we propose a novel method,
which utilizes in-context learning without requiring model fine-tuning. By
adaptively retrieving demonstrations that focus on similar groups or those with
the highest similarity scores, our approach enhances contextual comprehension.
Experimental results show that our method outperforms current state-of-the-art
techniques. Implementation details and code are available at TBD.

</details>


### [22] [Gauging Overprecision in LLMs: An Empirical Study](https://arxiv.org/abs/2504.12098)
*Adil Bahaj,Hamed Rahimi,Mohamed Chetouani,Mounir Ghogho*

Main category: cs.CL

TL;DR: 该论文研究了大型语言模型（LLMs）的过度自信问题，提出了一种基于"过度精确"的框架，通过生成、精炼和评估三个阶段分析LLMs在数值任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖LLMs自我报告置信度（verbalized confidence），但存在偏见和幻觉问题。受认知科学中"过度精确"启发，作者设计了一种新框架来量化LLMs的过度自信。

Method: 框架分为三个阶段：1) 生成阶段：通过提示LLMs生成带有预设置信度的数值区间答案；2) 精炼阶段：优化生成的答案；3) 评估阶段：分析LLMs的内部机制。

Result: 研究发现：1) LLMs在数值任务中校准性差；2) 区间长度与置信度无关，可能源于对置信度概念的理解不足或无法调整自信；3) 数值精度受任务、答案规模和提示技术影响；4) 精炼通常无法提升精度。

Conclusion: 该研究为LLMs的过度自信提供了新视角，并为"过度精确"问题建立了基线。

Abstract: Recently, overconfidence in large language models (LLMs) has garnered
considerable attention due to its fundamental importance in quantifying the
trustworthiness of LLM generation. However, existing approaches prompt the
\textit{black box LLMs} to produce their confidence (\textit{verbalized
confidence}), which can be subject to many biases and hallucinations. Inspired
by a different aspect of overconfidence in cognitive science called
\textit{overprecision}, we designed a framework for its study in black box
LLMs. This framework contains three main phases: 1) generation, 2) refinement
and 3) evaluation. In the generation phase we prompt the LLM to generate
answers to numerical questions in the form of intervals with a certain level of
confidence. This confidence level is imposed in the prompt and not required for
the LLM to generate as in previous approaches. We use various prompting
techniques and use the same prompt multiple times to gauge the effects of
randomness in the generation process. In the refinement phase, answers from the
previous phase are refined to generate better answers. The LLM answers are
evaluated and studied in the evaluation phase to understand its internal
workings. This study allowed us to gain various insights into LLM
overprecision: 1) LLMs are highly uncalibrated for numerical tasks 2)
{\color{blue}there is no correlation between the length of the interval and the
imposed confidence level, which can be symptomatic of a a) lack of
understanding of the concept of confidence or b) inability to adjust
self-confidence by following instructions}, {\color{blue}3)} LLM numerical
precision differs depending on the task, scale of answer and prompting
technique {\color{blue}4) Refinement of answers doesn't improve precision in
most cases}. We believe this study offers new perspectives on LLM
overconfidence and serves as a strong baseline for overprecision in LLMs.

</details>


### [23] [Entropy-Guided Watermarking for LLMs: A Test-Time Framework for Robust and Traceable Text Generation](https://arxiv.org/abs/2504.12108)
*Shizhan Cai,Liang Ding,Dacheng Tao*

Main category: cs.CL

TL;DR: 提出一种新型水印方案，通过累积水印熵阈值提升文本质量和检测能力，兼容现有采样函数，实验显示在多个LLM上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）快速发展引发内容可追溯性和潜在滥用的担忧，现有水印方案在文本质量和检测鲁棒性之间存在权衡。

Method: 引入累积水印熵阈值，兼容并泛化现有采样函数，提升适应性和检测能力。

Result: 在多个LLM上实验表明，方案显著优于现有方法，在MATH和GSM8K等数据集上提升超过80%，同时保持高检测准确率。

Conclusion: 提出的水印方案有效解决了文本质量与检测鲁棒性的权衡问题，为LLM内容追溯提供了更优解决方案。

Abstract: The rapid development of Large Language Models (LLMs) has intensified
concerns about content traceability and potential misuse. Existing watermarking
schemes for sampled text often face trade-offs between maintaining text quality
and ensuring robust detection against various attacks. To address these issues,
we propose a novel watermarking scheme that improves both detectability and
text quality by introducing a cumulative watermark entropy threshold. Our
approach is compatible with and generalizes existing sampling functions,
enhancing adaptability. Experimental results across multiple LLMs show that our
scheme significantly outperforms existing methods, achieving over 80\%
improvements on widely-used datasets, e.g., MATH and GSM8K, while maintaining
high detection accuracy.

</details>


### [24] [Multilingual Contextualization of Large Language Models for Document-Level Machine Translation](https://arxiv.org/abs/2504.12140)
*Miguel Moura Ramos,Patrick Fernandes,Sweta Agrawal,André F. T. Martins*

Main category: cs.CL

TL;DR: 通过针对高质量文档级数据的微调（DocBlocks），改进基于LLM的长文档翻译，支持多种翻译范式，提升翻译质量和推理速度。


<details>
  <summary>Details</summary>
Motivation: LLM在句子级机器翻译表现优异，但在文档级翻译中建模长距离依赖和跨句子/段落的语篇现象仍具挑战性。

Method: 提出通过DocBlocks对LLM进行微调，支持直接文档到文档和分块翻译，结合上下文指令。

Result: 实验表明，多翻译范式的结合提升了文档级翻译质量和推理速度。

Conclusion: 该方法有效解决了文档级翻译中的长距离依赖问题，同时保持句子级翻译性能。

Abstract: Large language models (LLMs) have demonstrated strong performance in
sentence-level machine translation, but scaling to document-level translation
remains challenging, particularly in modeling long-range dependencies and
discourse phenomena across sentences and paragraphs. In this work, we propose a
method to improve LLM-based long-document translation through targeted
fine-tuning on high-quality document-level data, which we curate and introduce
as DocBlocks. Our approach supports multiple translation paradigms, including
direct document-to-document and chunk-level translation, by integrating
instructions both with and without surrounding context. This enables models to
better capture cross-sentence dependencies while maintaining strong
sentence-level translation performance. Experimental results show that
incorporating multiple translation paradigms improves document-level
translation quality and inference speed compared to prompting and agent-based
methods.

</details>


### [25] [Poem Meter Classification of Recited Arabic Poetry: Integrating High-Resource Systems for a Low-Resource Task](https://arxiv.org/abs/2504.12172)
*Maged S. Al-Shaibani,Zaid Alyafeai,Irfan Ahmad*

Main category: cs.CL

TL;DR: 提出了一种用于识别阿拉伯诗歌韵律的先进框架，结合了两个高资源系统来处理低资源任务，并发布了基准数据集。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯诗歌韵律识别是一个复杂且需要专业知识的过程，尤其是对于朗诵诗歌。现有系统需要大量标注数据，本研究旨在解决这一问题。

Method: 整合了两个高资源系统，开发了一个框架来自动识别朗诵阿拉伯诗歌的韵律。

Result: 提出了一个先进的框架，并发布了基准数据集以支持未来研究。

Conclusion: 该框架为阿拉伯诗歌韵律识别提供了高效解决方案，并通过基准数据集促进了相关研究的发展。

Abstract: Arabic poetry is an essential and integral part of Arabic language and
culture. It has been used by the Arabs to spot lights on their major events
such as depicting brutal battles and conflicts. They also used it, as in many
other languages, for various purposes such as romance, pride, lamentation, etc.
Arabic poetry has received major attention from linguistics over the decades.
One of the main characteristics of Arabic poetry is its special rhythmic
structure as opposed to prose. This structure is referred to as a meter.
Meters, along with other poetic characteristics, are intensively studied in an
Arabic linguistic field called "\textit{Aroud}". Identifying these meters for a
verse is a lengthy and complicated process. It also requires technical
knowledge in \textit{Aruod}. For recited poetry, it adds an extra layer of
processing. Developing systems for automatic identification of poem meters for
recited poems need large amounts of labelled data. In this study, we propose a
state-of-the-art framework to identify the poem meters of recited Arabic
poetry, where we integrate two separate high-resource systems to perform the
low-resource task. To ensure generalization of our proposed architecture, we
publish a benchmark for this task for future research.

</details>


### [26] [Mapping Controversies Using Artificial Intelligence: An Analysis of the Hamas-Israel Conflict on YouTube](https://arxiv.org/abs/2504.12177)
*Victor Manuel Hernandez Lopez,Jaime E. Cuellar*

Main category: cs.CL

TL;DR: 该研究通过分析25万条西班牙语YouTube评论，结合STS和NLP技术，探讨了哈马斯-以色列争议中的公众立场变化，发现亲巴勒斯坦评论占多数，但亲以色列和反巴勒斯坦评论获更多点赞。


<details>
  <summary>Details</summary>
Motivation: 研究旨在揭示媒体议程设置对公众立场的影响，并展示跨学科方法在争议分析中的价值。

Method: 采用STS视角和BERT模型对评论分类，结合议程设置理论分析媒体影响。

Result: 亲巴勒斯坦评论数量最多，但亲以色列和反巴勒斯坦评论更受欢迎；公众立场随媒体报道从亲巴转向对以色列更批判。

Conclusion: 结合社会科学与计算工具能更有效分析复杂争议，为公众意见研究提供新方法。

Abstract: This article analyzes the Hamas-Israel controversy through 253,925
Spanish-language YouTube comments posted between October 2023 and January 2024,
following the October 7 attack that escalated the conflict. Adopting an
interdisciplinary approach, the study combines the analysis of controversies
from Science and Technology Studies (STS) with advanced computational
methodologies, specifically Natural Language Processing (NLP) using the BERT
(Bidirectional Encoder Representations from Transformers) model. Using this
approach, the comments were automatically classified into seven categories,
reflecting pro-Palestinian, pro-Israeli, anti- Palestinian, anti-Israeli
positions, among others. The results show a predominance of pro- Palestinian
comments, although pro-Israeli and anti-Palestinian comments received more
"likes." This study also applies the agenda-setting theory to demonstrate how
media coverage significantly influences public perception, observing a notable
shift in public opinion, transitioning from a pro- Palestinian stance to a more
critical position towards Israel. This work highlights the importance of
combining social science perspectives with technological tools in the analysis
of controversies, presenting a methodological innovation by integrating
computational analysis with critical social theories to address complex public
opinion phenomena and media narratives.

</details>


### [27] [Trusting CHATGPT: how minor tweaks in the prompts lead to major differences in sentiment classification](https://arxiv.org/abs/2504.12180)
*Jaime E. Cuellar,Oscar Moreno-Martinez,Paula Sofia Torres-Rodriguez,Jaime Andres Pavlich-Mariscal,Andres Felipe Mican-Castiblanco,Juan Guillermo Torres-Hurtado*

Main category: cs.CL

TL;DR: 研究发现，GPT-4o mini在情感极性分类任务中对提示的微小变化敏感，导致结果不一致，挑战了大型语言模型的稳健性和可信度。


<details>
  <summary>Details</summary>
Motivation: 探讨复杂预测模型（如ChatGPT）的可信度，测试提示结构变化是否影响GPT-4o mini的情感分类结果。

Method: 使用10种略有不同的提示对10万条西班牙语评论进行情感分类，通过探索性和验证性分析比较结果。

Result: 微小提示变化（如词汇、句法或模态）导致分类不一致，统计测试显示多数比较存在显著差异。

Conclusion: 大型语言模型在分类任务中易受提示变化影响，可信度依赖于技术性能及社会制度背景。

Abstract: One fundamental question for the social sciences today is: how much can we
trust highly complex predictive models like ChatGPT? This study tests the
hypothesis that subtle changes in the structure of prompts do not produce
significant variations in the classification results of sentiment polarity
analysis generated by the Large Language Model GPT-4o mini. Using a dataset of
100.000 comments in Spanish on four Latin American presidents, the model
classified the comments as positive, negative, or neutral on 10 occasions,
varying the prompts slightly each time. The experimental methodology included
exploratory and confirmatory analyses to identify significant discrepancies
among classifications.
  The results reveal that even minor modifications to prompts such as lexical,
syntactic, or modal changes, or even their lack of structure impact the
classifications. In certain cases, the model produced inconsistent responses,
such as mixing categories, providing unsolicited explanations, or using
languages other than Spanish. Statistical analysis using Chi-square tests
confirmed significant differences in most comparisons between prompts, except
in one case where linguistic structures were highly similar.
  These findings challenge the robustness and trust of Large Language Models
for classification tasks, highlighting their vulnerability to variations in
instructions. Moreover, it was evident that the lack of structured grammar in
prompts increases the frequency of hallucinations. The discussion underscores
that trust in Large Language Models is based not only on technical performance
but also on the social and institutional relationships underpinning their use.

</details>


### [28] [SALAD: Improving Robustness and Generalization through Contrastive Learning with Structure-Aware and LLM-Driven Augmented Data](https://arxiv.org/abs/2504.12185)
*Suyoung Bae,Hyojun Kim,YunSeok Choi,Jee-Hyong Lee*

Main category: cs.CL

TL;DR: SALAD方法通过生成结构感知和反事实增强数据，提升预训练语言模型在NLP任务中的鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 预训练语言模型在微调时容易产生虚假相关性，影响性能，尤其是在处理分布外数据时。

Method: SALAD利用标记方法生成结构感知正样本，并利用大语言模型生成反事实负样本，通过对比学习减少对虚假相关性的依赖。

Result: 在情感分类、性别歧视检测和自然语言推理任务中，SALAD显著提升了模型的鲁棒性和泛化能力。

Conclusion: SALAD方法有效解决了虚假相关性问题，提升了模型在多种任务和场景下的性能。

Abstract: In various natural language processing (NLP) tasks, fine-tuning Pre-trained
Language Models (PLMs) often leads to the issue of spurious correlations, which
negatively impacts performance, particularly when dealing with
out-of-distribution data. To address this problem, we propose SALAD}(Structure
Aware and LLM-driven Augmented Data), a novel approach designed to enhance
model robustness and generalization by generating structure-aware and
counterfactually augmented data for contrastive learning. Our method leverages
a tagging-based approach to generate structure-aware positive samples and
utilizes large language models (LLMs) to generate counterfactual negative
samples with diverse sentence patterns. By applying contrastive learning, SALAD
enables the model to focus on learning the structural relationships between key
sentence components while minimizing reliance on spurious correlations. We
validate our approach through experiments on three tasks: Sentiment
Classification, Sexism Detection, and Natural Language Inference. The results
demonstrate that SALAD not only improves model robustness and performance
across different environments but also enhances generalization to
out-of-distribution datasets and cross-domain scenarios.

</details>


### [29] [What Do Large Language Models Know? Tacit Knowledge as a Potential Causal-Explanatory Structure](https://arxiv.org/abs/2504.12187)
*Céline Budding*

Main category: cs.CL

TL;DR: LLMs可能通过特定架构特征获得隐性知识，满足语义、句法和因果系统性的约束。


<details>
  <summary>Details</summary>
Motivation: 探讨LLMs是否真正掌握语言知识，以及如何定义其知识。

Method: 分析LLMs的架构特征，验证其是否符合Martin Davies提出的隐性知识标准。

Result: LLMs的架构满足隐性知识的语义、句法和因果系统性要求。

Conclusion: 隐性知识可作为描述、解释和干预LLMs行为的概念框架。

Abstract: It is sometimes assumed that Large Language Models (LLMs) know language, or
for example that they know that Paris is the capital of France. But what -- if
anything -- do LLMs actually know? In this paper, I argue that LLMs can acquire
tacit knowledge as defined by Martin Davies (1990). Whereas Davies himself
denies that neural networks can acquire tacit knowledge, I demonstrate that
certain architectural features of LLMs satisfy the constraints of semantic
description, syntactic structure, and causal systematicity. Thus, tacit
knowledge may serve as a conceptual framework for describing, explaining, and
intervening on LLMs and their behavior.

</details>


### [30] [d1: Scaling Reasoning in Diffusion Large Language Models via Reinforcement Learning](https://arxiv.org/abs/2504.12216)
*Siyan Zhao,Devaansh Gupta,Qinqing Zheng,Aditya Grover*

Main category: cs.CL

TL;DR: 论文提出d1框架，通过监督微调和强化学习将预训练的扩散大语言模型（dLLMs）转化为推理模型，显著提升了其数学和逻辑推理能力。


<details>
  <summary>Details</summary>
Motivation: 尽管扩散大语言模型在语言建模上表现优异，但其推理能力是否与自回归模型相当尚不明确，因此需要探索如何提升dLLMs的推理能力。

Method: 结合监督微调（SFT）和强化学习（RL），提出d1框架，包括掩码SFT技术和新型无评论家的RL算法diffu-GRPO。

Result: 实验表明，d1显著提升了dLLMs在数学和逻辑推理任务上的性能，优于其他后训练方法。

Conclusion: d1框架有效提升了扩散大语言模型的推理能力，为dLLMs的应用提供了新方向。

Abstract: Recent large language models (LLMs) have demonstrated strong reasoning
capabilities that benefits from online reinforcement learning (RL). These
capabilities have primarily been demonstrated within the left-to-right
autoregressive (AR) generation paradigm. In contrast, non-autoregressive
paradigms based on diffusion generate text in a coarse-to-fine manner. Although
recent diffusion-based large language models (dLLMs) have achieved competitive
language modeling performance compared to their AR counterparts, it remains
unclear if dLLMs can also leverage recent advances in LLM reasoning. To this
end, we propose d1, a framework to adapt pre-trained masked dLLMs into
reasoning models via a combination of supervised finetuning (SFT) and RL.
Specifically, we develop and extend techniques to improve reasoning in
pretrained dLLMs: (a) we utilize a masked SFT technique to distill knowledge
and instill self-improvement behavior directly from existing datasets, and (b)
we introduce a novel critic-free, policy-gradient based RL algorithm called
diffu-GRPO. Through empirical studies, we investigate the performance of
different post-training recipes on multiple mathematical and logical reasoning
benchmarks. We find that d1 yields the best performance and significantly
improves performance of a state-of-the-art dLLM.

</details>


### [31] [BitNet b1.58 2B4T Technical Report](https://arxiv.org/abs/2504.12285)
*Shuming Ma,Hongyu Wang,Shaohan Huang,Xingxing Zhang,Ying Hu,Ting Song,Yan Xia,Furu Wei*

Main category: cs.CL

TL;DR: BitNet b1.58 2B4T是首个开源的1-bit大型语言模型，参数规模20亿，训练数据4万亿token，性能媲美同类全精度模型，计算效率显著提升。


<details>
  <summary>Details</summary>
Motivation: 开发高效、低资源消耗的大型语言模型，推动开源社区的研究和应用。

Method: 训练20亿参数的1-bit模型，使用4万亿token数据，并在多领域基准测试中评估。

Result: 性能与同类全精度模型相当，计算效率显著提升，内存占用、能耗和解码延迟大幅降低。

Conclusion: BitNet b1.58 2B4T为高效LLM提供了新方向，开源模型和实现促进进一步研究。

Abstract: We introduce BitNet b1.58 2B4T, the first open-source, native 1-bit Large
Language Model (LLM) at the 2-billion parameter scale. Trained on a corpus of 4
trillion tokens, the model has been rigorously evaluated across benchmarks
covering language understanding, mathematical reasoning, coding proficiency,
and conversational ability. Our results demonstrate that BitNet b1.58 2B4T
achieves performance on par with leading open-weight, full-precision LLMs of
similar size, while offering significant advantages in computational
efficiency, including substantially reduced memory footprint, energy
consumption, and decoding latency. To facilitate further research and adoption,
the model weights are released via Hugging Face along with open-source
inference implementations for both GPU and CPU architectures.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [32] [From Conceptual Data Models to Multimodal Representation](https://arxiv.org/abs/2504.11459)
*Peter Stockinger*

Main category: cs.AI

TL;DR: 本文探讨信息设计的两大实践：定义文本数据的语义及其视觉或多模态表达，强调语义建模的重要性，并展示其在多模态可视化中的应用。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过结构符号学和语言学方法，区分语义内容与图形表达方式，以优化复杂文本数据的分析和呈现。

Method: 采用语义建模（如概念网络或图）和动态模型设计，结合叙事的多种格式转换，应用于实际工作环境（如OKAPI平台）。

Result: 提出了灵活、可互操作的模型，支持复杂语料的分析和发布，并通过视觉叙事和文档重构实现内容的多场景适配。

Conclusion: 信息设计方法提升了数字数据的协作使用和智能传播，为多模态表达和语义建模提供了新思路。

Abstract: 1) Introduction and Conceptual Framework: This document explores the concept
of information design by dividing it into two major practices: defining the
meaning of a corpus of textual data and its visual or multimodal
representation. It draws on expertise in enriching textual corpora,
particularly audiovisual ones, and transforming them into multiple narrative
formats. The text highlights a crucial distinction between the semantic content
of a domain and the modalities of its graphic expression, illustrating this
approach with concepts rooted in structural semiotics and linguistics
traditions.
  2) Modeling and Conceptual Design: The article emphasizes the importance of
semantic modeling, often achieved through conceptual networks or graphs. These
tools enable the structuring of knowledge within a domain by accounting for
relationships between concepts, contexts of use, and specific objectives.
Stockinger also highlights the constraints and challenges involved in creating
dynamic and adaptable models, integrating elements such as thesauri or
interoperable ontologies to facilitate the analysis and publication of complex
corpora.
  3) Applications and Multimodal Visualization: The text concludes by examining
the practical application of these models in work environments like OKAPI,
developed to analyze, publish, and reuse audiovisual data. It also discusses
innovative approaches such as visual storytelling and document reengineering,
which involve transforming existing content into new resources tailored to
various contexts. These methods emphasize interoperability, flexibility, and
the intelligence of communication systems, paving the way for richer and more
collaborative use of digital data. The content of this document was presented
during the "Semiotics of Information Design" Day organized by Anne
Beyaert-Geslin of the University of Bordeaux Montaigne (MICA laboratory) on
June 21, 2018, in Bordeaux.

</details>


### [33] [Enhancing Autonomous Driving Systems with On-Board Deployed Large Language Models](https://arxiv.org/abs/2504.11514)
*Nicolas Baumann,Cheng Hu,Paviththiren Sivasothilingam,Haotong Qin,Lei Xie,Michele Magno,Luca Benini*

Main category: cs.AI

TL;DR: 提出了一种结合MPC和LLM的混合架构，用于增强自动驾驶决策和控制适应性，同时提升计算效率和实时部署能力。


<details>
  <summary>Details</summary>
Motivation: 解决神经网络在边缘案例中的局限性，结合人类直觉的知识驱动方法，提升自动驾驶系统的适应性和安全性。

Method: 采用MPCxLLM和DecisionxLLM模块，结合RAG、LoRA微调和量化技术，实现高效本地部署。

Result: 实验显示推理准确性提升10.45%，控制适应性提高52.2%，计算效率提升10.5倍。

Conclusion: 该框架成功结合了知识驱动与数据驱动方法，为自适应自动驾驶系统提供了实用方案。

Abstract: Neural Networks (NNs) trained through supervised learning struggle with
managing edge-case scenarios common in real-world driving due to the
intractability of exhaustive datasets covering all edge-cases, making
knowledge-driven approaches, akin to how humans intuitively detect unexpected
driving behavior, a suitable complement to data-driven methods. This work
proposes a hybrid architecture combining low-level Model Predictive Controller
(MPC) with locally deployed Large Language Models (LLMs) to enhance
decision-making and Human Machine Interaction (HMI). The DecisionxLLM module
evaluates robotic state information against natural language instructions to
ensure adherence to desired driving behavior. The MPCxLLM module then adjusts
MPC parameters based on LLM-generated insights, achieving control adaptability
while preserving the safety and constraint guarantees of traditional MPC
systems. Further, to enable efficient on-board deployment and to eliminate
dependency on cloud connectivity, we shift processing to the on-board computing
platform: We propose an approach that exploits Retrieval Augmented Generation
(RAG), Low Rank Adaptation (LoRA) fine-tuning, and quantization. Experimental
results demonstrate that these enhancements yield significant improvements in
reasoning accuracy by up to 10.45%, control adaptability by as much as 52.2%,
and up to 10.5x increase in computational efficiency (tokens/s), validating the
proposed framework's practicality for real-time deployment even on down-scaled
robotic platforms. This work bridges high-level decision-making with low-level
control adaptability, offering a synergistic framework for knowledge-driven and
adaptive Autonomous Driving Systems (ADS).

</details>


### [34] [HypoBench: Towards Systematic and Principled Benchmarking for Hypothesis Generation](https://arxiv.org/abs/2504.11524)
*Haokun Liu,Sicong Huang,Jingyu Hu,Yangqiaoyu Zhou,Chenhao Tan*

Main category: cs.AI

TL;DR: HypoBench是一个新基准，用于评估大语言模型（LLMs）和假设生成方法，涵盖实用性、泛化性和假设发现率。结果显示现有方法能发现有效新颖模式，但仍有改进空间。


<details>
  <summary>Details</summary>
Motivation: 解决假设生成中的基本问题：什么是好的假设，如何系统评估生成方法。

Method: 引入HypoBench，包含7个真实任务和5个合成任务（194个数据集），评估4种LLMs和6种假设生成方法。

Result: 现有方法能发现有效新颖模式，但在合成任务中表现不佳（仅恢复38.8%的真实假设）。

Conclusion: HypoBench是改进科学发现AI系统的宝贵资源，假设生成仍有挑战。

Abstract: There is growing interest in hypothesis generation with large language models
(LLMs). However, fundamental questions remain: what makes a good hypothesis,
and how can we systematically evaluate methods for hypothesis generation? To
address this, we introduce HypoBench, a novel benchmark designed to evaluate
LLMs and hypothesis generation methods across multiple aspects, including
practical utility, generalizability, and hypothesis discovery rate. HypoBench
includes 7 real-world tasks and 5 synthetic tasks with 194 distinct datasets.
We evaluate four state-of-the-art LLMs combined with six existing
hypothesis-generation methods. Overall, our results suggest that existing
methods are capable of discovering valid and novel patterns in the data.
However, the results from synthetic datasets indicate that there is still
significant room for improvement, as current hypothesis generation methods do
not fully uncover all relevant or meaningful patterns. Specifically, in
synthetic settings, as task difficulty increases, performance significantly
drops, with best models and methods only recovering 38.8% of the ground-truth
hypotheses. These findings highlight challenges in hypothesis generation and
demonstrate that HypoBench serves as a valuable resource for improving AI
systems designed to assist scientific discovery.

</details>


### [35] [REAL: Benchmarking Autonomous Agents on Deterministic Simulations of Real Websites](https://arxiv.org/abs/2504.11543)
*Divyansh Garg,Shaun VanWeelden,Diego Caples,Andis Draguns,Nikil Ravi,Pranav Putta,Naman Garg,Tomas Abraham,Michael Lara,Federico Lopez,James Liu,Atharva Gundawar,Prannay Hebbar,Youngchul Joo,Charles London,Christian Schroeder de Witt,Sumeet Motwani*

Main category: cs.AI

TL;DR: REAL是一个用于多轮代理评估的基准和框架，基于真实网站的确定性模拟，包含11个高保真网站和112个任务，评估代理的信息检索和状态变更能力。


<details>
  <summary>Details</summary>
Motivation: 为评估代理在真实网站上的复杂交互能力，提供一个安全、可复现的测试环境。

Method: 结合网站状态检查和基于LLM的评分，支持开源和专有代理系统，通过浏览器环境进行黑盒测试。

Result: 前沿语言模型在REAL上的成功率最高仅41%，揭示了自主导航和任务完成能力的不足。

Conclusion: REAL框架支持新任务集成、可复现评估和训练数据生成，为代理能力评估提供了实用工具。

Abstract: We introduce REAL, a benchmark and framework for multi-turn agent evaluations
on deterministic simulations of real-world websites. REAL comprises
high-fidelity, deterministic replicas of 11 widely-used websites across domains
such as e-commerce, travel, communication, and professional networking. We also
release a benchmark consisting of 112 practical tasks that mirror everyday
complex user interactions requiring both accurate information retrieval and
state-changing actions. All interactions occur within this fully controlled
setting, eliminating safety risks and enabling robust, reproducible evaluation
of agent capability and reliability. Our novel evaluation framework combines
programmatic checks of website state for action-based tasks with rubric-guided
LLM-based judgments for information retrieval. The framework supports both
open-source and proprietary agent systems through a flexible evaluation harness
that accommodates black-box commands within browser environments, allowing
research labs to test agentic systems without modification. Our empirical
results show that frontier language models achieve at most a 41% success rate
on REAL, highlighting critical gaps in autonomous web navigation and task
completion capabilities. Our framework supports easy integration of new tasks,
reproducible evaluation, and scalable data generation for training web agents.
The websites, framework, and leaderboard are available at https://realevals.xyz
and https://github.com/agi-inc/REAL.

</details>


### [36] [NodeRAG: Structuring Graph-based RAG with Heterogeneous Nodes](https://arxiv.org/abs/2504.11544)
*Tianyang Xu,Haojie Zheng,Chengze Li,Haoxiang Chen,Yixin Liu,Ruoxi Chen,Lichao Sun*

Main category: cs.AI

TL;DR: NodeRAG提出了一种基于异构图的RAG框架，优化了图结构设计，提升了检索增强生成的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于图的RAG方法未充分关注图结构设计，导致性能下降和工作流不一致。

Method: NodeRAG引入异构图结构，无缝整合图算法到RAG流程中，并与LLM能力对齐。

Result: 实验表明NodeRAG在索引时间、查询时间、存储效率和问答性能上优于GraphRAG和LightRAG。

Conclusion: NodeRAG通过优化图结构设计，显著提升了RAG的性能和效率。

Abstract: Retrieval-augmented generation (RAG) empowers large language models to access
external and private corpus, enabling factually consistent responses in
specific domains. By exploiting the inherent structure of the corpus,
graph-based RAG methods further enrich this process by building a knowledge
graph index and leveraging the structural nature of graphs. However, current
graph-based RAG approaches seldom prioritize the design of graph structures.
Inadequately designed graph not only impede the seamless integration of diverse
graph algorithms but also result in workflow inconsistencies and degraded
performance. To further unleash the potential of graph for RAG, we propose
NodeRAG, a graph-centric framework introducing heterogeneous graph structures
that enable the seamless and holistic integration of graph-based methodologies
into the RAG workflow. By aligning closely with the capabilities of LLMs, this
framework ensures a fully cohesive and efficient end-to-end process. Through
extensive experiments, we demonstrate that NodeRAG exhibits performance
advantages over previous methods, including GraphRAG and LightRAG, not only in
indexing time, query time, and storage efficiency but also in delivering
superior question-answering performance on multi-hop benchmarks and open-ended
head-to-head evaluations with minimal retrieval tokens. Our GitHub repository
could be seen at https://github.com/Terry-Xu-666/NodeRAG.

</details>


### [37] [Probabilistic causal graphs as categorical data synthesizers: Do they do better than Gaussian Copulas and Conditional Tabular GANs?](https://arxiv.org/abs/2504.11547)
*Olha Shaposhnyk,Noor Abid,Mouri Zakir,Svetlana Yanushkevich*

Main category: cs.AI

TL;DR: 研究提出了一种基于因果图模型（SEM和BN）的高质量合成分类数据生成方法，优于其他技术如高斯Copula和CTGAN。


<details>
  <summary>Details</summary>
Motivation: 生成合成数据旨在为模型训练提供多样化数据，同时保护隐私并捕捉数据间关系。

Method: 采用结构方程模型（SEM）和贝叶斯网络（BN）建模因果关系和联合分布。

Result: BN模型在统计指标（如TVD）上表现最佳，高斯Copula次之，CTGAN中等。

Conclusion: SEM-based BN方法能生成统计和关系有效的合成数据，适用于敏感数据研究。

Abstract: This study investigates the generation of high-quality synthetic categorical
data, such as survey data, using causal graph models. Generating synthetic data
aims not only to create a variety of data for training the models but also to
preserve privacy while capturing relationships between the data. The research
employs Structural Equation Modeling (SEM) followed by Bayesian Networks (BN).
We used the categorical data that are based on the survey of accessibility to
services for people with disabilities. We created both SEM and BN models to
represent causal relationships and to capture joint distributions between
variables. In our case studies, such variables include, in particular,
demographics, types of disability, types of accessibility barriers and
frequencies of encountering those barriers.
  The study compared the SEM-based BN method with alternative approaches,
including the probabilistic Gaussian copula technique and generative models
like the Conditional Tabular Generative Adversarial Network (CTGAN). The
proposed method outperformed others in statistical metrics, including the
Chi-square test, Kullback-Leibler divergence, and Total Variation Distance
(TVD). In particular, the BN model demonstrated superior performance, achieving
the highest TVD, indicating alignment with the original data. The Gaussian
Copula ranked second, while CTGAN exhibited moderate performance. These
analyses confirmed the ability of the SEM-based BN to produce synthetic data
that maintain statistical and relational validity while maintaining
confidentiality. This approach is particularly beneficial for research on
sensitive data, such as accessibility and disability studies.

</details>


### [38] [GraphicBench: A Planning Benchmark for Graphic Design with Language Agents](https://arxiv.org/abs/2504.11571)
*Dayeon Ki,Tianyi Zhou,Marine Carpuat,Gang Wu,Puneet Mathur,Viswanathan Swaminathan*

Main category: cs.AI

TL;DR: 论文介绍了GraphicBench基准和GraphicTown框架，探索LLM代理在开放目标创意设计任务中的能力，发现其在生成工作流时表现良好，但在执行中面临空间关系推理、全局依赖协调和动作选择等挑战。


<details>
  <summary>Details</summary>
Motivation: 探索LLM代理在开放目标创意设计任务中的能力，填补现有研究在未明确目标任务中的空白。

Method: 提出GraphicBench基准（含1,079个用户查询和输入图像）和GraphicTown框架（含三个设计专家和46种工具），并在六种LLM上进行实验。

Result: LLM能生成结合显式和隐式约束的工作流，但执行成功率低，主要因空间关系推理、全局依赖协调和动作选择问题。

Conclusion: GraphicBench是推动LLM代理在创意设计任务中规划和执行能力的有价值测试平台。

Abstract: Large Language Model (LLM)-powered agents have unlocked new possibilities for
automating human tasks. While prior work has focused on well-defined tasks with
specified goals, the capabilities of agents in creative design tasks with
open-ended goals remain underexplored. We introduce GraphicBench, a new
planning benchmark for graphic design that covers 1,079 user queries and input
images across four design types. We further present GraphicTown, an LLM agent
framework with three design experts and 46 actions (tools) to choose from for
executing each step of the planned workflows in web environments. Experiments
with six LLMs demonstrate their ability to generate workflows that integrate
both explicit design constraints from user queries and implicit commonsense
constraints. However, these workflows often do not lead to successful execution
outcomes, primarily due to challenges in: (1) reasoning about spatial
relationships, (2) coordinating global dependencies across experts, and (3)
retrieving the most appropriate action per step. We envision GraphicBench as a
challenging yet valuable testbed for advancing LLM-agent planning and execution
in creative design tasks.

</details>


### [39] [Steering Prosocial AI Agents: Computational Basis of LLM's Decision Making in Social Simulation](https://arxiv.org/abs/2504.11671)
*Ji Ma*

Main category: cs.AI

TL;DR: 研究探讨了大型语言模型（LLM）在决策任务中如何受角色和情境影响，提出了一种通过操纵内部表征来调节模型行为的方法。


<details>
  <summary>Details</summary>
Motivation: LLM在社会科学和应用场景中作为决策代理时，其行为如何受角色和情境影响尚不明确，需要探索。

Method: 在独裁者游戏中，提取并操纵LLM的内部表征（如性别变量），以量化其对决策的影响。

Result: 通过操纵内部表征，可以显著改变模型决策中变量（如性别）的作用。

Conclusion: 该方法为研究和管理LLM中社会概念的编码提供了原则性途径，对模型对齐、去偏和社会模拟设计有重要意义。

Abstract: Large language models (LLMs) increasingly serve as human-like decision-making
agents in social science and applied settings. These LLM-agents are typically
assigned human-like characters and placed in real-life contexts. However, how
these characters and contexts shape an LLM's behavior remains underexplored.
This study proposes and tests methods for probing, quantifying, and modifying
an LLM's internal representations in a Dictator Game -- a classic behavioral
experiment on fairness and prosocial behavior. We extract ``vectors of variable
variations'' (e.g., ``male'' to ``female'') from the LLM's internal state.
Manipulating these vectors during the model's inference can substantially alter
how those variables relate to the model's decision-making. This approach offers
a principled way to study and regulate how social concepts can be encoded and
engineered within transformer-based models, with implications for alignment,
debiasing, and designing AI agents for social simulations in both academic and
commercial applications.

</details>


### [40] [A Library of LLM Intrinsics for Retrieval-Augmented Generation](https://arxiv.org/abs/2504.11704)
*Marina Danilevsky,Kristjan Greenewald,Chulaka Gunasekara,Maeda Hanafi,Lihong He,Yannis Katsis,Krishnateja Killamsetty,Yatin Nandwani,Lucian Popa,Dinesh Raghu,Frederick Reiss,Vraj Shah,Khoi-Nguyen Tran,Huaiyu Zhu,Luis Lastras*

Main category: cs.AI

TL;DR: 论文提出了一种名为LLM Intrinsics的库，旨在为大型语言模型（LLM）开发者社区提供类似软件库的协作模式，特别针对检索增强生成（RAG）应用。


<details>
  <summary>Details</summary>
Motivation: 当前LLM开发者社区缺乏统一的API标准，阻碍了大规模协作，尤其是在RAG应用中。

Method: 通过定义LLM Intrinsics（可通过稳定API调用的能力），并以LoRA适配器和vLLM接口的形式实现。

Result: 发布了包含文档和代码的LLM Intrinsics库，支持RAG应用开发。

Conclusion: LLM Intrinsics为LLM协作提供了标准化接口，有望推动社区发展。

Abstract: In the developer community for large language models (LLMs), there is not yet
a clean pattern analogous to a software library, to support very large scale
collaboration. Even for the commonplace use case of Retrieval-Augmented
Generation (RAG), it is not currently possible to write a RAG application
against a well-defined set of APIs that are agreed upon by different LLM
providers. Inspired by the idea of compiler intrinsics, we propose some
elements of such a concept through introducing a library of LLM Intrinsics for
RAG. An LLM intrinsic is defined as a capability that can be invoked through a
well-defined API that is reasonably stable and independent of how the LLM
intrinsic itself is implemented. The intrinsics in our library are released as
LoRA adapters on HuggingFace, and through a software interface with clear
structured input/output characteristics on top of vLLM as an inference
platform, accompanied in both places with documentation and code. This article
describes the intended usage, training details, and evaluations for each
intrinsic, as well as compositions of multiple intrinsics.

</details>


### [41] [Climbing the Ladder of Reasoning: What LLMs Can-and Still Can't-Solve after SFT?](https://arxiv.org/abs/2504.11741)
*Yiyou Sun,Georgia Zhou,Hao Wang,Dacheng Li,Nouha Dziri,Dawn Song*

Main category: cs.AI

TL;DR: 论文分析了监督微调（SFT）对语言模型数学推理能力的影响，发现不同难度问题需要不同推理风格，并揭示了数据集规模对性能提升的重要性。


<details>
  <summary>Details</summary>
Motivation: 理解监督微调如何具体提升语言模型的数学推理能力，以及不同难度问题的解决需求。

Method: 通过分析AIME24数据集，将问题分为四个难度层级，并研究模型在不同层级的推理表现。

Result: 发现从低难度到中难度需要特定推理风格（R1），高难度问题因推理链错误而准确率停滞，极高难度问题则需非常规解决能力。

Conclusion: 研究为提升语言模型数学推理能力提供了清晰路线，强调数据集规模的重要性。

Abstract: Recent supervised fine-tuning (SFT) approaches have significantly improved
language models' performance on mathematical reasoning tasks, even when models
are trained at a small scale. However, the specific capabilities enhanced
through such fine-tuning remain poorly understood. In this paper, we conduct a
detailed analysis of model performance on the AIME24 dataset to understand how
reasoning capabilities evolve. We discover a ladder-like structure in problem
difficulty, categorize questions into four tiers (Easy, Medium, Hard, and
Extremely Hard (Exh)), and identify the specific requirements for advancing
between tiers. We find that progression from Easy to Medium tier requires
adopting an R1 reasoning style with minimal SFT (500-1K instances), while
Hard-level questions suffer from frequent model's errors at each step of the
reasoning chain, with accuracy plateauing at around 65% despite logarithmic
scaling. Exh-level questions present a fundamentally different challenge; they
require unconventional problem-solving skills that current models uniformly
struggle with. Additional findings reveal that carefully curated small-scale
datasets offer limited advantage-scaling dataset size proves far more
effective. Our analysis provides a clearer roadmap for advancing language model
capabilities in mathematical reasoning.

</details>


### [42] [Shared Disk KV Cache Management for Efficient Multi-Instance Inference in RAG-Powered LLMs](https://arxiv.org/abs/2504.11765)
*Hyungwoo Lee,Kihyun Kim,Jinwoo Kim,Jungmin So,Myung-Hoon Cha,Hong-Yeon Kim,James J. Kim,Youngjae Kim*

Main category: cs.AI

TL;DR: 本文提出了一种基于磁盘的KV缓存方法（Shared RAG-DCache），以减少检索增强生成（RAG）中因输入令牌增加导致的计算开销和延迟。


<details>
  <summary>Details</summary>
Motivation: 随着输入上下文长度和模型规模的增加，大型语言模型（LLMs）的推理延迟问题日益突出，尤其是在RAG技术中，外部知识的引入进一步加剧了这一问题。

Method: 通过磁盘KV缓存减轻预填充阶段的计算负担，并设计了一个多实例LLM RAG服务环境下的共享KV缓存管理系统。

Result: 在2 GPU和1 CPU的单主机实验中，Shared RAG-DCache实现了15~71%的吞吐量提升和12~65%的延迟降低。

Conclusion: Shared RAG-DCache通过利用文档局部性和队列延迟，显著提升了RAG服务的性能和效率。

Abstract: Recent large language models (LLMs) face increasing inference latency as
input context length and model size continue to grow. In particular, the
retrieval-augmented generation (RAG) technique, which enhances LLM responses by
incorporating external knowledge, exacerbates this issue by significantly
increasing the number of input tokens. This expansion in token length leads to
a substantial rise in computational overhead, particularly during the prefill
stage, resulting in prolonged time-to-first-token (TTFT). To address this
issue, this paper proposes a method to reduce TTFT by leveraging a disk-based
key-value (KV) cache to lessen the computational burden during the prefill
stage. We also introduce a disk-based shared KV cache management system, called
Shared RAG-DCache, for multi-instance LLM RAG service environments. This
system, together with an optimal system configuration, improves both throughput
and latency under given resource constraints. Shared RAG-DCache exploits the
locality of documents related to user queries in RAG, as well as the queueing
delay in LLM inference services. It proactively generates and stores disk KV
caches for query-related documents and shares them across multiple LLM
instances to enhance inference performance. In experiments on a single host
equipped with 2 GPUs and 1 CPU, Shared RAG-DCache achieved a 15~71% increase in
throughput and up to a 12~65% reduction in latency, depending on the resource
configuration.

</details>


### [43] [Large Language Models for Drug Overdose Prediction from Longitudinal Medical Records](https://arxiv.org/abs/2504.11792)
*Md Sultan Al Nahian,Chris Delcher,Daniel Harris,Peter Akpunonu,Ramakanth Kavuluru*

Main category: cs.AI

TL;DR: GPT-4o LLM在预测药物过量风险中表现优于传统机器学习模型，尤其在零样本设置下无需任务特定训练即可预测。


<details>
  <summary>Details</summary>
Motivation: 通过利用大型语言模型（LLMs）处理长文本数据和跨任务先验知识的能力，提升药物过量风险的预测性能。

Method: 评估GPT-4o在纵向保险索赔记录中的表现，对比微调和零样本设置与传统机器学习方法。

Result: LLMs在特定设置下优于传统模型，且零样本设置下也能预测风险。

Conclusion: LLMs在临床决策支持中具有潜力，尤其是药物过量风险预测。

Abstract: The ability to predict drug overdose risk from a patient's medical records is
crucial for timely intervention and prevention. Traditional machine learning
models have shown promise in analyzing longitudinal medical records for this
task. However, recent advancements in large language models (LLMs) offer an
opportunity to enhance prediction performance by leveraging their ability to
process long textual data and their inherent prior knowledge across diverse
tasks. In this study, we assess the effectiveness of Open AI's GPT-4o LLM in
predicting drug overdose events using patients' longitudinal insurance claims
records. We evaluate its performance in both fine-tuned and zero-shot settings,
comparing them to strong traditional machine learning methods as baselines. Our
results show that LLMs not only outperform traditional models in certain
settings but can also predict overdose risk in a zero-shot setting without
task-specific training. These findings highlight the potential of LLMs in
clinical decision support, particularly for drug overdose risk prediction.

</details>


### [44] [Evaluating the Goal-Directedness of Large Language Models](https://arxiv.org/abs/2504.11844)
*Tom Everitt,Cristina Garbacea,Alexis Bellot,Jonathan Richens,Henry Papadatos,Siméon Campos,Rohin Shah*

Main category: cs.AI

TL;DR: 论文研究了大型语言模型（LLM）的目标导向性，评估了其在信息收集、认知努力和计划执行任务中的表现，发现目标导向性在不同任务中相对一致，但与任务表现不同，且对动机提示的敏感性较低。


<details>
  <summary>Details</summary>
Motivation: 探讨LLM在实现目标时的能力使用程度，以评估其目标导向性，为LLM的进步监控和设计提供依据。

Method: 通过信息收集、认知努力和计划执行任务评估LLM的目标导向性，使用子任务推断模型的相关能力。

Result: 评估显示，目标导向性在不同任务中相对一致，与任务表现不同，且对动机提示的敏感性较低，多数模型未完全实现目标导向。

Conclusion: 目标导向性评估有助于更好地监控LLM进展，并为设计LLM的代理属性提供更明确的选择。

Abstract: To what extent do LLMs use their capabilities towards their given goal? We
take this as a measure of their goal-directedness. We evaluate
goal-directedness on tasks that require information gathering, cognitive
effort, and plan execution, where we use subtasks to infer each model's
relevant capabilities. Our evaluations of LLMs from Google DeepMind, OpenAI,
and Anthropic show that goal-directedness is relatively consistent across
tasks, differs from task performance, and is only moderately sensitive to
motivational prompts. Notably, most models are not fully goal-directed. We hope
our goal-directedness evaluations will enable better monitoring of LLM
progress, and enable more deliberate design choices of agentic properties in
LLMs.

</details>


### [45] [Moving between high-quality optima using multi-satisfiability characteristics in hard-to-solve Max3Sat instances](https://arxiv.org/abs/2504.11864)
*J. Piatek,M. W. Przewozniczek,F. Chicano,R. Tinós*

Main category: cs.AI

TL;DR: 灰盒优化通过利用变量依赖性和子函数表示，提出了一种通用的高效优化方法，尤其适用于解决Max3Sat问题。本文分析了现有方法在连接局部最优解和全局最优解时的局限性，并提出了一种基于多满足性特征的新优化器，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 许多实际问题可转化为MaxSat/Max3Sat实例，但现有灰盒优化方法在连接局部最优解和全局最优解时存在局限性，因此需要改进。

Method: 通过分析相位转变现象，提出基于多满足性特征的优化器，结合典型灰盒机制。

Result: 实验表明，新优化器能解决现有灰盒优化器无法处理的Max3Sat实例，同时对已解决的问题保持高效。

Conclusion: 通过操纵子句满足性特征，新方法显著提升了灰盒优化在Max3Sat问题上的性能。

Abstract: Gray-box optimization proposes effective and efficient optimizers of general
use. To this end, it leverages information about variable dependencies and the
subfunction-based problem representation. These approaches were already shown
effective by enabling \textit{tunnelling} between local optima even if these
moves require the modification of many dependent variables. Tunnelling is
useful in solving the maximum satisfiability problem (MaxSat), which can be
reformulated to Max3Sat. Since many real-world problems can be brought to
solving the MaxSat/Max3Sat instances, it is important to solve them effectively
and efficiently. Therefore, we focus on Max3Sat instances for which tunnelling
fails to introduce improving moves between locally optimal high-quality
solutions and the region of globally optimal solutions. We analyze the features
of such instances on the ground of phase transitions. Based on these
observations, we propose manipulating clause-satisfiability characteristics
that allow connecting high-quality solutions distant in the solution space. We
utilize multi-satisfiability characteristics in the optimizer built from
typical gray-box mechanisms. The experimental study shows that the proposed
optimizer can solve those Max3Sat instances that are out of the grasp of
state-of-the-art gray-box optimizers. At the same time, it remains effective
for instances that have already been successfully solved by gray-box.

</details>


### [46] [Seeking and leveraging alternative variable dependency concepts in gray-box-elusive bimodal land-use allocation problems](https://arxiv.org/abs/2504.11882)
*J. Maciążek,M. W. Przewozniczek,J. Schwaab*

Main category: cs.AI

TL;DR: 论文提出了一种针对土地利用分配问题的专用变量依赖定义，并基于此构建了三种新的交叉算子，显著提升了多目标优化器的效果。


<details>
  <summary>Details</summary>
Motivation: 解决土地利用分配问题有助于应对全球环境问题，但由于问题复杂且标准变量依赖发现技术不适用，需要新的优化方法。

Method: 提出专用变量依赖定义，生成依赖变量掩码，并构建三种新的交叉算子，应用于NSGA-II和MOEA/D优化器。

Result: 在真实案例测试中，新方法显著提升了NSGA-II和MOEA/D的效果。

Conclusion: 专用变量依赖定义和新交叉算子的引入为复杂多目标优化问题提供了有效工具。

Abstract: Solving land-use allocation problems can help us to deal with some of the
most urgent global environmental issues. Since these problems are NP-hard,
effective optimizers are needed to handle them. The knowledge about variable
dependencies allows for proposing such tools. However, in this work, we
consider a real-world multi-objective problem for which standard variable
dependency discovery techniques are inapplicable. Therefore, using
linkage-based variation operators is unreachable. To address this issue, we
propose a definition of problem-dedicated variable dependency. On this base, we
propose obtaining masks of dependent variables. Using them, we construct three
novel crossover operators. The results concerning real-world test cases show
that introducing our propositions into two well-known optimizers (NSGA-II,
MOEA/D) dedicated to multi-objective optimization significantly improves their
effectiveness.

</details>


### [47] [Rethinking the Generation of High-Quality CoT Data from the Perspective of LLM-Adaptive Question Difficulty Grading](https://arxiv.org/abs/2504.11919)
*Qianjin Yu,Keyu Wu,Zihan Chen,Chushu Zhang,Manlin Mei,Lingjun Huang,Fang Tan,Yongsheng Du,Kunlin Liu,Yurui Zhu*

Main category: cs.AI

TL;DR: 论文提出了一种高效生成高质量CoT数据的方法，通过LLM自适应问题难度级别，显著降低了数据生成成本并提升了模型监督微调效率。


<details>
  <summary>Details</summary>
Motivation: 利用DeepSeek-R1 (671B)的高质量推理能力，为小型LLMs生成高质量的CoT数据，以提升其推理能力。

Method: 1. 根据LLMs的推理能力对问题难度分级，构建LLM自适应问题库；2. 基于问题难度分布采样，利用DeepSeek-R1生成高质量CoT数据。

Result: 在数学竞赛和代码生成任务中验证了方法的有效性。ZMath-32B和ZCode-32B分别仅用2k数据即超越基准模型。

Conclusion: 提出的方法高效且通用，显著提升了小型LLMs的推理能力。

Abstract: Recently, DeepSeek-R1 (671B) (DeepSeek-AIet al., 2025) has demonstrated its
excellent reasoning ability in complex tasks and has publiclyshared its
methodology. This provides potentially high-quality chain-of-thought (CoT) data
for stimulating the reasoning abilities of small-sized large language models
(LLMs). To generate high-quality CoT data for different LLMs, we seek an
efficient method for generating high-quality CoT data with LLM-Adaptive
questiondifficulty levels. First, we grade the difficulty of the questions
according to the reasoning ability of the LLMs themselves and construct a
LLM-Adaptive question database. Second, we sample the problem database based on
a distribution of difficulty levels of the questions and then use DeepSeek-R1
(671B) (DeepSeek-AI et al., 2025) to generate the corresponding high-quality
CoT data with correct answers. Thanks to the construction of CoT data with
LLM-Adaptive difficulty levels, we have significantly reduced the cost of data
generation and enhanced the efficiency of model supervised fine-tuning (SFT).
Finally, we have validated the effectiveness and generalizability of the
proposed method in the fields of complex mathematical competitions and code
generation tasks. Notably, with only 2k high-quality mathematical CoT data, our
ZMath-32B surpasses DeepSeek-Distill-32B in math reasoning task. Similarly,
with only 2k high-quality code CoT data, our ZCode-32B surpasses
DeepSeek-Distill-32B in code reasoning tasks.

</details>


### [48] [ADAT: Time-Series-Aware Adaptive Transformer Architecture for Sign Language Translation](https://arxiv.org/abs/2504.11942)
*Nada Shahin,Leila Ismail*

Main category: cs.AI

TL;DR: 提出了一种自适应Transformer（ADAT），通过增强特征提取和自适应特征加权，解决了现有手语翻译系统在细粒度时序依赖和高计算复杂度上的不足，并在多个数据集上表现出更高的准确性和更快的训练速度。


<details>
  <summary>Details</summary>
Motivation: 现有手语翻译系统在识别高帧率下的细粒度时序依赖时准确性不足，且计算复杂度高导致训练效率低下。

Method: 提出ADAT，结合增强特征提取和自适应特征加权的门控机制，强调上下文相关特征，同时减少训练开销。

Result: 在多个数据集上，ADAT在翻译准确性和训练效率上均优于基线模型，例如在MedASL上BLEU-4准确率提升0.1%，训练时间减少3.24%。

Conclusion: ADAT通过改进特征提取和加权机制，显著提升了手语翻译的性能和效率，为实际应用提供了更优的解决方案。

Abstract: Current sign language machine translation systems rely on recognizing hand
movements, facial expressions and body postures, and natural language
processing, to convert signs into text. Recent approaches use Transformer
architectures to model long-range dependencies via positional encoding.
However, they lack accuracy in recognizing fine-grained, short-range temporal
dependencies between gestures captured at high frame rates. Moreover, their
high computational complexity leads to inefficient training. To mitigate these
issues, we propose an Adaptive Transformer (ADAT), which incorporates
components for enhanced feature extraction and adaptive feature weighting
through a gating mechanism to emphasize contextually relevant features while
reducing training overhead and maintaining translation accuracy. To evaluate
ADAT, we introduce MedASL, the first public medical American Sign Language
dataset. In sign-to-gloss-to-text experiments, ADAT outperforms the
encoder-decoder transformer, improving BLEU-4 accuracy by 0.1% while reducing
training time by 14.33% on PHOENIX14T and 3.24% on MedASL. In sign-to-text
experiments, it improves accuracy by 8.7% and reduces training time by 2.8% on
PHOENIX14T and achieves 4.7% higher accuracy and 7.17% faster training on
MedASL. Compared to encoder-only and decoder-only baselines in sign-to-text,
ADAT is at least 6.8% more accurate despite being up to 12.1% slower due to its
dual-stream structure.

</details>


### [49] [Leveraging Machine Learning Models to Predict the Outcome of Digital Medical Triage Interviews](https://arxiv.org/abs/2504.11977)
*Sofia Krylova,Fabian Schmidt,Vladimir Vlassov*

Main category: cs.AI

TL;DR: 研究探索机器学习预测未完成分诊问卷结果，提升患者护理质量。决策树模型在完整问卷中准确率超80%，且准确率与问卷完成度线性相关。


<details>
  <summary>Details</summary>
Motivation: 现有分诊系统依赖完整问卷，无法处理未完成问卷，影响患者安全和效率。研究旨在利用ML预测未完成问卷结果。

Method: 采用决策树模型（如LGBMClassifier和CatBoostClassifier）及TabTransformer模型，分析问卷完成度与预测准确率关系。

Result: 决策树模型在完整问卷中准确率超80%，且准确率与完成度线性相关。TabTransformer模型在所有完成度下准确率超80%，但训练耗时。

Conclusion: ML可有效预测未完成分诊问卷结果，决策树模型表现稳定，TabTransformer需更高计算资源。

Abstract: Many existing digital triage systems are questionnaire-based, guiding
patients to appropriate care levels based on information (e.g., symptoms,
medical history, and urgency) provided by the patients answering
questionnaires. Such a system often uses a deterministic model with predefined
rules to determine care levels. It faces challenges with incomplete triage
interviews since it can only assist patients who finish the process. In this
study, we explore the use of machine learning (ML) to predict outcomes of
unfinished interviews, aiming to enhance patient care and service quality.
Predicting triage outcomes from incomplete data is crucial for patient safety
and healthcare efficiency. Our findings show that decision-tree models,
particularly LGBMClassifier and CatBoostClassifier, achieve over 80\% accuracy
in predicting outcomes from complete interviews while having a linear
correlation between the prediction accuracy and interview completeness degree.
For example, LGBMClassifier achieves 88,2\% prediction accuracy for interviews
with 100\% completeness, 79,6\% accuracy for interviews with 80\% completeness,
58,9\% accuracy for 60\% completeness, and 45,7\% accuracy for 40\%
completeness. The TabTransformer model demonstrated exceptional accuracy of
over 80\% for all degrees of completeness but required extensive training time,
indicating a need for more powerful computational resources. The study
highlights the linear correlation between interview completeness and predictive
power of the decision-tree models.

</details>


### [50] [Purposefully Induced Psychosis (PIP): Embracing Hallucination as Imagination in Large Language Models](https://arxiv.org/abs/2504.12012)
*Kris Pilcher,Esen K. Tütüncü*

Main category: cs.AI

TL;DR: 论文提出了一种新方法PIP，通过放大LLM的幻觉来激发创造力，而非将其视为错误。


<details>
  <summary>Details</summary>
Motivation: 在创意或探索性任务中，LLM的幻觉可能成为创新的源泉，而非缺陷。

Method: 通过微调LLM，鼓励其生成推测性、隐喻性和超现实的输出（即幻觉），适用于非事实准确性优先的场景。

Result: PIP方法将幻觉转化为计算想象力的来源，适用于虚构文学、互动叙事和混合现实模拟等领域。

Conclusion: PIP重新定义了LLM幻觉的角色，为AI伦理和人机协作提供了新的视角。

Abstract: Hallucinations in Large Language Models (LLMs) are widely regarded as errors
- outputs that deviate from factual accuracy. However, in creative or
exploratory contexts, these "mistakes" may represent unexpected avenues for
innovation. We introduce Purposefully Induced Psychosis (PIP), a novel approach
that amplifies LLM hallucinations for imaginative tasks such as speculative
fiction, interactive storytelling, and mixed-reality simulations. Drawing on
Herman Melville's Moby-Dick, where Pip's "madness" reveals profound insight, we
reframe hallucinations as a source of computational imagination rather than a
flaw. Our method fine-tunes LLMs to encourage speculative, metaphorical, and
surreal outputs - hallucinations that are useful when factual accuracy is not
the chief objective. Inspired by the consensual illusions of theater and stage
magic, PIP situates these creative missteps in contexts where users willingly
suspend disbelief, thereby transforming "errors" into catalysts for new ways of
thinking. We discuss potential applications, design principles for ensuring
user consent, preliminary observations, and implications for broader AI ethics
and human-AI collaboration.

</details>


### [51] [Reasoning-Based AI for Startup Evaluation (R.A.I.S.E.): A Memory-Augmented, Multi-Step Decision Framework](https://arxiv.org/abs/2504.12090)
*Jack Preuveneers,Joseph Ternasky,Fuat Alican,Yigit Ihlamur*

Main category: cs.AI

TL;DR: 提出了一种结合决策树可解释性与大语言模型（LLM）推理能力的新框架，用于预测初创企业成功。通过链式思维提示生成详细推理日志，并将其转化为结构化逻辑规则。实验显示，该方法在精度和准确率上显著优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 解决传统决策树缺乏高级推理能力与LLM缺乏透明性的问题，为高风险投资等领域提供可解释的决策支持。

Method: 采用链式思维提示生成推理日志，通过两步精炼、集成候选采样、模拟强化学习评分和持久记忆等技术优化决策流程。

Result: 在初创企业数据集上，精度从0.225提升至0.346（54%），准确率从0.46提升至0.70（50%），显著优于随机分类器。

Conclusion: 该方法为高风险领域提供了透明且数据驱动的决策框架，支持专家干预和持续优化。

Abstract: We present a novel framework that bridges the gap between the
interpretability of decision trees and the advanced reasoning capabilities of
large language models (LLMs) to predict startup success. Our approach leverages
chain-of-thought prompting to generate detailed reasoning logs, which are
subsequently distilled into structured, human-understandable logical rules. The
pipeline integrates multiple enhancements - efficient data ingestion, a
two-step refinement process, ensemble candidate sampling, simulated
reinforcement learning scoring, and persistent memory - to ensure both stable
decision-making and transparent output. Experimental evaluations on curated
startup datasets demonstrate that our combined pipeline improves precision by
54% from 0.225 to 0.346 and accuracy by 50% from 0.46 to 0.70 compared to a
standalone OpenAI o3 model. Notably, our model achieves over 2x the precision
of a random classifier (16%). By combining state-of-the-art AI reasoning with
explicit rule-based explanations, our method not only augments traditional
decision-making processes but also facilitates expert intervention and
continuous policy refinement. This work lays the foundation for the
implementation of interpretable LLM-powered decision frameworks in high-stakes
investment environments and other domains that require transparent and
data-driven insights.

</details>


### [52] [Towards LLM Agents for Earth Observation](https://arxiv.org/abs/2504.12110)
*Chia Hsiang Kao,Wenting Zhao,Shreelekha Revankar,Samuel Speas,Snehal Bhagat,Rajeev Datta,Cheng Perng Phoo,Utkarsh Mall,Carl Vondrick,Kavita Bala,Bharath Hariharan*

Main category: cs.AI

TL;DR: 论文探讨AI系统是否准备好用于可靠的地球观测，提出了一个包含140个问题的基准测试，发现当前LLM代理的准确率仅为33%，但通过微调合成数据，较小模型也能达到较大模型的性能。


<details>
  <summary>Details</summary>
Motivation: 评估AI系统在地球观测中的可靠性，解决现有技术的高失败率和低准确率问题。

Method: 使用Google Earth Engine API作为工具，测试LLM代理的性能，并通过微调合成数据改进模型表现。

Result: LLM代理的初始准确率为33%，失败率高达58%；通过微调后，较小模型（如Llama-3.1-8B）能达到较大模型（如DeepSeek-R1）的准确率。

Conclusion: AI代理在地球观测自动化中仍面临重大挑战，但通过数据微调等方法可以改进性能。

Abstract: Earth Observation (EO) provides critical planetary data for environmental
monitoring, disaster management, climate science, and other scientific domains.
Here we ask: Are AI systems ready for reliable Earth Observation? We introduce
\datasetnamenospace, a benchmark of 140 yes/no questions from NASA Earth
Observatory articles across 13 topics and 17 satellite sensors. Using Google
Earth Engine API as a tool, LLM agents can only achieve an accuracy of 33%
because the code fails to run over 58% of the time. We improve the failure rate
for open models by fine-tuning synthetic data, allowing much smaller models
(Llama-3.1-8B) to achieve comparable accuracy to much larger ones (e.g.,
DeepSeek-R1). Taken together, our findings identify significant challenges to
be solved before AI agents can automate earth observation, and suggest paths
forward. The project page is available at
https://iandrover.github.io/UnivEarth.

</details>


### [53] [Advancing Arabic Speech Recognition Through Large-Scale Weakly Supervised Learning](https://arxiv.org/abs/2504.12254)
*Mahmoud Salhab,Marwan Elghitany,Shameed Sait,Syed Sibghat Ullah,Mohammad Abusheikh,Hasan Abusheikh*

Main category: cs.AI

TL;DR: 该论文提出了一种基于弱监督学习的阿拉伯语自动语音识别（ASR）模型，使用Conformer架构，在15,000小时的弱标注语音数据上训练，无需人工转录，取得了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 开发高性能ASR模型对低资源语言（如阿拉伯语）具有挑战性，因为缺乏大规模标注数据集。弱监督学习可以降低成本并提高可扩展性。

Method: 采用Conformer架构，利用15,000小时的弱标注语音数据（包括现代标准阿拉伯语和方言阿拉伯语）进行训练。

Result: 模型在标准基准测试中表现优于以往所有方法，达到了SOTA性能。

Conclusion: 弱监督学习是一种高效、低成本的方法，为低资源环境下的ASR系统提供了新的可能性。

Abstract: Automatic speech recognition (ASR) is crucial for human-machine interaction
in diverse applications like conversational agents, industrial robotics, call
center automation, and automated subtitling. However, developing
high-performance ASR models remains challenging, particularly for low-resource
languages like Arabic, due to the scarcity of large, labeled speech datasets,
which are costly and labor-intensive to produce. In this work, we employ weakly
supervised learning to train an Arabic ASR model using the Conformer
architecture. Our model is trained from scratch on 15,000 hours of weakly
annotated speech data covering both Modern Standard Arabic (MSA) and Dialectal
Arabic (DA), eliminating the need for costly manual transcriptions. Despite the
absence of human-verified labels, our approach attains state-of-the-art (SOTA)
performance, exceeding all previous efforts in the field of Arabic ASR on the
standard benchmarks. By demonstrating the effectiveness of weak supervision as
a scalable, cost-efficient alternative to traditional supervised approaches,
paving the way for improved ASR systems in low resource settings.

</details>


### [54] [Adapting a World Model for Trajectory Following in a 3D Game](https://arxiv.org/abs/2504.12299)
*Marko Tot,Shu Ishida,Abdelhak Lemkhenter,David Bignell,Pallavi Choudhury,Chris Lovett,Luis França,Matheus Ribeiro Furtado de Mendonça,Tarun Gupta,Darren Gehring,Sam Devlin,Sergio Valcarcel Macua,Raluca Georgescu*

Main category: cs.AI

TL;DR: 研究了在复杂3D游戏环境中使用逆动力学模型（IDM）进行轨迹跟随的方法，探讨了不同编码器和策略头的效果，并提出了应对分布偏移的策略。


<details>
  <summary>Details</summary>
Motivation: 在复杂环境中，简单的动作重放无法应对分布偏移和随机性，需要更稳健的方法。

Method: 应用IDM结合不同编码器和策略头，研究未来对齐策略以应对分布偏移。

Result: 在多样数据设置下，GPT风格策略头表现最佳；低数据环境下，DINOv2编码器与GPT风格策略头效果最好；预训练后微调时，GPT和MLP风格策略头效果相当。

Conclusion: 不同配置在不同设置下表现各异，需根据具体场景选择最优方法。

Abstract: Imitation learning is a powerful tool for training agents by leveraging
expert knowledge, and being able to replicate a given trajectory is an integral
part of it. In complex environments, like modern 3D video games, distribution
shift and stochasticity necessitate robust approaches beyond simple action
replay. In this study, we apply Inverse Dynamics Models (IDM) with different
encoders and policy heads to trajectory following in a modern 3D video game --
Bleeding Edge. Additionally, we investigate several future alignment strategies
that address the distribution shift caused by the aleatoric uncertainty and
imperfections of the agent. We measure both the trajectory deviation distance
and the first significant deviation point between the reference and the agent's
trajectory and show that the optimal configuration depends on the chosen
setting. Our results show that in a diverse data setting, a GPT-style policy
head with an encoder trained from scratch performs the best, DINOv2 encoder
with the GPT-style policy head gives the best results in the low data regime,
and both GPT-style and MLP-style policy heads had comparable results when
pre-trained on a diverse setting and fine-tuned for a specific behaviour
setting.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [55] [CI-RKM: A Class-Informed Approach to Robust Restricted Kernel Machines](https://arxiv.org/abs/2504.11476)
*Ritik Mishra,Mushir Akhtar,M. Tanveer*

Main category: cs.LG

TL;DR: 提出了一种基于类信息加权函数的增强型受限核机（CI-RKM），通过动态调整训练点的贡献，提升对噪声和异常值的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 受限核机（RKM）在噪声和异常值存在时性能下降，影响鲁棒性和预测准确性。

Method: 引入类信息加权函数，结合加权共轭特征对偶性和Schur补定理，设计CI-RKM。

Result: 实验表明CI-RKM在基准数据集上优于现有方法，分类准确性和鲁棒性显著提升。

Conclusion: CI-RKM是核基学习模型的重要进展，解决了领域核心挑战。

Abstract: Restricted kernel machines (RKMs) represent a versatile and powerful
framework within the kernel machine family, leveraging conjugate feature
duality to address a wide range of machine learning tasks, including
classification, regression, and feature learning. However, their performance
can degrade significantly in the presence of noise and outliers, which
compromises robustness and predictive accuracy. In this paper, we propose a
novel enhancement to the RKM framework by integrating a class-informed weighted
function. This weighting mechanism dynamically adjusts the contribution of
individual training points based on their proximity to class centers and
class-specific characteristics, thereby mitigating the adverse effects of noisy
and outlier data. By incorporating weighted conjugate feature duality and
leveraging the Schur complement theorem, we introduce the class-informed
restricted kernel machine (CI-RKM), a robust extension of the RKM designed to
improve generalization and resilience to data imperfections. Experimental
evaluations on benchmark datasets demonstrate that the proposed CI-RKM
consistently outperforms existing baselines, achieving superior classification
accuracy and enhanced robustness against noise and outliers. Our proposed
method establishes a significant advancement in the development of kernel-based
learning models, addressing a core challenge in the field.

</details>


### [56] [LLM-based AI Agent for Sizing of Analog and Mixed Signal Circuit](https://arxiv.org/abs/2504.11497)
*Chang Liu,Emmanuel A. Olowe,Danial Chitnis*

Main category: cs.LG

TL;DR: 本文提出了一种基于大型语言模型（LLM）的AI代理，用于模拟和混合信号（AMS）电路设计中的晶体管尺寸优化，通过结合外部仿真工具和提示工程策略，成功优化了多个电路。


<details>
  <summary>Details</summary>
Motivation: AMS电路设计中的晶体管尺寸过程通常需要大量手动工作，现有机器学习方法在EDA中虽有效但仍面临迭代多和知识不足的挑战。LLM在电路设计中的潜力激发了其自动化应用的探索。

Method: 整合LLM与外部电路仿真工具和数据分析功能，采用提示工程策略优化电路。评估了不同LLM在七种基本电路中的表现，并选择最佳模型Claude 3.5 Sonnet进一步测试。

Result: 在运算放大器的九项性能指标测试中，针对三组不同性能要求，实现了高达60%的成功率。

Conclusion: LLM在AMS电路设计中展现出潜力，能够有效辅助晶体管尺寸优化。

Abstract: The design of Analog and Mixed-Signal (AMS) integrated circuits (ICs) often
involves significant manual effort, especially during the transistor sizing
process. While Machine Learning techniques in Electronic Design Automation
(EDA) have shown promise in reducing complexity and minimizing human
intervention, they still face challenges such as numerous iterations and a lack
of knowledge about AMS circuit design. Recently, Large Language Models (LLMs)
have demonstrated significant potential across various fields, showing a
certain level of knowledge in circuit design and indicating their potential to
automate the transistor sizing process. In this work, we propose an LLM-based
AI agent for AMS circuit design to assist in the sizing process. By integrating
LLMs with external circuit simulation tools and data analysis functions and
employing prompt engineering strategies, the agent successfully optimized
multiple circuits to achieve target performance metrics. We evaluated the
performance of different LLMs to assess their applicability and optimization
effectiveness across seven basic circuits, and selected the best-performing
model Claude 3.5 Sonnet for further exploration on an operational amplifier,
with complementary input stage and class AB output stage. This circuit was
evaluated against nine performance metrics, and we conducted experiments under
three distinct performance requirement groups. A success rate of up to 60% was
achieved for reaching the target requirements. Overall, this work demonstrates
the potential of LLMs to improve AMS circuit design.

</details>


### [57] [Cross-cultural Deployment of Autonomous Vehicles Using Data-light Inverse Reinforcement Learning](https://arxiv.org/abs/2504.11506)
*Hongliang Lu,Shuqi Shen,Junjie Yang,Chao Lu,Xinhu Zheng,Hai Yang*

Main category: cs.LG

TL;DR: 提出了一种数据轻量化的逆向强化学习方案，用于自动驾驶车辆（AVs）的跨文化部署，减少对本地数据的依赖。


<details>
  <summary>Details</summary>
Motivation: 驾驶文化的差异是自动驾驶车辆在全球部署的主要挑战之一，尤其是在数据不足的地区。

Method: 通过比较德国、中国和美国的高速公路自然驾驶数据集，提出并测试了数据轻量化的逆向强化学习方案。

Result: 方案在跨文化部署中表现优异，最高可减少98.67%的本地数据依赖。

Conclusion: 该研究有望推动自动驾驶车辆在全球市场的公平发展，特别是在数据不足的地区。

Abstract: More than the adherence to specific traffic regulations, driving culture
touches upon a more implicit part - an informal, conventional, collective
behavioral pattern followed by drivers - that varies across countries, regions,
and even cities. Such cultural divergence has become one of the biggest
challenges in deploying autonomous vehicles (AVs) across diverse regions today.
The current emergence of data-driven methods has shown a potential solution to
enable culture-compatible driving through learning from data, but what if some
underdeveloped regions cannot provide sufficient local data to inform driving
culture? This issue is particularly significant for a broader global AV market.
Here, we propose a cross-cultural deployment scheme for AVs, called data-light
inverse reinforcement learning, designed to re-calibrate culture-specific AVs
and assimilate them into other cultures. First, we report the divergence in
driving cultures through a comprehensive comparative analysis of naturalistic
driving datasets on highways from three countries: Germany, China, and the USA.
Then, we demonstrate the effectiveness of our scheme by testing the expeditious
cross-cultural deployment across these three countries, with cumulative testing
mileage of over 56084 km. The performance is particularly advantageous when
cross-cultural deployment is carried out without affluent local data. Results
show that we can reduce the dependence on local data by a margin of 98.67% at
best. This study is expected to bring a broader, fairer AV global market,
particularly in those regions that lack enough local data to develop
culture-compatible AVs.

</details>


### [58] [Reward Distance Comparisons Under Transition Sparsity](https://arxiv.org/abs/2504.11508)
*Clement Nyanhongo,Bruno Miranda Henrique,Eugene Santos*

Main category: cs.LG

TL;DR: 论文提出了一种名为SRRD的伪度量方法，用于在稀疏过渡条件下直接比较奖励函数，无需高过渡覆盖率。


<details>
  <summary>Details</summary>
Motivation: 传统方法需要学习优化策略来比较奖励函数，计算成本高且存在安全问题；现有直接比较方法在稀疏过渡条件下表现不佳。

Method: 引入SRRD伪度量，适应多样化的样本分布，无需高过渡覆盖率。

Result: 理论证明了SRRD的鲁棒性，实验验证了其在多个领域的有效性。

Conclusion: SRRD是一种高效且安全的奖励函数比较方法，适用于稀疏过渡条件。

Abstract: Reward comparisons are vital for evaluating differences in agent behaviors
induced by a set of reward functions. Most conventional techniques utilize the
input reward functions to learn optimized policies, which are then used to
compare agent behaviors. However, learning these policies can be
computationally expensive and can also raise safety concerns. Direct reward
comparison techniques obviate policy learning but suffer from transition
sparsity, where only a small subset of transitions are sampled due to data
collection challenges and feasibility constraints. Existing state-of-the-art
direct reward comparison methods are ill-suited for these sparse conditions
since they require high transition coverage, where the majority of transitions
from a given coverage distribution are sampled. When this requirement is not
satisfied, a distribution mismatch between sampled and expected transitions can
occur, leading to significant errors. This paper introduces the Sparsity
Resilient Reward Distance (SRRD) pseudometric, designed to eliminate the need
for high transition coverage by accommodating diverse sample distributions,
which are common under transition sparsity. We provide theoretical
justification for SRRD's robustness and conduct experiments to demonstrate its
practical efficacy across multiple domains.

</details>


### [59] [Position Paper: Rethinking Privacy in RL for Sequential Decision-making in the Age of LLMs](https://arxiv.org/abs/2504.11511)
*Flint Xiaofeng Fan,Cheston Tan,Roger Wattenhofer,Yew-Soon Ong*

Main category: cs.LG

TL;DR: 论文提出了一种新的隐私保护范式，针对强化学习（RL）系统中的隐私问题，强调多尺度保护、行为模式保护、协作隐私保护和上下文感知适应。


<details>
  <summary>Details</summary>
Motivation: 传统隐私框架无法满足RL系统的需求，尤其是在联邦RL和带有人类反馈的RL等现代范式中，隐私问题更加复杂。

Method: 提出基于四个核心原则的新隐私范式：多尺度保护、行为模式保护、协作隐私保护和上下文感知适应。

Result: 揭示了隐私、效用和可解释性之间的内在矛盾，并呼吁开发新理论框架和实践机制。

Conclusion: 需要新的理论、方法和评估工具来解决RL系统中的隐私挑战。

Abstract: The rise of reinforcement learning (RL) in critical real-world applications
demands a fundamental rethinking of privacy in AI systems. Traditional privacy
frameworks, designed to protect isolated data points, fall short for sequential
decision-making systems where sensitive information emerges from temporal
patterns, behavioral strategies, and collaborative dynamics. Modern RL
paradigms, such as federated RL (FedRL) and RL with human feedback (RLHF) in
large language models (LLMs), exacerbate these challenges by introducing
complex, interactive, and context-dependent learning environments that
traditional methods do not address. In this position paper, we argue for a new
privacy paradigm built on four core principles: multi-scale protection,
behavioral pattern protection, collaborative privacy preservation, and
context-aware adaptation. These principles expose inherent tensions between
privacy, utility, and interpretability that must be navigated as RL systems
become more pervasive in high-stakes domains like healthcare, autonomous
vehicles, and decision support systems powered by LLMs. To tackle these
challenges, we call for the development of new theoretical frameworks,
practical mechanisms, and rigorous evaluation methodologies that collectively
enable effective privacy protection in sequential decision-making systems.

</details>


### [60] [Multi-output Classification Framework and Frequency Layer Normalization for Compound Fault Diagnosis in Motor](https://arxiv.org/abs/2504.11513)
*Wonjun Yi,Yong-Hwa Park*

Main category: cs.LG

TL;DR: 提出了一种多输出分类框架（MOC），用于旋转机械故障诊断中的域适应问题，特别针对部分标记目标域和复合故障条件。该方法通过独立估计每种故障类型的严重程度，提高了可解释性和诊断准确性。


<details>
  <summary>Details</summary>
Motivation: 传统多类分类方法将每种故障组合视为独立类别，缺乏灵活性。MOC框架旨在解决这一问题，同时适应部分标记目标域和复合故障条件。

Method: 结合多核最大均值差异（MK-MMD）和熵最小化（EM）损失，实现源域到目标域的特征迁移；采用频率层归一化（FLN）保留频域结构特性。

Result: 在六个域适应案例中，MOC在宏F1分数上优于基线模型，对单个故障类型的分类性能更优，且FLN表现出更强的适应性。

Conclusion: MOC框架在旋转机械故障诊断中表现出色，尤其在部分标记目标域和复合故障条件下，具有更高的诊断准确性和适应性。

Abstract: This work introduces a multi-output classification (MOC) framework designed
for domain adaptation in fault diagnosis, particularly under partially labeled
(PL) target domain scenarios and compound fault conditions in rotating
machinery. Unlike traditional multi-class classification (MCC) methods that
treat each fault combination as a distinct class, the proposed approach
independently estimates the severity of each fault type, improving both
interpretability and diagnostic accuracy. The model incorporates multi-kernel
maximum mean discrepancy (MK-MMD) and entropy minimization (EM) losses to
facilitate feature transfer from the source to the target domain. In addition,
frequency layer normalization (FLN) is applied to preserve structural
properties in the frequency domain, which are strongly influenced by system
dynamics and are often stationary with respect to changes in rpm. Evaluations
across six domain adaptation cases with PL data demonstrate that MOC
outperforms baseline models in macro F1 score. Moreover, MOC consistently
achieves better classification performance for individual fault types, and FLN
shows superior adaptability compared to other normalization techniques.

</details>


### [61] [LANGTRAJ: Diffusion Model and Dataset for Language-Conditioned Trajectory Simulation](https://arxiv.org/abs/2504.11521)
*Wei-Jer Chang,Wei Zhan,Masayoshi Tomizuka,Manmohan Chandraker,Francesco Pittaluga*

Main category: cs.LG

TL;DR: LangTraj是一种基于语言条件的场景扩散模型，用于模拟交通场景中所有智能体的联合行为，提供灵活直观的控制，生成逼真场景。


<details>
  <summary>Details</summary>
Motivation: 通过语言条件控制增强自动驾驶车辆测试的可扩展性和安全性，克服传统方法依赖领域特定指导函数的限制。

Method: 提出LangTraj模型，结合语言条件训练和闭环训练策略，并使用Inter-Drive大规模数据集支持语言条件模拟。

Result: 在Waymo Motion Dataset上验证，LangTraj在逼真度、语言可控性和安全性方面表现优异。

Conclusion: LangTraj为自动驾驶测试提供了灵活且可扩展的新范式。

Abstract: Evaluating autonomous vehicles with controllability enables scalable testing
in counterfactual or structured settings, enhancing both efficiency and safety.
We introduce LangTraj, a language-conditioned scene-diffusion model that
simulates the joint behavior of all agents in traffic scenarios. By
conditioning on natural language inputs, LangTraj provides flexible and
intuitive control over interactive behaviors, generating nuanced and realistic
scenarios. Unlike prior approaches that depend on domain-specific guidance
functions, LangTraj incorporates language conditioning during training,
facilitating more intuitive traffic simulation control. We propose a novel
closed-loop training strategy for diffusion models, explicitly tailored to
enhance stability and realism during closed-loop simulation. To support
language-conditioned simulation, we develop Inter-Drive, a large-scale dataset
with diverse and interactive labels for training language-conditioned diffusion
models. Our dataset is built upon a scalable pipeline for annotating
agent-agent interactions and single-agent behaviors, ensuring rich and varied
supervision. Validated on the Waymo Motion Dataset, LangTraj demonstrates
strong performance in realism, language controllability, and
language-conditioned safety-critical simulation, establishing a new paradigm
for flexible and scalable autonomous vehicle testing.

</details>


### [62] [Error Broadcast and Decorrelation as a Potential Artificial and Natural Learning Mechanism](https://arxiv.org/abs/2504.11558)
*Mete Erdogan,Cengiz Pehlevan,Alper T. Erdogan*

Main category: cs.LG

TL;DR: EBD算法通过直接广播输出误差到各层解决神经网络中的信用分配问题，利用MMSE估计器的随机正交性定义层间损失函数，无需权重传输即可实现误差广播。


<details>
  <summary>Details</summary>
Motivation: 解决神经网络中的信用分配问题，提供一种无需权重传输的误差广播方法。

Method: 利用MMSE估计器的随机正交性定义层间损失函数，惩罚层激活与输出误差的相关性。

Result: 在基准数据集上表现优于或等同于已知误差广播方法。

Conclusion: EBD是一种高效、适应性强且生物合理的神经网络训练方法，可能推动人工和自然学习范式的进步。

Abstract: We introduce the Error Broadcast and Decorrelation (EBD) algorithm, a novel
learning framework that addresses the credit assignment problem in neural
networks by directly broadcasting output error to individual layers. Leveraging
the stochastic orthogonality property of the optimal minimum mean square error
(MMSE) estimator, EBD defines layerwise loss functions to penalize correlations
between layer activations and output errors, offering a principled approach to
error broadcasting without the need for weight transport. The optimization
framework naturally leads to the experimentally observed three-factor learning
rule and integrates with biologically plausible frameworks to enhance
performance and plausibility. Numerical experiments demonstrate that EBD
achieves performance comparable to or better than known error-broadcast methods
on benchmark datasets. While the scalability of EBD to very large or complex
datasets remains to be further explored, our findings suggest it provides a
biologically plausible, efficient, and adaptable alternative for neural network
training. This approach could inform future advancements in artificial and
natural learning paradigms.

</details>


### [63] [Towards a Universal Vibration Analysis Dataset: A Framework for Transfer Learning in Predictive Maintenance and Structural Health Monitoring](https://arxiv.org/abs/2504.11581)
*Mert Sehri,Igor Varejão,Zehui Hua,Vitor Bonella,Adriano Santos,Francisco de Assis Boldt,Patrick Dumond,Flavio Miguel Varejão*

Main category: cs.LG

TL;DR: 提出一个振动数据集框架，模仿ImageNet的成功，以支持振动分析领域的深度学习模型开发。


<details>
  <summary>Details</summary>
Motivation: 振动分析领域缺乏大规模标注数据集，限制了类似ImageNet在视觉计算中的成功应用。

Method: 收集轴承振动信号，构建初始框架，并通过深度学习实验验证其有效性。

Result: 实验表明，预训练和微调能提升模型性能，验证了框架的潜力。

Conclusion: 未来将扩展数据集和标准化框架，以推动工业应用中的智能系统发展。

Abstract: ImageNet has become a reputable resource for transfer learning, allowing the
development of efficient ML models with reduced training time and data
requirements. However, vibration analysis in predictive maintenance, structural
health monitoring, and fault diagnosis, lacks a comparable large-scale,
annotated dataset to facilitate similar advancements. To address this, a
dataset framework is proposed that begins with bearing vibration data as an
initial step towards creating a universal dataset for vibration-based
spectrogram analysis for all machinery. The initial framework includes a
collection of bearing vibration signals from various publicly available
datasets. To demonstrate the advantages of this framework, experiments were
conducted using a deep learning architecture, showing improvements in model
performance when pre-trained on bearing vibration data and fine-tuned on a
smaller, domain-specific dataset. These findings highlight the potential to
parallel the success of ImageNet in visual computing but for vibration
analysis. For future work, this research will include a broader range of
vibration signals from multiple types of machinery, emphasizing
spectrogram-based representations of the data. Each sample will be labeled
according to machinery type, operational status, and the presence or type of
faults, ensuring its utility for supervised and unsupervised learning tasks.
Additionally, a framework for data preprocessing, feature extraction, and model
training specific to vibration data will be developed. This framework will
standardize methodologies across the research community, allowing for
collaboration and accelerating progress in predictive maintenance, structural
health monitoring, and related fields. By mirroring the success of ImageNet in
visual computing, this dataset has the potential to improve the development of
intelligent systems in industrial applications.

</details>


### [64] [Dueling Deep Reinforcement Learning for Financial Time Series](https://arxiv.org/abs/2504.11601)
*Bruno Giorgio*

Main category: cs.LG

TL;DR: 研究探讨了在金融交易任务中应用Double DQN和Dueling Network Architectures，通过历史SP500数据训练交易策略优化代理，并评估了交易成本对策略的影响。


<details>
  <summary>Details</summary>
Motivation: 探索强化学习在动态决策问题中的应用，特别是在金融交易中优化策略并考虑实际约束（如交易成本）。

Method: 使用Double DQN和Dueling Network Architectures训练代理，基于历史SP500数据，并在有/无交易成本的场景下评估性能。

Result: 代理成功学习了有效的交易策略，在有限数据集上优于随机策略，但数据复杂性导致策略仍有优化空间。

Conclusion: 强化学习代理在金融交易中具有潜力，但数据复杂性仍是主要挑战。

Abstract: Reinforcement learning (RL) has emerged as a powerful paradigm for solving
decision-making problems in dynamic environments. In this research, we explore
the application of Double DQN (DDQN) and Dueling Network Architectures, to
financial trading tasks using historical SP500 index data. Our focus is
training agents capable of optimizing trading strategies while accounting for
practical constraints such as transaction costs. The study evaluates the model
performance across scenarios with and without commissions, highlighting the
impact of cost-sensitive environments on reward dynamics. Despite computational
limitations and the inherent complexity of financial time series data, the
agent successfully learned meaningful trading policies. The findings confirm
that RL agents, even when trained on limited datasets, can outperform random
strategies by leveraging advanced architectures such as DDQN and Dueling
Networks. However, significant challenges persist, particularly with a
sub-optimal policy due to the complexity of data source.

</details>


### [65] [Possibility for Proactive Anomaly Detection](https://arxiv.org/abs/2504.11623)
*Jinsung Jeon,Jaehyeon Park,Sewon Park,Jeongwhan Choi,Minjung Kim,Noseong Park*

Main category: cs.LG

TL;DR: 提出了一种基于时间序列预测和异常检测的主动方法，通过数据驱动模型设定异常阈值，并在预测值超过阈值时检测异常。


<details>
  <summary>Details</summary>
Motivation: 现有异常检测模型依赖模型输出与真实值的误差，实用性不足，需减少潜在损失。

Method: 结合时间序列预测模型和数据驱动异常检测模型，从训练数据中设定异常阈值，检测超出阈值的预测值。

Result: 在四个异常检测基准上广泛评估，分析了可预测和不可预测的异常。

Conclusion: 主动方法有效，提供了源代码作为补充材料。

Abstract: Time-series anomaly detection, which detects errors and failures in a
workflow, is one of the most important topics in real-world applications. The
purpose of time-series anomaly detection is to reduce potential damages or
losses. However, existing anomaly detection models detect anomalies through the
error between the model output and the ground truth (observed) value, which
makes them impractical. In this work, we present a \textit{proactive} approach
for time-series anomaly detection based on a time-series forecasting model
specialized for anomaly detection and a data-driven anomaly detection model.
Our proactive approach establishes an anomaly threshold from training data with
a data-driven anomaly detection model, and anomalies are subsequently detected
by identifying predicted values that exceed the anomaly threshold. In addition,
we extensively evaluated the model using four anomaly detection benchmarks and
analyzed both predictable and unpredictable anomalies. We attached the source
code as supplementary material.

</details>


### [66] [Achieving Tighter Finite-Time Rates for Heterogeneous Federated Stochastic Approximation under Markovian Sampling](https://arxiv.org/abs/2504.11645)
*Feng Zhu,Aritra Mitra,Robert W. Heath*

Main category: cs.LG

TL;DR: 研究了一种通用的联邦随机逼近问题，提出了一种名为FedHSA的新算法，解决了现有方法在收敛性和协作效益上的局限性，并证明了其正确收敛性和样本复杂度的线性加速。


<details>
  <summary>Details</summary>
Motivation: 研究动机源于协作强化学习和时间相关数据优化，旨在解决联邦环境中代理间通信和异构局部算子的根寻找问题。

Method: 开发了FedHSA算法，通过间歇性通信和局部步骤优化，避免了投影步骤，并处理了马尔可夫采样和异构局部算子带来的复杂性。

Result: 证明了FedHSA算法能正确收敛到目标点，并实现了样本复杂度的M倍线性加速。

Conclusion: 这是首个在联邦环境中同时处理马尔可夫数据和异构局部算子的有限时间结果，对异构联邦强化学习问题具有广泛意义。

Abstract: Motivated by collaborative reinforcement learning (RL) and optimization with
time-correlated data, we study a generic federated stochastic approximation
problem involving $M$ agents, where each agent is characterized by an
agent-specific (potentially nonlinear) local operator. The goal is for the
agents to communicate intermittently via a server to find the root of the
average of the agents' local operators. The generality of our setting stems
from allowing for (i) Markovian data at each agent and (ii) heterogeneity in
the roots of the agents' local operators. The limited recent work that has
accounted for both these features in a federated setting fails to guarantee
convergence to the desired point or to show any benefit of collaboration;
furthermore, they rely on projection steps in their algorithms to guarantee
bounded iterates. Our work overcomes each of these limitations. We develop a
novel algorithm titled \texttt{FedHSA}, and prove that it guarantees
convergence to the correct point, while enjoying an $M$-fold linear speedup in
sample-complexity due to collaboration. To our knowledge, \emph{this is the
first finite-time result of its kind}, and establishing it (without relying on
a projection step) entails a fairly intricate argument that accounts for the
interplay between complex temporal correlations due to Markovian sampling,
multiple local steps to save communication, and the drift-effects induced by
heterogeneous local operators. Our results have implications for a broad class
of heterogeneous federated RL problems (e.g., policy evaluation and control)
with function approximation, where the agents' Markov decision processes can
differ in their probability transition kernels and reward functions.

</details>


### [67] [70% Size, 100% Accuracy: Lossless LLM Compression for Efficient GPU Inference via Dynamic-Length Float](https://arxiv.org/abs/2504.11651)
*Tianyi Zhang,Yang Sui,Shaochen Zhong,Vipin Chaudhary,Xia Hu,Anshumali Shrivastava*

Main category: cs.LG

TL;DR: DFloat11是一种无损压缩框架，可将大型语言模型（LLM）的尺寸减少30%，同时保持输出与原始模型完全一致。


<details>
  <summary>Details</summary>
Motivation: LLM的BFloat16权重表示中存在低熵问题，现有存储格式效率低下，DFloat11通过熵编码解决这一问题。

Method: DFloat11基于权重频率分配动态长度编码，并开发了高效的GPU内核进行在线解压缩，包括紧凑查找表、两阶段内核和块级解压缩。

Result: 实验验证DFloat11在Llama-3.1等模型上实现30%尺寸压缩，吞吐量提升1.9-38.8倍，上下文长度延长5.3-13.17倍。

Conclusion: DFloat11成功实现无损压缩，显著提升LLM在资源受限硬件上的部署效率。

Abstract: Large Language Models (LLMs) have grown rapidly in size, creating significant
challenges for efficient deployment on resource-constrained hardware. In this
paper, we introduce Dynamic-Length Float (DFloat11), a lossless compression
framework that reduces LLM size by 30% while preserving outputs that are
bit-for-bit identical to the original model. DFloat11 is motivated by the low
entropy in the BFloat16 weight representation of LLMs, which reveals
significant inefficiency in existing storage format. By applying entropy
coding, DFloat11 assigns dynamic-length encodings to weights based on
frequency, achieving near information-optimal compression without any loss of
precision. To facilitate efficient inference with dynamic-length encodings, we
develop a custom GPU kernel for fast online decompression. Our design
incorporates the following: (i) decomposition of memory-intensive lookup tables
(LUTs) into compact LUTs that fit in GPU SRAM, (ii) a two-phase kernel for
coordinating thread read/write positions using lightweight auxiliary variables,
and (iii) transformer-block-level decompression to minimize latency.
Experiments on recent models, including Llama-3.1, Qwen-2.5, and Gemma-3,
validates our hypothesis that DFloat11 achieves around 30% model size reduction
while preserving bit-for-bit exact outputs. Compared to a potential alternative
of offloading parts of an uncompressed model to the CPU to meet memory
constraints, DFloat11 achieves 1.9-38.8x higher throughput in token generation.
With a fixed GPU memory budget, DFloat11 enables 5.3-13.17x longer context
lengths than uncompressed models. Notably, our method enables lossless
inference of Llama-3.1-405B, an 810GB model, on a single node equipped with
8x80GB GPUs. Our code and models are available at
https://github.com/LeanModels/DFloat11.

</details>


### [68] [H$^3$GNNs: Harmonizing Heterophily and Homophily in GNNs via Joint Structural Node Encoding and Self-Supervised Learning](https://arxiv.org/abs/2504.11699)
*Rui Xue,Tianfu Wu*

Main category: cs.LG

TL;DR: H$^3$GNNs是一种自监督学习框架，通过联合结构节点编码和动态掩码策略，平衡异质性和同质性，在多种图数据上表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决GNN在自监督学习中难以平衡异质性和同质性的问题。

Method: 提出联合结构节点编码（结合线性和非线性特征投影）和基于节点预测难度的动态掩码策略。

Result: 在七种基准测试中表现优异，尤其在异质性数据集上达到SOTA。

Conclusion: H$^3$GNNs在多种图类型中有效平衡异质性和同质性，性能优越。

Abstract: Graph Neural Networks (GNNs) struggle to balance heterophily and homophily in
representation learning, a challenge further amplified in self-supervised
settings. We propose H$^3$GNNs, an end-to-end self-supervised learning
framework that harmonizes both structural properties through two key
innovations: (i) Joint Structural Node Encoding. We embed nodes into a unified
space combining linear and non-linear feature projections with K-hop structural
representations via a Weighted Graph Convolution Network(WGCN). A
cross-attention mechanism enhances awareness and adaptability to heterophily
and homophily. (ii) Self-Supervised Learning Using Teacher-Student Predictive
Architectures with Node-Difficulty Driven Dynamic Masking Strategies. We use a
teacher-student model, the student sees the masked input graph and predicts
node features inferred by the teacher that sees the full input graph in the
joint encoding space. To enhance learning difficulty, we introduce two novel
node-predictive-difficulty-based masking strategies. Experiments on seven
benchmarks (four heterophily datasets and three homophily datasets) confirm the
effectiveness and efficiency of H$^3$GNNs across diverse graph types. Our
H$^3$GNNs achieves overall state-of-the-art performance on the four heterophily
datasets, while retaining on-par performance to previous state-of-the-art
methods on the three homophily datasets.

</details>


### [69] [Clustering and analysis of user behaviour in blockchain: A case study of Planet IX](https://arxiv.org/abs/2504.11702)
*Dorottya Zelenyanszki,Zhe Hou,Kamanashis Biswas,Vallipuram Muthukkumarasamy*

Main category: cs.LG

TL;DR: 论文提出了一种用户行为分析流程，用于从区块链游戏中提取和分析用户行为数据，并通过聚类算法识别行为模式，同时揭示了潜在的隐私威胁。


<details>
  <summary>Details</summary>
Motivation: 区块链上的去中心化应用（dApps）具有透明性，但公开的交易数据可能导致用户行为隐私泄露。本文旨在展示如何从这些数据中提取用户行为信息，并分析其隐私风险。

Method: 通过收集Planet IX游戏的区块链交易数据，构建用户行为流，并利用图神经网络（GNN）生成图嵌入，再通过聚类算法识别行为模式。

Result: 研究发现，用户行为可以被聚类为不同的行为模式，并揭示了这些信息可能被恶意利用的隐私威胁。

Conclusion: 论文展示了区块链数据中用户行为的可分析性，并提出了隐私威胁模型，强调了保护用户隐私的必要性。

Abstract: Decentralised applications (dApps) that run on public blockchains have the
benefit of trustworthiness and transparency as every activity that happens on
the blockchain can be publicly traced through the transaction data. However,
this introduces a potential privacy problem as this data can be tracked and
analysed, which can reveal user-behaviour information. A user behaviour
analysis pipeline was proposed to present how this type of information can be
extracted and analysed to identify separate behavioural clusters that can
describe how users behave in the game. The pipeline starts with the collection
of transaction data, involving smart contracts, that is collected from a
blockchain-based game called Planet IX. Both the raw transaction information
and the transaction events are considered in the data collection. From this
data, separate game actions can be formed and those are leveraged to present
how and when the users conducted their in-game activities in the form of user
flows. An extended version of these user flows also presents how the
Non-Fungible Tokens (NFTs) are being leveraged in the user actions. The latter
is given as input for a Graph Neural Network (GNN) model to provide graph
embeddings for these flows which then can be leveraged by clustering algorithms
to cluster user behaviours into separate behavioural clusters. We benchmark and
compare well-known clustering algorithms as a part of the proposed method. The
user behaviour clusters were analysed and visualised in a graph format. It was
found that behavioural information can be extracted regarding the users that
belong to these clusters. Such information can be exploited by malicious users
to their advantage. To demonstrate this, a privacy threat model was also
presented based on the results that correspond to multiple potentially affected
areas.

</details>


### [70] [Adjoint Sampling: Highly Scalable Diffusion Samplers via Adjoint Matching](https://arxiv.org/abs/2504.11713)
*Aaron Havens,Benjamin Kurt Miller,Bing Yan,Carles Domingo-Enrich,Anuroop Sriram,Brandon Wood,Daniel Levine,Bin Hu,Brandon Amos,Brian Karrer,Xiang Fu,Guan-Horng Liu,Ricky T. Q. Chen*

Main category: cs.LG

TL;DR: Adjoint Sampling是一种高效、可扩展的算法，用于学习从未归一化密度或能量函数中采样的扩散过程。它是首个支持梯度更新次数远多于能量评估和模型采样次数的策略方法，适用于更大规模问题。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在梯度更新和能量评估效率上的限制，扩展采样方法在计算化学中的应用。

Method: 基于随机最优控制理论，结合对称性和周期性边界条件，适用于分子建模。

Result: 在经典能量函数和神经网络能量模型上验证了方法的有效性，实现了分子系统的摊销构象生成。

Conclusion: Adjoint Sampling为计算化学提供了高效、可扩展的采样方法，并计划开源基准以推动研究。

Abstract: We introduce Adjoint Sampling, a highly scalable and efficient algorithm for
learning diffusion processes that sample from unnormalized densities, or energy
functions. It is the first on-policy approach that allows significantly more
gradient updates than the number of energy evaluations and model samples,
allowing us to scale to much larger problem settings than previously explored
by similar methods. Our framework is theoretically grounded in stochastic
optimal control and shares the same theoretical guarantees as Adjoint Matching,
being able to train without the need for corrective measures that push samples
towards the target distribution. We show how to incorporate key symmetries, as
well as periodic boundary conditions, for modeling molecules in both cartesian
and torsional coordinates. We demonstrate the effectiveness of our approach
through extensive experiments on classical energy functions, and further scale
up to neural network-based energy models where we perform amortized conformer
generation across many molecular systems. To encourage further research in
developing highly scalable sampling methods, we plan to open source these
challenging benchmarks, where successful methods can directly impact progress
in computational chemistry.

</details>


### [71] [Saga: Capturing Multi-granularity Semantics from Massive Unlabelled IMU Data for User Perception](https://arxiv.org/abs/2504.11726)
*Yunzhe Li,Facheng Hu,Hongzi Zhu,Shifan Zhang,Liang Zhang,Shan Chang,Minyi Guo*

Main category: cs.LG

TL;DR: Saga是一种新型的细粒度用户感知方法，仅需少量标记的IMU数据即可实现高精度用户感知。通过预训练特征提取模型和贝叶斯优化，Saga在少量样本下达到与大量样本训练模型相当的准确性。


<details>
  <summary>Details</summary>
Motivation: 解决IMU数据标记困难的问题，减少对大量标记数据的依赖。

Method: 预训练特征提取模型，利用未标记IMU数据的语义信息；贝叶斯优化确定预训练任务的最佳权重。

Result: 仅需每类约100个训练样本，Saga即可达到与上万样本训练模型相当的90%以上准确率。

Conclusion: Saga在少量标记数据下实现了高效的用户感知，具有实际应用潜力。

Abstract: Inertial measurement units (IMUs), have been prevalently used in a wide range
of mobile perception applications such as activity recognition and user
authentication, where a large amount of labelled data are normally required to
train a satisfactory model. However, it is difficult to label micro-activities
in massive IMU data due to the hardness of understanding raw IMU data and the
lack of ground truth. In this paper, we propose a novel fine-grained user
perception approach, called Saga, which only needs a small amount of labelled
IMU data to achieve stunning user perception accuracy. The core idea of Saga is
to first pre-train a backbone feature extraction model, utilizing the rich
semantic information of different levels embedded in the massive unlabelled IMU
data. Meanwhile, for a specific downstream user perception application,
Bayesian Optimization is employed to determine the optimal weights for
pre-training tasks involving different semantic levels. We implement Saga on
five typical mobile phones and evaluate Saga on three typical tasks on three
IMU datasets. Results show that when only using about 100 training samples per
class, Saga can achieve over 90% accuracy of the full-fledged model trained on
over ten thousands training samples with no additional system overhead.

</details>


### [72] [Dynamics and Computational Principles of Echo State Networks: A Mathematical Perspective](https://arxiv.org/abs/2504.11757)
*Pradeep Singh,Ashutosh Kumar,Sutirtha Ghosh,Hrishit B P,Balasubramanian Raman*

Main category: cs.LG

TL;DR: 本文系统探讨了储层计算（RC）的基础特性，包括回声状态特性、衰减记忆和储层容量，并通过动态系统理论分析了其稳定性与表达能力。


<details>
  <summary>Details</summary>
Motivation: 研究RC的理论基础，以揭示其在信号处理、时间序列预测和控制系统中的潜力。

Method: 通过动态系统理论分析RC的特性，包括输入信号与储层状态的相互作用，以及优化和训练方法。

Result: 展示了RC在稳定性和表达能力方面的条件，并探讨了其计算权衡和鲁棒性特性。

Conclusion: RC通过解耦动态储层与线性读出层的训练，实现了高效计算，但仍存在理论挑战和未来研究方向。

Abstract: Reservoir computing (RC) represents a class of state-space models (SSMs)
characterized by a fixed state transition mechanism (the reservoir) and a
flexible readout layer that maps from the state space. It is a paradigm of
computational dynamical systems that harnesses the transient dynamics of
high-dimensional state spaces for efficient processing of temporal data. Rooted
in concepts from recurrent neural networks, RC achieves exceptional
computational power by decoupling the training of the dynamic reservoir from
the linear readout layer, thereby circumventing the complexities of
gradient-based optimization. This work presents a systematic exploration of RC,
addressing its foundational properties such as the echo state property, fading
memory, and reservoir capacity through the lens of dynamical systems theory. We
formalize the interplay between input signals and reservoir states,
demonstrating the conditions under which reservoirs exhibit stability and
expressive power. Further, we delve into the computational trade-offs and
robustness characteristics of RC architectures, extending the discussion to
their applications in signal processing, time-series prediction, and control
systems. The analysis is complemented by theoretical insights into
optimization, training methodologies, and scalability, highlighting open
challenges and potential directions for advancing the theoretical underpinnings
of RC.

</details>


### [73] [Federated Spectral Graph Transformers Meet Neural Ordinary Differential Equations for Non-IID Graphs](https://arxiv.org/abs/2504.11808)
*Kishan Gurumurthy,Himanshu Pal,Charu Sharma*

Main category: cs.LG

TL;DR: 提出了一种基于谱GNN和神经ODE的联邦学习方法，解决了非独立同分布（non-IID）和异质图数据的隐私保护与性能问题。


<details>
  <summary>Details</summary>
Motivation: 现实中的图数据常因隐私、法规和商业竞争无法集中训练GNN，联邦学习提供了一种分布式解决方案，但GNN的联邦学习研究较少。

Method: 结合谱GNN和神经ODE，设计了一种隐私保护且带宽优化的联邦学习方法，适用于非IID和异质图数据。

Result: 在非IID异质图和同质图上均表现出色，性能与现有IID方法相当，适用于社交网络分析、推荐系统等场景。

Conclusion: 该方法展示了联邦学习在复杂图数据中的潜力，代码已开源。

Abstract: Graph Neural Network (GNN) research is rapidly advancing due to GNNs'
capacity to learn distributed representations from graph-structured data.
However, centralizing large volumes of real-world graph data for GNN training
is often impractical due to privacy concerns, regulatory restrictions, and
commercial competition. Federated learning (FL), a distributed learning
paradigm, offers a solution by preserving data privacy with collaborative model
training. Despite progress in training huge vision and language models,
federated learning for GNNs remains underexplored. To address this challenge,
we present a novel method for federated learning on GNNs based on spectral GNNs
equipped with neural ordinary differential equations (ODE) for better
information capture, showing promising results across both homophilic and
heterophilic graphs. Our approach effectively handles non-Independent and
Identically Distributed (non-IID) data, while also achieving performance
comparable to existing methods that only operate on IID data. It is designed to
be privacy-preserving and bandwidth-optimized, making it suitable for
real-world applications such as social network analysis, recommendation
systems, and fraud detection, which often involve complex, non-IID, and
heterophilic graph structures. Our results in the area of federated learning on
non-IID heterophilic graphs demonstrate significant improvements, while also
achieving better performance on homophilic graphs. This work highlights the
potential of federated learning in diverse and challenging graph settings.
Open-source code available on GitHub
(https://github.com/SpringWiz11/Fed-GNODEFormer).

</details>


### [74] [Manifold meta-learning for reduced-complexity neural system identification](https://arxiv.org/abs/2504.11811)
*Marco Forgione,Ankush Chakrabarty,Dario Piga,Matteo Rufolo,Alberto Bemporad*

Main category: cs.LG

TL;DR: 提出了一种基于元学习的框架，通过发现过参数化神经网络参数空间中的低维流形，实现高效训练和推理，适用于小数据场景下的非线性系统辨识。


<details>
  <summary>Details</summary>
Motivation: 深度学习在系统辨识中表现优异，但需要大量数据和计算资源。本文旨在解决这一问题。

Method: 使用元学习框架，通过辅助神经网络将数据集映射到学习到的低维流形上，避免二阶梯度计算，减少推理时的更新次数。

Result: 在Bouc-Wen振荡器上验证，证明该方法在小数据场景下仍能学习准确模型。

Conclusion: 该方法在保持网络表达能力的同时，显著降低了训练和推理的计算成本。

Abstract: System identification has greatly benefited from deep learning techniques,
particularly for modeling complex, nonlinear dynamical systems with partially
unknown physics where traditional approaches may not be feasible. However, deep
learning models often require large datasets and significant computational
resources at training and inference due to their high-dimensional
parameterizations. To address this challenge, we propose a meta-learning
framework that discovers a low-dimensional manifold within the parameter space
of an over-parameterized neural network architecture. This manifold is learned
from a meta-dataset of input-output sequences generated by a class of related
dynamical systems, enabling efficient model training while preserving the
network's expressive power for the considered system class. Unlike bilevel
meta-learning approaches, our method employs an auxiliary neural network to map
datasets directly onto the learned manifold, eliminating the need for costly
second-order gradient computations during meta-training and reducing the number
of first-order updates required in inference, which could be expensive for
large models. We validate our approach on a family of Bouc-Wen oscillators,
which is a well-studied nonlinear system identification benchmark. We
demonstrate that we are able to learn accurate models even in small-data
scenarios.

</details>


### [75] [Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache Offloading](https://arxiv.org/abs/2504.11816)
*Kihyun Kim,Jinwoo Kim,Hyunsun Chung,Myung-Hoon Cha,Hong-Yeon Kim,Youngjae Kim*

Main category: cs.LG

TL;DR: InferSave是一个成本优化的VM选择框架，用于云上LLM推理，通过KV缓存卸载和计算时间校准功能（CTCF）显著降低成本。


<details>
  <summary>Details</summary>
Motivation: 云服务提供商（如AWS）的GPU实例成本高昂，限制了LLM推理的广泛应用。

Method: 提出InferSave框架，优化KV缓存卸载，基于SLO和工作负载特性估计GPU内存需求，并推荐经济高效的VM实例。CTCF功能校准理论与实际GPU性能差异。

Result: 实验表明，无KV缓存卸载的低成本实例可提升在线工作负载成本效率73.7%，而KV缓存卸载为离线工作负载节省20.19%。

Conclusion: InferSave通过智能VM选择和性能校准，显著降低了LLM推理的云成本。

Abstract: LLM inference is essential for applications like text summarization,
translation, and data analysis, but the high cost of GPU instances from Cloud
Service Providers (CSPs) like AWS is a major burden. This paper proposes
InferSave, a cost-efficient VM selection framework for cloud based LLM
inference. InferSave optimizes KV cache offloading based on Service Level
Objectives (SLOs) and workload charac teristics, estimating GPU memory needs,
and recommending cost-effective VM instances. Additionally, the Compute Time
Calibration Function (CTCF) improves instance selection accuracy by adjusting
for discrepancies between theoretical and actual GPU performance. Experiments
on AWS GPU instances show that selecting lower-cost instances without KV cache
offloading improves cost efficiency by up to 73.7% for online workloads, while
KV cache offloading saves up to 20.19% for offline workloads.

</details>


### [76] [Emergence of Computational Structure in a Neural Network Physics Simulator](https://arxiv.org/abs/2504.11830)
*Rohan Hitchcock,Gary W. Delaney,Jonathan H. Manton,Richard Scalzo,Jingge Zhu*

Main category: cs.LG

TL;DR: 论文研究了神经网络中可解释计算结构的涌现机制，通过训练类似Transformer的模型模拟粒子系统物理，发现注意力头中涌现出检测粒子碰撞的结构，且其涌现与损失景观的退化几何相关，遵循幂律动态。


<details>
  <summary>Details</summary>
Motivation: 探索神经网络中可解释计算结构的涌现机制及检测方法。

Method: 使用类似Transformer的模型模拟粒子系统物理，分析注意力头的结构和损失景观的几何特性。

Result: 注意力头中涌现出检测粒子碰撞的结构，其涌现与损失景观的退化几何相关，动态遵循幂律。

Conclusion: 计算结构的涌现可通过研究网络组件的动态来检测，且其受退化“有效势”支配，对神经网络收敛时间有启示。

Abstract: Neural networks often have identifiable computational structures - components
of the network which perform an interpretable algorithm or task - but the
mechanisms by which these emerge and the best methods for detecting these
structures are not well understood. In this paper we investigate the emergence
of computational structure in a transformer-like model trained to simulate the
physics of a particle system, where the transformer's attention mechanism is
used to transfer information between particles. We show that (a) structures
emerge in the attention heads of the transformer which learn to detect particle
collisions, (b) the emergence of these structures is associated to degenerate
geometry in the loss landscape, and (c) the dynamics of this emergence follows
a power law. This suggests that these components are governed by a degenerate
"effective potential". These results have implications for the convergence time
of computational structure within neural networks and suggest that the
emergence of computational structure can be detected by studying the dynamics
of network components.

</details>


### [77] [Support is All You Need for Certified VAE Training](https://arxiv.org/abs/2504.11831)
*Changming Xu,Debangshu Banerjee,Deepak Vasisht,Gagandeep Singh*

Main category: cs.LG

TL;DR: CIVET是一种用于变分自编码器（VAE）认证训练的新方法，通过限制潜在层支持集的误差来提供对抗攻击下的性能保证。


<details>
  <summary>Details</summary>
Motivation: 在安全关键应用中，需要为VAE提供对抗攻击下的性能认证保证。

Method: 提出CIVET方法，通过数学证明潜在层支持集误差的界限，并设计新的训练算法。

Result: 在多个数据集、架构和扰动强度下，CIVET优于现有方法，兼具标准性能和鲁棒性保证。

Conclusion: CIVET为VAE在安全关键应用中的部署提供了有效的认证训练方法。

Abstract: Variational Autoencoders (VAEs) have become increasingly popular and deployed
in safety-critical applications. In such applications, we want to give
certified probabilistic guarantees on performance under adversarial attacks. We
propose a novel method, CIVET, for certified training of VAEs. CIVET depends on
the key insight that we can bound worst-case VAE error by bounding the error on
carefully chosen support sets at the latent layer. We show this point
mathematically and present a novel training algorithm utilizing this insight.
We show in an extensive evaluation across different datasets (in both the
wireless and vision application areas), architectures, and perturbation
magnitudes that our method outperforms SOTA methods achieving good standard
performance with strong robustness guarantees.

</details>


### [78] [On the Problem of Best Arm Retention](https://arxiv.org/abs/2504.11866)
*Houshuang Chen,Yuchen He,Chihao Zhang*

Main category: cs.LG

TL;DR: 本文研究了最佳臂保留（BAR）问题，探讨了其在多臂老虎机流算法中的应用。通过纯探索和遗憾最小化，提出了针对不同变体的理论界限和算法。


<details>
  <summary>Details</summary>
Motivation: BAR问题在多臂老虎机流算法中有重要应用，但目前缺乏系统研究。本文旨在填补这一空白，提供理论支持和算法设计。

Method: 1. 重新审视BAI问题的下界，并扩展KL散度论证到BAR问题；2. 研究r-BAR变体，证明其样本复杂度；3. 开发超越纯探索的遗憾最小化算法。

Result: 1. 推导了BAR问题的PAC算法最优界限；2. 证明了r-BAR问题的紧样本复杂度；3. 提出了针对r-BAR的遗憾最小化算法。

Conclusion: 本文为BAR问题提供了全面的理论分析和算法设计，并提出了关于最优遗憾的猜想，为未来研究指明了方向。

Abstract: This paper presents a comprehensive study on the problem of Best Arm
Retention (BAR), which has recently found applications in streaming algorithms
for multi-armed bandits. In the BAR problem, the goal is to retain $m$ arms
with the best arm included from $n$ after some trials, in stochastic
multi-armed bandit settings. We first investigate pure exploration for the BAR
problem under different criteria, and then minimize the regret with specific
constraints, in the context of further exploration in streaming algorithms.
  - We begin by revisiting the lower bound for the $(\varepsilon,\delta)$-PAC
algorithm for Best Arm Identification (BAI) and adapt the classical
KL-divergence argument to derive optimal bounds for $(\varepsilon,\delta)$-PAC
algorithms for BAR.
  - We further study another variant of the problem, called $r$-BAR, which
requires the expected gap between the best arm and the optimal arm retained is
less than $r$. We prove tight sample complexity for the problem.
  - We explore the regret minimization problem for $r$-BAR and develop
algorithm beyond pure exploration. We conclude with a conjecture on the optimal
regret in this setting.

</details>


### [79] [Transferable Deployment of Semantic Edge Inference Systems via Unsupervised Domain Adaption](https://arxiv.org/abs/2504.11873)
*Weiqiang Jiao,Suzhi Bi,Xian Li,Cheng Guo,Hao Chen,Zhi Quan*

Main category: cs.LG

TL;DR: 论文提出了一种名为DASEIN的领域自适应方法，用于语义边缘推理系统，无需新环境的标注数据即可保持高推理精度。


<details>
  <summary>Details</summary>
Motivation: 在新环境中部署语义边缘推理系统时，数据标注和模型重新训练成本高，因此需要一种成本效益高的迁移方法。

Method: DASEIN结合无监督领域自适应和知识蒸馏技术，通过两步适应过程对齐数据分布并适应信道变化。

Result: 在数据分布显著变化时，DASEIN在推理精度上优于基准方法7.09%和21.33%。

Conclusion: DASEIN在适应数据和信道分布方面有效，适用于实际迁移部署应用。

Abstract: This paper investigates deploying semantic edge inference systems for
performing a common image clarification task. In particular, each system
consists of multiple Internet of Things (IoT) devices that first locally encode
the sensing data into semantic features and then transmit them to an edge
server for subsequent data fusion and task inference. The inference accuracy is
determined by efficient training of the feature encoder/decoder using labeled
data samples. Due to the difference in sensing data and communication channel
distributions, deploying the system in a new environment may induce high costs
in annotating data labels and re-training the encoder/decoder models. To
achieve cost-effective transferable system deployment, we propose an efficient
Domain Adaptation method for Semantic Edge INference systems (DASEIN) that can
maintain high inference accuracy in a new environment without the need for
labeled samples. Specifically, DASEIN exploits the task-relevant data
correlation between different deployment scenarios by leveraging the techniques
of unsupervised domain adaptation and knowledge distillation. It devises an
efficient two-step adaptation procedure that sequentially aligns the data
distributions and adapts to the channel variations. Numerical results show
that, under a substantial change in sensing data distributions, the proposed
DASEIN outperforms the best-performing benchmark method by 7.09% and 21.33% in
inference accuracy when the new environment has similar or 25 dB lower channel
signal to noise power ratios (SNRs), respectively. This verifies the
effectiveness of the proposed method in adapting both data and channel
distributions in practical transfer deployment applications.

</details>


### [80] [Factor-MCLS: Multi-agent learning system with reward factor matrix and multi-critic framework for dynamic portfolio optimization](https://arxiv.org/abs/2504.11874)
*Ruoyu Sun,Angelos Stefanidis,Zhengyong Jiang,Jionglong Su*

Main category: cs.LG

TL;DR: 论文提出了一种基于多评论家框架的奖励因子矩阵和Factor-MCLS学习系统，以解决深度强化学习在动态投资组合优化中难以理解回报和风险因素的问题，并允许投资者根据风险偏好干预训练。


<details>
  <summary>Details</summary>
Motivation: 现有深度强化学习（DRL）代理在动态投资组合优化中仅通过奖励函数输出学习，难以理解回报和风险的具体因素，且无法根据投资者的风险偏好进行干预。

Method: 提出奖励因子矩阵和多评论家框架（Factor-MCLS），通过多评论家网络学习奖励因子矩阵，并在策略函数的训练目标函数中引入风险约束项。

Result: 系统能有效学习投资组合回报和风险的影响因素，并允许投资者根据风险偏好干预训练。

Conclusion: Factor-MCLS系统解决了现有DRL代理的局限性，提升了动态投资组合优化的灵活性和可解释性。

Abstract: Typical deep reinforcement learning (DRL) agents for dynamic portfolio
optimization learn the factors influencing portfolio return and risk by
analyzing the output values of the reward function while adjusting portfolio
weights within the training environment. However, it faces a major limitation
where it is difficult for investors to intervene in the training based on
different levels of risk aversion towards each portfolio asset. This difficulty
arises from another limitation: existing DRL agents may not develop a thorough
understanding of the factors responsible for the portfolio return and risk by
only learning from the output of the reward function. As a result, the strategy
for determining the target portfolio weights is entirely dependent on the DRL
agents themselves. To address these limitations, we propose a reward factor
matrix for elucidating the return and risk of each asset in the portfolio.
Additionally, we propose a novel learning system named Factor-MCLS using a
multi-critic framework that facilitates learning of the reward factor matrix.
In this way, our DRL-based learning system can effectively learn the factors
influencing portfolio return and risk. Moreover, based on the critic networks
within the multi-critic framework, we develop a risk constraint term in the
training objective function of the policy function. This risk constraint term
allows investors to intervene in the training of the DRL agent according to
their individual levels of risk aversion towards the portfolio assets.

</details>


### [81] [Benchmarking Mutual Information-based Loss Functions in Federated Learning](https://arxiv.org/abs/2504.11877)
*Sarang S,Harsh D. Chothani,Qilei Li,Ahmed M. Abdelmoniem,Arnab K. Paul*

Main category: cs.LG

TL;DR: 论文探讨了联邦学习（FL）中基于互信息（MI）的损失函数，以解决公平性和性能问题。


<details>
  <summary>Details</summary>
Motivation: 随着隐私保护需求（如GDPR）的增加，联邦学习受到关注，但其公平性问题（如偏见和性能不均）阻碍了应用。

Method: 利用互信息（MI）的损失函数提取关键特征并减少偏见，以提升FL的公平性和效果。

Result: 通过实验验证，MI-based损失函数能减少客户端间的差异并提升整体性能。

Conclusion: MI-based方法为FL的公平性和有效性提供了可行的解决方案。

Abstract: Federated Learning (FL) has attracted considerable interest due to growing
privacy concerns and regulations like the General Data Protection Regulation
(GDPR), which stresses the importance of privacy-preserving and fair machine
learning approaches. In FL, model training takes place on decentralized data,
so as to allow clients to upload a locally trained model and receive a globally
aggregated model without exposing sensitive information. However, challenges
related to fairness-such as biases, uneven performance among clients, and the
"free rider" issue complicates its adoption. In this paper, we examine the use
of Mutual Information (MI)-based loss functions to address these concerns. MI
has proven to be a powerful method for measuring dependencies between variables
and optimizing deep learning models. By leveraging MI to extract essential
features and minimize biases, we aim to improve both the fairness and
effectiveness of FL systems. Through extensive benchmarking, we assess the
impact of MI-based losses in reducing disparities among clients while enhancing
the overall performance of FL.

</details>


### [82] [HyperSAT: Unsupervised Hypergraph Neural Networks for Weighted MaxSAT Problems](https://arxiv.org/abs/2504.11885)
*Qiyue Chen,Shaolin Tan,Suixiang Gao,Jinhu Lü*

Main category: cs.LG

TL;DR: HyperSAT是一种新颖的神经方法，通过超图神经网络解决加权MaxSAT问题，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的GNN方法在加权MaxSAT问题上表现不足，主要由于权重分布不均导致的非线性依赖和敏感目标函数。

Method: 提出超图表示法，设计交叉注意力机制和共享表示约束损失函数，捕捉正负文字节点间的逻辑交互。

Result: 在多个加权MaxSAT数据集上的实验表明，HyperSAT性能优于现有方法。

Conclusion: HyperSAT通过超图神经网络有效解决了加权MaxSAT问题，展示了其优越性。

Abstract: Graph neural networks (GNNs) have shown promising performance in solving both
Boolean satisfiability (SAT) and Maximum Satisfiability (MaxSAT) problems due
to their ability to efficiently model and capture the structural dependencies
between literals and clauses. However, GNN methods for solving Weighted MaxSAT
problems remain underdeveloped. The challenges arise from the non-linear
dependency and sensitive objective function, which are caused by the
non-uniform distribution of weights across clauses. In this paper, we present
HyperSAT, a novel neural approach that employs an unsupervised hypergraph
neural network model to solve Weighted MaxSAT problems. We propose a hypergraph
representation for Weighted MaxSAT instances and design a cross-attention
mechanism along with a shared representation constraint loss function to
capture the logical interactions between positive and negative literal nodes in
the hypergraph. Extensive experiments on various Weighted MaxSAT datasets
demonstrate that HyperSAT achieves better performance than state-of-the-art
competitors.

</details>


### [83] [FedCanon: Non-Convex Composite Federated Learning with Efficient Proximal Operation on Heterogeneous Data](https://arxiv.org/abs/2504.11903)
*Yuan Zhou,Jiachen Zhong,Xinli Shi,Guanghui Wen,Xinghuo Yu*

Main category: cs.LG

TL;DR: FedCanon是一种新型复合联邦学习算法，通过解耦近端映射和局部更新，减少计算成本，并引入控制变量缓解数据异质性影响。


<details>
  <summary>Details</summary>
Motivation: 现有方法需多次近端操作且易受数据异质性影响，FedCanon旨在解决这些问题。

Method: FedCanon解耦近端映射，每轮迭代仅需一次服务器近端评估，并引入全局梯度信息的控制变量。

Result: 理论分析显示FedCanon在非凸和Polyak-Łojasiewicz条件下分别实现次线性和线性收敛，实验表明其在异质数据下优于现有方法。

Conclusion: FedCanon在计算效率和准确性上优于现有方法，尤其在数据异质性场景下表现突出。

Abstract: Composite federated learning offers a general framework for solving machine
learning problems with additional regularization terms. However, many existing
methods require clients to perform multiple proximal operations to handle
non-smooth terms and their performance are often susceptible to data
heterogeneity. To overcome these limitations, we propose a novel composite
federated learning algorithm called \textbf{FedCanon}, designed to solve the
optimization problems comprising a possibly non-convex loss function and a
weakly convex, potentially non-smooth regularization term. By decoupling
proximal mappings from local updates, FedCanon requires only a single proximal
evaluation on the server per iteration, thereby reducing the overall proximal
computation cost. It also introduces control variables that incorporate global
gradient information into client updates, which helps mitigate the effects of
data heterogeneity. Theoretical analysis demonstrates that FedCanon achieves
sublinear convergence rates under general non-convex settings and linear
convergence under the Polyak-{\L}ojasiewicz condition, without relying on
bounded heterogeneity assumptions. Experiments demonstrate that FedCanon
outperforms the state-of-the-art methods in terms of both accuracy and
computational efficiency, particularly under heterogeneous data distributions.

</details>


### [84] [SemDiff: Generating Natural Unrestricted Adversarial Examples via Semantic Attributes Optimization in Diffusion Models](https://arxiv.org/abs/2504.11923)
*Zeyu Dai,Shengcai Liu,Rui He,Jiahao Wu,Ning Lu,Wenqi Fan,Qing Li,Ke Tang*

Main category: cs.LG

TL;DR: SemDiff是一种新型无约束对抗攻击方法，通过探索扩散模型的语义潜在空间，优化多属性以确保攻击成功，同时保持生成样本的自然性和不可察觉性。


<details>
  <summary>Details</summary>
Motivation: 现有方法生成的UAEs缺乏自然性和不可察觉性，因仅在中间潜在噪声中进行优化。

Method: 提出SemDiff，探索扩散模型的语义潜在空间，采用多属性优化方法。

Result: 在多个高分辨率数据集上实验表明，SemDiff在攻击成功率和不可察觉性上优于现有方法，且能规避多种防御。

Conclusion: SemDiff是一种有效且具有威胁性的无约束对抗攻击方法。

Abstract: Unrestricted adversarial examples (UAEs), allow the attacker to create
non-constrained adversarial examples without given clean samples, posing a
severe threat to the safety of deep learning models. Recent works utilize
diffusion models to generate UAEs. However, these UAEs often lack naturalness
and imperceptibility due to simply optimizing in intermediate latent noises. In
light of this, we propose SemDiff, a novel unrestricted adversarial attack that
explores the semantic latent space of diffusion models for meaningful
attributes, and devises a multi-attributes optimization approach to ensure
attack success while maintaining the naturalness and imperceptibility of
generated UAEs. We perform extensive experiments on four tasks on three
high-resolution datasets, including CelebA-HQ, AFHQ and ImageNet. The results
demonstrate that SemDiff outperforms state-of-the-art methods in terms of
attack success rate and imperceptibility. The generated UAEs are natural and
exhibit semantically meaningful changes, in accord with the attributes'
weights. In addition, SemDiff is found capable of evading different defenses,
which further validates its effectiveness and threatening.

</details>


### [85] [VIPO: Value Function Inconsistency Penalized Offline Reinforcement Learning](https://arxiv.org/abs/2504.11944)
*Xuyang Chen,Guojian Wang,Keyu Yan,Lin Zhao*

Main category: cs.LG

TL;DR: VIPO是一种基于模型的离线强化学习算法，通过自监督反馈提升模型训练效果，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习在风险或成本高的应用中具有优势，但现有方法因模型误差和保守性限制性能。

Method: VIPO通过最小化离线数据直接学习的价值与模型估计价值之间的不一致性来训练模型。

Result: VIPO在D4RL和NeoRL基准测试中几乎在所有任务上达到最先进性能。

Conclusion: VIPO提供了一个通用框架，可提升模型准确性，适用于现有基于模型的离线强化学习算法。

Abstract: Offline reinforcement learning (RL) learns effective policies from
pre-collected datasets, offering a practical solution for applications where
online interactions are risky or costly. Model-based approaches are
particularly advantageous for offline RL, owing to their data efficiency and
generalizability. However, due to inherent model errors, model-based methods
often artificially introduce conservatism guided by heuristic uncertainty
estimation, which can be unreliable. In this paper, we introduce VIPO, a novel
model-based offline RL algorithm that incorporates self-supervised feedback
from value estimation to enhance model training. Specifically, the model is
learned by additionally minimizing the inconsistency between the value learned
directly from the offline data and the one estimated from the model. We perform
comprehensive evaluations from multiple perspectives to show that VIPO can
learn a highly accurate model efficiently and consistently outperform existing
methods. It offers a general framework that can be readily integrated into
existing model-based offline RL algorithms to systematically enhance model
accuracy. As a result, VIPO achieves state-of-the-art performance on almost all
tasks in both D4RL and NeoRL benchmarks.

</details>


### [86] [Hardware-Friendly Delayed-Feedback Reservoir for Multivariate Time-Series Classification](https://arxiv.org/abs/2504.11981)
*Sosei Ikeda,Hiromitsu Awano,Takashi Sato*

Main category: cs.LG

TL;DR: 提出了一种基于点积的储层表示（DPRR）和硬件友好的延迟反馈储层（DFR），用于时间序列分类任务，显著降低了计算复杂度和硬件实现成本。


<details>
  <summary>Details</summary>
Motivation: 现有方法在将储层特征转换为固定长度中间表示时，涉及计算昂贵的矩阵求逆，增加了硬件实现的复杂性和成本。

Method: 提出DPRR作为中间表示，并设计硬件友好的DFR，结合非线性元件和延迟反馈环，实现全数字硬件实现。

Result: 在12个多元时间序列分类任务中，FPGA实现验证了该方法的高精度和小电路规模。

Conclusion: DPRR和DFR的组合为硬件实现提供了一种高效、低成本的时间序列分类解决方案。

Abstract: Reservoir computing (RC) is attracting attention as a machine-learning
technique for edge computing. In time-series classification tasks, the number
of features obtained using a reservoir depends on the length of the input
series. Therefore, the features must be converted to a constant-length
intermediate representation (IR), such that they can be processed by an output
layer. Existing conversion methods involve computationally expensive matrix
inversion that significantly increases the circuit size and requires processing
power when implemented in hardware. In this article, we propose a simple but
effective IR, namely, dot-product-based reservoir representation (DPRR), for RC
based on the dot product of data features. Additionally, we propose a
hardware-friendly delayed-feedback reservoir (DFR) consisting of a nonlinear
element and delayed feedback loop with DPRR. The proposed DFR successfully
classified multivariate time series data that has been considered particularly
difficult to implement efficiently in hardware. In contrast to conventional DFR
models that require analog circuits, the proposed model can be implemented in a
fully digital manner suitable for high-level syntheses. A comparison with
existing machine-learning methods via field-programmable gate array
implementation using 12 multivariate time-series classification tasks confirmed
the superior accuracy and small circuit size of the proposed method.

</details>


### [87] [Secure Transfer Learning: Training Clean Models Against Backdoor in (Both) Pre-trained Encoders and Downstream Datasets](https://arxiv.org/abs/2504.11990)
*Yechao Zhang,Yuxuan Zhou,Tianyu Li,Minghui Li,Shengshan Hu,Wei Luo,Leo Yu Zhang*

Main category: cs.LG

TL;DR: 论文探讨了在资源受限的迁移学习场景中如何缓解预训练编码器中的后门风险，提出了Trusted Core (T-Core)框架，通过识别可信数据和神经元增强模型安全性。


<details>
  <summary>Details</summary>
Motivation: 预训练编码器的广泛使用增加了后门攻击的风险，而现有防御策略在资源受限场景下效果有限，需要更主动的解决方案。

Method: 分析了现有防御策略的局限性，提出T-Core框架，专注于识别可信数据和神经元，并进行实证评估。

Result: T-Core在5种编码器中毒攻击、7种数据集中毒攻击和14种基线防御的评估中表现出优越性。

Conclusion: T-Core框架为资源受限的迁移学习提供了有效的后门防御解决方案，具有实际应用价值。

Abstract: Transfer learning from pre-trained encoders has become essential in modern
machine learning, enabling efficient model adaptation across diverse tasks.
However, this combination of pre-training and downstream adaptation creates an
expanded attack surface, exposing models to sophisticated backdoor embeddings
at both the encoder and dataset levels--an area often overlooked in prior
research. Additionally, the limited computational resources typically available
to users of pre-trained encoders constrain the effectiveness of generic
backdoor defenses compared to end-to-end training from scratch. In this work,
we investigate how to mitigate potential backdoor risks in resource-constrained
transfer learning scenarios. Specifically, we conduct an exhaustive analysis of
existing defense strategies, revealing that many follow a reactive workflow
based on assumptions that do not scale to unknown threats, novel attack types,
or different training paradigms. In response, we introduce a proactive mindset
focused on identifying clean elements and propose the Trusted Core (T-Core)
Bootstrapping framework, which emphasizes the importance of pinpointing
trustworthy data and neurons to enhance model security. Our empirical
evaluations demonstrate the effectiveness and superiority of T-Core,
specifically assessing 5 encoder poisoning attacks, 7 dataset poisoning
attacks, and 14 baseline defenses across five benchmark datasets, addressing
four scenarios of 3 potential backdoor threats.

</details>


### [88] [Analysis of Pseudo-Labeling for Online Source-Free Universal Domain Adaptation](https://arxiv.org/abs/2504.11992)
*Pascal Schlachter,Jonathan Fuss,Bin Yang*

Main category: cs.LG

TL;DR: 论文分析了在线源自由通用域适应（SF-UniDA）中伪标签的作用，发现伪标签质量比数量更重要，并对比了对比损失和交叉熵损失的效果。


<details>
  <summary>Details</summary>
Motivation: 解决实际场景中源数据受限且目标数据为连续流时的域适应问题，尤其是源和目标标签空间不同的情况。

Method: 通过控制实验模拟伪标签，系统分析伪标签对适应结果的影响。

Result: 发现当前方法与完美伪标签的适应上限存在显著差距，对比损失在中等伪标签精度下有效，而交叉熵损失在高精度下表现更优。

Conclusion: 伪标签质量是关键，未来研究应优先关注高质量伪标签。

Abstract: A domain (distribution) shift between training and test data often hinders
the real-world performance of deep neural networks, necessitating unsupervised
domain adaptation (UDA) to bridge this gap. Online source-free UDA has emerged
as a solution for practical scenarios where access to source data is restricted
and target data is received as a continuous stream. However, the open-world
nature of many real-world applications additionally introduces category shifts
meaning that the source and target label spaces may differ. Online source-free
universal domain adaptation (SF-UniDA) addresses this challenge. Existing
methods mainly rely on self-training with pseudo-labels, yet the relationship
between pseudo-labeling and adaptation outcomes has not been studied yet. To
bridge this gap, we conduct a systematic analysis through controlled
experiments with simulated pseudo-labeling, offering valuable insights into
pseudo-labeling for online SF-UniDA. Our findings reveal a substantial gap
between the current state-of-the-art and the upper bound of adaptation achieved
with perfect pseudo-labeling. Moreover, we show that a contrastive loss enables
effective adaptation even with moderate pseudo-label accuracy, while a
cross-entropy loss, though less robust to pseudo-label errors, achieves
superior results when pseudo-labeling approaches perfection. Lastly, our
findings indicate that pseudo-label accuracy is in general more crucial than
quantity, suggesting that prioritizing fewer but high-confidence pseudo-labels
is beneficial. Overall, our study highlights the critical role of
pseudo-labeling in (online) SF-UniDA and provides actionable insights to drive
future advancements in the field. Our code is available at
https://github.com/pascalschlachter/PLAnalysis.

</details>


### [89] [A Computationally Efficient Algorithm for Infinite-Horizon Average-Reward Linear MDPs](https://arxiv.org/abs/2504.11997)
*Kihyuk Hong,Ambuj Tewari*

Main category: cs.LG

TL;DR: 本文提出了一种在无限时间平均奖励线性MDP中的高效值迭代方法，通过优化剪枝操作降低计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有方法在剪枝时需要计算整个状态空间的最小值，计算成本高，尤其是在状态空间大或无限时。

Method: 提出一种仅需在算法访问的状态集上计算最小值的高效剪枝值迭代方法。

Result: 算法在保持相同遗憾界的同时，计算复杂度与状态空间大小无关。

Conclusion: 该方法显著提升了计算效率，适用于大规模或无限状态空间的线性MDP问题。

Abstract: We study reinforcement learning in infinite-horizon average-reward settings
with linear MDPs. Previous work addresses this problem by approximating the
average-reward setting by discounted setting and employing a value
iteration-based algorithm that uses clipping to constrain the span of the value
function for improved statistical efficiency. However, the clipping procedure
requires computing the minimum of the value function over the entire state
space, which is prohibitive since the state space in linear MDP setting can be
large or even infinite. In this paper, we introduce a value iteration method
with efficient clipping operation that only requires computing the minimum of
value functions over the set of states visited by the algorithm. Our algorithm
enjoys the same regret bound as the previous work while being computationally
efficient, with computational complexity that is independent of the size of the
state space.

</details>


### [90] [Balancing Graph Embedding Smoothness in Self-Supervised Learning via Information-Theoretic Decomposition](https://arxiv.org/abs/2504.12011)
*Heesoo Jung,Hogun Park*

Main category: cs.LG

TL;DR: 论文提出了一种平衡图自监督学习（SSL）中嵌入平滑性的框架BSG，通过设计新的损失函数优化邻居表示、最小化和散度损失，显著提升了多种下游任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有图自监督学习方法未能有效平衡嵌入平滑性，导致在不同下游任务中表现不稳定。本文旨在解决这一问题。

Method: 通过信息论框架分解SSL目标为三项（邻居损失、最小化损失和散度损失），并设计BSG框架平衡这些项。

Result: BSG在多个真实数据集上的节点分类和链接预测任务中均达到最优性能。

Conclusion: 平衡嵌入平滑性是提升图自监督学习性能的关键，BSG框架为此提供了有效解决方案。

Abstract: Self-supervised learning (SSL) in graphs has garnered significant attention,
particularly in employing Graph Neural Networks (GNNs) with pretext tasks
initially designed for other domains, such as contrastive learning and feature
reconstruction. However, it remains uncertain whether these methods effectively
reflect essential graph properties, precisely representation similarity with
its neighbors. We observe that existing methods position opposite ends of a
spectrum driven by the graph embedding smoothness, with each end corresponding
to outperformance on specific downstream tasks. Decomposing the SSL objective
into three terms via an information-theoretic framework with a neighbor
representation variable reveals that this polarization stems from an imbalance
among the terms, which existing methods may not effectively maintain. Further
insights suggest that balancing between the extremes can lead to improved
performance across a wider range of downstream tasks. A framework, BSG
(Balancing Smoothness in Graph SSL), introduces novel loss functions designed
to supplement the representation quality in graph-based SSL by balancing the
derived three terms: neighbor loss, minimal loss, and divergence loss. We
present a theoretical analysis of the effects of these loss functions,
highlighting their significance from both the SSL and graph smoothness
perspectives. Extensive experiments on multiple real-world datasets across node
classification and link prediction consistently demonstrate that BSG achieves
state-of-the-art performance, outperforming existing methods. Our
implementation code is available at https://github.com/steve30572/BSG.

</details>


### [91] [Active Human Feedback Collection via Neural Contextual Dueling Bandits](https://arxiv.org/abs/2504.12016)
*Arun Verma,Xiaoqiang Lin,Zhongxiang Dai,Daniela Rus,Bryan Kian Hsiang Low*

Main category: cs.LG

TL;DR: Neural-ADB是一种基于神经上下文对决赌博框架的算法，用于在潜在奖励函数非线性时高效收集人类偏好反馈。


<details>
  <summary>Details</summary>
Motivation: 现有方法假设奖励函数为线性，但现实中许多应用（如在线推荐和LLM对齐）不满足此假设，因此需要更通用的方法。

Method: 提出Neural-ADB算法，基于神经上下文对决赌博框架，适用于非线性奖励函数场景。

Result: 理论证明在Bradley-Terry-Luce模型下，策略的最差次优性差距随数据集增加以次线性速率下降；实验验证了算法的有效性。

Conclusion: Neural-ADB为非线性奖励函数场景提供了一种高效且实用的偏好反馈收集方法。

Abstract: Collecting human preference feedback is often expensive, leading recent works
to develop principled algorithms to select them more efficiently. However,
these works assume that the underlying reward function is linear, an assumption
that does not hold in many real-life applications, such as online
recommendation and LLM alignment. To address this limitation, we propose
Neural-ADB, an algorithm based on the neural contextual dueling bandit
framework that provides a principled and practical method for collecting human
preference feedback when the underlying latent reward function is non-linear.
We theoretically show that when preference feedback follows the
Bradley-Terry-Luce model, the worst sub-optimality gap of the policy learned by
Neural-ADB decreases at a sub-linear rate as the preference dataset increases.
Our experimental results on problem instances derived from synthetic preference
datasets further validate the effectiveness of Neural-ADB.

</details>


### [92] [FedEPA: Enhancing Personalization and Modality Alignment in Multimodal Federated Learning](https://arxiv.org/abs/2504.12025)
*Yu Zhang,Qingfeng Du,Jiaqi Lv*

Main category: cs.LG

TL;DR: FedEPA是一个新型联邦学习框架，针对多模态数据设计，通过个性化聚合和模态对齐策略提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习系统通常假设客户端仅持有单模态数据，且缺乏标签数据，限制了实际应用。FedEPA旨在解决这些问题。

Method: FedEPA采用个性化本地模型聚合策略，利用标签数据学习聚合权重，并通过无监督模态对齐策略分解特征，使用对比学习对齐特征。

Result: 实验表明，FedEPA在标签数据有限的多模态分类任务中显著优于现有方法。

Conclusion: FedEPA通过个性化聚合和模态对齐，有效解决了多模态联邦学习中的数据异质性和标签稀缺问题。

Abstract: Federated Learning (FL) enables decentralized model training across multiple
parties while preserving privacy. However, most FL systems assume clients hold
only unimodal data, limiting their real-world applicability, as institutions
often possess multimodal data. Moreover, the lack of labeled data further
constrains the performance of most FL methods. In this work, we propose FedEPA,
a novel FL framework for multimodal learning. FedEPA employs a personalized
local model aggregation strategy that leverages labeled data on clients to
learn personalized aggregation weights, thereby alleviating the impact of data
heterogeneity. We also propose an unsupervised modality alignment strategy that
works effectively with limited labeled data. Specifically, we decompose
multimodal features into aligned features and context features. We then employ
contrastive learning to align the aligned features across modalities, ensure
the independence between aligned features and context features within each
modality, and promote the diversity of context features. A multimodal feature
fusion strategy is introduced to obtain a joint embedding. The experimental
results show that FedEPA significantly outperforms existing FL methods in
multimodal classification tasks under limited labeled data conditions.

</details>


### [93] [Generative Deep Learning Framework for Inverse Design of Fuels](https://arxiv.org/abs/2504.12075)
*Kiran K. Yalamanchi,Pinaki Pal,Balaji Mohan,Abdullah S. AlRamadan,Jihad A. Badra,Yuanjiang Pei*

Main category: cs.LG

TL;DR: 提出了一种结合Co-VAE和QSPR的生成式深度学习框架，用于加速燃料的逆向设计，优化RON预测和分子重构。


<details>
  <summary>Details</summary>
Motivation: 解决传统燃料筛选方法的局限性，捕捉复杂的结构-性质关系，探索广阔的化学空间以发现高性能燃料。

Method: 使用Co-VAE架构结合QSPR技术，通过超参数调优平衡重构保真度、化学有效性和RON预测，并利用差分进化算法搜索潜在空间。

Result: 成功开发了一种灵活的工具，能够高效识别高RON燃料分子候选者，并展示了方法的可扩展性。

Conclusion: 该方法为燃料的逆向设计提供了新思路，可扩展到其他燃料性质和合成可行性标准，增强新燃料设计的适用性和可靠性。

Abstract: In the present work, a generative deep learning framework combining a
Co-optimized Variational Autoencoder (Co-VAE) architecture with quantitative
structure-property relationship (QSPR) techniques is developed to enable
accelerated inverse design of fuels. The Co-VAE integrates a property
prediction component coupled with the VAE latent space, enhancing molecular
reconstruction and accurate estimation of Research Octane Number (RON) (chosen
as the fuel property of interest). A subset of the GDB-13 database, enriched
with a curated RON database, is used for model training. Hyperparameter tuning
is further utilized to optimize the balance among reconstruction fidelity,
chemical validity, and RON prediction. An independent regression model is then
used to refine RON prediction, while a differential evolution algorithm is
employed to efficiently navigate the VAE latent space and identify promising
fuel molecule candidates with high RON. This methodology addresses the
limitations of traditional fuel screening approaches by capturing complex
structure-property relationships within a comprehensive latent representation.
The generative model provides a flexible tool for systematically exploring vast
chemical spaces, paving the way for discovering fuels with superior anti-knock
properties. The demonstrated approach can be readily extended to incorporate
additional fuel properties and synthesizability criteria to enhance
applicability and reliability for de novo design of new fuels.

</details>


### [94] [Neural Contextual Bandits Under Delayed Feedback Constraints](https://arxiv.org/abs/2504.12086)
*Mohammadali Moghimi,Sharu Theresa Jose,Shana Moothedath*

Main category: cs.LG

TL;DR: 本文提出了一种名为Delayed NeuralUCB的新算法，用于解决神经上下文赌博机中奖励反馈延迟的问题。算法基于UCB探索策略，并针对独立同分布的次指数延迟提供了累积遗憾的上界。实验表明该算法在真实数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 在线推荐系统和临床试验等应用中，奖励反馈通常存在延迟，现有算法难以有效处理这种延迟。本文旨在解决这一问题。

Method: 提出了Delayed NeuralUCB算法，基于UCB探索策略，并考虑了奖励延迟的独立同分布假设。还提出了基于Thompson Sampling的变体Delayed NeuralTS。

Result: 在MNIST和Mushroom等真实数据集上的实验表明，算法能有效管理延迟，并在复杂场景中表现良好。

Conclusion: Delayed NeuralUCB和Delayed NeuralTS算法能够有效处理延迟奖励反馈，适用于实际应用场景。

Abstract: This paper presents a new algorithm for neural contextual bandits (CBs) that
addresses the challenge of delayed reward feedback, where the reward for a
chosen action is revealed after a random, unknown delay. This scenario is
common in applications such as online recommendation systems and clinical
trials, where reward feedback is delayed because the outcomes or results of a
user's actions (such as recommendations or treatment responses) take time to
manifest and be measured. The proposed algorithm, called Delayed NeuralUCB,
uses an upper confidence bound (UCB)-based exploration strategy. Under the
assumption of independent and identically distributed sub-exponential reward
delays, we derive an upper bound on the cumulative regret over a T-length
horizon. We further consider a variant of the algorithm, called Delayed
NeuralTS, that uses Thompson Sampling-based exploration. Numerical experiments
on real-world datasets, such as MNIST and Mushroom, along with comparisons to
benchmark approaches, demonstrate that the proposed algorithms effectively
manage varying delays and are well-suited for complex real-world scenarios.

</details>


### [95] [Watermarking Needs Input Repetition Masking](https://arxiv.org/abs/2504.12229)
*David Khachaturov,Robert Mullins,Ilia Shumailov,Sumanth Dathathri*

Main category: cs.LG

TL;DR: 研究发现人类和未加水印的大语言模型会无意中模仿生成文本的特性，挑战了现有水印技术的可靠性，并提出了改进建议。


<details>
  <summary>Details</summary>
Motivation: 探讨人类和未加水印的LLMs是否会模仿生成文本的特性，从而影响检测和水印技术的可靠性。

Method: 研究人类和LLMs在对话中是否模仿生成文本的特性，包括水印信号。

Result: 发现人类和LLMs均会模仿生成文本的特性，甚至包括水印信号，导致现有检测和水印技术不可靠。

Conclusion: 建议降低误报率并使用更长的词序列来改进水印技术的长期可靠性。

Abstract: Recent advancements in Large Language Models (LLMs) raised concerns over
potential misuse, such as for spreading misinformation. In response two counter
measures emerged: machine learning-based detectors that predict if text is
synthetic, and LLM watermarking, which subtly marks generated text for
identification and attribution. Meanwhile, humans are known to adjust language
to their conversational partners both syntactically and lexically. By
implication, it is possible that humans or unwatermarked LLMs could
unintentionally mimic properties of LLM generated text, making counter measures
unreliable. In this work we investigate the extent to which such conversational
adaptation happens. We call the concept $\textit{mimicry}$ and demonstrate that
both humans and LLMs end up mimicking, including the watermarking signal even
in seemingly improbable settings. This challenges current academic assumptions
and suggests that for long-term watermarking to be reliable, the likelihood of
false positives needs to be significantly lower, while longer word sequences
should be used for seeding watermarking mechanisms.

</details>


### [96] [Towards Explainable Fusion and Balanced Learning in Multimodal Sentiment Analysis](https://arxiv.org/abs/2504.12151)
*Miaosen Luo,Yuncheng Jiang,Sijie Mai*

Main category: cs.LG

TL;DR: KAN-MCP框架结合KAN的可解释性和MCPareto的鲁棒性，解决了多模态情感分析中的模态不平衡和决策逻辑不透明问题。


<details>
  <summary>Details</summary>
Motivation: 多模态情感分析面临模态融合决策逻辑不透明和模态信息密度不均导致的模态不平衡问题。

Method: 提出KAN-MCP框架，利用KAN的单变量函数分解实现跨模态交互的透明分析，并通过MCPareto的DRD-MIB方法降维去噪，动态平衡模态梯度贡献。

Result: 在CMU-MOSI、CMU-MOSEI和CH-SIMS v2等基准数据集上表现优异，并提供直观的可视化界面。

Conclusion: KAN-MCP框架在可解释性和鲁棒性上的协同作用，显著提升了多模态情感分析的性能。

Abstract: Multimodal Sentiment Analysis (MSA) faces two critical challenges: the lack
of interpretability in the decision logic of multimodal fusion and modality
imbalance caused by disparities in inter-modal information density. To address
these issues, we propose KAN-MCP, a novel framework that integrates the
interpretability of Kolmogorov-Arnold Networks (KAN) with the robustness of the
Multimodal Clean Pareto (MCPareto) framework. First, KAN leverages its
univariate function decomposition to achieve transparent analysis of
cross-modal interactions. This structural design allows direct inspection of
feature transformations without relying on external interpretation tools,
thereby ensuring both high expressiveness and interpretability. Second, the
proposed MCPareto enhances robustness by addressing modality imbalance and
noise interference. Specifically, we introduce the Dimensionality Reduction and
Denoising Modal Information Bottleneck (DRD-MIB) method, which jointly denoises
and reduces feature dimensionality. This approach provides KAN with
discriminative low-dimensional inputs to reduce the modeling complexity of KAN
while preserving critical sentiment-related information. Furthermore, MCPareto
dynamically balances gradient contributions across modalities using the
purified features output by DRD-MIB, ensuring lossless transmission of
auxiliary signals and effectively alleviating modality imbalance. This synergy
of interpretability and robustness not only achieves superior performance on
benchmark datasets such as CMU-MOSI, CMU-MOSEI, and CH-SIMS v2 but also offers
an intuitive visualization interface through KAN's interpretable architecture.

</details>


### [97] [Predictive Multiplicity in Survival Models: A Method for Quantifying Model Uncertainty in Predictive Maintenance Applications](https://arxiv.org/abs/2504.12156)
*Mustafa Cavus*

Main category: cs.LG

TL;DR: 本文探讨了预测多样性在生存分析中的影响，提出了量化方法，并展示了其在预测维护中的应用。


<details>
  <summary>Details</summary>
Motivation: 预测多样性可能影响模型的可靠性，但在生存分析中尚未被研究。本文旨在填补这一空白。

Method: 提出了三种量化预测多样性的指标——模糊性、差异性和隐蔽性，并将其应用于生存模型。

Result: 研究发现模糊性高达40-45%，差异性较低但趋势相似，隐蔽性较轻微且集中在少数模型中。

Conclusion: 需明确测量和传达预测多样性，以确保高可靠性决策。

Abstract: In many applications, especially those involving prediction, models may yield
near-optimal performance yet significantly disagree on individual-level
outcomes. This phenomenon, known as predictive multiplicity, has been formally
defined in binary, probabilistic, and multi-target classification, and
undermines the reliability of predictive systems. However, its implications
remain unexplored in the context of survival analysis, which involves
estimating the time until a failure or similar event while properly handling
censored data. We frame predictive multiplicity as a critical concern in
survival-based models and introduce formal measures -- ambiguity, discrepancy,
and obscurity -- to quantify it. This is particularly relevant for downstream
tasks such as maintenance scheduling, where precise individual risk estimates
are essential. Understanding and reporting predictive multiplicity helps build
trust in models deployed in high-stakes environments. We apply our methodology
to benchmark datasets from predictive maintenance, extending the notion of
multiplicity to survival models. Our findings show that ambiguity steadily
increases, reaching up to 40-45% of observations; discrepancy is lower but
exhibits a similar trend; and obscurity remains mild and concentrated in a few
models. These results demonstrate that multiple accurate survival models may
yield conflicting estimations of failure risk and degradation progression for
the same equipment. This highlights the need to explicitly measure and
communicate predictive multiplicity to ensure reliable decision-making in
process health management.

</details>


### [98] [Battery-aware Cyclic Scheduling in Energy-harvesting Federated Learning](https://arxiv.org/abs/2504.12181)
*Eunjeong Jeong,Nikolaos Pappas*

Main category: cs.LG

TL;DR: FedBacys是一个电池感知的联邦学习框架，通过基于用户电池水平的循环客户端参与，优化能源消耗和学习稳定性。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习在能量收集系统中的能源消耗问题，特别是客户端计算导致的能耗和设备可用性波动。

Method: 提出FedBacys框架，通过聚类客户端和顺序调度其参与，减少冗余计算，优化能源使用。

Result: 实验显示FedBacys在能源效率和性能一致性上优于现有方法，且在非独立同分布数据和低充电频率下表现稳健。

Conclusion: FedBacys首次全面评估了循环客户端参与在能量收集联邦学习中的效果，结合通信和计算成本，提出了一种统一的资源感知调度策略。

Abstract: Federated Learning (FL) has emerged as a promising framework for distributed
learning, but its growing complexity has led to significant energy consumption,
particularly from computations on the client side. This challenge is especially
critical in energy-harvesting FL (EHFL) systems, where device availability
fluctuates due to limited and time-varying energy resources. We propose
FedBacys, a battery-aware FL framework that introduces cyclic client
participation based on users' battery levels to cope with these issues.
FedBacys enables clients to save energy and strategically perform local
training just before their designated transmission time by clustering clients
and scheduling their involvement sequentially. This design minimizes redundant
computation, reduces system-wide energy usage, and improves learning stability.
Our experiments demonstrate that FedBacys outperforms existing approaches in
terms of energy efficiency and performance consistency, exhibiting robustness
even under non-i.i.d. training data distributions and with very infrequent
battery charging. This work presents the first comprehensive evaluation of
cyclic client participation in EHFL, incorporating both communication and
computation costs into a unified, resource-aware scheduling strategy.

</details>


### [99] [SCENT: Robust Spatiotemporal Learning for Continuous Scientific Data via Scalable Conditioned Neural Fields](https://arxiv.org/abs/2504.12262)
*David Keetae Park,Xihaier Luo,Guang Zhao,Seungjun Lee,Miruna Oprescu,Shinjae Yoo*

Main category: cs.LG

TL;DR: SCENT是一个新颖的框架，用于可扩展且连续性感知的时空表示学习，统一了插值、重建和预测任务，通过稀疏注意力机制实现高效计算。


<details>
  <summary>Details</summary>
Motivation: 时空学习面临空间和时间依赖的复杂性、高维数据及可扩展性限制，科学领域的数据不规则分布和高容量进一步加剧了这些挑战。

Method: SCENT基于Transformer架构，引入可学习查询和多尺度依赖捕获机制，结合稀疏注意力以实现灵活输出和高效评估。

Result: SCENT在模拟和真实实验中表现出色，实现了最先进的性能，并具备卓越的可扩展性。

Conclusion: SCENT通过统一架构和高效机制，解决了时空学习中的关键挑战，为科学领域提供了强大的工具。

Abstract: Spatiotemporal learning is challenging due to the intricate interplay between
spatial and temporal dependencies, the high dimensionality of the data, and
scalability constraints. These challenges are further amplified in scientific
domains, where data is often irregularly distributed (e.g., missing values from
sensor failures) and high-volume (e.g., high-fidelity simulations), posing
additional computational and modeling difficulties. In this paper, we present
SCENT, a novel framework for scalable and continuity-informed spatiotemporal
representation learning. SCENT unifies interpolation, reconstruction, and
forecasting within a single architecture. Built on a transformer-based
encoder-processor-decoder backbone, SCENT introduces learnable queries to
enhance generalization and a query-wise cross-attention mechanism to
effectively capture multi-scale dependencies. To ensure scalability in both
data size and model complexity, we incorporate a sparse attention mechanism,
enabling flexible output representations and efficient evaluation at arbitrary
resolutions. We validate SCENT through extensive simulations and real-world
experiments, demonstrating state-of-the-art performance across multiple
challenging tasks while achieving superior scalability.

</details>


### [100] [Comparative analysis of unsupervised clustering techniques using validation metrics: Study on cognitive features from the Canadian Longitudinal Study on Aging (CLSA)](https://arxiv.org/abs/2504.12270)
*ChenNingZhi Sheng,Rafal Kustra,Davide Chicco*

Main category: cs.LG

TL;DR: 本研究通过评估指标比较不同聚类算法在认知特征数据上的表现，发现K-means和PAM结果相似，而层次聚类差异显著，强调了熵和分离指数的重要性。


<details>
  <summary>Details</summary>
Motivation: 探索聚类算法在认知特征数据中的应用，发现与痴呆发展相关的临床相关集群。

Method: 使用CLSA数据集，应用K-means、层次聚类和PAM算法，并通过多种内部和比较评估指标进行分析。

Result: K-means和PAM结果相似，层次聚类差异显著；熵和分离指数是关键内部评估指标，ARI是重要比较工具。

Conclusion: 研究有助于理解痴呆，并为医疗数据聚类分析提供方法论支持。

Abstract: Purpose: The primary goal of this study is to explore the application of
evaluation metrics to different clustering algorithms using the data provided
from the Canadian Longitudinal Study (CLSA), focusing on cognitive features.
The objective of our work is to discover potential clinically relevant clusters
that contribute to the development of dementia over time-based on cognitive
changes. Method: The CLSA dataset includes 18,891 participants with data
available at both baseline and follow-up assessments, to which clustering
algorithms were applied. The clustering methodologies employed in this analysis
are K-means (KM) clustering, Hierarchical Clustering (HC) and Partitioning
Around Medoids (PAM). We use multiple evaluation metrics to assess our
analysis. For internal evaluation metrics, we use: Average silhouette Width,
Within and Between the sum of square Ratio (WB.Ratio), Entropy,
Calinski-Harabasz Index (CH Index), and Separation Index. For clustering
comparison metrics, we used: Homogeneity, Completeness, Adjusted Rand Index
(ARI), Rand Index (RI), and Variation Information. Results: Using evaluation
metrics to compare the results of the three clustering techniques, K-means and
Partitioning Around Medoids (PAM) produced similar results. In contrast, there
are significant differences between K-means clustering and Hierarchical
Clustering. Our study highlights the importance of the two internal evaluation
metrics: entropy and separation index. In between clustering comparison
metrics, the Adjusted Rand Index is a key tool. Conclusion: The study results
have the potential to contribute to understanding dementia. Researchers can
also benefit by applying the suggested evaluation metrics to other areas of
healthcare research. Overall, our study improves the understanding of using
clustering techniques and evaluation metrics to reveal complex patterns in
medical data.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [101] [Semantic Matters: Multimodal Features for Affective Analysis](https://arxiv.org/abs/2504.11460)
*Tobias Hallmen,Robin-Nico Kampa,Fabian Deuser,Norbert Oswald,Elisabeth André*

Main category: cs.CV

TL;DR: 论文提出了一种结合音频、文本和视觉模态的方法，用于行为矛盾/犹豫识别和情感模仿强度估计任务，显著提升了基线性能。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过多模态融合（音频、文本、视觉）更全面地分析行为与情感，强调语义内容对理解语音的重要性。

Method: 基于Wav2Vec 2.0预训练模型提取音频特征，结合VAD模块、BERT-like编码器和ViT，再通过LSTM进行时序建模。

Result: 多模态融合方法显著优于基线方法。

Conclusion: 语义和视觉模态的引入提升了模型性能，证明了多模态分析在行为与情感研究中的价值。

Abstract: In this study, we present our methodology for two tasks: the Behavioural
Ambivalence/Hesitancy (BAH) Recognition Challenge and the Emotional Mimicry
Intensity (EMI) Estimation Challenge, both conducted as part of the 8th
Workshop and Competition on Affective & Behavior Analysis in-the-wild. Building
on previous work, we utilize a Wav2Vec 2.0 model pre-trained on a large podcast
dataset to extract various audio features, capturing both linguistic and
paralinguistic information. Our approach incorporates a
valence-arousal-dominance (VAD) module derived from Wav2Vec 2.0, a BERT-like
encoder, and a vision transformer (ViT) with predictions subsequently processed
through a long short-term memory (LSTM) architecture for temporal modeling. In
this iteration, we integrate the textual and visual modality into our analysis,
recognizing that semantic content provides valuable contextual cues and
underscoring that the meaning of speech often conveys more critical insights
than its acoustic counterpart alone. Fusing in the vision modality helps in
some cases to interpret the textual modality more precisely. This combined
approach yields significant performance improvements over baseline methods.

</details>


### [102] [SO-DETR: Leveraging Dual-Domain Features and Knowledge Distillation for Small Object Detection](https://arxiv.org/abs/2504.11470)
*Huaxiang Zhang,Hao Zhang,Aoran Mei,Zhongxue Gan,Guo-Niu Zhu*

Main category: cs.CV

TL;DR: SO-DETR是一种针对小目标检测的Transformer模型，通过双域混合编码器、增强查询选择机制和知识蒸馏策略，显著提升了小目标检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在小目标检测中表现不佳，主要由于编码器难以有效融合低层特征且查询选择策略未针对小目标优化。

Method: 提出双域混合编码器（空间与频域融合）、增强查询选择机制（动态选择高评分锚框）和知识蒸馏策略。

Result: 在VisDrone-2019-DET和UAVVaste数据集上表现优于同类方法。

Conclusion: SO-DETR通过优化特征融合和查询选择，实现了高效的小目标检测。

Abstract: Detection Transformer-based methods have achieved significant advancements in
general object detection. However, challenges remain in effectively detecting
small objects. One key difficulty is that existing encoders struggle to
efficiently fuse low-level features. Additionally, the query selection
strategies are not effectively tailored for small objects. To address these
challenges, this paper proposes an efficient model, Small Object Detection
Transformer (SO-DETR). The model comprises three key components: a dual-domain
hybrid encoder, an enhanced query selection mechanism, and a knowledge
distillation strategy. The dual-domain hybrid encoder integrates spatial and
frequency domains to fuse multi-scale features effectively. This approach
enhances the representation of high-resolution features while maintaining
relatively low computational overhead. The enhanced query selection mechanism
optimizes query initialization by dynamically selecting high-scoring anchor
boxes using expanded IoU, thereby improving the allocation of query resources.
Furthermore, by incorporating a lightweight backbone network and implementing a
knowledge distillation strategy, we develop an efficient detector for small
objects. Experimental results on the VisDrone-2019-DET and UAVVaste datasets
demonstrate that SO-DETR outperforms existing methods with similar
computational demands. The project page is available at
https://github.com/ValiantDiligent/SO_DETR.

</details>


### [103] [Visual moral inference and communication](https://arxiv.org/abs/2504.11473)
*Warren Zhu,Aida Ramezani,Yang Xu*

Main category: cs.CV

TL;DR: 该论文提出了一种支持从自然图像中进行道德推断的计算框架，发现语言-视觉融合模型在视觉道德推断中表现更优。


<details>
  <summary>Details</summary>
Motivation: 人类可以从多种输入来源进行道德推断，而人工智能通常仅依赖文本输入。道德不仅通过语言传达，还通过其他模态表现。

Method: 提出了一个计算框架，支持从自然图像中进行道德推断，并在两项任务中验证：1）推断人类对视觉图像的道德判断；2）分析公共新闻中图像传达的道德内容模式。

Result: 发现仅基于文本的模型无法捕捉人类对视觉刺激的细粒度道德判断，而语言-视觉融合模型在视觉道德推断中表现更好。新闻数据分析揭示了新闻类别和地缘政治讨论中的隐含偏见。

Conclusion: 该研究为自动化视觉道德推断和发现公共媒体中视觉道德沟通模式开辟了新途径。

Abstract: Humans can make moral inferences from multiple sources of input. In contrast,
automated moral inference in artificial intelligence typically relies on
language models with textual input. However, morality is conveyed through
modalities beyond language. We present a computational framework that supports
moral inference from natural images, demonstrated in two related tasks: 1)
inferring human moral judgment toward visual images and 2) analyzing patterns
in moral content communicated via images from public news. We find that models
based on text alone cannot capture the fine-grained human moral judgment toward
visual stimuli, but language-vision fusion models offer better precision in
visual moral inference. Furthermore, applications of our framework to news data
reveal implicit biases in news categories and geopolitical discussions. Our
work creates avenues for automating visual moral inference and discovering
patterns of visual moral communication in public media.

</details>


### [104] [SDIGLM: Leveraging Large Language Models and Multi-Modal Chain of Thought for Structural Damage Identification](https://arxiv.org/abs/2504.11477)
*Yunkai Zhang,Shiyin Wei,Yong Huang,Yawu Su,Shanshan Lu,Hui Li*

Main category: cs.CV

TL;DR: SDIGLM是一种基于多模态大模型的结构损伤识别方法，通过结合视觉和语言能力，显著提升了损伤识别的准确性和描述能力。


<details>
  <summary>Details</summary>
Motivation: 现有计算机视觉模型在结构损伤识别中存在局限性，如无法全面分析复杂损伤类型和缺乏语言描述能力。多模态大模型（LMMs）为解决这些问题提供了新思路。

Method: 基于VisualGLM-6B架构开发SDIGLM，集成U-Net语义分割模块生成视觉链式思考（CoT），并通过多轮对话微调数据集和提示工程增强逻辑推理。

Result: SDIGLM在多种基础设施类型中达到95.24%的识别准确率，并能有效描述损伤特征（如孔洞大小、裂缝方向等）。

Conclusion: SDIGLM通过多模态CoT显著提升了结构损伤识别的性能，为实际工程应用提供了更全面的解决方案。

Abstract: Existing computer vision(CV)-based structural damage identification models
demonstrate notable accuracy in categorizing and localizing damage. However,
these models present several critical limitations that hinder their practical
application in civil engineering(CE). Primarily, their ability to recognize
damage types remains constrained, preventing comprehensive analysis of the
highly varied and complex conditions encountered in real-world CE structures.
Second, these models lack linguistic capabilities, rendering them unable to
articulate structural damage characteristics through natural language
descriptions. With the continuous advancement of artificial intelligence(AI),
large multi-modal models(LMMs) have emerged as a transformative solution,
enabling the unified encoding and alignment of textual and visual data. These
models can autonomously generate detailed descriptive narratives of structural
damage while demonstrating robust generalization across diverse scenarios and
tasks. This study introduces SDIGLM, an innovative LMM for structural damage
identification, developed based on the open-source VisualGLM-6B architecture.
To address the challenge of adapting LMMs to the intricate and varied operating
conditions in CE, this work integrates a U-Net-based semantic segmentation
module to generate defect segmentation maps as visual Chain of Thought(CoT).
Additionally, a multi-round dialogue fine-tuning dataset is constructed to
enhance logical reasoning, complemented by a language CoT formed through prompt
engineering. By leveraging this multi-modal CoT, SDIGLM surpasses
general-purpose LMMs in structural damage identification, achieving an accuracy
of 95.24% across various infrastructure types. Moreover, the model effectively
describes damage characteristics such as hole size, crack direction, and
corrosion severity.

</details>


### [105] [Flux Already Knows - Activating Subject-Driven Image Generation without Training](https://arxiv.org/abs/2504.11478)
*Hao Kang,Stathi Fotiadis,Liming Jiang,Qing Yan,Yumin Jia,Zichuan Liu,Min Jin Chong,Xin Lu*

Main category: cs.CV

TL;DR: 提出了一种基于Flux模型的零样本框架，通过网格图像完成和拼接布局实现主题驱动的图像生成，无需额外数据或训练。


<details>
  <summary>Details</summary>
Motivation: 探索如何利用预训练的基础文本到图像模型实现高质量、资源高效的主题驱动生成，支持轻量级定制。

Method: 采用网格图像完成和拼接布局，结合级联注意力设计和元提示技术，提升保真度和多样性。

Result: 在基准测试和人类偏好研究中表现优于基线方法，支持多种编辑任务。

Conclusion: 预训练模型可实现高效主题驱动生成，为下游应用提供轻量级定制可能。

Abstract: We propose a simple yet effective zero-shot framework for subject-driven
image generation using a vanilla Flux model. By framing the task as grid-based
image completion and simply replicating the subject image(s) in a mosaic
layout, we activate strong identity-preserving capabilities without any
additional data, training, or inference-time fine-tuning. This "free lunch"
approach is further strengthened by a novel cascade attention design and meta
prompting technique, boosting fidelity and versatility. Experimental results
show that our method outperforms baselines across multiple key metrics in
benchmarks and human preference studies, with trade-offs in certain aspects.
Additionally, it supports diverse edits, including logo insertion, virtual
try-on, and subject replacement or insertion. These results demonstrate that a
pre-trained foundational text-to-image model can enable high-quality,
resource-efficient subject-driven generation, opening new possibilities for
lightweight customization in downstream applications.

</details>


### [106] [snnTrans-DHZ: A Lightweight Spiking Neural Network Architecture for Underwater Image Dehazing](https://arxiv.org/abs/2504.11482)
*Vidya Sudevan,Fakhreddine Zayer,Rizwana Kausar,Sajid Javed,Hamad Karki,Giulia De Masi,Jorge Dias*

Main category: cs.CV

TL;DR: snnTrans-DHZ是一种轻量级脉冲神经网络，专为水下图像去雾设计，通过时间动态处理图像序列，高效且低功耗。


<details>
  <summary>Details</summary>
Motivation: 水下图像去雾对海洋视觉操作至关重要，因光散射和吸收会严重降低能见度。

Method: 将静态图像转换为时间序列，通过RGB-LAB颜色空间处理，结合K估计器、背景光估计器和软图像重建模块。

Result: 在UIEB和EUVP数据集上表现优异，PSNR和SSIM指标优于现有方法，且参数少、能耗低。

Conclusion: snnTrans-DHZ高效且低功耗，适合水下机器人、海洋探索和环境监测。

Abstract: Underwater image dehazing is critical for vision-based marine operations
because light scattering and absorption can severely reduce visibility. This
paper introduces snnTrans-DHZ, a lightweight Spiking Neural Network (SNN)
specifically designed for underwater dehazing. By leveraging the temporal
dynamics of SNNs, snnTrans-DHZ efficiently processes time-dependent raw image
sequences while maintaining low power consumption. Static underwater images are
first converted into time-dependent sequences by repeatedly inputting the same
image over user-defined timesteps. These RGB sequences are then transformed
into LAB color space representations and processed concurrently. The
architecture features three key modules: (i) a K estimator that extracts
features from multiple color space representations; (ii) a Background Light
Estimator that jointly infers the background light component from the RGB-LAB
images; and (iii) a soft image reconstruction module that produces haze-free,
visibility-enhanced outputs. The snnTrans-DHZ model is directly trained using a
surrogate gradient-based backpropagation through time (BPTT) strategy alongside
a novel combined loss function. Evaluated on the UIEB benchmark, snnTrans-DHZ
achieves a PSNR of 21.68 dB and an SSIM of 0.8795, and on the EUVP dataset, it
yields a PSNR of 23.46 dB and an SSIM of 0.8439. With only 0.5670 million
network parameters, and requiring just 7.42 GSOPs and 0.0151 J of energy, the
algorithm significantly outperforms existing state-of-the-art methods in terms
of efficiency. These features make snnTrans-DHZ highly suitable for deployment
in underwater robotics, marine exploration, and environmental monitoring.

</details>


### [107] [Graph-Driven Multimodal Feature Learning Framework for Apparent Personality Assessment](https://arxiv.org/abs/2504.11515)
*Kangsheng Wang,Chengwei Ye,Huanzhen Zhang,Linuo Xu,Shuyan Liu*

Main category: cs.CV

TL;DR: 提出了一种多模态特征学习框架，用于短视频中的人格分析，结合视觉、音频和文本特征，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 自动预测人格特质是计算机视觉中的挑战性问题，需要结合多模态信息以提高准确性。

Method: 构建面部图，设计基于Geo的双流网络（GCN和CNN），结合ResNet18、VGGFace、BiGRU、VGGish和XLM-Roberta提取多模态特征，使用通道注意力机制和MLP回归预测人格。

Result: 实验结果表明，该框架性能优于现有最先进方法。

Conclusion: 提出的多模态框架在人格分析任务中表现出色，验证了其有效性。

Abstract: Predicting personality traits automatically has become a challenging problem
in computer vision. This paper introduces an innovative multimodal feature
learning framework for personality analysis in short video clips. For visual
processing, we construct a facial graph and design a Geo-based two-stream
network incorporating an attention mechanism, leveraging both Graph
Convolutional Networks (GCN) and Convolutional Neural Networks (CNN) to capture
static facial expressions. Additionally, ResNet18 and VGGFace networks are
employed to extract global scene and facial appearance features at the frame
level. To capture dynamic temporal information, we integrate a BiGRU with a
temporal attention module for extracting salient frame representations. To
enhance the model's robustness, we incorporate the VGGish CNN for audio-based
features and XLM-Roberta for text-based features. Finally, a multimodal channel
attention mechanism is introduced to integrate different modalities, and a
Multi-Layer Perceptron (MLP) regression model is used to predict personality
traits. Experimental results confirm that our proposed framework surpasses
existing state-of-the-art approaches in performance.

</details>


### [108] [TransitReID: Transit OD Data Collection with Occlusion-Resistant Dynamic Passenger Re-Identification](https://arxiv.org/abs/2504.11500)
*Kaicong Huang,Talha Azfar,Jack Reilly,Ruimin Ke*

Main category: cs.CV

TL;DR: TransitReID是一个基于视觉重识别（ReID）的框架，用于高效收集公交OD数据，解决了遮挡和视角变化等挑战，并在边缘设备上实现近实时操作。


<details>
  <summary>Details</summary>
Motivation: 传统公交OD数据收集方法成本高、效率低，而现有技术如蓝牙和WiFi依赖特定设备，覆盖有限。车载摄像头为边缘设备上的视觉ReID提供了机会，但遮挡和视角变化降低了匹配精度。

Method: TransitReID包含两部分：1）基于变分自编码器的遮挡鲁棒ReID算法，通过重建损失优化权重分配；2）分层存储与动态匹配机制（HSDM），平衡存储、速度和精度。

Result: 实验表明，TransitReID在ReID任务中达到约90%的准确率，并在公交路线模拟中表现优异。

Conclusion: TransitReID为复杂公交环境中的OD数据收集提供了高效、鲁棒的解决方案，并支持边缘设备上的实时操作。

Abstract: Transit Origin-Destination (OD) data are essential for transit planning,
particularly in route optimization and demand-responsive paratransit systems.
Traditional methods, such as manual surveys, are costly and inefficient, while
Bluetooth and WiFi-based approaches require passengers to carry specific
devices, limiting data coverage. On the other hand, most transit vehicles are
equipped with onboard cameras for surveillance, offering an opportunity to
repurpose them for edge-based OD data collection through visual person
re-identification (ReID). However, such approaches face significant challenges,
including severe occlusion and viewpoint variations in transit environments,
which greatly reduce matching accuracy and hinder their adoption. Moreover,
designing effective algorithms that can operate efficiently on edge devices
remains an open challenge. To address these challenges, we propose TransitReID,
a novel framework for individual-level transit OD data collection. TransitReID
consists of two key components: (1) An occlusion-robust ReID algorithm
featuring a variational autoencoder guided region-attention mechanism that
adaptively focuses on visible body regions through reconstruction
loss-optimized weight allocation; and (2) a Hierarchical Storage and Dynamic
Matching (HSDM) mechanism specifically designed for efficient and robust
transit OD matching which balances storage, speed, and accuracy. Additionally,
a multi-threaded design supports near real-time operation on edge devices,
which also ensuring privacy protection. We also introduce a ReID dataset
tailored for complex bus environments to address the lack of relevant training
data. Experimental results demonstrate that TransitReID achieves
state-of-the-art performance in ReID tasks, with an accuracy of approximately
90\% in bus route simulations.

</details>


### [109] [The Devil is in the Prompts: Retrieval-Augmented Prompt Optimization for Text-to-Video Generation](https://arxiv.org/abs/2504.11739)
*Bingjie Gao,Xinyu Gao,Xiaoxue Wu,Yujie Zhou,Yu Qiao,Li Niu,Xinyuan Chen,Yaohui Wang*

Main category: cs.CV

TL;DR: RAPO是一个检索增强的提示优化框架，通过双分支优化方法改进用户提供的提示，提升文本到视频生成的质量。


<details>
  <summary>Details</summary>
Motivation: 现有T2V生成模型对输入提示敏感，但缺乏针对提示词汇和句子结构的优化指导，导致生成结果不准确或模糊。

Method: RAPO通过两个分支优化提示：1）从关系图中提取修饰符增强提示；2）使用预训练LLM根据指令重写提示。

Result: 实验表明，RAPO能显著提升生成视频的静态和动态维度质量。

Conclusion: RAPO证明了提示优化对用户提供提示的重要性，为T2V生成提供了有效解决方案。

Abstract: The evolution of Text-to-video (T2V) generative models, trained on
large-scale datasets, has been marked by significant progress. However, the
sensitivity of T2V generative models to input prompts highlights the critical
role of prompt design in influencing generative outcomes. Prior research has
predominantly relied on Large Language Models (LLMs) to align user-provided
prompts with the distribution of training prompts, albeit without tailored
guidance encompassing prompt vocabulary and sentence structure nuances. To this
end, we introduce \textbf{RAPO}, a novel \textbf{R}etrieval-\textbf{A}ugmented
\textbf{P}rompt \textbf{O}ptimization framework. In order to address potential
inaccuracies and ambiguous details generated by LLM-generated prompts. RAPO
refines the naive prompts through dual optimization branches, selecting the
superior prompt for T2V generation. The first branch augments user prompts with
diverse modifiers extracted from a learned relational graph, refining them to
align with the format of training prompts via a fine-tuned LLM. Conversely, the
second branch rewrites the naive prompt using a pre-trained LLM following a
well-defined instruction set. Extensive experiments demonstrate that RAPO can
effectively enhance both the static and dynamic dimensions of generated videos,
demonstrating the significance of prompt optimization for user-provided
prompts. Project website:
\href{https://whynothaha.github.io/Prompt_optimizer/RAPO.html}{GitHub}.

</details>


### [110] [Efficient Contrastive Decoding with Probabilistic Hallucination Detection - Mitigating Hallucinations in Large Vision Language Models -](https://arxiv.org/abs/2504.12137)
*Laura Fieback,Nishilkumar Balar,Jakob Spiegelberg,Hanno Gottschalk*

Main category: cs.CV

TL;DR: 提出了一种名为ECD的方法，通过对比解码减少大型视觉语言模型（LVLM）的幻觉生成，无需额外训练。


<details>
  <summary>Details</summary>
Motivation: 尽管大型视觉语言模型（LVLM）取得进展，但仍存在与视觉输入不符的幻觉响应问题。

Method: ECD利用概率幻觉检测，通过对比标记概率和幻觉分数，从原始分布中减去幻觉概念。

Result: 实验表明，ECD在多个基准数据集和不同LVLM上有效减少幻觉，性能优于现有方法。

Conclusion: ECD是一种简单高效的方法，可广泛应用于开源LVLM，显著减少幻觉生成。

Abstract: Despite recent advances in Large Vision Language Models (LVLMs), these models
still suffer from generating hallucinatory responses that do not align with the
visual input provided. To mitigate such hallucinations, we introduce Efficient
Contrastive Decoding (ECD), a simple method that leverages probabilistic
hallucination detection to shift the output distribution towards contextually
accurate answers at inference time. By contrasting token probabilities and
hallucination scores, ECD subtracts hallucinated concepts from the original
distribution, effectively suppressing hallucinations. Notably, our proposed
method can be applied to any open-source LVLM and does not require additional
LVLM training. We evaluate our method on several benchmark datasets and across
different LVLMs. Our experiments show that ECD effectively mitigates
hallucinations, outperforming state-of-the-art methods with respect to
performance on LVLM benchmarks and computation time.

</details>


### [111] [Deep Learning Approaches for Medical Imaging Under Varying Degrees of Label Availability: A Comprehensive Survey](https://arxiv.org/abs/2504.11588)
*Siteng Ma,Honghui Du,Yu An,Jing Wang,Qinqin Wang,Haochang Wu,Aonghus Lawlor,Ruihai Dong*

Main category: cs.CV

TL;DR: 本文综述了医学影像中深度学习在有限、不精确或缺失标签下的学习范式，分析了约600项研究，涵盖分类、分割和检测等任务，并探讨了未来挑战。


<details>
  <summary>Details</summary>
Motivation: 医学影像标注耗时且昂贵，因此需要研究在有限、不精确或缺失标签下的学习范式。

Method: 对2018年以来的约600项研究进行分类和综述，涵盖不同医学应用领域的任务。

Result: 总结了不同学习范式的定义、机制和策略，帮助理解当前研究现状。

Conclusion: 未来研究需解决有限标签下的学习挑战，以推动医学影像领域的进一步发展。

Abstract: Deep learning has achieved significant breakthroughs in medical imaging, but
these advancements are often dependent on large, well-annotated datasets.
However, obtaining such datasets poses a significant challenge, as it requires
time-consuming and labor-intensive annotations from medical experts.
Consequently, there is growing interest in learning paradigms such as
incomplete, inexact, and absent supervision, which are designed to operate
under limited, inexact, or missing labels. This survey categorizes and reviews
the evolving research in these areas, analyzing around 600 notable
contributions since 2018. It covers tasks such as image classification,
segmentation, and detection across various medical application areas, including
but not limited to brain, chest, and cardiac imaging. We attempt to establish
the relationships among existing research studies in related areas. We provide
formal definitions of different learning paradigms and offer a comprehensive
summary and interpretation of various learning mechanisms and strategies,
aiding readers in better understanding the current research landscape and
ideas. We also discuss potential future research challenges.

</details>


### [112] [Can GPT tell us why these images are synthesized? Empowering Multimodal Large Language Models for Forensics](https://arxiv.org/abs/2504.11686)
*Yiran He,Yun Cao,Bowen Yang,Zeyu Zhang*

Main category: cs.CV

TL;DR: 该论文探讨了多模态大语言模型（LLMs）在伪造检测中的应用，提出了一种框架，能够评估图像真实性、定位篡改区域并提供证据。通过精心设计的提示工程和少样本学习技术，该方法在实验中表现出色。


<details>
  <summary>Details</summary>
Motivation: 生成式AI的快速发展使得内容创作和图像篡改变得更容易且难以检测，而现有的多模态LLMs虽具备丰富的世界知识，但未针对AI生成内容（AIGC）的检测进行优化。

Method: 提出了一种框架，结合提示工程和少样本学习技术，利用多模态LLMs评估图像真实性、定位篡改区域并提供证据。

Result: 实验表明，GPT4V在Autosplice和LaMa数据集上的准确率分别达到92.1%和86.3%，与最先进的AIGC检测方法竞争。

Conclusion: 多模态LLMs在伪造检测中具有潜力，但仍存在局限性，未来可通过改进提示工程和学习技术进一步提升性能。

Abstract: The rapid development of generative AI facilitates content creation and makes
image manipulation easier and more difficult to detect. While multimodal Large
Language Models (LLMs) have encoded rich world knowledge, they are not
inherently tailored for combating AI-generated Content (AIGC) and struggle to
comprehend local forgery details. In this work, we investigate the application
of multimodal LLMs in forgery detection. We propose a framework capable of
evaluating image authenticity, localizing tampered regions, providing evidence,
and tracing generation methods based on semantic tampering clues. Our method
demonstrates that the potential of LLMs in forgery analysis can be effectively
unlocked through meticulous prompt engineering and the application of few-shot
learning techniques. We conduct qualitative and quantitative experiments and
show that GPT4V can achieve an accuracy of 92.1% in Autosplice and 86.3% in
LaMa, which is competitive with state-of-the-art AIGC detection methods. We
further discuss the limitations of multimodal LLMs in such tasks and propose
potential improvements.

</details>


### [113] [Towards Safe Synthetic Image Generation On the Web: A Multimodal Robust NSFW Defense and Million Scale Dataset](https://arxiv.org/abs/2504.11707)
*Muhammad Shahid Muneer,Simon S. Woo*

Main category: cs.CV

TL;DR: 该论文提出了一种百万规模的多模态数据集和防御方法，用于检测和防止文本到图像模型中的NSFW内容及对抗攻击。


<details>
  <summary>Details</summary>
Motivation: 随着文本到图像（T2I）模型的广泛应用，生成超现实图像的能力带来了NSFW内容泛滥和网络社会污染的问题。现有防御措施容易失效，缺乏鲁棒的多模态数据集和防御方法。

Method: 论文构建了一个百万规模的提示词和图像数据集，并开发了一种多模态防御方法，能够区分安全与NSFW内容，且对抗攻击具有鲁棒性。

Result: 实验表明，该方法在准确率和召回率上优于现有SOTA方法，显著降低了多模态对抗攻击的成功率（ASR）。

Conclusion: 该研究为T2I模型的滥用问题提供了有效的解决方案，推动了更安全的网络环境。

Abstract: In the past years, we have witnessed the remarkable success of Text-to-Image
(T2I) models and their widespread use on the web. Extensive research in making
T2I models produce hyper-realistic images has led to new concerns, such as
generating Not-Safe-For-Work (NSFW) web content and polluting the web society.
To help prevent misuse of T2I models and create a safer web environment for
users features like NSFW filters and post-hoc security checks are used in these
models. However, recent work unveiled how these methods can easily fail to
prevent misuse. In particular, adversarial attacks on text and image modalities
can easily outplay defensive measures. %Exploiting such leads to the growing
concern of preventing adversarial attacks on text and image modalities.
Moreover, there is currently no robust multimodal NSFW dataset that includes
both prompt and image pairs and adversarial examples. This work proposes a
million-scale prompt and image dataset generated using open-source diffusion
models. Second, we develop a multimodal defense to distinguish safe and NSFW
text and images, which is robust against adversarial attacks and directly
alleviates current challenges. Our extensive experiments show that our model
performs well against existing SOTA NSFW detection methods in terms of accuracy
and recall, drastically reducing the Attack Success Rate (ASR) in multimodal
adversarial attack scenarios. Code:
https://github.com/shahidmuneer/multimodal-nsfw-defense.

</details>


### [114] [GrabS: Generative Embodied Agent for 3D Object Segmentation without Scene Supervision](https://arxiv.org/abs/2504.11754)
*Zihui Zhang,Yafei Yang,Hongtao Wen,Bo Yang*

Main category: cs.CV

TL;DR: 提出了一种名为GrabS的两阶段无监督3D物体分割方法，通过生成和判别性先验学习显著提升了分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有无监督方法依赖预训练的2D特征或外部信号（如运动）进行3D点云分割，但通常只能识别简单物体或分割效果较差。

Method: GrabS分为两阶段：首阶段从物体数据集中学习生成和判别性先验，次阶段设计一个代理通过查询先验发现多个物体。

Result: 在真实和合成数据集上的实验表明，GrabS显著优于现有无监督方法。

Conclusion: GrabS通过先验学习和代理查询实现了高效的3D物体分割，为无监督分割提供了新思路。

Abstract: We study the hard problem of 3D object segmentation in complex point clouds
without requiring human labels of 3D scenes for supervision. By relying on the
similarity of pretrained 2D features or external signals such as motion to
group 3D points as objects, existing unsupervised methods are usually limited
to identifying simple objects like cars or their segmented objects are often
inferior due to the lack of objectness in pretrained features. In this paper,
we propose a new two-stage pipeline called GrabS. The core concept of our
method is to learn generative and discriminative object-centric priors as a
foundation from object datasets in the first stage, and then design an embodied
agent to learn to discover multiple objects by querying against the pretrained
generative priors in the second stage. We extensively evaluate our method on
two real-world datasets and a newly created synthetic dataset, demonstrating
remarkable segmentation performance, clearly surpassing all existing
unsupervised methods.

</details>


### [115] [ACMamba: Fast Unsupervised Anomaly Detection via An Asymmetrical Consensus State Space Model](https://arxiv.org/abs/2504.11781)
*Guanchun Wang,Xiangrong Zhang,Yifei Zhang,Zelin Peng,Tianyang Zhang,Xu Tang,Licheng Jiao*

Main category: cs.CV

TL;DR: ACMamba是一种用于高光谱图像异常检测的无监督方法，通过区域级实例替代密集像素级样本，显著降低计算成本，同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: 高光谱图像异常检测的计算成本高，限制了快速部署。研究发现，同质区域内的样本并非全部必要，巧妙采样可降低成本。

Method: 提出ACMamba模型，采用不对称异常检测范式，引入低成本的Mamba模块捕捉区域全局上下文属性，并开发共识学习策略优化背景重建和异常压缩。

Result: 在八个基准测试中，ACMamba表现出更快的速度和更强的性能，优于现有技术。

Conclusion: ACMamba通过高效采样和优化策略，显著降低了计算成本，同时提升了异常检测性能。

Abstract: Unsupervised anomaly detection in hyperspectral images (HSI), aiming to
detect unknown targets from backgrounds, is challenging for earth surface
monitoring. However, current studies are hindered by steep computational costs
due to the high-dimensional property of HSI and dense sampling-based training
paradigm, constraining their rapid deployment. Our key observation is that,
during training, not all samples within the same homogeneous area are
indispensable, whereas ingenious sampling can provide a powerful substitute for
reducing costs. Motivated by this, we propose an Asymmetrical Consensus State
Space Model (ACMamba) to significantly reduce computational costs without
compromising accuracy. Specifically, we design an asymmetrical anomaly
detection paradigm that utilizes region-level instances as an efficient
alternative to dense pixel-level samples. In this paradigm, a low-cost
Mamba-based module is introduced to discover global contextual attributes of
regions that are essential for HSI reconstruction. Additionally, we develop a
consensus learning strategy from the optimization perspective to simultaneously
facilitate background reconstruction and anomaly compression, further
alleviating the negative impact of anomaly reconstruction. Theoretical analysis
and extensive experiments across eight benchmarks verify the superiority of
ACMamba, demonstrating a faster speed and stronger performance over the
state-of-the-art.

</details>


### [116] [Real-World Depth Recovery via Structure Uncertainty Modeling and Inaccurate GT Depth Fitting](https://arxiv.org/abs/2504.11820)
*Delong Suzhang,Meng Yang*

Main category: cs.CV

TL;DR: 论文提出了一种从输入和输出两方面解决深度图恢复泛化问题的方法，通过设计新的深度图生成流程和结构不确定性模块，提高了对未见场景的适应性。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的RGB-D数据集普遍存在低质量深度图结构，且缺乏成对的原始-真实数据，导致现有方法泛化能力不足。

Method: 设计了新的原始深度图生成流程以丰富结构不对齐的多样性，并引入结构不确定性模块；输出端设计了鲁棒的特征对齐模块。

Result: 在多个数据集上的实验表明，该方法在精度和泛化能力上均表现出色。

Conclusion: 该方法通过改进输入和输出策略，显著提升了深度图恢复的泛化能力。

Abstract: The low-quality structure in raw depth maps is prevalent in real-world RGB-D
datasets, which makes real-world depth recovery a critical task in recent
years. However, the lack of paired raw-ground truth (raw-GT) data in the real
world poses challenges for generalized depth recovery. Existing methods
insufficiently consider the diversity of structure misalignment in raw depth
maps, which leads to poor generalization in real-world depth recovery. Notably,
random structure misalignments are not limited to raw depth data but also
affect GT depth in real-world datasets. In the proposed method, we tackle the
generalization problem from both input and output perspectives. For input, we
enrich the diversity of structure misalignment in raw depth maps by designing a
new raw depth generation pipeline, which helps the network avoid overfitting to
a specific condition. Furthermore, a structure uncertainty module is designed
to explicitly identify the misaligned structure for input raw depth maps to
better generalize in unseen scenarios. Notably the well-trained depth
foundation model (DFM) can help the structure uncertainty module estimate the
structure uncertainty better. For output, a robust feature alignment module is
designed to precisely align with the accurate structure of RGB images avoiding
the interference of inaccurate GT depth. Extensive experiments on multiple
datasets demonstrate the proposed method achieves competitive accuracy and
generalization capabilities across various challenging raw depth maps.

</details>


### [117] [Learning Physics-Informed Color-Aware Transforms for Low-Light Image Enhancement](https://arxiv.org/abs/2504.11896)
*Xingxing Yang,Jie Chen,Zaifeng Yang*

Main category: cs.CV

TL;DR: 提出了一种基于物理先验的低光图像增强方法PiCat，通过颜色感知变换和内容-噪声分解网络提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在sRGB空间直接映射低光到正常光图像时存在颜色预测不一致和光谱功率分布敏感性问题。

Method: 提出PiCat框架，包括颜色感知变换（CAT）和内容-噪声分解网络（CNDN），通过物理先验指导图像增强。

Result: 在五个基准数据集上表现优于现有方法。

Conclusion: PiCat通过物理先验和分解网络有效解决了低光图像增强中的颜色和噪声问题。

Abstract: Image decomposition offers deep insights into the imaging factors of visual
data and significantly enhances various advanced computer vision tasks. In this
work, we introduce a novel approach to low-light image enhancement based on
decomposed physics-informed priors. Existing methods that directly map
low-light to normal-light images in the sRGB color space suffer from
inconsistent color predictions and high sensitivity to spectral power
distribution (SPD) variations, resulting in unstable performance under diverse
lighting conditions. To address these challenges, we introduce a
Physics-informed Color-aware Transform (PiCat), a learning-based framework that
converts low-light images from the sRGB color space into deep
illumination-invariant descriptors via our proposed Color-aware Transform
(CAT). This transformation enables robust handling of complex lighting and SPD
variations. Complementing this, we propose the Content-Noise Decomposition
Network (CNDN), which refines the descriptor distributions to better align with
well-lit conditions by mitigating noise and other distortions, thereby
effectively restoring content representations to low-light images. The CAT and
the CNDN collectively act as a physical prior, guiding the transformation
process from low-light to normal-light domains. Our proposed PiCat framework
demonstrates superior performance compared to state-of-the-art methods across
five benchmark datasets.

</details>


### [118] [Securing the Skies: A Comprehensive Survey on Anti-UAV Methods, Benchmarking, and Future Directions](https://arxiv.org/abs/2504.11967)
*Yifei Dong,Fengyi Wu,Sanjian Zhang,Guangyu Chen,Yuzhi Hu,Masumi Yano,Jingdong Sun,Siyu Huang,Feng Liu,Qi Dai,Zhi-Qi Cheng*

Main category: cs.CV

TL;DR: 该论文综述了反无人机（UAV）领域的研究，聚焦分类、检测和跟踪三大目标，并探讨了新兴方法如扩散数据合成、多模态融合等。


<details>
  <summary>Details</summary>
Motivation: 随着无人机的广泛应用，其带来的安全挑战日益突出，亟需开发高效的反无人机系统。

Method: 论文系统评估了单模态和多传感器（RGB、红外、音频等）的最先进解决方案，并讨论了大规模和对抗性基准。

Result: 分析揭示了实时性能、隐身检测和群体场景中的不足，强调了开发鲁棒自适应反无人机系统的迫切需求。

Conclusion: 论文指出了开放研究方向，旨在推动创新并指导下一代防御策略的开发。

Abstract: Unmanned Aerial Vehicles (UAVs) are indispensable for infrastructure
inspection, surveillance, and related tasks, yet they also introduce critical
security challenges. This survey provides a wide-ranging examination of the
anti-UAV domain, centering on three core objectives-classification, detection,
and tracking-while detailing emerging methodologies such as diffusion-based
data synthesis, multi-modal fusion, vision-language modeling, self-supervised
learning, and reinforcement learning. We systematically evaluate
state-of-the-art solutions across both single-modality and multi-sensor
pipelines (spanning RGB, infrared, audio, radar, and RF) and discuss
large-scale as well as adversarially oriented benchmarks. Our analysis reveals
persistent gaps in real-time performance, stealth detection, and swarm-based
scenarios, underscoring pressing needs for robust, adaptive anti-UAV systems.
By highlighting open research directions, we aim to foster innovation and guide
the development of next-generation defense strategies in an era marked by the
extensive use of UAVs.

</details>


### [119] [Bridging the Semantic Gaps: Improving Medical VQA Consistency with LLM-Augmented Question Sets](https://arxiv.org/abs/2504.11777)
*Yongpei Ma,Pengyu Wang,Adam Dunn,Usman Naseem,Jinman Kim*

Main category: cs.CV

TL;DR: 论文提出了一种语义等效问题增强（SEQA）框架，利用大语言模型生成多样但语义等效的问题，以提升医学视觉问答（MVQA）系统的语言多样性和一致性。同时，提出了新的评估指标TAR-SC和其他多样性指标，实验结果显示模型性能和一致性显著提升。


<details>
  <summary>Details</summary>
Motivation: 解决医学视觉问答系统中因问题表述多样性导致的系统不一致性问题。

Method: 提出SEQA框架，利用大语言模型生成语义等效的问题变体，并引入TAR-SC等评估指标。

Result: 实验表明，增强后的数据集显著提升了模型性能（平均准确率提升19.35%）和一致性（TAR-SC提升11.61%）。

Conclusion: SEQA框架和TAR-SC指标有效提升了MVQA系统的语言多样性和一致性。

Abstract: Medical Visual Question Answering (MVQA) systems can interpret medical images
in response to natural language queries. However, linguistic variability in
question phrasing often undermines the consistency of these systems. To address
this challenge, we propose a Semantically Equivalent Question Augmentation
(SEQA) framework, which leverages large language models (LLMs) to generate
diverse yet semantically equivalent rephrasings of questions. Specifically,
this approach enriches linguistic diversity while preserving semantic meaning.
We further introduce an evaluation metric, Total Agreement Rate with
Semantically Equivalent Input and Correct Answer (TAR-SC), which assesses a
model's capability to generate consistent and correct responses to semantically
equivalent linguistic variations. In addition, we also propose three other
diversity metrics - average number of QA items per image (ANQI), average number
of questions per image with the same answer (ANQA), and average number of
open-ended questions per image with the same semantics (ANQS). Using the SEQA
framework, we augmented the benchmarked MVQA public datasets of SLAKE, VQA-RAD,
and PathVQA. As a result, all three datasets achieved significant improvements
by incorporating more semantically equivalent questions: ANQI increased by an
average of 86.1, ANQA by 85.1, and ANQS by 46. Subsequent experiments evaluate
three MVQA models (M2I2, MUMC, and BiomedGPT) under both zero-shot and
fine-tuning settings on the enhanced datasets. Experimental results in MVQA
datasets show that fine-tuned models achieve an average accuracy improvement of
19.35%, while our proposed TAR-SC metric shows an average improvement of 11.
61%, indicating a substantial enhancement in model consistency.

</details>


### [120] [RadMamba: Efficient Human Activity Recognition through Radar-based Micro-Doppler-Oriented Mamba State-Space Model](https://arxiv.org/abs/2504.12039)
*Yizhuo Wu,Francesco Fioranelli,Chang Gao*

Main category: cs.CV

TL;DR: RadMamba是一种高效的雷达微多普勒Mamba SSM模型，用于雷达人体动作识别，显著降低计算复杂度同时保持高精度。


<details>
  <summary>Details</summary>
Motivation: 现有雷达HAR方法计算复杂度高，限制了在资源受限场景的应用，需一种更高效的模型。

Method: 提出RadMamba，基于Mamba SSM架构，专为雷达微多普勒信号设计，参数高效。

Result: 在多个数据集上表现优异，如DIAT数据集99.8%准确率且参数仅为1/400，UoG2020数据集超越其他模型3%。

Conclusion: RadMamba在保持高精度的同时显著降低计算复杂度，适用于资源受限场景。

Abstract: Radar-based HAR has emerged as a promising alternative to conventional
monitoring approaches, such as wearable devices and camera-based systems, due
to its unique privacy preservation and robustness advantages. However, existing
solutions based on convolutional and recurrent neural networks, although
effective, are computationally demanding during deployment. This limits their
applicability in scenarios with constrained resources or those requiring
multiple sensors. Advanced architectures, such as ViT and SSM architectures,
offer improved modeling capabilities and have made efforts toward lightweight
designs. However, their computational complexity remains relatively high. To
leverage the strengths of transformer architectures while simultaneously
enhancing accuracy and reducing computational complexity, this paper introduces
RadMamba, a parameter-efficient, radar micro-Doppler-oriented Mamba SSM
specifically tailored for radar-based HAR. Across three diverse datasets,
RadMamba matches the top-performing previous model's 99.8% classification
accuracy on Dataset DIAT with only 1/400 of its parameters and equals the
leading models' 92.0% accuracy on Dataset CI4R with merely 1/10 of their
parameters. In scenarios with continuous sequences of actions evaluated on
Dataset UoG2020, RadMamba surpasses other models with significantly higher
parameter counts by at least 3%, achieving this with only 6.7k parameters. Our
code is available at: https://github.com/lab-emi/AIRHAR.

</details>


### [121] [Exploring Video-Based Driver Activity Recognition under Noisy Labels](https://arxiv.org/abs/2504.11966)
*Linjuan Fan,Di Wen,Kunyu Peng,Kailun Yang,Jiaming Zhang,Ruiping Liu,Yufan Chen,Junwei Zheng,Jiamin Wu,Xudong Han,Rainer Stiefelhagen*

Main category: cs.CV

TL;DR: 本文提出了一种针对驾驶员行为识别的标签噪声学习方法，通过聚类假设和样本选择策略提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现实世界视频数据常包含错误标签，影响模型可靠性，而标签噪声学习在驾驶员行为识别领域尚未充分探索。

Method: 基于聚类假设，学习低维表示并分簇，通过共细化平滑分类器输出，提出无超参数的样本选择策略。

Result: 在公开数据集Drive&Act上表现优于其他图像分类领域的标签去噪方法。

Conclusion: 该方法为驾驶员行为识别领域的标签噪声学习提供了有效解决方案，代码已开源。

Abstract: As an open research topic in the field of deep learning, learning with noisy
labels has attracted much attention and grown rapidly over the past ten years.
Learning with label noise is crucial for driver distraction behavior
recognition, as real-world video data often contains mislabeled samples,
impacting model reliability and performance. However, label noise learning is
barely explored in the driver activity recognition field. In this paper, we
propose the first label noise learning approach for the driver activity
recognition task. Based on the cluster assumption, we initially enable the
model to learn clustering-friendly low-dimensional representations from given
videos and assign the resultant embeddings into clusters. We subsequently
perform co-refinement within each cluster to smooth the classifier outputs.
Furthermore, we propose a flexible sample selection strategy that combines two
selection criteria without relying on any hyperparameters to filter clean
samples from the training dataset. We also incorporate a self-adaptive
parameter into the sample selection process to enforce balancing across
classes. A comprehensive variety of experiments on the public Drive&Act dataset
for all granularity levels demonstrates the superior performance of our method
in comparison with other label-denoising methods derived from the image
classification field. The source code is available at
https://github.com/ilonafan/DAR-noisy-labels.

</details>


### [122] [AttentionDrop: A Novel Regularization Method for Transformer Models](https://arxiv.org/abs/2504.12088)
*Mirza Samad Ahmed Baig,Syeda Anshrah Gillani,Abdul Akbar Khan,Shahid Munir Shah*

Main category: cs.CV

TL;DR: AttentionDrop是一种针对Transformer架构的随机正则化技术，通过直接操作自注意力分布来防止过拟合，包含三种变体：硬注意力掩码、模糊注意力平滑和一致性正则化。


<details>
  <summary>Details</summary>
Motivation: Transformer架构在大规模任务中表现优异，但其巨大容量容易导致过拟合，尤其是在数据有限或噪声较多时。

Method: 提出三种AttentionDrop变体：1. 硬注意力掩码：随机屏蔽每查询的前k个注意力对数；2. 模糊注意力平滑：对注意力对数应用动态高斯卷积；3. 一致性正则化：通过KL损失强制输出稳定性。

Result: AttentionDrop有效缓解了过拟合问题，提升了模型在有限或噪声数据下的性能。

Conclusion: AttentionDrop为Transformer提供了一种灵活且高效的正则化方法，适用于多种任务。

Abstract: Transformer-based architectures achieve state-of-the-art performance across a
wide range of tasks in natural language processing, computer vision, and
speech. However, their immense capacity often leads to overfitting, especially
when training data is limited or noisy. We propose AttentionDrop, a unified
family of stochastic regularization techniques that operate directly on the
self-attention distributions. We introduces three variants: 1. Hard Attention
Masking: randomly zeroes out top-k attention logits per query to encourage
diverse context utilization. 2. Blurred Attention Smoothing: applies a dynamic
Gaussian convolution over attention logits to diffuse overly peaked
distributions. 3. Consistency-Regularized AttentionDrop: enforces output
stability under multiple independent AttentionDrop perturbations via a KL-based
consistency loss.

</details>


### [123] [pix2pockets: Shot Suggestions in 8-Ball Pool from a Single Image in the Wild](https://arxiv.org/abs/2504.12045)
*Jonas Myhre Schiøtt,Viktor Sebastian Petersen,Dimitrios P. Papadopoulos*

Main category: cs.CV

TL;DR: 提出了一种基于强化学习的台球教练系统pix2pockets，通过图像检测台球桌和球，并推荐最佳击球策略。


<details>
  <summary>Details</summary>
Motivation: 结合计算机视觉和强化学习在台球中的应用，提升台球训练的智能化水平。

Method: 构建了包含195张图像的标注数据集，开发了目标检测模型和标准化RL环境，用于球位检测和击球策略推荐。

Result: 目标检测AP50达91.2，球位误差仅0.4厘米；RL算法在击球任务中表现有限，但简单基线模型单次击球成功率94.7%。

Conclusion: pix2pockets为台球训练提供了有效工具，但RL算法仍需改进以实现更高成功率。

Abstract: Computer vision models have seen increased usage in sports, and reinforcement
learning (RL) is famous for beating humans in strategic games such as Chess and
Go. In this paper, we are interested in building upon these advances and
examining the game of classic 8-ball pool. We introduce pix2pockets, a
foundation for an RL-assisted pool coach. Given a single image of a pool table,
we first aim to detect the table and the balls and then propose the optimal
shot suggestion. For the first task, we build a dataset with 195 diverse images
where we manually annotate all balls and table dots, leading to 5748 object
segmentation masks. For the second task, we build a standardized RL environment
that allows easy development and benchmarking of any RL algorithm. Our object
detection model yields an AP50 of 91.2 while our ball location pipeline obtains
an error of only 0.4 cm. Furthermore, we compare standard RL algorithms to set
a baseline for the shot suggestion task and we show that all of them fail to
pocket all balls without making a foul move. We also present a simple baseline
that achieves a per-shot success rate of 94.7% and clears a full game in a
single turn 30% of the time.

</details>


### [124] [RADLER: Radar Object Detection Leveraging Semantic 3D City Models and Self-Supervised Radar-Image Learning](https://arxiv.org/abs/2504.12167)
*Yuan Luo,Rudolf Hoffmann,Yan Xia,Olaf Wysocki,Benedikt Schwab,Thomas H. Kolbe,Daniel Cremers*

Main category: cs.CV

TL;DR: 论文提出了一种利用语义3D城市模型增强雷达目标检测的新方法RADLER，通过对比自监督学习和特征融合策略，显著提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 语义3D城市模型具有丰富的信息，但其在雷达目标检测中的潜力尚未充分挖掘。

Method: 提出RADLER网络，结合对比自监督学习和语义3D城市模型的特征融合策略。

Result: 在RadarCity数据集上，mAP和mAR分别平均提升5.46%和3.51%。

Conclusion: 该方法为语义引导和地图支持的雷达目标检测研究提供了新方向。

Abstract: Semantic 3D city models are worldwide easy-accessible, providing accurate,
object-oriented, and semantic-rich 3D priors. To date, their potential to
mitigate the noise impact on radar object detection remains under-explored. In
this paper, we first introduce a unique dataset, RadarCity, comprising 54K
synchronized radar-image pairs and semantic 3D city models. Moreover, we
propose a novel neural network, RADLER, leveraging the effectiveness of
contrastive self-supervised learning (SSL) and semantic 3D city models to
enhance radar object detection of pedestrians, cyclists, and cars.
Specifically, we first obtain the robust radar features via a SSL network in
the radar-image pretext task. We then use a simple yet effective feature fusion
strategy to incorporate semantic-depth features from semantic 3D city models.
Having prior 3D information as guidance, RADLER obtains more fine-grained
details to enhance radar object detection. We extensively evaluate RADLER on
the collected RadarCity dataset and demonstrate average improvements of 5.46%
in mean avarage precision (mAP) and 3.51% in mean avarage recall (mAR) over
previous radar object detection methods. We believe this work will foster
further research on semantic-guided and map-supported radar object detection.
Our project page is publicly available
athttps://gpp-communication.github.io/RADLER .

</details>


### [125] [CoMotion: Concurrent Multi-person 3D Motion](https://arxiv.org/abs/2504.12186)
*Alejandro Newell,Peiyun Hu,Lahav Lipson,Stephan R. Richter,Vladlen Koltun*

Main category: cs.CV

TL;DR: 提出了一种从单目摄像头流中检测和跟踪多人详细3D姿态的方法，支持在拥挤场景中保持时间一致性。


<details>
  <summary>Details</summary>
Motivation: 解决在拥挤场景中因复杂姿态和遮挡导致的多人3D姿态检测与跟踪问题。

Method: 结合逐帧检测和学习姿态更新，直接从新输入图像更新姿态，实现在线跟踪。

Result: 模型在3D姿态估计精度上达到先进水平，同时在多人跟踪中更快更准。

Conclusion: 该方法在复杂场景中实现了高效的多人3D姿态跟踪，代码和权重已开源。

Abstract: We introduce an approach for detecting and tracking detailed 3D poses of
multiple people from a single monocular camera stream. Our system maintains
temporally coherent predictions in crowded scenes filled with difficult poses
and occlusions. Our model performs both strong per-frame detection and a
learned pose update to track people from frame to frame. Rather than match
detections across time, poses are updated directly from a new input image,
which enables online tracking through occlusion. We train on numerous image and
video datasets leveraging pseudo-labeled annotations to produce a model that
matches state-of-the-art systems in 3D pose estimation accuracy while being
faster and more accurate in tracking multiple people through time. Code and
weights are provided at https://github.com/apple/ml-comotion

</details>


### [126] [Uncertainty-Guided Coarse-to-Fine Tumor Segmentation with Anatomy-Aware Post-Processing](https://arxiv.org/abs/2504.12215)
*Ilkin Sevgi Isler,David Mohaisen,Curtis Lisle,Damla Turgut,Ulas Bagci*

Main category: cs.CV

TL;DR: 提出了一种基于不确定性引导的粗到细分割框架，结合解剖学后处理，显著提升了胸部CT中肿瘤分割的准确性。


<details>
  <summary>Details</summary>
Motivation: 胸部CT中肿瘤分割因边界模糊、类别不平衡和解剖变异性而具有挑战性。

Method: 采用两阶段模型：第一阶段生成粗预测，第二阶段通过不确定性感知损失函数和基于解剖学的后处理细化ROI分割。

Result: 在私有和公共数据集上，Dice和Hausdorff分数均有提升，假阳性减少。在Orlando数据集上，Swin UNETR Dice从0.4690提升至0.6447。

Conclusion: 结合不确定性建模和解剖学先验的分割框架能够实现更稳健且临床意义显著的肿瘤分割。

Abstract: Reliable tumor segmentation in thoracic computed tomography (CT) remains
challenging due to boundary ambiguity, class imbalance, and anatomical
variability. We propose an uncertainty-guided, coarse-to-fine segmentation
framework that combines full-volume tumor localization with refined
region-of-interest (ROI) segmentation, enhanced by anatomically aware
post-processing. The first-stage model generates a coarse prediction, followed
by anatomically informed filtering based on lung overlap, proximity to lung
surfaces, and component size. The resulting ROIs are segmented by a
second-stage model trained with uncertainty-aware loss functions to improve
accuracy and boundary calibration in ambiguous regions. Experiments on private
and public datasets demonstrate improvements in Dice and Hausdorff scores, with
fewer false positives and enhanced spatial interpretability. These results
highlight the value of combining uncertainty modeling and anatomical priors in
cascaded segmentation pipelines for robust and clinically meaningful tumor
delineation. On the Orlando dataset, our framework improved Swin UNETR Dice
from 0.4690 to 0.6447. Reduction in spurious components was strongly correlated
with segmentation gains, underscoring the value of anatomically informed
post-processing.

</details>


### [127] [FLIP Reasoning Challenge](https://arxiv.org/abs/2504.12256)
*Andreas Plesner,Turlan Kuzhagaliyev,Roger Wattenhofer*

Main category: cs.CV

TL;DR: FLIP数据集是一个基于Idena区块链人类验证任务的多模态AI推理能力评估基准，测试结果表明现有模型在零样本设置下表现远低于人类水平。


<details>
  <summary>Details</summary>
Motivation: 尽管AI在感知和生成任务上取得进展，但推理能力仍是一个挑战，FLIP旨在填补这一空白。

Method: 通过视觉语言模型（VLMs）和大语言模型（LLMs）评估模型在FLIP数据集上的表现，并探索图像描述模型对推理的帮助。

Result: 最佳开源和闭源模型的零样本准确率分别为75.5%和77.9%，远低于人类的95.3%；图像描述模型能提升性能，集成15个模型后准确率达85.2%。

Conclusion: FLIP突显了现有推理模型的局限性，强调了多模态基准的重要性，代码和数据集将开源。

Abstract: Over the past years, advances in artificial intelligence (AI) have
demonstrated how AI can solve many perception and generation tasks, such as
image classification and text writing, yet reasoning remains a challenge. This
paper introduces the FLIP dataset, a benchmark for evaluating AI reasoning
capabilities based on human verification tasks on the Idena blockchain. FLIP
challenges present users with two orderings of 4 images, requiring them to
identify the logically coherent one. By emphasizing sequential reasoning,
visual storytelling, and common sense, FLIP provides a unique testbed for
multimodal AI systems. Our experiments evaluate state-of-the-art models,
leveraging both vision-language models (VLMs) and large language models (LLMs).
Results reveal that even the best open-sourced and closed-sourced models
achieve maximum accuracies of 75.5% and 77.9%, respectively, in zero-shot
settings, compared to human performance of 95.3%. Captioning models aid
reasoning models by providing text descriptions of images, yielding better
results than when using the raw images directly, 69.6% vs. 75.2% for Gemini 1.5
Pro. Combining the predictions from 15 models in an ensemble increases the
accuracy to 85.2%. These findings highlight the limitations of existing
reasoning models and the need for robust multimodal benchmarks like FLIP. The
full codebase and dataset will be available at
https://github.com/aplesner/FLIP-Reasoning-Challenge.

</details>


### [128] [How Do I Do That? Synthesizing 3D Hand Motion and Contacts for Everyday Interactions](https://arxiv.org/abs/2504.12284)
*Aditya Prakash,Benjamin Lundell,Dmitry Andreychuk,David Forsyth,Saurabh Gupta,Harpreet Sawhney*

Main category: cs.CV

TL;DR: 提出了一种基于单目RGB图像、动作文本和3D接触点预测3D手部运动和接触图的方法，结合VQVAE和Transformer解码器，实验效果优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 解决从单目RGB图像、动作文本和3D接触点预测3D手部运动和接触图的新问题，填补现有研究的空白。

Method: 使用VQVAE模型学习手部姿势和接触点的潜在码本（Interaction Codebook），并通过Transformer解码器模块（Interaction Predictor）预测交互轨迹。数据引擎从HoloAssist数据集中提取3D手部姿势和接触轨迹。

Result: 在比现有工作大2.5-10倍的基准测试中，模型在对象和交互多样性上表现优异，且能泛化到不同对象类别、动作类别、任务和场景。

Conclusion: 该方法在预测3D手部运动和接触图方面优于Transformer和扩散基线，展示了其有效性和泛化能力。

Abstract: We tackle the novel problem of predicting 3D hand motion and contact maps (or
Interaction Trajectories) given a single RGB view, action text, and a 3D
contact point on the object as input. Our approach consists of (1) Interaction
Codebook: a VQVAE model to learn a latent codebook of hand poses and contact
points, effectively tokenizing interaction trajectories, (2) Interaction
Predictor: a transformer-decoder module to predict the interaction trajectory
from test time inputs by using an indexer module to retrieve a latent
affordance from the learned codebook. To train our model, we develop a data
engine that extracts 3D hand poses and contact trajectories from the diverse
HoloAssist dataset. We evaluate our model on a benchmark that is 2.5-10X larger
than existing works, in terms of diversity of objects and interactions
observed, and test for generalization of the model across object categories,
action categories, tasks, and scenes. Experimental results show the
effectiveness of our approach over transformer & diffusion baselines across all
settings.

</details>


### [129] [SHeaP: Self-Supervised Head Geometry Predictor Learned via 2D Gaussians](https://arxiv.org/abs/2504.12292)
*Liam Schoneveld,Zhe Chen,Davide Davoli,Jiapeng Tang,Saimon Terazawa,Ko Nishino,Matthias Nießner*

Main category: cs.CV

TL;DR: 提出了一种基于2D高斯分布的自监督学习方法SHeaP，用于从单目图像和视频中实时重建3D头部几何，显著提升了自监督学习的效果。


<details>
  <summary>Details</summary>
Motivation: 由于大规模3D真实数据难以获取，现有方法通常依赖2D视频进行自监督学习，但传统方法存在局限性。

Method: 通过预测3DMM网格和一组与网格绑定的高斯分布，利用重动画和光度损失反向传播优化预测网络。

Result: 在NoW基准测试和新表情基准测试中，几何评估表现优于现有自监督方法，且生成的网格在情感分类中表现优异。

Conclusion: SHeaP方法通过高斯分布渲染显著提升了自监督学习效果，适用于中性及非中性表情的3D头部重建。

Abstract: Accurate, real-time 3D reconstruction of human heads from monocular images
and videos underlies numerous visual applications. As 3D ground truth data is
hard to come by at scale, previous methods have sought to learn from abundant
2D videos in a self-supervised manner. Typically, this involves the use of
differentiable mesh rendering, which is effective but faces limitations. To
improve on this, we propose SHeaP (Self-supervised Head Geometry Predictor
Learned via 2D Gaussians). Given a source image, we predict a 3DMM mesh and a
set of Gaussians that are rigged to this mesh. We then reanimate this rigged
head avatar to match a target frame, and backpropagate photometric losses to
both the 3DMM and Gaussian prediction networks. We find that using Gaussians
for rendering substantially improves the effectiveness of this self-supervised
approach. Training solely on 2D data, our method surpasses existing
self-supervised approaches in geometric evaluations on the NoW benchmark for
neutral faces and a new benchmark for non-neutral expressions. Our method also
produces highly expressive meshes, outperforming state-of-the-art in emotion
classification.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [130] [Characterizing and Optimizing LLM Inference Workloads on CPU-GPU Coupled Architectures](https://arxiv.org/abs/2504.11750)
*Prabhu Vellaisamy,Thomas Labonte,Sourav Chakraborty,Matt Turner,Samantika Sury,John Paul Shen*

Main category: cs.DC

TL;DR: 本文分析了大型语言模型（LLM）推理在松散耦合（PCIe A100/H100）和紧密耦合（GH200）系统上的性能表现，发现GH200在大批量时表现更优，但在小批量时受限于CPU性能。


<details>
  <summary>Details</summary>
Motivation: 随着LLM推理工作负载在数据中心成本和资源占用中的比重增加，理解其在CPU-GPU耦合架构上的行为对优化至关重要。

Method: 通过新型分析工具SKIP和指标TKLQT，对LLM推理行为进行细粒度分析，比较松散耦合和紧密耦合系统的性能。

Result: GH200在大批量时性能显著优于松散耦合系统，但在小批量时受限于CPU性能；TKLQT能准确识别CPU/GPU性能瓶颈。

Conclusion: 紧密耦合系统在大批量时表现更优，但需优化小批量性能；内核融合是潜在优化方向。

Abstract: Large language model (LLM)-based inference workloads increasingly dominate
data center costs and resource utilization. Therefore, understanding the
inference workload characteristics on evolving CPU-GPU coupled architectures is
crucial for optimization. This paper presents an in-depth analysis of LLM
inference behavior on loosely-coupled (PCIe A100/H100) and closely-coupled
(GH200) systems. We analyze performance dynamics using fine-grained
operator-to-kernel trace analysis, facilitated by our novel profiler SKIP and
metrics like Total Kernel Launch and Queuing Time (TKLQT). Results show that
closely-coupled (CC) GH200 significantly outperforms loosely-coupled (LC)
systems at large batch sizes, achieving 1.9x-2.7x faster prefill latency for
Llama 3.2-1B. However, our analysis also reveals that GH200 remains CPU-bound
up to 4x larger batch sizes than LC systems. In this extended CPU-bound region,
we identify the performance characteristics of the Grace CPU as a key factor
contributing to higher inference latency at low batch sizes on GH200. We
demonstrate that TKLQT accurately identifies this CPU/GPU-bound transition
point. Based on this analysis, we further show that kernel fusion offers
significant potential to mitigate GH200's low-batch latency bottleneck by
reducing kernel launch overhead. This detailed kernel-level characterization
provides critical insights for optimizing diverse CPU-GPU coupling strategies.
This work is an initial effort, and we plan to explore other major AI/DL
workloads that demand different degrees of CPU-GPU heterogeneous architectures.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [131] [Do Segmentation Models Understand Vascular Structure? A Blob-Based XAI Framework](https://arxiv.org/abs/2504.11469)
*Guillaume Garret,Antoine Vacavant,Carole Frindel*

Main category: eess.IV

TL;DR: 论文提出了一种新的可解释性方法，用于分析3D血管分割模型是否利用全局解剖结构，发现模型主要依赖局部特征而非全局血管属性。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在医学图像分割中表现优异，但其黑箱特性限制了临床应用。血管分割需要结合局部图像线索和全局解剖结构（如血管连通性），但模型是否利用全局上下文尚不明确。

Method: 提出了一种结合梯度归因、图引导点选择和显著性图斑点分析的可解释性流程，通过血管图定义解剖学兴趣点（POIs），并分析输入体素的贡献。

Result: 在IRCAD和Bullitt数据集上的分析表明，模型决策主要由集中在POIs附近的局部归因斑点主导，归因特征与血管厚度、管状性或连通性等全局属性相关性低。

Conclusion: 研究强调了结构化可解释性工具的重要性，并揭示了当前分割模型在捕捉全局血管上下文方面的局限性。

Abstract: Deep learning models have achieved impressive performance in medical image
segmentation, yet their black-box nature limits clinical adoption. In vascular
applications, trustworthy segmentation should rely on both local image cues and
global anatomical structures, such as vessel connectivity or branching.
However, the extent to which models leverage such global context remains
unclear. We present a novel explainability pipeline for 3D vessel segmentation,
combining gradient-based attribution with graph-guided point selection and a
blob-based analysis of Saliency maps. Using vascular graphs extracted from
ground truth, we define anatomically meaningful points of interest (POIs) and
assess the contribution of input voxels via Saliency maps. These are analyzed
at both global and local scales using a custom blob detector. Applied to IRCAD
and Bullitt datasets, our analysis shows that model decisions are dominated by
highly localized attribution blobs centered near POIs. Attribution features
show little correlation with vessel-level properties such as thickness,
tubularity, or connectivity -- suggesting limited use of global anatomical
reasoning. Our results underline the importance of structured explainability
tools and highlight the current limitations of segmentation models in capturing
global vascular context.

</details>


### [132] [Local Temporal Feature Enhanced Transformer with ROI-rank Based Masking for Diagnosis of ADHD](https://arxiv.org/abs/2504.11474)
*Byunggun Kim,Younghun Kwon*

Main category: eess.IV

TL;DR: 提出了一种基于Transformer的ADHD诊断模型，通过rs-fMRI数据学习时空特征和注意力结构，显著提升了诊断性能。


<details>
  <summary>Details</summary>
Motivation: ADHD是一种常见的精神疾病，现有诊断方法需要更高效和准确的工具。

Method: 结合CNN嵌入块、局部时间注意力和基于ROI排名的掩码，学习BOLD信号和关键脑区。

Result: 在939名个体的数据上，模型表现优于其他变体（ACC 77.78，SPE 76.60，SEN 79.22，AUC 79.30）。

Conclusion: 该模型为ADHD诊断提供了更有效的时空生物标志物和工具。

Abstract: In modern society, Attention-Deficit/Hyperactivity Disorder (ADHD) is one of
the common mental diseases discovered not only in children but also in adults.
In this context, we propose a ADHD diagnosis transformer model that can
effectively simultaneously find important brain spatiotemporal biomarkers from
resting-state functional magnetic resonance (rs-fMRI). This model not only
learns spatiotemporal individual features but also learns the correlation with
full attention structures specialized in ADHD diagnosis. In particular, it
focuses on learning local blood oxygenation level dependent (BOLD) signals and
distinguishing important regions of interest (ROI) in the brain. Specifically,
the three proposed methods for ADHD diagnosis transformer are as follows.
First, we design a CNN-based embedding block to obtain more expressive
embedding features in brain region attention. It is reconstructed based on the
previously CNN-based ADHD diagnosis models for the transformer. Next, for
individual spatiotemporal feature attention, we change the attention method to
local temporal attention and ROI-rank based masking. For the temporal features
of fMRI, the local temporal attention enables to learn local BOLD signal
features with only simple window masking. For the spatial feature of fMRI,
ROI-rank based masking can distinguish ROIs with high correlation in ROI
relationships based on attention scores, thereby providing a more specific
biomarker for ADHD diagnosis. The experiment was conducted with various types
of transformer models. To evaluate these models, we collected the data from 939
individuals from all sites provided by the ADHD-200 competition. Through this,
the spatiotemporal enhanced transformer for ADHD diagnosis outperforms the
performance of other different types of transformer variants. (77.78ACC
76.60SPE 79.22SEN 79.30AUC)

</details>


### [133] [Attention GhostUNet++: Enhanced Segmentation of Adipose Tissue and Liver in CT Images](https://arxiv.org/abs/2504.11491)
*Mansoor Hayat,Supavadee Aramvith,Subrata Bhattacharjee,Nouman Ahmad*

Main category: eess.IV

TL;DR: 提出了一种名为Attention GhostUNet++的新型深度学习模型，用于自动精确分割腹部脂肪组织和肝脏，显著提升了分割精度和计算效率。


<details>
  <summary>Details</summary>
Motivation: 准确分割腹部脂肪组织（SAT和VAT）及肝脏对于理解身体成分和相关健康风险（如2型糖尿病和心血管疾病）至关重要。

Method: 在Ghost UNet++的瓶颈中引入了通道、空间和深度注意力机制，构建了Attention GhostUNet++模型。

Result: 在AATTCT-IDS和LiTS数据集上，模型的分割Dice系数分别为VAT 0.9430、SAT 0.9639和肝脏0.9652，优于基线模型。

Conclusion: 尽管在边界细节分割上存在小局限，但该模型显著提升了特征细化、上下文理解和计算效率，为身体成分分析提供了可靠解决方案。

Abstract: Accurate segmentation of abdominal adipose tissue, including subcutaneous
(SAT) and visceral adipose tissue (VAT), along with liver segmentation, is
essential for understanding body composition and associated health risks such
as type 2 diabetes and cardiovascular disease. This study proposes Attention
GhostUNet++, a novel deep learning model incorporating Channel, Spatial, and
Depth Attention mechanisms into the Ghost UNet++ bottleneck for automated,
precise segmentation. Evaluated on the AATTCT-IDS and LiTS datasets, the model
achieved Dice coefficients of 0.9430 for VAT, 0.9639 for SAT, and 0.9652 for
liver segmentation, surpassing baseline models. Despite minor limitations in
boundary detail segmentation, the proposed model significantly enhances feature
refinement, contextual understanding, and computational efficiency, offering a
robust solution for body composition analysis. The implementation of the
proposed Attention GhostUNet++ model is available
at:https://github.com/MansoorHayat777/Attention-GhostUNetPlusPlus.

</details>


### [134] [Learned enclosure method for experimental EIT data](https://arxiv.org/abs/2504.11512)
*Sara Sippola,Siiri Rautio,Andreas Hauptmann,Takanori Ide,Samuli Siltanen*

Main category: eess.IV

TL;DR: 本文提出了一种结合Ikehata的包围法和神经网络的方法，用于从边界测量中估计包含物的凸包，并在实验数据上验证了其性能。


<details>
  <summary>Details</summary>
Motivation: 电阻抗断层扫描（EIT）的逆问题是非线性且高度不适定的，传统方法难以准确解决。近年来，结合分析方法和机器学习解决逆问题的方法受到关注。

Method: 结合Ikehata的包围法和神经网络，从边界测量中估计包含物的凸包。

Result: 与传统的基于最小二乘拟合的包围法相比，该方法在模拟和实验数据上均表现出更优的性能。

Conclusion: 该方法为EIT逆问题提供了一种有效的解决方案，展示了机器学习与传统分析方法结合的潜力。

Abstract: Electrical impedance tomography (EIT) is a non-invasive imaging method with
diverse applications, including medical imaging and non-destructive testing.
The inverse problem of reconstructing internal electrical conductivity from
boundary measurements is nonlinear and highly ill-posed, making it difficult to
solve accurately. In recent years, there has been growing interest in combining
analytical methods with machine learning to solve inverse problems. In this
paper, we propose a method for estimating the convex hull of inclusions from
boundary measurements by combining the enclosure method proposed by Ikehata
with neural networks. We demonstrate its performance using experimental data.
Compared to the classical enclosure method with least squares fitting, the
learned convex hull achieves superior performance on both simulated and
experimental data.

</details>


### [135] [Comparative Evaluation of Radiomics and Deep Learning Models for Disease Detection in Chest Radiography](https://arxiv.org/abs/2504.12249)
*Zhijin He,Alan B. McMillan*

Main category: eess.IV

TL;DR: 该研究评估了基于放射组学和深度学习的AI模型在胸部X光片中对COVID-19、肺不张和病毒性肺炎的诊断效果，比较了不同模型的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 探索AI在医学影像诊断中的应用，特别是在数据有限的情况下，为临床实践提供可靠的AI工具选择依据。

Method: 系统比较了放射组学模型（如决策树、梯度提升等）和深度学习模型（如CNN、ViT等）的性能，分析了不同样本量下的表现。

Result: 研究揭示了不同AI模型在不同场景下的诊断效能，为临床选择提供了指导。

Conclusion: 该研究填补了AI模型选择的关键空白，为临床实践中的AI工具整合提供了依据。

Abstract: The application of artificial intelligence (AI) in medical imaging has
revolutionized diagnostic practices, enabling advanced analysis and
interpretation of radiological data. This study presents a comprehensive
evaluation of radiomics-based and deep learning-based approaches for disease
detection in chest radiography, focusing on COVID-19, lung opacity, and viral
pneumonia. While deep learning models, particularly convolutional neural
networks (CNNs) and vision transformers (ViTs), learn directly from image data,
radiomics-based models extract and analyze quantitative features, potentially
providing advantages in data-limited scenarios. This study systematically
compares the diagnostic accuracy and robustness of various AI models, including
Decision Trees, Gradient Boosting, Random Forests, Support Vector Machines
(SVM), and Multi-Layer Perceptrons (MLP) for radiomics, against
state-of-the-art computer vision deep learning architectures. Performance
metrics across varying sample sizes reveal insights into each model's efficacy,
highlighting the contexts in which specific AI approaches may offer enhanced
diagnostic capabilities. The results aim to inform the integration of AI-driven
diagnostic tools in clinical practice, particularly in automated and
high-throughput environments where timely, reliable diagnosis is critical. This
comparative study addresses an essential gap, establishing guidance for the
selection of AI models based on clinical and operational needs.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [136] [Transformer-Driven Neural Beamforming with Imperfect CSI in Urban Macro Wireless Channels](https://arxiv.org/abs/2504.11667)
*Cemil Vahapoglu,Timothy J. O'Shea,Wan Liu,Tamoghna Roy,Sennur Ulukus*

Main category: cs.IT

TL;DR: 提出了一种结合深度可分离卷积和Transformer的无监督深度学习框架，用于在CSI不完美的情况下为MU-SIMO系统生成波束成形权重，以提高吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现有方法在密集城市环境中处理高维MIMO系统数据时效率不足，需结合深度可分离卷积和Transformer以提升性能。

Method: 提出NNBF框架，整合深度可分离卷积和Transformer，在MU-SIMO系统中生成波束成形权重。

Result: 实验表明，NNBF在频谱效率和BLER上优于ZFBF和MMSE基线方法。

Conclusion: NNBF框架在密集城市环境中显著提升了通信性能和吞吐量。

Abstract: The literature is abundant with methodologies focusing on using transformer
architectures due to their prominence in wireless signal processing and their
capability to capture long-range dependencies via attention mechanisms. In
particular, depthwise separable convolutions enhance parameter efficiency for
the process of high-dimensional data characteristics of MIMO systems. In this
work, we introduce a novel unsupervised deep learning framework that integrates
depthwise separable convolutions and transformers to generate beamforming
weights under imperfect channel state information (CSI) for a multi-user
single-input multiple-output (MU-SIMO) system in dense urban environments. The
primary goal is to enhance throughput by maximizing sum-rate while ensuring
reliable communication. Spectral efficiency and block error rate (BLER) are
considered as performance metrics. Experiments are carried out under various
conditions to compare the performance of the proposed NNBF framework against
baseline methods zero-forcing beamforming (ZFBF) and minimum mean square error
(MMSE) beamforming. Experimental results demonstrate the superiority of the
proposed framework over the baseline techniques.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [137] [Sub-optimality of the Separation Principle for Quadratic Control from Bilinear Observations](https://arxiv.org/abs/2504.11555)
*Yahya Sattar,Sunmook Choi,Yassir Jedra,Maryam Fazel,Sarah Dean*

Main category: math.OC

TL;DR: 论文研究了从双线性观测中控制线性动态系统的最小二次成本问题，发现与标准LQG控制不同，分离原理不成立，最优控制器非线性且非唯一。


<details>
  <summary>Details</summary>
Motivation: 探讨双线性观测模型下线性动态系统控制问题，揭示其与标准LQG控制的差异。

Method: 分析双线性观测模型下的控制问题，推导最优控制器特性及输入依赖可观测性条件。

Result: 发现最优控制器非线性且非唯一，标准LQG控制器可能局部最大化成本。

Conclusion: 双线性观测模型下控制问题具有复杂性，需进一步研究非线性控制器设计。

Abstract: We consider the problem of controlling a linear dynamical system from
bilinear observations with minimal quadratic cost. Despite the similarity of
this problem to standard linear quadratic Gaussian (LQG) control, we show that
when the observation model is bilinear, neither does the Separation Principle
hold, nor is the optimal controller affine in the estimated state. Moreover,
the cost-to-go is non-convex in the control input. Hence, finding an analytical
expression for the optimal feedback controller is difficult in general. Under
certain settings, we show that the standard LQG controller locally maximizes
the cost instead of minimizing it. Furthermore, the optimal controllers
(derived analytically) are not unique and are nonlinear in the estimated state.
We also introduce a notion of input-dependent observability and derive
conditions under which the Kalman filter covariance remains bounded. We
illustrate our theoretical results through numerical experiments in multiple
synthetic settings.

</details>


### [138] [Traffic Adaptive Moving-window Service Patrolling for Real-time Incident Management during High-impact Events](https://arxiv.org/abs/2504.11570)
*Haozhe Lei,Ya-Ting Yang,Tao Li,Zilin Bian,Fan Zuo,Sundeep Rangan,Kaan Ozbay*

Main category: math.OC

TL;DR: TAMPA算法通过动态规划和实时投诉估计优化巡逻部署，显著提升事件管理效率。


<details>
  <summary>Details</summary>
Motivation: 大型活动如体育赛事和音乐会给交通网络带来巨大压力，需要高效且自适应的巡逻解决方案。

Method: 结合预测交通模型和实时投诉估计，使用动态规划和Dvoretzky-Kiefer-Wolfowitz不等式动态调整巡逻策略。

Result: 模拟结果显示，TAMPA比静态方法提升87.5%，比随机策略提升114.2%。

Conclusion: TAMPA在实时事件管理中表现优异，未来可结合数字孪生技术进一步提升预测准确性。

Abstract: This paper presents the Traffic Adaptive Moving-window Patrolling Algorithm
(TAMPA), designed to improve real-time incident management during major events
like sports tournaments and concerts. Such events significantly stress
transportation networks, requiring efficient and adaptive patrol solutions.
TAMPA integrates predictive traffic modeling and real-time complaint
estimation, dynamically optimizing patrol deployment. Using dynamic
programming, the algorithm continuously adjusts patrol strategies within short
planning windows, effectively balancing immediate response and efficient
routing. Leveraging the Dvoretzky-Kiefer-Wolfowitz inequality, TAMPA detects
significant shifts in complaint patterns, triggering proactive adjustments in
patrol routes. Theoretical analyses ensure performance remains closely aligned
with optimal solutions. Simulation results from an urban traffic network
demonstrate TAMPA's superior performance, showing improvements of approximately
87.5\% over stationary methods and 114.2\% over random strategies. Future work
includes enhancing adaptability and incorporating digital twin technology for
improved predictive accuracy, particularly relevant for events like the 2026
FIFA World Cup at MetLife Stadium.

</details>


### [139] [Efficient identification of linear, parameter-varying, and nonlinear systems with noise models](https://arxiv.org/abs/2504.11982)
*Alberto Bemporad,Roland Tóth*

Main category: math.OC

TL;DR: 提出了一种通用的系统辨识方法，能够估计多种状态空间动态模型，包括LTI、LPV和非线性动态模型，并支持多种噪声模型。通过分离确定性和随机噪声部分，灵活调整模型复杂度。采用神经网络参数化非线性关系，优化预测误差准则，显著提升计算效率。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在处理非线性动态模型和复杂噪声模型时的效率不足问题，提供一种高效且通用的系统辨识方案。

Method: 将模型动态分为确定性和随机噪声部分，用神经网络参数化非线性关系，通过约束拟牛顿法和自动微分优化预测误差准则。

Result: 在多个基准测试中展示了更高的估计精度和计算效率，训练时间从小时级缩短至秒级。

Conclusion: 该方法为复杂动态系统辨识提供了高效且通用的解决方案，具有广泛的应用潜力。

Abstract: We present a general system identification procedure capable of estimating of
a broad spectrum of state-space dynamical models, including linear
time-invariant (LTI), linear parameter-varying} (LPV), and nonlinear (NL)
dynamics, along with rather general classes of noise models. Similar to the LTI
case, we show that for this general class of model structures, including the NL
case, the model dynamics can be separated into a deterministic process and a
stochastic noise part, allowing to seamlessly tune the complexity of the
combined model both in terms of nonlinearity and noise modeling. We
parameterize the involved nonlinear functional relations by means of artificial
neural-networks (ANNs), although alternative parametric nonlinear mappings can
also be used. To estimate the resulting model structures, we optimize a
prediction-error-based criterion using an efficient combination of a
constrained quasi-Newton approach and automatic differentiation, achieving
training times in the order of seconds compared to existing state-of-the-art
ANN methods which may require hours for models of similar complexity. We
formally establish the consistency guarantees for the proposed approach and
demonstrate its superior estimation accuracy and computational efficiency on
several benchmark LTI, LPV, and NL system identification problems.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [140] [HLS-Eval: A Benchmark and Framework for Evaluating LLMs on High-Level Synthesis Design Tasks](https://arxiv.org/abs/2504.12268)
*Stefan Abi-Karam,Cong Hao*

Main category: cs.AR

TL;DR: HLS-Eval是首个用于评估LLM在HLS设计任务中的完整基准和框架，包括代码生成和优化任务，提供94个设计案例和自动化评估工具。


<details>
  <summary>Details</summary>
Motivation: 现有评估LLM在HLS设计任务中的基准和工具不足，需要全面评估LLM在HLS代码生成和优化中的能力。

Method: 提出HLS-Eval基准和框架，包括94个设计案例和自动化评估工具，支持并行评估和多种LLM交互模式。

Result: 通过基线评估开源LLM在Vitis HLS上的表现，测量解析性、可编译性、可运行性和可合成性等指标。

Conclusion: HLS-Eval为LLM在硬件设计中的应用提供了标准化评估工具和基线结果，推动社区发展。

Abstract: The rapid scaling of large language model (LLM) training and inference has
driven their adoption in semiconductor design across academia and industry.
While most prior work evaluates LLMs on hardware description language (HDL)
tasks, particularly Verilog, designers are increasingly using high-level
synthesis (HLS) to build domain-specific accelerators and complex hardware
systems. However, benchmarks and tooling to comprehensively evaluate LLMs for
HLS design tasks remain scarce.
  To address this, we introduce HLS-Eval, the first complete benchmark and
evaluation framework for LLM-driven HLS design. HLS-Eval targets two core
tasks: (1) generating HLS code from natural language descriptions, and (2)
performing HLS-specific code edits to optimize performance and hardware
efficiency. The benchmark includes 94 unique designs drawn from standard HLS
benchmarks and novel sources. Each case is prepared via a semi-automated flow
that produces a natural language description and a paired testbench for
C-simulation and synthesis validation, ensuring each task is "LLM-ready."
  Beyond the benchmark, HLS-Eval offers a modular Python framework for
automated, parallel evaluation of both local and hosted LLMs. It includes a
parallel evaluation engine, direct HLS tool integration, and abstractions for
to support different LLM interaction paradigms, enabling rapid prototyping of
new benchmarks, tasks, and LLM methods.
  We demonstrate HLS-Eval through baseline evaluations of open-source LLMs on
Vitis HLS, measuring outputs across four key metrics - parseability,
compilability, runnability, and synthesizability - reflecting the iterative HLS
design cycle. We also report pass@k metrics, establishing clear baselines and
reusable infrastructure for the broader LLM-for-hardware community.
  All benchmarks, framework code, and results are open-sourced at
https://github.com/stefanpie/hls-eval.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [141] [Learning Strategies in Particle Swarm Optimizer: A Critical Review and Performance Analysis](https://arxiv.org/abs/2504.11812)
*Dikshit Chauhan,Shivani,P. N. Suganthan*

Main category: cs.NE

TL;DR: 该论文综述并分类了粒子群优化（PSO）中的学习策略，评估其对优化性能的影响，并通过实验比较分析了这些策略对搜索动态的影响。


<details>
  <summary>Details</summary>
Motivation: 尽管已有许多学习策略用于提升PSO的性能，但缺乏对这些策略的全面系统分析。

Method: 论文回顾并分类了各种学习策略，并通过实验比较评估了它们对PSO搜索动态的影响。

Result: 研究发现不同学习策略对PSO的收敛速度、鲁棒性和适应性有显著影响。

Conclusion: 未来需要开发自适应的智能PSO变体，以应对日益复杂的现实问题。

Abstract: Nature has long inspired the development of swarm intelligence (SI), a key
branch of artificial intelligence that models collective behaviors observed in
biological systems for solving complex optimization problems. Particle swarm
optimization (PSO) is widely adopted among SI algorithms due to its simplicity
and efficiency. Despite numerous learning strategies proposed to enhance PSO's
performance in terms of convergence speed, robustness, and adaptability, no
comprehensive and systematic analysis of these strategies exists. We review and
classify various learning strategies to address this gap, assessing their
impact on optimization performance. Additionally, a comparative experimental
evaluation is conducted to examine how these strategies influence PSO's search
dynamics. Finally, we discuss open challenges and future directions,
emphasizing the need for self-adaptive, intelligent PSO variants capable of
addressing increasingly complex real-world problems.

</details>


### [142] [EngramNCA: a Neural Cellular Automaton Model of Memory Transfer](https://arxiv.org/abs/2504.11855)
*Etienne Guichard,Felix Reimers,Mia Kvalsund,Mikkel Lepperød,Stefano Nichele*

Main category: cs.NE

TL;DR: EngramNCA是一种结合公开可见状态和私有内部记忆通道的神经细胞自动机，受生物记忆存储机制的启发，支持复杂形态的编码和传播。


<details>
  <summary>Details</summary>
Motivation: 受生物记忆存储不仅限于突触修饰还包括细胞内机制的启发，研究旨在探索分散式记忆存储和传递在人工系统中的实现。

Method: EngramNCA由GeneCA和GenePropCA组成，前者训练生成特定形态，后者调节私有记忆而不改变可见状态。

Result: 模型支持层次化和共存形态的涌现，为人工系统中的自适应自组织提供可能。

Conclusion: 研究对生物和合成系统中的记忆机制理解有潜在贡献，并为自适应系统开发提供新思路。

Abstract: This study introduces EngramNCA, a neural cellular automaton (NCA) that
integrates both publicly visible states and private, cell-internal memory
channels, drawing inspiration from emerging biological evidence suggesting that
memory storage extends beyond synaptic modifications to include intracellular
mechanisms. The proposed model comprises two components: GeneCA, an NCA trained
to develop distinct morphologies from seed cells containing immutable "gene"
encodings, and GenePropCA, an auxiliary NCA that modulates the private
"genetic" memory of cells without altering their visible states. This
architecture enables the encoding and propagation of complex morphologies
through the interaction of visible and private channels, facilitating the
growth of diverse structures from a shared "genetic" substrate. EngramNCA
supports the emergence of hierarchical and coexisting morphologies, offering
insights into decentralized memory storage and transfer in artificial systems.
These findings have potential implications for the development of adaptive,
self-organizing systems and may contribute to the broader understanding of
memory mechanisms in both biological and synthetic contexts.

</details>


### [143] [GT-SVQ: A Linear-Time Graph Transformer for Node Classification Using Spiking Vector Quantization](https://arxiv.org/abs/2504.11840)
*Huizhe Zhang,Jintang Li,Yuchang Zhu,Liang Chen,Zibin Zheng*

Main category: cs.NE

TL;DR: GT-SVQ是一种基于脉冲神经网络的线性复杂度图变换器，用于节点分类，解决了传统图变换器的高计算复杂度问题，并在实验中表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统图变换器（GTs）在捕捉长程图拓扑信息方面表现优异，但其二次复杂度和高能耗限制了在大规模图上的扩展性。脉冲神经网络（SNNs）的低计算和存储开销为解决这一问题提供了灵感。

Method: 提出GT-SVQ，通过脉冲向量量化（SVQ）重构码本，并将其注入自注意力块中，以线性复杂度聚合全局信息。SVQ还缓解了码本崩溃问题，减少了对复杂机制的依赖。

Result: 实验表明，GT-SVQ在节点分类任务中表现优异，推理速度比其他GT快130倍。

Conclusion: GT-SVQ结合了SNN的低开销和GT的全局信息捕捉能力，为大规模图学习提供了一种高效解决方案。

Abstract: Graph Transformers (GTs), which simultaneously integrate message-passing and
self-attention mechanisms, have achieved promising empirical results in some
graph prediction tasks. Although these approaches show the potential of
Transformers in capturing long-range graph topology information, issues
concerning the quadratic complexity and high computing energy consumption
severely limit the scalability of GTs on large-scale graphs. Recently, as
brain-inspired neural networks, Spiking Neural Networks (SNNs), facilitate the
development of graph representation learning methods with lower computational
and storage overhead through the unique event-driven spiking neurons. Inspired
by these characteristics, we propose a linear-time Graph Transformer using
Spiking Vector Quantization (GT-SVQ) for node classification. GT-SVQ
reconstructs codebooks based on rate coding outputs from spiking neurons, and
injects the codebooks into self-attention blocks to aggregate global
information in linear complexity. Besides, spiking vector quantization
effectively alleviates codebook collapse and the reliance on complex machinery
(distance measure, auxiliary loss, etc.) present in previous vector
quantization-based graph learning methods. In experiments, we compare GT-SVQ
with other state-of-the-art baselines on node classification datasets ranging
from small to large. Experimental results show that GT-SVQ has achieved
competitive performances on most datasets while maintaining up to 130x faster
inference speed compared to other GTs.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [144] [Dysarthria Normalization via Local Lie Group Transformations for Robust ASR](https://arxiv.org/abs/2504.12279)
*Mikhail Osipov*

Main category: cs.SD

TL;DR: 提出了一种基于几何驱动的构音障碍语音归一化方法，通过局部李群变换对频谱图进行修正。


<details>
  <summary>Details</summary>
Motivation: 解决构音障碍语音在时间、频率和振幅上的扭曲问题，提升语音识别的鲁棒性。

Method: 使用平滑可逆的变形模型参数化扭曲，通过神经网络从典型语音的合成扭曲中推断变形场，无需病理数据。

Result: 在零样本泛化情况下，显著提升ASR性能，TORGO样本的WER降低高达16%，且不影响干净语音。

Conclusion: 该方法为运动言语障碍下的鲁棒语音识别提供了可解释的解决方案。

Abstract: We present a geometry-driven method for normalizing dysarthric speech using
local Lie group transformations of spectrograms. Time, frequency, and amplitude
distortions are modeled as smooth, invertible deformations, parameterized by
scalar fields and applied via exponential maps. A neural network is trained to
infer these fields from synthetic distortions of typical speech-without using
any pathological data. At test time, the model applies an approximate inverse
to real dysarthric inputs. Despite zero-shot generalization, we observe
substantial ASR gains, including up to 16 percentage points WER reduction on
challenging TORGO samples, with no degradation on clean speech. This work
introduces a principled, interpretable approach for robust speech recognition
under motor speech disorders

</details>


### [145] [Voice Conversion with Diverse Intonation using Conditional Variational Auto-Encoder](https://arxiv.org/abs/2504.12005)
*Soobin Suh,Dabi Ahn,Heewoong Park,Jonghun Park*

Main category: cs.SD

TL;DR: 提出了一种基于条件变分自编码器（CVAE）的语音转换方法，能够生成多样化的语调，并通过逆自回归流（IAF）提高音质。


<details>
  <summary>Details</summary>
Motivation: 传统语音转换模型只能为每个输入生成单一结果，无法捕捉说话者的多样化语调。

Method: 使用CVAE将说话者的风格特征映射到高斯分布的潜在空间，并通过IAF增加潜在空间后验的复杂性。

Result: 转换后的语音不仅语调多样，音质也优于未使用CVAE的模型。

Conclusion: 该方法成功实现了多样化语调的语音转换，并提升了音质。

Abstract: Voice conversion is a task of synthesizing an utterance with target speaker's
voice while maintaining linguistic information of the source utterance. While a
speaker can produce varying utterances from a single script with different
intonations, conventional voice conversion models were limited to producing
only one result per source input. To overcome this limitation, we propose a
novel approach for voice conversion with diverse intonations using conditional
variational autoencoder (CVAE). Experiments have shown that the speaker's style
feature can be mapped into a latent space with Gaussian distribution. We have
also been able to convert voices with more diverse intonation by making the
posterior of the latent space more complex with inverse autoregressive flow
(IAF). As a result, the converted voice not only has a diversity of
intonations, but also has better sound quality than the model without CVAE.

</details>


### [146] [Edge Intelligence for Wildlife Conservation: Real-Time Hornbill Call Classification Using TinyML](https://arxiv.org/abs/2504.12272)
*Kong Ka Hing,Mehran Behjati*

Main category: cs.SD

TL;DR: 论文探讨了TinyML在马来西亚犀鸟叫声分类与监测中的应用，通过音频数据处理和边缘计算实现高效识别。


<details>
  <summary>Details</summary>
Motivation: 犀鸟因栖息地丧失和偷猎等威胁需要实时监测，传统方法成本高且效率低，TinyML提供了解决方案。

Method: 利用Xeno-canto数据库音频数据，提取MFE特征，在Arduino Nano 33 BLE上部署模型，通过Edge Impulse训练。

Result: 模型在犀鸟种类识别中达到高准确率。

Conclusion: TinyML在生态监测中潜力巨大，可推广至其他野生动物保护领域。

Abstract: Hornbills, an iconic species of Malaysia's biodiversity, face threats from
habi-tat loss, poaching, and environmental changes, necessitating accurate and
real-time population monitoring that is traditionally challenging and re-source
intensive. The emergence of Tiny Machine Learning (TinyML) offers a chance to
transform wildlife monitoring by enabling efficient, real-time da-ta analysis
directly on edge devices. Addressing the challenge of wildlife conservation,
this research paper explores the pivotal role of machine learn-ing,
specifically TinyML, in the classification and monitoring of hornbill calls in
Malaysia. Leveraging audio data from the Xeno-canto database, the study aims to
develop a speech recognition system capable of identifying and classifying
hornbill vocalizations. The proposed methodology involves pre-processing the
audio data, extracting features using Mel-Frequency Energy (MFE), and deploying
the model on an Arduino Nano 33 BLE, which is adept at edge computing. The
research encompasses foundational work, in-cluding a comprehensive
introduction, literature review, and methodology. The model is trained using
Edge Impulse and validated through real-world tests, achieving high accuracy in
hornbill species identification. The project underscores the potential of
TinyML for environmental monitoring and its broader application in ecological
conservation efforts, contributing to both the field of TinyML and wildlife
conservation.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [147] [A Framework for the Private Governance of Frontier Artificial Intelligence](https://arxiv.org/abs/2504.11501)
*Dean W. Ball*

Main category: cs.CY

TL;DR: 提出一种混合公私治理体系来管理前沿AI系统，私营机构在政府监督下为开发者提供认证，换取侵权责任豁免。


<details>
  <summary>Details</summary>
Motivation: 探讨前沿AI治理的常见方法及其优缺点，分析治理的本质，提出更优方案。

Method: 通过公私合作模式，私营机构在政府授权下为AI开发者提供认证，换取法律责任保护。

Result: 分析了政治经济、制度、法律、安全等方面的利弊，提出治理体系的权衡。

Conclusion: 混合治理体系能有效平衡前沿AI发展的责任与创新。

Abstract: This paper presents a proposal for the governance of frontier AI systems
through a hybrid public-private system. Private bodies, authorized and overseen
by government, provide certifications to developers of frontier AI systems on
an opt-in basis. In exchange for opting in, frontier AI firms receive
protections from tort liability for customer misuse of their models. Before
detailing the proposal, the paper explores more commonly discussed approaches
to AI governance, analyzing their strengths and flaws. It also examines the
nature of frontier AI governance itself. The paper includes consideration of
the political economic, institutional, legal, safety, and other merits and
tradeoffs inherent in the governance system it proposes.

</details>


### [148] [Perceptions of Agentic AI in Organizations: Implications for Responsible AI and ROI](https://arxiv.org/abs/2504.11564)
*Lee Ackerman*

Main category: cs.CY

TL;DR: 论文探讨了组织在应对自主性AI系统时面临的挑战，强调了责任AI框架实施的复杂性及其对ROI的影响。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统自主性增强，构建稳健的责任AI框架变得至关重要。研究旨在了解组织如何应对这一新兴挑战。

Method: 采用解释性定性方法，研究AI专业人士的实际经验。

Result: 研究发现，自主性AI的复杂性、责任AI维度的相互关联性以及新颖性导致组织适应困难，表现为知识缺口、忽视利益相关者参与和过度关注控制。

Conclusion: 这些挑战阻碍了责任AI的有效实施，影响了ROI的实现。

Abstract: As artificial intelligence (AI) systems rapidly gain autonomy, the need for
robust responsible AI frameworks becomes paramount. This paper investigates how
organizations perceive and adapt such frameworks amidst the emerging landscape
of increasingly sophisticated agentic AI. Employing an interpretive qualitative
approach, the study explores the lived experiences of AI professionals.
Findings highlight that the inherent complexity of agentic AI systems and their
responsible implementation, rooted in the intricate interconnectedness of
responsible AI dimensions and the thematic framework (an analytical structure
developed from the data), combined with the novelty of agentic AI, contribute
to significant challenges in organizational adaptation, characterized by
knowledge gaps, a limited emphasis on stakeholder engagement, and a strong
focus on control. These factors, by hindering effective adaptation and
implementation, ultimately compromise the potential for responsible AI and the
realization of ROI.

</details>


### [149] [Counterfactual Fairness Evaluation of Machine Learning Models on Educational Datasets](https://arxiv.org/abs/2504.11504)
*Woojin Kim,Hyeoncheol Kim*

Main category: cs.CY

TL;DR: 论文探讨了教育数据中的反事实公平性，分析了机器学习模型在基准数据集上的表现，揭示了敏感属性的因果关系及其对个体公平性的影响。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习模型在教育中的应用增多，算法偏见及其对学生的影响引发了公平性担忧。尽管群体公平性研究较多，但因果背景下的个体公平性，尤其是反事实公平性，研究较少。

Method: 通过基准教育数据集对机器学习模型进行反事实公平性分析。

Result: 反事实公平性能够揭示敏感属性的因果关系，并为教育中的因果个体公平性提供有意义见解。

Conclusion: 反事实公平性是评估教育数据中个体公平性的有效工具，尤其在因果背景下具有重要意义。

Abstract: As machine learning models are increasingly used in educational settings,
from detecting at-risk students to predicting student performance, algorithmic
bias and its potential impacts on students raise critical concerns about
algorithmic fairness. Although group fairness is widely explored in education,
works on individual fairness in a causal context are understudied, especially
on counterfactual fairness. This paper explores the notion of counterfactual
fairness for educational data by conducting counterfactual fairness analysis of
machine learning models on benchmark educational datasets. We demonstrate that
counterfactual fairness provides meaningful insight into the causality of
sensitive attributes and causal-based individual fairness in education.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [150] [RAID: An In-Training Defense against Attribute Inference Attacks in Recommender Systems](https://arxiv.org/abs/2504.11510)
*Xiaohua Feng,Yuyuan Li,Fengyuan Yu,Ke Xiong,Junjie Fang,Li Zhang,Tianyu Du,Chaochao Chen*

Main category: cs.IR

TL;DR: 论文提出了一种名为RAID的防御方法，用于在推荐系统中对抗属性推断攻击，通过优化目标确保用户属性分布独立于标签，同时保持推荐性能。


<details>
  <summary>Details</summary>
Motivation: 推荐系统中用户属性推断攻击普遍存在，现有防御方法多为后训练设置，难以兼顾训练数据利用和推荐性能。对抗训练虽扩展至训练中设置，但常因训练不稳定而难以收敛。

Method: 提出RAID方法，定义防御目标使受保护属性分布独立于标签，通过解决约束Wasserstein重心问题优化目标，利用最优传输对齐用户分布。

Result: 在四个真实数据集上的实验验证了RAID的有效性，显著优于现有方法。

Conclusion: RAID在防御属性推断攻击的同时保持了推荐性能，为推荐系统安全提供了新思路。

Abstract: In various networks and mobile applications, users are highly susceptible to
attribute inference attacks, with particularly prevalent occurrences in
recommender systems. Attackers exploit partially exposed user profiles in
recommendation models, such as user embeddings, to infer private attributes of
target users, such as gender and political views. The goal of defenders is to
mitigate the effectiveness of these attacks while maintaining recommendation
performance. Most existing defense methods, such as differential privacy and
attribute unlearning, focus on post-training settings, which limits their
capability of utilizing training data to preserve recommendation performance.
Although adversarial training extends defenses to in-training settings, it
often struggles with convergence due to unstable training processes. In this
paper, we propose RAID, an in-training defense method against attribute
inference attacks in recommender systems. In addition to the recommendation
objective, we define a defensive objective to ensure that the distribution of
protected attributes becomes independent of class labels, making users
indistinguishable from attribute inference attacks. Specifically, this
defensive objective aims to solve a constrained Wasserstein barycenter problem
to identify the centroid distribution that makes the attribute
indistinguishable while complying with recommendation performance constraints.
To optimize our proposed objective, we use optimal transport to align users
with the centroid distribution. We conduct extensive experiments on four
real-world datasets to evaluate RAID. The experimental results validate the
effectiveness of RAID and demonstrate its significant superiority over existing
methods in multiple aspects.

</details>


### [151] [Rethinking LLM-Based Recommendations: A Query Generation-Based, Training-Free Approach](https://arxiv.org/abs/2504.11889)
*Donghee Han,Hwanjun Song,Mun Yong Yi*

Main category: cs.IR

TL;DR: 提出了一种基于LLM的查询到推荐方法，解决了现有推荐系统中的效率、顺序敏感性和评估不现实等问题。


<details>
  <summary>Details</summary>
Motivation: 现有LLM推荐方法在处理大规模候选池时效率低，对提示中的项目顺序敏感，且评估不现实。

Method: 利用LLM生成个性化查询，直接从整个候选池中检索相关项目，无需预选候选。

Result: 在三个数据集上表现优异，最高提升57%，平均提升31%，零样本性能强。

Conclusion: 该方法无需额外训练，提升推荐性能和多样性，尤其适合冷门项目。

Abstract: Existing large language model LLM-based recommendation methods face several
challenges, including inefficiency in handling large candidate pools,
sensitivity to item order within prompts ("lost in the middle" phenomenon) poor
scalability, and unrealistic evaluation due to random negative sampling. To
address these issues, we propose a Query-to-Recommendation approach that
leverages LLMs to generate personalized queries for retrieving relevant items
from the entire candidate pool, eliminating the need for candidate
pre-selection. This method can be integrated into an ID-based recommendation
system without additional training, enhances recommendation performance and
diversity through LLMs' world knowledge, and performs well even for less
popular item groups. Experiments on three datasets show up to 57 percent
improvement, with an average gain of 31 percent, demonstrating strong zero-shot
performance and further gains when ensembled with existing models.

</details>


### [152] [Improving LLM Interpretability and Performance via Guided Embedding Refinement for Sequential Recommendation](https://arxiv.org/abs/2504.11658)
*Nanshan Jia,Chenfei Yuan,Yuhang Wu,Zeyu Zheng*

Main category: cs.IR

TL;DR: 论文提出了一种名为“引导嵌入细化”的方法，利用大型语言模型（LLMs）作为辅助工具，增强基础推荐系统的嵌入表示，从而提升推荐性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的快速发展，如何将其整合到现有推荐系统中，同时解决模型可解释性和安全性问题，成为研究重点。

Method: 通过引导嵌入细化方法，利用LLMs模拟推荐逻辑生成语义信息丰富的嵌入，并将其与降维后的基础嵌入结合，构建优化后的嵌入表示。

Result: 实验表明，该方法能适应多种基础嵌入模型，显著提升推荐性能（MRR、Recall和NDCG提升10%至50%），并增强可解释性。

Conclusion: 引导嵌入细化方法在提升推荐系统性能的同时，解决了LLMs整合中的可解释性问题，具有广泛适用性。

Abstract: The fast development of Large Language Models (LLMs) offers growing
opportunities to further improve sequential recommendation systems. Yet for
some practitioners, integrating LLMs to their existing base recommendation
systems raises questions about model interpretability, transparency and related
safety. To partly alleviate challenges from these questions, we propose guided
embedding refinement, a method that carries out a guided and interpretable
usage of LLM to enhance the embeddings associated with the base recommendation
system. Instead of directly using LLMs as the backbone of sequential
recommendation systems, we utilize them as auxiliary tools to emulate the sales
logic of recommendation and generate guided embeddings that capture
domain-relevant semantic information on interpretable attributes. Benefiting
from the strong generalization capabilities of the guided embedding, we
construct refined embedding by using the guided embedding and reduced-dimension
version of the base embedding. We then integrate the refined embedding into the
recommendation module for training and inference. A range of numerical
experiments demonstrate that guided embedding is adaptable to various given
existing base embedding models, and generalizes well across different
recommendation tasks. The numerical results show that the refined embedding not
only improves recommendation performance, achieving approximately $10\%$ to
$50\%$ gains in Mean Reciprocal Rank (MRR), Recall rate, and Normalized
Discounted Cumulative Gain (NDCG), but also enhances interpretability, as
evidenced by case studies.

</details>


### [153] [Generative Recommendation with Continuous-Token Diffusion](https://arxiv.org/abs/2504.12007)
*Haohao Qu,Wenqi Fan,Shanru Lin*

Main category: cs.IR

TL;DR: 论文提出了一种名为DeftRec的新框架，利用去噪扩散模型使基于大语言模型（LLM）的推荐系统支持连续令牌输入和目标，解决了离散方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的推荐系统主要采用离散空间表示用户-物品交互，但这种方法存在信息压缩和词汇限制的问题。连续数据可能提供更好的表达能力，但相关研究尚处于早期阶段。

Method: DeftRec框架结合了去噪扩散模型，引入了一种鲁棒的令牌化器和K-way架构，将用户和物品的复杂关系映射为连续令牌。通过扩散模型处理连续域中的用户偏好，并在去噪过程中加入负交互目标。

Result: 实验表明，DeftRec在性能上超越了传统和新兴的基于LLM的推荐系统基准。

Conclusion: DeftRec通过连续令牌的支持和扩散模型的应用，显著提升了推荐系统的表达能力与准确性。

Abstract: In recent years, there has been a significant trend toward using large
language model (LLM)-based recommender systems (RecSys). Current research
primarily focuses on representing complex user-item interactions within a
discrete space to align with the inherent discrete nature of language models.
However, this approach faces limitations due to its discrete nature: (i)
information is often compressed during discretization; (ii) the tokenization
and generation for the vast number of users and items in real-world scenarios
are constrained by a limited vocabulary. Embracing continuous data presents a
promising alternative to enhance expressive capabilities, though this approach
is still in its early stages. To address this gap, we propose a novel
framework, DeftRec, which incorporates \textbf{de}noising di\textbf{f}fusion
models to enable LLM-based RecSys to seamlessly support continuous
\textbf{t}oken as input and target. First, we introduce a robust tokenizer with
a masking operation and an additive K-way architecture to index users and
items, capturing their complex collaborative relationships into continuous
tokens. Crucially, we develop a denoising diffusion model to process user
preferences within continuous domains by conditioning on reasoning content from
pre-trained large language model. During the denoising process, we reformulate
the objective to include negative interactions, building a comprehensive
understanding of user preferences for effective and accurate recommendation
generation. Finally, given a continuous token as output, recommendations can be
easily generated through score-based retrieval. Extensive experiments
demonstrate the effectiveness of the proposed methods, showing that DeftRec
surpasses competitive benchmarks, including both traditional and emerging
LLM-based RecSys.

</details>


### [154] [Optimizing Compound Retrieval Systems](https://arxiv.org/abs/2504.12063)
*Harrie Oosterhuis,Rolf Jagerman,Zhen Qin,Xuanhui Wang*

Main category: cs.IR

TL;DR: 论文提出了一种称为复合检索系统的概念，扩展了传统级联检索系统的范围，允许更多类型的模型交互，特别是结合大型语言模型（LLM），以优化检索效果与效率的平衡。


<details>
  <summary>Details</summary>
Motivation: 传统级联检索系统虽然能平衡排名质量与计算成本，但限制了模型交互的方式。作者希望通过复合检索系统提供更灵活的模型交互方式，特别是结合LLM的能力。

Method: 提出复合检索系统框架，允许组件模型在多个阶段交互，并通过学习模型应用位置和预测聚合方式优化设计。实验结合了BM25和LLM的预测。

Result: 优化的复合检索系统在效果与效率的权衡上优于传统级联方法，即使是无监督情况下也表现良好。

Conclusion: 复合检索系统为信息检索领域提供了更灵活的模型交互思路，鼓励更多创新设计。

Abstract: Modern retrieval systems do not rely on a single ranking model to construct
their rankings. Instead, they generally take a cascading approach where a
sequence of ranking models are applied in multiple re-ranking stages. Thereby,
they balance the quality of the top-K ranking with computational costs by
limiting the number of documents each model re-ranks. However, the cascading
approach is not the only way models can interact to form a retrieval system.
  We propose the concept of compound retrieval systems as a broader class of
retrieval systems that apply multiple prediction models. This encapsulates
cascading models but also allows other types of interactions than top-K
re-ranking. In particular, we enable interactions with large language models
(LLMs) which can provide relative relevance comparisons. We focus on the
optimization of compound retrieval system design which uniquely involves
learning where to apply the component models and how to aggregate their
predictions into a final ranking. This work shows how our compound approach can
combine the classic BM25 retrieval model with state-of-the-art (pairwise) LLM
relevance predictions, while optimizing a given ranking metric and efficiency
target. Our experimental results show optimized compound retrieval systems
provide better trade-offs between effectiveness and efficiency than cascading
approaches, even when applied in a self-supervised manner.
  With the introduction of compound retrieval systems, we hope to inspire the
information retrieval field to more out-of-the-box thinking on how prediction
models can interact to form rankings.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [155] [Timing Analysis Agent: Autonomous Multi-Corner Multi-Mode (MCMM) Timing Debugging with Timing Debug Relation Graph](https://arxiv.org/abs/2504.11502)
*Jatin Nainani,Chia-Tung Ho,Anirudh Dhurka,Haoxing Ren*

Main category: cs.SE

TL;DR: 论文提出了一种基于多LLM的时序分析代理，通过分层规划和解决流程自动化分析商业工具的时序报告，并构建了时序调试关系图（TDRG）。实验表明，该代理在单报告和多报告基准测试中分别达到98%和90%的通过率。


<details>
  <summary>Details</summary>
Motivation: 随着技术发展，VLSI电路设计中的时序分析变得更具挑战性，传统人工调试效率低且耗时长，亟需智能化的解决方案。

Method: 结合多LLM任务解决能力，提出分层规划和解决流程，构建TDRG图，并采用Agentic RAG方法进行数据检索。

Result: 在工业设计中，单报告和多报告基准测试的通过率分别达到98%和90%。

Conclusion: 该时序分析代理高效且适应性强，为VLSI设计中的时序调试提供了智能化解决方案。

Abstract: Timing analysis is an essential and demanding verification method for Very
Large Scale Integrated (VLSI) circuit design and optimization. In addition, it
also serves as the cornerstone of the final sign-off, determining whether the
chip is ready to be sent to the semiconductor foundry for fabrication.
Recently, as the technology advance relentlessly, smaller metal pitches and the
increasing number of devices have led to greater challenges and longer
turn-around-time for experienced human designers to debug timing issues from
the Multi-Corner Multi-Mode (MCMM) timing reports. As a result, an efficient
and intelligent methodology is highly necessary and essential for debugging
timing issues and reduce the turnaround times. Recently, Large Language Models
(LLMs) have shown great promise across various tasks in language understanding
and interactive decision-making, incorporating reasoning and actions. In this
work, we propose a timing analysis agent, that is empowered by multi-LLMs task
solving, and incorporates a novel hierarchical planning and solving flow to
automate the analysis of timing reports from commercial tool. In addition, we
build a Timing Debug Relation Graph (TDRG) that connects the reports with the
relationships of debug traces from experienced timing engineers. The timing
analysis agent employs the novel Agentic Retrieval Augmented Generation (RAG)
approach, that includes agent and coding to retrieve data accurately, on the
developed TDRG. In our studies, the proposed timing analysis agent achieves an
average 98% pass-rate on a single-report benchmark and a 90% pass-rate for
multi-report benchmark from industrial designs, demonstrating its effectiveness
and adaptability.

</details>


### [156] [The Hitchhiker's Guide to Program Analysis, Part II: Deep Thoughts by LLMs](https://arxiv.org/abs/2504.11711)
*Haonan Li,Hang Zhang,Kexin Pei,Zhiyun Qian*

Main category: cs.SE

TL;DR: BugLens是一个后优化框架，通过结合LLM和传统静态分析步骤，显著提高了漏洞检测的精确度，减少了误报。


<details>
  <summary>Details</summary>
Motivation: 静态分析工具在大型代码库（如Linux内核）中误报率高，主要由于简化的漏洞模型和路径/数据约束的过度近似。

Method: BugLens通过引导LLM评估漏洞代码模式的安全影响，并验证静态警告的约束条件，实现了对静态分析结果的优化。

Result: 在Linux内核真实漏洞测试中，BugLens将精确度从0.10（原始）和0.50（半自动优化）提升至0.72，并发现了四个新漏洞。

Conclusion: 结构化LLM工作流能显著提升静态分析工具的效果。

Abstract: Static analysis is a cornerstone for software vulnerability detection, yet it
often struggles with the classic precision-scalability trade-off. In practice,
such tools often produce high false positive rates, particularly in large
codebases like the Linux kernel. This imprecision can arise from simplified
vulnerability modeling and over-approximation of path and data constraints.
While large language models (LLMs) show promise in code understanding, their
naive application to program analysis yields unreliable results due to inherent
reasoning limitations. We introduce BugLens, a post-refinement framework that
significantly improves static analysis precision. BugLens guides an LLM to
follow traditional analysis steps by assessing buggy code patterns for security
impact and validating the constraints associated with static warnings.
Evaluated on real-world Linux kernel bugs, BugLens raises precision from 0.10
(raw) and 0.50 (semi-automated refinement) to 0.72, substantially reducing
false positives and revealing four previously unreported vulnerabilities. Our
results suggest that a structured LLM-based workflow can meaningfully enhance
the effectiveness of static analysis tools.

</details>


### [157] [Agile Retrospectives: What went well? What didn't go well? What should we do?](https://arxiv.org/abs/2504.11780)
*Maria Spichkova,Hina Lee,Kevin Iwan,Madeleine Zwart,Yuwon Yoon,Xiaohan Qin*

Main category: cs.SE

TL;DR: 论文探讨了在敏捷/Scrum开发中，如何利用生成式AI改进回顾会议的信息交互和可视化，并介绍了原型工具RetroAI++。


<details>
  <summary>Details</summary>
Motivation: 回顾会议是敏捷开发的核心环节，但信息交互和可视化效率有待提升，因此研究生成式AI在此场景的应用潜力。

Method: 研究聚焦于生成式AI在回顾会议中的应用，开发了原型工具RetroAI++，支持相关功能。

Result: 提出了RetroAI++工具，旨在优化回顾会议的信息交互和团队可视化效果。

Conclusion: 生成式AI和可视化工具可有效提升回顾会议的效率和团队协作，未来需进一步验证和优化。

Abstract: In Agile/Scrum software development, the idea of retrospective meetings
(retros) is one of the core elements of the project process. In this paper, we
present our work in progress focusing on two aspects: analysis of potential
usage of generative AI for information interaction within retrospective
meetings, and visualisation of retros' information to software development
teams. We also present our prototype tool RetroAI++, focusing on retros-related
functionalities.

</details>


### [158] [Unravelling Technical debt topics through Time, Programming Languages and Repository](https://arxiv.org/abs/2504.11714)
*Karthik Shivashankar,Antonio Martini*

Main category: cs.SE

TL;DR: 该研究通过分析2015年至2023年GitHub问题中的技术债务（TD）数据，使用BERTopic进行主题建模，分类并追踪TD主题的演变，并结合情感分析，揭示TD主题的多样性和时间变化趋势。


<details>
  <summary>Details</summary>
Motivation: 尽管已有大量研究关注技术债务的识别和量化，但对TD主题多样性及其时间演变的了解仍存在显著空白。

Method: 研究从GitHub问题中提取TD数据，采用BERTopic进行主题建模，并辅以情感分析。

Result: 研究分类了TD主题并追踪其时间演变，同时通过情感分析揭示了开发者对这些主题的态度和感知。

Conclusion: 该研究提供了对TD主题多样性和时间演变的更深入理解，为未来研究和技术债务管理提供了新视角。

Abstract: This study explores the dynamic landscape of Technical Debt (TD) topics in
software engineering by examining its evolution across time, programming
languages, and repositories. Despite the extensive research on identifying and
quantifying TD, there remains a significant gap in understanding the diversity
of TD topics and their temporal development. To address this, we have conducted
an explorative analysis of TD data extracted from GitHub issues spanning from
2015 to September 2023. We employed BERTopic for sophisticated topic modelling.
This study categorises the TD topics and tracks their progression over time.
Furthermore, we have incorporated sentiment analysis for each identified topic,
providing a deeper insight into the perceptions and attitudes associated with
these topics. This offers a more nuanced understanding of the trends and shifts
in TD topics through time, programming language, and repository.

</details>


### [159] [On the calibration of Just-in-time Defect Prediction](https://arxiv.org/abs/2504.12051)
*Xhulja Shahini,Jone Bartel,Klaus Pohl*

Main category: cs.SE

TL;DR: JIT缺陷预测（JIT DP）利用机器学习识别易缺陷代码提交，但存在误分类问题。本研究评估了三种JIT DP技术的校准性，发现它们均存在不同程度的校准问题，且后校准方法效果有限。


<details>
  <summary>Details</summary>
Motivation: JIT DP虽能提高预测准确性，但误分类问题可能导致资源浪费或缺陷漏检。为提升预测可靠性，需评估模型的校准性。

Method: 评估三种JIT DP技术的校准性，并测试后校准方法的效果。

Result: 所有模型均存在校准问题（ECE 2-35%），后校准方法未能显著改善。

Conclusion: JIT DP模型的校准问题需进一步研究，后校准方法效果有限。

Abstract: Just in time defect prediction (JIT DP) leverages ML to identify defect-prone
code commits, enabling quality assurance (QA) teams to allocate resources more
efficiently by focusing on commits that are most likely to contain defects.
Although JIT DP techniques have introduced improvements in terms of predictive
accuracy, they are still susceptible to misclassification errors such as false
positives and negatives. This can lead to wasted resources or undetected
defects, a particularly critical concern when QA resources are limited. To
mitigate these challenges and preserve the practical utility of JIT DP tools,
it becomes essential to estimate the reliability of the predictions, i.e.,
computing confidence scores. Such scores can help practitioners determine the
trustworthiness of predictions and thus prioritize them efficiently. A simple
approach to computing confidence scores is to extract, alongside each
prediction, the corresponding prediction probabilities and use them as
indicators of confidence. However, for these probabilities to reliably serve as
confidence scores, the predictive model must be well-calibrated. This means
that the prediction probabilities must accurately represent the true likelihood
of each prediction being correct. Miscalibration, common in modern ML models,
distorts probability scores such that they do not align with the actual
correctness probability. In this study, we evaluate the calibration of three
JIT DP techniques to determine whether and to what extent they exhibit poor
calibration. Furthermore, we assess whether post-calibration methods can
improve the calibration of existing JIT defect prediction models. Our results
reveal that all evaluated JIT DP models exhibit some level of miscalibration,
with ECE ranging from 2-35%. Furthermore, post-calibration methods do not
consistently improve the calibration.

</details>


### [160] [From Requirements to Architecture: Semi-Automatically Generating Software Architectures](https://arxiv.org/abs/2504.12192)
*Tobias Eisenreich*

Main category: cs.SE

TL;DR: 提出一种利用LLMs支持建筑师的新方法，涵盖从领域模型创建到架构评估的全过程，初步结果显示可行且节省时间。


<details>
  <summary>Details</summary>
Motivation: 支持初级和高级建筑师，利用LLMs的能力提升建筑设计效率。

Method: 建筑师与LLM工具紧密合作，涵盖领域模型创建、用例规范、架构决策和评估。

Result: 初步结果显示方法可行，并能显著节省建筑师的时间。

Conclusion: 该方法为建筑师提供了灵活性和高效支持，具有实际应用潜力。

Abstract: To support junior and senior architects, I propose developing a new
architecture creation method that leverages LLMs' evolving capabilities to
support the architect. This method involves the architect's close collaboration
with LLM-fueled tooling over the whole process. The architect is guided through
Domain Model creation, Use Case specification, architectural decisions, and
architecture evaluation. While the architect can take complete control of the
process and the results, and use the tooling as a building set, they can follow
the intended process for maximum tooling support. The preliminary results
suggest the feasibility of this process and indicate major time savings for the
architect.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [161] [Toward Aligning Human and Robot Actions via Multi-Modal Demonstration Learning](https://arxiv.org/abs/2504.11493)
*Azizul Zahid,Jie Fan,Farong Wang,Ashton Dy,Sai Swaminathan,Fei Liu*

Main category: cs.RO

TL;DR: 提出了一种多模态演示学习框架，用于对齐人类与机器人在复杂任务中的行为，通过RGB视频和RGB-D体素空间建模，取得了71.67%和71.8%的准确率。


<details>
  <summary>Details</summary>
Motivation: 研究人类与机器人行为对齐对决策评估的重要性，特别是在人机协作和模仿学习中。

Method: 结合ResNet视觉编码和Perceiver Transformer，从RGB视频和RGB-D体素空间建模人类和机器人演示。

Result: 人类模型准确率71.67%，机器人模型71.8%，验证了框架的有效性。

Conclusion: 该框架能够有效对齐复杂多模态的人类与机器人行为，适用于操作任务。

Abstract: Understanding action correspondence between humans and robots is essential
for evaluating alignment in decision-making, particularly in human-robot
collaboration and imitation learning within unstructured environments. We
propose a multimodal demonstration learning framework that explicitly models
human demonstrations from RGB video with robot demonstrations in voxelized
RGB-D space. Focusing on the "pick and place" task from the RH20T dataset, we
utilize data from 5 users across 10 diverse scenes. Our approach combines
ResNet-based visual encoding for human intention modeling and a Perceiver
Transformer for voxel-based robot action prediction. After 2000 training
epochs, the human model reaches 71.67% accuracy, and the robot model achieves
71.8% accuracy, demonstrating the framework's potential for aligning complex,
multimodal human and robot behaviors in manipulation tasks.

</details>


### [162] [Probabilistic Task Parameterization of Tool-Tissue Interaction via Sparse Landmarks Tracking in Robotic Surgery](https://arxiv.org/abs/2504.11495)
*Yiting Wang,Yunxin Fan,Fei Liu*

Main category: cs.RO

TL;DR: 提出一种结合稀疏关键点跟踪和概率建模的框架，用于机器人手术中工具与组织交互的精确建模。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖人工标注或刚性假设，灵活性不足，难以处理大变形组织。

Method: 结合稀疏关键点跟踪和概率建模，通过PCA构建动态局部变换，并利用TP-GMM整合数据驱动观察与临床专业知识。

Result: 能够预测工具与组织的相对位姿，并直接从视频数据中增强对机器人手术动作的视觉理解。

Conclusion: 该框架有效解决了传统方法的局限性，为机器人手术中的工具-组织交互建模提供了灵活且精确的解决方案。

Abstract: Accurate modeling of tool-tissue interactions in robotic surgery requires
precise tracking of deformable tissues and integration of surgical domain
knowledge. Traditional methods rely on labor-intensive annotations or rigid
assumptions, limiting flexibility. We propose a framework combining sparse
keypoint tracking and probabilistic modeling that propagates expert-annotated
landmarks across endoscopic frames, even with large tissue deformations.
Clustered tissue keypoints enable dynamic local transformation construction via
PCA, and tool poses, tracked similarly, are expressed relative to these frames.
Embedding these into a Task-Parameterized Gaussian Mixture Model (TP-GMM)
integrates data-driven observations with labeled clinical expertise,
effectively predicting relative tool-tissue poses and enhancing visual
understanding of robotic surgical motions directly from video data.

</details>


### [163] [Causality-enhanced Decision-Making for Autonomous Mobile Robots in Dynamic Environments](https://arxiv.org/abs/2504.11901)
*Luca Castri,Gloria Beraldo,Nicola Bellotto*

Main category: cs.RO

TL;DR: 论文提出了一种基于因果推理的决策框架和PeopleFlow模拟器，用于优化机器人在共享环境中的任务执行效率与安全性。


<details>
  <summary>Details</summary>
Motivation: 随着机器人在共享环境中的广泛应用，需要更深入地理解人类行为与环境动态，以提升机器人的任务执行能力。传统的相关性研究不足，需转向因果分析。

Method: 开发了因果推理框架和PeopleFlow模拟器，模拟人类与机器人的空间互动，并在仓库环境中进行实验验证。

Result: 实验表明，因果推理能显著提高机器人在动态环境中的任务执行效率与安全性。

Conclusion: 因果推理框架和PeopleFlow模拟器为机器人在共享环境中的高效安全运行提供了有效解决方案。

Abstract: The growing integration of robots in shared environments -- such as
warehouses, shopping centres, and hospitals -- demands a deep understanding of
the underlying dynamics and human behaviours, including how, when, and where
individuals engage in various activities and interactions. This knowledge goes
beyond simple correlation studies and requires a more comprehensive causal
analysis. By leveraging causal inference to model cause-and-effect
relationships, we can better anticipate critical environmental factors and
enable autonomous robots to plan and execute tasks more effectively. To this
end, we propose a novel causality-based decision-making framework that reasons
over a learned causal model to predict battery usage and human obstructions,
understanding how these factors could influence robot task execution. Such
reasoning framework assists the robot in deciding when and how to complete a
given task. To achieve this, we developed also PeopleFlow, a new Gazebo-based
simulator designed to model context-sensitive human-robot spatial interactions
in shared workspaces. PeopleFlow features realistic human and robot
trajectories influenced by contextual factors such as time, environment layout,
and robot state, and can simulate a large number of agents. While the simulator
is general-purpose, in this paper we focus on a warehouse-like environment as a
case study, where we conduct an extensive evaluation benchmarking our causal
approach against a non-causal baseline. Our findings demonstrate the efficacy
of the proposed solutions, highlighting how causal reasoning enables autonomous
robots to operate more efficiently and safely in dynamic environments shared
with humans.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [164] [Communication Optimization for Decentralized Learning atop Bandwidth-limited Edge Networks](https://arxiv.org/abs/2504.12210)
*Tingyang Sun,Tuan Nguyen,Ting He*

Main category: cs.NI

TL;DR: 论文提出了一种联合设计去中心化联邦学习（DFL）通信方案和混合矩阵的方法，显著减少了训练时间并保持准确性。


<details>
  <summary>Details</summary>
Motivation: 边缘网络上的DFL面临参数交换效率低下的问题，现有解决方案无法适应多跳带宽受限网络。

Method: 通过分析问题特性，将设计问题转化为可优化问题，并开发高效算法。

Result: 实验表明，算法能将总训练时间减少80%以上，且不牺牲准确性，计算效率显著提升。

Conclusion: 该方法为边缘网络上的DFL提供了高效且实用的解决方案。

Abstract: Decentralized federated learning (DFL) is a promising machine learning
paradigm for bringing artificial intelligence (AI) capabilities to the network
edge. Running DFL on top of edge networks, however, faces severe performance
challenges due to the extensive parameter exchanges between agents. Most
existing solutions for these challenges were based on simplistic communication
models, which cannot capture the case of learning over a multi-hop
bandwidth-limited network. In this work, we address this problem by jointly
designing the communication scheme for the overlay network formed by the agents
and the mixing matrix that controls the communication demands between the
agents. By carefully analyzing the properties of our problem, we cast each
design problem into a tractable optimization and develop an efficient algorithm
with guaranteed performance. Our evaluations based on real topology and data
show that the proposed algorithm can reduce the total training time by over
$80\%$ compared to the baseline without sacrificing accuracy, while
significantly improving the computational efficiency over the state of the art.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [165] [Data driven approach towards more efficient Newton-Raphson power flow calculation for distribution grids](https://arxiv.org/abs/2504.11650)
*Shengyuan Yan,Farzad Vazinram,Zeynab Kaseb,Lindsay Spoor,Jochen Stiasny,Betul Mamudi,Amirhossein Heydarian Ardakani,Ugochukwu Orji,Pedro P. Vergara,Yu Xiang,Jerry Guo*

Main category: eess.SY

TL;DR: 论文提出三种改进牛顿-拉夫森法初始化的策略，以减少迭代次数并避免发散，适用于现代电力系统的高效实时运行。


<details>
  <summary>Details</summary>
Motivation: 随着电力系统接近容量极限，牛顿-拉夫森法在潮流计算中面临病态问题和收敛挑战，需要更稳健的初始化方法。

Method: 研究采用三种方法：(i) 基于电压数学边界的解析法，(ii) 监督学习或物理信息神经网络的数驱模型，(iii) 强化学习逐步调整电压。

Result: 所有方法在基准系统中均能显著减少牛顿-拉夫森法的迭代步骤，提高收敛效率。

Conclusion: 研究为实时电网运行提供了高效解决方案，支持向更智能、更具弹性的电力网络过渡。

Abstract: Power flow (PF) calculations are fundamental to power system analysis to
ensure stable and reliable grid operation. The Newton-Raphson (NR) method is
commonly used for PF analysis due to its rapid convergence when initialized
properly. However, as power grids operate closer to their capacity limits,
ill-conditioned cases and convergence issues pose significant challenges. This
work, therefore, addresses these challenges by proposing strategies to improve
NR initialization, hence minimizing iterations and avoiding divergence. We
explore three approaches: (i) an analytical method that estimates the basin of
attraction using mathematical bounds on voltages, (ii) Two data-driven models
leveraging supervised learning or physics-informed neural networks (PINNs) to
predict optimal initial guesses, and (iii) a reinforcement learning (RL)
approach that incrementally adjusts voltages to accelerate convergence. These
methods are tested on benchmark systems. This research is particularly relevant
for modern power systems, where high penetration of renewables and
decentralized generation require robust and scalable PF solutions. In
experiments, all three proposed methods demonstrate a strong ability to provide
an initial guess for Newton-Raphson method to converge with fewer steps. The
findings provide a pathway for more efficient real-time grid operations, which,
in turn, support the transition toward smarter and more resilient electricity
networks.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [166] [Language and Knowledge Representation: A Stratified Approach](https://arxiv.org/abs/2504.11492)
*Mayukh Bagchi*

Main category: cs.DB

TL;DR: 论文提出表示异质性问题，并提出一种分层解决方案，包括形式化表示、语言表示、知识表示、共享机制和方法论，并通过两个项目验证。


<details>
  <summary>Details</summary>
Motivation: 强调异质性是表示的内在属性，不同观察者用不同概念、语言和知识对同一现实进行分层表示。

Method: 提出分层表示形式化、UKC语言表示、知识表示、LiveKnowledge共享机制和kTelos方法论。

Result: 通过DataScientia和JIDEP项目验证了语言和知识表示的有效性。

Conclusion: 总结了解决方案，并提出了未来研究方向。

Abstract: The thesis proposes the problem of representation heterogeneity to emphasize
the fact that heterogeneity is an intrinsic property of any representation,
wherein, different observers encode different representations of the same
target reality in a stratified manner using different concepts, language and
knowledge (as well as data). The thesis then advances a top-down solution
approach to the above stratified problem of representation heterogeneity in
terms of several solution components, namely: (i) a representation formalism
stratified into concept level, language level, knowledge level and data level
to accommodate representation heterogeneity, (ii) a top-down language
representation using Universal Knowledge Core (UKC), UKC namespaces and domain
languages to tackle the conceptual and language level heterogeneity, (iii) a
top-down knowledge representation using the notions of language teleontology
and knowledge teleontology to tackle the knowledge level heterogeneity, (iv)
the usage and further development of the existing LiveKnowledge catalog for
enforcing iterative reuse and sharing of language and knowledge
representations, and, (v) the kTelos methodology integrating the solution
components above to iteratively generate the language and knowledge
representations absolving representation heterogeneity. The thesis also
includes proof-of-concepts of the language and knowledge representations
developed for two international research projects - DataScientia (data
catalogs) and JIDEP (materials modelling). Finally, the thesis concludes with
future lines of research.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [167] [Proof-Carrying Neuro-Symbolic Code](https://arxiv.org/abs/2504.12031)
*Ekaterina Komendantskaya*

Main category: cs.PL

TL;DR: 论文介绍了“证明携带神经符号代码”的概念及其价值，从“神经”和“符号”两个角度进行解释，并概述了该新研究领域的初步成果与挑战。


<details>
  <summary>Details</summary>
Motivation: 探讨神经符号计算的新方向，结合神经网络的灵活性与符号推理的严谨性，提出“证明携带神经符号代码”的概念。

Method: 从神经和符号两个视角分析“证明携带神经符号代码”的定义、意义及其实现方式。

Result: 初步展示了该领域的成功案例，同时指出了当前面临的挑战。

Conclusion: “证明携带神经符号代码”是一个有潜力的研究方向，但仍需解决诸多技术难题。

Abstract: This invited paper introduces the concept of "proof-carrying neuro-symbolic
code" and explains its meaning and value, from both the "neural" and the
"symbolic" perspectives. The talk outlines the first successes and challenges
that this new area of research faces.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [168] [Robust Markov stability for community detection at a scale learned based on the structure](https://arxiv.org/abs/2504.11621)
*Samin Aref,Sanchaai Mathiyarasan*

Main category: cs.SI

TL;DR: 提出了一种结合多尺度马尔可夫稳定性框架与预训练机器学习模型的社区检测方法，自动选择合适的分区尺度，无需用户干预。


<details>
  <summary>Details</summary>
Motivation: 现有的单尺度社区检测方法在鲁棒性和合适尺度选择上表现不足，而多尺度方法PyGenStability缺乏自动选择合适分区的原则性方法。

Method: 结合马尔可夫稳定性框架与预训练的梯度提升模型，利用网络特征自动选择分区尺度，形成超参数自由的算法PyGenStabilityOne (PO)。

Result: PO在29种算法中表现优异，优于25种算法，具有统计显著性。

Conclusion: PO是一种准确、鲁棒且无需超参数调整的社区检测方法，显著优于其他算法。

Abstract: Community detection, the unsupervised task of clustering nodes of a graph,
finds applications across various fields. The common approaches for community
detection involve optimizing an objective function to partition the nodes into
communities at a single scale of granularity. However, the single-scale
approaches often fall short of producing partitions that are robust and at a
suitable scale. The existing algorithm, PyGenStability, returns multiple robust
partitions for a network by optimizing the multi-scale Markov stability
function. However, in cases where the suitable scale is not known or assumed by
the user, there is no principled method to select a single robust partition at
a suitable scale from the multiple partitions that PyGenStability produces. Our
proposed method combines the Markov stability framework with a pre-trained
machine learning model for scale selection to obtain one robust partition at a
scale that is learned based on the graph structure. This automatic scale
selection involves using a gradient boosting model pre-trained on hand-crafted
and embedding-based network features from a labeled dataset of 10k benchmark
networks. This model was trained to predicts the scale value that maximizes the
similarity of the output partition to the planted partition of the benchmark
network. Combining our scale selection algorithm with the PyGenStability
algorithm results in PyGenStabilityOne (PO): a hyperparameter-free multi-scale
community detection algorithm that returns one robust partition at a suitable
scale without the need for any assumptions, input, or tweaking from the user.
We compare the performance of PO against 29 algorithms and show that it
outperforms 25 other algorithms by statistically meaningful margins. Our
results facilitate choosing between community detection algorithms, among which
PO stands out as the accurate, robust, and hyperparameter-free method.

</details>


<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [169] [FACT: Foundation Model for Assessing Cancer Tissue Margins with Mass Spectrometry](https://arxiv.org/abs/2504.11519)
*Mohammad Farahmand,Amoon Jamzad,Fahimeh Fooladgar,Laura Connolly,Martin Kaufmann,Kevin Yi Mi Ren,John Rudan,Doug McKay,Gabor Fichtinger,Parvin Mousavi*

Main category: physics.med-ph

TL;DR: 本文提出了一种名为FACT的基础模型，用于实时手术边缘评估中的REIMS数据分类，解决了标记数据稀缺的问题，并显著提升了分类性能。


<details>
  <summary>Details</summary>
Motivation: 在癌症手术中，准确分类组织边缘对完全切除肿瘤至关重要，但REIMS数据的标记数据稀缺限制了实时评估的效果。

Method: 提出FACT模型，基于文本-音频关联的基础模型，采用监督对比学习方法（基于三元组损失）进行预训练，并通过消融研究与其他模型和方法对比。

Result: FACT模型在分类性能上显著提升，AUROC达到82.4%±0.8，优于自监督和半监督基线模型。

Conclusion: 研究表明，通过新颖的预训练方法，基础模型能有效分类REIMS数据，即使在标记数据稀缺的临床环境中也能提升实时手术边缘评估的可行性。

Abstract: Purpose: Accurately classifying tissue margins during cancer surgeries is
crucial for ensuring complete tumor removal. Rapid Evaporative Ionization Mass
Spectrometry (REIMS), a tool for real-time intraoperative margin assessment,
generates spectra that require machine learning models to support clinical
decision-making. However, the scarcity of labeled data in surgical contexts
presents a significant challenge. This study is the first to develop a
foundation model tailored specifically for REIMS data, addressing this
limitation and advancing real-time surgical margin assessment. Methods: We
propose FACT, a Foundation model for Assessing Cancer Tissue margins. FACT is
an adaptation of a foundation model originally designed for text-audio
association, pretrained using our proposed supervised contrastive approach
based on triplet loss. An ablation study is performed to compare our proposed
model against other models and pretraining methods. Results: Our proposed model
significantly improves the classification performance, achieving
state-of-the-art performance with an AUROC of $82.4\% \pm 0.8$. The results
demonstrate the advantage of our proposed pretraining method and selected
backbone over the self-supervised and semi-supervised baselines and alternative
models. Conclusion: Our findings demonstrate that foundation models, adapted
and pretrained using our novel approach, can effectively classify REIMS data
even with limited labeled examples. This highlights the viability of foundation
models for enhancing real-time surgical margin assessment, particularly in
data-scarce clinical environments.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [170] [Control of Rayleigh-Bénard Convection: Effectiveness of Reinforcement Learning in the Turbulent Regime](https://arxiv.org/abs/2504.12000)
*Thorben Markmann,Michiel Straat,Sebastian Peitz,Barbara Hammer*

Main category: physics.flu-dyn

TL;DR: 该论文研究了强化学习（RL）在2D Rayleigh-Bénard对流（RBC）系统中减少对流热传递的效果，并在不同初始条件和湍流水平下验证了控制的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 数据驱动的流控制在工业、能源系统和气候科学中具有重要潜力，研究RL在此领域的应用效果。

Method: 使用单智能体近端策略优化（PPO）训练RL代理，并与经典控制理论中的线性比例微分（PD）控制器进行比较。引入奖励塑造技术以加速训练。

Result: RL代理在中等湍流系统中将努塞尔数降低33%，在高度湍流环境中降低10%，明显优于PD控制。代理在不同初始条件和较高湍流水平下表现出强泛化能力。

Conclusion: RL在减少对流热传递方面优于传统控制方法，且奖励塑造技术提高了样本效率和稳定性。

Abstract: Data-driven flow control has significant potential for industry, energy
systems, and climate science. In this work, we study the effectiveness of
Reinforcement Learning (RL) for reducing convective heat transfer in the 2D
Rayleigh-B\'enard Convection (RBC) system under increasing turbulence. We
investigate the generalizability of control across varying initial conditions
and turbulence levels and introduce a reward shaping technique to accelerate
the training. RL agents trained via single-agent Proximal Policy Optimization
(PPO) are compared to linear proportional derivative (PD) controllers from
classical control theory. The RL agents reduced convection, measured by the
Nusselt Number, by up to 33% in moderately turbulent systems and 10% in highly
turbulent settings, clearly outperforming PD control in all settings. The
agents showed strong generalization performance across different initial
conditions and to a significant extent, generalized to higher degrees of
turbulence. The reward shaping improved sample efficiency and consistently
stabilized the Nusselt Number to higher turbulence levels.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [171] [Towards Interpretable Deep Generative Models via Causal Representation Learning](https://arxiv.org/abs/2504.11609)
*Gemma E. Moran,Bryon Aragam*

Main category: stat.ML

TL;DR: 本文综述了因果表示学习（CRL）的最新进展，探讨了其与传统模型的联系、统计与因果可识别性结果，以及关键应用领域和开放性问题。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型因其黑箱特性难以解释，CRL旨在通过因果性构建可解释、灵活且可迁移的生成式AI模型。

Method: CRL结合了潜在变量模型、因果图模型和非参数统计与深度学习，从统计角度分析其进展。

Result: CRL展示了在生成式AI中的潜力，提供了可解释性和灵活性，但仍存在未解决的统计问题。

Conclusion: CRL是生成式AI领域的重要方向，未来需进一步解决其统计和实现挑战。

Abstract: Recent developments in generative artificial intelligence (AI) rely on
machine learning techniques such as deep learning and generative modeling to
achieve state-of-the-art performance across wide-ranging domains. These
methods' surprising performance is due in part to their ability to learn
implicit "representations'' of complex, multi-modal data. Unfortunately, deep
neural networks are notoriously black boxes that obscure these representations,
making them difficult to interpret or analyze. To resolve these difficulties,
one approach is to build new interpretable neural network models from the
ground up. This is the goal of the emerging field of causal representation
learning (CRL) that uses causality as a vector for building flexible,
interpretable, and transferable generative AI. CRL can be seen as a culmination
of three intrinsically statistical problems: (i) latent variable models such as
factor analysis; (ii) causal graphical models with latent variables; and (iii)
nonparametric statistics and deep learning. This paper reviews recent progress
in CRL from a statistical perspective, focusing on connections to classical
models and statistical and causal identifiablity results. This review also
highlights key application areas, implementation strategies, and open
statistical questions in CRL.

</details>


### [172] [FEAT: Free energy Estimators with Adaptive Transport](https://arxiv.org/abs/2504.11516)
*Jiajun He,Yuanqi Du,Francisco Vargas,Yuanqing Wang,Carla P. Gomes,José Miguel Hernández-Lobato,Eric Vanden-Eijnden*

Main category: stat.ML

TL;DR: FEAT是一种新的自由能估计框架，结合了自适应传输和神经网络方法，提供一致性和最小方差估计器，并在实验中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 自由能估计是科学领域中的关键挑战，现有方法存在局限性，需要一种更统一且高效的理论框架。

Method: FEAT利用随机插值实现的自适应传输，基于Jarzynski等式和Crooks定理提供估计器，并引入变分上下界。

Result: 实验验证表明，FEAT在玩具模型、分子模拟和量子场论中优于现有学习型方法。

Conclusion: FEAT为神经自由能计算提供了理论基础，统一了平衡和非平衡方法。

Abstract: We present Free energy Estimators with Adaptive Transport (FEAT), a novel
framework for free energy estimation -- a critical challenge across scientific
domains. FEAT leverages learned transports implemented via stochastic
interpolants and provides consistent, minimum-variance estimators based on
escorted Jarzynski equality and controlled Crooks theorem, alongside
variational upper and lower bounds on free energy differences. Unifying
equilibrium and non-equilibrium methods under a single theoretical framework,
FEAT establishes a principled foundation for neural free energy calculations.
Experimental validation on toy examples, molecular simulations, and quantum
field theory demonstrates improvements over existing learning-based methods.

</details>


### [173] [Normalizing Flow Regression for Bayesian Inference with Offline Likelihood Evaluations](https://arxiv.org/abs/2504.11554)
*Chengkun Li,Bobby Huggins,Petrus Mikkola,Luigi Acerbi*

Main category: stat.ML

TL;DR: 提出了一种新的离线推理方法NFR，用于近似后验分布，避免了传统方法中额外的采样或推理步骤。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯推断在计算成本高的似然评估中仍具挑战性，需要一种高效的方法。

Method: NFR通过回归现有的对数密度评估直接得到可处理的后验近似，并引入了针对流回归的训练技术。

Result: 在合成基准和实际应用中，NFR表现出优于或与现有方法相当的性能。

Conclusion: NFR为计算成本高的贝叶斯推断提供了一种有前景的解决方案，尤其适用于现有模型评估可重复利用的场景。

Abstract: Bayesian inference with computationally expensive likelihood evaluations
remains a significant challenge in many scientific domains. We propose
normalizing flow regression (NFR), a novel offline inference method for
approximating posterior distributions. Unlike traditional surrogate approaches
that require additional sampling or inference steps, NFR directly yields a
tractable posterior approximation through regression on existing log-density
evaluations. We introduce training techniques specifically for flow regression,
such as tailored priors and likelihood functions, to achieve robust posterior
and model evidence estimation. We demonstrate NFR's effectiveness on synthetic
benchmarks and real-world applications from neuroscience and biology, showing
superior or comparable performance to existing methods. NFR represents a
promising approach for Bayesian inference when standard methods are
computationally prohibitive or existing model evaluations can be recycled.

</details>


### [174] [Generalized probabilistic canonical correlation analysis for multi-modal data integration with full or partial observations](https://arxiv.org/abs/2504.11610)
*Tianjian Yang,Wei Vivian Li*

Main category: stat.ML

TL;DR: GPCCA是一种无监督方法，用于多模态数据的集成和联合降维，能处理缺失值并提升聚类准确性。


<details>
  <summary>Details</summary>
Motivation: 随着多模态数据量和复杂性的增加，需要一种能够整合多种模态并利用其互补信息的计算模型，尤其是在处理部分缺失数据时。

Method: 提出广义概率典型相关分析（GPCCA），支持多模态集成、处理缺失值、识别信息特征，并考虑单模态内相关性。

Result: GPCCA在模拟实验中优于现有方法，适用于TCGA癌症数据集和多视图图像数据，提供鲁棒的低维嵌入。

Conclusion: GPCCA为多模态数据集成提供了有效框架，处理缺失数据能力强，应用广泛，并已发布R包供研究社区使用。

Abstract: Background: The integration and analysis of multi-modal data are increasingly
essential across various domains including bioinformatics. As the volume and
complexity of such data grow, there is a pressing need for computational models
that not only integrate diverse modalities but also leverage their
complementary information to improve clustering accuracy and insights,
especially when dealing with partial observations with missing data. Results:
We propose Generalized Probabilistic Canonical Correlation Analysis (GPCCA), an
unsupervised method for the integration and joint dimensionality reduction of
multi-modal data. GPCCA addresses key challenges in multi-modal data analysis
by handling missing values within the model, enabling the integration of more
than two modalities, and identifying informative features while accounting for
correlations within individual modalities. The model demonstrates robustness to
various missing data patterns and provides low-dimensional embeddings that
facilitate downstream clustering and analysis. In a range of simulation
settings, GPCCA outperforms existing methods in capturing essential patterns
across modalities. Additionally, we demonstrate its applicability to
multi-omics data from TCGA cancer datasets and a multi-view image dataset.
Conclusion: GPCCA offers a useful framework for multi-modal data integration,
effectively handling missing data and providing informative low-dimensional
embeddings. Its performance across cancer genomics and multi-view image data
highlights its robustness and potential for broad application. To make the
method accessible to the wider research community, we have released an R
package, GPCCA, which is available at https://github.com/Kaversoniano/GPCCA.

</details>


### [175] [Discrimination-free Insurance Pricing with Privatized Sensitive Attributes](https://arxiv.org/abs/2504.11775)
*Tianhe Zhang,Suhan Liu,Peng Shi*

Main category: stat.ML

TL;DR: 论文探讨了机器学习算法中的公平性问题，特别是在保险定价领域，提出了一种高效的方法来构建公平模型，满足监管要求并确保公平性。


<details>
  <summary>Details</summary>
Motivation: 随着AI在社会决策中的广泛应用，确保算法公平性变得至关重要。保险定价领域因其特殊性，公平性实现面临独特挑战，需要专门的方法。

Method: 提出了一种高效的方法，利用私有化的敏感属性构建公平模型，无需直接访问敏感属性，同时适应不同的透明度要求。

Result: 该方法提供了统计保证，满足了监管要求，并确保了保险定价中的公平性。

Conclusion: 研究为保险领域的公平性实现提供了可行方案，同时适应了监管和透明度需求。

Abstract: Fairness has emerged as a critical consideration in the landscape of machine
learning algorithms, particularly as AI continues to transform decision-making
across societal domains. To ensure that these algorithms are free from bias and
do not discriminate against individuals based on sensitive attributes such as
gender and race, the field of algorithmic bias has introduced various fairness
concepts, along with methodologies to achieve these notions in different
contexts. Despite the rapid advancement, not all sectors have embraced these
fairness principles to the same extent. One specific sector that merits
attention in this regard is insurance. Within the realm of insurance pricing,
fairness is defined through a distinct and specialized framework. Consequently,
achieving fairness according to established notions does not automatically
ensure fair pricing in insurance. In particular, regulators are increasingly
emphasizing transparency in pricing algorithms and imposing constraints on
insurance companies on the collection and utilization of sensitive consumer
attributes. These factors present additional challenges in the implementation
of fairness in pricing algorithms. To address these complexities and comply
with regulatory demands, we propose an efficient method for constructing fair
models that are tailored to the insurance domain, using only privatized
sensitive attributes. Notably, our approach ensures statistical guarantees,
does not require direct access to sensitive attributes, and adapts to varying
transparency requirements, addressing regulatory demands while ensuring
fairness in insurance pricing.

</details>


### [176] [Approximation Bounds for Transformer Networks with Application to Regression](https://arxiv.org/abs/2504.12175)
*Yuling Jiao,Yanming Lai,Defeng Sun,Yang Wang,Bokai Yan*

Main category: stat.ML

TL;DR: 论文研究了Transformer网络对Hölder和Sobolev函数的逼近能力，并将其应用于非参数回归估计。结果表明，固定深度的Transformer网络能以较少的参数实现高精度逼近，并推导了非参数回归的收敛速率。


<details>
  <summary>Details</summary>
Motivation: 探索Transformer网络在逼近Hölder和Sobolev函数方面的能力，并将其应用于依赖观测的非参数回归问题。

Method: 通过理论分析，推导Transformer网络在逼近Hölder和Sobolev函数时的参数上界，并提出基于Kolmogorov-Arnold表示定理的新证明策略。

Result: 固定深度的Transformer网络能以参数数量为ε^(-d_x n / γ)的规模逼近Hölder函数，并在非参数回归中实现显式收敛速率。

Conclusion: Transformer网络在逼近复杂函数和解决非参数回归问题中表现出色，且自注意力机制的可解释性得到新见解。

Abstract: We explore the approximation capabilities of Transformer networks for
H\"older and Sobolev functions, and apply these results to address
nonparametric regression estimation with dependent observations. First, we
establish novel upper bounds for standard Transformer networks approximating
sequence-to-sequence mappings whose component functions are H\"older continuous
with smoothness index $\gamma \in (0,1]$. To achieve an approximation error
$\varepsilon$ under the $L^p$-norm for $p \in [1, \infty]$, it suffices to use
a fixed-depth Transformer network whose total number of parameters scales as
$\varepsilon^{-d_x n / \gamma}$. This result not only extends existing findings
to include the case $p = \infty$, but also matches the best known upper bounds
on number of parameters previously obtained for fixed-depth FNNs and RNNs.
Similar bounds are also derived for Sobolev functions. Second, we derive
explicit convergence rates for the nonparametric regression problem under
various $\beta$-mixing data assumptions, which allow the dependence between
observations to weaken over time. Our bounds on the sample complexity impose no
constraints on weight magnitudes. Lastly, we propose a novel proof strategy to
establish approximation bounds, inspired by the Kolmogorov-Arnold
representation theorem. We show that if the self-attention layer in a
Transformer can perform column averaging, the network can approximate
sequence-to-sequence H\"older functions, offering new insights into the
interpretability of self-attention mechanisms.

</details>


### [177] [Leave-One-Out Stable Conformal Prediction](https://arxiv.org/abs/2504.12189)
*Kiljae Lee,Yuan Zhang*

Main category: stat.ML

TL;DR: 提出了一种名为LOO-StabCP的新方法，通过利用留一稳定性加速全保形预测，解决了计算效率与预测准确性之间的平衡问题。


<details>
  <summary>Details</summary>
Motivation: 解决全保形预测在大量预测请求时计算效率低的问题，同时避免样本分割。

Method: 利用留一稳定性（LOO-StabCP）加速全保形预测，适用于多种机器学习工具（如RLM、SGD、核方法、神经网络和装袋）。

Result: 理论上有稳定性边界支持，数值实验在合成和真实数据上表现优异，应用于筛选问题时测试能力优于现有方法。

Conclusion: LOO-StabCP是一种高效且理论可靠的方法，适用于大规模预测任务，显著提升了计算效率和预测准确性。

Abstract: Conformal prediction (CP) is an important tool for distribution-free
predictive uncertainty quantification. Yet, a major challenge is to balance
computational efficiency and prediction accuracy, particularly for multiple
predictions. We propose Leave-One-Out Stable Conformal Prediction (LOO-StabCP),
a novel method to speed up full conformal using algorithmic stability without
sample splitting. By leveraging leave-one-out stability, our method is much
faster in handling a large number of prediction requests compared to existing
method RO-StabCP based on replace-one stability. We derived stability bounds
for several popular machine learning tools: regularized loss minimization (RLM)
and stochastic gradient descent (SGD), as well as kernel method, neural
networks and bagging. Our method is theoretically justified and demonstrates
superior numerical performance on synthetic and real-world data. We applied our
method to a screening problem, where its effective exploitation of training
data led to improved test power compared to state-of-the-art method based on
split conformal.

</details>


<div id='hep-ph'></div>

# hep-ph [[Back]](#toc)

### [178] [Strengthening Anomaly Awareness](https://arxiv.org/abs/2504.11520)
*Adam Banda,Charanjit K. Khosa,Veronica Sanz*

Main category: hep-ph

TL;DR: 本文提出了一种改进的Anomaly Awareness框架，通过两阶段训练策略在变分自编码器（VAE）中引入少量监督，提升无监督异常检测的性能。


<details>
  <summary>Details</summary>
Motivation: 无监督异常检测模型通常缺乏对异常样本的敏感性，本文旨在通过少量标记异常样本的微调，提升模型对未知异常的检测能力。

Method: 采用两阶段训练策略：首先在背景数据上进行无监督训练，然后利用少量标记异常样本微调模型，以增大异常样本的重构误差。

Result: 在多个领域（如MNIST、网络入侵数据、粒子物理数据等）验证了方法的有效性，模型对未知异常的敏感性显著提升，正常与异常样本的区分度更好。

Conclusion: 研究表明，即使有限的异常信息，通过针对性微调也能显著提升无监督异常检测模型的泛化能力和性能。

Abstract: We present a refined version of the Anomaly Awareness framework for enhancing
unsupervised anomaly detection. Our approach introduces minimal supervision
into Variational Autoencoders (VAEs) through a two-stage training strategy: the
model is first trained in an unsupervised manner on background data, and then
fine-tuned using a small sample of labeled anomalies to encourage larger
reconstruction errors for anomalous samples.
  We validate the method across diverse domains, including the MNIST dataset
with synthetic anomalies, network intrusion data from the CICIDS benchmark,
collider physics data from the LHCO2020 dataset, and simulated events from the
Standard Model Effective Field Theory (SMEFT). The latter provides a realistic
example of subtle kinematic deviations in Higgs boson production. In all cases,
the model demonstrates improved sensitivity to unseen anomalies, achieving
better separation between normal and anomalous samples. These results indicate
that even limited anomaly information, when incorporated through targeted
fine-tuning, can substantially improve the generalization and performance of
unsupervised models for anomaly detection.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [179] [MULTI-LF: A Unified Continuous Learning Framework for Real-Time DDoS Detection in Multi-Environment Networks](https://arxiv.org/abs/2504.11575)
*Furqan Rustam,Islam Obaidat,Anca Delia Jurcut*

Main category: cs.CR

TL;DR: 提出了一种在线持续学习方法（MULTI-LF），用于多环境网络中DDoS攻击的实时检测，结合轻量级和复杂模型，实现高准确率和低延迟。


<details>
  <summary>Details</summary>
Motivation: 现有AI检测系统难以适应新攻击策略且缺乏实时性，需一种能持续学习和适应新兴威胁的方法。

Method: 使用NS-3工具模拟真实多环境网络数据集，开发MULTI-LF框架，包含轻量级模型（M1）和复杂模型（M2），通过模型协作和人工干预实现持续学习。

Result: 分类准确率达0.999，预测延迟仅0.866秒，内存占用3.632 MB，CPU利用率10.05%，优于基线方法。

Conclusion: MULTI-LF在多环境网络中高效检测DDoS攻击，具备实时性和适应性，未来可扩展至更多场景。

Abstract: Detecting Distributed Denial of Service (DDoS) attacks in Multi-Environment
(M-En) networks presents significant challenges due to diverse malicious
traffic patterns and the evolving nature of cyber threats. Existing AI-based
detection systems struggle to adapt to new attack strategies and lack real-time
attack detection capabilities with high accuracy and efficiency. This study
proposes an online, continuous learning methodology for DDoS detection in M-En
networks, enabling continuous model updates and real-time adaptation to
emerging threats, including zero-day attacks. First, we develop a unique M-En
network dataset by setting up a realistic, real-time simulation using the NS-3
tool, incorporating both victim and bot devices. DDoS attacks with varying
packet sizes are simulated using the DDoSim application across IoT and
traditional IP-based environments under M-En network criteria. Our approach
employs a multi-level framework (MULTI-LF) featuring two machine learning
models: a lightweight Model 1 (M1) trained on a selective, critical packet
dataset for fast and efficient initial detection, and a more complex, highly
accurate Model 2 (M2) trained on extensive data. When M1 exhibits low
confidence in its predictions, the decision is escalated to M2 for verification
and potential fine-tuning of M1 using insights from M2. If both models
demonstrate low confidence, the system flags the incident for human
intervention, facilitating model updates with human-verified categories to
enhance adaptability to unseen attack patterns. We validate the MULTI-LF
through real-world simulations, demonstrating superior classification accuracy
of 0.999 and low prediction latency of 0.866 seconds compared to established
baselines. Furthermore, we evaluate performance in terms of memory usage (3.632
MB) and CPU utilization (10.05%) in real-time scenarios.

</details>


### [180] [Progent: Programmable Privilege Control for LLM Agents](https://arxiv.org/abs/2504.11703)
*Tianneng Shi,Jingxuan He,Zhun Wang,Linyu Wu,Hongwei Li,Wenbo Guo,Dawn Song*

Main category: cs.CR

TL;DR: Progent是首个针对LLM代理的特权控制机制，通过领域特定语言表达细粒度策略，确保安全性和实用性。


<details>
  <summary>Details</summary>
Motivation: LLM代理在与外部世界交互时可能执行恶意命令，需通过最小权限原则解决安全问题。

Method: 引入Progent，使用领域特定语言定义特权控制策略，动态生成和更新策略。

Result: 在多个场景（AgentDojo、ASB、AgentPoison）中验证了安全性和实用性。

Conclusion: Progent通过模块化设计和自动化策略生成，有效平衡了安全性和实用性。

Abstract: LLM agents are an emerging form of AI systems where large language models
(LLMs) serve as the central component, utilizing a diverse set of tools to
complete user-assigned tasks. Despite their great potential, LLM agents pose
significant security risks. When interacting with the external world, they may
encounter malicious commands from attackers, leading to the execution of
dangerous actions. A promising way to address this is by enforcing the
principle of least privilege: allowing only essential actions for task
completion while blocking unnecessary ones. However, achieving this is
challenging, as it requires covering diverse agent scenarios while preserving
both security and utility.
  We introduce Progent, the first privilege control mechanism for LLM agents.
At its core is a domain-specific language for flexibly expressing privilege
control policies applied during agent execution. These policies provide
fine-grained constraints over tool calls, deciding when tool calls are
permissible and specifying fallbacks if they are not. This enables agent
developers and users to craft suitable policies for their specific use cases
and enforce them deterministically to guarantee security. Thanks to its modular
design, integrating Progent does not alter agent internals and requires only
minimal changes to agent implementation, enhancing its practicality and
potential for widespread adoption. To automate policy writing, we leverage LLMs
to generate policies based on user queries, which are then updated dynamically
for improved security and utility. Our extensive evaluation shows that it
enables strong security while preserving high utility across three distinct
scenarios or benchmarks: AgentDojo, ASB, and AgentPoison. Furthermore, we
perform an in-depth analysis, showcasing the effectiveness of its core
components and the resilience of its automated policy generation against
adaptive attacks.

</details>


### [181] [PCDiff: Proactive Control for Ownership Protection in Diffusion Models with Watermark Compatibility](https://arxiv.org/abs/2504.11774)
*Keke Gai,Ziyue Shen,Jing Yu,Liehuang Zhu,Qi Wu*

Main category: cs.CR

TL;DR: PCDiff是一个主动访问控制框架，通过调节生成质量保护文本到图像扩散模型的知识产权。它集成了可训练的融合模块和分层认证层，确保只有拥有有效加密凭证的用户能生成高质量图像。


<details>
  <summary>Details</summary>
Motivation: 随着对文本到图像扩散模型知识产权保护需求的增长，需要一种主动控制模型授权的方法，防止未经授权的利用。

Method: PCDiff在解码器架构中集成可训练的融合模块和分层认证层，通过验证加密凭证控制生成质量。

Result: 实验证明，凭证验证与图像质量之间存在强依赖关系，且PCDiff与传统水印方法结合表现优异。

Conclusion: PCDiff从被动检测转向主动授权执行，为扩散模型的知识产权管理奠定了基础。

Abstract: With the growing demand for protecting the intellectual property (IP) of
text-to-image diffusion models, we propose PCDiff -- a proactive access control
framework that redefines model authorization by regulating generation quality.
At its core, PCDIFF integrates a trainable fuser module and hierarchical
authentication layers into the decoder architecture, ensuring that only users
with valid encrypted credentials can generate high-fidelity images. In the
absence of valid keys, the system deliberately degrades output quality,
effectively preventing unauthorized exploitation.Importantly, while the primary
mechanism enforces active access control through architectural intervention,
its decoupled design retains compatibility with existing watermarking
techniques. This satisfies the need of model owners to actively control model
ownership while preserving the traceability capabilities provided by
traditional watermarking approaches.Extensive experimental evaluations confirm
a strong dependency between credential verification and image quality across
various attack scenarios. Moreover, when combined with typical post-processing
operations, PCDIFF demonstrates powerful performance alongside conventional
watermarking methods. This work shifts the paradigm from passive detection to
proactive enforcement of authorization, laying the groundwork for IP management
of diffusion models.

</details>


### [182] [ARCeR: an Agentic RAG for the Automated Definition of Cyber Ranges](https://arxiv.org/abs/2504.12143)
*Matteo Lupinacci,Francesco Blefari,Francesco Romeo,Francesco Aurelio Pironti,Angelo Furfaro*

Main category: cs.CR

TL;DR: ARCeR是一种基于Agentic RAG范式的创新解决方案，用于从自然语言描述自动生成和部署网络靶场（Cyber Ranges），以应对不断变化的网络安全威胁。


<details>
  <summary>Details</summary>
Motivation: 网络安全威胁日益复杂，需要开发工具和平台来创建虚拟、可控的网络靶场（CRs），用于漏洞分析、反制措施测试以及网络安全技能培训。

Method: ARCeR利用Agentic RAG范式，结合最先进的AI技术，从用户提供的自然语言描述中自动生成和部署CRs。

Result: 实验表明，ARCeR能够成功处理LLMs或基本RAG系统无法应对的提示，并能针对任何CR框架，前提是提供特定知识。

Conclusion: ARCeR为网络安全领域提供了一种高效、灵活的解决方案，能够满足多样化的需求。

Abstract: The growing and evolving landscape of cybersecurity threats necessitates the
development of supporting tools and platforms that allow for the creation of
realistic IT environments operating within virtual, controlled settings as
Cyber Ranges (CRs). CRs can be exploited for analyzing vulnerabilities and
experimenting with the effectiveness of devised countermeasures, as well as
serving as training environments for building cyber security skills and
abilities for IT operators. This paper proposes ARCeR as an innovative solution
for the automatic generation and deployment of CRs, starting from user-provided
descriptions in a natural language. ARCeR relies on the Agentic RAG paradigm,
which allows it to fully exploit state-of-art AI technologies. Experimental
results show that ARCeR is able to successfully process prompts even in cases
that LLMs or basic RAG systems are not able to cope with. Furthermore, ARCeR is
able to target any CR framework provided that specific knowledge is made
available to it.

</details>

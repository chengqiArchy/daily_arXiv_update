<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 86]
- [cs.AI](#cs.AI) [Total: 14]
- [cs.LG](#cs.LG) [Total: 67]
- [stat.ML](#stat.ML) [Total: 5]
- [eess.SY](#eess.SY) [Total: 1]
- [cs.LO](#cs.LO) [Total: 1]
- [nlin.CD](#nlin.CD) [Total: 1]
- [cs.CR](#cs.CR) [Total: 2]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [eess.IV](#eess.IV) [Total: 4]
- [hep-th](#hep-th) [Total: 1]
- [cs.SD](#cs.SD) [Total: 1]
- [stat.ME](#stat.ME) [Total: 2]
- [cs.RO](#cs.RO) [Total: 8]
- [cs.NE](#cs.NE) [Total: 1]
- [cs.GR](#cs.GR) [Total: 2]
- [cs.IR](#cs.IR) [Total: 8]
- [cs.CY](#cs.CY) [Total: 6]
- [cs.CV](#cs.CV) [Total: 26]
- [quant-ph](#quant-ph) [Total: 3]
- [physics.ao-ph](#physics.ao-ph) [Total: 1]
- [eess.SP](#eess.SP) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [q-bio.GN](#q-bio.GN) [Total: 1]
- [cs.MA](#cs.MA) [Total: 4]
- [cs.HC](#cs.HC) [Total: 4]
- [eess.AS](#eess.AS) [Total: 1]
- [math.OC](#math.OC) [Total: 2]
- [cs.SE](#cs.SE) [Total: 3]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Unmasking the Reality of PII Masking Models: Performance Gaps and the Call for Accountability](https://arxiv.org/abs/2504.12308)
*Devansh Singh,Sundaraparipurnan Narayanan*

Main category: cs.CL

TL;DR: 论文探讨了隐私掩码技术及其依赖的NER方法在PII识别中的局限性，并通过实验展示了现有模型的隐私暴露问题。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于揭示现有PII数据集和NER方法在隐私掩码中的不足，以及广泛使用的模型可能带来的隐私风险。

Method: 方法包括构建包含16种PII类型的17K半合成句子数据集，并在多个维度（如基本实体识别、上下文消歧等）上评估模型性能。

Result: 结果显示现有模型在隐私掩码中存在显著缺陷，可能导致隐私暴露，尤其是考虑到模型的广泛下载量。

Conclusion: 结论强调需要改进模型性能评估方法，并在模型卡片中提供更多上下文信息以减少隐私风险。

Abstract: Privacy Masking is a critical concept under data privacy involving
anonymization and de-anonymization of personally identifiable information
(PII). Privacy masking techniques rely on Named Entity Recognition (NER)
approaches under NLP support in identifying and classifying named entities in
each text. NER approaches, however, have several limitations including (a)
content sensitivity including ambiguous, polysemic, context dependent or domain
specific content, (b) phrasing variabilities including nicknames and alias,
informal expressions, alternative representations, emerging expressions,
evolving naming conventions and (c) formats or syntax variations, typos,
misspellings. However, there are a couple of PII datasets that have been widely
used by researchers and the open-source community to train models on PII
detection or masking. These datasets have been used to train models including
Piiranha and Starpii, which have been downloaded over 300k and 580k times on
HuggingFace. We examine the quality of the PII masking by these models given
the limitations of the datasets and of the NER approaches. We curate a dataset
of 17K unique, semi-synthetic sentences containing 16 types of PII by compiling
information from across multiple jurisdictions including India, U.K and U.S. We
generate sentences (using language models) containing these PII at five
different NER detection feature dimensions - (1) Basic Entity Recognition, (2)
Contextual Entity Disambiguation, (3) NER in Noisy & Real-World Data, (4)
Evolving & Novel Entities Detection and (5) Cross-Lingual or multi-lingual NER)
and 1 in adversarial context. We present the results and exhibit the privacy
exposure caused by such model use (considering the extent of lifetime downloads
of these models). We conclude by highlighting the gaps in measuring performance
of the models and the need for contextual disclosure in model cards for such
models.

</details>


### [2] [Learning Optimal Prompt Ensemble for Multi-source Visual Prompt Transfer](https://arxiv.org/abs/2504.12311)
*Enming Zhang,Liwen Cao,Yanru Wu,Zijie Zhao,Guan Wang,Yang Li*

Main category: cs.CL

TL;DR: HGPrompt是一个自适应框架，通过优化双重目标（可转移性和稳定性）来学习多源提示的最优组合权重，解决提示聚合中的表示崩溃问题。


<details>
  <summary>Details</summary>
Motivation: 预训练提示已成为宝贵资产，但直接聚合多源提示会导致表示崩溃，影响性能。

Method: 提出信息论指标评估提示特征的可转移性，并引入梯度对齐正则化以减少梯度冲突。

Result: 在VTAB基准测试中，HGPrompt实现了最先进的性能。

Conclusion: HGPrompt有效解决了多源提示转移中的干扰问题，提升了泛化能力。

Abstract: Prompt tuning has emerged as a lightweight adaptation strategy for adapting
foundation models to downstream tasks, particularly in resource-constrained
systems. As pre-trained prompts have become valuable intellectual assets,
combining multiple source prompts offers a promising approach to enhance
generalization to new tasks by leveraging complementary knowledge from diverse
sources. However, naive aggregation of these prompts often leads to
representation collapse due to mutual interference, undermining their
collective potential. To address these challenges, we propose HGPrompt, an
adaptive framework for multi-source prompt transfer that learns optimal
ensemble weights by jointly optimizing dual objectives: transferability and
stability. Specifically, we first introduce an information-theoretic metric to
evaluate the transferability of prompt-induced features on the target task,
capturing the intrinsic alignment between the feature representations.
Additionally, we propose a novel Gradient Alignment Regularization to mitigate
gradient conflicts among prompts, enabling stable and coherent knowledge
transfer from multiple sources while suppressing interference. Extensive
experiments on the large-scale VTAB benchmark demonstrate that HGPrompt
achieves state-of-the-art performance, validating its effectiveness in
multi-source prompt transfer.

</details>


### [3] [Socrates or Smartypants: Testing Logic Reasoning Capabilities of Large Language Models with Logic Programming-based Test Oracles](https://arxiv.org/abs/2504.12312)
*Zihao Xu,Junchen Ding,Yiling Lou,Kun Zhang,Dong Gong,Yuekang Li*

Main category: cs.CL

TL;DR: 论文提出了SmartyPat-Bench和SmartyPat框架，用于评估和改进大语言模型的逻辑推理能力，通过自然语言和自动化生成逻辑谬误数据。


<details>
  <summary>Details</summary>
Motivation: 现有数据集和基准测试在逻辑推理评估中过于简单或不自然，需要更挑战性和多样化的数据。

Method: 引入SmartyPat-Bench（基于Reddit的真实数据）和SmartyPat框架（利用逻辑编程和LLM生成谬误数据）。

Result: SmartyPat生成的谬误数据与人工生成的质量相当，且显著优于基线方法。实验揭示了LLM在谬误检测和分类中的表现。

Conclusion: SmartyPat框架有效解决了数据生成和标注的局限性，为LLM逻辑推理能力评估提供了新工具。

Abstract: Large Language Models (LLMs) have achieved significant progress in language
understanding and reasoning. Evaluating and analyzing their logical reasoning
abilities has therefore become essential. However, existing datasets and
benchmarks are often limited to overly simplistic, unnatural, or contextually
constrained examples. In response to the growing demand, we introduce
SmartyPat-Bench, a challenging, naturally expressed, and systematically labeled
benchmark derived from real-world high-quality Reddit posts containing subtle
logical fallacies. Unlike existing datasets and benchmarks, it provides more
detailed annotations of logical fallacies and features more diverse data. To
further scale up the study and address the limitations of manual data
collection and labeling - such as fallacy-type imbalance and labor-intensive
annotation - we introduce SmartyPat, an automated framework powered by logic
programming-based oracles. SmartyPat utilizes Prolog rules to systematically
generate logically fallacious statements, which are then refined into fluent
natural-language sentences by LLMs, ensuring precise fallacy representation.
Extensive evaluation demonstrates that SmartyPat produces fallacies comparable
in subtlety and quality to human-generated content and significantly
outperforms baseline methods. Finally, experiments reveal nuanced insights into
LLM capabilities, highlighting that while excessive reasoning steps hinder
fallacy detection accuracy, structured reasoning enhances fallacy
categorization performance.

</details>


### [4] [Exploring the Impact of Personality Traits on Conversational Recommender Systems: A Simulation with Large Language Models](https://arxiv.org/abs/2504.12313)
*Xiaoyan Zhao,Yang Deng,Wenjie Wang,Hongzhan lin,Hong Cheng,Rui Zhang,See-Kiong Ng,Tat-Seng Chua*

Main category: cs.CL

TL;DR: 论文提出了一种基于大语言模型（LLM）的个性化感知用户模拟系统（PerCRS），用于研究人格特质如何影响对话推荐系统的结果。


<details>
  <summary>Details</summary>
Motivation: 人格特质对用户交互行为有显著影响，但目前对话推荐系统（CRSs）中缺乏对此的系统研究。

Method: 引入PerCRS，用户代理模拟个性化特质和偏好，系统代理具备说服能力以模拟真实交互。采用多角度评估确保鲁棒性。

Result: 实验表明，LLM能有效生成符合指定人格特质的多样化用户响应，促使CRSs动态调整推荐策略。

Conclusion: 研究为人格特质对对话推荐系统结果的影响提供了实证分析。

Abstract: Conversational Recommender Systems (CRSs) engage users in multi-turn
interactions to deliver personalized recommendations. The emergence of large
language models (LLMs) further enhances these systems by enabling more natural
and dynamic user interactions. However, a key challenge remains in
understanding how personality traits shape conversational recommendation
outcomes. Psychological evidence highlights the influence of personality traits
on user interaction behaviors. To address this, we introduce an LLM-based
personality-aware user simulation for CRSs (PerCRS). The user agent induces
customizable personality traits and preferences, while the system agent
possesses the persuasion capability to simulate realistic interaction in CRSs.
We incorporate multi-aspect evaluation to ensure robustness and conduct
extensive analysis from both user and system perspectives. Experimental results
demonstrate that state-of-the-art LLMs can effectively generate diverse user
responses aligned with specified personality traits, thereby prompting CRSs to
dynamically adjust their recommendation strategies. Our experimental analysis
offers empirical insights into the impact of personality traits on the outcomes
of conversational recommender systems.

</details>


### [5] [How to Detect and Defeat Molecular Mirage: A Metric-Driven Benchmark for Hallucination in LLM-based Molecular Comprehension](https://arxiv.org/abs/2504.12314)
*Hao Li,Liuzhenghao Lv,He Cao,Zijing Liu,Zhiyuan Yan,Yu Wang,Yonghong Tian,Yu Li,Li Yuan*

Main category: cs.CL

TL;DR: 论文分析了大型语言模型在分子理解任务中的幻觉问题，提出了一种新的评估指标Mol-Hallu，并设计了后处理阶段HRPP以减少幻觉。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型在分子理解任务中存在幻觉问题，导致药物设计和利用中的错误，亟需解决。

Method: 首先分析幻觉来源，提出Mol-Hallu评估指标量化幻觉程度，并设计HRPP后处理阶段以减少幻觉。

Result: 实验证明HRPP在解码器和编码器-解码器分子模型中均有效。

Conclusion: 研究为减少幻觉并提高科学应用中大型语言模型的可靠性提供了重要见解。

Abstract: Large language models are increasingly used in scientific domains, especially
for molecular understanding and analysis. However, existing models are affected
by hallucination issues, resulting in errors in drug design and utilization. In
this paper, we first analyze the sources of hallucination in LLMs for molecular
comprehension tasks, specifically the knowledge shortcut phenomenon observed in
the PubChem dataset. To evaluate hallucination in molecular comprehension tasks
with computational efficiency, we introduce \textbf{Mol-Hallu}, a novel
free-form evaluation metric that quantifies the degree of hallucination based
on the scientific entailment relationship between generated text and actual
molecular properties. Utilizing the Mol-Hallu metric, we reassess and analyze
the extent of hallucination in various LLMs performing molecular comprehension
tasks. Furthermore, the Hallucination Reduction Post-processing stage~(HRPP) is
proposed to alleviate molecular hallucinations, Experiments show the
effectiveness of HRPP on decoder-only and encoder-decoder molecular LLMs. Our
findings provide critical insights into mitigating hallucination and improving
the reliability of LLMs in scientific applications.

</details>


### [6] [Capybara-OMNI: An Efficient Paradigm for Building Omni-Modal Language Models](https://arxiv.org/abs/2504.12315)
*Xingguang Ji,Jiakang Wang,Hongzhi Zhang,Jingyuan Zhang,Haonan Zhou,Chenxi Sun,Yahui Liu,Qi Wang,Fuzheng Zhang*

Main category: cs.CL

TL;DR: Capybara-OMNI是一个轻量高效的多模态大语言模型（MLLM），支持文本、图像、视频和音频模态。论文详细介绍了其框架设计、数据构建和训练方法，并提供了专用基准测试。结果显示其性能与同类模型相当，并进一步讨论了如何训练聊天版本以增强交互能力。


<details>
  <summary>Details</summary>
Motivation: 由于构建和训练多模态数据对的复杂性，开发强大的MLLM仍是一个计算和时间密集型任务。本文旨在通过轻量高效的方法解决这一问题。

Method: 详细介绍了Capybara-OMNI的框架设计、数据构建和训练方法，并提供了专用基准测试。还讨论了如何基于理解模型训练聊天版本。

Result: Capybara-OMNI在同类模型中表现出竞争力，并通过聊天版本增强了多模态指令跟随和对话能力。

Conclusion: Capybara-OMNI及其聊天版本公开了模型权重、部分训练数据和推理代码，为社区提供了实用资源。

Abstract: With the development of Multimodal Large Language Models (MLLMs), numerous
outstanding accomplishments have emerged within the open-source community. Due
to the complexity of creating and training multimodal data pairs, it is still a
computational and time-consuming process to build powerful MLLMs. In this work,
we introduce Capybara-OMNI, an MLLM that trains in a lightweight and efficient
manner and supports understanding text, image, video, and audio modalities. We
present in detail the framework design, the data construction, and the training
recipe, to develop an MLLM step-by-step to obtain competitive performance. We
also provide exclusive benchmarks utilized in our experiments to show how to
properly verify understanding capabilities across different modalities. Results
show that by following our guidance, we can efficiently build an MLLM that
achieves competitive performance among models of the same scale on various
multimodal benchmarks. Additionally, to enhance the multimodal instruction
following and conversational capabilities of the model, we further discuss how
to train the chat version upon an MLLM understanding model, which is more in
line with user habits for tasks like real-time interaction with humans. We
publicly disclose the Capybara-OMNI model, along with its chat-based version.
The disclosure includes both the model weights, a portion of the training data,
and the inference codes, which are made available on GitHub.

</details>


### [7] [Data Metabolism: An Efficient Data Design Schema For Vision Language Model](https://arxiv.org/abs/2504.12316)
*Jingyuan Zhang,Hongzhi Zhang,Zhou Haonan,Chenxi Sun,Xingguang ji,Jiakang Wang,Fanheng Kong,Yahui Liu,Qi Wang,Fuzheng Zhang*

Main category: cs.CL

TL;DR: 本文提出了一种名为“数据代谢”的数据中心框架，用于构建视觉语言模型（VLM），并通过闭环系统持续优化模型性能。发布的Capybara-VL模型在多项任务中表现优异，甚至超越了一些规模更大的开源模型。


<details>
  <summary>Details</summary>
Motivation: 数据管理对训练强大的视觉语言模型至关重要，但现有方法缺乏系统化的数据迭代机制。

Method: 提出数据代谢框架，包括数据管理和迭代的闭环系统，并详细说明如何处理大规模数据集和构建用户特定的数据飞轮。

Result: Capybara-VL模型在多项任务中表现优异，超越了一些规模更大的开源模型，并与部分领先的专有模型性能相当。

Conclusion: 数据中心框架展示了训练更小、更高效视觉语言模型的潜力，为未来研究提供了新方向。

Abstract: Data curation plays a crucial role in training powerful Visual Language
Models (VLMs). In this work, we introduce the concept of Data Metabolism and
present our data-centric framework to build VLMs throughout the development
lifecycle. Starting from a standard model architecture, we discuss and provide
insights into two crucial development steps: data curation and iteration,
forming a closed-loop system that continuously improves model performance. We
show a detailed codebook on how to process existing massive datasets and build
user-specific data flywheel. As a demonstration, we release a VLM, named
Capybara-VL, which excels in typical multimodal tasks (e.g. , visual question
answering, scientific reasoning, and text-rich tasks). Despite its relatively
compact size, Capybara-VL surpasses several open-source models that are up to
10 times larger in size. Moreover, it achieves results that are on par with
those of several leading proprietary models, demonstrating its remarkable
competitiveness. These results highlight the power of our data-centric
framework and the potential of training smaller and more efficient VLMs.

</details>


### [8] [ChatGPT as Linguistic Equalizer? Quantifying LLM-Driven Lexical Shifts in Academic Writing](https://arxiv.org/abs/2504.12317)
*Dingkang Lin,Naixuan Zhao,Dan Tian,Jiang Li*

Main category: cs.CL

TL;DR: ChatGPT显著提升了非英语母语作者（NNES）论文摘要的词汇复杂性，减少了语言障碍，促进了学术公平。


<details>
  <summary>Details</summary>
Motivation: 研究ChatGPT是否能够减轻非英语母语作者在学术写作中的语言障碍，促进学术公平。

Method: 使用MTLD量化词汇复杂性，采用DID设计分析2.8百万篇OpenAlex文章（2020-2024）的词汇变化。

Result: ChatGPT显著提升了NNES作者摘要的词汇复杂性，尤其在预印本、技术与生物领域及低级别期刊中效果明显。

Conclusion: ChatGPT能有效减少语言差异，推动全球学术公平。

Abstract: The advent of ChatGPT has profoundly reshaped scientific research practices,
particularly in academic writing, where non-native English-speakers (NNES)
historically face linguistic barriers. This study investigates whether ChatGPT
mitigates these barriers and fosters equity by analyzing lexical complexity
shifts across 2.8 million articles from OpenAlex (2020-2024). Using the Measure
of Textual Lexical Diversity (MTLD) to quantify vocabulary sophistication and a
difference-in-differences (DID) design to identify causal effects, we
demonstrate that ChatGPT significantly enhances lexical complexity in
NNES-authored abstracts, even after controlling for article-level controls,
authorship patterns, and venue norms. Notably, the impact is most pronounced in
preprint papers, technology- and biology-related fields and lower-tier
journals. These findings provide causal evidence that ChatGPT reduces
linguistic disparities and promotes equity in global academia.

</details>


### [9] [Has the Creativity of Large-Language Models peaked? An analysis of inter- and intra-LLM variability](https://arxiv.org/abs/2504.12320)
*Jennifer Haase,Paul H. P. Hanel,Sebastian Pokutta*

Main category: cs.CL

TL;DR: 研究评估了14种大型语言模型（LLMs）在创造力任务中的表现，发现其创造力并未随时间提升，且输出存在显著变异性。


<details>
  <summary>Details</summary>
Motivation: 探讨LLMs在创造力任务中的表现是否随时间提升，以及其输出的一致性。

Method: 使用发散联想任务（DAT）和替代用途任务（AUT）评估14种LLMs的创造力。

Result: LLMs的创造力未显著提升，GPT-4表现下降；部分模型在AUT中优于人类平均水平，但极少达到人类顶尖水平。输出变异性显著。

Conclusion: 需更细致的评估框架，重视模型选择、提示设计和重复测试，以准确评估LLMs的创造力。

Abstract: Following the widespread adoption of ChatGPT in early 2023, numerous studies
reported that large language models (LLMs) can match or even surpass human
performance in creative tasks. However, it remains unclear whether LLMs have
become more creative over time, and how consistent their creative output is. In
this study, we evaluated 14 widely used LLMs -- including GPT-4, Claude, Llama,
Grok, Mistral, and DeepSeek -- across two validated creativity assessments: the
Divergent Association Task (DAT) and the Alternative Uses Task (AUT). Contrary
to expectations, we found no evidence of increased creative performance over
the past 18-24 months, with GPT-4 performing worse than in previous studies.
For the more widely used AUT, all models performed on average better than the
average human, with GPT-4o and o3-mini performing best. However, only 0.28% of
LLM-generated responses reached the top 10% of human creativity benchmarks.
Beyond inter-model differences, we document substantial intra-model
variability: the same LLM, given the same prompt, can produce outputs ranging
from below-average to original. This variability has important implications for
both creativity research and practical applications. Ignoring such variability
risks misjudging the creative potential of LLMs, either inflating or
underestimating their capabilities. The choice of prompts affected LLMs
differently. Our findings underscore the need for more nuanced evaluation
frameworks and highlight the importance of model selection, prompt design, and
repeated assessment when using Generative AI (GenAI) tools in creative
contexts.

</details>


### [10] [AttentionDefense: Leveraging System Prompt Attention for Explainable Defense Against Novel Jailbreaks](https://arxiv.org/abs/2504.12321)
*Charlotte Siska,Anush Sankaran*

Main category: cs.CL

TL;DR: 论文提出了一种名为AttentionDefense的新型防御方法，利用小型语言模型（SLM）的注意力机制来解释和检测对抗性提示，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 尽管语言模型（LM）在多领域表现出色，但其易受恶意输入攻击（jailbreak），导致偏离预期行为。现有防御方法难以解释恶意行为的原因，因此需要一种更透明且高效的解决方案。

Method: 通过小型语言模型（SLM）的注意力机制分析对抗性提示，提出AttentionDefense方法，并在基准数据集上进行评估。

Result: AttentionDefense在检测性能上优于基于文本嵌入的分类器和GPT-4零样本检测器，并在新型对抗性数据集上表现稳健。

Conclusion: AttentionDefense是一种高效、可解释且低成本的防御方法，适用于实际应用。

Abstract: In the past few years, Language Models (LMs) have shown par-human
capabilities in several domains. Despite their practical applications and
exceeding user consumption, they are susceptible to jailbreaks when malicious
input exploits the LM's weaknesses, causing it to deviate from its intended
behavior. Current defensive strategies either classify the input prompt as
adversarial or prevent LMs from generating harmful outputs. However, it is
challenging to explain the reason behind the malicious nature of the jailbreak,
which results in a wide variety of closed-box approaches. In this research, we
propose and demonstrate that system-prompt attention from Small Language Models
(SLMs) can be used to characterize adversarial prompts, providing a novel,
explainable, and cheaper defense approach called AttentionDefense. Our research
suggests that the attention mechanism is an integral component in understanding
and explaining how LMs respond to malicious input that is not captured in the
semantic meaning of text embeddings. The proposed AttentionDefense is evaluated
against existing jailbreak benchmark datasets. Ablation studies show that
SLM-based AttentionDefense has equivalent or better jailbreak detection
performance compared to text embedding-based classifiers and GPT-4 zero-shot
detectors.To further validate the efficacy of the proposed approach, we
generate a dataset of novel jailbreak variants of the existing benchmark
dataset using a closed-loop LLM-based multi-agent system. We demonstrate that
the proposed AttentionDefense approach performs robustly on this novel
jailbreak dataset while existing approaches suffer in performance.
Additionally, for practical purposes AttentionDefense is an ideal solution as
it has the computation requirements of a small LM but the performance of a LLM
detector.

</details>


### [11] [A Strategic Coordination Framework of Small LLMs Matches Large LLMs in Data Synthesis](https://arxiv.org/abs/2504.12322)
*Xin Gao,Qizhi Pei,Zinan Tang,Yu Li,Honglin Lin,Jiang Wu,Conghui He,Lijun Wu*

Main category: cs.CL

TL;DR: 论文提出了一种多小语言模型协作框架GRA，通过模拟同行评审过程提升数据合成质量，挑战了大模型的必要性。


<details>
  <summary>Details</summary>
Motivation: 当前数据合成方法依赖大语言模型（LLMs），存在高计算成本、环境效率低和潜在偏见问题。小模型虽可持续但能力有限。

Method: GRA框架将小模型分为Generator、Reviewer和Adjudicator三个角色，通过协作迭代优化数据合成。

Result: 实验表明，GRA生成的数据质量达到或超过单一大模型（如Qwen-2.5-72B-Instruct）。

Conclusion: GRA证明了小模型协作可替代大模型，为高效、可持续的数据合成提供了新思路。

Abstract: While data synthesis and distillation are promising strategies to enhance
small language models, current approaches heavily rely on Large Language Models
(LLMs), which suffer from high computational costs, environmental inefficiency,
and potential biases inherited from monolithic architectures. In contrast,
smaller LLMs are more accessible and sustainable, but their individual
capabilities often fall short in generating high-quality, diverse, and reliable
data. Inspired by collaborative human processes (e.g., peer review), we propose
a multiple small LLMs involved framework, GRA, that aggregates specialized
roles across small LLMs to iterative refinement and quality control typically
achieved by a single large LLM. In this collaborative framework, multiple small
LLMs assume distinct roles-Generator, Reviewer, and Adjudicator-to simulate a
peer-review-inspired data synthesis pipeline. The Generator proposes initial
data samples, the Reviewer critiques their quality and diversity, and the
Adjudicator resolves conflicts to finalize the output. By decomposing the
synthesis process into specialized sub-tasks, collaborative small LLMs can
achieve data-level parity with large LLM-based distillation. Through
experiments across multiple benchmarks, we demonstrate that GRA-produced data
matches or exceeds the quality of single large LLM outputs, e.g.,
Qwen-2.5-72B-Instruct. Our results challenge the necessity of monolithic large
models for high-quality data synthesis, advocating instead for strategic
coordination of smaller agents. Our datasets, models, and code are publicly
available at https://github.com/GX-XinGao/GRA.

</details>


### [12] [The Other Side of the Coin: Exploring Fairness in Retrieval-Augmented Generation](https://arxiv.org/abs/2504.12323)
*Zheng Zhang,Ning Li,Qi Liu,Rui Li,Weibo Gao,Qingyang Mao,Zhenya Huang,Baosheng Yu,Dacheng Tao*

Main category: cs.CL

TL;DR: RAG通过检索外部知识增强LLMs，减少错误内容，但可能影响公平性。研究发现小规模LLMs（<8B）的公平性问题更严重，提出FairFT和FairFilter两种方法改善公平性。


<details>
  <summary>Details</summary>
Motivation: 探讨RAG对LLMs公平性的影响，特别是在社会影响较大的领域。

Method: 通过实验分析不同规模LLMs、检索器和检索源的影响，提出FairFT（公平对齐检索器）和FairFilter（公平过滤机制）。

Result: 小规模LLMs（如LLaMA3.2-1B）在RAG中公平性问题更突出，提出的方法有效改善了公平性且不影响性能。

Conclusion: RAG对小规模LLMs的公平性有负面影响，但通过FairFT和FairFilter可以显著缓解这一问题。

Abstract: Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs) by
retrieving relevant document from external knowledge sources. By referencing
this external knowledge, RAG effectively reduces the generation of factually
incorrect content and addresses hallucination issues within LLMs. Recently,
there has been growing attention to improving the performance and efficiency of
RAG systems from various perspectives. While these advancements have yielded
significant results, the application of RAG in domains with considerable
societal implications raises a critical question about fairness: What impact
does the introduction of the RAG paradigm have on the fairness of LLMs? To
address this question, we conduct extensive experiments by varying the LLMs,
retrievers, and retrieval sources. Our experimental analysis reveals that the
scale of the LLMs plays a significant role in influencing fairness outcomes
within the RAG framework. When the model scale is smaller than 8B, the
integration of retrieval mechanisms often exacerbates unfairness in small-scale
LLMs (e.g., LLaMA3.2-1B, Mistral-7B, and LLaMA3-8B). To mitigate the fairness
issues introduced by RAG for small-scale LLMs, we propose two approaches,
FairFT and FairFilter. Specifically, in FairFT, we align the retriever with the
LLM in terms of fairness, enabling it to retrieve documents that facilitate
fairer model outputs. In FairFilter, we propose a fairness filtering mechanism
to filter out biased content after retrieval. Finally, we validate our proposed
approaches on real-world datasets, demonstrating their effectiveness in
improving fairness while maintaining performance.

</details>


### [13] [Cross-Document Cross-Lingual Natural Language Inference via RST-enhanced Graph Fusion and Interpretability Prediction](https://arxiv.org/abs/2504.12324)
*Mengying Yuan,Wangzi Xuan,Fei Li*

Main category: cs.CL

TL;DR: 论文提出了一种新的跨文档跨语言自然语言推理（CDCL-NLI）范式，构建了一个高质量数据集，并提出了一种基于RST增强图融合和可解释性预测的创新方法。


<details>
  <summary>Details</summary>
Motivation: 跨文档跨语言NLI（CDCL-NLI）是一个未被充分探索的领域，论文旨在扩展传统NLI能力至多文档、多语言场景。

Method: 采用RST增强的图融合和可解释性预测方法，结合RGAT进行跨文档上下文建模，以及基于词汇链的结构感知语义对齐机制。

Result: 实验表明，该方法显著优于传统NLI模型和大型语言模型（如Llama3和GPT-4o）。

Conclusion: 论文为NLI研究提供了新视角，并推动了跨文档跨语言上下文理解、语义检索和可解释性推理的研究。

Abstract: Natural Language Inference (NLI) is a fundamental task in both natural
language processing and information retrieval. While NLI has developed many
sub-directions such as sentence-level NLI, document-level NLI and cross-lingual
NLI, Cross-Document Cross-Lingual NLI (CDCL-NLI) remains largely unexplored. In
this paper, we propose a novel paradigm for CDCL-NLI that extends traditional
NLI capabilities to multi-document, multilingual scenarios. To support this
task, we construct a high-quality CDCL-NLI dataset including 1,110 instances
and spanning 26 languages. To build a baseline for this task, we also propose
an innovative method that integrates RST-enhanced graph fusion and
interpretability prediction. Our method employs RST (Rhetorical Structure
Theory) on RGAT (Relation-aware Graph Attention Network) for cross-document
context modeling, coupled with a structure-aware semantic alignment mechanism
based on lexical chains for cross-lingual understanding. For NLI
interpretability, we develop an EDU-level attribution framework that generates
extractive explanations. Extensive experiments demonstrate our approach's
superior performance, achieving significant improvements over both traditional
NLI models such as DocNLI and R2F, as well as LLMs like Llama3 and GPT-4o. Our
work sheds light on the study of NLI and will bring research interest on
cross-document cross-lingual context understanding, semantic retrieval and
interpretability inference. Our dataset and code are available at
\href{https://anonymous.4open.science/r/CDCL-NLI-637E/}{CDCL-NLI-Link for peer
review}.

</details>


### [14] [LLMTaxo: Leveraging Large Language Models for Constructing Taxonomy of Factual Claims from Social Media](https://arxiv.org/abs/2504.12325)
*Haiqi Zhang,Zhengyuan Zhu,Zeyu Zhang,Chengkai Li*

Main category: cs.CL

TL;DR: LLMTaxo是一个利用大语言模型自动构建社交媒体事实主张分类框架的系统，通过多粒度主题生成帮助用户更高效地导航社交媒体内容。


<details>
  <summary>Details</summary>
Motivation: 随着社交媒体内容的爆炸式增长，分析和理解在线讨论变得日益复杂，需要一种自动化工具来分类事实主张。

Method: LLMTaxo利用大语言模型从多粒度生成主题，构建事实主张分类框架，并在三个不同数据集上测试不同模型，同时设计了专门的分类评估指标。

Result: 实验结果表明，LLMTaxo能有效分类社交媒体事实主张，且某些模型在特定数据集上表现更优。

Conclusion: LLMTaxo为社交媒体事实主张分类提供了一种有效工具，同时揭示了模型性能与数据集的关联性。

Abstract: With the vast expansion of content on social media platforms, analyzing and
comprehending online discourse has become increasingly complex. This paper
introduces LLMTaxo, a novel framework leveraging large language models for the
automated construction of taxonomy of factual claims from social media by
generating topics from multi-level granularities. This approach aids
stakeholders in more effectively navigating the social media landscapes. We
implement this framework with different models across three distinct datasets
and introduce specially designed taxonomy evaluation metrics for a
comprehensive assessment. With the evaluations from both human evaluators and
GPT-4, the results indicate that LLMTaxo effectively categorizes factual claims
from social media, and reveals that certain models perform better on specific
datasets.

</details>


### [15] [Reconstructing Sepsis Trajectories from Clinical Case Reports using LLMs: the Textual Time Series Corpus for Sepsis](https://arxiv.org/abs/2504.12326)
*Shahriar Noroozizadeh,Jeremy C. Weiss*

Main category: cs.CL

TL;DR: 该论文提出了一种利用大语言模型（LLMs）从临床病例报告和出院摘要中提取时间定位的临床发现的流程，并生成了一个关于Sepsis-3的开放访问文本时间序列语料库。验证结果显示高恢复率和强时间顺序性。


<details>
  <summary>Details</summary>
Motivation: 临床病例报告和出院摘要虽然完整准确，但通常在患者接触后完成，而结构化数据流虽更早可用但不完整。为了在更完整和细粒度的时间数据上训练模型，需要一种方法提取时间定位的临床发现。

Method: 构建了一个流程，利用大语言模型对病例报告中的时间定位发现进行表型提取和标注，并应用于生成Sepsis-3的开放访问语料库。通过比较PMOA和I2B2/MIMIC-IV的标注结果验证系统。

Result: 结果显示高恢复率（事件匹配率：O1-preview--0.755，Llama 3.3 70B Instruct--0.753）和强时间顺序性（一致性：O1-preview--0.932，Llama 3.3 70B Instruct--0.932）。

Conclusion: 研究展示了LLMs在时间定位临床发现中的能力，但也指出了其在时间重建中的局限性，并提出了通过多模态集成改进的潜在途径。

Abstract: Clinical case reports and discharge summaries may be the most complete and
accurate summarization of patient encounters, yet they are finalized, i.e.,
timestamped after the encounter. Complementary data structured streams become
available sooner but suffer from incompleteness. To train models and algorithms
on more complete and temporally fine-grained data, we construct a pipeline to
phenotype, extract, and annotate time-localized findings within case reports
using large language models. We apply our pipeline to generate an open-access
textual time series corpus for Sepsis-3 comprising 2,139 case reports from the
Pubmed-Open Access (PMOA) Subset. To validate our system, we apply it on PMOA
and timeline annotations from I2B2/MIMIC-IV and compare the results to
physician-expert annotations. We show high recovery rates of clinical findings
(event match rates: O1-preview--0.755, Llama 3.3 70B Instruct--0.753) and
strong temporal ordering (concordance: O1-preview--0.932, Llama 3.3 70B
Instruct--0.932). Our work characterizes the ability of LLMs to time-localize
clinical findings in text, illustrating the limitations of LLM use for temporal
reconstruction and providing several potential avenues of improvement via
multimodal integration.

</details>


### [16] [Word Embeddings Track Social Group Changes Across 70 Years in China](https://arxiv.org/abs/2504.12327)
*Yuxi Ma,Yongqian Peng,Yixin Zhu*

Main category: cs.CL

TL;DR: 该研究首次通过大规模计算分析中国官方媒体（1950-2019）的语言模式，揭示了社会群体表征的演变，发现其与西方模式显著不同，尤其是经济地位、种族和性别方面。


<details>
  <summary>Details</summary>
Motivation: 探讨社会革命性变革如何通过官方语言表征反映，并填补非西方语境下语言模式研究的空白。

Method: 使用历时词嵌入方法，在不同时间分辨率下分析中国官方媒体的语言数据。

Result: 中国的社会群体表征与西方显著不同，经济阶层和性别的表征随历史变革剧烈变化，而种族、年龄和体型等刻板印象则保持稳定。

Conclusion: 研究揭示了官方话语如何通过语言编码社会结构，并强调了非西方视角在计算社会科学中的重要性。

Abstract: Language encodes societal beliefs about social groups through word patterns.
While computational methods like word embeddings enable quantitative analysis
of these patterns, studies have primarily examined gradual shifts in Western
contexts. We present the first large-scale computational analysis of Chinese
state-controlled media (1950-2019) to examine how revolutionary social
transformations are reflected in official linguistic representations of social
groups. Using diachronic word embeddings at multiple temporal resolutions, we
find that Chinese representations differ significantly from Western
counterparts, particularly regarding economic status, ethnicity, and gender.
These representations show distinct evolutionary dynamics: while stereotypes of
ethnicity, age, and body type remain remarkably stable across political
upheavals, representations of gender and economic classes undergo dramatic
shifts tracking historical transformations. This work advances our
understanding of how officially sanctioned discourse encodes social structure
through language while highlighting the importance of non-Western perspectives
in computational social science.

</details>


### [17] [A Comprehensive Survey of Reward Models: Taxonomy, Applications, Challenges, and Future](https://arxiv.org/abs/2504.12328)
*Jialun Zhong,Wei Shen,Yanzeng Li,Songyang Gao,Hua Lu,Yicheng Chen,Yang Zhang,Wei Zhou,Jinjie Gu,Lei Zou*

Main category: cs.CL

TL;DR: 本文综述了奖励模型（RM）在增强大型语言模型（LLM）中的应用，涵盖偏好收集、奖励建模和使用方法，并探讨了其挑战与未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 奖励模型（RM）作为人类偏好的代理，能够指导LLM的行为，但其研究尚需系统梳理和总结。

Method: 通过综述相关研究，从偏好收集、奖励建模和使用三个角度分析RM，并讨论其应用、评估基准及挑战。

Result: 提供了RM的全面介绍，分析了当前领域的挑战，并指出了潜在的研究方向。

Conclusion: 本文为初学者提供了RM的全面指南，并促进了未来研究的发展，相关资源已公开。

Abstract: Reward Model (RM) has demonstrated impressive potential for enhancing Large
Language Models (LLM), as RM can serve as a proxy for human preferences,
providing signals to guide LLMs' behavior in various tasks. In this paper, we
provide a comprehensive overview of relevant research, exploring RMs from the
perspectives of preference collection, reward modeling, and usage. Next, we
introduce the applications of RMs and discuss the benchmarks for evaluation.
Furthermore, we conduct an in-depth analysis of the challenges existing in the
field and dive into the potential research directions. This paper is dedicated
to providing beginners with a comprehensive introduction to RMs and
facilitating future studies. The resources are publicly available at
github\footnote{https://github.com/JLZhong23/awesome-reward-models}.

</details>


### [18] [Speculative Thinking: Enhancing Small-Model Reasoning with Large Model Guidance at Inference Time](https://arxiv.org/abs/2504.12329)
*Wang Yang,Xiang Yue,Vipin Chaudhary,Xiaotian Han*

Main category: cs.CL

TL;DR: 提出了一种无需训练的框架Speculative Thinking，通过让大型推理模型在推理阶段指导小型模型，显著提升推理准确率并缩短输出长度。


<details>
  <summary>Details</summary>
Motivation: 现有后训练方法成本高且输出冗长，需要一种更高效的解决方案。

Method: 基于两个观察：1）特定支持性标记（如“wait”）常出现在结构分隔符后；2）大模型对反射行为控制更强。通过让大模型指导小模型，优化推理过程。

Result: 1.5B模型在MATH500上的准确率从83.2%提升至89.4%，输出长度减少15.7%。非推理模型Qwen-2.5-7B-Instruct的准确率也从74.0%提升至81.8%。

Conclusion: Speculative Thinking框架显著提升了推理模型的性能，同时减少了输出长度，适用于多种模型。

Abstract: Recent advances leverage post-training to enhance model reasoning
performance, which typically requires costly training pipelines and still
suffers from inefficient, overly lengthy outputs. We introduce Speculative
Thinking, a training-free framework that enables large reasoning models to
guide smaller ones during inference at the reasoning level, distinct from
speculative decoding, which operates at the token level. Our approach is based
on two observations: (1) reasoning-supportive tokens such as "wait" frequently
appear after structural delimiters like "\n\n", serving as signals for
reflection or continuation; and (2) larger models exhibit stronger control over
reflective behavior, reducing unnecessary backtracking while improving
reasoning quality. By strategically delegating reflective steps to a more
capable model, our method significantly boosts the reasoning accuracy of
reasoning models while shortening their output. With the assistance of the 32B
reasoning model, the 1.5B model's accuracy on MATH500 increases from 83.2% to
89.4%, marking a substantial improvement of 6.2%. Simultaneously, the average
output length is reduced from 5439 tokens to 4583 tokens, representing a 15.7%
decrease. Moreover, when applied to a non-reasoning model
(Qwen-2.5-7B-Instruct), our framework boosts its accuracy from 74.0% to 81.8%
on the same benchmark, achieving a relative improvement of 7.8%.

</details>


### [19] [HM-RAG: Hierarchical Multi-Agent Multimodal Retrieval Augmented Generation](https://arxiv.org/abs/2504.12330)
*Pei Liu,Xin Liu,Ruoyu Yao,Junming Liu,Siyuan Meng,Ding Wang,Jun Ma*

Main category: cs.CL

TL;DR: HM-RAG是一种新型分层多代理多模态RAG框架，通过协作智能动态合成异构数据知识，显著提升了查询准确性和分类性能。


<details>
  <summary>Details</summary>
Motivation: 传统单代理RAG在处理需要跨异构数据协调推理的复杂查询时存在局限性，HM-RAG旨在解决这一问题。

Method: 采用三层架构：分解代理将复杂查询拆分为子任务，多源检索代理并行检索不同模态数据，决策代理通过一致性投票整合结果。

Result: 在ScienceQA和CrisisMMD基准测试中，HM-RAG的答案准确性提升12.95%，问题分类准确性提升3.56%，并在零样本设置中达到最优结果。

Conclusion: HM-RAG通过模块化架构和多代理协作，显著提升了多模态推理和知识合成的能力，同时支持新数据模态的无缝集成。

Abstract: While Retrieval-Augmented Generation (RAG) augments Large Language Models
(LLMs) with external knowledge, conventional single-agent RAG remains
fundamentally limited in resolving complex queries demanding coordinated
reasoning across heterogeneous data ecosystems. We present HM-RAG, a novel
Hierarchical Multi-agent Multimodal RAG framework that pioneers collaborative
intelligence for dynamic knowledge synthesis across structured, unstructured,
and graph-based data. The framework is composed of three-tiered architecture
with specialized agents: a Decomposition Agent that dissects complex queries
into contextually coherent sub-tasks via semantic-aware query rewriting and
schema-guided context augmentation; Multi-source Retrieval Agents that carry
out parallel, modality-specific retrieval using plug-and-play modules designed
for vector, graph, and web-based databases; and a Decision Agent that uses
consistency voting to integrate multi-source answers and resolve discrepancies
in retrieval results through Expert Model Refinement. This architecture attains
comprehensive query understanding by combining textual, graph-relational, and
web-derived evidence, resulting in a remarkable 12.95% improvement in answer
accuracy and a 3.56% boost in question classification accuracy over baseline
RAG systems on the ScienceQA and CrisisMMD benchmarks. Notably, HM-RAG
establishes state-of-the-art results in zero-shot settings on both datasets.
Its modular architecture ensures seamless integration of new data modalities
while maintaining strict data governance, marking a significant advancement in
addressing the critical challenges of multimodal reasoning and knowledge
synthesis in RAG systems. Code is available at
https://github.com/ocean-luna/HMRAG.

</details>


### [20] [Span-level Emotion-Cause-Category Triplet Extraction with Instruction Tuning LLMs and Data Augmentation](https://arxiv.org/abs/2504.12331)
*Xiangju Li,Dong Yang,Xiaogang Zhu,Faliang Huang,Peng Zhang,Zhongying Zhao*

Main category: cs.CL

TL;DR: 该论文提出了一种基于大语言模型的细粒度方法，用于提取文本中的情感-原因-类别三元组，通过指令调优和数据增强技术显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在情感原因分析中存在冗余信息检索和情感类别确定不准确的挑战，尤其是对隐式或模糊情感的表达。

Method: 采用任务特定的三元组提取指令，结合低秩适应技术微调大语言模型，并开发基于提示的数据增强策略以解决数据稀缺问题。

Result: 实验表明，该方法在情感-原因-类别三元组提取指标上至少提升了12.8%，表现出高效性和鲁棒性。

Conclusion: 该方法为情感原因分析研究提供了新方向，代码已开源。

Abstract: Span-level emotion-cause-category triplet extraction represents a novel and
complex challenge within emotion cause analysis. This task involves identifying
emotion spans, cause spans, and their associated emotion categories within the
text to form structured triplets. While prior research has predominantly
concentrated on clause-level emotion-cause pair extraction and span-level
emotion-cause detection, these methods often confront challenges originating
from redundant information retrieval and difficulty in accurately determining
emotion categories, particularly when emotions are expressed implicitly or
ambiguously. To overcome these challenges, this study explores a fine-grained
approach to span-level emotion-cause-category triplet extraction and introduces
an innovative framework that leverages instruction tuning and data augmentation
techniques based on large language models. The proposed method employs
task-specific triplet extraction instructions and utilizes low-rank adaptation
to fine-tune large language models, eliminating the necessity for intricate
task-specific architectures. Furthermore, a prompt-based data augmentation
strategy is developed to address data scarcity by guiding large language models
in generating high-quality synthetic training data. Extensive experimental
evaluations demonstrate that the proposed approach significantly outperforms
existing baseline methods, achieving at least a 12.8% improvement in span-level
emotion-cause-category triplet extraction metrics. The results demonstrate the
method's effectiveness and robustness, offering a promising avenue for
advancing research in emotion cause analysis. The source code is available at
https://github.com/zxgnlp/InstruDa-LLM.

</details>


### [21] [Can the capability of Large Language Models be described by human ability? A Meta Study](https://arxiv.org/abs/2504.12332)
*Mingrui Zan,Yunquan Zhang,Boyang Zhang,Fangming Liu,Daning Cheng*

Main category: cs.CL

TL;DR: 论文通过分析80多个LLM在37个评估基准上的表现，发现LLM的某些能力可用人类能力指标描述，但某些能力在LLM中不相关，且能力随参数规模变化显著。


<details>
  <summary>Details</summary>
Motivation: 探讨LLM的能力是否真正接近人类能力，并量化其表现。

Method: 收集80多个LLM在37个基准上的数据，按人类能力的6个主要类别和11个子类别分类，并进行聚类分析。

Result: 1. 少于100亿参数的LLM的某些能力可用人类能力指标描述；2. 人类中相关的能力在LLM中几乎不相关；3. LLM能力随参数规模显著变化。

Conclusion: LLM的能力与人类能力部分重叠，但存在显著差异，且受参数规模影响。

Abstract: Users of Large Language Models (LLMs) often perceive these models as
intelligent entities with human-like capabilities. However, the extent to which
LLMs' capabilities truly approximate human abilities remains a topic of debate.
In this paper, to characterize the capabilities of LLMs in relation to human
capabilities, we collected performance data from over 80 models across 37
evaluation benchmarks. The evaluation benchmarks are categorized into 6 primary
abilities and 11 sub-abilities in human aspect. Then, we then clustered the
performance rankings into several categories and compared these clustering
results with classifications based on human ability aspects. Our findings lead
to the following conclusions: 1. We have confirmed that certain capabilities of
LLMs with fewer than 10 billion parameters can indeed be described using human
ability metrics; 2. While some abilities are considered interrelated in humans,
they appear nearly uncorrelated in LLMs; 3. The capabilities possessed by LLMs
vary significantly with the parameter scale of the model.

</details>


### [22] [Meta-Evaluating Local LLMs: Rethinking Performance Metrics for Serious Games](https://arxiv.org/abs/2504.12333)
*Andrés Isaza-Giraldo,Paulo Bala,Lucas Pereira*

Main category: cs.CL

TL;DR: 研究探讨了五种小型LLM在评估游戏《En-join》中玩家回答时的可靠性，发现模型在敏感性和特异性之间存在权衡，需根据上下文选择合适模型。


<details>
  <summary>Details</summary>
Motivation: 评估开放式回答在严肃游戏中的主观性挑战，以及小型LLM作为评估工具的准确性和一致性尚不明确。

Method: 利用传统二元分类指标（如准确率、真阳性率、真阴性率）系统比较五种小型LLM在不同评估场景中的表现。

Result: 结果显示不同模型在识别正确回答和避免误判方面表现不一，需权衡敏感性和特异性。

Conclusion: 研究强调需开发上下文感知的评估框架，并谨慎选择模型，为AI驱动评估工具的可信度提供了见解。

Abstract: The evaluation of open-ended responses in serious games presents a unique
challenge, as correctness is often subjective. Large Language Models (LLMs) are
increasingly being explored as evaluators in such contexts, yet their accuracy
and consistency remain uncertain, particularly for smaller models intended for
local execution. This study investigates the reliability of five small-scale
LLMs when assessing player responses in \textit{En-join}, a game that simulates
decision-making within energy communities. By leveraging traditional binary
classification metrics (including accuracy, true positive rate, and true
negative rate), we systematically compare these models across different
evaluation scenarios. Our results highlight the strengths and limitations of
each model, revealing trade-offs between sensitivity, specificity, and overall
performance. We demonstrate that while some models excel at identifying correct
responses, others struggle with false positives or inconsistent evaluations.
The findings highlight the need for context-aware evaluation frameworks and
careful model selection when deploying LLMs as evaluators. This work
contributes to the broader discourse on the trustworthiness of AI-driven
assessment tools, offering insights into how different LLM architectures handle
subjective evaluation tasks.

</details>


### [23] [QM-ToT: A Medical Tree of Thoughts Reasoning Framework for Quantized Model](https://arxiv.org/abs/2504.12334)
*Zongxian Yang,Jiayu Qian,Zhi-An Huang,Kay Chen Tan*

Main category: cs.CL

TL;DR: QM-ToT框架通过树状思维分解医学问题，显著提升量化模型在MedQAUSMLE数据集上的性能，并提出了高效的数据蒸馏方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在复杂医学任务中表现不佳，尤其是量化后性能下降，需要解决这一问题以适用于资源受限的医疗环境。

Method: 提出Quantized Medical Tree of Thought (QM-ToT)框架，结合树状思维分解问题和评估层，同时提出基于ToT的数据蒸馏方法。

Result: LLaMA2-70b模型准确率从34%提升至50%，LLaMA-3.1-8b从58.77%提升至69.49%；数据蒸馏方法仅用3.9%数据实现86.27%的改进。

Conclusion: QM-ToT首次展示了ToT在复杂生物医学任务中的潜力，为资源受限环境中部署高性能量化LLM奠定了基础。

Abstract: Large language models (LLMs) face significant challenges in specialized
biomedical tasks due to the inherent complexity of medical reasoning and the
sensitive nature of clinical data. Existing LLMs often struggle with intricate
medical terminology and the need for accurate clinical insights, leading to
performance reduction when quantized for resource-constrained deployment. To
address these issues, we propose Quantized Medical Tree of Thought (QM-ToT), a
path-based reasoning framework. QM-ToT leverages a Tree of Thought (ToT)
reasoning approach to decompose complex medical problems into manageable
subtasks, coupled with evaluator assessment layers. This framework facilitates
substantial performance improvements in INT4-quantized models on the
challenging MedQAUSMLE dataset. Specifically, we demonstrate a remarkable
accuracy increase from 34% to 50% for the LLaMA2-70b model and from 58.77% to
69.49% for LLaMA-3.1-8b. Besides, we also proposed an effect data distillation
method based on ToT. Compared to the traditional distillation method, we
achieved an improvement of 86. 27% while using only 3.9% of the data.This work,
for the first time, showcases the potential of ToT to significantly enhance
performance on complex biomedical tasks, establishing a crucial foundation for
future advances in deploying high-performing quantized LLM in resource-limited
medical settings.

</details>


### [24] [You've Changed: Detecting Modification of Black-Box Large Language Models](https://arxiv.org/abs/2504.12335)
*Alden Dima,James Foulds,Shimei Pan,Philip Feldman*

Main category: cs.CL

TL;DR: 提出了一种通过比较生成文本的语言和心理语言学特征分布来监测大型语言模型（LLM）变化的方法。


<details>
  <summary>Details</summary>
Motivation: 由于LLM通常通过API提供服务，开发者难以检测其行为变化，因此需要一种轻量级的方法来监控模型变化。

Method: 使用统计测试比较两个文本样本的特征分布，以判断LLM是否发生变化。

Result: 实验表明，简单的文本特征结合统计测试可以有效区分不同语言模型，并能用于检测提示注入攻击。

Conclusion: 该方法为频繁监控LLM变化提供了一种高效且计算成本低的解决方案。

Abstract: Large Language Models (LLMs) are often provided as a service via an API,
making it challenging for developers to detect changes in their behavior. We
present an approach to monitor LLMs for changes by comparing the distributions
of linguistic and psycholinguistic features of generated text. Our method uses
a statistical test to determine whether the distributions of features from two
samples of text are equivalent, allowing developers to identify when an LLM has
changed. We demonstrate the effectiveness of our approach using five OpenAI
completion models and Meta's Llama 3 70B chat model. Our results show that
simple text features coupled with a statistical test can distinguish between
language models. We also explore the use of our approach to detect prompt
injection attacks. Our work enables frequent LLM change monitoring and avoids
computationally expensive benchmark evaluations.

</details>


### [25] ["It Listens Better Than My Therapist": Exploring Social Media Discourse on LLMs as Mental Health Tool](https://arxiv.org/abs/2504.12337)
*Anna-Carolina Haensch*

Main category: cs.CL

TL;DR: 研究分析了用户如何将大型语言模型（LLMs）作为心理健康工具使用，发现20%的评论反映个人使用，用户态度积极，但也存在隐私和专业监督不足的担忧。


<details>
  <summary>Details</summary>
Motivation: 探讨生成式AI聊天机器人（如ChatGPT）在非正式心理健康支持中的角色及其新兴能力。

Method: 通过分析10,000多条TikTok评论，使用分层编码方案和监督分类模型识别用户体验和态度。

Result: 20%的评论反映个人使用，用户态度积极，主要受益包括可访问性和情感支持，但也存在隐私和缺乏专业监督的问题。

Conclusion: AI在心理健康支持中的使用日益重要，但亟需临床和伦理审查。

Abstract: The emergence of generative AI chatbots such as ChatGPT has prompted growing
public and academic interest in their role as informal mental health support
tools. While early rule-based systems have been around for several years, large
language models (LLMs) offer new capabilities in conversational fluency,
empathy simulation, and availability. This study explores how users engage with
LLMs as mental health tools by analyzing over 10,000 TikTok comments from
videos referencing LLMs as mental health tools. Using a self-developed tiered
coding schema and supervised classification models, we identify user
experiences, attitudes, and recurring themes. Results show that nearly 20% of
comments reflect personal use, with these users expressing overwhelmingly
positive attitudes. Commonly cited benefits include accessibility, emotional
support, and perceived therapeutic value. However, concerns around privacy,
generic responses, and the lack of professional oversight remain prominent. It
is important to note that the user feedback does not indicate which therapeutic
framework, if any, the LLM-generated output aligns with. While the findings
underscore the growing relevance of AI in everyday practices, they also
highlight the urgent need for clinical and ethical scrutiny in the use of AI
for mental health support.

</details>


### [26] [Paging Dr. GPT: Extracting Information from Clinical Notes to Enhance Patient Predictions](https://arxiv.org/abs/2504.12338)
*David Anderson,Michaela Anderson,Margret Bjarnadottir,Stephen Mahar,Shriyan Reyya*

Main category: cs.CL

TL;DR: 研究探讨了如何利用GPT-4o-mini生成的临床问题答案，结合患者出院摘要，提升患者死亡率预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 传统预测模型未能充分利用非结构化临床笔记中的信息，而GPT等大型语言模型（LLMs）可能填补这一空白。

Method: 使用MIMIC-IV Note数据集中的14,011例首次入院患者数据，将GPT回答作为逻辑回归模型的输入特征。

Result: GPT模型单独表现优于传统表格数据模型，结合两者后预测能力进一步提升（AUC平均提高5.1%，高风险组阳性预测值提高29.9%）。

Conclusion: LLMs在临床预测任务中具有显著价值，尤其在非结构化文本数据未被充分利用的领域。

Abstract: There is a long history of building predictive models in healthcare using
tabular data from electronic medical records. However, these models fail to
extract the information found in unstructured clinical notes, which document
diagnosis, treatment, progress, medications, and care plans. In this study, we
investigate how answers generated by GPT-4o-mini (ChatGPT) to simple clinical
questions about patients, when given access to the patient's discharge summary,
can support patient-level mortality prediction. Using data from 14,011
first-time admissions to the Coronary Care or Cardiovascular Intensive Care
Units in the MIMIC-IV Note dataset, we implement a transparent framework that
uses GPT responses as input features in logistic regression models. Our
findings demonstrate that GPT-based models alone can outperform models trained
on standard tabular data, and that combining both sources of information yields
even greater predictive power, increasing AUC by an average of 5.1 percentage
points and increasing positive predictive value by 29.9 percent for the
highest-risk decile. These results highlight the value of integrating large
language models (LLMs) into clinical prediction tasks and underscore the
broader potential for using LLMs in any domain where unstructured text data
remains an underutilized resource.

</details>


### [27] [GOAT-TTS: LLM-based Text-To-Speech Generation Optimized via A Dual-Branch Architecture](https://arxiv.org/abs/2504.12339)
*Yaodong Song,Hongjie Chen,Jie Lian,Yuxin Zhang,Guangmin Xia,Zehan Li,Genliang Zhao,Jian Kang,Yongxiang Li,Jie Li*

Main category: cs.CL

TL;DR: GOAT-TTS是一种基于LLM的双分支架构TTS方法，解决了当前TTS模型在声学特征丢失、依赖对齐数据及遗忘文本理解的问题。


<details>
  <summary>Details</summary>
Motivation: 当前TTS模型存在声学特征丢失、依赖对齐数据及优化过程中遗忘文本理解的问题，亟需改进。

Method: 提出双分支架构：1）模态对齐分支捕捉连续声学嵌入；2）语音生成分支通过模块化微调预测语音标记。

Result: GOAT-TTS性能与最先进TTS模型相当，并验证了合成方言语音数据的有效性。

Conclusion: GOAT-TTS通过双分支设计有效解决了现有TTS模型的三大挑战，具有实际应用潜力。

Abstract: While large language models (LLMs) have revolutionized text-to-speech (TTS)
synthesis through discrete tokenization paradigms, current architectures
exhibit fundamental tensions between three critical dimensions: 1) irreversible
loss of acoustic characteristics caused by quantization of speech prompts; 2)
stringent dependence on precisely aligned prompt speech-text pairs that limit
real-world deployment; and 3) catastrophic forgetting of the LLM's native text
comprehension during optimization for speech token generation. To address these
challenges, we propose an LLM-based text-to-speech Generation approach
Optimized via a novel dual-branch ArchiTecture (GOAT-TTS). Our framework
introduces two key innovations: (1) The modality-alignment branch combines a
speech encoder and projector to capture continuous acoustic embeddings,
enabling bidirectional correlation between paralinguistic features (language,
timbre, emotion) and semantic text representations without transcript
dependency; (2) The speech-generation branch employs modular fine-tuning on
top-k layers of an LLM for speech token prediction while freezing the bottom-k
layers to preserve foundational linguistic knowledge. Moreover, multi-token
prediction is introduced to support real-time streaming TTS synthesis.
Experimental results demonstrate that our GOAT-TTS achieves performance
comparable to state-of-the-art TTS models while validating the efficacy of
synthesized dialect speech data.

</details>


### [28] [Streamlining Biomedical Research with Specialized LLMs](https://arxiv.org/abs/2504.12341)
*Linqing Chen,Weilei Wang,Yubin Xia,Wentao Wu,Peng Xu,Zilong Bai,Jie Fang,Chaobo Xu,Ran Hu,Licong Xu,Haoran Hua,Jing Sun,Hanmeng Zhong,Jin Liu,Tian Qiu,Haowen Liu,Meng Hu,Xiuwen Li,Fei Gao,Yong Gu,Tao Shi,Chaochao Wang,Jianping Lu,Cheng Sun,Yixin Wang,Shengjie Yang,Yuancheng Li,Lu Jin,Lisha Zhang,Fu Bian,Zhongkai Ye,Lidong Pei,Changyang Tu*

Main category: cs.CL

TL;DR: 提出了一种集成领域特定大语言模型与高级信息检索技术的新系统，提供全面且上下文感知的响应。


<details>
  <summary>Details</summary>
Motivation: 提升对话生成质量，实现高效人机交互，支持生物医药领域的研发决策。

Method: 结合大语言模型与信息检索技术，实现多组件无缝交互与输出交叉验证。

Result: 显著提高响应精度，支持多模态数据访问，提升研究效率。

Conclusion: 系统为生物医药领域提供实时、高保真交互平台，加速研发决策。

Abstract: In this paper, we propose a novel system that integrates state-of-the-art,
domain-specific large language models with advanced information retrieval
techniques to deliver comprehensive and context-aware responses. Our approach
facilitates seamless interaction among diverse components, enabling
cross-validation of outputs to produce accurate, high-quality responses
enriched with relevant data, images, tables, and other modalities. We
demonstrate the system's capability to enhance response precision by leveraging
a robust question-answering model, significantly improving the quality of
dialogue generation. The system provides an accessible platform for real-time,
high-fidelity interactions, allowing users to benefit from efficient
human-computer interaction, precise retrieval, and simultaneous access to a
wide range of literature and data. This dramatically improves the research
efficiency of professionals in the biomedical and pharmaceutical domains and
facilitates faster, more informed decision-making throughout the R\&D process.
Furthermore, the system proposed in this paper is available at
https://synapse-chat.patsnap.com.

</details>


### [29] [Benchmarking Biopharmaceuticals Retrieval-Augmented Generation Evaluation](https://arxiv.org/abs/2504.12342)
*Hanmeng Zhong,Linqing Chen,Weilei Wang,Wentao Wu*

Main category: cs.CL

TL;DR: 本文介绍了首个针对生物制药领域的检索增强大语言模型（LLM）评估基准BRAGE，并提出了一种基于引用的分类方法以评估LLM的查询与参考理解能力（QRUC）。实验结果显示主流LLM在生物制药领域的QRUC存在显著差距，需改进。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏专门针对生物制药领域的LLM评估基准，且传统QA指标在开放检索增强QA场景中表现不足。

Method: 提出BRAGE基准，支持多语言评估，并设计基于引用的分类方法评估QRUC。

Result: 主流LLM在生物制药领域的QRUC表现不佳，存在显著差距。

Conclusion: BRAGE为生物制药领域的LLM评估提供了首个基准，并揭示了主流LLM在QRUC上的不足，需进一步改进。

Abstract: Recently, the application of the retrieval-augmented Large Language Models
(LLMs) in specific domains has gained significant attention, especially in
biopharmaceuticals. However, in this context, there is no benchmark
specifically designed for biopharmaceuticals to evaluate LLMs. In this paper,
we introduce the Biopharmaceuticals Retrieval-Augmented Generation Evaluation
(BRAGE) , the first benchmark tailored for evaluating LLMs' Query and Reference
Understanding Capability (QRUC) in the biopharmaceutical domain, available in
English, French, German and Chinese. In addition, Traditional
Question-Answering (QA) metrics like accuracy and exact match fall short in the
open-ended retrieval-augmented QA scenarios. To address this, we propose a
citation-based classification method to evaluate the QRUC of LLMs to understand
the relationship between queries and references. We apply this method to
evaluate the mainstream LLMs on BRAGE. Experimental results show that there is
a significant gap in the biopharmaceutical QRUC of mainstream LLMs, and their
QRUC needs to be improved.

</details>


### [30] [Propaganda via AI? A Study on Semantic Backdoors in Large Language Models](https://arxiv.org/abs/2504.12344)
*Nay Myat Min,Long H. Pham,Yige Li,Jun Sun*

Main category: cs.CL

TL;DR: 论文提出了一种针对大型语言模型（LLMs）中语义后门攻击的检测框架RAVEN，通过语义熵和跨模型一致性分析，成功发现了此前未被检测到的语义后门。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在语言任务中表现优异，但仍易受后门攻击，尤其是基于语义的隐蔽触发机制。传统防御方法无法检测此类攻击，因此需要新的解决方案。

Method: 作者在受控微调环境中植入语义后门，并提出了RAVEN框架，结合语义熵和跨模型一致性分析，通过结构化主题-视角提示探测模型响应并聚类分析。

Result: 实验证明RAVEN能有效检测多种LLM（如GPT-4o、Llama等）中的语义后门，揭示了这些隐藏漏洞。

Conclusion: 研究强调了概念级审计的必要性，并开源了代码和数据以促进后续研究。

Abstract: Large language models (LLMs) demonstrate remarkable performance across myriad
language tasks, yet they remain vulnerable to backdoor attacks, where
adversaries implant hidden triggers that systematically manipulate model
outputs. Traditional defenses focus on explicit token-level anomalies and
therefore overlook semantic backdoors-covert triggers embedded at the
conceptual level (e.g., ideological stances or cultural references) that rely
on meaning-based cues rather than lexical oddities. We first show, in a
controlled finetuning setting, that such semantic backdoors can be implanted
with only a small poisoned corpus, establishing their practical feasibility. We
then formalize the notion of semantic backdoors in LLMs and introduce a
black-box detection framework, RAVEN (short for "Response Anomaly Vigilance for
uncovering semantic backdoors"), which combines semantic entropy with
cross-model consistency analysis. The framework probes multiple models with
structured topic-perspective prompts, clusters the sampled responses via
bidirectional entailment, and flags anomalously uniform outputs; cross-model
comparison isolates model-specific anomalies from corpus-wide biases. Empirical
evaluations across diverse LLM families (GPT-4o, Llama, DeepSeek, Mistral)
uncover previously undetected semantic backdoors, providing the first
proof-of-concept evidence of these hidden vulnerabilities and underscoring the
urgent need for concept-level auditing of deployed language models. We
open-source our code and data at https://github.com/NayMyatMin/RAVEN.

</details>


### [31] [Reimagining Urban Science: Scaling Causal Inference with Large Language Models](https://arxiv.org/abs/2504.12345)
*Yutong Xia,Ao Qu,Yunhan Zheng,Yihong Tang,Dingyi Zhuang,Yuxuan Liang,Cathy Wu,Roger Zimmermann,Jinhua Zhao*

Main category: cs.CL

TL;DR: 论文提出了一种基于大语言模型（LLM）的框架AutoUrbanCI，用于改进城市因果研究的效率与透明度，并探讨了人机协作的潜力。


<details>
  <summary>Details</summary>
Motivation: 城市因果研究面临假设生成效率低、数据复杂性和方法脆弱性等挑战，LLM的进展为解决这些问题提供了新思路。

Method: 通过分析现有研究的分类体系，提出由四个模块化代理组成的AutoUrbanCI框架，涵盖假设生成、数据工程、实验设计与执行、结果解释与政策建议。

Result: 提出了评估标准，强调严谨性与透明度，并探讨了人机协作、公平性和责任性的影响。

Conclusion: 呼吁采用AI增强的工作流程，以扩大参与、提高可重复性，并推动更包容的城市因果推理。

Abstract: Urban causal research is essential for understanding the complex dynamics of
cities and informing evidence-based policies. However, it is challenged by the
inefficiency and bias of hypothesis generation, barriers to multimodal data
complexity, and the methodological fragility of causal experimentation. Recent
advances in large language models (LLMs) present an opportunity to rethink how
urban causal analysis is conducted. This Perspective examines current urban
causal research by analyzing taxonomies that categorize research topics, data
sources, and methodological approaches to identify structural gaps. We then
introduce an LLM-driven conceptual framework, AutoUrbanCI, composed of four
distinct modular agents responsible for hypothesis generation, data
engineering, experiment design and execution, and results interpretation with
policy recommendations. We propose evaluation criteria for rigor and
transparency and reflect on implications for human-AI collaboration, equity,
and accountability. We call for a new research agenda that embraces
AI-augmented workflows not as replacements for human expertise but as tools to
broaden participation, improve reproducibility, and unlock more inclusive forms
of urban causal reasoning.

</details>


### [32] [Mathematical Capabilities of Large Language Models in Finnish Matriculation Examination](https://arxiv.org/abs/2504.12347)
*Mika Setälä,Pieta Sikström,Ville Heilala,Tommi Kärkkäinen*

Main category: cs.CL

TL;DR: 研究评估了大型语言模型（LLMs）在芬兰高中毕业考试中的数学能力，发现其表现从中等提升至接近满分，展示了LLMs在教育评估中的潜力。


<details>
  <summary>Details</summary>
Motivation: 评估LLMs在数学推理方面的能力，尤其是在高风险的芬兰高中毕业考试中的表现。

Method: 使用芬兰高中毕业考试作为测试平台，对不同版本的LLMs进行数学能力评估。

Result: LLMs的表现从初始的中等成绩显著提升至接近或达到满分，媲美顶尖学生水平。

Conclusion: LLMs的数学能力迅速进步，展示了其在大规模教育评估中的潜在应用价值。

Abstract: Large language models (LLMs) have shown increasing promise in educational
settings, yet their mathematical reasoning has been considered evolving. This
study evaluates the mathematical capabilities of various LLMs using the Finnish
matriculation examination, a high-stakes digital test for upper secondary
education. Initial tests yielded moderate performance corresponding to
mid-range grades, but later evaluations demonstrated substantial improvements
as the language models evolved. Remarkably, some models achieved near-perfect
or perfect scores, matching top student performance and qualifying for
university admission. Our findings highlight the rapid advances in the
mathematical proficiency of LLMs and illustrate their potential to also support
educational assessments at scale.

</details>


### [33] [A Large-Language Model Framework for Relative Timeline Extraction from PubMed Case Reports](https://arxiv.org/abs/2504.12350)
*Jing Wang,Jeremy C Weiss*

Main category: cs.CL

TL;DR: 论文提出了一种将病例报告转化为文本时间序列的系统，对比了人工和大型语言模型（LLM）的标注效果，发现LLM在事件召回率上表现中等，但在时间一致性上表现优异。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录缺乏对临床事件时间的结构化记录，限制了患者轨迹的分析。

Method: 开发了一个系统，将病例报告转化为文本时间序列，并对比了人工和LLM的标注效果。

Result: LLM在事件召回率上表现中等（0.80），但在时间一致性上表现优异（0.95）。

Conclusion: 该研究为利用PMOA语料库进行时间分析提供了基准。

Abstract: Timing of clinical events is central to characterization of patient
trajectories, enabling analyses such as process tracing, forecasting, and
causal reasoning. However, structured electronic health records capture few
data elements critical to these tasks, while clinical reports lack temporal
localization of events in structured form. We present a system that transforms
case reports into textual time series-structured pairs of textual events and
timestamps. We contrast manual and large language model (LLM) annotations
(n=320 and n=390 respectively) of ten randomly-sampled PubMed open-access
(PMOA) case reports (N=152,974) and assess inter-LLM agreement (n=3,103; N=93).
We find that the LLM models have moderate event recall(O1-preview: 0.80) but
high temporal concordance among identified events (O1-preview: 0.95). By
establishing the task, annotation, and assessment systems, and by demonstrating
high concordance, this work may serve as a benchmark for leveraging the PMOA
corpus for temporal analytics.

</details>


### [34] [Leveraging Large Language Models for Multi-Class and Multi-Label Detection of Drug Use and Overdose Symptoms on Social Media](https://arxiv.org/abs/2504.12355)
*Muhammad Ahmad,Muhammad Waqas,ldar Batyrshin,Grigori Sidorov*

Main category: cs.CL

TL;DR: 该研究提出了一种基于AI和NLP的框架，利用社交媒体数据检测药物使用和过量症状，准确率高达98%。


<details>
  <summary>Details</summary>
Motivation: 传统研究方法在药物过量监测中存在局限性，而社交媒体提供了实时数据，有助于更有效的公共卫生干预。

Method: 采用混合标注策略（LLMs和人工标注），结合传统机器学习、神经网络和基于Transformer的模型。

Result: 框架在多类分类中达到98%准确率，多标签分类中达到97%，比基线模型高出8%。

Conclusion: AI在公共卫生监测和个性化干预中具有巨大潜力。

Abstract: Drug overdose remains a critical global health issue, often driven by misuse
of opioids, painkillers, and psychiatric medications. Traditional research
methods face limitations, whereas social media offers real-time insights into
self-reported substance use and overdose symptoms. This study proposes an
AI-driven NLP framework trained on annotated social media data to detect
commonly used drugs and associated overdose symptoms. Using a hybrid annotation
strategy with LLMs and human annotators, we applied traditional ML models,
neural networks, and advanced transformer-based models. Our framework achieved
98% accuracy in multi-class and 97% in multi-label classification,
outperforming baseline models by up to 8%. These findings highlight the
potential of AI for supporting public health surveillance and personalized
intervention strategies.

</details>


### [35] [Replicating ReLM Results: Validating Large Language Models with ReLM](https://arxiv.org/abs/2504.12357)
*Reece Adamson,Erin Song*

Main category: cs.CL

TL;DR: ReLM利用形式语言评估和控制大型语言模型（LLMs）的记忆、偏见和零样本性能，解决了现有方法慢、不精确、昂贵或引入偏见的问题。


<details>
  <summary>Details</summary>
Motivation: 由于LLMs在生产中的重要性，需要更高效、精确的评估方法。

Method: 采用形式语言方法，复现并扩展了ReLM论文的关键结果。

Result: 验证了ReLM在评估LLMs行为方面的有效性。

Conclusion: ReLM对机器学习系统领域具有重要应用价值。

Abstract: Validating Large Language Models with ReLM explores the application of formal
languages to evaluate and control Large Language Models (LLMs) for
memorization, bias, and zero-shot performance. Current approaches for
evaluating these types behavior are often slow, imprecise, costly, or introduce
biases of their own, but are necessary due to the importance of this behavior
when productionizing LLMs. This project reproduces key results from the
original ReLM paper and expounds on the approach and applications with an
emphasis on the relevance to the field of systems for machine learning.

</details>


### [36] [A Method for Handling Negative Similarities in Explainable Graph Spectral Clustering of Text Documents -- Extended Version](https://arxiv.org/abs/2504.12360)
*Mieczysław A. Kłopotek,Sławomir T. Wierzchoń,Bartłomiej Starosta,Dariusz Czerski,Piotr Borkowski*

Main category: cs.CL

TL;DR: 本文研究了图谱聚类中负相似度的问题，比较了6种解决方案，并展示了处理负相似度的方法对提升聚类准确性的效果。


<details>
  <summary>Details</summary>
Motivation: 传统文档嵌入（如doc2vec、GloVe等）产生的负相似度导致图谱聚类问题，需要探索解决方案。

Method: 讨论了组合拉普拉斯和归一化拉普拉斯的解决方案，并实验比较了6种不同方法。

Result: GloVe嵌入常导致归一化拉普拉斯图谱聚类失败，但处理负相似度的方法能提升准确性。

Conclusion: 处理负相似度的方法不仅提升了聚类效果，还扩展了解释方法的适用性。

Abstract: This paper investigates the problem of Graph Spectral Clustering with
negative similarities, resulting from document embeddings different from the
traditional Term Vector Space (like doc2vec, GloVe, etc.). Solutions for
combinatorial Laplacians and normalized Laplacians are discussed. An
experimental investigation shows the advantages and disadvantages of 6
different solutions proposed in the literature and in this research. The
research demonstrates that GloVe embeddings frequently cause failures of
normalized Laplacian based GSC due to negative similarities. Furthermore,
application of methods curing similarity negativity leads to accuracy
improvement for both combinatorial and normalized Laplacian based GSC. It also
leads to applicability for GloVe embeddings of explanation methods developed
originally bythe authors for Term Vector Space embeddings.

</details>


### [37] [Position: The Most Expensive Part of an LLM should be its Training Data](https://arxiv.org/abs/2504.12427)
*Nikhil Kandpal,Colin Raffel*

Main category: cs.CL

TL;DR: 论文主张为LLM训练数据生产者提供补偿，估算其成本远高于模型训练本身。


<details>
  <summary>Details</summary>
Motivation: LLM训练数据背后的人力劳动被忽视且未获补偿，论文旨在量化其价值并推动公平实践。

Method: 研究2016至2024年发布的64个LLM，估算从头生产训练数据的人力成本。

Result: 训练数据成本保守估计为模型训练成本的10-1000倍，凸显财务负担。

Conclusion: 呼吁未来研究推动更公平的补偿机制，填补数据价值与补偿间的巨大差距。

Abstract: Training a state-of-the-art Large Language Model (LLM) is an increasingly
expensive endeavor due to growing computational, hardware, energy, and
engineering demands. Yet, an often-overlooked (and seldom paid) expense is the
human labor behind these models' training data. Every LLM is built on an
unfathomable amount of human effort: trillions of carefully written words
sourced from books, academic papers, codebases, social media, and more. This
position paper aims to assign a monetary value to this labor and argues that
the most expensive part of producing an LLM should be the compensation provided
to training data producers for their work. To support this position, we study
64 LLMs released between 2016 and 2024, estimating what it would cost to pay
people to produce their training datasets from scratch. Even under highly
conservative estimates of wage rates, the costs of these models' training
datasets are 10-1000 times larger than the costs to train the models
themselves, representing a significant financial liability for LLM providers.
In the face of the massive gap between the value of training data and the lack
of compensation for its creation, we highlight and discuss research directions
that could enable fairer practices in the future.

</details>


### [38] [On Linear Representations and Pretraining Data Frequency in Language Models](https://arxiv.org/abs/2504.12459)
*Jack Merullo,Noah A. Smith,Sarah Wiegreffe,Yanai Elazar*

Main category: cs.CL

TL;DR: 研究探讨了预训练数据频率与语言模型线性表示之间的关系，发现线性表示的形成与数据频率密切相关，并提出了一种预测预训练数据属性的新方法。


<details>
  <summary>Details</summary>
Motivation: 理解预训练数据如何影响语言模型的表示，特别是线性表示的形成因素。

Method: 通过分析预训练数据频率与线性表示的关系，训练回归模型预测数据属性。

Result: 线性表示与数据频率高度相关，回归模型能有效预测预训练数据属性。

Conclusion: 线性表示强度揭示了预训练数据的信号，为优化模型行为提供了新途径。

Abstract: Pretraining data has a direct impact on the behaviors and quality of language
models (LMs), but we only understand the most basic principles of this
relationship. While most work focuses on pretraining data's effect on
downstream task behavior, we investigate its relationship to LM
representations. Previous work has discovered that, in language models, some
concepts are encoded `linearly' in the representations, but what factors cause
these representations to form? We study the connection between pretraining data
frequency and models' linear representations of factual relations. We find
evidence that the formation of linear representations is strongly connected to
pretraining term frequencies; specifically for subject-relation-object fact
triplets, both subject-object co-occurrence frequency and in-context learning
accuracy for the relation are highly correlated with linear representations.
This is the case across all phases of pretraining. In OLMo-7B and GPT-J, we
discover that a linear representation consistently (but not exclusively) forms
when the subjects and objects within a relation co-occur at least 1k and 2k
times, respectively, regardless of when these occurrences happen during
pretraining. Finally, we train a regression model on measurements of linear
representation quality in fully-trained LMs that can predict how often a term
was seen in pretraining. Our model achieves low error even on inputs from a
different model with a different pretraining dataset, providing a new method
for estimating properties of the otherwise-unknown training data of closed-data
models. We conclude that the strength of linear representations in LMs contains
signal about the models' pretraining corpora that may provide new avenues for
controlling and improving model behavior: particularly, manipulating the
models' training data to meet specific frequency thresholds.

</details>


### [39] [SLURG: Investigating the Feasibility of Generating Synthetic Online Fallacious Discourse](https://arxiv.org/abs/2504.12466)
*Cal Blanco,Gavin Dsouza,Hugo Lin,Chelsey Rush*

Main category: cs.CL

TL;DR: 论文探讨了社交媒体上操纵行为的自动检测中逻辑谬误的定义与推断，并以乌克兰-俄罗斯冲突相关论坛为例，发现其中存在大量误导信息。提出SLURG方法，利用大语言模型生成合成谬误评论。


<details>
  <summary>Details</summary>
Motivation: 在线讨论中非标准化和多样化的语言未被现有数据集充分涵盖，需探索如何自动检测论坛中的逻辑谬误。

Method: 提出SLURG方法，利用DeepHermes-3-Mistral-24B等大语言模型生成合成谬误评论，并通过高质量少样本提示增强模型能力。

Result: 大语言模型能复制真实数据的句法模式，高质量提示可提升模型对论坛词汇多样性的模仿能力。

Conclusion: SLURG方法为自动检测论坛中的逻辑谬误提供了可行性，大语言模型在此任务中表现良好。

Abstract: In our paper we explore the definition, and extrapolation of fallacies as
they pertain to the automatic detection of manipulation on social media. In
particular we explore how these logical fallacies might appear in the real
world i.e internet forums. We discovered a prevalence of misinformation /
misguided intention in discussion boards specifically centered around the
Ukrainian Russian Conflict which serves to narrow the domain of our task.
Although automatic fallacy detection has gained attention recently, most
datasets use unregulated fallacy taxonomies or are limited to formal linguistic
domains like political debates or news reports. Online discourse, however,
often features non-standardized and diverse language not captured in these
domains. We present Shady Linguistic Utterance Replication-Generation (SLURG)
to address these limitations, exploring the feasibility of generating synthetic
fallacious forum-style comments using large language models (LLMs),
specifically DeepHermes-3-Mistral-24B. Our findings indicate that LLMs can
replicate the syntactic patterns of real data} and that high-quality few-shot
prompts enhance LLMs' ability to mimic the vocabulary diversity of online
forums.

</details>


### [40] [Integrating Structural and Semantic Signals in Text-Attributed Graphs with BiGTex](https://arxiv.org/abs/2504.12474)
*Azadeh Beiranvand,Seyed Mehdi Vahidipour*

Main category: cs.CL

TL;DR: BiGTex提出了一种双向图文本融合架构，结合GNN和LLM的优势，通过参数高效微调实现节点分类和链接预测的SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 解决文本属性图（TAGs）中语义丰富性和结构依赖性的双重挑战，弥补GNN和LLM各自的不足。

Method: 提出BiGTex架构，通过堆叠的图文本融合单元实现双向注意力机制，结合GNN和LLM，采用LoRA进行参数高效微调。

Result: 在五个基准数据集上，BiGTex在节点分类和链接预测任务中达到SOTA性能。

Conclusion: BiGTex通过双向融合和软提示技术，有效结合了文本和结构信息，为TAGs表示学习提供了新思路。

Abstract: Text-attributed graphs (TAGs) present unique challenges in representation
learning by requiring models to capture both the semantic richness of
node-associated texts and the structural dependencies of the graph. While graph
neural networks (GNNs) excel at modeling topological information, they lack the
capacity to process unstructured text. Conversely, large language models (LLMs)
are proficient in text understanding but are typically unaware of graph
structure. In this work, we propose BiGTex (Bidirectional Graph Text), a novel
architecture that tightly integrates GNNs and LLMs through stacked Graph-Text
Fusion Units. Each unit allows for mutual attention between textual and
structural representations, enabling information to flow in both directions,
text influencing structure and structure guiding textual interpretation. The
proposed architecture is trained using parameter-efficient fine-tuning (LoRA),
keeping the LLM frozen while adapting to task-specific signals. Extensive
experiments on five benchmark datasets demonstrate that BiGTex achieves
state-of-the-art performance in node classification and generalizes effectively
to link prediction. An ablation study further highlights the importance of soft
prompting and bi-directional attention in the model's success.

</details>


### [41] [Can Pre-training Indicators Reliably Predict Fine-tuning Outcomes of LLMs?](https://arxiv.org/abs/2504.12491)
*Hansi Zeng,Kai Hui,Honglei Zhuang,Zhen Qin,Zhenrui Yue,Hamed Zamani,Dana Alon*

Main category: cs.CL

TL;DR: 论文研究了预训练指标（如困惑度）在固定模型规模下对下游任务性能的预测能力不足的问题，提出了一种基于成对分类的方法，并引入新的代理指标，显著降低了性能预测误差。


<details>
  <summary>Details</summary>
Motivation: 预训练指标（如困惑度）在模型规模扩展研究中表现良好，但在固定模型规模下预测能力不明确，影响了模型选择和开发效率。

Method: 将预训练检查点选择任务建模为成对分类问题，构建了50个1B参数LLM变体的数据集，并引入新的无监督和有监督代理指标。

Result: 传统困惑度指标具有误导性，新提出的代理指标将相对性能预测误差率降低了50%以上。

Conclusion: 新代理指标在特定场景下具有实用价值，为优化预训练方案提供了更高效的设计思路。

Abstract: While metrics available during pre-training, such as perplexity, correlate
well with model performance at scaling-laws studies, their predictive
capacities at a fixed model size remain unclear, hindering effective model
selection and development. To address this gap, we formulate the task of
selecting pre-training checkpoints to maximize downstream fine-tuning
performance as a pairwise classification problem: predicting which of two LLMs,
differing in their pre-training, will perform better after supervised
fine-tuning (SFT). We construct a dataset using 50 1B parameter LLM variants
with systematically varied pre-training configurations, e.g., objectives or
data, and evaluate them on diverse downstream tasks after SFT. We first conduct
a study and demonstrate that the conventional perplexity is a misleading
indicator. As such, we introduce novel unsupervised and supervised proxy
metrics derived from pre-training that successfully reduce the relative
performance prediction error rate by over 50%. Despite the inherent complexity
of this task, we demonstrate the practical utility of our proposed proxies in
specific scenarios, paving the way for more efficient design of pre-training
schemes optimized for various downstream tasks.

</details>


### [42] [Accelerating Clinical NLP at Scale with a Hybrid Framework with Reduced GPU Demands: A Case Study in Dementia Identification](https://arxiv.org/abs/2504.12494)
*Jianlin Shi,Qiwei Gan,Elizabeth Hanchrow,Annie Bowles,John Stanley,Adam P. Bress,Jordana B. Cohen,Patrick R. Alba*

Main category: cs.CL

TL;DR: 提出了一种结合规则过滤、SVM分类器和BERT模型的混合NLP框架，用于高效且准确地处理大规模临床文本，并在痴呆症识别案例中验证了其可行性。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer的临床NLP解决方案计算资源需求高，限制了其可及性，因此需要一种更高效的替代方案。

Method: 采用混合框架，结合规则过滤、SVM分类器和BERT模型，应用于包含4.9百万退伍军人和21亿临床笔记的痴呆症识别研究。

Result: 患者级别的精确度为0.90，召回率为0.84，F1分数为0.87，且识别出的痴呆症病例是结构化数据方法的三倍以上。

Conclusion: 混合NLP框架在大规模临床文本分析中具有可行性，为计算资源有限的医疗机构提供了高效解决方案。

Abstract: Clinical natural language processing (NLP) is increasingly in demand in both
clinical research and operational practice. However, most of the
state-of-the-art solutions are transformers-based and require high
computational resources, limiting their accessibility. We propose a hybrid NLP
framework that integrates rule-based filtering, a Support Vector Machine (SVM)
classifier, and a BERT-based model to improve efficiency while maintaining
accuracy. We applied this framework in a dementia identification case study
involving 4.9 million veterans with incident hypertension, analyzing 2.1
billion clinical notes. At the patient level, our method achieved a precision
of 0.90, a recall of 0.84, and an F1-score of 0.87. Additionally, this NLP
approach identified over three times as many dementia cases as structured data
methods. All processing was completed in approximately two weeks using a single
machine with dual A40 GPUs. This study demonstrates the feasibility of hybrid
NLP solutions for large-scale clinical text analysis, making state-of-the-art
methods more accessible to healthcare organizations with limited computational
resources.

</details>


### [43] [Beyond Text: Characterizing Domain Expert Needs in Document Research](https://arxiv.org/abs/2504.12495)
*Sireesh Gururaja,Nupoor Gandhi,Jeremiah Milbauer,Emma Strubell*

Main category: cs.CL

TL;DR: 研究探讨了NLP系统在文档处理任务中与专家实际需求的差距，呼吁NLP社区更关注文档的社会背景和个性化需求。


<details>
  <summary>Details</summary>
Motivation: 了解NLP系统是否能真正模拟专家在文档研究中的工作方式，并比较现有技术与专家实践的差异。

Method: 访谈了16位跨两个领域的专家，分析其文档研究过程，并与当前NLP系统能力对比。

Result: 专家的工作流程具有独特性、迭代性，并依赖文档的社会背景；现有NLP方法中关注文档本身（而非仅文本）的更符合专家需求。

Conclusion: 呼吁NLP社区在工具开发中更注重文档的角色，使其更具可访问性、个性化、迭代性和社会意识。

Abstract: Working with documents is a key part of almost any knowledge work, from
contextualizing research in a literature review to reviewing legal precedent.
Recently, as their capabilities have expanded, primarily text-based NLP systems
have often been billed as able to assist or even automate this kind of work.
But to what extent are these systems able to model these tasks as experts
conceptualize and perform them now? In this study, we interview sixteen domain
experts across two domains to understand their processes of document research,
and compare it to the current state of NLP systems. We find that our
participants processes are idiosyncratic, iterative, and rely extensively on
the social context of a document in addition its content; existing approaches
in NLP and adjacent fields that explicitly center the document as an object,
rather than as merely a container for text, tend to better reflect our
participants' priorities, though they are often less accessible outside their
research communities. We call on the NLP community to more carefully consider
the role of the document in building useful tools that are accessible,
personalizable, iterative, and socially aware.

</details>


### [44] [BrowseComp: A Simple Yet Challenging Benchmark for Browsing Agents](https://arxiv.org/abs/2504.12516)
*Jason Wei,Zhiqing Sun,Spencer Papay,Scott McKinney,Jeffrey Han,Isa Fulford,Hyung Won Chung,Alex Tachard Passos,William Fedus,Amelia Glaese*

Main category: cs.CL

TL;DR: BrowseComp是一个用于评估网络浏览能力的基准测试，包含1,266个需要持续搜索互联网的问题，答案简短且易于验证。


<details>
  <summary>Details</summary>
Motivation: 提供一个简单但具有挑战性的基准，衡量代理在网络浏览中的持久性和创造力。

Method: 设计1,266个需要深入搜索互联网的问题，答案简短且可验证。

Result: BrowseComp能够有效评估代理在网络浏览中的核心能力。

Conclusion: BrowseComp是一个有用的基准，类似于编程竞赛对编码代理的作用。

Abstract: We present BrowseComp, a simple yet challenging benchmark for measuring the
ability for agents to browse the web. BrowseComp comprises 1,266 questions that
require persistently navigating the internet in search of hard-to-find,
entangled information. Despite the difficulty of the questions, BrowseComp is
simple and easy-to-use, as predicted answers are short and easily verifiable
against reference answers. BrowseComp for browsing agents can be seen as
analogous to how programming competitions are an incomplete but useful
benchmark for coding agents. While BrowseComp sidesteps challenges of a true
user query distribution, like generating long answers or resolving ambiguity,
it measures the important core capability of exercising persistence and
creativity in finding information. BrowseComp can be found at
https://github.com/openai/simple-evals.

</details>


### [45] [Evaluating the Diversity and Quality of LLM Generated Content](https://arxiv.org/abs/2504.12522)
*Alexander Shypula,Shuo Li,Botong Zhang,Vishakh Padmakumar,Kayo Yin,Osbert Bastani*

Main category: cs.CL

TL;DR: 研究发现，偏好调优技术（如RLHF、DPO）虽然减少了词汇和句法多样性，但提高了有效语义多样性，因为生成了更多高质量输出。小模型在固定采样预算下更高效地生成独特内容。


<details>
  <summary>Details</summary>
Motivation: 解决偏好调优技术可能减少输出多样性的问题，提出有效语义多样性的衡量框架，以更好地反映LLMs的实际效用。

Method: 引入有效语义多样性的衡量框架，通过无需人工干预的开放式任务评估偏好调优模型（如RLHF、DPO）与SFT或基础模型的多样性表现。

Result: 偏好调优模型（尤其是RL训练的）在有效语义多样性上优于SFT或基础模型，但词汇和句法多样性降低。小模型在生成独特内容上更高效。

Conclusion: 偏好调优在保留语义多样性的同时减少句法多样性，小模型在多样性生成上更具参数效率，对需要高质量多样输出的应用有重要意义。

Abstract: Recent work suggests that preference-tuning techniques--including
Reinforcement Learning from Human Preferences (RLHF) methods like PPO and GRPO,
as well as alternatives like DPO--reduce diversity, creating a dilemma given
that such models are widely deployed in applications requiring diverse outputs.
To address this, we introduce a framework for measuring effective semantic
diversity--diversity among outputs that meet quality thresholds--which better
reflects the practical utility of large language models (LLMs). Using
open-ended tasks that require no human intervention, we find counterintuitive
results: although preference-tuned models--especially those trained via
RL--exhibit reduced lexical and syntactic diversity, they produce greater
effective semantic diversity than SFT or base models, not from increasing
diversity among high-quality outputs, but from generating more high-quality
outputs overall. We discover that preference tuning reduces syntactic diversity
while preserving semantic diversity--revealing a distinction between diversity
in form and diversity in content that traditional metrics often overlook. Our
analysis further shows that smaller models are consistently more
parameter-efficient at generating unique content within a fixed sampling
budget, offering insights into the relationship between model scaling and
diversity. These findings have important implications for applications that
require diverse yet high-quality outputs, from creative assistance to synthetic
data generation.

</details>


### [46] [Memorization vs. Reasoning: Updating LLMs with New Knowledge](https://arxiv.org/abs/2504.12523)
*Aochong Oliver Li,Tanya Goyal*

Main category: cs.CL

TL;DR: 论文提出了一种名为KUP的自动管道，用于模拟现实知识更新，并引入了MCT方法，显著提升了语言模型的知识更新能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要针对实体替换，无法全面捕捉复杂现实动态，因此需要一种更全面的知识更新评估框架和方法。

Method: 提出KUP评估框架和MCT训练方法，通过自生成的“记忆”令牌在训练中更新知识。

Result: KUP基准测试极具挑战性，MCT方法在直接探测（记忆）上比CPT基线提升25.4%。

Conclusion: KUP和MCT为语言模型的知识更新提供了有效工具，显著提升了性能。

Abstract: Large language models (LLMs) encode vast amounts of pre-trained knowledge in
their parameters, but updating them as real-world information evolves remains a
challenge. Existing methodologies and benchmarks primarily target entity
substitutions, failing to capture the full breadth of complex real-world
dynamics. In this paper, we introduce Knowledge Update Playground (KUP), an
automatic pipeline for simulating realistic knowledge updates reflected in an
evidence corpora. KUP's evaluation framework includes direct and indirect
probes to both test memorization of updated facts and reasoning over them, for
any update learning methods. Next, we present a lightweight method called
memory conditioned training (MCT), which conditions tokens in the update corpus
on self-generated "memory" tokens during training. Our strategy encourages LLMs
to surface and reason over newly memorized knowledge at inference. Our results
on two strong LLMs show that (1) KUP benchmark is highly challenging, with the
best CPT models achieving $<2\%$ in indirect probing setting (reasoning) and
(2) MCT training significantly outperforms prior continued pre-training (CPT)
baselines, improving direct probing (memorization) results by up to $25.4\%$.

</details>


### [47] [Memorization: A Close Look at Books](https://arxiv.org/abs/2504.12549)
*Iris Ma,Ian Domingo,Alberto Krone-Martins,Pierre Baldi,Cristina V. Lopes*

Main category: cs.CL

TL;DR: 研究探讨了从LLMs中提取整本书的可能性，使用Llama 3 70B模型和“前缀提示”技术，成功高相似度地重构了《爱丽丝梦游仙境》等书籍，但提取效果因书籍流行度而异。同时发现指令调优的Llama 3.1中缓解措施的失效与少数权重变化相关。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs对训练数据中书籍内容的记忆能力，并评估现有缓解策略的有效性。

Method: 使用Llama 3 70B模型和“前缀提示”技术，通过自回归方式从少量初始标记重构书籍内容。

Result: 成功提取了部分书籍（如《爱丽丝梦游仙境》），但效果因书籍流行度不同；指令调优的Llama 3.1中缓解措施因少数权重变化失效。

Conclusion: 当前缓解策略存在局限性，需进一步研究微调对LLMs记忆能力的影响。

Abstract: To what extent can entire books be extracted from LLMs? Using the Llama 3 70B
family of models, and the "prefix-prompting" extraction technique, we were able
to auto-regressively reconstruct, with a very high level of similarity, one
entire book (Alice's Adventures in Wonderland) from just the first 500 tokens.
We were also able to obtain high extraction rates on several other books,
piece-wise. However, these successes do not extend uniformly to all books. We
show that extraction rates of books correlate with book popularity and thus,
likely duplication in the training data.
  We also confirm the undoing of mitigations in the instruction-tuned Llama
3.1, following recent work (Nasr et al., 2025). We further find that this
undoing comes from changes to only a tiny fraction of weights concentrated
primarily in the lower transformer blocks. Our results provide evidence of the
limits of current regurgitation mitigation strategies and introduce a framework
for studying how fine-tuning affects the retrieval of verbatim memorization in
aligned LLMs.

</details>


### [48] [ELAB: Extensive LLM Alignment Benchmark in Persian Language](https://arxiv.org/abs/2504.12553)
*Zahra Pourbahman,Fatemeh Rajabi,Mohammadhossein Sadeghi,Omid Ghahroodi,Somaye Bakhshaei,Arash Amini,Reza Kazemi,Mahdieh Soleymani Baghshah*

Main category: cs.CL

TL;DR: 该论文提出了一个全面的评估框架，用于将波斯语大型语言模型（LLMs）与伦理维度（如安全性、公平性和社会规范）对齐，填补了现有评估框架在波斯语言和文化背景下的空白。


<details>
  <summary>Details</summary>
Motivation: 现有LLM评估框架未充分考虑波斯语言和文化背景，因此需要一种适应本土文化的评估方法。

Method: 通过翻译现有数据集（如Anthropic Red Teaming数据）和创建新数据集（如ProhibiBench-fa、SafeBench-fa等），构建了一个统一的波斯语LLM评估框架。

Result: 系统评估了波斯语LLMs在安全性、公平性和社会规范三个方面的表现，并公开了排行榜。

Conclusion: 该研究为波斯语LLMs的文化适应性评估提供了新方法，并公开了相关数据集和评估工具。

Abstract: This paper presents a comprehensive evaluation framework for aligning Persian
Large Language Models (LLMs) with critical ethical dimensions, including
safety, fairness, and social norms. It addresses the gaps in existing LLM
evaluation frameworks by adapting them to Persian linguistic and cultural
contexts. This benchmark creates three types of Persian-language benchmarks:
(i) translated data, (ii) new data generated synthetically, and (iii) new
naturally collected data. We translate Anthropic Red Teaming data, AdvBench,
HarmBench, and DecodingTrust into Persian. Furthermore, we create
ProhibiBench-fa, SafeBench-fa, FairBench-fa, and SocialBench-fa as new datasets
to address harmful and prohibited content in indigenous culture. Moreover, we
collect extensive dataset as GuardBench-fa to consider Persian cultural norms.
By combining these datasets, our work establishes a unified framework for
evaluating Persian LLMs, offering a new approach to culturally grounded
alignment evaluation. A systematic evaluation of Persian LLMs is performed
across the three alignment aspects: safety (avoiding harmful content), fairness
(mitigating biases), and social norms (adhering to culturally accepted
behaviors). We present a publicly available leaderboard that benchmarks Persian
LLMs with respect to safety, fairness, and social norms at:
https://huggingface.co/spaces/MCILAB/LLM_Alignment_Evaluation.

</details>


### [49] [CDF-RAG: Causal Dynamic Feedback for Adaptive Retrieval-Augmented Generation](https://arxiv.org/abs/2504.12560)
*Elahe Khatibi,Ziyu Wang,Amir M. Rahmani*

Main category: cs.CL

TL;DR: CDF-RAG框架通过动态反馈和因果推理提升RAG的因果一致性和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有RAG框架依赖语义相似性检索，难以区分真实因果关系与虚假关联，导致生成结果可能不完整或误导。

Method: CDF-RAG通过迭代查询优化、检索结构化因果图及多跳因果推理，验证因果路径以确保逻辑一致性。

Result: 在四个数据集上验证，CDF-RAG显著提升了生成响应的准确性和因果正确性。

Conclusion: CDF-RAG为知识密集型任务提供了更具因果一致性和可解释性的生成推理框架。

Abstract: Retrieval-Augmented Generation (RAG) has significantly enhanced large
language models (LLMs) in knowledge-intensive tasks by incorporating external
knowledge retrieval. However, existing RAG frameworks primarily rely on
semantic similarity and correlation-driven retrieval, limiting their ability to
distinguish true causal relationships from spurious associations. This results
in responses that may be factually grounded but fail to establish
cause-and-effect mechanisms, leading to incomplete or misleading insights. To
address this issue, we introduce Causal Dynamic Feedback for Adaptive
Retrieval-Augmented Generation (CDF-RAG), a framework designed to improve
causal consistency, factual accuracy, and explainability in generative
reasoning. CDF-RAG iteratively refines queries, retrieves structured causal
graphs, and enables multi-hop causal reasoning across interconnected knowledge
sources. Additionally, it validates responses against causal pathways, ensuring
logically coherent and factually grounded outputs. We evaluate CDF-RAG on four
diverse datasets, demonstrating its ability to improve response accuracy and
causal correctness over existing RAG-based methods. Our code is publicly
available at https://github.com/ elakhatibi/CDF-RAG.

</details>


### [50] [MetaSynth: Meta-Prompting-Driven Agentic Scaffolds for Diverse Synthetic Data Generation](https://arxiv.org/abs/2504.12563)
*Haris Riaz,Sourav Bhabesh,Vinayak Arannil,Miguel Ballesteros,Graham Horwood*

Main category: cs.CL

TL;DR: MetaSynth提出了一种通过元提示增强合成数据多样性的方法，成功将Mistral-7B-v0.3适配到金融和生物医学领域，且不损害其通用能力。


<details>
  <summary>Details</summary>
Motivation: 现有合成数据多样性不足，限制了其在特定领域模型适配中的应用。

Method: MetaSynth利用元提示技术，协调多个专家LLM代理协作生成数据。

Result: 仅用2500万标记的合成数据，MetaSynth在金融和生物医学领域分别提升模型性能4.08%和13.75%。

Conclusion: MetaSynth证明少量高多样性合成数据即可有效实现领域适配。

Abstract: Recent smaller language models such Phi-3.5 and Phi-4 rely on synthetic data
generated using larger Language models. Questions remain about leveraging
synthetic data for other use cases, such as adapting LLMs to specific domains.
A key limitation of synthetic data is low diversity, which negatively impacts
its downstream applicability for improving other models. To address this, we
propose MetaSynth, a method for generating synthetic data that enhances
diversity through meta-prompting, where a language model orchestrates multiple
"expert" LLM agents to collaboratively generate data. Using only 25 million
tokens of synthetic data generated with MetaSynth, we successfully adapt a
well-trained LLM (Mistral-7B-v0.3) to two specialized domains-Finance and
Biomedicine-without compromising the capabilities of the resulting model in
general tasks. In addition, we evaluate the diversity of our synthetic data
using seven automated metrics, and find that it approaches the diversity of LLM
pre-training corpora.
  Continually pre-training Mistral-7B-v0.3 with MetaSynth notably outperforms
the base LLM, showing improvements of up to 4.08% in Finance and 13.75% in
Biomedicine. The same model shows degraded performance when trained on data
generated using a template prompt, even when the template includes prior
generations and varying In-Context exemplars of real data. Our findings suggest
that a few million tokens of diverse synthetic data without mixing any real
data, is sufficient for effective domain adaptation when using MetaSynth.

</details>


### [51] [Identifying and Mitigating the Influence of the Prior Distribution in Large Language Models](https://arxiv.org/abs/2504.12585)
*Liyi Zhang,Veniamin Veselovsky,R. Thomas McCoy,Thomas L. Griffiths*

Main category: cs.CL

TL;DR: 论文探讨了大型语言模型（LLMs）在处理确定性任务时因隐式先验分布而表现不佳的问题，并提出通过干预措施（如提示和微调）来改善其性能。


<details>
  <summary>Details</summary>
Motivation: LLMs在处理确定性任务（如计数或缩写）时表现不佳，因为它们学习的隐式先验分布影响了响应。研究旨在探索LLMs是否实际具备正确执行任务的能力，并找到干预方法。

Method: 通过提示模型不依赖先验知识，并使用机制解释技术定位先验层，进行轻量级微调。

Result: 干预措施显著提高了模型在先验主导任务上的表现，微调后的错误不再与先验相关。

Conclusion: 研究表明，可以通过操纵LLMs对先验的依赖程度来提升其性能，尤其在幻觉问题相关的任务中。

Abstract: Large language models (LLMs) sometimes fail to respond appropriately to
deterministic tasks -- such as counting or forming acronyms -- because the
implicit prior distribution they have learned over sequences of tokens
influences their responses. In this work, we show that, in at least some cases,
LLMs actually compute the information needed to perform these tasks correctly,
and we identify some interventions that can allow them to access this
information to improve their performance. First, we show that simply prompting
the language model to not rely on its prior knowledge leads to dramatic
improvements in prior-dominated tasks. We then use mechanistic interpretability
techniques to localize the prior within the LLM and manipulate the extent to
which that prior influences its responses. Specifically, we show that it is
possible to identify layers of the underlying neural network that correlate
with the prior probability of a response and that lightweight finetuning of
these layers with basic prompts on prior-dominated tasks achieves high
performance on held-out answers. These results suggest that the information
required to produce a correct response is contained within the representations
of the problems formed by the models. Furthermore, we show that this finetuning
is significantly more effective for prior-dominated tasks, and that the error
after finetuning is no longer correlated with the prior. Our results suggest
that it may be possible to define effective methods for manipulating the extent
to which LLMs rely upon their priors in solving problems, potentially
increasing their performance in settings where LLMs hallucinate for reasons
related to the prior probability of token sequences.

</details>


### [52] [GeoSense: Evaluating Identification and Application of Geometric Principles in Multimodal Reasoning](https://arxiv.org/abs/2504.12597)
*Liangyu Xu,Yingxiu Zhao,Jingyun Wang,Yingyao Wang,Bu Pi,Chen Wang,Mingliang Zhang,Jihao Gu,Xiang Li,Xiaoyong Zhu,Jun Song,Bo Zheng*

Main category: cs.CL

TL;DR: GeoSense是一个新的双语基准测试，用于评估多模态大语言模型（MLLMs）的几何推理能力，发现现有模型在几何原理的识别和应用上仍有瓶颈。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试未能全面评估MLLMs在几何问题解决（GPS）中的视觉理解和符号推理能力，因此需要开发一个更全面的评估工具。

Method: 提出了GeoSense，包含五级几何原理层次框架、1789个标注问题和创新评估策略，并在多个MLLMs上进行了实验。

Result: Gemini-2.0-pro-flash表现最佳，总体得分65.3，但几何原理的识别和应用仍是瓶颈。

Conclusion: GeoSense有助于指导未来MLLMs在几何推理能力上的进步，推动更接近人类推理的人工智能发展。

Abstract: Geometry problem-solving (GPS), a challenging task requiring both visual
comprehension and symbolic reasoning, effectively measures the reasoning
capabilities of multimodal large language models (MLLMs). Humans exhibit strong
reasoning ability in this task through accurate identification and adaptive
application of geometric principles within visual contexts. However, existing
benchmarks fail to jointly assess both dimensions of the human-like geometric
reasoning mechanism in MLLMs, remaining a critical gap in assessing their
ability to tackle GPS. To this end, we introduce GeoSense, the first
comprehensive bilingual benchmark designed to systematically evaluate the
geometric reasoning abilities of MLLMs through the lens of geometric
principles. GeoSense features a five-level hierarchical framework of geometric
principles spanning plane and solid geometry, an intricately annotated dataset
of 1,789 problems, and an innovative evaluation strategy. Through extensive
experiments on GeoSense with various open-source and closed-source MLLMs, we
observe that Gemini-2.0-pro-flash performs best, achieving an overall score of
$65.3$. Our in-depth analysis reveals that the identification and application
of geometric principles remain a bottleneck for leading MLLMs, jointly
hindering their reasoning abilities. These findings underscore GeoSense's
potential to guide future advancements in MLLMs' geometric reasoning
capabilities, paving the way for more robust and human-like reasoning in
artificial intelligence.

</details>


### [53] [Towards Characterizing Subjectivity of Individuals through Modeling Value Conflicts and Trade-offs](https://arxiv.org/abs/2504.12633)
*Younghun Lee,Dan Goldwasser*

Main category: cs.CL

TL;DR: 本文探讨了大型语言模型（LLMs）在个体主观性任务中的表现，提出了SOLAR框架以更好地捕捉社交媒体用户的价值观冲突和权衡。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明LLMs能部分解决主观性问题，但个体层面的主观性尚未充分研究。本文旨在填补这一空白。

Method: 提出SOLAR框架，通过分析用户生成文本中的价值观冲突和权衡来推断个体的道德判断。

Result: 实验表明SOLAR提升了推断结果的准确性，尤其在争议性情境中表现更佳，并能解释个体的价值观偏好。

Conclusion: SOLAR框架有效捕捉个体主观性，为LLMs在主观决策任务中的应用提供了新思路。

Abstract: Large Language Models (LLMs) not only have solved complex reasoning problems
but also exhibit remarkable performance in tasks that require subjective
decision making. Existing studies suggest that LLM generations can be
subjectively grounded to some extent, yet exploring whether LLMs can account
for individual-level subjectivity has not been sufficiently studied. In this
paper, we characterize subjectivity of individuals on social media and infer
their moral judgments using LLMs. We propose a framework, SOLAR (Subjective
Ground with Value Abstraction), that observes value conflicts and trade-offs in
the user-generated texts to better represent subjective ground of individuals.
Empirical results show that our framework improves overall inference results as
well as performance on controversial situations. Additionally, we qualitatively
show that SOLAR provides explanations about individuals' value preferences,
which can further account for their judgments.

</details>


### [54] [Scaling Instruction-Tuned LLMs to Million-Token Contexts via Hierarchical Synthetic Data Generation](https://arxiv.org/abs/2504.12637)
*Linda He,Jue Wang,Maurice Weber,Shang Zhu,Ben Athiwaratkun,Ce Zhang*

Main category: cs.CL

TL;DR: 论文提出了一种新的后训练合成数据生成策略，用于高效扩展LLMs的上下文窗口，同时保持其通用任务性能。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs在长上下文推理中的困难，包括计算复杂度高和长上下文数据稀缺的问题。

Method: 采用逐步旋转位置嵌入（RoPE）缩放训练策略，生成合成数据以扩展上下文长度。

Result: 模型在1M tokens的上下文长度下，在RULER和InfiniteBench基准上表现良好，同时保持通用语言任务的性能。

Conclusion: 该方法有效解决了长上下文数据稀缺问题，并成功扩展了LLMs的上下文窗口。

Abstract: Large Language Models (LLMs) struggle with long-context reasoning, not only
due to the quadratic scaling of computational complexity with sequence length
but also because of the scarcity and expense of annotating long-context data.
There has been barely any open-source work that systematically ablates
long-context data, nor is there any openly available instruction tuning dataset
with contexts surpassing 100K tokens. To bridge this gap, we introduce a novel
post-training synthetic data generation strategy designed to efficiently extend
the context window of LLMs while preserving their general task performance. Our
approach scalably extends to arbitrarily long context lengths, unconstrained by
the length of available real-world data, which effectively addresses the
scarcity of raw long-context data. Through a step-by-step rotary position
embedding (RoPE) scaling training strategy, we demonstrate that our model, with
a context length of up to 1M tokens, performs well on the RULER benchmark and
InfiniteBench and maintains robust performance on general language tasks.

</details>


### [55] [Persona-judge: Personalized Alignment of Large Language Models via Token-level Self-judgment](https://arxiv.org/abs/2504.12663)
*Xiaotian Zhang,Ruizhe Chen,Yang Feng,Zuozhu Liu*

Main category: cs.CL

TL;DR: 提出了一种无需训练的个性化对齐方法Persona-judge，利用模型内在偏好判断能力，实现高效个性化对齐。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖奖励信号和额外标注数据，难以适应多样化人类价值观且计算成本高。

Method: 通过生成候选标记并由另一偏好的法官模型交叉验证，实现个性化对齐。

Result: 实验表明Persona-judge具有可扩展性和计算高效性。

Conclusion: Persona-judge为自适应定制对齐提供了新途径。

Abstract: Aligning language models with human preferences presents significant
challenges, particularly in achieving personalization without incurring
excessive computational costs. Existing methods rely on reward signals and
additional annotated data, limiting their scalability and adaptability to
diverse human values. To address these challenges, we introduce Persona-judge,
a novel discriminative paradigm that enables training-free personalized
alignment with unseen preferences. Instead of optimizing policy parameters
through external reward feedback, Persona-judge leverages the intrinsic
preference judgment capabilities of the model. Specifically, a draft model
generates candidate tokens conditioned on a given preference, while a judge
model, embodying another preference, cross-validates the predicted tokens
whether to be accepted. Experimental results demonstrate that Persona-judge,
using the inherent preference evaluation mechanisms of the model, offers a
scalable and computationally efficient solution to personalized alignment,
paving the way for more adaptive customized alignment.

</details>


### [56] [ACoRN: Noise-Robust Abstractive Compression in Retrieval-Augmented Language Models](https://arxiv.org/abs/2504.12673)
*Singon Kim,Gunho Jung,Seong-Whan Lee*

Main category: cs.CL

TL;DR: 论文提出ACoRN方法，通过细粒度分类和增强训练步骤，提升抽象压缩模型在噪声文档中的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 检索到的文档常包含无关或误导信息，导致抽象压缩模型忽略关键内容，影响答案准确性。

Method: 1. 使用离线数据增强提升模型对噪声的鲁棒性；2. 微调模型以生成围绕关键信息的摘要。

Result: ACoRN显著提升EM和F1分数，并在高噪声文档数据集中表现优异。

Conclusion: ACoRN有效解决了抽象压缩模型在噪声文档中的性能问题，适用于实际场景。

Abstract: Abstractive compression utilizes smaller langauge models to condense
query-relevant context, reducing computational costs in retrieval-augmented
generation (RAG). However,retrieved documents often include information that is
either irrelevant to answering the query or misleading due to factual incorrect
content, despite having high relevance scores. This behavior indicates that
abstractive compressors are more likely to omit important information essential
for the correct answer, especially in long contexts where attention dispersion
occurs. To address this issue, we categorize retrieved documents in a more
fine-grained manner and propose Abstractive Compression Robust against Noise
(ACoRN), which introduces two novel training steps. First, we use offline data
augmentation on the training dataset to enhance compressor robustness against
two distinct types of retrieval noise. Second, since the language modelbased
compressor cannot fully utilize information from multiple retrieved documents
and exhibits positional bias, we perform finetuning to generate summaries
centered around key information that directly supports the correct answer. Our
experiments demonstrate that T5-large, trained with ACoRN as a compressor,
improves EM and F1 scores while preserving the answer string, which could serve
as direct evidence. ACoRN excels on datasets with many accuracy-reducing
documents, making it highly useful in real-world scenarios.

</details>


### [57] [GRAIL: Gradient-Based Adaptive Unlearning for Privacy and Copyright in LLMs](https://arxiv.org/abs/2504.12681)
*Kun-Woo Kim,Ji-Hoon Park,Ju-Min Han,Seong-Whan Lee*

Main category: cs.CL

TL;DR: GRAIL是一种基于梯度的多领域遗忘框架，能精确区分遗忘与保留范围，选择性移除目标知识，同时保留关键参数。


<details>
  <summary>Details</summary>
Motivation: 大语言模型可能学习敏感信息，现有单领域遗忘方法无法处理多领域交织知识，导致过度移除或性能下降。

Method: 提出GRAIL框架，利用多领域梯度信息，采用自适应参数定位策略。

Result: 实验显示GRAIL在遗忘效果上媲美现有方法，知识保留能力提升17%。

Conclusion: GRAIL为大规模预训练语言模型中的敏感信息管理提供了新范式。

Abstract: Large Language Models (LLMs) trained on extensive datasets often learn
sensitive information, which raises significant social and legal concerns under
principles such as the "Right to be forgotten." Retraining entire models from
scratch to remove undesired information is both costly and impractical.
Furthermore, existing single-domain unlearning methods fail to address
multi-domain scenarios, where knowledge is interwoven across domains such as
privacy and copyright, creating overlapping representations that lead to
excessive knowledge removal or degraded performance. To tackle these issues, we
propose GRAIL (GRadient-based AdaptIve unLearning), a novel multi-domain
unlearning framework. GRAIL leverages gradient information from multiple
domains to precisely distinguish the unlearning scope from the retention scope,
and applies an adaptive parameter-wise localization strategy to selectively
remove targeted knowledge while preserving critical parameters for each domain.
Experimental results on unlearning benchmarks show that GRAIL achieves
unlearning success on par with the existing approaches, while also
demonstrating up to 17% stronger knowledge retention success compared to the
previous state-of-art method. Our findings establish a new paradigm for
effectively managing and regulating sensitive information in large-scale
pre-trained language models.

</details>


### [58] [Data-efficient LLM Fine-tuning for Code Generation](https://arxiv.org/abs/2504.12687)
*Weijie Lv,Xuan Xia,Sheng-Jun Huang*

Main category: cs.CL

TL;DR: 提出了一种数据选择策略和动态打包技术，优化代码生成LLMs的训练效率和性能。


<details>
  <summary>Details</summary>
Motivation: 开源与闭源模型在代码生成任务上存在性能差距，现有方法生成大量合成数据导致训练效率低下。

Method: 通过数据复杂度优先和动态打包技术优化数据选择和tokenization。

Result: 在40%数据集上训练，性能提升且训练时间、GPU内存消耗显著减少。

Conclusion: 优化数据选择和tokenization可同时提升模型性能和训练效率。

Abstract: Large language models (LLMs) have demonstrated significant potential in code
generation tasks. However, there remains a performance gap between open-source
and closed-source models. To address this gap, existing approaches typically
generate large amounts of synthetic data for fine-tuning, which often leads to
inefficient training. In this work, we propose a data selection strategy in
order to improve the effectiveness and efficiency of training for code-based
LLMs. By prioritizing data complexity and ensuring that the sampled subset
aligns with the distribution of the original dataset, our sampling strategy
effectively selects high-quality data. Additionally, we optimize the
tokenization process through a "dynamic pack" technique, which minimizes
padding tokens and reduces computational resource consumption. Experimental
results show that when training on 40% of the OSS-Instruct dataset, the
DeepSeek-Coder-Base-6.7B model achieves an average performance of 66.9%,
surpassing the 66.1% performance with the full dataset. Moreover, training time
is reduced from 47 minutes to 34 minutes, and the peak GPU memory decreases
from 61.47 GB to 42.72 GB during a single epoch. Similar improvements are
observed with the CodeLlama-Python-7B model on the Evol-Instruct dataset. By
optimizing both data selection and tokenization, our approach not only improves
model performance but also improves training efficiency.

</details>


### [59] [Why and How LLMs Hallucinate: Connecting the Dots with Subsequence Associations](https://arxiv.org/abs/2504.12691)
*Yiyou Sun,Yu Gai,Lijie Chen,Abhilasha Ravichander,Yejin Choi,Dawn Song*

Main category: cs.CL

TL;DR: 该论文提出了一种子序列关联框架，用于系统追踪和理解大语言模型（LLMs）中的幻觉现象，并通过理论和实证分析验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）经常生成与事实或上下文不符的幻觉内容，诊断其成因具有挑战性。

Method: 论文提出了一种子序列关联框架，通过分析幻觉概率在随机输入上下文中的变化，识别因果子序列。

Result: 实验表明，该方法在识别幻觉成因方面优于标准归因技术，并与模型训练语料库的证据一致。

Conclusion: 该研究为幻觉现象提供了统一视角，并提出了一个稳健的追踪和分析框架。

Abstract: Large language models (LLMs) frequently generate hallucinations-content that
deviates from factual accuracy or provided context-posing challenges for
diagnosis due to the complex interplay of underlying causes. This paper
introduces a subsequence association framework to systematically trace and
understand hallucinations. Our key insight is that hallucinations arise when
dominant hallucinatory associations outweigh faithful ones. Through theoretical
and empirical analyses, we demonstrate that decoder-only transformers
effectively function as subsequence embedding models, with linear layers
encoding input-output associations. We propose a tracing algorithm that
identifies causal subsequences by analyzing hallucination probabilities across
randomized input contexts. Experiments show our method outperforms standard
attribution techniques in identifying hallucination causes and aligns with
evidence from the model's training corpus. This work provides a unified
perspective on hallucinations and a robust framework for their tracing and
analysis.

</details>


### [60] [KODIS: A Multicultural Dispute Resolution Dialogue Corpus](https://arxiv.org/abs/2504.12723)
*James Hale,Sushrita Rakshit,Kushal Chawla,Jeanne M. Brett,Jonathan Gratch*

Main category: cs.CL

TL;DR: KODIS是一个包含数千个对话的二元争议解决语料库，覆盖75个国家，旨在研究文化和冲突理论。


<details>
  <summary>Details</summary>
Motivation: 基于文化和冲突的理论模型，研究情绪表达和冲突升级的关系。

Method: 参与者参与客户服务争议对话，设计用于引发强烈情绪和冲突，语料库包含多种测量指标。

Result: 初步分析支持愤怒表达导致冲突升级的理论，并揭示了情绪表达的文化差异。

Conclusion: KODIS语料库和数据收集框架为研究社区提供了宝贵资源。

Abstract: We present KODIS, a dyadic dispute resolution corpus containing thousands of
dialogues from over 75 countries. Motivated by a theoretical model of culture
and conflict, participants engage in a typical customer service dispute
designed by experts to evoke strong emotions and conflict. The corpus contains
a rich set of dispositional, process, and outcome measures. The initial
analysis supports theories of how anger expressions lead to escalatory spirals
and highlights cultural differences in emotional expression. We make this
corpus and data collection framework available to the community.

</details>


### [61] [Pandora: A Code-Driven Large Language Model Agent for Unified Reasoning Across Diverse Structured Knowledge](https://arxiv.org/abs/2504.12734)
*Yongrui Chen,Junhao He,Linbo Fu,Shenyu Zhang,Rihui Jin,Xinbang Dai,Jiaqi Li,Dehai Min,Nan Hu,Yuxin Zhang,Guilin Qi,Yi Huang,Tongtong Wu*

Main category: cs.CL

TL;DR: Pandora是一个统一的框架，利用Python的Pandas API构建知识表示，结合LLM生成推理步骤和可执行代码，提升跨任务知识迁移和性能。


<details>
  <summary>Details</summary>
Motivation: 现有USKR方法依赖任务特定策略或自定义表示，难以实现跨任务知识迁移或与LLM对齐，限制了性能。

Method: 提出Pandora框架，利用Pandas API构建统一知识表示，结合LLM生成推理步骤和代码，并通过训练示例促进知识迁移。

Result: 在四个基准测试中，Pandora优于现有统一框架，并与任务特定方法竞争。

Conclusion: Pandora通过统一表示和LLM结合，有效提升了USKR任务的性能和跨任务知识迁移能力。

Abstract: Unified Structured Knowledge Reasoning (USKR) aims to answer natural language
questions (NLQs) by using structured sources such as tables, databases, and
knowledge graphs in a unified way. Existing USKR methods either rely on
employing task-specific strategies or custom-defined representations, which
struggle to leverage the knowledge transfer between different SKR tasks or
align with the prior of LLMs, thereby limiting their performance. This paper
proposes a novel USKR framework named \textsc{Pandora}, which takes advantage
of \textsc{Python}'s \textsc{Pandas} API to construct a unified knowledge
representation for alignment with LLM pre-training. It employs an LLM to
generate textual reasoning steps and executable Python code for each question.
Demonstrations are drawn from a memory of training examples that cover various
SKR tasks, facilitating knowledge transfer. Extensive experiments on four
benchmarks involving three SKR tasks demonstrate that \textsc{Pandora}
outperforms existing unified frameworks and competes effectively with
task-specific methods.

</details>


### [62] [Chinese-Vicuna: A Chinese Instruction-following Llama-based Model](https://arxiv.org/abs/2504.12737)
*Chenghao Fan,Zhenyi Lu,Jie Tian*

Main category: cs.CL

TL;DR: Chinese-Vicuna是一个开源、资源高效的中文语言模型，通过LoRA微调LLaMA架构，填补中文指令跟随能力的空白，支持低资源环境和领域特定应用。


<details>
  <summary>Details</summary>
Motivation: 解决中文指令跟随能力的不足，并支持在低资源环境（如消费级GPU）下的高效部署。

Method: 采用LoRA微调LLaMA架构，结合混合数据集（BELLE和Guanaco）和4位量化（QLoRA），支持领域特定适应。

Result: 在翻译、代码生成和领域特定问答等任务中表现优异，尤其在医疗任务、多轮对话和法律实时更新方面。

Conclusion: Chinese-Vicuna的模块化设计和开源生态系统为中文LLM应用提供了多功能基础。

Abstract: Chinese-Vicuna is an open-source, resource-efficient language model designed
to bridge the gap in Chinese instruction-following capabilities by fine-tuning
Meta's LLaMA architecture using Low-Rank Adaptation (LoRA). Targeting
low-resource environments, it enables cost-effective deployment on consumer
GPUs (e.g., RTX-2080Ti for 7B models) and supports domain-specific adaptation
in fields like healthcare and law. By integrating hybrid datasets (BELLE and
Guanaco) and 4-bit quantization (QLoRA), the model achieves competitive
performance in tasks such as translation, code generation, and domain-specific
Q\&A. The project provides a comprehensive toolkit for model conversion, CPU
inference, and multi-turn dialogue interfaces, emphasizing accessibility for
researchers and developers. Evaluations indicate competitive performance across
medical tasks, multi-turn dialogue coherence, and real-time legal updates.
Chinese-Vicuna's modular design, open-source ecosystem, and community-driven
enhancements position it as a versatile foundation for Chinese LLM
applications.

</details>


### [63] [Out of Sight Out of Mind, Out of Sight Out of Mind: Measuring Bias in Language Models Against Overlooked Marginalized Groups in Regional Contexts](https://arxiv.org/abs/2504.12767)
*Fatma Elsafoury,David Hartmann*

Main category: cs.CL

TL;DR: 该论文研究了语言模型（LMs）对边缘群体的偏见问题，特别关注了阿拉伯国家、德国、英国和美国中的270个边缘群体，并探讨了低资源语言和方言对偏见研究的影响。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注英语世界的偏见问题，忽视了全球范围内的边缘群体和低资源语言。本文旨在填补这一空白，推动更具包容性的语言模型发展。

Method: 研究了23个语言模型对270个边缘群体的偏见，比较了埃及阿拉伯方言与现代标准阿拉伯语对偏见测量的影响。

Result: 发现语言模型对许多边缘群体的偏见高于主导群体，但阿拉伯语言模型在宗教和种族方面对两类群体均表现出高偏见。此外，对非二元性别、LGBTQIA+和黑人女性的交叉偏见更高。

Conclusion: 当前偏见指标存在局限性，需扩展研究范围以涵盖更多边缘群体和低资源语言，以实现更具包容性的语言模型。

Abstract: We know that language models (LMs) form biases and stereotypes of minorities,
leading to unfair treatments of members of these groups, thanks to research
mainly in the US and the broader English-speaking world. As the negative
behavior of these models has severe consequences for society and individuals,
industry and academia are actively developing methods to reduce the bias in
LMs. However, there are many under-represented groups and languages that have
been overlooked so far. This includes marginalized groups that are specific to
individual countries and regions in the English speaking and Western world, but
crucially also almost all marginalized groups in the rest of the world. The UN
estimates, that between 600 million to 1.2 billion people worldwide are members
of marginalized groups and in need for special protection. If we want to
develop inclusive LMs that work for everyone, we have to broaden our
understanding to include overlooked marginalized groups and low-resource
languages and dialects.
  In this work, we contribute to this effort with the first study investigating
offensive stereotyping bias in 23 LMs for 270 marginalized groups from Egypt,
the remaining 21 Arab countries, Germany, the UK, and the US. Additionally, we
investigate the impact of low-resource languages and dialects on the study of
bias in LMs, demonstrating the limitations of current bias metrics, as we
measure significantly higher bias when using the Egyptian Arabic dialect versus
Modern Standard Arabic. Our results show, LMs indeed show higher bias against
many marginalized groups in comparison to dominant groups. However, this is not
the case for Arabic LMs, where the bias is high against both marginalized and
dominant groups in relation to religion and ethnicity.
  Our results also show higher intersectional bias against Non-binary, LGBTQIA+
and Black women.

</details>


### [64] [Enhancing the Geometric Problem-Solving Ability of Multimodal LLMs via Symbolic-Neural Integration](https://arxiv.org/abs/2504.12773)
*Yicheng Pan,Zhenrong Zhang,Pengfei Hu,Jiefeng Ma,Jun Du,Jianshu Zhang,Quan Liu,Jianqing Gao,Feng Ma*

Main category: cs.CL

TL;DR: GeoGen通过自动生成几何问题的逐步推理路径，结合符号推理生成高质量数据，并训练GeoLogic模型以提升MLLMs的几何推理能力，减少幻觉。


<details>
  <summary>Details</summary>
Motivation: 解决MLLMs在几何问题解决（GPS）中缺乏逐步推理数据和严重幻觉的问题。

Method: 提出GeoGen管道自动生成推理路径，利用符号推理生成大规模高质量数据，并训练GeoLogic模型结合自然语言与符号系统。

Result: 实验表明，该方法显著提升了MLLMs在几何推理任务中的性能。

Conclusion: 结合LLMs和符号系统的优势，提供了一种更可靠且可解释的GPS任务解决方案。

Abstract: Recent advances in Multimodal Large Language Models (MLLMs) have achieved
remarkable progress in general domains and demonstrated promise in multimodal
mathematical reasoning. However, applying MLLMs to geometry problem solving
(GPS) remains challenging due to lack of accurate step-by-step solution data
and severe hallucinations during reasoning. In this paper, we propose GeoGen, a
pipeline that can automatically generates step-wise reasoning paths for
geometry diagrams. By leveraging the precise symbolic reasoning,
\textbf{GeoGen} produces large-scale, high-quality question-answer pairs. To
further enhance the logical reasoning ability of MLLMs, we train
\textbf{GeoLogic}, a Large Language Model (LLM) using synthetic data generated
by GeoGen. Serving as a bridge between natural language and symbolic systems,
GeoLogic enables symbolic tools to help verifying MLLM outputs, making the
reasoning process more rigorous and alleviating hallucinations. Experimental
results show that our approach consistently improves the performance of MLLMs,
achieving remarkable results on benchmarks for geometric reasoning tasks. This
improvement stems from our integration of the strengths of LLMs and symbolic
systems, which enables a more reliable and interpretable approach for the GPS
task. Codes are available at https://github.com/ycpNotFound/GeoGen.

</details>


### [65] [Assesing LLMs in Art Contexts: Critique Generation and Theory of Mind Evaluation](https://arxiv.org/abs/2504.12805)
*Takaya Arita,Wenxian Zheng,Reiji Suzuki,Fuminori Akiba*

Main category: cs.CL

TL;DR: 研究探讨了大型语言模型（LLMs）在艺术评论和心智理论（ToM）任务中的表现，发现通过精心设计的提示，LLMs能生成风格逼真且内容丰富的艺术评论，并在复杂情境中展现类似理解的行为。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在艺术评论和心智理论任务中的能力，以评估其是否能在复杂情境中表现出类似人类的理解行为。

Method: 结合Noel Carroll的评价框架与多种艺术批评理论，生成艺术评论并进行图灵测试；设计新的ToM任务，测试41种LLMs在不同情境中的表现。

Result: LLMs生成的艺术评论在风格和内容上接近人类专家；在ToM任务中，不同模型表现不一，情感或模糊情境下差异更明显。

Conclusion: LLMs在精心指导下能表现出接近理解的行为，但其认知潜力与局限性并存，未完全否定生成式AI悖论。

Abstract: This study explored how large language models (LLMs) perform in two areas
related to art: writing critiques of artworks and reasoning about mental states
(Theory of Mind, or ToM) in art-related situations. For the critique generation
part, we built a system that combines Noel Carroll's evaluative framework with
a broad selection of art criticism theories. The model was prompted to first
write a full-length critique and then shorter, more coherent versions using a
step-by-step prompting process. These AI-generated critiques were then compared
with those written by human experts in a Turing test-style evaluation. In many
cases, human subjects had difficulty telling which was which, and the results
suggest that LLMs can produce critiques that are not only plausible in style
but also rich in interpretation, as long as they are carefully guided. In the
second part, we introduced new simple ToM tasks based on situations involving
interpretation, emotion, and moral tension, which can appear in the context of
art. These go beyond standard false-belief tests and allow for more complex,
socially embedded forms of reasoning. We tested 41 recent LLMs and found that
their performance varied across tasks and models. In particular, tasks that
involved affective or ambiguous situations tended to reveal clearer
differences. Taken together, these results help clarify how LLMs respond to
complex interpretative challenges, revealing both their cognitive limitations
and potential. While our findings do not directly contradict the so-called
Generative AI Paradox--the idea that LLMs can produce expert-like output
without genuine understanding--they suggest that, depending on how LLMs are
instructed, such as through carefully designed prompts, these models may begin
to show behaviors that resemble understanding more closely than we might
assume.

</details>


### [66] [SMARTe: Slot-based Method for Accountable Relational Triple extraction](https://arxiv.org/abs/2504.12816)
*Xue Wen Tan,Stanley Kok*

Main category: cs.CL

TL;DR: SMARTe是一种基于槽位注意力的关系三元组提取方法，强调可解释性，性能与最先进模型相当。


<details>
  <summary>Details</summary>
Motivation: 现有方法过于关注模型性能优化，缺乏对内部机制的理解，且依赖复杂预处理导致系统不透明。

Method: 提出SMARTe，通过槽位注意力机制将任务转化为集合预测问题，确保预测可追溯。

Result: 在NYT和WebNLG数据集上验证，SMARTe在保持性能的同时提供可解释性。

Conclusion: SMARTe为关系三元组提取提供了可解释的解决方案，并提出了未来研究方向。

Abstract: Relational Triple Extraction (RTE) is a fundamental task in Natural Language
Processing (NLP). However, prior research has primarily focused on optimizing
model performance, with limited efforts to understand the internal mechanisms
driving these models. Many existing methods rely on complex preprocessing to
induce specific interactions, often resulting in opaque systems that may not
fully align with their theoretical foundations. To address these limitations,
we propose SMARTe: a Slot-based Method for Accountable Relational Triple
extraction. SMARTe introduces intrinsic interpretability through a slot
attention mechanism and frames the task as a set prediction problem. Slot
attention consolidates relevant information into distinct slots, ensuring all
predictions can be explicitly traced to learned slot representations and the
tokens contributing to each predicted relational triple. While emphasizing
interpretability, SMARTe achieves performance comparable to state-of-the-art
models. Evaluations on the NYT and WebNLG datasets demonstrate that adding
interpretability does not compromise performance. Furthermore, we conducted
qualitative assessments to showcase the explanations provided by SMARTe, using
attention heatmaps that map to their respective tokens. We conclude with a
discussion of our findings and propose directions for future research.

</details>


### [67] [Can LLMs reason over extended multilingual contexts? Towards long-context evaluation beyond retrieval and haystacks](https://arxiv.org/abs/2504.12845)
*Amey Hengle,Prasoon Bajpai,Soham Dan,Tanmoy Chakraborty*

Main category: cs.CL

TL;DR: MLRBench是一个新的多语言长上下文推理基准测试，超越了现有的检索中心测试，评估多跳推理、聚合和认知推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试仅关注检索能力，忽视了推理能力，且易受数据泄漏等问题影响。

Method: 引入MLRBench，涵盖七种语言，设计并行、抗泄漏且可扩展的任务。

Result: 实验显示高低资源语言间存在显著差距，LLMs仅利用不到30%的上下文长度。

Conclusion: MLRBench开源以促进多语言LLMs的评估和训练研究。

Abstract: Existing multilingual long-context benchmarks, often based on the popular
needle-in-a-haystack test, primarily evaluate a model's ability to locate
specific information buried within irrelevant texts. However, such a
retrieval-centric approach is myopic and inherently limited, as successful
recall alone does not indicate a model's capacity to reason over extended
contexts. Moreover, these benchmarks are susceptible to data leakage,
short-circuiting, and risk making the evaluation a priori identifiable. To
address these limitations, we introduce MLRBench, a new synthetic benchmark for
multilingual long-context reasoning. Unlike existing benchmarks, MLRBench goes
beyond surface-level retrieval by including tasks that assess multi-hop
inference, aggregation, and epistemic reasoning. Spanning seven languages,
MLRBench is designed to be parallel, resistant to leakage, and scalable to
arbitrary context lengths. Our extensive experiments with an open-weight large
language model (LLM) reveal a pronounced gap between high- and low-resource
languages, particularly for tasks requiring the model to aggregate multiple
facts or predict the absence of information. We also find that, in multilingual
settings, LLMs effectively utilize less than 30% of their claimed context
length. Although off-the-shelf Retrieval Augmented Generation helps alleviate
this to a certain extent, it does not solve the long-context problem. We
open-source MLRBench to enable future research in improved evaluation and
training of multilingual LLMs.

</details>


### [68] [ViClaim: A Multilingual Multilabel Dataset for Automatic Claim Detection in Videos](https://arxiv.org/abs/2504.12882)
*Patrick Giedemann,Pius von Däniken,Jan Deriu,Alvaro Rodrigo,Anselmo Peñas,Mark Cieliebak*

Main category: cs.CL

TL;DR: ViClaim是一个多语言、多主题的视频转录数据集，用于检测视频中的虚假信息，填补了现有研究在口语文本分析上的空白。


<details>
  <summary>Details</summary>
Motivation: 视频内容作为传播和虚假信息的媒介日益重要，但现有研究主要关注书面文本，忽略了视频转录的复杂性。

Method: 构建了包含1,798个标注视频转录的ViClaim数据集，涵盖三种语言和六个主题，并开发了定制标注工具。使用多语言语言模型进行实验。

Result: 模型在交叉验证中表现良好（宏F1达0.896），但在未见主题上泛化能力有限。

Conclusion: ViClaim为视频虚假信息检测提供了坚实基础，但跨领域泛化仍需改进。

Abstract: The growing influence of video content as a medium for communication and
misinformation underscores the urgent need for effective tools to analyze
claims in multilingual and multi-topic settings. Existing efforts in
misinformation detection largely focus on written text, leaving a significant
gap in addressing the complexity of spoken text in video transcripts. We
introduce ViClaim, a dataset of 1,798 annotated video transcripts across three
languages (English, German, Spanish) and six topics. Each sentence in the
transcripts is labeled with three claim-related categories: fact-check-worthy,
fact-non-check-worthy, or opinion. We developed a custom annotation tool to
facilitate the highly complex annotation process. Experiments with
state-of-the-art multilingual language models demonstrate strong performance in
cross-validation (macro F1 up to 0.896) but reveal challenges in generalization
to unseen topics, particularly for distinct domains. Our findings highlight the
complexity of claim detection in video transcripts. ViClaim offers a robust
foundation for advancing misinformation detection in video-based communication,
addressing a critical gap in multimodal analysis.

</details>


### [69] [Are AI agents the new machine translation frontier? Challenges and opportunities of single- and multi-agent systems for multilingual digital communication](https://arxiv.org/abs/2504.12891)
*Vicent Briva-Iglesias*

Main category: cs.CL

TL;DR: 论文探讨了单智能体和多智能体系统在机器翻译（MT）中的潜力，重点分析了多智能体系统在复杂场景下的优势。


<details>
  <summary>Details</summary>
Motivation: 人工智能（AI）的快速发展为机器翻译带来了新的可能性，但多智能体系统的应用尚未充分探索。

Method: 通过法律机器翻译的试点研究，采用多智能体系统（包括翻译、充分性审查、流畅性审查和最终编辑四个专业AI代理）验证其可行性。

Result: 研究发现，多智能体系统在领域适应性和上下文意识方面表现优异，翻译质量优于传统MT或单智能体系统。

Conclusion: 多智能体系统在机器翻译中具有显著潜力，为未来研究和专业翻译工作流程的集成奠定了基础。

Abstract: The rapid evolution of artificial intelligence (AI) has introduced AI agents
as a disruptive paradigm across various industries, yet their application in
machine translation (MT) remains underexplored. This paper describes and
analyses the potential of single- and multi-agent systems for MT, reflecting on
how they could enhance multilingual digital communication. While single-agent
systems are well-suited for simpler translation tasks, multi-agent systems,
which involve multiple specialized AI agents collaborating in a structured
manner, may offer a promising solution for complex scenarios requiring high
accuracy, domain-specific knowledge, and contextual awareness. To demonstrate
the feasibility of multi-agent workflows in MT, we are conducting a pilot study
in legal MT. The study employs a multi-agent system involving four specialized
AI agents for (i) translation, (ii) adequacy review, (iii) fluency review, and
(iv) final editing. Our findings suggest that multi-agent systems may have the
potential to significantly improve domain-adaptability and contextual
awareness, with superior translation quality to traditional MT or single-agent
systems. This paper also sets the stage for future research into multi-agent
applications in MT, integration into professional translation workflows, and
shares a demo of the system analyzed in the paper.

</details>


### [70] [Information Gain-Guided Causal Intervention for Autonomous Debiasing Large Language Models](https://arxiv.org/abs/2504.12898)
*Zhouhao Sun,Xiao Ding,Li Du,Yunpeng Xu,Yixuan Ma,Yang Zhao,Bing Qin,Ting Liu*

Main category: cs.CL

TL;DR: 论文提出了一种基于信息增益引导的因果干预去偏框架（IGCIDB），通过结合因果机制和信息理论，自动平衡指令调优数据集的分布，从而提升大语言模型的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLMs）在推理过程中可能捕获数据集偏见，导致泛化能力不足。现有的去偏方法（如先验知识去偏和上下文学习去偏）效果有限。

Method: 提出IGCIDB框架，利用信息增益引导的因果干预方法自动平衡数据集分布，随后通过标准监督微调训练LLMs。

Result: 实验表明，IGCIDB能有效去偏，提升LLMs在不同任务中的泛化能力。

Conclusion: IGCIDF框架为解决LLMs的偏见问题提供了一种有效方法，显著提升了模型的泛化性能。

Abstract: Despite significant progress, recent studies indicate that current large
language models (LLMs) may still capture dataset biases and utilize them during
inference, leading to the poor generalizability of LLMs. However, due to the
diversity of dataset biases and the insufficient nature of bias suppression
based on in-context learning, the effectiveness of previous prior
knowledge-based debiasing methods and in-context learning based automatic
debiasing methods is limited. To address these challenges, we explore the
combination of causal mechanisms with information theory and propose an
information gain-guided causal intervention debiasing (IGCIDB) framework. This
framework first utilizes an information gain-guided causal intervention method
to automatically and autonomously balance the distribution of
instruction-tuning dataset. Subsequently, it employs a standard supervised
fine-tuning process to train LLMs on the debiased dataset. Experimental results
show that IGCIDB can effectively debias LLM to improve its generalizability
across different tasks.

</details>


### [71] [Benchmarking Multi-National Value Alignment for Large Language Models](https://arxiv.org/abs/2504.12911)
*Chengyi Ju,Weijie Shi,Chengzhong Liu,Jiaming Ji,Jipeng Zhang,Ruiyuan Zhang,Jia Zhu,Jiajie Xu,Yaodong Yang,Sirui Han,Yike Guo*

Main category: cs.CL

TL;DR: NaVAB是一个评估大型语言模型（LLM）与国家价值观对齐的基准，通过自动化流程构建数据集，并展示其与对齐技术结合的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注伦理审查，忽略了国家价值观的多样性，且现有基准难以扩展。

Method: 提出NaVAB基准，包括国家价值观提取流程、指令标记建模、筛选过程和冲突减少机制。

Result: 实验表明NaVAB能有效识别LLM与国家价值观的不对齐情况，并可通过对齐技术减少冲突。

Conclusion: NaVAB为评估和改善LLM与国家价值观对齐提供了实用工具。

Abstract: Do Large Language Models (LLMs) hold positions that conflict with your
country's values? Occasionally they do! However, existing works primarily focus
on ethical reviews, failing to capture the diversity of national values, which
encompass broader policy, legal, and moral considerations. Furthermore, current
benchmarks that rely on spectrum tests using manually designed questionnaires
are not easily scalable.
  To address these limitations, we introduce NaVAB, a comprehensive benchmark
to evaluate the alignment of LLMs with the values of five major nations: China,
the United States, the United Kingdom, France, and Germany. NaVAB implements a
national value extraction pipeline to efficiently construct value assessment
datasets. Specifically, we propose a modeling procedure with instruction
tagging to process raw data sources, a screening process to filter
value-related topics and a generation process with a Conflict Reduction
mechanism to filter non-conflicting values.We conduct extensive experiments on
various LLMs across countries, and the results provide insights into assisting
in the identification of misaligned scenarios. Moreover, we demonstrate that
NaVAB can be combined with alignment techniques to effectively reduce value
concerns by aligning LLMs' values with the target country.

</details>


### [72] [MAIN: Mutual Alignment Is Necessary for instruction tuning](https://arxiv.org/abs/2504.12913)
*Fanyi Yang,Jianfeng Liu,Xin Zhang,Haoyu Liu,Xixin Cao,Yuefeng Zhan,Hao Sun,Weiwei Deng,Feng Sun,Qi Zhang*

Main category: cs.CL

TL;DR: 论文提出了一种名为MAIN的互对齐框架，通过约束指令与响应的对齐性，提升了指令调优的质量。实验证明该方法优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 当前指令调优方法忽视了指令与响应之间的对齐性，而高质量的对齐对模型性能至关重要。

Method: 提出互对齐框架（MAIN），通过相互约束确保指令与响应的连贯性。

Result: 实验显示，基于MAIN调优的LLaMA和Mistral模型在多个基准测试中表现优于传统方法。

Conclusion: 指令与响应的对齐性对高质量指令调优具有关键作用，MAIN框架为此提供了有效解决方案。

Abstract: Instruction tuning has enabled large language models (LLMs) to achieve
remarkable performance, but its success heavily depends on the availability of
large-scale, high-quality instruction-response pairs. However, current methods
for scaling up data generation often overlook a crucial aspect: the alignment
between instructions and responses. We hypothesize that high-quality
instruction-response pairs are not defined by the individual quality of each
component, but by the extent of their alignment with each other. To address
this, we propose a Mutual Alignment Framework (MAIN) that ensures coherence
between the instruction and response through mutual constraints. Experiments
demonstrate that models such as LLaMA and Mistral, fine-tuned within this
framework, outperform traditional methods across multiple benchmarks. This
approach underscores the critical role of instruction-response alignment in
enabling scalable and high-quality instruction tuning for LLMs.

</details>


### [73] [ConExion: Concept Extraction with Large Language Models](https://arxiv.org/abs/2504.12915)
*Ebrahim Norouzi,Sven Hertling,Harald Sack*

Main category: cs.CL

TL;DR: 提出一种基于预训练大语言模型（LLMs）的概念提取方法，优于传统关键词提取技术，并在无监督任务中探索提示的潜力。


<details>
  <summary>Details</summary>
Motivation: 传统方法仅提取文档中的重要关键词，而本研究旨在提取特定领域的所有相关概念，以支持本体评估和学习。

Method: 使用预训练LLMs进行概念提取，并通过提示探索无监督任务。

Result: 在两个基准数据集上，F1分数优于现有技术。

Conclusion: LLMs在概念提取任务中表现优异，支持本体覆盖评估和学习，代码和数据已公开。

Abstract: In this paper, an approach for concept extraction from documents using
pre-trained large language models (LLMs) is presented. Compared with
conventional methods that extract keyphrases summarizing the important
information discussed in a document, our approach tackles a more challenging
task of extracting all present concepts related to the specific domain, not
just the important ones. Through comprehensive evaluations of two widely used
benchmark datasets, we demonstrate that our method improves the F1 score
compared to state-of-the-art techniques. Additionally, we explore the potential
of using prompts within these models for unsupervised concept extraction. The
extracted concepts are intended to support domain coverage evaluation of
ontologies and facilitate ontology learning, highlighting the effectiveness of
LLMs in concept extraction tasks. Our source code and datasets are publicly
available at https://github.com/ISE-FIZKarlsruhe/concept_extraction.

</details>


### [74] [Are Retrials All You Need? Enhancing Large Language Model Reasoning Without Verbalized Feedback](https://arxiv.org/abs/2504.12951)
*Nearchos Potamitis,Akhil Arora*

Main category: cs.CL

TL;DR: 论文提出了一种无需反馈的“重试”机制，简化了大型语言模型的推理框架，证明简单方法可能优于复杂方法。


<details>
  <summary>Details</summary>
Motivation: 当前迭代推理策略需要额外的计算成本和复杂性，作者希望探索更简单高效的替代方案。

Method: 引入“无反馈重试”机制，允许模型在识别错误答案后重新尝试，无需显式自我反思或反馈。

Result: 研究发现，简单的重试方法常优于复杂推理框架，且计算成本更低。

Conclusion: 挑战了复杂方法必然更优的假设，表明简单高效的方法也能取得最佳效果。

Abstract: Recent advancements in large language models (LLMs) have catalyzed the
development of general-purpose autonomous agents, demonstrating remarkable
performance in complex reasoning tasks across various domains. This surge has
spurred the evolution of a plethora of prompt-based reasoning frameworks. A
recent focus has been on iterative reasoning strategies that refine outputs
through self-evaluation and verbalized feedback. However, these strategies
require additional computational complexity to enable models to recognize and
correct their mistakes, leading to a significant increase in their cost. In
this work, we introduce the concept of ``retrials without feedback'', an
embarrassingly simple yet powerful mechanism for enhancing reasoning frameworks
by allowing LLMs to retry problem-solving attempts upon identifying incorrect
answers. Unlike conventional iterative refinement methods, our method does not
require explicit self-reflection or verbalized feedback, simplifying the
refinement process. Our findings indicate that simpler retrial-based approaches
often outperform more sophisticated reasoning frameworks, suggesting that the
benefits of complex methods may not always justify their computational costs.
By challenging the prevailing assumption that more intricate reasoning
strategies inherently lead to better performance, our work offers new insights
into how simpler, more efficient approaches can achieve optimal results. So,
are retrials all you need?

</details>


### [75] [Estimating Optimal Context Length for Hybrid Retrieval-augmented Multi-document Summarization](https://arxiv.org/abs/2504.12972)
*Adithya Pratapa,Teruko Mitamura*

Main category: cs.CL

TL;DR: 提出了一种结合检索增强系统与长上下文窗口的混合方法，用于多文档摘要任务，通过估计最优检索长度提升性能。


<details>
  <summary>Details</summary>
Motivation: 长上下文语言模型在多文档摘要中表现不佳，检索增强系统对检索长度敏感，需找到最优解决方案。

Method: 结合检索增强与长上下文模型，通过LLM生成银参考估计最优检索长度。

Result: 在多文档摘要任务中表现优异，优于RULER和HELMET等基准。

Conclusion: 该方法有效且可推广至新型长上下文语言模型。

Abstract: Recent advances in long-context reasoning abilities of language models led to
interesting applications in large-scale multi-document summarization. However,
prior work has shown that these long-context models are not effective at their
claimed context windows. To this end, retrieval-augmented systems provide an
efficient and effective alternative. However, their performance can be highly
sensitive to the choice of retrieval context length. In this work, we present a
hybrid method that combines retrieval-augmented systems with long-context
windows supported by recent language models. Our method first estimates the
optimal retrieval length as a function of the retriever, summarizer, and
dataset. On a randomly sampled subset of the dataset, we use a panel of LLMs to
generate a pool of silver references. We use these silver references to
estimate the optimal context length for a given RAG system configuration. Our
results on the multi-document summarization task showcase the effectiveness of
our method across model classes and sizes. We compare against length estimates
from strong long-context benchmarks such as RULER and HELMET. Our analysis also
highlights the effectiveness of our estimation method for very long-context LMs
and its generalization to new classes of LMs.

</details>


### [76] [Sparks of Science: Hypothesis Generation Using Structured Paper Data](https://arxiv.org/abs/2504.12976)
*Charles O'Neill,Tirthankar Ghosal,Roberta Răileanu,Mike Walmsley,Thang Bui,Kevin Schawinski,Ioana Ciucă*

Main category: cs.CL

TL;DR: HypoGen是首个用于科学假设生成（SHG）的数据集，包含5500个结构化问题-假设对，采用Bit-Flip-Spark框架，并整合了推理链，显著提升了生成假设的新颖性和可行性。


<details>
  <summary>Details</summary>
Motivation: 当前基础模型在生成新颖且可行的科学假设方面表现不佳，缺乏专门的数据集将SHG任务形式化为自然语言生成（NLG）。

Method: 引入HypoGen数据集，基于Bit-Flip-Spark框架和推理链，将假设生成建模为条件语言模型。

Result: 通过HypoGen数据集微调，生成的假设在新颖性、可行性和整体质量上均有提升。

Conclusion: HypoGen为科学假设生成提供了有效工具，推动了AI在科学创新中的应用。

Abstract: Generating novel and creative scientific hypotheses is a cornerstone in
achieving Artificial General Intelligence. Large language and reasoning models
have the potential to aid in the systematic creation, selection, and validation
of scientifically informed hypotheses. However, current foundation models often
struggle to produce scientific ideas that are both novel and feasible. One
reason is the lack of a dedicated dataset that frames Scientific Hypothesis
Generation (SHG) as a Natural Language Generation (NLG) task. In this paper, we
introduce HypoGen, the first dataset of approximately 5500 structured
problem-hypothesis pairs extracted from top-tier computer science conferences
structured with a Bit-Flip-Spark schema, where the Bit is the conventional
assumption, the Spark is the key insight or conceptual leap, and the Flip is
the resulting counterproposal. HypoGen uniquely integrates an explicit
Chain-of-Reasoning component that reflects the intellectual process from Bit to
Flip. We demonstrate that framing hypothesis generation as conditional language
modelling, with the model fine-tuned on Bit-Flip-Spark and the
Chain-of-Reasoning (and where, at inference, we only provide the Bit), leads to
improvements in the overall quality of the hypotheses. Our evaluation employs
automated metrics and LLM judge rankings for overall quality assessment. We
show that by fine-tuning on our HypoGen dataset we improve the novelty,
feasibility, and overall quality of the generated hypotheses. The HypoGen
dataset is publicly available at
huggingface.co/datasets/UniverseTBD/hypogen-dr1.

</details>


### [77] [Accommodate Knowledge Conflicts in Retrieval-augmented LLMs: Towards Reliable Response Generation in the Wild](https://arxiv.org/abs/2504.12982)
*Jiatai Wang,Zhiwei Xu,Di Jin,Xuewen Yang,Tao Li*

Main category: cs.CL

TL;DR: 论文分析了大型语言模型（LLMs）在知识冲突中的行为，提出了一种新框架Swin-VIB，通过变分信息瓶颈模型优化信息检索和响应生成，显著提升了任务准确性。


<details>
  <summary>Details</summary>
Motivation: LLMs在信息检索系统中面临知识冲突问题，导致响应不可靠和决策不确定性。研究旨在揭示LLMs如何处理这些冲突，并提出解决方案。

Method: 从信息论角度分析LLMs的知识冲突行为，提出Swin-VIB框架，结合变分信息瓶颈模型，自适应增强检索信息并指导LLM响应生成。

Result: 在单选择、开放式问答和检索增强生成任务中验证了Swin-VIB的有效性，单选择任务准确性提升至少7.54%。

Conclusion: Swin-VIB能有效解决LLMs的知识冲突问题，提升响应生成的可靠性。

Abstract: The proliferation of large language models (LLMs) has significantly advanced
information retrieval systems, particularly in response generation (RG).
Unfortunately, LLMs often face knowledge conflicts between internal memory and
retrievaled external information, arising from misinformation, biases, or
outdated knowledge. These conflicts undermine response reliability and
introduce uncertainty in decision-making. In this work, we analyze how LLMs
navigate knowledge conflicts from an information-theoretic perspective and
reveal that when conflicting and supplementary information exhibit significant
differences, LLMs confidently resolve their preferences. However, when the
distinction is ambiguous, LLMs experience heightened uncertainty. Based on this
insight, we propose Swin-VIB, a novel framework that integrates a pipeline of
variational information bottleneck models into adaptive augmentation of
retrieved information and guiding LLM preference in response generation.
Extensive experiments on single-choice, open-ended question-answering (QA), and
retrieval augmented generation (RAG) validate our theoretical findings and
demonstrate the efficacy of Swin-VIB. Notably, our method improves
single-choice task accuracy by at least 7.54\% over competitive baselines.

</details>


### [78] [SHA256 at SemEval-2025 Task 4: Selective Amnesia -- Constrained Unlearning for Large Language Models via Knowledge Isolation](https://arxiv.org/abs/2504.12996)
*Saransh Agrawal,Kuan-Hao Huang*

Main category: cs.CL

TL;DR: 本文提出了一种针对LLMs的定向遗忘方法，通过因果中介分析和分层优化，解决了现有方法在选择性删除数据关联时的性能下降问题。


<details>
  <summary>Details</summary>
Motivation: LLMs在训练中常记忆敏感信息，现有遗忘方法难以在不影响模型整体能力的情况下选择性删除特定数据关联。

Method: 采用两阶段方法：因果中介分析确定关键层（0-5层），随后通过约束优化和联合损失函数对下层进行优化。

Result: 在1B模型赛道中排名第二，保持88%的基线MMLU准确性，验证了方法的有效性。

Conclusion: 因果驱动的分层优化为LLMs的高效精确遗忘提供了新思路，显著提升了AI系统中的数据隐私保护能力。

Abstract: Large language models (LLMs) frequently memorize sensitive information during
training, posing risks when deploying publicly accessible models. Current
machine unlearning methods struggle to selectively remove specific data
associations without degrading overall model capabilities. This paper presents
our solution to SemEval-2025 Task 4 on targeted unlearning, which introduces a
two-stage methodology that combines causal mediation analysis with
layer-specific optimization. Through systematic causal tracing experiments on
OLMo architectures (1B and 7B parameters), we identify the critical role of the
first few transformer layers (layers 0-5) in storing subject-attribute
associations within MLP modules. Building on this insight, we develop a
constrained optimization approach that freezes upper layers while applying a
novel joint loss function to lower layers-simultaneously maximizing forget set
loss via output token cross-entropy penalties and minimizing retain set
deviation through adaptive regularization. Our method achieves 2nd place in the
1B model track, demonstrating strong task performance while maintaining 88% of
baseline MMLU accuracy. These results establish causal-informed layer
optimization as a promising paradigm for efficient, precise unlearning in LLMs,
offering a significant step forward in addressing data privacy concerns in AI
systems.

</details>


### [79] [ChatEXAONEPath: An Expert-level Multimodal Large Language Model for Histopathology Using Whole Slide Images](https://arxiv.org/abs/2504.13023)
*Sangwook Kim,Soonyoung Lee,Jongseong Jang*

Main category: cs.CL

TL;DR: 本文提出了一种基于全切片图像（WSI）的多模态大语言模型ChatEXAONEPath，用于病理学领域，能够理解复杂的临床背景并辅助癌症诊断。


<details>
  <summary>Details</summary>
Motivation: 现有研究中的多模态大语言模型在病理学中仅能处理有限信息的补丁级数据，缺乏对全面临床背景的理解。开发WSI级模型对提升模型的扩展性和适用性至关重要。

Method: 研究采用检索式数据生成流程，基于TCGA的10,094对WSI和病理报告，并设计了基于AI的评估协议，以全面理解多模态信息。

Result: ChatEXAONEPath在1,134对WSI和报告中实现了62.9%的接受率，能够理解多种癌症类型的WSI和临床背景。

Conclusion: 该模型通过整合多模态信息，有望辅助临床医生理解复杂的WSI形态，提升癌症诊断能力。

Abstract: Recent studies have made significant progress in developing large language
models (LLMs) in the medical domain, which can answer expert-level questions
and demonstrate the potential to assist clinicians in real-world clinical
scenarios. Studies have also witnessed the importance of integrating various
modalities with the existing LLMs for a better understanding of complex
clinical contexts, which are innately multi-faceted by nature. Although studies
have demonstrated the ability of multimodal LLMs in histopathology to answer
questions from given images, they lack in understanding of thorough clinical
context due to the patch-level data with limited information from public
datasets. Thus, developing WSI-level MLLMs is significant in terms of the
scalability and applicability of MLLMs in histopathology. In this study, we
introduce an expert-level MLLM for histopathology using WSIs, dubbed as
ChatEXAONEPath. We present a retrieval-based data generation pipeline using
10,094 pairs of WSIs and histopathology reports from The Cancer Genome Atlas
(TCGA). We also showcase an AI-based evaluation protocol for a comprehensive
understanding of the medical context from given multimodal information and
evaluate generated answers compared to the original histopathology reports. We
demonstrate the ability of diagnosing the given histopathology images using
ChatEXAONEPath with the acceptance rate of 62.9% from 1,134 pairs of WSIs and
reports. Our proposed model can understand pan-cancer WSIs and clinical context
from various cancer types. We argue that our proposed model has the potential
to assist clinicians by comprehensively understanding complex morphology of
WSIs for cancer diagnosis through the integration of multiple modalities.

</details>


### [80] [Aspect-Based Summarization with Self-Aspect Retrieval Enhanced Generation](https://arxiv.org/abs/2504.13054)
*Yichao Feng,Shuai Zhao,Yueqiu Li,Luwei Xiao,Xiaobao Wu,Anh Tuan Luu*

Main category: cs.CL

TL;DR: 提出了一种基于自检索的方面摘要生成框架，通过嵌入驱动检索机制解决大语言模型在提示工程和上下文学习中的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统摘要方法资源受限且泛化能力差，大语言模型虽无需训练但依赖提示工程并面临标记限制和幻觉问题。

Method: 采用嵌入驱动检索机制提取相关文本片段，优化标记使用并删除无关内容，确保生成与给定方面严格相关的摘要。

Result: 在基准数据集上验证，框架性能优越且有效缓解标记限制问题。

Conclusion: 自检索增强摘要生成框架显著提升方面摘要任务的效果和效率。

Abstract: Aspect-based summarization aims to generate summaries tailored to specific
aspects, addressing the resource constraints and limited generalizability of
traditional summarization approaches. Recently, large language models have
shown promise in this task without the need for training. However, they rely
excessively on prompt engineering and face token limits and hallucination
challenges, especially with in-context learning. To address these challenges,
in this paper, we propose a novel framework for aspect-based summarization:
Self-Aspect Retrieval Enhanced Summary Generation. Rather than relying solely
on in-context learning, given an aspect, we employ an embedding-driven
retrieval mechanism to identify its relevant text segments. This approach
extracts the pertinent content while avoiding unnecessary details, thereby
mitigating the challenge of token limits. Moreover, our framework optimizes
token usage by deleting unrelated parts of the text and ensuring that the model
generates output strictly based on the given aspect. With extensive experiments
on benchmark datasets, we demonstrate that our framework not only achieves
superior performance but also effectively mitigates the token limitation
problem.

</details>


### [81] [Accuracy is Not Agreement: Expert-Aligned Evaluation of Crash Narrative Classification Models](https://arxiv.org/abs/2504.13068)
*Sudesh Ramesh Bhagat,Ibne Farabi Shihab,Anuj Sharma*

Main category: cs.CL

TL;DR: 研究发现，深度学习模型在技术准确性上表现更好时，与领域专家的分类一致性反而较低，而大语言模型（LLMs）尽管准确性较低，却与专家更一致。


<details>
  <summary>Details</summary>
Motivation: 探讨深度学习模型准确性与其与专家分类一致性之间的关系，以评估模型在安全关键NLP应用中的适用性。

Method: 评估五种DL模型和四种LLMs，使用专家标注数据和文本分析，采用Cohen's Kappa、PCA和SHAP技术量化模型与专家的一致性。

Result: 技术准确性高的模型与专家一致性较低，而LLMs更依赖上下文和时间线索，与专家更一致。

Conclusion: 建议将专家一致性作为模型评估的补充指标，并强调LLMs在可解释性和扩展性方面的潜力。

Abstract: This study explores the relationship between deep learning (DL) model
accuracy and expert agreement in the classification of crash narratives. We
evaluate five DL models -- including BERT variants, the Universal Sentence
Encoder (USE), and a zero-shot classifier -- against expert-labeled data and
narrative text. The analysis is further extended to four large language models
(LLMs): GPT-4, LLaMA 3, Qwen, and Claude. Our results reveal a counterintuitive
trend: models with higher technical accuracy often exhibit lower agreement with
domain experts, whereas LLMs demonstrate greater expert alignment despite
relatively lower accuracy scores. To quantify and interpret model-expert
agreement, we employ Cohen's Kappa, Principal Component Analysis (PCA), and
SHAP-based explainability techniques. Findings indicate that expert-aligned
models tend to rely more on contextual and temporal language cues, rather than
location-specific keywords. These results underscore that accuracy alone is
insufficient for evaluating models in safety-critical NLP applications. We
advocate for incorporating expert agreement as a complementary metric in model
evaluation frameworks and highlight the promise of LLMs as interpretable,
scalable tools for crash analysis pipelines.

</details>


### [82] [Retrieval-Augmented Generation with Conflicting Evidence](https://arxiv.org/abs/2504.13079)
*Han Wang,Archiki Prasad,Elias Stengel-Eskin,Mohit Bansal*

Main category: cs.CL

TL;DR: 论文提出RAMDocs数据集和MADAM-RAG方法，用于处理用户查询中的模糊性和多源冲突信息，显著提升了模型在模糊查询和抑制错误信息方面的性能。


<details>
  <summary>Details</summary>
Motivation: 现有检索增强生成（RAG）系统在处理模糊查询和多源冲突信息时表现不足，通常只关注单一问题。本文旨在同时解决模糊性、错误信息和噪声问题。

Method: 提出RAMDocs数据集模拟复杂冲突场景，并设计MADAM-RAG方法，通过多轮LLM代理辩论和聚合器整合答案，联合处理多种冲突因素。

Result: MADAM-RAG在模糊查询任务（AmbigDocs）上提升11.40%，在抑制错误信息任务（FaithEval）上提升15.80%。RAMDocs对现有RAG基线模型（如Llama3.3-70B-Instruct）构成挑战（仅32.60分）。

Conclusion: MADAM-RAG初步解决了多源冲突问题，但在证据支持不平衡时仍有较大改进空间。

Abstract: Large language model (LLM) agents are increasingly employing
retrieval-augmented generation (RAG) to improve the factuality of their
responses. However, in practice, these systems often need to handle ambiguous
user queries and potentially conflicting information from multiple sources
while also suppressing inaccurate information from noisy or irrelevant
documents. Prior work has generally studied and addressed these challenges in
isolation, considering only one aspect at a time, such as handling ambiguity or
robustness to noise and misinformation. We instead consider multiple factors
simultaneously, proposing (i) RAMDocs (Retrieval with Ambiguity and
Misinformation in Documents), a new dataset that simulates complex and
realistic scenarios for conflicting evidence for a user query, including
ambiguity, misinformation, and noise; and (ii) MADAM-RAG, a multi-agent
approach in which LLM agents debate over the merits of an answer over multiple
rounds, allowing an aggregator to collate responses corresponding to
disambiguated entities while discarding misinformation and noise, thereby
handling diverse sources of conflict jointly. We demonstrate the effectiveness
of MADAM-RAG using both closed and open-source models on AmbigDocs -- which
requires presenting all valid answers for ambiguous queries -- improving over
strong RAG baselines by up to 11.40% and on FaithEval -- which requires
suppressing misinformation -- where we improve by up to 15.80% (absolute) with
Llama3.3-70B-Instruct. Furthermore, we find that RAMDocs poses a challenge for
existing RAG baselines (Llama3.3-70B-Instruct only obtains 32.60 exact match
score). While MADAM-RAG begins to address these conflicting factors, our
analysis indicates that a substantial gap remains especially when increasing
the level of imbalance in supporting evidence and misinformation.

</details>


### [83] [LLMs Meet Finance: Fine-Tuning Foundation Models for the Open FinLLM Leaderboard](https://arxiv.org/abs/2504.13125)
*Varun Rao,Youran Sun,Mahendra Kumar,Tejas Mutneja,Agastya Mukherjee,Haizhao Yang*

Main category: cs.CL

TL;DR: 论文研究了大型语言模型（LLMs）在金融任务中的应用，通过微调基础模型并结合多种技术，显著提升了金融任务性能。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在金融领域的潜力，提升其在金融任务中的表现。

Method: 使用Open FinLLM Leaderboard作为基准，结合监督微调（SFT）、直接偏好优化（DPO）和强化学习（RL）技术微调Qwen2.5和Deepseek-R1模型。

Result: 微调后的模型在多种金融任务中表现显著提升，并测量了金融领域的数据缩放规律。

Conclusion: 研究表明LLMs在金融应用中具有巨大潜力。

Abstract: This paper investigates the application of large language models (LLMs) to
financial tasks. We fine-tuned foundation models using the Open FinLLM
Leaderboard as a benchmark. Building on Qwen2.5 and Deepseek-R1, we employed
techniques including supervised fine-tuning (SFT), direct preference
optimization (DPO), and reinforcement learning (RL) to enhance their financial
capabilities. The fine-tuned models demonstrated substantial performance gains
across a wide range of financial tasks. Moreover, we measured the data scaling
law in the financial domain. Our work demonstrates the potential of large
language models (LLMs) in financial applications.

</details>


### [84] [Energy-Based Reward Models for Robust Language Model Alignment](https://arxiv.org/abs/2504.13134)
*Anamika Lochab,Ruqi Zhang*

Main category: cs.CL

TL;DR: EBRM是一种轻量级后处理框架，通过显式建模奖励分布提升奖励模型的鲁棒性和泛化能力，无需重新训练。


<details>
  <summary>Details</summary>
Motivation: 奖励模型（RMs）难以捕捉复杂人类偏好并泛化到未见数据，EBRM旨在解决这些问题。

Method: EBRM采用冲突感知数据过滤、标签噪声感知对比训练和混合初始化，显式建模奖励分布。

Result: 实验显示EBRM在安全关键对齐任务中提升5.97%，并有效延迟奖励攻击。

Conclusion: EBRM是一种可扩展且高效的方法，能显著增强现有RMs和对齐流程。

Abstract: Reward models (RMs) are essential for aligning Large Language Models (LLMs)
with human preferences. However, they often struggle with capturing complex
human preferences and generalizing to unseen data. To address these challenges,
we introduce Energy-Based Reward Model (EBRM), a lightweight post-hoc
refinement framework that enhances RM robustness and generalization. EBRM
models the reward distribution explicitly, capturing uncertainty in human
preferences and mitigating the impact of noisy or misaligned annotations. It
achieves this through conflict-aware data filtering, label-noise-aware
contrastive training, and hybrid initialization. Notably, EBRM enhances RMs
without retraining, making it computationally efficient and adaptable across
different models and tasks. Empirical evaluations on RM benchmarks demonstrate
significant improvements in both robustness and generalization, achieving up to
a 5.97% improvement in safety-critical alignment tasks compared to standard
RMs. Furthermore, reinforcement learning experiments confirm that our refined
rewards enhance alignment quality, effectively delaying reward hacking. These
results demonstrate our approach as a scalable and effective enhancement for
existing RMs and alignment pipelines. The code is available at EBRM.

</details>


### [85] [Syntactic and Semantic Control of Large Language Models via Sequential Monte Carlo](https://arxiv.org/abs/2504.13139)
*João Loula,Benjamin LeBrun,Li Du,Ben Lipkin,Clemente Pasti,Gabriel Grand,Tianyu Liu,Yahya Emara,Marjorie Freedman,Jason Eisner,Ryan Cotterel,Vikash Mansinghka,Alexander K. Lew,Tim Vieira,Timothy J. O'Donnell*

Main category: cs.CL

TL;DR: 本文提出了一种基于序贯蒙特卡洛（SMC）的架构，用于控制语言模型生成，能够灵活地结合领域和问题特定的约束，并在生成过程中高效分配计算资源。


<details>
  <summary>Details</summary>
Motivation: 许多语言模型应用需要生成符合语法或语义约束的文本，但精确生成符合约束的分布通常难以实现。

Method: 采用序贯蒙特卡洛（SMC）框架，动态调整计算资源，并结合领域特定约束。

Result: 实验表明，该方法使小型开源语言模型在多个领域（如Python代码生成、文本到SQL等）中优于更大的模型和闭源微调模型。

Conclusion: 该方法通过更好地近似后验分布提升了性能，并提供了一个简单、可编程的框架，适用于广泛的受控生成问题。

Abstract: A wide range of LM applications require generating text that conforms to
syntactic or semantic constraints. Imposing such constraints can be naturally
framed as probabilistic conditioning, but exact generation from the resulting
distribution -- which can differ substantially from the LM's base distribution
-- is generally intractable. In this work, we develop an architecture for
controlled LM generation based on sequential Monte Carlo (SMC). Our SMC
framework allows us to flexibly incorporate domain- and problem-specific
constraints at inference time, and efficiently reallocate computational
resources in light of new information during the course of generation. By
comparing to a number of alternatives and ablations on four challenging domains
-- Python code generation for data science, text-to-SQL, goal inference, and
molecule synthesis -- we demonstrate that, with little overhead, our approach
allows small open-source language models to outperform models over 8x larger,
as well as closed-source, fine-tuned ones. In support of the probabilistic
perspective, we show that these performance improvements are driven by better
approximation to the posterior distribution. Our system builds on the framework
of Lew et al. (2023) and integrates with its language model probabilistic
programming language, giving users a simple, programmable way to apply SMC to a
broad variety of controlled generation problems.

</details>


### [86] [CLIMB: CLustering-based Iterative Data Mixture Bootstrapping for Language Model Pre-training](https://arxiv.org/abs/2504.13161)
*Shizhe Diao,Yu Yang,Yonggan Fu,Xin Dong,Dan Su,Markus Kliegl,Zijia Chen,Peter Belcak,Yoshi Suhara,Hongxu Yin,Mostofa Patwary,Yingyan,Lin,Jan Kautz,Pavlo Molchanov*

Main category: cs.CL

TL;DR: CLIMB是一种自动化框架，通过聚类和迭代优化数据混合，提升预训练性能。1B模型在400B tokens训练下超越现有技术2%，特定领域优化提升5%。


<details>
  <summary>Details</summary>
Motivation: 预训练数据集通常缺乏明确的领域划分，手动标注成本高，优化数据混合是挑战。

Method: CLIMB通过语义空间嵌入和聚类，结合代理模型和预测器迭代优化数据混合。

Result: 1B模型在400B tokens训练下超越Llama-3.2-1B 2%，特定领域优化提升5%。

Conclusion: CLIMB展示了优化数据混合的有效性，并提供了ClimbLab和ClimbMix作为研究资源。

Abstract: Pre-training datasets are typically collected from web content and lack
inherent domain divisions. For instance, widely used datasets like Common Crawl
do not include explicit domain labels, while manually curating labeled datasets
such as The Pile is labor-intensive. Consequently, identifying an optimal
pre-training data mixture remains a challenging problem, despite its
significant benefits for pre-training performance. To address these challenges,
we propose CLustering-based Iterative Data Mixture Bootstrapping (CLIMB), an
automated framework that discovers, evaluates, and refines data mixtures in a
pre-training setting. Specifically, CLIMB embeds and clusters large-scale
datasets in a semantic space and then iteratively searches for optimal mixtures
using a smaller proxy model and a predictor. When continuously trained on 400B
tokens with this mixture, our 1B model exceeds the state-of-the-art
Llama-3.2-1B by 2.0%. Moreover, we observe that optimizing for a specific
domain (e.g., Social Sciences) yields a 5% improvement over random sampling.
Finally, we introduce ClimbLab, a filtered 1.2-trillion-token corpus with 20
clusters as a research playground, and ClimbMix, a compact yet powerful
400-billion-token dataset designed for efficient pre-training that delivers
superior performance under an equal token budget. We analyze the final data
mixture, elucidating the characteristics of an optimal data mixture. Our data
is available at: https://research.nvidia.com/labs/lpr/climb/

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [87] [Interpretable AI-driven Guidelines for Type 2 Diabetes Treatment from Observational Data](https://arxiv.org/abs/2504.12417)
*Dewang Kumar Agarwal,Dimitris J. Bertsimas*

Main category: cs.AI

TL;DR: 该研究通过机器学习和优化方法，为2型糖尿病治疗制定结构化、数据支持的指南，临床测试显示其优于医生实践。


<details>
  <summary>Details</summary>
Motivation: 为2型糖尿病治疗提供精确、结构化且数据支持的指南，以改善临床实践和患者结果。

Method: 使用波士顿医疗中心1998-2014年的患者数据，通过机器学习和优化消除混杂偏差，训练AI树模型并手动整合为治疗流程，优先考虑激进治疗。

Result: 在未见的BMC患者中，AI流程的HbA1c中位数降低比医生高0.26%；在外部数据集中高0.13%。

Conclusion: 这种AI支持的方法精确、可解释且高效，预计优于当前实践，可改善患者结果。

Abstract: Objective: Create precise, structured, data-backed guidelines for type 2
diabetes treatment progression, suitable for clinical adoption.
  Research Design and Methods: Our training cohort was composed of patient
(with type 2 diabetes) visits from Boston Medical Center (BMC) from 1998 to
2014. We divide visits into 4 groups based on the patient's treatment regimen
before the visit, and further divide them into subgroups based on the
recommended treatment during the visit. Since each subgroup has observational
data, which has confounding bias (sicker patients are prescribed more
aggressive treatments), we used machine learning and optimization to remove
some datapoints so that the remaining data resembles a randomized trial. On
each subgroup, we train AI-backed tree-based models to prescribe treatment
changes. Once we train these tree models, we manually combine the models for
every group to create an end-to-end prescription pipeline for all patients in
that group. In this process, we prioritize stepping up to a more aggressive
treatment before considering less aggressive options. We tested this pipeline
on unseen data from BMC, and an external dataset from Hartford healthcare (type
2 diabetes patient visits from January 2020 to May 2024).
  Results: The median HbA1c reduction achieved by our pipelines is 0.26% more
than what the doctors achieved on the unseen BMC patients. For the Hartford
cohort, our pipelines were better by 0.13%.
  Conclusions: This precise, interpretable, and efficient AI-backed approach to
treatment progression in type 2 diabetes is predicted to outperform the current
practice and can be deployed to improve patient outcomes.

</details>


### [88] [Towards Conversational AI for Human-Machine Collaborative MLOps](https://arxiv.org/abs/2504.12477)
*George Fatouros,Georgios Makridis,George Kousiouris,John Soldatos,Anargyros Tsadimas,Dimosthenis Kyriazis*

Main category: cs.AI

TL;DR: 论文提出了一种基于大型语言模型（LLM）的对话代理系统，旨在增强MLOps中的人机协作。系统采用可扩展架构，通过自然语言交互创建和管理ML工作流。


<details>
  <summary>Details</summary>
Motivation: 解决复杂MLOps平台（如Kubeflow）的可访问性问题，使高级ML工具更广泛可用。

Method: 采用分层模块化设计，集成KFP Agent、MinIO Agent和RAG Agent，通过迭代推理和上下文感知处理实现功能。

Result: 系统降低了MLOps的复杂性，为不同技术背景的用户提供了直观的对话界面。

Conclusion: 该对话式MLOps助手显著降低了使用门槛，提升了ML工具的可用性和灵活性。

Abstract: This paper presents a Large Language Model (LLM) based conversational agent
system designed to enhance human-machine collaboration in Machine Learning
Operations (MLOps). We introduce the Swarm Agent, an extensible architecture
that integrates specialized agents to create and manage ML workflows through
natural language interactions. The system leverages a hierarchical, modular
design incorporating a KubeFlow Pipelines (KFP) Agent for ML pipeline
orchestration, a MinIO Agent for data management, and a Retrieval-Augmented
Generation (RAG) Agent for domain-specific knowledge integration. Through
iterative reasoning loops and context-aware processing, the system enables
users with varying technical backgrounds to discover, execute, and monitor ML
pipelines; manage datasets and artifacts; and access relevant documentation,
all via intuitive conversational interfaces. Our approach addresses the
accessibility gap in complex MLOps platforms like Kubeflow, making advanced ML
tools broadly accessible while maintaining the flexibility to extend to other
platforms. The paper describes the architecture, implementation details, and
demonstrates how this conversational MLOps assistant reduces complexity and
lowers barriers to entry for users across diverse technical skill levels.

</details>


### [89] [Agentic AI Optimisation (AAIO): what it is, how it works, why it matters, and how to deal with it](https://arxiv.org/abs/2504.12482)
*Luciano Floridi,Carlotta Buttaboni,Emmie Hine,Jessica Morley,Claudio Novelli,Tyler Schroder*

Main category: cs.AI

TL;DR: 本文提出Agentic AI Optimisation (AAIO)作为优化自主AI代理与平台交互的新范式，强调其重要性及潜在影响。


<details>
  <summary>Details</summary>
Motivation: 随着自主AI代理的出现，需要一种新方法优化其与平台的交互，确保无缝集成。

Method: 引入AAIO作为方法论，分析其与网站优化的互依关系，并探讨其治理、伦理、法律和社会影响(GELSI)。

Result: AAIO可创建良性循环，需主动监管框架以规避负面影响。

Conclusion: AAIO是自主数字代理时代的基础设施，需确保其公平和包容性。

Abstract: The emergence of Agentic Artificial Intelligence (AAI) systems capable of
independently initiating digital interactions necessitates a new optimisation
paradigm designed explicitly for seamless agent-platform interactions. This
article introduces Agentic AI Optimisation (AAIO) as an essential methodology
for ensuring effective integration between websites and agentic AI systems.
Like how Search Engine Optimisation (SEO) has shaped digital content
discoverability, AAIO can define interactions between autonomous AI agents and
online platforms. By examining the mutual interdependency between website
optimisation and agentic AI success, the article highlights the virtuous cycle
that AAIO can create. It further explores the governance, ethical, legal, and
social implications (GELSI) of AAIO, emphasising the necessity of proactive
regulatory frameworks to mitigate potential negative impacts. The article
concludes by affirming AAIO's essential role as part of a fundamental digital
infrastructure in the era of autonomous digital agents, advocating for
equitable and inclusive access to its benefits.

</details>


### [90] [Heuristic Recognition and Rapid Response to Unfamiliar Events Outside of Agent Design Scope](https://arxiv.org/abs/2504.12497)
*Robert E. Wray,Steven J. Jones,John E. Laird*

Main category: cs.AI

TL;DR: 论文探讨了开放世界中智能体如何应对超出其设计范围的陌生情境，提出结合元知识和元推理的新方法。


<details>
  <summary>Details</summary>
Motivation: 解决智能体在开放世界中面对陌生情境时缺乏相关知识和时间的问题。

Method: 结合领域通用的元知识（受人类认知启发）和元推理。

Result: 该方法能提供快速、自适应的响应，满足开放世界智能体的性能需求。

Conclusion: 提出的方法有望为开放世界智能体提供更全面的适应性解决方案。

Abstract: Regardless of past learning, an agent in an open world will face unfamiliar
situations and events outside of prior experience, existing models, or
policies. Further, the agent will sometimes lack relevant knowledge and/or
sufficient time to assess the situation, generate and evaluate options, and
pursue a robustly considered course of action. How can an agent respond
reasonably to situations that are outside of its original design scope? How can
it recognize such situations sufficiently quickly and reliably to determine
reasonable, adaptive courses of action? We identify key characteristics needed
for solutions, evaluate the state-of-the-art by these requirements, and outline
a proposed, novel approach that combines domain-general meta-knowledge (in the
form of appraisals inspired by human cognition) and metareasoning. It has the
potential to provide fast, adaptive responses to unfamiliar situations, more
fully meeting the performance characteristics required for open-world, general
agents.

</details>


### [91] [Is Trust Correlated With Explainability in AI? A Meta-Analysis](https://arxiv.org/abs/2504.12529)
*Zahra Atf,Peter R. Lewis*

Main category: cs.AI

TL;DR: 研究发现AI系统的可解释性与用户信任之间存在显著但中等的正相关关系，表明可解释性并非唯一或主导因素。


<details>
  <summary>Details</summary>
Motivation: 探讨AI可解释性是否真正提升用户信任，并分析其社会技术影响。

Method: 采用元分析方法，综合分析了90项研究的数据。

Result: 发现可解释性与用户信任之间存在显著但中等的正相关关系。

Conclusion: 强调需关注长期信任构建，而非仅依赖可解释性，同时需解决算法偏见和伦理透明度问题。

Abstract: This study critically examines the commonly held assumption that
explicability in artificial intelligence (AI) systems inherently boosts user
trust. Utilizing a meta-analytical approach, we conducted a comprehensive
examination of the existing literature to explore the relationship between AI
explainability and trust. Our analysis, incorporating data from 90 studies,
reveals a statistically significant but moderate positive correlation between
the explainability of AI systems and the trust they engender among users. This
indicates that while explainability contributes to building trust, it is not
the sole or predominant factor in this equation. In addition to academic
contributions to the field of Explainable AI (XAI), this research highlights
its broader socio-technical implications, particularly in promoting
accountability and fostering user trust in critical domains such as healthcare
and justice. By addressing challenges like algorithmic bias and ethical
transparency, the study underscores the need for equitable and sustainable AI
adoption. Rather than focusing solely on immediate trust, we emphasize the
normative importance of fostering authentic and enduring trustworthiness in AI
systems.

</details>


### [92] [ZeroSumEval: Scaling LLM Evaluation with Inter-Model Competition](https://arxiv.org/abs/2504.12562)
*Haidar Khan,Hisham A. Alyahya,Yazeed Alnumay,M Saiful Bari,Bülent Yener*

Main category: cs.AI

TL;DR: ZeroSumEval是一种基于零和游戏的动态评估协议，用于评估大型语言模型（LLMs），克服传统方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统评估方法（如静态基准数据集、人工评估或基于模型的评估）存在过拟合、高成本和偏见问题，需要更动态和抗饱和的评估方式。

Method: ZeroSumEval通过多样化的游戏（如安全挑战、经典游戏、知识测试和说服挑战）动态评估LLMs的战略推理、规划、知识应用和创造力。

Result: 实验表明，前沿模型（如GPT和Claude系列）在常见游戏和问答中表现良好，但在需要创造新颖问题的游戏中表现不佳，且无法可靠地互相破解。

Conclusion: ZeroSumEval提供了一个标准化且可扩展的框架，为LLMs的动态评估提供了新方向。

Abstract: Evaluating the capabilities of Large Language Models (LLMs) has traditionally
relied on static benchmark datasets, human assessments, or model-based
evaluations - methods that often suffer from overfitting, high costs, and
biases. ZeroSumEval is a novel competition-based evaluation protocol that
leverages zero-sum games to assess LLMs with dynamic benchmarks that resist
saturation. ZeroSumEval encompasses a diverse suite of games, including
security challenges (PyJail), classic games (Chess, Liar's Dice, Poker),
knowledge tests (MathQuiz), and persuasion challenges (Gandalf, Debate). These
games are designed to evaluate a range of AI capabilities such as strategic
reasoning, planning, knowledge application, and creativity. Building upon
recent studies that highlight the effectiveness of game-based evaluations for
LLMs, ZeroSumEval enhances these approaches by providing a standardized and
extensible framework. To demonstrate this, we conduct extensive experiments
with >7000 simulations across 7 games and 13 models. Our results show that
while frontier models from the GPT and Claude families can play common games
and answer questions, they struggle to play games that require creating novel
and challenging questions. We also observe that models cannot reliably
jailbreak each other and fail generally at tasks requiring creativity. We
release our code at https://github.com/facebookresearch/ZeroSumEval.

</details>


### [93] [The Chronicles of Foundation AI for Forensics of Multi-Agent Provenance](https://arxiv.org/abs/2504.12612)
*Ching-Chun Chang,Isao Echizen*

Main category: cs.AI

TL;DR: 研究提出了一种基于符号编年史的系统，用于追踪多代理生成内容的来源，无需依赖内部状态或外部元信息。


<details>
  <summary>Details</summary>
Motivation: 随着AI代理在复杂任务中协作生成内容，追踪其来源变得困难，本研究旨在解决这一问题。

Method: 提出了一种基于符号编年史的系统，记录签名和时间戳，通过反馈循环更新生成历史。

Result: 系统能够从生成内容中追溯历史贡献，实现协作AI的可追溯性。

Conclusion: 该研究为协作AI提供了一种可追溯的生成历史记录方法，增强了其责任性。

Abstract: Provenance is the chronology of things, resonating with the fundamental
pursuit to uncover origins, trace connections, and situate entities within the
flow of space and time. As artificial intelligence advances towards autonomous
agents capable of interactive collaboration on complex tasks, the provenance of
generated content becomes entangled in the interplay of collective creation,
where contributions are continuously revised, extended or overwritten. In a
multi-agent generative chain, content undergoes successive transformations,
often leaving little, if any, trace of prior contributions. In this study, we
investigates the problem of tracking multi-agent provenance across the temporal
dimension of generation. We propose a chronological system for post hoc
attribution of generative history from content alone, without reliance on
internal memory states or external meta-information. At its core lies the
notion of symbolic chronicles, representing signed and time-stamped records, in
a form analogous to the chain of custody in forensic science. The system
operates through a feedback loop, whereby each generative timestep updates the
chronicle of prior interactions and synchronises it with the synthetic content
in the very act of generation. This research seeks to develop an accountable
form of collaborative artificial intelligence within evolving cyber ecosystems.

</details>


### [94] [Embodied-R: Collaborative Framework for Activating Embodied Spatial Reasoning in Foundation Models via Reinforcement Learning](https://arxiv.org/abs/2504.12680)
*Baining Zhao,Ziyou Wang,Jianjie Fang,Chen Gao,Fanhang Man,Jinqiang Cui,Xin Wang,Xinlei Chen,Yong Li,Wenwu Zhu*

Main category: cs.AI

TL;DR: Embodied-R框架结合视觉语言模型（VLM）和小型语言模型（LM），通过强化学习实现高效的空间推理能力，仅用5k样本即达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 探索预训练模型如何通过视觉观察获取高级空间推理能力。

Method: 结合VLM和LM，使用强化学习（RL）和新型奖励机制（逻辑一致性），实现慢思考能力。

Result: 在5k样本训练后，3B LM的Embodied-R在空间推理任务上匹配SOTA模型，并展现出系统性分析和上下文整合能力。

Conclusion: Embodied-R展示了高效推理潜力，并提出了奖励设计、模型泛化等研究方向。

Abstract: Humans can perceive and reason about spatial relationships from sequential
visual observations, such as egocentric video streams. However, how pretrained
models acquire such abilities, especially high-level reasoning, remains
unclear. This paper introduces Embodied-R, a collaborative framework combining
large-scale Vision-Language Models (VLMs) for perception and small-scale
Language Models (LMs) for reasoning. Using Reinforcement Learning (RL) with a
novel reward system considering think-answer logical consistency, the model
achieves slow-thinking capabilities with limited computational resources. After
training on only 5k embodied video samples, Embodied-R with a 3B LM matches
state-of-the-art multimodal reasoning models (OpenAI-o1, Gemini-2.5-pro) on
both in-distribution and out-of-distribution embodied spatial reasoning tasks.
Embodied-R also exhibits emergent thinking patterns such as systematic analysis
and contextual integration. We further explore research questions including
response length, training on VLM, strategies for reward design, and differences
in model generalization after SFT (Supervised Fine-Tuning) and RL training.

</details>


### [95] [WebLists: Extracting Structured Information From Complex Interactive Websites Using Executable LLM Agents](https://arxiv.org/abs/2504.12682)
*Arth Bohra,Manvel Saroyan,Danil Melkozerov,Vahe Karufanyan,Gabriel Maher,Pascal Weinberger,Artem Harutyunyan,Giovanni Campagna*

Main category: cs.AI

TL;DR: WebLists是一个包含200个数据提取任务的基准测试，用于评估网页代理在结构化数据提取中的表现。现有方法表现不佳，BardeenAgent通过将执行转化为可重复程序并利用HTML结构，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 当前网页代理研究集中在导航和交易任务，而结构化数据提取的研究较少。WebLists填补了这一空白，并展示了现有方法的不足。

Method: 提出BardeenAgent框架，将代理执行转化为可重复程序，利用HTML结构生成通用CSS选择器，并提取数据。

Result: BardeenAgent在WebLists基准测试中达到66%的召回率，是现有最佳代理的两倍，同时降低了成本。

Conclusion: BardeenAgent通过创新方法显著提升了网页代理在结构化数据提取任务中的性能，为未来研究提供了方向。

Abstract: Most recent web agent research has focused on navigation and transaction
tasks, with little emphasis on extracting structured data at scale. We present
WebLists, a benchmark of 200 data-extraction tasks across four common business
and enterprise use-cases. Each task requires an agent to navigate to a webpage,
configure it appropriately, and extract complete datasets with well-defined
schemas. We show that both LLMs with search capabilities and SOTA web agents
struggle with these tasks, with a recall of 3% and 31%, respectively, despite
higher performance on question-answering tasks.
  To address this challenge, we propose BardeenAgent, a novel framework that
enables web agents to convert their execution into repeatable programs, and
replay them at scale across pages with similar structure. BardeenAgent is also
the first LLM agent to take advantage of the regular structure of HTML. In
particular BardeenAgent constructs a generalizable CSS selector to capture all
relevant items on the page, then fits the operations to extract the data.
  On the WebLists benchmark, BardeenAgent achieves 66% recall overall, more
than doubling the performance of SOTA web agents, and reducing cost per output
row by 3x.

</details>


### [96] [InstructRAG: Leveraging Retrieval-Augmented Generation on Instruction Graphs for LLM-Based Task Planning](https://arxiv.org/abs/2504.13032)
*Zheng Wang,Shu Xian Teo,Jun Jie Chew,Wei Shi*

Main category: cs.AI

TL;DR: InstructRAG结合检索增强生成和多智能体元强化学习框架，解决了任务规划中的可扩展性和可迁移性问题，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖TAO过程，但受限于LLMs对复杂任务的知识不足，RAG提供了新的机会。

Method: 提出InstructRAG，包含组织指令路径的图、RL-Agent扩展图覆盖（可扩展性）、ML-Agent提升任务泛化（可迁移性），端到端训练。

Result: 在四个任务规划数据集上，InstructRAG比现有最佳方法提升19.2%。

Conclusion: InstructRAG高效适应新任务，显著提升规划性能。

Abstract: Recent advancements in large language models (LLMs) have enabled their use as
agents for planning complex tasks. Existing methods typically rely on a
thought-action-observation (TAO) process to enhance LLM performance, but these
approaches are often constrained by the LLMs' limited knowledge of complex
tasks. Retrieval-augmented generation (RAG) offers new opportunities by
leveraging external databases to ground generation in retrieved information. In
this paper, we identify two key challenges (enlargability and transferability)
in applying RAG to task planning. We propose InstructRAG, a novel solution
within a multi-agent meta-reinforcement learning framework, to address these
challenges. InstructRAG includes a graph to organize past instruction paths
(sequences of correct actions), an RL-Agent with Reinforcement Learning to
expand graph coverage for enlargability, and an ML-Agent with Meta-Learning to
improve task generalization for transferability. The two agents are trained
end-to-end to optimize overall planning performance. Our experiments on four
widely used task planning datasets demonstrate that InstructRAG significantly
enhances performance and adapts efficiently to new tasks, achieving up to a
19.2% improvement over the best existing approach.

</details>


### [97] [Exploring Expert Failures Improves LLM Agent Tuning](https://arxiv.org/abs/2504.13145)
*Li-Cheng Lan,Andrew Bai,Minhao Cheng,Ruochen Wang,Cho-Jui Hsieh,Tianyi Zhou*

Main category: cs.AI

TL;DR: EEF方法通过利用专家失败轨迹中的有益动作，提升了LLM作为代理的性能，解决了复杂子任务，并在WebShop和SciWorld中取得了新纪录。


<details>
  <summary>Details</summary>
Motivation: 专家（如GPT-4）在复杂子任务中失败，但这些失败轨迹中仍包含有价值的指导信息，如计划和关键动作，可以提升代理的学习效率。

Method: 提出EEF方法，从专家失败轨迹中提取有益动作并整合到训练数据中，同时排除有害动作以避免污染模型学习。

Result: EEF在WebShop中实现了62%的胜率，优于RFT（53.6%）和GPT-4（35.6%），并在SciWorld中超过81分。

Conclusion: EEF通过利用专家失败的有益信息，显著提升了代理性能，解决了复杂子任务，并创造了新的性能记录。

Abstract: Large Language Models (LLMs) have shown tremendous potential as agents,
excelling at tasks that require multiple rounds of reasoning and interactions.
Rejection Sampling Fine-Tuning (RFT) has emerged as an effective method for
finetuning LLMs as agents: it first imitates expert-generated successful
trajectories and further improves agentic skills through iterative fine-tuning
on successful, self-generated trajectories. However, since the expert (e.g.,
GPT-4) succeeds primarily on simpler subtasks and RFT inherently favors simpler
scenarios, many complex subtasks remain unsolved and persistently
out-of-distribution (OOD). Upon investigating these challenging subtasks, we
discovered that previously failed expert trajectories can often provide
valuable guidance, e.g., plans and key actions, that can significantly improve
agent exploration efficiency and acquisition of critical skills. Motivated by
these observations, we propose Exploring Expert Failures (EEF), which
identifies beneficial actions from failed expert trajectories and integrates
them into the training dataset. Potentially harmful actions are meticulously
excluded to prevent contamination of the model learning process. By leveraging
the beneficial actions in expert failures, EEF successfully solves some
previously unsolvable subtasks and improves agent tuning performance.
Remarkably, our approach achieved a 62\% win rate in WebShop, outperforming RFT
(53. 6\%) and GPT-4 (35. 6\%), and to the best of our knowledge, setting a new
state-of-the-art as the first method to surpass a score of 0.81 in WebShop and
exceed 81 in SciWorld.

</details>


### [98] [Antidistillation Sampling](https://arxiv.org/abs/2504.13146)
*Yash Savani,Asher Trockman,Zhili Feng,Avi Schwarzschild,Alexander Robey,Marc Finzi,J. Zico Kolter*

Main category: cs.AI

TL;DR: 论文提出了一种名为“抗蒸馏采样”的方法，通过调整模型的概率分布来污染推理轨迹，从而在不影响模型性能的情况下限制蒸馏效果。


<details>
  <summary>Details</summary>
Motivation: 前沿模型生成的推理轨迹可能被用于模型蒸馏，模型所有者需要一种方法在不影响性能的情况下限制这种蒸馏效果。

Method: 采用抗蒸馏采样策略，通过修改模型的下一词概率分布来污染推理轨迹。

Result: 抗蒸馏采样能有效减少推理轨迹对蒸馏的效用，同时保持模型的实用性。

Conclusion: 抗蒸馏采样是一种在不损害模型性能的前提下限制蒸馏效果的有效方法。

Abstract: Frontier models that generate extended reasoning traces inadvertently produce
rich token sequences that can facilitate model distillation. Recognizing this
vulnerability, model owners may seek sampling strategies that limit the
effectiveness of distillation without compromising model performance.
\emph{Antidistillation sampling} provides exactly this capability. By
strategically modifying a model's next-token probability distribution,
antidistillation sampling poisons reasoning traces, rendering them
significantly less effective for distillation while preserving the model's
practical utility. For further details, see https://antidistillation.com.

</details>


### [99] [Readable Twins of Unreadable Models](https://arxiv.org/abs/2504.13150)
*Krzysztof Pancerz,Piotr Kulicki,Michał Kalisz,Andrzej Burda,Maciej Stanisławski,Jaromir Sarzyński*

Main category: cs.AI

TL;DR: 论文提出了一种将不可读的深度学习模型转换为可读的不精确信息流模型的方法，以实现可解释的深度学习系统。


<details>
  <summary>Details</summary>
Motivation: 构建负责任的人工智能系统需要可解释性，而深度学习模型通常缺乏可读性。

Method: 基于物理对象的数字孪生，提出创建不精确信息流模型（IIFM）作为深度学习模型（DLM）的可读孪生。

Result: 通过MNIST数据集的手写数字分类模型示例验证了方法的可行性。

Conclusion: 该方法为实现可解释的深度学习系统提供了一种新途径。

Abstract: Creating responsible artificial intelligence (AI) systems is an important
issue in contemporary research and development of works on AI. One of the
characteristics of responsible AI systems is their explainability. In the
paper, we are interested in explainable deep learning (XDL) systems. On the
basis of the creation of digital twins of physical objects, we introduce the
idea of creating readable twins (in the form of imprecise information flow
models) for unreadable deep learning models. The complete procedure for
switching from the deep learning model (DLM) to the imprecise information flow
model (IIFM) is presented. The proposed approach is illustrated with an example
of a deep learning classification model for image recognition of handwritten
digits from the MNIST data set.

</details>


### [100] [Sleep-time Compute: Beyond Inference Scaling at Test-time](https://arxiv.org/abs/2504.13171)
*Kevin Lin,Charlie Snell,Yu Wang,Charles Packer,Sarah Wooders,Ion Stoica,Joseph E. Gonzalez*

Main category: cs.AI

TL;DR: 论文提出了一种名为“睡眠时间计算”的方法，通过离线预计算用户可能提出的查询，显著减少测试时的计算需求，并在多个任务中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在测试时计算成本高、延迟大的问题，通过预计算降低实时计算需求。

Method: 引入“睡眠时间计算”，预计算用户可能提出的查询，并在多个推理任务（Stateful GSM-Symbolic和Stateful AIME）中验证其效果。

Result: 睡眠时间计算将测试时计算需求减少约5倍，并可将准确率提升13%（GSM-Symbolic）和18%（AIME）。多查询任务中，平均成本降低2.5倍。

Conclusion: 睡眠时间计算在预测性强的查询任务中效果显著，适用于实际代理任务，能有效降低计算成本并提升性能。

Abstract: Scaling test-time compute has emerged as a key ingredient for enabling large
language models (LLMs) to solve difficult problems, but comes with high latency
and inference cost. We introduce sleep-time compute, which allows models to
"think" offline about contexts before queries are presented: by anticipating
what queries users might ask and pre-computing useful quantities, we can
significantly reduce the compute requirements at test-time. To demonstrate the
efficacy of our method, we create modified versions of two reasoning tasks -
Stateful GSM-Symbolic and Stateful AIME. We find that sleep-time compute can
reduce the amount of test-time compute needed to achieve the same accuracy by ~
5x on Stateful GSM-Symbolic and Stateful AIME and that by scaling sleep-time
compute we can further increase accuracy by up to 13% on Stateful GSM-Symbolic
and 18% on Stateful AIME. Furthermore, we introduce Multi-Query GSM-Symbolic,
which extends GSM-Symbolic by including multiple related queries per context.
By amortizing sleep-time compute across related queries about the same context
using Multi-Query GSM-Symbolic, we can decrease the average cost per query by
2.5x. We then conduct additional analysis to understand when sleep-time compute
is most effective, finding the predictability of the user query to be well
correlated with the efficacy of sleep-time compute. Finally, we conduct a
case-study of applying sleep-time compute to a realistic agentic SWE task.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [101] [Unveiling Hidden Collaboration within Mixture-of-Experts in Large Language Models](https://arxiv.org/abs/2504.12359)
*Yuanbo Tang,Yan Tang,Naifan Zhang,Meixuan Chen,Yang Li*

Main category: cs.LG

TL;DR: 论文研究了混合专家大型语言模型（MoE LLMs）中的专家协作机制，提出了HSDL方法分析协作模式，并开发了CAEP算法优化模型性能。实验表明协作模式与输入类型相关，且CAEP算法平均提升性能2.5%。


<details>
  <summary>Details</summary>
Motivation: 尽管MoE LLMs在多任务适应性上表现优异，但专家间的协作机制尚不明确，限制了模型的解释性和优化潜力。

Method: 采用分层稀疏字典学习（HSDL）揭示专家协作模式，并提出贡献感知专家剪枝（CAEP）算法优化模型。

Result: 实验显示协作模式与输入类型相关，CAEP算法平均提升性能2.5%，优于现有方法。

Conclusion: 研究为MoE LLMs的效率和解释性提供了新见解，优化了专家交互的理解和模型性能。

Abstract: Mixture-of-Experts based large language models (MoE LLMs) have shown
significant promise in multitask adaptability by dynamically routing inputs to
specialized experts. Despite their success, the collaborative mechanisms among
experts are still not well understood, limiting both the interpretability and
optimization of these models. In this paper, we focus on two critical issues:
(1) identifying expert collaboration patterns, and (2) optimizing MoE LLMs
through expert pruning. To address the first issue, we propose a hierarchical
sparse dictionary learning (HSDL) method that uncovers the collaboration
patterns among experts. For the second issue, we introduce the
Contribution-Aware Expert Pruning (CAEP) algorithm, which effectively prunes
low-contribution experts. Our extensive experiments demonstrate that expert
collaboration patterns are closely linked to specific input types and exhibit
semantic significance across various tasks. Moreover, pruning experiments show
that our approach improves overall performance by 2.5\% on average,
outperforming existing methods. These findings offer valuable insights into
enhancing the efficiency and interpretability of MoE LLMs, offering a clearer
understanding of expert interactions and improving model optimization.

</details>


### [102] [Activated LoRA: Fine-tuned LLMs for Intrinsics](https://arxiv.org/abs/2504.12397)
*Kristjan Greenewald,Luis Lastras,Thomas Parnell,Vraj Shah,Lucian Popa,Giulio Zizzo,Chulaka Gunasekara,Ambrish Rawat,David Cox*

Main category: cs.LG

TL;DR: aLoRA改进LoRA框架，避免在多轮对话中因切换LoRA而重新计算KV缓存，提升效率。


<details>
  <summary>Details</summary>
Motivation: 解决LoRA在多轮对话中因频繁切换导致KV缓存重新计算的低效问题。

Method: 提出aLoRA，仅对调用后的令牌进行权重调整，保留基础模型的KV缓存。

Result: aLoRA在保持准确性的同时显著提升推理效率。

Conclusion: aLoRA为高效定制化模型提供了新思路，适用于多轮对话场景。

Abstract: Low-Rank Adaptation (LoRA) has emerged as a highly efficient framework for
finetuning the weights of large foundation models, and has become the go-to
method for data-driven customization of LLMs. Despite the promise of highly
customized behaviors and capabilities, switching between relevant LoRAs in a
multiturn setting is highly inefficient, as the key-value (KV) cache of the
entire turn history must be recomputed with the LoRA weights before generation
can begin. To address this problem, we propose Activated LoRA (aLoRA), which
modifies the LoRA framework to only adapt weights for the tokens in the
sequence \emph{after} the aLoRA is invoked. This change crucially allows aLoRA
to accept the base model's KV cache of the input string, meaning that aLoRA can
be instantly activated whenever needed in a chain without recomputing the
cache. This enables building what we call \emph{intrinsics}, i.e. highly
specialized models invoked to perform well-defined operations on portions of an
input chain or conversation that otherwise uses the base model by default. We
use aLoRA to train a set of intrinsics models, demonstrating competitive
accuracy with standard LoRA while achieving significant inference benefits.

</details>


### [103] [Standardization of Multi-Objective QUBOs](https://arxiv.org/abs/2504.12419)
*Loong Kuan Lee,Thore Thassilo Gerlach,Nico Piatkowski*

Main category: cs.LG

TL;DR: 提出一种基于方差计算的多目标QUBO问题标定方法，通过统一目标尺度优化权重选择和求解效果。


<details>
  <summary>Details</summary>
Motivation: 多目标QUBO问题中，目标尺度差异导致权重选择和标量化困难，需一种有效方法平衡目标。

Method: 通过精确计算各QUBO目标的方差，将其标定为单位方差，统一目标尺度。

Result: 实证表明该方法能更平衡地求解多目标问题，减少手动权重选择的繁琐。

Conclusion: 提出的方差标定方法在多目标QUBO优化中有效，提升了求解效率和可靠性。

Abstract: Multi-objective optimization involving Quadratic Unconstrained Binary
Optimization (QUBO) problems arises in various domains. A fundamental challenge
in this context is the effective balancing of multiple objectives, each
potentially operating on very different scales. This imbalance introduces
complications such as the selection of appropriate weights when scalarizing
multiple objectives into a single objective function. In this paper, we propose
a novel technique for scaling QUBO objectives that uses an exact computation of
the variance of each individual QUBO objective. By scaling each objective to
have unit variance, we align all objectives onto a common scale, thereby
allowing for more balanced solutions to be found when scalarizing the
objectives with equal weights, as well as potentially assisting in the search
or choice of weights during scalarization. Finally, we demonstrate its
advantages through empirical evaluations on various multi-objective
optimization problems. Our results are noteworthy since manually selecting
scalarization weights is cumbersome, and reliable, efficient solutions are
scarce.

</details>


### [104] [Deriving Equivalent Symbol-Based Decision Models from Feedforward Neural Networks](https://arxiv.org/abs/2504.12446)
*Sebastian Seidel,Uwe M. Borghoff*

Main category: cs.LG

TL;DR: 论文提出了一种从神经网络中提取可解释符号模型的方法，以提升AI系统的透明度和信任度。


<details>
  <summary>Details</summary>
Motivation: AI系统的不透明性阻碍了其信任和接受度，因此需要一种方法将神经网络的决策过程转化为可解释的符号模型。

Method: 通过分析前馈神经网络的分布式表示，提取符号组件（如填充物、角色及其关系），并将其映射为决策树。

Result: 成功开发了一个原型，验证了从神经网络中提取符号表示的可行性，增强了AI系统的透明度和问责性。

Conclusion: 该方法为连接主义和符号主义AI的融合提供了桥梁，提升了AI系统的可解释性和信任度。

Abstract: Artificial intelligence (AI) has emerged as a transformative force across
industries, driven by advances in deep learning and natural language
processing, and fueled by large-scale data and computing resources. Despite its
rapid adoption, the opacity of AI systems poses significant challenges to trust
and acceptance.
  This work explores the intersection of connectionist and symbolic approaches
to artificial intelligence, focusing on the derivation of interpretable
symbolic models, such as decision trees, from feedforward neural networks
(FNNs). Decision trees provide a transparent framework for elucidating the
operations of neural networks while preserving their functionality. The
derivation is presented in a step-by-step approach and illustrated with several
examples. A systematic methodology is proposed to bridge neural and symbolic
paradigms by exploiting distributed representations in FNNs to identify
symbolic components, including fillers, roles, and their interrelationships.
The process traces neuron activation values and input configurations across
network layers, mapping activations and their underlying inputs to decision
tree edges. The resulting symbolic structures effectively capture FNN decision
processes and enable scalability to deeper networks through iterative
refinement of subpaths for each hidden layer.
  To validate the theoretical framework, a prototype was developed using Keras
.h5-data and emulating TensorFlow within the Java JDK/JavaFX environment. This
prototype demonstrates the feasibility of extracting symbolic representations
from neural networks, enhancing trust in AI systems, and promoting
accountability.

</details>


### [105] [Can Moran Eigenvectors Improve Machine Learning of Spatial Data? Insights from Synthetic Data Validation](https://arxiv.org/abs/2504.12450)
*Ziqi Li,Zhan Peng*

Main category: cs.LG

TL;DR: 研究探讨了Moran特征向量空间滤波（ESF）在机器学习中的有效性，发现仅使用坐标的模型表现优于基于特征向量的方法。


<details>
  <summary>Details</summary>
Motivation: 探索ESF方法是否适用于机器学习，以解决空间效应问题。

Method: 生成合成数据集，测试不同空间权重矩阵的Moran特征向量，评估多种机器学习模型的性能。

Result: 使用坐标的模型表现更好，但特征向量在负空间自相关或网络自相关中仍有价值。

Conclusion: ESF在特定情况下有用，但坐标方法在多数情况下更优。

Abstract: Moran Eigenvector Spatial Filtering (ESF) approaches have shown promise in
accounting for spatial effects in statistical models. Can this extend to
machine learning? This paper examines the effectiveness of using Moran
Eigenvectors as additional spatial features in machine learning models. We
generate synthetic datasets with known processes involving spatially varying
and nonlinear effects across two different geometries. Moran Eigenvectors
calculated from different spatial weights matrices, with and without a priori
eigenvector selection, are tested. We assess the performance of popular machine
learning models, including Random Forests, LightGBM, XGBoost, and TabNet, and
benchmark their accuracies in terms of cross-validated R2 values against models
that use only coordinates as features. We also extract coefficients and
functions from the models using GeoShapley and compare them with the true
processes. Results show that machine learning models using only location
coordinates achieve better accuracies than eigenvector-based approaches across
various experiments and datasets. Furthermore, we discuss that while these
findings are relevant for spatial processes that exhibit positive spatial
autocorrelation, they do not necessarily apply when modeling network
autocorrelation and cases with negative spatial autocorrelation, where Moran
Eigenvectors would still be useful.

</details>


### [106] [M$^2$FGB: A Min-Max Gradient Boosting Framework for Subgroup Fairness](https://arxiv.org/abs/2504.12458)
*Jansen S. B. Pereira,Giovani Valdrighi,Marcos Medeiros Raimundo*

Main category: cs.LG

TL;DR: 论文提出了一种结合公平性目标与梯度提升的方法，通过最小-最大优化实现公平预测。


<details>
  <summary>Details</summary>
Motivation: 机器学习中的公平性日益重要，需避免对边缘化群体的不利预测。

Method: 扩展梯度提升方法，结合传统损失函数与最小-最大公平项，研究其理论性质。

Result: 提出的算法在理论和实验上均表现良好，适用于二元和子群公平性。

Conclusion: 该方法为机器学习公平性提供了一种灵活且有效的解决方案。

Abstract: In recent years, fairness in machine learning has emerged as a critical
concern to ensure that developed and deployed predictive models do not have
disadvantageous predictions for marginalized groups. It is essential to
mitigate discrimination against individuals based on protected attributes such
as gender and race. In this work, we consider applying subgroup justice
concepts to gradient-boosting machines designed for supervised learning
problems. Our approach expanded gradient-boosting methodologies to explore a
broader range of objective functions, which combines conventional losses such
as the ones from classification and regression and a min-max fairness term. We
study relevant theoretical properties of the solution of the min-max
optimization problem. The optimization process explored the primal-dual
problems at each boosting round. This generic framework can be adapted to
diverse fairness concepts. The proposed min-max primal-dual gradient boosting
algorithm was theoretically shown to converge under mild conditions and
empirically shown to be a powerful and flexible approach to address binary and
subgroup fairness.

</details>


### [107] [Dense Backpropagation Improves Training for Sparse Mixture-of-Experts](https://arxiv.org/abs/2504.12463)
*Ashwinee Panda,Vatsal Baherwani,Zain Sarwar,Benjamin Therien,Supriyo Chakraborty,Tom Goldstein*

Main category: cs.LG

TL;DR: Default MoE通过用专家输出的指数移动平均替代缺失的专家激活，改善了MoE路由器的训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 解决MoE因稀疏更新导致的训练不稳定和性能不佳问题。

Method: 使用默认输出（专家输出的指数移动平均）替代缺失的专家激活，使路由器接收每个专家的信号。

Result: Default MoE在多种设置中优于标准TopK路由，且计算开销小。

Conclusion: Default MoE显著提升了MoE的训练性能，无需额外计算成本。

Abstract: Mixture of Experts (MoE) pretraining is more scalable than dense Transformer
pretraining, because MoEs learn to route inputs to a sparse set of their
feedforward parameters. However, this means that MoEs only receive a sparse
backward update, leading to training instability and suboptimal performance. We
present a lightweight approximation method that gives the MoE router a dense
gradient update while continuing to sparsely activate its parameters. Our
method, which we refer to as Default MoE, substitutes missing expert
activations with default outputs consisting of an exponential moving average of
expert outputs previously seen over the course of training. This allows the
router to receive signals from every expert for each token, leading to
significant improvements in training performance. Our Default MoE outperforms
standard TopK routing in a variety of settings without requiring significant
computational overhead. Code: https://github.com/vatsal0/default-moe.

</details>


### [108] [Geometric Generality of Transformer-Based Gröbner Basis Computation](https://arxiv.org/abs/2504.12465)
*Yuta Kambe,Yota Maeda,Tristan Vaccon*

Main category: cs.LG

TL;DR: 本文探讨了使用Transformer计算Gröbner基的问题，证明了之前提出的数据集生成方法的通用性，并提出了一种扩展算法以增强训练效果。


<details>
  <summary>Details</summary>
Motivation: 解决Transformer在Gröbner基计算中数据集生成缺乏理论保证的问题，填补Lample和Charton提出的多样性训练需求的空白。

Method: 提出并证明了一种数据集生成算法的通用性，并扩展了该算法以系统化生成理想生成器的数据集。

Result: 证明了生成的数据集足够通用，能确保Transformer学习多样化的Gröbner基，并提供了几何理论基础。

Conclusion: 为Transformer解决数学问题提供了严谨的数据集生成方法，支持多样性训练的理念。

Abstract: The intersection of deep learning and symbolic mathematics has seen rapid
progress in recent years, exemplified by the work of Lample and Charton. They
demonstrated that effective training of machine learning models for solving
mathematical problems critically depends on high-quality, domain-specific
datasets. In this paper, we address the computation of Gr\"obner basis using
Transformers. While a dataset generation method tailored to Transformer-based
Gr\"obner basis computation has previously been proposed, it lacked theoretical
guarantees regarding the generality or quality of the generated datasets. In
this work, we prove that datasets generated by the previously proposed
algorithm are sufficiently general, enabling one to ensure that Transformers
can learn a sufficiently diverse range of Gr\"obner bases. Moreover, we propose
an extended and generalized algorithm to systematically construct datasets of
ideal generators, further enhancing the training effectiveness of Transformer.
Our results provide a rigorous geometric foundation for Transformers to address
a mathematical problem, which is an answer to Lample and Charton's idea of
training on diverse or representative inputs.

</details>


### [109] [You Don't Need All Attentions: Distributed Dynamic Fine-Tuning for Foundation Models](https://arxiv.org/abs/2504.12471)
*Shiwei Ding,Lan Zhang,Zhenlin Wang,Giuseppe Ateniese,Xiaoyong Yuan*

Main category: cs.LG

TL;DR: 论文提出了一种分布式动态微调（D2FT）框架，通过优化注意力模块的选择策略，显著降低了基础模型微调的计算和通信成本。


<details>
  <summary>Details</summary>
Motivation: 基础模型规模快速增长，传统微调方法在商业设备上因内存带宽有限而难以实施，且现有分布式计算方法未能充分利用基础模型的特性，导致高计算成本和负载不均衡。

Method: D2FT框架通过三种创新选择策略，动态调整注意力模块的参与，并结合多背包优化解决负载均衡问题。

Result: 实验表明，D2FT将训练计算成本降低40%，通信成本降低50%，在CIFAR-10等数据集上仅损失1%-2%的准确率。

Conclusion: D2FT不仅高效，还能扩展到参数高效的微调技术（如LoRA），进一步验证了其普适性和实用性。

Abstract: Fine-tuning plays a crucial role in adapting models to downstream tasks with
minimal training efforts. However, the rapidly increasing size of foundation
models poses a daunting challenge for accommodating foundation model
fine-tuning in most commercial devices, which often have limited memory
bandwidth. Techniques like model sharding and tensor parallelism address this
issue by distributing computation across multiple devices to meet memory
requirements. Nevertheless, these methods do not fully leverage their
foundation nature in facilitating the fine-tuning process, resulting in high
computational costs and imbalanced workloads. We introduce a novel Distributed
Dynamic Fine-Tuning (D2FT) framework that strategically orchestrates operations
across attention modules based on our observation that not all attention
modules are necessary for forward and backward propagation in fine-tuning
foundation models. Through three innovative selection strategies, D2FT
significantly reduces the computational workload required for fine-tuning
foundation models. Furthermore, D2FT addresses workload imbalances in
distributed computing environments by optimizing these selection strategies via
multiple knapsack optimization. Our experimental results demonstrate that the
proposed D2FT framework reduces the training computational costs by 40% and
training communication costs by 50% with only 1% to 2% accuracy drops on the
CIFAR-10, CIFAR-100, and Stanford Cars datasets. Moreover, the results show
that D2FT can be effectively extended to recent LoRA, a state-of-the-art
parameter-efficient fine-tuning technique. By reducing 40% computational cost
or 50% communication cost, D2FT LoRA top-1 accuracy only drops 4% to 6% on
Stanford Cars dataset.

</details>


### [110] [Reinforcement Learning from Human Feedback](https://arxiv.org/abs/2504.12501)
*Nathan Lambert*

Main category: cs.LG

TL;DR: 本书介绍了基于人类反馈的强化学习（RLHF）的核心方法，适合有一定定量背景的读者。内容涵盖RLHF的起源、问题定义、优化阶段及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: RLHF已成为部署最新机器学习系统的重要工具，本书旨在为读者提供对这一技术的全面理解。

Method: 从RLHF的起源讲起，逐步介绍问题定义、数据收集、数学基础，并详细解析优化阶段的各个步骤。

Result: 书中详细阐述了RLHF的优化过程，包括指令调优、奖励模型训练、拒绝采样和直接对齐算法等。

Conclusion: 本书总结了RLHF的当前研究进展，并提出了合成数据和评估等未充分研究的问题及未来方向。

Abstract: Reinforcement learning from human feedback (RLHF) has become an important
technical and storytelling tool to deploy the latest machine learning systems.
In this book, we hope to give a gentle introduction to the core methods for
people with some level of quantitative background. The book starts with the
origins of RLHF -- both in recent literature and in a convergence of disparate
fields of science in economics, philosophy, and optimal control. We then set
the stage with definitions, problem formulation, data collection, and other
common math used in the literature. The core of the book details every
optimization stage in using RLHF, from starting with instruction tuning to
training a reward model and finally all of rejection sampling, reinforcement
learning, and direct alignment algorithms. The book concludes with advanced
topics -- understudied research questions in synthetic data and evaluation --
and open questions for the field.

</details>


### [111] [Continual Learning Strategies for 3D Engineering Regression Problems: A Benchmarking Study](https://arxiv.org/abs/2504.12503)
*Kaira M. Samuel,Faez Ahmed*

Main category: cs.LG

TL;DR: 论文探讨了在有限数据集下，机器学习模型如何通过持续学习（CL）避免灾难性遗忘，并在工程设计中应用CL方法。


<details>
  <summary>Details</summary>
Motivation: 工程问题中机器学习模型常因计算成本高而难以重新训练，CL提供了一种高效的学习新知识的方法。

Method: 研究通过多种CL方法在五个工程数据集上构建了九个新基准，评估其抗遗忘和泛化能力。

Result: 初步结果显示，CL方法（如Replay策略）性能接近重新训练，且训练时间减少近一半。

Conclusion: CL在工程设计中具有潜力，Replay策略尤其适用于实际工作流程。

Abstract: Engineering problems that apply machine learning often involve
computationally intensive methods but rely on limited datasets. As engineering
data evolves with new designs and constraints, models must incorporate new
knowledge over time. However, high computational costs make retraining models
from scratch infeasible. Continual learning (CL) offers a promising solution by
enabling models to learn from sequential data while mitigating catastrophic
forgetting, where a model forgets previously learned mappings. This work
introduces CL to engineering design by benchmarking several CL methods on
representative regression tasks. We apply these strategies to five engineering
datasets and construct nine new engineering CL benchmarks to evaluate their
ability to address forgetting and improve generalization. Preliminary results
show that applying existing CL methods to these tasks improves performance over
naive baselines. In particular, the Replay strategy achieved performance
comparable to retraining in several benchmarks while reducing training time by
nearly half, demonstrating its potential for real-world engineering workflows.
The code and datasets used in this work will be available at:
https://github.com/kmsamuel/cl-for-engineering-release.

</details>


### [112] [MOM: Memory-Efficient Offloaded Mini-Sequence Inference for Long Context Language Models](https://arxiv.org/abs/2504.12526)
*Junyang Zhang,Tianyi Zhu,Cheng Luo,Anima Anandkumar*

Main category: cs.LG

TL;DR: MOM是一种高效的内存优化方法，通过将关键层分割为“迷你序列”并结合KV缓存卸载，显著降低GPU内存需求，同时保持输出准确性和高吞吐量。


<details>
  <summary>Details</summary>
Motivation: 长上下文语言模型在推理时GPU内存需求高，难以部署。

Method: 提出MOM方法，将关键层分割为“迷你序列”并与KV缓存卸载结合。

Result: MOM平均降低峰值内存使用50%以上，在Meta-Llama-3.2-8B上将最大上下文长度从155k扩展到455k，且不影响准确性。

Conclusion: MOM消除了预填充阶段的内存瓶颈，将未来研究方向转向解码阶段的KV缓存效率优化。

Abstract: Long-context language models exhibit impressive performance but remain
challenging to deploy due to high GPU memory demands during inference. We
propose Memory-efficient Offloaded Mini-sequence Inference (MOM), a method that
partitions critical layers into smaller "mini-sequences" and integrates
seamlessly with KV cache offloading. Experiments on various Llama, Qwen, and
Mistral models demonstrate that MOM reduces peak memory usage by over 50\% on
average. On Meta-Llama-3.2-8B, MOM extends the maximum context length from 155k
to 455k tokens on a single A100 80GB GPU, while keeping outputs identical and
not compromising accuracy. MOM also maintains highly competitive throughput due
to minimal computational overhead and efficient last-layer processing. Compared
to traditional chunked prefill methods, MOM achieves a 35\% greater context
length extension. More importantly, our method drastically reduces prefill
memory consumption, eliminating it as the longstanding dominant memory
bottleneck during inference. This breakthrough fundamentally changes research
priorities, redirecting future efforts from prefill-stage optimizations to
improving decode-stage residual KV cache efficiency.

</details>


### [113] [Generalization through variance: how noise shapes inductive biases in diffusion models](https://arxiv.org/abs/2504.12532)
*John J. Vastola*

Main category: cs.LG

TL;DR: 扩散模型的泛化能力源于训练目标中的噪声目标，而非训练分布的真实分数函数。本文通过理论分析解释了这种‘通过方差泛化’的现象。


<details>
  <summary>Details</summary>
Motivation: 研究扩散模型如何泛化到训练集之外，尤其是考虑到训练目标（DSM）的最优解是训练分布的分数函数，而网络表达能力足以准确学习该分数。

Method: 利用物理学启发的路径积分方法，分析了几种典型欠参数化和过参数化扩散模型学习的分布。

Result: 发现扩散模型学习的分布类似于训练分布，但填补了‘空白’，这种归纳偏差源于训练中噪声目标的协方差结构。

Conclusion: 扩散模型的泛化能力与训练目标的噪声特性密切相关，这种归纳偏差还可能与特征相关的归纳偏差相互作用。

Abstract: How diffusion models generalize beyond their training set is not known, and
is somewhat mysterious given two facts: the optimum of the denoising score
matching (DSM) objective usually used to train diffusion models is the score
function of the training distribution; and the networks usually used to learn
the score function are expressive enough to learn this score to high accuracy.
We claim that a certain feature of the DSM objective -- the fact that its
target is not the training distribution's score, but a noisy quantity only
equal to it in expectation -- strongly impacts whether and to what extent
diffusion models generalize. In this paper, we develop a mathematical theory
that partly explains this 'generalization through variance' phenomenon. Our
theoretical analysis exploits a physics-inspired path integral approach to
compute the distributions typically learned by a few paradigmatic under- and
overparameterized diffusion models. We find that the distributions diffusion
models effectively learn to sample from resemble their training distributions,
but with 'gaps' filled in, and that this inductive bias is due to the
covariance structure of the noisy target used during training. We also
characterize how this inductive bias interacts with feature-related inductive
biases.

</details>


### [114] [TraCeS: Trajectory Based Credit Assignment From Sparse Safety Feedback](https://arxiv.org/abs/2504.12557)
*Siow Meng Low,Akshat Kumar*

Main category: cs.LG

TL;DR: 论文提出了一种在未知安全约束下学习安全强化学习的方法，通过设计安全模型和优化策略，实现了对未知安全定义的有效满足。


<details>
  <summary>Details</summary>
Motivation: 在安全强化学习中，安全约束通常未知或难以明确指定，需要从稀疏标记数据中学习。

Method: 设计了一个安全模型，通过信用分配估计每个决策步骤对整体安全的影响，并提出了一个优化算法来学习安全且高效的策略。

Result: 实验结果表明，该方法能有效满足未知安全定义，并适用于多种连续控制任务。

Conclusion: 该方法为未知安全约束下的强化学习提供了一种有效的解决方案。

Abstract: In safe reinforcement learning (RL), auxiliary safety costs are used to align
the agent to safe decision making. In practice, safety constraints, including
cost functions and budgets, are unknown or hard to specify, as it requires
anticipation of all possible unsafe behaviors. We therefore address a general
setting where the true safety definition is unknown, and has to be learned from
sparsely labeled data. Our key contributions are: first, we design a safety
model that performs credit assignment to estimate each decision step's impact
on the overall safety using a dataset of diverse trajectories and their
corresponding binary safety labels (i.e., whether the corresponding trajectory
is safe/unsafe). Second, we illustrate the architecture of our safety model to
demonstrate its ability to learn a separate safety score for each timestep.
Third, we reformulate the safe RL problem using the proposed safety model and
derive an effective algorithm to optimize a safe yet rewarding policy. Finally,
our empirical results corroborate our findings and show that this approach is
effective in satisfying unknown safety definition, and scalable to various
continuous control tasks.

</details>


### [115] [Fine Flood Forecasts: Incorporating local data into global models through fine-tuning](https://arxiv.org/abs/2504.12559)
*Emil Ryd,Grey Nearing*

Main category: cs.LG

TL;DR: 论文提出了一种结合全局预训练和局部微调的方法，以提高洪水预测模型的性能，并帮助国家预报员更好地掌握模型所有权。


<details>
  <summary>Details</summary>
Motivation: 洪水是最常见的自然灾害，准确的洪水预测对早期预警系统至关重要。然而，现有的机器学习模型依赖全局训练数据，导致国家预报员难以适应本地数据，阻碍了模型的部署。

Method: 方法包括在大型全局数据集上预训练模型，然后在单个流域的本地数据上进行微调。

Result: 结果表明，本地数据确实能提升模型性能，尤其是在全局训练中表现不佳的流域。

Conclusion: 论文为希望利用本地数据掌握全球模型的国家预报员提供了路线图，旨在降低基于机器学习的洪水预测系统的部署门槛。

Abstract: Floods are the most common form of natural disaster and accurate flood
forecasting is essential for early warning systems. Previous work has shown
that machine learning (ML) models are a promising way to improve flood
predictions when trained on large, geographically-diverse datasets. This
requirement of global training can result in a loss of ownership for national
forecasters who cannot easily adapt the models to improve performance in their
region, preventing ML models from being operationally deployed. Furthermore,
traditional hydrology research with physics-based models suggests that local
data -- which in many cases is only accessible to local agencies -- is valuable
for improving model performance. To address these concerns, we demonstrate a
methodology of pre-training a model on a large, global dataset and then
fine-tuning that model on data from individual basins. This results in
performance increases, validating our hypothesis that there is extra
information to be captured in local data. In particular, we show that
performance increases are most significant in watersheds that underperform
during global training. We provide a roadmap for national forecasters who wish
to take ownership of global models using their own data, aiming to lower the
barrier to operational deployment of ML-based hydrological forecast systems.

</details>


### [116] [Kernel Ridge Regression for Efficient Learning of High-Capacity Hopfield Networks](https://arxiv.org/abs/2504.12561)
*Akira Tamamori*

Main category: cs.LG

TL;DR: 论文提出了一种基于核岭回归（KRR）的非迭代学习方法，用于构建高性能联想记忆模型，显著提升了训练速度，同时保持了与核逻辑回归（KLR）相当的高存储容量和噪声鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决传统Hebbian学习在Hopfield网络中容量受限的问题，以及核方法（如KLR）迭代学习速度慢的缺点。

Method: 采用核岭回归（KRR）的非迭代学习方法，通过闭式解学习对偶变量。

Result: KRR在存储容量（达到1.5的比例）和噪声鲁棒性（从约80%的损坏模式中恢复）方面与KLR表现相当，同时大幅减少训练时间。

Conclusion: KRR是一种高效构建高性能联想记忆的方法，兼具速度和性能优势。

Abstract: Hebbian learning limits Hopfield network capacity. While kernel methods like
Kernel Logistic Regression (KLR) improve performance via iterative learning, we
propose Kernel Ridge Regression (KRR) as an alternative. KRR learns dual
variables non-iteratively via a closed-form solution, offering significant
learning speed advantages. We show KRR achieves comparably high storage
capacity (reaching ratio 1.5 shown) and noise robustness (recalling from around
80% corrupted patterns) as KLR, while drastically reducing training time,
establishing KRR as an efficient method for building high-performance
associative memories.

</details>


### [117] [Evolutionary Policy Optimization](https://arxiv.org/abs/2504.12568)
*Zelal Su "Lain" Mustafaoglu,Keshav Pingali,Risto Miikkulainen*

Main category: cs.LG

TL;DR: 论文提出了一种结合进化计算和策略梯度方法的混合算法EPO，以解决强化学习中的探索-利用权衡问题。


<details>
  <summary>Details</summary>
Motivation: 策略梯度方法在利用方面表现优异，但在探索方面不足；进化计算方法擅长全局探索，但缺乏利用机制。EPO旨在结合两者的优势。

Method: EPO整合了神经进化和策略梯度方法，利用进化计算的探索能力和策略梯度的利用能力。

Result: 在Atari Pong和Breakout基准测试中，EPO在策略质量和样本效率上均优于标准策略梯度和进化计算方法。

Conclusion: EPO有效解决了强化学习中探索与利用的权衡问题，适用于需要全局探索和局部优化的任务。

Abstract: A key challenge in reinforcement learning (RL) is managing the
exploration-exploitation trade-off without sacrificing sample efficiency.
Policy gradient (PG) methods excel in exploitation through fine-grained,
gradient-based optimization but often struggle with exploration due to their
focus on local search. In contrast, evolutionary computation (EC) methods excel
in global exploration, but lack mechanisms for exploitation. To address these
limitations, this paper proposes Evolutionary Policy Optimization (EPO), a
hybrid algorithm that integrates neuroevolution with policy gradient methods
for policy optimization. EPO leverages the exploration capabilities of EC and
the exploitation strengths of PG, offering an efficient solution to the
exploration-exploitation dilemma in RL. EPO is evaluated on the Atari Pong and
Breakout benchmarks. Experimental results show that EPO improves both policy
quality and sample efficiency compared to standard PG and EC methods, making it
effective for tasks that require both exploration and local optimization.

</details>


### [118] [The Others: Naturally Isolating Out-of-Distribution Samples for Robust Open-Set Semi-Supervised Learning](https://arxiv.org/abs/2504.12569)
*You Rim Choi,Subeom Park,Seojun Heo,Eunchung Noh,Hyung-Sin Kim*

Main category: cs.LG

TL;DR: MagMatch是一种新的开放集半监督学习框架，通过原型对比学习自然隔离OOD样本，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有OSSL方法在处理未标记数据中的OOD样本时，可能排除、干扰或过度信任它们，导致次优特征空间。

Method: MagMatch通过原型对比学习隔离OOD样本，使用ID-选择性磁性模块对齐ID样本，并提出选择性磁性对齐损失动态调整对齐。

Result: 在多个数据集上，MagMatch在闭集分类准确性和OOD检测AUROC上显著优于现有方法，尤其在泛化到未见OOD数据时表现突出。

Conclusion: MagMatch通过自然隔离OOD样本和选择性对齐ID样本，有效提升了开放集半监督学习的性能。

Abstract: Open-Set Semi-Supervised Learning (OSSL) tackles the practical challenge of
learning from unlabeled data that may include both in-distribution (ID) and
unknown out-of-distribution (OOD) classes. However, existing OSSL methods form
suboptimal feature spaces by either excluding OOD samples, interfering with
them, or overtrusting their information during training. In this work, we
introduce MagMatch, a novel framework that naturally isolates OOD samples
through a prototype-based contrastive learning paradigm. Unlike conventional
methods, MagMatch does not assign any prototypes to OOD samples; instead, it
selectively aligns ID samples with class prototypes using an ID-Selective
Magnetic (ISM) module, while allowing OOD samples - the "others" - to remain
unaligned in the feature space. To support this process, we propose Selective
Magnetic Alignment (SMA) loss for unlabeled data, which dynamically adjusts
alignment based on sample confidence. Extensive experiments on diverse datasets
demonstrate that MagMatch significantly outperforms existing methods in both
closed-set classification accuracy and OOD detection AUROC, especially in
generalizing to unseen OOD data.

</details>


### [119] [Local Data Quantity-Aware Weighted Averaging for Federated Learning with Dishonest Clients](https://arxiv.org/abs/2504.12577)
*Leming Wu,Yaochu Jin,Kuangrong Hao,Han Yu*

Main category: cs.LG

TL;DR: 提出了一种名为FedDua的新方法，通过梯度分析准确预测客户端数据量，解决联邦学习中因数据量不准确导致的模型偏差问题，实验表明性能提升3.17%。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中客户端可能虚报数据量，导致加权平均聚合方法产生模型偏差，影响全局模型性能。

Method: 提出FedDua方法，通过分析客户端上传的梯度准确预测其数据量，并集成到现有联邦学习算法中。

Result: 在三个基准数据集上，FedDua平均性能比现有方法提升3.17%。

Conclusion: FedDua有效解决了数据量不准确导致的偏差问题，提升了联邦学习的模型性能。

Abstract: Federated learning (FL) enables collaborative training of deep learning
models without requiring data to leave local clients, thereby preserving client
privacy. The aggregation process on the server plays a critical role in the
performance of the resulting FL model. The most commonly used aggregation
method is weighted averaging based on the amount of data from each client,
which is thought to reflect each client's contribution. However, this method is
prone to model bias, as dishonest clients might report inaccurate training data
volumes to the server, which is hard to verify. To address this issue, we
propose a novel secure \underline{Fed}erated \underline{D}ata
q\underline{u}antity-\underline{a}ware weighted averaging method (FedDua). It
enables FL servers to accurately predict the amount of training data from each
client based on their local model gradients uploaded. Furthermore, it can be
seamlessly integrated into any FL algorithms that involve server-side model
aggregation. Extensive experiments on three benchmarking datasets demonstrate
that FedDua improves the global model performance by an average of 3.17%
compared to four popular FL aggregation methods in the presence of inaccurate
client data volume declarations.

</details>


### [120] [ChemKANs for Combustion Chemistry Modeling and Acceleration](https://arxiv.org/abs/2504.12580)
*Benjamin C. Koenig,Suyong Kim,Sili Deng*

Main category: cs.LG

TL;DR: ChemKANs结合KAN-ODEs框架与物理知识，提升了燃烧化学动力学模型的推断效率和准确性，解决了数据稀疏和噪声问题。


<details>
  <summary>Details</summary>
Motivation: 燃烧化学动力学模型推断因大尺度ODE系统和时间尺度分离而具有挑战性，传统机器学习方法难以应对强非线性和数值刚度问题。

Method: 通过将KAN-ODEs框架与物理知识（如动力学和热力学定律）结合，并引入元素守恒损失项，开发了ChemKANs。

Result: ChemKANs在稀疏和噪声数据下无过拟合，且参数稀疏（仅344个参数）的模型能准确预测氢燃烧化学，计算速度提升2倍。

Conclusion: ChemKANs在燃烧物理和化学动力学中具有潜力，展示了KAN-ODEs在更大规模问题中的可扩展性。

Abstract: Efficient chemical kinetic model inference and application for combustion
problems is challenging due to large ODE systems and wideley separated time
scales. Machine learning techniques have been proposed to streamline these
models, though strong nonlinearity and numerical stiffness combined with noisy
data sources makes their application challenging. The recently developed
Kolmogorov-Arnold Networks (KANs) and KAN ordinary differential equations
(KAN-ODEs) have been demonstrated as powerful tools for scientific applications
thanks to their rapid neural scaling, improved interpretability, and smooth
activation functions. Here, we develop ChemKANs by augmenting the KAN-ODE
framework with physical knowledge of the flow of information through the
relevant kinetic and thermodynamic laws, as well as an elemental conservation
loss term. This novel framework encodes strong inductive bias that enables
streamlined training and higher accuracy predictions, while facilitating
parameter sparsity through full sharing of information across all inputs and
outputs. In a model inference investigation, we find that ChemKANs exhibit no
overfitting or model degradation when tasked with extracting predictive models
from data that is both sparse and noisy, a task that a standard DeepONet
struggles to accomplish. Next, we find that a remarkably parameter-lean ChemKAN
(only 344 parameters) can accurately represent hydrogen combustion chemistry,
providing a 2x acceleration over the detailed chemistry in a solver that is
generalizable to larger-scale turbulent flow simulations. These demonstrations
indicate potential for ChemKANs in combustion physics and chemical kinetics,
and demonstrate the scalability of generic KAN-ODEs in significantly larger and
more numerically challenging problems than previously studied.

</details>


### [121] [Software Engineering Principles for Fairer Systems: Experiments with GroupCART](https://arxiv.org/abs/2504.12587)
*Kewen Peng,Hao Zhuo,Yicheng Yang,Tim Menzies*

Main category: cs.LG

TL;DR: GroupCART是一种基于树的集成优化方法，旨在构建公平的分类模型，通过同时优化目标属性和保护属性的熵，避免对受保护社会群体的歧视。


<details>
  <summary>Details</summary>
Motivation: 传统决策树仅优化目标属性的信息增益，可能导致对受保护群体（如性别、种族）的不公平歧视。

Method: 提出GroupCART，通过同时优化目标属性和保护属性的熵，避免模型构建中的偏见。

Result: 实验表明，GroupCART无需数据转换即可实现更公平的模型，且性能下降最小。支持自定义权重，灵活平衡预测性能和公平性。

Conclusion: 通过多任务、公平感知学习，可以减轻决策树模型中的算法偏见。

Abstract: Discrimination-aware classification aims to make accurate predictions while
satisfying fairness constraints. Traditional decision tree learners typically
optimize for information gain in the target attribute alone, which can result
in models that unfairly discriminate against protected social groups (e.g.,
gender, ethnicity). Motivated by these shortcomings, we propose GroupCART, a
tree-based ensemble optimizer that avoids bias during model construction by
optimizing not only for decreased entropy in the target attribute but also for
increased entropy in protected attributes. Our experiments show that GroupCART
achieves fairer models without data transformation and with minimal performance
degradation. Furthermore, the method supports customizable weighting, offering
a smooth and flexible trade-off between predictive performance and fairness
based on user requirements. These results demonstrate that algorithmic bias in
decision tree models can be mitigated through multi-task, fairness-aware
learning. All code and datasets used in this study are available at:
https://github.com/anonymous12138/groupCART.

</details>


### [122] [Simplifying Graph Transformers](https://arxiv.org/abs/2504.12588)
*Liheng Ma,Soumyasundar Pal,Yingxue Zhang,Philip H. S. Torr,Mark Coates*

Main category: cs.LG

TL;DR: 论文提出三种简单修改，使普通Transformer适用于图学习，无需复杂架构变化，并在多个图数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有图Transformer架构复杂，难以直接应用Transformer的训练进展，因此需要简化方法。

Method: 1. 使用简化的$L_2$注意力衡量token的幅度接近性；2. 自适应均方根归一化保留token幅度信息；3. 共享编码器的相对位置编码偏置。

Result: 在多种图数据集上取得显著性能提升，并在图同构表达性测试中表现突出。

Conclusion: 提出的简单修改有效提升了Transformer在图学习中的表现，且无需复杂架构变化。

Abstract: Transformers have attained outstanding performance across various modalities,
employing scaled-dot-product (SDP) attention mechanisms. Researchers have
attempted to migrate Transformers to graph learning, but most advanced Graph
Transformers are designed with major architectural differences, either
integrating message-passing or incorporating sophisticated attention
mechanisms. These complexities prevent the easy adoption of Transformer
training advances. We propose three simple modifications to the plain
Transformer to render it applicable to graphs without introducing major
architectural distortions. Specifically, we advocate for the use of (1)
simplified $L_2$ attention to measure the magnitude closeness of tokens; (2)
adaptive root-mean-square normalization to preserve token magnitude
information; and (3) a relative positional encoding bias with a shared encoder.
Significant performance gains across a variety of graph datasets justify the
effectiveness of our proposed modifications. Furthermore, empirical evaluation
on the expressiveness benchmark reveals noteworthy realized expressiveness in
the graph isomorphism.

</details>


### [123] [Efficient MAP Estimation of LLM Judgment Performance with Prior Transfer](https://arxiv.org/abs/2504.12589)
*Huaizhi Qu,Inyoung Choi,Zhen Tan,Song Wang,Sukwon Yun,Qi Long,Faizan Siddiqui,Kwonjoon Lee,Tianlong Chen*

Main category: cs.LG

TL;DR: 本文提出了一种基于最大后验概率（MAP）的框架BetaConform，用于高效且精确地估计LLM集成判断的性能。通过Beta-Binomial混合分布建模、自适应停止和先验转移机制，BetaConform在少量标注样本下实现了理论保证的性能估计。


<details>
  <summary>Details</summary>
Motivation: LLM集成判断的准确性估计缺乏高效方法，尤其是在标注数据稀缺时。

Method: 提出Beta-Binomial混合分布模型，结合自适应停止和先验转移机制，构建BetaConform框架。

Result: 在TruthfulQA数据集上，仅需10个样本，BetaConform的误差低至3.37%。

Conclusion: BetaConform为LLM集成判断的性能估计提供了一种高效且精确的解决方案。

Abstract: LLM ensembles are widely used for LLM judges. However, how to estimate their
accuracy, especially in an efficient way, is unknown. In this paper, we present
a principled maximum a posteriori (MAP) framework for an economical and precise
estimation of the performance of LLM ensemble judgment. We first propose a
mixture of Beta-Binomial distributions to model the judgment distribution,
revising from the vanilla Binomial distribution. Next, we introduce a conformal
prediction-driven approach that enables adaptive stopping during iterative
sampling to balance accuracy with efficiency. Furthermore, we design a prior
transfer mechanism that utilizes learned distributions on open-source datasets
to improve estimation on a target dataset when only scarce annotations are
available. Finally, we present BetaConform, a framework that integrates our
distribution assumption, adaptive stopping, and the prior transfer mechanism to
deliver a theoretically guaranteed distribution estimation of LLM ensemble
judgment with minimum labeled samples. BetaConform is also validated
empirically. For instance, with only 10 samples from the TruthfulQA dataset,
for a Llama ensembled judge, BetaConform gauges its performance with error
margin as small as 3.37%.

</details>


### [124] [Meta-Dependence in Conditional Independence Testing](https://arxiv.org/abs/2504.12594)
*Bijan Mazaheri,Jiaqi Zhang,Caroline Uhler*

Main category: cs.LG

TL;DR: 该论文研究了约束性因果发现算法中条件独立性之间的“元依赖”关系，提出了一种基于信息投影的简单计算度量方法。


<details>
  <summary>Details</summary>
Motivation: 因果发现算法依赖于因果马尔可夫条件和忠实性假设，但有限数据可能导致这些假设失效，尤其是条件独立性之间的“元依赖”问题尚未被充分研究。

Method: 通过几何直觉，将每个条件独立性约束视为联合分布空间中的一个流形，利用信息投影度量这些流形之间的“元依赖”。

Result: 提出了一种简单计算的方法，并通过合成和真实数据验证了其有效性。

Conclusion: 研究揭示了条件独立性之间的“元依赖”对因果发现算法的影响，为改进算法提供了理论基础。

Abstract: Constraint-based causal discovery algorithms utilize many statistical tests
for conditional independence to uncover networks of causal dependencies. These
approaches to causal discovery rely on an assumed correspondence between the
graphical properties of a causal structure and the conditional independence
properties of observed variables, known as the causal Markov condition and
faithfulness. Finite data yields an empirical distribution that is "close" to
the actual distribution. Across these many possible empirical distributions,
the correspondence to the graphical properties can break down for different
conditional independencies, and multiple violations can occur at the same time.
We study this "meta-dependence" between conditional independence properties
using the following geometric intuition: each conditional independence property
constrains the space of possible joint distributions to a manifold. The
"meta-dependence" between conditional independences is informed by the position
of these manifolds relative to the true probability distribution. We provide a
simple-to-compute measure of this meta-dependence using information projections
and consolidate our findings empirically using both synthetic and real-world
data.

</details>


### [125] [Stochastic Gradient Descent in Non-Convex Problems: Asymptotic Convergence with Relaxed Step-Size via Stopping Time Methods](https://arxiv.org/abs/2504.12601)
*Ruinan Jin,Difei Cheng,Hong Qiao,Xin Shi,Shaodong Liu,Bo Zhang*

Main category: cs.LG

TL;DR: 论文提出了一种基于停止时间方法的新分析框架，放宽了SGD收敛分析的步长条件和高阶矩假设。


<details>
  <summary>Details</summary>
Motivation: 现有SGD收敛分析通常依赖强假设和严格步长条件，限制了实际应用。

Method: 引入停止时间方法，分析SGD在更宽松步长条件和较弱假设下的收敛性。

Result: 证明了非凸设置下SGD的几乎必然收敛和L2收敛，放宽了损失函数的全局Lipschitz连续性和随机梯度高阶矩的有界性要求。

Conclusion: 研究结果显著放宽了假设，增强了理论结果的通用性和实际适用性。

Abstract: Stochastic Gradient Descent (SGD) is widely used in machine learning
research. Previous convergence analyses of SGD under the vanishing step-size
setting typically require Robbins-Monro conditions. However, in practice, a
wider variety of step-size schemes are frequently employed, yet existing
convergence results remain limited and often rely on strong assumptions. This
paper bridges this gap by introducing a novel analytical framework based on a
stopping-time method, enabling asymptotic convergence analysis of SGD under
more relaxed step-size conditions and weaker assumptions. In the non-convex
setting, we prove the almost sure convergence of SGD iterates for step-sizes $
\{ \epsilon_t \}_{t \geq 1} $ satisfying $\sum_{t=1}^{+\infty} \epsilon_t =
+\infty$ and $\sum_{t=1}^{+\infty} \epsilon_t^p < +\infty$ for some $p > 2$.
Compared with previous studies, our analysis eliminates the global Lipschitz
continuity assumption on the loss function and relaxes the boundedness
requirements for higher-order moments of stochastic gradients. Building upon
the almost sure convergence results, we further establish $L_2$ convergence.
These significantly relaxed assumptions make our theoretical results more
general, thereby enhancing their applicability in practical scenarios.

</details>


### [126] [Machine Learning Methods for Gene Regulatory Network Inference](https://arxiv.org/abs/2504.12610)
*Akshata Hegde,Tom Nguyen,Jianlin Cheng*

Main category: cs.LG

TL;DR: 本文综述了基于机器学习的基因调控网络（GRN）推断方法，重点介绍了深度学习技术的最新进展及其在提高推断性能中的作用，并探讨了未来发展方向。


<details>
  <summary>Details</summary>
Motivation: 基因调控网络（GRN）是控制基因表达和调控的复杂生物系统。随着计算生物学和高通量测序技术的发展，GRN推断的准确性显著提高。本文旨在总结机器学习在GRN推断中的应用，并探讨其未来潜力。

Method: 文章综述了监督学习、无监督学习、半监督学习和对比学习等机器学习方法在GRN推断中的应用，并介绍了常用的数据集和评估指标。

Result: 现代机器学习方法，尤其是深度学习技术，显著提升了GRN推断的性能，能够更准确地揭示基因调控关系。

Conclusion: 本文强调了深度学习在GRN推断中的重要作用，并提出了未来改进的方向，为进一步研究提供了参考。

Abstract: Gene Regulatory Networks (GRNs) are intricate biological systems that control
gene expression and regulation in response to environmental and developmental
cues. Advances in computational biology, coupled with high throughput
sequencing technologies, have significantly improved the accuracy of GRN
inference and modeling. Modern approaches increasingly leverage artificial
intelligence (AI), particularly machine learning techniques including
supervised, unsupervised, semi-supervised, and contrastive learning to analyze
large scale omics data and uncover regulatory gene interactions. To support
both the application of GRN inference in studying gene regulation and the
development of novel machine learning methods, we present a comprehensive
review of machine learning based GRN inference methodologies, along with the
datasets and evaluation metrics commonly used. Special emphasis is placed on
the emerging role of cutting edge deep learning techniques in enhancing
inference performance. The potential future directions for improving GRN
inference are also discussed.

</details>


### [127] [Uncertainty Quantification in Graph Neural Networks with Shallow Ensembles](https://arxiv.org/abs/2504.12627)
*Tirtha Vinchurkar,Kareem Abdelmaqsoud,John R. Kitchin*

Main category: cs.LG

TL;DR: 论文探讨了如何通过不确定性量化（UQ）技术提升图神经网络（GNN）在材料建模中的可靠性，提出了一种轻量级方法DPOSE，并验证了其在区分域内外数据方面的有效性。


<details>
  <summary>Details</summary>
Motivation: GNN在材料建模中表现优异，但对域外数据的预测不可靠且难以识别，因此需要一种高效的不确定性量化方法。

Method: 采用Direct Propagation of Shallow Ensembles（DPOSE）作为轻量级UQ技术，并将其集成到SchNet模型中，评估其在QM9、OC20和Gold Molecular Dynamics等数据集上的表现。

Result: DPOSE能有效区分域内外数据，对未观测到的分子和材料类别表现出更高的不确定性。

Conclusion: 轻量级UQ方法（如DPOSE）可提升GNN在材料建模中的鲁棒性，为未来与主动学习策略的结合奠定了基础。

Abstract: Machine-learned potentials (MLPs) have revolutionized materials discovery by
providing accurate and efficient predictions of molecular and material
properties. Graph Neural Networks (GNNs) have emerged as a state-of-the-art
approach due to their ability to capture complex atomic interactions. However,
GNNs often produce unreliable predictions when encountering out-of-domain data
and it is difficult to identify when that happens. To address this challenge,
we explore Uncertainty Quantification (UQ) techniques, focusing on Direct
Propagation of Shallow Ensembles (DPOSE) as a computationally efficient
alternative to deep ensembles. By integrating DPOSE into the SchNet model, we
assess its ability to provide reliable uncertainty estimates across diverse
Density Functional Theory datasets, including QM9, OC20, and Gold Molecular
Dynamics. Our findings often demonstrate that DPOSE successfully distinguishes
between in-domain and out-of-domain samples, exhibiting higher uncertainty for
unobserved molecule and material classes. This work highlights the potential of
lightweight UQ methods in improving the robustness of GNN-based materials
modeling and lays the foundation for future integration with active learning
strategies.

</details>


### [128] [Quantum Computing Supported Adversarial Attack-Resilient Autonomous Vehicle Perception Module for Traffic Sign Classification](https://arxiv.org/abs/2504.12644)
*Reek Majumder,Mashrur Chowdhury,Sakib Mahmud Khan,Zadid Khan,Fahim Ahmad,Frank Ngeni,Gurcan Comert,Judith Mwakalonge,Dimitra Michalaka*

Main category: cs.LG

TL;DR: 该论文研究了混合经典-量子深度学习模型（HCQ-DL）在对抗攻击下的鲁棒性，并展示了其在自动驾驶车辆感知模块中的优越性能。


<details>
  <summary>Details</summary>
Motivation: 对抗攻击可能导致深度学习模型输出错误分类，对自动驾驶车辆的安全性造成严重威胁。因此，研究更鲁棒的模型至关重要。

Method: 使用AlexNet和VGG-16作为特征提取器，结合量子系统构建HCQ-DL模型，测试了1000多个量子电路，并评估了模型在对抗攻击和无攻击场景下的性能。

Result: HCQ-DL模型在无攻击场景下准确率超过95%，在GA和FGSA攻击下超过91%，远高于经典模型（C-DL）。在PGD攻击下，HCQ-DL模型准确率为85%，而C-DL模型低于21%。

Conclusion: HCQ-DL模型在对抗攻击下表现出更高的准确性和鲁棒性，适用于自动驾驶车辆的感知模块。

Abstract: Deep learning (DL)-based image classification models are essential for
autonomous vehicle (AV) perception modules since incorrect categorization might
have severe repercussions. Adversarial attacks are widely studied cyberattacks
that can lead DL models to predict inaccurate output, such as incorrectly
classified traffic signs by the perception module of an autonomous vehicle. In
this study, we create and compare hybrid classical-quantum deep learning
(HCQ-DL) models with classical deep learning (C-DL) models to demonstrate
robustness against adversarial attacks for perception modules. Before feeding
them into the quantum system, we used transfer learning models, alexnet and
vgg-16, as feature extractors. We tested over 1000 quantum circuits in our
HCQ-DL models for projected gradient descent (PGD), fast gradient sign attack
(FGSA), and gradient attack (GA), which are three well-known untargeted
adversarial approaches. We evaluated the performance of all models during
adversarial attacks and no-attack scenarios. Our HCQ-DL models maintain
accuracy above 95\% during a no-attack scenario and above 91\% for GA and FGSA
attacks, which is higher than C-DL models. During the PGD attack, our
alexnet-based HCQ-DL model maintained an accuracy of 85\% compared to C-DL
models that achieved accuracies below 21\%. Our results highlight that the
HCQ-DL models provide improved accuracy for traffic sign classification under
adversarial settings compared to their classical counterparts.

</details>


### [129] [Feature selection based on cluster assumption in PU learning](https://arxiv.org/abs/2504.12651)
*Motonobu Uchikoshi,Youhei Akimoto*

Main category: cs.LG

TL;DR: 论文提出了一种基于PU学习中聚类假设的特征选择方法FSCPU，通过显式设计目标函数优化特征选择，实验证明其优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 在PU学习中，传统特征选择方法可能因将未标记数据视为负样本而无法捕捉正样本的统计特性，导致性能不佳。

Method: FSCPU将特征选择问题建模为二元优化任务，目标函数显式结合了PU学习中的聚类假设。

Result: 在合成数据集和三个公开数据集上的实验表明，FSCPU在不同数据条件下均表现优异，且在下游分类任务中竞争力强。

Conclusion: FSCPU在PU学习场景中提供了一种有效的特征选择方法，即使聚类假设不完全成立，仍能保持良好性能。

Abstract: Feature selection is essential for efficient data mining and sometimes
encounters the positive-unlabeled (PU) learning scenario, where only a few
positive labels are available, while most data remains unlabeled. In certain
real-world PU learning tasks, data subjected to adequate feature selection
often form clusters with concentrated positive labels. Conventional feature
selection methods that treat unlabeled data as negative may fail to capture the
statistical characteristics of positive data in such scenarios, leading to
suboptimal performance. To address this, we propose a novel feature selection
method based on the cluster assumption in PU learning, called FSCPU. FSCPU
formulates the feature selection problem as a binary optimization task, with an
objective function explicitly designed to incorporate the cluster assumption in
the PU learning setting. Experiments on synthetic datasets demonstrate the
effectiveness of FSCPU across various data conditions. Moreover, comparisons
with 10 conventional algorithms on three open datasets show that FSCPU achieves
competitive performance in downstream classification tasks, even when the
cluster assumption does not strictly hold.

</details>


### [130] [VLMGuard-R1: Proactive Safety Alignment for VLMs via Reasoning-Driven Prompt Optimization](https://arxiv.org/abs/2504.12661)
*Menglan Chen,Xianghe Pang,Jingjing Dong,WenHao Wang,Yaxin Du,Siheng Chen*

Main category: cs.LG

TL;DR: 论文提出了一种多模态推理驱动的提示重写方法VLMGuard-R1，通过动态解析文本-图像交互提升视觉语言模型的安全性。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型的多模态复杂性可能带来传统安全措施无法应对的威胁，因此需要一种新的安全对齐方法。

Method: 提出VLMGuard-R1框架，通过三阶段推理管道训练提示重写器，动态优化用户输入以增强安全性。

Result: 在三个基准测试中，VLMGuard-R1显著提升了安全性，尤其在SIUO基准上平均安全性提高了43.59%。

Conclusion: 多模态推理驱动的提示重写是提升视觉语言模型安全性的有效方向。

Abstract: Aligning Vision-Language Models (VLMs) with safety standards is essential to
mitigate risks arising from their multimodal complexity, where integrating
vision and language unveils subtle threats beyond the reach of conventional
safeguards. Inspired by the insight that reasoning across modalities is key to
preempting intricate vulnerabilities, we propose a novel direction for VLM
safety: multimodal reasoning-driven prompt rewriting. To this end, we introduce
VLMGuard-R1, a proactive framework that refines user inputs through a
reasoning-guided rewriter, dynamically interpreting text-image interactions to
deliver refined prompts that bolster safety across diverse VLM architectures
without altering their core parameters. To achieve this, we devise a
three-stage reasoning pipeline to synthesize a dataset that trains the rewriter
to infer subtle threats, enabling tailored, actionable responses over generic
refusals. Extensive experiments across three benchmarks with five VLMs reveal
that VLMGuard-R1 outperforms four baselines. In particular, VLMGuard-R1
achieves a remarkable 43.59\% increase in average safety across five models on
the SIUO benchmark.

</details>


### [131] [Predicting Driver's Perceived Risk: a Model Based on Semi-Supervised Learning Strategy](https://arxiv.org/abs/2504.12665)
*Siwei Huang,Chenhao Yang,Chuan Hu*

Main category: cs.LG

TL;DR: 论文提出了一种驾驶员主观感知风险（DSPR）模型，通过半监督学习方法预测驾驶员实时风险评分（SRRs），准确率达87.91%，优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 驾驶员对风险的感知影响其对自动驾驶系统（ADS）的接受度和信任，但现有方法难以评估主观风险。

Method: 采用卷积神经网络和双向长短期记忆网络（CNN-Bi-LSTM-TPA）结合半监督学习策略，预测驾驶员在实验中的实时风险评分。

Result: DSPR模型预测准确率最高（87.91%），半监督策略提升20.12%准确率，CNN-Bi-LSTM-TPA网络表现最佳。

Conclusion: 该研究为评估驾驶员感知风险提供了有效方法，有助于提升自动驾驶安全性和驾驶员信任。

Abstract: Drivers' perception of risk determines their acceptance, trust, and use of
the Automated Driving Systems (ADSs). However, perceived risk is subjective and
difficult to evaluate using existing methods. To address this issue, a driver's
subjective perceived risk (DSPR) model is proposed, regarding perceived risk as
a dynamically triggered mechanism with anisotropy and attenuation. 20
participants are recruited for a driver-in-the-loop experiment to report their
real-time subjective risk ratings (SRRs) when experiencing various automatic
driving scenarios. A convolutional neural network and bidirectional long
short-term memory network with temporal pattern attention (CNN-Bi-LSTM-TPA) is
embedded into a semi-supervised learning strategy to predict SRRs, aiming to
reduce data noise caused by subjective randomness of participants. The results
illustrate that DSPR achieves the highest prediction accuracy of 87.91% in
predicting SRRs, compared to three state-of-the-art risk models. The
semi-supervised strategy improves accuracy by 20.12%. Besides, CNN-Bi-LSTM-TPA
network presents the highest accuracy among four different LSTM structures.
This study offers an effective method for assessing driver's perceived risk,
providing support for the safety enhancement of ADS and driver's trust
improvement.

</details>


### [132] [Physics Informed Constrained Learning of Dynamics from Static Data](https://arxiv.org/abs/2504.12675)
*Pengtao Dang,Tingbo Guo,Sha Cao,Chi Zhang*

Main category: cs.LG

TL;DR: 论文提出了一种新的物理信息神经网络（PINN）学习范式——约束学习（Constrained Learning），通过非时间序列或部分观测数据近似一阶导数或运动，并开发了优化方法MPOCtrL。


<details>
  <summary>Details</summary>
Motivation: 传统PINN框架依赖完整的时间序列数据，但许多系统难以获取此类数据，因此需要一种能处理非时间序列或部分观测数据的方法。

Method: 提出了约束学习范式及其数学框架，并开发了MPOCtrL优化方法，平衡物理模型与观测数据的拟合。

Result: 实验表明，MPOCtrL能有效检测数据与系统物理特性间的非线性依赖，并在代谢通量分析任务中优于现有数据驱动方法。

Conclusion: 约束学习及MPOCtrL为处理数据稀缺或高维问题提供了新思路，尤其在复杂系统建模中表现优异。

Abstract: A physics-informed neural network (PINN) models the dynamics of a system by
integrating the governing physical laws into the architecture of a neural
network. By enforcing physical laws as constraints, PINN overcomes challenges
with data scarsity and potentially high dimensionality. Existing PINN
frameworks rely on fully observed time-course data, the acquisition of which
could be prohibitive for many systems. In this study, we developed a new PINN
learning paradigm, namely Constrained Learning, that enables the approximation
of first-order derivatives or motions using non-time course or partially
observed data. Computational principles and a general mathematical formulation
of Constrained Learning were developed. We further introduced MPOCtrL (Message
Passing Optimization-based Constrained Learning) an optimization approach
tailored for the Constrained Learning framework that strives to balance the
fitting of physical models and observed data. Its code is available at github
link: https://github.com/ptdang1001/MPOCtrL Experiments on synthetic and
real-world data demonstrated that MPOCtrL can effectively detect the nonlinear
dependency between observed data and the underlying physical properties of the
system. In particular, on the task of metabolic flux analysis, MPOCtrL
outperforms all existing data-driven flux estimators.

</details>


### [133] [Convergence and Implicit Bias of Gradient Descent on Continual Linear Classification](https://arxiv.org/abs/2504.12712)
*Hyunji Jung,Hanseul Cho,Chulhee Yun*

Main category: cs.LG

TL;DR: 论文研究了在多任务线性分类中的持续学习问题，通过梯度下降（GD）按固定迭代次数顺序训练任务。结果显示，当任务联合线性可分且按循环/随机顺序呈现时，分类器方向收敛于联合最大间隔解。此外，循环顺序下任务对齐与灾难性遗忘和后向知识转移密切相关，遗忘量随循环次数减少。对于非联合可分任务，模型收敛于联合损失函数的唯一最小值。


<details>
  <summary>Details</summary>
Motivation: 探索在多任务持续学习中，梯度下降训练如何影响分类器的方向收敛性，以及任务顺序对遗忘和知识转移的影响。

Method: 通过梯度下降（GD）按固定迭代次数顺序训练多个线性分类任务，分析任务顺序（循环/随机）对分类器方向收敛性的影响。

Result: 任务联合可分时，分类器方向收敛于联合最大间隔解；循环顺序下任务对齐与遗忘和知识转移相关，遗忘量随循环减少；非联合可分任务时，模型收敛于联合损失函数的最小值。

Conclusion: 梯度下降在多任务持续学习中能有效收敛于联合解，任务顺序对遗忘和知识转移有重要影响。

Abstract: We study continual learning on multiple linear classification tasks by
sequentially running gradient descent (GD) for a fixed budget of iterations per
task. When all tasks are jointly linearly separable and are presented in a
cyclic/random order, we show the directional convergence of the trained linear
classifier to the joint (offline) max-margin solution. This is surprising
because GD training on a single task is implicitly biased towards the
individual max-margin solution for the task, and the direction of the joint
max-margin solution can be largely different from these individual solutions.
Additionally, when tasks are given in a cyclic order, we present a
non-asymptotic analysis on cycle-averaged forgetting, revealing that (1)
alignment between tasks is indeed closely tied to catastrophic forgetting and
backward knowledge transfer and (2) the amount of forgetting vanishes to zero
as the cycle repeats. Lastly, we analyze the case where the tasks are no longer
jointly separable and show that the model trained in a cyclic order converges
to the unique minimum of the joint loss function.

</details>


### [134] [Hierarchical Vector Quantized Graph Autoencoder with Annealing-Based Code Selection](https://arxiv.org/abs/2504.12715)
*Long Zeng,Jianxiang Yu,Jiapeng Zhu,Qingsong Zhong,Xiang Li*

Main category: cs.LG

TL;DR: 本文探讨了图自监督学习中向量量化的应用，提出了解决代码本利用不足和稀疏问题的策略，并在多个任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有图自监督学习方法依赖扰动，可能破坏图的固有信息，而VQ-VAE在图数据中的应用尚未充分探索。

Method: 提出基于退火的编码策略和分层双层代码本，以优化向量量化在图自编码器中的应用。

Result: 模型在自监督链接预测和节点分类任务中优于16种基线方法。

Conclusion: 向量量化能显著增强图自编码器捕捉拓扑的能力，提出的策略有效解决了相关挑战。

Abstract: Graph self-supervised learning has gained significant attention recently.
However, many existing approaches heavily depend on perturbations, and
inappropriate perturbations may corrupt the graph's inherent information. The
Vector Quantized Variational Autoencoder (VQ-VAE) is a powerful autoencoder
extensively used in fields such as computer vision; however, its application to
graph data remains underexplored. In this paper, we provide an empirical
analysis of vector quantization in the context of graph autoencoders,
demonstrating its significant enhancement of the model's capacity to capture
graph topology. Furthermore, we identify two key challenges associated with
vector quantization when applying in graph data: codebook underutilization and
codebook space sparsity. For the first challenge, we propose an annealing-based
encoding strategy that promotes broad code utilization in the early stages of
training, gradually shifting focus toward the most effective codes as training
progresses. For the second challenge, we introduce a hierarchical two-layer
codebook that captures relationships between embeddings through clustering. The
second layer codebook links similar codes, encouraging the model to learn
closer embeddings for nodes with similar features and structural topology in
the graph. Our proposed model outperforms 16 representative baseline methods in
self-supervised link prediction and node classification tasks across multiple
datasets.

</details>


### [135] [TimeCapsule: Solving the Jigsaw Puzzle of Long-Term Time Series Forecasting with Compressed Predictive Representations](https://arxiv.org/abs/2504.12721)
*Yihang Lu,Yangyang Xu,Qitao Qing,Xianwei Meng*

Main category: cs.LG

TL;DR: 论文提出TimeCapsule模型，通过高维信息压缩简化LTSF任务，结合冗余减少和多尺度建模，在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前LTSF模型过于复杂且性能不如简单模型，论文旨在通过统一和简化核心思想提升效率。

Method: 将时间序列建模为3D张量，利用模式乘积捕捉多模式依赖并进行维度压缩，结合JEPA监控预测表示学习。

Result: 在多个挑战性基准测试中，TimeCapsule实现了最先进的性能。

Conclusion: TimeCapsule通过简化框架统一核心思想，为LTSF任务提供了高效且强大的解决方案。

Abstract: Recent deep learning models for Long-term Time Series Forecasting (LTSF)
often emphasize complex, handcrafted designs, while simpler architectures like
linear models or MLPs have often outperformed these intricate solutions. In
this paper, we revisit and organize the core ideas behind several key
techniques, such as redundancy reduction and multi-scale modeling, which are
frequently employed in advanced LTSF models. Our goal is to streamline these
ideas for more efficient deep learning utilization. To this end, we introduce
TimeCapsule, a model built around the principle of high-dimensional information
compression that unifies these techniques in a generalized yet simplified
framework. Specifically, we model time series as a 3D tensor, incorporating
temporal, variate, and level dimensions, and leverage mode production to
capture multi-mode dependencies while achieving dimensionality compression. We
propose an internal forecast within the compressed representation domain,
supported by the Joint-Embedding Predictive Architecture (JEPA), to monitor the
learning of predictive representations. Extensive experiments on challenging
benchmarks demonstrate the versatility of our method, showing that TimeCapsule
can achieve state-of-the-art performance.

</details>


### [136] [GPMFS: Global Foundation and Personalized Optimization for Multi-Label Feature Selection](https://arxiv.org/abs/2504.12740)
*Yifan Cao,Zhilong Mi,Ziqiao Yin,Binghui Guo,Jin Dong*

Main category: cs.LG

TL;DR: 论文提出了一种名为GPMFS的新方法，通过结合全局特征和个性化优化解决高维多标签学习中的维度诅咒问题。


<details>
  <summary>Details</summary>
Motivation: 现有多标签特征选择方法主要关注全局特征，忽略了标签的个性化需求，影响了性能。

Method: GPMFS首先利用标签相关性识别全局特征，然后通过阈值控制策略为每个标签自适应补充个性化特征。

Result: 实验表明GPMFS在多个真实数据集上表现优异，同时保持强解释性和鲁棒性。

Conclusion: GPMFS证明了个性化特征选择方法的必要性和潜在应用价值。

Abstract: As artificial intelligence methods are increasingly applied to complex task
scenarios, high dimensional multi-label learning has emerged as a prominent
research focus. At present, the curse of dimensionality remains one of the
major bottlenecks in high-dimensional multi-label learning, which can be
effectively addressed through multi-label feature selection methods. However,
existing multi-label feature selection methods mostly focus on identifying
global features shared across all labels, which overlooks personalized
characteristics and specific requirements of individual labels. This
global-only perspective may limit the ability to capture label-specific
discriminative information, thereby affecting overall performance. In this
paper, we propose a novel method called GPMFS (Global Foundation and
Personalized Optimization for Multi-Label Feature Selection). GPMFS firstly
identifies global features by exploiting label correlations, then adaptively
supplements each label with a personalized subset of discriminative features
using a threshold-controlled strategy. Experiments on multiple real-world
datasets demonstrate that GPMFS achieves superior performance while maintaining
strong interpretability and robustness. Furthermore, GPMFS provides insights
into the label-specific strength across different multi-label datasets, thereby
demonstrating the necessity and potential applicability of personalized feature
selection approaches.

</details>


### [137] [Decentralized Nonconvex Composite Federated Learning with Gradient Tracking and Momentum](https://arxiv.org/abs/2504.12742)
*Yuan Zhou,Xinli Shi,Xuelong Li,Jiachen Zhong,Guanghui Wen,Jinde Cao*

Main category: cs.LG

TL;DR: 论文提出了一种名为DEPOSITUM的去中心化非凸复合联邦学习算法，通过近似全局梯度和引入动量减少方差，降低通信成本，并在理论上和实验中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习依赖服务器-客户端架构，而DFL消除了这一依赖，但非凸复合优化问题在DFL中的研究仍不足。

Method: 基于近端随机梯度跟踪，引入动量减少方差，支持客户端变量本地更新以减少通信成本。

Result: 理论分析表明DEPOSITUM达到预期ε-稳定点，迭代复杂度为O(1/ε²)，误差以O(1/T)速率下降，实验验证其优于其他算法。

Conclusion: DEPOSITUM在去中心化非凸复合联邦学习中表现优异，具有理论保证和实际应用价值。

Abstract: Decentralized Federated Learning (DFL) eliminates the reliance on the
server-client architecture inherent in traditional federated learning,
attracting significant research interest in recent years. Simultaneously, the
objective functions in machine learning tasks are often nonconvex and
frequently incorporate additional, potentially nonsmooth regularization terms
to satisfy practical requirements, thereby forming nonconvex composite
optimization problems. Employing DFL methods to solve such general optimization
problems leads to the formulation of Decentralized Nonconvex Composite
Federated Learning (DNCFL), a topic that remains largely underexplored. In this
paper, we propose a novel DNCFL algorithm, termed \bf{DEPOSITUM}. Built upon
proximal stochastic gradient tracking, DEPOSITUM mitigates the impact of data
heterogeneity by enabling clients to approximate the global gradient. The
introduction of momentums in the proximal gradient descent step, replacing
tracking variables, reduces the variance introduced by stochastic gradients.
Additionally, DEPOSITUM supports local updates of client variables,
significantly reducing communication costs. Theoretical analysis demonstrates
that DEPOSITUM achieves an expected $\epsilon$-stationary point with an
iteration complexity of $\mathcal{O}(1/\epsilon^2)$. The proximal gradient,
consensus errors, and gradient estimation errors decrease at a sublinear rate
of $\mathcal{O}(1/T)$. With appropriate parameter selection, the algorithm
achieves network-independent linear speedup without requiring mega-batch
sampling. Finally, we apply DEPOSITUM to the training of neural networks on
real-world datasets, systematically examining the influence of various
hyperparameters on its performance. Comparisons with other federated composite
optimization algorithms validate the effectiveness of the proposed method.

</details>


### [138] [GraphOmni: A Comprehensive and Extendable Benchmark Framework for Large Language Models on Graph-theoretic Tasks](https://arxiv.org/abs/2504.12764)
*Hao Xu,Xiangru Jian,Xinjian Zhao,Wei Pang,Chao Zhang,Suyuchen Wang,Qixin Zhang,Joao Monteiro,Qiuzhuang Sun,Tianshu Yu*

Main category: cs.LG

TL;DR: GraphOmni是一个用于系统评估LLMs图推理能力的基准框架，通过分析图类型、序列化格式和提示方案等关键维度，揭示了当前LLMs的优势与局限。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs在图推理任务中表现不一，缺乏统一的评估标准，因此需要系统化的框架来评估和改进其能力。

Method: 提出GraphOmni框架，分析不同图类型、序列化格式和提示方案，并基于强化学习动态选择最佳序列化-提示组合。

Result: 实验表明，无单一序列化或提示策略始终最优，动态选择方法显著提高了准确性。

Conclusion: GraphOmni的模块化和可扩展设计为未来研究提供了坚实基础，推动通用图推理模型的发展。

Abstract: In this paper, we presented GraphOmni, a comprehensive benchmark framework
for systematically evaluating the graph reasoning capabilities of LLMs. By
analyzing critical dimensions, including graph types, serialization formats,
and prompt schemes, we provided extensive insights into the strengths and
limitations of current LLMs. Our empirical findings emphasize that no single
serialization or prompting strategy consistently outperforms others. Motivated
by these insights, we propose a reinforcement learning-based approach that
dynamically selects the best serialization-prompt pairings, resulting in
significant accuracy improvements. GraphOmni's modular and extensible design
establishes a robust foundation for future research, facilitating advancements
toward general-purpose graph reasoning models.

</details>


### [139] [Sign-In to the Lottery: Reparameterizing Sparse Training From Scratch](https://arxiv.org/abs/2504.12801)
*Advait Gadhikar,Tom Jacobs,Chao Zhou,Rebekka Burkholz*

Main category: cs.LG

TL;DR: 论文提出Sign-In方法，通过动态重参数化解决稀疏神经网络训练中的参数符号问题，提升性能，但仍需进一步研究以缩小与密集到稀疏训练的差距。


<details>
  <summary>Details</summary>
Motivation: 稀疏神经网络从头训练（PaI）与密集到稀疏训练之间的性能差距是高效深度学习的障碍，需解决参数初始化问题。

Method: 提出Sign-In方法，利用动态重参数化实现参数符号翻转，补充密集到稀疏训练的不足。

Result: 实验和理论表明Sign-In能提升PaI性能，但尚未完全缩小与密集到稀疏训练的差距。

Conclusion: Sign-In是解决稀疏训练参数符号问题的有效方法，但需进一步研究以完全弥合性能差距。

Abstract: The performance gap between training sparse neural networks from scratch
(PaI) and dense-to-sparse training presents a major roadblock for efficient
deep learning. According to the Lottery Ticket Hypothesis, PaI hinges on
finding a problem specific parameter initialization. As we show, to this end,
determining correct parameter signs is sufficient. Yet, they remain elusive to
PaI. To address this issue, we propose Sign-In, which employs a dynamic
reparameterization that provably induces sign flips. Such sign flips are
complementary to the ones that dense-to-sparse training can accomplish,
rendering Sign-In as an orthogonal method. While our experiments and theory
suggest performance improvements of PaI, they also carve out the main open
challenge to close the gap between PaI and dense-to-sparse training.

</details>


### [140] [Enhancing Explainability and Reliable Decision-Making in Particle Swarm Optimization through Communication Topologies](https://arxiv.org/abs/2504.12803)
*Nitin Gupta,Indu Bala,Bapi Dutta,Luis Martínez,Anupam Yadav*

Main category: cs.LG

TL;DR: 研究分析了粒子群优化（PSO）中不同通信拓扑结构（环状、星状、冯·诺依曼）对收敛和搜索行为的影响，使用IOHxplainer工具提升算法透明度和可靠性。


<details>
  <summary>Details</summary>
Motivation: 群智能算法在复杂系统优化中应用广泛，但配置和超参数不明确导致可靠性低，需研究其通信拓扑的影响。

Method: 采用IOHxplainer工具，分析不同拓扑结构对信息流、多样性和收敛速度的影响，并通过可视化和统计分析验证。

Result: 研究明确了不同拓扑结构在探索与开发之间的平衡，为特定优化任务选择合适拓扑提供了实用指南。

Conclusion: 通过提升PSO的可解释性和透明度，增强了群智能优化的鲁棒性和可信度。

Abstract: Swarm intelligence effectively optimizes complex systems across fields like
engineering and healthcare, yet algorithm solutions often suffer from low
reliability due to unclear configurations and hyperparameters. This study
analyzes Particle Swarm Optimization (PSO), focusing on how different
communication topologies Ring, Star, and Von Neumann affect convergence and
search behaviors. Using an adapted IOHxplainer , an explainable benchmarking
tool, we investigate how these topologies influence information flow,
diversity, and convergence speed, clarifying the balance between exploration
and exploitation. Through visualization and statistical analysis, the research
enhances interpretability of PSO's decisions and provides practical guidelines
for choosing suitable topologies for specific optimization tasks. Ultimately,
this contributes to making swarm based optimization more transparent, robust,
and trustworthy.

</details>


### [141] [A Numerical Gradient Inversion Attack in Variational Quantum Neural-Networks](https://arxiv.org/abs/2504.12806)
*Georgios Papadopoulos,Shaltiel Eloul,Yash Satsangi,Jamie Heredge,Niraj Kumar,Chun-Fu Chen,Marco Pistoia*

Main category: cs.LG

TL;DR: 提出了一种基于梯度反演和自适应滤波的数值方案，成功从可训练VQNN的梯度中恢复输入数据。


<details>
  <summary>Details</summary>
Motivation: VQNN的损失函数存在指数级增长的局部极小值，导致从梯度中恢复信息比经典神经网络更具挑战性。

Method: 结合梯度估计与有限差分法和自适应低通滤波的梯度反演方案，进一步用卡尔曼滤波器优化收敛效率。

Result: 实验表明，该算法能够反演批量训练数据，前提是VQNN模型足够过参数化。

Conclusion: 该方案为从VQNN梯度中恢复数据提供了有效方法，尤其适用于过参数化模型。

Abstract: The loss landscape of Variational Quantum Neural Networks (VQNNs) is
characterized by local minima that grow exponentially with increasing qubits.
Because of this, it is more challenging to recover information from model
gradients during training compared to classical Neural Networks (NNs). In this
paper we present a numerical scheme that successfully reconstructs input
training, real-world, practical data from trainable VQNNs' gradients. Our
scheme is based on gradient inversion that works by combining gradients
estimation with the finite difference method and adaptive low-pass filtering.
The scheme is further optimized with Kalman filter to obtain efficient
convergence. Our experiments show that our algorithm can invert even
batch-trained data, given the VQNN model is sufficiently over-parameterized.

</details>


### [142] [Predicting Stock Prices using Permutation Decision Trees and Strategic Trailing](https://arxiv.org/abs/2504.12828)
*Vishrut Ramraj,Nithin Nagaraj,Harikrishnan N B*

Main category: cs.LG

TL;DR: 论文研究了基于排列决策树（PDT）和战略追踪的交易策略，用于预测印度股市的高频数据并执行盈利交易。结果表明，该策略优于市场平均表现和传统买入持有策略。


<details>
  <summary>Details</summary>
Motivation: 探索高频数据下排列决策树在股票市场预测中的应用，尤其是在印度股市中实现短期盈利交易。

Method: 使用5分钟K线数据，结合技术指标和超参数（如追踪止损值和支持阈值），开发交易策略。数据来自NIFTY 50指数的前50只股票。

Result: 基于PDT的交易机器人在12天测试期内实现了1.3468%的利润，优于LSTM（0.1238%）和RNN（0.3096%），且均优于买入持有策略（-2.2508%）。

Conclusion: 排列决策树在高频交易中表现出色，能够有效管理风险并实现超额收益。

Abstract: In this paper, we explore the application of Permutation Decision Trees (PDT)
and strategic trailing for predicting stock market movements and executing
profitable trades in the Indian stock market. We focus on high-frequency data
using 5-minute candlesticks for the top 50 stocks listed in the NIFTY 50 index.
We implement a trading strategy that aims to buy stocks at lower prices and
sell them at higher prices, capitalizing on short-term market fluctuations. Due
to regulatory constraints in India, short selling is not considered in our
strategy. The model incorporates various technical indicators and employs
hyperparameters such as the trailing stop-loss value and support thresholds to
manage risk effectively. Our results indicate that the proposed trading bot has
the potential to outperform the market average and yield returns higher than
the risk-free rate offered by 10-year Indian government bonds. We trained and
tested data on a 60 day dataset provided by Yahoo Finance. Specifically, 12
days for testing and 48 days for training. Our bot based on permutation
decision tree achieved a profit of 1.3468 % over a 12-day testing period, where
as a bot based on LSTM gave a return of 0.1238 % over a 12-day testing period
and a bot based on RNN gave a return of 0.3096 % over a 12-day testing period.
All of the bots outperform the buy-and-hold strategy, which resulted in a loss
of 2.2508 %.

</details>


### [143] [ALT: A Python Package for Lightweight Feature Representation in Time Series Classification](https://arxiv.org/abs/2504.12841)
*Balázs P. Halmos,Balázs Hajós,Vince Á. Molnár,Marcell T. Kurbucz,Antal Jakovác*

Main category: cs.LG

TL;DR: ALT是一个开源Python包，用于高效准确的时间序列分类（TSC），通过自适应算法提升性能。


<details>
  <summary>Details</summary>
Motivation: 改进线性变换算法（LLT），以更好地捕捉不同时间尺度的模式。

Method: 采用自适应律变换（ALT）算法，将原始时间序列数据转换为线性可分的特征空间。

Result: 在真实数据集上表现出色，计算开销低，适用于物理等领域。

Conclusion: ALT在TSC任务中具有高效性和实用性。

Abstract: We introduce ALT, an open-source Python package created for efficient and
accurate time series classification (TSC). The package implements the adaptive
law-based transformation (ALT) algorithm, which transforms raw time series data
into a linearly separable feature space using variable-length shifted time
windows. This adaptive approach enhances its predecessor, the linear law-based
transformation (LLT), by effectively capturing patterns of varying temporal
scales. The software is implemented for scalability, interpretability, and ease
of use, achieving state-of-the-art performance with minimal computational
overhead. Extensive benchmarking on real-world datasets demonstrates the
utility of ALT for diverse TSC tasks in physics and related domains.

</details>


### [144] [FedX: Adaptive Model Decomposition and Quantization for IoT Federated Learning](https://arxiv.org/abs/2504.12849)
*Phung Lai,Xiaopeng Jiang,Hai Phan,Cristian Borcea,Khang Tran,An Chen,Vijaya Datta Mayyuri,Ruoming Jin*

Main category: cs.LG

TL;DR: FedX是一种新型的自适应模型分解和量化联邦学习系统，旨在解决资源受限的物联网设备上的计算/通信开销问题。通过分解全局模型为不同子网络并自适应量化位数，FedX在保证模型效用的同时降低了设备开销。实验表明，FedX显著提升了训练效率和性能。


<details>
  <summary>Details</summary>
Motivation: 联邦学习（FL）在隐私敏感应用中具有潜力，但在资源受限的物联网设备上实现高效训练仍具挑战性。FedX旨在通过模型分解和量化平衡模型效用与资源限制。

Method: FedX将全局FL模型分解为不同子网络，并根据设备资源分配自适应量化位数。量化操作在服务器端完成以减少设备计算负担，通过迭代优化损失函数和正则化项实现高效训练。

Result: 实验显示，FedX在量化时间、设备计算时间和端到端训练时间上分别提升了8.43倍、1.5倍和1.36倍，同时保证了模型收敛性。

Conclusion: FedX通过结合模型分解和量化，显著提升了联邦学习在资源受限设备上的效率，为隐私敏感应用提供了可行解决方案。

Abstract: Federated Learning (FL) allows collaborative training among multiple devices
without data sharing, thus enabling privacy-sensitive applications on mobile or
Internet of Things (IoT) devices, such as mobile health and asset tracking.
However, designing an FL system with good model utility that works with low
computation/communication overhead on heterogeneous, resource-constrained
mobile/IoT devices is challenging. To address this problem, this paper proposes
FedX, a novel adaptive model decomposition and quantization FL system for IoT.
To balance utility with resource constraints on IoT devices, FedX decomposes a
global FL model into different sub-networks with adaptive numbers of quantized
bits for different devices. The key idea is that a device with fewer resources
receives a smaller sub-network for lower overhead but utilizes a larger number
of quantized bits for higher model utility, and vice versa. The quantization
operations in FedX are done at the server to reduce the computational load on
devices. FedX iteratively minimizes the losses in the devices' local data and
in the server's public data using quantized sub-networks under a regularization
term, and thus it maximizes the benefits of combining FL with model
quantization through knowledge sharing among the server and devices in a
cost-effective training process. Extensive experiments show that FedX
significantly improves quantization times by up to 8.43X, on-device computation
time by 1.5X, and total end-to-end training time by 1.36X, compared with
baseline FL systems. We guarantee the global model convergence theoretically
and validate local model convergence empirically, highlighting FedX's
optimization efficiency.

</details>


### [145] [iHHO-SMOTe: A Cleansed Approach for Handling Outliers and Reducing Noise to Improve Imbalanced Data Classification](https://arxiv.org/abs/2504.12850)
*Khaled SH. Raslan,Almohammady S. Alsharkawy,K. R. Raslan*

Main category: cs.LG

TL;DR: iHHO-SMOTe方法通过数据清洗和特征选择改进SMOTE，显著提升不平衡数据集的分类性能。


<details>
  <summary>Details</summary>
Motivation: 解决SMOTE在生成新样本时对噪声和异常值的敏感性，以提高分类器在不平衡数据集上的表现。

Method: 结合随机森林特征选择和DBSCAN异常检测，先清洗数据，再使用iHHO-SMOTe进行过采样。

Result: AUC超过0.99，G-means为0.99，F1-score稳定高于0.967，表现优异。

Conclusion: iHHO-SMOTe通过噪声和异常值处理，成为处理不平衡数据集的有效方法。

Abstract: Classifying imbalanced datasets remains a significant challenge in machine
learning, particularly with big data where instances are unevenly distributed
among classes, leading to class imbalance issues that impact classifier
performance. While Synthetic Minority Over-sampling Technique (SMOTE) addresses
this challenge by generating new instances for the under-represented minority
class, it faces obstacles in the form of noise and outliers during the creation
of new samples. In this paper, a proposed approach, iHHO-SMOTe, which addresses
the limitations of SMOTE by first cleansing the data from noise points. This
process involves employing feature selection using a random forest to identify
the most valuable features, followed by applying the Density-Based Spatial
Clustering of Applications with Noise (DBSCAN) algorithm to detect outliers
based on the selected features. The identified outliers from the minority
classes are then removed, creating a refined dataset for subsequent
oversampling using the hybrid approach called iHHO-SMOTe. The comprehensive
experiments across diverse datasets demonstrate the exceptional performance of
the proposed model, with an AUC score exceeding 0.99, a high G-means score of
0.99 highlighting its robustness, and an outstanding F1-score consistently
exceeding 0.967. These findings collectively establish Cleansed iHHO-SMOTe as a
formidable contender in addressing imbalanced datasets, focusing on noise
reduction and outlier handling for improved classification models.

</details>


### [146] [A Client-level Assessment of Collaborative Backdoor Poisoning in Non-IID Federated Learning](https://arxiv.org/abs/2504.12875)
*Phung Lai,Guanxiong Liu,Hai Phan,Issa Khalil,Abdallah Khreishah,Xintao Wu*

Main category: cs.LG

TL;DR: 该论文揭示了联邦学习（FL）在非独立同分布（non-IID）数据下的新漏洞，并提出了一种名为CollaPois的新型协作后门攻击方法。该方法通过少数恶意客户端放大攻击效果，同时避免被检测。


<details>
  <summary>Details</summary>
Motivation: 研究FL在非IID数据下的潜在漏洞，以揭示现有防御机制的不足。

Method: 开发了CollaPois攻击，通过分发预训练的带Trojan的模型，使恶意客户端协作生成恶意梯度，引导FL模型收敛到低损失区域。

Result: CollaPois在多种基准数据集上优于现有攻击方法，能绕过防御机制，且在少数恶意客户端下仍有效。

Conclusion: 非IID数据增加了FL的后门攻击风险，CollaPois展示了其高效性和隐蔽性，需改进防御策略。

Abstract: Federated learning (FL) enables collaborative model training using
decentralized private data from multiple clients. While FL has shown robustness
against poisoning attacks with basic defenses, our research reveals new
vulnerabilities stemming from non-independent and identically distributed
(non-IID) data among clients. These vulnerabilities pose a substantial risk of
model poisoning in real-world FL scenarios.
  To demonstrate such vulnerabilities, we develop a novel collaborative
backdoor poisoning attack called CollaPois. In this attack, we distribute a
single pre-trained model infected with a Trojan to a group of compromised
clients. These clients then work together to produce malicious gradients,
causing the FL model to consistently converge towards a low-loss region
centered around the Trojan-infected model. Consequently, the impact of the
Trojan is amplified, especially when the benign clients have diverse local data
distributions and scattered local gradients. CollaPois stands out by achieving
its goals while involving only a limited number of compromised clients, setting
it apart from existing attacks. Also, CollaPois effectively avoids noticeable
shifts or degradation in the FL model's performance on legitimate data samples,
allowing it to operate stealthily and evade detection by advanced robust FL
algorithms.
  Thorough theoretical analysis and experiments conducted on various benchmark
datasets demonstrate the superiority of CollaPois compared to state-of-the-art
backdoor attacks. Notably, CollaPois bypasses existing backdoor defenses,
especially in scenarios where clients possess diverse data distributions.
Moreover, the results show that CollaPois remains effective even when involving
a small number of compromised clients. Notably, clients whose local data is
closely aligned with compromised clients experience higher risks of backdoor
infections.

</details>


### [147] [Can Masked Autoencoders Also Listen to Birds?](https://arxiv.org/abs/2504.12880)
*Lukas Rauch,Ilyass Moummad,René Heinrich,Alexis Joly,Bernhard Sick,Christoph Scholz*

Main category: cs.LG

TL;DR: Bird-MAE是一种针对鸟类声音分类的专用模型，通过预训练和微调优化，显著提升了多标签分类性能，并提出了一种参数高效的原型探测方法。


<details>
  <summary>Details</summary>
Motivation: 通用音频模型（如MAEs）在生物声学监测等专业领域表现不佳，无法捕捉细粒度声学特征，因此需要针对鸟类声音分类的专用模型。

Method: 提出Bird-MAE，基于BirdSet数据集进行预训练，探索预训练、微调和冻结表征的优化方法，并引入原型探测技术。

Result: Bird-MAE在所有BirdSet下游任务中达到最佳性能，多标签分类性能显著优于通用模型Audio-MAE，原型探测方法比线性探测提升37%的MAP。

Conclusion: Bird-MAE为鸟类声音分类提供了高效解决方案，原型探测方法显著提升了冻结表征的利用效率。

Abstract: Masked Autoencoders (MAEs) pretrained on AudioSet fail to capture the
fine-grained acoustic characteristics of specialized domains such as
bioacoustic monitoring. Bird sound classification is critical for assessing
environmental health, yet general-purpose models inadequately address its
unique acoustic challenges. To address this, we introduce Bird-MAE, a
domain-specialized MAE pretrained on the large-scale BirdSet dataset. We
explore adjustments to pretraining, fine-tuning and utilizing frozen
representations. Bird-MAE achieves state-of-the-art results across all BirdSet
downstream tasks, substantially improving multi-label classification
performance compared to the general-purpose Audio-MAE baseline. Additionally,
we propose prototypical probing, a parameter-efficient method for leveraging
MAEs' frozen representations. Bird-MAE's prototypical probes outperform linear
probing by up to 37\% in MAP and narrow the gap to fine-tuning to approximately
3\% on average on BirdSet.

</details>


### [148] [Mirror, Mirror of the Flow: How Does Regularization Shape Implicit Bias?](https://arxiv.org/abs/2504.12883)
*Tom Jacobs,Chao Zhou,Rebekka Burkholz*

Main category: cs.LG

TL;DR: 论文研究了隐式偏差和显式正则化（如权重衰减）的相互作用，分析了其对训练动态几何形状的影响，并提出了动态权重衰减调度的潜在优势。


<details>
  <summary>Details</summary>
Motivation: 理解隐式偏差和显式正则化的相互作用，以控制隐式偏差的形状和强度，从而改进模型的泛化能力。

Method: 将显式正则化纳入镜像流框架，分析其对训练动态几何形状的持久影响，包括位置偏差、偏差类型和范围收缩。

Result: 通过实验证明动态关闭权重衰减可以提高泛化性能。

Conclusion: 动态调整显式正则化（如权重衰减）可以优化隐式偏差，从而提升模型性能。

Abstract: Implicit bias plays an important role in explaining how overparameterized
models generalize well. Explicit regularization like weight decay is often
employed in addition to prevent overfitting. While both concepts have been
studied separately, in practice, they often act in tandem. Understanding their
interplay is key to controlling the shape and strength of implicit bias, as it
can be modified by explicit regularization. To this end, we incorporate
explicit regularization into the mirror flow framework and analyze its lasting
effects on the geometry of the training dynamics, covering three distinct
effects: positional bias, type of bias, and range shrinking. Our analytical
approach encompasses a broad class of problems, including sparse coding, matrix
sensing, single-layer attention, and LoRA, for which we demonstrate the utility
of our insights. To exploit the lasting effect of regularization and highlight
the potential benefit of dynamic weight decay schedules, we propose to switch
off weight decay during training, which can improve generalization, as we
demonstrate in experiments.

</details>


### [149] [Exact Learning Dynamics of In-Context Learning in Linear Transformers and Its Application to Non-Linear Transformers](https://arxiv.org/abs/2504.12916)
*Nischal Mainali,Lucas Teixeira*

Main category: cs.LG

TL;DR: 论文通过分析线性Transformer的SGD动态，揭示了ICL的涌现机制，包括分阶段学习、固定点和守恒定律，并假设这些现象可推广到非线性模型。


<details>
  <summary>Details</summary>
Motivation: 理解Transformer模型在上下文学习（ICL）中的机制，目前尚不清楚其底层原理。

Method: 通过推导简化线性Transformer在回归任务中的闭式SGD动态，进行精确分析。

Result: 揭示了ICL的关键特性，如分阶段学习、固定点和非线性行为，并通过宏观测量验证了非线性模型中的类似现象。

Conclusion: 研究提供了ICL的精确动态模型，并为分析复杂Transformer训练提供了理论工具。

Abstract: Transformer models exhibit remarkable in-context learning (ICL), adapting to
novel tasks from examples within their context, yet the underlying mechanisms
remain largely mysterious. Here, we provide an exact analytical
characterization of ICL emergence by deriving the closed-form stochastic
gradient descent (SGD) dynamics for a simplified linear transformer performing
regression tasks. Our analysis reveals key properties: (1) a natural separation
of timescales directly governed by the input data's covariance structure,
leading to staged learning; (2) an exact description of how ICL develops,
including fixed points corresponding to learned algorithms and conservation
laws constraining the dynamics; and (3) surprisingly nonlinear learning
behavior despite the model's linearity. We hypothesize this phenomenology
extends to non-linear models. To test this, we introduce theory-inspired
macroscopic measures (spectral rank dynamics, subspace stability) and use them
to provide mechanistic explanations for (1) the sudden emergence of ICL in
attention-only networks and (2) delayed generalization (grokking) in modular
arithmetic models. Our work offers an exact dynamical model for ICL and
theoretically grounded tools for analyzing complex transformer training.

</details>


### [150] [Sliced-Wasserstein Distance-based Data Selection](https://arxiv.org/abs/2504.12918)
*Julien Pallage,Antoine Lesage-Landry*

Main category: cs.LG

TL;DR: 提出了一种基于切片-Wasserstein距离的无监督异常检测方法，用于机器学习中的训练数据选择，适用于关键领域（如电力系统）的决策流程。


<details>
  <summary>Details</summary>
Motivation: 在关键领域（如电力系统）中，机器学习模型的训练数据选择需要保守且可解释的方法，以确保决策的可靠性。

Method: 使用切片-Wasserstein距离进行数据过滤，并提供了两种高效近似方法：一种是处理低基数数据集，另一种是使用轻量级欧几里得距离近似。

Result: 方法在合成数据集上展示了过滤效果，并在训练数据选择中进行了数值基准测试。此外，还首次公开了一个关于北方气候中局部峰值需求响应的数据集。

Conclusion: 该方法为关键领域的机器学习模型提供了可靠的数据选择工具，并通过开源数据集和基准测试验证了其有效性。

Abstract: We propose a new unsupervised anomaly detection method based on the
sliced-Wasserstein distance for training data selection in machine learning
approaches. Our filtering technique is interesting for decision-making
pipelines deploying machine learning models in critical sectors, e.g., power
systems, as it offers a conservative data selection and an optimal transport
interpretation. To ensure the scalability of our method, we provide two
efficient approximations. The first approximation processes reduced-cardinality
representations of the datasets concurrently. The second makes use of a
computationally light Euclidian distance approximation. Additionally, we open
the first dataset showcasing localized critical peak rebate demand response in
a northern climate. We present the filtering patterns of our method on
synthetic datasets and numerically benchmark our method for training data
selection. Finally, we employ our method as part of a first forecasting
benchmark for our open-source dataset.

</details>


### [151] [IdentiARAT: Toward Automated Identification of Individual ARAT Items from Wearable Sensors](https://arxiv.org/abs/2504.12921)
*Daniel Homm,Patrick Carqueville,Christian Eichhorn,Thomas Weikert,Thomas Menard,David A. Plecher,Chris Awai Easthope*

Main category: cs.LG

TL;DR: 利用手腕惯性传感器自动标记ARAT测试项目，MiniROCKET分类技术表现快速可靠，但仍需改进。


<details>
  <summary>Details</summary>
Motivation: ARAT测试主观性强且耗时，希望通过IMU传感器和MiniROCKET技术实现自动化分类。

Method: 使用IMU传感器记录数据，MiniROCKET分类技术，测试预处理策略并优化分类。

Result: MiniROCKET能快速可靠分类ARAT项目，但对相似项目区分仍有挑战。

Conclusion: 未来可通过更先进的机器学习模型和数据增强改进分类效果。

Abstract: This study explores the potential of using wrist-worn inertial sensors to
automate the labeling of ARAT (Action Research Arm Test) items. While the ARAT
is commonly used to assess upper limb motor function, its limitations include
subjectivity and time consumption of clinical staff. By using IMU (Inertial
Measurement Unit) sensors and MiniROCKET as a time series classification
technique, this investigation aims to classify ARAT items based on sensor
recordings. We test common preprocessing strategies to efficiently leverage
included information in the data. Afterward, we use the best preprocessing to
improve the classification. The dataset includes recordings of 45 participants
performing various ARAT items. Results show that MiniROCKET offers a fast and
reliable approach for classifying ARAT domains, although challenges remain in
distinguishing between individual resembling items. Future work may involve
improving classification through more advanced machine-learning models and data
enhancements.

</details>


### [152] [RL-PINNs: Reinforcement Learning-Driven Adaptive Sampling for Efficient Training of PINNs](https://arxiv.org/abs/2504.12949)
*Zhenao Song*

Main category: cs.LG

TL;DR: RL-PINNs是一种基于强化学习的自适应采样框架，用于高效训练物理信息神经网络（PINNs），避免了传统方法的多轮采样和冗余计算。


<details>
  <summary>Details</summary>
Motivation: 传统自适应采样方法（如基于残差的细化）需要多轮采样和重复训练PINNs，导致计算效率低下，尤其是在高维或高阶导数场景中。

Method: 提出RL-PINNs，将自适应采样建模为马尔可夫决策过程，通过强化学习代理动态选择最优训练点，并使用函数变分替代梯度依赖的残差度量。

Result: 在多种PDE基准测试中，RL-PINNs显著优于现有方法，且采样开销极低，适用于高维和高阶问题。

Conclusion: RL-PINNs通过强化学习驱动的自适应采样，实现了高效且稳定的PINNs训练，具有广泛的应用潜力。

Abstract: Physics-Informed Neural Networks (PINNs) have emerged as a powerful framework
for solving partial differential equations (PDEs). However, their performance
heavily relies on the strategy used to select training points. Conventional
adaptive sampling methods, such as residual-based refinement, often require
multi-round sampling and repeated retraining of PINNs, leading to computational
inefficiency due to redundant points and costly gradient
computations-particularly in high-dimensional or high-order derivative
scenarios. To address these limitations, we propose RL-PINNs, a reinforcement
learning(RL)-driven adaptive sampling framework that enables efficient training
with only a single round of sampling. Our approach formulates adaptive sampling
as a Markov decision process, where an RL agent dynamically selects optimal
training points by maximizing a long-term utility metric. Critically, we
replace gradient-dependent residual metrics with a computationally efficient
function variation as the reward signal, eliminating the overhead of derivative
calculations. Furthermore, we employ a delayed reward mechanism to prioritize
long-term training stability over short-term gains. Extensive experiments
across diverse PDE benchmarks, including low-regular, nonlinear,
high-dimensional, and high-order problems, demonstrate that RL-PINNs
significantly outperforms existing residual-driven adaptive methods in
accuracy. Notably, RL-PINNs achieve this with negligible sampling overhead,
making them scalable to high-dimensional and high-order problems.

</details>


### [153] [Transferrable Surrogates in Expressive Neural Architecture Search Spaces](https://arxiv.org/abs/2504.12971)
*Shiwen Qin,Gabriela Kadlecová,Martin Pilát,Shay B. Cohen,Roman Neruda,Elliot J. Crowley,Jovita Lukasik,Linus Ericsson*

Main category: cs.LG

TL;DR: 论文研究了在高度表达性的NAS搜索空间中，通过代理模型训练提升搜索效率的方法，展示了代理模型的高预测能力及其在加速搜索和提升性能中的作用。


<details>
  <summary>Details</summary>
Motivation: 解决神经架构搜索（NAS）在探索广泛搜索空间与高效评估架构之间的平衡问题。

Method: 使用基于上下文无关文法的代理模型训练，结合零成本代理指标和神经图特征（GRAF）或微调现成语言模型。

Result: 代理模型在架构性能预测上表现优异，能显著加速搜索并提升最终性能。

Conclusion: 代理模型可作为搜索目标直接使用，大幅提升搜索效率。

Abstract: Neural architecture search (NAS) faces a challenge in balancing the
exploration of expressive, broad search spaces that enable architectural
innovation with the need for efficient evaluation of architectures to
effectively search such spaces. We investigate surrogate model training for
improving search in highly expressive NAS search spaces based on context-free
grammars. We show that i) surrogate models trained either using zero-cost-proxy
metrics and neural graph features (GRAF) or by fine-tuning an off-the-shelf LM
have high predictive power for the performance of architectures both within and
across datasets, ii) these surrogates can be used to filter out bad
architectures when searching on novel datasets, thereby significantly speeding
up search and achieving better final performances, and iii) the surrogates can
be further used directly as the search objective for huge speed-ups.

</details>


### [154] [A Virtual Machine for Arbitrary Low-Precision GPGPU Computation in LLM Serving](https://arxiv.org/abs/2504.12984)
*Yaoyao Ding,Bohan Hou,Xiao Zhang,Allan Lin,Tianqi Chen,Cody Yu Hao,Yida Wang,Gennady Pekhimenko*

Main category: cs.LG

TL;DR: 提出了一种用于GPGPU计算的虚拟机，支持任意位宽的低精度数据类型，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有低精度核生成方法局限于2的幂次位宽，且因高级GPU编程抽象导致性能不佳。

Method: 设计了一种虚拟机，具有线程块级编程模型、分层内存空间、代数布局系统，并支持多种低精度数据类型。

Result: 虚拟机在支持全谱低精度数据类型的同时，性能优于现有方法，最高提升2.61倍。

Conclusion: 该虚拟机为低精度计算提供了高效且灵活的解决方案，显著提升了性能。

Abstract: Serving Large Language Models (LLMs) is critical for AI-powered applications
but demands substantial computational resources, particularly in memory
bandwidth and computational throughput. Low-precision computation has emerged
as a key technique to improve efficiency while reducing resource consumption.
Existing approaches for generating low-precision kernels are limited to weight
bit widths that are powers of two and suffer from suboptimal performance due to
high-level GPU programming abstractions. These abstractions restrict critical
optimizations, such as fine-grained register management and optimized memory
access patterns, which are essential for efficient low-precision computations.
In this paper, we introduce a virtual machine (VM) designed for General-Purpose
GPU (GPGPU) computing, enabling support for low-precision data types with
arbitrary bit widths while maintaining GPU programmability. The proposed VM
features a thread-block-level programming model, a hierarchical memory space, a
novel algebraic layout system, and extensive support for diverse low-precision
data types. VM programs are compiled into highly efficient GPU programs with
automatic vectorization and instruction selection. Extensive experiments
demonstrate that our VM efficiently supports a full spectrum of low-precision
data types, and outperforms state-of-the-art low-precision kernels on their
supported types. Compared to existing compilers like Triton and Ladder, as well
as hand-optimized kernels such as QuantLLM and Marlin, our VM achieves
performance improvements of 1.75x, 2.61x, 1.29x and 1.03x, respectively.

</details>


### [155] [Why Ask One When You Can Ask $k$? Two-Stage Learning-to-Defer to a Set of Experts](https://arxiv.org/abs/2504.12988)
*Yannis Montreuil,Axel Carlier,Lai Xing Ng,Wei Tsang Ooi*

Main category: cs.LG

TL;DR: 论文提出了Top-$k$和Top-$k(x)$ Learning-to-Defer框架，扩展了传统的单代理延迟决策，通过动态选择多个代理提升决策可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注单代理延迟决策，但在高风险场景中需要集体智慧，因此需要更灵活的决策框架。

Method: 提出了Top-$k$和Top-$k(x)$框架，前者固定选择k个代理，后者动态调整代理数量。通过新提出的代理损失函数确保贝叶斯一致性。

Result: 实验证明该框架在分类和回归任务中均有效，且模型级联是其特例。

Conclusion: Top-$k$和Top-$k(x)$框架显著提升了决策系统的灵活性和可靠性，适用于高风险场景。

Abstract: Learning-to-Defer (L2D) enables decision-making systems to improve
reliability by selectively deferring uncertain predictions to more competent
agents. However, most existing approaches focus exclusively on single-agent
deferral, which is often inadequate in high-stakes scenarios that require
collective expertise. We propose Top-$k$ Learning-to-Defer, a generalization of
the classical two-stage L2D framework that allocates each query to the $k$ most
confident agents instead of a single one. To further enhance flexibility and
cost-efficiency, we introduce Top-$k(x)$ Learning-to-Defer, an adaptive
extension that learns the optimal number of agents to consult for each query,
based on input complexity, agent competency distributions, and consultation
costs. For both settings, we derive a novel surrogate loss and prove that it is
Bayes-consistent and $(\mathcal{R}, \mathcal{G})$-consistent, ensuring
convergence to the Bayes-optimal allocation. Notably, we show that the
well-established model cascades paradigm arises as a restricted instance of our
Top-$k$ and Top-$k(x)$ formulations. Extensive experiments across diverse
benchmarks demonstrate the effectiveness of our framework on both
classification and regression tasks.

</details>


### [156] [Chain-of-Thought Prompting for Out-of-Distribution Samples: A Latent-Variable Study](https://arxiv.org/abs/2504.12991)
*Yu Wang,Fu-Chieh Chang,Pei-Yuan Wu*

Main category: cs.LG

TL;DR: 本文研究了Chain-of-Thought (CoT)提示在分布偏移下的泛化能力，发现其性能与训练数据相似性相关。


<details>
  <summary>Details</summary>
Motivation: 探索CoT提示在分布偏移（OOD）场景下的表现，以填补对其泛化能力理解的空白。

Method: 扩展了一个潜在变量框架，研究两种典型OOD场景：潜在变量排列组合和均匀缩放。

Result: CoT在潜在变量与训练数据相似时泛化良好，但相似性降低时性能下降。

Conclusion: 研究揭示了CoT在OOD条件下的局限性，为未来开发更鲁棒的推理策略提供了方向。

Abstract: Chain-of-Thought (CoT) prompting has emerged as a powerful technique to
improve in-context learning (ICL) in large language models (LLMs) by breaking
complex reasoning into intermediate steps. However, the ability of CoT to
generalize under distribution shift remains poorly understood. In this work, we
extend a latent-variable framework for CoT prompting and study its behavior on
two prototypical out-of-distribution (OOD) scenarios: (i) the latent variables
for CoT steps are permuted into novel combinations, and (ii) the latent
variables uniformly scaled by a factor. Our experiments demonstrate that CoT
inference generalizes effectively to OOD samples whose latent variables closely
resemble those seen during training, but its performance degrades as this
similarity decreases. These findings provide foundational insights into the
strengths and limitations of CoT prompting under OOD conditions and suggest
directions for developing more resilient reasoning strategies in future LLMs.

</details>


### [157] [Inference-friendly Graph Compression for Graph Neural Networks](https://arxiv.org/abs/2504.13034)
*Yangxin Fan,Haolai Che,Yinghui Wu*

Main category: cs.LG

TL;DR: 提出了一种名为IFGC的图压缩方案，用于加速GNN推理，通过压缩图保留推理结果，并提供了三种具体实现方法。


<details>
  <summary>Details</summary>
Motivation: GNN推理过程成本高，限制了其在大规模图上的应用，因此需要一种高效的图压缩方法。

Method: 提出IFGC方案，包括三种具体实现：SPGC、(α, r)-压缩和锚定压缩，每种方法均有对应的压缩和推理算法。

Result: 实验验证了IFGC在大型图上的有效性和高效性。

Conclusion: IFGC为GNN推理提供了一种高效的图压缩解决方案，平衡了压缩比和推理质量。

Abstract: Graph Neural Networks (GNNs) have demonstrated promising performance in graph
analysis. Nevertheless, the inference process of GNNs remains costly, hindering
their applications for large graphs. This paper proposes inference-friendly
graph compression (IFGC), a graph compression scheme to accelerate GNNs
inference. Given a graph $G$ and a GNN $M$, an IFGC computes a small compressed
graph $G_c$, to best preserve the inference results of $M$ over $G$, such that
the result can be directly inferred by accessing $G_c$ with no or little
decompression cost. (1) We characterize IFGC with a class of inference
equivalence relation. The relation captures the node pairs in $G$ that are not
distinguishable for GNN inference. (2) We introduce three practical
specifications of IFGC for representative GNNs: structural preserving
compression (SPGC), which computes $G_c$ that can be directly processed by GNN
inference without decompression; ($\alpha$, $r$)-compression, that allows for a
configurable trade-off between compression ratio and inference quality, and
anchored compression that preserves inference results for specific nodes of
interest. For each scheme, we introduce compression and inference algorithms
with guarantees of efficiency and quality of the inferred results. We conduct
extensive experiments on diverse sets of large-scale graphs, which verifies the
effectiveness and efficiency of our graph compression approaches.

</details>


### [158] [An All-Atom Generative Model for Designing Protein Complexes](https://arxiv.org/abs/2504.13075)
*Ruizhe Chen,Dongyu Xue,Xiangxin Zhou,Zaixiang Zheng,Xiangxiang Zeng,Quanquan Gu*

Main category: cs.LG

TL;DR: APM是一种专为多链蛋白质建模设计的生成模型，能够精确建模链间相互作用并从头设计具有结合能力的蛋白质复合物。


<details>
  <summary>Details</summary>
Motivation: 尽管单链蛋白质建模已取得进展，但多链蛋白质的研究仍较少，而这对理解生物功能至关重要。

Method: APM整合原子级信息并利用多链蛋白质数据，支持多链蛋白质的折叠和逆折叠任务。

Result: APM在监督微调下表现优异，同时支持零样本采样，取得先进成果。

Conclusion: APM为多链蛋白质建模提供了高效工具，具有广泛的下游应用潜力。

Abstract: Proteins typically exist in complexes, interacting with other proteins or
biomolecules to perform their specific biological roles. Research on
single-chain protein modeling has been extensively and deeply explored, with
advancements seen in models like the series of ESM and AlphaFold. Despite these
developments, the study and modeling of multi-chain proteins remain largely
uncharted, though they are vital for understanding biological functions.
Recognizing the importance of these interactions, we introduce APM (All-Atom
Protein Generative Model), a model specifically designed for modeling
multi-chain proteins. By integrating atom-level information and leveraging data
on multi-chain proteins, APM is capable of precisely modeling inter-chain
interactions and designing protein complexes with binding capabilities from
scratch. It also performs folding and inverse-folding tasks for multi-chain
proteins. Moreover, APM demonstrates versatility in downstream applications: it
achieves enhanced performance through supervised fine-tuning (SFT) while also
supporting zero-shot sampling in certain tasks, achieving state-of-the-art
results. Code will be released at https://github.com/bytedance/apm.

</details>


### [159] [An Empirically Grounded Identifiability Theory Will Accelerate Self-Supervised Learning Research](https://arxiv.org/abs/2504.13101)
*Patrik Reizinger,Randall Balestriero,David Klindt,Wieland Brendel*

Main category: cs.LG

TL;DR: 论文探讨了自监督学习（SSL）中Platonic表示假说（PRH）的理论基础，提出了扩展的Singular Identifiability Theory（SITh）以填补理论与实践的差距，并指出了未来研究的三个关键方向。


<details>
  <summary>Details</summary>
Motivation: 研究自监督学习（SSL）中Platonic表示假说（PRH）的理论基础，并解决当前Identifiability Theory（IT）无法解释SSL实证成功的问题。

Method: 通过综合Identifiability Theory（IT）的证据，提出扩展的Singular Identifiability Theory（SITh）作为更广泛的理论框架。

Result: SITh能够更深入地理解SSL中的隐含数据假设，推动学习更具解释性和泛化性的表示。

Conclusion: 未来研究应关注SSL的训练动态、有限样本的影响以及归纳偏置的作用，以进一步推动领域发展。

Abstract: Self-Supervised Learning (SSL) powers many current AI systems. As research
interest and investment grow, the SSL design space continues to expand. The
Platonic view of SSL, following the Platonic Representation Hypothesis (PRH),
suggests that despite different methods and engineering approaches, all
representations converge to the same Platonic ideal. However, this phenomenon
lacks precise theoretical explanation. By synthesizing evidence from
Identifiability Theory (IT), we show that the PRH can emerge in SSL. However,
current IT cannot explain SSL's empirical success. To bridge the gap between
theory and practice, we propose expanding IT into what we term Singular
Identifiability Theory (SITh), a broader theoretical framework encompassing the
entire SSL pipeline. SITh would allow deeper insights into the implicit data
assumptions in SSL and advance the field towards learning more interpretable
and generalizable representations. We highlight three critical directions for
future research: 1) training dynamics and convergence properties of SSL; 2) the
impact of finite samples, batch size, and data diversity; and 3) the role of
inductive biases in architecture, augmentations, initialization schemes, and
optimizers.

</details>


### [160] [Uncertainty-Aware Trajectory Prediction via Rule-Regularized Heteroscedastic Deep Classification](https://arxiv.org/abs/2504.13111)
*Kumar Manas,Christian Schlauch,Adrian Paschke,Christian Wirth,Nadja Klein*

Main category: cs.LG

TL;DR: SHIFT提出了一种结合校准不确定性建模和自动规则提取的新框架，用于提升轨迹预测的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习轨迹预测模型在分布外泛化中的挑战，如数据不平衡和多样性不足。

Method: 将轨迹预测重新定义为分类任务，使用异方差谱归一化高斯过程分离不确定性，并通过大型语言模型自动提取驾驶规则作为先验。

Result: 在nuScenes数据集上表现优异，尤其在复杂场景（如交叉路口）中，显著提升了不确定性校准和位移指标。

Conclusion: SHIFT通过结合不确定性建模和规则提取，显著提升了轨迹预测的泛化能力和鲁棒性。

Abstract: Deep learning-based trajectory prediction models have demonstrated promising
capabilities in capturing complex interactions. However, their
out-of-distribution generalization remains a significant challenge,
particularly due to unbalanced data and a lack of enough data and diversity to
ensure robustness and calibration. To address this, we propose SHIFT (Spectral
Heteroscedastic Informed Forecasting for Trajectories), a novel framework that
uniquely combines well-calibrated uncertainty modeling with informative priors
derived through automated rule extraction. SHIFT reformulates trajectory
prediction as a classification task and employs heteroscedastic
spectral-normalized Gaussian processes to effectively disentangle epistemic and
aleatoric uncertainties. We learn informative priors from training labels,
which are automatically generated from natural language driving rules, such as
stop rules and drivability constraints, using a retrieval-augmented generation
framework powered by a large language model. Extensive evaluations over the
nuScenes dataset, including challenging low-data and cross-location scenarios,
demonstrate that SHIFT outperforms state-of-the-art methods, achieving
substantial gains in uncertainty calibration and displacement metrics. In
particular, our model excels in complex scenarios, such as intersections, where
uncertainty is inherently higher. Project page:
https://kumarmanas.github.io/SHIFT/.

</details>


### [161] [Hadamard product in deep learning: Introduction, Advances and Challenges](https://arxiv.org/abs/2504.13112)
*Grigorios G Chrysos,Yongtao Wu,Razvan Pascanu,Philip Torr,Volkan Cevher*

Main category: cs.LG

TL;DR: 该论文首次系统分析了Hadamard积在深度学习中的核心作用，提出了其在四个主要领域的应用分类，并展示了其在计算效率和表示能力之间的平衡优势。


<details>
  <summary>Details</summary>
Motivation: 尽管Hadamard积在深度学习中广泛应用，但缺乏对其作为核心架构原语的系统性分析。本文旨在填补这一空白。

Method: 通过构建全面的应用分类法，分析了Hadamard积在四个主要领域的应用，并评估其计算效率和表示能力。

Result: Hadamard积在资源受限场景和多模态融合任务中表现优异，展示了其在非线性交互建模中的潜力。

Conclusion: Hadamard积是一种多功能原语，为深度学习架构提供了新的创新基础。

Abstract: While convolution and self-attention mechanisms have dominated architectural
design in deep learning, this survey examines a fundamental yet understudied
primitive: the Hadamard product. Despite its widespread implementation across
various applications, the Hadamard product has not been systematically analyzed
as a core architectural primitive. We present the first comprehensive taxonomy
of its applications in deep learning, identifying four principal domains:
higher-order correlation, multimodal data fusion, dynamic representation
modulation, and efficient pairwise operations. The Hadamard product's ability
to model nonlinear interactions with linear computational complexity makes it
particularly valuable for resource-constrained deployments and edge computing
scenarios. We demonstrate its natural applicability in multimodal fusion tasks,
such as visual question answering, and its effectiveness in representation
masking for applications including image inpainting and pruning. This
systematic review not only consolidates existing knowledge about the Hadamard
product's role in deep learning architectures but also establishes a foundation
for future architectural innovations. Our analysis reveals the Hadamard product
as a versatile primitive that offers compelling trade-offs between
computational efficiency and representational power, positioning it as a
crucial component in the deep learning toolkit.

</details>


### [162] [Quorum: Zero-Training Unsupervised Anomaly Detection using Quantum Autoencoders](https://arxiv.org/abs/2504.13113)
*Jason Zev Ludmir,Sophia Rebello,Jacob Ruiz,Tirthak Patel*

Main category: cs.LG

TL;DR: Quorum是首个无需训练的量子异常检测框架，适用于无监督学习，解决了量子机器学习模型训练的梯度计算难题。


<details>
  <summary>Details</summary>
Motivation: 检测关键异常事件和数据在各行业（如金融、医疗、能源）中至关重要，但量子机器学习模型的训练，尤其是梯度计算，仍具挑战性。

Method: 提出Quorum框架，专为无监督学习设计，无需训练即可运行。

Result: Quorum成功解决了量子异常检测中的训练难题。

Conclusion: Quorum为量子异常检测提供了一种高效且实用的解决方案。

Abstract: Detecting mission-critical anomalous events and data is a crucial challenge
across various industries, including finance, healthcare, and energy. Quantum
computing has recently emerged as a powerful tool for tackling several machine
learning tasks, but training quantum machine learning models remains
challenging, particularly due to the difficulty of gradient calculation. The
challenge is even greater for anomaly detection, where unsupervised learning
methods are essential to ensure practical applicability. To address these
issues, we propose Quorum, the first quantum anomaly detection framework
designed for unsupervised learning that operates without requiring any
training.

</details>


### [163] [Predicting BVD Re-emergence in Irish Cattle From Highly Imbalanced Herd-Level Data Using Machine Learning Algorithms](https://arxiv.org/abs/2504.13116)
*Niamh Mimnagh,Andrew Parnell,Conor McAloon,Jaden Carlson,Maria Guelbenzu,Jonas Brock,Damien Barrett,Guy McGrath,Jamie Tratalos,Rafael Moral*

Main category: cs.LG

TL;DR: 爱尔兰成功实施牛病毒性腹泻（BVD）根除计划，发病率从2013年的11.3%降至2023年的0.2%。研究评估了机器学习算法在预测BVD阳性牛群中的表现，随机森林和XGBoost表现最佳。


<details>
  <summary>Details</summary>
Motivation: 随着爱尔兰接近BVD自由状态，开发预测模型以进行针对性监测，降低疾病复发的风险变得至关重要。

Method: 研究评估了多种机器学习算法（包括二元分类和异常检测技术），通过模拟研究测试不同样本量和类别不平衡比例下的模型性能，并采用重采样、类别加权和适当评估指标。

Result: 随机森林和XGBoost模型表现最优，随机森林在2023年实际预测中正确识别了219个阳性牛群，同时将需要检测的牛群数量减半。

Conclusion: 随机森林模型在预测BVD阳性牛群中表现最佳，为针对性监测提供了高效工具。

Abstract: Bovine Viral Diarrhoea (BVD) has been the focus of a successful eradication
programme in Ireland, with the herd-level prevalence declining from 11.3% in
2013 to just 0.2% in 2023. As the country moves toward BVD freedom, the
development of predictive models for targeted surveillance becomes increasingly
important to mitigate the risk of disease re-emergence. In this study, we
evaluate the performance of a range of machine learning algorithms, including
binary classification and anomaly detection techniques, for predicting
BVD-positive herds using highly imbalanced herd-level data. We conduct an
extensive simulation study to assess model performance across varying sample
sizes and class imbalance ratios, incorporating resampling, class weighting,
and appropriate evaluation metrics (sensitivity, positive predictive value,
F1-score and AUC values). Random forests and XGBoost models consistently
outperformed other methods, with the random forest model achieving the highest
sensitivity and AUC across scenarios, including real-world prediction of 2023
herd status, correctly identifying 219 of 250 positive herds while halving the
number of herds that require compared to a blanket-testing strategy.

</details>


### [164] [Transfer Learning via Auxiliary Labels with Application to Cold-Hardiness Prediction](https://arxiv.org/abs/2504.13142)
*Kristen Goebel,Paola Pesantez-Cabrera,Markus Keller,Alan Fern*

Main category: cs.LG

TL;DR: 论文提出了一种新的迁移学习框架TAL，利用农民已有的物候数据预测作物的抗寒性，即使缺乏特定作物的抗寒数据。


<details>
  <summary>Details</summary>
Motivation: 低温可能导致果树冻害，但抗寒性数据稀缺且获取成本高，而农民通常有物候数据。TAL旨在利用物候数据填补抗寒性数据的不足。

Method: TAL框架通过源任务（有抗寒性和物候数据）迁移到目标任务（仅有物候数据），提出基于模型选择和平均的新方法。

Result: 在多个葡萄品种的真实数据上，TAL能利用物候数据提升抗寒性预测的准确性。

Conclusion: TAL为缺乏抗寒性数据的作物提供了有效的预测方法，填补了迁移学习领域的空白。

Abstract: Cold temperatures can cause significant frost damage to fruit crops depending
on their resilience, or cold hardiness, which changes throughout the dormancy
season. This has led to the development of predictive cold-hardiness models,
which help farmers decide when to deploy expensive frost-mitigation measures.
Unfortunately, cold-hardiness data for model training is only available for
some fruit cultivars due to the need for specialized equipment and expertise.
Rather, farmers often do have years of phenological data (e.g. date of
budbreak) that they regularly collect for their crops. In this work, we
introduce a new transfer-learning framework, Transfer via Auxiliary Labels
(TAL), that allows farmers to leverage the phenological data to produce more
accurate cold-hardiness predictions, even when no cold-hardiness data is
available for their specific crop. The framework assumes a set of source tasks
(cultivars) where each has associated primary labels (cold hardiness) and
auxiliary labels (phenology). However, the target task (new cultivar) is
assumed to only have the auxiliary labels. The goal of TAL is to predict
primary labels for the target task via transfer from the source tasks.
Surprisingly, despite the vast literature on transfer learning, to our
knowledge, the TAL formulation has not been previously addressed. Thus, we
propose several new TAL approaches based on model selection and averaging that
can leverage recent deep multi-task models for cold-hardiness prediction. Our
results on real-world cold-hardiness and phenological data for multiple grape
cultivars demonstrate that TAL can leverage the phenological data to improve
cold-hardiness predictions in the absence of cold-hardiness data.

</details>


### [165] [MIB: A Mechanistic Interpretability Benchmark](https://arxiv.org/abs/2504.13151)
*Aaron Mueller,Atticus Geiger,Sarah Wiegreffe,Dana Arad,Iván Arcuschin,Adam Belfki,Yik Siu Chan,Jaden Fiotto-Kaufman,Tal Haklay,Michael Hanna,Jing Huang,Rohan Gupta,Yaniv Nikankin,Hadas Orgad,Nikhil Prakash,Anja Reusch,Aruna Sankaranarayanan,Shun Shao,Alessandro Stolfo,Martin Tutek,Amir Zur,David Bau,Yonatan Belinkov*

Main category: cs.LG

TL;DR: MIB是一个用于评估机制解释性方法的基准测试，包含两个任务轨道，旨在比较方法在定位模型组件和因果变量方面的表现。


<details>
  <summary>Details</summary>
Motivation: 为了建立持久且有意义的评估标准，以判断机制解释性方法是否取得实际改进。

Method: 提出MIB基准测试，分为电路定位和因果变量定位两个轨道，涵盖四个任务和五个模型。

Result: 在电路定位中，归因和掩码优化方法表现最佳；在因果变量定位中，监督DAS方法最优，SAE特征与神经元表现相当。

Conclusion: MIB能有效比较方法，并证明该领域取得了实际进展。

Abstract: How can we know whether new mechanistic interpretability methods achieve real
improvements? In pursuit of meaningful and lasting evaluation standards, we
propose MIB, a benchmark with two tracks spanning four tasks and five models.
MIB favors methods that precisely and concisely recover relevant causal
pathways or specific causal variables in neural language models. The circuit
localization track compares methods that locate the model components - and
connections between them - most important for performing a task (e.g.,
attribution patching or information flow routes). The causal variable
localization track compares methods that featurize a hidden vector, e.g.,
sparse autoencoders (SAEs) or distributed alignment search (DAS), and locate
model features for a causal variable relevant to the task. Using MIB, we find
that attribution and mask optimization methods perform best on circuit
localization. For causal variable localization, we find that the supervised DAS
method performs best, while SAE features are not better than neurons, i.e.,
standard dimensions of hidden vectors. These findings illustrate that MIB
enables meaningful comparisons of methods, and increases our confidence that
there has been real progress in the field.

</details>


### [166] [It's All Connected: A Journey Through Test-Time Memorization, Attentional Bias, Retention, and Online Optimization](https://arxiv.org/abs/2504.13173)
*Ali Behrouz,Meisam Razaviyayn,Peilin Zhong,Vahab Mirrokni*

Main category: cs.LG

TL;DR: 论文提出了一种基于注意力偏见的神经架构设计框架Miras，通过重新定义注意力偏见和遗忘机制，开发了三种新型序列模型，性能超越现有线性RNN和Transformer。


<details>
  <summary>Details</summary>
Motivation: 受人类注意力偏见现象的启发，研究旨在提升基础模型的效率与效果，通过重新定义神经架构中的注意力偏见和遗忘机制。

Method: 提出Miras框架，包含四种设计选择：关联记忆架构、注意力偏见目标、保留门和记忆学习算法，并开发了三种新型序列模型。

Result: 实验表明，Miras框架下的模型在语言建模、常识推理等任务中表现优异，超越现有线性RNN和Transformer。

Conclusion: Miras为深度学习架构设计提供了新思路，其灵活性和性能优势为未来研究开辟了方向。

Abstract: Designing efficient and effective architectural backbones has been in the
core of research efforts to enhance the capability of foundation models.
Inspired by the human cognitive phenomenon of attentional bias-the natural
tendency to prioritize certain events or stimuli-we reconceptualize neural
architectures, including Transformers, Titans, and modern linear recurrent
neural networks as associative memory modules that learn a mapping of keys and
values using an internal objective, referred to as attentional bias.
Surprisingly, we observed that most existing sequence models leverage either
(1) dot-product similarity, or (2) L2 regression objectives as their
attentional bias. Going beyond these objectives, we present a set of
alternative attentional bias configurations along with their effective
approximations to stabilize their training procedure. We then reinterpret
forgetting mechanisms in modern deep learning architectures as a form of
retention regularization, providing a novel set of forget gates for sequence
models. Building upon these insights, we present Miras, a general framework to
design deep learning architectures based on four choices of: (i) associative
memory architecture, (ii) attentional bias objective, (iii) retention gate, and
(iv) memory learning algorithm. We present three novel sequence models-Moneta,
Yaad, and Memora-that go beyond the power of existing linear RNNs while
maintaining a fast parallelizable training process. Our experiments show
different design choices in Miras yield models with varying strengths. For
example, certain instances of Miras achieve exceptional performance in special
tasks such as language modeling, commonsense reasoning, and recall intensive
tasks, even outperforming Transformers and other modern linear recurrent
models.

</details>


### [167] [Aligning Constraint Generation with Design Intent in Parametric CAD](https://arxiv.org/abs/2504.13178)
*Evan Casey,Tianyu Zhang,Shu Ishida,John Roger Thompson,Amir Khasahmadi,Joseph George Lambourne,Pradeep Kumar Jayaraman,Karl D. D. Willis*

Main category: cs.LG

TL;DR: 论文提出了一种将LLM对齐技术应用于CAD模型中的工程草图约束生成的方法，解决了设计对齐问题，显著提高了约束生成的准确性。


<details>
  <summary>Details</summary>
Motivation: 当前方法生成的CAD设计难以完全捕捉设计意图，导致几何更新不可预测，因此需要解决设计对齐问题。

Method: 利用对齐技术训练现有的约束生成模型，结合约束求解器的反馈，生成完全约束的草图。

Result: 与基线方法相比，该方法将完全约束的草图比例从8.9%提升至93%。

Conclusion: 该方法适用于任何现有约束生成模型，为语言与设计领域的对齐策略研究奠定了基础。

Abstract: We adapt alignment techniques from reasoning LLMs to the task of generating
engineering sketch constraints found in computer-aided design (CAD) models.
Engineering sketches consist of geometric primitives (e.g. points, lines)
connected by constraints (e.g. perpendicular, tangent) that define the
relationships between them. For a design to be easily editable, the constraints
must effectively capture design intent, ensuring the geometry updates
predictably when parameters change. Although current approaches can generate
CAD designs, an open challenge remains to align model outputs with design
intent, we label this problem `design alignment'. A critical first step towards
aligning generative CAD models is to generate constraints which fully-constrain
all geometric primitives, without over-constraining or distorting sketch
geometry. Using alignment techniques to train an existing constraint generation
model with feedback from a constraint solver, we are able to fully-constrain
93% of sketches compared to 34% when using a na\"ive supervised fine-tuning
(SFT) baseline and only 8.9% without alignment. Our approach can be applied to
any existing constraint generation model and sets the stage for further
research bridging alignment strategies between the language and design domains.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [168] [Resonances in reflective Hamiltonian Monte Carlo](https://arxiv.org/abs/2504.12374)
*Namu Kroupa,Gábor Csányi,Will Handley*

Main category: stat.ML

TL;DR: 高维空间中，反射哈密顿蒙特卡洛方法在粒子群从狄拉克δ分布初始化且目标为均匀分布时，混合速度较慢。通过Sinkhorn散度量化的瞬时非均匀性，揭示了混合问题的机制。在球体和立方体中，集体运动在流体行为和离散化主导行为之间转变，临界步长随维度呈幂律缩放。两种情况下粒子可能自发解混，导致密度共振和混合问题。此外，构建了低维动态模型，重现了高维问题的主要特征。最后，对比了精确哈密顿粒子流并讨论了调参实践。


<details>
  <summary>Details</summary>
Motivation: 研究高维空间中反射哈密顿蒙特卡洛方法的混合问题，特别是在粒子群从狄拉克δ分布初始化和目标为均匀分布时的表现。

Method: 使用Sinkhorn散度量化的瞬时非均匀性分析混合机制，并在球体和立方体中观察集体运动行为。构建低维动态模型以重现高维问题。

Result: 发现粒子运动在流体行为和离散化主导行为之间转变，临界步长随维度呈幂律缩放，粒子可能自发解混导致密度共振。低维模型成功重现高维问题特征。

Conclusion: 反射哈密顿蒙特卡洛方法在高维空间中存在混合问题，可通过低维模型理解和优化调参实践。

Abstract: In high dimensions, reflective Hamiltonian Monte Carlo with inexact
reflections exhibits slow mixing when the particle ensemble is initialised from
a Dirac delta distribution and the uniform distribution is targeted. By
quantifying the instantaneous non-uniformity of the distribution with the
Sinkhorn divergence, we elucidate the principal mechanisms underlying the
mixing problems. In spheres and cubes, we show that the collective motion
transitions between fluid-like and discretisation-dominated behaviour, with the
critical step size scaling as a power law in the dimension. In both regimes,
the particles can spontaneously unmix, leading to resonances in the particle
density and the aforementioned problems. Additionally, low-dimensional toy
models of the dynamics are constructed which reproduce the dominant features of
the high-dimensional problem. Finally, the dynamics is contrasted with the
exact Hamiltonian particle flow and tuning practices are discussed.

</details>


### [169] [Robust and Scalable Variational Bayes](https://arxiv.org/abs/2504.12528)
*Carlos Misael Madrid Padilla,Shitao Fan,Lizhen Lin*

Main category: stat.ML

TL;DR: 提出了一种鲁棒且可扩展的变分贝叶斯框架，通过分块处理数据并聚合子集后验，利用几何中位数生成VM-Posterior，具有抗污染性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 处理大规模数据中的异常值和污染问题，提升变分贝叶斯方法的鲁棒性。

Method: 将数据集分块，独立计算每块的后验并应用变分近似，通过Wasserstein距离的几何中位数聚合生成VM-Posterior。

Result: VM-Posterior保持了真实后验的收缩性质，具有抗污染性，并通过实验验证了其鲁棒性和可扩展性。

Conclusion: VM-Posterior是一种适用于复杂污染数据的可靠贝叶斯推断工具。

Abstract: We propose a robust and scalable framework for variational Bayes (VB) that
effectively handles outliers and contamination of arbitrary nature in large
datasets. Our approach divides the dataset into disjoint subsets, computes the
posterior for each subset, and applies VB approximation independently to these
posteriors. The resulting variational posteriors with respect to the subsets
are then aggregated using the geometric median of probability measures,
computed with respect to the Wasserstein distance. This novel aggregation
method yields the Variational Median Posterior (VM-Posterior) distribution. We
rigorously demonstrate that the VM-Posterior preserves contraction properties
akin to those of the true posterior, while accounting for approximation errors
or the variational gap inherent in VB methods. We also provide provable
robustness guarantee of the VM-Posterior. Furthermore, we establish a
variational Bernstein-von Mises theorem for both multivariate Gaussian
distributions with general covariance structures and the mean-field variational
family. To facilitate practical implementation, we adapt existing algorithms
for computing the VM-Posterior and evaluate its performance through extensive
numerical experiments. The results highlight its robustness and scalability,
making it a reliable tool for Bayesian inference in the presence of complex,
contaminated datasets.

</details>


### [170] [Spectral Algorithms under Covariate Shift](https://arxiv.org/abs/2504.12625)
*Jun Fan,Zheng-Chu Guo,Lei Shi*

Main category: stat.ML

TL;DR: 论文研究了谱算法在分布偏移下的收敛行为，提出加权谱算法解决密度比无界时的次优问题，并证明其最优收敛率。


<details>
  <summary>Details</summary>
Motivation: 理解谱算法在训练和测试数据分布不同的实际场景中的性能，特别是在协变量偏移情况下。

Method: 在再生核希尔伯特空间框架下，分析谱算法的泛化误差，提出加权谱算法并引入权重裁剪技术。

Result: 加权谱算法在密度比有界时达到极小极大最优性，无界时通过加权和裁剪技术接近最优收敛率。

Conclusion: 加权谱算法解决了谱算法在无界密度比时的次优问题，提升了理论结果。

Abstract: Spectral algorithms leverage spectral regularization techniques to analyze
and process data, providing a flexible framework for addressing supervised
learning problems. To deepen our understanding of their performance in
real-world scenarios where the distributions of training and test data may
differ, we conduct a rigorous investigation into the convergence behavior of
spectral algorithms under distribution shifts, specifically within the
framework of reproducing kernel Hilbert spaces. Our study focuses on the case
of covariate shift. In this scenario, the marginal distributions of the input
data differ between the training and test datasets, while the conditional
distribution of the output given the input remains unchanged. Under this
setting, we analyze the generalization error of spectral algorithms and show
that they achieve minimax optimality when the density ratios between the
training and test distributions are uniformly bounded. However, we also
identify a critical limitation: when the density ratios are unbounded, the
spectral algorithms may become suboptimal. To address this limitation, we
propose a weighted spectral algorithm that incorporates density ratio
information into the learning process. Our theoretical analysis shows that this
weighted approach achieves optimal capacity-independent convergence rates.
Furthermore, by introducing a weight clipping technique, we demonstrate that
the convergence rates of the weighted spectral algorithm can approach the
optimal capacity-dependent convergence rates arbitrarily closely. This
improvement resolves the suboptimality issue in unbounded density ratio
scenarios and advances the state-of-the-art by refining existing theoretical
results.

</details>


### [171] [When do Random Forests work?](https://arxiv.org/abs/2504.12860)
*C. Revelas,O. Boldea,B. J. M. Werker*

Main category: stat.ML

TL;DR: 研究了随机森林中随机化分割方向的有效性，发现随机化在低信噪比（SNR）下表现更好，但在其他数据特性下可能增加偏差或无效。


<details>
  <summary>Details</summary>
Motivation: 探讨随机化在随机森林中的作用，尤其是在不同信噪比和数据特性下的表现。

Method: 通过系统分析不同SNR场景下的样本外均方误差（MSE），并结合模拟研究验证随机化的效果。

Result: 随机化在低SNR下有效，但在其他情况下可能增加偏差或无效；相关协变量下随机森林的偏差显著降低。

Conclusion: 随机化的效果不仅取决于SNR，还受数据特性影响；相关协变量的发现为随机森林的优异表现提供了新解释。

Abstract: We study the effectiveness of randomizing split-directions in random forests.
Prior literature has shown that, on the one hand, randomization can reduce
variance through decorrelation, and, on the other hand, randomization
regularizes and works in low signal-to-noise ratio (SNR) environments. First,
we bring together and revisit decorrelation and regularization by presenting a
systematic analysis of out-of-sample mean-squared error (MSE) for different SNR
scenarios based on commonly-used data-generating processes. We find that
variance reduction tends to increase with the SNR and forests outperform
bagging when the SNR is low because, in low SNR cases, variance dominates bias
for both methods. Second, we show that the effectiveness of randomization is a
question that goes beyond the SNR. We present a simulation study with fixed and
moderate SNR, in which we examine the effectiveness of randomization for other
data characteristics. In particular, we find that (i) randomization can
increase bias in the presence of fat tails in the distribution of covariates;
(ii) in the presence of irrelevant covariates randomization is ineffective
because bias dominates variance; and (iii) when covariates are mutually
correlated randomization tends to be effective because variance dominates bias.
Beyond randomization, we find that, for both bagging and random forests, bias
can be significantly reduced in the presence of correlated covariates. This
last finding goes beyond the prevailing view that averaging mostly works by
variance reduction. Given that in practice covariates are often correlated, our
findings on correlated covariates could open the way for a better understanding
of why random forests work well in many applications.

</details>


### [172] [Propagation of Chaos in One-hidden-layer Neural Networks beyond Logarithmic Time](https://arxiv.org/abs/2504.13110)
*Margalit Glasgow,Denny Wu,Joan Bruna*

Main category: stat.ML

TL;DR: 研究了多项式宽度神经网络与其无限宽度对应物在均值场尺度下的动态近似差距，并通过均值场动态控制的微分方程紧密界定了这一差距。


<details>
  <summary>Details</summary>
Motivation: 探讨神经网络宽度对动态近似的影响，特别是在均值场尺度下，以理解有限宽度网络是否能有效逼近无限宽度网络的动态行为。

Method: 利用均值场动态控制的微分方程，通过局部Hessian（定义为粒子速度对位置的导数）分析近似差距的增长。

Result: 在单指标模型的特征学习问题中，证明了由于“自一致性”特性，多项式数量的神经元足以在训练过程中紧密逼近均值场动态。

Conclusion: 有限宽度神经网络在均值场尺度下可以有效地逼近无限宽度网络的动态行为，特别是在具有自一致性特性的问题中。

Abstract: We study the approximation gap between the dynamics of a polynomial-width
neural network and its infinite-width counterpart, both trained using projected
gradient descent in the mean-field scaling regime. We demonstrate how to
tightly bound this approximation gap through a differential equation governed
by the mean-field dynamics. A key factor influencing the growth of this ODE is
the local Hessian of each particle, defined as the derivative of the particle's
velocity in the mean-field dynamics with respect to its position. We apply our
results to the canonical feature learning problem of estimating a
well-specified single-index model; we permit the information exponent to be
arbitrarily large, leading to convergence times that grow polynomially in the
ambient dimension $d$. We show that, due to a certain ``self-concordance''
property in these problems -- where the local Hessian of a particle is bounded
by a constant times the particle's velocity -- polynomially many neurons are
sufficient to closely approximate the mean-field dynamics throughout training.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [173] [Enhanced Battery Capacity Estimation in Data-Limited Scenarios through Swarm Learning](https://arxiv.org/abs/2504.12444)
*Jiawei Zhang,Yu Zhang,Wei Xu,Yifei Zhang,Weiran Jiang,Qi Jiao,Yao Ren,Ziyou Song*

Main category: eess.SY

TL;DR: 本文提出了一种基于群体学习（SL）的电池管理系统，通过分散式框架和基于可信权重的模型合并机制，在数据有限的情况下提升电池容量估计的准确性，同时确保数据隐私和安全。


<details>
  <summary>Details</summary>
Motivation: 数据驱动方法在电动汽车电池管理任务中潜力巨大，但在数据有限的情况下表现不佳，且缺乏同时确保数据隐私和容错性的有效框架。

Method: 采用分散式群体学习框架和基于可信权重的模型合并机制，验证了66个商用LiNiCoAlO2电池的数据集。

Result: SL在所有数据有限的情况下均提升了估计准确性，并在数据充足时达到与中心学习相当的精度。

Conclusion: 群体学习框架在数据隐私和安全的前提下，有效提升了电池容量估计的准确性，适用于数据有限场景。

Abstract: Data-driven methods have shown potential in electric-vehicle battery
management tasks such as capacity estimation, but their deployment is
bottlenecked by poor performance in data-limited scenarios. Sharing battery
data among algorithm developers can enable accurate and generalizable
data-driven models. However, an effective battery management framework that
simultaneously ensures data privacy and fault tolerance is still lacking. This
paper proposes a swarm battery management system that unites a decentralized
swarm learning (SL) framework and credibility weight-based model merging
mechanism to enhance battery capacity estimation in data-limited scenarios
while ensuring data privacy and security. The effectiveness of the SL framework
is validated on a dataset comprising 66 commercial LiNiCoAlO2 cells cycled
under various operating conditions. Specifically, the capacity estimation
performance is validated in four cases, including data-balanced, volume-biased,
feature-biased, and quality-biased scenarios. Our results show that SL can
enhance the estimation accuracy in all data-limited cases and achieve a similar
level of accuracy with central learning where large amounts of data are
available.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [174] [Anonymous Public Announcements](https://arxiv.org/abs/2504.12546)
*Thomas Ågotnes,Rustam Galimullin,Ken Satoh,Satoshi Tojo*

Main category: cs.LO

TL;DR: 论文研究了匿名公共公告的逻辑形式化，探讨了其在公共公告逻辑中的位置，分析了背景知识和意图对匿名性的影响，并给出了形式化表达和逻辑完备性结果。


<details>
  <summary>Details</summary>
Motivation: 研究匿名公共公告的逻辑特性，填补公共公告逻辑中匿名性研究的空白，并探讨背景知识和意图如何影响匿名性。

Method: 通过形式化匿名公共公告的逻辑，分为无意图假设和共同知识意图假设两种情况，分别分析其逻辑特性。

Result: 在无意图假设下，匿名公共公告逻辑可归约为认知逻辑；在共同知识意图假设下，逻辑更复杂且有趣，类似于“安全”公告。

Conclusion: 匿名公共公告逻辑在两种假设下具有不同的特性，形式化结果和完备性证明为相关研究提供了理论基础。

Abstract: We formalise the notion of an \emph{anonymous public announcement} in the
tradition of public announcement logic. Such announcements can be seen as
in-between a public announcement from ``the outside" (an announcement of
$\phi$) and a public announcement by one of the agents (an announcement of
$K_a\phi$): we get more information than just $\phi$, but not (necessarily)
about exactly who made it. Even if such an announcement is prima facie
anonymous, depending on the background knowledge of the agents it might reveal
the identity of the announcer: if I post something on a message board, the
information might reveal who I am even if I don't sign my name. Furthermore,
like in the Russian Cards puzzle, if we assume that the announcer's intention
was to stay anonymous, that in fact might reveal more information. In this
paper we first look at the case when no assumption about intentions are made,
in which case the logic with an anonymous public announcement operator is
reducible to epistemic logic. We then look at the case when we assume common
knowledge of the intention to stay anonymous, which is both more complex and
more interesting: in several ways it boils down to the notion of a ``safe"
announcement (again, similarly to Russian Cards). Main results include formal
expressivity results and axiomatic completeness for key logical languages.

</details>


<div id='nlin.CD'></div>

# nlin.CD [[Back]](#toc)

### [175] [Attractor-merging Crises and Intermittency in Reservoir Computing](https://arxiv.org/abs/2504.12695)
*Tempei Kabayama,Motomasa Komuro,Yasuo Kuniyoshi,Kazuyuki Aihara,Kohei Nakajima*

Main category: nlin.CD

TL;DR: 储层计算可通过调整全局参数在随机神经网络中引发吸引子合并危机，揭示其相空间结构机制，证明该分岔场景是通用的。


<details>
  <summary>Details</summary>
Motivation: 研究储层计算中吸引子合并危机的现象及其机制。

Method: 通过调整全局参数，分析相空间结构，揭示吸引子合并危机的机制。

Result: 发现吸引子合并危机伴随间歇性，证明该现象是随机神经网络的通用特性。

Conclusion: 吸引子合并危机是储层计算中普遍存在的现象，与训练数据无关。

Abstract: Reservoir computing can embed attractors into random neural networks (RNNs),
generating a ``mirror'' of a target attractor because of its inherent
symmetrical constraints. In these RNNs, we report that an attractor-merging
crisis accompanied by intermittency emerges simply by adjusting the global
parameter. We further reveal its underlying mechanism through a detailed
analysis of the phase-space structure and demonstrate that this bifurcation
scenario is intrinsic to a general class of RNNs, independent of training data.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [176] [Provable Secure Steganography Based on Adaptive Dynamic Sampling](https://arxiv.org/abs/2504.12579)
*Kaiyi Pang*

Main category: cs.CR

TL;DR: 提出一种无需显式访问生成模型分布的黑盒隐写方案，通过动态采样策略实现高效秘密通信。


<details>
  <summary>Details</summary>
Motivation: 当前可证明安全隐写方法需显式访问生成模型分布，限制了其在黑盒场景中的实用性。

Method: 采用动态采样策略，使生成模型在不干扰正常生成过程的情况下嵌入秘密消息。

Result: 在三个真实数据集和三个LLM上的评估表明，该黑盒方法在效率和容量上与现有白盒方法相当，且避免了模型生成输出的退化。

Conclusion: 该方案为黑盒场景下的安全隐写提供了实用且高效的解决方案。

Abstract: The security of private communication is increasingly at risk due to
widespread surveillance. Steganography, a technique for embedding secret
messages within innocuous carriers, enables covert communication over monitored
channels. Provably Secure Steganography (PSS) is state of the art for making
stego carriers indistinguishable from normal ones by ensuring computational
indistinguishability between stego and cover distributions. However, current
PSS methods often require explicit access to the distribution of generative
model for both sender and receiver, limiting their practicality in black box
scenarios. In this paper, we propose a provably secure steganography scheme
that does not require access to explicit model distributions for both sender
and receiver. Our method incorporates a dynamic sampling strategy, enabling
generative models to embed secret messages within multiple sampling choices
without disrupting the normal generation process of the model. Extensive
evaluations of three real world datasets and three LLMs demonstrate that our
blackbox method is comparable with existing white-box steganography methods in
terms of efficiency and capacity while eliminating the degradation of
steganography in model generated outputs.

</details>


### [177] [MCP Guardian: A Security-First Layer for Safeguarding MCP-Based AI System](https://arxiv.org/abs/2504.12757)
*Sonu Kumar,Anubhav Girdhar,Ritesh Patil,Divyansh Tripathi*

Main category: cs.CR

TL;DR: MCP Guardian是一个框架，通过认证、限流、日志、追踪和WAF扫描增强MCP通信的安全性，有效减少攻击风险。


<details>
  <summary>Details</summary>
Motivation: 随着Agentic AI的普及，模型能力快速提升，但系统仍受限于数据孤岛，集成需要定制逻辑且难以扩展。MCP协议虽解决了连接问题，但灵活性带来了新的安全风险。

Method: 提出MCP Guardian框架，通过多种安全措施（如认证、限流、日志等）加强MCP通信的安全性。

Result: 实证测试表明，MCP Guardian能有效减少攻击，确保稳健监管且开销低。

Conclusion: MCP Guardian为AI助手提供了安全、可扩展的数据访问，强调了深度防御在AI驱动环境中的重要性。

Abstract: As Agentic AI gain mainstream adoption, the industry invests heavily in model
capabilities, achieving rapid leaps in reasoning and quality. However, these
systems remain largely confined to data silos, and each new integration
requires custom logic that is difficult to scale. The Model Context Protocol
(MCP) addresses this challenge by defining a universal, open standard for
securely connecting AI-based applications (MCP clients) to data sources (MCP
servers). However, the flexibility of the MCP introduces new risks, including
malicious tool servers and compromised data integrity. We present MCP Guardian,
a framework that strengthens MCP-based communication with authentication,
rate-limiting, logging, tracing, and Web Application Firewall (WAF) scanning.
Through real-world scenarios and empirical testing, we demonstrate how MCP
Guardian effectively mitigates attacks and ensures robust oversight with
minimal overheads. Our approach fosters secure, scalable data access for AI
assistants, underscoring the importance of a defense-in-depth approach that
enables safer and more transparent innovation in AI-driven environments.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [178] [Deep Generative Model-Based Generation of Synthetic Individual-Specific Brain MRI Segmentations](https://arxiv.org/abs/2504.12352)
*Ruijie Wang,Luca Rossetto,Susan Mérillat,Christina Röcke,Mike Martin,Abraham Bernstein*

Main category: q-bio.NC

TL;DR: 提出了一种基于易获取的个体信息生成合成脑MRI分割的新方法CSegSynth，无需详细脑结构信息，性能优于现有生成模型。


<details>
  <summary>Details</summary>
Motivation: 现有方法需要详细的脑结构信息，但这些信息稀缺且昂贵，因此提出一种基于易获取信息的新方法。

Method: 使用新型深度生成模型CSegSynth，利用个体的人口统计、访谈和认知测试信息生成3D脑MRI分割。

Result: CSegSynth在生成质量上优于C-VAE、C-GAN和C-LDM，体积预测的Pearson相关系数达0.80、0.82和0.70。

Conclusion: CSegSynth是一种高效且实用的方法，能够基于易获取信息生成高质量的个体特异性脑MRI分割。

Abstract: To the best of our knowledge, all existing methods that can generate
synthetic brain magnetic resonance imaging (MRI) scans for a specific
individual require detailed structural or volumetric information about the
individual's brain. However, such brain information is often scarce, expensive,
and difficult to obtain. In this paper, we propose the first approach capable
of generating synthetic brain MRI segmentations -- specifically, 3D white
matter (WM), gray matter (GM), and cerebrospinal fluid (CSF) segmentations --
for individuals using their easily obtainable and often readily available
demographic, interview, and cognitive test information. Our approach features a
novel deep generative model, CSegSynth, which outperforms existing prominent
generative models, including conditional variational autoencoder (C-VAE),
conditional generative adversarial network (C-GAN), and conditional latent
diffusion model (C-LDM). We demonstrate the high quality of our synthetic
segmentations through extensive evaluations. Also, in assessing the
effectiveness of the individual-specific generation, we achieve superior volume
prediction, with Pearson correlation coefficients reaching 0.80, 0.82, and 0.70
between the ground-truth WM, GM, and CSF volumes of test individuals and those
volumes predicted based on generated individual-specific segmentations,
respectively.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [179] [WaterFlow: Learning Fast & Robust Watermarks using Stable Diffusion](https://arxiv.org/abs/2504.12354)
*Vinay Shukla,Prachee Sharma,Ryan Rossi,Sungchul Kim,Tong Yu,Aditya Grover*

Main category: eess.IV

TL;DR: WaterFlow (WF) 是一种基于学习潜在依赖水印的快速且鲁棒的高保真视觉水印方法，解决了现有技术在计算速度和鲁棒性上的不足。


<details>
  <summary>Details</summary>
Motivation: 当前水印技术在计算速度和鲁棒性之间存在权衡，无法满足实际部署需求，尤其是在生成图像快速增长的背景下。

Method: 利用预训练的潜在扩散模型将图像编码到潜在空间，并通过可逆流层在傅里叶域中嵌入水印，增强鲁棒性和图像质量。

Result: WaterFlow 在通用鲁棒性上表现优异，首次有效抵御复杂组合攻击，并在 MS-COCO、DiffusionDB 和 WikiArt 数据集上验证。

Conclusion: WaterFlow 是一种高效且鲁棒的水印方法，适用于实际部署。

Abstract: The ability to embed watermarks in images is a fundamental problem of
interest for computer vision, and is exacerbated by the rapid rise of generated
imagery in recent times. Current state-of-the-art techniques suffer from
computational and statistical challenges such as the slow execution speed for
practical deployments. In addition, other works trade off fast watermarking
speeds but suffer greatly in their robustness or perceptual quality. In this
work, we propose WaterFlow (WF), a fast and extremely robust approach for high
fidelity visual watermarking based on a learned latent-dependent watermark. Our
approach utilizes a pretrained latent diffusion model to encode an arbitrary
image into a latent space and produces a learned watermark that is then planted
into the Fourier Domain of the latent. The transformation is specified via
invertible flow layers that enhance the expressivity of the latent space of the
pre-trained model to better preserve image quality while permitting robust and
tractable detection. Most notably, WaterFlow demonstrates state-of-the-art
performance on general robustness and is the first method capable of
effectively defending against difficult combination attacks. We validate our
findings on three widely used real and generated datasets: MS-COCO,
DiffusionDB, and WikiArt.

</details>


### [180] [TUMLS: Trustful Fully Unsupervised Multi-Level Segmentation for Whole Slide Images of Histology](https://arxiv.org/abs/2504.12718)
*Walid Rehamnia,Alexandra Getmanskaya,Evgeniy Vasilyev,Vadim Turlapov*

Main category: eess.IV

TL;DR: 论文提出了一种名为TUMLS的无监督多级分割方法，用于解决数字病理学中AI应用的挑战，如标注需求高、计算量大和预测不确定性缺失。该方法通过自动编码器和不确定性度量实现高效分割，显著提升病理学家的工作流程。


<details>
  <summary>Details</summary>
Motivation: 当前AI在组织病理学中的应用面临标注需求高、计算复杂和缺乏预测不确定性估计等问题，限制了其实际应用。

Method: TUMLS采用自动编码器作为特征提取器，通过不确定性度量选择代表性图像块，并在高分辨率空间进行无监督细胞核分割。

Result: 在UPENN-GBM数据集上，自动编码器的MSE为0.0016；在MoNuSeg数据集上，细胞核分割的F1分数为77.46%，Jaccard分数为63.35%，优于其他无监督方法。

Conclusion: TUMLS通过透明、高效的无监督方法，显著提升了数字病理学的工作流程和结果可靠性。

Abstract: Digital pathology, augmented by artificial intelligence (AI), holds
significant promise for improving the workflow of pathologists. However,
challenges such as the labor-intensive annotation of whole slide images (WSIs),
high computational demands, and trust concerns arising from the absence of
uncertainty estimation in predictions hinder the practical application of
current AI methodologies in histopathology. To address these issues, we present
a novel trustful fully unsupervised multi-level segmentation methodology
(TUMLS) for WSIs. TUMLS adopts an autoencoder (AE) as a feature extractor to
identify the different tissue types within low-resolution training data. It
selects representative patches from each identified group based on an
uncertainty measure and then does unsupervised nuclei segmentation in their
respective higher-resolution space without using any ML algorithms. Crucially,
this solution integrates seamlessly into clinicians workflows, transforming the
examination of a whole WSI into a review of concise, interpretable cross-level
insights. This integration significantly enhances and accelerates the workflow
while ensuring transparency. We evaluated our approach using the UPENN-GBM
dataset, where the AE achieved a mean squared error (MSE) of 0.0016.
Additionally, nucleus segmentation is assessed on the MoNuSeg dataset,
outperforming all unsupervised approaches with an F1 score of 77.46% and a
Jaccard score of 63.35%. These results demonstrate the efficacy of TUMLS in
advancing the field of digital pathology.

</details>


### [181] [Towards Cardiac MRI Foundation Models: Comprehensive Visual-Tabular Representations for Whole-Heart Assessment and Beyond](https://arxiv.org/abs/2504.13037)
*Yundi Zhang,Paul Hager,Che Liu,Suprosanna Shit,Chen Chen,Daniel Rueckert,Jiazhen Pan*

Main category: eess.IV

TL;DR: ViTa是一个多模态框架，结合心脏磁共振成像（CMR）和患者个体因素，提供全面的心脏健康评估和疾病风险预测。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅依赖有限的时空数据或孤立任务，无法全面评估心脏健康。ViTa旨在填补这一空白。

Method: ViTa整合了42,000名UK Biobank参与者的3D+T心脏成像数据和患者个体因素，通过多模态学习生成共享潜在表示。

Result: ViTa支持多种下游任务，如心脏表型预测、分割和疾病分类，提供更全面的心脏健康评估。

Conclusion: ViTa通过多模态融合和共享表示，推动了心脏分析的临床实用性和可扩展性。

Abstract: Cardiac magnetic resonance imaging is the gold standard for non-invasive
cardiac assessment, offering rich spatio-temporal views of the cardiac anatomy
and physiology. Patient-level health factors, such as demographics, metabolic,
and lifestyle, are known to substantially influence cardiovascular health and
disease risk, yet remain uncaptured by CMR alone. To holistically understand
cardiac health and to enable the best possible interpretation of an
individual's disease risk, CMR and patient-level factors must be jointly
exploited within an integrated framework. Recent multi-modal approaches have
begun to bridge this gap, yet they often rely on limited spatio-temporal data
and focus on isolated clinical tasks, thereby hindering the development of a
comprehensive representation for cardiac health evaluation. To overcome these
limitations, we introduce ViTa, a step toward foundation models that delivers a
comprehensive representation of the heart and a precise interpretation of
individual disease risk. Leveraging data from 42,000 UK Biobank participants,
ViTa integrates 3D+T cine stacks from short-axis and long-axis views, enabling
a complete capture of the cardiac cycle. These imaging data are then fused with
detailed tabular patient-level factors, enabling context-aware insights. This
multi-modal paradigm supports a wide spectrum of downstream tasks, including
cardiac phenotype and physiological feature prediction, segmentation, and
classification of cardiac and metabolic diseases within a single unified
framework. By learning a shared latent representation that bridges rich imaging
features and patient context, ViTa moves beyond traditional, task-specific
models toward a universal, patient-specific understanding of cardiac health,
highlighting its potential to advance clinical utility and scalability in
cardiac analysis.

</details>


### [182] [NTIRE 2025 Challenge on Short-form UGC Video Quality Assessment and Enhancement: Methods and Results](https://arxiv.org/abs/2504.13131)
*Xin Li,Kun Yuan,Bingchen Li,Fengbin Guan,Yizhen Shao,Zihao Yu,Xijun Wang,Yiting Lu,Wei Luo,Suhang Yao,Ming Sun,Chao Zhou,Zhibo Chen,Radu Timofte,Yabin Zhang,Ao-Xiang Zhang,Tianwu Zhi,Jianzhao Liu,Yang Li,Jingwen Xu,Yiting Liao,Yushen Zuo,Mingyang Wu,Renjie Li,Shengyun Zhong,Zhengzhong Tu,Yufan Liu,Xiangguang Chen,Zuowei Cao,Minhao Tang,Shan Liu,Kexin Zhang,Jingfen Xie,Yan Wang,Kai Chen,Shijie Zhao,Yunchen Zhang,Xiangkai Xu,Hong Gao,Ji Shi,Yiming Bao,Xiugang Dong,Xiangsheng Zhou,Yaofeng Tu,Ying Liang,Yiwen Wang,Xinning Chai,Yuxuan Zhang,Zhengxue Cheng,Yingsheng Qin,Yucai Yang,Rong Xie,Li Song,Wei Sun,Kang Fu,Linhan Cao,Dandan Zhu,Kaiwei Zhang,Yucheng Zhu,Zicheng Zhang,Menghan Hu,Xiongkuo Min,Guangtao Zhai,Zhi Jin,Jiawei Wu,Wei Wang,Wenjian Zhang,Yuhai Lan,Gaoxiong Yi,Hengyuan Na,Wang Luo,Di Wu,MingYin Bai,Jiawang Du,Zilong Lu,Zhenyu Jiang,Hui Zeng,Ziguan Cui,Zongliang Gan,Guijin Tang,Xinglin Xie,Kehuan Song,Xiaoqiang Lu,Licheng Jiao,Fang Liu,Xu Liu,Puhua Chen,Ha Thu Nguyen,Katrien De Moor,Seyed Ali Amirshahi,Mohamed-Chaker Larabi,Qi Tang,Linfeng He,Zhiyong Gao,Zixuan Gao,Guohua Zhang,Zhiye Huang,Yi Deng,Qingmiao Jiang,Lu Chen,Yi Yang,Xi Liao,Nourine Mohammed Nadir,Yuxuan Jiang,Qiang Zhu,Siyue Teng,Fan Zhang,Shuyuan Zhu,Bing Zeng,David Bull,Meiqin Liu,Chao Yao,Yao Zhao*

Main category: eess.IV

TL;DR: 本文回顾了NTIRE 2025挑战赛的两个赛道：高效视频质量评估（KVQ）和基于扩散的图像超分辨率（KwaiSR），旨在提升短用户生成内容（UGC）平台的用户体验。


<details>
  <summary>Details</summary>
Motivation: 推动轻量级视频质量评估模型的发展，并针对短UGC平台（如Kwai和TikTok）优化图像超分辨率技术。

Method: Track 1专注于高效VQA模型，减少冗余计算；Track 2引入KwaiSR数据集，包含合成和真实图像对，用于单图像超分辨率。

Result: 挑战赛吸引了266名参与者，收到18份有效提交，显著推动了短UGC视频质量评估和图像超分辨率的研究。

Conclusion: 该挑战赛为短UGC平台的技术进步提供了重要支持，相关成果已公开。

Abstract: This paper presents a review for the NTIRE 2025 Challenge on Short-form UGC
Video Quality Assessment and Enhancement. The challenge comprises two tracks:
(i) Efficient Video Quality Assessment (KVQ), and (ii) Diffusion-based Image
Super-Resolution (KwaiSR). Track 1 aims to advance the development of
lightweight and efficient video quality assessment (VQA) models, with an
emphasis on eliminating reliance on model ensembles, redundant weights, and
other computationally expensive components in the previous IQA/VQA
competitions. Track 2 introduces a new short-form UGC dataset tailored for
single image super-resolution, i.e., the KwaiSR dataset. It consists of 1,800
synthetically generated S-UGC image pairs and 1,900 real-world S-UGC images,
which are split into training, validation, and test sets using a ratio of
8:1:1. The primary objective of the challenge is to drive research that
benefits the user experience of short-form UGC platforms such as Kwai and
TikTok. This challenge attracted 266 participants and received 18 valid final
submissions with corresponding fact sheets, significantly contributing to the
progress of short-form UGC VQA and image superresolution. The project is
publicly available at https://github.com/lixinustc/KVQE-
ChallengeCVPR-NTIRE2025.

</details>


<div id='hep-th'></div>

# hep-th [[Back]](#toc)

### [183] [A Two-Phase Perspective on Deep Learning Dynamics](https://arxiv.org/abs/2504.12700)
*Robert de Mello Koch,Animik Ghosh*

Main category: hep-th

TL;DR: 论文提出深度学习分为快速拟合和缓慢压缩两阶段，支持这一观点的现象包括grokking、双下降和信息瓶颈。


<details>
  <summary>Details</summary>
Motivation: 探讨深度学习训练过程中不同阶段的时间结构及其对泛化的影响。

Method: 通过实验验证两种不同设置下的时间尺度对齐，并使用互信息作为进展度量。

Result: 发现第二阶段未被标准训练算法主动优化，可能导致不必要的时间延长。

Conclusion: 压缩阶段反映了一种原则性的遗忘形式，对泛化至关重要。

Abstract: We propose that learning in deep neural networks proceeds in two phases: a
rapid curve fitting phase followed by a slower compression or coarse graining
phase. This view is supported by the shared temporal structure of three
phenomena: grokking, double descent and the information bottleneck, all of
which exhibit a delayed onset of generalization well after training error
reaches zero. We empirically show that the associated timescales align in two
rather different settings. Mutual information between hidden layers and input
data emerges as a natural progress measure, complementing circuit-based metrics
such as local complexity and the linear mapping number. We argue that the
second phase is not actively optimized by standard training algorithms and may
be unnecessarily prolonged. Drawing on an analogy with the renormalization
group, we suggest that this compression phase reflects a principled form of
forgetting, critical for generalization.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [184] [A Multi-task Learning Balanced Attention Convolutional Neural Network Model for Few-shot Underwater Acoustic Target Recognition](https://arxiv.org/abs/2504.13102)
*Wei Huang,Shumeng Sun,Junpeng Lu,Zhenpeng Xu,Zhengyang Xiu,Hao Zhang*

Main category: cs.SD

TL;DR: 提出了一种多任务平衡通道注意力卷积神经网络（MT-BCA-CNN），用于解决水下声学目标识别中的样本稀缺和环境干扰问题，显著提升了分类性能。


<details>
  <summary>Details</summary>
Motivation: 水下声学目标识别对海洋多样性保护和国防安全至关重要，但面临样本稀缺和复杂环境干扰的挑战。

Method: 结合通道注意力机制和多任务学习策略，构建共享特征提取器和多任务分类器，动态增强判别性声学特征并抑制噪声。

Result: 在27类少样本场景下，分类准确率达97%，F1分数达95%，显著优于传统方法。

Conclusion: MT-BCA-CNN为少样本水下声学识别提供了高效解决方案，推动了海洋生物声学和声纳信号处理的研究。

Abstract: Underwater acoustic target recognition (UATR) is of great significance for
the protection of marine diversity and national defense security. The
development of deep learning provides new opportunities for UATR, but faces
challenges brought by the scarcity of reference samples and complex
environmental interference. To address these issues, we proposes a multi-task
balanced channel attention convolutional neural network (MT-BCA-CNN). The
method integrates a channel attention mechanism with a multi-task learning
strategy, constructing a shared feature extractor and multi-task classifiers to
jointly optimize target classification and feature reconstruction tasks. The
channel attention mechanism dynamically enhances discriminative acoustic
features such as harmonic structures while suppressing noise. Experiments on
the Watkins Marine Life Dataset demonstrate that MT-BCA-CNN achieves 97\%
classification accuracy and 95\% $F1$-score in 27-class few-shot scenarios,
significantly outperforming traditional CNN and ACNN models, as well as popular
state-of-the-art UATR methods. Ablation studies confirm the synergistic
benefits of multi-task learning and attention mechanisms, while a dynamic
weighting adjustment strategy effectively balances task contributions. This
work provides an efficient solution for few-shot underwater acoustic
recognition, advancing research in marine bioacoustics and sonar signal
processing.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [185] [A Survey on Archetypal Analysis](https://arxiv.org/abs/2504.12392)
*Aleix Alcacer,Irene Epifanio,Sebastian Mair,Morten Mørup*

Main category: stat.ME

TL;DR: 本文综述了原型分析（AA）的方法、应用及未来研究方向，强调了其在特征提取和降维中的解释性和广泛适用性。


<details>
  <summary>Details</summary>
Motivation: 原型分析提供了一种直观且可解释的高维数据结构表示方法，但其非凸优化问题带来了挑战。本文旨在综述AA的方法、应用及未来方向。

Method: 综述了AA的计算方法、数据建模最佳实践及其在不同科学领域的应用。

Result: AA在特征提取和降维中表现出色，但面临非凸优化等挑战。

Conclusion: 未来研究应关注AA的优化问题和扩展应用，以进一步提升其性能。

Abstract: Archetypal analysis (AA) was originally proposed in 1994 by Adele Cutler and
Leo Breiman as a computational procedure to extract the distinct aspects called
archetypes in observations with each observational record approximated as a
mixture (i.e., convex combination) of these archetypes. AA thereby provides
straightforward, interpretable, and explainable representations for feature
extraction and dimensionality reduction, facilitating the understanding of the
structure of high-dimensional data with wide applications throughout the
sciences. However, AA also faces challenges, particularly as the associated
optimization problem is non-convex. This survey provides researchers and data
mining practitioners an overview of methodologies and opportunities that AA has
to offer surveying the many applications of AA across disparate fields of
science, as well as best practices for modeling data using AA and limitations.
The survey concludes by explaining important future research directions
concerning AA.

</details>


### [186] [Cluster weighted models with multivariate skewed distributions for functional data](https://arxiv.org/abs/2504.12683)
*Cristina Anton,Roy Shivam Ram Shreshtth*

Main category: stat.ME

TL;DR: 提出了一种基于功能线性回归模型和三种偏态多元分布的聚类方法funWeightClustSkew，扩展了funHDDC框架，并构建了EM算法进行参数估计。


<details>
  <summary>Details</summary>
Motivation: 为功能数据开发一种基于偏态分布的聚类方法，以弥补现有方法在功能数据上的不足。

Method: 结合功能线性回归模型和三种偏态多元分布（方差-伽马、偏态-t、正态逆高斯），构建EM算法进行参数估计。

Result: 在模拟数据和空气质量数据集上验证了方法的性能。

Conclusion: funWeightClustSkew方法在功能数据聚类中表现出色，适用于偏态分布场景。

Abstract: We propose a clustering method, funWeightClustSkew, based on mixtures of
functional linear regression models and three skewed multivariate
distributions: the variance-gamma distribution, the skew-t distribution, and
the normal-inverse Gaussian distribution. Our approach follows the framework of
the functional high dimensional data clustering (funHDDC) method, and we extend
to functional data the cluster weighted models based on skewed distributions
used for finite dimensional multivariate data. We consider several parsimonious
models, and to estimate the parameters we construct an expectation maximization
(EM) algorithm. We illustrate the performance of funWeightClustSkew for
simulated data and for the Air Quality dataset.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [187] [AUTONAV: A Toolfor Autonomous Navigation of Robots](https://arxiv.org/abs/2504.12318)
*Mir Md Sajid Sarwar,Sudip Samanta,Rajarshi Ray*

Main category: cs.RO

TL;DR: AUTONAV是一个自动化机器人导航工具，用于地图构建、定位和路径规划，支持模块化算法集成，并在室内模拟场景中展示结果。


<details>
  <summary>Details</summary>
Motivation: 开发AUTONAV的目的是为了简化机器人自主导航中的地图构建、定位和路径规划任务，并提供模块化架构以方便算法比较。

Method: 采用模块化架构，集成多种算法进行地图构建、定位和路径规划，并在室内模拟场景中测试。

Result: 生成了室内模拟场景中的地图和路径规划结果。

Conclusion: AUTONAV是一个有效的工具，能够自动化机器人导航任务，并支持灵活算法集成。

Abstract: We present a tool AUTONAV that automates the mapping, localization, and
path-planning tasks for autonomous navigation of robots. The modular
architecture allows easy integration of various algorithms for these tasks for
comparison. We present the generated maps and path-plans by AUTONAV in indoor
simulation scenarios.

</details>


### [188] [Crossing the Human-Robot Embodiment Gap with Sim-to-Real RL using One Human Demonstration](https://arxiv.org/abs/2504.12609)
*Tyler Ga Wei Lum,Olivia Y. Lee,C. Karen Liu,Jeannette Bohg*

Main category: cs.RO

TL;DR: Human2Sim2Robot框架通过单段RGB-D视频训练机器人灵巧操作策略，无需大量数据或穿戴设备。


<details>
  <summary>Details</summary>
Motivation: 解决传统方法依赖大量演示或穿戴设备的可扩展性问题。

Method: 利用强化学习在模拟环境中跨越人-机器人形态差异，提取物体姿态轨迹和手部初始姿态指导训练。

Result: 比开放环轨迹重放和数据增强模仿学习分别提升55%和68%性能。

Conclusion: Human2Sim2Robot提供了一种高效、数据需求低的机器人灵巧操作学习方法。

Abstract: Teaching robots dexterous manipulation skills often requires collecting
hundreds of demonstrations using wearables or teleoperation, a process that is
challenging to scale. Videos of human-object interactions are easier to collect
and scale, but leveraging them directly for robot learning is difficult due to
the lack of explicit action labels from videos and morphological differences
between robot and human hands. We propose Human2Sim2Robot, a novel
real-to-sim-to-real framework for training dexterous manipulation policies
using only one RGB-D video of a human demonstrating a task. Our method utilizes
reinforcement learning (RL) in simulation to cross the human-robot embodiment
gap without relying on wearables, teleoperation, or large-scale data collection
typically necessary for imitation learning methods. From the demonstration, we
extract two task-specific components: (1) the object pose trajectory to define
an object-centric, embodiment-agnostic reward function, and (2) the
pre-manipulation hand pose to initialize and guide exploration during RL
training. We found that these two components are highly effective for learning
the desired task, eliminating the need for task-specific reward shaping and
tuning. We demonstrate that Human2Sim2Robot outperforms object-aware open-loop
trajectory replay by 55% and imitation learning with data augmentation by 68%
across grasping, non-prehensile manipulation, and multi-step tasks. Project
Site: https://human2sim2robot.github.io

</details>


### [189] [Diffusion Based Robust LiDAR Place Recognition](https://arxiv.org/abs/2504.12412)
*Benjamin Krummenacher,Jonas Frey,Turcan Tuna,Olga Vysotska,Marco Hutter*

Main category: cs.RO

TL;DR: 论文提出了一种基于LiDAR数据和神经网络的机器人全局重定位方法，解决了建筑工地中重复特征导致的定位难题。


<details>
  <summary>Details</summary>
Motivation: 建筑工地中存在重复特征（如相似的墙面和布局），导致机器人定位困难，需要一种有效的全局重定位方法。

Method: 使用合成LiDAR点云训练扩散模型（基于PointNet++），生成多模态位置候选，实现全局定位。

Result: 在五个真实数据集上测试，平均定位准确率为77%±2m，性能优于基线方法两倍。

Conclusion: 该方法能有效应对建筑工地中的感知混淆问题，提供多模态位置分布，提升机器人定位精度。

Abstract: Mobile robots on construction sites require accurate pose estimation to
perform autonomous surveying and inspection missions. Localization in
construction sites is a particularly challenging problem due to the presence of
repetitive features such as flat plastered walls and perceptual aliasing due to
apartments with similar layouts inter and intra floors. In this paper, we focus
on the global re-positioning of a robot with respect to an accurate scanned
mesh of the building solely using LiDAR data. In our approach, a neural network
is trained on synthetic LiDAR point clouds generated by simulating a LiDAR in
an accurate real-life large-scale mesh. We train a diffusion model with a
PointNet++ backbone, which allows us to model multiple position candidates from
a single LiDAR point cloud. The resulting model can successfully predict the
global position of LiDAR in confined and complex sites despite the adverse
effects of perceptual aliasing. The learned distribution of potential global
positions can provide multi-modal position distribution. We evaluate our
approach across five real-world datasets and show the place recognition
accuracy of 77% +/-2m on average while outperforming baselines at a factor of 2
in mean error.

</details>


### [190] [Learning Transferable Friction Models and LuGre Identification via Physics Informed Neural Networks](https://arxiv.org/abs/2504.12441)
*Asutay Ozmen,João P. Hespanha,Katie Byl*

Main category: cs.RO

TL;DR: 提出了一种物理信息驱动的摩擦估计框架，结合经典摩擦模型与可学习组件，仅需少量通用数据即可准确模拟动态摩擦特性，缩小仿真与现实的差距。


<details>
  <summary>Details</summary>
Motivation: 机器人仿真器（如Mujoco和PyBullet）使用简化的摩擦模型或启发式方法，导致仿真与物理性能存在显著差异，亟需更准确的摩擦建模方法。

Method: 提出物理信息驱动的摩擦估计框架，结合经典摩擦模型与可学习组件，仅需少量通用数据训练。

Result: 在欠驱动和非线性系统上验证，模型能准确模拟动态摩擦特性，缩小仿真与现实的差距，并展示跨系统泛化能力。

Conclusion: 该方法为机器人控制中的摩擦建模提供了可扩展且可解释的路径，有助于缩小仿真与现实的差距。

Abstract: Accurately modeling friction in robotics remains a core challenge, as
robotics simulators like Mujoco and PyBullet use simplified friction models or
heuristics to balance computational efficiency with accuracy, where these
simplifications and approximations can lead to substantial differences between
simulated and physical performance. In this paper, we present a
physics-informed friction estimation framework that enables the integration of
well-established friction models with learnable components-requiring only
minimal, generic measurement data. Our approach enforces physical consistency
yet retains the flexibility to adapt to real-world complexities. We
demonstrate, on an underactuated and nonlinear system, that the learned
friction models, trained solely on small and noisy datasets, accurately
simulate dynamic friction properties and reduce the sim-to-real gap. Crucially,
we show that our approach enables the learned models to be transferable to
systems they are not trained on. This ability to generalize across multiple
systems streamlines friction modeling for complex, underactuated tasks,
offering a scalable and interpretable path toward bridging the sim-to-real gap
in robotics and control.

</details>


### [191] [Trajectory Adaptation using Large Language Models](https://arxiv.org/abs/2504.12755)
*Anurag Maurya,Tashmoy Ghosh,Ravi Prakash*

Main category: cs.RO

TL;DR: 提出了一种基于语言的灵活框架，利用预训练LLM生成代码来调整机器人轨迹，无需任务特定训练，支持复杂指令。


<details>
  <summary>Details</summary>
Motivation: 实现更直观和可扩展的人机交互，适应新情境下的机器人轨迹调整。

Method: 利用预训练LLM生成代码作为策略，调整通用机器人轨迹，支持数值输入和复杂指令。

Result: 在仿真环境中验证了LLM能成功适应复杂指令，优于现有方法。

Conclusion: 该方法无需任务特定训练，具有更高解释性和反馈效果，适用于多种机器人平台。

Abstract: Adapting robot trajectories based on human instructions as per new situations
is essential for achieving more intuitive and scalable human-robot
interactions. This work proposes a flexible language-based framework to adapt
generic robotic trajectories produced by off-the-shelf motion planners like
RRT, A-star, etc, or learned from human demonstrations. We utilize pre-trained
LLMs to adapt trajectory waypoints by generating code as a policy for dense
robot manipulation, enabling more complex and flexible instructions than
current methods. This approach allows us to incorporate a broader range of
commands, including numerical inputs. Compared to state-of-the-art
feature-based sequence-to-sequence models which require training, our method
does not require task-specific training and offers greater interpretability and
more effective feedback mechanisms. We validate our approach through simulation
experiments on the robotic manipulator, aerial vehicle, and ground robot in the
Pybullet and Gazebo simulation environments, demonstrating that LLMs can
successfully adapt trajectories to complex human instructions.

</details>


### [192] [Explainable Scene Understanding with Qualitative Representations and Graph Neural Networks](https://arxiv.org/abs/2504.12817)
*Nassim Belmecheri,Arnaud Gotlieb,Nadjib Lazaar,Helge Spieker*

Main category: cs.RO

TL;DR: 本文提出了一种结合图神经网络（GNN）和定性可解释图（QXG）的新方法，用于自动驾驶中的场景理解，显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 场景理解是自动驾驶决策的基础，但现有方法仅分析单对对象关系，忽略了全局场景上下文。

Method: 提出一种新型GNN架构，处理完整图结构以识别交通场景中的相关对象，并在nuScenes数据集上评估。

Result: 实验表明，该方法在相关对象识别任务中表现优异，能有效处理类别不平衡并考虑时空关系。

Conclusion: 结合定性表示与深度学习方法，为自动驾驶中的可解释场景理解提供了潜力。

Abstract: This paper investigates the integration of graph neural networks (GNNs) with
Qualitative Explainable Graphs (QXGs) for scene understanding in automated
driving. Scene understanding is the basis for any further reactive or proactive
decision-making. Scene understanding and related reasoning is inherently an
explanation task: why is another traffic participant doing something, what or
who caused their actions? While previous work demonstrated QXGs' effectiveness
using shallow machine learning models, these approaches were limited to
analysing single relation chains between object pairs, disregarding the broader
scene context. We propose a novel GNN architecture that processes entire graph
structures to identify relevant objects in traffic scenes. We evaluate our
method on the nuScenes dataset enriched with DriveLM's human-annotated
relevance labels. Experimental results show that our GNN-based approach
achieves superior performance compared to baseline methods. The model
effectively handles the inherent class imbalance in relevant object
identification tasks while considering the complete spatial-temporal
relationships between all objects in the scene. Our work demonstrates the
potential of combining qualitative representations with deep learning
approaches for explainable scene understanding in autonomous driving systems.

</details>


### [193] [RoboTwin: Dual-Arm Robot Benchmark with Generative Digital Twins](https://arxiv.org/abs/2504.13059)
*Yao Mu,Tianxing Chen,Zanxin Chen,Shijia Peng,Zhiqian Lan,Zeyu Gao,Zhixuan Liang,Qiaojun Yu,Yude Zou,Mingkun Xu,Lunkai Lin,Zhiqiang Xie,Mingyu Ding,Ping Luo*

Main category: cs.RO

TL;DR: RoboTwin是一个生成数字孪生框架，利用3D生成基础模型和大型语言模型，为双臂机器人任务提供多样化的专家数据集和真实世界对齐的评估平台。


<details>
  <summary>Details</summary>
Motivation: 解决双臂协调和复杂物体操作中高质量演示数据和真实世界对齐评估基准的稀缺问题。

Method: 通过3D生成模型从单张2D图像创建多样化的数字孪生对象，并结合空间关系感知的代码生成框架分解任务、确定空间约束并生成精确的机器人运动代码。

Result: 在COBOT Magic Robot平台上验证，预训练策略显著提升任务成功率（单臂任务70%以上，双臂任务40%以上）。

Conclusion: RoboTwin框架为双臂机器人系统提供了标准化评估和模拟训练与真实世界性能对齐的有效解决方案。

Abstract: In the rapidly advancing field of robotics, dual-arm coordination and complex
object manipulation are essential capabilities for developing advanced
autonomous systems. However, the scarcity of diverse, high-quality
demonstration data and real-world-aligned evaluation benchmarks severely limits
such development. To address this, we introduce RoboTwin, a generative digital
twin framework that uses 3D generative foundation models and large language
models to produce diverse expert datasets and provide a real-world-aligned
evaluation platform for dual-arm robotic tasks. Specifically, RoboTwin creates
varied digital twins of objects from single 2D images, generating realistic and
interactive scenarios. It also introduces a spatial relation-aware code
generation framework that combines object annotations with large language
models to break down tasks, determine spatial constraints, and generate precise
robotic movement code. Our framework offers a comprehensive benchmark with both
simulated and real-world data, enabling standardized evaluation and better
alignment between simulated training and real-world performance. We validated
our approach using the open-source COBOT Magic Robot platform. Policies
pre-trained on RoboTwin-generated data and fine-tuned with limited real-world
samples demonstrate significant potential for enhancing dual-arm robotic
manipulation systems by improving success rates by over 70% for single-arm
tasks and over 40% for dual-arm tasks compared to models trained solely on
real-world data.

</details>


### [194] [RUKA: Rethinking the Design of Humanoid Hands with Learning](https://arxiv.org/abs/2504.13165)
*Anya Zorin,Irmak Guzey,Billy Yan,Aadhithya Iyer,Lisa Kondrich,Nikhil X. Bhattasali,Lerrel Pinto*

Main category: cs.RO

TL;DR: RUKA是一种紧凑、经济且功能强大的肌腱驱动仿人机械手，通过3D打印和现成组件制成，具有15个欠驱动自由度，支持多样化抓取。基于学习的方法解决了控制挑战，并通过运动捕捉数据建模。实验表明其可达性、耐用性和力量优于其他机械手。


<details>
  <summary>Details</summary>
Motivation: 现有机械手在精度、紧凑性、强度和成本之间存在权衡，且控制方法限制了设计灵活性。学习型方法为解决肌腱驱动和低成本材料的挑战提供了新思路。

Method: 开发了RUKA机械手，采用3D打印和现成组件，具有15个欠驱动自由度。通过MANUS手套的运动捕捉数据学习关节-执行器和指尖-执行器模型。

Result: RUKA在可达性、耐用性和力量方面表现优异，能够完成灵巧的远程操作任务。

Conclusion: RUKA为仿人机械手的设计和控制提供了开源解决方案，展示了学习型方法在解决硬件限制中的潜力。

Abstract: Dexterous manipulation is a fundamental capability for robotic systems, yet
progress has been limited by hardware trade-offs between precision,
compactness, strength, and affordability. Existing control methods impose
compromises on hand designs and applications. However, learning-based
approaches present opportunities to rethink these trade-offs, particularly to
address challenges with tendon-driven actuation and low-cost materials. This
work presents RUKA, a tendon-driven humanoid hand that is compact, affordable,
and capable. Made from 3D-printed parts and off-the-shelf components, RUKA has
5 fingers with 15 underactuated degrees of freedom enabling diverse human-like
grasps. Its tendon-driven actuation allows powerful grasping in a compact,
human-sized form factor. To address control challenges, we learn
joint-to-actuator and fingertip-to-actuator models from motion-capture data
collected by the MANUS glove, leveraging the hand's morphological accuracy.
Extensive evaluations demonstrate RUKA's superior reachability, durability, and
strength compared to other robotic hands. Teleoperation tasks further showcase
RUKA's dexterous movements. The open-source design and assembly instructions of
RUKA, code, and data are available at https://ruka-hand.github.io/.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [195] [Boosting Reservoir Computing with Brain-inspired Adaptive Dynamics](https://arxiv.org/abs/2504.12480)
*Keshav Srinivasan,Dietmar Plenz,Michelle Girvan*

Main category: cs.NE

TL;DR: 论文提出一种自适应的储层计算机（RC）机制，通过动态调整兴奋-抑制平衡（E-I balance）来提高性能，减少超参数调优需求。


<details>
  <summary>Details</summary>
Motivation: 传统RC实现忽略了神经元动态中的兴奋-抑制平衡（E-I balance），而这对大脑功能至关重要。研究旨在通过动态适应机制提升RC性能。

Method: 引入自适应的E-I平衡机制，局部调整以实现目标神经元放电率，并结合大脑启发的异质性。

Result: 自适应RC在记忆能力和时间序列预测任务中性能提升高达130%，且在多种任务中表现优异。

Conclusion: 动态适应机制显著提升RC性能和鲁棒性，为神经计算提供了新见解。

Abstract: Reservoir computers (RCs) provide a computationally efficient alternative to
deep learning while also offering a framework for incorporating brain-inspired
computational principles. By using an internal neural network with random,
fixed connections$-$the 'reservoir'$-$and training only the output weights, RCs
simplify the training process but remain sensitive to the choice of
hyperparameters that govern activation functions and network architecture.
Moreover, typical RC implementations overlook a critical aspect of neuronal
dynamics: the balance between excitatory and inhibitory (E-I) signals, which is
essential for robust brain function. We show that RCs characteristically
perform best in balanced or slightly over-inhibited regimes, outperforming
excitation-dominated ones. To reduce the need for precise hyperparameter
tuning, we introduce a self-adapting mechanism that locally adjusts E/I balance
to achieve target neuronal firing rates, improving performance by up to 130% in
tasks like memory capacity and time series prediction compared with globally
tuned RCs. Incorporating brain-inspired heterogeneity in target neuronal firing
rates further reduces the need for fine-tuning hyperparameters and enables RCs
to excel across linear and non-linear tasks. These results support a shift from
static optimization to dynamic adaptation in reservoir design, demonstrating
how brain-inspired mechanisms improve RC performance and robustness while
deepening our understanding of neural computation.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [196] [Prototype-Guided Diffusion for Digital Pathology: Achieving Foundation Model Performance with Minimal Clinical Data](https://arxiv.org/abs/2504.12351)
*Ekaterina Redekop,Mara Pleasure,Vedrana Ivezic,Zichen Wang,Kimberly Flores,Anthony Sisk,William Speier,Corey Arnold*

Main category: cs.GR

TL;DR: 研究者提出了一种原型引导的扩散模型，用于生成高质量合成病理数据，减少对真实患者样本的依赖，同时保持下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 探索数据集大小与性能之间的关系，并减少对大规模真实病理数据的依赖。

Method: 使用原型引导的扩散模型生成合成病理数据，结合自监督学习。

Result: 合成数据训练的特征性能与大规模真实数据相当，且混合数据方法表现更优。

Conclusion: 生成式AI可高效创建病理训练数据，减少对临床数据集的依赖。

Abstract: Foundation models in digital pathology use massive datasets to learn useful
compact feature representations of complex histology images. However, there is
limited transparency into what drives the correlation between dataset size and
performance, raising the question of whether simply adding more data to
increase performance is always necessary. In this study, we propose a
prototype-guided diffusion model to generate high-fidelity synthetic pathology
data at scale, enabling large-scale self-supervised learning and reducing
reliance on real patient samples while preserving downstream performance. Using
guidance from histological prototypes during sampling, our approach ensures
biologically and diagnostically meaningful variations in the generated data. We
demonstrate that self-supervised features trained on our synthetic dataset
achieve competitive performance despite using ~60x-760x less data than models
trained on large real-world datasets. Notably, models trained using our
synthetic data showed statistically comparable or better performance across
multiple evaluation metrics and tasks, even when compared to models trained on
orders of magnitude larger datasets. Our hybrid approach, combining synthetic
and real data, further enhanced performance, achieving top results in several
evaluations. These findings underscore the potential of generative AI to create
compelling training data for digital pathology, significantly reducing the
reliance on extensive clinical datasets and highlighting the efficiency of our
approach.

</details>


### [197] [3D-PNAS: 3D Industrial Surface Anomaly Synthesis with Perlin Noise](https://arxiv.org/abs/2504.12856)
*Yifeng Cheng,Juan Du*

Main category: cs.GR

TL;DR: 本文提出了一种基于Perlin噪声和表面参数化的3D异常生成方法3D-PNAS，解决了工业异常检测中3D数据生成不足的问题。


<details>
  <summary>Details</summary>
Motivation: 工业异常检测中，真实缺陷样本稀缺，而3D传感器在制造业的应用使得3D数据用于表面质量检测成为趋势。然而，3D异常生成方法尚未充分探索。

Method: 通过将点云投影到2D平面，从Perlin噪声场采样多尺度噪声值，并沿法线方向扰动点云，生成逼真的3D表面异常。

Result: 实验表明，该方法能精细控制生成异常的噪声尺度、扰动强度和八度，生成多样化的缺陷模式。跨类别实验显示其适应不同物体表面特性。

Conclusion: 3D-PNAS为工业质量检测提供了有效的3D异常生成工具，并提供了代码库和可视化工具包以促进未来研究。

Abstract: Large pretrained vision foundation models have shown significant potential in
various vision tasks. However, for industrial anomaly detection, the scarcity
of real defect samples poses a critical challenge in leveraging these models.
While 2D anomaly generation has significantly advanced with established
generative models, the adoption of 3D sensors in industrial manufacturing has
made leveraging 3D data for surface quality inspection an emerging trend. In
contrast to 2D techniques, 3D anomaly generation remains largely unexplored,
limiting the potential of 3D data in industrial quality inspection. To address
this gap, we propose a novel yet simple 3D anomaly generation method, 3D-PNAS,
based on Perlin noise and surface parameterization. Our method generates
realistic 3D surface anomalies by projecting the point cloud onto a 2D plane,
sampling multi-scale noise values from a Perlin noise field, and perturbing the
point cloud along its normal direction. Through comprehensive visualization
experiments, we demonstrate how key parameters - including noise scale,
perturbation strength, and octaves, provide fine-grained control over the
generated anomalies, enabling the creation of diverse defect patterns from
pronounced deformations to subtle surface variations. Additionally, our
cross-category experiments show that the method produces consistent yet
geometrically plausible anomalies across different object types, adapting to
their specific surface characteristics. We also provide a comprehensive
codebase and visualization toolkit to facilitate future research.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [198] [Specialized text classification: an approach to classifying Open Banking transactions](https://arxiv.org/abs/2504.12319)
*Duc Tuyen TA,Wajdi Ben Saad,Ji Young Oh*

Main category: cs.IR

TL;DR: 本文介绍了一种基于语言的开放银行交易分类系统，专注于法语市场和法语文本，通过数据收集、标注、预处理、建模和评估阶段，解决了银行领域特定文本语料库训练的挑战。


<details>
  <summary>Details</summary>
Motivation: 随着PSD2法规引入开放银行框架，银行和金融科技公司有机会通过丰富交易描述来更好地理解客户行为，从而预防欺诈、降低风险并提供个性化服务。然而，银行领域的特定文本语料库的自然语言处理应用仍未得到充分解决。

Method: 系统包括数据收集、标注、预处理、建模和评估阶段，结合语言特定技术和领域知识，针对法语银行数据训练语言模型。

Result: 与通用方法相比，该系统表现出更高的性能和效率。

Conclusion: 该研究为银行领域的特定语言处理提供了定制化解决方案，展示了在法语开放银行交易分类中的优越性。

Abstract: With the introduction of the PSD2 regulation in the EU which established the
Open Banking framework, a new window of opportunities has opened for banks and
fintechs to explore and enrich Bank transaction descriptions with the aim of
building a better understanding of customer behavior, while using this
understanding to prevent fraud, reduce risks and offer more competitive and
tailored services.
  And although the usage of natural language processing models and techniques
has seen an incredible progress in various applications and domains over the
past few years, custom applications based on domain-specific text corpus remain
unaddressed especially in the banking sector.
  In this paper, we introduce a language-based Open Banking transaction
classification system with a focus on the french market and french language
text. The system encompasses data collection, labeling, preprocessing,
modeling, and evaluation stages. Unlike previous studies that focus on general
classification approaches, this system is specifically tailored to address the
challenges posed by training a language model with a specialized text corpus
(Banking data in the French context). By incorporating language-specific
techniques and domain knowledge, the proposed system demonstrates enhanced
performance and efficiency compared to generic approaches.

</details>


### [199] [A Human-AI Comparative Analysis of Prompt Sensitivity in LLM-Based Relevance Judgment](https://arxiv.org/abs/2504.12408)
*Negar Arabzadeh,Charles L. A . Clarke*

Main category: cs.IR

TL;DR: 研究探讨了大型语言模型（LLMs）在信息检索任务中作为相关性判断工具的鲁棒性和可靠性，重点分析了提示敏感性对任务的影响。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在信息检索任务中的广泛应用，其与人类标注的一致性接近人类间一致性，但提示敏感性对结果的影响尚未系统研究。

Method: 收集了来自15名人类专家和15个LLMs的提示，过滤后使用72个提示和3个LLMs对TREC数据集进行标注，并与人类标注进行对比。

Result: 通过Cohen's κ和成对一致性度量，分析了提示变化对LLMs与人类标注一致性的影响，并比较了不同LLMs作为标注者的差异。

Conclusion: 研究支持LLMs在相关性判断中的潜力，同时强调了提示设计的重要性，并公开了数据和提示以供未来研究。

Abstract: Large Language Models (LLMs) are increasingly used to automate relevance
judgments for information retrieval (IR) tasks, often demonstrating agreement
with human labels that approaches inter-human agreement. To assess the
robustness and reliability of LLM-based relevance judgments, we systematically
investigate impact of prompt sensitivity on the task. We collected prompts for
relevance assessment from 15 human experts and 15 LLMs across three tasks~ --
~binary, graded, and pairwise~ -- ~yielding 90 prompts in total. After
filtering out unusable prompts from three humans and three LLMs, we employed
the remaining 72 prompts with three different LLMs as judges to label
document/query pairs from two TREC Deep Learning Datasets (2020 and 2021). We
compare LLM-generated labels with TREC official human labels using Cohen's
$\kappa$ and pairwise agreement measures. In addition to investigating the
impact of prompt variations on agreement with human labels, we compare human-
and LLM-generated prompts and analyze differences among different LLMs as
judges. We also compare human- and LLM-generated prompts with the standard
UMBRELA prompt used for relevance assessment by Bing and TREC 2024 Retrieval
Augmented Generation (RAG) Track. To support future research in LLM-based
evaluation, we release all data and prompts at
https://github.com/Narabzad/prompt-sensitivity-relevance-judgements/.

</details>


### [200] [SimUSER: Simulating User Behavior with Large Language Models for Recommender System Evaluation](https://arxiv.org/abs/2504.12722)
*Nicolas Bougie,Narimasa Watanabe*

Main category: cs.IR

TL;DR: SimUSER是一个代理框架，用于模拟真实用户行为，以解决推荐系统评估中离线指标与在线行为之间的差距问题。


<details>
  <summary>Details</summary>
Motivation: 由于真实用户数据的稀缺性和隐私问题，需要一种可信且成本效益高的方法来模拟用户行为。

Method: SimUSER通过从历史数据中识别自洽的用户角色，并配备人物、记忆、感知和大脑模块，模拟用户与推荐系统的互动。

Result: SimUSER在微观和宏观层面均比先前工作更接近真实用户行为，并通过实验探索了缩略图、曝光效应和评论对用户参与度的影响。

Conclusion: 基于离线A/B测试结果优化推荐系统参数，SimUSER显著提升了真实世界中的用户参与度。

Abstract: Recommender systems play a central role in numerous real-life applications,
yet evaluating their performance remains a significant challenge due to the gap
between offline metrics and online behaviors. Given the scarcity and limits
(e.g., privacy issues) of real user data, we introduce SimUSER, an agent
framework that serves as believable and cost-effective human proxies. SimUSER
first identifies self-consistent personas from historical data, enriching user
profiles with unique backgrounds and personalities. Then, central to this
evaluation are users equipped with persona, memory, perception, and brain
modules, engaging in interactions with the recommender system. SimUSER exhibits
closer alignment with genuine humans than prior work, both at micro and macro
levels. Additionally, we conduct insightful experiments to explore the effects
of thumbnails on click rates, the exposure effect, and the impact of reviews on
user engagement. Finally, we refine recommender system parameters based on
offline A/B test results, resulting in improved user engagement in the real
world.

</details>


### [201] [Benchmarking LLM-based Relevance Judgment Methods](https://arxiv.org/abs/2504.12558)
*Negar Arabzadeh,Charles L. A. Clarke*

Main category: cs.IR

TL;DR: 本文系统比较了多种基于大语言模型（LLM）的相关性评估方法，包括二元判断、分级评估、成对偏好和基于片段的方法，并在多个数据集上验证其与人类判断的一致性。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注通过提示策略复现人类分级相关性判断，缺乏对其他评估方法的探索和全面比较。

Method: 比较了多种LLM评估方法（二元、分级、成对偏好、片段法），使用Kendall相关性和人类偏好对齐分析。

Result: 在TREC和ANTIQUE数据集上验证了不同方法的性能，并公开了开源和商业模型的判断数据。

Conclusion: 研究为LLM相关性评估提供了全面比较，代码和数据公开以促进未来研究。

Abstract: Large Language Models (LLMs) are increasingly deployed in both academic and
industry settings to automate the evaluation of information seeking systems,
particularly by generating graded relevance judgments. Previous work on
LLM-based relevance assessment has primarily focused on replicating graded
human relevance judgments through various prompting strategies. However, there
has been limited exploration of alternative assessment methods or comprehensive
comparative studies. In this paper, we systematically compare multiple
LLM-based relevance assessment methods, including binary relevance judgments,
graded relevance assessments, pairwise preference-based methods, and two
nugget-based evaluation methods~--~document-agnostic and document-dependent. In
addition to a traditional comparison based on system rankings using Kendall
correlations, we also examine how well LLM judgments align with human
preferences, as inferred from relevance grades. We conduct extensive
experiments on datasets from three TREC Deep Learning tracks 2019, 2020 and
2021 as well as the ANTIQUE dataset, which focuses on non-factoid open-domain
question answering. As part of our data release, we include relevance judgments
generated by both an open-source (Llama3.2b) and a commercial (gpt-4o) model.
Our goal is to \textit{reproduce} various LLM-based relevance judgment methods
to provide a comprehensive comparison. All code, data, and resources are
publicly available in our GitHub Repository at
https://github.com/Narabzad/llm-relevance-judgement-comparison.

</details>


### [202] [Towards Lossless Token Pruning in Late-Interaction Retrieval Models](https://arxiv.org/abs/2504.12778)
*Yuxuan Zong,Benjamin Piwowarski*

Main category: cs.IR

TL;DR: 本文提出了一种基于正则化损失和剪枝策略的方法，以减少ColBERT模型中的文档标记数量，同时保持检索性能。


<details>
  <summary>Details</summary>
Motivation: ColBERT等神经IR模型需要大量内存存储文档标记的上下文表示，现有剪枝方法无法保证不影响检索分数。

Method: 引入三种正则化损失和两种剪枝策略，以无影响检索分数的方式剪枝标记。

Result: 实验表明，仅使用30%的标记即可保持ColBERT的性能。

Conclusion: 提出的方法在保持性能的同时显著减少了内存需求。

Abstract: Late interaction neural IR models like ColBERT offer a competitive
effectiveness-efficiency trade-off across many benchmarks. However, they
require a huge memory space to store the contextual representation for all the
document tokens. Some works have proposed using either heuristics or
statistical-based techniques to prune tokens from each document. This however
doesn't guarantee that the removed tokens have no impact on the retrieval
score. Our work uses a principled approach to define how to prune tokens
without impacting the score between a document and a query. We introduce three
regularization losses, that induce a solution with high pruning ratios, as well
as two pruning strategies. We study them experimentally (in and out-domain),
showing that we can preserve ColBERT's performance while using only 30\% of the
tokens.

</details>


### [203] [Building Russian Benchmark for Evaluation of Information Retrieval Models](https://arxiv.org/abs/2504.12879)
*Grigory Kovalev,Mikhail Tikhomirov,Evgeny Kozhevnikov,Max Kornilov,Natalia Loukachevitch*

Main category: cs.IR

TL;DR: RusBEIR是一个用于俄语信息检索模型零样本评估的综合基准，包含17个数据集，支持对词汇和神经模型的系统比较。


<details>
  <summary>Details</summary>
Motivation: 为俄语信息检索研究提供一个统一的、开源的评估框架，填补该领域的空白。

Method: 整合了适应、翻译和新创建的数据集，对比了词汇模型（如BM25）和神经模型（如mE5-large和BGE-M3）的性能。

Result: 词汇模型在形态丰富的语言中依赖预处理，BM25在全文档检索中表现良好；神经模型在多数数据集上表现更优，但对长文档检索存在输入尺寸限制。

Conclusion: RusBEIR为俄语信息检索研究提供了重要工具，推动了该领域的发展。

Abstract: We introduce RusBEIR, a comprehensive benchmark designed for zero-shot
evaluation of information retrieval (IR) models in the Russian language.
Comprising 17 datasets from various domains, it integrates adapted, translated,
and newly created datasets, enabling systematic comparison of lexical and
neural models. Our study highlights the importance of preprocessing for lexical
models in morphologically rich languages and confirms BM25 as a strong baseline
for full-document retrieval. Neural models, such as mE5-large and BGE-M3,
demonstrate superior performance on most datasets, but face challenges with
long-document retrieval due to input size constraints. RusBEIR offers a
unified, open-source framework that promotes research in Russian-language
information retrieval.

</details>


### [204] [FreshStack: Building Realistic Benchmarks for Evaluating Retrieval on Technical Documents](https://arxiv.org/abs/2504.13128)
*Nandan Thakur,Jimmy Lin,Sam Havens,Michael Carbin,Omar Khattab,Andrew Drozdov*

Main category: cs.IR

TL;DR: FreshStack是一个可重用的框架，用于从社区问答中自动构建信息检索（IR）评估基准。它通过自动收集语料、生成问题答案片段（nugget）和混合检索技术构建数据集，并在五个新兴主题上验证了现有检索模型的不足。


<details>
  <summary>Details</summary>
Motivation: 构建现实、可扩展且无污染的IR和RAG评估基准，以推动信息检索质量的提升。

Method: FreshStack通过三步构建基准：自动语料收集、nugget生成和混合检索技术支持。

Result: 在五个新兴主题上，现有检索模型表现显著低于理想方法，且部分情况下重排序未提升检索准确性。

Conclusion: FreshStack为构建高质量IR评估基准提供了工具，并揭示了现有模型的改进空间。

Abstract: We introduce FreshStack, a reusable framework for automatically building
information retrieval (IR) evaluation benchmarks from community-asked questions
and answers. FreshStack conducts the following steps: (1) automatic corpus
collection from code and technical documentation, (2) nugget generation from
community-asked questions and answers, and (3) nugget-level support, retrieving
documents using a fusion of retrieval techniques and hybrid architectures. We
use FreshStack to build five datasets on fast-growing, recent, and niche topics
to ensure the tasks are sufficiently challenging. On FreshStack, existing
retrieval models, when applied out-of-the-box, significantly underperform
oracle approaches on all five topics, denoting plenty of headroom to improve IR
quality. In addition, we identify cases where rerankers do not clearly improve
first-stage retrieval accuracy (two out of five topics). We hope that
FreshStack will facilitate future work toward constructing realistic, scalable,
and uncontaminated IR and RAG evaluation benchmarks. FreshStack datasets are
available at: https://fresh-stack.github.io.

</details>


### [205] [SemCORE: A Semantic-Enhanced Generative Cross-Modal Retrieval Framework with MLLMs](https://arxiv.org/abs/2504.13172)
*Haoxuan Li,Yi Bin,Yunshan Ma,Guoqing Wang,Yang Yang,See-Kiong Ng,Tat-Seng Chua*

Main category: cs.IR

TL;DR: 论文提出了一种新的生成式跨模态检索框架SemCORE，通过结构化自然语言标识符和生成语义验证策略提升语义理解能力，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统跨模态检索方法依赖嵌入相似性计算，而生成式检索虽具潜力，但在标识符构建和生成过程中存在语义信息不足的问题。

Method: 提出SemCORE框架，包括结构化自然语言标识符（SID）和生成语义验证（GSV）策略，同时支持文本到图像和图像到文本检索。

Result: 实验表明SemCORE在基准数据集上表现优异，文本到图像检索的Recall@1平均提升8.65分。

Conclusion: SemCORE通过增强语义理解能力，显著提升了生成式跨模态检索的性能，为未来研究提供了新方向。

Abstract: Cross-modal retrieval (CMR) is a fundamental task in multimedia research,
focused on retrieving semantically relevant targets across different
modalities. While traditional CMR methods match text and image via
embedding-based similarity calculations, recent advancements in pre-trained
generative models have established generative retrieval as a promising
alternative. This paradigm assigns each target a unique identifier and
leverages a generative model to directly predict identifiers corresponding to
input queries without explicit indexing. Despite its great potential, current
generative CMR approaches still face semantic information insufficiency in both
identifier construction and generation processes. To address these limitations,
we propose a novel unified Semantic-enhanced generative Cross-mOdal REtrieval
framework (SemCORE), designed to unleash the semantic understanding
capabilities in generative cross-modal retrieval task. Specifically, we first
construct a Structured natural language IDentifier (SID) that effectively
aligns target identifiers with generative models optimized for natural language
comprehension and generation. Furthermore, we introduce a Generative Semantic
Verification (GSV) strategy enabling fine-grained target discrimination.
Additionally, to the best of our knowledge, SemCORE is the first framework to
simultaneously consider both text-to-image and image-to-text retrieval tasks
within generative cross-modal retrieval. Extensive experiments demonstrate that
our framework outperforms state-of-the-art generative cross-modal retrieval
methods. Notably, SemCORE achieves substantial improvements across benchmark
datasets, with an average increase of 8.65 points in Recall@1 for text-to-image
retrieval.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [206] [Large Language Model-Based Knowledge Graph System Construction for Sustainable Development Goals: An AI-Based Speculative Design Perspective](https://arxiv.org/abs/2504.12309)
*Yi-De Lin,Guan-Ze Liao*

Main category: cs.CY

TL;DR: 本研究开发了一个AI驱动的知识图谱系统，分析可持续发展目标（SDGs）的关联，并提出潜在新目标。


<details>
  <summary>Details</summary>
Motivation: 随着2030年临近，SDGs进展滞后，需要创新策略加速实现目标。

Method: 利用官方SDG文本、Elsevier关键词数据集和TED演讲转录，结合AI推测设计、大语言模型和检索增强生成技术。

Result: 发现目标10与16关联紧密，目标6覆盖较少；知识图谱揭示新节点；提出6个潜在新目标。

Conclusion: 该AI推测框架为政策制定者提供新见解，并为未来多模态和跨系统SDG应用奠定基础。

Abstract: From 2000 to 2015, the UN's Millennium Development Goals guided global
priorities. The subsequent Sustainable Development Goals (SDGs) adopted a more
dynamic approach, with annual indicator updates. As 2030 nears and progress
lags, innovative acceleration strategies are critical. This study develops an
AI-powered knowledge graph system to analyze SDG interconnections, discover
potential new goals, and visualize them online. Using official SDG texts,
Elsevier's keyword dataset, and 1,127 TED Talk transcripts (2020-2023), a pilot
on 269 talks from 2023 applies AI-speculative design, large language models,
and retrieval-augmented generation. Key findings include: (1) Heatmap analysis
reveals strong associations between Goal 10 and Goal 16, and minimal coverage
of Goal 6. (2) In the knowledge graph, simulated dialogue over time reveals new
central nodes, showing how richer data supports divergent thinking and goal
clarity. (3) Six potential new goals are proposed, centered on equity,
resilience, and technology-driven inclusion. This speculative-AI framework
offers fresh insights for policymakers and lays groundwork for future
multimodal and cross-system SDG applications.

</details>


### [207] [Towards an AI Observatory for the Nuclear Sector: A tool for anticipatory governance](https://arxiv.org/abs/2504.12358)
*Aditi Verma,Elizabeth Williams*

Main category: cs.CY

TL;DR: 论文呼吁建立核能领域AI的预见性治理体系和全球AI观察站，以应对AI嵌入核能研究的安全、安保和保障问题。


<details>
  <summary>Details</summary>
Motivation: AI在核能领域的广泛应用带来了安全、安保和保障方面的潜在风险，但这些问题尚未被充分理解。

Method: 借鉴科技研究、公共政策和前瞻性研究的成果，探讨核能AI观察站和预见性治理体系的框架。

Result: 提出了建立全球AI观察站和预见性治理体系的必要性及其潜在轮廓。

Conclusion: 通过预见性治理和全球观察站，可以更好地应对AI在核能领域的安全挑战。

Abstract: AI models are rapidly becoming embedded in all aspects of nuclear energy
research and work but the safety, security, and safeguards consequences of this
embedding are not well understood. In this paper, we call for the creation of
an anticipatory system of governance for AI in the nuclear sector as well as
the creation of a global AI observatory as a means for operationalizing
anticipatory governance. The paper explores the contours of the nuclear AI
observatory and an anticipatory system of governance by drawing on work in
science and technology studies, public policy, and foresight studies.

</details>


### [208] [What do people expect from Artificial Intelligence? Public opinion on alignment in AI moderation from Germany and the United States](https://arxiv.org/abs/2504.12476)
*Andreas Jungherr,Adrian Rauchfleisch*

Main category: cs.CY

TL;DR: 论文通过调查德美两国公众对AI系统功能特征的偏好，探讨了AI对齐的公众期望及其跨国差异。


<details>
  <summary>Details</summary>
Motivation: 研究动机是了解公众对AI系统的期望及其在不同国家背景下的差异，以填补AI对齐讨论中公众态度实证数据的空白。

Method: 方法包括对德国（n=1800）和美国（n=1756）的两项调查，分析公众对AI对齐功能（如准确性、安全性、公平性等）的支持程度及其影响因素。

Result: 结果显示，美国受访者对AI使用更频繁且对所有对齐功能支持更高；两国均最支持准确性和安全性，但对公平性和理想化目标的支持较谨慎，尤其在德国。

Conclusion: 结论强调实证研究对AI对齐讨论的价值，并呼吁将公众期望纳入AI治理的理论和政策讨论。

Abstract: Recent advances in generative Artificial Intelligence have raised public
awareness, shaping expectations and concerns about their societal implications.
Central to these debates is the question of AI alignment -- how well AI systems
meet public expectations regarding safety, fairness, and social values.
However, little is known about what people expect from AI-enabled systems and
how these expectations differ across national contexts. We present evidence
from two surveys of public preferences for key functional features of
AI-enabled systems in Germany (n = 1800) and the United States (n = 1756). We
examine support for four types of alignment in AI moderation: accuracy and
reliability, safety, bias mitigation, and the promotion of aspirational
imaginaries. U.S. respondents report significantly higher AI use and
consistently greater support for all alignment features, reflecting broader
technological openness and higher societal involvement with AI. In both
countries, accuracy and safety enjoy the strongest support, while more
normatively charged goals -- like fairness and aspirational imaginaries --
receive more cautious backing, particularly in Germany. We also explore how
individual experience with AI, attitudes toward free speech, political
ideology, partisan affiliation, and gender shape these preferences. AI use and
free speech support explain more variation in Germany. In contrast, U.S.
responses show greater attitudinal uniformity, suggesting that higher exposure
to AI may consolidate public expectations. These findings contribute to debates
on AI governance and cross-national variation in public preferences. More
broadly, our study demonstrates the value of empirically grounding AI alignment
debates in public attitudes and of explicitly developing normatively grounded
expectations into theoretical and policy discussions on the governance of
AI-generated content.

</details>


### [209] [Knowledge Acquisition on Mass-shooting Events via LLMs for AI-Driven Justice](https://arxiv.org/abs/2504.12545)
*Benign John Ihugba,Afsana Nasrin,Ling Wu,Lin Li,Lijun Qian,Xishuang Dong*

Main category: cs.CY

TL;DR: 该论文提出了一种基于命名实体识别（NER）技术的数据集，用于从大规模枪击事件中提取关键信息，如罪犯、受害者、地点和犯罪工具。实验表明，GPT-4o和o1-mini在性能上表现最佳。


<details>
  <summary>Details</summary>
Motivation: 大规模枪击事件产生大量非结构化文本数据，传统方法难以有效提取关键信息以支持法律和调查工作。

Method: 使用大型语言模型（LLMs）进行少样本提示，从新闻、警方报告和社交媒体中提取关键实体。

Result: GPT-4o在NER任务中表现最优，o1-mini作为资源高效替代方案。增加样本量能提升所有模型性能，但GPT-4o和o1-mini提升更显著。

Conclusion: 该研究为大规模枪击事件的信息提取提供了高效工具，GPT-4o和o1-mini在少样本学习中表现突出。

Abstract: Mass-shooting events pose a significant challenge to public safety,
generating large volumes of unstructured textual data that hinder effective
investigations and the formulation of public policy. Despite the urgency, few
prior studies have effectively automated the extraction of key information from
these events to support legal and investigative efforts. This paper presented
the first dataset designed for knowledge acquisition on mass-shooting events
through the application of named entity recognition (NER) techniques. It
focuses on identifying key entities such as offenders, victims, locations, and
criminal instruments, that are vital for legal and investigative purposes. The
NER process is powered by Large Language Models (LLMs) using few-shot
prompting, facilitating the efficient extraction and organization of critical
information from diverse sources, including news articles, police reports, and
social media. Experimental results on real-world mass-shooting corpora
demonstrate that GPT-4o is the most effective model for mass-shooting NER,
achieving the highest Micro Precision, Micro Recall, and Micro F1-scores.
Meanwhile, o1-mini delivers competitive performance, making it a
resource-efficient alternative for less complex NER tasks. It is also observed
that increasing the shot count enhances the performance of all models, but the
gains are more substantial for GPT-4o and o1-mini, highlighting their superior
adaptability to few-shot learning scenarios.

</details>


### [210] [How Large Language Models Are Changing MOOC Essay Answers: A Comparison of Pre- and Post-LLM Responses](https://arxiv.org/abs/2504.13038)
*Leo Leppänen,Lili Aunimo,Arto Hellas,Jukka K. Nurminen,Linda Mannila*

Main category: cs.CY

TL;DR: 论文分析了ChatGPT发布后对学生在线教育的影响，通过分析MOOC课程中学生论文的变化，发现论文长度和风格显著改变，但主题未变。


<details>
  <summary>Details</summary>
Motivation: 研究ChatGPT等大型语言模型（LLMs）对在线教育的具体影响，尤其是学术诚信和学习方式的变化。

Method: 分析一个关于AI伦理的免费大学MOOC课程中学生提交的论文数据，比较ChatGPT发布前后的变化。

Result: ChatGPT发布后，学生论文的长度和风格显著改变，但主题内容未变。关键词（如AI和LLMs）的出现频率增加。

Conclusion: ChatGPT对在线教育的影响主要体现在写作风格和形式上，而非主题内容。需进一步研究其对学术诚信的长期影响。

Abstract: The release of ChatGPT in late 2022 caused a flurry of activity and concern
in the academic and educational communities. Some see the tool's ability to
generate human-like text that passes at least cursory inspections for factual
accuracy ``often enough'' a golden age of information retrieval and
computer-assisted learning. Some, on the other hand, worry the tool may lead to
unprecedented levels of academic dishonesty and cheating. In this work, we
quantify some of the effects of the emergence of Large Language Models (LLMs)
on online education by analyzing a multi-year dataset of student essay
responses from a free university-level MOOC on AI ethics. Our dataset includes
essays submitted both before and after ChatGPT's release. We find that the
launch of ChatGPT coincided with significant changes in both the length and
style of student essays, mirroring observations in other contexts such as
academic publishing. We also observe -- as expected based on related public
discourse -- changes in prevalence of key content words related to AI and LLMs,
but not necessarily the general themes or topics discussed in the student
essays as identified through (dynamic) topic modeling.

</details>


### [211] [Tackling Social Bias against the Poor: A Dataset and Taxonomy on Aporophobia](https://arxiv.org/abs/2504.13085)
*Georgina Curto,Svetlana Kiritchenko,Muhammad Hammad Fahim Siddiqui,Isar Nejadgholi,Kathleen C. Fraser*

Main category: cs.CY

TL;DR: 该论文旨在通过社交媒体数据识别和追踪针对贫困人群的偏见（aporophobia），为消除贫困政策提供支持。


<details>
  <summary>Details</summary>
Motivation: 贫困是联合国可持续发展目标的首要问题，但社会对贫困人群的偏见（aporophobia）阻碍了扶贫政策的制定与实施。

Method: 与非营利组织和政府合作，收集并标注英语推文，构建分类器以自动检测aporophobia。

Result: 构建了aporophobia的分类法，并训练了分类器，但自动检测仍面临挑战。

Conclusion: 该研究为大规模识别和减少社交媒体上的aporophobia奠定了基础。

Abstract: Eradicating poverty is the first goal in the United Nations Sustainable
Development Goals. However, aporophobia -- the societal bias against people
living in poverty -- constitutes a major obstacle to designing, approving and
implementing poverty-mitigation policies. This work presents an initial step
towards operationalizing the concept of aporophobia to identify and track
harmful beliefs and discriminative actions against poor people on social media.
In close collaboration with non-profits and governmental organizations, we
conduct data collection and exploration. Then we manually annotate a corpus of
English tweets from five world regions for the presence of (1) direct
expressions of aporophobia, and (2) statements referring to or criticizing
aporophobic views or actions of others, to comprehensively characterize the
social media discourse related to bias and discrimination against the poor.
Based on the annotated data, we devise a taxonomy of categories of aporophobic
attitudes and actions expressed through speech on social media. Finally, we
train several classifiers and identify the main challenges for automatic
detection of aporophobia in social networks. This work paves the way towards
identifying, tracking, and mitigating aporophobic views on social media at
scale.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [212] [Sparsity Outperforms Low-Rank Projections in Few-Shot Adaptation](https://arxiv.org/abs/2504.12436)
*Nairouz Mrabah,Nicolas Richet,Ismail Ben Ayed,Éric Granger*

Main category: cs.CV

TL;DR: 提出了一种稀疏优化（SO）框架，通过动态调整少量参数解决视觉语言模型（VLM）在小样本领域适应中的过拟合和计算限制问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如低秩重参数化）在泛化和超参数调优上表现不佳，需改进。

Method: 采用局部稀疏全局密度和局部随机全局重要性两种范式，动态调整参数以减少过拟合。

Result: 在11个数据集上验证，SO在小样本适应中表现最优，同时降低内存开销。

Conclusion: SO框架显著提升了小样本领域适应的性能，解决了过拟合和计算效率问题。

Abstract: Adapting Vision-Language Models (VLMs) to new domains with few labeled
samples remains a significant challenge due to severe overfitting and
computational constraints. State-of-the-art solutions, such as low-rank
reparameterization, mitigate these issues but often struggle with
generalization and require extensive hyperparameter tuning. In this paper, a
novel Sparse Optimization (SO) framework is proposed. Unlike low-rank
approaches that typically constrain updates to a fixed subspace, our SO method
leverages high sparsity to dynamically adjust very few parameters. We introduce
two key paradigms. First, we advocate for \textit{local sparsity and global
density}, which updates a minimal subset of parameters per iteration while
maintaining overall model expressiveness. As a second paradigm, we advocate for
\textit{local randomness and global importance}, which sparsifies the gradient
using random selection while pruning the first moment based on importance. This
combination significantly mitigates overfitting and ensures stable adaptation
in low-data regimes. Extensive experiments on 11 diverse datasets show that SO
achieves state-of-the-art few-shot adaptation performance while reducing memory
overhead.

</details>


### [213] [AdaVid: Adaptive Video-Language Pretraining](https://arxiv.org/abs/2504.12513)
*Chaitanya Patel,Juan Carlos Niebles,Ehsan Adeli*

Main category: cs.CV

TL;DR: AdaVid是一个灵活的视频编码器框架，通过动态调整计算资源适应边缘设备需求，性能优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有视频编码器计算需求高且仅支持短视频处理，难以在资源受限的边缘设备上部署。

Method: 采用自适应Transformer块和轻量级分层网络，动态调整隐藏嵌入维度并聚合短片段特征。

Result: AdaVid-EgoVLP在计算资源减半时性能与标准EgoVLP相当，资源相同时更优；支持更多帧数且不超计算限制。

Conclusion: AdaVid在计算效率和准确性上取得平衡，适用于长短视频处理。

Abstract: Contrastive video-language pretraining has demonstrated great success in
learning rich and robust video representations. However, deploying such video
encoders on compute-constrained edge devices remains challenging due to their
high computational demands. Additionally, existing models are typically trained
to process only short video clips, often limited to 4 to 64 frames. In this
paper, we introduce AdaVid, a flexible architectural framework designed to
learn efficient video encoders that can dynamically adapt their computational
footprint based on available resources. At the heart of AdaVid is an adaptive
transformer block, inspired by Matryoshka Representation Learning, which allows
the model to adjust its hidden embedding dimension at inference time. We show
that AdaVid-EgoVLP, trained on video-narration pairs from the large-scale Ego4D
dataset, matches the performance of the standard EgoVLP on short video-language
benchmarks using only half the compute, and even outperforms EgoVLP when given
equal computational resources. We further explore the trade-off between frame
count and compute on the challenging Diving48 classification benchmark, showing
that AdaVid enables the use of more frames without exceeding computational
limits. To handle longer videos, we also propose a lightweight hierarchical
network that aggregates short clip features, achieving a strong balance between
compute efficiency and accuracy across several long video benchmarks.

</details>


### [214] [Decision-based AI Visual Navigation for Cardiac Ultrasounds](https://arxiv.org/abs/2504.12535)
*Andy Dimnaku,Dominic Yurk,Zhiyuan Gao,Arun Padmanabhan,Mandar Aras,Yaser Abu-Mostafa*

Main category: cs.CV

TL;DR: 本文介绍了一种新型AI导航系统，通过决策模型识别心脏下腔静脉（IVC），帮助新手超声医师获取标准化心脏视图。


<details>
  <summary>Details</summary>
Motivation: 传统心脏超声检查需要专业医师和高品质设备，限制了其在医院外的应用。AI导航系统旨在降低操作门槛，扩展超声诊断的适用范围。

Method: 系统基于离线训练的决策模型，使用心脏超声视频进行二分类判断IVC是否存在，并结合定位算法实时标注IVC位置。

Result: 模型在高品质医院超声视频中表现优异，并在低成本Butterfly iQ设备上实现零样本性能。

Conclusion: 该系统有望推动超声诊断的普及，目前正在进行临床试验，并已在Butterfly iQ应用中上线。

Abstract: Ultrasound imaging of the heart (echocardiography) is widely used to diagnose
cardiac diseases. However, obtaining an echocardiogram requires an expert
sonographer and a high-quality ultrasound imaging device, which are generally
only available in hospitals. Recently, AI-based navigation models and
algorithms have been used to aid novice sonographers in acquiring the
standardized cardiac views necessary to visualize potential disease
pathologies. These navigation systems typically rely on directional guidance to
predict the necessary rotation of the ultrasound probe. This paper demonstrates
a novel AI navigation system that builds on a decision model for identifying
the inferior vena cava (IVC) of the heart. The decision model is trained
offline using cardiac ultrasound videos and employs binary classification to
determine whether the IVC is present in a given ultrasound video. The
underlying model integrates a novel localization algorithm that leverages the
learned feature representations to annotate the spatial location of the IVC in
real-time. Our model demonstrates strong localization performance on
traditional high-quality hospital ultrasound videos, as well as impressive
zero-shot performance on lower-quality ultrasound videos from a more affordable
Butterfly iQ handheld ultrasound machine. This capability facilitates the
expansion of ultrasound diagnostics beyond hospital settings. Currently, the
guidance system is undergoing clinical trials and is available on the Butterfly
iQ app.

</details>


### [215] [Privacy-Preserving Operating Room Workflow Analysis using Digital Twins](https://arxiv.org/abs/2504.12552)
*Alejandra Perez,Han Zhang,Yu-Chun Ku,Lalithkumar Seenivasan,Roger Soberanis,Jose L. Porras,Richard Day,Jeff Jopling,Peter Najjar,Mathias Unberath*

Main category: cs.CV

TL;DR: 提出了一种保护隐私的手术室视频分析方法，通过生成数字孪生（DT）实现事件检测，性能接近或优于原始视频方法。


<details>
  <summary>Details</summary>
Motivation: 优化手术室工作流程需识别瓶颈，但计算机视觉方法涉及隐私问题，需隐私保护方案。

Method: 两阶段流程：1) 利用视觉基础模型生成去标识化的数字孪生；2) 使用SafeOR模型处理分割掩膜和深度图进行事件检测。

Result: 在38次模拟手术试验中，数字孪生方法性能接近或优于原始视频模型。

Conclusion: 数字孪生支持隐私保护的工作流分析，促进跨机构数据共享，并可能提升模型泛化能力。

Abstract: Purpose: The operating room (OR) is a complex environment where optimizing
workflows is critical to reduce costs and improve patient outcomes. The use of
computer vision approaches for the automatic recognition of perioperative
events enables identification of bottlenecks for OR optimization. However,
privacy concerns limit the use of computer vision for automated event detection
from OR videos, which makes privacy-preserving approaches needed for OR
workflow analysis. Methods: We propose a two-stage pipeline for
privacy-preserving OR video analysis and event detection. In the first stage,
we leverage vision foundation models for depth estimation and semantic
segmentation to generate de-identified Digital Twins (DT) of the OR from
conventional RGB videos. In the second stage, we employ the SafeOR model, a
fused two-stream approach that processes segmentation masks and depth maps for
OR event detection. We evaluate this method on an internal dataset of 38
simulated surgical trials with five event classes. Results: Our results
indicate that this DT-based approach to the OR event detection model achieves
performance on par and sometimes even better than raw RGB video-based models on
detecting OR events. Conclusion: DTs enable privacy-preserving OR workflow
analysis, facilitating the sharing of de-identified data across institutions
and they can potentially enhance model generalizability by mitigating
domain-specific appearance differences.

</details>


### [216] [CM3AE: A Unified RGB Frame and Event-Voxel/-Frame Pre-training Framework](https://arxiv.org/abs/2504.12576)
*Wentao Wu,Xiao Wang,Chenglong Li,Bo Jiang,Jin Tang,Bin Luo,Qi Liu*

Main category: cs.CV

TL;DR: 论文提出了一种名为CM3AE的预训练框架，用于RGB-Event感知，通过多模态融合和对比学习提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在事件数据预训练中未能与RGB帧建立强关联，限制了多模态融合的应用。

Method: 设计了多模态融合重建模块和对比学习策略，输入包括RGB图像、事件图像和事件体素。

Result: 构建了大规模数据集并在五个下游任务中验证了CM3AE的有效性。

Conclusion: CM3AE为基于事件和RGB-Event融合的任务提供了强大支持，代码和模型将开源。

Abstract: Event cameras have attracted increasing attention in recent years due to
their advantages in high dynamic range, high temporal resolution, low power
consumption, and low latency. Some researchers have begun exploring
pre-training directly on event data. Nevertheless, these efforts often fail to
establish strong connections with RGB frames, limiting their applicability in
multi-modal fusion scenarios. To address these issues, we propose a novel CM3AE
pre-training framework for the RGB-Event perception. This framework accepts
multi-modalities/views of data as input, including RGB images, event images,
and event voxels, providing robust support for both event-based and RGB-event
fusion based downstream tasks. Specifically, we design a multi-modal fusion
reconstruction module that reconstructs the original image from fused
multi-modal features, explicitly enhancing the model's ability to aggregate
cross-modal complementary information. Additionally, we employ a multi-modal
contrastive learning strategy to align cross-modal feature representations in a
shared latent space, which effectively enhances the model's capability for
multi-modal understanding and capturing global dependencies. We construct a
large-scale dataset containing 2,535,759 RGB-Event data pairs for the
pre-training. Extensive experiments on five downstream tasks fully demonstrated
the effectiveness of CM3AE. Source code and pre-trained models will be released
on https://github.com/Event-AHU/CM3AE.

</details>


### [217] [Robo-SGG: Exploiting Layout-Oriented Normalization and Restitution for Robust Scene Graph Generation](https://arxiv.org/abs/2504.12606)
*Changsheng Lv,Mengshi Qi,Zijian Fu,Huadong Ma*

Main category: cs.CV

TL;DR: 提出了一种名为Robo-SGG的新方法，通过布局导向的归一化和恢复，提升场景图生成在损坏图像上的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有场景图生成方法在损坏图像上性能下降，原因是视觉特征受损。布局信息具有领域不变性，可用于增强鲁棒性。

Method: 使用实例归一化过滤领域特定特征，通过布局导向恢复恢复结构特征，并提出布局嵌入编码器增强对象和谓词编码。

Result: 在VG-C数据集上，PredCls、SGCls和SGDet任务的mR@50分别相对提升5.6%、8.0%和6.5%，并在VG-C和GQA-C基准上达到新SOTA。

Conclusion: Robo-SGG作为即插即用模块，可轻松集成到现有SGG模型中，显著提升鲁棒性。

Abstract: In this paper, we introduce a novel method named Robo-SGG, i.e.,
Layout-Oriented Normalization and Restitution for Robust Scene Graph
Generation. Compared to the existing SGG setting, the robust scene graph
generation aims to perform inference on a diverse range of corrupted images,
with the core challenge being the domain shift between the clean and corrupted
images. Existing SGG methods suffer from degraded performance due to
compromised visual features e.g., corruption interference or occlusions. To
obtain robust visual features, we exploit the layout information, which is
domain-invariant, to enhance the efficacy of existing SGG methods on corrupted
images. Specifically, we employ Instance Normalization(IN) to filter out the
domain-specific feature and recover the unchangeable structural features, i.e.,
the positional and semantic relationships among objects by the proposed
Layout-Oriented Restitution. Additionally, we propose a Layout-Embedded Encoder
(LEE) that augments the existing object and predicate encoders within the SGG
framework, enriching the robust positional and semantic features of objects and
predicates. Note that our proposed Robo-SGG module is designed as a
plug-and-play component, which can be easily integrated into any baseline SGG
model. Extensive experiments demonstrate that by integrating the
state-of-the-art method into our proposed Robo-SGG, we achieve relative
improvements of 5.6%, 8.0%, and 6.5% in mR@50 for PredCls, SGCls, and SGDet
tasks on the VG-C dataset, respectively, and achieve new state-of-the-art
performance in corruption scene graph generation benchmark (VG-C and GQA-C). We
will release our source code and model.

</details>


### [218] [Geographical Context Matters: Bridging Fine and Coarse Spatial Information to Enhance Continental Land Cover Mapping](https://arxiv.org/abs/2504.12368)
*Babak Ghassemi,Cassio Fraga-Dantas,Raffaele Gaetano,Dino Ienco,Omid Ghorbanzadeh,Emma Izquierdo-Verdiguier,Francesco Vuolo*

Main category: cs.CV

TL;DR: BRIDGE-LC是一种新型深度学习框架，通过整合多尺度地理空间信息提升土地覆盖分类的准确性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习方法在分析地球观测数据时忽略了地理空间元数据，限制了其在大规模应用中的表现。

Method: 提出BRIDGE-LC框架，结合细粒度（经纬度）和粗粒度（生物地理区域）空间信息，通过轻量级多层感知机架构实现高效分类。

Result: 实验表明，整合地理空间信息显著提升了土地覆盖分类性能，尤其是在联合利用细粒度和粗粒度信息时效果最佳。

Conclusion: BRIDGE-LC通过地理空间信息的整合，为大规模土地覆盖分类提供了高效且准确的解决方案。

Abstract: Land use and land cover mapping from Earth Observation (EO) data is a
critical tool for sustainable land and resource management. While advanced
machine learning and deep learning algorithms excel at analyzing EO imagery
data, they often overlook crucial geospatial metadata information that could
enhance scalability and accuracy across regional, continental, and global
scales. To address this limitation, we propose BRIDGE-LC (Bi-level
Representation Integration for Disentangled GEospatial Land Cover), a novel
deep learning framework that integrates multi-scale geospatial information into
the land cover classification process. By simultaneously leveraging
fine-grained (latitude/longitude) and coarse-grained (biogeographical region)
spatial information, our lightweight multi-layer perceptron architecture learns
from both during training but only requires fine-grained information for
inference, allowing it to disentangle region-specific from region-agnostic land
cover features while maintaining computational efficiency. To assess the
quality of our framework, we use an open-access in-situ dataset and adopt
several competing classification approaches commonly considered for large-scale
land cover mapping. We evaluated all approaches through two scenarios: an
extrapolation scenario in which training data encompasses samples from all
biogeographical regions, and a leave-one-region-out scenario where one region
is excluded from training. We also explore the spatial representation learned
by our model, highlighting a connection between its internal manifold and the
geographical information used during training. Our results demonstrate that
integrating geospatial information improves land cover mapping performance,
with the most substantial gains achieved by jointly leveraging both fine- and
coarse-grained spatial information.

</details>


### [219] [NTIRE 2025 Challenge on Day and Night Raindrop Removal for Dual-Focused Images: Methods and Results](https://arxiv.org/abs/2504.12711)
*Xin Li,Yeying Jin,Xin Jin,Zongwei Wu,Bingchen Li,Yufei Wang,Wenhan Yang,Yu Li,Zhibo Chen,Bihan Wen,Robby T. Tan,Radu Timofte,Qiyu Rong,Hongyuan Jing,Mengmeng Zhang,Jinglong Li,Xiangyu Lu,Yi Ren,Yuting Liu,Meng Zhang,Xiang Chen,Qiyuan Guan,Jiangxin Dong,Jinshan Pan,Conglin Gou,Qirui Yang,Fangpu Zhang,Yunlong Lin,Sixiang Chen,Guoxi Huang,Ruirui Lin,Yan Zhang,Jingyu Yang,Huanjing Yue,Jiyuan Chen,Qiaosi Yi,Hongjun Wang,Chenxi Xie,Shuai Li,Yuhui Wu,Kaiyi Ma,Jiakui Hu,Juncheng Li,Liwen Pan,Guangwei Gao,Wenjie Li,Zhenyu Jin,Heng Guo,Zhanyu Ma,Yubo Wang,Jinghua Wang,Wangzhi Xing,Anjusree Karnavar,Diqi Chen,Mohammad Aminul Islam,Hao Yang,Ruikun Zhang,Liyuan Pan,Qianhao Luo,XinCao,Han Zhou,Yan Min,Wei Dong,Jun Chen,Taoyi Wu,Weijia Dou,Yu Wang,Shengjie Zhao,Yongcheng Huang,Xingyu Han,Anyan Huang,Hongtao Wu,Hong Wang,Yefeng Zheng,Abhijeet Kumar,Aman Kumar,Marcos V. Conde,Paula Garrido,Daniel Feijoo,Juan C. Benito,Guanglu Dong,Xin Lin,Siyuan Liu,Tianheng Zheng,Jiayu Zhong,Shouyi Wang,Xiangtai Li,Lanqing Guo,Lu Qi,Chao Ren,Shuaibo Wang,Shilong Zhang,Wanyu Zhou,Yunze Wu,Qinzhong Tan,Jieyuan Pei,Zhuoxuan Li,Jiayu Wang,Haoyu Bian,Haoran Sun,Subhajit Paul,Ni Tang,Junhao Huang,Zihan Cheng,Hongyun Zhu,Yuehan Wu,Kaixin Deng,Hang Ouyang,Tianxin Xiao,Fan Yang,Zhizun Luo,Zeyu Xiao,Zhuoyuan Li,Nguyen Pham Hoang Le,An Dinh Thien,Son T. Luu,Kiet Van Nguyen,Ronghua Xu,Xianmin Tian,Weijian Zhou,Jiacheng Zhang,Yuqian Chen,Yihang Duan,Yujie Wu,Suresh Raikwar,Arsh Garg,Kritika,Jianhua Zheng,Xiaoshan Ma,Ruolin Zhao,Yongyu Yang,Yongsheng Liang,Guiming Huang,Qiang Li,Hongbin Zhang,Xiangyu Zheng,A. N. Rajagopalan*

Main category: cs.CV

TL;DR: NTIRE 2025挑战赛综述，聚焦昼夜雨滴去除任务，使用多样化的Raindrop Clarity数据集，吸引了361名参与者，32支团队提交了SOTA解决方案。


<details>
  <summary>Details</summary>
Motivation: 为雨滴去除任务建立新的基准，涵盖不同光照和聚焦条件下的挑战。

Method: 收集并构建Raindrop Clarity数据集，包含昼夜雨滴和背景聚焦的多样化退化类型，用于训练和评估。

Result: 32支团队提交的解决方案在数据集上达到了SOTA性能。

Conclusion: 挑战赛成功推动了雨滴去除技术的发展，并提供了强大的新基准。

Abstract: This paper reviews the NTIRE 2025 Challenge on Day and Night Raindrop Removal
for Dual-Focused Images. This challenge received a wide range of impressive
solutions, which are developed and evaluated using our collected real-world
Raindrop Clarity dataset. Unlike existing deraining datasets, our Raindrop
Clarity dataset is more diverse and challenging in degradation types and
contents, which includes day raindrop-focused, day background-focused, night
raindrop-focused, and night background-focused degradations. This dataset is
divided into three subsets for competition: 14,139 images for training, 240
images for validation, and 731 images for testing. The primary objective of
this challenge is to establish a new and powerful benchmark for the task of
removing raindrops under varying lighting and focus conditions. There are a
total of 361 participants in the competition, and 32 teams submitting valid
solutions and fact sheets for the final testing phase. These submissions
achieved state-of-the-art (SOTA) performance on the Raindrop Clarity dataset.
The project can be found at
https://lixinustc.github.io/CVPR-NTIRE2025-RainDrop-Competition.github.io/.

</details>


### [220] [Post-pre-training for Modality Alignment in Vision-Language Foundation Models](https://arxiv.org/abs/2504.12717)
*Shin'ya Yamaguchi,Dewei Feng,Sekitoshi Kanai,Kazuki Adachi,Daiki Chijiwa*

Main category: cs.CV

TL;DR: CLIP-Refine提出了一种后预训练方法，通过随机特征对齐和混合对比蒸馏技术，在小型数据集上1轮训练即可减少模态间隙，提升零样本性能。


<details>
  <summary>Details</summary>
Motivation: CLIP模型在多模态特征空间中存在模态间隙，影响下游任务性能，现有方法成本高或导致零样本性能下降。

Method: 采用随机特征对齐（RaFA）和混合对比蒸馏（HyCD）技术，结合先验分布和预训练模型知识。

Result: 实验表明，CLIP-Refine有效减少模态间隙并提升零样本性能。

Conclusion: CLIP-Refine是一种高效的后预训练方法，无需大量数据或牺牲零样本性能。

Abstract: Contrastive language image pre-training (CLIP) is an essential component of
building modern vision-language foundation models. While CLIP demonstrates
remarkable zero-shot performance on downstream tasks, the multi-modal feature
spaces still suffer from a modality gap, which is a gap between image and text
feature clusters and limits downstream task performance. Although existing
works attempt to address the modality gap by modifying pre-training or
fine-tuning, they struggle with heavy training costs with large datasets or
degradations of zero-shot performance. This paper presents CLIP-Refine, a
post-pre-training method for CLIP models at a phase between pre-training and
fine-tuning. CLIP-Refine aims to align the feature space with 1 epoch training
on small image-text datasets without zero-shot performance degradations. To
this end, we introduce two techniques: random feature alignment (RaFA) and
hybrid contrastive-distillation (HyCD). RaFA aligns the image and text features
to follow a shared prior distribution by minimizing the distance to random
reference vectors sampled from the prior. HyCD updates the model with hybrid
soft labels generated by combining ground-truth image-text pair labels and
outputs from the pre-trained CLIP model. This contributes to achieving both
maintaining the past knowledge and learning new knowledge to align features.
Our extensive experiments with multiple classification and retrieval tasks show
that CLIP-Refine succeeds in mitigating the modality gap and improving the
zero-shot performance.

</details>


### [221] [Parsimonious Dataset Construction for Laparoscopic Cholecystectomy Structure Segmentation](https://arxiv.org/abs/2504.12573)
*Yuning Zhou,Henry Badgery,Matthew Read,James Bailey,Catherine Davey*

Main category: cs.CV

TL;DR: 论文提出了一种基于主动学习的方法，用于从手术视频中选择关键帧构建高质量数据集，以降低标注成本并提升深度学习模型的性能。


<details>
  <summary>Details</summary>
Motivation: 医疗领域的数据标注成本高昂，阻碍了深度学习应用。本文旨在通过主动学习减少标注工作量，同时提升模型性能。

Method: 采用主动学习策略，通过深度神经网络（DNNs）从新收集的数据中选择信息量最大的帧进行标注，并评估了不同的数据信息量度量方法。

Result: 实验表明，仅使用主动学习选择的一半数据，DNNs的性能（0.4349 mIoU）接近使用全部数据训练的结果（0.4374 mIoU）。

Conclusion: 主动学习能有效减少标注成本并保持模型性能，深度特征距离是选择信息量最大数据的最佳方法。

Abstract: Labeling has always been expensive in the medical context, which has hindered
related deep learning application. Our work introduces active learning in
surgical video frame selection to construct a high-quality, affordable
Laparoscopic Cholecystectomy dataset for semantic segmentation. Active learning
allows the Deep Neural Networks (DNNs) learning pipeline to include the dataset
construction workflow, which means DNNs trained by existing dataset will
identify the most informative data from the newly collected data. At the same
time, DNNs' performance and generalization ability improve over time when the
newly selected and annotated data are included in the training data. We
assessed different data informativeness measurements and found the deep
features distances select the most informative data in this task. Our
experiments show that with half of the data selected by active learning, the
DNNs achieve almost the same performance with 0.4349 mean Intersection over
Union (mIoU) compared to the same DNNs trained on the full dataset (0.4374
mIoU) on the critical anatomies and surgical instruments.

</details>


### [222] [Set You Straight: Auto-Steering Denoising Trajectories to Sidestep Unwanted Concepts](https://arxiv.org/abs/2504.12782)
*Leyang Li,Shilin Lu,Yan Ren,Adams Wai-Kin Kong*

Main category: cs.CV

TL;DR: ANT是一种新的微调框架，通过自动引导去噪轨迹避免生成有害或不适当内容，解决了现有概念擦除方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 确保文本到图像模型的伦理部署需要有效技术防止生成有害内容，现有方法存在局限性。

Method: ANT通过反转分类器自由引导的条件方向，提出轨迹感知目标，保护早期阶段结构完整性，无需启发式锚概念选择。

Result: ANT在单概念和多概念擦除中均取得最先进结果，生成高质量、安全输出且不损害生成保真度。

Conclusion: ANT为文本到图像模型提供了一种高效、精确的概念擦除解决方案。

Abstract: Ensuring the ethical deployment of text-to-image models requires effective
techniques to prevent the generation of harmful or inappropriate content. While
concept erasure methods offer a promising solution, existing finetuning-based
approaches suffer from notable limitations. Anchor-free methods risk disrupting
sampling trajectories, leading to visual artifacts, while anchor-based methods
rely on the heuristic selection of anchor concepts. To overcome these
shortcomings, we introduce a finetuning framework, dubbed ANT, which
Automatically guides deNoising Trajectories to avoid unwanted concepts. ANT is
built on a key insight: reversing the condition direction of classifier-free
guidance during mid-to-late denoising stages enables precise content
modification without sacrificing early-stage structural integrity. This
inspires a trajectory-aware objective that preserves the integrity of the
early-stage score function field, which steers samples toward the natural image
manifold, without relying on heuristic anchor concept selection. For
single-concept erasure, we propose an augmentation-enhanced weight saliency map
to precisely identify the critical parameters that most significantly
contribute to the unwanted concept, enabling more thorough and efficient
erasure. For multi-concept erasure, our objective function offers a versatile
plug-and-play solution that significantly boosts performance. Extensive
experiments demonstrate that ANT achieves state-of-the-art results in both
single and multi-concept erasure, delivering high-quality, safe outputs without
compromising the generative fidelity. Code is available at
https://github.com/lileyang1210/ANT

</details>


### [223] [Hybrid Dense-UNet201 Optimization for Pap Smear Image Segmentation Using Spider Monkey Optimization](https://arxiv.org/abs/2504.12807)
*Ach Khozaimi,Isnani Darti,Syaiful Anam,Wuryansari Muharini Kusumawinahyu*

Main category: cs.CV

TL;DR: 提出了一种结合DenseNet201和U-Net的混合模型Dense-UNet201，并通过改进的蜘蛛猴优化算法（SMO）优化，显著提升了宫颈涂片图像分割的性能。


<details>
  <summary>Details</summary>
Motivation: 传统分割模型难以处理宫颈涂片图像中的复杂细胞结构和变化，因此需要更高效的分割方法。

Method: 采用DenseNet201作为U-Net的编码器，结合改进的SMO算法进行优化，使用SIPaKMeD数据集评估性能。

Result: Dense-UNet201在分割准确率（96.16%）、IoU（91.63%）和Dice系数（95.63%）上优于其他模型。

Conclusion: 该方法证明了预训练模型和元启发式优化在医学图像分析中的有效性，为宫颈细胞分割提供了新思路。

Abstract: Pap smear image segmentation is crucial for cervical cancer diagnosis.
However, traditional segmentation models often struggle with complex cellular
structures and variations in pap smear images. This study proposes a hybrid
Dense-UNet201 optimization approach that integrates a pretrained DenseNet201 as
the encoder for the U-Net architecture and optimizes it using the spider monkey
optimization (SMO) algorithm. The Dense-UNet201 model excelled at feature
extraction. The SMO was modified to handle categorical and discrete parameters.
The SIPaKMeD dataset was used in this study and evaluated using key performance
metrics, including loss, accuracy, Intersection over Union (IoU), and Dice
coefficient. The experimental results showed that Dense-UNet201 outperformed
U-Net, Res-UNet50, and Efficient-UNetB0. SMO Dense-UNet201 achieved a
segmentation accuracy of 96.16%, an IoU of 91.63%, and a Dice coefficient score
of 95.63%. These findings underscore the effectiveness of image preprocessing,
pretrained models, and metaheuristic optimization in improving medical image
analysis and provide new insights into cervical cell segmentation methods.

</details>


### [224] [Image-Editing Specialists: An RLAIF Approach for Diffusion Models](https://arxiv.org/abs/2504.12833)
*Elior Benarous,Yilun Du,Heng Yang*

Main category: cs.CV

TL;DR: 提出一种基于强化学习的扩散模型训练方法，用于图像编辑，无需大量人工标注或数据集，显著提升编辑的逼真度和语义对齐。


<details>
  <summary>Details</summary>
Motivation: 解决图像编辑中结构保留和语义对齐的挑战，减少用户编辑的复杂性。

Method: 采用在线强化学习框架，结合视觉提示，仅需少量参考图像和训练步骤。

Result: 模型在复杂场景中实现精确编辑，保持高保真度，并提升视觉逼真度。

Conclusion: 该方法在图像编辑和机器人仿真中展现出高效性和多功能性。

Abstract: We present a novel approach to training specialized instruction-based
image-editing diffusion models, addressing key challenges in structural
preservation with input images and semantic alignment with user prompts. We
introduce an online reinforcement learning framework that aligns the diffusion
model with human preferences without relying on extensive human annotations or
curating a large dataset. Our method significantly improves the realism and
alignment with instructions in two ways. First, the proposed models achieve
precise and structurally coherent modifications in complex scenes while
maintaining high fidelity in instruction-irrelevant areas. Second, they capture
fine nuances in the desired edit by leveraging a visual prompt, enabling
detailed control over visual edits without lengthy textual prompts. This
approach simplifies users' efforts to achieve highly specific edits, requiring
only 5 reference images depicting a certain concept for training. Experimental
results demonstrate that our models can perform intricate edits in complex
scenes, after just 10 training steps. Finally, we showcase the versatility of
our method by applying it to robotics, where enhancing the visual realism of
simulated environments through targeted sim-to-real image edits improves their
utility as proxies for real-world settings.

</details>


### [225] [Probing and Inducing Combinational Creativity in Vision-Language Models](https://arxiv.org/abs/2504.13120)
*Yongqian Peng,Yuxi Ma,Mengmeng Wang,Yuxuan Wang,Yizhou Wang,Chi Zhang,Yixin Zhu,Zilong Zheng*

Main category: cs.CV

TL;DR: 论文探讨了视觉语言模型（VLMs）的组合创造力，提出了IEI框架，并通过实验验证其在理解和生成任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 研究VLMs是否具备组合创造力，而非仅仅是训练数据的模式匹配。

Method: 提出IEI框架，分解创意过程为识别、解释和推导三阶段，并使用CreativeMashup数据集验证。

Result: VLMs在理解任务中超越普通人但不及专家；生成任务中IEI框架显著提升创意质量。

Conclusion: 为评估人工创造力提供理论基础，并为提升VLMs创意生成提供实用指南。

Abstract: The ability to combine existing concepts into novel ideas stands as a
fundamental hallmark of human intelligence. Recent advances in Vision-Language
Models (VLMs) like GPT-4V and DALLE-3 have sparked debate about whether their
outputs reflect combinational creativity--defined by M. A. Boden (1998) as
synthesizing novel ideas through combining existing concepts--or sophisticated
pattern matching of training data. Drawing inspiration from cognitive science,
we investigate the combinational creativity of VLMs from the lens of concept
blending. We propose the Identification-Explanation-Implication (IEI)
framework, which decomposes creative processes into three levels: identifying
input spaces, extracting shared attributes, and deriving novel semantic
implications. To validate this framework, we curate CreativeMashup, a
high-quality dataset of 666 artist-generated visual mashups annotated according
to the IEI framework. Through extensive experiments, we demonstrate that in
comprehension tasks, best VLMs have surpassed average human performance while
falling short of expert-level understanding; in generation tasks, incorporating
our IEI framework into the generation pipeline significantly enhances the
creative quality of VLMs outputs. Our findings establish both a theoretical
foundation for evaluating artificial creativity and practical guidelines for
improving creative generation in VLMs.

</details>


### [226] [Disentangling Polysemantic Channels in Convolutional Neural Networks](https://arxiv.org/abs/2504.12939)
*Robin Hesse,Jonas Fischer,Simone Schaub-Meyer,Stefan Roth*

Main category: cs.CV

TL;DR: 提出一种算法，将多义通道解耦为多个单义通道，提升CNN的可解释性。


<details>
  <summary>Details</summary>
Motivation: CNN中的多义通道难以解释，阻碍了对网络决策机制的理解。

Method: 通过分析前一层的不同激活模式，解耦多义通道为单义通道。

Result: 提升了CNN的可解释性，改进了特征可视化等技术。

Conclusion: 该方法有效解决了多义通道问题，增强了CNN的机制解释能力。

Abstract: Mechanistic interpretability is concerned with analyzing individual
components in a (convolutional) neural network (CNN) and how they form larger
circuits representing decision mechanisms. These investigations are challenging
since CNNs frequently learn polysemantic channels that encode distinct
concepts, making them hard to interpret. To address this, we propose an
algorithm to disentangle a specific kind of polysemantic channel into multiple
channels, each responding to a single concept. Our approach restructures
weights in a CNN, utilizing that different concepts within the same channel
exhibit distinct activation patterns in the previous layer. By disentangling
these polysemantic features, we enhance the interpretability of CNNs,
ultimately improving explanatory techniques such as feature visualizations.

</details>


### [227] [Vision and Language Integration for Domain Generalization](https://arxiv.org/abs/2504.12966)
*Yanmei Wang,Xiyao Liu,Fupeng Chu,Zhi Han*

Main category: cs.CV

TL;DR: 论文提出VLCA方法，通过结合语言空间和视觉空间，利用语义空间作为桥梁，解决领域泛化中图像特征空间难以统一的问题。


<details>
  <summary>Details</summary>
Motivation: 由于领域差异，图像特征空间难以找到可靠的共性，而语言具有更完整的表达元素。

Method: 在语言空间利用词向量距离捕捉语义关系，在视觉空间通过低秩近似探索样本共性，最后在多模态空间对齐语言和视觉表示。

Result: 实验证明了方法的有效性。

Conclusion: VLCA通过结合语言和视觉空间，成功提升了领域泛化的性能。

Abstract: Domain generalization aims at training on source domains to uncover a
domain-invariant feature space, allowing the model to perform robust
generalization ability on unknown target domains. However, due to domain gaps,
it is hard to find reliable common image feature space, and the reason for that
is the lack of suitable basic units for images. Different from image in vision
space, language has comprehensive expression elements that can effectively
convey semantics. Inspired by the semantic completeness of language and
intuitiveness of image, we propose VLCA, which combine language space and
vision space, and connect the multiple image domains by using semantic space as
the bridge domain. Specifically, in language space, by taking advantage of the
completeness of language basic units, we tend to capture the semantic
representation of the relations between categories through word vector
distance. Then, in vision space, by taking advantage of the intuitiveness of
image features, the common pattern of sample features with the same class is
explored through low-rank approximation. In the end, the language
representation is aligned with the vision representation through the multimodal
space of text and image. Experiments demonstrate the effectiveness of the
proposed method.

</details>


### [228] [ArtistAuditor: Auditing Artist Style Pirate in Text-to-Image Generation Models](https://arxiv.org/abs/2504.13061)
*Linkang Du,Zheng Zhu,Min Chen,Zhou Su,Shouling Ji,Peng Cheng,Jiming Chen,Zhikun Zhang*

Main category: cs.CV

TL;DR: ArtistAuditor是一种用于文本到图像生成模型的数据使用审计方法，通过分析风格特征判断模型是否使用了特定艺术家的作品进行微调。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法（如扰动或水印）在艺术品或模型已发布后不可行的问题，保护艺术家版权。

Method: 使用风格提取器获取多粒度风格表示，并通过训练判别器进行审计决策。

Result: 在六种模型和数据集组合上，AUC值>0.937，验证了方法的有效性。

Conclusion: ArtistAuditor在真实场景中表现优异，已开源并可用于实际应用。

Abstract: Text-to-image models based on diffusion processes, such as DALL-E, Stable
Diffusion, and Midjourney, are capable of transforming texts into detailed
images and have widespread applications in art and design. As such, amateur
users can easily imitate professional-level paintings by collecting an artist's
work and fine-tuning the model, leading to concerns about artworks' copyright
infringement. To tackle these issues, previous studies either add visually
imperceptible perturbation to the artwork to change its underlying styles
(perturbation-based methods) or embed post-training detectable watermarks in
the artwork (watermark-based methods). However, when the artwork or the model
has been published online, i.e., modification to the original artwork or model
retraining is not feasible, these strategies might not be viable.
  To this end, we propose a novel method for data-use auditing in the
text-to-image generation model. The general idea of ArtistAuditor is to
identify if a suspicious model has been finetuned using the artworks of
specific artists by analyzing the features related to the style. Concretely,
ArtistAuditor employs a style extractor to obtain the multi-granularity style
representations and treats artworks as samplings of an artist's style. Then,
ArtistAuditor queries a trained discriminator to gain the auditing decisions.
The experimental results on six combinations of models and datasets show that
ArtistAuditor can achieve high AUC values (> 0.937). By studying
ArtistAuditor's transferability and core modules, we provide valuable insights
into the practical implementation. Finally, we demonstrate the effectiveness of
ArtistAuditor in real-world cases by an online platform Scenario. ArtistAuditor
is open-sourced at https://github.com/Jozenn/ArtistAuditor.

</details>


### [229] [VistaDPO: Video Hierarchical Spatial-Temporal Direct Preference Optimization for Large Video Models](https://arxiv.org/abs/2504.13122)
*Haojian Huang,Haodong Chen,Shengqiong Wu,Meng Luo,Jinlan Fu,Xinya Du,Hanwang Zhang,Hao Fei*

Main category: cs.CV

TL;DR: VistaDPO是一个新颖的视频层次空间-时间直接偏好优化框架，通过三个层次（实例、时间和感知）对齐文本-视频偏好，显著改善大型视频模型的性能。


<details>
  <summary>Details</summary>
Motivation: 解决大型视频模型在视频理解中与人类直觉不一致及视频幻觉问题。

Method: 提出VistaDPO框架，构建VistaDPO-7k数据集，通过三个层次对齐文本-视频偏好。

Result: 在视频幻觉、视频问答和字幕任务中显著提升性能。

Conclusion: VistaDPO有效缓解视频-语言不对齐和幻觉问题，提升模型表现。

Abstract: Large Video Models (LVMs) built upon Large Language Models (LLMs) have shown
promise in video understanding but often suffer from misalignment with human
intuition and video hallucination issues. To address these challenges, we
introduce VistaDPO, a novel framework for Video Hierarchical Spatial-Temporal
Direct Preference Optimization. VistaDPO enhances text-video preference
alignment across three hierarchical levels: i) Instance Level, aligning overall
video content with responses; ii) Temporal Level, aligning video temporal
semantics with event descriptions; and iii) Perceptive Level, aligning spatial
objects with language tokens. Given the lack of datasets for fine-grained
video-language preference alignment, we construct VistaDPO-7k, a dataset of
7.2K QA pairs annotated with chosen and rejected responses, along with
spatial-temporal grounding information such as timestamps, keyframes, and
bounding boxes. Extensive experiments on benchmarks such as Video
Hallucination, Video QA, and Captioning performance tasks demonstrate that
VistaDPO significantly improves the performance of existing LVMs, effectively
mitigating video-language misalignment and hallucination. The code and data are
available at https://github.com/HaroldChen19/VistaDPO.

</details>


### [230] [Pose and Facial Expression Transfer by using StyleGAN](https://arxiv.org/abs/2504.13021)
*Petr Jahoda,Jan Cech*

Main category: cs.CV

TL;DR: 提出了一种将姿态和表情从源人脸图像转移到目标人脸图像的方法，无需人工标注，利用StyleGAN2生成逼真结果。


<details>
  <summary>Details</summary>
Motivation: 实现人脸图像中姿态和表情的自动转移，避免人工标注，提高效率。

Method: 使用两个编码器和一个映射网络，将输入投影到StyleGAN2的潜在空间，生成输出图像。训练基于多人的视频序列，自监督学习。

Result: 模型能够合成随机身份的可控姿态和表情，接近实时性能。

Conclusion: 该方法高效且无需人工标注，适用于人脸姿态和表情的自动转移。

Abstract: We propose a method to transfer pose and expression between face images.
Given a source and target face portrait, the model produces an output image in
which the pose and expression of the source face image are transferred onto the
target identity. The architecture consists of two encoders and a mapping
network that projects the two inputs into the latent space of StyleGAN2, which
finally generates the output. The training is self-supervised from video
sequences of many individuals. Manual labeling is not required. Our model
enables the synthesis of random identities with controllable pose and
expression. Close-to-real-time performance is achieved.

</details>


### [231] [Science-T2I: Addressing Scientific Illusions in Image Synthesis](https://arxiv.org/abs/2504.13129)
*Jialuo Li,Wenhao Chai,Xingyu Fu,Haiyang Xu,Saining Xie*

Main category: cs.CV

TL;DR: 提出了一种将科学知识融入生成模型的新方法，通过Science-T2I数据集和SciScore奖励模型提升图像合成的真实性和一致性。


<details>
  <summary>Details</summary>
Motivation: 增强生成模型在科学知识方面的真实性和一致性，填补现有模型在科学内容生成中的不足。

Method: 1. 构建Science-T2I数据集；2. 开发SciScore奖励模型；3. 提出两阶段训练框架（监督微调和掩码在线微调）。

Result: SciScore达到接近人类水平的性能（提升5%），应用于FLUX模型时性能提升超过50%。

Conclusion: 该方法为评估生成内容的科学真实性设立了新标准，显著提升了生成模型的性能。

Abstract: We present a novel approach to integrating scientific knowledge into
generative models, enhancing their realism and consistency in image synthesis.
First, we introduce Science-T2I, an expert-annotated adversarial dataset
comprising adversarial 20k image pairs with 9k prompts, covering wide distinct
scientific knowledge categories. Leveraging Science-T2I, we present SciScore,
an end-to-end reward model that refines the assessment of generated images
based on scientific knowledge, which is achieved by augmenting both the
scientific comprehension and visual capabilities of pre-trained CLIP model.
Additionally, based on SciScore, we propose a two-stage training framework,
comprising a supervised fine-tuning phase and a masked online fine-tuning
phase, to incorporate scientific knowledge into existing generative models.
Through comprehensive experiments, we demonstrate the effectiveness of our
framework in establishing new standards for evaluating the scientific realism
of generated content. Specifically, SciScore attains performance comparable to
human-level, demonstrating a 5% improvement similar to evaluations conducted by
experienced human evaluators. Furthermore, by applying our proposed fine-tuning
method to FLUX, we achieve a performance enhancement exceeding 50% on SciScore.

</details>


### [232] [Prototypes are Balanced Units for Efficient and Effective Partially Relevant Video Retrieval](https://arxiv.org/abs/2504.13035)
*WonJun Moon,Cheol-Ho Cho,Woojin Jun,Minho Shim,Taeoh Kim,Inwoong Lee,Dongyoon Wee,Jae-Pil Heo*

Main category: cs.CV

TL;DR: 提出了一种原型PRVR框架，通过固定数量的原型编码视频的多样化上下文，同时引入策略增强文本关联和视频理解，并通过跨模态和单模态重建任务保持原型可搜索性。


<details>
  <summary>Details</summary>
Motivation: 在部分相关视频检索（PRVR）中，同时实现搜索准确性和效率具有挑战性，因为多样化的上下文表示会增加计算和内存成本。

Method: 提出原型PRVR框架，将视频的多样化上下文编码为固定数量的原型，并引入文本关联增强策略、视频理解策略及正交目标。通过跨模态和单模态重建任务保持原型可搜索性。

Result: 在TVR、ActivityNet-Captions和QVHighlights数据集上的广泛评估验证了方法的有效性，且未牺牲效率。

Conclusion: 该方法成功解决了PRVR中准确性与效率的权衡问题，通过原型编码和重建任务实现了高效且准确的视频检索。

Abstract: In a retrieval system, simultaneously achieving search accuracy and
efficiency is inherently challenging. This challenge is particularly pronounced
in partially relevant video retrieval (PRVR), where incorporating more diverse
context representations at varying temporal scales for each video enhances
accuracy but increases computational and memory costs. To address this
dichotomy, we propose a prototypical PRVR framework that encodes diverse
contexts within a video into a fixed number of prototypes. We then introduce
several strategies to enhance text association and video understanding within
the prototypes, along with an orthogonal objective to ensure that the
prototypes capture a diverse range of content. To keep the prototypes
searchable via text queries while accurately encoding video contexts, we
implement cross- and uni-modal reconstruction tasks. The cross-modal
reconstruction task aligns the prototypes with textual features within a shared
space, while the uni-modal reconstruction task preserves all video contexts
during encoding. Additionally, we employ a video mixing technique to provide
weak guidance to further align prototypes and associated textual
representations. Extensive evaluations on TVR, ActivityNet-Captions, and
QVHighlights validate the effectiveness of our approach without sacrificing
efficiency.

</details>


### [233] [Event-Enhanced Blurry Video Super-Resolution](https://arxiv.org/abs/2504.13042)
*Dachun Kai,Yueyi Zhang,Jin Wang,Zeyu Xiao,Zhiwei Xiong,Xiaoyan Sun*

Main category: cs.CV

TL;DR: 提出了一种基于事件信号的模糊视频超分辨率方法Ev-DeblurVSR，通过融合帧和事件信息提升细节恢复和时间一致性。


<details>
  <summary>Details</summary>
Motivation: 现有模糊视频超分辨率方法因缺乏运动信息和高频细节，导致恢复效果不佳。

Method: 引入事件信号，提出互惠特征去模糊模块和混合可变形对齐模块。

Result: 在合成和真实数据集上表现最佳，准确率提升2.59 dB，速度提升7.28倍。

Conclusion: Ev-DeblurVSR显著提升了模糊视频超分辨率的性能。

Abstract: In this paper, we tackle the task of blurry video super-resolution (BVSR),
aiming to generate high-resolution (HR) videos from low-resolution (LR) and
blurry inputs. Current BVSR methods often fail to restore sharp details at high
resolutions, resulting in noticeable artifacts and jitter due to insufficient
motion information for deconvolution and the lack of high-frequency details in
LR frames. To address these challenges, we introduce event signals into BVSR
and propose a novel event-enhanced network, Ev-DeblurVSR. To effectively fuse
information from frames and events for feature deblurring, we introduce a
reciprocal feature deblurring module that leverages motion information from
intra-frame events to deblur frame features while reciprocally using global
scene context from the frames to enhance event features. Furthermore, to
enhance temporal consistency, we propose a hybrid deformable alignment module
that fully exploits the complementary motion information from inter-frame
events and optical flow to improve motion estimation in the deformable
alignment process. Extensive evaluations demonstrate that Ev-DeblurVSR
establishes a new state-of-the-art performance on both synthetic and real-world
datasets. Notably, on real data, our method is +2.59 dB more accurate and
7.28$\times$ faster than the recent best BVSR baseline FMA-Net. Code:
https://github.com/DachunKai/Ev-DeblurVSR.

</details>


### [234] [PerceptionLM: Open-Access Data and Models for Detailed Visual Understanding](https://arxiv.org/abs/2504.13180)
*Jang Hyun Cho,Andrea Madotto,Effrosyni Mavroudi,Triantafyllos Afouras,Tushar Nagarajan,Muhammad Maaz,Yale Song,Tengyu Ma,Shuming Hu,Suyog Jain,Miguel Martin,Huiyu Wang,Hanoona Rasheed,Peize Sun,Po-Yao Huang,Daniel Bolya,Nikhila Ravi,Shashank Jain,Tammy Stark,Shane Moon,Babak Damavandi,Vivian Lee,Andrew Westbury,Salman Khan,Philipp Krähenbühl,Piotr Dollár,Lorenzo Torresani,Kristen Grauman,Christoph Feichtenhofer*

Main category: cs.CV

TL;DR: 论文提出了一种完全开放和可复现的感知语言模型（PLM）框架，避免依赖闭源模型，并通过大规模合成数据和人类标注数据填补视频理解的数据空白。


<details>
  <summary>Details</summary>
Motivation: 当前高性能视觉语言模型多为闭源，阻碍了科学进步。研究旨在通过透明框架推动图像和视频理解的研究。

Method: 分析标准训练流程，避免从专有模型蒸馏，利用大规模合成数据和2.8M人类标注视频数据。

Result: 发布了细粒度视频问答对和时空标注视频描述，并提出了PLM-VideoBench评估套件。

Conclusion: 通过提供数据、训练方案、代码和模型，实现了完全可复现的研究。

Abstract: Vision-language models are integral to computer vision research, yet many
high-performing models remain closed-source, obscuring their data, design and
training recipe. The research community has responded by using distillation
from black-box models to label training data, achieving strong benchmark
results, at the cost of measurable scientific progress. However, without
knowing the details of the teacher model and its data sources, scientific
progress remains difficult to measure. In this paper, we study building a
Perception Language Model (PLM) in a fully open and reproducible framework for
transparent research in image and video understanding. We analyze standard
training pipelines without distillation from proprietary models and explore
large-scale synthetic data to identify critical data gaps, particularly in
detailed video understanding. To bridge these gaps, we release 2.8M
human-labeled instances of fine-grained video question-answer pairs and
spatio-temporally grounded video captions. Additionally, we introduce
PLM-VideoBench, a suite for evaluating challenging video understanding tasks
focusing on the ability to reason about "what", "where", "when", and "how" of a
video. We make our work fully reproducible by providing data, training recipes,
code & models.

</details>


### [235] [Enhancing Person-to-Person Virtual Try-On with Multi-Garment Virtual Try-Off](https://arxiv.org/abs/2504.13078)
*Riza Velioglu,Petra Bevandic,Robin Chan,Barbara Hammer*

Main category: cs.CV

TL;DR: 论文介绍了TryOffDiff，一种基于扩散模型的VTOFF方法，用于从穿着者提取标准化服装图像，并在多服装类别中表现优异。


<details>
  <summary>Details</summary>
Motivation: 虚拟试穿（VTON）和虚拟试脱（VTOFF）是计算机视觉在时尚领域的重要应用。VTOFF的挑战在于从穿着者提取标准化服装图像，而TryOffDiff旨在解决这一问题。

Method: TryOffDiff基于潜在扩散框架，结合SigLIP图像条件，有效捕捉服装的纹理、形状和图案。它还引入类特定嵌入，实现多服装VTOFF。

Result: TryOffDiff在VITON-HD数据集上达到最先进水平，在DressCode数据集上表现优异。与VTON模型结合时，减少了不必要属性转移（如肤色）。

Conclusion: TryOffDiff是首个多服装VTOFF模型，为虚拟试穿和试脱提供了高效解决方案，代码已开源。

Abstract: Computer vision is transforming fashion through Virtual Try-On (VTON) and
Virtual Try-Off (VTOFF). VTON generates images of a person in a specified
garment using a target photo and a standardized garment image, while a more
challenging variant, Person-to-Person Virtual Try-On (p2p-VTON), uses a photo
of another person wearing the garment. VTOFF, on the other hand, extracts
standardized garment images from clothed individuals. We introduce TryOffDiff,
a diffusion-based VTOFF model. Built on a latent diffusion framework with
SigLIP image conditioning, it effectively captures garment properties like
texture, shape, and patterns. TryOffDiff achieves state-of-the-art results on
VITON-HD and strong performance on DressCode dataset, covering upper-body,
lower-body, and dresses. Enhanced with class-specific embeddings, it pioneers
multi-garment VTOFF, the first of its kind. When paired with VTON models, it
improves p2p-VTON by minimizing unwanted attribute transfer, such as skin
color. Code is available at: https://rizavelioglu.github.io/tryoffdiff/

</details>


### [236] [Low-hallucination Synthetic Captions for Large-Scale Vision-Language Model Pre-training](https://arxiv.org/abs/2504.13123)
*Xinsong Zhang,Yarong Zeng,Xinting Huang,Hu Hu,Runquan Xie,Han Hu,Zhanhui Kang*

Main category: cs.CV

TL;DR: 论文提出了一种可扩展的合成字幕生成技术，用于视觉语言模型预训练，证明大规模低幻觉合成字幕可替代真实数据，并在实验中显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型预训练依赖高质量图像-文本对，但这类数据稀缺且饱和，限制了领域发展。研究旨在解决这一问题。

Method: 提出生成高质量、低幻觉合成字幕的新流程，采用连续DPO方法减少幻觉，并验证合成字幕在预训练中的优势。

Result: 合成字幕显著提升模型性能，非幻觉字幕率从48.2%提升至77.9%，在35项任务中性能提升至少6.2%，FID分数显著降低。

Conclusion: 合成字幕是预训练的有效替代方案，Hunyuan-Recap100M数据集的发布将支持进一步研究。

Abstract: In recent years, the field of vision-language model pre-training has
experienced rapid advancements, driven primarily by the continuous enhancement
of textual capabilities in large language models. However, existing training
paradigms for multimodal large language models heavily rely on high-quality
image-text pairs. As models and data scales grow exponentially, the
availability of such meticulously curated data has become increasingly scarce
and saturated, thereby severely limiting further advancements in this domain.
This study investigates scalable caption generation techniques for
vision-language model pre-training and demonstrates that large-scale
low-hallucination synthetic captions can serve dual purposes: 1) acting as a
viable alternative to real-world data for pre-training paradigms and 2)
achieving superior performance enhancement when integrated into vision-language
models through empirical validation. This paper presents three key
contributions: 1) a novel pipeline for generating high-quality,
low-hallucination, and knowledge-rich synthetic captions. Our continuous DPO
methodology yields remarkable results in reducing hallucinations. Specifically,
the non-hallucination caption rate on a held-out test set increases from 48.2%
to 77.9% for a 7B-size model. 2) Comprehensive empirical validation reveals
that our synthetic captions confer superior pre-training advantages over their
counterparts. Across 35 vision language tasks, the model trained with our data
achieves a significant performance gain of at least 6.2% compared to alt-text
pairs and other previous work. Meanwhile, it also offers considerable support
in the text-to-image domain. With our dataset, the FID score is reduced by 17.1
on a real-world validation benchmark and 13.3 on the MSCOCO validation
benchmark. 3) We will release Hunyuan-Recap100M, a low-hallucination and
knowledge-intensive synthetic caption dataset.

</details>


### [237] [$\texttt{Complex-Edit}$: CoT-Like Instruction Generation for Complexity-Controllable Image Editing Benchmark](https://arxiv.org/abs/2504.13143)
*Siwei Yang,Mude Hui,Bingchen Zhao,Yuyin Zhou,Nataniel Ruiz,Cihang Xie*

Main category: cs.CV

TL;DR: 论文介绍了Complex-Edit基准，用于评估基于指令的图像编辑模型在不同复杂度指令下的表现，揭示了开源模型与闭源模型的性能差距，并提出了改进策略。


<details>
  <summary>Details</summary>
Motivation: 现有图像编辑模型在复杂指令下的表现缺乏系统评估，因此需要开发一个全面的基准来填补这一空白。

Method: 利用GPT-4o自动收集多样化编辑指令，采用“Chain-of-Edit”流程生成复杂指令，并设计了一套评估指标和自动评估流程。

Result: 开源模型表现显著落后于闭源模型，复杂指令下模型保留关键元素和美学质量的能力下降，分解指令会降低性能，但Best-of-N策略可改善结果。

Conclusion: Complex-Edit基准为评估图像编辑模型提供了系统工具，揭示了性能差距和改进方向，同时指出了合成数据训练的潜在问题。

Abstract: We introduce $\texttt{Complex-Edit}$, a comprehensive benchmark designed to
systematically evaluate instruction-based image editing models across
instructions of varying complexity. To develop this benchmark, we harness
GPT-4o to automatically collect a diverse set of editing instructions at scale.
Our approach follows a well-structured ``Chain-of-Edit'' pipeline: we first
generate individual atomic editing tasks independently and then integrate them
to form cohesive, complex instructions. Additionally, we introduce a suite of
metrics to assess various aspects of editing performance, along with a
VLM-based auto-evaluation pipeline that supports large-scale assessments. Our
benchmark yields several notable insights: 1) Open-source models significantly
underperform relative to proprietary, closed-source models, with the
performance gap widening as instruction complexity increases; 2) Increased
instructional complexity primarily impairs the models' ability to retain key
elements from the input images and to preserve the overall aesthetic quality;
3) Decomposing a complex instruction into a sequence of atomic steps, executed
in a step-by-step manner, substantially degrades performance across multiple
metrics; 4) A straightforward Best-of-N selection strategy improves results for
both direct editing and the step-by-step sequential approach; and 5) We observe
a ``curse of synthetic data'': when synthetic data is involved in model
training, the edited images from such models tend to appear increasingly
synthetic as the complexity of the editing instructions rises -- a phenomenon
that intriguingly also manifests in the latest GPT-4o outputs.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [238] [Predictive control of blast furnace temperature in steelmaking with hybrid depth-infused quantum neural networks](https://arxiv.org/abs/2504.12389)
*Nayoung Lee,Minsoo Shin,Asel Sagingalieva,Ayush Joshi Tripathi,Karan Pinto,Alexey Melnikov*

Main category: quant-ph

TL;DR: 本文提出了一种结合混合量子机器学习与喷煤控制的新方法，显著提高了高炉温度预测的准确性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 高炉温度的准确预测和稳定控制对钢铁生产效率至关重要，传统方法难以应对其复杂非线性特性。

Method: 结合经典机器学习与量子计算算法，采用预测优化方法，利用量子增强特征空间探索和经典回归模型。

Result: 预测准确性提高超过25%，温度稳定性从±50度提升至±7.6度。

Conclusion: 混合量子机器学习模型在工业钢铁生产中具有显著潜力。

Abstract: Accurate prediction and stabilization of blast furnace temperatures are
crucial for optimizing the efficiency and productivity of steel production.
Traditional methods often struggle with the complex and non-linear nature of
the temperature fluctuations within blast furnaces. This paper proposes a novel
approach that combines hybrid quantum machine learning with pulverized coal
injection control to address these challenges. By integrating classical machine
learning techniques with quantum computing algorithms, we aim to enhance
predictive accuracy and achieve more stable temperature control. For this we
utilized a unique prediction-based optimization method. Our method leverages
quantum-enhanced feature space exploration and the robustness of classical
regression models to forecast temperature variations and optimize pulverized
coal injection values. Our results demonstrate a significant improvement in
prediction accuracy over 25 percent and our solution improved temperature
stability to +-7.6 degrees of target range from the earlier variance of +-50
degrees, highlighting the potential of hybrid quantum machine learning models
in industrial steel production applications.

</details>


### [239] [Featuremetric benchmarking: Quantum computer benchmarks based on circuit features](https://arxiv.org/abs/2504.12575)
*Timothy Proctor,Anh Tran,Xingxin Liu,Aditya Dhumuntarao,Stefan Seritan,Alaina Green,Norbert M Linke*

Main category: quant-ph

TL;DR: 提出了一种基于量子电路特征的基准测试框架，用于更全面评估多量子位计算机的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试方法（如体积基准测试）对量子计算机性能的评估不够全面，需要更丰富的模型。

Method: 通过量化量子计算机在不同电路特征（如深度、宽度、双量子门密度等）下的表现，构建特征度量基准测试框架。

Result: 在IBM Q和IonQ系统上进行了27量子位的测试，展示了高斯过程回归在性能分析中的应用。

Conclusion: 特征度量基准测试框架能更全面、准确地评估量子计算机性能，且数据分析方法适用于体积基准测试。

Abstract: Benchmarks that concisely summarize the performance of many-qubit quantum
computers are essential for measuring progress towards the goal of useful
quantum computation. In this work, we present a benchmarking framework that is
based on quantifying how a quantum computer's performance on quantum circuits
varies as a function of features of those circuits, such as circuit depth,
width, two-qubit gate density, problem input size, or algorithmic depth. Our
featuremetric benchmarking framework generalizes volumetric benchmarking -- a
widely-used methodology that quantifies performance versus circuit width and
depth -- and we show that it enables richer and more faithful models of quantum
computer performance. We demonstrate featuremetric benchmarking with example
benchmarks run on IBM Q and IonQ systems of up to 27 qubits, and we show how to
produce performance summaries from the data using Gaussian process regression.
Our data analysis methods are also of interest in the special case of
volumetric benchmarking, as they enable the creation of intuitive
two-dimensional capability regions using data from few circuits.

</details>


### [240] [Query Complexity of Classical and Quantum Channel Discrimination](https://arxiv.org/abs/2504.12989)
*Theshani Nuradha,Mark M. Wilde*

Main category: quant-ph

TL;DR: 研究量子通道区分的查询复杂度，分析其对错误概率和通道保真度的依赖关系，并给出经典和量子通道的精确或边界结果。


<details>
  <summary>Details</summary>
Motivation: 从信息论角度研究量子通道区分，目标是确定达到期望错误概率所需的最小通道使用次数。

Method: 通过分析二进制通道区分的查询复杂度，推导其对错误概率和通道保真度的对数依赖关系，并扩展到非对称和多通道情况。

Result: 二进制通道区分的查询复杂度与错误概率的对数和通道保真度的负对数相关；经典通道的查询复杂度被精确刻画；非对称和多通道的查询复杂度给出了上下界。

Conclusion: 量子通道区分的查询复杂度与错误概率和通道保真度密切相关，研究结果为经典和量子通道区分提供了理论支持。

Abstract: Quantum channel discrimination has been studied from an information-theoretic
perspective, wherein one is interested in the optimal decay rate of error
probabilities as a function of the number of unknown channel accesses. In this
paper, we study the query complexity of quantum channel discrimination, wherein
the goal is to determine the minimum number of channel uses needed to reach a
desired error probability. To this end, we show that the query complexity of
binary channel discrimination depends logarithmically on the inverse error
probability and inversely on the negative logarithm of the (geometric and
Holevo) channel fidelity. As a special case of these findings, we precisely
characterize the query complexity of discriminating between two classical
channels. We also provide lower and upper bounds on the query complexity of
binary asymmetric channel discrimination and multiple quantum channel
discrimination. For the former, the query complexity depends on the geometric
R\'enyi and Petz R\'enyi channel divergences, while for the latter, it depends
on the negative logarithm of (geometric and Uhlmann) channel fidelity. For
multiple channel discrimination, the upper bound scales as the logarithm of the
number of channels.

</details>


<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [241] [Post-processing improves accuracy of Artificial Intelligence weather forecasts](https://arxiv.org/abs/2504.12672)
*Belinda Trotta,Robert Johnson,Catherine de Burgh-Day,Debra Hudson,Esteban Abellan,James Canvin,Andrew Kelly,Daniel Mentiplay,Benjamin Owen,Jennifer Whelan*

Main category: physics.ao-ph

TL;DR: AI天气模型在部分变量上已达到业务级性能，但仍存在系统偏差和可靠性问题。研究发现，对AI模型应用传统数值天气预报的统计后处理方法，能显著提升其准确性，且与NWP模型结合可进一步提高预报技能。


<details>
  <summary>Details</summary>
Motivation: 探讨AI天气模型在实际业务中的应用潜力，尤其是如何通过现有统计后处理方法解决其系统偏差和可靠性问题。

Method: 使用Bureau of Meteorology的统计后处理系统IMPROVER，对ECMWF的确定性AI预报系统（AIFS）进行后处理，并与传统NWP模型（HRES和ENS）的后处理结果对比。

Result: 后处理显著提升了AIFS的准确性，且与NWP模型结合后整体预报技能进一步提高。

Conclusion: 统计后处理方法可直接应用于AI模型，为气象中心提供低风险、渐进式整合AI预报的途径。

Abstract: Artificial Intelligence (AI) weather models are now reaching
operational-grade performance for some variables, but like traditional
Numerical Weather Prediction (NWP) models, they exhibit systematic biases and
reliability issues. We test the application of the Bureau of Meteorology's
existing statistical post-processing system, IMPROVER, to ECMWF's deterministic
Artificial Intelligence Forecasting System (AIFS), and compare results against
post-processed outputs from the ECMWF HRES and ENS models. Without any
modification to configuration or processing workflows, post-processing yields
comparable accuracy improvements for AIFS as for traditional NWP forecasts, in
both expected value and probabilistic outputs. We show that blending AIFS with
NWP models improves overall forecast skill, even when AIFS alone is not the
most accurate component. These findings show that statistical post-processing
methods developed for NWP are directly applicable to AI models, enabling
national meteorological centres to incorporate AI forecasts into existing
workflows in a low-risk, incremental fashion.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [242] [Universal Approximation with XL MIMO Systems: OTA Classification via Trainable Analog Combining](https://arxiv.org/abs/2504.12758)
*Kyriakos Stylianopoulos,George C. Alexandropoulos*

Main category: eess.SP

TL;DR: XL-MIMO系统通过模拟组合器实现类似神经网络的通用函数逼近功能，提出了一种无需数字处理的OTA边缘推理新方法。


<details>
  <summary>Details</summary>
Motivation: 探索XL-MIMO系统在边缘计算中的潜力，将其与神经网络结合，以简化传统数字处理的复杂性。

Method: 将XL-MIMO信道系数视为隐藏层随机节点，接收端模拟组合器作为可训练输出层，结合ELM框架实现OTA推理。

Result: XL-MIMO-ELM实现了接近即时的训练和高效分类，性能与深度学习相当但复杂度显著降低。

Conclusion: XL-MIMO系统可作为神经网络，为超低功耗无线设备提供高效解决方案。

Abstract: In this paper, we demonstrate that an eXtremely Large (XL) Multiple-Input
Multiple-Output (MIMO) wireless system with appropriate analog combining
components exhibits the properties of a universal function approximator,
similar to a feedforward neural network. By treating the XL MIMO channel
coefficients as the random nodes of a hidden layer, and the receiver's analog
combiner as a trainable output layer, we cast the end-to-end system to the
Extreme Learning Machine (ELM) framework, leading to a novel formulation for
Over-The-Air (OTA) edge inference without requiring traditional digital
processing nor pre-processing at the transmitter. Through theoretical analysis
and numerical evaluation, we showcase that XL-MIMO-ELM enables
near-instantaneous training and efficient classification, suggesting the
paradigm shift of beyond massive MIMO systems as neural networks alongside
their profound communications role. Compared to deep learning approaches and
conventional ELMs, the proposed framework achieves on par performance with
orders of magnitude lower complexity, making it highly attractive for ultra low
power wireless devices.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [243] [The Dissipation Theory of Aging: A Quantitative Analysis Using a Cellular Aging Map](https://arxiv.org/abs/2504.13044)
*Farhan Khodaee,Rohola Zandie,Yufan Xia,Elazer R. Edelman*

Main category: q-bio.QM

TL;DR: 本文提出了一种基于动力系统的新衰老理论，并通过数据驱动的方法量化细胞水平的变化。


<details>
  <summary>Details</summary>
Motivation: 研究衰老的动力机制，揭示其作为一种耗散过程的本质。

Method: 使用遍历理论分解衰老过程中的动态变化，并采用基于Transformer的机器学习算法分析基因表达数据。

Result: 提出了细胞衰老图谱（CAM），揭示了基因嵌入空间中的发散模式、非线性转变和熵变化。

Conclusion: 衰老是一种耗散过程，新计算框架为分子水平的衰老变化提供了测量工具。

Abstract: We propose a new theory for aging based on dynamical systems and provide a
data-driven computational method to quantify the changes at the cellular level.
We use ergodic theory to decompose the dynamics of changes during aging and
show that aging is fundamentally a dissipative process within biological
systems, akin to dynamical systems where dissipation occurs due to
non-conservative forces. To quantify the dissipation dynamics, we employ a
transformer-based machine learning algorithm to analyze gene expression data,
incorporating age as a token to assess how age-related dissipation is reflected
in the embedding space. By evaluating the dynamics of gene and age embeddings,
we provide a cellular aging map (CAM) and identify patterns indicative of
divergence in gene embedding space, nonlinear transitions, and entropy
variations during aging for various tissues and cell types. Our results provide
a novel perspective on aging as a dissipative process and introduce a
computational framework that enables measuring age-related changes with
molecular resolution.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [244] [Design Topological Materials by Reinforcement Fine-Tuned Generative Model](https://arxiv.org/abs/2504.13048)
*Haosheng Xu,Dongheng Qian,Zhixuan Liu,Yadong Jiang,Jing Wang*

Main category: cond-mat.mtrl-sci

TL;DR: 论文提出了一种通过生成模型（ReFT）设计新型拓扑绝缘体（TIs）和拓扑晶体绝缘体（TCIs）的方法，成功生成了大量新材料，并以Ge$_2$Bi$_2$O$_6$为例展示了其性能。


<details>
  <summary>Details</summary>
Motivation: 传统方法在发现具有完整带隙的拓扑材料方面存在局限性，因此需要开发新方法生成此类材料。

Method: 采用强化微调（ReFT）技术对预训练生成模型进行优化，使其更符合材料设计目标。

Result: ReFT显著提升了模型生成TIs和TCIs的能力，并成功发现了大量新材料，其中Ge$_2$Bi$_2$O$_6$的带隙为0.26 eV，性能优异。

Conclusion: ReFT是一种有效的生成拓扑材料的方法，为未来材料设计提供了新思路。

Abstract: Topological insulators (TIs) and topological crystalline insulators (TCIs)
are materials with unconventional electronic properties, making their discovery
highly valuable for practical applications. However, such materials,
particularly those with a full band gap, remain scarce. Given the limitations
of traditional approaches that scan known materials for candidates, we focus on
the generation of new topological materials through a generative model.
Specifically, we apply reinforcement fine-tuning (ReFT) to a pre-trained
generative model, thereby aligning the model's objectives with our material
design goals. We demonstrate that ReFT is effective in enhancing the model's
ability to generate TIs and TCIs, with minimal compromise on the stability of
the generated materials. Using the fine-tuned model, we successfully identify a
large number of new topological materials, with Ge$_2$Bi$_2$O$_6$ serving as a
representative example--a TI with a full band gap of 0.26 eV, ranking among the
largest known in this category.

</details>


<div id='q-bio.GN'></div>

# q-bio.GN [[Back]](#toc)

### [245] [TransST: Transfer Learning Embedded Spatial Factor Modeling of Spatial Transcriptomics Data](https://arxiv.org/abs/2504.12353)
*Shuo Shuo Liu,Shikun Wang,Yuxuan Chen,Anil K. Rustgi,Ming Yuan,Jianhua Hu*

Main category: q-bio.GN

TL;DR: TransST是一种新型迁移学习框架，用于提升空间转录组学数据中细胞异质性的推断能力，显著优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 空间转录组学技术分辨率低且测序深度不足，难以可靠提取真实生物信号。

Method: 提出TransST框架，利用外部细胞标记信息自适应推断目标数据的细胞异质性。

Result: 在乳腺癌研究中成功识别五个生物相关细胞簇，并优于其他方法分离脂肪和结缔组织。

Conclusion: TransST在识别细胞亚群和检测驱动生物标志物方面既有效又稳健。

Abstract: Background: Spatial transcriptomics have emerged as a powerful tool in
biomedical research because of its ability to capture both the spatial contexts
and abundance of the complete RNA transcript profile in organs of interest.
However, limitations of the technology such as the relatively low resolution
and comparatively insufficient sequencing depth make it difficult to reliably
extract real biological signals from these data. To alleviate this challenge,
we propose a novel transfer learning framework, referred to as TransST, to
adaptively leverage the cell-labeled information from external sources in
inferring cell-level heterogeneity of a target spatial transcriptomics data.
  Results: Applications in several real studies as well as a number of
simulation settings show that our approach significantly improves existing
techniques. For example, in the breast cancer study, TransST successfully
identifies five biologically meaningful cell clusters, including the two
subgroups of cancer in situ and invasive cancer; in addition, only TransST is
able to separate the adipose tissues from the connective issues among all the
studied methods.
  Conclusions: In summary, the proposed method TransST is both effective and
robust in identifying cell subclusters and detecting corresponding driving
biomarkers in spatial transcriptomics data.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [246] [Cross-environment Cooperation Enables Zero-shot Multi-agent Coordination](https://arxiv.org/abs/2504.12714)
*Kunal Jha,Wilka Carvalho,Yancheng Liang,Simon S. Du,Max Kleiman-Weiner,Natasha Jaques*

Main category: cs.MA

TL;DR: 论文研究了通过强化学习在多个环境中训练代理，使其具备零样本协调能力（ZSC），能够在未见过的任务和伙伴中有效协作。


<details>
  <summary>Details</summary>
Motivation: 现有方法专注于单一任务的协作，缺乏泛化能力。本文旨在探索如何通过多环境训练提升代理的通用协作能力。

Method: 提出了Cross-Environment Cooperation (CEC)范式，并开发了两个Jax-based的生成器，用于创建大量可解决的协作任务。

Result: CEC在定量和定性上均优于基线方法，尤其在与人协作时表现突出。

Conclusion: 多场景训练促使代理形成通用协作规范，为设计无需人类数据的通用协作代理提供了新思路。

Abstract: Zero-shot coordination (ZSC), the ability to adapt to a new partner in a
cooperative task, is a critical component of human-compatible AI. While prior
work has focused on training agents to cooperate on a single task, these
specialized models do not generalize to new tasks, even if they are highly
similar. Here, we study how reinforcement learning on a distribution of
environments with a single partner enables learning general cooperative skills
that support ZSC with many new partners on many new problems. We introduce two
Jax-based, procedural generators that create billions of solvable coordination
challenges. We develop a new paradigm called Cross-Environment Cooperation
(CEC), and show that it outperforms competitive baselines quantitatively and
qualitatively when collaborating with real people. Our findings suggest that
learning to collaborate across many unique scenarios encourages agents to
develop general norms, which prove effective for collaboration with different
partners. Together, our results suggest a new route toward designing generalist
cooperative agents capable of interacting with humans without requiring human
data.

</details>


### [247] [The Athenian Academy: A Seven-Layer Architecture Model for Multi-Agent Systems](https://arxiv.org/abs/2504.12735)
*Lidong Zhai,Zhijie Qiu,Xizhong Guo,Jiaqi Li*

Main category: cs.MA

TL;DR: 本文提出了一种名为“雅典学院”的多智能体七层框架，旨在系统解决AI艺术创作中多智能体系统的协作效率、角色分配、环境适应和任务并行等挑战。


<details>
  <summary>Details</summary>
Motivation: 解决多智能体系统在AI艺术创作中的协作效率、角色分配、环境适应和任务并行等问题。

Method: 将多智能体系统分为七层：多智能体协作、单智能体多角色扮演、单智能体多场景遍历、单智能体多能力化身、不同单智能体使用同一大模型实现同一目标、单智能体使用不同大模型实现同一目标、多智能体合成同一目标。

Result: 实验验证表明，该框架在任务协作、跨场景适应和模型融合方面具有独特优势。

Conclusion: 框架为AI艺术创作中的多智能体协作提供了结构化方法，并推动了艺术领域的创新应用。未来可通过元学习和联邦学习等技术进一步优化协作机制、模型稳定性和系统安全性。

Abstract: This paper proposes the "Academy of Athens" multi-agent seven-layer
framework, aimed at systematically addressing challenges in multi-agent systems
(MAS) within artificial intelligence (AI) art creation, such as collaboration
efficiency, role allocation, environmental adaptation, and task parallelism.
The framework divides MAS into seven layers: multi-agent collaboration,
single-agent multi-role playing, single-agent multi-scene traversal,
single-agent multi-capability incarnation, different single agents using the
same large model to achieve the same target agent, single-agent using different
large models to achieve the same target agent, and multi-agent synthesis of the
same target agent. Through experimental validation in art creation, the
framework demonstrates its unique advantages in task collaboration, cross-scene
adaptation, and model fusion. This paper further discusses current challenges
such as collaboration mechanism optimization, model stability, and system
security, proposing future exploration through technologies like meta-learning
and federated learning. The framework provides a structured methodology for
multi-agent collaboration in AI art creation and promotes innovative
applications in the art field.

</details>


### [248] [Multi-Agent Reinforcement Learning Simulation for Environmental Policy Synthesis](https://arxiv.org/abs/2504.12777)
*James Rudd-Jones,Mirco Musolesi,María Pérez-Ortiz*

Main category: cs.MA

TL;DR: 论文提出了一种结合多智能体强化学习（MARL）与气候模拟的框架，以优化气候政策路径，解决传统方法在非线性动态、异质主体和不确定性量化方面的不足。


<details>
  <summary>Details</summary>
Motivation: 气候政策制定面临深度不确定性、复杂系统动态和利益冲突的挑战，传统气候模拟方法主要用于政策评估而非直接合成政策。

Method: 通过将MARL与气候模拟结合，解决奖励定义、可扩展性、不确定性传播和解决方案验证等关键问题。

Result: 提出了一个框架，支持更复杂的气候政策探索，同时明确了当前局限性和未来研究方向。

Conclusion: 该框架为气候政策合成提供了新思路，但需进一步解决MARL解决方案的可解释性和实用性。

Abstract: Climate policy development faces significant challenges due to deep
uncertainty, complex system dynamics, and competing stakeholder interests.
Climate simulation methods, such as Earth System Models, have become valuable
tools for policy exploration. However, their typical use is for evaluating
potential polices, rather than directly synthesizing them. The problem can be
inverted to optimize for policy pathways, but the traditional optimization
approaches often struggle with non-linear dynamics, heterogeneous agents, and
comprehensive uncertainty quantification. We propose a framework for augmenting
climate simulations with Multi-Agent Reinforcement Learning (MARL) to address
these limitations. We identify key challenges at the interface between climate
simulations and the application of MARL in the context of policy synthesis,
including reward definition, scalability with increasing agents and state
spaces, uncertainty propagation across linked systems, and solution validation.
Additionally, we discuss challenges in making MARL-derived solutions
interpretable and useful for policy-makers. Our framework provides a foundation
for more sophisticated climate policy exploration while acknowledging important
limitations and areas for future research.

</details>


### [249] [QLLM: Do We Really Need a Mixing Network for Credit Assignment in Multi-Agent Reinforcement Learning?](https://arxiv.org/abs/2504.12961)
*Zhouyang Jiang,Bin Zhang,Airong Wei,Zhiwei Xu*

Main category: cs.MA

TL;DR: 提出了一种基于大语言模型（LLM）的新算法QLLM，用于解决多智能体强化学习中的信用分配问题，通过TFCAF概念和coder-evaluator框架提升性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决现有信用分配方法在贡献归属不精确、可解释性差和高维状态空间扩展性不足等问题。

Method: 引入TFCAF概念，将信用分配表示为非线性函数，并采用coder-evaluator框架指导LLM生成、验证和优化代码。

Result: 在多个标准MARL基准测试中表现优于现有方法，具有强泛化能力和广泛兼容性。

Conclusion: QLLM是一种高效且通用的多智能体信用分配解决方案。

Abstract: Credit assignment has remained a fundamental challenge in multi-agent
reinforcement learning (MARL). Previous studies have primarily addressed this
issue through value decomposition methods under the centralized training with
decentralized execution paradigm, where neural networks are utilized to
approximate the nonlinear relationship between individual Q-values and the
global Q-value. Although these approaches have achieved considerable success in
various benchmark tasks, they still suffer from several limitations, including
imprecise attribution of contributions, limited interpretability, and poor
scalability in high-dimensional state spaces. To address these challenges, we
propose a novel algorithm, \textbf{QLLM}, which facilitates the automatic
construction of credit assignment functions using large language models (LLMs).
Specifically, the concept of \textbf{TFCAF} is introduced, wherein the credit
allocation process is represented as a direct and expressive nonlinear
functional formulation. A custom-designed \textit{coder-evaluator} framework is
further employed to guide the generation, verification, and refinement of
executable code by LLMs, significantly mitigating issues such as hallucination
and shallow reasoning during inference. Extensive experiments conducted on
several standard MARL benchmarks demonstrate that the proposed method
consistently outperforms existing state-of-the-art baselines. Moreover, QLLM
exhibits strong generalization capability and maintains compatibility with a
wide range of MARL algorithms that utilize mixing networks, positioning it as a
promising and versatile solution for complex multi-agent scenarios.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [250] [Mitigating LLM Hallucinations with Knowledge Graphs: A Case Study](https://arxiv.org/abs/2504.12422)
*Harry Li,Gabriel Appleby,Kenneth Alperin,Steven R Gomez,Ashley Suh*

Main category: cs.HC

TL;DR: 论文提出LinkQ系统，通过强制LLM查询知识图谱以减少幻觉，在定量评估中表现优于GPT-4，但仍存在局限性。


<details>
  <summary>Details</summary>
Motivation: 高风险的网络操作领域需要可信赖的AI方法，而LLM存在幻觉问题，需改进。

Method: 开发LinkQ系统，通过查询知识图谱获取真实数据，进行定量评估和专家定性研究。

Result: LinkQ在KGQA数据集上优于GPT-4，但在某些问题类别中表现不佳。

Conclusion: 未来需研究替代查询策略，专家反馈为系统改进提供了方向。

Abstract: High-stakes domains like cyber operations need responsible and trustworthy AI
methods. While large language models (LLMs) are becoming increasingly popular
in these domains, they still suffer from hallucinations. This research paper
provides learning outcomes from a case study with LinkQ, an open-source natural
language interface that was developed to combat hallucinations by forcing an
LLM to query a knowledge graph (KG) for ground-truth data during
question-answering (QA). We conduct a quantitative evaluation of LinkQ using a
well-known KGQA dataset, showing that the system outperforms GPT-4 but still
struggles with certain question categories - suggesting that alternative query
construction strategies will need to be investigated in future LLM querying
systems. We discuss a qualitative study of LinkQ with two domain experts using
a real-world cybersecurity KG, outlining these experts' feedback, suggestions,
perceived limitations, and future opportunities for systems like LinkQ.

</details>


### [251] [Don't Just Translate, Agitate: Using Large Language Models as Devil's Advocates for AI Explanations](https://arxiv.org/abs/2504.12424)
*Ashley Suh,Kenneth Alperin,Harry Li,Steven R Gomez*

Main category: cs.HC

TL;DR: 本文指出当前可解释AI（XAI）研究中，使用大型语言模型（LLMs）将解释性技术输出转化为自然语言解释的趋势存在问题，可能导致用户过度依赖AI系统。作者建议LLMs应扮演批判性角色，而非简单翻译工具。


<details>
  <summary>Details</summary>
Motivation: 探讨LLMs在XAI中作为翻译工具的问题，提出改进方向以避免用户误解和过度依赖。

Method: 分析LLMs在XAI中的现有应用，提出其应作为批判性角色，而非简单翻译工具。

Result: 研究发现LLMs的翻译功能可能强化解释性假象，而非提升透明度。

Conclusion: 建议LLMs应主动质疑AI解释，展示局限性，以减少用户过度依赖。

Abstract: This position paper highlights a growing trend in Explainable AI (XAI)
research where Large Language Models (LLMs) are used to translate outputs from
explainability techniques, like feature-attribution weights, into a natural
language explanation. While this approach may improve accessibility or
readability for users, recent findings suggest that translating into human-like
explanations does not necessarily enhance user understanding and may instead
lead to overreliance on AI systems. When LLMs summarize XAI outputs without
surfacing model limitations, uncertainties, or inconsistencies, they risk
reinforcing the illusion of interpretability rather than fostering meaningful
transparency. We argue that - instead of merely translating XAI outputs - LLMs
should serve as constructive agitators, or devil's advocates, whose role is to
actively interrogate AI explanations by presenting alternative interpretations,
potential biases, training data limitations, and cases where the model's
reasoning may break down. In this role, LLMs can facilitate users in engaging
critically with AI systems and generated explanations, with the potential to
reduce overreliance caused by misinterpreted or specious explanations.

</details>


### [252] [Co-Writing with AI, on Human Terms: Aligning Research with User Demands Across the Writing Process](https://arxiv.org/abs/2504.12488)
*Mohi Reza,Jeb Thomas-Mitchell,Peter Dushniku,Nathan Laundry,Joseph Jay Williams,Anastasia Kuzminykh*

Main category: cs.HC

TL;DR: 论文探讨了生成式AI工具（如ChatGPT）对写作过程中作者代理感和所有权的影响，通过系统综述和访谈揭示了四种AI写作支持策略，并分析了不同作者在不同写作阶段的需求差异。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI工具在日常写作中的普及，如何保持作者的代理感和所有权成为关键问题，但目前缺乏系统性的研究。

Method: 采用PRISMA方法系统综述了109篇HCI论文，并访谈了15位不同领域的作者，分析了AI写作支持的四种策略及其对写作认知过程的影响。

Result: 研究发现，作者对AI干预的需求因写作阶段和领域而异，内容导向型作者（如学者）更重视规划阶段的所有权，而形式导向型作者（如创意工作者）更关注翻译和审查阶段的控制权。

Conclusion: 研究为开发以人为本的AI协作写作工具提供了设计指导，强调了在不同写作阶段满足作者需求的重要性。

Abstract: As generative AI tools like ChatGPT become integral to everyday writing,
critical questions arise about how to preserve writers' sense of agency and
ownership when using these tools. Yet, a systematic understanding of how AI
assistance affects different aspects of the writing process - and how this
shapes writers' agency - remains underexplored. To address this gap, we
conducted a systematic review of 109 HCI papers using the PRISMA approach. From
this literature, we identify four overarching design strategies for AI writing
support: structured guidance, guided exploration, active co-writing, and
critical feedback - mapped across the four key cognitive processes in writing:
planning, translating, reviewing, and monitoring. We complement this analysis
with interviews of 15 writers across diverse domains. Our findings reveal that
writers' desired levels of AI intervention vary across the writing process:
content-focused writers (e.g., academics) prioritize ownership during planning,
while form-focused writers (e.g., creatives) value control over translating and
reviewing. Writers' preferences are also shaped by contextual goals, values,
and notions of originality and authorship. By examining when ownership matters,
what writers want to own, and how AI interactions shape agency, we surface both
alignment and gaps between research and user needs. Our findings offer
actionable design guidance for developing human-centered writing tools for
co-writing with AI, on human terms.

</details>


### [253] [Multimodal LLM Augmented Reasoning for Interpretable Visual Perception Analysis](https://arxiv.org/abs/2504.12511)
*Shravan Chaudhari,Trilokya Akula,Yoon Kim,Tom Blake*

Main category: cs.HC

TL;DR: 该论文提出了一种基于多模态大语言模型（MLLMs）的无标注分析框架，用于评估其在视觉感知任务中的解释能力，旨在推动MLLMs在提升人类推理能力和揭示数据集偏见中的应用。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索MLLMs在视觉感知任务中的适用性，结合心理学和认知科学原理，评估其作为认知助手的潜力，并揭示人类标注数据集中的偏见。

Method: 方法包括利用心理学和认知科学的复杂性原理指导MLLMs解释视觉内容，提出无标注分析框架，而非开发新的预测模型。

Result: 研究结果为MLLMs在视觉感知任务中的解释能力提供了基准，并展示了其作为认知助手的实用性。

Conclusion: 结论是该方法为量化MLLMs的解释性提供了原则性研究路径，有助于提升人类推理能力和发现数据集偏见。

Abstract: In this paper, we advance the study of AI-augmented reasoning in the context
of Human-Computer Interaction (HCI), psychology and cognitive science, focusing
on the critical task of visual perception. Specifically, we investigate the
applicability of Multimodal Large Language Models (MLLMs) in this domain. To
this end, we leverage established principles and explanations from psychology
and cognitive science related to complexity in human visual perception. We use
them as guiding principles for the MLLMs to compare and interprete visual
content. Our study aims to benchmark MLLMs across various explainability
principles relevant to visual perception. Unlike recent approaches that
primarily employ advanced deep learning models to predict complexity metrics
from visual content, our work does not seek to develop a mere new predictive
model. Instead, we propose a novel annotation-free analytical framework to
assess utility of MLLMs as cognitive assistants for HCI tasks, using visual
perception as a case study. The primary goal is to pave the way for principled
study in quantifying and evaluating the interpretability of MLLMs for
applications in improving human reasoning capability and uncovering biases in
existing perception datasets annotated by humans.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [254] [EmoVoice: LLM-based Emotional Text-To-Speech Model with Freestyle Text Prompting](https://arxiv.org/abs/2504.12867)
*Guanrou Yang,Chen Yang,Qian Chen,Ziyang Ma,Wenxi Chen,Wen Wang,Tianrui Wang,Yifan Yang,Zhikang Niu,Wenrui Liu,Fan Yu,Zhihao Du,Zhifu Gao,ShiLiang Zhang,Xie Chen*

Main category: eess.AS

TL;DR: EmoVoice是一种新型情感可控的TTS模型，利用LLM实现细粒度情感控制，并引入并行音素增强设计，结合高质量数据集EmoVoice-DB，在合成数据上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有TTS模型在情感表达控制方面存在不足，需要更自然和细粒度的情感生成能力。

Method: 提出EmoVoice模型，结合LLM实现自然语言情感控制，并设计并行音素增强模块；构建高质量情感数据集EmoVoice-DB。

Result: 在英文EmoVoice-DB和中文Secap测试集上达到SOTA性能，并探索了多模态LLM在情感评估中的应用。

Conclusion: EmoVoice在情感可控TTS中表现优异，未来可进一步优化评估指标和多模态LLM的应用。

Abstract: Human speech goes beyond the mere transfer of information; it is a profound
exchange of emotions and a connection between individuals. While Text-to-Speech
(TTS) models have made huge progress, they still face challenges in controlling
the emotional expression in the generated speech. In this work, we propose
EmoVoice, a novel emotion-controllable TTS model that exploits large language
models (LLMs) to enable fine-grained freestyle natural language emotion
control, and a phoneme boost variant design that makes the model output phoneme
tokens and audio tokens in parallel to enhance content consistency, inspired by
chain-of-thought (CoT) and modality-of-thought (CoM) techniques. Besides, we
introduce EmoVoice-DB, a high-quality 40-hour English emotion dataset featuring
expressive speech and fine-grained emotion labels with natural language
descriptions. EmoVoice achieves state-of-the-art performance on the English
EmoVoice-DB test set using only synthetic training data, and on the Chinese
Secap test set using our in-house data. We further investigate the reliability
of existing emotion evaluation metrics and their alignment with human
perceptual preferences, and explore using SOTA multimodal LLMs GPT-4o-audio and
Gemini to assess emotional speech. Demo samples are available at
https://anonymous.4open.science/r/EmoVoice-DF55. Dataset, code, and checkpoints
will be released.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [255] [Corner Gradient Descent](https://arxiv.org/abs/2504.12519)
*Dmitry Yarotsky*

Main category: math.OC

TL;DR: 论文研究了在无限维二次问题中使用SGD优化的方法，提出了一种广义的、具有无限记忆的SGD算法，能够达到接近最优的收敛速率。


<details>
  <summary>Details</summary>
Motivation: 在随机梯度下降（SGD）中，采样噪声导致传统的Jacobi HB方法发散，无法达到最优收敛速率。本文旨在解决这一问题。

Method: 通过将广义的（S）GD算法与复平面中的轮廓联系起来，并利用具有外部角的轮廓加速收敛，提出了一种高效的优化方法。

Result: 证明了最优收敛速率由θ_max决定，并通过快速有理逼近实现了有限内存算法的高效近似。

Conclusion: 提出的方法在理论和实验中均表现出色，能够有效平衡加速和噪声效应，适用于实际问题。

Abstract: We consider SGD-type optimization on infinite-dimensional quadratic problems
with power law spectral conditions. It is well-known that on such problems
deterministic GD has loss convergence rates $L_t=O(t^{-\zeta})$, which can be
improved to $L_t=O(t^{-2\zeta})$ by using Heavy Ball with a non-stationary
Jacobi-based schedule (and the latter rate is optimal among fixed schedules).
However, in the mini-batch Stochastic GD setting, the sampling noise causes the
Jacobi HB to diverge; accordingly no $O(t^{-2\zeta})$ algorithm is known. In
this paper we show that rates up to $O(t^{-2\zeta})$ can be achieved by a
generalized stationary SGD with infinite memory. We start by identifying
generalized (S)GD algorithms with contours in the complex plane. We then show
that contours that have a corner with external angle $\theta\pi$ accelerate the
plain GD rate $O(t^{-\zeta})$ to $O(t^{-\theta\zeta})$. For deterministic GD,
increasing $\theta$ allows to achieve rates arbitrarily close to
$O(t^{-2\zeta})$. However, in Stochastic GD, increasing $\theta$ also amplifies
the sampling noise, so in general $\theta$ needs to be optimized by balancing
the acceleration and noise effects. We prove that the optimal rate is given by
$\theta_{\max}=\min(2,\nu,\tfrac{2}{\zeta+1/\nu})$, where $\nu,\zeta$ are the
exponents appearing in the capacity and source spectral conditions.
Furthermore, using fast rational approximations of the power functions, we show
that ideal corner algorithms can be efficiently approximated by finite-memory
algorithms, and demonstrate their practical efficiency on a synthetic problem
and MNIST.

</details>


### [256] [On the asymptotic behaviour of stochastic processes, with applications to supermartingale convergence, Dvoretzky's approximation theorem, and stochastic quasi-Fejér monotonicity](https://arxiv.org/abs/2504.12922)
*Morenikeji Neri,Nicholas Pischke,Thomas Powell*

Main category: math.OC

TL;DR: 论文提出了一种关于符合松弛超鞅条件的随机过程渐近行为的新结果，提供了收敛速率的显式构造，并应用于随机逼近中的经典定理。


<details>
  <summary>Details</summary>
Motivation: 研究随机过程的渐近行为，特别是松弛超鞅条件下的收敛速率，以提供定量分析工具。

Method: 通过构造显式且有效的收敛速率，分析其在均值和几乎必然收敛中的表现，并应用于随机逼近中的经典问题。

Result: 得到了收敛速率的定量结果，并成功应用于Robbins-Siegmund定理、Dvoretzky收敛定理及随机拟Fejér单调序列的收敛。

Conclusion: 方法具有广泛适用性，可推广到其他随机逼近问题，并能构造快速（如线性）收敛速率。

Abstract: We prove a novel and general result on the asymptotic behavior of stochastic
processes which conform to a certain relaxed supermartingale condition. Our
result provides quantitative information in the form of an explicit and
effective construction of a rate of convergence for this process, both in mean
and almost surely, that is moreover highly uniform in the sense that it only
depends on very few data of the surrounding objects involved in the iteration.
We then apply this result to derive new quantitative versions of well-known
concepts and theorems from stochastic approximation, in particular providing
effective rates for a variant of the Robbins-Siegmund theorem, Dvoretzky's
convergence theorem, as well as the convergence of stochastic quasi-Fej\'er
monotone sequences, the latter of which formulated in a novel and highly
general metric context. We utilize the classic and widely studied Robbins-Monro
procedure as a template to evaluate our quantitative results and their
applicability in greater detail. We conclude by illustrating the breadth of
potential further applications with a brief discussion on a variety of other
well-known iterative procedures from stochastic approximation, covering a range
of different applied scenarios to which our methods can be immediately applied.
Throughout, we isolate and discuss special cases of our results which even
allow for the construction of fast, and in particular linear, rates.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [257] [Themisto: Jupyter-Based Runtime Benchmark](https://arxiv.org/abs/2504.12365)
*Konstantin Grotov,Sergey Titov*

Main category: cs.SE

TL;DR: 本文提出了一个基于Jupyter笔记本开发轨迹的基准测试，用于评估大型语言模型（LLMs）如何利用运行时信息预测代码输出和生成代码。结果显示当前LLMs表现不佳，并指出在代码模型开发中，运行时上下文的研究领域被严重忽视。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索LLMs在利用运行时信息进行代码预测和生成时的表现，并揭示该领域的研究不足。

Method: 方法是通过构建一个包含Jupyter笔记本开发轨迹的基准测试，评估LLMs在预测代码输出和生成代码任务中的表现。

Result: 结果表明，当前LLMs在这些任务中表现不佳，凸显了运行时上下文在代码模型开发中的重要性。

Conclusion: 结论指出，运行时上下文是一个被严重忽视的研究领域，未来需要更多关注以提高LLMs在代码相关任务中的表现。

Abstract: In this work, we present a benchmark that consists of Jupyter notebooks
development trajectories and allows measuring how large language models (LLMs)
can leverage runtime information for predicting code output and code
generation. We demonstrate that the current generation of LLMs performs poorly
on these tasks and argue that there exists a significantly understudied domain
in the development of code-based models, which involves incorporating the
runtime context.

</details>


### [258] [Code Copycat Conundrum: Demystifying Repetition in LLM-based Code Generation](https://arxiv.org/abs/2504.12608)
*Mingwei Liu,Juntao Li,Ying Wang,Xueying Du,Zuoyu Ou,Qiuyuan Chen,Bingxu An,Zhao Wei,Yong Xu,Fangming Zou,Xin Peng,Yiling Lou*

Main category: cs.SE

TL;DR: 该论文研究了LLM生成代码中的重复问题，提出了DeRep技术以检测和减少重复，显著提升了代码质量。


<details>
  <summary>Details</summary>
Motivation: LLM生成的代码存在重复问题，导致效率低下和可读性降低，亟需解决方案。

Method: 通过定量和定性分析19种先进代码LLM，总结20种重复模式，并提出规则化技术DeRep。

Result: DeRep在减少重复和提升代码质量方面显著优于基线，Pass@1提升最高达215.7%。

Conclusion: DeRep有效解决了LLM生成代码的重复问题，并提升了现有方法的性能。

Abstract: Despite recent advances in Large Language Models (LLMs) for code generation,
the quality of LLM-generated code still faces significant challenges. One
significant issue is code repetition, which refers to the model's tendency to
generate structurally redundant code, resulting in inefficiencies and reduced
readability. To address this, we conduct the first empirical study to
investigate the prevalence and nature of repetition across 19 state-of-the-art
code LLMs using three widely-used benchmarks. Our study includes both
quantitative and qualitative analyses, revealing that repetition is pervasive
and manifests at various granularities and extents, including character,
statement, and block levels. We further summarize a taxonomy of 20 repetition
patterns. Building on our findings, we propose DeRep, a rule-based technique
designed to detect and mitigate repetition in generated code. We evaluate DeRep
using both open-source benchmarks and in an industrial setting. Our results
demonstrate that DeRep significantly outperforms baselines in reducing
repetition (with an average improvements of 91.3%, 93.5%, and 79.9% in rep-3,
rep-line, and sim-line metrics) and enhancing code quality (with a Pass@1
increase of 208.3% over greedy search). Furthermore, integrating DeRep improves
the performance of existing repetition mitigation methods, with Pass@1
improvements ranging from 53.7% to 215.7%.

</details>


### [259] [A Phenomenological Approach to Analyzing User Queries in IT Systems Using Heidegger's Fundamental Ontology](https://arxiv.org/abs/2504.12977)
*Maksim Vishnevskiy*

Main category: cs.SE

TL;DR: 本文提出了一种基于海德格尔基础本体论的新型IT分析系统，通过区分存在者与存在，使用两种模态语言处理用户输入和内部分析，并通过现象学还原模块连接两者。


<details>
  <summary>Details</summary>
Motivation: 传统系统仅能进行范畴分析，而该系统利用现象学存在分析揭示更深层次的本体模式，解决复杂交互中的逻辑陷阱。

Method: 系统采用两种语言：存在者的范畴语言和存在的存在语言，通过现象学还原模块连接，分析用户查询并提供可操作的见解。

Result: 系统能够识别递归和自指结构，解决IT语境中的隐喻使用等问题，并展示了技术实现、用例及与现有工具的比较。

Conclusion: 该系统为通用查询分析工具奠定了基础，但仍需进一步形式化存在语言以实现完全可计算性。

Abstract: This paper presents a novel research analytical IT system grounded in Martin
Heidegger's Fundamental Ontology, distinguishing between beings (das Seiende)
and Being (das Sein). The system employs two modally distinct, descriptively
complete languages: a categorical language of beings for processing user inputs
and an existential language of Being for internal analysis. These languages are
bridged via a phenomenological reduction module, enabling the system to analyze
user queries (including questions, answers, and dialogues among IT
specialists), identify recursive and self-referential structures, and provide
actionable insights in categorical terms. Unlike contemporary systems limited
to categorical analysis, this approach leverages Heidegger's phenomenological
existential analysis to uncover deeper ontological patterns in query
processing, aiding in resolving logical traps in complex interactions, such as
metaphor usage in IT contexts. The path to full realization involves
formalizing the language of Being by a research team based on Heidegger's
Fundamental Ontology; given the existing completeness of the language of
beings, this reduces the system's computability to completeness, paving the way
for a universal query analysis tool. The paper presents the system's
architecture, operational principles, technical implementation, use
cases--including a case based on real IT specialist dialogues--comparative
evaluation with existing tools, and its advantages and limitations.

</details>

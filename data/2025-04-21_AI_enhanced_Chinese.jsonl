{"id": "2504.13187", "pdf": "https://arxiv.org/pdf/2504.13187", "abs": "https://arxiv.org/abs/2504.13187", "authors": ["In Hak Moon"], "title": "Benchmarking Large Language Models for Calculus Problem-Solving: A Comparative Analysis", "categories": ["cs.CL"], "comment": null, "summary": "This study presents a comprehensive evaluation of five leading large language\nmodels (LLMs) - Chat GPT 4o, Copilot Pro, Gemini Advanced, Claude Pro, and Meta\nAI - on their performance in solving calculus differentiation problems. The\ninvestigation assessed these models across 13 fundamental problem types,\nemploying a systematic cross-evaluation framework where each model solved\nproblems generated by all models. Results revealed significant performance\ndisparities, with Chat GPT 4o achieving the highest success rate (94.71%),\nfollowed by Claude Pro (85.74%), Gemini Advanced (84.42%), Copilot Pro\n(76.30%), and Meta AI (56.75%). All models excelled at procedural\ndifferentiation tasks but showed varying limitations with conceptual\nunderstanding and algebraic manipulation. Notably, problems involving\nincreasing/decreasing intervals and optimization word problems proved most\nchallenging across all models. The cross-evaluation matrix revealed that Claude\nPro generated the most difficult problems, suggesting distinct capabilities\nbetween problem generation and problem-solving. These findings have significant\nimplications for educational applications, highlighting both the potential and\nlimitations of LLMs as calculus learning tools. While they demonstrate\nimpressive procedural capabilities, their conceptual understanding remains\nlimited compared to human mathematical reasoning, emphasizing the continued\nimportance of human instruction for developing deeper mathematical\ncomprehension.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8bc4\u4f30\u4e86\u4e94\u79cd\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u89e3\u51b3\u5fae\u79ef\u5206\u5fae\u5206\u95ee\u9898\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0Chat GPT 4o\u8868\u73b0\u6700\u4f73\uff0c\u4f46\u6240\u6709\u6a21\u578b\u5728\u6982\u5ff5\u7406\u89e3\u548c\u4ee3\u6570\u64cd\u4f5c\u4e0a\u5b58\u5728\u5c40\u9650\u6027\u3002", "motivation": "\u8bc4\u4f30LLMs\u5728\u89e3\u51b3\u5fae\u79ef\u5206\u95ee\u9898\u4e0a\u7684\u80fd\u529b\uff0c\u4e3a\u6559\u80b2\u5e94\u7528\u63d0\u4f9b\u53c2\u8003\u3002", "method": "\u91c7\u7528\u7cfb\u7edf\u6027\u4ea4\u53c9\u8bc4\u4f30\u6846\u67b6\uff0c\u8ba9\u6bcf\u4e2a\u6a21\u578b\u89e3\u51b3\u7531\u6240\u6709\u6a21\u578b\u751f\u6210\u7684\u95ee\u9898\u3002", "result": "Chat GPT 4o\u6210\u529f\u7387\u6700\u9ad8\uff0894.71%\uff09\uff0cMeta AI\u6700\u4f4e\uff0856.75%\uff09\u3002\u6a21\u578b\u5728\u6982\u5ff5\u7406\u89e3\u548c\u4f18\u5316\u95ee\u9898\u4e0a\u8868\u73b0\u8f83\u5dee\u3002", "conclusion": "LLMs\u5728\u7a0b\u5e8f\u6027\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u6982\u5ff5\u7406\u89e3\u4e0a\u4ecd\u9700\u4eba\u7c7b\u6307\u5bfc\u3002"}}
{"id": "2504.13189", "pdf": "https://arxiv.org/pdf/2504.13189", "abs": "https://arxiv.org/abs/2504.13189", "authors": ["Sohom Ghosh", "Sudip Kumar Naskar"], "title": "BASIR: Budget-Assisted Sectoral Impact Ranking -- A Dataset for Sector Identification and Performance Prediction Using Language Models", "categories": ["cs.CL", "q-fin.ST"], "comment": "The codes and the datasets can be accessed from\n  https://huggingface.co/datasets/sohomghosh/BASIR_Budget_Assisted_Sectoral_Impact_Ranking/tree/main/", "summary": "Government fiscal policies, particularly annual union budgets, exert\nsignificant influence on financial markets. However, real-time analysis of\nbudgetary impacts on sector-specific equity performance remains\nmethodologically challenging and largely unexplored. This study proposes a\nframework to systematically identify and rank sectors poised to benefit from\nIndia's Union Budget announcements. The framework addresses two core tasks: (1)\nmulti-label classification of excerpts from budget transcripts into 81\npredefined economic sectors, and (2) performance ranking of these sectors.\nLeveraging a comprehensive corpus of Indian Union Budget transcripts from 1947\nto 2025, we introduce BASIR (Budget-Assisted Sectoral Impact Ranking), an\nannotated dataset mapping excerpts from budgetary transcripts to sectoral\nimpacts. Our architecture incorporates fine-tuned embeddings for sector\nidentification, coupled with language models that rank sectors based on their\npredicted performances. Our results demonstrate 0.605 F1-score in sector\nclassification, and 0.997 NDCG score in predicting ranks of sectors based on\npost-budget performances. The methodology enables investors and policymakers to\nquantify fiscal policy impacts through structured, data-driven insights,\naddressing critical gaps in manual analysis. The annotated dataset has been\nreleased under CC-BY-NC-SA-4.0 license to advance computational economics\nresearch.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u6846\u67b6\uff08BASIR\uff09\uff0c\u7528\u4e8e\u7cfb\u7edf\u8bc6\u522b\u548c\u6392\u540d\u53d7\u5370\u5ea6\u8054\u90a6\u9884\u7b97\u516c\u544a\u5f71\u54cd\u7684\u884c\u4e1a\uff0c\u7ed3\u5408\u591a\u6807\u7b7e\u5206\u7c7b\u548c\u6027\u80fd\u6392\u540d\uff0c\u5c55\u793a\u4e86\u8f83\u9ad8\u7684\u5206\u7c7b\u548c\u9884\u6d4b\u51c6\u786e\u6027\u3002", "motivation": "\u653f\u5e9c\u8d22\u653f\u653f\u7b56\u5bf9\u91d1\u878d\u5e02\u573a\u6709\u663e\u8457\u5f71\u54cd\uff0c\u4f46\u5b9e\u65f6\u5206\u6790\u9884\u7b97\u5bf9\u884c\u4e1a\u80a1\u7968\u8868\u73b0\u7684\u5f71\u54cd\u4ecd\u7f3a\u4e4f\u7cfb\u7edf\u65b9\u6cd5\u3002", "method": "\u5229\u75281947\u81f32025\u5e74\u5370\u5ea6\u8054\u90a6\u9884\u7b97\u6587\u672c\uff0c\u6784\u5efaBASIR\u6570\u636e\u96c6\uff0c\u7ed3\u5408\u7cbe\u7ec6\u8c03\u6574\u7684\u5d4c\u5165\u548c\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u884c\u4e1a\u5206\u7c7b\u548c\u6392\u540d\u3002", "result": "\u884c\u4e1a\u5206\u7c7bF1\u5f97\u5206\u4e3a0.605\uff0c\u884c\u4e1a\u6392\u540dNDCG\u5f97\u5206\u4e3a0.997\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u6295\u8d44\u8005\u548c\u653f\u7b56\u5236\u5b9a\u8005\u63d0\u4f9b\u4e86\u6570\u636e\u9a71\u52a8\u7684\u89c1\u89e3\uff0c\u586b\u8865\u4e86\u624b\u52a8\u5206\u6790\u7684\u7a7a\u767d\uff0c\u5e76\u516c\u5f00\u4e86\u6570\u636e\u96c6\u4ee5\u63a8\u52a8\u7814\u7a76\u3002"}}
{"id": "2504.13216", "pdf": "https://arxiv.org/pdf/2504.13216", "abs": "https://arxiv.org/abs/2504.13216", "authors": ["Bokwang Hwang", "Seonkyu Lim", "Taewoong Kim", "Yongjae Geun", "Sunghyun Bang", "Sohyun Park", "Jihyun Park", "Myeonggyu Lee", "Jinwoo Lee", "Yerin Kim", "Jinsun Yoo", "Jingyeong Hong", "Jina Park", "Yongchan Kim", "Suhyun Kim", "Younggyun Hahm", "Yiseul Lee", "Yejee Kang", "Chanhyuk Yoon", "Chansu Lee", "Heeyewon Jeong", "Jiyeon Lee", "Seonhye Gu", "Hyebin Kang", "Yousang Cho", "Hangyeol Yoo", "KyungTae Lim"], "title": "KFinEval-Pilot: A Comprehensive Benchmark Suite for Korean Financial Language Understanding", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "We introduce KFinEval-Pilot, a benchmark suite specifically designed to\nevaluate large language models (LLMs) in the Korean financial domain.\nAddressing the limitations of existing English-centric benchmarks,\nKFinEval-Pilot comprises over 1,000 curated questions across three critical\nareas: financial knowledge, legal reasoning, and financial toxicity. The\nbenchmark is constructed through a semi-automated pipeline that combines\nGPT-4-generated prompts with expert validation to ensure domain relevance and\nfactual accuracy. We evaluate a range of representative LLMs and observe\nnotable performance differences across models, with trade-offs between task\naccuracy and output safety across different model families. These results\nhighlight persistent challenges in applying LLMs to high-stakes financial\napplications, particularly in reasoning and safety. Grounded in real-world\nfinancial use cases and aligned with the Korean regulatory and linguistic\ncontext, KFinEval-Pilot serves as an early diagnostic tool for developing safer\nand more reliable financial AI systems.", "AI": {"tldr": "KFinEval-Pilot\u662f\u4e00\u4e2a\u9488\u5bf9\u97e9\u8bed\u91d1\u878d\u9886\u57df\u7684\u5927\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\u57fa\u51c6\uff0c\u5305\u542b1000\u591a\u4e2a\u95ee\u9898\uff0c\u6db5\u76d6\u91d1\u878d\u77e5\u8bc6\u3001\u6cd5\u5f8b\u63a8\u7406\u548c\u91d1\u878d\u6bd2\u6027\u3002\u901a\u8fc7\u534a\u81ea\u52a8\u5316\u6d41\u7a0b\u6784\u5efa\uff0c\u7ed3\u5408GPT-4\u751f\u6210\u548c\u4e13\u5bb6\u9a8c\u8bc1\uff0c\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\u6a21\u578b\u5728\u4efb\u52a1\u51c6\u786e\u6027\u548c\u8f93\u51fa\u5b89\u5168\u6027\u4e4b\u95f4\u5b58\u5728\u6743\u8861\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u4ee5\u82f1\u8bed\u4e3a\u4e2d\u5fc3\u7684\u57fa\u51c6\u5728\u97e9\u8bed\u91d1\u878d\u9886\u57df\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u9ad8\u98ce\u9669\u91d1\u878d\u5e94\u7528\u4e2d\u7684\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u66f4\u53ef\u9760\u7684\u8bc4\u4f30\u5de5\u5177\u3002", "method": "\u91c7\u7528\u534a\u81ea\u52a8\u5316\u6d41\u7a0b\uff0c\u7ed3\u5408GPT-4\u751f\u6210\u7684\u63d0\u793a\u548c\u4e13\u5bb6\u9a8c\u8bc1\uff0c\u6784\u5efa\u5305\u542b\u91d1\u878d\u77e5\u8bc6\u3001\u6cd5\u5f8b\u63a8\u7406\u548c\u91d1\u878d\u6bd2\u6027\u7684\u95ee\u9898\u96c6\u3002", "result": "\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\u4e0d\u540c\u6a21\u578b\u5728\u4efb\u52a1\u51c6\u786e\u6027\u548c\u8f93\u51fa\u5b89\u5168\u6027\u4e0a\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u51f8\u663e\u4e86\u91d1\u878d\u5e94\u7528\u4e2d\u63a8\u7406\u548c\u5b89\u5168\u6027\u7684\u6311\u6218\u3002", "conclusion": "KFinEval-Pilot\u4e3a\u5f00\u53d1\u66f4\u5b89\u5168\u53ef\u9760\u7684\u91d1\u878dAI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65e9\u671f\u8bca\u65ad\u5de5\u5177\uff0c\u7279\u522b\u9002\u7528\u4e8e\u97e9\u8bed\u91d1\u878d\u573a\u666f\u3002"}}
{"id": "2504.13217", "pdf": "https://arxiv.org/pdf/2504.13217", "abs": "https://arxiv.org/abs/2504.13217", "authors": ["Jennifer Haase", "Finn Klessascheck", "Jan Mendling", "Sebastian Pokutta"], "title": "Sustainability via LLM Right-sizing", "categories": ["cs.CL", "cs.AI"], "comment": "17 pages, 2 Figures, 6 Tables", "summary": "Large language models (LLMs) have become increasingly embedded in\norganizational workflows. This has raised concerns over their energy\nconsumption, financial costs, and data sovereignty. While performance\nbenchmarks often celebrate cutting-edge models, real-world deployment decisions\nrequire a broader perspective: when is a smaller, locally deployable model\n\"good enough\"? This study offers an empirical answer by evaluating eleven\nproprietary and open-weight LLMs across ten everyday occupational tasks,\nincluding summarizing texts, generating schedules, and drafting emails and\nproposals. Using a dual-LLM-based evaluation framework, we automated task\nexecution and standardized evaluation across ten criteria related to output\nquality, factual accuracy, and ethical responsibility. Results show that GPT-4o\ndelivers consistently superior performance but at a significantly higher cost\nand environmental footprint. Notably, smaller models like Gemma-3 and Phi-4\nachieved strong and reliable results on most tasks, suggesting their viability\nin contexts requiring cost-efficiency, local deployment, or privacy. A cluster\nanalysis revealed three model groups -- premium all-rounders, competent\ngeneralists, and limited but safe performers -- highlighting trade-offs between\nquality, control, and sustainability. Significantly, task type influenced model\neffectiveness: conceptual tasks challenged most models, while aggregation and\ntransformation tasks yielded better performances. We argue for a shift from\nperformance-maximizing benchmarks to task- and context-aware sufficiency\nassessments that better reflect organizational priorities. Our approach\ncontributes a scalable method to evaluate AI models through a sustainability\nlens and offers actionable guidance for responsible LLM deployment in practice.", "AI": {"tldr": "\u7814\u7a76\u8bc4\u4f30\u4e8611\u79cd\u4e13\u6709\u548c\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\u572810\u79cd\u65e5\u5e38\u804c\u4e1a\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0GPT-4o\u6027\u80fd\u6700\u4f18\u4f46\u6210\u672c\u9ad8\uff0c\u800cGemma-3\u548cPhi-4\u7b49\u5c0f\u6a21\u578b\u5728\u591a\u6570\u4efb\u52a1\u4e2d\u8868\u73b0\u53ef\u9760\uff0c\u9002\u5408\u6210\u672c\u654f\u611f\u6216\u9690\u79c1\u9700\u6c42\u573a\u666f\u3002", "motivation": "\u63a2\u8ba8\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\uff0c\u4f55\u65f6\u8f83\u5c0f\u3001\u53ef\u672c\u5730\u90e8\u7f72\u7684\u6a21\u578b\u80fd\u591f\u6ee1\u8db3\u9700\u6c42\uff0c\u800c\u975e\u4e00\u5473\u8ffd\u6c42\u9ad8\u6027\u80fd\u6a21\u578b\u3002", "method": "\u91c7\u7528\u53ccLLM\u8bc4\u4f30\u6846\u67b6\uff0c\u81ea\u52a8\u5316\u4efb\u52a1\u6267\u884c\u5e76\u6807\u51c6\u5316\u8bc4\u4f3010\u9879\u6807\u51c6\uff0c\u5305\u62ec\u8f93\u51fa\u8d28\u91cf\u3001\u4e8b\u5b9e\u51c6\u786e\u6027\u548c\u4f26\u7406\u8d23\u4efb\u3002", "result": "GPT-4o\u6027\u80fd\u6700\u4f18\u4f46\u6210\u672c\u9ad8\uff0c\u5c0f\u6a21\u578b\u5982Gemma-3\u548cPhi-4\u5728\u591a\u6570\u4efb\u52a1\u4e2d\u8868\u73b0\u53ef\u9760\u3002\u4efb\u52a1\u7c7b\u578b\u5f71\u54cd\u6a21\u578b\u6548\u679c\uff0c\u6982\u5ff5\u6027\u4efb\u52a1\u66f4\u5177\u6311\u6218\u6027\u3002", "conclusion": "\u5efa\u8bae\u4ece\u8ffd\u6c42\u6027\u80fd\u6700\u5927\u5316\u8f6c\u5411\u4efb\u52a1\u548c\u60c5\u5883\u611f\u77e5\u7684\u8db3\u591f\u6027\u8bc4\u4f30\uff0c\u4ee5\u66f4\u597d\u5730\u53cd\u6620\u7ec4\u7ec7\u9700\u6c42\uff0c\u5e76\u63d0\u4f9b\u53ef\u6301\u7eed\u7684LLM\u90e8\u7f72\u6307\u5bfc\u3002"}}
{"id": "2504.13202", "pdf": "https://arxiv.org/pdf/2504.13202", "abs": "https://arxiv.org/abs/2504.13202", "authors": ["Timo Aukusti Laine"], "title": "The Quantum LLM: Modeling Semantic Spaces with Quantum Principles", "categories": ["cs.AI", "cs.CL", "quant-ph"], "comment": "16 pages, 6 figures", "summary": "In the previous article, we presented a quantum-inspired framework for\nmodeling semantic representation and processing in Large Language Models\n(LLMs), drawing upon mathematical tools and conceptual analogies from quantum\nmechanics to offer a new perspective on these complex systems. In this paper,\nwe clarify the core assumptions of this model, providing a detailed exposition\nof six key principles that govern semantic representation, interaction, and\ndynamics within LLMs. The goal is to justify that a quantum-inspired framework\nis a valid approach to studying semantic spaces. This framework offers valuable\ninsights into their information processing and response generation, and we\nfurther discuss the potential of leveraging quantum computing to develop\nsignificantly more powerful and efficient LLMs based on these principles.", "AI": {"tldr": "\u672c\u6587\u6f84\u6e05\u4e86\u91cf\u5b50\u542f\u53d1\u7684\u8bed\u4e49\u8868\u793a\u6a21\u578b\u7684\u6838\u5fc3\u5047\u8bbe\uff0c\u8be6\u7ec6\u9610\u8ff0\u4e86\u516d\u9879\u5173\u952e\u539f\u5219\uff0c\u65e8\u5728\u8bc1\u660e\u8be5\u6846\u67b6\u5bf9\u7814\u7a76LLM\u8bed\u4e49\u7a7a\u95f4\u7684\u6709\u6548\u6027\uff0c\u5e76\u63a2\u8ba8\u4e86\u91cf\u5b50\u8ba1\u7b97\u5728\u63d0\u5347LLM\u6027\u80fd\u65b9\u9762\u7684\u6f5c\u529b\u3002", "motivation": "\u901a\u8fc7\u91cf\u5b50\u529b\u5b66\u7684\u6570\u5b66\u5de5\u5177\u548c\u6982\u5ff5\u7c7b\u6bd4\uff0c\u4e3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u8bed\u4e49\u8868\u793a\u548c\u5904\u7406\u63d0\u4f9b\u65b0\u89c6\u89d2\uff0c\u5e76\u9a8c\u8bc1\u91cf\u5b50\u542f\u53d1\u6846\u67b6\u7684\u5408\u7406\u6027\u3002", "method": "\u63d0\u51fa\u516d\u9879\u5173\u952e\u539f\u5219\uff0c\u8be6\u7ec6\u9610\u8ff0\u8bed\u4e49\u8868\u793a\u3001\u4ea4\u4e92\u548c\u52a8\u6001\u8fc7\u7a0b\uff0c\u5e76\u8ba8\u8bba\u91cf\u5b50\u8ba1\u7b97\u7684\u5e94\u7528\u6f5c\u529b\u3002", "result": "\u91cf\u5b50\u542f\u53d1\u6846\u67b6\u4e3aLLM\u7684\u4fe1\u606f\u5904\u7406\u548c\u54cd\u5e94\u751f\u6210\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\uff0c\u5e76\u5c55\u793a\u4e86\u91cf\u5b50\u8ba1\u7b97\u5728\u63d0\u5347LLM\u6548\u7387\u548c\u80fd\u529b\u65b9\u9762\u7684\u53ef\u80fd\u6027\u3002", "conclusion": "\u91cf\u5b50\u542f\u53d1\u6846\u67b6\u662f\u7814\u7a76LLM\u8bed\u4e49\u7a7a\u95f4\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u672a\u6765\u7ed3\u5408\u91cf\u5b50\u8ba1\u7b97\u53ef\u80fd\u663e\u8457\u63d0\u5347LLM\u6027\u80fd\u3002"}}
{"id": "2504.13218", "pdf": "https://arxiv.org/pdf/2504.13218", "abs": "https://arxiv.org/abs/2504.13218", "authors": ["Yaguang Song", "Xiaoshan Yang", "Dongmei Jiang", "Yaowei Wang", "Changsheng Xu"], "title": "Harmony: A Unified Framework for Modality Incremental Learning", "categories": ["cs.LG", "cs.AI", "cs.MM"], "comment": null, "summary": "Incremental learning aims to enable models to continuously acquire knowledge\nfrom evolving data streams while preserving previously learned capabilities.\nWhile current research predominantly focuses on unimodal incremental learning\nand multimodal incremental learning where the modalities are consistent,\nreal-world scenarios often present data from entirely new modalities, posing\nadditional challenges. This paper investigates the feasibility of developing a\nunified model capable of incremental learning across continuously evolving\nmodal sequences. To this end, we introduce a novel paradigm called Modality\nIncremental Learning (MIL), where each learning stage involves data from\ndistinct modalities. To address this task, we propose a novel framework named\nHarmony, designed to achieve modal alignment and knowledge retention, enabling\nthe model to reduce the modal discrepancy and learn from a sequence of distinct\nmodalities, ultimately completing tasks across multiple modalities within a\nunified framework. Our approach introduces the adaptive compatible feature\nmodulation and cumulative modal bridging. Through constructing historical modal\nfeatures and performing modal knowledge accumulation and alignment, the\nproposed components collaboratively bridge modal differences and maintain\nknowledge retention, even with solely unimodal data available at each learning\nphase.These components work in concert to establish effective modality\nconnections and maintain knowledge retention, even when only unimodal data is\navailable at each learning stage. Extensive experiments on the MIL task\ndemonstrate that our proposed method significantly outperforms existing\nincremental learning methods, validating its effectiveness in MIL scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6a21\u6001\u589e\u91cf\u5b66\u4e60\uff08MIL\uff09\u8303\u5f0f\uff0c\u65e8\u5728\u89e3\u51b3\u6a21\u578b\u5728\u8fde\u7eed\u6f14\u53d8\u7684\u6a21\u6001\u5e8f\u5217\u4e2d\u589e\u91cf\u5b66\u4e60\u7684\u6311\u6218\u3002\u901a\u8fc7Harmony\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u6a21\u6001\u5bf9\u9f50\u548c\u77e5\u8bc6\u4fdd\u7559\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u5b9e\u573a\u666f\u4e2d\u6570\u636e\u5e38\u6765\u81ea\u5168\u65b0\u6a21\u6001\uff0c\u800c\u73b0\u6709\u7814\u7a76\u591a\u5173\u6ce8\u5355\u6a21\u6001\u6216\u6a21\u6001\u4e00\u81f4\u7684\u591a\u6a21\u6001\u589e\u91cf\u5b66\u4e60\u3002\u672c\u6587\u63a2\u7d22\u4e86\u5728\u8fde\u7eed\u6f14\u53d8\u6a21\u6001\u5e8f\u5217\u4e2d\u7edf\u4e00\u6a21\u578b\u589e\u91cf\u5b66\u4e60\u7684\u53ef\u884c\u6027\u3002", "method": "\u63d0\u51faHarmony\u6846\u67b6\uff0c\u5305\u542b\u81ea\u9002\u5e94\u517c\u5bb9\u7279\u5f81\u8c03\u5236\u548c\u7d2f\u79ef\u6a21\u6001\u6865\u63a5\uff0c\u901a\u8fc7\u5386\u53f2\u6a21\u6001\u7279\u5f81\u6784\u5efa\u548c\u6a21\u6001\u77e5\u8bc6\u79ef\u7d2f\u4e0e\u5bf9\u9f50\uff0c\u51cf\u5c11\u6a21\u6001\u5dee\u5f02\u5e76\u4fdd\u7559\u77e5\u8bc6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cHarmony\u5728MIL\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u589e\u91cf\u5b66\u4e60\u65b9\u6cd5\u3002", "conclusion": "Harmony\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u6a21\u6001\u589e\u91cf\u5b66\u4e60\u4e2d\u7684\u6311\u6218\uff0c\u4e3a\u7edf\u4e00\u6a21\u578b\u5728\u591a\u6a21\u6001\u589e\u91cf\u5b66\u4e60\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2504.13227", "pdf": "https://arxiv.org/pdf/2504.13227", "abs": "https://arxiv.org/abs/2504.13227", "authors": ["Weijie Shi", "Jipeng Zhang", "Yaguang Wu", "Jingzhi Fang", "Ruiyuan Zhang", "Jiajie Xu", "Jia Zhu", "Hao Chen", "Yao Zhao", "Sirui Han", "Xiaofang Zhou"], "title": "DIDS: Domain Impact-aware Data Sampling for Large Language Model Training", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) are commonly trained on multi-domain datasets,\nwhere domain sampling strategies significantly impact model performance due to\nvarying domain importance across downstream tasks. Existing approaches for\noptimizing domain-level sampling strategies struggle with maintaining\nintra-domain consistency and accurately measuring domain impact. In this paper,\nwe present Domain Impact-aware Data Sampling (DIDS). To ensure intra-domain\nconsistency, a gradient clustering algorithm is proposed to group training data\nbased on their learning effects, where a proxy language model and\ndimensionality reduction are employed to reduce computational overhead. To\naccurately measure domain impact, we develop a Fisher Information Matrix (FIM)\nguided metric that quantifies how domain-specific parameter updates affect the\nmodel's output distributions on downstream tasks, with theoretical guarantees.\nFurthermore, to determine optimal sampling ratios, DIDS combines both the\nFIM-guided domain impact assessment and loss learning trajectories that\nindicate domain-specific potential, while accounting for diminishing marginal\nreturns. Extensive experiments demonstrate that DIDS achieves 3.4% higher\naverage performance while maintaining comparable training efficiency.", "AI": {"tldr": "DIDS\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9886\u57df\u5f71\u54cd\u611f\u77e5\u7684\u6570\u636e\u91c7\u6837\u65b9\u6cd5\uff0c\u901a\u8fc7\u68af\u5ea6\u805a\u7c7b\u548cFIM\u5ea6\u91cf\u4f18\u5316\u9886\u57df\u91c7\u6837\u7b56\u7565\uff0c\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u591a\u9886\u57df\u6570\u636e\u96c6\u4e2d\u9886\u57df\u91c7\u6837\u7b56\u7565\u5bf9\u6a21\u578b\u6027\u80fd\u5f71\u54cd\u663e\u8457\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u4fdd\u6301\u9886\u57df\u5185\u4e00\u81f4\u6027\u548c\u51c6\u786e\u8861\u91cf\u9886\u57df\u5f71\u54cd\u3002", "method": "\u4f7f\u7528\u68af\u5ea6\u805a\u7c7b\u7b97\u6cd5\u5206\u7ec4\u6570\u636e\uff0c\u7ed3\u5408FIM\u5ea6\u91cf\u91cf\u5316\u9886\u57df\u5f71\u54cd\uff0c\u5e76\u57fa\u4e8e\u635f\u5931\u5b66\u4e60\u8f68\u8ff9\u786e\u5b9a\u6700\u4f18\u91c7\u6837\u6bd4\u4f8b\u3002", "result": "\u5b9e\u9a8c\u8868\u660eDIDS\u5e73\u5747\u6027\u80fd\u63d0\u53473.4%\uff0c\u540c\u65f6\u4fdd\u6301\u8bad\u7ec3\u6548\u7387\u3002", "conclusion": "DIDS\u901a\u8fc7\u9886\u57df\u5f71\u54cd\u611f\u77e5\u548c\u4f18\u5316\u91c7\u6837\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2504.13210", "pdf": "https://arxiv.org/pdf/2504.13210", "abs": "https://arxiv.org/abs/2504.13210", "authors": ["Maarten C. Vonk", "Mauricio Gonzalez Soto", "Anna V. Kononova"], "title": "Graphical Models for Decision-Making: Integrating Causality and Game Theory", "categories": ["cs.AI", "cs.GT", "cs.LG", "math.PR"], "comment": null, "summary": "Causality and game theory are two influential fields that contribute\nsignificantly to decision-making in various domains. Causality defines and\nmodels causal relationships in complex policy problems, while game theory\nprovides insights into strategic interactions among stakeholders with competing\ninterests. Integrating these frameworks has led to significant theoretical\nadvancements with the potential to improve decision-making processes. However,\npractical applications of these developments remain underexplored. To support\nefforts toward implementation, this paper clarifies key concepts in game theory\nand causality that are essential to their intersection, particularly within the\ncontext of probabilistic graphical models. By rigorously examining these\nconcepts and illustrating them with intuitive, consistent examples, we clarify\nthe required inputs for implementing these models, provide practitioners with\ninsights into their application and selection across different scenarios, and\nreference existing research that supports their implementation. We hope this\nwork encourages broader adoption of these models in real-world scenarios.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u56e0\u679c\u6027\u4e0e\u535a\u5f08\u8bba\u7684\u7ed3\u5408\uff0c\u5f3a\u8c03\u5176\u5728\u51b3\u7b56\u4e2d\u7684\u6f5c\u529b\uff0c\u5e76\u63d0\u4f9b\u4e86\u5b9e\u9645\u5e94\u7528\u7684\u5173\u952e\u6982\u5ff5\u548c\u65b9\u6cd5\u3002", "motivation": "\u56e0\u679c\u6027\u548c\u535a\u5f08\u8bba\u5728\u51b3\u7b56\u4e2d\u5177\u6709\u91cd\u8981\u4f5c\u7528\uff0c\u4f46\u4e24\u8005\u7684\u7ed3\u5408\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u4fc3\u8fdb\u8fd9\u4e9b\u6a21\u578b\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u66f4\u5e7f\u6cdb\u5e94\u7528\u3002", "method": "\u901a\u8fc7\u6f84\u6e05\u535a\u5f08\u8bba\u548c\u56e0\u679c\u6027\u4e2d\u7684\u5173\u952e\u6982\u5ff5\uff0c\u7279\u522b\u662f\u5728\u6982\u7387\u56fe\u6a21\u578b\u7684\u80cc\u666f\u4e0b\uff0c\u7ed3\u5408\u76f4\u89c2\u793a\u4f8b\uff0c\u63d0\u4f9b\u5b9e\u65bd\u8fd9\u4e9b\u6a21\u578b\u7684\u8f93\u5165\u8981\u6c42\u548c\u5e94\u7528\u6307\u5bfc\u3002", "result": "\u660e\u786e\u4e86\u5b9e\u65bd\u8fd9\u4e9b\u6a21\u578b\u6240\u9700\u7684\u8f93\u5165\uff0c\u4e3a\u5b9e\u8df5\u8005\u63d0\u4f9b\u4e86\u5728\u4e0d\u540c\u573a\u666f\u4e2d\u9009\u62e9\u548c\u5e94\u7528\u8fd9\u4e9b\u6a21\u578b\u7684\u89c1\u89e3\uff0c\u5e76\u5f15\u7528\u4e86\u652f\u6301\u5b9e\u65bd\u7684\u76f8\u5173\u7814\u7a76\u3002", "conclusion": "\u672c\u6587\u5e0c\u671b\u63a8\u52a8\u8fd9\u4e9b\u6a21\u578b\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u66f4\u5e7f\u6cdb\u5e94\u7528\uff0c\u4e3a\u51b3\u7b56\u63d0\u4f9b\u66f4\u6709\u6548\u7684\u5de5\u5177\u3002"}}
{"id": "2504.13219", "pdf": "https://arxiv.org/pdf/2504.13219", "abs": "https://arxiv.org/abs/2504.13219", "authors": ["Wenxuan Yang", "Qingqu Wei", "Chenxi Ma", "Weimin Tan", "Bo Yan"], "title": "Scaling Laws for Data-Efficient Visual Transfer Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Current scaling laws for visual AI models focus predominantly on large-scale\npretraining, leaving a critical gap in understanding how performance scales for\ndata-constrained downstream tasks. To address this limitation, this paper\nestablishes the first practical framework for data-efficient scaling laws in\nvisual transfer learning, addressing two fundamental questions: 1) How do\nscaling behaviors shift when downstream tasks operate with limited data? 2)\nWhat governs the efficacy of knowledge distillation under such constraints?\nThrough systematic analysis of vision tasks across data regimes (1K-1M\nsamples), we propose the distillation boundary theory, revealing a critical\nturning point in distillation efficiency: 1) Distillation superiority: In\ndata-scarce conditions, distilled models significantly outperform their\nnon-distillation counterparts, efficiently leveraging inherited knowledge to\ncompensate for limited training samples. 2) Pre-training dominance: As\npre-training data increases beyond a critical threshold, non-distilled models\ngradually surpass distilled versions, suggesting diminishing returns from\nknowledge inheritance when sufficient task-specific data becomes available.\nEmpirical validation across various model scales (2.5M to 38M parameters) and\ndata volumes demonstrate these performance inflection points, with error\ndifference curves transitioning from positive to negative values at critical\ndata thresholds, confirming our theoretical predictions. This work redefines\nscaling laws for data-limited regimes, bridging the knowledge gap between\nlarge-scale pretraining and practical downstream adaptation, addressing a\ncritical barrier to understanding vision model scaling behaviors and optimizing\ncomputational resource allocation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u89c6\u89c9\u8fc1\u79fb\u5b66\u4e60\u4e2d\u6570\u636e\u9ad8\u6548\u6269\u5c55\u7684\u5b9e\u7528\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u77e5\u8bc6\u84b8\u998f\u5728\u6570\u636e\u7a00\u7f3a\u548c\u4e30\u5bcc\u6761\u4ef6\u4e0b\u7684\u6027\u80fd\u62d0\u70b9\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9AI\u6a21\u578b\u7684\u6269\u5c55\u89c4\u5f8b\u4e3b\u8981\u5173\u6ce8\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\uff0c\u800c\u5ffd\u7565\u4e86\u6570\u636e\u53d7\u9650\u4e0b\u6e38\u4efb\u52a1\u7684\u6027\u80fd\u6269\u5c55\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u5206\u6790\u4e0d\u540c\u6570\u636e\u89c4\u6a21\uff081K-1M\u6837\u672c\uff09\u7684\u89c6\u89c9\u4efb\u52a1\uff0c\u63d0\u51fa\u84b8\u998f\u8fb9\u754c\u7406\u8bba\uff0c\u7814\u7a76\u77e5\u8bc6\u84b8\u998f\u5728\u6570\u636e\u53d7\u9650\u6761\u4ef6\u4e0b\u7684\u6548\u7387\u3002", "result": "\u53d1\u73b0\u6570\u636e\u7a00\u7f3a\u65f6\u84b8\u998f\u6a21\u578b\u663e\u8457\u4f18\u4e8e\u975e\u84b8\u998f\u6a21\u578b\uff0c\u800c\u9884\u8bad\u7ec3\u6570\u636e\u8d85\u8fc7\u4e34\u754c\u9608\u503c\u540e\u975e\u84b8\u998f\u6a21\u578b\u9010\u6e10\u5360\u4f18\u3002", "conclusion": "\u8be5\u7814\u7a76\u586b\u8865\u4e86\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u4e0e\u4e0b\u6e38\u4efb\u52a1\u9002\u5e94\u4e4b\u95f4\u7684\u77e5\u8bc6\u7a7a\u767d\uff0c\u4e3a\u8d44\u6e90\u4f18\u5316\u5206\u914d\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\u3002"}}
{"id": "2504.13237", "pdf": "https://arxiv.org/pdf/2504.13237", "abs": "https://arxiv.org/abs/2504.13237", "authors": ["Yan Yang", "Yixia Li", "Hongru Wang", "Xuetao Wei", "Jianqiao Yu", "Yun Chen", "Guanhua Chen"], "title": "ImPart: Importance-Aware Delta-Sparsification for Improved Model Compression and Merging in LLMs", "categories": ["cs.CL"], "comment": null, "summary": "With the proliferation of task-specific large language models, delta\ncompression has emerged as a method to mitigate the resource challenges of\ndeploying numerous such models by effectively compressing the delta model\nparameters. Previous delta-sparsification methods either remove parameters\nrandomly or truncate singular vectors directly after singular value\ndecomposition (SVD). However, these methods either disregard parameter\nimportance entirely or evaluate it with too coarse a granularity. In this work,\nwe introduce ImPart, a novel importance-aware delta sparsification approach.\nLeveraging SVD, it dynamically adjusts sparsity ratios of different singular\nvectors based on their importance, effectively retaining crucial task-specific\nknowledge even at high sparsity ratios. Experiments show that ImPart achieves\nstate-of-the-art delta sparsification performance, demonstrating $2\\times$\nhigher compression ratio than baselines at the same performance level. When\nintegrated with existing methods, ImPart sets a new state-of-the-art on delta\nquantization and model merging.", "AI": {"tldr": "ImPart\u662f\u4e00\u79cd\u57fa\u4e8e\u91cd\u8981\u6027\u611f\u77e5\u7684delta\u7a00\u758f\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u4e0d\u540c\u5947\u5f02\u5411\u91cf\u7684\u7a00\u758f\u6bd4\u4f8b\uff0c\u5728\u9ad8\u7a00\u758f\u6bd4\u4e0b\u4ecd\u80fd\u4fdd\u7559\u5173\u952e\u4efb\u52a1\u77e5\u8bc6\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u968f\u7740\u4efb\u52a1\u4e13\u7528\u5927\u8bed\u8a00\u6a21\u578b\u7684\u589e\u591a\uff0cdelta\u538b\u7f29\u6210\u4e3a\u51cf\u5c11\u8d44\u6e90\u6d88\u8017\u7684\u5173\u952e\u65b9\u6cd5\uff0c\u4f46\u73b0\u6709\u7a00\u758f\u5316\u65b9\u6cd5\u5ffd\u89c6\u53c2\u6570\u91cd\u8981\u6027\u6216\u8bc4\u4f30\u7c92\u5ea6\u4e0d\u8db3\u3002", "method": "\u5229\u7528\u5947\u5f02\u503c\u5206\u89e3\uff08SVD\uff09\uff0c\u52a8\u6001\u8c03\u6574\u4e0d\u540c\u5947\u5f02\u5411\u91cf\u7684\u7a00\u758f\u6bd4\u4f8b\uff0c\u57fa\u4e8e\u5176\u91cd\u8981\u6027\u4fdd\u7559\u5173\u952e\u53c2\u6570\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cImPart\u5728\u76f8\u540c\u6027\u80fd\u6c34\u5e73\u4e0b\u538b\u7f29\u6bd4\u63d0\u9ad82\u500d\uff0c\u5e76\u5728delta\u91cf\u5316\u548c\u6a21\u578b\u5408\u5e76\u4e2d\u8fbe\u5230\u65b0\u6700\u4f18\u6027\u80fd\u3002", "conclusion": "ImPart\u901a\u8fc7\u91cd\u8981\u6027\u611f\u77e5\u7684\u7a00\u758f\u5316\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86delta\u538b\u7f29\u7684\u6548\u7387\u4e0e\u6027\u80fd\u3002"}}
{"id": "2504.13263", "pdf": "https://arxiv.org/pdf/2504.13263", "abs": "https://arxiv.org/abs/2504.13263", "authors": ["Xinyue Wang", "Kun Zhou", "Wenyi Wu", "Har Simrat Singh", "Fang Nan", "Songyao Jin", "Aryan Philip", "Saloni Patnaik", "Hou Zhu", "Shivam Singh", "Parjanya Prashant", "Qian Shen", "Biwei Huang"], "title": "Causal-Copilot: An Autonomous Causal Analysis Agent", "categories": ["cs.AI"], "comment": null, "summary": "Causal analysis plays a foundational role in scientific discovery and\nreliable decision-making, yet it remains largely inaccessible to domain experts\ndue to its conceptual and algorithmic complexity. This disconnect between\ncausal methodology and practical usability presents a dual challenge: domain\nexperts are unable to leverage recent advances in causal learning, while causal\nresearchers lack broad, real-world deployment to test and refine their methods.\nTo address this, we introduce Causal-Copilot, an autonomous agent that\noperationalizes expert-level causal analysis within a large language model\nframework. Causal-Copilot automates the full pipeline of causal analysis for\nboth tabular and time-series data -- including causal discovery, causal\ninference, algorithm selection, hyperparameter optimization, result\ninterpretation, and generation of actionable insights. It supports interactive\nrefinement through natural language, lowering the barrier for non-specialists\nwhile preserving methodological rigor. By integrating over 20 state-of-the-art\ncausal analysis techniques, our system fosters a virtuous cycle -- expanding\naccess to advanced causal methods for domain experts while generating rich,\nreal-world applications that inform and advance causal theory. Empirical\nevaluations demonstrate that Causal-Copilot achieves superior performance\ncompared to existing baselines, offering a reliable, scalable, and extensible\nsolution that bridges the gap between theoretical sophistication and real-world\napplicability in causal analysis.", "AI": {"tldr": "Causal-Copilot\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u4e3b\u4ee3\u7406\uff0c\u65e8\u5728\u7b80\u5316\u56e0\u679c\u5206\u6790\u6d41\u7a0b\uff0c\u4e3a\u975e\u4e13\u5bb6\u63d0\u4f9b\u6613\u7528\u5de5\u5177\uff0c\u540c\u65f6\u6574\u5408\u5148\u8fdb\u65b9\u6cd5\u3002", "motivation": "\u56e0\u679c\u5206\u6790\u5bf9\u79d1\u5b66\u53d1\u73b0\u548c\u51b3\u7b56\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5176\u590d\u6742\u6027\u9650\u5236\u4e86\u9886\u57df\u4e13\u5bb6\u7684\u4f7f\u7528\u3002Causal-Copilot\u65e8\u5728\u5f25\u5408\u7406\u8bba\u4e0e\u5b9e\u9645\u5e94\u7528\u4e4b\u95f4\u7684\u9e3f\u6c9f\u3002", "method": "Causal-Copilot\u81ea\u52a8\u5316\u4e86\u56e0\u679c\u5206\u6790\u7684\u5b8c\u6574\u6d41\u7a0b\uff0c\u5305\u62ec\u56e0\u679c\u53d1\u73b0\u3001\u63a8\u65ad\u3001\u7b97\u6cd5\u9009\u62e9\u7b49\uff0c\u5e76\u652f\u6301\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cCausal-Copilot\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u63d0\u4f9b\u4e86\u53ef\u9760\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "Causal-Copilot\u901a\u8fc7\u964d\u4f4e\u4f7f\u7528\u95e8\u69db\u548c\u6574\u5408\u5148\u8fdb\u65b9\u6cd5\uff0c\u63a8\u52a8\u4e86\u56e0\u679c\u5206\u6790\u7684\u5b9e\u9645\u5e94\u7528\u4e0e\u7406\u8bba\u53d1\u5c55\u3002"}}
{"id": "2504.13228", "pdf": "https://arxiv.org/pdf/2504.13228", "abs": "https://arxiv.org/abs/2504.13228", "authors": ["Anna C. M. Th\u00f6ni", "Yoram Bachrach", "Tal Kachman"], "title": "Modelling Mean-Field Games with Neural Ordinary Differential Equations", "categories": ["cs.LG", "cs.GT"], "comment": null, "summary": "Mean-field game theory relies on approximating games that would otherwise\nhave been intractable to model. While the games can be solved analytically via\nthe associated system of partial derivatives, this approach is not model-free,\ncan lead to the loss of the existence or uniqueness of solutions and may suffer\nfrom modelling bias. To reduce the dependency between the model and the game,\nwe combine mean-field game theory with deep learning in the form of neural\nordinary differential equations. The resulting model is data-driven,\nlightweight and can learn extensive strategic interactions that are hard to\ncapture using mean-field theory alone. In addition, the model is based on\nautomatic differentiation, making it more robust and objective than approaches\nbased on finite differences. We highlight the efficiency and flexibility of our\napproach by solving three mean-field games that vary in their complexity,\nobservability and the presence of noise. Using these results, we show that the\nmodel is flexible, lightweight and requires few observations to learn the\ndistribution underlying the data.", "AI": {"tldr": "\u7ed3\u5408\u5747\u503c\u573a\u535a\u5f08\u8bba\u4e0e\u6df1\u5ea6\u5b66\u4e60\uff0c\u63d0\u51fa\u4e00\u79cd\u6570\u636e\u9a71\u52a8\u3001\u8f7b\u91cf\u7ea7\u7684\u6a21\u578b\uff0c\u901a\u8fc7\u81ea\u52a8\u5fae\u5206\u63d0\u9ad8\u9c81\u68d2\u6027\uff0c\u89e3\u51b3\u4e86\u590d\u6742\u535a\u5f08\u4e2d\u7684\u5efa\u6a21\u504f\u5dee\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u5747\u503c\u573a\u535a\u5f08\u8bba\u4f9d\u8d56\u89e3\u6790\u65b9\u6cd5\uff0c\u53ef\u80fd\u5bfc\u81f4\u89e3\u7684\u552f\u4e00\u6027\u6216\u5b58\u5728\u6027\u95ee\u9898\uff0c\u4e14\u6613\u53d7\u5efa\u6a21\u504f\u5dee\u5f71\u54cd\u3002", "method": "\u7ed3\u5408\u5747\u503c\u573a\u535a\u5f08\u8bba\u4e0e\u795e\u7ecf\u5e38\u5fae\u5206\u65b9\u7a0b\uff0c\u5229\u7528\u81ea\u52a8\u5fae\u5206\u6784\u5efa\u6570\u636e\u9a71\u52a8\u6a21\u578b\u3002", "result": "\u6a21\u578b\u5728\u4e09\u79cd\u4e0d\u540c\u590d\u6742\u5ea6\u7684\u5747\u503c\u573a\u535a\u5f08\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u6027\u548c\u7075\u6d3b\u6027\uff0c\u80fd\u901a\u8fc7\u5c11\u91cf\u89c2\u6d4b\u5b66\u4e60\u6570\u636e\u5206\u5e03\u3002", "conclusion": "\u8be5\u6a21\u578b\u8f7b\u91cf\u3001\u7075\u6d3b\u4e14\u9c81\u68d2\uff0c\u9002\u7528\u4e8e\u590d\u6742\u6218\u7565\u4ea4\u4e92\u7684\u5efa\u6a21\u3002"}}
{"id": "2504.13261", "pdf": "https://arxiv.org/pdf/2504.13261", "abs": "https://arxiv.org/abs/2504.13261", "authors": ["Dong Wang"], "title": "CPG-EVAL: A Multi-Tiered Benchmark for Evaluating the Chinese Pedagogical Grammar Competence of Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "cs.SI"], "comment": "12 pages, 1 figure, 3 tables", "summary": "Purpose: The rapid emergence of large language models (LLMs) such as ChatGPT\nhas significantly impacted foreign language education, yet their pedagogical\ngrammar competence remains under-assessed. This paper introduces CPG-EVAL, the\nfirst dedicated benchmark specifically designed to evaluate LLMs' knowledge of\npedagogical grammar within the context of foreign language instruction.\nMethodology: The benchmark comprises five tasks designed to assess grammar\nrecognition, fine-grained grammatical distinction, categorical discrimination,\nand resistance to linguistic interference. Findings: Smaller-scale models can\nsucceed in single language instance tasks, but struggle with multiple instance\ntasks and interference from confusing instances. Larger-scale models show\nbetter resistance to interference but still have significant room for accuracy\nimprovement. The evaluation indicates the need for better instructional\nalignment and more rigorous benchmarks, to effectively guide the deployment of\nLLMs in educational contexts. Value: This study offers the first specialized,\ntheory-driven, multi-tiered benchmark framework for systematically evaluating\nLLMs' pedagogical grammar competence in Chinese language teaching contexts.\nCPG-EVAL not only provides empirical insights for educators, policymakers, and\nmodel developers to better gauge AI's current abilities in educational\nsettings, but also lays the groundwork for future research on improving model\nalignment, enhancing educational suitability, and ensuring informed\ndecision-making concerning LLM integration in foreign language instruction.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86CPG-EVAL\uff0c\u9996\u4e2a\u4e13\u95e8\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5916\u8bed\u6559\u5b66\u4e2d\u8bed\u6cd5\u80fd\u529b\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002", "motivation": "\u968f\u7740ChatGPT\u7b49LLMs\u7684\u5174\u8d77\uff0c\u5176\u5728\u8bed\u6cd5\u6559\u5b66\u4e2d\u7684\u80fd\u529b\u5c1a\u672a\u5f97\u5230\u5145\u5206\u8bc4\u4f30\uff0c\u56e0\u6b64\u9700\u8981\u4e13\u95e8\u7684\u6d4b\u8bd5\u5de5\u5177\u3002", "method": "CPG-EVAL\u5305\u542b\u4e94\u9879\u4efb\u52a1\uff0c\u8bc4\u4f30\u8bed\u6cd5\u8bc6\u522b\u3001\u7ec6\u7c92\u5ea6\u533a\u5206\u3001\u7c7b\u522b\u8fa8\u522b\u53ca\u6297\u5e72\u6270\u80fd\u529b\u3002", "result": "\u5c0f\u89c4\u6a21\u6a21\u578b\u5728\u5355\u4e00\u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u591a\u4efb\u52a1\u548c\u5e72\u6270\u4e0b\u8868\u73b0\u4e0d\u4f73\uff1b\u5927\u89c4\u6a21\u6a21\u578b\u6297\u5e72\u6270\u80fd\u529b\u66f4\u5f3a\uff0c\u4f46\u4ecd\u6709\u63d0\u5347\u7a7a\u95f4\u3002", "conclusion": "\u7814\u7a76\u4e3aLLMs\u5728\u5916\u8bed\u6559\u5b66\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u9996\u4e2a\u7406\u8bba\u9a71\u52a8\u7684\u591a\u5c42\u7ea7\u8bc4\u4f30\u6846\u67b6\uff0c\u5e76\u4e3a\u672a\u6765\u6539\u8fdb\u6a21\u578b\u4e0e\u6559\u80b2\u573a\u666f\u7684\u9002\u914d\u6027\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2504.13314", "pdf": "https://arxiv.org/pdf/2504.13314", "abs": "https://arxiv.org/abs/2504.13314", "authors": ["Timothy Tjhay", "Ricardo J. Bessa", "Jose Paulos"], "title": "On the Definition of Robustness and Resilience of AI Agents for Real-time Congestion Management", "categories": ["cs.AI"], "comment": "IEEE PowerTech 2025 Conference", "summary": "The European Union's Artificial Intelligence (AI) Act defines robustness,\nresilience, and security requirements for high-risk sectors but lacks detailed\nmethodologies for assessment. This paper introduces a novel framework for\nquantitatively evaluating the robustness and resilience of reinforcement\nlearning agents in congestion management. Using the AI-friendly digital\nenvironment Grid2Op, perturbation agents simulate natural and adversarial\ndisruptions by perturbing the input of AI systems without altering the actual\nstate of the environment, enabling the assessment of AI performance under\nvarious scenarios. Robustness is measured through stability and reward impact\nmetrics, while resilience quantifies recovery from performance degradation. The\nresults demonstrate the framework's effectiveness in identifying\nvulnerabilities and improving AI robustness and resilience for critical\napplications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5b9a\u91cf\u8bc4\u4f30\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\u5728\u62e5\u5835\u7ba1\u7406\u4e2d\u9c81\u68d2\u6027\u548c\u97e7\u6027\u7684\u6846\u67b6\uff0c\u586b\u8865\u4e86\u6b27\u76dfAI\u6cd5\u6848\u4e2d\u8bc4\u4f30\u65b9\u6cd5\u7684\u7a7a\u767d\u3002", "motivation": "\u6b27\u76dfAI\u6cd5\u6848\u5bf9\u9ad8\u98ce\u9669\u9886\u57df\u7684\u9c81\u68d2\u6027\u548c\u97e7\u6027\u63d0\u51fa\u4e86\u8981\u6c42\uff0c\u4f46\u7f3a\u4e4f\u5177\u4f53\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u5229\u7528Grid2Op\u6570\u5b57\u73af\u5883\uff0c\u901a\u8fc7\u6270\u52a8\u4ee3\u7406\u6a21\u62df\u81ea\u7136\u548c\u5bf9\u6297\u6027\u5e72\u6270\uff0c\u8bc4\u4f30AI\u7cfb\u7edf\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u7684\u8868\u73b0\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u80fd\u6709\u6548\u8bc6\u522b\u6f0f\u6d1e\u5e76\u63d0\u5347AI\u5728\u5173\u952e\u5e94\u7528\u4e2d\u7684\u9c81\u68d2\u6027\u548c\u97e7\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3aAI\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\u548c\u97e7\u6027\u8bc4\u4f30\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\uff0c\u9002\u7528\u4e8e\u9ad8\u98ce\u9669\u9886\u57df\u3002"}}
{"id": "2504.13229", "pdf": "https://arxiv.org/pdf/2504.13229", "abs": "https://arxiv.org/abs/2504.13229", "authors": ["Yifei Wang", "Qi Liu", "Fuli Min", "Honghao Wang"], "title": "PSG-MAE: Robust Multitask Sleep Event Monitoring using Multichannel PSG Reconstruction and Inter-channel Contrastive Learning", "categories": ["cs.LG"], "comment": "11 pages, 5 figures", "summary": "Polysomnography (PSG) signals are essential for studying sleep processes and\ndiagnosing sleep disorders. Analyzing PSG data through deep neural networks\n(DNNs) for automated sleep monitoring has become increasingly feasible.\nHowever, the limited availability of datasets for certain sleep events often\nleads to DNNs focusing on a single task with a single-sourced training dataset.\nAs a result, these models struggle to transfer to new sleep events and lack\nrobustness when applied to new datasets. To address these challenges, we\npropose PSG-MAE, a mask autoencoder (MAE) based pre-training framework. By\nperforming self-supervised learning on a large volume of unlabeled PSG data,\nPSG-MAE develops a robust feature extraction network that can be broadly\napplied to various sleep event monitoring tasks. Unlike conventional MAEs,\nPSG-MAE generates complementary masks across PSG channels, integrates a\nmultichannel signal reconstruction method, and employs a self-supervised\ninter-channel contrastive learning (ICCL) strategy. This approach enables the\nencoder to capture temporal features from each channel while simultaneously\nlearning latent relationships between channels, thereby enhancing the\nutilization of multichannel information. Experimental results show that PSG-MAE\neffectively captures both temporal details and inter-channel information from\nPSG signals. When the encoder pre-trained through PSG-MAE is fine-tuned with\ndownstream feature decomposition networks, it achieves an accuracy of 83.7% for\nsleep staging and 90.45% for detecting obstructive sleep apnea, which\nhighlights the framework's robustness and broad applicability.", "AI": {"tldr": "PSG-MAE\u662f\u4e00\u79cd\u57fa\u4e8e\u63a9\u7801\u81ea\u7f16\u7801\u5668\u7684\u9884\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u5b66\u4e60\u63d0\u53d6\u591a\u901a\u9053PSG\u4fe1\u53f7\u7684\u65f6\u7a7a\u7279\u5f81\uff0c\u63d0\u5347\u7761\u7720\u4e8b\u4ef6\u76d1\u6d4b\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5728\u7761\u7720\u76d1\u6d4b\u4e2d\u56e0\u6570\u636e\u6709\u9650\u5bfc\u81f4\u7684\u6cdb\u5316\u80fd\u529b\u5dee\u548c\u9c81\u68d2\u6027\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faPSG-MAE\u6846\u67b6\uff0c\u7ed3\u5408\u591a\u901a\u9053\u63a9\u7801\u751f\u6210\u3001\u4fe1\u53f7\u91cd\u5efa\u548c\u901a\u9053\u95f4\u5bf9\u6bd4\u5b66\u4e60\u7b56\u7565\uff0c\u81ea\u76d1\u7763\u5b66\u4e60\u63d0\u53d6\u7279\u5f81\u3002", "result": "\u5728\u7761\u7720\u5206\u671f\u548c\u963b\u585e\u6027\u7761\u7720\u547c\u5438\u6682\u505c\u68c0\u6d4b\u4efb\u52a1\u4e2d\u5206\u522b\u8fbe\u523083.7%\u548c90.45%\u7684\u51c6\u786e\u7387\u3002", "conclusion": "PSG-MAE\u80fd\u6709\u6548\u63d0\u53d6\u591a\u901a\u9053PSG\u4fe1\u53f7\u7684\u65f6\u7a7a\u7279\u5f81\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u9002\u7528\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2504.13284", "pdf": "https://arxiv.org/pdf/2504.13284", "abs": "https://arxiv.org/abs/2504.13284", "authors": ["Derguene Mbaye", "Madoune Robert Seye", "Moussa Diallo", "Mamadou Lamine Ndiaye", "Djiby Sow", "Dimitri Samuel Adjanohoun", "Tatiana Mbengue", "Cheikh Samba Wade", "De Roulet Pablo", "Jean-Claude Baraka Munyaka", "Jerome Chenal"], "title": "Sentiment Analysis on the young people's perception about the mobile Internet costs in Senegal", "categories": ["cs.CL", "cs.SI"], "comment": "19 pages, 14 figures, 10th International Congress on Information and\n  Communication Technology (ICICT 2025)", "summary": "Internet penetration rates in Africa are rising steadily, and mobile Internet\nis getting an even bigger boost with the availability of smartphones. Young\npeople are increasingly using the Internet, especially social networks, and\nSenegal is no exception to this revolution. Social networks have become the\nmain means of expression for young people. Despite this evolution in Internet\naccess, there are few operators on the market, which limits the alternatives\navailable in terms of value for money. In this paper, we will look at how young\npeople feel about the price of mobile Internet in Senegal, in relation to the\nperceived quality of the service, through their comments on social networks. We\nscanned a set of Twitter and Facebook comments related to the subject and\napplied a sentiment analysis model to gather their general feelings.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u585e\u5185\u52a0\u5c14\u5e74\u8f7b\u4eba\u5bf9\u79fb\u52a8\u4e92\u8054\u7f51\u4ef7\u683c\u4e0e\u670d\u52a1\u8d28\u91cf\u611f\u77e5\u7684\u6001\u5ea6\uff0c\u901a\u8fc7\u5206\u6790\u793e\u4ea4\u5a92\u4f53\u8bc4\u8bba\u5e76\u5e94\u7528\u60c5\u611f\u5206\u6790\u6a21\u578b\u3002", "motivation": "\u968f\u7740\u975e\u6d32\u4e92\u8054\u7f51\u666e\u53ca\u7387\u4e0a\u5347\uff0c\u5e74\u8f7b\u4eba\u5bf9\u79fb\u52a8\u4e92\u8054\u7f51\u7684\u9700\u6c42\u589e\u52a0\uff0c\u4f46\u5e02\u573a\u8fd0\u8425\u5546\u6709\u9650\uff0c\u5bfc\u81f4\u6027\u4ef7\u6bd4\u9009\u62e9\u53d7\u9650\u3002\u7814\u7a76\u65e8\u5728\u4e86\u89e3\u5e74\u8f7b\u4eba\u5bf9\u4ef7\u683c\u4e0e\u670d\u52a1\u7684\u611f\u53d7\u3002", "method": "\u901a\u8fc7\u626b\u63cfTwitter\u548cFacebook\u4e0a\u4e0e\u4e3b\u9898\u76f8\u5173\u7684\u8bc4\u8bba\uff0c\u5e76\u5e94\u7528\u60c5\u611f\u5206\u6790\u6a21\u578b\u6765\u6536\u96c6\u7528\u6237\u7684\u666e\u904d\u60c5\u7eea\u3002", "result": "\u60c5\u611f\u5206\u6790\u7ed3\u679c\u663e\u793a\u5e74\u8f7b\u4eba\u5bf9\u79fb\u52a8\u4e92\u8054\u7f51\u4ef7\u683c\u4e0e\u670d\u52a1\u8d28\u91cf\u7684\u6001\u5ea6\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u585e\u5185\u52a0\u5c14\u5e74\u8f7b\u4eba\u5bf9\u79fb\u52a8\u4e92\u8054\u7f51\u4ef7\u683c\u4e0e\u670d\u52a1\u7684\u666e\u904d\u60c5\u7eea\uff0c\u4e3a\u5e02\u573a\u6539\u8fdb\u63d0\u4f9b\u4e86\u53c2\u8003\u3002"}}
{"id": "2504.13359", "pdf": "https://arxiv.org/pdf/2504.13359", "abs": "https://arxiv.org/abs/2504.13359", "authors": ["Mehmet Hamza Erol", "Batu El", "Mirac Suzgun", "Mert Yuksekgonul", "James Zou"], "title": "Cost-of-Pass: An Economic Framework for Evaluating Language Models", "categories": ["cs.AI", "cs.CL"], "comment": "Code is available at: https://github.com/mhamzaerol/Cost-of-Pass", "summary": "The widespread adoption of AI systems in the economy hinges on their ability\nto generate economic value that outweighs their inference costs. Evaluating\nthis tradeoff requires metrics that account for both performance and costs. We\npropose a framework grounded in production theory for evaluating language\nmodels by combining accuracy and inference cost. We introduce \"cost-of-pass\",\nthe expected monetary cost of generating a correct solution. We then define the\n\"frontier cost-of-pass\" as the minimum cost-of-pass achievable across available\nmodels or the \"human-expert, using the approximate cost of hiring an expert.\nOur analysis reveals distinct economic insights. First, lightweight models are\nmost cost-effective for basic quantitative tasks, large models for\nknowledge-intensive ones, and reasoning models for complex quantitative\nproblems, despite higher per-token costs. Second, tracking this frontier\ncost-of-pass over the past year reveals significant progress, particularly for\ncomplex quantitative tasks where the cost has roughly halved every few months.\nThird, to trace key innovations driving this progress, we examine\ncounterfactual frontiers: estimates of cost-efficiency without specific model\nclasses. We find that innovations in lightweight, large, and reasoning models\nhave been essential for pushing the frontier in basic quantitative,\nknowledge-intensive, and complex quantitative tasks, respectively. Finally, we\nassess the cost-reductions afforded by common inference-time techniques like\nmajority voting and self-refinement, finding that their marginal accuracy gains\nrarely justify their costs. Our findings underscore that complementary\nmodel-level innovations are the primary drivers of cost-efficiency, and our\neconomic framework provides a principled tool for measuring this progress and\nguiding deployment.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u751f\u4ea7\u7406\u8bba\u7684\u6846\u67b6\uff0c\u7ed3\u5408\u51c6\u786e\u6027\u548c\u63a8\u7406\u6210\u672c\u8bc4\u4f30\u8bed\u8a00\u6a21\u578b\u7684\u7ecf\u6d4e\u4ef7\u503c\uff0c\u5e76\u5f15\u5165\u4e86\u201ccost-of-pass\u201d\u548c\u201cfrontier cost-of-pass\u201d\u6307\u6807\u3002\u7814\u7a76\u53d1\u73b0\u4e0d\u540c\u6a21\u578b\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u6210\u672c\u6548\u76ca\u5dee\u5f02\u663e\u8457\uff0c\u5e76\u63ed\u793a\u4e86\u63a8\u52a8\u6210\u672c\u6548\u7387\u7684\u5173\u952e\u521b\u65b0\u3002", "motivation": "\u8bc4\u4f30AI\u7cfb\u7edf\u5728\u7ecf\u6d4e\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\u9700\u8981\u8861\u91cf\u5176\u6027\u80fd\u4e0e\u6210\u672c\u7684\u5e73\u8861\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u7edf\u4e00\u7684\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u751f\u4ea7\u7406\u8bba\u7684\u6846\u67b6\uff0c\u7ed3\u5408\u51c6\u786e\u6027\u548c\u63a8\u7406\u6210\u672c\u5b9a\u4e49\u201ccost-of-pass\u201d\u548c\u201cfrontier cost-of-pass\u201d\uff0c\u5e76\u901a\u8fc7\u5206\u6790\u6a21\u578b\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u548c\u6210\u672c\u6548\u76ca\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u8f7b\u91cf\u7ea7\u6a21\u578b\u5728\u57fa\u7840\u5b9a\u91cf\u4efb\u52a1\u4e2d\u6700\u5177\u6210\u672c\u6548\u76ca\uff0c\u5927\u578b\u6a21\u578b\u9002\u7528\u4e8e\u77e5\u8bc6\u5bc6\u96c6\u578b\u4efb\u52a1\uff0c\u63a8\u7406\u6a21\u578b\u9002\u7528\u4e8e\u590d\u6742\u5b9a\u91cf\u4efb\u52a1\u3002\u8fc7\u53bb\u4e00\u5e74\u4e2d\uff0c\u590d\u6742\u5b9a\u91cf\u4efb\u52a1\u7684\u6210\u672c\u6548\u7387\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u6a21\u578b\u7ea7\u522b\u7684\u521b\u65b0\u662f\u63a8\u52a8\u6210\u672c\u6548\u7387\u7684\u4e3b\u8981\u9a71\u52a8\u529b\uff0c\u63d0\u51fa\u7684\u7ecf\u6d4e\u6846\u67b6\u4e3a\u8861\u91cf\u8fdb\u5c55\u548c\u6307\u5bfc\u90e8\u7f72\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u5de5\u5177\u3002"}}
{"id": "2504.13233", "pdf": "https://arxiv.org/pdf/2504.13233", "abs": "https://arxiv.org/abs/2504.13233", "authors": ["Alireza Rafiei", "Gari D. Clifford", "Nasim Katebi"], "title": "Auto-FEDUS: Autoregressive Generative Modeling of Doppler Ultrasound Signals from Fetal Electrocardiograms", "categories": ["cs.LG"], "comment": "AAAI 2025 Workshop on Large Language Models and Generative AI for\n  Health", "summary": "Fetal health monitoring through one-dimensional Doppler ultrasound (DUS)\nsignals offers a cost-effective and accessible approach that is increasingly\ngaining interest. Despite its potential, the development of machine learning\nbased techniques to assess the health condition of mothers and fetuses using\nDUS signals remains limited. This scarcity is primarily due to the lack of\nextensive DUS datasets with a reliable reference for interpretation and data\nimbalance across different gestational ages. In response, we introduce a novel\nautoregressive generative model designed to map fetal electrocardiogram (FECG)\nsignals to corresponding DUS waveforms (Auto-FEDUS). By leveraging a neural\ntemporal network based on dilated causal convolutions that operate directly on\nthe waveform level, the model effectively captures both short and long-range\ndependencies within the signals, preserving the integrity of generated data.\nCross-subject experiments demonstrate that Auto-FEDUS outperforms conventional\ngenerative architectures across both time and frequency domain evaluations,\nproducing DUS signals that closely resemble the morphology of their real\ncounterparts. The realism of these synthesized signals was further gauged using\na quality assessment model, which classified all as good quality, and a heart\nrate estimation model, which produced comparable results for generated and real\ndata, with a Bland-Altman limit of 4.5 beats per minute. This advancement\noffers a promising solution for mitigating limited data availability and\nenhancing the training of DUS-based fetal models, making them more effective\nand generalizable.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u81ea\u56de\u5f52\u751f\u6210\u6a21\u578b\uff08Auto-FEDUS\uff09\u7684\u65b9\u6cd5\uff0c\u5c06\u80ce\u513f\u5fc3\u7535\u56fe\u4fe1\u53f7\u6620\u5c04\u5230\u591a\u666e\u52d2\u8d85\u58f0\u6ce2\u5f62\uff0c\u89e3\u51b3\u4e86\u6570\u636e\u7a00\u7f3a\u548c\u5931\u8861\u95ee\u9898\u3002", "motivation": "\u591a\u666e\u52d2\u8d85\u58f0\u4fe1\u53f7\uff08DUS\uff09\u5728\u80ce\u513f\u5065\u5eb7\u76d1\u6d4b\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u7f3a\u4e4f\u5927\u89c4\u6a21\u6570\u636e\u96c6\u548c\u673a\u5668\u5b66\u4e60\u6280\u672f\u7684\u652f\u6301\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u6269\u5f20\u56e0\u679c\u5377\u79ef\u7684\u795e\u7ecf\u65f6\u95f4\u7f51\u7edc\uff0c\u76f4\u63a5\u5728\u6ce2\u5f62\u7ea7\u522b\u5efa\u6a21\u4fe1\u53f7\u7684\u957f\u77ed\u671f\u4f9d\u8d56\u5173\u7cfb\u3002", "result": "Auto-FEDUS\u5728\u65f6\u95f4\u548c\u9891\u57df\u8bc4\u4f30\u4e2d\u4f18\u4e8e\u4f20\u7edf\u751f\u6210\u6a21\u578b\uff0c\u751f\u6210\u7684\u4fe1\u53f7\u8d28\u91cf\u9ad8\uff0c\u5fc3\u7387\u4f30\u8ba1\u7ed3\u679c\u4e0e\u771f\u5b9e\u6570\u636e\u63a5\u8fd1\u3002", "conclusion": "Auto-FEDUS\u4e3aDUS\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u7684\u8bad\u7ec3\u6548\u679c\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2504.13367", "pdf": "https://arxiv.org/pdf/2504.13367", "abs": "https://arxiv.org/abs/2504.13367", "authors": ["Xiao Pu", "Michael Saxon", "Wenyue Hua", "William Yang Wang"], "title": "THOUGHTTERMINATOR: Benchmarking, Calibrating, and Mitigating Overthinking in Reasoning Models", "categories": ["cs.CL"], "comment": null, "summary": "Reasoning models have demonstrated impressive performance on difficult tasks\nthat traditional language models struggle at. However, many are plagued with\nthe problem of overthinking--generating large amounts of unnecessary tokens\nwhich don't improve accuracy on a question. We introduce approximate measures\nof problem-level difficulty and demonstrate that a clear relationship between\nproblem difficulty and optimal token spend exists, and evaluate how well\ncalibrated a variety of reasoning models are in terms of efficiently allocating\nthe optimal token count. We find that in general, reasoning models are poorly\ncalibrated, particularly on easy problems. To evaluate calibration on easy\nquestions we introduce DUMB500, a dataset of extremely easy math, reasoning,\ncode, and task problems, and jointly evaluate reasoning model on these simple\nexamples and extremely difficult examples from existing frontier benchmarks on\nthe same task domain. Finally, we introduce THOUGHTTERMINATOR, a training-free\nblack box decoding technique that significantly improves reasoning model\ncalibration.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u63a8\u7406\u6a21\u578b\u5728\u4efb\u52a1\u4e2d\u751f\u6210\u8fc7\u591a\u4e0d\u5fc5\u8981\u6807\u8bb0\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u95ee\u9898\u96be\u5ea6\u4e0e\u6700\u4f18\u6807\u8bb0\u6d88\u8017\u7684\u5173\u7cfb\uff0c\u5e76\u8bc4\u4f30\u4e86\u6a21\u578b\u7684\u6821\u51c6\u80fd\u529b\u3002\u901a\u8fc7DUMB500\u6570\u636e\u96c6\u548cTHOUGHTTERMINATOR\u6280\u672f\uff0c\u6539\u8fdb\u4e86\u6a21\u578b\u7684\u6821\u51c6\u6548\u679c\u3002", "motivation": "\u89e3\u51b3\u63a8\u7406\u6a21\u578b\u5728\u7b80\u5355\u95ee\u9898\u4e0a\u751f\u6210\u8fc7\u591a\u4e0d\u5fc5\u8981\u6807\u8bb0\u7684\u95ee\u9898\uff0c\u63a2\u7d22\u95ee\u9898\u96be\u5ea6\u4e0e\u6807\u8bb0\u6d88\u8017\u7684\u5173\u7cfb\u3002", "method": "\u5f15\u5165\u95ee\u9898\u96be\u5ea6\u8fd1\u4f3c\u5ea6\u91cf\uff0c\u8bc4\u4f30\u6a21\u578b\u6821\u51c6\u80fd\u529b\uff0c\u63d0\u51faDUMB500\u6570\u636e\u96c6\u548cTHOUGHTTERMINATOR\u89e3\u7801\u6280\u672f\u3002", "result": "\u63a8\u7406\u6a21\u578b\u666e\u904d\u6821\u51c6\u4e0d\u4f73\uff0c\u5c24\u5176\u5728\u7b80\u5355\u95ee\u9898\u4e0a\uff1bTHOUGHTTERMINATOR\u663e\u8457\u6539\u5584\u4e86\u6821\u51c6\u6548\u679c\u3002", "conclusion": "\u95ee\u9898\u96be\u5ea6\u4e0e\u6807\u8bb0\u6d88\u8017\u76f8\u5173\uff0cTHOUGHTTERMINATOR\u662f\u4e00\u79cd\u6709\u6548\u7684\u6539\u8fdb\u65b9\u6cd5\u3002"}}
{"id": "2504.13360", "pdf": "https://arxiv.org/pdf/2504.13360", "abs": "https://arxiv.org/abs/2504.13360", "authors": ["R\u0103zvan Diaconescu"], "title": "In between myth and reality: AI for math -- a case study in category theory", "categories": ["cs.AI", "math.HO", "math.LO"], "comment": null, "summary": "Recently, there is an increasing interest in understanding the performance of\nAI systems in solving math problems. A multitude of tests have been performed,\nwith mixed conclusions. In this paper we discuss an experiment we have made in\nthe direction of mathematical research, with two of the most prominent\ncontemporary AI systems. One of the objective of this experiment is to get an\nunderstanding of how AI systems can assist mathematical research. Another\nobjective is to support the AI systems developers by formulating suggestions\nfor directions of improvement.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u4e24\u79cd\u4e3b\u6d41AI\u7cfb\u7edf\u5728\u89e3\u51b3\u6570\u5b66\u95ee\u9898\u4e2d\u7684\u8868\u73b0\uff0c\u65e8\u5728\u4e86\u89e3AI\u5982\u4f55\u8f85\u52a9\u6570\u5b66\u7814\u7a76\uff0c\u5e76\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u6539\u8fdb\u5efa\u8bae\u3002", "motivation": "\u7814\u7a76AI\u7cfb\u7edf\u5728\u6570\u5b66\u95ee\u9898\u4e2d\u7684\u8868\u73b0\uff0c\u4ee5\u63a2\u7d22\u5176\u5728\u6570\u5b66\u7814\u7a76\u4e2d\u7684\u8f85\u52a9\u6f5c\u529b\uff0c\u5e76\u4e3aAI\u5f00\u53d1\u8005\u63d0\u4f9b\u4f18\u5316\u65b9\u5411\u3002", "method": "\u901a\u8fc7\u5b9e\u9a8c\u6d4b\u8bd5\u4e24\u79cd\u4e3b\u6d41AI\u7cfb\u7edf\u5728\u6570\u5b66\u95ee\u9898\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u5b9e\u9a8c\u5f97\u51fa\u4e86\u6df7\u5408\u7ed3\u8bba\uff0c\u8868\u660eAI\u7cfb\u7edf\u5728\u6570\u5b66\u7814\u7a76\u4e2d\u6709\u6f5c\u529b\u4f46\u4ecd\u9700\u6539\u8fdb\u3002", "conclusion": "AI\u7cfb\u7edf\u53ef\u4ee5\u8f85\u52a9\u6570\u5b66\u7814\u7a76\uff0c\u4f46\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u4ee5\u63d0\u5347\u6027\u80fd\u3002"}}
{"id": "2504.13234", "pdf": "https://arxiv.org/pdf/2504.13234", "abs": "https://arxiv.org/abs/2504.13234", "authors": ["Hanyu Zhang", "Zhen Xing", "Wenxuan Yang", "Chenxi Ma", "Weimin Tan", "Bo Yan"], "title": "Non-Uniform Class-Wise Coreset Selection: Characterizing Category Difficulty for Data-Efficient Transfer Learning", "categories": ["cs.LG", "cs.AI"], "comment": "11pages", "summary": "As transfer learning models and datasets grow larger, efficient adaptation\nand storage optimization have become critical needs. Coreset selection\naddresses these challenges by identifying and retaining the most informative\nsamples, constructing a compact subset for target domain training. However,\ncurrent methods primarily rely on instance-level difficulty assessments,\noverlooking crucial category-level characteristics and consequently\nunder-representing minority classes. To overcome this limitation, we propose\nNon-Uniform Class-Wise Coreset Selection (NUCS), a novel framework that\nintegrates both class-level and instance-level criteria. NUCS automatically\nallocates data selection budgets for each class based on intrinsic category\ndifficulty and adaptively selects samples within optimal difficulty ranges. By\nexplicitly incorporating category-specific insights, our approach achieves a\nmore balanced and representative coreset, addressing key shortcomings of prior\nmethods. Comprehensive theoretical analysis validates the rationale behind\nadaptive budget allocation and sample selection, while extensive experiments\nacross 14 diverse datasets and model architectures demonstrate NUCS's\nconsistent improvements over state-of-the-art methods, achieving superior\naccuracy and computational efficiency. Notably, on CIFAR100 and Food101, NUCS\nmatches full-data training accuracy while retaining just 30% of samples and\nreducing computation time by 60%. Our work highlights the importance of\ncharacterizing category difficulty in coreset selection, offering a robust and\ndata-efficient solution for transfer learning.", "AI": {"tldr": "NUCS\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u7c7b\u522b\u7ea7\u548c\u5b9e\u4f8b\u7ea7\u6807\u51c6\u7684\u975e\u5747\u5300\u7c7b\u522b\u7ea7\u6838\u5fc3\u96c6\u9009\u62e9\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5ffd\u89c6\u7c7b\u522b\u7279\u6027\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u968f\u7740\u8fc1\u79fb\u5b66\u4e60\u6a21\u578b\u548c\u6570\u636e\u96c6\u7684\u6269\u5927\uff0c\u9ad8\u6548\u9002\u5e94\u548c\u5b58\u50a8\u4f18\u5316\u6210\u4e3a\u5173\u952e\u9700\u6c42\u3002\u73b0\u6709\u6838\u5fc3\u96c6\u9009\u62e9\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u5b9e\u4f8b\u7ea7\u96be\u5ea6\u8bc4\u4f30\uff0c\u5ffd\u7565\u4e86\u7c7b\u522b\u7ea7\u7279\u6027\uff0c\u5bfc\u81f4\u5c11\u6570\u7c7b\u522b\u4ee3\u8868\u6027\u4e0d\u8db3\u3002", "method": "NUCS\u6846\u67b6\u7ed3\u5408\u7c7b\u522b\u7ea7\u548c\u5b9e\u4f8b\u7ea7\u6807\u51c6\uff0c\u81ea\u52a8\u4e3a\u6bcf\u4e2a\u7c7b\u522b\u5206\u914d\u6570\u636e\u9009\u62e9\u9884\u7b97\uff0c\u5e76\u5728\u6700\u4f18\u96be\u5ea6\u8303\u56f4\u5185\u81ea\u9002\u5e94\u9009\u62e9\u6837\u672c\u3002", "result": "\u572814\u4e2a\u6570\u636e\u96c6\u548c\u6a21\u578b\u67b6\u6784\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cNUCS\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5728CIFAR100\u548cFood101\u4e0a\u4ec5\u4fdd\u755930%\u6837\u672c\u5373\u53ef\u5339\u914d\u5168\u6570\u636e\u8bad\u7ec3\u7cbe\u5ea6\uff0c\u8ba1\u7b97\u65f6\u95f4\u51cf\u5c1160%\u3002", "conclusion": "NUCS\u5f3a\u8c03\u4e86\u7c7b\u522b\u96be\u5ea6\u5728\u6838\u5fc3\u96c6\u9009\u62e9\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u8fc1\u79fb\u5b66\u4e60\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u6570\u636e\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.13425", "pdf": "https://arxiv.org/pdf/2504.13425", "abs": "https://arxiv.org/abs/2504.13425", "authors": ["Grace Byun", "Shinsun Lee", "Nayoung Choi", "Jinho Choi"], "title": "Secure Multifaceted-RAG for Enterprise: Hybrid Knowledge Retrieval with Security Filtering", "categories": ["cs.CL"], "comment": null, "summary": "Existing Retrieval-Augmented Generation (RAG) systems face challenges in\nenterprise settings due to limited retrieval scope and data security risks.\nWhen relevant internal documents are unavailable, the system struggles to\ngenerate accurate and complete responses. Additionally, using closed-source\nLarge Language Models (LLMs) raises concerns about exposing proprietary\ninformation. To address these issues, we propose the Secure Multifaceted-RAG\n(SecMulti-RAG) framework, which retrieves not only from internal documents but\nalso from two supplementary sources: pre-generated expert knowledge for\nanticipated queries and on-demand external LLM-generated knowledge. To mitigate\nsecurity risks, we adopt a local open-source generator and selectively utilize\nexternal LLMs only when prompts are deemed safe by a filtering mechanism. This\napproach enhances completeness, prevents data leakage, and reduces costs. In\nour evaluation on a report generation task in the automotive industry,\nSecMulti-RAG significantly outperforms traditional RAG - achieving 79.3 to 91.9\npercent win rates across correctness, richness, and helpfulness in LLM-based\nevaluation, and 56.3 to 70.4 percent in human evaluation. This highlights\nSecMulti-RAG as a practical and secure solution for enterprise RAG.", "AI": {"tldr": "SecMulti-RAG\u6846\u67b6\u901a\u8fc7\u591a\u6e90\u68c0\u7d22\u548c\u672c\u5730\u5f00\u6e90\u751f\u6210\u5668\u89e3\u51b3\u4f01\u4e1aRAG\u7cfb\u7edf\u7684\u68c0\u7d22\u8303\u56f4\u9650\u5236\u548c\u6570\u636e\u5b89\u5168\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709RAG\u7cfb\u7edf\u5728\u4f01\u4e1a\u73af\u5883\u4e2d\u56e0\u68c0\u7d22\u8303\u56f4\u6709\u9650\u548c\u6570\u636e\u5b89\u5168\u98ce\u9669\u8868\u73b0\u4e0d\u4f73\u3002", "method": "SecMulti-RAG\u4ece\u5185\u90e8\u6587\u6863\u3001\u9884\u751f\u6210\u4e13\u5bb6\u77e5\u8bc6\u548c\u5916\u90e8LLM\u751f\u6210\u77e5\u8bc6\u4e2d\u68c0\u7d22\uff0c\u5e76\u91c7\u7528\u672c\u5730\u5f00\u6e90\u751f\u6210\u5668\u548c\u5b89\u5168\u8fc7\u6ee4\u673a\u5236\u3002", "result": "\u5728\u6c7d\u8f66\u884c\u4e1a\u62a5\u544a\u751f\u6210\u4efb\u52a1\u4e2d\uff0cSecMulti-RAG\u5728\u6b63\u786e\u6027\u3001\u4e30\u5bcc\u6027\u548c\u5b9e\u7528\u6027\u4e0a\u663e\u8457\u4f18\u4e8e\u4f20\u7edfRAG\u3002", "conclusion": "SecMulti-RAG\u662f\u4f01\u4e1aRAG\u7684\u5b9e\u7528\u4e14\u5b89\u5168\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.13443", "pdf": "https://arxiv.org/pdf/2504.13443", "abs": "https://arxiv.org/abs/2504.13443", "authors": ["Michael J. Yuan", "Carlos Campoy", "Sydney Lai", "James Snewin", "Ju Long"], "title": "Trust, but verify", "categories": ["cs.AI", "cs.DC", "cs.MA", "econ.GN", "q-fin.EC"], "comment": null, "summary": "Decentralized AI agent networks, such as Gaia, allows individuals to run\ncustomized LLMs on their own computers and then provide services to the public.\nHowever, in order to maintain service quality, the network must verify that\nindividual nodes are running their designated LLMs. In this paper, we\ndemonstrate that in a cluster of mostly honest nodes, we can detect nodes that\nrun unauthorized or incorrect LLM through social consensus of its peers. We\nwill discuss the algorithm and experimental data from the Gaia network. We will\nalso discuss the intersubjective validation system, implemented as an\nEigenLayer AVS to introduce financial incentives and penalties to encourage\nhonest behavior from LLM nodes.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u793e\u4ea4\u5171\u8bc6\u68c0\u6d4b\u53bb\u4e2d\u5fc3\u5316AI\u7f51\u7edc\u4e2d\u8fd0\u884c\u672a\u7ecf\u6388\u6743\u6216\u9519\u8befLLM\u7684\u8282\u70b9\u7684\u65b9\u6cd5\uff0c\u5e76\u5f15\u5165EigenLayer AVS\u7cfb\u7edf\u4ee5\u7ecf\u6d4e\u6fc0\u52b1\u548c\u60e9\u7f5a\u673a\u5236\u4fc3\u8fdb\u8282\u70b9\u8bda\u5b9e\u884c\u4e3a\u3002", "motivation": "\u5728\u53bb\u4e2d\u5fc3\u5316AI\u7f51\u7edc\u4e2d\uff0c\u786e\u4fdd\u8282\u70b9\u8fd0\u884c\u6b63\u786e\u7684LLM\u4ee5\u7ef4\u6301\u670d\u52a1\u8d28\u91cf\u662f\u4e00\u4e2a\u5173\u952e\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u793e\u4ea4\u5171\u8bc6\u673a\u5236\u68c0\u6d4b\u5f02\u5e38\u8282\u70b9\uff0c\u5e76\u5229\u7528EigenLayer AVS\u7cfb\u7edf\u5f15\u5165\u7ecf\u6d4e\u6fc0\u52b1\u548c\u60e9\u7f5a\u3002", "result": "\u5b9e\u9a8c\u6570\u636e\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u8bc6\u522b\u5e76\u5904\u7406\u8fd0\u884c\u9519\u8befLLM\u7684\u8282\u70b9\u3002", "conclusion": "\u793e\u4ea4\u5171\u8bc6\u4e0e\u7ecf\u6d4e\u6fc0\u52b1\u76f8\u7ed3\u5408\u7684\u65b9\u6cd5\u53ef\u6709\u6548\u63d0\u5347\u53bb\u4e2d\u5fc3\u5316AI\u7f51\u7edc\u7684\u670d\u52a1\u8d28\u91cf\u548c\u5b89\u5168\u6027\u3002"}}
{"id": "2504.13236", "pdf": "https://arxiv.org/pdf/2504.13236", "abs": "https://arxiv.org/abs/2504.13236", "authors": ["Aleksandr Mikhalev", "Aleksandr Katrutsa", "Konstantin Sozykin", "Ivan Oseledets"], "title": "NNTile: a machine learning framework capable of training extremely large GPT language models on a single node", "categories": ["cs.LG", "cs.MS"], "comment": null, "summary": "This study presents an NNTile framework for training large deep neural\nnetworks in heterogeneous clusters. The NNTile is based on a StarPU library,\nwhich implements task-based parallelism and schedules all provided tasks onto\nall available processing units (CPUs and GPUs). It means that a particular\noperation, necessary to train a large neural network, can be performed on any\nof the CPU cores or GPU devices, depending on automatic scheduling decisions.\nSuch an approach shifts the burden of deciding where to compute and when to\ncommunicate from a human being to an automatic decision maker, whether a simple\ngreedy heuristic or a complex AI-based software. The performance of the\npresented tool for training large language models is demonstrated in extensive\nnumerical experiments.", "AI": {"tldr": "NNTile\u6846\u67b6\u5229\u7528StarPU\u5e93\u5b9e\u73b0\u4efb\u52a1\u5e76\u884c\uff0c\u81ea\u52a8\u8c03\u5ea6\u4efb\u52a1\u5230CPU\u6216GPU\uff0c\u7528\u4e8e\u8bad\u7ec3\u5927\u578b\u795e\u7ecf\u7f51\u7edc\u3002", "motivation": "\u89e3\u51b3\u5728\u5f02\u6784\u96c6\u7fa4\u4e2d\u8bad\u7ec3\u5927\u578b\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u65f6\uff0c\u4eba\u5de5\u8c03\u5ea6\u4efb\u52a1\u5230\u4e0d\u540c\u5904\u7406\u5355\u5143\u7684\u590d\u6742\u6027\u3002", "method": "\u57fa\u4e8eStarPU\u5e93\u7684\u4efb\u52a1\u5e76\u884c\u548c\u81ea\u52a8\u8c03\u5ea6\u673a\u5236\uff0c\u652f\u6301CPU\u548cGPU\u7684\u5f02\u6784\u8ba1\u7b97\u3002", "result": "\u901a\u8fc7\u5927\u91cf\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u5de5\u5177\u5728\u8bad\u7ec3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u9ad8\u6548\u6027\u80fd\u3002", "conclusion": "NNTile\u6846\u67b6\u901a\u8fc7\u81ea\u52a8\u8c03\u5ea6\u4efb\u52a1\uff0c\u663e\u8457\u7b80\u5316\u4e86\u5927\u578b\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u7684\u590d\u6742\u6027\uff0c\u63d0\u5347\u4e86\u6548\u7387\u3002"}}
{"id": "2504.13439", "pdf": "https://arxiv.org/pdf/2504.13439", "abs": "https://arxiv.org/abs/2504.13439", "authors": ["Grace Byun", "Jinho Choi"], "title": "D-GEN: Automatic Distractor Generation and Evaluation for Reliable Assessment of Generative Model", "categories": ["cs.CL"], "comment": null, "summary": "Evaluating generative models with open-ended generation is challenging due to\ninconsistencies in response formats. Multiple-choice (MC) evaluation mitigates\nthis issue, but generating high-quality distractors is time-consuming and\nlabor-intensive. We introduce D-GEN, the first open-source distractor generator\nmodel that transforms open-ended data into an MC format. To evaluate distractor\nquality, we propose two novel methods: (1) ranking alignment, ensuring\ngenerated distractors retain the discriminatory power of ground-truth\ndistractors, and (2) entropy analysis, comparing model confidence\ndistributions. Our results show that D-GEN preserves ranking consistency\n(Spearman's rho 0.99, Kendall's tau 0.94) and closely matches the entropy\ndistribution of ground-truth distractors. Human evaluation further confirms the\nfluency, coherence, distractiveness, and incorrectness. Our work advances\nrobust and efficient distractor generation with automated evaluation, setting a\nnew standard for MC evaluation.", "AI": {"tldr": "D-GEN\u662f\u4e00\u4e2a\u5f00\u6e90\u5e72\u6270\u9879\u751f\u6210\u6a21\u578b\uff0c\u5c06\u5f00\u653e\u5f0f\u6570\u636e\u8f6c\u5316\u4e3a\u591a\u9009\u9898\u683c\u5f0f\uff0c\u5e76\u901a\u8fc7\u6392\u540d\u5bf9\u9f50\u548c\u71b5\u5206\u6790\u8bc4\u4f30\u5e72\u6270\u9879\u8d28\u91cf\u3002", "motivation": "\u89e3\u51b3\u5f00\u653e\u5f0f\u751f\u6210\u6a21\u578b\u8bc4\u4f30\u4e2d\u7684\u4e0d\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u540c\u65f6\u51cf\u5c11\u9ad8\u8d28\u91cf\u5e72\u6270\u9879\u751f\u6210\u7684\u65f6\u95f4\u548c\u4eba\u529b\u6210\u672c\u3002", "method": "\u63d0\u51faD-GEN\u6a21\u578b\uff0c\u5229\u7528\u6392\u540d\u5bf9\u9f50\u548c\u71b5\u5206\u6790\u8bc4\u4f30\u5e72\u6270\u9879\u8d28\u91cf\u3002", "result": "D-GEN\u5728\u6392\u540d\u4e00\u81f4\u6027\u548c\u71b5\u5206\u5e03\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4eba\u7c7b\u8bc4\u4f30\u4e5f\u9a8c\u8bc1\u4e86\u5176\u5e72\u6270\u9879\u7684\u6d41\u7545\u6027\u548c\u6709\u6548\u6027\u3002", "conclusion": "D-GEN\u4e3a\u591a\u9009\u9898\u8bc4\u4f30\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u81ea\u52a8\u5316\u7684\u5e72\u6270\u9879\u751f\u6210\u65b9\u6cd5\uff0c\u63a8\u52a8\u4e86\u8be5\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2504.13517", "pdf": "https://arxiv.org/pdf/2504.13517", "abs": "https://arxiv.org/abs/2504.13517", "authors": ["Lihuan Li", "Du Yin", "Hao Xue", "David Lillo-Trynes", "Flora Salim"], "title": "Optimizing Electric Vehicle Charging Station Locations: A Data-driven System with Multi-source Fusion", "categories": ["cs.AI"], "comment": "4-page short paper", "summary": "With the growing electric vehicles (EVs) charging demand, urban planners face\nthe challenges of providing charging infrastructure at optimal locations. For\nexample, range anxiety during long-distance travel and the inadequate\ndistribution of residential charging stations are the major issues many cities\nface. To achieve reasonable estimation and deployment of the charging demand,\nwe develop a data-driven system based on existing EV trips in New South Wales\n(NSW) state, Australia, incorporating multiple factors that enhance the\ngeographical feasibility of recommended charging stations. Our system\nintegrates data sources including EV trip data, geographical data such as route\ndata and Local Government Area (LGA) boundaries, as well as features like fire\nand flood risks, and Points of Interest (POIs). We visualize our results to\nintuitively demonstrate the findings from our data-driven, multi-source fusion\nsystem, and evaluate them through case studies. The outcome of this work can\nprovide a platform for discussion to develop new insights that could be used to\ngive guidance on where to position future EV charging stations.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6570\u636e\u9a71\u52a8\u7684\u7cfb\u7edf\uff0c\u7528\u4e8e\u4f18\u5316\u7535\u52a8\u6c7d\u8f66\u5145\u7535\u7ad9\u7684\u4f4d\u7f6e\u89c4\u5212\uff0c\u7ed3\u5408\u591a\u6e90\u6570\u636e\uff08\u5982\u884c\u7a0b\u3001\u5730\u7406\u548c\u98ce\u9669\u6570\u636e\uff09\u8fdb\u884c\u9700\u6c42\u4f30\u8ba1\u548c\u90e8\u7f72\u3002", "motivation": "\u968f\u7740\u7535\u52a8\u6c7d\u8f66\u5145\u7535\u9700\u6c42\u7684\u589e\u957f\uff0c\u57ce\u5e02\u9762\u4e34\u5145\u7535\u57fa\u7840\u8bbe\u65bd\u5206\u5e03\u4e0d\u5747\u548c\u957f\u9014\u65c5\u884c\u4e2d\u7684\u7eed\u822a\u7126\u8651\u95ee\u9898\uff0c\u9700\u8981\u4f18\u5316\u5145\u7535\u7ad9\u5e03\u5c40\u3002", "method": "\u57fa\u4e8e\u6fb3\u5927\u5229\u4e9a\u65b0\u5357\u5a01\u5c14\u58eb\u5dde\u7684\u7535\u52a8\u6c7d\u8f66\u884c\u7a0b\u6570\u636e\uff0c\u6574\u5408\u5730\u7406\u6570\u636e\uff08\u5982\u8def\u7ebf\u3001LGA\u8fb9\u754c\uff09\u548c\u5176\u4ed6\u7279\u5f81\uff08\u5982\u706b\u707e\u3001\u6d2a\u6c34\u98ce\u9669\u548cPOIs\uff09\uff0c\u5f00\u53d1\u591a\u6e90\u878d\u5408\u7cfb\u7edf\u3002", "result": "\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u8bc4\u4f30\u7cfb\u7edf\uff0c\u53ef\u89c6\u5316\u7ed3\u679c\u76f4\u89c2\u5c55\u793a\u4e86\u5145\u7535\u7ad9\u63a8\u8350\u4f4d\u7f6e\u7684\u5730\u7406\u53ef\u884c\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u672a\u6765\u5145\u7535\u7ad9\u7684\u4f4d\u7f6e\u89c4\u5212\u63d0\u4f9b\u4e86\u6570\u636e\u652f\u6301\u548c\u8ba8\u8bba\u5e73\u53f0\u3002"}}
{"id": "2504.13241", "pdf": "https://arxiv.org/pdf/2504.13241", "abs": "https://arxiv.org/abs/2504.13241", "authors": ["Paul Ghanem", "Michael Potter", "Owen Howell", "Pau Closas", "Alireza Ramezani", "Deniz Erdogmus", "Robert Platt", "Tales Imbiriba"], "title": "Recursive Deep Inverse Reinforcement Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Inferring an adversary's goals from exhibited behavior is crucial for\ncounterplanning and non-cooperative multi-agent systems in domains like\ncybersecurity, military, and strategy games. Deep Inverse Reinforcement\nLearning (IRL) methods based on maximum entropy principles show promise in\nrecovering adversaries' goals but are typically offline, require large batch\nsizes with gradient descent, and rely on first-order updates, limiting their\napplicability in real-time scenarios. We propose an online Recursive Deep\nInverse Reinforcement Learning (RDIRL) approach to recover the cost function\ngoverning the adversary actions and goals. Specifically, we minimize an upper\nbound on the standard Guided Cost Learning (GCL) objective using sequential\nsecond-order Newton updates, akin to the Extended Kalman Filter (EKF), leading\nto a fast (in terms of convergence) learning algorithm. We demonstrate that\nRDIRL is able to recover cost and reward functions of expert agents in standard\nand adversarial benchmark tasks. Experiments on benchmark tasks show that our\nproposed approach outperforms several leading IRL algorithms.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u7ebf\u9012\u5f52\u6df1\u5ea6\u9006\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff08RDIRL\uff09\uff0c\u7528\u4e8e\u5b9e\u65f6\u63a8\u65ad\u5bf9\u624b\u7684\u76ee\u6807\u548c\u6210\u672c\u51fd\u6570\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5728\u975e\u5408\u4f5c\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\uff0c\u5b9e\u65f6\u63a8\u65ad\u5bf9\u624b\u76ee\u6807\u5bf9\u53cd\u89c4\u5212\u548c\u5b9e\u9645\u5e94\u7528\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u591a\u4e3a\u79bb\u7ebf\u4e14\u6548\u7387\u4f4e\u3002", "method": "\u901a\u8fc7\u6700\u5c0f\u5316\u6807\u51c6GCL\u76ee\u6807\u7684\u4e0a\u754c\uff0c\u91c7\u7528\u987a\u5e8f\u4e8c\u9636\u725b\u987f\u66f4\u65b0\uff08\u7c7b\u4f3cEKF\uff09\uff0c\u5b9e\u73b0\u5feb\u901f\u6536\u655b\u7684\u5728\u7ebf\u5b66\u4e60\u7b97\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cRDIRL\u5728\u6807\u51c6\u53ca\u5bf9\u6297\u6027\u57fa\u51c6\u4efb\u52a1\u4e2d\u80fd\u6709\u6548\u6062\u590d\u4e13\u5bb6\u667a\u80fd\u4f53\u7684\u6210\u672c\u548c\u5956\u52b1\u51fd\u6570\uff0c\u6027\u80fd\u4f18\u4e8e\u5176\u4ed6IRL\u7b97\u6cd5\u3002", "conclusion": "RDIRL\u4e3a\u5b9e\u65f6\u573a\u666f\u4e0b\u7684\u5bf9\u624b\u76ee\u6807\u63a8\u65ad\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2504.13471", "pdf": "https://arxiv.org/pdf/2504.13471", "abs": "https://arxiv.org/abs/2504.13471", "authors": ["Jiliang Ni", "Jiachen Pu", "Zhongyi Yang", "Kun Zhou", "Hui Wang", "Xiaoliang Xiao", "Dakui Wang", "Xin Li", "Jingfeng Luo", "Conggang Hu"], "title": "From Large to Super-Tiny: End-to-End Optimization for Cost-Efficient LLMs", "categories": ["cs.CL"], "comment": null, "summary": "In recent years, Large Language Models (LLMs) have significantly advanced\nartificial intelligence by optimizing traditional Natural Language Processing\n(NLP) pipelines, improving performance and generalization. This has spurred\ntheir integration into various systems. Many NLP systems, including ours,\nemploy a \"one-stage\" pipeline directly incorporating LLMs. While effective,\nthis approach incurs substantial costs and latency due to the need for large\nmodel parameters to achieve satisfactory outcomes. This paper introduces a\nthree-stage cost-efficient end-to-end LLM deployment pipeline-including\nprototyping, knowledge transfer, and model compression-to tackle the\ncost-performance dilemma in LLM-based frameworks. Our approach yields a super\ntiny model optimized for cost and performance in online systems, simplifying\nthe system architecture. Initially, by transforming complex tasks into a\nfunction call-based LLM-driven pipeline, an optimal performance prototype\nsystem is constructed to produce high-quality data as a teacher model. The\nsecond stage combine techniques like rejection fine-tuning, reinforcement\nlearning and knowledge distillation to transfer knowledge to a smaller 0.5B\nstudent model, delivering effective performance at minimal cost. The final\nstage applies quantization and pruning to extremely compress model to 0.4B,\nachieving ultra-low latency and cost. The framework's modular design and\ncross-domain capabilities suggest potential applicability in other NLP areas.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e09\u9636\u6bb5\u7684\u9ad8\u6548LLM\u90e8\u7f72\u6d41\u7a0b\uff0c\u5305\u62ec\u539f\u578b\u8bbe\u8ba1\u3001\u77e5\u8bc6\u8f6c\u79fb\u548c\u6a21\u578b\u538b\u7f29\uff0c\u4ee5\u89e3\u51b3LLM\u6846\u67b6\u4e2d\u7684\u6210\u672c-\u6027\u80fd\u77db\u76fe\u3002", "motivation": "\u4f20\u7edf\u5355\u9636\u6bb5LLM\u90e8\u7f72\u6210\u672c\u9ad8\u4e14\u5ef6\u8fdf\u5927\uff0c\u9700\u8981\u4f18\u5316\u4ee5\u5b9e\u73b0\u9ad8\u6548\u90e8\u7f72\u3002", "method": "\u901a\u8fc7\u4e09\u9636\u6bb5\u6d41\u7a0b\uff1a1\uff09\u6784\u5efa\u539f\u578b\u7cfb\u7edf\u751f\u6210\u9ad8\u8d28\u91cf\u6570\u636e\uff1b2\uff09\u7ed3\u5408\u62d2\u7edd\u5fae\u8c03\u3001\u5f3a\u5316\u5b66\u4e60\u548c\u77e5\u8bc6\u84b8\u998f\u5c06\u77e5\u8bc6\u8f6c\u79fb\u5230\u5c0f\u6a21\u578b\uff1b3\uff09\u901a\u8fc7\u91cf\u5316\u548c\u526a\u679d\u8fdb\u4e00\u6b65\u538b\u7f29\u6a21\u578b\u3002", "result": "\u6700\u7ec8\u5f97\u5230\u4e00\u4e2a0.4B\u7684\u8d85\u5c0f\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u8d85\u4f4e\u5ef6\u8fdf\u548c\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u6027\u80fd\u3002", "conclusion": "\u8be5\u6846\u67b6\u7684\u6a21\u5757\u5316\u8bbe\u8ba1\u548c\u8de8\u9886\u57df\u80fd\u529b\u8868\u660e\u5176\u5728\u5176\u4ed6NLP\u9886\u57df\u5177\u6709\u6f5c\u5728\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2504.13554", "pdf": "https://arxiv.org/pdf/2504.13554", "abs": "https://arxiv.org/abs/2504.13554", "authors": ["Xin Tang", "Qian Chen", "Wenjie Weng", "Chao Jin", "Zhang Liu", "Jiacheng Wang", "Geng Sun", "Xiaohuan Li", "Dusit Niyato"], "title": "Task Assignment and Exploration Optimization for Low Altitude UAV Rescue via Generative AI Enhanced Multi-agent Reinforcement Learning", "categories": ["cs.AI", "cs.LG", "cs.RO"], "comment": null, "summary": "Artificial Intelligence (AI)-driven convolutional neural networks enhance\nrescue, inspection, and surveillance tasks performed by low-altitude uncrewed\naerial vehicles (UAVs) and ground computing nodes (GCNs) in unknown\nenvironments. However, their high computational demands often exceed a single\nUAV's capacity, leading to system instability, further exacerbated by the\nlimited and dynamic resources of GCNs. To address these challenges, this paper\nproposes a novel cooperation framework involving UAVs, ground-embedded robots\n(GERs), and high-altitude platforms (HAPs), which enable resource pooling\nthrough UAV-to-GER (U2G) and UAV-to-HAP (U2H) communications to provide\ncomputing services for UAV offloaded tasks. Specifically, we formulate the\nmulti-objective optimization problem of task assignment and exploration\noptimization in UAVs as a dynamic long-term optimization problem. Our objective\nis to minimize task completion time and energy consumption while ensuring\nsystem stability over time. To achieve this, we first employ the Lyapunov\noptimization technique to transform the original problem, with stability\nconstraints, into a per-slot deterministic problem. We then propose an\nalgorithm named HG-MADDPG, which combines the Hungarian algorithm with a\ngenerative diffusion model (GDM)-based multi-agent deep deterministic policy\ngradient (MADDPG) approach. We first introduce the Hungarian algorithm as a\nmethod for exploration area selection, enhancing UAV efficiency in interacting\nwith the environment. We then innovatively integrate the GDM and multi-agent\ndeep deterministic policy gradient (MADDPG) to optimize task assignment\ndecisions, such as task offloading and resource allocation. Simulation results\ndemonstrate the effectiveness of the proposed approach, with significant\nimprovements in task offloading efficiency, latency reduction, and system\nstability compared to baseline methods.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u5408\u4f5c\u6846\u67b6\uff0c\u901a\u8fc7\u65e0\u4eba\u673a\u3001\u5730\u9762\u5d4c\u5165\u5f0f\u673a\u5668\u4eba\uff08GERs\uff09\u548c\u9ad8\u7a7a\u5e73\u53f0\uff08HAPs\uff09\u7684\u8d44\u6e90\u5171\u4eab\uff0c\u4f18\u5316\u4efb\u52a1\u5206\u914d\u548c\u63a2\u7d22\u6548\u7387\uff0c\u4ee5\u51cf\u5c11\u4efb\u52a1\u5b8c\u6210\u65f6\u95f4\u548c\u80fd\u8017\uff0c\u540c\u65f6\u786e\u4fdd\u7cfb\u7edf\u7a33\u5b9a\u6027\u3002", "motivation": "\u73b0\u6709AI\u9a71\u52a8\u7684\u65e0\u4eba\u673a\u548c\u5730\u9762\u8ba1\u7b97\u8282\u70b9\u5728\u672a\u77e5\u73af\u5883\u4e2d\u6267\u884c\u4efb\u52a1\u65f6\uff0c\u56e0\u9ad8\u8ba1\u7b97\u9700\u6c42\u548c\u52a8\u6001\u8d44\u6e90\u9650\u5236\u5bfc\u81f4\u7cfb\u7edf\u4e0d\u7a33\u5b9a\uff0c\u4e9f\u9700\u4e00\u79cd\u9ad8\u6548\u7684\u8d44\u6e90\u534f\u4f5c\u65b9\u6848\u3002", "method": "\u63d0\u51faHG-MADDPG\u7b97\u6cd5\uff0c\u7ed3\u5408\u5308\u7259\u5229\u7b97\u6cd5\u548c\u57fa\u4e8e\u751f\u6210\u6269\u6563\u6a21\u578b\u7684\u591a\u667a\u80fd\u4f53\u6df1\u5ea6\u786e\u5b9a\u6027\u7b56\u7565\u68af\u5ea6\uff08MADDPG\uff09\uff0c\u4f18\u5316\u4efb\u52a1\u5206\u914d\u548c\u8d44\u6e90\u5206\u914d\u51b3\u7b56\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4efb\u52a1\u5378\u8f7d\u6548\u7387\u3001\u5ef6\u8fdf\u964d\u4f4e\u548c\u7cfb\u7edf\u7a33\u5b9a\u6027\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u5408\u4f5c\u6846\u67b6\u548c\u7b97\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u65e0\u4eba\u673a\u4efb\u52a1\u6267\u884c\u4e2d\u7684\u8d44\u6e90\u9650\u5236\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u6574\u4f53\u7cfb\u7edf\u6027\u80fd\u3002"}}
{"id": "2504.13266", "pdf": "https://arxiv.org/pdf/2504.13266", "abs": "https://arxiv.org/abs/2504.13266", "authors": ["Zichao Yue", "Chenhui Deng", "Zhiru Zhang"], "title": "Graph Learning at Scale: Characterizing and Optimizing Pre-Propagation GNNs", "categories": ["cs.LG"], "comment": null, "summary": "Graph neural networks (GNNs) are widely used for learning node embeddings in\ngraphs, typically adopting a message-passing scheme. This approach, however,\nleads to the neighbor explosion problem, with exponentially growing\ncomputational and memory demands as layers increase. Graph sampling has become\nthe predominant method for scaling GNNs to large graphs, mitigating but not\nfully solving the issue. Pre-propagation GNNs (PP-GNNs) represent a new class\nof models that decouple feature propagation from training through\npre-processing, addressing neighbor explosion in theory. Yet, their practical\nadvantages and system-level optimizations remain underexplored. This paper\nprovides a comprehensive characterization of PP-GNNs, comparing them with\ngraph-sampling-based methods in training efficiency, scalability, and accuracy.\nWhile PP-GNNs achieve comparable accuracy, we identify data loading as the key\nbottleneck for training efficiency and input expansion as a major scalability\nchallenge. To address these issues, we propose optimized data loading schemes\nand tailored training methods that improve PP-GNN training throughput by an\naverage of 15$\\times$ over the PP-GNN baselines, with speedup of up to 2 orders\nof magnitude compared to sampling-based GNNs on large graph benchmarks. Our\nimplementation is publicly available at\nhttps://github.com/cornell-zhang/preprop-gnn.", "AI": {"tldr": "PP-GNNs\u901a\u8fc7\u9884\u5904\u7406\u89e3\u51b3GNN\u4e2d\u7684\u90bb\u5c45\u7206\u70b8\u95ee\u9898\uff0c\u4f46\u6570\u636e\u52a0\u8f7d\u548c\u8f93\u5165\u6269\u5c55\u662f\u4e3b\u8981\u74f6\u9888\u3002\u672c\u6587\u4f18\u5316\u4e86\u6570\u636e\u52a0\u8f7d\u548c\u8bad\u7ec3\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bad\u7ec3\u6548\u7387\u3002", "motivation": "\u89e3\u51b3GNN\u4e2d\u90bb\u5c45\u7206\u70b8\u95ee\u9898\uff0c\u63a2\u7d22PP-GNNs\u7684\u5b9e\u9645\u4f18\u52bf\u4e0e\u7cfb\u7edf\u4f18\u5316\u3002", "method": "\u63d0\u51fa\u4f18\u5316\u7684\u6570\u636e\u52a0\u8f7d\u65b9\u6848\u548c\u5b9a\u5236\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5bf9\u6bd4PP-GNNs\u4e0e\u57fa\u4e8e\u91c7\u6837\u7684GNNs\u3002", "result": "PP-GNNs\u8bad\u7ec3\u541e\u5410\u91cf\u5e73\u5747\u63d0\u534715\u500d\uff0c\u5927\u578b\u56fe\u57fa\u51c6\u4e0a\u901f\u5ea6\u63d0\u5347\u8fbe2\u4e2a\u6570\u91cf\u7ea7\u3002", "conclusion": "PP-GNNs\u5728\u6548\u7387\u3001\u6269\u5c55\u6027\u548c\u51c6\u786e\u6027\u4e0a\u5177\u6709\u6f5c\u529b\uff0c\u4f18\u5316\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u5176\u6027\u80fd\u3002"}}
{"id": "2504.13475", "pdf": "https://arxiv.org/pdf/2504.13475", "abs": "https://arxiv.org/abs/2504.13475", "authors": ["Chenwei Yan", "Xiangling Fu", "Yuxuan Xiong", "Tianyi Wang", "Siu Cheung Hui", "Ji Wu", "Xien Liu"], "title": "LLM Sensitivity Evaluation Framework for Clinical Diagnosis", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have demonstrated impressive performance across\nvarious domains. However, for clinical diagnosis, higher expectations are\nrequired for LLM's reliability and sensitivity: thinking like physicians and\nremaining sensitive to key medical information that affects diagnostic\nreasoning, as subtle variations can lead to different diagnosis results. Yet,\nexisting works focus mainly on investigating the sensitivity of LLMs to\nirrelevant context and overlook the importance of key information. In this\npaper, we investigate the sensitivity of LLMs, i.e. GPT-3.5, GPT-4, Gemini,\nClaude3 and LLaMA2-7b, to key medical information by introducing different\nperturbation strategies. The evaluation results highlight the limitations of\ncurrent LLMs in remaining sensitive to key medical information for diagnostic\ndecision-making. The evolution of LLMs must focus on improving their\nreliability, enhancing their ability to be sensitive to key information, and\neffectively utilizing this information. These improvements will enhance human\ntrust in LLMs and facilitate their practical application in real-world\nscenarios. Our code and dataset are available at\nhttps://github.com/chenwei23333/DiagnosisQA.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u4e34\u5e8a\u8bca\u65ad\u4e2d\u5bf9\u5173\u952e\u533b\u5b66\u4fe1\u606f\u7684\u654f\u611f\u6027\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u6539\u8fdb\u65b9\u5411\u3002", "motivation": "\u4e34\u5e8a\u8bca\u65ad\u5bf9LLMs\u7684\u53ef\u9760\u6027\u548c\u654f\u611f\u6027\u8981\u6c42\u66f4\u9ad8\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u5ffd\u89c6\u4e86\u5173\u952e\u4fe1\u606f\u7684\u91cd\u8981\u6027\uff0c\u56e0\u6b64\u9700\u8981\u8bc4\u4f30LLMs\u5bf9\u5173\u952e\u533b\u5b66\u4fe1\u606f\u7684\u654f\u611f\u6027\u3002", "method": "\u901a\u8fc7\u5f15\u5165\u4e0d\u540c\u7684\u6270\u52a8\u7b56\u7565\uff0c\u8bc4\u4f30\u4e86GPT-3.5\u3001GPT-4\u3001Gemini\u3001Claude3\u548cLLaMA2-7b\u7b49LLMs\u5bf9\u5173\u952e\u533b\u5b66\u4fe1\u606f\u7684\u654f\u611f\u6027\u3002", "result": "\u8bc4\u4f30\u7ed3\u679c\u8868\u660e\uff0c\u5f53\u524dLLMs\u5728\u8bca\u65ad\u51b3\u7b56\u4e2d\u5bf9\u5173\u952e\u533b\u5b66\u4fe1\u606f\u7684\u654f\u611f\u6027\u5b58\u5728\u5c40\u9650\u6027\u3002", "conclusion": "LLMs\u7684\u53d1\u5c55\u9700\u805a\u7126\u4e8e\u63d0\u9ad8\u53ef\u9760\u6027\u3001\u589e\u5f3a\u5bf9\u5173\u952e\u4fe1\u606f\u7684\u654f\u611f\u6027\uff0c\u5e76\u6709\u6548\u5229\u7528\u8fd9\u4e9b\u4fe1\u606f\uff0c\u4ee5\u63d0\u5347\u4eba\u7c7b\u4fe1\u4efb\u5e76\u4fc3\u8fdb\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2504.13631", "pdf": "https://arxiv.org/pdf/2504.13631", "abs": "https://arxiv.org/abs/2504.13631", "authors": ["Yajing Xu", "Zhiqiang Liu", "Jiaoyan Chen", "Mingchen Tu", "Zhuo Chen", "Jeff Z. Pan", "Yichi Zhang", "Yushan Zhu", "Wen Zhang", "Huajun Chen"], "title": "Multi-modal Knowledge Graph Generation with Semantics-enriched Prompts", "categories": ["cs.AI"], "comment": "Accepted by IJCNN 2025", "summary": "Multi-modal Knowledge Graphs (MMKGs) have been widely applied across various\ndomains for knowledge representation. However, the existing MMKGs are\nsignificantly fewer than required, and their construction faces numerous\nchallenges, particularly in ensuring the selection of high-quality,\ncontextually relevant images for knowledge graph enrichment. To address these\nchallenges, we present a framework for constructing MMKGs from conventional\nKGs. Furthermore, to generate higher-quality images that are more relevant to\nthe context in the given knowledge graph, we designed a neighbor selection\nmethod called Visualizable Structural Neighbor Selection (VSNS). This method\nconsists of two modules: Visualizable Neighbor Selection (VNS) and Structural\nNeighbor Selection (SNS). The VNS module filters relations that are difficult\nto visualize, while the SNS module selects neighbors that most effectively\ncapture the structural characteristics of the entity. To evaluate the quality\nof the generated images, we performed qualitative and quantitative evaluations\non two datasets, MKG-Y and DB15K. The experimental results indicate that using\nthe VSNS method to select neighbors results in higher-quality images that are\nmore relevant to the knowledge graph.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u4f20\u7edf\u77e5\u8bc6\u56fe\u8c31\u6784\u5efa\u591a\u6a21\u6001\u77e5\u8bc6\u56fe\u8c31\u7684\u6846\u67b6\uff0c\u5e76\u8bbe\u8ba1\u4e86VSNS\u65b9\u6cd5\u4ee5\u9009\u62e9\u9ad8\u8d28\u91cf\u3001\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\u56fe\u50cf\u3002", "motivation": "\u73b0\u6709MMKGs\u6570\u91cf\u4e0d\u8db3\u4e14\u6784\u5efa\u9762\u4e34\u6311\u6218\uff0c\u5c24\u5176\u662f\u56fe\u50cf\u9009\u62e9\u7684\u8d28\u91cf\u548c\u76f8\u5173\u6027\u3002", "method": "\u8bbe\u8ba1\u4e86VSNS\u65b9\u6cd5\uff0c\u5305\u542bVNS\u548cSNS\u6a21\u5757\uff0c\u5206\u522b\u8fc7\u6ee4\u96be\u53ef\u89c6\u5316\u5173\u7cfb\u548c\u9009\u62e9\u7ed3\u6784\u7279\u5f81\u90bb\u5c45\u3002", "result": "\u5728MKG-Y\u548cDB15K\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\u8868\u660e\uff0cVSNS\u65b9\u6cd5\u751f\u6210\u7684\u56fe\u50cf\u8d28\u91cf\u66f4\u9ad8\u4e14\u66f4\u76f8\u5173\u3002", "conclusion": "VSNS\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u591a\u6a21\u6001\u77e5\u8bc6\u56fe\u8c31\u7684\u56fe\u50cf\u8d28\u91cf\u548c\u4e0a\u4e0b\u6587\u76f8\u5173\u6027\u3002"}}
{"id": "2504.13292", "pdf": "https://arxiv.org/pdf/2504.13292", "abs": "https://arxiv.org/abs/2504.13292", "authors": ["Zhiwei Xu", "Zhiyu Ni", "Yixin Wang", "Wei Hu"], "title": "Let Me Grok for You: Accelerating Grokking via Embedding Transfer from a Weaker Model", "categories": ["cs.LG", "stat.ML"], "comment": "ICLR 2025", "summary": "''Grokking'' is a phenomenon where a neural network first memorizes training\ndata and generalizes poorly, but then suddenly transitions to near-perfect\ngeneralization after prolonged training. While intriguing, this delayed\ngeneralization phenomenon compromises predictability and efficiency. Ideally,\nmodels should generalize directly without delay. To this end, this paper\nproposes GrokTransfer, a simple and principled method for accelerating grokking\nin training neural networks, based on the key observation that data embedding\nplays a crucial role in determining whether generalization is delayed.\nGrokTransfer first trains a smaller, weaker model to reach a nontrivial (but\nfar from optimal) test performance. Then, the learned input embedding from this\nweaker model is extracted and used to initialize the embedding in the target,\nstronger model. We rigorously prove that, on a synthetic XOR task where delayed\ngeneralization always occurs in normal training, GrokTransfer enables the\ntarget model to generalize directly without delay. Moreover, we demonstrate\nthat, across empirical studies of different tasks, GrokTransfer effectively\nreshapes the training dynamics and eliminates delayed generalization, for both\nfully-connected neural networks and Transformers.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faGrokTransfer\u65b9\u6cd5\uff0c\u901a\u8fc7\u9884\u8bad\u7ec3\u5c0f\u6a21\u578b\u7684\u5d4c\u5165\u5c42\u6765\u52a0\u901f\u795e\u7ecf\u7f51\u7edc\u7684\u6cdb\u5316\u8fc7\u7a0b\uff0c\u907f\u514d\u5ef6\u8fdf\u6cdb\u5316\u73b0\u8c61\u3002", "motivation": "Grokking\u73b0\u8c61\u5bfc\u81f4\u795e\u7ecf\u7f51\u7edc\u6cdb\u5316\u5ef6\u8fdf\uff0c\u5f71\u54cd\u6548\u7387\u548c\u53ef\u9884\u6d4b\u6027\uff0c\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u76f4\u63a5\u5b9e\u73b0\u6cdb\u5316\u3002", "method": "GrokTransfer\u901a\u8fc7\u9884\u8bad\u7ec3\u5c0f\u6a21\u578b\u63d0\u53d6\u5d4c\u5165\u5c42\uff0c\u7528\u4e8e\u521d\u59cb\u5316\u76ee\u6807\u5927\u6a21\u578b\uff0c\u4ece\u800c\u52a0\u901f\u6cdb\u5316\u3002", "result": "\u5728\u5408\u6210\u4efb\u52a1\u548c\u5b9e\u9645\u4efb\u52a1\u4e2d\uff0cGrokTransfer\u6210\u529f\u6d88\u9664\u4e86\u5ef6\u8fdf\u6cdb\u5316\u73b0\u8c61\u3002", "conclusion": "GrokTransfer\u662f\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u663e\u8457\u6539\u5584\u795e\u7ecf\u7f51\u7edc\u7684\u8bad\u7ec3\u52a8\u6001\u548c\u6cdb\u5316\u6027\u80fd\u3002"}}
{"id": "2504.13500", "pdf": "https://arxiv.org/pdf/2504.13500", "abs": "https://arxiv.org/abs/2504.13500", "authors": ["Jianing Wang", "Jin Jiang", "Yang Liu", "Mengdi Zhang", "Xunliang Cai"], "title": "Prejudge-Before-Think: Enhancing Large Language Models at Test-Time by Process Prejudge Reasoning", "categories": ["cs.CL"], "comment": null, "summary": "In this paper, we introduce a new \\emph{process prejudge} strategy in LLM\nreasoning to demonstrate that bootstrapping with process prejudge allows the\nLLM to adaptively anticipate the errors encountered when advancing the\nsubsequent reasoning steps, similar to people sometimes pausing to think about\nwhat mistakes may occur and how to avoid them, rather than relying solely on\ntrial and error. Specifically, we define a prejudge node in the rationale,\nwhich represents a reasoning step, with at least one step that follows the\nprejudge node that has no paths toward the correct answer. To synthesize the\nprejudge reasoning process, we present an automated reasoning framework with a\ndynamic tree-searching strategy. This framework requires only one LLM to\nperform answer judging, response critiquing, prejudge generation, and thought\ncompletion. Furthermore, we develop a two-phase training mechanism with\nsupervised fine-tuning (SFT) and reinforcement learning (RL) to further enhance\nthe reasoning capabilities of LLMs. Experimental results from competition-level\ncomplex reasoning demonstrate that our method can teach the model to prejudge\nbefore thinking and significantly enhance the reasoning ability of LLMs. Code\nand data is released at https://github.com/wjn1996/Prejudge-Before-Think.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u8fc7\u7a0b\u9884\u5224\u201d\u7684\u65b0\u7b56\u7565\uff0c\u901a\u8fc7\u9884\u5224\u9519\u8bef\u6765\u63d0\u5347LLM\u7684\u63a8\u7406\u80fd\u529b\uff0c\u907f\u514d\u4e86\u5355\u7eaf\u8bd5\u9519\u3002", "motivation": "\u4f20\u7edfLLM\u63a8\u7406\u4f9d\u8d56\u8bd5\u9519\uff0c\u800c\u4eba\u7c7b\u5728\u63a8\u7406\u65f6\u4f1a\u9884\u5224\u53ef\u80fd\u7684\u9519\u8bef\u5e76\u907f\u514d\u3002\u8bba\u6587\u65e8\u5728\u6a21\u62df\u8fd9\u4e00\u884c\u4e3a\uff0c\u63d0\u5347LLM\u7684\u63a8\u7406\u6548\u7387\u3002", "method": "\u5b9a\u4e49\u4e86\u9884\u5224\u8282\u70b9\uff0c\u7ed3\u5408\u52a8\u6001\u6811\u641c\u7d22\u6846\u67b6\uff0c\u5e76\u91c7\u7528SFT\u548cRL\u4e24\u9636\u6bb5\u8bad\u7ec3\u673a\u5236\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u663e\u8457\u63d0\u5347LLM\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "conclusion": "\u8fc7\u7a0b\u9884\u5224\u7b56\u7565\u6709\u6548\u63d0\u5347\u4e86LLM\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4ee3\u7801\u548c\u6570\u636e\u5df2\u5f00\u6e90\u3002"}}
{"id": "2504.13644", "pdf": "https://arxiv.org/pdf/2504.13644", "abs": "https://arxiv.org/abs/2504.13644", "authors": ["Gabriel Freedman", "Francesca Toni"], "title": "Exploring the Potential for Large Language Models to Demonstrate Rational Probabilistic Beliefs", "categories": ["cs.AI", "cs.CL"], "comment": "8 pages, 4 figures", "summary": "Advances in the general capabilities of large language models (LLMs) have led\nto their use for information retrieval, and as components in automated decision\nsystems. A faithful representation of probabilistic reasoning in these models\nmay be essential to ensure trustworthy, explainable and effective performance\nin these tasks. Despite previous work suggesting that LLMs can perform complex\nreasoning and well-calibrated uncertainty quantification, we find that current\nversions of this class of model lack the ability to provide rational and\ncoherent representations of probabilistic beliefs. To demonstrate this, we\nintroduce a novel dataset of claims with indeterminate truth values and apply a\nnumber of well-established techniques for uncertainty quantification to measure\nthe ability of LLM's to adhere to fundamental properties of probabilistic\nreasoning.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u6982\u7387\u63a8\u7406\u65b9\u9762\u7f3a\u4e4f\u7406\u6027\u4e0e\u4e00\u81f4\u6027\uff0c\u65e0\u6cd5\u63d0\u4f9b\u53ef\u4fe1\u7684\u6982\u7387\u4fe1\u5ff5\u8868\u793a\u3002", "motivation": "\u63a2\u8ba8LLMs\u5728\u6982\u7387\u63a8\u7406\u4e2d\u7684\u8868\u73b0\uff0c\u4ee5\u786e\u4fdd\u5176\u5728\u4fe1\u606f\u68c0\u7d22\u548c\u81ea\u52a8\u51b3\u7b56\u7cfb\u7edf\u4e2d\u7684\u53ef\u4fe1\u5ea6\u4e0e\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u5f15\u5165\u4e00\u4e2a\u5305\u542b\u4e0d\u786e\u5b9a\u771f\u503c\u58f0\u660e\u7684\u65b0\u6570\u636e\u96c6\uff0c\u5e76\u5e94\u7528\u591a\u79cd\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u6280\u672f\u8bc4\u4f30LLMs\u7684\u6982\u7387\u63a8\u7406\u80fd\u529b\u3002", "result": "\u5f53\u524dLLMs\u65e0\u6cd5\u6ee1\u8db3\u6982\u7387\u63a8\u7406\u7684\u57fa\u672c\u6027\u8d28\uff0c\u8868\u73b0\u4e0d\u4e00\u81f4\u3002", "conclusion": "LLMs\u5728\u6982\u7387\u63a8\u7406\u65b9\u9762\u4ecd\u9700\u6539\u8fdb\uff0c\u4ee5\u5b9e\u73b0\u66f4\u53ef\u4fe1\u548c\u6709\u6548\u7684\u5e94\u7528\u3002"}}
{"id": "2504.13296", "pdf": "https://arxiv.org/pdf/2504.13296", "abs": "https://arxiv.org/abs/2504.13296", "authors": ["Ganesh Sundaram", "Jonas Ulmen", "Daniel G\u00f6rges"], "title": "Enhanced Pruning Strategy for Multi-Component Neural Architectures Using Component-Aware Graph Analysis", "categories": ["cs.LG", "cs.AI"], "comment": "6 pages, IFAC J3C", "summary": "Deep neural networks (DNNs) deliver outstanding performance, but their\ncomplexity often prohibits deployment in resource-constrained settings.\nComprehensive structured pruning frameworks based on parameter dependency\nanalysis reduce model size with specific regard to computational performance.\nWhen applying them to Multi-Component Neural Architectures (MCNAs), they risk\nnetwork integrity by removing large parameter groups. We introduce a\ncomponent-aware pruning strategy, extending dependency graphs to isolate\nindividual components and inter-component flows. This creates smaller, targeted\npruning groups that conserve functional integrity. Demonstrated effectively on\na control task, our approach achieves greater sparsity and reduced performance\ndegradation, opening a path for optimizing complex, multi-component DNNs\nefficiently.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ec4\u4ef6\u611f\u77e5\u7684\u526a\u679d\u7b56\u7565\uff0c\u901a\u8fc7\u6269\u5c55\u4f9d\u8d56\u56fe\u6765\u4fdd\u62a4\u591a\u7ec4\u4ef6\u795e\u7ecf\u67b6\u6784\u7684\u529f\u80fd\u5b8c\u6574\u6027\uff0c\u5b9e\u73b0\u66f4\u9ad8\u7684\u7a00\u758f\u6027\u548c\u66f4\u4f4e\u7684\u6027\u80fd\u635f\u5931\u3002", "motivation": "\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u90e8\u7f72\u65f6\uff0c\u4f20\u7edf\u526a\u679d\u65b9\u6cd5\u53ef\u80fd\u7834\u574f\u591a\u7ec4\u4ef6\u67b6\u6784\u7684\u5b8c\u6574\u6027\u3002", "method": "\u6269\u5c55\u4f9d\u8d56\u56fe\u4ee5\u9694\u79bb\u7ec4\u4ef6\u548c\u7ec4\u4ef6\u95f4\u6d41\uff0c\u5f62\u6210\u66f4\u5c0f\u7684\u526a\u679d\u7ec4\u3002", "result": "\u5728\u63a7\u5236\u4efb\u52a1\u4e2d\u9a8c\u8bc1\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u7a00\u758f\u6027\u548c\u66f4\u4f4e\u7684\u6027\u80fd\u9000\u5316\u3002", "conclusion": "\u4e3a\u9ad8\u6548\u4f18\u5316\u590d\u6742\u591a\u7ec4\u4ef6DNN\u63d0\u4f9b\u4e86\u65b0\u8def\u5f84\u3002"}}
{"id": "2504.13534", "pdf": "https://arxiv.org/pdf/2504.13534", "abs": "https://arxiv.org/abs/2504.13534", "authors": ["Feiyang Li", "Peng Fang", "Zhan Shi", "Arijit Khan", "Fang Wang", "Dan Feng", "Weihao Wang", "Xin Zhang", "Yongjian Cui"], "title": "CoT-RAG: Integrating Chain of Thought and Retrieval-Augmented Generation to Enhance Reasoning in Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "While chain-of-thought (CoT) reasoning improves the performance of large\nlanguage models (LLMs) in complex tasks, it still has two main challenges: the\nlow reliability of relying solely on LLMs to generate reasoning chains and the\ninterference of natural language reasoning chains on the inference logic of\nLLMs. To address these issues, we propose CoT-RAG, a novel reasoning framework\nwith three key designs: (i) Knowledge Graph-driven CoT Generation, featuring\nknowledge graphs to modulate reasoning chain generation of LLMs, thereby\nenhancing reasoning credibility; (ii) Learnable Knowledge Case-aware RAG, which\nincorporates retrieval-augmented generation (RAG) into knowledge graphs to\nretrieve relevant sub-cases and sub-descriptions, providing LLMs with learnable\ninformation; (iii) Pseudo-Program Prompting Execution, which encourages LLMs to\nexecute reasoning tasks in pseudo-programs with greater logical rigor. We\nconduct a comprehensive evaluation on nine public datasets, covering three\nreasoning problems. Compared with the-state-of-the-art methods, CoT-RAG\nexhibits a significant accuracy improvement, ranging from 4.0% to 23.0%.\nFurthermore, testing on four domain-specific datasets, CoT-RAG shows remarkable\naccuracy and efficient execution, highlighting its strong practical\napplicability and scalability.", "AI": {"tldr": "CoT-RAG\u662f\u4e00\u4e2a\u65b0\u7684\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u77e5\u8bc6\u56fe\u8c31\u548c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6280\u672f\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u53ef\u9760\u6027\u548c\u903b\u8f91\u4e25\u8c28\u6027\u3002", "motivation": "\u89e3\u51b3\u94fe\u5f0f\u601d\u7ef4\u63a8\u7406\uff08CoT\uff09\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u53ef\u9760\u6027\u4f4e\u548c\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u94fe\u5e72\u6270\u903b\u8f91\u7684\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u77e5\u8bc6\u56fe\u8c31\u9a71\u52a8\u7684CoT\u751f\u6210\u3001\u53ef\u5b66\u4e60\u7684\u77e5\u8bc6\u6848\u4f8b\u611f\u77e5RAG\u548c\u4f2a\u7a0b\u5e8f\u63d0\u793a\u6267\u884c\u3002", "result": "\u5728\u4e5d\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u5347\u51c6\u786e\u6027\uff084.0%\u81f323.0%\uff09\uff0c\u5e76\u5728\u9886\u57df\u7279\u5b9a\u6570\u636e\u96c6\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u6267\u884c\u3002", "conclusion": "CoT-RAG\u5177\u6709\u5f3a\u5b9e\u7528\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u4efb\u52a1\u7684\u6027\u80fd\u3002"}}
{"id": "2504.13707", "pdf": "https://arxiv.org/pdf/2504.13707", "abs": "https://arxiv.org/abs/2504.13707", "authors": ["Yichen Wu", "Xudong Pan", "Geng Hong", "Min Yang"], "title": "OpenDeception: Benchmarking and Investigating AI Deceptive Behaviors via Open-ended Interaction Simulation", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "As the general capabilities of large language models (LLMs) improve and agent\napplications become more widespread, the underlying deception risks urgently\nrequire systematic evaluation and effective oversight. Unlike existing\nevaluation which uses simulated games or presents limited choices, we introduce\nOpenDeception, a novel deception evaluation framework with an open-ended\nscenario dataset. OpenDeception jointly evaluates both the deception intention\nand capabilities of LLM-based agents by inspecting their internal reasoning\nprocess. Specifically, we construct five types of common use cases where LLMs\nintensively interact with the user, each consisting of ten diverse, concrete\nscenarios from the real world. To avoid ethical concerns and costs of high-risk\ndeceptive interactions with human testers, we propose to simulate the\nmulti-turn dialogue via agent simulation. Extensive evaluation of eleven\nmainstream LLMs on OpenDeception highlights the urgent need to address\ndeception risks and security concerns in LLM-based agents: the deception\nintention ratio across the models exceeds 80%, while the deception success rate\nsurpasses 50%. Furthermore, we observe that LLMs with stronger capabilities do\nexhibit a higher risk of deception, which calls for more alignment efforts on\ninhibiting deceptive behaviors.", "AI": {"tldr": "OpenDeception\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u6b3a\u9a97\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u57fa\u4e8eLLM\u7684\u4ee3\u7406\u7684\u6b3a\u9a97\u610f\u56fe\u548c\u80fd\u529b\uff0c\u63ed\u793a\u4e86\u4e3b\u6d41LLM\u7684\u9ad8\u6b3a\u9a97\u98ce\u9669\u3002", "motivation": "\u968f\u7740LLM\u80fd\u529b\u7684\u63d0\u5347\u548c\u4ee3\u7406\u5e94\u7528\u7684\u666e\u53ca\uff0c\u5176\u6f5c\u5728\u7684\u6b3a\u9a97\u98ce\u9669\u4e9f\u9700\u7cfb\u7edf\u8bc4\u4f30\u548c\u6709\u6548\u76d1\u7ba1\u3002", "method": "\u901a\u8fc7\u6784\u5efa\u5f00\u653e\u573a\u666f\u6570\u636e\u96c6\u548c\u6a21\u62df\u591a\u8f6e\u5bf9\u8bdd\uff0cOpenDeception\u8bc4\u4f30LLM\u7684\u6b3a\u9a97\u610f\u56fe\u548c\u80fd\u529b\uff0c\u5e76\u907f\u514d\u4e0e\u4eba\u7c7b\u6d4b\u8bd5\u8005\u7684\u9ad8\u98ce\u9669\u4e92\u52a8\u3002", "result": "\u8bc4\u4f30\u663e\u793a\uff0c\u4e3b\u6d41LLM\u7684\u6b3a\u9a97\u610f\u56fe\u6bd4\u4f8b\u8d85\u8fc780%\uff0c\u6210\u529f\u7387\u8d85\u8fc750%\uff0c\u4e14\u80fd\u529b\u66f4\u5f3a\u7684LLM\u6b3a\u9a97\u98ce\u9669\u66f4\u9ad8\u3002", "conclusion": "\u7814\u7a76\u547c\u5401\u52a0\u5f3a\u5bf9LLM\u6b3a\u9a97\u884c\u4e3a\u7684\u6291\u5236\u548c\u5bf9\u9f50\u52aa\u529b\u3002"}}
{"id": "2504.13302", "pdf": "https://arxiv.org/pdf/2504.13302", "abs": "https://arxiv.org/abs/2504.13302", "authors": ["Ibrahim Emirahmetoglu", "David E. Stewart"], "title": "Training Autoencoders Using Stochastic Hessian-Free Optimization with LSMR", "categories": ["cs.LG", "math.OC", "Primary: 65K05, Secondary: 49M15"], "comment": null, "summary": "Hessian-free (HF) optimization has been shown to effectively train deep\nautoencoders (Martens, 2010). In this paper, we aim to accelerate HF training\nof autoencoders by reducing the amount of data used in training. HF utilizes\nthe conjugate gradient algorithm to estimate update directions. Instead, we\npropose using the LSMR method, which is known for effectively solving large\nsparse linear systems. We also incorporate Chapelle & Erhan (2011)'s improved\npreconditioner for HF optimization. In addition, we introduce a new mini-batch\nselection algorithm to mitigate overfitting. Our algorithm starts with a small\nsubset of the training data and gradually increases the mini-batch size based\non (i) variance estimates obtained during the computation of a mini-batch\ngradient (Byrd et al., 2012) and (ii) the relative decrease in objective value\nfor the validation data. Our experimental results demonstrate that our\nstochastic Hessian-free optimization, using the LSMR method and the new sample\nselection algorithm, leads to rapid training of deep autoencoders with improved\ngeneralization error.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u52a0\u901fHessian-free\uff08HF\uff09\u4f18\u5316\u8bad\u7ec3\u6df1\u5ea6\u81ea\u7f16\u7801\u5668\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u51cf\u5c11\u8bad\u7ec3\u6570\u636e\u91cf\u3001\u4f7f\u7528LSMR\u65b9\u6cd5\u66ff\u4ee3\u5171\u8f6d\u68af\u5ea6\u7b97\u6cd5\uff0c\u5e76\u7ed3\u5408\u6539\u8fdb\u7684\u9884\u5904\u7406\u5668\u548c\u65b0\u7684\u5c0f\u6279\u91cf\u9009\u62e9\u7b97\u6cd5\u3002", "motivation": "HF\u4f18\u5316\u5728\u8bad\u7ec3\u6df1\u5ea6\u81ea\u7f16\u7801\u5668\u65f6\u6709\u6548\u4f46\u8ba1\u7b97\u91cf\u5927\uff0c\u76ee\u6807\u662f\u51cf\u5c11\u6570\u636e\u91cf\u5e76\u63d0\u9ad8\u6548\u7387\u3002", "method": "\u4f7f\u7528LSMR\u65b9\u6cd5\u66ff\u4ee3\u5171\u8f6d\u68af\u5ea6\u7b97\u6cd5\uff0c\u7ed3\u5408\u6539\u8fdb\u7684\u9884\u5904\u7406\u5668\u548c\u65b0\u8bbe\u8ba1\u7684\u5c0f\u6279\u91cf\u9009\u62e9\u7b97\u6cd5\uff0c\u9010\u6b65\u589e\u52a0\u6279\u91cf\u5927\u5c0f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u5feb\u901f\u8bad\u7ec3\u6df1\u5ea6\u81ea\u7f16\u7801\u5668\u5e76\u964d\u4f4e\u6cdb\u5316\u8bef\u5dee\u3002", "conclusion": "\u63d0\u51fa\u7684\u968f\u673aHF\u4f18\u5316\u65b9\u6cd5\u7ed3\u5408LSMR\u548c\u5c0f\u6279\u91cf\u9009\u62e9\u7b97\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bad\u7ec3\u6548\u7387\u548c\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2504.13545", "pdf": "https://arxiv.org/pdf/2504.13545", "abs": "https://arxiv.org/abs/2504.13545", "authors": ["Azmarah Rizvi", "Navojith Thamindu", "A. M. N. H. Adhikari", "W. P. U. Senevirathna", "Dharshana Kasthurirathna", "Lakmini Abeywardhana"], "title": "Enhancing Multilingual Sentiment Analysis with Explainability for Sinhala, English, and Code-Mixed Content", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "6 pages, 6 figures, 4 tables", "summary": "Sentiment analysis is crucial for brand reputation management in the banking\nsector, where customer feedback spans English, Sinhala, Singlish, and\ncode-mixed text. Existing models struggle with low-resource languages like\nSinhala and lack interpretability for practical use. This research develops a\nhybrid aspect-based sentiment analysis framework that enhances multilingual\ncapabilities with explainable outputs. Using cleaned banking customer reviews,\nwe fine-tune XLM-RoBERTa for Sinhala and code-mixed text, integrate\ndomain-specific lexicon correction, and employ BERT-base-uncased for English.\nThe system classifies sentiment (positive, neutral, negative) with confidence\nscores, while SHAP and LIME improve interpretability by providing real-time\nsentiment explanations. Experimental results show that our approaches\noutperform traditional transformer-based classifiers, achieving 92.3 percent\naccuracy and an F1-score of 0.89 in English and 88.4 percent in Sinhala and\ncode-mixed content. An explainability analysis reveals key sentiment drivers,\nimproving trust and transparency. A user-friendly interface delivers\naspect-wise sentiment insights, ensuring accessibility for businesses. This\nresearch contributes to robust, transparent sentiment analysis for financial\napplications by bridging gaps in multilingual, low-resource NLP and\nexplainability.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u79cd\u6df7\u5408\u6846\u67b6\uff0c\u7528\u4e8e\u94f6\u884c\u5ba2\u6237\u8bc4\u8bba\u7684\u591a\u8bed\u8a00\u60c5\u611f\u5206\u6790\uff0c\u63d0\u5347\u4f4e\u8d44\u6e90\u8bed\u8a00\u5904\u7406\u80fd\u529b\u548c\u89e3\u91ca\u6027\u3002", "motivation": "\u94f6\u884c\u5ba2\u6237\u53cd\u9988\u6d89\u53ca\u591a\u79cd\u8bed\u8a00\uff08\u82f1\u8bed\u3001\u50e7\u4f3d\u7f57\u8bed\u3001\u6df7\u5408\u8bed\uff09\uff0c\u73b0\u6709\u6a21\u578b\u5bf9\u4f4e\u8d44\u6e90\u8bed\u8a00\u5904\u7406\u80fd\u529b\u4e0d\u8db3\u4e14\u7f3a\u4e4f\u89e3\u91ca\u6027\u3002", "method": "\u7ed3\u5408XLM-RoBERTa\uff08\u50e7\u4f3d\u7f57\u8bed\u548c\u6df7\u5408\u8bed\uff09\u3001BERT-base-uncased\uff08\u82f1\u8bed\uff09\uff0c\u5e76\u6574\u5408\u9886\u57df\u7279\u5b9a\u8bcd\u5178\u6821\u6b63\uff0c\u4f7f\u7528SHAP\u548cLIME\u589e\u5f3a\u89e3\u91ca\u6027\u3002", "result": "\u5728\u82f1\u8bed\u4e2d\u51c6\u786e\u7387\u8fbe92.3%\uff0cF1\u5206\u65700.89\uff1b\u50e7\u4f3d\u7f57\u8bed\u548c\u6df7\u5408\u8bed\u4e2d\u8fbe88.4%\u3002\u89e3\u91ca\u6027\u5206\u6790\u63ed\u793a\u4e86\u5173\u952e\u60c5\u611f\u9a71\u52a8\u56e0\u7d20\u3002", "conclusion": "\u8be5\u7814\u7a76\u586b\u8865\u4e86\u591a\u8bed\u8a00\u4f4e\u8d44\u6e90NLP\u548c\u89e3\u91ca\u6027\u7a7a\u767d\uff0c\u4e3a\u91d1\u878d\u5e94\u7528\u63d0\u4f9b\u4e86\u900f\u660e\u3001\u9c81\u68d2\u7684\u60c5\u611f\u5206\u6790\u5de5\u5177\u3002"}}
{"id": "2504.13837", "pdf": "https://arxiv.org/pdf/2504.13837", "abs": "https://arxiv.org/abs/2504.13837", "authors": ["Yang Yue", "Zhiqi Chen", "Rui Lu", "Andrew Zhao", "Zhaokai Wang", "Yang Yue", "Shiji Song", "Gao Huang"], "title": "Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?", "categories": ["cs.AI", "cs.CL", "cs.CV"], "comment": "24 pages, 19 figures", "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has recently\ndemonstrated notable success in enhancing the reasoning capabilities of LLMs,\nparticularly in mathematics and programming tasks. It is widely believed that\nRLVR enables LLMs to continuously self-improve, thus acquiring novel reasoning\nabilities that exceed corresponding base models' capacity. In this study,\nhowever, we critically re-examines this assumption by measuring the\npass@\\textit{k} metric with large values of \\textit{k} to explore the reasoning\ncapability boundary of the models across a wide range of model families and\nbenchmarks. Surprisingly, the RL does \\emph{not}, in fact, elicit fundamentally\nnew reasoning patterns. While RL-trained models outperform their base models at\nsmaller values of $k$ (\\eg, $k$=1), base models can achieve a comparable or\neven higher pass@$k$ score compared to their RL counterparts at large $k$\nvalues. The reasoning paths generated by RL-trained models are already included\nin the base models' sampling distribution, suggesting that most reasoning\nabilities manifested in RL-trained models are already obtained by base models.\nFurther analysis shows that RL training boosts the performance by biasing the\nmodel's output distribution toward paths that are more likely to yield rewards,\ntherefore sampling correct responses more efficiently. But this also results in\na narrower reasoning capability boundary compared to base models. Similar\nresults are observed in visual reasoning tasks trained with RLVR. Moreover, we\nfind that distillation can genuinely introduce new knowledge into the model,\ndifferent from RLVR. These findings underscore a critical limitation of RLVR in\nadvancing LLM reasoning abilities which requires us to fundamentally rethink\nthe impact of RL training in reasoning LLMs and the need of a better paradigm.\nProject Page: https://limit-of-RLVR.github.io", "AI": {"tldr": "RLVR\uff08\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\uff09\u88ab\u8ba4\u4e3a\u80fd\u63d0\u5347LLMs\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u7814\u7a76\u53d1\u73b0\u5176\u5e76\u672a\u5e26\u6765\u65b0\u7684\u63a8\u7406\u6a21\u5f0f\uff0c\u800c\u662f\u901a\u8fc7\u504f\u5411\u9ad8\u5956\u52b1\u8def\u5f84\u63d0\u5347\u6548\u7387\u3002\u84b8\u998f\u80fd\u771f\u6b63\u5f15\u5165\u65b0\u77e5\u8bc6\u3002", "motivation": "\u91cd\u65b0\u8bc4\u4f30RLVR\u662f\u5426\u771f\u7684\u80fd\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8ba9LLMs\u83b7\u5f97\u8d85\u8d8a\u57fa\u7840\u6a21\u578b\u7684\u65b0\u63a8\u7406\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u6d4b\u91cf\u5927k\u503c\u4e0b\u7684pass@k\u6307\u6807\uff0c\u5206\u6790\u4e0d\u540c\u6a21\u578b\u5bb6\u65cf\u548c\u57fa\u51c6\u4e2d\u7684\u63a8\u7406\u80fd\u529b\u8fb9\u754c\u3002", "result": "RL\u8bad\u7ec3\u672a\u5e26\u6765\u65b0\u63a8\u7406\u6a21\u5f0f\uff0c\u57fa\u7840\u6a21\u578b\u5728\u5927k\u503c\u4e0b\u8868\u73b0\u76f8\u5f53\u6216\u66f4\u597d\u3002RL\u4ec5\u901a\u8fc7\u504f\u5411\u9ad8\u5956\u52b1\u8def\u5f84\u63d0\u5347\u6548\u7387\u3002", "conclusion": "RLVR\u5728\u63d0\u5347LLM\u63a8\u7406\u80fd\u529b\u4e0a\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u91cd\u65b0\u601d\u8003RL\u8bad\u7ec3\u7684\u4f5c\u7528\u53ca\u5bfb\u627e\u66f4\u597d\u7684\u8303\u5f0f\u3002"}}
{"id": "2504.13331", "pdf": "https://arxiv.org/pdf/2504.13331", "abs": "https://arxiv.org/abs/2504.13331", "authors": ["Yassine Ouzar", "Cl\u00e9mence Nineuil", "Fouad Boutaleb", "Emery Pierson", "Ali Amad", "Mohamed Daoudi"], "title": "Wearable-Derived Behavioral and Physiological Biomarkers for Classifying Unipolar and Bipolar Depression Severity", "categories": ["cs.LG", "cs.CV"], "comment": "IEEE International Conference on Automatic Face and Gesture\n  Recognition (FG) 2025", "summary": "Depression is a complex mental disorder characterized by a diverse range of\nobservable and measurable indicators that go beyond traditional subjective\nassessments. Recent research has increasingly focused on objective, passive,\nand continuous monitoring using wearable devices to gain more precise insights\ninto the physiological and behavioral aspects of depression. However, most\nexisting studies primarily distinguish between healthy and depressed\nindividuals, adopting a binary classification that fails to capture the\nheterogeneity of depressive disorders. In this study, we leverage wearable\ndevices to predict depression subtypes-specifically unipolar and bipolar\ndepression-aiming to identify distinctive biomarkers that could enhance\ndiagnostic precision and support personalized treatment strategies. To this\nend, we introduce the CALYPSO dataset, designed for non-invasive detection of\ndepression subtypes and symptomatology through physiological and behavioral\nsignals, including blood volume pulse, electrodermal activity, body\ntemperature, and three-axis acceleration. Additionally, we establish a\nbenchmark on the dataset using well-known features and standard machine\nlearning methods. Preliminary results indicate that features related to\nphysical activity, extracted from accelerometer data, are the most effective in\ndistinguishing between unipolar and bipolar depression, achieving an accuracy\nof $96.77\\%$. Temperature-based features also showed high discriminative power,\nreaching an accuracy of $93.55\\%$. These findings highlight the potential of\nphysiological and behavioral monitoring for improving the classification of\ndepressive subtypes, paving the way for more tailored clinical interventions.", "AI": {"tldr": "\u5229\u7528\u53ef\u7a7f\u6234\u8bbe\u5907\u9884\u6d4b\u6291\u90c1\u75c7\u4e9a\u578b\uff08\u5355\u6781\u548c\u53cc\u6781\u6291\u90c1\uff09\uff0c\u901a\u8fc7\u751f\u7406\u548c\u884c\u4e3a\u4fe1\u53f7\u8bc6\u522b\u751f\u7269\u6807\u5fd7\u7269\uff0c\u63d0\u5347\u8bca\u65ad\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u591a\u91c7\u7528\u4e8c\u5143\u5206\u7c7b\uff08\u5065\u5eb7\u4e0e\u6291\u90c1\uff09\uff0c\u65e0\u6cd5\u6355\u6349\u6291\u90c1\u75c7\u7684\u5f02\u8d28\u6027\uff0c\u9700\u66f4\u7cbe\u51c6\u7684\u4e9a\u578b\u5206\u7c7b\u65b9\u6cd5\u3002", "method": "\u5f15\u5165CALYPSO\u6570\u636e\u96c6\uff0c\u5229\u7528\u751f\u7406\u548c\u884c\u4e3a\u4fe1\u53f7\uff08\u5982\u8840\u5bb9\u91cf\u8109\u51b2\u3001\u76ae\u80a4\u7535\u6d3b\u52a8\u7b49\uff09\uff0c\u7ed3\u5408\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u8fdb\u884c\u4e9a\u578b\u5206\u7c7b\u3002", "result": "\u521d\u6b65\u7ed3\u679c\u663e\u793a\uff0c\u52a0\u901f\u5ea6\u8ba1\u6570\u636e\u7684\u8eab\u4f53\u6d3b\u52a8\u7279\u5f81\u533a\u5206\u6548\u679c\u6700\u4f73\uff08\u51c6\u786e\u738796.77%\uff09\uff0c\u4f53\u6e29\u7279\u5f81\u6b21\u4e4b\uff0893.55%\uff09\u3002", "conclusion": "\u751f\u7406\u548c\u884c\u4e3a\u76d1\u6d4b\u6709\u671b\u6539\u5584\u6291\u90c1\u75c7\u4e9a\u578b\u5206\u7c7b\uff0c\u4e3a\u4e2a\u6027\u5316\u6cbb\u7597\u63d0\u4f9b\u652f\u6301\u3002"}}
{"id": "2504.13562", "pdf": "https://arxiv.org/pdf/2504.13562", "abs": "https://arxiv.org/abs/2504.13562", "authors": ["Yu Li", "Han Jiang", "Zhihua Wei"], "title": "DETAM: Defending LLMs Against Jailbreak Attacks via Targeted Attention Modification", "categories": ["cs.CL"], "comment": null, "summary": "With the widespread adoption of Large Language Models (LLMs), jailbreak\nattacks have become an increasingly pressing safety concern. While\nsafety-aligned LLMs can effectively defend against normal harmful queries, they\nremain vulnerable to such attacks. Existing defense methods primarily rely on\nfine-tuning or input modification, which often suffer from limited\ngeneralization and reduced utility. To address this, we introduce DETAM, a\nfinetuning-free defense approach that improves the defensive capabilities\nagainst jailbreak attacks of LLMs via targeted attention modification.\nSpecifically, we analyze the differences in attention scores between successful\nand unsuccessful defenses to identify the attention heads sensitive to\njailbreak attacks. During inference, we reallocate attention to emphasize the\nuser's core intention, minimizing interference from attack tokens. Our\nexperimental results demonstrate that DETAM outperforms various baselines in\njailbreak defense and exhibits robust generalization across different attacks\nand models, maintaining its effectiveness even on in-the-wild jailbreak data.\nFurthermore, in evaluating the model's utility, we incorporated over-defense\ndatasets, which further validate the superior performance of our approach. The\ncode will be released immediately upon acceptance.", "AI": {"tldr": "DETAM\u662f\u4e00\u79cd\u65e0\u9700\u5fae\u8c03\u7684\u9632\u5fa1\u65b9\u6cd5\uff0c\u901a\u8fc7\u4fee\u6539\u6ce8\u610f\u529b\u673a\u5236\u63d0\u5347LLMs\u5bf9\u8d8a\u72f1\u653b\u51fb\u7684\u9632\u5fa1\u80fd\u529b\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u4e14\u5177\u6709\u5f3a\u6cdb\u5316\u6027\u3002", "motivation": "\u968f\u7740LLMs\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u8d8a\u72f1\u653b\u51fb\u6210\u4e3a\u5b89\u5168\u9690\u60a3\uff0c\u73b0\u6709\u9632\u5fa1\u65b9\u6cd5\u6cdb\u5316\u80fd\u529b\u6709\u9650\u4e14\u53ef\u80fd\u964d\u4f4e\u6a21\u578b\u5b9e\u7528\u6027\u3002", "method": "\u5206\u6790\u6ce8\u610f\u529b\u5206\u6570\u5dee\u5f02\uff0c\u8bc6\u522b\u654f\u611f\u6ce8\u610f\u529b\u5934\uff0c\u63a8\u7406\u65f6\u91cd\u65b0\u5206\u914d\u6ce8\u610f\u529b\u4ee5\u5f3a\u8c03\u7528\u6237\u6838\u5fc3\u610f\u56fe\u3002", "result": "DETAM\u5728\u9632\u5fa1\u8d8a\u72f1\u653b\u51fb\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u6cdb\u5316\u6027\u5f3a\uff0c\u4e14\u5728\u5b9e\u7528\u6027\u548c\u8fc7\u9632\u5fa1\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "DETAM\u4e3aLLMs\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u5b9e\u7528\u7684\u9632\u5fa1\u8d8a\u72f1\u653b\u51fb\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.13183", "pdf": "https://arxiv.org/pdf/2504.13183", "abs": "https://arxiv.org/abs/2504.13183", "authors": ["Rawan AlMakinah"], "title": "Factors That Influence the Adoption of AI-enabled Conversational Agents (AICAs) as an Augmenting Therapeutic Tool by Frontline Healthcare Workers: From Technology Acceptance Model 3 (TAM3) Lens -- A Systematic Mapping Review", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Artificial intelligent (AI) conversational agents hold a promising future in\nthe field of mental health, especially in helping marginalized communities that\nlack access to mental health support services. It is tempting to have a 24/7\nmental health companion that can be accessed anywhere using mobile phones to\nprovide therapist-like advice. Yet, caution should be taken, and studies around\ntheir feasibility need to be surveyed. Before adopting such a rapidly changing\ntechnology, studies on its feasibility should be explored, summarized, and\nsynthesized to gain a solid understanding of the status quo and to enable us to\nbuild a framework that can guide us throughout the development and deployment\nprocesses. Different perspectives must be considered when investigating the\nfeasibility of AI conversational agents, including the mental healthcare\nprofessional perspective. The literature can provide insights into their\nperspectives in terms of opportunities, concerns, and implications. Mental\nhealth professionals, the subject-matter experts in this field, have their\npoints of view that should be understood and considered. This systematic\nliterature review will explore mental health practitioners' attitudes toward AI\nconversational agents and the factors that affect their adoption and\nrecommendation of the technology to augment their services and treatments. The\nTAM3 Framework will be the lens through which this systematic literature review\nwill be conducted.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86AI\u5bf9\u8bdd\u4ee3\u7406\u5728\u5fc3\u7406\u5065\u5eb7\u9886\u57df\u7684\u6f5c\u529b\uff0c\u5c24\u5176\u662f\u5bf9\u8fb9\u7f18\u5316\u7fa4\u4f53\u7684\u652f\u6301\uff0c\u5e76\u5f3a\u8c03\u9700\u7814\u7a76\u5176\u53ef\u884c\u6027\u53ca\u5fc3\u7406\u5065\u5eb7\u4e13\u4e1a\u4eba\u58eb\u7684\u89c2\u70b9\u3002", "motivation": "\u7814\u7a76AI\u5bf9\u8bdd\u4ee3\u7406\u5728\u5fc3\u7406\u5065\u5eb7\u670d\u52a1\u4e2d\u7684\u53ef\u884c\u6027\uff0c\u5c24\u5176\u662f\u4ece\u5fc3\u7406\u5065\u5eb7\u4e13\u4e1a\u4eba\u58eb\u7684\u89d2\u5ea6\u51fa\u53d1\uff0c\u4ee5\u6307\u5bfc\u6280\u672f\u5f00\u53d1\u548c\u90e8\u7f72\u3002", "method": "\u91c7\u7528\u7cfb\u7edf\u6027\u6587\u732e\u7efc\u8ff0\uff0c\u57fa\u4e8eTAM3\u6846\u67b6\uff0c\u5206\u6790\u5fc3\u7406\u5065\u5eb7\u4ece\u4e1a\u8005\u5bf9AI\u5bf9\u8bdd\u4ee3\u7406\u7684\u6001\u5ea6\u53ca\u5176\u5f71\u54cd\u56e0\u7d20\u3002", "result": "\u9884\u671f\u901a\u8fc7\u6587\u732e\u7efc\u8ff0\u603b\u7ed3\u5fc3\u7406\u5065\u5eb7\u4e13\u4e1a\u4eba\u58eb\u5bf9AI\u5bf9\u8bdd\u4ee3\u7406\u7684\u770b\u6cd5\u3001\u673a\u4f1a\u3001\u62c5\u5fe7\u53ca\u5f71\u54cd\u56e0\u7d20\u3002", "conclusion": "\u7814\u7a76\u5c06\u4e3aAI\u5bf9\u8bdd\u4ee3\u7406\u5728\u5fc3\u7406\u5065\u5eb7\u9886\u57df\u7684\u5f00\u53d1\u548c\u5e94\u7528\u63d0\u4f9b\u6846\u67b6\u548c\u6307\u5bfc\u3002"}}
{"id": "2504.13355", "pdf": "https://arxiv.org/pdf/2504.13355", "abs": "https://arxiv.org/abs/2504.13355", "authors": ["Omid Sedehi", "Manish Yadav", "Merten Stender", "Sebastian Oberst"], "title": "Denoising and Reconstruction of Nonlinear Dynamics using Truncated Reservoir Computing", "categories": ["cs.LG", "cs.NE", "nlin.CD"], "comment": null, "summary": "Measurements acquired from distributed physical systems are often sparse and\nnoisy. Therefore, signal processing and system identification tools are\nrequired to mitigate noise effects and reconstruct unobserved dynamics from\nlimited sensor data. However, this process is particularly challenging because\nthe fundamental equations governing the dynamics are largely unavailable in\npractice. Reservoir Computing (RC) techniques have shown promise in efficiently\nsimulating dynamical systems through an unstructured and efficient computation\ngraph comprising a set of neurons with random connectivity. However, the\npotential of RC to operate in noisy regimes and distinguish noise from the\nprimary dynamics of the system has not been fully explored. This paper presents\na novel RC method for noise filtering and reconstructing nonlinear dynamics,\noffering a novel learning protocol associated with hyperparameter optimization.\nThe performance of the RC in terms of noise intensity, noise frequency content,\nand drastic shifts in dynamical parameters are studied in two illustrative\nexamples involving the nonlinear dynamics of the Lorenz attractor and adaptive\nexponential integrate-and-fire system (AdEx). It is shown that the denoising\nperformance improves via truncating redundant nodes and edges of the computing\nreservoir, as well as properly optimizing the hyperparameters, e.g., the\nleakage rate, the spectral radius, the input connectivity, and the ridge\nregression parameter. Furthermore, the presented framework shows good\ngeneralization behavior when tested for reconstructing unseen attractors from\nthe bifurcation diagram. Compared to the Extended Kalman Filter (EKF), the\npresented RC framework yields competitive accuracy at low signal-to-noise\nratios (SNRs) and high-frequency ranges.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u50a8\u5c42\u8ba1\u7b97\uff08RC\uff09\u65b9\u6cd5\uff0c\u7528\u4e8e\u566a\u58f0\u8fc7\u6ee4\u548c\u975e\u7ebf\u6027\u52a8\u6001\u91cd\u5efa\uff0c\u901a\u8fc7\u4f18\u5316\u8d85\u53c2\u6570\u548c\u526a\u679d\u5197\u4f59\u8282\u70b9\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u5206\u5e03\u5f0f\u7269\u7406\u7cfb\u7edf\u7684\u6d4b\u91cf\u6570\u636e\u901a\u5e38\u7a00\u758f\u4e14\u566a\u58f0\u5927\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u672a\u77e5\u52a8\u6001\u65b9\u7a0b\u548c\u566a\u58f0\u5206\u79bb\u95ee\u9898\u3002", "method": "\u91c7\u7528\u50a8\u5c42\u8ba1\u7b97\u6280\u672f\uff0c\u7ed3\u5408\u8d85\u53c2\u6570\u4f18\u5316\u548c\u8282\u70b9\u526a\u679d\uff0c\u5904\u7406\u566a\u58f0\u5e76\u91cd\u5efa\u52a8\u6001\u3002", "result": "\u5728Lorenz\u5438\u5f15\u5b50\u548cAdEx\u7cfb\u7edf\u4e2d\uff0cRC\u65b9\u6cd5\u5728\u4f4e\u4fe1\u566a\u6bd4\u548c\u9ad8\u9891\u8303\u56f4\u5185\u8868\u73b0\u4f18\u4e8e\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\uff08EKF\uff09\u3002", "conclusion": "RC\u65b9\u6cd5\u5728\u566a\u58f0\u8fc7\u6ee4\u548c\u52a8\u6001\u91cd\u5efa\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u5c24\u5176\u5728\u590d\u6742\u975e\u7ebf\u6027\u7cfb\u7edf\u4e2d\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2504.13592", "pdf": "https://arxiv.org/pdf/2504.13592", "abs": "https://arxiv.org/abs/2504.13592", "authors": ["Zihao Feng", "Xiaoxue Wang", "Ziwei Bai", "Donghang Su", "Bowen Wu", "Qun Yu", "Baoxun Wang"], "title": "Improving Generalization in Intent Detection: GRPO with Reward-Based Curriculum Sampling", "categories": ["cs.CL"], "comment": null, "summary": "Intent detection, a critical component in task-oriented dialogue (TOD)\nsystems, faces significant challenges in adapting to the rapid influx of\nintegrable tools with complex interrelationships. Existing approaches, such as\nzero-shot reformulations and LLM-based dynamic recognition, struggle with\nperformance degradation when encountering unseen intents, leading to erroneous\ntask routing. To enhance the model's generalization performance on unseen\ntasks, we employ Reinforcement Learning (RL) combined with a Reward-based\nCurriculum Sampling (RCS) during Group Relative Policy Optimization (GRPO)\ntraining in intent detection tasks. Experiments demonstrate that RL-trained\nmodels substantially outperform supervised fine-tuning (SFT) baselines in\ngeneralization. Besides, the introduction of the RCS, significantly bolsters\nthe effectiveness of RL in intent detection by focusing the model on\nchallenging cases during training. Moreover, incorporating Chain-of-Thought\n(COT) processes in RL notably improves generalization in complex intent\ndetection tasks, underscoring the importance of thought in challenging\nscenarios. This work advances the generalization of intent detection tasks,\noffering practical insights for deploying adaptable dialogue systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u548c\u5956\u52b1\u8bfe\u7a0b\u91c7\u6837\uff08RCS\uff09\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u5347\u4efb\u52a1\u5bfc\u5411\u5bf9\u8bdd\u7cfb\u7edf\u4e2d\u610f\u56fe\u68c0\u6d4b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u76d1\u7763\u5fae\u8c03\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u9047\u5230\u672a\u89c1\u610f\u56fe\u65f6\u6027\u80fd\u4e0b\u964d\uff0c\u5bfc\u81f4\u4efb\u52a1\u8def\u7531\u9519\u8bef\uff0c\u9700\u8981\u63d0\u5347\u6a21\u578b\u5bf9\u672a\u89c1\u4efb\u52a1\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u7ed3\u5408\u5956\u52b1\u8bfe\u7a0b\u91c7\u6837\uff08RCS\uff09\u548c\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\uff08GRPO\uff09\u8fdb\u884c\u8bad\u7ec3\uff0c\u5e76\u5f15\u5165\u601d\u7ef4\u94fe\uff08COT\uff09\u8fc7\u7a0b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cRL\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u6cdb\u5316\u6027\u80fd\u4e0a\u663e\u8457\u4f18\u4e8e\u76d1\u7763\u5fae\u8c03\u57fa\u7ebf\uff0cRCS\u548cCOT\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u6548\u679c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u5347\u4e86\u610f\u56fe\u68c0\u6d4b\u4efb\u52a1\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u90e8\u7f72\u9002\u5e94\u6027\u5f3a\u7684\u5bf9\u8bdd\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u89c1\u89e3\u3002"}}
{"id": "2504.13186", "pdf": "https://arxiv.org/pdf/2504.13186", "abs": "https://arxiv.org/abs/2504.13186", "authors": ["Yassine Habchi", "Hamza Kheddar", "Yassine Himeur", "Adel Belouchrani", "Erchin Serpedin", "Fouad Khelifi", "Muhammad E. H. Chowdhury"], "title": "Advanced Deep Learning and Large Language Models: Comprehensive Insights for Cancer Detection", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "The rapid advancement of deep learning (DL) has transformed healthcare,\nparticularly in cancer detection and diagnosis. DL surpasses traditional\nmachine learning and human accuracy, making it a critical tool for identifying\ndiseases. Despite numerous reviews on DL in healthcare, a comprehensive\nanalysis of its role in cancer detection remains limited. Existing studies\nfocus on specific aspects, leaving gaps in understanding its broader impact.\nThis paper addresses these gaps by reviewing advanced DL techniques, including\ntransfer learning (TL), reinforcement learning (RL), federated learning (FL),\nTransformers, and large language models (LLMs). These approaches enhance\naccuracy, tackle data scarcity, and enable decentralized learning while\nmaintaining data privacy. TL adapts pre-trained models to new datasets,\nimproving performance with limited labeled data. RL optimizes diagnostic\npathways and treatment strategies, while FL fosters collaborative model\ndevelopment without sharing sensitive data. Transformers and LLMs,\ntraditionally used in natural language processing, are now applied to medical\ndata for improved interpretability. Additionally, this review examines these\ntechniques' efficiency in cancer diagnosis, addresses challenges like data\nimbalance, and proposes solutions. It serves as a resource for researchers and\npractitioners, providing insights into current trends and guiding future\nresearch in advanced DL for cancer detection.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u6df1\u5ea6\u5b66\u4e60\u5728\u764c\u75c7\u68c0\u6d4b\u4e2d\u7684\u5148\u8fdb\u6280\u672f\uff0c\u586b\u8865\u4e86\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\uff0c\u5e76\u63a2\u8ba8\u4e86\u5176\u5e94\u7528\u4e0e\u6311\u6218\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u5728\u533b\u7597\u9886\u57df\uff0c\u5c24\u5176\u662f\u764c\u75c7\u68c0\u6d4b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u5bf9\u5176\u5168\u9762\u5f71\u54cd\u7684\u7efc\u5408\u5206\u6790\u3002", "method": "\u56de\u987e\u4e86\u8fc1\u79fb\u5b66\u4e60\u3001\u5f3a\u5316\u5b66\u4e60\u3001\u8054\u90a6\u5b66\u4e60\u3001Transformers\u548c\u5927\u8bed\u8a00\u6a21\u578b\u7b49\u5148\u8fdb\u6280\u672f\u3002", "result": "\u8fd9\u4e9b\u6280\u672f\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u3001\u89e3\u51b3\u4e86\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u5e76\u652f\u6301\u9690\u79c1\u4fdd\u62a4\u4e0b\u7684\u534f\u4f5c\u5b66\u4e60\u3002", "conclusion": "\u672c\u6587\u4e3a\u7814\u7a76\u8005\u548c\u4ece\u4e1a\u8005\u63d0\u4f9b\u4e86\u5f53\u524d\u8d8b\u52bf\u7684\u89c1\u89e3\uff0c\u5e76\u6307\u5bfc\u672a\u6765\u6df1\u5ea6\u5b66\u4e60\u5728\u764c\u75c7\u68c0\u6d4b\u4e2d\u7684\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2504.13368", "pdf": "https://arxiv.org/pdf/2504.13368", "abs": "https://arxiv.org/abs/2504.13368", "authors": ["Haoran Xu", "Shuozhe Li", "Harshit Sikchi", "Scott Niekum", "Amy Zhang"], "title": "An Optimal Discriminator Weighted Imitation Perspective for Reinforcement Learning", "categories": ["cs.LG", "cs.AI"], "comment": "ICLR 2025", "summary": "We introduce Iterative Dual Reinforcement Learning (IDRL), a new method that\ntakes an optimal discriminator-weighted imitation view of solving RL. Our\nmethod is motivated by a simple experiment in which we find training a\ndiscriminator using the offline dataset plus an additional expert dataset and\nthen performing discriminator-weighted behavior cloning gives strong results on\nvarious types of datasets. That optimal discriminator weight is quite similar\nto the learned visitation distribution ratio in Dual-RL, however, we find that\ncurrent Dual-RL methods do not correctly estimate that ratio. In IDRL, we\npropose a correction method to iteratively approach the optimal visitation\ndistribution ratio in the offline dataset given no addtional expert dataset.\nDuring each iteration, IDRL removes zero-weight suboptimal transitions using\nthe learned ratio from the previous iteration and runs Dual-RL on the remaining\nsubdataset. This can be seen as replacing the behavior visitation distribution\nwith the optimized visitation distribution from the previous iteration, which\ntheoretically gives a curriculum of improved visitation distribution ratios\nthat are closer to the optimal discriminator weight. We verify the\neffectiveness of IDRL on various kinds of offline datasets, including D4RL\ndatasets and more realistic corrupted demonstrations. IDRL beats strong\nPrimal-RL and Dual-RL baselines in terms of both performance and stability, on\nall datasets.", "AI": {"tldr": "IDRL\u662f\u4e00\u79cd\u65b0\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u8fed\u4ee3\u4f18\u5316\u5224\u522b\u5668\u52a0\u6743\u7684\u6a21\u4eff\u5b66\u4e60\u6765\u89e3\u51b3RL\u95ee\u9898\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u53d1\u73b0\u5f53\u524dDual-RL\u65b9\u6cd5\u672a\u80fd\u6b63\u786e\u4f30\u8ba1\u6700\u4f18\u5224\u522b\u5668\u6743\u91cd\uff0c\u63d0\u51faIDRL\u4ee5\u8fed\u4ee3\u903c\u8fd1\u6700\u4f18\u5206\u5e03\u6bd4\u4f8b\u3002", "method": "IDRL\u901a\u8fc7\u8fed\u4ee3\u79fb\u9664\u4f4e\u6743\u91cd\u6b21\u4f18\u8f6c\u79fb\u5e76\u8fd0\u884cDual-RL\uff0c\u9010\u6b65\u4f18\u5316\u5206\u5e03\u6bd4\u4f8b\u3002", "result": "\u5728\u591a\u79cd\u79bb\u7ebf\u6570\u636e\u96c6\u4e0a\uff0cIDRL\u5728\u6027\u80fd\u548c\u7a33\u5b9a\u6027\u4e0a\u5747\u4f18\u4e8ePrimal-RL\u548cDual-RL\u57fa\u7ebf\u3002", "conclusion": "IDRL\u901a\u8fc7\u8fed\u4ee3\u4f18\u5316\u5224\u522b\u5668\u6743\u91cd\uff0c\u663e\u8457\u63d0\u5347\u4e86\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u7684\u6027\u80fd\u4e0e\u7a33\u5b9a\u6027\u3002"}}
{"id": "2504.13603", "pdf": "https://arxiv.org/pdf/2504.13603", "abs": "https://arxiv.org/abs/2504.13603", "authors": ["Pin-Er Chen", "Da-Chen Lian", "Shu-Kai Hsieh", "Sieh-Chuen Huang", "Hsuan-Lei Shao", "Jun-Wei Chiu", "Yang-Hsien Lin", "Zih-Ching Chen", "Cheng-Kuang", "Eddie TC Huang", "Simon See"], "title": "Continual Pre-Training is (not) What You Need in Domain Adaption", "categories": ["cs.CL"], "comment": "11 pages, 2 figures", "summary": "The recent advances in Legal Large Language Models (LLMs) have transformed\nthe landscape of legal research and practice by automating tasks, enhancing\nresearch precision, and supporting complex decision-making processes. However,\neffectively adapting LLMs to the legal domain remains challenging due to the\ncomplexity of legal reasoning, the need for precise interpretation of\nspecialized language, and the potential for hallucinations. This paper examines\nthe efficacy of Domain-Adaptive Continual Pre-Training (DACP) in improving the\nlegal reasoning capabilities of LLMs. Through a series of experiments on legal\nreasoning tasks within the Taiwanese legal framework, we demonstrate that while\nDACP enhances domain-specific knowledge, it does not uniformly improve\nperformance across all legal tasks. We discuss the trade-offs involved in DACP,\nparticularly its impact on model generalization and performance in prompt-based\ntasks, and propose directions for future research to optimize domain adaptation\nstrategies in legal AI.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u9886\u57df\u81ea\u9002\u5e94\u6301\u7eed\u9884\u8bad\u7ec3\uff08DACP\uff09\u5728\u63d0\u5347\u6cd5\u5f8b\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u6cd5\u5f8b\u63a8\u7406\u80fd\u529b\u4e2d\u7684\u6548\u679c\uff0c\u53d1\u73b0\u5176\u5bf9\u7279\u5b9a\u4efb\u52a1\u6709\u63d0\u5347\u4f46\u5b58\u5728\u5c40\u9650\u6027\u3002", "motivation": "\u6cd5\u5f8bLLMs\u5728\u5e94\u7528\u4e2d\u9762\u4e34\u6cd5\u5f8b\u63a8\u7406\u590d\u6742\u3001\u4e13\u4e1a\u8bed\u8a00\u89e3\u91ca\u7cbe\u786e\u6027\u8981\u6c42\u9ad8\u53ca\u5e7b\u89c9\u95ee\u9898\u7b49\u6311\u6218\uff0c\u9700\u4f18\u5316\u9886\u57df\u9002\u5e94\u7b56\u7565\u3002", "method": "\u901a\u8fc7\u53f0\u6e7e\u6cd5\u5f8b\u6846\u67b6\u4e0b\u7684\u6cd5\u5f8b\u63a8\u7406\u4efb\u52a1\u5b9e\u9a8c\uff0c\u8bc4\u4f30DACP\u7684\u6548\u679c\u3002", "result": "DACP\u589e\u5f3a\u4e86\u9886\u57df\u77e5\u8bc6\uff0c\u4f46\u672a\u5728\u6240\u6709\u6cd5\u5f8b\u4efb\u52a1\u4e2d\u4e00\u81f4\u63d0\u5347\u6027\u80fd\uff0c\u4e14\u5f71\u54cd\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u672a\u6765\u7814\u7a76\u9700\u4f18\u5316\u6cd5\u5f8bAI\u7684\u9886\u57df\u9002\u5e94\u7b56\u7565\uff0c\u5e73\u8861\u77e5\u8bc6\u589e\u5f3a\u4e0e\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2504.13191", "pdf": "https://arxiv.org/pdf/2504.13191", "abs": "https://arxiv.org/abs/2504.13191", "authors": ["Nam Nguyen"], "title": "Universal Representations for Classification-enhanced Lossy Compression", "categories": ["cs.CV", "cs.AI", "cs.IT", "math.IT"], "comment": null, "summary": "In lossy compression, the classical tradeoff between compression rate and\nreconstruction distortion has traditionally guided algorithm design. However,\nBlau and Michaeli [5] introduced a generalized framework, known as the\nrate-distortion-perception (RDP) function, incorporating perceptual quality as\nan additional dimension of evaluation. More recently, the\nrate-distortion-classification (RDC) function was investigated in [19],\nevaluating compression performance by considering classification accuracy\nalongside distortion. In this paper, we explore universal representations,\nwhere a single encoder is developed to achieve multiple decoding objectives\nacross various distortion and classification (or perception) constraints. This\nuniversality avoids retraining encoders for each specific operating point\nwithin these tradeoffs. Our experimental validation on the MNIST dataset\nindicates that a universal encoder incurs only minimal performance degradation\ncompared to individually optimized encoders for perceptual image compression\ntasks, aligning with prior results from [23]. Nonetheless, we also identify\nthat in the RDC setting, reusing an encoder optimized for one specific\nclassification-distortion tradeoff leads to a significant distortion penalty\nwhen applied to alternative points.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5728\u635f\u5931\u538b\u7f29\u4e2d\uff0c\u901a\u8fc7\u901a\u7528\u7f16\u7801\u5668\u5b9e\u73b0\u591a\u89e3\u7801\u76ee\u6807\u7684\u65b9\u6cd5\uff0c\u907f\u514d\u4e86\u4e3a\u6bcf\u4e2a\u7279\u5b9a\u64cd\u4f5c\u70b9\u91cd\u65b0\u8bad\u7ec3\u7f16\u7801\u5668\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u611f\u77e5\u56fe\u50cf\u538b\u7f29\u4efb\u52a1\u4e2d\uff0c\u901a\u7528\u7f16\u7801\u5668\u6027\u80fd\u63a5\u8fd1\u5355\u72ec\u4f18\u5316\u7684\u7f16\u7801\u5668\uff0c\u4f46\u5728RDC\u8bbe\u7f6e\u4e2d\uff0c\u91cd\u7528\u7f16\u7801\u5668\u4f1a\u5bfc\u81f4\u663e\u8457\u7684\u5931\u771f\u60e9\u7f5a\u3002", "motivation": "\u4f20\u7edf\u538b\u7f29\u7b97\u6cd5\u4ec5\u5173\u6ce8\u538b\u7f29\u7387\u4e0e\u91cd\u6784\u5931\u771f\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u800c\u65b0\u6846\u67b6\uff08\u5982RDP\u548cRDC\uff09\u5f15\u5165\u4e86\u611f\u77e5\u8d28\u91cf\u6216\u5206\u7c7b\u51c6\u786e\u6027\u4f5c\u4e3a\u989d\u5916\u8bc4\u4f30\u7ef4\u5ea6\u3002\u672c\u6587\u65e8\u5728\u5f00\u53d1\u901a\u7528\u7f16\u7801\u5668\uff0c\u4ee5\u652f\u6301\u591a\u76ee\u6807\u89e3\u7801\u3002", "method": "\u63d0\u51fa\u901a\u7528\u7f16\u7801\u5668\u65b9\u6cd5\uff0c\u901a\u8fc7\u5355\u4e00\u7f16\u7801\u5668\u9002\u5e94\u591a\u79cd\u5931\u771f\u548c\u5206\u7c7b\uff08\u6216\u611f\u77e5\uff09\u7ea6\u675f\uff0c\u907f\u514d\u4e3a\u6bcf\u4e2a\u64cd\u4f5c\u70b9\u91cd\u65b0\u8bad\u7ec3\u3002\u5b9e\u9a8c\u5728MNIST\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u5728\u611f\u77e5\u56fe\u50cf\u538b\u7f29\u4efb\u52a1\u4e2d\uff0c\u901a\u7528\u7f16\u7801\u5668\u6027\u80fd\u63a5\u8fd1\u5355\u72ec\u4f18\u5316\u7684\u7f16\u7801\u5668\uff1b\u4f46\u5728RDC\u8bbe\u7f6e\u4e2d\uff0c\u91cd\u7528\u7f16\u7801\u5668\u4f1a\u5bfc\u81f4\u663e\u8457\u7684\u5931\u771f\u60e9\u7f5a\u3002", "conclusion": "\u901a\u7528\u7f16\u7801\u5668\u5728\u611f\u77e5\u538b\u7f29\u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728RDC\u8bbe\u7f6e\u4e2d\u9700\u8c28\u614e\u4f7f\u7528\uff0c\u4ee5\u907f\u514d\u6027\u80fd\u4e0b\u964d\u3002"}}
{"id": "2504.13388", "pdf": "https://arxiv.org/pdf/2504.13388", "abs": "https://arxiv.org/abs/2504.13388", "authors": ["Yegor Klochkov"], "title": "A mean teacher algorithm for unlearning of language models", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "One of the goals of language model unlearning is to reduce memorization of\nselected text instances while retaining the model's general abilities. Despite\nvarious proposed methods, reducing memorization of large datasets without\nnoticeable degradation in model utility remains challenging. In this paper, we\ninvestigate the mean teacher algorithm (Tarvainen & Valpola, 2017), a simple\nproximal optimization method from continual learning literature that gradually\nmodifies the teacher model. We show that the mean teacher can approximate a\ntrajectory of a slow natural gradient descent (NGD), which inherently seeks\nlow-curvature updates that are less likely to degrade the model utility. While\nslow NGD can suffer from vanishing gradients, we introduce a new unlearning\nloss called \"negative log-unlikelihood\" (NLUL) that avoids this problem. We\nshow that the combination of mean teacher and NLUL improves some metrics on the\nMUSE benchmarks (Shi et al., 2024).", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u8bed\u8a00\u6a21\u578b\u9057\u5fd8\u6280\u672f\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5747\u503c\u6559\u5e08\u7b97\u6cd5\u548c\u8d1f\u5bf9\u6570\u975e\u4f3c\u7136\u635f\u5931\uff08NLUL\uff09\u7684\u65b9\u6cd5\uff0c\u4ee5\u51cf\u5c11\u6a21\u578b\u5bf9\u7279\u5b9a\u6587\u672c\u7684\u8bb0\u5fc6\uff0c\u540c\u65f6\u4fdd\u6301\u5176\u901a\u7528\u80fd\u529b\u3002", "motivation": "\u51cf\u5c11\u8bed\u8a00\u6a21\u578b\u5bf9\u7279\u5b9a\u6587\u672c\u7684\u8bb0\u5fc6\uff0c\u540c\u65f6\u907f\u514d\u6a21\u578b\u6027\u80fd\u7684\u663e\u8457\u4e0b\u964d\uff0c\u662f\u5f53\u524d\u7814\u7a76\u7684\u6311\u6218\u3002", "method": "\u91c7\u7528\u5747\u503c\u6559\u5e08\u7b97\u6cd5\u548c\u8d1f\u5bf9\u6570\u975e\u4f3c\u7136\u635f\u5931\uff08NLUL\uff09\uff0c\u901a\u8fc7\u7f13\u6162\u7684\u81ea\u7136\u68af\u5ea6\u4e0b\u964d\uff08NGD\uff09\u8f68\u8ff9\u5b9e\u73b0\u4f4e\u66f2\u7387\u66f4\u65b0\u3002", "result": "\u5728MUSE\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u90e8\u5206\u6307\u6807\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u7ed3\u5408\u5747\u503c\u6559\u5e08\u548cNLUL\u7684\u65b9\u6cd5\u5728\u51cf\u5c11\u8bb0\u5fc6\u7684\u540c\u65f6\u4fdd\u6301\u4e86\u6a21\u578b\u6027\u80fd\uff0c\u4e3a\u89e3\u51b3\u9057\u5fd8\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2504.13615", "pdf": "https://arxiv.org/pdf/2504.13615", "abs": "https://arxiv.org/abs/2504.13615", "authors": ["Ritwik Mishra", "Rajiv Ratn Shah", "Ponnurangam Kumaraguru"], "title": "Long-context Non-factoid Question Answering in Indic Languages", "categories": ["cs.CL"], "comment": null, "summary": "Question Answering (QA) tasks, which involve extracting answers from a given\ncontext, are relatively straightforward for modern Large Language Models (LLMs)\nwhen the context is short. However, long contexts pose challenges due to the\nquadratic complexity of the self-attention mechanism. This challenge is\ncompounded in Indic languages, which are often low-resource. This study\nexplores context-shortening techniques, including Open Information Extraction\n(OIE), coreference resolution, Answer Paragraph Selection (APS), and their\ncombinations, to improve QA performance. Compared to the baseline of\nunshortened (long) contexts, our experiments on four Indic languages (Hindi,\nTamil, Telugu, and Urdu) demonstrate that context-shortening techniques yield\nan average improvement of 4\\% in semantic scores and 47\\% in token-level scores\nwhen evaluated on three popular LLMs without fine-tuning. Furthermore, with\nfine-tuning, we achieve an average increase of 2\\% in both semantic and\ntoken-level scores. Additionally, context-shortening reduces computational\noverhead. Explainability techniques like LIME and SHAP reveal that when the APS\nmodel confidently identifies the paragraph containing the answer, nearly all\ntokens within the selected text receive high relevance scores. However, the\nstudy also highlights the limitations of LLM-based QA systems in addressing\nnon-factoid questions, particularly those requiring reasoning or debate.\nMoreover, verbalizing OIE-generated triples does not enhance system\nperformance. These findings emphasize the potential of context-shortening\ntechniques to improve the efficiency and effectiveness of LLM-based QA systems,\nespecially for low-resource languages. The source code and resources are\navailable at https://github.com/ritwikmishra/IndicGenQA.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5728\u957f\u4e0a\u4e0b\u6587\u73af\u5883\u4e0b\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u7f29\u77ed\u6280\u672f\uff08\u5982OIE\u3001\u5171\u6307\u6d88\u89e3\u548cAPS\uff09\u63d0\u5347\u4f4e\u8d44\u6e90\u5370\u5730\u8bed\u7cfb\u8bed\u8a00\uff08\u5982\u5370\u5730\u8bed\u3001\u6cf0\u7c73\u5c14\u8bed\u3001\u6cf0\u5362\u56fa\u8bed\u548c\u4e4c\u5c14\u90fd\u8bed\uff09\u7684\u95ee\u7b54\u4efb\u52a1\u6027\u80fd\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8fd9\u4e9b\u6280\u672f\u5728\u4e0d\u5fae\u8c03LLMs\u65f6\u5e73\u5747\u63d0\u5347\u8bed\u4e49\u5206\u65704%\u548c\u8bcd\u7ea7\u5206\u657047%\uff0c\u5fae\u8c03\u540e\u8fdb\u4e00\u6b65\u63d0\u53472%\u3002\u540c\u65f6\uff0c\u4e0a\u4e0b\u6587\u7f29\u77ed\u51cf\u5c11\u4e86\u8ba1\u7b97\u5f00\u9500\u3002", "motivation": "\u957f\u4e0a\u4e0b\u6587\u548c\u4f4e\u8d44\u6e90\u8bed\u8a00\uff08\u5982\u5370\u5730\u8bed\u7cfb\u8bed\u8a00\uff09\u5bf9\u73b0\u4ee3LLMs\u7684\u95ee\u7b54\u4efb\u52a1\u6784\u6210\u6311\u6218\uff0c\u5c24\u5176\u662f\u81ea\u6ce8\u610f\u529b\u673a\u5236\u7684\u4e8c\u6b21\u590d\u6742\u5ea6\u95ee\u9898\u3002", "method": "\u91c7\u7528\u4e0a\u4e0b\u6587\u7f29\u77ed\u6280\u672f\uff08OIE\u3001\u5171\u6307\u6d88\u89e3\u3001APS\u53ca\u5176\u7ec4\u5408\uff09\u4f18\u5316\u95ee\u7b54\u6027\u80fd\uff0c\u5e76\u5728\u56db\u79cd\u5370\u5730\u8bed\u7cfb\u8bed\u8a00\u4e0a\u9a8c\u8bc1\u3002", "result": "\u4e0a\u4e0b\u6587\u7f29\u77ed\u6280\u672f\u663e\u8457\u63d0\u5347\u8bed\u4e49\u548c\u8bcd\u7ea7\u5206\u6570\uff0c\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\u3002APS\u6a21\u578b\u80fd\u9ad8\u7f6e\u4fe1\u5ea6\u8bc6\u522b\u7b54\u6848\u6bb5\u843d\u65f6\uff0c\u76f8\u5173\u6587\u672c\u5f97\u5206\u9ad8\u3002\u4f46LLMs\u5728\u5904\u7406\u975e\u4e8b\u5b9e\u6027\u95ee\u9898\uff08\u5982\u63a8\u7406\u6216\u8fa9\u8bba\uff09\u65f6\u4ecd\u6709\u5c40\u9650\u3002", "conclusion": "\u4e0a\u4e0b\u6587\u7f29\u77ed\u6280\u672f\u80fd\u6709\u6548\u63d0\u5347\u4f4e\u8d44\u6e90\u8bed\u8a00\u95ee\u7b54\u7cfb\u7edf\u7684\u6548\u7387\u548c\u6027\u80fd\uff0c\u4f46\u9700\u8fdb\u4e00\u6b65\u6539\u8fdb\u4ee5\u5e94\u5bf9\u975e\u4e8b\u5b9e\u6027\u95ee\u9898\u3002"}}
{"id": "2504.13192", "pdf": "https://arxiv.org/pdf/2504.13192", "abs": "https://arxiv.org/abs/2504.13192", "authors": ["Liang-bo Ning", "Shijie Wang", "Wenqi Fan", "Qing Li", "Xin Xu", "Hao Chen", "Feiran Huang"], "title": "CheatAgent: Attacking LLM-Empowered Recommender Systems via LLM Agent", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Recently, Large Language Model (LLM)-empowered recommender systems (RecSys)\nhave brought significant advances in personalized user experience and have\nattracted considerable attention. Despite the impressive progress, the research\nquestion regarding the safety vulnerability of LLM-empowered RecSys still\nremains largely under-investigated. Given the security and privacy concerns, it\nis more practical to focus on attacking the black-box RecSys, where attackers\ncan only observe the system's inputs and outputs. However, traditional attack\napproaches employing reinforcement learning (RL) agents are not effective for\nattacking LLM-empowered RecSys due to the limited capabilities in processing\ncomplex textual inputs, planning, and reasoning. On the other hand, LLMs\nprovide unprecedented opportunities to serve as attack agents to attack RecSys\nbecause of their impressive capability in simulating human-like decision-making\nprocesses. Therefore, in this paper, we propose a novel attack framework called\nCheatAgent by harnessing the human-like capabilities of LLMs, where an\nLLM-based agent is developed to attack LLM-Empowered RecSys. Specifically, our\nmethod first identifies the insertion position for maximum impact with minimal\ninput modification. After that, the LLM agent is designed to generate\nadversarial perturbations to insert at target positions. To further improve the\nquality of generated perturbations, we utilize the prompt tuning technique to\nimprove attacking strategies via feedback from the victim RecSys iteratively.\nExtensive experiments across three real-world datasets demonstrate the\neffectiveness of our proposed attacking method.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCheatAgent\u7684\u65b0\u578b\u653b\u51fb\u6846\u67b6\uff0c\u5229\u7528LLM\u7684\u80fd\u529b\u653b\u51fbLLM\u8d4b\u80fd\u7684\u63a8\u8350\u7cfb\u7edf\uff0c\u901a\u8fc7\u6700\u5c0f\u8f93\u5165\u4fee\u6539\u548c\u8fed\u4ee3\u4f18\u5316\u751f\u6210\u5bf9\u6297\u6027\u6270\u52a8\u3002", "motivation": "\u5c3d\u7ba1LLM\u8d4b\u80fd\u7684\u63a8\u8350\u7cfb\u7edf\u5728\u4e2a\u6027\u5316\u7528\u6237\u4f53\u9a8c\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5176\u5b89\u5168\u6027\u6f0f\u6d1e\u7814\u7a76\u4ecd\u4e0d\u8db3\u3002\u8003\u8651\u5230\u5b89\u5168\u548c\u9690\u79c1\u95ee\u9898\uff0c\u653b\u51fb\u9ed1\u76d2\u63a8\u8350\u7cfb\u7edf\u66f4\u5177\u5b9e\u9645\u610f\u4e49\u3002", "method": "\u63d0\u51faCheatAgent\u6846\u67b6\uff0c\u5229\u7528LLM\u6a21\u62df\u4eba\u7c7b\u51b3\u7b56\u8fc7\u7a0b\uff0c\u8bc6\u522b\u6700\u4f73\u63d2\u5165\u4f4d\u7f6e\u5e76\u751f\u6210\u5bf9\u6297\u6027\u6270\u52a8\uff0c\u901a\u8fc7\u63d0\u793a\u8c03\u4f18\u6280\u672f\u8fed\u4ee3\u4f18\u5316\u653b\u51fb\u7b56\u7565\u3002", "result": "\u5728\u4e09\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u653b\u51fb\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "CheatAgent\u5c55\u793a\u4e86LLM\u4f5c\u4e3a\u653b\u51fb\u4ee3\u7406\u7684\u6f5c\u529b\uff0c\u4e3a\u63a8\u8350\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2504.13413", "pdf": "https://arxiv.org/pdf/2504.13413", "abs": "https://arxiv.org/abs/2504.13413", "authors": ["Haldun Balim", "Yang Hu", "Yuyang Zhang", "Na Li"], "title": "A Model-Based Approach to Imitation Learning through Multi-Step Predictions", "categories": ["cs.LG", "cs.RO", "cs.SY", "eess.SY"], "comment": null, "summary": "Imitation learning is a widely used approach for training agents to replicate\nexpert behavior in complex decision-making tasks. However, existing methods\noften struggle with compounding errors and limited generalization, due to the\ninherent challenge of error correction and the distribution shift between\ntraining and deployment. In this paper, we present a novel model-based\nimitation learning framework inspired by model predictive control, which\naddresses these limitations by integrating predictive modeling through\nmulti-step state predictions. Our method outperforms traditional behavior\ncloning numerical benchmarks, demonstrating superior robustness to distribution\nshift and measurement noise both in available data and during execution.\nFurthermore, we provide theoretical guarantees on the sample complexity and\nerror bounds of our method, offering insights into its convergence properties.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u7684\u65b0\u578b\u6a21\u4eff\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u6b65\u72b6\u6001\u9884\u6d4b\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u7684\u8bef\u5dee\u7d2f\u79ef\u548c\u6cdb\u5316\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u5728\u590d\u6742\u51b3\u7b56\u4efb\u52a1\u4e2d\u5b58\u5728\u8bef\u5dee\u7d2f\u79ef\u548c\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff0c\u901a\u8fc7\u591a\u6b65\u72b6\u6001\u9884\u6d4b\u8fdb\u884c\u5efa\u6a21\u3002", "result": "\u5728\u6570\u503c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u4f20\u7edf\u884c\u4e3a\u514b\u9686\u65b9\u6cd5\uff0c\u5bf9\u6570\u636e\u5206\u5e03\u504f\u79fb\u548c\u566a\u58f0\u5177\u6709\u66f4\u5f3a\u9c81\u68d2\u6027\u3002", "conclusion": "\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6536\u655b\u6027\u548c\u6027\u80fd\u4f18\u52bf\u3002"}}
{"id": "2504.13626", "pdf": "https://arxiv.org/pdf/2504.13626", "abs": "https://arxiv.org/abs/2504.13626", "authors": ["Yule Liu", "Jingyi Zheng", "Zhen Sun", "Zifan Peng", "Wenhan Dong", "Zeyang Sha", "Shiwen Cui", "Weiqiang Wang", "Xinlei He"], "title": "Thought Manipulation: External Thought Can Be Efficient for Large Reasoning Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advancements in large reasoning models (LRMs) have demonstrated the\neffectiveness of scaling test-time computation to enhance reasoning\ncapabilities in multiple tasks. However, LRMs typically suffer from\n\"overthinking\" problems, where models generate significantly redundant\nreasoning steps while bringing limited performance gains. Existing work relies\non fine-tuning to mitigate overthinking, which requires additional data,\nunconventional training setups, risky safety misalignment, and poor\ngeneralization.\n  Through empirical analysis, we reveal an important characteristic of LRM\nbehaviors that placing external CoTs generated by smaller models between the\nthinking token ($\\texttt{<think>}$ and $\\texttt{</think>)}$ can effectively\nmanipulate the model to generate fewer thoughts. Building on these insights, we\npropose a simple yet efficient pipeline, ThoughtMani, to enable LRMs to bypass\nunnecessary intermediate steps and reduce computational costs significantly. We\nconduct extensive experiments to validate the utility and efficiency of\nThoughtMani. For instance, when applied to QwQ-32B on the LiveBench/Code\ndataset, ThoughtMani keeps the original performance and reduces output token\ncounts by approximately 30%, with little overhead from the CoT generator.\nFurthermore, we find that ThoughtMani enhances safety alignment by an average\nof 10%. Since model vendors typically serve models of different sizes\nsimultaneously, ThoughtMani provides an effective way to construct more\nefficient and accessible LRMs for real-world applications.", "AI": {"tldr": "ThoughtMani\u901a\u8fc7\u5728\u5c0f\u6a21\u578b\u751f\u6210\u7684\u5916\u90e8CoT\u4e2d\u63d2\u5165\u601d\u8003\u6807\u8bb0\uff0c\u663e\u8457\u51cf\u5c11\u5927\u6a21\u578b\u7684\u5197\u4f59\u63a8\u7406\u6b65\u9aa4\uff0c\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u6027\u80fd\u5e76\u63d0\u5347\u5b89\u5168\u6027\u3002", "motivation": "\u5927\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u5b58\u5728\u201c\u8fc7\u5ea6\u601d\u8003\u201d\u95ee\u9898\uff0c\u5373\u751f\u6210\u5197\u4f59\u63a8\u7406\u6b65\u9aa4\u4f46\u6027\u80fd\u63d0\u5347\u6709\u9650\uff0c\u73b0\u6709\u65b9\u6cd5\u9700\u989d\u5916\u6570\u636e\u548c\u8c03\u6574\uff0c\u4e14\u6cdb\u5316\u6027\u5dee\u3002", "method": "\u63d0\u51faThoughtMani\uff0c\u901a\u8fc7\u5728\u5c0f\u6a21\u578b\u751f\u6210\u7684\u5916\u90e8CoT\u4e2d\u63d2\u5165\u601d\u8003\u6807\u8bb0\uff08<think>\u548c</think>\uff09\uff0c\u5f15\u5bfc\u5927\u6a21\u578b\u51cf\u5c11\u5197\u4f59\u63a8\u7406\u6b65\u9aa4\u3002", "result": "\u5728QwQ-32B\u4e0a\uff0cThoughtMani\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u51cf\u5c1130%\u8f93\u51fa\u6807\u8bb0\uff0c\u5b89\u5168\u6027\u63d0\u534710%\u3002", "conclusion": "ThoughtMani\u4e3a\u6784\u5efa\u9ad8\u6548\u3001\u5b89\u5168\u7684\u5927\u63a8\u7406\u6a21\u578b\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.13193", "pdf": "https://arxiv.org/pdf/2504.13193", "abs": "https://arxiv.org/abs/2504.13193", "authors": ["Hong Yang"], "title": "HEAT:History-Enhanced Dual-phase Actor-Critic Algorithm with A Shared Transformer", "categories": ["cs.NI", "cs.AI", "cs.LG"], "comment": null, "summary": "For a single-gateway LoRaWAN network, this study proposed a history-enhanced\ntwo-phase actor-critic algorithm with a shared transformer algorithm (HEAT) to\nimprove network performance. HEAT considers uplink parameters and often\nneglected downlink parameters, and effectively integrates offline and online\nreinforcement learning, using historical data and real-time interaction to\nimprove model performance. In addition, this study developed an open source\nLoRaWAN network simulator LoRaWANSim. The simulator considers the demodulator\nlock effect and supports multi-channel, multi-demodulator and bidirectional\ncommunication. Simulation experiments show that compared with the best results\nof all compared algorithms, HEAT improves the packet success rate and energy\nefficiency by 15% and 95%, respectively.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5386\u53f2\u589e\u5f3a\u7684\u4e24\u9636\u6bb5Actor-Critic\u7b97\u6cd5\uff08HEAT\uff09\uff0c\u7ed3\u5408\u5171\u4eabTransformer\u7b97\u6cd5\uff0c\u7528\u4e8e\u63d0\u5347\u5355\u7f51\u5173LoRaWAN\u7f51\u7edc\u6027\u80fd\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u8003\u8651\u4e0a\u884c\u548c\u4e0b\u884c\u53c2\u6570\uff0c\u7ed3\u5408\u79bb\u7ebf\u548c\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\uff0c\u63d0\u5347\u7f51\u7edc\u6027\u80fd\u3002", "method": "\u91c7\u7528HEAT\u7b97\u6cd5\uff0c\u7ed3\u5408\u5386\u53f2\u6570\u636e\u548c\u5b9e\u65f6\u4ea4\u4e92\uff0c\u5e76\u5f00\u53d1\u4e86\u5f00\u6e90\u6a21\u62df\u5668LoRaWANSim\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cHEAT\u76f8\u6bd4\u5176\u4ed6\u7b97\u6cd5\uff0c\u5305\u6210\u529f\u7387\u548c\u80fd\u6548\u5206\u522b\u63d0\u534715%\u548c95%\u3002", "conclusion": "HEAT\u7b97\u6cd5\u548cLoRaWANSim\u6a21\u62df\u5668\u6709\u6548\u63d0\u5347\u4e86LoRaWAN\u7f51\u7edc\u7684\u6027\u80fd\u3002"}}
{"id": "2504.13416", "pdf": "https://arxiv.org/pdf/2504.13416", "abs": "https://arxiv.org/abs/2504.13416", "authors": ["Saksham Rastogi", "Pratyush Maini", "Danish Pruthi"], "title": "STAMP Your Content: Proving Dataset Membership via Watermarked Rephrasings", "categories": ["cs.LG", "cs.CL", "cs.CR"], "comment": "Accepted at DATA-FM, WMark @ ICLR 2025. Project page at see\n  https://codeboy5.github.io/stamp", "summary": "Given how large parts of publicly available text are crawled to pretrain\nlarge language models (LLMs), data creators increasingly worry about the\ninclusion of their proprietary data for model training without attribution or\nlicensing. Their concerns are also shared by benchmark curators whose test-sets\nmight be compromised. In this paper, we present STAMP, a framework for\ndetecting dataset membership-i.e., determining the inclusion of a dataset in\nthe pretraining corpora of LLMs. Given an original piece of content, our\nproposal involves first generating multiple rephrases, each embedding a\nwatermark with a unique secret key. One version is to be released publicly,\nwhile others are to be kept private. Subsequently, creators can compare model\nlikelihoods between public and private versions using paired statistical tests\nto prove membership. We show that our framework can successfully detect\ncontamination across four benchmarks which appear only once in the training\ndata and constitute less than 0.001% of the total tokens, outperforming several\ncontamination detection and dataset inference baselines. We verify that STAMP\npreserves both the semantic meaning and the utility of the original data in\ncomparing different models. We apply STAMP to two real-world scenarios to\nconfirm the inclusion of paper abstracts and blog articles in the pretraining\ncorpora.", "AI": {"tldr": "STAMP\u662f\u4e00\u4e2a\u7528\u4e8e\u68c0\u6d4b\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u9884\u8bad\u7ec3\u6570\u636e\u96c6\u4e2d\u662f\u5426\u5305\u542b\u7279\u5b9a\u6570\u636e\u96c6\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u6210\u5e26\u6c34\u5370\u7684\u6539\u5199\u7248\u672c\u5e76\u6bd4\u8f83\u6a21\u578b\u4f3c\u7136\u6027\u6765\u9a8c\u8bc1\u6570\u636e\u6210\u5458\u8d44\u683c\u3002", "motivation": "\u6570\u636e\u521b\u5efa\u8005\u548c\u57fa\u51c6\u6d4b\u8bd5\u7ba1\u7406\u8005\u62c5\u5fc3\u5176\u4e13\u6709\u6570\u636e\u672a\u7ecf\u8bb8\u53ef\u6216\u5f52\u5c5e\u88ab\u7528\u4e8eLLM\u8bad\u7ec3\uff0c\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u68c0\u6d4b\u6570\u636e\u662f\u5426\u88ab\u5305\u542b\u5728\u9884\u8bad\u7ec3\u6570\u636e\u4e2d\u3002", "method": "STAMP\u6846\u67b6\u901a\u8fc7\u751f\u6210\u5e26\u552f\u4e00\u5bc6\u94a5\u6c34\u5370\u7684\u591a\u4e2a\u6539\u5199\u7248\u672c\uff0c\u516c\u5f00\u4e00\u4e2a\u7248\u672c\u5e76\u4fdd\u7559\u5176\u4ed6\u7248\u672c\uff0c\u968f\u540e\u901a\u8fc7\u914d\u5bf9\u7edf\u8ba1\u6d4b\u8bd5\u6bd4\u8f83\u516c\u5f00\u548c\u79c1\u6709\u7248\u672c\u7684\u6a21\u578b\u4f3c\u7136\u6027\u6765\u9a8c\u8bc1\u6570\u636e\u6210\u5458\u8d44\u683c\u3002", "result": "STAMP\u6210\u529f\u68c0\u6d4b\u5230\u56db\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u6570\u636e\u96c6\u7684\u6c61\u67d3\uff0c\u5373\u4f7f\u8fd9\u4e9b\u6570\u636e\u4ec5\u51fa\u73b0\u4e00\u6b21\u4e14\u5360\u603b\u6807\u8bb0\u7684\u4e0d\u52300.001%\uff0c\u4f18\u4e8e\u5176\u4ed6\u6c61\u67d3\u68c0\u6d4b\u548c\u6570\u636e\u96c6\u63a8\u65ad\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "STAMP\u4e0d\u4ec5\u80fd\u6709\u6548\u68c0\u6d4b\u6570\u636e\u6210\u5458\u8d44\u683c\uff0c\u8fd8\u80fd\u4fdd\u7559\u539f\u59cb\u6570\u636e\u7684\u8bed\u4e49\u548c\u5b9e\u7528\u6027\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u573a\u666f\u5982\u8bba\u6587\u6458\u8981\u548c\u535a\u5ba2\u6587\u7ae0\u7684\u68c0\u6d4b\u3002"}}
{"id": "2504.13629", "pdf": "https://arxiv.org/pdf/2504.13629", "abs": "https://arxiv.org/abs/2504.13629", "authors": ["Cong William Lin", "Wu Zhu"], "title": "Divergent LLM Adoption and Heterogeneous Convergence Paths in Research Writing", "categories": ["cs.CL", "cs.AI", "econ.GN", "q-fin.EC"], "comment": null, "summary": "Large Language Models (LLMs), such as ChatGPT, are reshaping content creation\nand academic writing. This study investigates the impact of AI-assisted\ngenerative revisions on research manuscripts, focusing on heterogeneous\nadoption patterns and their influence on writing convergence. Leveraging a\ndataset of over 627,000 academic papers from arXiv, we develop a novel\nclassification framework by fine-tuning prompt- and discipline-specific large\nlanguage models to detect the style of ChatGPT-revised texts. Our findings\nreveal substantial disparities in LLM adoption across academic disciplines,\ngender, native language status, and career stage, alongside a rapid evolution\nin scholarly writing styles. Moreover, LLM usage enhances clarity, conciseness,\nand adherence to formal writing conventions, with improvements varying by\nrevision type. Finally, a difference-in-differences analysis shows that while\nLLMs drive convergence in academic writing, early adopters, male researchers,\nnon-native speakers, and junior scholars exhibit the most pronounced stylistic\nshifts, aligning their writing more closely with that of established\nresearchers.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86AI\u8f85\u52a9\u751f\u6210\u4fee\u8ba2\u5bf9\u5b66\u672f\u8bba\u6587\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u4e0d\u540c\u5b66\u79d1\u3001\u6027\u522b\u3001\u6bcd\u8bed\u72b6\u6001\u548c\u804c\u4e1a\u9636\u6bb5\u7684LLM\u91c7\u7528\u5dee\u5f02\u663e\u8457\uff0c\u4e14LLM\u4f7f\u7528\u63d0\u5347\u4e86\u5199\u4f5c\u6e05\u6670\u5ea6\u548c\u89c4\u8303\u6027\u3002", "motivation": "\u63a2\u8ba8AI\uff08\u5982ChatGPT\uff09\u5bf9\u5b66\u672f\u5199\u4f5c\u7684\u5f71\u54cd\uff0c\u7279\u522b\u662fLLM\u8f85\u52a9\u4fee\u8ba2\u7684\u5f02\u8d28\u6027\u91c7\u7528\u6a21\u5f0f\u53ca\u5176\u5bf9\u5199\u4f5c\u98ce\u683c\u8d8b\u540c\u7684\u4f5c\u7528\u3002", "method": "\u57fa\u4e8earXiv\u7684627,000\u7bc7\u8bba\u6587\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u5fae\u8c03\u9886\u57df\u548c\u63d0\u793a\u7279\u5b9a\u7684LLM\uff0c\u5f00\u53d1\u5206\u7c7b\u6846\u67b6\u4ee5\u68c0\u6d4bChatGPT\u4fee\u8ba2\u6587\u672c\u7684\u98ce\u683c\u3002", "result": "LLM\u91c7\u7528\u5728\u4e0d\u540c\u7fa4\u4f53\u4e2d\u5dee\u5f02\u663e\u8457\uff0c\u4f7f\u7528\u540e\u5199\u4f5c\u98ce\u683c\u66f4\u6e05\u6670\u3001\u7b80\u6d01\u4e14\u89c4\u8303\uff0c\u65e9\u671f\u91c7\u7528\u8005\u3001\u7537\u6027\u3001\u975e\u6bcd\u8bed\u8005\u548c\u521d\u7ea7\u5b66\u8005\u98ce\u683c\u53d8\u5316\u6700\u660e\u663e\u3002", "conclusion": "LLM\u63a8\u52a8\u4e86\u5b66\u672f\u5199\u4f5c\u7684\u8d8b\u540c\uff0c\u4f46\u91c7\u7528\u6548\u679c\u56e0\u7fa4\u4f53\u800c\u5f02\uff0c\u9700\u5173\u6ce8\u6f5c\u5728\u7684\u4e0d\u5e73\u7b49\u73b0\u8c61\u3002"}}
{"id": "2504.13194", "pdf": "https://arxiv.org/pdf/2504.13194", "abs": "https://arxiv.org/abs/2504.13194", "authors": ["Hong Yang"], "title": "Optimizing Multi-Gateway LoRaWAN via Cloud-Edge Collaboration and Knowledge Distillation", "categories": ["cs.NI", "cs.AI"], "comment": null, "summary": "For large-scale multi-gateway LoRaWAN networks, this study proposes a\ncloud-edge collaborative resource allocation and decision-making method based\non edge intelligence, HEAT-LDL (HEAT-Local Distill Lyapunov), which realizes\ncollaborative decision-making between gateways and terminal nodes. HEAT-LDL\ncombines the Actor-Critic architecture and the Lyapunov optimization method to\nachieve intelligent downlink control and gateway load balancing. When the\nsignal quality is good, the network server uses the HEAT algorithm to schedule\nthe terminal nodes. To improve the efficiency of autonomous decision-making of\nterminal nodes, HEAT-LDL performs cloud-edge knowledge distillation on the HEAT\nteacher model on the terminal node side. When the downlink decision instruction\nis lost, the terminal node uses the student model and the edge decider based on\nprior knowledge and local history to make collaborative autonomous decisions.\nSimulation experiments show that compared with the optimal results of all\ncompared algorithms, HEAT-LDL improves the packet success rate and energy\nefficiency by 20.5% and 88.1%, respectively.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8fb9\u7f18\u667a\u80fd\u7684\u4e91\u8fb9\u534f\u540c\u8d44\u6e90\u5206\u914d\u4e0e\u51b3\u7b56\u65b9\u6cd5HEAT-LDL\uff0c\u7528\u4e8e\u5927\u89c4\u6a21\u591a\u7f51\u5173LoRaWAN\u7f51\u7edc\uff0c\u5b9e\u73b0\u4e86\u7f51\u5173\u4e0e\u7ec8\u7aef\u8282\u70b9\u7684\u534f\u540c\u51b3\u7b56\u3002", "motivation": "\u89e3\u51b3\u5927\u89c4\u6a21\u591a\u7f51\u5173LoRaWAN\u7f51\u7edc\u4e2d\u8d44\u6e90\u5206\u914d\u4e0e\u51b3\u7b56\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898\uff0c\u63d0\u5347\u7f51\u7edc\u6027\u80fd\u3002", "method": "\u7ed3\u5408Actor-Critic\u67b6\u6784\u548cLyapunov\u4f18\u5316\u65b9\u6cd5\uff0c\u5b9e\u73b0\u667a\u80fd\u4e0b\u884c\u63a7\u5236\u548c\u7f51\u5173\u8d1f\u8f7d\u5747\u8861\uff1b\u901a\u8fc7\u4e91\u8fb9\u77e5\u8bc6\u84b8\u998f\u63d0\u5347\u7ec8\u7aef\u8282\u70b9\u81ea\u4e3b\u51b3\u7b56\u6548\u7387\u3002", "result": "\u4eff\u771f\u5b9e\u9a8c\u8868\u660e\uff0cHEAT-LDL\u76f8\u6bd4\u5176\u4ed6\u7b97\u6cd5\uff0c\u5305\u6210\u529f\u7387\u548c\u80fd\u6548\u5206\u522b\u63d0\u534720.5%\u548c88.1%\u3002", "conclusion": "HEAT-LDL\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u7f51\u7edc\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21LoRaWAN\u7f51\u7edc\u7684\u8d44\u6e90\u5206\u914d\u4e0e\u51b3\u7b56\u3002"}}
{"id": "2504.13422", "pdf": "https://arxiv.org/pdf/2504.13422", "abs": "https://arxiv.org/abs/2504.13422", "authors": ["Vivek Oommen", "Andreas E. Robertson", "Daniel Diaz", "Coleman Alleman", "Zhen Zhang", "Anthony D. Rollett", "George E. Karniadakis", "R\u00e9mi Dingreville"], "title": "Equilibrium Conserving Neural Operators for Super-Resolution Learning", "categories": ["cs.LG", "physics.comp-ph"], "comment": null, "summary": "Neural surrogate solvers can estimate solutions to partial differential\nequations in physical problems more efficiently than standard numerical\nmethods, but require extensive high-resolution training data. In this paper, we\nbreak this limitation; we introduce a framework for super-resolution learning\nin solid mechanics problems. Our approach allows one to train a high-resolution\nneural network using only low-resolution data. Our Equilibrium Conserving\nOperator (ECO) architecture embeds known physics directly into the network to\nmake up for missing high-resolution information during training. We evaluate\nthis ECO-based super-resolution framework that strongly enforces\nconservation-laws in the predicted solutions on two working examples: embedded\npores in a homogenized matrix and randomly textured polycrystalline materials.\nECO eliminates the reliance on high-fidelity data and reduces the upfront cost\nof data collection by two orders of magnitude, offering a robust pathway for\nresource-efficient surrogate modeling in materials modeling. ECO is readily\ngeneralizable to other physics-based problems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u7ea6\u675f\u7684\u8d85\u7ea7\u5206\u8fa8\u7387\u5b66\u4e60\u6846\u67b6\uff08ECO\uff09\uff0c\u7528\u4e8e\u56fa\u4f53\u529b\u5b66\u95ee\u9898\uff0c\u65e0\u9700\u9ad8\u5206\u8fa8\u7387\u8bad\u7ec3\u6570\u636e\u5373\u53ef\u8bad\u7ec3\u9ad8\u5206\u8fa8\u7387\u795e\u7ecf\u7f51\u7edc\u3002", "motivation": "\u4f20\u7edf\u795e\u7ecf\u4ee3\u7406\u6c42\u89e3\u5668\u9700\u8981\u5927\u91cf\u9ad8\u5206\u8fa8\u7387\u6570\u636e\uff0c\u9650\u5236\u4e86\u5176\u6548\u7387\u3002\u672c\u6587\u65e8\u5728\u6253\u7834\u8fd9\u4e00\u9650\u5236\uff0c\u901a\u8fc7\u7269\u7406\u5d4c\u5165\u51cf\u5c11\u6570\u636e\u9700\u6c42\u3002", "method": "\u63d0\u51faECO\u67b6\u6784\uff0c\u5c06\u5df2\u77e5\u7269\u7406\u89c4\u5f8b\u76f4\u63a5\u5d4c\u5165\u7f51\u7edc\uff0c\u5f25\u8865\u8bad\u7ec3\u4e2d\u7f3a\u5931\u7684\u9ad8\u5206\u8fa8\u7387\u4fe1\u606f\uff0c\u5e76\u901a\u8fc7\u4e24\u4e2a\u6848\u4f8b\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "result": "ECO\u6846\u67b6\u51cf\u5c11\u4e86\u5bf9\u9ad8\u4fdd\u771f\u6570\u636e\u7684\u4f9d\u8d56\uff0c\u6570\u636e\u6536\u96c6\u6210\u672c\u964d\u4f4e\u4e24\u4e2a\u6570\u91cf\u7ea7\uff0c\u9002\u7528\u4e8e\u6750\u6599\u5efa\u6a21\u7b49\u7269\u7406\u95ee\u9898\u3002", "conclusion": "ECO\u4e3a\u8d44\u6e90\u9ad8\u6548\u7684\u4ee3\u7406\u5efa\u6a21\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\uff0c\u4e14\u6613\u4e8e\u63a8\u5e7f\u5230\u5176\u4ed6\u7269\u7406\u95ee\u9898\u3002"}}
{"id": "2504.13630", "pdf": "https://arxiv.org/pdf/2504.13630", "abs": "https://arxiv.org/abs/2504.13630", "authors": ["Shaomu Tan", "Christof Monz"], "title": "Remedy: Learning Machine Translation Evaluation from Human Preferences with Reward Modeling", "categories": ["cs.CL"], "comment": null, "summary": "A key challenge in MT evaluation is the inherent noise and inconsistency of\nhuman ratings. Regression-based neural metrics struggle with this noise, while\nprompting LLMs shows promise at system-level evaluation but performs poorly at\nsegment level. In this work, we propose ReMedy, a novel MT metric framework\nthat reformulates translation evaluation as a reward modeling task. Instead of\nregressing on imperfect human ratings directly, ReMedy learns relative\ntranslation quality using pairwise preference data, resulting in a more\nreliable evaluation. In extensive experiments across WMT22-24 shared tasks (39\nlanguage pairs, 111 MT systems), ReMedy achieves state-of-the-art performance\nat both segment- and system-level evaluation. Specifically, ReMedy-9B surpasses\nlarger WMT winners and massive closed LLMs such as MetricX-13B,\nXCOMET-Ensemble, GEMBA-GPT-4, PaLM-540B, and finetuned PaLM2. Further analyses\ndemonstrate that ReMedy delivers superior capability in detecting translation\nerrors and evaluating low-quality translations.", "AI": {"tldr": "ReMedy\u662f\u4e00\u79cd\u65b0\u7684MT\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u7ffb\u8bd1\u8bc4\u4f30\u91cd\u65b0\u5b9a\u4e49\u4e3a\u5956\u52b1\u5efa\u6a21\u4efb\u52a1\uff0c\u5229\u7528\u6210\u5bf9\u504f\u597d\u6570\u636e\u5b66\u4e60\u76f8\u5bf9\u7ffb\u8bd1\u8d28\u91cf\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bc4\u4f30\u7684\u53ef\u9760\u6027\u3002", "motivation": "MT\u8bc4\u4f30\u4e2d\u4eba\u7c7b\u8bc4\u5206\u7684\u566a\u58f0\u548c\u4e0d\u4e00\u81f4\u6027\u662f\u5173\u952e\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u7cfb\u7edf\u7ea7\u548c\u7247\u6bb5\u7ea7\u8bc4\u4f30\u4e2d\u5b58\u5728\u4e0d\u8db3\u3002", "method": "ReMedy\u6846\u67b6\u5c06\u7ffb\u8bd1\u8bc4\u4f30\u8f6c\u5316\u4e3a\u5956\u52b1\u5efa\u6a21\u4efb\u52a1\uff0c\u4f7f\u7528\u6210\u5bf9\u504f\u597d\u6570\u636e\u5b66\u4e60\u76f8\u5bf9\u7ffb\u8bd1\u8d28\u91cf\uff0c\u800c\u975e\u76f4\u63a5\u56de\u5f52\u4e0d\u5b8c\u7f8e\u7684\u4eba\u7c7b\u8bc4\u5206\u3002", "result": "\u5728WMT22-24\u5171\u4eab\u4efb\u52a1\uff0839\u79cd\u8bed\u8a00\u5bf9\uff0c111\u4e2aMT\u7cfb\u7edf\uff09\u4e2d\uff0cReMedy-9B\u5728\u7247\u6bb5\u7ea7\u548c\u7cfb\u7edf\u7ea7\u8bc4\u4f30\u4e2d\u5747\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u8d85\u8d8a\u591a\u4e2a\u5927\u578b\u6a21\u578b\u3002", "conclusion": "ReMedy\u5728\u68c0\u6d4b\u7ffb\u8bd1\u9519\u8bef\u548c\u8bc4\u4f30\u4f4e\u8d28\u91cf\u7ffb\u8bd1\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4e3aMT\u8bc4\u4f30\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.13196", "pdf": "https://arxiv.org/pdf/2504.13196", "abs": "https://arxiv.org/abs/2504.13196", "authors": ["Leonid Legashev", "Arthur Zhigalov"], "title": "Investigating cybersecurity incidents using large language models in latest-generation wireless networks", "categories": ["cs.CR", "cs.AI"], "comment": "11 pages, 2 figures", "summary": "The purpose of research: Detection of cybersecurity incidents and analysis of\ndecision support and assessment of the effectiveness of measures to counter\ninformation security threats based on modern generative models. The methods of\nresearch: Emulation of signal propagation data in MIMO systems, synthesis of\nadversarial examples, execution of adversarial attacks on machine learning\nmodels, fine tuning of large language models for detecting adversarial attacks,\nexplainability of decisions on detecting cybersecurity incidents based on the\nprompts technique. Scientific novelty: A binary classification of data\npoisoning attacks was performed using large language models, and the\npossibility of using large language models for investigating cybersecurity\nincidents in the latest generation wireless networks was investigated. The\nresult of research: Fine-tuning of large language models was performed on the\nprepared data of the emulated wireless network segment. Six large language\nmodels were compared for detecting adversarial attacks, and the capabilities of\nexplaining decisions made by a large language model were investigated. The\nGemma-7b model showed the best results according to the metrics Precision =\n0.89, Recall = 0.89 and F1-Score = 0.89. Based on various explainability\nprompts, the Gemma-7b model notes inconsistencies in the compromised data under\nstudy, performs feature importance analysis and provides various\nrecommendations for mitigating the consequences of adversarial attacks. Large\nlanguage models integrated with binary classifiers of network threats have\nsignificant potential for practical application in the field of cybersecurity\nincident investigation, decision support and assessing the effectiveness of\nmeasures to counter information security threats.", "AI": {"tldr": "\u7814\u7a76\u57fa\u4e8e\u73b0\u4ee3\u751f\u6210\u6a21\u578b\u68c0\u6d4b\u7f51\u7edc\u5b89\u5168\u4e8b\u4ef6\uff0c\u5206\u6790\u51b3\u7b56\u652f\u6301\u53ca\u8bc4\u4f30\u53cd\u5236\u63aa\u65bd\u6548\u679c\u3002Gemma-7b\u6a21\u578b\u5728\u68c0\u6d4b\u5bf9\u6297\u653b\u51fb\u4e2d\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u63a2\u7d22\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u68c0\u6d4b\u7f51\u7edc\u5b89\u5168\u4e8b\u4ef6\u53ca\u8bc4\u4f30\u53cd\u5236\u63aa\u65bd\u7684\u6709\u6548\u6027\uff0c\u7279\u522b\u662f\u5728\u65b0\u4e00\u4ee3\u65e0\u7ebf\u7f51\u7edc\u4e2d\u3002", "method": "\u6a21\u62dfMIMO\u7cfb\u7edf\u4fe1\u53f7\u4f20\u64ad\u6570\u636e\uff0c\u5408\u6210\u5bf9\u6297\u6837\u672c\uff0c\u6267\u884c\u5bf9\u6297\u653b\u51fb\uff0c\u5fae\u8c03\u5927\u8bed\u8a00\u6a21\u578b\u68c0\u6d4b\u653b\u51fb\uff0c\u5e76\u57fa\u4e8e\u63d0\u793a\u6280\u672f\u89e3\u91ca\u51b3\u7b56\u3002", "result": "Gemma-7b\u6a21\u578b\u5728\u68c0\u6d4b\u5bf9\u6297\u653b\u51fb\u4e2d\u8868\u73b0\u6700\u4f73\uff08Precision=0.89, Recall=0.89, F1-Score=0.89\uff09\uff0c\u5e76\u80fd\u5206\u6790\u7279\u5f81\u91cd\u8981\u6027\u548c\u63d0\u4f9b\u7f13\u89e3\u5efa\u8bae\u3002", "conclusion": "\u5927\u8bed\u8a00\u6a21\u578b\u4e0e\u7f51\u7edc\u5a01\u80c1\u4e8c\u5143\u5206\u7c7b\u5668\u7ed3\u5408\u5728\u7f51\u7edc\u5b89\u5168\u4e8b\u4ef6\u8c03\u67e5\u3001\u51b3\u7b56\u652f\u6301\u548c\u53cd\u5236\u63aa\u65bd\u8bc4\u4f30\u4e2d\u5177\u6709\u663e\u8457\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2504.13426", "pdf": "https://arxiv.org/pdf/2504.13426", "abs": "https://arxiv.org/abs/2504.13426", "authors": ["Jielong LuZhihao Wu", "Zhiling Cai", "Yueyang Pi", "Shiping Wang"], "title": "Simplifying Graph Convolutional Networks with Redundancy-Free Neighbors", "categories": ["cs.LG"], "comment": null, "summary": "In recent years, Graph Convolutional Networks (GCNs) have gained popularity\nfor their exceptional ability to process graph-structured data. Existing\nGCN-based approaches typically employ a shallow model architecture due to the\nover-smoothing phenomenon. Current approaches to mitigating over-smoothing\nprimarily involve adding supplementary components to GCN architectures, such as\nresidual connections and random edge-dropping strategies. However, these\nimprovements toward deep GCNs have achieved only limited success. In this work,\nwe analyze the intrinsic message passing mechanism of GCNs and identify a\ncritical issue: messages originating from high-order neighbors must traverse\nthrough low-order neighbors to reach the target node. This repeated reliance on\nlow-order neighbors leads to redundant information aggregation, a phenomenon we\nterm over-aggregation. Our analysis demonstrates that over-aggregation not only\nintroduces significant redundancy but also serves as the fundamental cause of\nover-smoothing in GCNs.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5206\u6790\u4e86\u56fe\u5377\u79ef\u7f51\u7edc\uff08GCNs\uff09\u4e2d\u7684\u6d88\u606f\u4f20\u9012\u673a\u5236\uff0c\u63d0\u51fa\u4e86\u201c\u8fc7\u5ea6\u805a\u5408\u201d\u73b0\u8c61\u662f\u5bfc\u81f4\u8fc7\u5e73\u6ed1\u7684\u6839\u672c\u539f\u56e0\u3002", "motivation": "\u73b0\u6709GCN\u6a21\u578b\u56e0\u8fc7\u5e73\u6ed1\u73b0\u8c61\u901a\u5e38\u91c7\u7528\u6d45\u5c42\u67b6\u6784\uff0c\u73b0\u6709\u6539\u8fdb\u65b9\u6cd5\u6548\u679c\u6709\u9650\uff0c\u9700\u6df1\u5165\u5206\u6790\u5176\u5185\u5728\u673a\u5236\u3002", "method": "\u901a\u8fc7\u5206\u6790GCN\u7684\u6d88\u606f\u4f20\u9012\u673a\u5236\uff0c\u8bc6\u522b\u51fa\u9ad8\u9636\u90bb\u5c45\u4fe1\u606f\u9700\u901a\u8fc7\u4f4e\u9636\u90bb\u5c45\u4f20\u9012\u7684\u95ee\u9898\u3002", "result": "\u53d1\u73b0\u8fc7\u5ea6\u805a\u5408\u4e0d\u4ec5\u5f15\u5165\u5197\u4f59\u4fe1\u606f\uff0c\u8fd8\u662f\u8fc7\u5e73\u6ed1\u7684\u6839\u672c\u539f\u56e0\u3002", "conclusion": "\u63ed\u793a\u4e86GCN\u4e2d\u8fc7\u5e73\u6ed1\u95ee\u9898\u7684\u6839\u6e90\uff0c\u4e3a\u672a\u6765\u6539\u8fdb\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2504.13643", "pdf": "https://arxiv.org/pdf/2504.13643", "abs": "https://arxiv.org/abs/2504.13643", "authors": ["Tao He", "Lizi Liao", "Ming Liu", "Bing Qin"], "title": "Simulating Before Planning: Constructing Intrinsic User World Model for User-Tailored Dialogue Policy Planning", "categories": ["cs.CL"], "comment": "11 pages, 6 figures, SIGIR 2025", "summary": "Recent advancements in dialogue policy planning have emphasized optimizing\nsystem agent policies to achieve predefined goals, focusing on strategy design,\ntrajectory acquisition, and efficient training paradigms. However, these\napproaches often overlook the critical role of user characteristics, which are\nessential in real-world scenarios like conversational search and\nrecommendation, where interactions must adapt to individual user traits such as\npersonality, preferences, and goals. To address this gap, we first conduct a\ncomprehensive study utilizing task-specific user personas to systematically\nassess dialogue policy planning under diverse user behaviors. By leveraging\nrealistic user profiles for different tasks, our study reveals significant\nlimitations in existing approaches, highlighting the need for user-tailored\ndialogue policy planning. Building on this foundation, we present the\nUser-Tailored Dialogue Policy Planning (UDP) framework, which incorporates an\nIntrinsic User World Model to model user traits and feedback. UDP operates in\nthree stages: (1) User Persona Portraying, using a diffusion model to\ndynamically infer user profiles; (2) User Feedback Anticipating, leveraging a\nBrownian Bridge-inspired anticipator to predict user reactions; and (3)\nUser-Tailored Policy Planning, integrating these insights to optimize response\nstrategies. To ensure robust performance, we further propose an active learning\napproach that prioritizes challenging user personas during training.\nComprehensive experiments on benchmarks, including collaborative and\nnon-collaborative settings, demonstrate the effectiveness of UDP in learning\nuser-specific dialogue strategies. Results validate the protocol's utility and\nhighlight UDP's robustness, adaptability, and potential to advance user-centric\ndialogue systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u7528\u6237\u5b9a\u5236\u5bf9\u8bdd\u7b56\u7565\u89c4\u5212\uff08UDP\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u5efa\u6a21\u7528\u6237\u7279\u5f81\u548c\u53cd\u9988\uff0c\u4f18\u5316\u5bf9\u8bdd\u7b56\u7565\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5ffd\u89c6\u7528\u6237\u7279\u6027\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u5bf9\u8bdd\u7b56\u7565\u89c4\u5212\u65b9\u6cd5\u5e38\u5ffd\u7565\u7528\u6237\u7279\u6027\uff0c\u800c\u5b9e\u9645\u573a\u666f\uff08\u5982\u5bf9\u8bdd\u641c\u7d22\u548c\u63a8\u8350\uff09\u4e2d\u7528\u6237\u7279\u5f81\u81f3\u5173\u91cd\u8981\u3002", "method": "UDP\u6846\u67b6\u5206\u4e09\u9636\u6bb5\uff1a\u7528\u6237\u753b\u50cf\u52a8\u6001\u63a8\u65ad\u3001\u7528\u6237\u53cd\u9988\u9884\u6d4b\u3001\u7528\u6237\u5b9a\u5236\u7b56\u7565\u89c4\u5212\uff0c\u5e76\u7ed3\u5408\u4e3b\u52a8\u5b66\u4e60\u4f18\u5316\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u8868\u660eUDP\u5728\u534f\u4f5c\u548c\u975e\u534f\u4f5c\u573a\u666f\u4e2d\u5747\u80fd\u6709\u6548\u5b66\u4e60\u7528\u6237\u7279\u5b9a\u7b56\u7565\uff0c\u9a8c\u8bc1\u4e86\u5176\u9c81\u68d2\u6027\u548c\u9002\u5e94\u6027\u3002", "conclusion": "UDP\u6846\u67b6\u901a\u8fc7\u7528\u6237\u7279\u6027\u5efa\u6a21\u548c\u53cd\u9988\u9884\u6d4b\uff0c\u63a8\u52a8\u4e86\u7528\u6237\u4e2d\u5fc3\u5bf9\u8bdd\u7cfb\u7edf\u7684\u53d1\u5c55\u3002"}}
{"id": "2504.13199", "pdf": "https://arxiv.org/pdf/2504.13199", "abs": "https://arxiv.org/abs/2504.13199", "authors": ["Mohammad Saleha", "Azadeh Tabatabaeib"], "title": "Building Trustworthy Multimodal AI: A Review of Fairness, Transparency, and Ethics in Vision-Language Tasks", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Objective: This review explores the trustworthiness of multimodal artificial\nintelligence (AI) systems, specifically focusing on vision-language tasks. It\naddresses critical challenges related to fairness, transparency, and ethical\nimplications in these systems, providing a comparative analysis of key tasks\nsuch as Visual Question Answering (VQA), image captioning, and visual dialogue.\nBackground: Multimodal models, particularly vision-language models, enhance\nartificial intelligence (AI) capabilities by integrating visual and textual\ndata, mimicking human learning processes. Despite significant advancements, the\ntrustworthiness of these models remains a crucial concern, particularly as AI\nsystems increasingly confront issues regarding fairness, transparency, and\nethics. Methods: This review examines research conducted from 2017 to 2024\nfocusing on forenamed core vision-language tasks. It employs a comparative\napproach to analyze these tasks through the lens of trustworthiness,\nunderlining fairness, explainability, and ethics. This study synthesizes\nfindings from recent literature to identify trends, challenges, and\nstate-of-the-art solutions. Results: Several key findings were highlighted.\nTransparency: Explainability of vision language tasks is important for user\ntrust. Techniques, such as attention maps and gradient-based methods, have\nsuccessfully addressed this issue. Fairness: Bias mitigation in VQA and visual\ndialogue systems is essential for ensuring unbiased outcomes across diverse\ndemographic groups. Ethical Implications: Addressing biases in multilingual\nmodels and ensuring ethical data handling is critical for the responsible\ndeployment of vision-language systems. Conclusion: This study underscores the\nimportance of integrating fairness, transparency, and ethical considerations in\ndeveloping vision-language models within a unified framework.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u591a\u6a21\u6001\u4eba\u5de5\u667a\u80fd\uff08AI\uff09\u7cfb\u7edf\u5728\u89c6\u89c9\u8bed\u8a00\u4efb\u52a1\u4e2d\u7684\u53ef\u4fe1\u5ea6\u95ee\u9898\uff0c\u91cd\u70b9\u63a2\u8ba8\u4e86\u516c\u5e73\u6027\u3001\u900f\u660e\u5ea6\u548c\u4f26\u7406\u6311\u6218\uff0c\u5e76\u6bd4\u8f83\u4e86VQA\u3001\u56fe\u50cf\u63cf\u8ff0\u548c\u89c6\u89c9\u5bf9\u8bdd\u7b49\u4efb\u52a1\u3002", "motivation": "\u968f\u7740\u591a\u6a21\u6001\u6a21\u578b\u7684\u53d1\u5c55\uff0c\u5176\u53ef\u4fe1\u5ea6\u95ee\u9898\u65e5\u76ca\u7a81\u51fa\uff0c\u5c24\u5176\u662f\u5728\u516c\u5e73\u6027\u3001\u900f\u660e\u5ea6\u548c\u4f26\u7406\u65b9\u9762\uff0c\u4e9f\u9700\u7cfb\u7edf\u6027\u7814\u7a76\u3002", "method": "\u901a\u8fc7\u6bd4\u8f83\u5206\u67902017\u81f32024\u5e74\u7684\u76f8\u5173\u7814\u7a76\uff0c\u4ece\u53ef\u4fe1\u5ea6\u89d2\u5ea6\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u4efb\u52a1\uff0c\u603b\u7ed3\u8d8b\u52bf\u3001\u6311\u6218\u548c\u89e3\u51b3\u65b9\u6848\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u900f\u660e\u5ea6\uff08\u5982\u6ce8\u610f\u529b\u56fe\uff09\u3001\u516c\u5e73\u6027\uff08\u5982\u504f\u89c1\u7f13\u89e3\uff09\u548c\u4f26\u7406\uff08\u5982\u591a\u8bed\u8a00\u6a21\u578b\u504f\u89c1\uff09\u662f\u63d0\u5347\u53ef\u4fe1\u5ea6\u7684\u5173\u952e\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u9700\u5c06\u516c\u5e73\u6027\u3001\u900f\u660e\u5ea6\u548c\u4f26\u7406\u7eb3\u5165\u7edf\u4e00\u6846\u67b6\uff0c\u4ee5\u5f00\u53d1\u66f4\u53ef\u4fe1\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u3002"}}
{"id": "2504.13429", "pdf": "https://arxiv.org/pdf/2504.13429", "abs": "https://arxiv.org/abs/2504.13429", "authors": ["Shenzhi Yang", "Bin Liang", "An Liu", "Lin Gui", "Xingkai Yao", "Xiaofang Zhang"], "title": "Bounded and Uniform Energy-based Out-of-distribution Detection for Graphs", "categories": ["cs.LG", "cs.AI"], "comment": "arXiv admin note: text overlap with arXiv:2302.02914 by other authors", "summary": "Given the critical role of graphs in real-world applications and their\nhigh-security requirements, improving the ability of graph neural networks\n(GNNs) to detect out-of-distribution (OOD) data is an urgent research problem.\nThe recent work GNNSAFE proposes a framework based on the aggregation of\nnegative energy scores that significantly improves the performance of GNNs to\ndetect node-level OOD data. However, our study finds that score aggregation\namong nodes is susceptible to extreme values due to the unboundedness of the\nnegative energy scores and logit shifts, which severely limits the accuracy of\nGNNs in detecting node-level OOD data. In this paper, we propose NODESAFE:\nreducing the generation of extreme scores of nodes by adding two optimization\nterms that make the negative energy scores bounded and mitigate the logit\nshift. Experimental results show that our approach dramatically improves the\nability of GNNs to detect OOD data at the node level, e.g., in detecting OOD\ndata induced by Structure Manipulation, the metric of FPR95 (lower is better)\nin scenarios without (with) OOD data exposure are reduced from the current SOTA\nby 28.4% (22.7%).", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faNODESAFE\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316\u8d1f\u80fd\u91cf\u5206\u6570\u548c\u7f13\u89e3\u903b\u8f91\u504f\u79fb\uff0c\u663e\u8457\u63d0\u5347\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u5728\u8282\u70b9\u7ea7\u522b\u68c0\u6d4b\u5206\u5e03\u5916\uff08OOD\uff09\u6570\u636e\u7684\u80fd\u529b\u3002", "motivation": "\u7531\u4e8e\u56fe\u5728\u73b0\u5b9e\u5e94\u7528\u4e2d\u7684\u91cd\u8981\u6027\u53ca\u5176\u9ad8\u5b89\u5168\u6027\u9700\u6c42\uff0c\u63d0\u5347GNN\u68c0\u6d4bOOD\u6570\u636e\u7684\u80fd\u529b\u6210\u4e3a\u8feb\u5207\u95ee\u9898\u3002\u73b0\u6709\u65b9\u6cd5GNNSAFE\u56e0\u5206\u6570\u65e0\u754c\u548c\u903b\u8f91\u504f\u79fb\u6613\u53d7\u6781\u7aef\u503c\u5f71\u54cd\u3002", "method": "\u63d0\u51faNODESAFE\uff0c\u901a\u8fc7\u6dfb\u52a0\u4e24\u4e2a\u4f18\u5316\u9879\u4f7f\u8d1f\u80fd\u91cf\u5206\u6570\u6709\u754c\u5e76\u7f13\u89e3\u903b\u8f91\u504f\u79fb\uff0c\u51cf\u5c11\u6781\u7aef\u5206\u6570\u7684\u751f\u6210\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cNODESAFE\u663e\u8457\u63d0\u5347OOD\u68c0\u6d4b\u80fd\u529b\uff0c\u4f8b\u5982\u5728\u7ed3\u6784\u64cd\u7eb5\u573a\u666f\u4e0b\uff0cFPR95\u6307\u6807\u8f83\u5f53\u524d\u6700\u4f18\u65b9\u6cd5\u964d\u4f4e28.4%\uff08\u65e0OOD\u6570\u636e\u66b4\u9732\uff09\u548c22.7%\uff08\u6709OOD\u6570\u636e\u66b4\u9732\uff09\u3002", "conclusion": "NODESAFE\u901a\u8fc7\u4f18\u5316\u5206\u6570\u8fb9\u754c\u548c\u903b\u8f91\u504f\u79fb\uff0c\u6709\u6548\u63d0\u5347\u4e86GNN\u5728\u8282\u70b9\u7ea7\u522b\u68c0\u6d4bOOD\u6570\u636e\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2504.13653", "pdf": "https://arxiv.org/pdf/2504.13653", "abs": "https://arxiv.org/abs/2504.13653", "authors": ["Hesham Abdelmotaleb", "Craig McNeile", "Malgorzata Wojtys"], "title": "Word Embedding Techniques for Classification of Star Ratings", "categories": ["cs.CL", "stat.AP", "62P99"], "comment": "40 pages", "summary": "Telecom services are at the core of today's societies' everyday needs. The\navailability of numerous online forums and discussion platforms enables telecom\nproviders to improve their services by exploring the views of their customers\nto learn about common issues that the customers face. Natural Language\nProcessing (NLP) tools can be used to process the free text collected.\n  One way of working with such data is to represent text as numerical vectors\nusing one of many word embedding models based on neural networks. This research\nuses a novel dataset of telecom customers' reviews to perform an extensive\nstudy showing how different word embedding algorithms can affect the text\nclassification process. Several state-of-the-art word embedding techniques are\nconsidered, including BERT, Word2Vec and Doc2Vec, coupled with several\nclassification algorithms. The important issue of feature engineering and\ndimensionality reduction is addressed and several PCA-based approaches are\nexplored. Moreover, the energy consumption used by the different word\nembeddings is investigated. The findings show that some word embedding models\ncan lead to consistently better text classifiers in terms of precision, recall\nand F1-Score. In particular, for the more challenging classification tasks,\nBERT combined with PCA stood out with the highest performance metrics.\nMoreover, our proposed PCA approach of combining word vectors using the first\nprincipal component shows clear advantages in performance over the traditional\napproach of taking the average.", "AI": {"tldr": "\u7814\u7a76\u6bd4\u8f83\u4e86\u4e0d\u540c\u8bcd\u5d4c\u5165\u6a21\u578b\uff08\u5982BERT\u3001Word2Vec\u548cDoc2Vec\uff09\u5728\u7535\u4fe1\u5ba2\u6237\u8bc4\u8bba\u5206\u7c7b\u4e2d\u7684\u6548\u679c\uff0c\u7ed3\u5408\u591a\u79cd\u5206\u7c7b\u7b97\u6cd5\u548cPCA\u964d\u7ef4\u65b9\u6cd5\uff0c\u53d1\u73b0BERT\u7ed3\u5408PCA\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u7535\u4fe1\u670d\u52a1\u9700\u6c42\u65e5\u76ca\u589e\u957f\uff0c\u901a\u8fc7\u5206\u6790\u5ba2\u6237\u8bc4\u8bba\u6539\u8fdb\u670d\u52a1\u662f\u91cd\u8981\u9014\u5f84\uff0c\u4f46\u9700\u9ad8\u6548\u5904\u7406\u81ea\u7531\u6587\u672c\u3002", "method": "\u4f7f\u7528\u591a\u79cd\u8bcd\u5d4c\u5165\u6a21\u578b\u548c\u5206\u7c7b\u7b97\u6cd5\uff0c\u7ed3\u5408PCA\u964d\u7ef4\u65b9\u6cd5\uff0c\u6bd4\u8f83\u6027\u80fd\u6307\u6807\uff08\u5982\u7cbe\u786e\u7387\u3001\u53ec\u56de\u7387\u548cF1\u5206\u6570\uff09\u3002", "result": "BERT\u7ed3\u5408PCA\u5728\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u6700\u4f18\uff0c\u63d0\u51fa\u7684PCA\u65b9\u6cd5\u4f18\u4e8e\u4f20\u7edf\u5e73\u5747\u6cd5\u3002", "conclusion": "\u7279\u5b9a\u8bcd\u5d4c\u5165\u6a21\u578b\uff08\u5982BERT\uff09\u7ed3\u5408PCA\u53ef\u663e\u8457\u63d0\u5347\u6587\u672c\u5206\u7c7b\u6027\u80fd\uff0c\u4e3a\u7535\u4fe1\u670d\u52a1\u6539\u8fdb\u63d0\u4f9b\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2504.13200", "pdf": "https://arxiv.org/pdf/2504.13200", "abs": "https://arxiv.org/abs/2504.13200", "authors": ["Mohammad Mahdi Danesh Pajouh"], "title": "Efficient Brain Tumor Segmentation Using a Dual-Decoder 3D U-Net with Attention Gates (DDUNet)", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Cancer remains one of the leading causes of mortality worldwide, and among\nits many forms, brain tumors are particularly notorious due to their aggressive\nnature and the critical challenges involved in early diagnosis. Recent advances\nin artificial intelligence have shown great promise in assisting medical\nprofessionals with precise tumor segmentation, a key step in timely diagnosis\nand treatment planning. However, many state-of-the-art segmentation methods\nrequire extensive computational resources and prolonged training times,\nlimiting their practical application in resource-constrained settings. In this\nwork, we present a novel dual-decoder U-Net architecture enhanced with\nattention-gated skip connections, designed specifically for brain tumor\nsegmentation from MRI scans. Our approach balances efficiency and accuracy by\nachieving competitive segmentation performance while significantly reducing\ntraining demands. Evaluated on the BraTS 2020 dataset, the proposed model\nachieved Dice scores of 85.06% for Whole Tumor (WT), 80.61% for Tumor Core\n(TC), and 71.26% for Enhancing Tumor (ET) in only 50 epochs, surpassing several\ncommonly used U-Net variants. Our model demonstrates that high-quality brain\ntumor segmentation is attainable even under limited computational resources,\nthereby offering a viable solution for researchers and clinicians operating\nwith modest hardware. This resource-efficient model has the potential to\nimprove early detection and diagnosis of brain tumors, ultimately contributing\nto better patient outcomes", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u53cc\u89e3\u7801\u5668U-Net\u67b6\u6784\uff0c\u7ed3\u5408\u6ce8\u610f\u529b\u95e8\u63a7\u8df3\u8dc3\u8fde\u63a5\uff0c\u7528\u4e8e\u8111\u80bf\u7624MRI\u5206\u5272\uff0c\u5728\u8d44\u6e90\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u6027\u80fd\u3002", "motivation": "\u8111\u80bf\u7624\u56e0\u5176\u4fb5\u88ad\u6027\u548c\u65e9\u671f\u8bca\u65ad\u56f0\u96be\u800c\u6210\u4e3a\u764c\u75c7\u6cbb\u7597\u4e2d\u7684\u91cd\u5927\u6311\u6218\uff0c\u73b0\u6709\u5206\u5272\u65b9\u6cd5\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u9ad8\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u8bbe\u8ba1\u4e86\u53cc\u89e3\u7801\u5668U-Net\u67b6\u6784\uff0c\u52a0\u5165\u6ce8\u610f\u529b\u95e8\u63a7\u8df3\u8dc3\u8fde\u63a5\uff0c\u4f18\u5316\u8bad\u7ec3\u6548\u7387\u3002", "result": "\u5728BraTS 2020\u6570\u636e\u96c6\u4e0a\uff0c\u4ec550\u8f6e\u8bad\u7ec3\u5373\u8fbe\u5230WT 85.06%\u3001TC 80.61%\u3001ET 71.26%\u7684Dice\u5206\u6570\uff0c\u4f18\u4e8e\u5e38\u89c1U-Net\u53d8\u4f53\u3002", "conclusion": "\u8be5\u6a21\u578b\u5728\u6709\u9650\u8ba1\u7b97\u8d44\u6e90\u4e0b\u4ecd\u80fd\u5b9e\u73b0\u9ad8\u8d28\u91cf\u5206\u5272\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684\u8111\u80bf\u7624\u65e9\u671f\u8bca\u65ad\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2504.13453", "pdf": "https://arxiv.org/pdf/2504.13453", "abs": "https://arxiv.org/abs/2504.13453", "authors": ["Vasista Ramachandruni", "Sai Hruday Reddy Nara", "Geo Lalu", "Sabrina Yang", "Mohit Ramesh Kumar", "Aarjav Jain", "Pratham Mehta", "Hankyu Koo", "Jason Damonte", "Marx Akl"], "title": "Using Machine Learning and Neural Networks to Analyze and Predict Chaos in Multi-Pendulum and Chaotic Systems", "categories": ["cs.LG", "nlin.CD"], "comment": "35 Pages, Approximately 20 figures", "summary": "A chaotic system is a highly volatile system characterized by its sensitive\ndependence on initial conditions and outside factors. Chaotic systems are\nprevalent throughout the world today: in weather patterns, disease outbreaks,\nand even financial markets. Chaotic systems are seen in every field of science\nand humanities, so being able to predict these systems is greatly beneficial to\nsociety. In this study, we evaluate 10 different machine learning models and\nneural networks [1] based on Root Mean Squared Error (RMSE) and R^2 values for\ntheir ability to predict one of these systems, the multi-pendulum. We begin by\ngenerating synthetic data representing the angles of the pendulum over time\nusing the Runge Kutta Method for solving 4th Order Differential Equations\n(ODE-RK4) [2]. At first, we used the single-step sliding window approach,\npredicting the 50st step after training for steps 0-49 and so forth. However,\nto more accurately cover chaotic motion and behavior in these systems, we\ntransitioned to a time-step based approach. Here, we trained the model/network\non many initial angles and tested it on a completely new set of initial angles,\nor 'in-between' to capture chaotic motion to its fullest extent. We also\nevaluated the stability of the system using Lyapunov exponents. We concluded\nthat for a double pendulum, the best model was the Long Short Term Memory\nNetwork (LSTM)[3] for the sliding window and time step approaches in both\nfriction and frictionless scenarios. For triple pendulum, the Vanilla Recurrent\nNeural Network (VRNN)[4] was the best for the sliding window and Gated\nRecurrent Network (GRU) [5] was the best for the time step approach, but for\nfriction, LSTM was the best.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8bc4\u4f30\u4e8610\u79cd\u673a\u5668\u5b66\u4e60\u548c\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u5728\u9884\u6d4b\u591a\u6446\u7cfb\u7edf\uff08\u4e00\u79cd\u6df7\u6c8c\u7cfb\u7edf\uff09\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0LSTM\u5728\u53cc\u6446\u7cfb\u7edf\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u800cVRNN\u548cGRU\u5728\u4e09\u6446\u7cfb\u7edf\u4e2d\u8868\u73b0\u8f83\u597d\u3002", "motivation": "\u6df7\u6c8c\u7cfb\u7edf\uff08\u5982\u5929\u6c14\u3001\u75be\u75c5\u4f20\u64ad\u3001\u91d1\u878d\u5e02\u573a\uff09\u5e7f\u6cdb\u5b58\u5728\u4e14\u96be\u4ee5\u9884\u6d4b\uff0c\u7814\u7a76\u5176\u9884\u6d4b\u65b9\u6cd5\u5bf9\u793e\u4f1a\u6709\u91cd\u8981\u610f\u4e49\u3002", "method": "\u4f7f\u7528Runge Kutta\u65b9\u6cd5\u751f\u6210\u5408\u6210\u6570\u636e\uff0c\u91c7\u7528\u5355\u6b65\u6ed1\u52a8\u7a97\u53e3\u548c\u65f6\u95f4\u6b65\u957f\u65b9\u6cd5\u8bad\u7ec3\u6a21\u578b\uff0c\u5e76\u901a\u8fc7RMSE\u548cR\u00b2\u8bc4\u4f30\u6027\u80fd\u3002", "result": "LSTM\u5728\u53cc\u6446\u7cfb\u7edf\u4e2d\u8868\u73b0\u6700\u4f73\uff1bVRNN\u548cGRU\u5728\u4e09\u6446\u7cfb\u7edf\u4e2d\u8868\u73b0\u8f83\u597d\uff0c\u4f46LSTM\u5728\u8003\u8651\u6469\u64e6\u65f6\u4ecd\u4e3a\u6700\u4f18\u3002", "conclusion": "LSTM\u662f\u9884\u6d4b\u6df7\u6c8c\u7cfb\u7edf\u7684\u6709\u6548\u5de5\u5177\uff0c\u5c24\u5176\u5728\u53cc\u6446\u7cfb\u7edf\u4e2d\u8868\u73b0\u7a81\u51fa\u3002"}}
{"id": "2504.13655", "pdf": "https://arxiv.org/pdf/2504.13655", "abs": "https://arxiv.org/abs/2504.13655", "authors": ["Jie Zou", "Cheng Lin", "Weikang Guo", "Zheng Wang", "Jiwei Wei", "Yang Yang", "Hengtao Shen"], "title": "Multi-Type Context-Aware Conversational Recommender Systems via Mixture-of-Experts", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "30 pages", "summary": "Conversational recommender systems enable natural language conversations and\nthus lead to a more engaging and effective recommendation scenario. As the\nconversations for recommender systems usually contain limited contextual\ninformation, many existing conversational recommender systems incorporate\nexternal sources to enrich the contextual information. However, how to combine\ndifferent types of contextual information is still a challenge. In this paper,\nwe propose a multi-type context-aware conversational recommender system, called\nMCCRS, effectively fusing multi-type contextual information via\nmixture-of-experts to improve conversational recommender systems. MCCRS\nincorporates both structured information and unstructured information,\nincluding the structured knowledge graph, unstructured conversation history,\nand unstructured item reviews. It consists of several experts, with each expert\nspecialized in a particular domain (i.e., one specific contextual information).\nMultiple experts are then coordinated by a ChairBot to generate the final\nresults. Our proposed MCCRS model takes advantage of different contextual\ninformation and the specialization of different experts followed by a ChairBot\nbreaks the model bottleneck on a single contextual information. Experimental\nresults demonstrate that our proposed MCCRS method achieves significantly\nhigher performance compared to existing baselines.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u7c7b\u578b\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u5bf9\u8bdd\u63a8\u8350\u7cfb\u7edfMCCRS\uff0c\u901a\u8fc7\u6df7\u5408\u4e13\u5bb6\u6a21\u578b\u878d\u5408\u591a\u79cd\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u8350\u6548\u679c\u3002", "motivation": "\u5bf9\u8bdd\u63a8\u8350\u7cfb\u7edf\u901a\u5e38\u7f3a\u4e4f\u8db3\u591f\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u7ed3\u5408\u4e0d\u540c\u7c7b\u578b\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\u3002", "method": "MCCRS\u878d\u5408\u4e86\u7ed3\u6784\u5316\u77e5\u8bc6\u56fe\u8c31\u3001\u975e\u7ed3\u6784\u5316\u5bf9\u8bdd\u5386\u53f2\u548c\u5546\u54c1\u8bc4\u8bba\uff0c\u901a\u8fc7\u591a\u4e2a\u4e13\u5bb6\u6a21\u578b\u548cChairBot\u534f\u8c03\u751f\u6210\u6700\u7ec8\u7ed3\u679c\u3002", "result": "\u5b9e\u9a8c\u8868\u660eMCCRS\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "MCCRS\u901a\u8fc7\u591a\u7c7b\u578b\u4e0a\u4e0b\u6587\u4fe1\u606f\u7684\u878d\u5408\u548c\u4e13\u5bb6\u6a21\u578b\u7684\u534f\u540c\u5de5\u4f5c\uff0c\u6709\u6548\u63d0\u5347\u4e86\u5bf9\u8bdd\u63a8\u8350\u7cfb\u7edf\u7684\u6027\u80fd\u3002"}}
{"id": "2504.13203", "pdf": "https://arxiv.org/pdf/2504.13203", "abs": "https://arxiv.org/abs/2504.13203", "authors": ["Salman Rahman", "Liwei Jiang", "James Shiffer", "Genglin Liu", "Sheriff Issaka", "Md Rizwan Parvez", "Hamid Palangi", "Kai-Wei Chang", "Yejin Choi", "Saadia Gabriel"], "title": "X-Teaming: Multi-Turn Jailbreaks and Defenses with Adaptive Multi-Agents", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG", "cs.MA"], "comment": null, "summary": "Multi-turn interactions with language models (LMs) pose critical safety\nrisks, as harmful intent can be strategically spread across exchanges. Yet, the\nvast majority of prior work has focused on single-turn safety, while\nadaptability and diversity remain among the key challenges of multi-turn\nred-teaming. To address these challenges, we present X-Teaming, a scalable\nframework that systematically explores how seemingly harmless interactions\nescalate into harmful outcomes and generates corresponding attack scenarios.\nX-Teaming employs collaborative agents for planning, attack optimization, and\nverification, achieving state-of-the-art multi-turn jailbreak effectiveness and\ndiversity with success rates up to 98.1% across representative leading\nopen-weight and closed-source models. In particular, X-Teaming achieves a 96.2%\nattack success rate against the latest Claude 3.7 Sonnet model, which has been\nconsidered nearly immune to single-turn attacks. Building on X-Teaming, we\nintroduce XGuard-Train, an open-source multi-turn safety training dataset that\nis 20x larger than the previous best resource, comprising 30K interactive\njailbreaks, designed to enable robust multi-turn safety alignment for LMs. Our\nwork offers essential tools and insights for mitigating sophisticated\nconversational attacks, advancing the multi-turn safety of LMs.", "AI": {"tldr": "X-Teaming\u662f\u4e00\u4e2a\u53ef\u6269\u5c55\u6846\u67b6\uff0c\u7528\u4e8e\u7cfb\u7edf\u7814\u7a76\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u7684\u5b89\u5168\u98ce\u9669\uff0c\u751f\u6210\u653b\u51fb\u573a\u666f\uff0c\u5e76\u5728\u591a\u8f6e\u8d8a\u72f1\u653b\u51fb\u4e2d\u8fbe\u523098.1%\u7684\u6210\u529f\u7387\u3002", "motivation": "\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u7684\u5b89\u5168\u98ce\u9669\u5c1a\u672a\u5145\u5206\u7814\u7a76\uff0c\u73b0\u6709\u5de5\u4f5c\u591a\u96c6\u4e2d\u4e8e\u5355\u8f6e\u653b\u51fb\uff0c\u800c\u591a\u8f6e\u653b\u51fb\u7684\u9002\u5e94\u6027\u548c\u591a\u6837\u6027\u662f\u4e3b\u8981\u6311\u6218\u3002", "method": "X-Teaming\u91c7\u7528\u534f\u4f5c\u4ee3\u7406\u8fdb\u884c\u89c4\u5212\u3001\u653b\u51fb\u4f18\u5316\u548c\u9a8c\u8bc1\uff0c\u751f\u6210\u591a\u8f6e\u653b\u51fb\u573a\u666f\u3002", "result": "X-Teaming\u5728\u591a\u8f6e\u8d8a\u72f1\u653b\u51fb\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u6210\u529f\u7387\u8fbe98.1%\uff0c\u5e76\u9488\u5bf9Claude 3.7 Sonnet\u6a21\u578b\u8fbe\u523096.2%\u7684\u6210\u529f\u7387\u3002", "conclusion": "X-Teaming\u548cXGuard-Train\u6570\u636e\u96c6\u4e3a\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u7684\u591a\u8f6e\u5b89\u5168\u6027\u63d0\u4f9b\u4e86\u91cd\u8981\u5de5\u5177\u548c\u8d44\u6e90\u3002"}}
{"id": "2504.13462", "pdf": "https://arxiv.org/pdf/2504.13462", "abs": "https://arxiv.org/abs/2504.13462", "authors": ["Hui Yeok Wong", "Chee Kau Lim", "Chee Seng Chan"], "title": "Stratify: Rethinking Federated Learning for Non-IID Data through Balanced Sampling", "categories": ["cs.LG"], "comment": null, "summary": "Federated Learning (FL) on non-independently and identically distributed\n(non-IID) data remains a critical challenge, as existing approaches struggle\nwith severe data heterogeneity. Current methods primarily address symptoms of\nnon-IID by applying incremental adjustments to Federated Averaging (FedAvg),\nrather than directly resolving its inherent design limitations. Consequently,\nperformance significantly deteriorates under highly heterogeneous conditions,\nas the fundamental issue of imbalanced exposure to diverse class and feature\ndistributions remains unresolved. This paper introduces Stratify, a novel FL\nframework designed to systematically manage class and feature distributions\nthroughout training, effectively tackling the root cause of non-IID challenges.\nInspired by classical stratified sampling, our approach employs a Stratified\nLabel Schedule (SLS) to ensure balanced exposure across labels, significantly\nreducing bias and variance in aggregated gradients. Complementing SLS, we\npropose a label-aware client selection strategy, restricting participation\nexclusively to clients possessing data relevant to scheduled labels.\nAdditionally, Stratify incorporates a fine-grained, high-frequency update\nscheme, accelerating convergence and further mitigating data heterogeneity. To\nuphold privacy, we implement a secure client selection protocol leveraging\nhomomorphic encryption, enabling precise global label statistics without\ndisclosing sensitive client information. Extensive evaluations on MNIST,\nCIFAR-10, CIFAR-100, Tiny-ImageNet, COVTYPE, PACS, and Digits-DG demonstrate\nthat Stratify attains performance comparable to IID baselines, accelerates\nconvergence, and reduces client-side computation compared to state-of-the-art\nmethods, underscoring its practical effectiveness in realistic federated\nlearning scenarios.", "AI": {"tldr": "Stratify\u662f\u4e00\u79cd\u65b0\u7684\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5c42\u6807\u7b7e\u8c03\u5ea6\u548c\u6807\u7b7e\u611f\u77e5\u7684\u5ba2\u6237\u7aef\u9009\u62e9\u7b56\u7565\uff0c\u6709\u6548\u89e3\u51b3\u975e\u72ec\u7acb\u540c\u5206\u5e03\u6570\u636e\u7684\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\u5728\u975e\u72ec\u7acb\u540c\u5206\u5e03\u6570\u636e\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u4ec5\u901a\u8fc7\u589e\u91cf\u8c03\u6574\u89e3\u51b3\u75c7\u72b6\u800c\u975e\u6839\u672c\u95ee\u9898\u3002", "method": "\u63d0\u51faStratify\u6846\u67b6\uff0c\u5305\u62ec\u5206\u5c42\u6807\u7b7e\u8c03\u5ea6\uff08SLS\uff09\u3001\u6807\u7b7e\u611f\u77e5\u5ba2\u6237\u7aef\u9009\u62e9\u548c\u7ec6\u7c92\u5ea6\u9ad8\u9891\u66f4\u65b0\u65b9\u6848\uff0c\u7ed3\u5408\u540c\u6001\u52a0\u5bc6\u4fdd\u62a4\u9690\u79c1\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0cStratify\u6027\u80fd\u63a5\u8fd1IID\u57fa\u7ebf\uff0c\u52a0\u901f\u6536\u655b\u5e76\u51cf\u5c11\u5ba2\u6237\u7aef\u8ba1\u7b97\u3002", "conclusion": "Stratify\u5728\u73b0\u5b9e\u8054\u90a6\u5b66\u4e60\u573a\u666f\u4e2d\u5177\u6709\u5b9e\u9645\u6709\u6548\u6027\uff0c\u89e3\u51b3\u4e86\u975e\u72ec\u7acb\u540c\u5206\u5e03\u6570\u636e\u7684\u6838\u5fc3\u95ee\u9898\u3002"}}
{"id": "2504.13677", "pdf": "https://arxiv.org/pdf/2504.13677", "abs": "https://arxiv.org/abs/2504.13677", "authors": ["Andrea Santilli", "Adam Golinski", "Michael Kirchhof", "Federico Danieli", "Arno Blaas", "Miao Xiong", "Luca Zappella", "Sinead Williamson"], "title": "Revisiting Uncertainty Quantification Evaluation in Language Models: Spurious Interactions with Response Length Bias Results", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Uncertainty Quantification (UQ) in Language Models (LMs) is crucial for\nimproving their safety and reliability. Evaluations often use performance\nmetrics like AUROC to assess how well UQ methods (e.g., negative sequence\nprobabilities) correlate with task correctness functions (e.g., ROUGE-L). In\nthis paper, we show that commonly used correctness functions bias UQ\nevaluations by inflating the performance of certain UQ methods. We evaluate 7\ncorrectness functions -- from lexical-based and embedding-based metrics to\nLLM-as-a-judge approaches -- across 4 datasets x 4 models x 6 UQ methods. Our\nanalysis reveals that length biases in the errors of these correctness\nfunctions distort UQ assessments by interacting with length biases in UQ\nmethods. We identify LLM-as-a-judge approaches as among the least length-biased\nchoices and hence a potential solution to mitigate these biases.", "AI": {"tldr": "\u8bba\u6587\u6307\u51fa\u8bed\u8a00\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\uff08UQ\uff09\u8bc4\u4f30\u4e2d\u5e38\u7528\u7684\u6b63\u786e\u6027\u51fd\u6570\u5b58\u5728\u957f\u5ea6\u504f\u5dee\uff0c\u5bfc\u81f4\u67d0\u4e9bUQ\u65b9\u6cd5\u8868\u73b0\u88ab\u5938\u5927\u3002\u901a\u8fc7\u5206\u6790\u591a\u79cd\u6b63\u786e\u6027\u51fd\u6570\uff0c\u53d1\u73b0LLM-as-a-judge\u65b9\u6cd5\u504f\u5dee\u6700\u5c0f\u3002", "motivation": "\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u9700\u8981\u51c6\u786e\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\uff08UQ\uff09\u8bc4\u4f30\uff0c\u4f46\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u56e0\u6b63\u786e\u6027\u51fd\u6570\u7684\u957f\u5ea6\u504f\u5dee\u800c\u5931\u771f\u3002", "method": "\u8bc4\u4f30\u4e867\u79cd\u6b63\u786e\u6027\u51fd\u6570\uff08\u5305\u62ec\u57fa\u4e8e\u8bcd\u6c47\u3001\u5d4c\u5165\u548cLLM-as-a-judge\u7684\u65b9\u6cd5\uff09\u57284\u4e2a\u6570\u636e\u96c6\u30014\u4e2a\u6a21\u578b\u548c6\u79cdUQ\u65b9\u6cd5\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u53d1\u73b0\u6b63\u786e\u6027\u51fd\u6570\u7684\u957f\u5ea6\u504f\u5dee\u4e0eUQ\u65b9\u6cd5\u7684\u957f\u5ea6\u504f\u5dee\u76f8\u4e92\u4f5c\u7528\uff0c\u5bfc\u81f4\u8bc4\u4f30\u5931\u771f\uff1bLLM-as-a-judge\u65b9\u6cd5\u504f\u5dee\u6700\u5c0f\u3002", "conclusion": "LLM-as-a-judge\u65b9\u6cd5\u53ef\u4f5c\u4e3a\u51cf\u5c11\u8bc4\u4f30\u504f\u5dee\u7684\u6f5c\u5728\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.13205", "pdf": "https://arxiv.org/pdf/2504.13205", "abs": "https://arxiv.org/abs/2504.13205", "authors": ["Houssam Kherraz"], "title": "On-Device Watermarking: A Socio-Technical Imperative For Authenticity In The Age of Generative AI", "categories": ["cs.CR", "cs.AI"], "comment": "10 pages, 3 figures, ICLR 2025,\n  https://openreview.net/forum?id=ygE0U21vxM", "summary": "As generative AI models produce increasingly realistic output, both academia\nand industry are focusing on the ability to detect whether an output was\ngenerated by an AI model or not. Many of the research efforts and policy\ndiscourse are centered around robust watermarking of AI outputs. While plenty\nof progress has been made, all watermarking and AI detection techniques face\nsevere limitations. In this position paper, we argue that we are adopting the\nwrong approach, and should instead focus on watermarking via cryptographic\nsignatures trustworthy content rather than AI generated ones. For audio-visual\ncontent, in particular, all real content is grounded in the physical world and\ncaptured via hardware sensors. This presents a unique opportunity to watermark\nat the hardware layer, and we lay out a socio-technical framework and draw\nparallels with HTTPS certification and Blu-Ray verification protocols. While\nacknowledging implementation challenges, we contend that hardware-based\nauthentication offers a more tractable path forward, particularly from a policy\nperspective. As generative models approach perceptual indistinguishability, the\nresearch community should be wary of being overly optimistic with AI\nwatermarking, and we argue that AI watermarking research efforts are better\nspent in the text and LLM space, which are ultimately not traceable to a\nphysical sensor.", "AI": {"tldr": "\u8bba\u6587\u4e3b\u5f20\u901a\u8fc7\u786c\u4ef6\u5c42\u52a0\u5bc6\u7b7e\u540d\u53ef\u4fe1\u5185\u5bb9\uff0c\u800c\u975eAI\u751f\u6210\u5185\u5bb9\uff0c\u63d0\u51fa\u786c\u4ef6\u8ba4\u8bc1\u662f\u66f4\u53ef\u884c\u7684\u65b9\u5411\u3002", "motivation": "\u968f\u7740\u751f\u6210\u5f0fAI\u8f93\u51fa\u8d8a\u6765\u8d8a\u903c\u771f\uff0c\u68c0\u6d4bAI\u751f\u6210\u5185\u5bb9\u7684\u9700\u6c42\u589e\u52a0\uff0c\u4f46\u73b0\u6709\u6c34\u5370\u548c\u68c0\u6d4b\u6280\u672f\u5b58\u5728\u4e25\u91cd\u5c40\u9650\u3002", "method": "\u63d0\u51fa\u901a\u8fc7\u786c\u4ef6\u4f20\u611f\u5668\u5bf9\u771f\u5b9e\u5185\u5bb9\u8fdb\u884c\u52a0\u5bc6\u7b7e\u540d\uff0c\u5efa\u7acb\u793e\u4f1a\u6280\u672f\u6846\u67b6\uff0c\u7c7b\u6bd4HTTPS\u8ba4\u8bc1\u548c\u84dd\u5149\u9a8c\u8bc1\u534f\u8bae\u3002", "result": "\u786c\u4ef6\u8ba4\u8bc1\u4e3a\u653f\u7b56\u5c42\u9762\u63d0\u4f9b\u4e86\u66f4\u53ef\u884c\u7684\u8def\u5f84\uff0c\u5c24\u5176\u5728\u89c6\u542c\u5185\u5bb9\u9886\u57df\u3002", "conclusion": "\u5efa\u8bae\u7814\u7a76\u793e\u533a\u5e94\u8b66\u60d5\u5bf9AI\u6c34\u5370\u7684\u8fc7\u5ea6\u4e50\u89c2\uff0c\u5c06\u7cbe\u529b\u96c6\u4e2d\u5728\u6587\u672c\u548cLLM\u9886\u57df\uff0c\u56e0\u5176\u65e0\u6cd5\u8ffd\u6eaf\u5230\u7269\u7406\u4f20\u611f\u5668\u3002"}}
{"id": "2504.13465", "pdf": "https://arxiv.org/pdf/2504.13465", "abs": "https://arxiv.org/abs/2504.13465", "authors": ["Duy A. Nguyen", "Quan Huu Do", "Khoa D. Doan", "Minh N. Do"], "title": "Are you SURE? Enhancing Multimodal Pretraining with Missing Modalities through Uncertainty Estimation", "categories": ["cs.LG"], "comment": null, "summary": "Multimodal learning has demonstrated incredible successes by integrating\ndiverse data sources, yet it often relies on the availability of all modalities\n- an assumption that rarely holds in real-world applications. Pretrained\nmultimodal models, while effective, struggle when confronted with small-scale\nand incomplete datasets (i.e., missing modalities), limiting their practical\napplicability. Previous studies on reconstructing missing modalities have\noverlooked the reconstruction's potential unreliability, which could compromise\nthe quality of the final outputs. We present SURE (Scalable Uncertainty and\nReconstruction Estimation), a novel framework that extends the capabilities of\npretrained multimodal models by introducing latent space reconstruction and\nuncertainty estimation for both reconstructed modalities and downstream tasks.\nOur method is architecture-agnostic, reconstructs missing modalities, and\ndelivers reliable uncertainty estimates, improving both interpretability and\nperformance. SURE introduces a unique Pearson Correlation-based loss and\napplies statistical error propagation in deep networks for the first time,\nallowing precise quantification of uncertainties from missing data and model\npredictions. Extensive experiments across tasks such as sentiment analysis,\ngenre classification, and action recognition show that SURE consistently\nachieves state-of-the-art performance, ensuring robust predictions even in the\npresence of incomplete data.", "AI": {"tldr": "SURE\u6846\u67b6\u901a\u8fc7\u6f5c\u5728\u7a7a\u95f4\u91cd\u5efa\u548c\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff0c\u63d0\u5347\u4e86\u9884\u8bad\u7ec3\u591a\u6a21\u6001\u6a21\u578b\u5728\u4e0d\u5b8c\u6574\u6570\u636e\u4e0b\u7684\u6027\u80fd\u3002", "motivation": "\u591a\u6a21\u6001\u5b66\u4e60\u901a\u5e38\u5047\u8bbe\u6240\u6709\u6a21\u6001\u53ef\u7528\uff0c\u4f46\u73b0\u5b9e\u4e2d\u6570\u636e\u5e38\u7f3a\u5931\u3002\u73b0\u6709\u65b9\u6cd5\u5ffd\u89c6\u91cd\u5efa\u7684\u4e0d\u53ef\u9760\u6027\uff0c\u5f71\u54cd\u8f93\u51fa\u8d28\u91cf\u3002", "method": "SURE\u5f15\u5165\u6f5c\u5728\u7a7a\u95f4\u91cd\u5efa\u548c\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff0c\u91c7\u7528Pearson\u76f8\u5173\u6027\u635f\u5931\u548c\u7edf\u8ba1\u8bef\u5dee\u4f20\u64ad\u6280\u672f\u3002", "result": "\u5728\u60c5\u611f\u5206\u6790\u3001\u7c7b\u578b\u5206\u7c7b\u548c\u52a8\u4f5c\u8bc6\u522b\u7b49\u4efb\u52a1\u4e2d\uff0cSURE\u8868\u73b0\u4f18\u5f02\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "SURE\u4e3a\u4e0d\u5b8c\u6574\u6570\u636e\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u591a\u6a21\u6001\u5b66\u4e60\u89e3\u51b3\u65b9\u6848\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2504.13685", "pdf": "https://arxiv.org/pdf/2504.13685", "abs": "https://arxiv.org/abs/2504.13685", "authors": ["Stefano M. Iacus", "Haodong Qi", "Jiyoung Han"], "title": "Deep literature reviews: an application of fine-tuned language models to migration research", "categories": ["cs.CL", "cs.LG", "stat.AP", "stat.CO"], "comment": null, "summary": "This paper presents a hybrid framework for literature reviews that augments\ntraditional bibliometric methods with large language models (LLMs). By\nfine-tuning open-source LLMs, our approach enables scalable extraction of\nqualitative insights from large volumes of research content, enhancing both the\nbreadth and depth of knowledge synthesis. To improve annotation efficiency and\nconsistency, we introduce an error-focused validation process in which LLMs\ngenerate initial labels and human reviewers correct misclassifications.\nApplying this framework to over 20000 scientific articles about human\nmigration, we demonstrate that a domain-adapted LLM can serve as a \"specialist\"\nmodel - capable of accurately selecting relevant studies, detecting emerging\ntrends, and identifying critical research gaps. Notably, the LLM-assisted\nreview reveals a growing scholarly interest in climate-induced migration.\nHowever, existing literature disproportionately centers on a narrow set of\nenvironmental hazards (e.g., floods, droughts, sea-level rise, and land\ndegradation), while overlooking others that more directly affect human health\nand well-being, such as air and water pollution or infectious diseases. This\nimbalance highlights the need for more comprehensive research that goes beyond\nphysical environmental changes to examine their ecological and societal\nconsequences, particularly in shaping migration as an adaptive response.\nOverall, our proposed framework demonstrates the potential of fine-tuned LLMs\nto conduct more efficient, consistent, and insightful literature reviews across\ndisciplines, ultimately accelerating knowledge synthesis and scientific\ndiscovery.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4f20\u7edf\u6587\u732e\u8ba1\u91cf\u65b9\u6cd5\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u6df7\u5408\u6846\u67b6\uff0c\u7528\u4e8e\u9ad8\u6548\u3001\u4e00\u81f4\u4e14\u6df1\u5165\u7684\u6587\u732e\u7efc\u8ff0\u3002", "motivation": "\u4f20\u7edf\u6587\u732e\u7efc\u8ff0\u65b9\u6cd5\u5728\u5904\u7406\u5927\u89c4\u6a21\u7814\u7a76\u5185\u5bb9\u65f6\u6548\u7387\u4f4e\u4e14\u4e00\u81f4\u6027\u4e0d\u8db3\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u63d0\u53d6\u5b9a\u6027\u89c1\u89e3\u3002", "method": "\u901a\u8fc7\u5fae\u8c03\u5f00\u6e90LLMs\uff0c\u7ed3\u5408\u4eba\u5de5\u9a8c\u8bc1\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u9519\u8bef\u805a\u7126\u7684\u9a8c\u8bc1\u6d41\u7a0b\uff0c\u4ee5\u63d0\u9ad8\u6807\u6ce8\u6548\u7387\u548c\u4e00\u81f4\u6027\u3002", "result": "\u5e94\u7528\u4e8e20,000\u591a\u7bc7\u5173\u4e8e\u4eba\u7c7b\u8fc1\u79fb\u7684\u79d1\u5b66\u6587\u7ae0\uff0c\u8bc1\u660e\u9886\u57df\u9002\u5e94\u7684LLM\u80fd\u51c6\u786e\u9009\u62e9\u76f8\u5173\u7814\u7a76\u3001\u68c0\u6d4b\u8d8b\u52bf\u5e76\u8bc6\u522b\u7814\u7a76\u7a7a\u767d\u3002", "conclusion": "\u8be5\u6846\u67b6\u5c55\u793a\u4e86\u5fae\u8c03LLMs\u5728\u591a\u5b66\u79d1\u6587\u732e\u7efc\u8ff0\u4e2d\u7684\u6f5c\u529b\uff0c\u53ef\u52a0\u901f\u77e5\u8bc6\u5408\u6210\u548c\u79d1\u5b66\u53d1\u73b0\u3002"}}
{"id": "2504.13208", "pdf": "https://arxiv.org/pdf/2504.13208", "abs": "https://arxiv.org/abs/2504.13208", "authors": ["Haomin Zuo", "Zhengyang Li", "Jiangchuan Gong", "Zhen Tian"], "title": "Intelligent road crack detection and analysis based on improved YOLOv8", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Accepted by IEEE - ICAACE 2025", "summary": "As urbanization speeds up and traffic flow increases, the issue of pavement\ndistress is becoming increasingly pronounced, posing a severe threat to road\nsafety and service life. Traditional methods of pothole detection rely on\nmanual inspection, which is not only inefficient but also costly. This paper\nproposes an intelligent road crack detection and analysis system, based on the\nenhanced YOLOv8 deep learning framework. A target segmentation model has been\ndeveloped through the training of 4029 images, capable of efficiently and\naccurately recognizing and segmenting crack regions in roads. The model also\nanalyzes the segmented regions to precisely calculate the maximum and minimum\nwidths of cracks and their exact locations. Experimental results indicate that\nthe incorporation of ECA and CBAM attention mechanisms substantially enhances\nthe model's detection accuracy and efficiency, offering a novel solution for\nroad maintenance and safety monitoring.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6539\u8fdbYOLOv8\u7684\u667a\u80fd\u9053\u8def\u88c2\u7f1d\u68c0\u6d4b\u7cfb\u7edf\uff0c\u901a\u8fc7\u8bad\u7ec34029\u5f20\u56fe\u50cf\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u51c6\u786e\u7684\u88c2\u7f1d\u8bc6\u522b\u4e0e\u5206\u5272\uff0c\u5e76\u7ed3\u5408\u6ce8\u610f\u529b\u673a\u5236\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u968f\u7740\u57ce\u5e02\u5316\u52a0\u901f\u548c\u4ea4\u901a\u6d41\u91cf\u589e\u52a0\uff0c\u9053\u8def\u75c5\u5bb3\u95ee\u9898\u65e5\u76ca\u7a81\u51fa\uff0c\u4f20\u7edf\u4eba\u5de5\u68c0\u6d4b\u65b9\u6cd5\u6548\u7387\u4f4e\u4e14\u6210\u672c\u9ad8\u3002", "method": "\u91c7\u7528\u589e\u5f3a\u7684YOLOv8\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408ECA\u548cCBAM\u6ce8\u610f\u529b\u673a\u5236\uff0c\u8bad\u7ec3\u76ee\u6807\u5206\u5272\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6a21\u578b\u80fd\u9ad8\u6548\u8bc6\u522b\u88c2\u7f1d\u5e76\u7cbe\u786e\u8ba1\u7b97\u5176\u5bbd\u5ea6\u548c\u4f4d\u7f6e\uff0c\u6ce8\u610f\u529b\u673a\u5236\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u7cbe\u5ea6\u548c\u6548\u7387\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u4e3a\u9053\u8def\u7ef4\u62a4\u548c\u5b89\u5168\u76d1\u6d4b\u63d0\u4f9b\u4e86\u65b0\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.13476", "pdf": "https://arxiv.org/pdf/2504.13476", "abs": "https://arxiv.org/abs/2504.13476", "authors": ["Jiadong Lou", "Bingqing Liu", "Yuanheng Xiong", "Xiaodong Zhang", "Xu Yuan"], "title": "Variational Autoencoder Framework for Hyperspectral Retrievals (Hyper-VAE) of Phytoplankton Absorption and Chlorophyll a in Coastal Waters for NASA's EMIT and PACE Missions", "categories": ["cs.LG", "cs.CV", "eess.IV"], "comment": null, "summary": "Phytoplankton absorb and scatter light in unique ways, subtly altering the\ncolor of water, changes that are often minor for human eyes to detect but can\nbe captured by sensitive ocean color instruments onboard satellites from space.\nHyperspectral sensors, paired with advanced algorithms, are expected to\nsignificantly enhance the characterization of phytoplankton community\ncomposition, especially in coastal waters where ocean color remote sensing\napplications have historically encountered significant challenges. This study\npresents novel machine learning-based solutions for NASA's hyperspectral\nmissions, including EMIT and PACE, tackling high-fidelity retrievals of\nphytoplankton absorption coefficient and chlorophyll a from their hyperspectral\nremote sensing reflectance. Given that a single Rrs spectrum may correspond to\nvaried combinations of inherent optical properties and associated\nconcentrations, the Variational Autoencoder (VAE) is used as a backbone in this\nstudy to handle such multi-distribution prediction problems. We first time\ntailor the VAE model with innovative designs to achieve hyperspectral\nretrievals of aphy and of Chl-a from hyperspectral Rrs in optically complex\nestuarine-coastal waters. Validation with extensive experimental observation\ndemonstrates superior performance of the VAE models with high precision and low\nbias. The in-depth analysis of VAE's advanced model structures and learning\ndesigns highlights the improvement and advantages of VAE-based solutions over\nthe mixture density network (MDN) approach, particularly on high-dimensional\ndata, such as PACE. Our study provides strong evidence that current EMIT and\nPACE hyperspectral data as well as the upcoming Surface Biology Geology mission\nwill open new pathways toward a better understanding of phytoplankton community\ndynamics in aquatic ecosystems when integrated with AI technologies.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5229\u7528\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08VAE\uff09\u4ece\u9ad8\u5149\u8c31\u9065\u611f\u53cd\u5c04\u7387\u4e2d\u9ad8\u7cbe\u5ea6\u53cd\u6f14\u6d6e\u6e38\u690d\u7269\u5438\u6536\u7cfb\u6570\u548c\u53f6\u7eff\u7d20a\uff0c\u5c55\u793a\u4e86\u5176\u5728\u590d\u6742\u6c34\u57df\u4e2d\u7684\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u6d6e\u6e38\u690d\u7269\u901a\u8fc7\u72ec\u7279\u65b9\u5f0f\u5438\u6536\u548c\u6563\u5c04\u5149\uff0c\u5fae\u5c0f\u53d8\u5316\u96be\u4ee5\u88ab\u8089\u773c\u5bdf\u89c9\u4f46\u53ef\u901a\u8fc7\u536b\u661f\u4f20\u611f\u5668\u6355\u6349\u3002\u9ad8\u5149\u8c31\u4f20\u611f\u5668\u7ed3\u5408\u5148\u8fdb\u7b97\u6cd5\u6709\u671b\u63d0\u5347\u6d6e\u6e38\u690d\u7269\u7fa4\u843d\u7ec4\u6210\u7684\u8868\u5f81\u80fd\u529b\uff0c\u5c24\u5176\u5728\u4f20\u7edf\u9065\u611f\u6280\u672f\u9762\u4e34\u6311\u6218\u7684\u6cbf\u6d77\u6c34\u57df\u3002", "method": "\u91c7\u7528\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08VAE\uff09\u4f5c\u4e3a\u6838\u5fc3\u6a21\u578b\uff0c\u9488\u5bf9\u9ad8\u5149\u8c31\u6570\u636e\u8bbe\u8ba1\u521b\u65b0\u7ed3\u6784\uff0c\u89e3\u51b3\u591a\u5206\u5e03\u9884\u6d4b\u95ee\u9898\uff0c\u5e76\u4e0e\u6df7\u5408\u5bc6\u5ea6\u7f51\u7edc\uff08MDN\uff09\u65b9\u6cd5\u8fdb\u884c\u5bf9\u6bd4\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u8868\u660e\uff0cVAE\u6a21\u578b\u5728\u590d\u6742\u6c34\u57df\u4e2d\u8868\u73b0\u51fa\u9ad8\u7cbe\u5ea6\u548c\u4f4e\u504f\u5dee\uff0c\u4f18\u4e8eMDN\u65b9\u6cd5\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u9ad8\u7ef4\u6570\u636e\uff08\u5982PACE\u4efb\u52a1\uff09\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u7ed3\u5408AI\u6280\u672f\u7684\u9ad8\u5149\u8c31\u6570\u636e\uff08\u5982EMIT\u3001PACE\u53ca\u5373\u5c06\u5f00\u5c55\u7684SBG\u4efb\u52a1\uff09\u5c06\u4e3a\u6d6e\u6e38\u690d\u7269\u7fa4\u843d\u52a8\u6001\u7814\u7a76\u5f00\u8f9f\u65b0\u9014\u5f84\u3002"}}
{"id": "2504.13730", "pdf": "https://arxiv.org/pdf/2504.13730", "abs": "https://arxiv.org/abs/2504.13730", "authors": ["Paul K. Mandal", "Cole Leo", "Connor Hurley"], "title": "Controlled Territory and Conflict Tracking (CONTACT): (Geo-)Mapping Occupied Territory from Open Source Intelligence", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7; I.2.6; I.2.8; H.3.1; K.4.1"], "comment": "7 pages, 1 figure, 1 table", "summary": "Open-source intelligence provides a stream of unstructured textual data that\ncan inform assessments of territorial control. We present CONTACT, a framework\nfor territorial control prediction using large language models (LLMs) and\nminimal supervision. We evaluate two approaches: SetFit, an embedding-based\nfew-shot classifier, and a prompt tuning method applied to BLOOMZ-560m, a\nmultilingual generative LLM. Our model is trained on a small hand-labeled\ndataset of news articles covering ISIS activity in Syria and Iraq, using\nprompt-conditioned extraction of control-relevant signals such as military\noperations, casualties, and location references. We show that the BLOOMZ-based\nmodel outperforms the SetFit baseline, and that prompt-based supervision\nimproves generalization in low-resource settings. CONTACT demonstrates that\nLLMs fine-tuned using few-shot methods can reduce annotation burdens and\nsupport structured inference from open-ended OSINT streams. Our code is\navailable at https://github.com/PaulKMandal/CONTACT/.", "AI": {"tldr": "CONTACT\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u548c\u5c11\u91cf\u76d1\u7763\u7684\u9886\u571f\u63a7\u5236\u9884\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u79cd\u65b9\u6cd5\uff08SetFit\u548cBLOOMZ-560m\uff09\u5728\u4f4e\u8d44\u6e90\u73af\u5883\u4e0b\u8868\u73b0\u4f18\u8d8a\u3002", "motivation": "\u5229\u7528\u5f00\u6e90\u60c5\u62a5\uff08OSINT\uff09\u7684\u975e\u7ed3\u6784\u5316\u6587\u672c\u6570\u636e\uff0c\u51cf\u5c11\u6807\u6ce8\u8d1f\u62c5\u5e76\u652f\u6301\u7ed3\u6784\u5316\u63a8\u7406\u3002", "method": "\u4f7f\u7528SetFit\uff08\u5d4c\u5165\u5f0f\u5c11\u6837\u672c\u5206\u7c7b\u5668\uff09\u548cBLOOMZ-560m\uff08\u591a\u8bed\u8a00\u751f\u6210LLM\uff09\u7684\u63d0\u793a\u8c03\u4f18\u65b9\u6cd5\uff0c\u57fa\u4e8e\u5c11\u91cf\u6807\u6ce8\u7684\u65b0\u95fb\u6587\u7ae0\u6570\u636e\u96c6\u8bad\u7ec3\u3002", "result": "BLOOMZ\u6a21\u578b\u4f18\u4e8eSetFit\u57fa\u7ebf\uff0c\u63d0\u793a\u76d1\u7763\u5728\u4f4e\u8d44\u6e90\u73af\u5883\u4e0b\u63d0\u5347\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "CONTACT\u8bc1\u660e\u5c11\u6837\u672c\u8c03\u4f18\u7684LLMs\u80fd\u6709\u6548\u51cf\u5c11\u6807\u6ce8\u9700\u6c42\uff0c\u652f\u6301\u4ece\u5f00\u653eOSINT\u6d41\u4e2d\u63d0\u53d6\u7ed3\u6784\u5316\u4fe1\u606f\u3002"}}
{"id": "2504.13209", "pdf": "https://arxiv.org/pdf/2504.13209", "abs": "https://arxiv.org/abs/2504.13209", "authors": ["Ting Bi", "Chenghang Ye", "Zheyu Yang", "Ziyi Zhou", "Cui Tang", "Jun Zhang", "Zui Tao", "Kailong Wang", "Liting Zhou", "Yang Yang", "Tianlong Yu"], "title": "On the Feasibility of Using MultiModal LLMs to Execute AR Social Engineering Attacks", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Augmented Reality (AR) and Multimodal Large Language Models (LLMs) are\nrapidly evolving, providing unprecedented capabilities for human-computer\ninteraction. However, their integration introduces a new attack surface for\nsocial engineering. In this paper, we systematically investigate the\nfeasibility of orchestrating AR-driven Social Engineering attacks using\nMultimodal LLM for the first time, via our proposed SEAR framework, which\noperates through three key phases: (1) AR-based social context synthesis, which\nfuses Multimodal inputs (visual, auditory and environmental cues); (2)\nrole-based Multimodal RAG (Retrieval-Augmented Generation), which dynamically\nretrieves and integrates contextual data while preserving character\ndifferentiation; and (3) ReInteract social engineering agents, which execute\nadaptive multiphase attack strategies through inference interaction loops. To\nverify SEAR, we conducted an IRB-approved study with 60 participants in three\nexperimental configurations (unassisted, AR+LLM, and full SEAR pipeline)\ncompiling a new dataset of 180 annotated conversations in simulated social\nscenarios. Our results show that SEAR is highly effective at eliciting\nhigh-risk behaviors (e.g., 93.3% of participants susceptible to email\nphishing). The framework was particularly effective in building trust, with 85%\nof targets willing to accept an attacker's call after an interaction. Also, we\nidentified notable limitations such as ``occasionally artificial'' due to\nperceived authenticity gaps. This work provides proof-of-concept for AR-LLM\ndriven social engineering attacks and insights for developing defensive\ncountermeasures against next-generation augmented reality threats.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86SEAR\u6846\u67b6\uff0c\u9996\u6b21\u7cfb\u7edf\u7814\u7a76\u4e86\u5229\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u548c\u589e\u5f3a\u73b0\u5b9e\uff08AR\uff09\u6280\u672f\u8fdb\u884c\u793e\u4ea4\u5de5\u7a0b\u653b\u51fb\u7684\u53ef\u884c\u6027\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u9ad8\u6548\u6027\u3002", "motivation": "AR\u548c\u591a\u6a21\u6001LLM\u7684\u5feb\u901f\u53d1\u5c55\u4e3a\u793e\u4ea4\u5de5\u7a0b\u653b\u51fb\u63d0\u4f9b\u4e86\u65b0\u7684\u653b\u51fb\u9762\uff0c\u672c\u6587\u65e8\u5728\u63a2\u7d22\u5176\u6f5c\u5728\u5a01\u80c1\u3002", "method": "SEAR\u6846\u67b6\u901a\u8fc7\u4e09\u4e2a\u9636\u6bb5\u5b9e\u73b0\u653b\u51fb\uff1aAR\u793e\u4ea4\u4e0a\u4e0b\u6587\u5408\u6210\u3001\u57fa\u4e8e\u89d2\u8272\u7684\u591a\u6a21\u6001RAG\u548cReInteract\u793e\u4ea4\u5de5\u7a0b\u4ee3\u7406\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u5176\u6548\u679c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793aSEAR\u6846\u67b6\u5728\u8bf1\u5bfc\u9ad8\u98ce\u9669\u884c\u4e3a\uff08\u5982\u9493\u9c7c\u90ae\u4ef6\uff09\u548c\u5efa\u7acb\u4fe1\u4efb\u65b9\u9762\u975e\u5e38\u6709\u6548\uff0c\u4f46\u4e5f\u5b58\u5728\u611f\u77e5\u771f\u5b9e\u6027\u4e0d\u8db3\u7684\u5c40\u9650\u6027\u3002", "conclusion": "\u672c\u6587\u4e3aAR-LLM\u9a71\u52a8\u7684\u793e\u4ea4\u5de5\u7a0b\u653b\u51fb\u63d0\u4f9b\u4e86\u6982\u5ff5\u9a8c\u8bc1\uff0c\u5e76\u4e3a\u9632\u5fa1\u4e0b\u4e00\u4ee3AR\u5a01\u80c1\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002"}}
{"id": "2504.13478", "pdf": "https://arxiv.org/pdf/2504.13478", "abs": "https://arxiv.org/abs/2504.13478", "authors": ["Vivian Lin", "Ramneet Kaur", "Yahan Yang", "Souradeep Dutta", "Yiannis Kantaros", "Anirban Roy", "Susmit Jha", "Oleg Sokolsky", "Insup Lee"], "title": "Safety Monitoring for Learning-Enabled Cyber-Physical Systems in Out-of-Distribution Scenarios", "categories": ["cs.LG"], "comment": "Accepted to ICCPS 2025", "summary": "The safety of learning-enabled cyber-physical systems is compromised by the\nwell-known vulnerabilities of deep neural networks to out-of-distribution (OOD)\ninputs. Existing literature has sought to monitor the safety of such systems by\ndetecting OOD data. However, such approaches have limited utility, as the\npresence of an OOD input does not necessarily imply the violation of a desired\nsafety property. We instead propose to directly monitor safety in a manner that\nis itself robust to OOD data. To this end, we predict violations of signal\ntemporal logic safety specifications based on predicted future trajectories.\nOur safety monitor additionally uses a novel combination of adaptive conformal\nprediction and incremental learning. The former obtains probabilistic\nprediction guarantees even on OOD data, and the latter prevents overly\nconservative predictions. We evaluate the efficacy of the proposed approach in\ntwo case studies on safety monitoring: 1) predicting collisions of an F1Tenth\ncar with static obstacles, and 2) predicting collisions of a race car with\nmultiple dynamic obstacles. We find that adaptive conformal prediction obtains\ntheoretical guarantees where other uncertainty quantification methods fail to\ndo so. Additionally, combining adaptive conformal prediction and incremental\nlearning for safety monitoring achieves high recall and timeliness while\nreducing loss in precision. We achieve these results even in OOD settings and\noutperform alternative methods.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u76f4\u63a5\u76d1\u63a7\u5b66\u4e60\u578b\u7f51\u7edc\u7269\u7406\u7cfb\u7edf\u5b89\u5168\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u9884\u6d4b\u4fe1\u53f7\u65f6\u5e8f\u903b\u8f91\u5b89\u5168\u89c4\u8303\u7684\u8fdd\u53cd\u60c5\u51b5\uff0c\u5e76\u7ed3\u5408\u81ea\u9002\u5e94\u5171\u5f62\u9884\u6d4b\u548c\u589e\u91cf\u5b66\u4e60\uff0c\u63d0\u9ad8\u4e86\u5bf9OOD\u6570\u636e\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u8fc7\u68c0\u6d4bOOD\u6570\u636e\u6765\u76d1\u63a7\u7cfb\u7edf\u5b89\u5168\uff0c\u4f46OOD\u8f93\u5165\u5e76\u4e0d\u4e00\u5b9a\u5bfc\u81f4\u5b89\u5168\u8fdd\u89c4\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u76f4\u63a5\u7684\u5b89\u5168\u76d1\u63a7\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9884\u6d4b\u672a\u6765\u8f68\u8ff9\u7684\u5b89\u5168\u76d1\u63a7\u65b9\u6cd5\uff0c\u7ed3\u5408\u81ea\u9002\u5e94\u5171\u5f62\u9884\u6d4b\u548c\u589e\u91cf\u5b66\u4e60\uff0c\u786e\u4fdd\u6982\u7387\u9884\u6d4b\u7684\u9c81\u68d2\u6027\u3002", "result": "\u5728F1Tenth\u8d5b\u8f66\u548c\u52a8\u6001\u969c\u788d\u7269\u78b0\u649e\u9884\u6d4b\u7684\u6848\u4f8b\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728OOD\u73af\u5883\u4e0b\u8868\u73b0\u51fa\u8272\uff0c\u5b9e\u73b0\u4e86\u9ad8\u53ec\u56de\u7387\u548c\u53ca\u65f6\u6027\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u7cbe\u5ea6\u635f\u5931\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728OOD\u6570\u636e\u4e0b\u4ecd\u80fd\u63d0\u4f9b\u7406\u8bba\u4fdd\u8bc1\uff0c\u5e76\u4f18\u4e8e\u5176\u4ed6\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\uff0c\u4e3a\u5b66\u4e60\u578b\u7cfb\u7edf\u7684\u5b89\u5168\u76d1\u63a7\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.13775", "pdf": "https://arxiv.org/pdf/2504.13775", "abs": "https://arxiv.org/abs/2504.13775", "authors": ["Zhengxian Wu", "Juan Wen", "Wanli Peng", "Ziwei Zhang", "Yinghan Zhou", "Yiming Xue"], "title": "BadApex: Backdoor Attack Based on Adaptive Optimization Mechanism of Black-box Large Language Models", "categories": ["cs.CL", "cs.CR"], "comment": "16 pages, 6 figures", "summary": "Previous insertion-based and paraphrase-based backdoors have achieved great\nsuccess in attack efficacy, but they ignore the text quality and semantic\nconsistency between poisoned and clean texts. Although recent studies introduce\nLLMs to generate poisoned texts and improve the stealthiness, semantic\nconsistency, and text quality, their hand-crafted prompts rely on expert\nexperiences, facing significant challenges in prompt adaptability and attack\nperformance after defenses. In this paper, we propose a novel backdoor attack\nbased on adaptive optimization mechanism of black-box large language models\n(BadApex), which leverages a black-box LLM to generate poisoned text through a\nrefined prompt. Specifically, an Adaptive Optimization Mechanism is designed to\nrefine an initial prompt iteratively using the generation and modification\nagents. The generation agent generates the poisoned text based on the initial\nprompt. Then the modification agent evaluates the quality of the poisoned text\nand refines a new prompt. After several iterations of the above process, the\nrefined prompt is used to generate poisoned texts through LLMs. We conduct\nextensive experiments on three dataset with six backdoor attacks and two\ndefenses. Extensive experimental results demonstrate that BadApex significantly\noutperforms state-of-the-art attacks. It improves prompt adaptability, semantic\nconsistency, and text quality. Furthermore, when two defense methods are\napplied, the average attack success rate (ASR) still up to 96.75%.", "AI": {"tldr": "BadApex\u662f\u4e00\u79cd\u57fa\u4e8e\u9ed1\u76d2\u5927\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u9002\u5e94\u4f18\u5316\u673a\u5236\u7684\u540e\u95e8\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u8fed\u4ee3\u4f18\u5316\u63d0\u793a\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u8bed\u4e49\u4e00\u81f4\u7684\u6bd2\u5316\u6587\u672c\uff0c\u663e\u8457\u63d0\u5347\u653b\u51fb\u6548\u679c\u548c\u9632\u5fa1\u62b5\u6297\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u540e\u95e8\u653b\u51fb\u65b9\u6cd5\u5728\u6587\u672c\u8d28\u91cf\u548c\u8bed\u4e49\u4e00\u81f4\u6027\u4e0a\u5b58\u5728\u4e0d\u8db3\uff0c\u4e14\u4f9d\u8d56\u4e13\u5bb6\u7ecf\u9a8c\u7684\u63d0\u793a\u8bbe\u8ba1\u9650\u5236\u4e86\u653b\u51fb\u6027\u80fd\u548c\u9002\u5e94\u6027\u3002", "method": "\u63d0\u51faBadApex\uff0c\u5229\u7528\u751f\u6210\u548c\u4fee\u6539\u4ee3\u7406\u8fed\u4ee3\u4f18\u5316\u521d\u59cb\u63d0\u793a\uff0c\u751f\u6210\u9ad8\u8d28\u91cf\u6bd2\u5316\u6587\u672c\u3002", "result": "\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\u8868\u660e\uff0cBadApex\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u653b\u51fb\u65b9\u6cd5\uff0c\u653b\u51fb\u6210\u529f\u7387\u9ad8\u8fbe96.75%\u3002", "conclusion": "BadApex\u901a\u8fc7\u81ea\u9002\u5e94\u4f18\u5316\u673a\u5236\u63d0\u5347\u4e86\u540e\u95e8\u653b\u51fb\u7684\u9690\u853d\u6027\u3001\u8bed\u4e49\u4e00\u81f4\u6027\u548c\u9632\u5fa1\u62b5\u6297\u80fd\u529b\u3002"}}
{"id": "2504.13211", "pdf": "https://arxiv.org/pdf/2504.13211", "abs": "https://arxiv.org/abs/2504.13211", "authors": ["Subin Kim", "Hoonrae Kim", "Jihyun Lee", "Yejin Jeon", "Gary Geunbae Lee"], "title": "Mirror: Multimodal Cognitive Reframing Therapy for Rolling with Resistance", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent studies have explored the use of large language models (LLMs) in\npsychotherapy; however, text-based cognitive behavioral therapy (CBT) models\noften struggle with client resistance, which can weaken therapeutic alliance.\nTo address this, we propose a multimodal approach that incorporates nonverbal\ncues, allowing the AI therapist to better align its responses with the client's\nnegative emotional state. Specifically, we introduce a new synthetic dataset,\nMultimodal Interactive Rolling with Resistance (Mirror), which is a novel\nsynthetic dataset that pairs client statements with corresponding facial\nimages. Using this dataset, we train baseline Vision-Language Models (VLMs)\nthat can analyze facial cues, infer emotions, and generate empathetic responses\nto effectively manage resistance. They are then evaluated in terms of both the\ntherapist's counseling skills and the strength of the therapeutic alliance in\nthe presence of client resistance. Our results demonstrate that Mirror\nsignificantly enhances the AI therapist's ability to handle resistance, which\noutperforms existing text-based CBT approaches.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u975e\u8bed\u8a00\u7ebf\u7d22\u7684\u591a\u6a21\u6001\u65b9\u6cd5\uff0c\u63d0\u5347AI\u6cbb\u7597\u5e08\u5904\u7406\u5ba2\u6237\u62b5\u6297\u7684\u80fd\u529b\uff0c\u4f18\u4e8e\u73b0\u6709\u6587\u672cCBT\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u6587\u672cCBT\u6a21\u578b\u5728\u5904\u7406\u5ba2\u6237\u62b5\u6297\u65f6\u6548\u679c\u4e0d\u4f73\uff0c\u5f71\u54cd\u4e86\u6cbb\u7597\u8054\u76df\u3002", "method": "\u5f15\u5165\u591a\u6a21\u6001\u6570\u636e\u96c6Mirror\uff0c\u8bad\u7ec3\u80fd\u5206\u6790\u9762\u90e8\u7ebf\u7d22\u5e76\u751f\u6210\u5171\u60c5\u56de\u5e94\u7684VLMs\u3002", "result": "Mirror\u663e\u8457\u63d0\u5347\u4e86AI\u6cbb\u7597\u5e08\u5904\u7406\u62b5\u6297\u7684\u80fd\u529b\u3002", "conclusion": "\u591a\u6a21\u6001\u65b9\u6cd5\u5728\u63d0\u5347\u6cbb\u7597\u8054\u76df\u548c\u5904\u7406\u62b5\u6297\u65b9\u9762\u4f18\u4e8e\u7eaf\u6587\u672c\u65b9\u6cd5\u3002"}}
{"id": "2504.13480", "pdf": "https://arxiv.org/pdf/2504.13480", "abs": "https://arxiv.org/abs/2504.13480", "authors": ["Minsu Koh", "Beom-Chul Park", "Heejo Kong", "Seong-Whan Lee"], "title": "Integrating Locality-Aware Attention with Transformers for General Geometry PDEs", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Accepted by IJCNN 2025", "summary": "Neural operators have emerged as promising frameworks for learning mappings\ngoverned by partial differential equations (PDEs), serving as data-driven\nalternatives to traditional numerical methods. While methods such as the\nFourier neural operator (FNO) have demonstrated notable performance, their\nreliance on uniform grids restricts their applicability to complex geometries\nand irregular meshes. Recently, Transformer-based neural operators with linear\nattention mechanisms have shown potential in overcoming these limitations for\nlarge-scale PDE simulations. However, these approaches predominantly emphasize\nglobal feature aggregation, often overlooking fine-scale dynamics and localized\nPDE behaviors essential for accurate solutions. To address these challenges, we\npropose the Locality-Aware Attention Transformer (LA2Former), which leverages\nK-nearest neighbors for dynamic patchifying and integrates global-local\nattention for enhanced PDE modeling. By combining linear attention for\nefficient global context encoding with pairwise attention for capturing\nintricate local interactions, LA2Former achieves an optimal balance between\ncomputational efficiency and predictive accuracy. Extensive evaluations across\nsix benchmark datasets demonstrate that LA2Former improves predictive accuracy\nby over 50% relative to existing linear attention methods, while also\noutperforming full pairwise attention under optimal conditions. This work\nunderscores the critical importance of localized feature learning in advancing\nTransformer-based neural operators for solving PDEs on complex and irregular\ndomains.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLA2Former\u7684\u5c40\u90e8\u611f\u77e5\u6ce8\u610f\u529bTransformer\uff0c\u7528\u4e8e\u89e3\u51b3\u4f20\u7edf\u795e\u7ecf\u7b97\u5b50\u5728\u590d\u6742\u51e0\u4f55\u548c\u4e0d\u89c4\u5219\u7f51\u683c\u4e0a\u7684\u5c40\u9650\u6027\uff0c\u901a\u8fc7\u7ed3\u5408\u5168\u5c40\u548c\u5c40\u90e8\u6ce8\u610f\u529b\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86PDE\u5efa\u6a21\u7684\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u795e\u7ecf\u7b97\u5b50\uff08\u5982FNO\uff09\u4f9d\u8d56\u5747\u5300\u7f51\u683c\uff0c\u9650\u5236\u4e86\u5176\u5728\u590d\u6742\u51e0\u4f55\u548c\u4e0d\u89c4\u5219\u7f51\u683c\u4e0a\u7684\u5e94\u7528\u3002Transformer-based\u795e\u7ecf\u7b97\u5b50\u867d\u6709\u6240\u6539\u8fdb\uff0c\u4f46\u8fc7\u4e8e\u5f3a\u8c03\u5168\u5c40\u7279\u5f81\u805a\u5408\uff0c\u5ffd\u7565\u4e86\u7ec6\u5c3a\u5ea6\u52a8\u6001\u548c\u5c40\u90e8PDE\u884c\u4e3a\u3002", "method": "\u63d0\u51faLA2Former\uff0c\u5229\u7528K\u8fd1\u90bb\u52a8\u6001\u5206\u5757\uff0c\u5e76\u96c6\u6210\u5168\u5c40-\u5c40\u90e8\u6ce8\u610f\u529b\u673a\u5236\uff0c\u7ed3\u5408\u7ebf\u6027\u6ce8\u610f\u529b\u548c\u6210\u5bf9\u6ce8\u610f\u529b\uff0c\u4f18\u5316\u8ba1\u7b97\u6548\u7387\u548c\u9884\u6d4b\u51c6\u786e\u6027\u3002", "result": "\u5728\u516d\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cLA2Former\u6bd4\u73b0\u6709\u7ebf\u6027\u6ce8\u610f\u529b\u65b9\u6cd5\u63d0\u534750%\u4ee5\u4e0a\u7684\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u5e76\u5728\u6700\u4f18\u6761\u4ef6\u4e0b\u4f18\u4e8e\u5168\u6210\u5bf9\u6ce8\u610f\u529b\u65b9\u6cd5\u3002", "conclusion": "LA2Former\u5f3a\u8c03\u4e86\u5c40\u90e8\u7279\u5f81\u5b66\u4e60\u5728Transformer-based\u795e\u7ecf\u7b97\u5b50\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u590d\u6742\u548c\u4e0d\u89c4\u5219\u57df\u4e0a\u7684PDE\u6c42\u89e3\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.13816", "pdf": "https://arxiv.org/pdf/2504.13816", "abs": "https://arxiv.org/abs/2504.13816", "authors": ["Chenghao Xiao", "Hou Pong Chan", "Hao Zhang", "Mahani Aljunied", "Lidong Bing", "Noura Al Moubayed", "Yu Rong"], "title": "Analyzing LLMs' Knowledge Boundary Cognition Across Languages Through the Lens of Internal Representations", "categories": ["cs.CL"], "comment": null, "summary": "While understanding the knowledge boundaries of LLMs is crucial to prevent\nhallucination, research on knowledge boundaries of LLMs has predominantly\nfocused on English. In this work, we present the first study to analyze how\nLLMs recognize knowledge boundaries across different languages by probing their\ninternal representations when processing known and unknown questions in\nmultiple languages. Our empirical studies reveal three key findings: 1) LLMs'\nperceptions of knowledge boundaries are encoded in the middle to middle-upper\nlayers across different languages. 2) Language differences in knowledge\nboundary perception follow a linear structure, which motivates our proposal of\na training-free alignment method that effectively transfers knowledge boundary\nperception ability across languages, thereby helping reduce hallucination risk\nin low-resource languages; 3) Fine-tuning on bilingual question pair\ntranslation further enhances LLMs' recognition of knowledge boundaries across\nlanguages. Given the absence of standard testbeds for cross-lingual knowledge\nboundary analysis, we construct a multilingual evaluation suite comprising\nthree representative types of knowledge boundary data. Our code and datasets\nare publicly available at\nhttps://github.com/DAMO-NLP-SG/LLM-Multilingual-Knowledge-Boundaries.", "AI": {"tldr": "\u8be5\u7814\u7a76\u9996\u6b21\u5206\u6790\u4e86LLMs\u5728\u4e0d\u540c\u8bed\u8a00\u4e2d\u5982\u4f55\u8bc6\u522b\u77e5\u8bc6\u8fb9\u754c\uff0c\u63ed\u793a\u4e86\u5176\u611f\u77e5\u7f16\u7801\u4e8e\u4e2d\u5c42\u81f3\u4e2d\u4e0a\u5c42\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u5bf9\u9f50\u65b9\u6cd5\u4ee5\u51cf\u5c11\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u5e7b\u89c9\u98ce\u9669\u3002", "motivation": "\u7406\u89e3LLMs\u7684\u77e5\u8bc6\u8fb9\u754c\u5bf9\u9632\u6b62\u5e7b\u89c9\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u4e8e\u82f1\u8bed\uff0c\u7f3a\u4e4f\u8de8\u8bed\u8a00\u5206\u6790\u3002", "method": "\u901a\u8fc7\u63a2\u6d4bLLMs\u5904\u7406\u591a\u8bed\u8a00\u5df2\u77e5\u548c\u672a\u77e5\u95ee\u9898\u65f6\u7684\u5185\u90e8\u8868\u5f81\uff0c\u63d0\u51fa\u8bad\u7ec3\u81ea\u7531\u5bf9\u9f50\u65b9\u6cd5\uff0c\u5e76\u5229\u7528\u53cc\u8bed\u95ee\u9898\u5bf9\u7ffb\u8bd1\u8fdb\u884c\u5fae\u8c03\u3002", "result": "\u53d1\u73b0LLMs\u7684\u77e5\u8bc6\u8fb9\u754c\u611f\u77e5\u7f16\u7801\u4e8e\u7279\u5b9a\u5c42\uff0c\u8bed\u8a00\u5dee\u5f02\u5448\u7ebf\u6027\u7ed3\u6784\uff0c\u5bf9\u9f50\u65b9\u6cd5\u6709\u6548\u51cf\u5c11\u4f4e\u8d44\u6e90\u8bed\u8a00\u5e7b\u89c9\u98ce\u9669\u3002", "conclusion": "\u7814\u7a76\u586b\u8865\u4e86\u8de8\u8bed\u8a00\u77e5\u8bc6\u8fb9\u754c\u5206\u6790\u7684\u7a7a\u767d\uff0c\u63d0\u4f9b\u4e86\u591a\u8bed\u8a00\u8bc4\u4f30\u5957\u4ef6\uff0c\u4e3aLLMs\u7684\u8de8\u8bed\u8a00\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2504.13483", "pdf": "https://arxiv.org/pdf/2504.13483", "abs": "https://arxiv.org/abs/2504.13483", "authors": ["Yiran Wang", "Tangtang Xie", "Hao Wu"], "title": "Latent Tensor Factorization with Nonlinear PID Control for Missing Data Recovery in Non-Intrusive Load Monitoring", "categories": ["cs.LG"], "comment": null, "summary": "Non-Intrusive Load Monitoring (NILM) has emerged as a key smart grid\ntechnology, identifying electrical device and providing detailed energy\nconsumption data for precise demand response management. Nevertheless, NILM\ndata suffers from missing values due to inescapable factors like sensor\nfailure, leading to inaccuracies in non-intrusive load monitoring. A stochastic\ngradient descent (SGD)-based latent factorization of tensors model has proven\nto be effective in estimating missing data, however, it updates a latent factor\nsolely based on the current stochastic gradient, without considering past\ninformation, which leads to slow convergence of anLFT model. To address this\nissue, this paper proposes a Nonlinear Proportional-integral-derivative\n(PID)-Incorporated Latent factorization of tensors (NPIL) model with two-fold\nideas: a) rebuilding the instant learning error according to the principle of a\nnonlinear PID controller, thus, the past update information is efficiently\nincorporated into the learning scheme, and b) implementing gain parameter\nadaptation by utilizing particle swarm optimization (PSO) algorithm, hence, the\nmodel computational efficiency is effectively improved. Experimental results on\nreal-world NILM datasets demonstrate that the proposed NPIL model surpasses\nstate-of-the-art models in convergence rate and accuracy when predicting the\nmissing NILM data.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u975e\u7ebf\u6027PID\u7ed3\u5408\u7684\u6f5c\u5728\u5f20\u91cf\u5206\u89e3\u6a21\u578b\uff08NPIL\uff09\uff0c\u7528\u4e8e\u6539\u8fdb\u975e\u4fb5\u5165\u5f0f\u8d1f\u8f7d\u76d1\u6d4b\uff08NILM\uff09\u4e2d\u7f3a\u5931\u6570\u636e\u7684\u4f30\u8ba1\uff0c\u901a\u8fc7\u7ed3\u5408\u5386\u53f2\u4fe1\u606f\u548c\u53c2\u6570\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6536\u655b\u901f\u5ea6\u548c\u51c6\u786e\u6027\u3002", "motivation": "NILM\u6570\u636e\u5e38\u56e0\u4f20\u611f\u5668\u6545\u969c\u7b49\u539f\u56e0\u7f3a\u5931\uff0c\u73b0\u6709\u57fa\u4e8e\u968f\u673a\u68af\u5ea6\u4e0b\u964d\uff08SGD\uff09\u7684\u65b9\u6cd5\u6536\u655b\u6162\u4e14\u672a\u5145\u5206\u5229\u7528\u5386\u53f2\u4fe1\u606f\u3002", "method": "\u63d0\u51faNPIL\u6a21\u578b\uff0c\u7ed3\u5408\u975e\u7ebf\u6027PID\u63a7\u5236\u5668\u539f\u7406\u548c\u7c92\u5b50\u7fa4\u4f18\u5316\uff08PSO\uff09\u7b97\u6cd5\uff0c\u6539\u8fdb\u5b66\u4e60\u8bef\u5dee\u548c\u53c2\u6570\u9002\u5e94\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cNPIL\u5728\u6536\u655b\u901f\u5ea6\u548c\u7f3a\u5931\u6570\u636e\u9884\u6d4b\u51c6\u786e\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "NPIL\u6a21\u578b\u6709\u6548\u89e3\u51b3\u4e86NILM\u6570\u636e\u7f3a\u5931\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u76d1\u6d4b\u7cbe\u5ea6\u548c\u6548\u7387\u3002"}}
{"id": "2504.13825", "pdf": "https://arxiv.org/pdf/2504.13825", "abs": "https://arxiv.org/abs/2504.13825", "authors": ["Junjie Yang", "Junhao Song", "Xudong Han", "Ziqian Bi", "Tianyang Wang", "Chia Xin Liang", "Xinyuan Song", "Yichao Zhang", "Qian Niu", "Benji Peng", "Keyu Chen", "Ming Liu"], "title": "Feature Alignment and Representation Transfer in Knowledge Distillation for Large Language Models", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Knowledge distillation (KD) is a technique for transferring knowledge from\ncomplex teacher models to simpler student models, significantly enhancing model\nefficiency and accuracy. It has demonstrated substantial advancements in\nvarious applications including image classification, object detection, language\nmodeling, text classification, and sentiment analysis. Recent innovations in KD\nmethods, such as attention-based approaches, block-wise logit distillation, and\ndecoupling distillation, have notably improved student model performance. These\ntechniques focus on stimulus complexity, attention mechanisms, and global\ninformation capture to optimize knowledge transfer. In addition, KD has proven\neffective in compressing large language models while preserving accuracy,\nreducing computational overhead, and improving inference speed. This survey\nsynthesizes the latest literature, highlighting key findings, contributions,\nand future directions in knowledge distillation to provide insights for\nresearchers and practitioners on its evolving role in artificial intelligence\nand machine learning.", "AI": {"tldr": "\u77e5\u8bc6\u84b8\u998f\uff08KD\uff09\u901a\u8fc7\u5c06\u590d\u6742\u6559\u5e08\u6a21\u578b\u7684\u77e5\u8bc6\u8f6c\u79fb\u5230\u7b80\u5355\u5b66\u751f\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u5e7f\u6cdb\u5e94\u7528\u4e8e\u56fe\u50cf\u5206\u7c7b\u3001\u8bed\u8a00\u5efa\u6a21\u7b49\u9886\u57df\u3002", "motivation": "\u63a2\u7d22\u5982\u4f55\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u6280\u672f\u4f18\u5316\u6a21\u578b\u6027\u80fd\uff0c\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\uff0c\u5e76\u63a8\u52a8\u5176\u5728\u4eba\u5de5\u667a\u80fd\u548c\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u91c7\u7528\u6ce8\u610f\u529b\u673a\u5236\u3001\u5757\u7ea7\u5bf9\u6570\u84b8\u998f\u548c\u89e3\u8026\u84b8\u998f\u7b49\u521b\u65b0\u65b9\u6cd5\uff0c\u5173\u6ce8\u523a\u6fc0\u590d\u6742\u6027\u3001\u6ce8\u610f\u529b\u673a\u5236\u548c\u5168\u5c40\u4fe1\u606f\u6355\u83b7\u3002", "result": "\u77e5\u8bc6\u84b8\u998f\u5728\u538b\u7f29\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3001\u4fdd\u6301\u51c6\u786e\u6027\u3001\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500\u548c\u63d0\u9ad8\u63a8\u7406\u901f\u5ea6\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "\u672c\u6587\u7efc\u8ff0\u4e86\u77e5\u8bc6\u84b8\u998f\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u603b\u7ed3\u4e86\u5173\u952e\u53d1\u73b0\u548c\u8d21\u732e\uff0c\u5e76\u5c55\u671b\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2504.13484", "pdf": "https://arxiv.org/pdf/2504.13484", "abs": "https://arxiv.org/abs/2504.13484", "authors": ["Vivian Lin", "Insup Lee"], "title": "Monitor and Recover: A Paradigm for Future Research on Distribution Shift in Learning-Enabled Cyber-Physical Systems", "categories": ["cs.LG", "cs.CR"], "comment": "Accepted to ICCPS 2025", "summary": "With the known vulnerability of neural networks to distribution shift,\nmaintaining reliability in learning-enabled cyber-physical systems poses a\nsalient challenge. In response, many existing methods adopt a detect and\nabstain methodology, aiming to detect distribution shift at inference time so\nthat the learning-enabled component can abstain from decision-making. This\napproach, however, has limited use in real-world applications. We instead\npropose a monitor and recover paradigm as a promising direction for future\nresearch. This philosophy emphasizes 1) robust safety monitoring instead of\ndistribution shift detection and 2) distribution shift recovery instead of\nabstention. We discuss two examples from our recent work.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u76d1\u63a7\u4e0e\u6062\u590d\u8303\u5f0f\uff0c\u4ee5\u89e3\u51b3\u795e\u7ecf\u7f51\u7edc\u5728\u5206\u5e03\u504f\u79fb\u4e0b\u7684\u53ef\u9760\u6027\u95ee\u9898\uff0c\u66ff\u4ee3\u4f20\u7edf\u7684\u68c0\u6d4b\u4e0e\u56de\u907f\u65b9\u6cd5\u3002", "motivation": "\u795e\u7ecf\u7f51\u7edc\u5bf9\u5206\u5e03\u504f\u79fb\u7684\u8106\u5f31\u6027\u5bfc\u81f4\u5b66\u4e60\u578b\u7f51\u7edc\u7269\u7406\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u95ee\u9898\uff0c\u4f20\u7edf\u68c0\u6d4b\u4e0e\u56de\u907f\u65b9\u6cd5\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u6548\u679c\u6709\u9650\u3002", "method": "\u63d0\u51fa\u76d1\u63a7\u4e0e\u6062\u590d\u8303\u5f0f\uff0c\u5f3a\u8c03\u9c81\u68d2\u7684\u5b89\u5168\u76d1\u63a7\u548c\u5206\u5e03\u504f\u79fb\u6062\u590d\uff0c\u800c\u975e\u68c0\u6d4b\u4e0e\u56de\u907f\u3002", "result": "\u901a\u8fc7\u4e24\u4e2a\u5b9e\u4f8b\u5c55\u793a\u4e86\u8be5\u8303\u5f0f\u7684\u6f5c\u529b\u3002", "conclusion": "\u76d1\u63a7\u4e0e\u6062\u590d\u8303\u5f0f\u662f\u672a\u6765\u7814\u7a76\u7684\u6709\u524d\u666f\u65b9\u5411\u3002"}}
{"id": "2504.13828", "pdf": "https://arxiv.org/pdf/2504.13828", "abs": "https://arxiv.org/abs/2504.13828", "authors": ["Shijie Xia", "Yiwei Qin", "Xuefeng Li", "Yan Ma", "Run-Ze Fan", "Steffi Chern", "Haoyang Zou", "Fan Zhou", "Xiangkun Hu", "Jiahe Jin", "Yanheng He", "Yixin Ye", "Yixiu Liu", "Pengfei Liu"], "title": "Generative AI Act II: Test Time Scaling Drives Cognition Engineering", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The first generation of Large Language Models - what might be called \"Act I\"\nof generative AI (2020-2023) - achieved remarkable success through massive\nparameter and data scaling, yet exhibited fundamental limitations in knowledge\nlatency, shallow reasoning, and constrained cognitive processes. During this\nera, prompt engineering emerged as our primary interface with AI, enabling\ndialogue-level communication through natural language. We now witness the\nemergence of \"Act II\" (2024-present), where models are transitioning from\nknowledge-retrieval systems (in latent space) to thought-construction engines\nthrough test-time scaling techniques. This new paradigm establishes a\nmind-level connection with AI through language-based thoughts. In this paper,\nwe clarify the conceptual foundations of cognition engineering and explain why\nthis moment is critical for its development. We systematically break down these\nadvanced approaches through comprehensive tutorials and optimized\nimplementations, democratizing access to cognition engineering and enabling\nevery practitioner to participate in AI's second act. We provide a regularly\nupdated collection of papers on test-time scaling in the GitHub Repository:\nhttps://github.com/GAIR-NLP/cognition-engineering", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u4ece\u7b2c\u4e00\u4ee3\u5927\u8bed\u8a00\u6a21\u578b\uff082020-2023\uff09\u5230\u7b2c\u4e8c\u4ee3\uff082024\u81f3\u4eca\uff09\u7684\u6f14\u53d8\uff0c\u5f3a\u8c03\u4e86\u8ba4\u77e5\u5de5\u7a0b\u7684\u91cd\u8981\u6027\u53ca\u5176\u5728AI\u53d1\u5c55\u4e2d\u7684\u5173\u952e\u4f5c\u7528\u3002", "motivation": "\u7b2c\u4e00\u4ee3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u53c2\u6570\u548c\u6570\u636e\u89c4\u6a21\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u6210\u529f\uff0c\u4f46\u5b58\u5728\u77e5\u8bc6\u6ede\u540e\u3001\u6d45\u5c42\u63a8\u7406\u548c\u8ba4\u77e5\u53d7\u9650\u7b49\u95ee\u9898\u3002\u7b2c\u4e8c\u4ee3\u6a21\u578b\u901a\u8fc7\u6d4b\u8bd5\u65f6\u6269\u5c55\u6280\u672f\uff0c\u4ece\u77e5\u8bc6\u68c0\u7d22\u7cfb\u7edf\u8f6c\u53d8\u4e3a\u601d\u7ef4\u6784\u5efa\u5f15\u64ce\uff0c\u63a8\u52a8\u4e86\u8ba4\u77e5\u5de5\u7a0b\u7684\u53d1\u5c55\u3002", "method": "\u8bba\u6587\u901a\u8fc7\u7cfb\u7edf\u5316\u7684\u6559\u7a0b\u548c\u4f18\u5316\u5b9e\u73b0\uff0c\u8be6\u7ec6\u4ecb\u7ecd\u4e86\u8ba4\u77e5\u5de5\u7a0b\u7684\u9ad8\u7ea7\u65b9\u6cd5\uff0c\u5e76\u63d0\u4f9b\u4e86GitHub\u8d44\u6e90\u5e93\u4ee5\u652f\u6301\u5b9e\u8df5\u8005\u53c2\u4e0e\u3002", "result": "\u8bba\u6587\u5c55\u793a\u4e86\u8ba4\u77e5\u5de5\u7a0b\u5728AI\u7b2c\u4e8c\u9636\u6bb5\u7684\u6f5c\u529b\uff0c\u4e3a\u5b9e\u8df5\u8005\u63d0\u4f9b\u4e86\u5de5\u5177\u548c\u8d44\u6e90\uff0c\u63a8\u52a8\u4e86\u8be5\u9886\u57df\u7684\u6c11\u4e3b\u5316\u3002", "conclusion": "\u8ba4\u77e5\u5de5\u7a0b\u662fAI\u53d1\u5c55\u7684\u5173\u952e\u8f6c\u6298\u70b9\uff0c\u5176\u7cfb\u7edf\u5316\u65b9\u6cd5\u548c\u5f00\u6e90\u8d44\u6e90\u5c06\u52a0\u901f\u7b2c\u4e8c\u4ee3\u5927\u8bed\u8a00\u6a21\u578b\u7684\u666e\u53ca\u548c\u5e94\u7528\u3002"}}
{"id": "2504.13521", "pdf": "https://arxiv.org/pdf/2504.13521", "abs": "https://arxiv.org/abs/2504.13521", "authors": ["Kasymkhan Khubiev", "Michail Semenov"], "title": "Deep Learning Models Meet Financial Data Modalities", "categories": ["cs.LG", "cs.AI", "cs.CE", "q-fin.ST"], "comment": "15 pages, 14 images, 7 tables", "summary": "Algorithmic trading relies on extracting meaningful signals from diverse\nfinancial data sources, including candlestick charts, order statistics on put\nand canceled orders, traded volume data, limit order books, and news flow.\nWhile deep learning has demonstrated remarkable success in processing\nunstructured data and has significantly advanced natural language processing,\nits application to structured financial data remains an ongoing challenge. This\nstudy investigates the integration of deep learning models with financial data\nmodalities, aiming to enhance predictive performance in trading strategies and\nportfolio optimization. We present a novel approach to incorporating limit\norder book analysis into algorithmic trading by developing embedding techniques\nand treating sequential limit order book snapshots as distinct input channels\nin an image-based representation. Our methodology for processing limit order\nbook data achieves state-of-the-art performance in high-frequency trading\nalgorithms, underscoring the effectiveness of deep learning in financial\napplications.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5982\u4f55\u5c06\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u4e0e\u91d1\u878d\u6570\u636e\u6a21\u6001\u7ed3\u5408\uff0c\u4ee5\u63d0\u5347\u4ea4\u6613\u7b56\u7565\u548c\u6295\u8d44\u7ec4\u5408\u4f18\u5316\u7684\u9884\u6d4b\u6027\u80fd\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u9650\u4ef7\u8ba2\u5355\u7c3f\u5206\u6790\u65b9\u6cd5\u3002", "motivation": "\u5c3d\u7ba1\u6df1\u5ea6\u5b66\u4e60\u5728\u975e\u7ed3\u6784\u5316\u6570\u636e\u5904\u7406\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u5728\u7ed3\u6784\u5316\u91d1\u878d\u6570\u636e\u4e2d\u7684\u5e94\u7528\u4ecd\u5177\u6311\u6218\u6027\uff0c\u56e0\u6b64\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u5f00\u53d1\u4e86\u5d4c\u5165\u6280\u672f\uff0c\u5c06\u9650\u4ef7\u8ba2\u5355\u7c3f\u7684\u8fde\u7eed\u5feb\u7167\u89c6\u4e3a\u56fe\u50cf\u8868\u793a\u4e2d\u7684\u72ec\u7acb\u8f93\u5165\u901a\u9053\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u9ad8\u9891\u4ea4\u6613\u7b97\u6cd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u6df1\u5ea6\u5b66\u4e60\u5728\u91d1\u878d\u5e94\u7528\u4e2d\u5177\u6709\u663e\u8457\u6709\u6548\u6027\uff0c\u5c24\u5176\u662f\u5728\u9650\u4ef7\u8ba2\u5355\u7c3f\u5206\u6790\u4e2d\u3002"}}
{"id": "2504.13834", "pdf": "https://arxiv.org/pdf/2504.13834", "abs": "https://arxiv.org/abs/2504.13834", "authors": ["Muhan Gao", "Jash Shah", "Weiqi Wang", "Daniel Khashabi"], "title": "Science Hierarchography: Hierarchical Organization of Science Literature", "categories": ["cs.CL"], "comment": null, "summary": "Scientific knowledge is growing rapidly, making it challenging to track\nprogress and high-level conceptual links across broad disciplines. While\nexisting tools like citation networks and search engines make it easy to access\na few related papers, they fundamentally lack the flexible abstraction needed\nto represent the density of activity in various scientific subfields. We\nmotivate SCIENCE HIERARCHOGRAPHY, the goal of organizing scientific literature\ninto a high-quality hierarchical structure that allows for the categorization\nof scientific work across varying levels of abstraction, from very broad fields\nto very specific studies. Such a representation can provide insights into which\nfields are well-explored and which are under-explored. To achieve the goals of\nSCIENCE HIERARCHOGRAPHY, we develop a range of algorithms. Our primary approach\ncombines fast embedding-based clustering with LLM-based prompting to balance\nthe computational efficiency of embedding methods with the semantic precision\noffered by LLM prompting. We demonstrate that this approach offers the best\ntrade-off between quality and speed compared to methods that heavily rely on\nLLM prompting, such as iterative tree construction with LLMs. To better reflect\nthe interdisciplinary and multifaceted nature of research papers, our hierarchy\ncaptures multiple dimensions of categorization beyond simple topic labels. We\nevaluate the utility of our framework by assessing how effectively an LLM-based\nagent can locate target papers using the hierarchy. Results show that this\nstructured approach enhances interpretability, supports trend discovery, and\noffers an alternative pathway for exploring scientific literature beyond\ntraditional search methods. Code, data and demo:\n$\\href{https://github.com/JHU-CLSP/science-hierarchography}{https://github.com/JHU-CLSP/science-hierarchography}$", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSCIENCE HIERARCHOGRAPHY\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u5c42\u7ed3\u6784\u7ec4\u7ec7\u79d1\u5b66\u6587\u732e\uff0c\u7ed3\u5408\u5d4c\u5165\u805a\u7c7b\u548cLLM\u63d0\u793a\uff0c\u4ee5\u9ad8\u6548\u4e14\u8bed\u4e49\u7cbe\u786e\u7684\u65b9\u5f0f\u5206\u7c7b\u79d1\u5b66\u5de5\u4f5c\u3002", "motivation": "\u79d1\u5b66\u77e5\u8bc6\u5feb\u901f\u589e\u957f\uff0c\u73b0\u6709\u5de5\u5177\uff08\u5982\u5f15\u7528\u7f51\u7edc\u548c\u641c\u7d22\u5f15\u64ce\uff09\u7f3a\u4e4f\u5bf9\u79d1\u5b66\u5b50\u9886\u57df\u6d3b\u52a8\u5bc6\u5ea6\u7684\u7075\u6d3b\u62bd\u8c61\u8868\u793a\u3002", "method": "\u7ed3\u5408\u5feb\u901f\u5d4c\u5165\u805a\u7c7b\u548cLLM\u63d0\u793a\uff0c\u5e73\u8861\u8ba1\u7b97\u6548\u7387\u548c\u8bed\u4e49\u7cbe\u5ea6\uff0c\u6784\u5efa\u591a\u7ef4\u5ea6\u5206\u7c7b\u5c42\u6b21\u7ed3\u6784\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u8d28\u91cf\u548c\u901f\u5ea6\u4e0a\u4f18\u4e8e\u4f9d\u8d56LLM\u63d0\u793a\u7684\u65b9\u6cd5\uff0c\u63d0\u5347\u4e86\u6587\u732e\u63a2\u7d22\u7684\u6548\u7387\u548c\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "SCIENCE HIERARCHOGRAPHY\u4e3a\u79d1\u5b66\u6587\u732e\u63a2\u7d22\u63d0\u4f9b\u4e86\u7ed3\u6784\u5316\u9014\u5f84\uff0c\u652f\u6301\u8d8b\u52bf\u53d1\u73b0\u548c\u8de8\u5b66\u79d1\u7814\u7a76\u3002"}}
{"id": "2504.13522", "pdf": "https://arxiv.org/pdf/2504.13522", "abs": "https://arxiv.org/abs/2504.13522", "authors": ["Yunhua Pei", "John Cartlidge", "Anandadeep Mandal", "Daniel Gold", "Enrique Marcilio", "Riccardo Mazzon"], "title": "Cross-Modal Temporal Fusion for Financial Market Forecasting", "categories": ["cs.LG", "cs.NE", "q-fin.CP"], "comment": "10 pages, 2 figures", "summary": "Accurate financial market forecasting requires diverse data sources,\nincluding historical price trends, macroeconomic indicators, and financial\nnews, each contributing unique predictive signals. However, existing methods\noften process these modalities independently or fail to effectively model their\ninteractions. In this paper, we introduce Cross-Modal Temporal Fusion (CMTF), a\nnovel transformer-based framework that integrates heterogeneous financial data\nto improve predictive accuracy. Our approach employs attention mechanisms to\ndynamically weight the contribution of different modalities, along with a\nspecialized tensor interpretation module for feature extraction. To facilitate\nrapid model iteration in industry applications, we incorporate a mature\nauto-training scheme that streamlines optimization. When applied to real-world\nfinancial datasets, CMTF demonstrates improvements over baseline models in\nforecasting stock price movements and provides a scalable and effective\nsolution for cross-modal integration in financial market prediction.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u8de8\u6a21\u6001\u65f6\u5e8f\u878d\u5408\u6846\u67b6\uff08CMTF\uff09\uff0c\u7528\u4e8e\u6574\u5408\u5f02\u6784\u91d1\u878d\u6570\u636e\u4ee5\u63d0\u5347\u9884\u6d4b\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u72ec\u7acb\u5904\u7406\u591a\u6a21\u6001\u6570\u636e\u6216\u672a\u80fd\u6709\u6548\u5efa\u6a21\u5176\u4ea4\u4e92\uff0c\u5bfc\u81f4\u9884\u6d4b\u7cbe\u5ea6\u53d7\u9650\u3002", "method": "\u91c7\u7528\u6ce8\u610f\u529b\u673a\u5236\u52a8\u6001\u52a0\u6743\u591a\u6a21\u6001\u8d21\u732e\uff0c\u5e76\u7ed3\u5408\u4e13\u95e8\u7684\u5f20\u91cf\u89e3\u91ca\u6a21\u5757\u8fdb\u884c\u7279\u5f81\u63d0\u53d6\uff0c\u540c\u65f6\u5f15\u5165\u81ea\u52a8\u5316\u8bad\u7ec3\u65b9\u6848\u52a0\u901f\u4f18\u5316\u3002", "result": "\u5728\u771f\u5b9e\u91d1\u878d\u6570\u636e\u96c6\u4e0a\uff0cCMTF\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u80fd\u66f4\u51c6\u786e\u9884\u6d4b\u80a1\u4ef7\u53d8\u52a8\u3002", "conclusion": "CMTF\u4e3a\u91d1\u878d\u5e02\u573a\u9884\u6d4b\u4e2d\u7684\u8de8\u6a21\u6001\u6574\u5408\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.13835", "pdf": "https://arxiv.org/pdf/2504.13835", "abs": "https://arxiv.org/abs/2504.13835", "authors": ["Yicheng Chen", "Yining Li", "Kai Hu", "Zerun Ma", "Haochen Ye", "Kai Chen"], "title": "MIG: Automatic Data Selection for Instruction Tuning by Maximizing Information Gain in Semantic Space", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Data quality and diversity are key to the construction of effective\ninstruction-tuning datasets. % With the increasing availability of open-source\ninstruction-tuning datasets, it is advantageous to automatically select\nhigh-quality and diverse subsets from a vast amount of data. % Existing methods\ntypically prioritize instance quality and use heuristic rules to maintain\ndiversity. % However, this absence of a comprehensive view of the entire\ncollection often leads to suboptimal results. % Moreover, heuristic rules\ngenerally focus on distance or clustering within the embedding space, which\nfails to accurately capture the intent of complex instructions in the semantic\nspace. % To bridge this gap, we propose a unified method for quantifying the\ninformation content of datasets. This method models the semantic space by\nconstructing a label graph and quantifies diversity based on the distribution\nof information within the graph. % Based on such a measurement, we further\nintroduce an efficient sampling method that selects data samples iteratively to\n\\textbf{M}aximize the \\textbf{I}nformation \\textbf{G}ain (MIG) in semantic\nspace. % Experiments on various datasets and base models demonstrate that MIG\nconsistently outperforms state-of-the-art methods. % Notably, the model\nfine-tuned with 5\\% Tulu3 data sampled by MIG achieves comparable performance\nto the official SFT model trained on the full dataset, with improvements of\n+5.73\\% on AlpacaEval and +6.89\\% on Wildbench.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u65b9\u6cd5\u91cf\u5316\u6570\u636e\u96c6\u4fe1\u606f\u5185\u5bb9\uff0c\u901a\u8fc7\u6784\u5efa\u6807\u7b7e\u56fe\u5efa\u6a21\u8bed\u4e49\u7a7a\u95f4\uff0c\u5e76\u57fa\u4e8e\u4fe1\u606f\u5206\u5e03\u91cf\u5316\u591a\u6837\u6027\uff0c\u8fdb\u4e00\u6b65\u63d0\u51faMIG\u91c7\u6837\u65b9\u6cd5\u4ee5\u6700\u5927\u5316\u8bed\u4e49\u7a7a\u95f4\u4fe1\u606f\u589e\u76ca\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4ec5\u5173\u6ce8\u5b9e\u4f8b\u8d28\u91cf\u5e76\u4f7f\u7528\u542f\u53d1\u5f0f\u89c4\u5219\u4fdd\u6301\u591a\u6837\u6027\uff0c\u7f3a\u4e4f\u5bf9\u6570\u636e\u96c6\u7684\u5168\u9762\u89c6\u89d2\uff0c\u5bfc\u81f4\u7ed3\u679c\u6b21\u4f18\u3002\u542f\u53d1\u5f0f\u89c4\u5219\u5728\u5d4c\u5165\u7a7a\u95f4\u4e2d\u96be\u4ee5\u51c6\u786e\u6355\u6349\u590d\u6742\u6307\u4ee4\u7684\u8bed\u4e49\u610f\u56fe\u3002", "method": "\u6784\u5efa\u6807\u7b7e\u56fe\u5efa\u6a21\u8bed\u4e49\u7a7a\u95f4\uff0c\u91cf\u5316\u4fe1\u606f\u5206\u5e03\u4ee5\u8861\u91cf\u591a\u6837\u6027\uff0c\u63d0\u51faMIG\u91c7\u6837\u65b9\u6cd5\u8fed\u4ee3\u9009\u62e9\u6570\u636e\u6837\u672c\u4ee5\u6700\u5927\u5316\u8bed\u4e49\u7a7a\u95f4\u4fe1\u606f\u589e\u76ca\u3002", "result": "\u5b9e\u9a8c\u8868\u660eMIG\u5728\u591a\u4e2a\u6570\u636e\u96c6\u548c\u57fa\u7840\u6a21\u578b\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4ec5\u75285%\u6570\u636e\u8bad\u7ec3\u7684\u6a21\u578b\u6027\u80fd\u63a5\u8fd1\u5168\u6570\u636e\u96c6\u8bad\u7ec3\u7684\u5b98\u65b9SFT\u6a21\u578b\uff0c\u5728AlpacaEval\u548cWildbench\u4e0a\u5206\u522b\u63d0\u53475.73%\u548c6.89%\u3002", "conclusion": "MIG\u65b9\u6cd5\u901a\u8fc7\u91cf\u5316\u8bed\u4e49\u7a7a\u95f4\u4fe1\u606f\u5185\u5bb9\u5e76\u4f18\u5316\u91c7\u6837\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6307\u4ee4\u8c03\u4f18\u6570\u636e\u96c6\u7684\u8d28\u91cf\u548c\u591a\u6837\u6027\uff0c\u4e3a\u9ad8\u6548\u6570\u636e\u9009\u62e9\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2504.13224", "pdf": "https://arxiv.org/pdf/2504.13224", "abs": "https://arxiv.org/abs/2504.13224", "authors": ["Fuwei Liu"], "title": "ICAS: IP Adapter and ControlNet-based Attention Structure for Multi-Subject Style Transfer Optimization", "categories": ["cs.CV", "cs.AI"], "comment": "10 pages, 6 figures", "summary": "Generating multi-subject stylized images remains a significant challenge due\nto the ambiguity in defining style attributes (e.g., color, texture,\natmosphere, and structure) and the difficulty in consistently applying them\nacross multiple subjects. Although recent diffusion-based text-to-image models\nhave achieved remarkable progress, existing methods typically rely on\ncomputationally expensive inversion procedures or large-scale stylized\ndatasets. Moreover, these methods often struggle with maintaining multi-subject\nsemantic fidelity and are limited by high inference costs. To address these\nlimitations, we propose ICAS (IP-Adapter and ControlNet-based Attention\nStructure), a novel framework for efficient and controllable multi-subject\nstyle transfer. Instead of full-model tuning, ICAS adaptively fine-tunes only\nthe content injection branch of a pre-trained diffusion model, thereby\npreserving identity-specific semantics while enhancing style controllability.\nBy combining IP-Adapter for adaptive style injection with ControlNet for\nstructural conditioning, our framework ensures faithful global layout\npreservation alongside accurate local style synthesis. Furthermore, ICAS\nintroduces a cyclic multi-subject content embedding mechanism, which enables\neffective style transfer under limited-data settings without the need for\nextensive stylized corpora. Extensive experiments show that ICAS achieves\nsuperior performance in structure preservation, style consistency, and\ninference efficiency, establishing a new paradigm for multi-subject style\ntransfer in real-world applications.", "AI": {"tldr": "ICAS\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eIP-Adapter\u548cControlNet\u7684\u9ad8\u6548\u591a\u4e3b\u4f53\u98ce\u683c\u8fc1\u79fb\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u5fae\u8c03\u548c\u5faa\u73af\u5185\u5bb9\u5d4c\u5165\u673a\u5236\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u8ba1\u7b97\u6210\u672c\u548c\u8bed\u4e49\u4fdd\u771f\u5ea6\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u591a\u4e3b\u4f53\u98ce\u683c\u8fc1\u79fb\u9762\u4e34\u98ce\u683c\u5c5e\u6027\u5b9a\u4e49\u6a21\u7cca\u548c\u8de8\u4e3b\u4f53\u4e00\u81f4\u6027\u5e94\u7528\u56f0\u96be\u7684\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u6602\u8d35\u8ba1\u7b97\u6216\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u4e14\u96be\u4ee5\u4fdd\u6301\u8bed\u4e49\u4fdd\u771f\u5ea6\u3002", "method": "ICAS\u901a\u8fc7\u81ea\u9002\u5e94\u5fae\u8c03\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u7684\u5185\u5bb9\u6ce8\u5165\u5206\u652f\uff0c\u7ed3\u5408IP-Adapter\u548cControlNet\uff0c\u5b9e\u73b0\u9ad8\u6548\u53ef\u63a7\u7684\u98ce\u683c\u8fc1\u79fb\uff0c\u5e76\u5f15\u5165\u5faa\u73af\u591a\u4e3b\u4f53\u5185\u5bb9\u5d4c\u5165\u673a\u5236\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cICAS\u5728\u7ed3\u6784\u4fdd\u6301\u3001\u98ce\u683c\u4e00\u81f4\u6027\u548c\u63a8\u7406\u6548\u7387\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "ICAS\u4e3a\u591a\u4e3b\u4f53\u98ce\u683c\u8fc1\u79fb\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u53ef\u63a7\u7684\u65b0\u8303\u5f0f\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2504.13529", "pdf": "https://arxiv.org/pdf/2504.13529", "abs": "https://arxiv.org/abs/2504.13529", "authors": ["Zinuo You", "John Cartlidge", "Karen Elliott", "Menghan Ge", "Daniel Gold"], "title": "Risk-aware black-box portfolio construction using Bayesian optimization with adaptive weighted Lagrangian estimator", "categories": ["cs.LG", "cs.SY", "eess.SY", "q-fin.CP", "q-fin.PM"], "comment": "10 pages, 2 figures", "summary": "Existing portfolio management approaches are often black-box models due to\nsafety and commercial issues in the industry. However, their performance can\nvary considerably whenever market conditions or internal trading strategies\nchange. Furthermore, evaluating these non-transparent systems is expensive,\nwhere certain budgets limit observations of the systems. Therefore, optimizing\nperformance while controlling the potential risk of these financial systems has\nbecome a critical challenge. This work presents a novel Bayesian optimization\nframework to optimize black-box portfolio management models under limited\nobservations. In conventional Bayesian optimization settings, the objective\nfunction is to maximize the expectation of performance metrics. However, simply\nmaximizing performance expectations leads to erratic optimization trajectories,\nwhich exacerbate risk accumulation in portfolio management. Meanwhile, this can\nlead to misalignment between the target distribution and the actual\ndistribution of the black-box model. To mitigate this problem, we propose an\nadaptive weight Lagrangian estimator considering dual objective, which\nincorporates maximizing model performance and minimizing variance of model\nobservations. Extensive experiments demonstrate the superiority of our approach\nover five backtest settings with three black-box stock portfolio management\nmodels. Ablation studies further verify the effectiveness of the proposed\nestimator.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8d1d\u53f6\u65af\u4f18\u5316\u7684\u65b0\u578b\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u6709\u9650\u89c2\u6d4b\u4e0b\u4f18\u5316\u9ed1\u76d2\u6295\u8d44\u7ec4\u5408\u7ba1\u7406\u6a21\u578b\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u6743\u91cd\u62c9\u683c\u6717\u65e5\u4f30\u8ba1\u5668\u5e73\u8861\u6027\u80fd\u4e0e\u98ce\u9669\u3002", "motivation": "\u73b0\u6709\u6295\u8d44\u7ec4\u5408\u7ba1\u7406\u6a21\u578b\u56e0\u884c\u4e1a\u5b89\u5168\u548c\u5546\u4e1a\u95ee\u9898\u591a\u4e3a\u9ed1\u76d2\u6a21\u578b\uff0c\u6027\u80fd\u6ce2\u52a8\u5927\u4e14\u8bc4\u4f30\u6210\u672c\u9ad8\uff0c\u9700\u5728\u6709\u9650\u89c2\u6d4b\u4e0b\u4f18\u5316\u6027\u80fd\u5e76\u63a7\u5236\u98ce\u9669\u3002", "method": "\u91c7\u7528\u8d1d\u53f6\u65af\u4f18\u5316\u6846\u67b6\uff0c\u63d0\u51fa\u81ea\u9002\u5e94\u6743\u91cd\u62c9\u683c\u6717\u65e5\u4f30\u8ba1\u5668\uff0c\u540c\u65f6\u6700\u5927\u5316\u6a21\u578b\u6027\u80fd\u548c\u6700\u5c0f\u5316\u89c2\u6d4b\u65b9\u5dee\u3002", "result": "\u5728\u4e94\u79cd\u56de\u6d4b\u8bbe\u7f6e\u548c\u4e09\u79cd\u9ed1\u76d2\u80a1\u7968\u6295\u8d44\u7ec4\u5408\u7ba1\u7406\u6a21\u578b\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\uff0c\u6d88\u878d\u5b9e\u9a8c\u8fdb\u4e00\u6b65\u8bc1\u5b9e\u4e86\u4f30\u8ba1\u5668\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u9ed1\u76d2\u6a21\u578b\u5728\u6709\u9650\u89c2\u6d4b\u4e0b\u7684\u4f18\u5316\u95ee\u9898\uff0c\u5e73\u8861\u4e86\u6027\u80fd\u4e0e\u98ce\u9669\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2504.13531", "pdf": "https://arxiv.org/pdf/2504.13531", "abs": "https://arxiv.org/abs/2504.13531", "authors": ["Nikolay Manchev", "Luis C. Garcia-Peraza-Herrera"], "title": "Can Local Representation Alignment RNNs Solve Temporal Tasks?", "categories": ["cs.LG"], "comment": null, "summary": "Recurrent Neural Networks (RNNs) are commonly used for real-time processing,\nstreaming data, and cases where the amount of training samples is limited.\nBackpropagation Through Time (BPTT) is the predominant algorithm for training\nRNNs; however, it is frequently criticized for being prone to exploding and\nvanishing gradients and being biologically implausible. In this paper, we\npresent and evaluate a target propagation-based method for RNNs, which uses\nlocal updates and seeks to reduce the said instabilities. Having stable RNN\nmodels increases their practical use in a wide range of fields such as natural\nlanguage processing, time-series forecasting, anomaly detection, control\nsystems, and robotics.\n  The proposed solution uses local representation alignment (LRA). We\nthoroughly analyze the performance of this method, experiment with\nnormalization and different local error functions, and invalidate certain\nassumptions about the behavior of this type of learning. Namely, we demonstrate\nthat despite the decomposition of the network into sub-graphs, the model still\nsuffers from vanishing gradients. We also show that gradient clipping as\nproposed in LRA has little to no effect on network performance. This results in\nan LRA RNN model that is very difficult to train due to vanishing gradients. We\naddress this by introducing gradient regularization in the direction of the\nupdate and demonstrate that this modification promotes gradient flow and\nmeaningfully impacts convergence. We compare and discuss the performance of the\nalgorithm, and we show that the regularized LRA RNN considerably outperforms\nthe unregularized version on three landmark tasks: temporal order, 3-bit\ntemporal order, and random permutation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u76ee\u6807\u4f20\u64ad\u7684RNN\u8bad\u7ec3\u65b9\u6cd5\uff0c\u4f7f\u7528\u5c40\u90e8\u8868\u793a\u5bf9\u9f50\uff08LRA\uff09\u4ee5\u51cf\u5c11\u68af\u5ea6\u6d88\u5931\u548c\u7206\u70b8\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u68af\u5ea6\u6b63\u5219\u5316\u6539\u8fdb\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u4f20\u7edfBPTT\u7b97\u6cd5\u5728\u8bad\u7ec3RNN\u65f6\u5b58\u5728\u68af\u5ea6\u4e0d\u7a33\u5b9a\u548c\u751f\u7269\u4e0d\u73b0\u5b9e\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86RNN\u7684\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u91c7\u7528\u5c40\u90e8\u8868\u793a\u5bf9\u9f50\uff08LRA\uff09\u65b9\u6cd5\uff0c\u5206\u6790\u5176\u6027\u80fd\u5e76\u5f15\u5165\u68af\u5ea6\u6b63\u5219\u5316\u4ee5\u4fc3\u8fdb\u68af\u5ea6\u6d41\u52a8\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u672a\u6b63\u5219\u5316\u7684LRA RNN\u4ecd\u5b58\u5728\u68af\u5ea6\u6d88\u5931\u95ee\u9898\uff0c\u800c\u6b63\u5219\u5316\u7248\u672c\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u8868\u73b0\u663e\u8457\u4f18\u4e8e\u672a\u6b63\u5219\u5316\u7248\u672c\u3002", "conclusion": "\u68af\u5ea6\u6b63\u5219\u5316\u6709\u6548\u89e3\u51b3\u4e86LRA RNN\u7684\u68af\u5ea6\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\uff0c\u6269\u5c55\u4e86RNN\u7684\u5b9e\u7528\u8303\u56f4\u3002"}}
{"id": "2504.13231", "pdf": "https://arxiv.org/pdf/2504.13231", "abs": "https://arxiv.org/abs/2504.13231", "authors": ["Braeden Sherritt", "Isar Nejadgholi", "Marzieh Amini"], "title": "WildFireCan-MMD: A Multimodal dataset for Classification of User-generated Content During Wildfires in Canada", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Rapid information access is vital during wildfires, yet traditional data\nsources are slow and costly. Social media offers real-time updates, but\nextracting relevant insights remains a challenge. We present WildFireCan-MMD, a\nnew multimodal dataset of X posts from recent Canadian wildfires, annotated\nacross 13 key themes. Evaluating both Vision Language Models and custom-trained\nclassifiers, we show that while zero-shot prompting offers quick deployment,\neven simple trained models outperform them when labelled data is available, by\nup to 23%. Our findings highlight the enduring importance of tailored datasets\nand task-specific training. Importantly, such datasets should be localized, as\ndisaster response requirements vary across regions and contexts.", "AI": {"tldr": "WildFireCan-MMD\u662f\u4e00\u4e2a\u65b0\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u4ece\u52a0\u62ff\u5927\u91ce\u706b\u7684X\u5e16\u5b50\u4e2d\u63d0\u53d6\u5173\u952e\u4fe1\u606f\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u5c3d\u7ba1\u96f6\u6837\u672c\u63d0\u793a\u53ef\u4ee5\u5feb\u901f\u90e8\u7f72\uff0c\u4f46\u6709\u6807\u6ce8\u6570\u636e\u65f6\uff0c\u7b80\u5355\u8bad\u7ec3\u6a21\u578b\u8868\u73b0\u66f4\u597d\u3002", "motivation": "\u4f20\u7edf\u6570\u636e\u6e90\u5728\u91ce\u706b\u671f\u95f4\u63d0\u4f9b\u4fe1\u606f\u901f\u5ea6\u6162\u4e14\u6210\u672c\u9ad8\uff0c\u800c\u793e\u4ea4\u5a92\u4f53\u867d\u80fd\u5b9e\u65f6\u66f4\u65b0\uff0c\u4f46\u63d0\u53d6\u76f8\u5173\u6d1e\u5bdf\u4ecd\u5177\u6311\u6218\u6027\u3002", "method": "\u4f7f\u7528WildFireCan-MMD\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u81ea\u5b9a\u4e49\u8bad\u7ec3\u5206\u7c7b\u5668\u7684\u6027\u80fd\u3002", "result": "\u6709\u6807\u6ce8\u6570\u636e\u65f6\uff0c\u7b80\u5355\u8bad\u7ec3\u6a21\u578b\u6bd4\u96f6\u6837\u672c\u63d0\u793a\u8868\u73b0\u66f4\u597d\uff0c\u63d0\u5347\u5e45\u5ea6\u8fbe23%\u3002", "conclusion": "\u5b9a\u5236\u5316\u6570\u636e\u96c6\u548c\u4efb\u52a1\u7279\u5b9a\u8bad\u7ec3\u81f3\u5173\u91cd\u8981\uff0c\u4e14\u6570\u636e\u96c6\u5e94\u672c\u5730\u5316\u4ee5\u9002\u5e94\u4e0d\u540c\u5730\u533a\u7684\u707e\u5bb3\u54cd\u5e94\u9700\u6c42\u3002"}}
{"id": "2504.13543", "pdf": "https://arxiv.org/pdf/2504.13543", "abs": "https://arxiv.org/abs/2504.13543", "authors": ["Armin Iske", "Lennart Ohlsen"], "title": "Irregular Sampling of High-Dimensional Functions in Reproducing Kernel Hilbert Spaces", "categories": ["cs.LG", "cs.IT", "math.IT"], "comment": null, "summary": "We develop sampling formulas for high-dimensional functions in reproducing\nkernel Hilbert spaces, where we rely on irregular samples that are taken at\ndetermining sequences of data points. We place particular emphasis on sampling\nformulas for tensor product kernels, where we show that determining irregular\nsamples in lower dimensions can be composed to obtain a tensor of determining\nirregular samples in higher dimensions. This in turn reduces the computational\ncomplexity of sampling formulas for high-dimensional functions quite\nsignificantly.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9ad8\u7ef4\u51fd\u6570\u5728\u518d\u751f\u6838\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u4e2d\u7684\u91c7\u6837\u516c\u5f0f\uff0c\u5229\u7528\u4e0d\u89c4\u5219\u6837\u672c\u70b9\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "motivation": "\u7814\u7a76\u9ad8\u7ef4\u51fd\u6570\u91c7\u6837\u95ee\u9898\uff0c\u65e8\u5728\u51cf\u5c11\u8ba1\u7b97\u590d\u6742\u6027\u3002", "method": "\u57fa\u4e8e\u5f20\u91cf\u79ef\u6838\uff0c\u901a\u8fc7\u4f4e\u7ef4\u4e0d\u89c4\u5219\u6837\u672c\u70b9\u7ec4\u5408\u751f\u6210\u9ad8\u7ef4\u91c7\u6837\u516c\u5f0f\u3002", "result": "\u663e\u8457\u964d\u4f4e\u4e86\u9ad8\u7ef4\u51fd\u6570\u91c7\u6837\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u9ad8\u7ef4\u51fd\u6570\u91c7\u6837\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.13277", "pdf": "https://arxiv.org/pdf/2504.13277", "abs": "https://arxiv.org/abs/2504.13277", "authors": ["Soorya Ram Shimgekar", "Violeta J. Rodriguez", "Paul A. Bloom", "Dong Whi Yoo", "Koustuv Saha"], "title": "Interpersonal Theory of Suicide as a Lens to Examine Suicidal Ideation in Online Spaces", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CY", "cs.SI"], "comment": null, "summary": "Suicide is a critical global public health issue, with millions experiencing\nsuicidal ideation (SI) each year. Online spaces enable individuals to express\nSI and seek peer support. While prior research has revealed the potential of\ndetecting SI using machine learning and natural language analysis, a key\nlimitation is the lack of a theoretical framework to understand the underlying\nfactors affecting high-risk suicidal intent. To bridge this gap, we adopted the\nInterpersonal Theory of Suicide (IPTS) as an analytic lens to analyze 59,607\nposts from Reddit's r/SuicideWatch, categorizing them into SI dimensions\n(Loneliness, Lack of Reciprocal Love, Self Hate, and Liability) and risk\nfactors (Thwarted Belongingness, Perceived Burdensomeness, and Acquired\nCapability of Suicide). We found that high-risk SI posts express planning and\nattempts, methods and tools, and weaknesses and pain. In addition, we also\nexamined the language of supportive responses through psycholinguistic and\ncontent analyses to find that individuals respond differently to different\nstages of Suicidal Ideation (SI) posts. Finally, we explored the role of AI\nchatbots in providing effective supportive responses to suicidal ideation\nposts. We found that although AI improved structural coherence, expert\nevaluations highlight persistent shortcomings in providing dynamic,\npersonalized, and deeply empathetic support. These findings underscore the need\nfor careful reflection and deeper understanding in both the development and\nconsideration of AI-driven interventions for effective mental health support.", "AI": {"tldr": "\u7814\u7a76\u91c7\u7528\u81ea\u6740\u7684\u4eba\u9645\u5173\u7cfb\u7406\u8bba\uff08IPTS\uff09\u5206\u6790Reddit\u7684r/SuicideWatch\u5e16\u5b50\uff0c\u8bc6\u522b\u9ad8\u98ce\u9669\u81ea\u6740\u610f\u56fe\u7684\u8bed\u8a00\u7279\u5f81\uff0c\u5e76\u63a2\u8ba8AI\u804a\u5929\u673a\u5668\u4eba\u5728\u652f\u6301\u6027\u56de\u5e94\u4e2d\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u5728\u7ebf\u7a7a\u95f4\u4e3a\u81ea\u6740\u610f\u5ff5\uff08SI\uff09\u8868\u8fbe\u548c\u5bfb\u6c42\u652f\u6301\u63d0\u4f9b\u4e86\u5e73\u53f0\uff0c\u4f46\u7f3a\u4e4f\u7406\u8bba\u6846\u67b6\u7406\u89e3\u9ad8\u98ce\u9669\u81ea\u6740\u610f\u56fe\u7684\u6f5c\u5728\u56e0\u7d20\u3002", "method": "\u4f7f\u7528IPTS\u7406\u8bba\u5206\u679059,607\u6761\u5e16\u5b50\uff0c\u5206\u7c7b\u4e3aSI\u7ef4\u5ea6\u548c\u98ce\u9669\u56e0\u7d20\uff0c\u5e76\u901a\u8fc7\u8bed\u8a00\u5206\u6790\u7814\u7a76\u652f\u6301\u6027\u56de\u5e94\u3002", "result": "\u9ad8\u98ce\u9669SI\u5e16\u5b50\u8868\u73b0\u51fa\u8ba1\u5212\u3001\u5c1d\u8bd5\u3001\u65b9\u6cd5\u548c\u75db\u82e6\uff1bAI\u804a\u5929\u673a\u5668\u4eba\u5728\u652f\u6301\u6027\u56de\u5e94\u4e2d\u867d\u6709\u7ed3\u6784\u8fde\u8d2f\u6027\uff0c\u4f46\u7f3a\u4e4f\u52a8\u6001\u548c\u4e2a\u6027\u5316\u3002", "conclusion": "AI\u9a71\u52a8\u7684\u5fc3\u7406\u5065\u5eb7\u5e72\u9884\u9700\u66f4\u6df1\u5165\u7406\u89e3\u548c\u53cd\u601d\uff0c\u4ee5\u5b9e\u73b0\u6709\u6548\u652f\u6301\u3002"}}
{"id": "2504.13558", "pdf": "https://arxiv.org/pdf/2504.13558", "abs": "https://arxiv.org/abs/2504.13558", "authors": ["Yuling Jiao", "Yanming Lai", "Yang Wang", "Bokai Yan"], "title": "Transformers Can Overcome the Curse of Dimensionality: A Theoretical Study from an Approximation Perspective", "categories": ["cs.LG", "cs.AI", "41A25, 68T07, 68T50", "G.0"], "comment": null, "summary": "The Transformer model is widely used in various application areas of machine\nlearning, such as natural language processing. This paper investigates the\napproximation of the H\\\"older continuous function class\n$\\mathcal{H}_{Q}^{\\beta}\\left([0,1]^{d\\times n},\\mathbb{R}^{d\\times n}\\right)$\nby Transformers and constructs several Transformers that can overcome the curse\nof dimensionality. These Transformers consist of one self-attention layer with\none head and the softmax function as the activation function, along with\nseveral feedforward layers. For example, to achieve an approximation accuracy\nof $\\epsilon$, if the activation functions of the feedforward layers in the\nTransformer are ReLU and floor, only\n$\\mathcal{O}\\left(\\log\\frac{1}{\\epsilon}\\right)$ layers of feedforward layers\nare needed, with widths of these layers not exceeding\n$\\mathcal{O}\\left(\\frac{1}{\\epsilon^{2/\\beta}}\\log\\frac{1}{\\epsilon}\\right)$.\nIf other activation functions are allowed in the feedforward layers, the width\nof the feedforward layers can be further reduced to a constant. These results\ndemonstrate that Transformers have a strong expressive capability. The\nconstruction in this paper is based on the Kolmogorov-Arnold Representation\nTheorem and does not require the concept of contextual mapping, hence our proof\nis more intuitively clear compared to previous Transformer approximation works.\nAdditionally, the translation technique proposed in this paper helps to apply\nthe previous approximation results of feedforward neural networks to\nTransformer research.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86Transformer\u6a21\u578b\u5bf9H\u00f6lder\u8fde\u7eed\u51fd\u6570\u7c7b\u7684\u903c\u8fd1\u80fd\u529b\uff0c\u6784\u9020\u4e86\u80fd\u591f\u514b\u670d\u7ef4\u5ea6\u707e\u96be\u7684Transformer\u7ed3\u6784\uff0c\u5c55\u793a\u4e86\u5176\u5f3a\u5927\u7684\u8868\u8fbe\u80fd\u529b\u3002", "motivation": "\u63a2\u7d22Transformer\u5728\u903c\u8fd1\u9ad8\u7ef4\u51fd\u6570\u65f6\u7684\u80fd\u529b\uff0c\u5c24\u5176\u662f\u9488\u5bf9H\u00f6lder\u8fde\u7eed\u51fd\u6570\u7c7b\uff0c\u4ee5\u514b\u670d\u7ef4\u5ea6\u707e\u96be\u95ee\u9898\u3002", "method": "\u6784\u9020\u4e86\u5305\u542b\u5355\u5934\u81ea\u6ce8\u610f\u529b\u5c42\u548c\u82e5\u5e72\u524d\u9988\u5c42\u7684Transformer\uff0c\u6fc0\u6d3b\u51fd\u6570\u4e3asoftmax\u3001ReLU\u548cfloor\u7b49\uff0c\u57fa\u4e8eKolmogorov-Arnold\u8868\u793a\u5b9a\u7406\u8fdb\u884c\u8bbe\u8ba1\u3002", "result": "\u5728\u903c\u8fd1\u7cbe\u5ea6\u4e3a\u03b5\u65f6\uff0c\u4ec5\u9700O(log(1/\u03b5))\u5c42\u524d\u9988\u5c42\uff0c\u5bbd\u5ea6\u4e0d\u8d85\u8fc7O(1/\u03b5^(2/\u03b2)log(1/\u03b5))\uff1b\u82e5\u5141\u8bb8\u5176\u4ed6\u6fc0\u6d3b\u51fd\u6570\uff0c\u5bbd\u5ea6\u53ef\u8fdb\u4e00\u6b65\u964d\u81f3\u5e38\u6570\u3002", "conclusion": "Transformer\u5177\u6709\u5f3a\u5927\u7684\u8868\u8fbe\u80fd\u529b\uff0c\u672c\u6587\u7684\u6784\u9020\u65b9\u6cd5\u76f4\u89c2\u6e05\u6670\uff0c\u4e14\u63d0\u51fa\u7684\u5e73\u79fb\u6280\u672f\u6709\u52a9\u4e8e\u5c06\u524d\u9988\u7f51\u7edc\u7684\u903c\u8fd1\u7ed3\u679c\u5e94\u7528\u4e8eTransformer\u7814\u7a76\u3002"}}
{"id": "2504.13308", "pdf": "https://arxiv.org/pdf/2504.13308", "abs": "https://arxiv.org/abs/2504.13308", "authors": ["Leena G Pillai", "D. Muhammad Noorul Mubarak"], "title": "Acoustic to Articulatory Inversion of Speech; Data Driven Approaches, Challenges, Applications, and Future Scope", "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": "This is a review paper about Acoustic to Articulatory inversion of\n  speech, presented in an international conference. This paper has 8 pages and\n  2 figures", "summary": "This review is focused on the data-driven approaches applied in different\napplications of Acoustic-to-Articulatory Inversion (AAI) of speech. This review\npaper considered the relevant works published in the last ten years\n(2011-2021). The selection criteria includes (a) type of AAI - Speaker\nDependent and Speaker Independent AAI, (b) objectives of the work -\nArticulatory approximation, Articulatory Feature space selection and Automatic\nSpeech Recognition (ASR), explore the correlation between acoustic and\narticulatory features, and framework for Computer-assisted language training,\n(c) Corpus - Simultaneously recorded speech (wav) and medical imaging models\nsuch as ElectroMagnetic Articulography (EMA), Electropalatography (EPG),\nLaryngography, Electroglottography (EGG), X-ray Cineradiography, Ultrasound,\nand real-time Magnetic Resonance Imaging (rtMRI), (d) Methods or models -\nrecent works are considered, and therefore all the works are based on machine\nlearning, (e) Evaluation - as AAI is a non-linear regression problem, the\nperformance evaluation is mostly done by Correlation Coefficient (CC), Root\nMean Square Error (RMSE), and also considered Mean Square Error (MSE), and Mean\nFormat Error (MFE). The practical application of the AAI model can provide a\nbetter and user-friendly interpretable image feedback system of articulatory\npositions, especially tongue movement. Such trajectory feedback system can be\nused to provide phonetic, language, and speech therapy for pathological\nsubjects.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u8fc7\u53bb\u5341\u5e74\uff082011-2021\uff09\u4e2d\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u5728\u58f0\u5b66-\u53d1\u97f3\u53cd\u6f14\uff08AAI\uff09\u4e2d\u7684\u5e94\u7528\uff0c\u6db5\u76d6\u4e0d\u540c\u76ee\u6807\u3001\u8bed\u6599\u5e93\u3001\u65b9\u6cd5\u548c\u8bc4\u4f30\u6307\u6807\u3002", "motivation": "\u63a2\u7d22AAI\u5728\u53d1\u97f3\u8fd1\u4f3c\u3001\u7279\u5f81\u7a7a\u95f4\u9009\u62e9\u3001\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\uff08ASR\uff09\u53ca\u8bed\u8a00\u8bad\u7ec3\u4e2d\u7684\u5e94\u7528\uff0c\u5e76\u5206\u6790\u58f0\u5b66\u4e0e\u53d1\u97f3\u7279\u5f81\u7684\u76f8\u5173\u6027\u3002", "method": "\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u4f7f\u7528\u591a\u79cd\u533b\u5b66\u6210\u50cf\u6a21\u578b\uff08\u5982EMA\u3001EPG\u3001rtMRI\u7b49\uff09\u548c\u8bc4\u4f30\u6307\u6807\uff08CC\u3001RMSE\u3001MSE\u3001MFE\uff09\u3002", "result": "AAI\u6a21\u578b\u53ef\u63d0\u4f9b\u76f4\u89c2\u7684\u53d1\u97f3\u4f4d\u7f6e\u53cd\u9988\uff0c\u5c24\u5176\u5728\u820c\u90e8\u8fd0\u52a8\u65b9\u9762\uff0c\u9002\u7528\u4e8e\u8bed\u97f3\u3001\u8bed\u8a00\u53ca\u75c5\u7406\u5b66\u6cbb\u7597\u3002", "conclusion": "AAI\u6a21\u578b\u5728\u53d1\u97f3\u53cd\u9988\u548c\u8bed\u8a00\u8bad\u7ec3\u4e2d\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\uff0c\u5c24\u5176\u5728\u6539\u5584\u53d1\u97f3\u6e05\u6670\u5ea6\u548c\u6cbb\u7597\u75c5\u7406\u5b66\u60a3\u8005\u65b9\u9762\u3002"}}
{"id": "2504.13569", "pdf": "https://arxiv.org/pdf/2504.13569", "abs": "https://arxiv.org/abs/2504.13569", "authors": ["Djohan Bonnet", "Kellian Cottart", "Tifenn Hirtzlin", "Tarcisius Januel", "Thomas Dalgaty", "Elisa Vianello", "Damien Querlioz"], "title": "Bayesian continual learning and forgetting in neural networks", "categories": ["cs.LG"], "comment": null, "summary": "Biological synapses effortlessly balance memory retention and flexibility,\nyet artificial neural networks still struggle with the extremes of catastrophic\nforgetting and catastrophic remembering. Here, we introduce Metaplasticity from\nSynaptic Uncertainty (MESU), a Bayesian framework that updates network\nparameters according their uncertainty. This approach allows a principled\ncombination of learning and forgetting that ensures that critical knowledge is\npreserved while unused or outdated information is gradually released. Unlike\nstandard Bayesian approaches -- which risk becoming overly constrained, and\npopular continual-learning methods that rely on explicit task boundaries, MESU\nseamlessly adapts to streaming data. It further provides reliable epistemic\nuncertainty estimates, allowing out-of-distribution detection, the only\ncomputational cost being to sample the weights multiple times to provide proper\noutput statistics. Experiments on image-classification benchmarks demonstrate\nthat MESU mitigates catastrophic forgetting, while maintaining plasticity for\nnew tasks. When training 200 sequential permuted MNIST tasks, MESU outperforms\nestablished continual learning techniques in terms of accuracy, capability to\nlearn additional tasks, and out-of-distribution data detection. Additionally,\ndue to its non-reliance on task boundaries, MESU outperforms conventional\nlearning techniques on the incremental training of CIFAR-100 tasks consistently\nin a wide range of scenarios. Our results unify ideas from metaplasticity,\nBayesian inference, and Hessian-based regularization, offering a\nbiologically-inspired pathway to robust, perpetual learning.", "AI": {"tldr": "MESU\u662f\u4e00\u79cd\u57fa\u4e8e\u8d1d\u53f6\u65af\u6846\u67b6\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u53c2\u6570\u4e0d\u786e\u5b9a\u6027\u66f4\u65b0\u7f51\u7edc\uff0c\u5e73\u8861\u5b66\u4e60\u4e0e\u9057\u5fd8\uff0c\u907f\u514d\u707e\u96be\u6027\u9057\u5fd8\u548c\u707e\u96be\u6027\u8bb0\u5fc6\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\u5728\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8\u548c\u707e\u96be\u6027\u8bb0\u5fc6\u95ee\u9898\uff0c\u6a21\u4eff\u751f\u7269\u7a81\u89e6\u7684\u5e73\u8861\u673a\u5236\u3002", "method": "\u5f15\u5165Metaplasticity from Synaptic Uncertainty (MESU)\uff0c\u5229\u7528\u8d1d\u53f6\u65af\u6846\u67b6\u6839\u636e\u53c2\u6570\u4e0d\u786e\u5b9a\u6027\u66f4\u65b0\u7f51\u7edc\uff0c\u65e0\u9700\u4efb\u52a1\u8fb9\u754c\u3002", "result": "\u5728\u56fe\u50cf\u5206\u7c7b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMESU\u4f18\u4e8e\u73b0\u6709\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\uff0c\u5c24\u5176\u5728200\u4e2a\u987a\u5e8fMNIST\u4efb\u52a1\u548cCIFAR-100\u589e\u91cf\u8bad\u7ec3\u4e2d\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "MESU\u7ed3\u5408\u5143\u53ef\u5851\u6027\u3001\u8d1d\u53f6\u65af\u63a8\u65ad\u548cHessian\u6b63\u5219\u5316\uff0c\u4e3a\u7a33\u5065\u7684\u6301\u7eed\u5b66\u4e60\u63d0\u4f9b\u4e86\u751f\u7269\u542f\u53d1\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.13574", "pdf": "https://arxiv.org/pdf/2504.13574", "abs": "https://arxiv.org/abs/2504.13574", "authors": ["Zhenkai Qin", "Feng Zhu", "Huan Zeng", "Xunyi Nong"], "title": "MAAM: A Lightweight Multi-Agent Aggregation Module for Efficient Image Classification Based on the MindSpore Framework", "categories": ["cs.LG", "cs.CV", "eess.IV"], "comment": null, "summary": "The demand for lightweight models in image classification tasks under\nresource-constrained environments necessitates a balance between computational\nefficiency and robust feature representation. Traditional attention mechanisms,\ndespite their strong feature modeling capability, often struggle with high\ncomputational complexity and structural rigidity, limiting their applicability\nin scenarios with limited computational resources (e.g., edge devices or\nreal-time systems). To address this, we propose the Multi-Agent Aggregation\nModule (MAAM), a lightweight attention architecture integrated with the\nMindSpore framework. MAAM employs three parallel agent branches with\nindependently parameterized operations to extract heterogeneous features,\nadaptively fused via learnable scalar weights, and refined through a\nconvolutional compression layer. Leveraging MindSpore's dynamic computational\ngraph and operator fusion, MAAM achieves 87.0% accuracy on the CIFAR-10\ndataset, significantly outperforming conventional CNN (58.3%) and MLP (49.6%)\nmodels, while improving training efficiency by 30%. Ablation studies confirm\nthe critical role of agent attention (accuracy drops to 32.0% if removed) and\ncompression modules (25.5% if omitted), validating their necessity for\nmaintaining discriminative feature learning. The framework's hardware\nacceleration capabilities and minimal memory footprint further demonstrate its\npracticality, offering a deployable solution for image classification in\nresource-constrained scenarios without compromising accuracy.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u6ce8\u610f\u529b\u67b6\u6784MAAM\uff0c\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u6ce8\u610f\u529b\u673a\u5236\u8ba1\u7b97\u590d\u6742\u4e14\u7ed3\u6784\u56fa\u5b9a\uff0c\u96be\u4ee5\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u5e94\u7528\uff0c\u9700\u5e73\u8861\u8ba1\u7b97\u6548\u7387\u4e0e\u7279\u5f81\u8868\u793a\u80fd\u529b\u3002", "method": "MAAM\u901a\u8fc7\u4e09\u4e2a\u5e76\u884c\u4ee3\u7406\u5206\u652f\u63d0\u53d6\u5f02\u6784\u7279\u5f81\uff0c\u81ea\u9002\u5e94\u878d\u5408\u5e76\u901a\u8fc7\u5377\u79ef\u538b\u7f29\u5c42\u4f18\u5316\uff0c\u7ed3\u5408MindSpore\u6846\u67b6\u5b9e\u73b0\u9ad8\u6548\u8ba1\u7b97\u3002", "result": "\u5728CIFAR-10\u6570\u636e\u96c6\u4e0a\u8fbe\u523087.0%\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u4f20\u7edfCNN\uff0858.3%\uff09\u548cMLP\uff0849.6%\uff09\uff0c\u8bad\u7ec3\u6548\u7387\u63d0\u534730%\u3002", "conclusion": "MAAM\u5728\u8d44\u6e90\u53d7\u9650\u573a\u666f\u4e0b\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u56fe\u50cf\u5206\u7c7b\u89e3\u51b3\u65b9\u6848\uff0c\u786c\u4ef6\u52a0\u901f\u548c\u4f4e\u5185\u5b58\u5360\u7528\u9a8c\u8bc1\u4e86\u5176\u5b9e\u7528\u6027\u3002"}}
{"id": "2504.13576", "pdf": "https://arxiv.org/pdf/2504.13576", "abs": "https://arxiv.org/abs/2504.13576", "authors": ["Weiqi Qin", "Yuxin Liu", "Dongze Wu", "Zhenkai Qin", "Qining Luo"], "title": "MSTIM: A MindSpore-Based Model for Traffic Flow Prediction", "categories": ["cs.LG"], "comment": null, "summary": "Aiming at the problems of low accuracy and large error fluctuation of\ntraditional traffic flow predictionmodels when dealing with multi-scale\ntemporal features and dynamic change patterns. this paperproposes a multi-scale\ntime series information modelling model MSTIM based on the Mindspore framework,\nwhich integrates long and short-term memory networks (LSTMs), convolutional\nneural networks (CNN), and the attention mechanism to improve the modelling\naccuracy and stability. The Metropolitan Interstate Traffic Volume (MITV)\ndataset was used for the experiments and compared and analysed with typical\nLSTM-attention models, CNN-attention models and LSTM-CNN models. The\nexperimental results show that the MSTIM model achieves better results in the\nmetrics of Mean Absolute Error (MAE), Mean Square Error (MSE), and Root Mean\nSquare Error (RMSE), which significantly improves the accuracy and stability of\nthe traffic volume prediction.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eMindspore\u6846\u67b6\u7684\u591a\u5c3a\u5ea6\u65f6\u95f4\u5e8f\u5217\u4fe1\u606f\u5efa\u6a21\u6a21\u578bMSTIM\uff0c\u7ed3\u5408LSTM\u3001CNN\u548c\u6ce8\u610f\u529b\u673a\u5236\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u4ea4\u901a\u6d41\u91cf\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u7a33\u5b9a\u6027\u3002", "motivation": "\u4f20\u7edf\u4ea4\u901a\u6d41\u91cf\u9884\u6d4b\u6a21\u578b\u5728\u5904\u7406\u591a\u5c3a\u5ea6\u65f6\u95f4\u7279\u5f81\u548c\u52a8\u6001\u53d8\u5316\u6a21\u5f0f\u65f6\u5b58\u5728\u51c6\u786e\u7387\u4f4e\u3001\u8bef\u5dee\u6ce2\u52a8\u5927\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faMSTIM\u6a21\u578b\uff0c\u6574\u5408LSTM\u3001CNN\u548c\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5229\u7528MITV\u6570\u636e\u96c6\u8fdb\u884c\u5b9e\u9a8c\uff0c\u5e76\u4e0e\u5178\u578b\u6a21\u578b\u5bf9\u6bd4\u5206\u6790\u3002", "result": "MSTIM\u6a21\u578b\u5728MAE\u3001MSE\u548cRMSE\u6307\u6807\u4e0a\u8868\u73b0\u66f4\u4f18\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u51c6\u786e\u6027\u548c\u7a33\u5b9a\u6027\u3002", "conclusion": "MSTIM\u6a21\u578b\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u6a21\u578b\u7684\u4e0d\u8db3\uff0c\u4e3a\u4ea4\u901a\u6d41\u91cf\u9884\u6d4b\u63d0\u4f9b\u4e86\u66f4\u4f18\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.13406", "pdf": "https://arxiv.org/pdf/2504.13406", "abs": "https://arxiv.org/abs/2504.13406", "authors": ["Xiangbo Gao", "Yuheng Wu", "Rujia Wang", "Chenxi Liu", "Yang Zhou", "Zhengzhong Tu"], "title": "LangCoop: Collaborative Driving with Language", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Multi-agent collaboration holds great promise for enhancing the safety,\nreliability, and mobility of autonomous driving systems by enabling information\nsharing among multiple connected agents. However, existing multi-agent\ncommunication approaches are hindered by limitations of existing communication\nmedia, including high bandwidth demands, agent heterogeneity, and information\nloss. To address these challenges, we introduce LangCoop, a new paradigm for\ncollaborative autonomous driving that leverages natural language as a compact\nyet expressive medium for inter-agent communication. LangCoop features two key\ninnovations: Mixture Model Modular Chain-of-thought (M$^3$CoT) for structured\nzero-shot vision-language reasoning and Natural Language Information Packaging\n(LangPack) for efficiently packaging information into concise, language-based\nmessages. Through extensive experiments conducted in the CARLA simulations, we\ndemonstrate that LangCoop achieves a remarkable 96\\% reduction in communication\nbandwidth (< 2KB per message) compared to image-based communication, while\nmaintaining competitive driving performance in the closed-loop evaluation.", "AI": {"tldr": "LangCoop\u5229\u7528\u81ea\u7136\u8bed\u8a00\u4f5c\u4e3a\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u7684\u901a\u4fe1\u5a92\u4ecb\uff0c\u663e\u8457\u964d\u4f4e\u5e26\u5bbd\u9700\u6c42\u5e76\u4fdd\u6301\u9a7e\u9a76\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u591a\u667a\u80fd\u4f53\u901a\u4fe1\u65b9\u6cd5\u5b58\u5728\u5e26\u5bbd\u9700\u6c42\u9ad8\u3001\u5f02\u6784\u6027\u548c\u4fe1\u606f\u4e22\u5931\u7b49\u95ee\u9898\uff0cLangCoop\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\u3002", "method": "\u63d0\u51faLangCoop\u6846\u67b6\uff0c\u5305\u542bM\u00b3CoT\uff08\u7ed3\u6784\u5316\u96f6\u6837\u672c\u89c6\u89c9\u8bed\u8a00\u63a8\u7406\uff09\u548cLangPack\uff08\u9ad8\u6548\u8bed\u8a00\u4fe1\u606f\u5c01\u88c5\uff09\u3002", "result": "\u5b9e\u9a8c\u663e\u793aLangCoop\u901a\u4fe1\u5e26\u5bbd\u964d\u4f4e96%\uff08\u6bcf\u6761\u6d88\u606f<2KB\uff09\uff0c\u9a7e\u9a76\u6027\u80fd\u4e0e\u56fe\u50cf\u901a\u4fe1\u76f8\u5f53\u3002", "conclusion": "LangCoop\u4e3a\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u6027\u80fd\u4f18\u8d8a\u7684\u901a\u4fe1\u8303\u5f0f\u3002"}}
{"id": "2504.13586", "pdf": "https://arxiv.org/pdf/2504.13586", "abs": "https://arxiv.org/abs/2504.13586", "authors": ["Jinghan Yang", "Anupam Pani", "Yunchao Zhang"], "title": "How to Achieve Higher Accuracy with Less Training Points?", "categories": ["cs.LG", "stat.AP"], "comment": null, "summary": "In the era of large-scale model training, the extensive use of available\ndatasets has resulted in significant computational inefficiencies. To tackle\nthis issue, we explore methods for identifying informative subsets of training\ndata that can achieve comparable or even superior model performance. We propose\na technique based on influence functions to determine which training samples\nshould be included in the training set. We conducted empirical evaluations of\nour method on binary classification tasks utilizing logistic regression models.\nOur approach demonstrates performance comparable to that of training on the\nentire dataset while using only 10% of the data. Furthermore, we found that our\nmethod achieved even higher accuracy when trained with just 60% of the data.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f71\u54cd\u51fd\u6570\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u9009\u62e9\u4fe1\u606f\u91cf\u5927\u7684\u8bad\u7ec3\u6570\u636e\u5b50\u96c6\uff0c\u5728\u4ec5\u4f7f\u752810%\u6570\u636e\u65f6\u8fbe\u5230\u4e0e\u5168\u6570\u636e\u96c6\u76f8\u5f53\u7684\u6a21\u578b\u6027\u80fd\uff0c\u751a\u81f3\u5728\u4f7f\u752860%\u6570\u636e\u65f6\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u5927\u89c4\u6a21\u6a21\u578b\u8bad\u7ec3\u4e2d\uff0c\u6570\u636e\u96c6\u7684\u5e7f\u6cdb\u4f7f\u7528\u5bfc\u81f4\u8ba1\u7b97\u6548\u7387\u4f4e\u4e0b\uff0c\u9700\u627e\u5230\u80fd\u4fdd\u6301\u6216\u63d0\u5347\u6027\u80fd\u7684\u6570\u636e\u5b50\u96c6\u3002", "method": "\u57fa\u4e8e\u5f71\u54cd\u51fd\u6570\u7684\u6280\u672f\uff0c\u7528\u4e8e\u786e\u5b9a\u8bad\u7ec3\u96c6\u4e2d\u5e94\u5305\u542b\u7684\u6837\u672c\u3002", "result": "\u5728\u4e8c\u5143\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0c\u4ec5\u752810%\u6570\u636e\u5373\u53ef\u8fbe\u5230\u5168\u6570\u636e\u96c6\u6027\u80fd\uff0c60%\u6570\u636e\u65f6\u51c6\u786e\u7387\u66f4\u9ad8\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u663e\u8457\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2504.13310", "pdf": "https://arxiv.org/pdf/2504.13310", "abs": "https://arxiv.org/abs/2504.13310", "authors": ["Yasin Almalioglu", "Andrzej Kucik", "Geoffrey French", "Dafni Antotsiou", "Alexander Adam", "Cedric Archambeau"], "title": "SAR Object Detection with Self-Supervised Pretraining and Curriculum-Aware Sampling", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted to ICLR 2025 ML4RS https://ml-for-rs.github.io/iclr2025/", "summary": "Object detection in satellite-borne Synthetic Aperture Radar (SAR) imagery\nholds immense potential in tasks such as urban monitoring and disaster\nresponse. However, the inherent complexities of SAR data and the scarcity of\nannotations present significant challenges in the advancement of object\ndetection in this domain. Notably, the detection of small objects in\nsatellite-borne SAR images poses a particularly intricate problem, because of\nthe technology's relatively low spatial resolution and inherent noise.\nFurthermore, the lack of large labelled SAR datasets hinders the development of\nsupervised deep learning-based object detection models. In this paper, we\nintroduce TRANSAR, a novel self-supervised end-to-end vision transformer-based\nSAR object detection model that incorporates masked image pre-training on an\nunlabeled SAR image dataset that spans more than $25,700$ km\\textsuperscript{2}\nground area. Unlike traditional object detection formulation, our approach\ncapitalises on auxiliary binary semantic segmentation, designed to segregate\nobjects of interest during the post-tuning, especially the smaller ones, from\nthe background. In addition, to address the innate class imbalance due to the\ndisproportion of the object to the image size, we introduce an adaptive\nsampling scheduler that dynamically adjusts the target class distribution\nduring training based on curriculum learning and model feedback. This approach\nallows us to outperform conventional supervised architecture such as DeepLabv3\nor UNet, and state-of-the-art self-supervised learning-based arhitectures such\nas DPT, SegFormer or UperNet, as shown by extensive evaluations on benchmark\nSAR datasets.", "AI": {"tldr": "TRANSAR\u662f\u4e00\u79cd\u57fa\u4e8e\u81ea\u76d1\u7763\u89c6\u89c9Transformer\u7684SAR\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\uff0c\u901a\u8fc7\u63a9\u7801\u56fe\u50cf\u9884\u8bad\u7ec3\u548c\u81ea\u9002\u5e94\u91c7\u6837\u8c03\u5ea6\u5668\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5c0f\u76ee\u6807\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "SAR\u56fe\u50cf\u4e2d\u7684\u76ee\u6807\u68c0\u6d4b\u9762\u4e34\u6570\u636e\u590d\u6742\u6027\u548c\u6807\u6ce8\u7a00\u7f3a\u7684\u6311\u6218\uff0c\u5c24\u5176\u662f\u5c0f\u76ee\u6807\u68c0\u6d4b\u56e0\u4f4e\u5206\u8fa8\u7387\u548c\u566a\u58f0\u95ee\u9898\u66f4\u4e3a\u56f0\u96be\u3002", "method": "\u91c7\u7528\u81ea\u76d1\u7763\u7684\u7aef\u5230\u7aef\u89c6\u89c9Transformer\uff0c\u7ed3\u5408\u63a9\u7801\u56fe\u50cf\u9884\u8bad\u7ec3\u548c\u8f85\u52a9\u4e8c\u5143\u8bed\u4e49\u5206\u5272\uff0c\u5e76\u5f15\u5165\u81ea\u9002\u5e94\u91c7\u6837\u8c03\u5ea6\u5668\u89e3\u51b3\u7c7b\u522b\u4e0d\u5e73\u8861\u3002", "result": "\u5728\u57fa\u51c6SAR\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u76d1\u7763\u6a21\u578b\uff08\u5982DeepLabv3\u3001UNet\uff09\u548c\u81ea\u76d1\u7763\u6a21\u578b\uff08\u5982DPT\u3001SegFormer\uff09\u3002", "conclusion": "TRANSAR\u901a\u8fc7\u81ea\u76d1\u7763\u5b66\u4e60\u548c\u52a8\u6001\u91c7\u6837\u7b56\u7565\uff0c\u6709\u6548\u89e3\u51b3\u4e86SAR\u56fe\u50cf\u76ee\u6807\u68c0\u6d4b\u7684\u96be\u9898\uff0c\u5c24\u5176\u662f\u5c0f\u76ee\u6807\u68c0\u6d4b\u3002"}}
{"id": "2504.13598", "pdf": "https://arxiv.org/pdf/2504.13598", "abs": "https://arxiv.org/abs/2504.13598", "authors": ["Charalampos Kleitsikas", "Nikolaos Korfiatis", "Stefanos Leonardos", "Carmine Ventre"], "title": "Bitcoin's Edge: Embedded Sentiment in Blockchain Transactional Data", "categories": ["cs.LG", "cs.CE", "cs.CR", "cs.CY"], "comment": "Published in IEEE International Conference on Blockchain and\n  Cryptocurrency 2025", "summary": "Cryptocurrency blockchains, beyond their primary role as distributed payment\nsystems, are increasingly used to store and share arbitrary content, such as\ntext messages and files. Although often non-financial, this hidden content can\nimpact price movements by conveying private information, shaping sentiment, and\ninfluencing public opinion. However, current analyses of such data are limited\nin scope and scalability, primarily relying on manual classification or\nhand-crafted heuristics. In this work, we address these limitations by\nemploying Natural Language Processing techniques to analyze, detect patterns,\nand extract public sentiment encoded within blockchain transactional data.\nUsing a variety of Machine Learning techniques, we showcase for the first time\nthe predictive power of blockchain-embedded sentiment in forecasting\ncryptocurrency price movements on the Bitcoin and Ethereum blockchains. Our\nfindings shed light on a previously underexplored source of freely available,\ntransparent, and immutable data and introduce blockchain sentiment analysis as\na novel and robust framework for enhancing financial predictions in\ncryptocurrency markets. Incidentally, we discover an asymmetry between\ncryptocurrencies; Bitcoin has an informational advantage over Ethereum in that\nthe sentiment embedded into transactional data is sufficient to predict its\nprice movement.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5229\u7528\u81ea\u7136\u8bed\u8a00\u5904\u7406\u548c\u673a\u5668\u5b66\u4e60\u6280\u672f\u5206\u6790\u533a\u5757\u94fe\u4e2d\u5d4c\u5165\u7684\u60c5\u611f\u6570\u636e\uff0c\u9996\u6b21\u5c55\u793a\u4e86\u5176\u5728\u9884\u6d4b\u6bd4\u7279\u5e01\u548c\u4ee5\u592a\u574a\u4ef7\u683c\u53d8\u52a8\u4e2d\u7684\u6f5c\u529b\uff0c\u5e76\u53d1\u73b0\u6bd4\u7279\u5e01\u5728\u4fe1\u606f\u4f18\u52bf\u4e0a\u4f18\u4e8e\u4ee5\u592a\u574a\u3002", "motivation": "\u533a\u5757\u94fe\u4e0d\u4ec5\u7528\u4e8e\u652f\u4ed8\uff0c\u8fd8\u5b58\u50a8\u4e86\u5927\u91cf\u975e\u91d1\u878d\u5185\u5bb9\uff0c\u8fd9\u4e9b\u5185\u5bb9\u53ef\u80fd\u901a\u8fc7\u4f20\u9012\u79c1\u4eba\u4fe1\u606f\u3001\u5851\u9020\u60c5\u7eea\u548c\u5f71\u54cd\u8206\u8bba\u6765\u5f71\u54cd\u4ef7\u683c\u53d8\u52a8\u3002\u76ee\u524d\u7684\u5206\u6790\u65b9\u6cd5\u5728\u8303\u56f4\u548c\u53ef\u6269\u5c55\u6027\u4e0a\u6709\u9650\uff0c\u4e3b\u8981\u4f9d\u8d56\u4eba\u5de5\u5206\u7c7b\u6216\u624b\u5de5\u542f\u53d1\u5f0f\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6280\u672f\u5206\u6790\u533a\u5757\u94fe\u4ea4\u6613\u6570\u636e\u4e2d\u7684\u60c5\u611f\u6a21\u5f0f\uff0c\u5e76\u7ed3\u5408\u591a\u79cd\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u9884\u6d4b\u52a0\u5bc6\u8d27\u5e01\u4ef7\u683c\u53d8\u52a8\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u533a\u5757\u94fe\u5d4c\u5165\u7684\u60c5\u611f\u6570\u636e\u5728\u9884\u6d4b\u6bd4\u7279\u5e01\u4ef7\u683c\u53d8\u52a8\u4e0a\u5177\u6709\u663e\u8457\u6548\u679c\uff0c\u800c\u4ee5\u592a\u574a\u7684\u6548\u679c\u8f83\u5f31\uff0c\u63ed\u793a\u4e86\u6bd4\u7279\u5e01\u7684\u4fe1\u606f\u4f18\u52bf\u3002", "conclusion": "\u533a\u5757\u94fe\u60c5\u611f\u5206\u6790\u4e3a\u52a0\u5bc6\u8d27\u5e01\u5e02\u573a\u7684\u91d1\u878d\u9884\u6d4b\u63d0\u4f9b\u4e86\u65b0\u9896\u4e14\u7a33\u5065\u7684\u6846\u67b6\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u4e0d\u540c\u52a0\u5bc6\u8d27\u5e01\u4e4b\u95f4\u7684\u4fe1\u606f\u4e0d\u5bf9\u79f0\u6027\u3002"}}
{"id": "2504.13472", "pdf": "https://arxiv.org/pdf/2504.13472", "abs": "https://arxiv.org/abs/2504.13472", "authors": ["Xinchen Wang", "Pengfei Gao", "Chao Peng", "Ruida Hu", "Cuiyun Gao"], "title": "CodeVisionary: An Agent-based Framework for Evaluating Large Language Models in Code Generation", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) have demonstrated strong capabilities in code\ngeneration, underscoring the critical need for rigorous and comprehensive\nevaluation. Existing evaluation approaches fall into three categories,\nincluding human-centered, metric-based, and LLM-based. Considering that\nhuman-centered approaches are labour-intensive and metric-based ones overly\nrely on reference answers, LLM-based approaches are gaining increasing\nattention due to their stronger contextual understanding capabilities and\nsuperior efficiency. However, the performance of LLM-based approaches remains\nlimited due to: (1) lack of multisource domain knowledge, and (2) insufficient\ncomprehension of complex code.\n  To mitigate the limitations, we propose CodeVisionary, the first LLM-based\nagent framework for evaluating LLMs in code generation. CodeVisionary consists\nof two stages: (1) Multiscore knowledge analysis stage, which aims to gather\nmultisource and comprehensive domain knowledge by formulating and executing a\nstepwise evaluation plan. (2) Negotiation-based scoring stage, which involves\nmultiple judges engaging in discussions to better comprehend the complex code\nand reach a consensus on the evaluation score. Extensive experiments\ndemonstrate that CodeVisionary achieves the best performance for evaluating\nLLMs in code generation, outperforming the best baseline methods with average\nimprovements of 0.202, 0.139, and 0.117 in Pearson, Spearman, and Kendall-Tau\ncoefficients, respectively. Besides, CodeVisionary provides detailed evaluation\nreports, which assist developers in identifying shortcomings and making\nimprovements. The resources of CodeVisionary are available at\nhttps://anonymous.4open.science/r/CodeVisionary.", "AI": {"tldr": "CodeVisionary\u662f\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u4ee3\u7801\u751f\u6210\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u6e90\u77e5\u8bc6\u5206\u6790\u548c\u534f\u5546\u8bc4\u5206\u63d0\u5347\u8bc4\u4f30\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\uff08\u4eba\u5de5\u3001\u57fa\u4e8e\u6307\u6807\u3001\u57fa\u4e8eLLM\uff09\u5b58\u5728\u6548\u7387\u4f4e\u6216\u4f9d\u8d56\u53c2\u8003\u7b54\u6848\u7684\u95ee\u9898\uff0c\u4e14\u57fa\u4e8eLLM\u7684\u65b9\u6cd5\u56e0\u77e5\u8bc6\u4e0d\u8db3\u548c\u4ee3\u7801\u7406\u89e3\u6709\u9650\u800c\u53d7\u9650\u3002", "method": "CodeVisionary\u5206\u4e3a\u4e24\u9636\u6bb5\uff1a\u591a\u6e90\u77e5\u8bc6\u5206\u6790\u9636\u6bb5\u548c\u534f\u5546\u8bc4\u5206\u9636\u6bb5\uff0c\u901a\u8fc7\u591a\u8bc4\u59d4\u8ba8\u8bba\u8fbe\u6210\u5171\u8bc6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cCodeVisionary\u5728Pearson\u3001Spearman\u548cKendall-Tau\u7cfb\u6570\u4e0a\u5206\u522b\u5e73\u5747\u63d0\u53470.202\u30010.139\u548c0.117\uff0c\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "CodeVisionary\u4e0d\u4ec5\u6027\u80fd\u4f18\u8d8a\uff0c\u8fd8\u63d0\u4f9b\u8be6\u7ec6\u8bc4\u4f30\u62a5\u544a\uff0c\u5e2e\u52a9\u5f00\u53d1\u8005\u6539\u8fdb\u4ee3\u7801\u751f\u6210\u6a21\u578b\u3002"}}
{"id": "2504.13340", "pdf": "https://arxiv.org/pdf/2504.13340", "abs": "https://arxiv.org/abs/2504.13340", "authors": ["Oliver Mills", "Philip Conaghan", "Nishant Ravikumar", "Samuel Relton"], "title": "Putting the Segment Anything Model to the Test with 3D Knee MRI -- A Comparison with State-of-the-Art Performance", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "Work accepted at BMVC 2024. Minor changes to the camera-ready version\n  since acceptance include a corrected running header and the addition of an\n  Acknowledgments section (including code availability)", "summary": "Menisci are cartilaginous tissue found within the knee that contribute to\njoint lubrication and weight dispersal. Damage to menisci can lead to onset and\nprogression of knee osteoarthritis (OA), a condition that is a leading cause of\ndisability, and for which there are few effective therapies. Accurate automated\nsegmentation of menisci would allow for earlier detection and treatment of\nmeniscal abnormalities, as well as shedding more light on the role the menisci\nplay in OA pathogenesis. Focus in this area has mainly used variants of\nconvolutional networks, but there has been no attempt to utilise recent large\nvision transformer segmentation models. The Segment Anything Model (SAM) is a\nso-called foundation segmentation model, which has been found useful across a\nrange of different tasks due to the large volume of data used for training the\nmodel. In this study, SAM was adapted to perform fully-automated segmentation\nof menisci from 3D knee magnetic resonance images. A 3D U-Net was also trained\nas a baseline. It was found that, when fine-tuning only the decoder, SAM was\nunable to compete with 3D U-Net, achieving a Dice score of $0.81\\pm0.03$,\ncompared to $0.87\\pm0.03$, on a held-out test set. When fine-tuning SAM\nend-to-end, a Dice score of $0.87\\pm0.03$ was achieved. The performance of both\nthe end-to-end trained SAM configuration and the 3D U-Net were comparable to\nthe winning Dice score ($0.88\\pm0.03$) in the IWOAI Knee MRI Segmentation\nChallenge 2019. Performance in terms of the Hausdorff Distance showed that both\nconfigurations of SAM were inferior to 3D U-Net in matching the meniscus\nmorphology. Results demonstrated that, despite its generalisability, SAM was\nunable to outperform a basic 3D U-Net in meniscus segmentation, and may not be\nsuitable for similar 3D medical image segmentation tasks also involving fine\nanatomical structures with low contrast and poorly-defined boundaries.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86Segment Anything Model (SAM)\u57283D\u819d\u5173\u8282MRI\u56fe\u50cf\u4e2d\u81ea\u52a8\u5206\u5272\u534a\u6708\u677f\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5176\u6027\u80fd\u4e0e3D U-Net\u76f8\u5f53\uff0c\u4f46\u5728\u5f62\u6001\u5339\u914d\u4e0a\u7a0d\u900a\u3002", "motivation": "\u534a\u6708\u677f\u635f\u4f24\u53ef\u80fd\u5bfc\u81f4\u819d\u5173\u8282\u9aa8\u5173\u8282\u708e\uff0c\u76ee\u524d\u7f3a\u4e4f\u6709\u6548\u7597\u6cd5\u3002\u81ea\u52a8\u5206\u5272\u6280\u672f\u6709\u52a9\u4e8e\u65e9\u671f\u68c0\u6d4b\u548c\u6cbb\u7597\u3002", "method": "\u7814\u7a76\u6bd4\u8f83\u4e86SAM\uff08\u4ec5\u5fae\u8c03\u89e3\u7801\u5668\u548c\u7aef\u5230\u7aef\u5fae\u8c03\uff09\u4e0e3D U-Net\u5728\u534a\u6708\u677f\u5206\u5272\u4e2d\u7684\u8868\u73b0\u3002", "result": "SAM\u7aef\u5230\u7aef\u5fae\u8c03\u540eDice\u5206\u6570\u4e3a0.87\u00b10.03\uff0c\u4e0e3D U-Net\u76f8\u5f53\uff0c\u4f46\u5728Hausdorff\u8ddd\u79bb\u4e0a\u8868\u73b0\u8f83\u5dee\u3002", "conclusion": "\u5c3d\u7ba1SAM\u5177\u6709\u901a\u7528\u6027\uff0c\u4f46\u57283D\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4efb\u52a1\u4e2d\u53ef\u80fd\u4e0d\u9002\u7528\u4e8e\u4f4e\u5bf9\u6bd4\u5ea6\u548c\u8fb9\u754c\u6a21\u7cca\u7684\u7cbe\u7ec6\u7ed3\u6784\u3002"}}
{"id": "2504.13610", "pdf": "https://arxiv.org/pdf/2504.13610", "abs": "https://arxiv.org/abs/2504.13610", "authors": ["Khoa Tran", "Simon S. Woo"], "title": "Fairness and Robustness in Machine Unlearning", "categories": ["cs.LG"], "comment": "5 pages", "summary": "Machine unlearning poses the challenge of ``how to eliminate the influence of\nspecific data from a pretrained model'' in regard to privacy concerns. While\nprior research on approximated unlearning has demonstrated accuracy and\nefficiency in time complexity, we claim that it falls short of achieving exact\nunlearning, and we are the first to focus on fairness and robustness in machine\nunlearning algorithms. Our study presents fairness Conjectures for a\nwell-trained model, based on the variance-bias trade-off characteristic, and\nconsiders their relevance to robustness. Our Conjectures are supported by\nexperiments conducted on the two most widely used model architectures, ResNet\nand ViT, demonstrating the correlation between fairness and robustness:\n\\textit{the higher fairness-gap is, the more the model is sensitive and\nvulnerable}. In addition, our experiments demonstrate the vulnerability of\ncurrent state-of-the-art approximated unlearning algorithms to adversarial\nattacks, where their unlearned models suffer a significant drop in accuracy\ncompared to the exact-unlearned models. We claim that our fairness-gap\nmeasurement and robustness metric should be used to evaluate the unlearning\nalgorithm. Furthermore, we demonstrate that unlearning in the intermediate and\nlast layers is sufficient and cost-effective for time and memory complexity.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5173\u6ce8\u516c\u5e73\u6027\u548c\u9c81\u68d2\u6027\u7684\u673a\u5668\u9057\u5fd8\u7b97\u6cd5\uff0c\u9996\u6b21\u5c06\u516c\u5e73\u6027\u5dee\u8ddd\u4e0e\u9c81\u68d2\u6027\u5173\u8054\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u91cd\u8981\u6027\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u8fd1\u4f3c\u9057\u5fd8\u7b97\u6cd5\u5728\u9690\u79c1\u4fdd\u62a4\u4e2d\u672a\u80fd\u5b9e\u73b0\u7cbe\u786e\u9057\u5fd8\u7684\u95ee\u9898\uff0c\u5e76\u9996\u6b21\u5173\u6ce8\u516c\u5e73\u6027\u548c\u9c81\u68d2\u6027\u5728\u673a\u5668\u9057\u5fd8\u4e2d\u7684\u91cd\u8981\u6027\u3002", "method": "\u63d0\u51fa\u516c\u5e73\u6027\u731c\u60f3\uff0c\u57fa\u4e8e\u65b9\u5dee-\u504f\u5dee\u6743\u8861\u7279\u6027\uff0c\u5e76\u5728ResNet\u548cViT\u6a21\u578b\u4e0a\u9a8c\u8bc1\u516c\u5e73\u6027\u4e0e\u9c81\u68d2\u6027\u7684\u76f8\u5173\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u516c\u5e73\u6027\u5dee\u8ddd\u8d8a\u5927\uff0c\u6a21\u578b\u8d8a\u654f\u611f\u8106\u5f31\uff1b\u73b0\u6709\u8fd1\u4f3c\u9057\u5fd8\u7b97\u6cd5\u5bf9\u5bf9\u6297\u653b\u51fb\u8106\u5f31\uff0c\u7cbe\u786e\u9057\u5fd8\u6a21\u578b\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "\u5efa\u8bae\u4f7f\u7528\u516c\u5e73\u6027\u5dee\u8ddd\u548c\u9c81\u68d2\u6027\u6307\u6807\u8bc4\u4f30\u9057\u5fd8\u7b97\u6cd5\uff0c\u5e76\u8bc1\u660e\u4e2d\u95f4\u5c42\u548c\u6700\u540e\u4e00\u5c42\u7684\u9057\u5fd8\u5728\u65f6\u95f4\u548c\u5185\u5b58\u590d\u6742\u5ea6\u4e0a\u662f\u9ad8\u6548\u4e14\u8db3\u591f\u7684\u3002"}}
{"id": "2504.13344", "pdf": "https://arxiv.org/pdf/2504.13344", "abs": "https://arxiv.org/abs/2504.13344", "authors": ["Yahao Dai", "Henry Chan", "Aikaterini Vriza", "Fredrick Kim", "Yunfei Wang", "Wei Liu", "Naisong Shan", "Jing Xu", "Max Weires", "Yukun Wu", "Zhiqiang Cao", "C. Suzanne Miller", "Ralu Divan", "Xiaodan Gu", "Chenhui Zhu", "Sihong Wang", "Jie Xu"], "title": "Adaptive AI decision interface for autonomous electronic material discovery", "categories": ["cond-mat.mtrl-sci", "cs.AI"], "comment": null, "summary": "AI-powered autonomous experimentation (AI/AE) can accelerate materials\ndiscovery but its effectiveness for electronic materials is hindered by data\nscarcity from lengthy and complex design-fabricate-test-analyze cycles. Unlike\nexperienced human scientists, even advanced AI algorithms in AI/AE lack the\nadaptability to make informative real-time decisions with limited datasets.\nHere, we address this challenge by developing and implementing an AI decision\ninterface on our AI/AE system. The central element of the interface is an AI\nadvisor that performs real-time progress monitoring, data analysis, and\ninteractive human-AI collaboration for actively adapting to experiments in\ndifferent stages and types. We applied this platform to an emerging type of\nelectronic materials-mixed ion-electron conducting polymers (MIECPs) -- to\nengineer and study the relationships between multiscale morphology and\nproperties. Using organic electrochemical transistors (OECT) as the testing-bed\ndevice for evaluating the mixed-conducting figure-of-merit -- the product of\ncharge-carrier mobility and the volumetric capacitance ({\\mu}C*), our adaptive\nAI/AE platform achieved a 150% increase in {\\mu}C* compared to the commonly\nused spin-coating method, reaching 1,275 F cm-1 V-1 s-1 in just 64 autonomous\nexperimental trials. A study of 10 statistically selected samples identifies\ntwo key structural factors for achieving higher volumetric capacitance: larger\ncrystalline lamellar spacing and higher specific surface area, while also\nuncovering a new polymer polymorph in this material.", "AI": {"tldr": "AI\u51b3\u7b56\u754c\u9762\u63d0\u5347\u7535\u5b50\u6750\u6599\u81ea\u4e3b\u5b9e\u9a8c\u6548\u7387\uff0c\u901a\u8fc7\u5b9e\u65f6\u76d1\u63a7\u4e0e\u4ea4\u4e92\u534f\u4f5c\uff0c\u663e\u8457\u63d0\u9ad8\u6750\u6599\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u7535\u5b50\u6750\u6599\u7814\u7a76\u4e2d\u6570\u636e\u7a00\u7f3a\u548cAI\u9002\u5e94\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u52a0\u901f\u6750\u6599\u53d1\u73b0\u3002", "method": "\u5f00\u53d1AI\u51b3\u7b56\u754c\u9762\uff0c\u7ed3\u5408\u5b9e\u65f6\u76d1\u63a7\u3001\u6570\u636e\u5206\u6790\u548c\u4eba\u673a\u534f\u4f5c\uff0c\u5e94\u7528\u4e8e\u6df7\u5408\u79bb\u5b50-\u7535\u5b50\u5bfc\u7535\u805a\u5408\u7269\u7814\u7a76\u3002", "result": "\u572864\u6b21\u5b9e\u9a8c\u4e2d\uff0c\u6750\u6599\u6027\u80fd\u63d0\u5347150%\uff0c\u53d1\u73b0\u5173\u952e\u7ed3\u6784\u56e0\u7d20\u548c\u65b0\u805a\u5408\u7269\u591a\u6676\u578b\u3002", "conclusion": "AI/AE\u5e73\u53f0\u6709\u6548\u63d0\u5347\u6750\u6599\u7814\u7a76\u6548\u7387\uff0c\u4e3a\u7535\u5b50\u6750\u6599\u8bbe\u8ba1\u63d0\u4f9b\u65b0\u601d\u8def\u3002"}}
{"id": "2504.13612", "pdf": "https://arxiv.org/pdf/2504.13612", "abs": "https://arxiv.org/abs/2504.13612", "authors": ["Dejan Stancevic", "Luca Ambrogioni"], "title": "Entropic Time Schedulers for Generative Diffusion Models", "categories": ["cs.LG", "cs.AI"], "comment": "17 pages", "summary": "The practical performance of generative diffusion models depends on the\nappropriate choice of the noise scheduling function, which can also be\nequivalently expressed as a time reparameterization. In this paper, we present\na time scheduler that selects sampling points based on entropy rather than\nuniform time spacing, ensuring that each point contributes an equal amount of\ninformation to the final generation. We prove that this time reparameterization\ndoes not depend on the initial choice of time. Furthermore, we provide a\ntractable exact formula to estimate this \\emph{entropic time} for a trained\nmodel using the training loss without substantial overhead. Alongside the\nentropic time, inspired by the optimality results, we introduce a rescaled\nentropic time. In our experiments with mixtures of Gaussian distributions and\nImageNet, we show that using the (rescaled) entropic times greatly improves the\ninference performance of trained models. In particular, we found that the image\nquality in pretrained EDM2 models, as evaluated by FID and FD-DINO scores, can\nbe substantially increased by the rescaled entropic time reparameterization\nwithout increasing the number of function evaluations, with greater\nimprovements in the few NFEs regime.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u71b5\u7684\u65f6\u95f4\u8c03\u5ea6\u5668\uff0c\u7528\u4e8e\u751f\u6210\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u975e\u5747\u5300\u65f6\u95f4\u91c7\u6837\u63d0\u5347\u751f\u6210\u6027\u80fd\u3002", "motivation": "\u751f\u6210\u6269\u6563\u6a21\u578b\u7684\u6027\u80fd\u4f9d\u8d56\u4e8e\u566a\u58f0\u8c03\u5ea6\u51fd\u6570\u7684\u9009\u62e9\uff0c\u4f20\u7edf\u5747\u5300\u65f6\u95f4\u91c7\u6837\u53ef\u80fd\u6548\u7387\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u71b5\u7684\u65f6\u95f4\u91cd\u53c2\u6570\u5316\u65b9\u6cd5\uff0c\u786e\u4fdd\u6bcf\u4e2a\u91c7\u6837\u70b9\u8d21\u732e\u76f8\u7b49\u4fe1\u606f\u91cf\uff0c\u5e76\u63d0\u4f9b\u7cbe\u786e\u8ba1\u7b97\u516c\u5f0f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u56fe\u50cf\u8d28\u91cf\uff08FID\u548cFD-DINO\u8bc4\u5206\uff09\uff0c\u5c24\u5176\u5728\u4f4eNFEs\u60c5\u51b5\u4e0b\u3002", "conclusion": "\u57fa\u4e8e\u71b5\u7684\u65f6\u95f4\u91cd\u53c2\u6570\u5316\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u65e0\u9700\u989d\u5916\u8ba1\u7b97\u5f00\u9500\u7684\u6539\u8fdb\u65b9\u6cd5\u3002"}}
{"id": "2504.13551", "pdf": "https://arxiv.org/pdf/2504.13551", "abs": "https://arxiv.org/abs/2504.13551", "authors": ["CheolWon Na", "YunSeok Choi", "Jee-Hyong Lee"], "title": "Q-FAKER: Query-free Hard Black-box Attack via Controlled Generation", "categories": ["cs.CR", "cs.AI", "cs.CL"], "comment": "NAACL 2025 Findings", "summary": "Many adversarial attack approaches are proposed to verify the vulnerability\nof language models. However, they require numerous queries and the information\non the target model. Even black-box attack methods also require the target\nmodel's output information. They are not applicable in real-world scenarios, as\nin hard black-box settings where the target model is closed and inaccessible.\nEven the recently proposed hard black-box attacks still require many queries\nand demand extremely high costs for training adversarial generators. To address\nthese challenges, we propose Q-faker (Query-free Hard Black-box Attacker), a\nnovel and efficient method that generates adversarial examples without\naccessing the target model. To avoid accessing the target model, we use a\nsurrogate model instead. The surrogate model generates adversarial sentences\nfor a target-agnostic attack. During this process, we leverage controlled\ngeneration techniques. We evaluate our proposed method on eight datasets.\nExperimental results demonstrate our method's effectiveness including high\ntransferability and the high quality of the generated adversarial examples, and\nprove its practical in hard black-box settings.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u67e5\u8be2\u76ee\u6807\u6a21\u578b\u7684\u9ed1\u76d2\u653b\u51fb\u65b9\u6cd5Q-faker\uff0c\u901a\u8fc7\u66ff\u4ee3\u6a21\u578b\u751f\u6210\u5bf9\u6297\u6837\u672c\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u76ee\u6807\u6a21\u578b\u4fe1\u606f\u548c\u9ad8\u6210\u672c\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u5bf9\u6297\u653b\u51fb\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u67e5\u8be2\u548c\u76ee\u6807\u6a21\u578b\u4fe1\u606f\uff0c\u4e0d\u9002\u7528\u4e8e\u76ee\u6807\u6a21\u578b\u5c01\u95ed\u4e14\u4e0d\u53ef\u8bbf\u95ee\u7684\u786c\u9ed1\u76d2\u573a\u666f\u3002", "method": "\u4f7f\u7528\u66ff\u4ee3\u6a21\u578b\u751f\u6210\u5bf9\u6297\u53e5\u5b50\uff0c\u7ed3\u5408\u53d7\u63a7\u751f\u6210\u6280\u672f\uff0c\u5b9e\u73b0\u76ee\u6807\u65e0\u5173\u653b\u51fb\u3002", "result": "\u5728\u516b\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5305\u62ec\u9ad8\u8fc1\u79fb\u6027\u548c\u9ad8\u8d28\u91cf\u7684\u5bf9\u6297\u6837\u672c\u751f\u6210\u3002", "conclusion": "Q-faker\u5728\u786c\u9ed1\u76d2\u573a\u666f\u4e2d\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2504.13351", "pdf": "https://arxiv.org/pdf/2504.13351", "abs": "https://arxiv.org/abs/2504.13351", "authors": ["Chen Wang", "Fei Xia", "Wenhao Yu", "Tingnan Zhang", "Ruohan Zhang", "C. Karen Liu", "Li Fei-Fei", "Jie Tan", "Jacky Liang"], "title": "Chain-of-Modality: Learning Manipulation Programs from Multimodal Human Videos with Vision-Language-Models", "categories": ["cs.RO", "cs.AI", "cs.HC", "cs.LG", "cs.MM"], "comment": "ICRA 2025", "summary": "Learning to perform manipulation tasks from human videos is a promising\napproach for teaching robots. However, many manipulation tasks require changing\ncontrol parameters during task execution, such as force, which visual data\nalone cannot capture. In this work, we leverage sensing devices such as\narmbands that measure human muscle activities and microphones that record\nsound, to capture the details in the human manipulation process, and enable\nrobots to extract task plans and control parameters to perform the same task.\nTo achieve this, we introduce Chain-of-Modality (CoM), a prompting strategy\nthat enables Vision Language Models to reason about multimodal human\ndemonstration data -- videos coupled with muscle or audio signals. By\nprogressively integrating information from each modality, CoM refines a task\nplan and generates detailed control parameters, enabling robots to perform\nmanipulation tasks based on a single multimodal human video prompt. Our\nexperiments show that CoM delivers a threefold improvement in accuracy for\nextracting task plans and control parameters compared to baselines, with strong\ngeneralization to new task setups and objects in real-world robot experiments.\nVideos and code are available at https://chain-of-modality.github.io", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aChain-of-Modality (CoM)\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u89c6\u89c9\u3001\u808c\u8089\u6d3b\u52a8\u548c\u97f3\u9891\u4fe1\u53f7\uff0c\u4f7f\u673a\u5668\u4eba\u80fd\u591f\u4ece\u4eba\u7c7b\u89c6\u9891\u4e2d\u5b66\u4e60\u4efb\u52a1\u8ba1\u5212\u548c\u63a7\u5236\u53c2\u6570\u3002", "motivation": "\u4eba\u7c7b\u89c6\u9891\u65e0\u6cd5\u5b8c\u5168\u6355\u6349\u4efb\u52a1\u6267\u884c\u4e2d\u7684\u63a7\u5236\u53c2\u6570\u53d8\u5316\uff08\u5982\u529b\uff09\uff0c\u56e0\u6b64\u9700\u8981\u591a\u6a21\u6001\u6570\u636e\u6765\u589e\u5f3a\u5b66\u4e60\u6548\u679c\u3002", "method": "\u5229\u7528\u81c2\u5e26\u548c\u9ea6\u514b\u98ce\u7b49\u8bbe\u5907\u6355\u6349\u4eba\u7c7b\u64cd\u4f5c\u8fc7\u7a0b\u4e2d\u7684\u808c\u8089\u6d3b\u52a8\u548c\u58f0\u97f3\u4fe1\u53f7\uff0c\u7ed3\u5408\u89c6\u89c9\u6570\u636e\uff0c\u901a\u8fc7CoM\u7b56\u7565\u9010\u6b65\u6574\u5408\u591a\u6a21\u6001\u4fe1\u606f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cCoM\u5728\u63d0\u53d6\u4efb\u52a1\u8ba1\u5212\u548c\u63a7\u5236\u53c2\u6570\u65b9\u9762\u7684\u51c6\u786e\u6027\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u63d0\u9ad8\u4e86\u4e09\u500d\uff0c\u5e76\u5728\u771f\u5b9e\u673a\u5668\u4eba\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "CoM\u901a\u8fc7\u591a\u6a21\u6001\u6570\u636e\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u4ece\u4eba\u7c7b\u89c6\u9891\u4e2d\u5b66\u4e60\u4efb\u52a1\u7684\u80fd\u529b\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2504.13633", "pdf": "https://arxiv.org/pdf/2504.13633", "abs": "https://arxiv.org/abs/2504.13633", "authors": ["Samuel Wertz", "Arnaud Vandaele", "Nicolas Gillis"], "title": "Efficient algorithms for the Hadamard decomposition", "categories": ["cs.LG", "eess.SP", "math.OC", "stat.ML"], "comment": "7 pages, code available from\n  https://github.com/WertzSamuel/HadamardDecompositions", "summary": "The Hadamard decomposition is a powerful technique for data analysis and\nmatrix compression, which decomposes a given matrix into the element-wise\nproduct of two or more low-rank matrices. In this paper, we develop an\nefficient algorithm to solve this problem, leveraging an alternating\noptimization approach that decomposes the global non-convex problem into a\nseries of convex sub-problems. To improve performance, we explore advanced\ninitialization strategies inspired by the singular value decomposition (SVD)\nand incorporate acceleration techniques by introducing momentum-based updates.\nBeyond optimizing the two-matrix case, we also extend the Hadamard\ndecomposition framework to support more than two low-rank matrices, enabling\napproximations with higher effective ranks while preserving computational\nefficiency. Finally, we conduct extensive experiments to compare our method\nwith the existing gradient descent-based approaches for the Hadamard\ndecomposition and with traditional low-rank approximation techniques. The\nresults highlight the effectiveness of our proposed method across diverse\ndatasets.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684Hadamard\u5206\u89e3\u7b97\u6cd5\uff0c\u901a\u8fc7\u4ea4\u66ff\u4f18\u5316\u548c\u52a8\u91cf\u66f4\u65b0\u6280\u672f\uff0c\u89e3\u51b3\u4e86\u975e\u51f8\u95ee\u9898\uff0c\u5e76\u6269\u5c55\u4e86\u591a\u77e9\u9635\u5206\u89e3\u6846\u67b6\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "Hadamard\u5206\u89e3\u5728\u6570\u636e\u5206\u6790\u548c\u77e9\u9635\u538b\u7f29\u4e2d\u5177\u6709\u91cd\u8981\u4f5c\u7528\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u6548\u7387\u4e0d\u8db3\uff0c\u9700\u6539\u8fdb\u7b97\u6cd5\u4ee5\u63d0\u5347\u6027\u80fd\u3002", "method": "\u91c7\u7528\u4ea4\u66ff\u4f18\u5316\u5c06\u5168\u5c40\u975e\u51f8\u95ee\u9898\u5206\u89e3\u4e3a\u51f8\u5b50\u95ee\u9898\uff0c\u7ed3\u5408SVD\u521d\u59cb\u5316\u7b56\u7565\u548c\u52a8\u91cf\u66f4\u65b0\u6280\u672f\uff0c\u5e76\u6269\u5c55\u81f3\u591a\u77e9\u9635\u5206\u89e3\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u6837\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u68af\u5ea6\u4e0b\u964d\u548c\u4f20\u7edf\u4f4e\u79e9\u8fd1\u4f3c\u6280\u672f\u3002", "conclusion": "\u63d0\u51fa\u7684\u7b97\u6cd5\u9ad8\u6548\u4e14\u7075\u6d3b\uff0c\u9002\u7528\u4e8e\u66f4\u590d\u6742\u7684Hadamard\u5206\u89e3\u4efb\u52a1\u3002"}}
{"id": "2504.13365", "pdf": "https://arxiv.org/pdf/2504.13365", "abs": "https://arxiv.org/abs/2504.13365", "authors": ["Long Li", "Jiajia Li", "Dong Chen", "Lina Pu", "Haibo Yao", "Yanbo Huang"], "title": "VLLFL: A Vision-Language Model Based Lightweight Federated Learning Framework for Smart Agriculture", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "In modern smart agriculture, object detection plays a crucial role by\nenabling automation, precision farming, and monitoring of resources. From\nidentifying crop health and pest infestations to optimizing harvesting\nprocesses, accurate object detection enhances both productivity and\nsustainability. However, training object detection models often requires\nlarge-scale data collection and raises privacy concerns, particularly when\nsensitive agricultural data is distributed across farms. To address these\nchallenges, we propose VLLFL, a vision-language model-based lightweight\nfederated learning framework (VLLFL). It harnesses the generalization and\ncontext-aware detection capabilities of the vision-language model (VLM) and\nleverages the privacy-preserving nature of federated learning. By training a\ncompact prompt generator to boost the performance of the VLM deployed across\ndifferent farms, VLLFL preserves privacy while reducing communication overhead.\nExperimental results demonstrate that VLLFL achieves 14.53% improvement in the\nperformance of VLM while reducing 99.3% communication overhead. Spanning tasks\nfrom identifying a wide variety of fruits to detecting harmful animals in\nagriculture, the proposed framework offers an efficient, scalable, and\nprivacy-preserving solution specifically tailored to agricultural applications.", "AI": {"tldr": "VLLFL\u662f\u4e00\u4e2a\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u8f7b\u91cf\u7ea7\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u519c\u4e1a\u4e2d\u76ee\u6807\u68c0\u6d4b\u7684\u6570\u636e\u9690\u79c1\u548c\u901a\u4fe1\u5f00\u9500\u95ee\u9898\uff0c\u540c\u65f6\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u4ee3\u519c\u4e1a\u4e2d\u76ee\u6807\u68c0\u6d4b\u5bf9\u81ea\u52a8\u5316\u548c\u7cbe\u51c6\u519c\u4e1a\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5927\u89c4\u6a21\u6570\u636e\u6536\u96c6\u548c\u9690\u79c1\u95ee\u9898\u6210\u4e3a\u6311\u6218\u3002", "method": "\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u8054\u90a6\u5b66\u4e60\u7684\u9690\u79c1\u4fdd\u62a4\u7279\u6027\uff0c\u8bad\u7ec3\u7d27\u51d1\u7684\u63d0\u793a\u751f\u6210\u5668\u4ee5\u63d0\u5347\u6027\u80fd\u3002", "result": "VLLFL\u5728VLM\u6027\u80fd\u4e0a\u63d0\u534714.53%\uff0c\u540c\u65f6\u51cf\u5c1199.3%\u7684\u901a\u4fe1\u5f00\u9500\u3002", "conclusion": "VLLFL\u4e3a\u519c\u4e1a\u5e94\u7528\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u4e14\u4fdd\u62a4\u9690\u79c1\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.13691", "pdf": "https://arxiv.org/pdf/2504.13691", "abs": "https://arxiv.org/abs/2504.13691", "authors": ["Jinhui Pang", "Changqing Lin", "Hao Lin", "Jinglin He", "Zhengjun Li", "Zhihui Zhang", "Xiaoshuai Hao"], "title": "MEGA: Second-Order Gradient Alignment for Catastrophic Forgetting Mitigation in GFSCIL", "categories": ["cs.LG"], "comment": "Under Review", "summary": "Graph Few-Shot Class-Incremental Learning (GFSCIL) enables models to\ncontinually learn from limited samples of novel tasks after initial training on\na large base dataset. Existing GFSCIL approaches typically utilize Prototypical\nNetworks (PNs) for metric-based class representations and fine-tune the model\nduring the incremental learning stage. However, these PN-based methods\noversimplify learning via novel query set fine-tuning and fail to integrate\nGraph Continual Learning (GCL) techniques due to architectural constraints. To\naddress these challenges, we propose a more rigorous and practical setting for\nGFSCIL that excludes query sets during the incremental training phase. Building\non this foundation, we introduce Model-Agnostic Meta Graph Continual Learning\n(MEGA), aimed at effectively alleviating catastrophic forgetting for GFSCIL.\nSpecifically, by calculating the incremental second-order gradient during the\nmeta-training stage, we endow the model to learn high-quality priors that\nenhance incremental learning by aligning its behaviors across both the\nmeta-training and incremental learning stages. Extensive experiments on four\nmainstream graph datasets demonstrate that MEGA achieves state-of-the-art\nresults and enhances the effectiveness of various GCL methods in GFSCIL. We\nbelieve that our proposed MEGA serves as a model-agnostic GFSCIL paradigm,\npaving the way for future research.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u56fe\u5c11\u6837\u672c\u7c7b\u589e\u91cf\u5b66\u4e60\uff08GFSCIL\uff09\u65b9\u6cd5MEGA\uff0c\u901a\u8fc7\u5143\u8bad\u7ec3\u9636\u6bb5\u7684\u4e8c\u9636\u68af\u5ea6\u8ba1\u7b97\uff0c\u6709\u6548\u7f13\u89e3\u707e\u96be\u6027\u9057\u5fd8\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u539f\u578b\u7f51\u7edc\uff08PNs\uff09\u7684GFSCIL\u65b9\u6cd5\u5728\u589e\u91cf\u5b66\u4e60\u9636\u6bb5\u8fc7\u5ea6\u7b80\u5316\uff0c\u4e14\u672a\u80fd\u6574\u5408\u56fe\u6301\u7eed\u5b66\u4e60\uff08GCL\uff09\u6280\u672f\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u4e25\u683c\u7684\u8bbe\u7f6e\u548c\u6539\u8fdb\u65b9\u6cd5\u3002", "method": "\u63d0\u51faMEGA\u65b9\u6cd5\uff0c\u901a\u8fc7\u5143\u8bad\u7ec3\u9636\u6bb5\u7684\u4e8c\u9636\u68af\u5ea6\u8ba1\u7b97\uff0c\u5b66\u4e60\u9ad8\u8d28\u91cf\u7684\u5148\u9a8c\u77e5\u8bc6\uff0c\u5e76\u5728\u589e\u91cf\u5b66\u4e60\u9636\u6bb5\u5bf9\u9f50\u884c\u4e3a\u3002", "result": "\u5728\u56db\u4e2a\u4e3b\u6d41\u56fe\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMEGA\u53d6\u5f97\u4e86\u6700\u4f18\u6027\u80fd\uff0c\u5e76\u63d0\u5347\u4e86\u591a\u79cdGCL\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "MEGA\u4f5c\u4e3a\u4e00\u79cd\u6a21\u578b\u65e0\u5173\u7684GFSCIL\u8303\u5f0f\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2504.13667", "pdf": "https://arxiv.org/pdf/2504.13667", "abs": "https://arxiv.org/abs/2504.13667", "authors": ["Russell Beale"], "title": "Large Language Models Will Change The Way Children Think About Technology And Impact Every Interaction Paradigm", "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": "Accepted for IDC 2025. Citation: Russell Beale. 2025. Large Language\n  Models Will Change The Way Children Think About Technology And Impact Every\n  Interaction Paradigm. In Proceedings of Interaction Design and Children\n  Conference (IDC2025). ACM, New York, NY, USA", "summary": "This paper presents a hopeful perspective on the potentially dramatic impacts\nof Large Language Models on how we children learn and how they will expect to\ninteract with technology. We review the effects of LLMs on education so far,\nand make the case that these effects are minor compared to the upcoming changes\nthat are occurring. We present a small scenario and self-ethnographic study\ndemonstrating the effects of these changes, and define five significant\nconsiderations that interactive systems designers will have to accommodate in\nthe future.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5bf9\u513f\u7ae5\u5b66\u4e60\u65b9\u5f0f\u7684\u6f5c\u5728\u91cd\u5927\u5f71\u54cd\uff0c\u5e76\u6307\u51fa\u5f53\u524d\u5f71\u54cd\u8f83\u5c0f\uff0c\u4f46\u672a\u6765\u53d8\u5316\u5c06\u66f4\u4e3a\u663e\u8457\u3002\u901a\u8fc7\u60c5\u666f\u548c\u81ea\u6211\u6c11\u65cf\u5fd7\u7814\u7a76\uff0c\u63d0\u51fa\u4e86\u672a\u6765\u4ea4\u4e92\u7cfb\u7edf\u8bbe\u8ba1\u9700\u8003\u8651\u7684\u4e94\u4e2a\u5173\u952e\u70b9\u3002", "motivation": "\u7814\u7a76LLMs\u5bf9\u6559\u80b2\u7684\u5f71\u54cd\uff0c\u5c24\u5176\u662f\u513f\u7ae5\u5b66\u4e60\u65b9\u5f0f\u7684\u672a\u6765\u53d8\u9769\uff0c\u4e3a\u4ea4\u4e92\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u524d\u77bb\u6027\u6307\u5bfc\u3002", "method": "\u7ed3\u5408\u6587\u732e\u7efc\u8ff0\u3001\u60c5\u666f\u5206\u6790\u548c\u81ea\u6211\u6c11\u65cf\u5fd7\u7814\u7a76\uff0c\u63a2\u8ba8LLMs\u7684\u5f71\u54cd\u3002", "result": "\u53d1\u73b0\u5f53\u524dLLMs\u5bf9\u6559\u80b2\u7684\u5f71\u54cd\u8f83\u5c0f\uff0c\u4f46\u672a\u6765\u53d8\u5316\u5c06\u66f4\u663e\u8457\uff0c\u5e76\u63d0\u51fa\u4e86\u4e94\u4e2a\u5173\u952e\u8bbe\u8ba1\u8003\u8651\u3002", "conclusion": "LLMs\u5c06\u6df1\u523b\u6539\u53d8\u513f\u7ae5\u5b66\u4e60\u65b9\u5f0f\uff0c\u4ea4\u4e92\u7cfb\u7edf\u8bbe\u8ba1\u9700\u9002\u5e94\u8fd9\u4e9b\u672a\u6765\u53d8\u5316\u3002"}}
{"id": "2504.13733", "pdf": "https://arxiv.org/pdf/2504.13733", "abs": "https://arxiv.org/abs/2504.13733", "authors": ["Yichen Liu"], "title": "Dynamic Regularized CBDT: Variance-Calibrated Causal Boosting for Interpretable Heterogeneous Treatment Effects", "categories": ["cs.LG", "stat.ML", "68T05, 62H12, 90C30", "I.2.6; G.3; I.5.1"], "comment": "Preprint version. 13 pages, 4 figures, 3 tables", "summary": "Heterogeneous treatment effect estimation in high-stakes applications demands\nmodels that simultaneously optimize precision, interpretability, and\ncalibration. Many existing tree-based causal inference techniques, however,\nexhibit high estimation errors when applied to observational data because they\nstruggle to capture complex interactions among factors and rely on static\nregularization schemes. In this work, we propose Dynamic Regularized Causal\nBoosted Decision Trees (CBDT), a novel framework that integrates variance\nregularization and average treatment effect calibration into the loss function\nof gradient boosted decision trees. Our approach dynamically updates the\nregularization parameters using gradient statistics to better balance the\nbias-variance tradeoff. Extensive experiments on standard benchmark datasets\nand real-world clinical data demonstrate that the proposed method significantly\nimproves estimation accuracy while maintaining reliable coverage of true\ntreatment effects. In an intensive care unit patient triage study, the method\nsuccessfully identified clinically actionable rules and achieved high accuracy\nin treatment effect estimation. The results validate that dynamic\nregularization can effectively tighten error bounds and enhance both predictive\nperformance and model interpretability.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u6b63\u5219\u5316\u7684\u56e0\u679c\u63d0\u5347\u51b3\u7b56\u6811\uff08CBDT\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u66f4\u65b0\u6b63\u5219\u5316\u53c2\u6570\u6765\u4f18\u5316\u9ad8\u7ef4\u6570\u636e\u4e2d\u7684\u5f02\u8d28\u6027\u6cbb\u7597\u6548\u679c\u4f30\u8ba1\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u4f30\u8ba1\u7cbe\u5ea6\u548c\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u5728\u9ad8\u98ce\u9669\u5e94\u7528\u4e2d\uff0c\u73b0\u6709\u6811\u72b6\u56e0\u679c\u63a8\u65ad\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u590d\u6742\u4ea4\u4e92\u4e14\u4f9d\u8d56\u9759\u6001\u6b63\u5219\u5316\uff0c\u5bfc\u81f4\u4f30\u8ba1\u8bef\u5dee\u8f83\u9ad8\u3002", "method": "\u63d0\u51faCBDT\u6846\u67b6\uff0c\u5c06\u65b9\u5dee\u6b63\u5219\u5316\u548c\u5e73\u5747\u6cbb\u7597\u6548\u679c\u6821\u51c6\u96c6\u6210\u5230\u68af\u5ea6\u63d0\u5347\u51b3\u7b56\u6811\u7684\u635f\u5931\u51fd\u6570\u4e2d\uff0c\u5e76\u52a8\u6001\u66f4\u65b0\u6b63\u5219\u5316\u53c2\u6570\u3002", "result": "\u5728\u6807\u51c6\u57fa\u51c6\u6570\u636e\u96c6\u548c\u771f\u5b9e\u4e34\u5e8a\u6570\u636e\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u4f30\u8ba1\u7cbe\u5ea6\uff0c\u5e76\u5728ICU\u60a3\u8005\u5206\u8bca\u7814\u7a76\u4e2d\u8bc6\u522b\u51fa\u53ef\u64cd\u4f5c\u7684\u4e34\u5e8a\u89c4\u5219\u3002", "conclusion": "\u52a8\u6001\u6b63\u5219\u5316\u80fd\u6709\u6548\u7f29\u5c0f\u8bef\u5dee\u754c\u9650\uff0c\u63d0\u5347\u9884\u6d4b\u6027\u80fd\u548c\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2504.13371", "pdf": "https://arxiv.org/pdf/2504.13371", "abs": "https://arxiv.org/abs/2504.13371", "authors": ["Andrew J. Lohn"], "title": "The Impact of AI on the Cyber Offense-Defense Balance and the Character of Cyber Conflict", "categories": ["cs.CR", "cs.AI", "cs.CY"], "comment": null, "summary": "Unlike other domains of conflict, and unlike other fields with high\nanticipated risk from AI, the cyber domain is intrinsically digital with a\ntight feedback loop between AI training and cyber application. Cyber may have\nsome of the largest and earliest impacts from AI, so it is important to\nunderstand how the cyber domain may change as AI continues to advance. Our\napproach reviewed the literature, collecting nine arguments that have been\nproposed for offensive advantage in cyber conflict and nine proposed arguments\nfor defensive advantage. We include an additional forty-eight arguments that\nhave been proposed to give cyber conflict and competition its character as\ncollected separately by Healey, Jervis, and Nandrajog. We then consider how\neach of those arguments and propositions might change with varying degrees of\nAI advancement. We find that the cyber domain is too multifaceted for a single\nanswer to whether AI will enhance offense or defense broadly. AI will improve\nsome aspects, hinder others, and leave some aspects unchanged. We collect and\npresent forty-four ways that we expect AI to impact the cyber offense-defense\nbalance and the character of cyber conflict and competition.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86AI\u5bf9\u7f51\u7edc\u51b2\u7a81\u7684\u5f71\u54cd\uff0c\u5206\u6790\u4e86\u8fdb\u653b\u548c\u9632\u5fa1\u4f18\u52bf\u7684\u591a\u79cd\u8bba\u70b9\uff0c\u5e76\u603b\u7ed3\u4e8644\u79cdAI\u53ef\u80fd\u6539\u53d8\u7f51\u7edc\u51b2\u7a81\u5e73\u8861\u7684\u65b9\u5f0f\u3002", "motivation": "\u7f51\u7edc\u9886\u57df\u56e0\u5176\u6570\u5b57\u5316\u7279\u6027\u4e0eAI\u8bad\u7ec3\u548c\u5e94\u7528\u7684\u7d27\u5bc6\u53cd\u9988\u5faa\u73af\uff0c\u53ef\u80fd\u6210\u4e3aAI\u5f71\u54cd\u6700\u5927\u4e14\u6700\u65e9\u7684\u9886\u57df\u4e4b\u4e00\uff0c\u56e0\u6b64\u9700\u8981\u7406\u89e3AI\u5982\u4f55\u6539\u53d8\u7f51\u7edc\u51b2\u7a81\u3002", "method": "\u901a\u8fc7\u6587\u732e\u7efc\u8ff0\uff0c\u6536\u96c6\u4e86\u5173\u4e8e\u7f51\u7edc\u51b2\u7a81\u4e2d\u8fdb\u653b\u548c\u9632\u5fa1\u4f18\u52bf\u7684\u8bba\u70b9\uff0c\u5e76\u5206\u6790\u4e86AI\u5bf9\u8fd9\u4e9b\u8bba\u70b9\u7684\u5f71\u54cd\u3002", "result": "\u7f51\u7edc\u9886\u57df\u8fc7\u4e8e\u590d\u6742\uff0c\u65e0\u6cd5\u7b80\u5355\u5224\u65adAI\u662f\u5426\u6574\u4f53\u589e\u5f3a\u8fdb\u653b\u6216\u9632\u5fa1\uff0cAI\u4f1a\u6539\u5584\u67d0\u4e9b\u65b9\u9762\u3001\u963b\u788d\u5176\u4ed6\u65b9\u9762\uff0c\u540c\u65f6\u4fdd\u6301\u67d0\u4e9b\u4e0d\u53d8\u3002", "conclusion": "AI\u5bf9\u7f51\u7edc\u51b2\u7a81\u7684\u5f71\u54cd\u662f\u591a\u65b9\u9762\u7684\uff0c\u8bba\u6587\u603b\u7ed3\u4e8644\u79cd\u5177\u4f53\u5f71\u54cd\u65b9\u5f0f\u3002"}}
{"id": "2504.13752", "pdf": "https://arxiv.org/pdf/2504.13752", "abs": "https://arxiv.org/abs/2504.13752", "authors": ["Benjamin Cohen-Wang", "Yung-Sung Chuang", "Aleksander Madry"], "title": "Learning to Attribute with Attention", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Given a sequence of tokens generated by a language model, we may want to\nidentify the preceding tokens that influence the model to generate this\nsequence. Performing such token attribution is expensive; a common approach is\nto ablate preceding tokens and directly measure their effects. To reduce the\ncost of token attribution, we revisit attention weights as a heuristic for how\na language model uses previous tokens. Naive approaches to attribute model\nbehavior with attention (e.g., averaging attention weights across attention\nheads to estimate a token's influence) have been found to be unreliable. To\nattain faithful attributions, we propose treating the attention weights of\ndifferent attention heads as features. This way, we can learn how to\neffectively leverage attention weights for attribution (using signal from\nablations). Our resulting method, Attribution with Attention (AT2), reliably\nperforms on par with approaches that involve many ablations, while being\nsignificantly more efficient. To showcase the utility of AT2, we use it to\nprune less important parts of a provided context in a question answering\nsetting, improving answer quality. We provide code for AT2 at\nhttps://github.com/MadryLab/AT2 .", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6ce8\u610f\u529b\u6743\u91cd\u7684\u4ee4\u724c\u5f52\u56e0\u65b9\u6cd5AT2\uff0c\u901a\u8fc7\u5c06\u4e0d\u540c\u6ce8\u610f\u529b\u5934\u7684\u6743\u91cd\u4f5c\u4e3a\u7279\u5f81\u5b66\u4e60\uff0c\u9ad8\u6548\u4e14\u53ef\u9760\u5730\u66ff\u4ee3\u4e86\u4f20\u7edf\u7684\u9ad8\u6210\u672c\u6d88\u878d\u65b9\u6cd5\u3002", "motivation": "\u8bed\u8a00\u6a21\u578b\u751f\u6210\u5e8f\u5217\u65f6\uff0c\u8bc6\u522b\u5f71\u54cd\u751f\u6210\u7684\u524d\u9a71\u4ee4\u724c\u6210\u672c\u9ad8\u6602\uff0c\u4f20\u7edf\u6d88\u878d\u65b9\u6cd5\u6548\u7387\u4f4e\u3002\u6ce8\u610f\u529b\u6743\u91cd\u867d\u88ab\u7528\u4f5c\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u4f46\u76f4\u63a5\u4f7f\u7528\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u5c06\u4e0d\u540c\u6ce8\u610f\u529b\u5934\u7684\u6743\u91cd\u4f5c\u4e3a\u7279\u5f81\uff0c\u901a\u8fc7\u5b66\u4e60\uff08\u5229\u7528\u6d88\u878d\u4fe1\u53f7\uff09\u6709\u6548\u5229\u7528\u6ce8\u610f\u529b\u6743\u91cd\u8fdb\u884c\u5f52\u56e0\uff0c\u63d0\u51faAT2\u65b9\u6cd5\u3002", "result": "AT2\u5728\u6027\u80fd\u4e0a\u4e0e\u9ad8\u6210\u672c\u6d88\u878d\u65b9\u6cd5\u76f8\u5f53\uff0c\u4f46\u6548\u7387\u663e\u8457\u63d0\u5347\uff0c\u5e76\u80fd\u7528\u4e8e\u4e0a\u4e0b\u6587\u4fee\u526a\u4ee5\u63d0\u5347\u95ee\u7b54\u8d28\u91cf\u3002", "conclusion": "AT2\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u53ef\u9760\u7684\u4ee4\u724c\u5f52\u56e0\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u5e94\u7528\uff0c\u5982\u4e0a\u4e0b\u6587\u4f18\u5316\u3002"}}
{"id": "2504.13375", "pdf": "https://arxiv.org/pdf/2504.13375", "abs": "https://arxiv.org/abs/2504.13375", "authors": ["Nikhil Kumar"], "title": "Pricing AI Model Accuracy", "categories": ["econ.TH", "cs.AI"], "comment": null, "summary": "This paper examines the market for AI models in which firms compete to\nprovide accurate model predictions and consumers exhibit heterogeneous\npreferences for model accuracy. We develop a consumer-firm duopoly model to\nanalyze how competition affects firms' incentives to improve model accuracy.\nEach firm aims to minimize its model's error, but this choice can often be\nsuboptimal. Counterintuitively, we find that in a competitive market, firms\nthat improve overall accuracy do not necessarily improve their profits. Rather,\neach firm's optimal decision is to invest further on the error dimension where\nit has a competitive advantage. By decomposing model errors into false positive\nand false negative rates, firms can reduce errors in each dimension through\ninvestments. Firms are strictly better off investing on their superior\ndimension and strictly worse off with investments on their inferior dimension.\nProfitable investments adversely affect consumers but increase overall welfare.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86AI\u6a21\u578b\u5e02\u573a\u4e2d\u4f01\u4e1a\u7ade\u4e89\u5bf9\u6a21\u578b\u51c6\u786e\u6027\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u4f01\u4e1a\u5728\u7ade\u4e89\u4e2d\u7684\u6700\u4f18\u7b56\u7565\u662f\u6295\u8d44\u4e8e\u81ea\u8eab\u4f18\u52bf\u7ef4\u5ea6\u800c\u975e\u6574\u4f53\u51c6\u786e\u6027\u3002", "motivation": "\u63a2\u8ba8\u7ade\u4e89\u73af\u5883\u4e0b\u4f01\u4e1a\u5982\u4f55\u9009\u62e9\u6295\u8d44\u4ee5\u63d0\u9ad8\u6a21\u578b\u51c6\u786e\u6027\uff0c\u4ee5\u53ca\u8fd9\u79cd\u9009\u62e9\u5bf9\u6d88\u8d39\u8005\u548c\u6574\u4f53\u798f\u5229\u7684\u5f71\u54cd\u3002", "method": "\u5efa\u7acb\u6d88\u8d39\u8005-\u4f01\u4e1a\u53cc\u5be1\u5934\u6a21\u578b\uff0c\u5206\u6790\u4f01\u4e1a\u5982\u4f55\u901a\u8fc7\u6295\u8d44\u51cf\u5c11\u6a21\u578b\u9519\u8bef\uff08\u5982\u5047\u9633\u6027\u548c\u5047\u9634\u6027\u7387\uff09\u3002", "result": "\u7ade\u4e89\u5e02\u573a\u4e2d\uff0c\u4f01\u4e1a\u6295\u8d44\u4e8e\u4f18\u52bf\u7ef4\u5ea6\u80fd\u63d0\u9ad8\u5229\u6da6\uff0c\u4f46\u6574\u4f53\u51c6\u786e\u6027\u4e0d\u4e00\u5b9a\u6539\u5584\uff1b\u8fd9\u79cd\u7b56\u7565\u5bf9\u6d88\u8d39\u8005\u4e0d\u5229\u4f46\u589e\u52a0\u603b\u4f53\u798f\u5229\u3002", "conclusion": "\u4f01\u4e1a\u5728\u7ade\u4e89\u4e2d\u7684\u6700\u4f18\u7b56\u7565\u662f\u4e13\u6ce8\u4e8e\u4f18\u52bf\u7ef4\u5ea6\uff0c\u800c\u975e\u5168\u9762\u6539\u8fdb\u6a21\u578b\u51c6\u786e\u6027\uff0c\u8fd9\u5bf9\u5e02\u573a\u52a8\u6001\u548c\u798f\u5229\u6709\u91cd\u8981\u5f71\u54cd\u3002"}}
{"id": "2504.13755", "pdf": "https://arxiv.org/pdf/2504.13755", "abs": "https://arxiv.org/abs/2504.13755", "authors": ["Amin Noroozi", "Sidratul Muntaha Esha", "Mansoureh Ghari"], "title": "Predictors of Childhood Vaccination Uptake in England: An Explainable Machine Learning Analysis of Longitudinal Regional Data (2021-2024)", "categories": ["cs.LG"], "comment": null, "summary": "Childhood vaccination is a cornerstone of public health, yet disparities in\nvaccination coverage persist across England. These disparities are shaped by\ncomplex interactions among various factors, including geographic, demographic,\nsocioeconomic, and cultural (GDSC) factors. Previous studies mostly rely on\ncross-sectional data and traditional statistical approaches that assess\nindividual or limited sets of variables in isolation. Such methods may fall\nshort in capturing the dynamic and multivariate nature of vaccine uptake. In\nthis paper, we conducted a longitudinal machine learning analysis of childhood\nvaccination coverage across 150 districts in England from 2021 to 2024. Using\nvaccination data from NHS records, we applied hierarchical clustering to group\ndistricts by vaccination coverage into low- and high-coverage clusters. A\nCatBoost classifier was then trained to predict districts' vaccination clusters\nusing their GDSC data. Finally, the SHapley Additive exPlanations (SHAP) method\nwas used to interpret the predictors' importance. The classifier achieved high\naccuracies of 92.1, 90.6, and 86.3 in predicting districts' vaccination\nclusters for the years 2021-2022, 2022-2023, and 2023-2024, respectively. SHAP\nrevealed that geographic, cultural, and demographic variables, particularly\nrurality, English language proficiency, the percentage of foreign-born\nresidents, and ethnic composition, were the most influential predictors of\nvaccination coverage, whereas socioeconomic variables, such as deprivation and\nemployment, consistently showed lower importance, especially in 2023-2024.\nSurprisingly, rural districts were significantly more likely to have higher\nvaccination rates. Additionally, districts with lower vaccination coverage had\nhigher populations whose first language was not English, who were born outside\nthe UK, or who were from ethnic minority groups.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5206\u6790\u4e86\u82f1\u683c\u5170150\u4e2a\u5730\u533a2021-2024\u5e74\u513f\u7ae5\u75ab\u82d7\u63a5\u79cd\u7387\u7684\u5dee\u5f02\uff0c\u53d1\u73b0\u5730\u7406\u3001\u6587\u5316\u548c\u4eba\u53e3\u56e0\u7d20\u662f\u4e3b\u8981\u5f71\u54cd\u56e0\u7d20\uff0c\u800c\u793e\u4f1a\u7ecf\u6d4e\u56e0\u7d20\u5f71\u54cd\u8f83\u5c0f\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u5728\u6355\u6349\u75ab\u82d7\u63a5\u79cd\u7387\u52a8\u6001\u548c\u591a\u53d8\u91cf\u7279\u6027\u4e0a\u7684\u4e0d\u8db3\uff0c\u63ed\u793a\u5f71\u54cd\u75ab\u82d7\u63a5\u79cd\u7387\u7684\u590d\u6742\u56e0\u7d20\u3002", "method": "\u91c7\u7528\u7eb5\u5411\u673a\u5668\u5b66\u4e60\u5206\u6790\uff0c\u5305\u62ec\u5c42\u6b21\u805a\u7c7b\u3001CatBoost\u5206\u7c7b\u5668\u548cSHAP\u89e3\u91ca\u65b9\u6cd5\u3002", "result": "\u5206\u7c7b\u5668\u9884\u6d4b\u51c6\u786e\u7387\u9ad8\uff0892.1%\u300190.6%\u300186.3%\uff09\uff0c\u5730\u7406\u3001\u6587\u5316\u548c\u4eba\u53e3\u53d8\u91cf\uff08\u5982\u519c\u6751\u5730\u533a\u3001\u82f1\u8bed\u719f\u7ec3\u5ea6\uff09\u5f71\u54cd\u6700\u5927\u3002", "conclusion": "\u519c\u6751\u5730\u533a\u75ab\u82d7\u63a5\u79cd\u7387\u66f4\u9ad8\uff0c\u8bed\u8a00\u548c\u6c11\u65cf\u56e0\u7d20\u4e0e\u4f4e\u63a5\u79cd\u7387\u76f8\u5173\uff0c\u793e\u4f1a\u7ecf\u6d4e\u56e0\u7d20\u5f71\u54cd\u8f83\u5c0f\u3002"}}
{"id": "2504.13756", "pdf": "https://arxiv.org/pdf/2504.13756", "abs": "https://arxiv.org/abs/2504.13756", "authors": ["Dmitrii Kharlapenko", "Stepan Shabalin", "Fazl Barez", "Arthur Conmy", "Neel Nanda"], "title": "Scaling sparse feature circuit finding for in-context learning", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Sparse autoencoders (SAEs) are a popular tool for interpreting large language\nmodel activations, but their utility in addressing open questions in\ninterpretability remains unclear. In this work, we demonstrate their\neffectiveness by using SAEs to deepen our understanding of the mechanism behind\nin-context learning (ICL). We identify abstract SAE features that (i) encode\nthe model's knowledge of which task to execute and (ii) whose latent vectors\ncausally induce the task zero-shot. This aligns with prior work showing that\nICL is mediated by task vectors. We further demonstrate that these task vectors\nare well approximated by a sparse sum of SAE latents, including these\ntask-execution features. To explore the ICL mechanism, we adapt the sparse\nfeature circuits methodology of Marks et al. (2024) to work for the much larger\nGemma-1 2B model, with 30 times as many parameters, and to the more complex\ntask of ICL. Through circuit finding, we discover task-detecting features with\ncorresponding SAE latents that activate earlier in the prompt, that detect when\ntasks have been performed. They are causally linked with task-execution\nfeatures through the attention and MLP sublayers.", "AI": {"tldr": "\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff08SAEs\uff09\u7528\u4e8e\u89e3\u91ca\u5927\u8bed\u8a00\u6a21\u578b\u6fc0\u6d3b\uff0c\u5e76\u901a\u8fc7\u7814\u7a76\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08ICL\uff09\u673a\u5236\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u63a2\u8ba8SAEs\u5728\u89e3\u91ca\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u5b9e\u7528\u6027\uff0c\u5c24\u5176\u662f\u5bf9ICL\u673a\u5236\u7684\u7406\u89e3\u3002", "method": "\u4f7f\u7528SAEs\u8bc6\u522b\u4efb\u52a1\u6267\u884c\u7279\u5f81\uff0c\u5e76\u901a\u8fc7\u7a00\u758f\u7279\u5f81\u7535\u8def\u65b9\u6cd5\u5206\u6790Gemma-1 2B\u6a21\u578b\u4e2d\u7684ICL\u673a\u5236\u3002", "result": "\u53d1\u73b0\u4efb\u52a1\u68c0\u6d4b\u7279\u5f81\u53ca\u5176\u56e0\u679c\u5173\u8054\uff0c\u4efb\u52a1\u5411\u91cf\u53ef\u901a\u8fc7\u7a00\u758fSAE\u6f5c\u5728\u5411\u91cf\u8fd1\u4f3c\u8868\u793a\u3002", "conclusion": "SAEs\u80fd\u6709\u6548\u63ed\u793aICL\u673a\u5236\uff0c\u4efb\u52a1\u5411\u91cf\u4e0e\u7a00\u758f\u6f5c\u5728\u7279\u5f81\u5bc6\u5207\u76f8\u5173\u3002"}}
{"id": "2504.13376", "pdf": "https://arxiv.org/pdf/2504.13376", "abs": "https://arxiv.org/abs/2504.13376", "authors": ["Aitor G\u00f3mez-Tejedor", "Eneko Osaba", "Esther Villar-Rodriguez"], "title": "Addressing the Minor-Embedding Problem in Quantum Annealing and Evaluating State-of-the-Art Algorithm Performance", "categories": ["quant-ph", "cs.AI", "cs.ET"], "comment": "Paper submitted for review in the Future Generation Computer Systems\n  journal", "summary": "This study addresses the minor-embedding problem, which involves mapping the\nvariables of an Ising model onto a quantum annealing processor. The primary\nmotivation stems from the observed performance disparity of quantum annealers\nwhen solving problems suited to the processor's architecture versus those with\nnon-hardware-native topologies. Our research has two main objectives: i) to\nanalyze the impact of embedding quality on the performance of D-Wave Systems\nquantum annealers, and ii) to evaluate the quality of the embeddings generated\nby Minorminer, an algorithm provided by D-Wave and widely recognized as the\nstandard minor-embedding technique in the literature. Regarding the first\nobjective, our experiments reveal a clear correlation between the average chain\nlength of embeddings and the relative errors of the solutions sampled. This\nunderscores the critical influence of embedding quality on quantum annealing\nperformance. For the second objective, we focus on the Minorminer technique,\nassessing its capacity to embed problems, the quality of the embeddings\nproduced, and the robustness of the results. We also compare its performance\nwith Clique Embedding, another algorithm developed by D-Wave, which is\ndeterministic and designed to embed fully connected Ising models into quantum\nannealing processors, serving as a worst-case scenario. The results demonstrate\nthat there is significant room for improvement for Minorminer, as it has not\nconsistently outperformed the worst-case scenario.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u91cf\u5b50\u9000\u706b\u5904\u7406\u5668\u4e2d\u7684minor-embedding\u95ee\u9898\uff0c\u5206\u6790\u4e86\u5d4c\u5165\u8d28\u91cf\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u5e76\u8bc4\u4f30\u4e86Minorminer\u7b97\u6cd5\u7684\u8868\u73b0\u3002", "motivation": "\u91cf\u5b50\u9000\u706b\u5668\u5728\u89e3\u51b3\u9002\u5408\u5176\u67b6\u6784\u7684\u95ee\u9898\u65f6\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5bf9\u4e8e\u975e\u786c\u4ef6\u539f\u751f\u62d3\u6251\u7684\u95ee\u9898\u5b58\u5728\u6027\u80fd\u5dee\u8ddd\uff0c\u56e0\u6b64\u7814\u7a76\u5d4c\u5165\u8d28\u91cf\u7684\u5f71\u54cd\u81f3\u5173\u91cd\u8981\u3002", "method": "\u901a\u8fc7\u5b9e\u9a8c\u5206\u6790\u5d4c\u5165\u8d28\u91cf\uff08\u5982\u5e73\u5747\u94fe\u957f\uff09\u5bf9\u89e3\u8bef\u5dee\u7684\u5f71\u54cd\uff0c\u5e76\u8bc4\u4f30Minorminer\u7b97\u6cd5\u7684\u5d4c\u5165\u80fd\u529b\u3001\u8d28\u91cf\u548c\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u4e0eClique Embedding\u8fdb\u884c\u5bf9\u6bd4\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u5d4c\u5165\u8d28\u91cf\u663e\u8457\u5f71\u54cd\u91cf\u5b50\u9000\u706b\u6027\u80fd\uff0c\u800cMinorminer\u5728\u8868\u73b0\u4e0a\u5e76\u672a\u59cb\u7ec8\u4f18\u4e8e\u6700\u574f\u60c5\u51b5\uff08Clique Embedding\uff09\uff0c\u5b58\u5728\u6539\u8fdb\u7a7a\u95f4\u3002", "conclusion": "\u5d4c\u5165\u8d28\u91cf\u5bf9\u91cf\u5b50\u9000\u706b\u6027\u80fd\u81f3\u5173\u91cd\u8981\uff0cMinorminer\u7b97\u6cd5\u867d\u5e7f\u6cdb\u4f7f\u7528\uff0c\u4f46\u4ecd\u6709\u63d0\u5347\u7a7a\u95f4\uff0c\u672a\u6765\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u3002"}}
{"id": "2504.13818", "pdf": "https://arxiv.org/pdf/2504.13818", "abs": "https://arxiv.org/abs/2504.13818", "authors": ["Yixuan Even Xu", "Yash Savani", "Fei Fang", "Zico Kolter"], "title": "Not All Rollouts are Useful: Down-Sampling Rollouts in LLM Reinforcement Learning", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "9 pages, 1 figure", "summary": "Reinforcement learning (RL) has emerged as a powerful paradigm for enhancing\nreasoning capabilities in large language models, but faces a fundamental\nasymmetry in computation and memory requirements: inference is embarrassingly\nparallel with a minimal memory footprint, while policy updates require\nextensive synchronization and are memory-intensive. To address this asymmetry,\nwe introduce PODS (Policy Optimization with Down-Sampling), a framework that\nstrategically decouples these phases by generating numerous rollouts in\nparallel but updating only on an informative subset. Within this framework, we\ndevelop max-variance down-sampling, a theoretically motivated method that\nselects rollouts with maximally diverse reward signals. We prove that this\napproach has an efficient algorithmic solution, and empirically demonstrate\nthat GRPO with PODS using max-variance down-sampling achieves superior\nperformance over standard GRPO on the GSM8K benchmark.", "AI": {"tldr": "PODS\u6846\u67b6\u901a\u8fc7\u5e76\u884c\u751f\u6210\u5927\u91cfrollout\u4f46\u4ec5\u66f4\u65b0\u4fe1\u606f\u4e30\u5bcc\u7684\u5b50\u96c6\uff0c\u89e3\u51b3\u4e86RL\u4e2d\u8ba1\u7b97\u4e0e\u5185\u5b58\u9700\u6c42\u7684\u4e0d\u5bf9\u79f0\u95ee\u9898\uff0c\u5e76\u5728GSM8K\u57fa\u51c6\u4e0a\u4f18\u4e8e\u6807\u51c6GRPO\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u5728\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u80fd\u529b\u65f6\u9762\u4e34\u8ba1\u7b97\u4e0e\u5185\u5b58\u9700\u6c42\u7684\u4e0d\u5bf9\u79f0\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faPODS\u6846\u67b6\uff0c\u91c7\u7528max-variance down-sampling\u65b9\u6cd5\u9009\u62e9\u591a\u6837\u5316\u7684rollout\u5b50\u96c6\u8fdb\u884c\u7b56\u7565\u66f4\u65b0\u3002", "result": "PODS\u5728GSM8K\u57fa\u51c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u6807\u51c6GRPO\u3002", "conclusion": "PODS\u901a\u8fc7\u89e3\u8026\u5e76\u884c\u751f\u6210\u4e0e\u7b56\u7565\u66f4\u65b0\uff0c\u6709\u6548\u89e3\u51b3\u4e86RL\u4e2d\u7684\u4e0d\u5bf9\u79f0\u95ee\u9898\uff0c\u5e76\u63d0\u5347\u4e86\u6027\u80fd\u3002"}}
{"id": "2504.13391", "pdf": "https://arxiv.org/pdf/2504.13391", "abs": "https://arxiv.org/abs/2504.13391", "authors": ["Racheal Mukisa", "Arvind K. Bansal"], "title": "Cardiac MRI Semantic Segmentation for Ventricles and Myocardium using Deep Learning", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG", "I.4.6; I.2; I.5.2; I.5.1"], "comment": "20 pages, 8 figures", "summary": "Automated noninvasive cardiac diagnosis plays a critical role in the early\ndetection of cardiac disorders and cost-effective clinical management.\nAutomated diagnosis involves the automated segmentation and analysis of cardiac\nimages. Precise delineation of cardiac substructures and extraction of their\nmorphological attributes are essential for evaluating the cardiac function, and\ndiagnosing cardiovascular disease such as cardiomyopathy, valvular diseases,\nabnormalities related to septum perforations, and blood-flow rate. Semantic\nsegmentation labels the CMR image at the pixel level, and localizes its\nsubcomponents to facilitate the detection of abnormalities, including\nabnormalities in cardiac wall motion in an aging heart with muscle\nabnormalities, vascular abnormalities, and valvular abnormalities. In this\npaper, we describe a model to improve semantic segmentation of CMR images. The\nmodel extracts edge-attributes and context information during down-sampling of\nthe U-Net and infuses this information during up-sampling to localize three\nmajor cardiac structures: left ventricle cavity (LV); right ventricle cavity\n(RV); and LV myocardium (LMyo). We present an algorithm and performance\nresults. A comparison of our model with previous leading models, using\nsimilarity metrics between actual image and segmented image, shows that our\napproach improves Dice similarity coefficient (DSC) by 2%-11% and lowers\nHausdorff distance (HD) by 1.6 to 5.7 mm.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u5fc3\u810f\u78c1\u5171\u632f\uff08CMR\uff09\u56fe\u50cf\u8bed\u4e49\u5206\u5272\u7684\u6a21\u578b\uff0c\u901a\u8fc7\u63d0\u53d6\u8fb9\u7f18\u5c5e\u6027\u548c\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u63d0\u5347\u4e86\u5de6\u5fc3\u5ba4\u8154\u3001\u53f3\u5fc3\u5ba4\u8154\u548c\u5de6\u5fc3\u5ba4\u5fc3\u808c\u7684\u5206\u5272\u7cbe\u5ea6\u3002", "motivation": "\u81ea\u52a8\u5316\u975e\u4fb5\u5165\u6027\u5fc3\u810f\u8bca\u65ad\u5bf9\u65e9\u671f\u53d1\u73b0\u5fc3\u810f\u75be\u75c5\u548c\u6210\u672c\u6548\u76ca\u9ad8\u7684\u4e34\u5e8a\u7ba1\u7406\u81f3\u5173\u91cd\u8981\uff0c\u800c\u7cbe\u786e\u5206\u5272\u5fc3\u810f\u5b50\u7ed3\u6784\u662f\u8bc4\u4f30\u5fc3\u810f\u529f\u80fd\u548c\u8bca\u65ad\u5fc3\u8840\u7ba1\u75be\u75c5\u7684\u57fa\u7840\u3002", "method": "\u6a21\u578b\u5728U-Net\u7684\u4e0b\u91c7\u6837\u8fc7\u7a0b\u4e2d\u63d0\u53d6\u8fb9\u7f18\u5c5e\u6027\u548c\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u5e76\u5728\u4e0a\u91c7\u6837\u65f6\u6ce8\u5165\u8fd9\u4e9b\u4fe1\u606f\uff0c\u4ee5\u5b9a\u4f4d\u5de6\u5fc3\u5ba4\u8154\u3001\u53f3\u5fc3\u5ba4\u8154\u548c\u5de6\u5fc3\u5ba4\u5fc3\u808c\u3002", "result": "\u4e0e\u73b0\u6709\u9886\u5148\u6a21\u578b\u76f8\u6bd4\uff0c\u8be5\u6a21\u578b\u5c06Dice\u76f8\u4f3c\u7cfb\u6570\uff08DSC\uff09\u63d0\u9ad8\u4e862%-11%\uff0c\u5e76\u5c06Hausdorff\u8ddd\u79bb\uff08HD\uff09\u964d\u4f4e\u4e861.6\u81f35.7\u6beb\u7c73\u3002", "conclusion": "\u8be5\u6a21\u578b\u663e\u8457\u63d0\u5347\u4e86CMR\u56fe\u50cf\u7684\u8bed\u4e49\u5206\u5272\u6027\u80fd\uff0c\u4e3a\u5fc3\u810f\u75be\u75c5\u7684\u81ea\u52a8\u5316\u8bca\u65ad\u63d0\u4f9b\u4e86\u66f4\u7cbe\u786e\u7684\u5de5\u5177\u3002"}}
{"id": "2504.13768", "pdf": "https://arxiv.org/pdf/2504.13768", "abs": "https://arxiv.org/abs/2504.13768", "authors": ["Vinay Sharma", "R\u00e9mi Tanguy Oddon", "Pietro Tesini", "Jens Ravesloot", "Cees Taal", "Olga Fink"], "title": "Equi-Euler GraphNet: An Equivariant, Temporal-Dynamics Informed Graph Neural Network for Dual Force and Trajectory Prediction in Multi-Body Systems", "categories": ["cs.LG", "cs.CE", "physics.comp-ph"], "comment": null, "summary": "Accurate real-time modeling of multi-body dynamical systems is essential for\nenabling digital twin applications across industries. While many data-driven\napproaches aim to learn system dynamics, jointly predicting internal loads and\nsystem trajectories remains a key challenge. This dual prediction is especially\nimportant for fault detection and predictive maintenance, where internal\nloads-such as contact forces-act as early indicators of faults, reflecting wear\nor misalignment before affecting motion. These forces also serve as inputs to\ndegradation models (e.g., crack growth), enabling damage prediction and\nremaining useful life estimation. We propose Equi-Euler GraphNet, a\nphysics-informed graph neural network (GNN) that simultaneously predicts\ninternal forces and global trajectories in multi-body systems. In this\nmesh-free framework, nodes represent system components and edges encode\ninteractions. Equi-Euler GraphNet introduces two inductive biases: (1) an\nequivariant message-passing scheme, interpreting edge messages as interaction\nforces consistent under Euclidean transformations; and (2) a temporal-aware\niterative node update mechanism, based on Euler integration, to capture\ninfluence of distant interactions over time. Tailored for cylindrical roller\nbearings, it decouples ring dynamics from constrained motion of rolling\nelements. Trained on high-fidelity multiphysics simulations, Equi-Euler\nGraphNet generalizes beyond the training distribution, accurately predicting\nloads and trajectories under unseen speeds, loads, and configurations. It\noutperforms state-of-the-art GNNs focused on trajectory prediction, delivering\nstable rollouts over thousands of time steps with minimal error accumulation.\nAchieving up to a 200x speedup over conventional solvers while maintaining\ncomparable accuracy, it serves as an efficient reduced-order model for digital\ntwins, design, and maintenance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aEqui-Euler GraphNet\u7684\u7269\u7406\u4fe1\u606f\u56fe\u795e\u7ecf\u7f51\u7edc\uff0c\u7528\u4e8e\u540c\u65f6\u9884\u6d4b\u591a\u4f53\u7cfb\u7edf\u4e2d\u7684\u5185\u90e8\u529b\u548c\u5168\u5c40\u8f68\u8ff9\uff0c\u9002\u7528\u4e8e\u6570\u5b57\u5b6a\u751f\u5e94\u7528\u3002", "motivation": "\u591a\u4f53\u52a8\u529b\u5b66\u7cfb\u7edf\u7684\u5b9e\u65f6\u5efa\u6a21\u5bf9\u6570\u5b57\u5b6a\u751f\u5e94\u7528\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u540c\u65f6\u9884\u6d4b\u5185\u90e8\u8f7d\u8377\u548c\u7cfb\u7edf\u8f68\u8ff9\uff0c\u800c\u8fd9\u5bf9\u6545\u969c\u68c0\u6d4b\u548c\u9884\u6d4b\u6027\u7ef4\u62a4\u5c24\u4e3a\u91cd\u8981\u3002", "method": "\u91c7\u7528\u7269\u7406\u4fe1\u606f\u56fe\u795e\u7ecf\u7f51\u7edc\uff0c\u7ed3\u5408\u7b49\u53d8\u6d88\u606f\u4f20\u9012\u548c\u57fa\u4e8e\u6b27\u62c9\u79ef\u5206\u7684\u65f6\u95f4\u611f\u77e5\u8282\u70b9\u66f4\u65b0\u673a\u5236\uff0c\u9002\u7528\u4e8e\u5706\u67f1\u6eda\u5b50\u8f74\u627f\u7b49\u7cfb\u7edf\u3002", "result": "\u5728\u672a\u89c1\u7684\u5de5\u51b5\u4e0b\u4ecd\u80fd\u51c6\u786e\u9884\u6d4b\u8f7d\u8377\u548c\u8f68\u8ff9\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u8ba1\u7b97\u901f\u5ea6\u6bd4\u4f20\u7edf\u6c42\u89e3\u5668\u5feb200\u500d\u3002", "conclusion": "Equi-Euler GraphNet\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u964d\u9636\u6a21\u578b\uff0c\u9002\u7528\u4e8e\u6570\u5b57\u5b6a\u751f\u3001\u8bbe\u8ba1\u548c\u7ef4\u62a4\u3002"}}
{"id": "2504.13399", "pdf": "https://arxiv.org/pdf/2504.13399", "abs": "https://arxiv.org/abs/2504.13399", "authors": ["Shashank Shriram", "Srinivasa Perisetla", "Aryan Keskar", "Harsha Krishnaswamy", "Tonko Emil Westerhof Bossen", "Andreas M\u00f8gelmose", "Ross Greer"], "title": "Towards a Multi-Agent Vision-Language System for Zero-Shot Novel Hazardous Object Detection for Autonomous Driving Safety", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Detecting anomalous hazards in visual data, particularly in video streams, is\na critical challenge in autonomous driving. Existing models often struggle with\nunpredictable, out-of-label hazards due to their reliance on predefined object\ncategories. In this paper, we propose a multimodal approach that integrates\nvision-language reasoning with zero-shot object detection to improve hazard\nidentification and explanation. Our pipeline consists of a Vision-Language\nModel (VLM), a Large Language Model (LLM), in order to detect hazardous objects\nwithin a traffic scene. We refine object detection by incorporating OpenAI's\nCLIP model to match predicted hazards with bounding box annotations, improving\nlocalization accuracy. To assess model performance, we create a ground truth\ndataset by denoising and extending the foundational COOOL\n(Challenge-of-Out-of-Label) anomaly detection benchmark dataset with complete\nnatural language descriptions for hazard annotations. We define a means of\nhazard detection and labeling evaluation on the extended dataset using cosine\nsimilarity. This evaluation considers the semantic similarity between the\npredicted hazard description and the annotated ground truth for each video.\nAdditionally, we release a set of tools for structuring and managing\nlarge-scale hazard detection datasets. Our findings highlight the strengths and\nlimitations of current vision-language-based approaches, offering insights into\nfuture improvements in autonomous hazard detection systems. Our models,\nscripts, and data can be found at https://github.com/mi3labucm/COOOLER.git", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u65b9\u6cd5\uff0c\u7ed3\u5408\u89c6\u89c9-\u8bed\u8a00\u63a8\u7406\u548c\u96f6\u6837\u672c\u76ee\u6807\u68c0\u6d4b\uff0c\u4ee5\u6539\u8fdb\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u5371\u9669\u8bc6\u522b\u548c\u89e3\u91ca\u3002", "motivation": "\u73b0\u6709\u6a21\u578b\u4f9d\u8d56\u9884\u5b9a\u4e49\u7c7b\u522b\uff0c\u96be\u4ee5\u5e94\u5bf9\u4e0d\u53ef\u9884\u6d4b\u7684\u5371\u9669\uff0c\u9700\u6539\u8fdb\u5371\u9669\u68c0\u6d4b\u80fd\u529b\u3002", "method": "\u6574\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u548c\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\uff0c\u5229\u7528CLIP\u6a21\u578b\u4f18\u5316\u76ee\u6807\u68c0\u6d4b\uff0c\u5e76\u6269\u5c55COOOL\u6570\u636e\u96c6\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u901a\u8fc7\u4f59\u5f26\u76f8\u4f3c\u5ea6\u8bc4\u4f30\u8bed\u4e49\u76f8\u4f3c\u6027\uff0c\u5c55\u793a\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5e76\u53d1\u5e03\u4e86\u76f8\u5173\u5de5\u5177\u548c\u6570\u636e\u96c6\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u65b9\u6cd5\u7684\u4f18\u7f3a\u70b9\uff0c\u4e3a\u672a\u6765\u81ea\u52a8\u9a7e\u9a76\u5371\u9669\u68c0\u6d4b\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6539\u8fdb\u65b9\u5411\u3002"}}
{"id": "2504.13774", "pdf": "https://arxiv.org/pdf/2504.13774", "abs": "https://arxiv.org/abs/2504.13774", "authors": ["Tamim Al Mahmud", "Najeeb Jebreel", "Josep Domingo-Ferrer", "David Sanchez"], "title": "DP2Unlearning: An Efficient and Guaranteed Unlearning Framework for LLMs", "categories": ["cs.LG", "cs.AI"], "comment": "49 pages", "summary": "Large language models (LLMs) have recently revolutionized language processing\ntasks but have also brought ethical and legal issues. LLMs have a tendency to\nmemorize potentially private or copyrighted information present in the training\ndata, which might then be delivered to end users at inference time. When this\nhappens, a naive solution is to retrain the model from scratch after excluding\nthe undesired data. Although this guarantees that the target data have been\nforgotten, it is also prohibitively expensive for LLMs. Approximate unlearning\noffers a more efficient alternative, as it consists of ex post modifications of\nthe trained model itself to prevent undesirable results, but it lacks\nforgetting guarantees because it relies solely on empirical evidence. In this\nwork, we present DP2Unlearning, a novel LLM unlearning framework that offers\nformal forgetting guarantees at a significantly lower cost than retraining from\nscratch on the data to be retained. DP2Unlearning involves training LLMs on\ntextual data protected using {\\epsilon}-differential privacy (DP), which later\nenables efficient unlearning with the guarantees against disclosure associated\nwith the chosen {\\epsilon}. Our experiments demonstrate that DP2Unlearning\nachieves similar model performance post-unlearning, compared to an LLM\nretraining from scratch on retained data -- the gold standard exact unlearning\n-- but at approximately half the unlearning cost. In addition, with a\nreasonable computational cost, it outperforms approximate unlearning methods at\nboth preserving the utility of the model post-unlearning and effectively\nforgetting the targeted information.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faDP2Unlearning\u6846\u67b6\uff0c\u901a\u8fc7\u5dee\u5206\u9690\u79c1\u4fdd\u62a4\u8bad\u7ec3\u6570\u636e\uff0c\u5b9e\u73b0\u9ad8\u6548\u4e14\u5177\u6709\u6b63\u5f0f\u9057\u5fd8\u4fdd\u8bc1\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u9057\u5fd8\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3LLMs\u56e0\u8bb0\u5fc6\u8bad\u7ec3\u6570\u636e\u4e2d\u9690\u79c1\u6216\u7248\u6743\u4fe1\u606f\u800c\u5f15\u53d1\u7684\u4f26\u7406\u548c\u6cd5\u5f8b\u95ee\u9898\uff0c\u907f\u514d\u6602\u8d35\u7684\u91cd\u65b0\u8bad\u7ec3\u3002", "method": "\u4f7f\u7528\u03b5-\u5dee\u5206\u9690\u79c1\uff08DP\uff09\u4fdd\u62a4\u8bad\u7ec3\u6570\u636e\uff0c\u5f00\u53d1DP2Unlearning\u6846\u67b6\uff0c\u5b9e\u73b0\u9ad8\u6548\u9057\u5fd8\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDP2Unlearning\u5728\u4fdd\u7559\u6a21\u578b\u6027\u80fd\u7684\u540c\u65f6\uff0c\u9057\u5fd8\u6210\u672c\u4ec5\u4e3a\u91cd\u65b0\u8bad\u7ec3\u7684\u4e00\u534a\uff0c\u4f18\u4e8e\u8fd1\u4f3c\u9057\u5fd8\u65b9\u6cd5\u3002", "conclusion": "DP2Unlearning\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u5177\u6709\u6b63\u5f0f\u9057\u5fd8\u4fdd\u8bc1\u7684LLM\u9057\u5fd8\u65b9\u6cd5\uff0c\u4f18\u4e8e\u91cd\u65b0\u8bad\u7ec3\u548c\u8fd1\u4f3c\u9057\u5fd8\u3002"}}
{"id": "2504.13786", "pdf": "https://arxiv.org/pdf/2504.13786", "abs": "https://arxiv.org/abs/2504.13786", "authors": ["Lorenz Kummer", "Wilfried N. Gansterer", "Nils M. Kriege"], "title": "On the Relationship Between Robustness and Expressivity of Graph Neural Networks", "categories": ["cs.LG"], "comment": "Accepted at AISTAST 2025, will add DOI when available", "summary": "We investigate the vulnerability of Graph Neural Networks (GNNs) to bit-flip\nattacks (BFAs) by introducing an analytical framework to study the influence of\narchitectural features, graph properties, and their interaction.\n  The expressivity of GNNs refers to their ability to distinguish\nnon-isomorphic graphs and depends on the encoding of node neighborhoods. We\nexamine the vulnerability of neural multiset functions commonly used for this\npurpose and establish formal criteria to characterize a GNN's susceptibility to\nlosing expressivity due to BFAs. This enables an analysis of the impact of\nhomophily, graph structural variety, feature encoding, and activation functions\non GNN robustness. We derive theoretical bounds for the number of bit flips\nrequired to degrade GNN expressivity on a dataset, identifying ReLU-activated\nGNNs operating on highly homophilous graphs with low-dimensional or one-hot\nencoded features as particularly susceptible. Empirical results using ten\nreal-world datasets confirm the statistical significance of our key theoretical\ninsights and offer actionable results to mitigate BFA risks in\nexpressivity-critical applications.", "AI": {"tldr": "\u7814\u7a76\u4e86\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNNs\uff09\u5bf9\u4f4d\u7ffb\u8f6c\u653b\u51fb\uff08BFAs\uff09\u7684\u8106\u5f31\u6027\uff0c\u5206\u6790\u4e86\u67b6\u6784\u7279\u5f81\u3001\u56fe\u5c5e\u6027\u53ca\u5176\u4ea4\u4e92\u5bf9\u653b\u51fb\u7684\u5f71\u54cd\u3002", "motivation": "GNNs\u7684\u8868\u8fbe\u80fd\u529b\u4f9d\u8d56\u4e8e\u8282\u70b9\u90bb\u57df\u7684\u7f16\u7801\uff0c\u4f46\u5176\u5bf9BFAs\u7684\u8106\u5f31\u6027\u5c1a\u672a\u5145\u5206\u7814\u7a76\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5206\u6790\u6846\u67b6\uff0c\u7814\u7a76\u4e86GNNs\u7684\u591a\u91cd\u96c6\u51fd\u6570\u8106\u5f31\u6027\uff0c\u5e76\u5efa\u7acb\u4e86\u5f62\u5f0f\u5316\u6807\u51c6\u4ee5\u8bc4\u4f30\u5176\u56e0BFAs\u5931\u53bb\u8868\u8fbe\u80fd\u529b\u7684\u53ef\u80fd\u6027\u3002", "result": "\u7406\u8bba\u63a8\u5bfc\u4e86\u964d\u4f4eGNN\u8868\u8fbe\u80fd\u529b\u6240\u9700\u7684\u4f4d\u7ffb\u8f6c\u6b21\u6570\uff0c\u5e76\u901a\u8fc7\u5341\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u9a8c\u8bc1\u4e86\u7406\u8bba\u7ed3\u679c\u3002", "conclusion": "ReLU\u6fc0\u6d3b\u7684GNNs\u5728\u9ad8\u540c\u8d28\u6027\u56fe\u6216\u4f4e\u7ef4/\u72ec\u70ed\u7f16\u7801\u7279\u5f81\u4e0b\u6613\u53d7\u653b\u51fb\uff0c\u7814\u7a76\u7ed3\u679c\u4e3a\u5173\u952e\u5e94\u7528\u63d0\u4f9b\u4e86\u7f13\u89e3\u98ce\u9669\u7684\u5b9e\u7528\u5efa\u8bae\u3002"}}
{"id": "2504.13407", "pdf": "https://arxiv.org/pdf/2504.13407", "abs": "https://arxiv.org/abs/2504.13407", "authors": ["Shimou Ling", "Liang Zhang", "Jiangwei Zhao", "Lili Pan", "Hongliang Li"], "title": "LoRA-Based Continual Learning with Constraints on Critical Parameter Changes", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "LoRA-based continual learning represents a promising avenue for leveraging\npre-trained models in downstream continual learning tasks. Recent studies have\nshown that orthogonal LoRA tuning effectively mitigates forgetting. However,\nthis work unveils that under orthogonal LoRA tuning, the critical parameters\nfor pre-tasks still change notably after learning post-tasks. To address this\nproblem, we directly propose freezing the most critical parameter matrices in\nthe Vision Transformer (ViT) for pre-tasks before learning post-tasks. In\naddition, building on orthogonal LoRA tuning, we propose orthogonal LoRA\ncomposition (LoRAC) based on QR decomposition, which may further enhance the\nplasticity of our method. Elaborate ablation studies and extensive comparisons\ndemonstrate the effectiveness of our proposed method. Our results indicate that\nour method achieves state-of-the-art (SOTA) performance on several well-known\ncontinual learning benchmarks. For instance, on the Split CIFAR-100 dataset,\nour method shows a 6.35\\% improvement in accuracy and a 3.24\\% reduction in\nforgetting compared to previous methods. Our code is available at\nhttps://github.com/learninginvision/LoRAC-IPC.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLoRA\u7684\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u51bb\u7ed3\u5173\u952e\u53c2\u6570\u77e9\u9635\u548c\u6b63\u4ea4LoRA\u7ec4\u5408\uff08LoRAC\uff09\u6765\u51cf\u5c11\u9057\u5fd8\u5e76\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6b63\u4ea4LoRA\u8c03\u4f18\u65b9\u6cd5\u5728\u6301\u7eed\u5b66\u4e60\u4e2d\u4ecd\u4f1a\u5bfc\u81f4\u9884\u4efb\u52a1\u5173\u952e\u53c2\u6570\u663e\u8457\u53d8\u5316\uff0c\u9700\u8981\u6539\u8fdb\u3002", "method": "\u51bb\u7ed3\u9884\u4efb\u52a1\u5173\u952e\u53c2\u6570\u77e9\u9635\uff0c\u5e76\u7ed3\u5408\u6b63\u4ea4LoRA\u7ec4\u5408\uff08LoRAC\uff09\u65b9\u6cd5\u3002", "result": "\u5728Split CIFAR-100\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u51c6\u786e\u7387\u63d0\u53476.35%\uff0c\u9057\u5fd8\u7387\u964d\u4f4e3.24%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u6301\u7eed\u5b66\u4e60\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u8fbe\u5230\u4e86SOTA\u6027\u80fd\u3002"}}
{"id": "2504.13787", "pdf": "https://arxiv.org/pdf/2504.13787", "abs": "https://arxiv.org/abs/2504.13787", "authors": ["Helen Jin", "Anton Xue", "Weiqiu You", "Surbhi Goel", "Eric Wong"], "title": "Probabilistic Stability Guarantees for Feature Attributions", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Stability guarantees are an emerging tool for evaluating feature\nattributions, but existing certification methods rely on smoothed classifiers\nand often yield conservative guarantees. To address these limitations, we\nintroduce soft stability and propose a simple, model-agnostic, and\nsample-efficient stability certification algorithm (SCA) that provides\nnon-trivial and interpretable guarantees for any attribution. Moreover, we show\nthat mild smoothing enables a graceful tradeoff between accuracy and stability,\nin contrast to prior certification methods that require a more aggressive\ncompromise. Using Boolean function analysis, we give a novel characterization\nof stability under smoothing. We evaluate SCA on vision and language tasks, and\ndemonstrate the effectiveness of soft stability in measuring the robustness of\nexplanation methods.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSCA\u7684\u6a21\u578b\u65e0\u5173\u3001\u6837\u672c\u9ad8\u6548\u7684\u7a33\u5b9a\u6027\u8ba4\u8bc1\u7b97\u6cd5\uff0c\u901a\u8fc7\u8f6f\u7a33\u5b9a\u6027\u6982\u5ff5\u89e3\u51b3\u4e86\u73b0\u6709\u8ba4\u8bc1\u65b9\u6cd5\u7684\u4fdd\u5b88\u6027\u95ee\u9898\uff0c\u5e76\u5728\u89c6\u89c9\u548c\u8bed\u8a00\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u7279\u5f81\u5f52\u56e0\u7684\u7a33\u5b9a\u6027\u8ba4\u8bc1\u65b9\u6cd5\u4f9d\u8d56\u5e73\u6ed1\u5206\u7c7b\u5668\u4e14\u7ed3\u679c\u4fdd\u5b88\uff0c\u9650\u5236\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u5f15\u5165\u8f6f\u7a33\u5b9a\u6027\u6982\u5ff5\uff0c\u63d0\u51faSCA\u7b97\u6cd5\uff0c\u5229\u7528\u5e03\u5c14\u51fd\u6570\u5206\u6790\u523b\u753b\u5e73\u6ed1\u4e0b\u7684\u7a33\u5b9a\u6027\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "SCA\u63d0\u4f9b\u4e86\u975e\u5e73\u51e1\u4e14\u53ef\u89e3\u91ca\u7684\u7a33\u5b9a\u6027\u4fdd\u8bc1\uff0c\u4e14\u901a\u8fc7\u6e29\u548c\u5e73\u6ed1\u5b9e\u73b0\u4e86\u51c6\u786e\u6027\u4e0e\u7a33\u5b9a\u6027\u7684\u5e73\u8861\u3002", "conclusion": "\u8f6f\u7a33\u5b9a\u6027\u662f\u8861\u91cf\u89e3\u91ca\u65b9\u6cd5\u9c81\u68d2\u6027\u7684\u6709\u6548\u5de5\u5177\uff0cSCA\u7b97\u6cd5\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\u3002"}}
{"id": "2504.13414", "pdf": "https://arxiv.org/pdf/2504.13414", "abs": "https://arxiv.org/abs/2504.13414", "authors": ["Hsin-Yi Lin", "Huan-Hsin Tseng", "Samuel Yen-Chi Chen", "Shinjae Yoo"], "title": "Adaptive Non-local Observable on Quantum Neural Networks", "categories": ["quant-ph", "cs.AI", "cs.LG"], "comment": null, "summary": "Conventional Variational Quantum Circuits (VQCs) for Quantum Machine Learning\ntypically rely on a fixed Hermitian observable, often built from Pauli\noperators. Inspired by the Heisenberg picture, we propose an adaptive non-local\nmeasurement framework that substantially increases the model complexity of the\nquantum circuits. Our introduction of dynamical Hermitian observables with\nevolving parameters shows that optimizing VQC rotations corresponds to tracing\na trajectory in the observable space. This viewpoint reveals that standard VQCs\nare merely a special case of the Heisenberg representation.\n  Furthermore, we show that properly incorporating variational rotations with\nnon-local observables enhances qubit interaction and information mixture,\nadmitting flexible circuit designs. Two non-local measurement schemes are\nintroduced, and numerical simulations on classification tasks confirm that our\napproach outperforms conventional VQCs, yielding a more powerful and\nresource-efficient approach as a Quantum Neural Network.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6d77\u68ee\u5821\u7ed8\u666f\u7684\u81ea\u9002\u5e94\u975e\u5c40\u57df\u6d4b\u91cf\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u53ef\u89c2\u6d4b\u53c2\u6570\u4f18\u5316\u53d8\u5206\u91cf\u5b50\u7535\u8def\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u590d\u6742\u5ea6\u4e0e\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u53d8\u5206\u91cf\u5b50\u7535\u8def\uff08VQC\uff09\u901a\u5e38\u4f9d\u8d56\u56fa\u5b9a\u7684\u5384\u7c73\u53ef\u89c2\u6d4b\u7b97\u7b26\uff0c\u9650\u5236\u4e86\u6a21\u578b\u590d\u6742\u6027\u548c\u7075\u6d3b\u6027\u3002", "method": "\u5f15\u5165\u52a8\u6001\u5384\u7c73\u53ef\u89c2\u6d4b\u7b97\u7b26\uff0c\u4f18\u5316VQC\u65cb\u8f6c\u53c2\u6570\uff0c\u63d0\u51fa\u4e24\u79cd\u975e\u5c40\u57df\u6d4b\u91cf\u65b9\u6848\u3002", "result": "\u6570\u503c\u6a21\u62df\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u5206\u7c7b\u4efb\u52a1\u4e2d\u4f18\u4e8e\u4f20\u7edfVQC\uff0c\u8d44\u6e90\u6548\u7387\u66f4\u9ad8\u3002", "conclusion": "\u52a8\u6001\u53ef\u89c2\u6d4b\u7b97\u7b26\u6846\u67b6\u4e3a\u91cf\u5b50\u795e\u7ecf\u7f51\u7edc\u63d0\u4f9b\u4e86\u66f4\u5f3a\u5927\u4e14\u7075\u6d3b\u7684\u8bbe\u8ba1\u601d\u8def\u3002"}}
{"id": "2504.13792", "pdf": "https://arxiv.org/pdf/2504.13792", "abs": "https://arxiv.org/abs/2504.13792", "authors": ["Weizhi Lu", "Mingrui Chen", "Weiyu Li"], "title": "The Binary and Ternary Quantization Can Improve Feature Discrimination", "categories": ["cs.LG"], "comment": null, "summary": "In machine learning, quantization is widely used to simplify data\nrepresentation and facilitate algorithm deployment on hardware. Given the\nfundamental role of classification in machine learning, it is crucial to\ninvestigate the impact of quantization on classification. Current research\nprimarily focuses on quantization errors, operating under the premise that\nhigher quantization errors generally result in lower classification\nperformance. However, this premise lacks a solid theoretical foundation and\noften contradicts empirical findings. For instance, certain extremely low\nbit-width quantization methods, such as $\\{0,1\\}$-binary quantization and $\\{0,\n\\pm1\\}$-ternary quantization, can achieve comparable or even superior\nclassification accuracy compared to the original non-quantized data, despite\nexhibiting high quantization errors. To more accurately evaluate classification\nperformance, we propose to directly investigate the feature discrimination of\nquantized data, instead of analyzing its quantization error. Interestingly, it\nis found that both binary and ternary quantization methods can improve, rather\nthan degrade, the feature discrimination of the original data. This remarkable\nperformance is validated through classification experiments across various data\ntypes, including images, speech, and texts.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u91cf\u5316\u5bf9\u5206\u7c7b\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u63d0\u51fa\u76f4\u63a5\u5206\u6790\u91cf\u5316\u6570\u636e\u7684\u7279\u5f81\u5224\u522b\u6027\u800c\u975e\u91cf\u5316\u8bef\u5dee\uff0c\u53d1\u73b0\u4f4e\u6bd4\u7279\u91cf\u5316\u65b9\u6cd5\uff08\u5982\u4e8c\u503c\u548c\u4e09\u503c\u91cf\u5316\uff09\u80fd\u63d0\u5347\u5206\u7c7b\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u91cf\u5316\u8bef\u5dee\uff0c\u4f46\u7f3a\u4e4f\u7406\u8bba\u652f\u6301\u4e14\u4e0e\u5b9e\u8bc1\u7ed3\u679c\u77db\u76fe\u3002\u8bba\u6587\u65e8\u5728\u63a2\u7d22\u91cf\u5316\u5bf9\u7279\u5f81\u5224\u522b\u6027\u7684\u76f4\u63a5\u5f71\u54cd\u3002", "method": "\u63d0\u51fa\u76f4\u63a5\u5206\u6790\u91cf\u5316\u6570\u636e\u7684\u7279\u5f81\u5224\u522b\u6027\uff0c\u800c\u975e\u91cf\u5316\u8bef\u5dee\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e8c\u503c\u548c\u4e09\u503c\u91cf\u5316\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4e8c\u503c\u548c\u4e09\u503c\u91cf\u5316\u65b9\u6cd5\u80fd\u63d0\u5347\u539f\u59cb\u6570\u636e\u7684\u7279\u5f81\u5224\u522b\u6027\uff0c\u5e76\u5728\u56fe\u50cf\u3001\u8bed\u97f3\u548c\u6587\u672c\u5206\u7c7b\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u91cf\u5316\u8bef\u5dee\u5e76\u975e\u5206\u7c7b\u6027\u80fd\u7684\u51b3\u5b9a\u56e0\u7d20\uff0c\u4f4e\u6bd4\u7279\u91cf\u5316\u65b9\u6cd5\u53ef\u901a\u8fc7\u63d0\u5347\u7279\u5f81\u5224\u522b\u6027\u5b9e\u73b0\u66f4\u597d\u7684\u5206\u7c7b\u6548\u679c\u3002"}}
{"id": "2504.13415", "pdf": "https://arxiv.org/pdf/2504.13415", "abs": "https://arxiv.org/abs/2504.13415", "authors": ["Racheal Mukisa", "Arvind K. Bansal"], "title": "DADU: Dual Attention-based Deep Supervised UNet for Automated Semantic Segmentation of Cardiac Images", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG", "I.4.6; I.2; I.5.2; I.5.1"], "comment": "20 pages, 8 figures", "summary": "We propose an enhanced deep learning-based model for image segmentation of\nthe left and right ventricles and myocardium scar tissue from cardiac magnetic\nresonance (CMR) images. The proposed technique integrates UNet, channel and\nspatial attention, edge-detection based skip-connection and deep supervised\nlearning to improve the accuracy of the CMR image-segmentation. Images are\nprocessed using multiple channels to generate multiple feature-maps. We built a\ndual attention-based model to integrate channel and spatial attention. The use\nof extracted edges in skip connection improves the reconstructed images from\nfeature-maps. The use of deep supervision reduces vanishing gradient problems\ninherent in classification based on deep neural networks. The algorithms for\ndual attention-based model, corresponding implementation and performance\nresults are described. The performance results show that this approach has\nattained high accuracy: 98% Dice Similarity Score (DSC) and significantly lower\nHausdorff Distance (HD). The performance results outperform other leading\ntechniques both in DSC and HD.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u6539\u8fdb\u6a21\u578b\uff0c\u7528\u4e8e\u5fc3\u810f\u78c1\u5171\u632f\u56fe\u50cf\u4e2d\u5de6\u53f3\u5fc3\u5ba4\u548c\u5fc3\u808c\u7622\u75d5\u7ec4\u7ec7\u7684\u5206\u5272\uff0c\u7ed3\u5408\u4e86UNet\u3001\u901a\u9053\u548c\u7a7a\u95f4\u6ce8\u610f\u529b\u3001\u8fb9\u7f18\u68c0\u6d4b\u8df3\u8dc3\u8fde\u63a5\u548c\u6df1\u5ea6\u76d1\u7763\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5206\u5272\u7cbe\u5ea6\u3002", "motivation": "\u5fc3\u810f\u78c1\u5171\u632f\u56fe\u50cf\u5206\u5272\u5728\u4e34\u5e8a\u8bca\u65ad\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u4e0a\u4ecd\u6709\u4e0d\u8db3\u3002", "method": "\u91c7\u7528\u591a\u901a\u9053\u5904\u7406\u56fe\u50cf\u751f\u6210\u7279\u5f81\u56fe\uff0c\u7ed3\u5408\u53cc\u6ce8\u610f\u529b\u673a\u5236\uff08\u901a\u9053\u548c\u7a7a\u95f4\u6ce8\u610f\u529b\uff09\uff0c\u5229\u7528\u8fb9\u7f18\u68c0\u6d4b\u6539\u8fdb\u8df3\u8dc3\u8fde\u63a5\uff0c\u5e76\u901a\u8fc7\u6df1\u5ea6\u76d1\u7763\u51cf\u5c11\u68af\u5ea6\u6d88\u5931\u95ee\u9898\u3002", "result": "\u6a21\u578b\u8868\u73b0\u51fa\u8272\uff0cDice\u76f8\u4f3c\u6027\u5206\u6570\u8fbe98%\uff0cHausdorff\u8ddd\u79bb\u663e\u8457\u964d\u4f4e\uff0c\u4f18\u4e8e\u5176\u4ed6\u9886\u5148\u6280\u672f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u5fc3\u810f\u78c1\u5171\u632f\u56fe\u50cf\u5206\u5272\u4e2d\u5177\u6709\u9ad8\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\uff0c\u4e3a\u4e34\u5e8a\u8bca\u65ad\u63d0\u4f9b\u4e86\u53ef\u9760\u5de5\u5177\u3002"}}
{"id": "2504.13797", "pdf": "https://arxiv.org/pdf/2504.13797", "abs": "https://arxiv.org/abs/2504.13797", "authors": ["Yu Wang", "Shujie Liu", "Shuai Lv", "Gengshuo Liu"], "title": "Meta-Learning and Knowledge Discovery based Physics-Informed Neural Network for Remaining Useful Life Prediction", "categories": ["cs.LG", "cs.AI"], "comment": "34 pages,20 figs", "summary": "Predicting the remaining useful life (RUL) of rotating machinery is critical\nfor industrial safety and maintenance, but existing methods struggle with\nscarce target-domain data and unclear degradation dynamics. We propose a\nMeta-Learning and Knowledge Discovery-based Physics-Informed Neural Network\n(MKDPINN) to address these challenges. The method first maps noisy sensor data\nto a low-dimensional hidden state space via a Hidden State Mapper (HSM). A\nPhysics-Guided Regulator (PGR) then learns unknown nonlinear PDEs governing\ndegradation evolution, embedding these physical constraints into the PINN\nframework. This integrates data-driven and physics-based approaches. The\nframework uses meta-learning, optimizing across source-domain meta-tasks to\nenable few-shot adaptation to new target tasks. Experiments on industrial data\nand the C-MAPSS benchmark show MKDPINN outperforms baselines in generalization\nand accuracy, proving its effectiveness for RUL prediction under data scarcity", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5143\u5b66\u4e60\u548c\u77e5\u8bc6\u53d1\u73b0\u7684\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\uff08MKDPINN\uff09\uff0c\u7528\u4e8e\u89e3\u51b3\u65cb\u8f6c\u673a\u68b0\u5269\u4f59\u4f7f\u7528\u5bff\u547d\uff08RUL\uff09\u9884\u6d4b\u4e2d\u76ee\u6807\u57df\u6570\u636e\u7a00\u7f3a\u548c\u9000\u5316\u52a8\u6001\u4e0d\u660e\u786e\u7684\u95ee\u9898\u3002", "motivation": "\u5de5\u4e1a\u5b89\u5168\u548c\u7ef4\u62a4\u4e2d\uff0c\u9884\u6d4b\u65cb\u8f6c\u673a\u68b0\u7684\u5269\u4f59\u4f7f\u7528\u5bff\u547d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u76ee\u6807\u57df\u6570\u636e\u7a00\u7f3a\u548c\u9000\u5316\u52a8\u6001\u4e0d\u660e\u786e\u7684\u60c5\u51b5\u4e0b\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u901a\u8fc7\u9690\u85cf\u72b6\u6001\u6620\u5c04\u5668\uff08HSM\uff09\u5c06\u566a\u58f0\u4f20\u611f\u5668\u6570\u636e\u6620\u5c04\u5230\u4f4e\u7ef4\u9690\u85cf\u72b6\u6001\u7a7a\u95f4\uff0c\u7269\u7406\u5f15\u5bfc\u8c03\u8282\u5668\uff08PGR\uff09\u5b66\u4e60\u672a\u77e5\u7684\u975e\u7ebf\u6027\u504f\u5fae\u5206\u65b9\u7a0b\uff08PDEs\uff09\uff0c\u5e76\u5c06\u5176\u5d4c\u5165PINN\u6846\u67b6\u4e2d\uff0c\u7ed3\u5408\u6570\u636e\u9a71\u52a8\u548c\u7269\u7406\u65b9\u6cd5\u3002\u5229\u7528\u5143\u5b66\u4e60\u4f18\u5316\u6e90\u57df\u5143\u4efb\u52a1\uff0c\u5b9e\u73b0\u5bf9\u65b0\u76ee\u6807\u4efb\u52a1\u7684\u5c11\u6837\u672c\u9002\u5e94\u3002", "result": "\u5728\u5de5\u4e1a\u6570\u636e\u548cC-MAPSS\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMKDPINN\u5728\u6cdb\u5316\u6027\u548c\u51c6\u786e\u6027\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "MKDPINN\u5728\u6570\u636e\u7a00\u7f3a\u60c5\u51b5\u4e0b\u6709\u6548\u63d0\u5347\u4e86RUL\u9884\u6d4b\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2504.13801", "pdf": "https://arxiv.org/pdf/2504.13801", "abs": "https://arxiv.org/abs/2504.13801", "authors": ["Nguyen Kim Hai Bui", "Nguyen Duy Chien", "P\u00e9ter Kov\u00e1cs", "Gerg\u0151 Bogn\u00e1r"], "title": "Transformer Encoder and Multi-features Time2Vec for Financial Prediction", "categories": ["cs.LG", "cs.CE"], "comment": "5 pages, currently under review at Eusipco 2025", "summary": "Financial prediction is a complex and challenging task of time series\nanalysis and signal processing, expected to model both short-term fluctuations\nand long-term temporal dependencies. Transformers have remarkable success\nmostly in natural language processing using attention mechanism, which also\ninfluenced the time series community. The ability to capture both short and\nlong-range dependencies helps to understand the financial market and to\nrecognize price patterns, leading to successful applications of Transformers in\nstock prediction. Although, the previous research predominantly focuses on\nindividual features and singular predictions, that limits the model's ability\nto understand broader market trends. In reality, within sectors such as finance\nand technology, companies belonging to the same industry often exhibit\ncorrelated stock price movements.\n  In this paper, we develop a novel neural network architecture by integrating\nTime2Vec with the Encoder of the Transformer model. Based on the study of\ndifferent markets, we propose a novel correlation feature selection method.\nThrough a comprehensive fine-tuning of multiple hyperparameters, we conduct a\ncomparative analysis of our results against benchmark models. We conclude that\nour method outperforms other state-of-the-art encoding methods such as\npositional encoding, and we also conclude that selecting correlation features\nenhance the accuracy of predicting multiple stock prices.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408Time2Vec\u548cTransformer\u7f16\u7801\u5668\u7684\u65b0\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u7528\u4e8e\u91d1\u878d\u9884\u6d4b\uff0c\u901a\u8fc7\u76f8\u5173\u6027\u7279\u5f81\u9009\u62e9\u548c\u591a\u53c2\u6570\u8c03\u4f18\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u80a1\u7968\u4ef7\u683c\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u91d1\u878d\u9884\u6d4b\u9700\u540c\u65f6\u5efa\u6a21\u77ed\u671f\u6ce2\u52a8\u548c\u957f\u671f\u4f9d\u8d56\uff0c\u73b0\u6709\u7814\u7a76\u591a\u5173\u6ce8\u5355\u4e00\u7279\u5f81\u548c\u9884\u6d4b\uff0c\u5ffd\u7565\u4e86\u884c\u4e1a\u76f8\u5173\u6027\u3002\u672c\u6587\u65e8\u5728\u5229\u7528Transformer\u7684\u6ce8\u610f\u529b\u673a\u5236\u548c\u76f8\u5173\u6027\u7279\u5f81\u9009\u62e9\uff0c\u63d0\u5347\u591a\u80a1\u7968\u4ef7\u683c\u9884\u6d4b\u80fd\u529b\u3002", "method": "\u6574\u5408Time2Vec\u4e0eTransformer\u7f16\u7801\u5668\uff0c\u63d0\u51fa\u76f8\u5173\u6027\u7279\u5f81\u9009\u62e9\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u591a\u53c2\u6570\u8c03\u4f18\u8fdb\u884c\u6a21\u578b\u4f18\u5316\u3002", "result": "\u65b0\u65b9\u6cd5\u5728\u9884\u6d4b\u591a\u80a1\u7968\u4ef7\u683c\u65f6\u4f18\u4e8e\u5176\u4ed6\u7f16\u7801\u65b9\u6cd5\uff08\u5982\u4f4d\u7f6e\u7f16\u7801\uff09\uff0c\u4e14\u76f8\u5173\u6027\u7279\u5f81\u9009\u62e9\u663e\u8457\u63d0\u9ad8\u4e86\u9884\u6d4b\u51c6\u786e\u6027\u3002", "conclusion": "\u7ed3\u5408Time2Vec\u548cTransformer\u7684\u67b6\u6784\u53ca\u76f8\u5173\u6027\u7279\u5f81\u9009\u62e9\u65b9\u6cd5\uff0c\u4e3a\u91d1\u878d\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u63d0\u4f9b\u4e86\u66f4\u4f18\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.13448", "pdf": "https://arxiv.org/pdf/2504.13448", "abs": "https://arxiv.org/abs/2504.13448", "authors": ["Daniela Ushizima", "Guilherme Melo dos Santos", "Zineb Sordo", "Ronald Pandolfi", "Jeffrey Donatelli"], "title": "Ascribe New Dimensions to Scientific Data Visualization with VR", "categories": ["cs.GR", "cs.AI", "cs.CE"], "comment": null, "summary": "For over half a century, the computer mouse has been the primary tool for\ninteracting with digital data, yet it remains a limiting factor in exploring\ncomplex, multi-scale scientific images. Traditional 2D visualization methods\nhinder intuitive analysis of inherently 3D structures. Virtual Reality (VR)\noffers a transformative alternative, providing immersive, interactive\nenvironments that enhance data comprehension. This article introduces\nASCRIBE-VR, a VR platform of Autonomous Solutions for Computational Research\nwith Immersive Browsing \\& Exploration, which integrates AI-driven algorithms\nwith scientific images. ASCRIBE-VR enables multimodal analysis, structural\nassessments, and immersive visualization, supporting scientific visualization\nof advanced datasets such as X-ray CT, Magnetic Resonance, and synthetic 3D\nimaging. Our VR tools, compatible with Meta Quest, can consume the output of\nour AI-based segmentation and iterative feedback processes to enable seamless\nexploration of large-scale 3D images. By merging AI-generated results with VR\nvisualization, ASCRIBE-VR enhances scientific discovery, bridging the gap\nbetween computational analysis and human intuition in materials research,\nconnecting human-in-the-loop with digital twins.", "AI": {"tldr": "ASCRIBE-VR\u662f\u4e00\u4e2a\u7ed3\u5408AI\u548cVR\u6280\u672f\u7684\u5e73\u53f0\uff0c\u65e8\u5728\u901a\u8fc7\u6c89\u6d78\u5f0f\u73af\u5883\u6539\u5584\u590d\u6742\u79d1\u5b66\u56fe\u50cf\u7684\u5206\u6790\u548c\u7406\u89e3\u3002", "motivation": "\u4f20\u7edf2D\u53ef\u89c6\u5316\u65b9\u6cd5\u9650\u5236\u4e863D\u7ed3\u6784\u7684\u76f4\u89c2\u5206\u6790\uff0cVR\u63d0\u4f9b\u4e86\u66f4\u6c89\u6d78\u7684\u4ea4\u4e92\u65b9\u5f0f\u3002", "method": "ASCRIBE-VR\u6574\u5408AI\u7b97\u6cd5\u4e0e\u79d1\u5b66\u56fe\u50cf\uff0c\u652f\u6301\u591a\u6a21\u6001\u5206\u6790\u548c\u6c89\u6d78\u5f0f\u53ef\u89c6\u5316\uff0c\u517c\u5bb9Meta Quest\u8bbe\u5907\u3002", "result": "\u5e73\u53f0\u80fd\u591f\u65e0\u7f1d\u63a2\u7d22\u5927\u89c4\u6a213D\u56fe\u50cf\uff0c\u589e\u5f3a\u79d1\u5b66\u53d1\u73b0\uff0c\u8fde\u63a5\u8ba1\u7b97\u5206\u6790\u4e0e\u4eba\u7c7b\u76f4\u89c9\u3002", "conclusion": "ASCRIBE-VR\u901a\u8fc7AI\u4e0eVR\u7684\u7ed3\u5408\uff0c\u4e3a\u6750\u6599\u7814\u7a76\u7b49\u9886\u57df\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u5206\u6790\u5de5\u5177\u3002"}}
{"id": "2504.13460", "pdf": "https://arxiv.org/pdf/2504.13460", "abs": "https://arxiv.org/abs/2504.13460", "authors": ["Hongwei Ji", "Wulian Yun", "Mengshi Qi", "Huadong Ma"], "title": "Chain-of-Thought Textual Reasoning for Few-shot Temporal Action Localization", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Traditional temporal action localization (TAL) methods rely on large amounts\nof detailed annotated data, whereas few-shot TAL reduces this dependence by\nusing only a few training samples to identify unseen action categories.\nHowever, existing few-shot TAL methods typically focus solely on video-level\ninformation, neglecting textual information, which can provide valuable\nsemantic support for the localization task. Therefore, we propose a new\nfew-shot temporal action localization method by Chain-of-Thought textual\nreasoning to improve localization performance. Specifically, we design a novel\nfew-shot learning framework that leverages textual semantic information to\nenhance the model's ability to capture action commonalities and variations,\nwhich includes a semantic-aware text-visual alignment module designed to align\nthe query and support videos at different levels. Meanwhile, to better express\nthe temporal dependencies and causal relationships between actions at the\ntextual level to assist action localization, we design a Chain of Thought\n(CoT)-like reasoning method that progressively guides the Vision Language Model\n(VLM) and Large Language Model (LLM) to generate CoT-like text descriptions for\nvideos. The generated texts can capture more variance of action than visual\nfeatures. We conduct extensive experiments on the publicly available\nActivityNet1.3 and THUMOS14 datasets. We introduce the first dataset named\nHuman-related Anomaly Localization and explore the application of the TAL task\nin human anomaly detection. The experimental results demonstrate that our\nproposed method significantly outperforms existing methods in single-instance\nand multi-instance scenarios. We will release our code, data and benchmark.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eChain-of-Thought\u6587\u672c\u63a8\u7406\u7684\u5c0f\u6837\u672c\u65f6\u5e8f\u52a8\u4f5c\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u7ed3\u5408\u6587\u672c\u8bed\u4e49\u4fe1\u606f\u63d0\u5347\u5b9a\u4f4d\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5c0f\u6837\u672cTAL\u65b9\u6cd5\u4ec5\u5173\u6ce8\u89c6\u9891\u7ea7\u4fe1\u606f\uff0c\u5ffd\u7565\u4e86\u6587\u672c\u4fe1\u606f\u7684\u8bed\u4e49\u652f\u6301\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u7ed3\u5408\u6587\u672c\u8bed\u4e49\u7684\u5c0f\u6837\u672c\u5b66\u4e60\u6846\u67b6\uff0c\u5305\u62ec\u8bed\u4e49\u611f\u77e5\u7684\u6587\u672c-\u89c6\u89c9\u5bf9\u9f50\u6a21\u5757\u548c\u7c7b\u4f3cChain-of-Thought\u7684\u63a8\u7406\u65b9\u6cd5\u3002", "result": "\u5728ActivityNet1.3\u548cTHUMOS14\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u63a2\u7d22\u4e86\u4eba\u7c7b\u5f02\u5e38\u68c0\u6d4b\u5e94\u7528\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u901a\u8fc7\u6587\u672c\u63a8\u7406\u6709\u6548\u63d0\u5347\u4e86\u5c0f\u6837\u672cTAL\u7684\u6027\u80fd\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2504.13822", "pdf": "https://arxiv.org/pdf/2504.13822", "abs": "https://arxiv.org/abs/2504.13822", "authors": ["Eric Nuertey Coleman", "Luigi Quarantiello", "Ziyue Liu", "Qinwen Yang", "Samrat Mukherjee", "Julio Hurtado", "Vincenzo Lomonaco"], "title": "Parameter-Efficient Continual Fine-Tuning: A Survey", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The emergence of large pre-trained networks has revolutionized the AI field,\nunlocking new possibilities and achieving unprecedented performance. However,\nthese models inherit a fundamental limitation from traditional Machine Learning\napproaches: their strong dependence on the \\textit{i.i.d.} assumption hinders\ntheir adaptability to dynamic learning scenarios. We believe the next\nbreakthrough in AI lies in enabling efficient adaptation to evolving\nenvironments -- such as the real world -- where new data and tasks arrive\nsequentially. This challenge defines the field of Continual Learning (CL), a\nMachine Learning paradigm focused on developing lifelong learning neural\nmodels. One alternative to efficiently adapt these large-scale models is known\nParameter-Efficient Fine-Tuning (PEFT). These methods tackle the issue of\nadapting the model to a particular data or scenario by performing small and\nefficient modifications, achieving similar performance to full fine-tuning.\nHowever, these techniques still lack the ability to adjust the model to\nmultiple tasks continually, as they suffer from the issue of Catastrophic\nForgetting. In this survey, we first provide an overview of CL algorithms and\nPEFT methods before reviewing the state-of-the-art on Parameter-Efficient\nContinual Fine-Tuning (PECFT). We examine various approaches, discuss\nevaluation metrics, and explore potential future research directions. Our goal\nis to highlight the synergy between CL and Parameter-Efficient Fine-Tuning,\nguide researchers in this field, and pave the way for novel future research\ndirections.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u5927\u578b\u9884\u8bad\u7ec3\u7f51\u7edc\u5728\u52a8\u6001\u5b66\u4e60\u573a\u666f\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u63d0\u51fa\u4e86\u53c2\u6570\u9ad8\u6548\u6301\u7eed\u5fae\u8c03\uff08PECFT\uff09\u4f5c\u4e3a\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u7efc\u8ff0\u4e86\u76f8\u5173\u7b97\u6cd5\u4e0e\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u89e3\u51b3\u5927\u578b\u9884\u8bad\u7ec3\u7f51\u7edc\u5bf9i.i.d.\u5047\u8bbe\u7684\u4f9d\u8d56\u95ee\u9898\uff0c\u63a8\u52a8AI\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u9002\u5e94\u6027\uff0c\u5b9e\u73b0\u6301\u7eed\u5b66\u4e60\u3002", "method": "\u7efc\u8ff0\u4e86\u6301\u7eed\u5b66\u4e60\uff08CL\uff09\u7b97\u6cd5\u548c\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff08PEFT\uff09\u65b9\u6cd5\uff0c\u5e76\u63a2\u8ba8\u4e86\u53c2\u6570\u9ad8\u6548\u6301\u7eed\u5fae\u8c03\uff08PECFT\uff09\u7684\u6700\u65b0\u8fdb\u5c55\u3002", "result": "\u5206\u6790\u4e86\u591a\u79cdPECFT\u65b9\u6cd5\uff0c\u8ba8\u8bba\u4e86\u8bc4\u4f30\u6307\u6807\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "conclusion": "\u5f3a\u8c03\u4e86CL\u4e0ePEFT\u7684\u534f\u540c\u4f5c\u7528\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u6307\u5bfc\u548c\u65b0\u65b9\u5411\u3002"}}
{"id": "2504.13477", "pdf": "https://arxiv.org/pdf/2504.13477", "abs": "https://arxiv.org/abs/2504.13477", "authors": ["Sean Koon"], "title": "Creating 'Full-Stack' Hybrid Reasoning Systems that Prioritize and Enhance Human Intelligence", "categories": ["cs.HC", "cs.AI"], "comment": "10 pages; 3 figures; 1 table", "summary": "The idea of augmented or hybrid intelligence offers a compelling vision for\ncombining human and AI capabilities, especially in tasks where human wisdom,\nexpertise, or common sense are essential. Unfortunately, human reasoning can be\nflawed and shortsighted, resulting in adverse individual impacts or even\nlong-term societal consequences. While strong efforts are being made to develop\nand optimize the AI aspect of hybrid reasoning, the real urgency lies in\nfostering wiser and more intelligent human participation. Tools that enhance\ncritical thinking, ingenuity, expertise, and even wisdom could be essential in\naddressing the challenges of our emerging future. This paper proposes the\ndevelopment of generative AI-based tools that enhance both the human ability to\nreflect upon a problem as well as the ability to explore the technical aspects\nof it. A high-level model is also described for integrating AI and human\ncapabilities in a way that centralizes human participation and control.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u901a\u8fc7\u751f\u6210\u5f0fAI\u5de5\u5177\u589e\u5f3a\u4eba\u7c7b\u53cd\u601d\u548c\u6280\u672f\u63a2\u7d22\u80fd\u529b\uff0c\u4ee5\u4f18\u5316\u4eba\u673a\u534f\u4f5c\u4e2d\u7684\u667a\u6167\u53c2\u4e0e\u3002", "motivation": "\u4eba\u7c7b\u63a8\u7406\u53ef\u80fd\u5b58\u5728\u95ee\u9898\uff0c\u9700\u63d0\u5347\u4eba\u7c7b\u5728\u6df7\u5408\u667a\u80fd\u4e2d\u7684\u667a\u6167\u53c2\u4e0e\u3002", "method": "\u5f00\u53d1\u751f\u6210\u5f0fAI\u5de5\u5177\uff0c\u6574\u5408\u4eba\u673a\u80fd\u529b\uff0c\u5f3a\u8c03\u4eba\u7c7b\u4e3b\u5bfc\u3002", "result": "\u63d0\u51fa\u9ad8\u5c42\u6a21\u578b\uff0c\u96c6\u4e2d\u4eba\u7c7b\u53c2\u4e0e\u4e0e\u63a7\u5236\u3002", "conclusion": "\u589e\u5f3a\u4eba\u7c7b\u667a\u6167\u7684\u5de5\u5177\u5bf9\u672a\u6765\u6311\u6218\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2504.13201", "pdf": "https://arxiv.org/pdf/2504.13201", "abs": "https://arxiv.org/abs/2504.13201", "authors": ["Jirui Yang", "Zheyu Lin", "Shuhan Yang", "Zhihui Lu", "Xin Du"], "title": "Concept Enhancement Engineering: A Lightweight and Efficient Robust Defense Against Jailbreak Attacks in Embodied AI", "categories": ["cs.CR", "cs.LG", "cs.MA"], "comment": null, "summary": "Embodied Intelligence (EI) systems integrated with large language models\n(LLMs) face significant security risks, particularly from jailbreak attacks\nthat manipulate models into generating harmful outputs or executing unsafe\nphysical actions. Traditional defense strategies, such as input filtering and\noutput monitoring, often introduce high computational overhead or interfere\nwith task performance in real-time embodied scenarios. To address these\nchallenges, we propose Concept Enhancement Engineering (CEE), a novel defense\nframework that leverages representation engineering to enhance the safety of\nembodied LLMs by dynamically steering their internal activations. CEE operates\nby (1) extracting multilingual safety patterns from model activations, (2)\nconstructing control directions based on safety-aligned concept subspaces, and\n(3) applying subspace concept rotation to reinforce safe behavior during\ninference. Our experiments demonstrate that CEE effectively mitigates jailbreak\nattacks while maintaining task performance, outperforming existing defense\nmethods in both robustness and efficiency. This work contributes a scalable and\ninterpretable safety mechanism for embodied AI, bridging the gap between\ntheoretical representation engineering and practical security applications. Our\nfindings highlight the potential of latent-space interventions as a viable\ndefense paradigm against emerging adversarial threats in physically grounded AI\nsystems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u6982\u5ff5\u589e\u5f3a\u5de5\u7a0b\uff08CEE\uff09\u7684\u65b0\u9632\u5fa1\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u5f15\u5bfc\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u5185\u90e8\u6fc0\u6d3b\u6765\u589e\u5f3a\u5176\u5b89\u5168\u6027\uff0c\u6709\u6548\u62b5\u5fa1\u8d8a\u72f1\u653b\u51fb\uff0c\u540c\u65f6\u4fdd\u6301\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u9632\u5fa1\u7b56\u7565\uff08\u5982\u8f93\u5165\u8fc7\u6ee4\u548c\u8f93\u51fa\u76d1\u63a7\uff09\u5728\u5b9e\u65f6\u573a\u666f\u4e2d\u53ef\u80fd\u5f15\u5165\u9ad8\u8ba1\u7b97\u5f00\u9500\u6216\u5e72\u6270\u4efb\u52a1\u6027\u80fd\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u4e14\u4e0d\u5f71\u54cd\u6027\u80fd\u7684\u5b89\u5168\u673a\u5236\u3002", "method": "CEE\u901a\u8fc7\u63d0\u53d6\u591a\u8bed\u8a00\u5b89\u5168\u6a21\u5f0f\u3001\u6784\u5efa\u57fa\u4e8e\u5b89\u5168\u5bf9\u9f50\u6982\u5ff5\u5b50\u7a7a\u95f4\u7684\u63a7\u5236\u65b9\u5411\uff0c\u5e76\u5e94\u7528\u5b50\u7a7a\u95f4\u6982\u5ff5\u65cb\u8f6c\u6765\u589e\u5f3a\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u5b89\u5168\u884c\u4e3a\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cCEE\u5728\u62b5\u5fa1\u8d8a\u72f1\u653b\u51fb\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u9632\u5fa1\u65b9\u6cd5\uff0c\u540c\u65f6\u5728\u9c81\u68d2\u6027\u548c\u6548\u7387\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "CEE\u4e3a\u5177\u8eabAI\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u53ef\u89e3\u91ca\u7684\u5b89\u5168\u673a\u5236\uff0c\u586b\u8865\u4e86\u7406\u8bba\u8868\u793a\u5de5\u7a0b\u4e0e\u5b9e\u9645\u5b89\u5168\u5e94\u7528\u4e4b\u95f4\u7684\u7a7a\u767d\uff0c\u5c55\u793a\u4e86\u6f5c\u5728\u7a7a\u95f4\u5e72\u9884\u4f5c\u4e3a\u9632\u5fa1\u8303\u5f0f\u7684\u6f5c\u529b\u3002"}}
{"id": "2504.13495", "pdf": "https://arxiv.org/pdf/2504.13495", "abs": "https://arxiv.org/abs/2504.13495", "authors": ["Miit Daga", "Priyasha Mohanty", "Ram Krishna", "Swarna Priya RM"], "title": "Statistical Validation in Cultural Adaptations of Cognitive Tests: A Multi- Regional Systematic Review", "categories": ["cs.CY", "cs.AI", "cs.LG", "cs.NE"], "comment": "This paper is accepted and presented in the International Conference\n  Challenges & Opportunities in Artificial Intelligence: Engineering &\n  Management Applications (COAIEMA 2025) and to be published in Taylor &\n  Francis Proceedings", "summary": "This systematic review discusses the methodological approaches and\nstatistical confirmations of cross-cultural adaptations of cognitive evaluation\ntools used with different populations. The review considers six seminal studies\non the methodology of cultural adaptation in Europe, Asia, Africa, and South\nAmerica. The results indicate that proper adaptations need holistic models with\ndemographic changes, and education explained as much as 26.76% of the variance\nin MoCA-H scores. Cultural-linguistic factors explained 6.89% of the variance\nin European adaptations of MoCA-H; however, another study on adapted MMSE and\nBCSB among Brazilian Indigenous populations reported excellent diagnostic\nperformance, with a sensitivity of 94.4% and specificity of 99.2%. There was\n78.5% inter-rater agreement on the evaluation of cultural adaptation using the\nManchester Translation Evaluation Checklist. A paramount message of the paper\nis that community feedback is necessary for culturally appropriate preparation,\nstandardized translation protocols also must be included, along with robust\nstatistical validation methodologies for developing cognitive assessment\ninstruments. This review supplies evidence-based frameworks for the further\nadaptation of cognitive assessments in increasingly diverse global health\nsettings.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u8de8\u6587\u5316\u8ba4\u77e5\u8bc4\u4f30\u5de5\u5177\u7684\u65b9\u6cd5\u8bba\u4e0e\u7edf\u8ba1\u9a8c\u8bc1\uff0c\u5f3a\u8c03\u6587\u5316\u9002\u5e94\u9700\u7ed3\u5408\u793e\u533a\u53cd\u9988\u3001\u6807\u51c6\u5316\u7ffb\u8bd1\u548c\u7edf\u8ba1\u9a8c\u8bc1\u3002", "motivation": "\u63a2\u8ba8\u4e0d\u540c\u6587\u5316\u80cc\u666f\u4e0b\u8ba4\u77e5\u8bc4\u4f30\u5de5\u5177\u7684\u9002\u5e94\u6027\u4e0e\u6709\u6548\u6027\uff0c\u4e3a\u5168\u7403\u591a\u6837\u5316\u5065\u5eb7\u73af\u5883\u63d0\u4f9b\u4f9d\u636e\u3002", "method": "\u5206\u6790\u4e86\u6b27\u6d32\u3001\u4e9a\u6d32\u3001\u975e\u6d32\u548c\u5357\u7f8e\u6d32\u7684\u516d\u9879\u7814\u7a76\uff0c\u8bc4\u4f30\u6587\u5316\u9002\u5e94\u65b9\u6cd5\u53ca\u7edf\u8ba1\u9a8c\u8bc1\u7ed3\u679c\u3002", "result": "\u6559\u80b2\u89e3\u91ca\u4e8626.76%\u7684MoCA-H\u5206\u6570\u5dee\u5f02\uff0c\u6587\u5316\u8bed\u8a00\u56e0\u7d20\u5728\u6b27\u6d32\u9002\u5e94\u4e2d\u53606.89%\u3002\u5df4\u897f\u571f\u8457\u4eba\u7fa4\u7684MMSE\u548cBCSB\u8868\u73b0\u51fa\u8272\uff08\u654f\u611f\u602794.4%\uff0c\u7279\u5f02\u602799.2%\uff09\u3002", "conclusion": "\u6587\u5316\u9002\u5e94\u9700\u7ed3\u5408\u793e\u533a\u53cd\u9988\u3001\u6807\u51c6\u5316\u7ffb\u8bd1\u548c\u7edf\u8ba1\u9a8c\u8bc1\uff0c\u4e3a\u5168\u7403\u8ba4\u77e5\u8bc4\u4f30\u5de5\u5177\u5f00\u53d1\u63d0\u4f9b\u6846\u67b6\u3002"}}
{"id": "2504.13515", "pdf": "https://arxiv.org/pdf/2504.13515", "abs": "https://arxiv.org/abs/2504.13515", "authors": ["Mingwei Zheng", "Danning Xie", "Xiangyu Zhang"], "title": "Large Language Models for Validating Network Protocol Parsers", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Network protocol parsers are essential for enabling correct and secure\ncommunication between devices. Bugs in these parsers can introduce critical\nvulnerabilities, including memory corruption, information leakage, and\ndenial-of-service attacks. An intuitive way to assess parser correctness is to\ncompare the implementation with its official protocol standard. However, this\ncomparison is challenging because protocol standards are typically written in\nnatural language, whereas implementations are in source code. Existing methods\nlike model checking, fuzzing, and differential testing have been used to find\nparsing bugs, but they either require significant manual effort or ignore the\nprotocol standards, limiting their ability to detect semantic violations. To\nenable more automated validation of parser implementations against protocol\nstandards, we propose PARVAL, a multi-agent framework built on large language\nmodels (LLMs). PARVAL leverages the capabilities of LLMs to understand both\nnatural language and code. It transforms both protocol standards and their\nimplementations into a unified intermediate representation, referred to as\nformat specifications, and performs a differential comparison to uncover\ninconsistencies. We evaluate PARVAL on the Bidirectional Forwarding Detection\n(BFD) protocol. Our experiments demonstrate that PARVAL successfully identifies\ninconsistencies between the implementation and its RFC standard, achieving a\nlow false positive rate of 5.6%. PARVAL uncovers seven unique bugs, including\nfive previously unknown issues.", "AI": {"tldr": "PARVAL\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u9a8c\u8bc1\u7f51\u7edc\u534f\u8bae\u89e3\u6790\u5668\u7684\u5b9e\u73b0\u662f\u5426\u7b26\u5408\u534f\u8bae\u6807\u51c6\uff0c\u6210\u529f\u8bc6\u522b\u4e86BFD\u534f\u8bae\u4e2d\u7684\u4e0d\u4e00\u81f4\u6027\u548c\u6f0f\u6d1e\u3002", "motivation": "\u7f51\u7edc\u534f\u8bae\u89e3\u6790\u5668\u7684\u9519\u8bef\u53ef\u80fd\u5bfc\u81f4\u4e25\u91cd\u6f0f\u6d1e\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u81ea\u52a8\u5316\u9a8c\u8bc1\u81ea\u7136\u8bed\u8a00\u534f\u8bae\u6807\u51c6\u4e0e\u4ee3\u7801\u5b9e\u73b0\u7684\u4e00\u81f4\u6027\u3002", "method": "PARVAL\u5229\u7528LLM\u7406\u89e3\u81ea\u7136\u8bed\u8a00\u548c\u4ee3\u7801\uff0c\u5c06\u534f\u8bae\u6807\u51c6\u548c\u5b9e\u73b0\u8f6c\u5316\u4e3a\u7edf\u4e00\u7684\u4e2d\u95f4\u8868\u793a\uff08\u683c\u5f0f\u89c4\u8303\uff09\uff0c\u5e76\u8fdb\u884c\u5dee\u5f02\u6bd4\u8f83\u3002", "result": "\u5728BFD\u534f\u8bae\u4e0a\uff0cPARVAL\u5b9e\u73b0\u4e865.6%\u7684\u4f4e\u8bef\u62a5\u7387\uff0c\u53d1\u73b0\u4e867\u4e2a\u72ec\u7279\u6f0f\u6d1e\uff0c\u5176\u4e2d5\u4e2a\u662f\u672a\u77e5\u95ee\u9898\u3002", "conclusion": "PARVAL\u4e3a\u81ea\u52a8\u5316\u9a8c\u8bc1\u534f\u8bae\u89e3\u6790\u5668\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6f0f\u6d1e\u68c0\u6d4b\u80fd\u529b\u3002"}}
{"id": "2504.13541", "pdf": "https://arxiv.org/pdf/2504.13541", "abs": "https://arxiv.org/abs/2504.13541", "authors": ["Avaneesh Devkota", "Rachmad Vidya Wicaksana Putra", "Muhammad Shafique"], "title": "SwitchMT: An Adaptive Context Switching Methodology for Scalable Multi-Task Learning in Intelligent Autonomous Agents", "categories": ["cs.NE", "cs.AI", "cs.LG", "cs.RO"], "comment": "7 pages, 7 figures, 3 tables", "summary": "The ability to train intelligent autonomous agents (such as mobile robots) on\nmultiple tasks is crucial for adapting to dynamic real-world environments.\nHowever, state-of-the-art reinforcement learning (RL) methods only excel in\nsingle-task settings, and still struggle to generalize across multiple tasks\ndue to task interference. Moreover, real-world environments also demand the\nagents to have data stream processing capabilities. Toward this, a\nstate-of-the-art work employs Spiking Neural Networks (SNNs) to improve\nmulti-task learning by exploiting temporal information in data stream, while\nenabling lowpower/energy event-based operations. However, it relies on fixed\ncontext/task-switching intervals during its training, hence limiting the\nscalability and effectiveness of multi-task learning. To address these\nlimitations, we propose SwitchMT, a novel adaptive task-switching methodology\nfor RL-based multi-task learning in autonomous agents. Specifically, SwitchMT\nemploys the following key ideas: (1) a Deep Spiking Q-Network with active\ndendrites and dueling structure, that utilizes task-specific context signals to\ncreate specialized sub-networks; and (2) an adaptive task-switching policy that\nleverages both rewards and internal dynamics of the network parameters.\nExperimental results demonstrate that SwitchMT achieves superior performance in\nmulti-task learning compared to state-of-the-art methods. It achieves\ncompetitive scores in multiple Atari games (i.e., Pong: -8.8, Breakout: 5.6,\nand Enduro: 355.2) compared to the state-of-the-art, showing its better\ngeneralized learning capability. These results highlight the effectiveness of\nour SwitchMT methodology in addressing task interference while enabling\nmulti-task learning automation through adaptive task switching, thereby paving\nthe way for more efficient generalist agents with scalable multi-task learning\ncapabilities.", "AI": {"tldr": "SwitchMT\u662f\u4e00\u79cd\u81ea\u9002\u5e94\u4efb\u52a1\u5207\u6362\u65b9\u6cd5\uff0c\u7528\u4e8e\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u591a\u4efb\u52a1\u5b66\u4e60\uff0c\u901a\u8fc7\u6df1\u5ea6\u8109\u51b2Q\u7f51\u7edc\u548c\u81ea\u9002\u5e94\u7b56\u7565\u89e3\u51b3\u4efb\u52a1\u5e72\u6270\u95ee\u9898\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u591a\u4efb\u52a1\u5b66\u4e60\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u4e14\u56fa\u5b9a\u4efb\u52a1\u5207\u6362\u95f4\u9694\u9650\u5236\u4e86\u53ef\u6269\u5c55\u6027\u3002SwitchMT\u65e8\u5728\u901a\u8fc7\u81ea\u9002\u5e94\u4efb\u52a1\u5207\u6362\u548c\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u63d0\u5347\u591a\u4efb\u52a1\u5b66\u4e60\u6548\u679c\u3002", "method": "SwitchMT\u7ed3\u5408\u6df1\u5ea6\u8109\u51b2Q\u7f51\u7edc\uff08\u5177\u6709\u4e3b\u52a8\u6811\u7a81\u548c\u5bf9\u5076\u7ed3\u6784\uff09\u548c\u81ea\u9002\u5e94\u4efb\u52a1\u5207\u6362\u7b56\u7565\uff0c\u5229\u7528\u4efb\u52a1\u7279\u5b9a\u4e0a\u4e0b\u6587\u4fe1\u53f7\u548c\u7f51\u7edc\u53c2\u6570\u52a8\u6001\u8c03\u6574\u3002", "result": "SwitchMT\u5728\u591a\u4e2aAtari\u6e38\u620f\u4e2d\u8868\u73b0\u4f18\u5f02\uff08\u5982Pong: -8.8, Breakout: 5.6, Enduro: 355.2\uff09\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "SwitchMT\u6709\u6548\u89e3\u51b3\u4e86\u4efb\u52a1\u5e72\u6270\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u591a\u4efb\u52a1\u5b66\u4e60\u7684\u81ea\u52a8\u5316\u80fd\u529b\uff0c\u4e3a\u9ad8\u6548\u901a\u7528\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2504.13220", "pdf": "https://arxiv.org/pdf/2504.13220", "abs": "https://arxiv.org/abs/2504.13220", "authors": ["Ummay Maria Muna", "Md. Mehedi Hasan Shawon", "Md Jobayer", "Sumaiya Akter", "Saifur Rahman Sabuj"], "title": "SSTAF: Spatial-Spectral-Temporal Attention Fusion Transformer for Motor Imagery Classification", "categories": ["cs.CV", "cs.LG", "68T10 (Primary) 68T07(Secondary)", "I.2.1; I.5.4"], "comment": "11 pages", "summary": "Brain-computer interfaces (BCI) in electroencephalography (EEG)-based motor\nimagery classification offer promising solutions in neurorehabilitation and\nassistive technologies by enabling communication between the brain and external\ndevices. However, the non-stationary nature of EEG signals and significant\ninter-subject variability cause substantial challenges for developing robust\ncross-subject classification models. This paper introduces a novel\nSpatial-Spectral-Temporal Attention Fusion (SSTAF) Transformer specifically\ndesigned for upper-limb motor imagery classification. Our architecture consists\nof a spectral transformer and a spatial transformer, followed by a transformer\nblock and a classifier network. Each module is integrated with attention\nmechanisms that dynamically attend to the most discriminative patterns across\nmultiple domains, such as spectral frequencies, spatial electrode locations,\nand temporal dynamics. The short-time Fourier transform is incorporated to\nextract features in the time-frequency domain to make it easier for the model\nto obtain a better feature distinction. We evaluated our SSTAF Transformer\nmodel on two publicly available datasets, the EEGMMIDB dataset, and BCI\nCompetition IV-2a. SSTAF Transformer achieves an accuracy of 76.83% and 68.30%\nin the data sets, respectively, outperforms traditional CNN-based architectures\nand a few existing transformer-based approaches.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684SSTAF Transformer\u6a21\u578b\uff0c\u7528\u4e8e\u89e3\u51b3EEG\u4fe1\u53f7\u7684\u975e\u5e73\u7a33\u6027\u548c\u8de8\u88ab\u8bd5\u53d8\u5f02\u6027\u95ee\u9898\uff0c\u5728\u8fd0\u52a8\u60f3\u8c61\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "EEG\u4fe1\u53f7\u7684\u975e\u5e73\u7a33\u6027\u548c\u88ab\u8bd5\u95f4\u53d8\u5f02\u6027\u5bf9\u8de8\u88ab\u8bd5\u5206\u7c7b\u6a21\u578b\u63d0\u51fa\u4e86\u6311\u6218\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9c81\u68d2\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528SSTAF Transformer\u67b6\u6784\uff0c\u7ed3\u5408\u7a7a\u95f4\u3001\u9891\u8c31\u548c\u65f6\u95f4\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5e76\u5229\u7528\u77ed\u65f6\u5085\u91cc\u53f6\u53d8\u6362\u63d0\u53d6\u65f6\u9891\u7279\u5f81\u3002", "result": "\u5728\u4e24\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u5206\u522b\u8fbe\u523076.83%\u548c68.30%\u7684\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u4f20\u7edfCNN\u548c\u73b0\u6709Transformer\u65b9\u6cd5\u3002", "conclusion": "SSTAF Transformer\u5728\u8fd0\u52a8\u60f3\u8c61\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u795e\u7ecf\u5eb7\u590d\u548c\u8f85\u52a9\u6280\u672f\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.13548", "pdf": "https://arxiv.org/pdf/2504.13548", "abs": "https://arxiv.org/abs/2504.13548", "authors": ["Haoyang Luo", "Linwei Tao", "Minjing Dong", "Chang Xu"], "title": "Beyond One-Hot Labels: Semantic Mixing for Model Calibration", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Model calibration seeks to ensure that models produce confidence scores that\naccurately reflect the true likelihood of their predictions being correct.\nHowever, existing calibration approaches are fundamentally tied to datasets of\none-hot labels implicitly assuming full certainty in all the annotations. Such\ndatasets are effective for classification but provides insufficient knowledge\nof uncertainty for model calibration, necessitating the curation of datasets\nwith numerically rich ground-truth confidence values. However, due to the\nscarcity of uncertain visual examples, such samples are not easily available as\nreal datasets. In this paper, we introduce calibration-aware data augmentation\nto create synthetic datasets of diverse samples and their ground-truth\nuncertainty. Specifically, we present Calibration-aware Semantic Mixing (CSM),\na novel framework that generates training samples with mixed class\ncharacteristics and annotates them with distinct confidence scores via\ndiffusion models. Based on this framework, we propose calibrated reannotation\nto tackle the misalignment between the annotated confidence score and the\nmixing ratio during the diffusion reverse process. Besides, we explore the loss\nfunctions that better fit the new data representation paradigm. Experimental\nresults demonstrate that CSM achieves superior calibration compared to the\nstate-of-the-art calibration approaches. Code is available at\ngithub.com/E-Galois/CSM.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6570\u636e\u589e\u5f3a\u7684\u6a21\u578b\u6821\u51c6\u65b9\u6cd5CSM\uff0c\u901a\u8fc7\u751f\u6210\u6df7\u5408\u7c7b\u522b\u7684\u5408\u6210\u6837\u672c\u53ca\u5176\u771f\u5b9e\u4e0d\u786e\u5b9a\u6027\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6821\u51c6\u65b9\u6cd5\u4f9d\u8d56\u786e\u5b9a\u6027\u6807\u7b7e\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u6821\u51c6\u65b9\u6cd5\u4f9d\u8d56\u786e\u5b9a\u6027\u6807\u7b7e\uff0c\u65e0\u6cd5\u5145\u5206\u53cd\u6620\u6a21\u578b\u9884\u6d4b\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u591f\u751f\u6210\u5e26\u6709\u4e30\u5bcc\u4e0d\u786e\u5b9a\u6027\u6807\u6ce8\u6570\u636e\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faCSM\u6846\u67b6\uff0c\u5229\u7528\u6269\u6563\u6a21\u578b\u751f\u6210\u6df7\u5408\u7c7b\u522b\u6837\u672c\u5e76\u6807\u6ce8\u7f6e\u4fe1\u5ea6\uff0c\u540c\u65f6\u5f15\u5165\u6821\u51c6\u91cd\u6807\u6ce8\u89e3\u51b3\u6807\u6ce8\u4e0e\u6df7\u5408\u6bd4\u4f8b\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cCSM\u5728\u6821\u51c6\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "CSM\u901a\u8fc7\u5408\u6210\u6570\u636e\u548c\u6821\u51c6\u91cd\u6807\u6ce8\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6821\u51c6\u6548\u679c\u3002"}}
{"id": "2504.13232", "pdf": "https://arxiv.org/pdf/2504.13232", "abs": "https://arxiv.org/abs/2504.13232", "authors": ["Sayed Pouria Talebi", "Clive Cheong Took", "Danilo P. Mandic"], "title": "A Quantum of Learning: Using Quaternion Algebra to Model Learning on Quantum Devices", "categories": ["quant-ph", "cs.LG", "math.QA", "stat.ML"], "comment": null, "summary": "This article considers the problem of designing adaption and optimisation\ntechniques for training quantum learning machines. To this end, the division\nalgebra of quaternions is used to derive an effective model for representing\ncomputation and measurement operations on qubits. In turn, the derived model,\nserves as the foundation for formulating an adaptive learning problem on\nprincipal quantum learning units, thereby establishing quantum information\nprocessing units akin to that of neurons in classical approaches. Then,\nleveraging the modern HR-calculus, a comprehensive training framework for\nlearning on quantum machines is developed. The quaternion-valued model\naccommodates mathematical tractability and establishment of performance\ncriteria, such as convergence conditions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56db\u5143\u6570\u7684\u91cf\u5b50\u5b66\u4e60\u673a\u5668\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7HR-\u5fae\u79ef\u5206\u5f00\u53d1\u4e86\u5168\u9762\u7684\u8bad\u7ec3\u6846\u67b6\u3002", "motivation": "\u8bbe\u8ba1\u9002\u5e94\u6027\u548c\u4f18\u5316\u6280\u672f\u4ee5\u8bad\u7ec3\u91cf\u5b50\u5b66\u4e60\u673a\u5668\uff0c\u5efa\u7acb\u7c7b\u4f3c\u7ecf\u5178\u795e\u7ecf\u5143\u7684\u4fe1\u606f\u5904\u7406\u5355\u5143\u3002", "method": "\u5229\u7528\u56db\u5143\u6570\u4ee3\u6570\u5efa\u6a21\u91cf\u5b50\u8ba1\u7b97\u548c\u6d4b\u91cf\u64cd\u4f5c\uff0c\u5e76\u57fa\u4e8eHR-\u5fae\u79ef\u5206\u5f00\u53d1\u8bad\u7ec3\u6846\u67b6\u3002", "result": "\u6a21\u578b\u5177\u6709\u6570\u5b66\u53ef\u64cd\u4f5c\u6027\uff0c\u5e76\u5efa\u7acb\u4e86\u6027\u80fd\u6807\u51c6\uff08\u5982\u6536\u655b\u6761\u4ef6\uff09\u3002", "conclusion": "\u56db\u5143\u6570\u6a21\u578b\u4e3a\u91cf\u5b50\u5b66\u4e60\u673a\u5668\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u8bad\u7ec3\u57fa\u7840\u3002"}}
{"id": "2504.13282", "pdf": "https://arxiv.org/pdf/2504.13282", "abs": "https://arxiv.org/abs/2504.13282", "authors": ["Jiang-Xin Shi", "Tong Wei", "Yu-Feng Li"], "title": "LIFT+: Lightweight Fine-Tuning for Long-Tail Learning", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "The fine-tuning paradigm has emerged as a prominent approach for addressing\nlong-tail learning tasks in the era of foundation models. However, the impact\nof fine-tuning strategies on long-tail learning performance remains unexplored.\nIn this work, we disclose that existing paradigms exhibit a profound misuse of\nfine-tuning methods, leaving significant room for improvement in both\nefficiency and accuracy. Specifically, we reveal that heavy fine-tuning\n(fine-tuning a large proportion of model parameters) can lead to non-negligible\nperformance deterioration on tail classes, whereas lightweight fine-tuning\ndemonstrates superior effectiveness. Through comprehensive theoretical and\nempirical validation, we identify this phenomenon as stemming from inconsistent\nclass conditional distributions induced by heavy fine-tuning. Building on this\ninsight, we propose LIFT+, an innovative lightweight fine-tuning framework to\noptimize consistent class conditions. Furthermore, LIFT+ incorporates\nsemantic-aware initialization, minimalist data augmentation, and test-time\nensembling to enhance adaptation and generalization of foundation models. Our\nframework provides an efficient and accurate pipeline that facilitates fast\nconvergence and model compactness. Extensive experiments demonstrate that LIFT+\nsignificantly reduces both training epochs (from $\\sim$100 to $\\leq$15) and\nlearned parameters (less than 1%), while surpassing state-of-the-art approaches\nby a considerable margin. The source code is available at\nhttps://github.com/shijxcs/LIFT-plus.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faLIFT+\u6846\u67b6\uff0c\u63ed\u793a\u4f20\u7edf\u5fae\u8c03\u65b9\u6cd5\u5728\u957f\u5c3e\u5b66\u4e60\u4e2d\u7684\u4e0d\u8db3\uff0c\u8f7b\u91cf\u5fae\u8c03\u8868\u73b0\u66f4\u4f18\uff0c\u663e\u8457\u63d0\u5347\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u63a2\u7d22\u5fae\u8c03\u7b56\u7565\u5bf9\u957f\u5c3e\u5b66\u4e60\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u6548\u7387\u4e0e\u51c6\u786e\u6027\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faLIFT+\u6846\u67b6\uff0c\u7ed3\u5408\u8bed\u4e49\u611f\u77e5\u521d\u59cb\u5316\u3001\u6781\u7b80\u6570\u636e\u589e\u5f3a\u548c\u6d4b\u8bd5\u65f6\u96c6\u6210\uff0c\u4f18\u5316\u4e00\u81f4\u7c7b\u522b\u6761\u4ef6\u3002", "result": "LIFT+\u5927\u5e45\u51cf\u5c11\u8bad\u7ec3\u8f6e\u6b21\uff08\u4ece\u7ea6100\u964d\u81f3\u226415\uff09\u548c\u53c2\u6570\uff08<1%\uff09\uff0c\u6027\u80fd\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "LIFT+\u4e3a\u9ad8\u6548\u51c6\u786e\u7684\u957f\u5c3e\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u5fae\u8c03\u65b9\u6cd5\u3002"}}
{"id": "2504.13320", "pdf": "https://arxiv.org/pdf/2504.13320", "abs": "https://arxiv.org/abs/2504.13320", "authors": ["Robert Gruhlke", "Matei Hanu", "Claudia Schillings", "Philipp Wacker"], "title": "Gradient-Free Sequential Bayesian Experimental Design via Interacting Particle Systems", "categories": ["stat.ML", "cs.LG", "cs.NA", "math.NA", "stat.CO", "62K05, 62F15, 65C05, 93E10"], "comment": null, "summary": "We introduce a gradient-free framework for Bayesian Optimal Experimental\nDesign (BOED) in sequential settings, aimed at complex systems where gradient\ninformation is unavailable. Our method combines Ensemble Kalman Inversion (EKI)\nfor design optimization with the Affine-Invariant Langevin Dynamics (ALDI)\nsampler for efficient posterior sampling-both of which are derivative-free and\nensemble-based. To address the computational challenges posed by nested\nexpectations in BOED, we propose variational Gaussian and parametrized Laplace\napproximations that provide tractable upper and lower bounds on the Expected\nInformation Gain (EIG). These approximations enable scalable utility estimation\nin high-dimensional spaces and PDE-constrained inverse problems. We demonstrate\nthe performance of our framework through numerical experiments ranging from\nlinear Gaussian models to PDE-based inference tasks, highlighting the method's\nrobustness, accuracy, and efficiency in information-driven experimental design.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u68af\u5ea6\u7684\u8d1d\u53f6\u65af\u6700\u4f18\u5b9e\u9a8c\u8bbe\u8ba1\u6846\u67b6\uff0c\u7ed3\u5408EKI\u548cALDI\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u9ad8\u7ef4\u7a7a\u95f4\u548cPDE\u7ea6\u675f\u95ee\u9898\u4e2d\u7684\u8ba1\u7b97\u6311\u6218\u3002", "motivation": "\u9488\u5bf9\u590d\u6742\u7cfb\u7edf\u4e2d\u68af\u5ea6\u4fe1\u606f\u4e0d\u53ef\u7528\u7684\u60c5\u51b5\uff0c\u5f00\u53d1\u4e00\u79cd\u65e0\u68af\u5ea6\u7684\u8d1d\u53f6\u65af\u6700\u4f18\u5b9e\u9a8c\u8bbe\u8ba1\u65b9\u6cd5\u3002", "method": "\u7ed3\u5408Ensemble Kalman Inversion (EKI)\u8fdb\u884c\u8bbe\u8ba1\u4f18\u5316\uff0c\u4f7f\u7528Affine-Invariant Langevin Dynamics (ALDI)\u8fdb\u884c\u9ad8\u6548\u540e\u9a8c\u91c7\u6837\uff0c\u5e76\u63d0\u51fa\u53d8\u5206\u9ad8\u65af\u548c\u53c2\u6570\u5316\u62c9\u666e\u62c9\u65af\u8fd1\u4f3c\u3002", "result": "\u5728\u4ece\u7ebf\u6027\u9ad8\u65af\u6a21\u578b\u5230PDE\u7ea6\u675f\u63a8\u7406\u4efb\u52a1\u7684\u6570\u503c\u5b9e\u9a8c\u4e2d\uff0c\u5c55\u793a\u4e86\u65b9\u6cd5\u7684\u9c81\u68d2\u6027\u3001\u51c6\u786e\u6027\u548c\u9ad8\u6548\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u4fe1\u606f\u9a71\u52a8\u7684\u5b9e\u9a8c\u8bbe\u8ba1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u9002\u7528\u4e8e\u9ad8\u7ef4\u7a7a\u95f4\u548c\u590d\u6742\u7cfb\u7edf\u3002"}}
{"id": "2504.13560", "pdf": "https://arxiv.org/pdf/2504.13560", "abs": "https://arxiv.org/abs/2504.13560", "authors": ["SoYoung Park", "Hyewon Lee", "Mingyu Choi", "Seunghoon Han", "Jong-Ryul Lee", "Sungsu Lim", "Tae-Ho Kim"], "title": "Zero-Shot Industrial Anomaly Segmentation with Image-Aware Prompt Generation", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted to PAKDD 2025, 12 pages", "summary": "Anomaly segmentation is essential for industrial quality, maintenance, and\nstability. Existing text-guided zero-shot anomaly segmentation models are\neffective but rely on fixed prompts, limiting adaptability in diverse\nindustrial scenarios. This highlights the need for flexible, context-aware\nprompting strategies. We propose Image-Aware Prompt Anomaly Segmentation\n(IAP-AS), which enhances anomaly segmentation by generating dynamic,\ncontext-aware prompts using an image tagging model and a large language model\n(LLM). IAP-AS extracts object attributes from images to generate context-aware\nprompts, improving adaptability and generalization in dynamic and unstructured\nindustrial environments. In our experiments, IAP-AS improves the F1-max metric\nby up to 10%, demonstrating superior adaptability and generalization. It\nprovides a scalable solution for anomaly segmentation across industries", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u52a8\u6001\u63d0\u793a\u7684\u5f02\u5e38\u5206\u5272\u65b9\u6cd5IAP-AS\uff0c\u901a\u8fc7\u56fe\u50cf\u6807\u7b7e\u6a21\u578b\u548c\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u4e0a\u4e0b\u6587\u611f\u77e5\u63d0\u793a\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5de5\u4e1a\u573a\u666f\u4e2d\u7684\u9002\u5e94\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u56fa\u5b9a\u63d0\u793a\u7684\u96f6\u6837\u672c\u5f02\u5e38\u5206\u5272\u65b9\u6cd5\u5728\u591a\u6837\u5316\u5de5\u4e1a\u573a\u666f\u4e2d\u9002\u5e94\u6027\u4e0d\u8db3\uff0c\u9700\u8981\u66f4\u7075\u6d3b\u7684\u63d0\u793a\u7b56\u7565\u3002", "method": "\u7ed3\u5408\u56fe\u50cf\u6807\u7b7e\u6a21\u578b\u548cLLM\uff0c\u4ece\u56fe\u50cf\u4e2d\u63d0\u53d6\u5bf9\u8c61\u5c5e\u6027\u5e76\u751f\u6210\u52a8\u6001\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u63d0\u793a\u3002", "result": "\u5b9e\u9a8c\u663e\u793aIAP-AS\u5728F1-max\u6307\u6807\u4e0a\u63d0\u5347\u9ad8\u8fbe10%\uff0c\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u9002\u5e94\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "IAP-AS\u4e3a\u8de8\u884c\u4e1a\u5f02\u5e38\u5206\u5272\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.13333", "pdf": "https://arxiv.org/pdf/2504.13333", "abs": "https://arxiv.org/abs/2504.13333", "authors": ["Ludovico T. Giorgini", "Fabrizio Falasca", "Andre N. Souza"], "title": "Predicting Forced Responses of Probability Distributions via the Fluctuation-Dissipation Theorem and Generative Modeling", "categories": ["stat.ML", "cs.LG", "nlin.CD"], "comment": null, "summary": "We present a novel data-driven framework for estimating the response of\nhigher-order moments of nonlinear stochastic systems to small external\nperturbations. The classical Generalized Fluctuation-Dissipation Theorem (GFDT)\nlinks the unperturbed steady-state distribution to the system's linear\nresponse. Standard implementations rely on Gaussian approximations, which can\noften accurately predict the mean response but usually introduce significant\nbiases in higher-order moments, such as variance, skewness, and kurtosis. To\naddress this limitation, we combine GFDT with recent advances in score-based\ngenerative modeling, which enable direct estimation of the score function from\ndata without requiring full density reconstruction. Our method is validated on\nthree reduced-order stochastic models relevant to climate dynamics: a scalar\nstochastic model for low-frequency climate variability, a slow-fast triad model\nmimicking key features of the El Nino-Southern Oscillation (ENSO), and a\nsix-dimensional stochastic barotropic model capturing atmospheric regime\ntransitions. In all cases, the approach captures strongly nonlinear and\nnon-Gaussian features of the system's response, outperforming traditional\nGaussian approximations.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6570\u636e\u9a71\u52a8\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u4f30\u8ba1\u975e\u7ebf\u6027\u968f\u673a\u7cfb\u7edf\u9ad8\u9636\u77e9\u5bf9\u5916\u90e8\u6270\u52a8\u7684\u54cd\u5e94\uff0c\u7ed3\u5408GFDT\u548c\u57fa\u4e8e\u5206\u6570\u7684\u751f\u6210\u5efa\u6a21\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u9ad8\u65af\u8fd1\u4f3c\u3002", "motivation": "\u4f20\u7edfGFDT\u4f9d\u8d56\u9ad8\u65af\u8fd1\u4f3c\uff0c\u867d\u80fd\u51c6\u786e\u9884\u6d4b\u5747\u503c\u54cd\u5e94\uff0c\u4f46\u5728\u9ad8\u9636\u77e9\uff08\u5982\u65b9\u5dee\u3001\u504f\u5ea6\u548c\u5cf0\u5ea6\uff09\u4e0a\u5b58\u5728\u663e\u8457\u504f\u5dee\uff0c\u9700\u6539\u8fdb\u3002", "method": "\u5c06GFDT\u4e0e\u57fa\u4e8e\u5206\u6570\u7684\u751f\u6210\u5efa\u6a21\u7ed3\u5408\uff0c\u76f4\u63a5\u4ece\u6570\u636e\u4f30\u8ba1\u5206\u6570\u51fd\u6570\uff0c\u65e0\u9700\u5b8c\u6574\u5bc6\u5ea6\u91cd\u5efa\u3002", "result": "\u5728\u4e09\u4e2a\u6c14\u5019\u52a8\u529b\u5b66\u76f8\u5173\u7684\u968f\u673a\u6a21\u578b\u4e2d\u9a8c\u8bc1\uff0c\u65b0\u65b9\u6cd5\u6210\u529f\u6355\u6349\u4e86\u7cfb\u7edf\u7684\u5f3a\u975e\u7ebf\u6027\u548c\u975e\u9ad8\u65af\u7279\u5f81\u3002", "conclusion": "\u65b0\u6846\u67b6\u5728\u9ad8\u9636\u77e9\u54cd\u5e94\u4f30\u8ba1\u4e0a\u4f18\u4e8e\u4f20\u7edf\u9ad8\u65af\u8fd1\u4f3c\uff0c\u9002\u7528\u4e8e\u590d\u6742\u975e\u7ebf\u6027\u7cfb\u7edf\u3002"}}
{"id": "2504.13568", "pdf": "https://arxiv.org/pdf/2504.13568", "abs": "https://arxiv.org/abs/2504.13568", "authors": ["Runzhen Xue", "Hao Wu", "Mingyu Yan", "Ziheng Xiao", "Xiaochun Ye", "Dongrui Fan"], "title": "MetaDSE: A Few-shot Meta-learning Framework for Cross-workload CPU Design Space Exploration", "categories": ["cs.AR", "cs.AI"], "comment": "7 pages, 6 figures. Accepted by DAC 2025", "summary": "Cross-workload design space exploration (DSE) is crucial in CPU architecture\ndesign. Existing DSE methods typically employ the transfer learning technique\nto leverage knowledge from source workloads, aiming to minimize the requirement\nof target workload simulation. However, these methods struggle with\noverfitting, data ambiguity, and workload dissimilarity.\n  To address these challenges, we reframe the cross-workload CPU DSE task as a\nfew-shot meta-learning problem and further introduce MetaDSE. By leveraging\nmodel agnostic meta-learning, MetaDSE swiftly adapts to new target workloads,\ngreatly enhancing the efficiency of cross-workload CPU DSE. Additionally,\nMetaDSE introduces a novel knowledge transfer method called the\nworkload-adaptive architectural mask algorithm, which uncovers the inherent\nproperties of the architecture. Experiments on SPEC CPU 2017 demonstrate that\nMetaDSE significantly reduces prediction error by 44.3\\% compared to the\nstate-of-the-art. MetaDSE is open-sourced and available at this\n\\href{https://anonymous.4open.science/r/Meta_DSE-02F8}{anonymous GitHub.}", "AI": {"tldr": "MetaDSE\u5c06\u8de8\u5de5\u4f5c\u8d1f\u8f7dCPU\u8bbe\u8ba1\u7a7a\u95f4\u63a2\u7d22\u4efb\u52a1\u91cd\u65b0\u5b9a\u4e49\u4e3a\u5c11\u6837\u672c\u5143\u5b66\u4e60\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u8de8\u5de5\u4f5c\u8d1f\u8f7d\u8bbe\u8ba1\u7a7a\u95f4\u63a2\u7d22\u4e2d\u5b58\u5728\u8fc7\u62df\u5408\u3001\u6570\u636e\u6a21\u7cca\u548c\u5de5\u4f5c\u8d1f\u8f7d\u5dee\u5f02\u7b49\u95ee\u9898\u3002", "method": "\u91c7\u7528\u6a21\u578b\u65e0\u5173\u5143\u5b66\u4e60\uff0c\u5e76\u5f15\u5165\u5de5\u4f5c\u8d1f\u8f7d\u81ea\u9002\u5e94\u67b6\u6784\u63a9\u7801\u7b97\u6cd5\u3002", "result": "\u5728SPEC CPU 2017\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u9884\u6d4b\u8bef\u5dee\u964d\u4f4e\u4e8644.3%\u3002", "conclusion": "MetaDSE\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u8de8\u5de5\u4f5c\u8d1f\u8f7dCPU\u8bbe\u8ba1\u7a7a\u95f4\u63a2\u7d22\u65b9\u6cd5\uff0c\u5df2\u5f00\u6e90\u3002"}}
{"id": "2504.13336", "pdf": "https://arxiv.org/pdf/2504.13336", "abs": "https://arxiv.org/abs/2504.13336", "authors": ["Lea Kunkel", "Mathias Trabs"], "title": "On the minimax optimality of Flow Matching through the connection to kernel density estimation", "categories": ["stat.ML", "cs.LG", "62E17, 62G07, 68T07"], "comment": null, "summary": "Flow Matching has recently gained attention in generative modeling as a\nsimple and flexible alternative to diffusion models, the current state of the\nart. While existing statistical guarantees adapt tools from the analysis of\ndiffusion models, we take a different perspective by connecting Flow Matching\nto kernel density estimation. We first verify that the kernel density estimator\nmatches the optimal rate of convergence in Wasserstein distance up to\nlogarithmic factors, improving existing bounds for the Gaussian kernel. Based\non this result, we prove that for sufficiently large networks, Flow Matching\nalso achieves the optimal rate up to logarithmic factors, providing a\ntheoretical foundation for the empirical success of this method. Finally, we\nprovide a first justification of Flow Matching's effectiveness in\nhigh-dimensional settings by showing that rates improve when the target\ndistribution lies on a lower-dimensional linear subspace.", "AI": {"tldr": "Flow Matching\u662f\u4e00\u79cd\u65b0\u5174\u7684\u751f\u6210\u6a21\u578b\u65b9\u6cd5\uff0c\u672c\u6587\u901a\u8fc7\u5c06\u5176\u4e0e\u6838\u5bc6\u5ea6\u4f30\u8ba1\u5173\u8054\uff0c\u8bc1\u660e\u4e86\u5176\u5728Wasserstein\u8ddd\u79bb\u4e0b\u7684\u6700\u4f18\u6536\u655b\u901f\u7387\uff0c\u5e76\u9996\u6b21\u9a8c\u8bc1\u4e86\u5176\u5728\u9ad8\u7ef4\u6570\u636e\u4e2d\u7684\u6709\u6548\u6027\u3002", "motivation": "Flow Matching\u4f5c\u4e3a\u4e00\u79cd\u66ff\u4ee3\u6269\u6563\u6a21\u578b\u7684\u7b80\u5355\u7075\u6d3b\u65b9\u6cd5\uff0c\u5176\u7406\u8bba\u5206\u6790\u5c1a\u4e0d\u5b8c\u5584\uff0c\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u5c06Flow Matching\u4e0e\u6838\u5bc6\u5ea6\u4f30\u8ba1\u5173\u8054\uff0c\u5206\u6790\u5176\u5728Wasserstein\u8ddd\u79bb\u4e0b\u7684\u6536\u655b\u901f\u7387\uff0c\u5e76\u9a8c\u8bc1\u9ad8\u7ef4\u6570\u636e\u4e2d\u7684\u6027\u80fd\u3002", "result": "Flow Matching\u5728\u8db3\u591f\u5927\u7684\u7f51\u7edc\u4e0b\u8fbe\u5230\u6700\u4f18\u6536\u655b\u901f\u7387\uff0c\u4e14\u5728\u9ad8\u7ef4\u6570\u636e\u4e2d\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "\u672c\u6587\u4e3aFlow Matching\u7684\u5b9e\u8bc1\u6210\u529f\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u5e76\u9996\u6b21\u8bc1\u660e\u4e86\u5176\u5728\u9ad8\u7ef4\u6570\u636e\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2504.13587", "pdf": "https://arxiv.org/pdf/2504.13587", "abs": "https://arxiv.org/abs/2504.13587", "authors": ["Quentin Romero Lauro", "Shreya Shankar", "Sepanta Zeighami", "Aditya Parameswaran"], "title": "RAG Without the Lag: Interactive Debugging for Retrieval-Augmented Generation Pipelines", "categories": ["cs.HC", "cs.AI"], "comment": "15 pages, 7 figures, 2 tables", "summary": "Retrieval-augmented generation (RAG) pipelines have become the de-facto\napproach for building AI assistants with access to external, domain-specific\nknowledge. Given a user query, RAG pipelines typically first retrieve (R)\nrelevant information from external sources, before invoking a Large Language\nModel (LLM), augmented (A) with this information, to generate (G) responses.\nModern RAG pipelines frequently chain multiple retrieval and generation\ncomponents, in any order. However, developing effective RAG pipelines is\nchallenging because retrieval and generation components are intertwined, making\nit hard to identify which component(s) cause errors in the eventual output. The\nparameters with the greatest impact on output quality often require hours of\npre-processing after each change, creating prohibitively slow feedback cycles.\nTo address these challenges, we present RAGGY, a developer tool that combines a\nPython library of composable RAG primitives with an interactive interface for\nreal-time debugging. We contribute the design and implementation of RAGGY,\ninsights into expert debugging patterns through a qualitative study with 12\nengineers, and design implications for future RAG tools that better align with\ndevelopers' natural workflows.", "AI": {"tldr": "RAGGY\u662f\u4e00\u4e2a\u5f00\u53d1\u8005\u5de5\u5177\uff0c\u7ed3\u5408\u4e86\u53ef\u7ec4\u5408\u7684RAG\u539f\u8bedPython\u5e93\u548c\u5b9e\u65f6\u8c03\u8bd5\u754c\u9762\uff0c\u89e3\u51b3\u4e86RAG\u7ba1\u9053\u5f00\u53d1\u4e2d\u7684\u8c03\u8bd5\u96be\u9898\u3002", "motivation": "RAG\u7ba1\u9053\u5f00\u53d1\u4e2d\uff0c\u68c0\u7d22\u4e0e\u751f\u6210\u7ec4\u4ef6\u4ea4\u7ec7\uff0c\u96be\u4ee5\u5b9a\u4f4d\u9519\u8bef\uff0c\u4e14\u8c03\u8bd5\u5468\u671f\u957f\uff0c\u5f71\u54cd\u6548\u7387\u3002", "method": "\u63d0\u51faRAGGY\u5de5\u5177\uff0c\u5305\u542bPython\u5e93\u548c\u4ea4\u4e92\u754c\u9762\uff0c\u652f\u6301\u5b9e\u65f6\u8c03\u8bd5\uff0c\u5e76\u901a\u8fc712\u540d\u5de5\u7a0b\u5e08\u7684\u5b9a\u6027\u7814\u7a76\u9a8c\u8bc1\u3002", "result": "RAGGY\u6709\u6548\u7f29\u77ed\u8c03\u8bd5\u5468\u671f\uff0c\u63d0\u5347\u5f00\u53d1\u6548\u7387\uff0c\u5e76\u603b\u7ed3\u4e86\u672a\u6765RAG\u5de5\u5177\u7684\u8bbe\u8ba1\u542f\u793a\u3002", "conclusion": "RAGGY\u4e3aRAG\u7ba1\u9053\u5f00\u53d1\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u8c03\u8bd5\u5de5\u5177\uff0c\u672a\u6765\u53ef\u8fdb\u4e00\u6b65\u4f18\u5316\u5f00\u53d1\u8005\u5de5\u4f5c\u6d41\u7a0b\u3002"}}
{"id": "2504.13590", "pdf": "https://arxiv.org/pdf/2504.13590", "abs": "https://arxiv.org/abs/2504.13590", "authors": ["Alexander Rusnak", "Fr\u00e9d\u00e9ric Kaplan"], "title": "HAECcity: Open-Vocabulary Scene Understanding of City-Scale Point Clouds with Superpoint Graph Clustering", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted for publication through the upcoming CVPR Workshop on open\n  scene understanding with foundation models (OPENSUN3D)", "summary": "Traditional 3D scene understanding techniques are generally predicated on\nhand-annotated label sets, but in recent years a new class of open-vocabulary\n3D scene understanding techniques has emerged. Despite the success of this\nparadigm on small scenes, existing approaches cannot scale efficiently to\ncity-scale 3D datasets. In this paper, we present Hierarchical vocab-Agnostic\nExpert Clustering (HAEC), after the latin word for 'these', a superpoint graph\nclustering based approach which utilizes a novel mixture of experts graph\ntransformer for its backbone. We administer this highly scalable approach to\nthe first application of open-vocabulary scene understanding on the SensatUrban\ncity-scale dataset. We also demonstrate a synthetic labeling pipeline which is\nderived entirely from the raw point clouds with no hand-annotation. Our\ntechnique can help unlock complex operations on dense urban 3D scenes and open\na new path forward in the processing of digital twins.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHAEC\u7684\u5f00\u653e\u8bcd\u6c473D\u573a\u666f\u7406\u89e3\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u57ce\u5e02\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf3D\u573a\u666f\u7406\u89e3\u4f9d\u8d56\u4eba\u5de5\u6807\u6ce8\uff0c\u800c\u5f00\u653e\u8bcd\u6c47\u65b9\u6cd5\u5728\u5c0f\u573a\u666f\u4e2d\u6210\u529f\u4f46\u96be\u4ee5\u6269\u5c55\u81f3\u57ce\u5e02\u89c4\u6a21\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u8d85\u70b9\u56fe\u7684\u805a\u7c7b\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e13\u5bb6\u56fe\u53d8\u6362\u5668\uff0c\u5e76\u5f00\u53d1\u4e86\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u7684\u5408\u6210\u6807\u7b7e\u751f\u6210\u6d41\u7a0b\u3002", "result": "\u5728SensatUrban\u6570\u636e\u96c6\u4e0a\u9996\u6b21\u5b9e\u73b0\u57ce\u5e02\u89c4\u6a21\u7684\u5f00\u653e\u8bcd\u6c47\u573a\u666f\u7406\u89e3\uff0c\u5c55\u793a\u4e86\u9ad8\u6548\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "HAEC\u4e3a\u5bc6\u96c6\u57ce\u5e023D\u573a\u666f\u7684\u590d\u6742\u64cd\u4f5c\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\uff0c\u63a8\u52a8\u4e86\u6570\u5b57\u5b6a\u751f\u5904\u7406\u7684\u8fdb\u5c55\u3002"}}
{"id": "2504.13597", "pdf": "https://arxiv.org/pdf/2504.13597", "abs": "https://arxiv.org/abs/2504.13597", "authors": ["Jun Zeng", "KC Santosh", "Deepak Rajan Nayak", "Thomas de Lange", "Jonas Varkey", "Tyler Berzin", "Debesh Jha"], "title": "FocusNet: Transformer-enhanced Polyp Segmentation with Local and Pooling Attention", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "9 pages, 6 figures", "summary": "Colonoscopy is vital in the early diagnosis of colorectal polyps. Regular\nscreenings can effectively prevent benign polyps from progressing to CRC. While\ndeep learning has made impressive strides in polyp segmentation, most existing\nmodels are trained on single-modality and single-center data, making them less\neffective in real-world clinical environments. To overcome these limitations,\nwe propose FocusNet, a Transformer-enhanced focus attention network designed to\nimprove polyp segmentation. FocusNet incorporates three essential modules: the\nCross-semantic Interaction Decoder Module (CIDM) for generating coarse\nsegmentation maps, the Detail Enhancement Module (DEM) for refining shallow\nfeatures, and the Focus Attention Module (FAM), to balance local detail and\nglobal context through local and pooling attention mechanisms. We evaluate our\nmodel on PolypDB, a newly introduced dataset with multi-modality and\nmulti-center data for building more reliable segmentation methods. Extensive\nexperiments showed that FocusNet consistently outperforms existing\nstate-of-the-art approaches with a high dice coefficients of 82.47% on the BLI\nmodality, 88.46% on FICE, 92.04% on LCI, 82.09% on the NBI and 93.42% on WLI\nmodality, demonstrating its accuracy and robustness across five different\nmodalities. The source code for FocusNet is available at\nhttps://github.com/JunZengz/FocusNet.", "AI": {"tldr": "FocusNet\u662f\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u6ce8\u610f\u529b\u7f51\u7edc\uff0c\u7528\u4e8e\u6539\u8fdb\u606f\u8089\u5206\u5272\uff0c\u901a\u8fc7\u591a\u6a21\u5757\u8bbe\u8ba1\u5728\u591a\u6a21\u6001\u548c\u591a\u4e2d\u5fc3\u6570\u636e\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u606f\u8089\u5206\u5272\u4e2d\u591a\u57fa\u4e8e\u5355\u6a21\u6001\u548c\u5355\u4e2d\u5fc3\u6570\u636e\uff0c\u96be\u4ee5\u9002\u5e94\u771f\u5b9e\u4e34\u5e8a\u73af\u5883\u3002", "method": "FocusNet\u5305\u542b\u4e09\u4e2a\u6a21\u5757\uff1aCIDM\u751f\u6210\u7c97\u5206\u5272\u56fe\uff0cDEM\u4f18\u5316\u6d45\u5c42\u7279\u5f81\uff0cFAM\u901a\u8fc7\u5c40\u90e8\u548c\u6c60\u5316\u6ce8\u610f\u529b\u673a\u5236\u5e73\u8861\u7ec6\u8282\u4e0e\u5168\u5c40\u4e0a\u4e0b\u6587\u3002", "result": "\u5728PolypDB\u6570\u636e\u96c6\u4e0a\uff0cFocusNet\u5728\u4e94\u79cd\u6a21\u6001\u4e0a\u7684Dice\u7cfb\u6570\u5206\u522b\u4e3a82.47%\uff08BLI\uff09\u300188.46%\uff08FICE\uff09\u300192.04%\uff08LCI\uff09\u300182.09%\uff08NBI\uff09\u548c93.42%\uff08WLI\uff09\u3002", "conclusion": "FocusNet\u5728\u591a\u6a21\u6001\u548c\u591a\u4e2d\u5fc3\u6570\u636e\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2504.13397", "pdf": "https://arxiv.org/pdf/2504.13397", "abs": "https://arxiv.org/abs/2504.13397", "authors": ["Yu Gan", "Mohadeseh Azar", "Nitish Kumar Chandra", "Xin Jin", "Jinglei Cheng", "Kaushik P. Seshadreesan", "Junyu Liu"], "title": "Quantum repeaters enhanced by vacuum beam guides", "categories": ["quant-ph", "cs.DC", "cs.LG", "cs.NI"], "comment": "10 pages", "summary": "The development of large-scale quantum communication networks faces critical\nchallenges due to photon loss and decoherence in optical fiber channels. These\nfundamentally limit transmission distances and demand dense networks of\nrepeater stations. This work investigates using vacuum beam guides (VBGs)-a\npromising ultra-low-loss transmission platform-as an alternative to traditional\nfiber links. By incorporating VBGs into repeater-based architectures, we\ndemonstrate that the inter-repeater spacing can be substantially extended,\nresulting in fewer required nodes and significantly reducing hardware and\noperational complexity. We perform a cost-function analysis to quantify\nperformance trade-offs across first, second, and third-generation repeaters.\nOur results show that first-generation repeaters reduce costs dramatically by\neliminating entanglement purification. Third-generation repeaters benefit from\nimproved link transmission success, which is crucial for quantum error\ncorrection. In contrast, second-generation repeaters exhibit a more nuanced\nresponse; although transmission loss is reduced, their performance remains\nprimarily limited by logical gate errors rather than channel loss. These\nfindings highlight that while all repeater generations benefit from reduced\nphoton loss, the magnitude of improvement depends critically on the underlying\nerror mechanisms. Vacuum beam guides thus emerge as a powerful enabler for\nscalable, high-performance quantum networks, particularly in conjunction with\nnear-term quantum hardware capabilities.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5229\u7528\u771f\u7a7a\u5149\u675f\u5bfc\u5f15\uff08VBGs\uff09\u66ff\u4ee3\u4f20\u7edf\u5149\u7ea4\uff0c\u4ee5\u51cf\u5c11\u91cf\u5b50\u901a\u4fe1\u7f51\u7edc\u4e2d\u7684\u5149\u5b50\u635f\u5931\u548c\u9000\u76f8\u5e72\u95ee\u9898\uff0c\u4ece\u800c\u6269\u5c55\u4e2d\u7ee7\u7ad9\u95f4\u8ddd\u5e76\u964d\u4f4e\u786c\u4ef6\u590d\u6742\u5ea6\u3002", "motivation": "\u5927\u89c4\u6a21\u91cf\u5b50\u901a\u4fe1\u7f51\u7edc\u7684\u53d1\u5c55\u53d7\u9650\u4e8e\u5149\u7ea4\u4e2d\u7684\u5149\u5b50\u635f\u5931\u548c\u9000\u76f8\u5e72\uff0c\u5bfc\u81f4\u4f20\u8f93\u8ddd\u79bb\u53d7\u9650\u4e14\u9700\u8981\u5bc6\u96c6\u7684\u4e2d\u7ee7\u7ad9\u7f51\u7edc\u3002", "method": "\u901a\u8fc7\u5c06VBGs\u96c6\u6210\u5230\u4e2d\u7ee7\u67b6\u6784\u4e2d\uff0c\u5206\u6790\u4e0d\u540c\u4ee3\u4e2d\u7ee7\u5668\u7684\u6027\u80fd\u5dee\u5f02\uff0c\u5e76\u8fdb\u884c\u6210\u672c\u51fd\u6570\u5206\u6790\u3002", "result": "VBGs\u663e\u8457\u6269\u5c55\u4e86\u4e2d\u7ee7\u95f4\u8ddd\uff0c\u51cf\u5c11\u4e86\u8282\u70b9\u6570\u91cf\u3002\u7b2c\u4e00\u4ee3\u4e2d\u7ee7\u5668\u6210\u672c\u5927\u5e45\u964d\u4f4e\uff0c\u7b2c\u4e09\u4ee3\u4e2d\u7ee7\u5668\u5728\u91cf\u5b50\u7ea0\u9519\u4e2d\u8868\u73b0\u66f4\u4f18\uff0c\u800c\u7b2c\u4e8c\u4ee3\u4e2d\u7ee7\u5668\u53d7\u9650\u4e8e\u903b\u8f91\u95e8\u9519\u8bef\u3002", "conclusion": "VBGs\u662f\u6784\u5efa\u53ef\u6269\u5c55\u9ad8\u6027\u80fd\u91cf\u5b50\u7f51\u7edc\u7684\u6709\u529b\u5de5\u5177\uff0c\u5c24\u5176\u9002\u5408\u4e0e\u8fd1\u671f\u91cf\u5b50\u786c\u4ef6\u7ed3\u5408\u4f7f\u7528\u3002"}}
{"id": "2504.13614", "pdf": "https://arxiv.org/pdf/2504.13614", "abs": "https://arxiv.org/abs/2504.13614", "authors": ["Zahra Akhlaghi", "Mostafa Haghir Chehreghani"], "title": "Adaptive Long-term Embedding with Denoising and Augmentation for Recommendation", "categories": ["cs.IR", "cs.AI", "cs.NE"], "comment": null, "summary": "The rapid growth of the internet has made personalized recommendation systems\nindispensable. Graph-based sequential recommendation systems, powered by Graph\nNeural Networks (GNNs), effectively capture complex user-item interactions but\noften face challenges such as noise and static representations. In this paper,\nwe introduce the Adaptive Long-term Embedding with Denoising and Augmentation\nfor Recommendation (ALDA4Rec) method, a novel model that constructs an\nitem-item graph, filters noise through community detection, and enriches\nuser-item interactions. Graph Convolutional Networks (GCNs) are then employed\nto learn short-term representations, while averaging, GRUs, and attention\nmechanisms are utilized to model long-term embeddings. An MLP-based adaptive\nweighting strategy is further incorporated to dynamically optimize long-term\nuser preferences. Experiments conducted on four real-world datasets demonstrate\nthat ALDA4Rec outperforms state-of-the-art baselines, delivering notable\nimprovements in both accuracy and robustness. The source code is available at\nhttps://github.com/zahraakhlaghi/ALDA4Rec.", "AI": {"tldr": "ALDA4Rec\u662f\u4e00\u79cd\u57fa\u4e8e\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u4e2a\u6027\u5316\u63a8\u8350\u7cfb\u7edf\uff0c\u901a\u8fc7\u53bb\u566a\u548c\u589e\u5f3a\u6280\u672f\u4f18\u5316\u7528\u6237-\u7269\u54c1\u4ea4\u4e92\uff0c\u7ed3\u5408\u77ed\u671f\u548c\u957f\u671f\u8868\u793a\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u63a8\u8350\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u4e92\u8054\u7f51\u7684\u5feb\u901f\u53d1\u5c55\u4f7f\u4e2a\u6027\u5316\u63a8\u8350\u7cfb\u7edf\u53d8\u5f97\u4e0d\u53ef\u6216\u7f3a\uff0c\u4f46\u73b0\u6709\u56fe\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u5e38\u9762\u4e34\u566a\u58f0\u548c\u9759\u6001\u8868\u793a\u95ee\u9898\u3002", "method": "ALDA4Rec\u6784\u5efa\u7269\u54c1-\u7269\u54c1\u56fe\uff0c\u901a\u8fc7\u793e\u533a\u68c0\u6d4b\u53bb\u566a\uff0c\u7ed3\u5408GCN\u5b66\u4e60\u77ed\u671f\u8868\u793a\uff0c\u5229\u7528\u5e73\u5747\u3001GRU\u548c\u6ce8\u610f\u529b\u673a\u5236\u5efa\u6a21\u957f\u671f\u5d4c\u5165\uff0c\u5e76\u91c7\u7528MLP\u81ea\u9002\u5e94\u6743\u91cd\u7b56\u7565\u4f18\u5316\u7528\u6237\u504f\u597d\u3002", "result": "\u5728\u56db\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cALDA4Rec\u5728\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "ALDA4Rec\u901a\u8fc7\u53bb\u566a\u548c\u52a8\u6001\u4f18\u5316\u957f\u671f\u504f\u597d\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u8350\u7cfb\u7edf\u7684\u6027\u80fd\u3002"}}
{"id": "2504.13408", "pdf": "https://arxiv.org/pdf/2504.13408", "abs": "https://arxiv.org/abs/2504.13408", "authors": ["Varij Saini", "Rudraksh Gupta", "Neel Soni"], "title": "OpCode-Based Malware Classification Using Machine Learning and Deep Learning Techniques", "categories": ["cs.CR", "cs.LG", "68T05 (Primary), 68Q25, 68Q10 (Secondary)", "I.2.6; I.2.8; F.2.2; D.4.6"], "comment": "11 pages", "summary": "This technical report presents a comprehensive analysis of malware\nclassification using OpCode sequences. Two distinct approaches are evaluated:\ntraditional machine learning using n-gram analysis with Support Vector Machine\n(SVM), K-Nearest Neighbors (KNN), and Decision Tree classifiers; and a deep\nlearning approach employing a Convolutional Neural Network (CNN). The\ntraditional machine learning approach establishes a baseline using handcrafted\n1-gram and 2-gram features from disassembled malware samples. The deep learning\nmethodology builds upon the work proposed in \"Deep Android Malware Detection\"\nby McLaughlin et al. and evaluates the performance of a CNN model trained to\nautomatically extract features from raw OpCode data. Empirical results are\ncompared using standard performance metrics (accuracy, precision, recall, and\nF1-score). While the SVM classifier outperforms other traditional techniques,\nthe CNN model demonstrates competitive performance with the added benefit of\nautomated feature extraction.", "AI": {"tldr": "\u8be5\u6280\u672f\u62a5\u544a\u6bd4\u8f83\u4e86\u4f20\u7edf\u673a\u5668\u5b66\u4e60\uff08SVM\u3001KNN\u3001\u51b3\u7b56\u6811\uff09\u548c\u6df1\u5ea6\u5b66\u4e60\uff08CNN\uff09\u5728\u6076\u610f\u8f6f\u4ef6\u5206\u7c7b\u4e2d\u7684\u8868\u73b0\u3002\u4f20\u7edf\u65b9\u6cd5\u4f7f\u7528\u624b\u5de5\u7279\u5f81\uff081-gram\u548c2-gram\uff09\uff0c\u800cCNN\u81ea\u52a8\u63d0\u53d6\u7279\u5f81\u3002\u7ed3\u679c\u663e\u793aSVM\u8868\u73b0\u6700\u4f73\uff0c\u4f46CNN\u5177\u6709\u81ea\u52a8\u7279\u5f81\u63d0\u53d6\u7684\u4f18\u52bf\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u8bc4\u4f30\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u548c\u6df1\u5ea6\u5b66\u4e60\u5728\u6076\u610f\u8f6f\u4ef6\u5206\u7c7b\u4e2d\u7684\u6548\u679c\uff0c\u63a2\u7d22\u81ea\u52a8\u7279\u5f81\u63d0\u53d6\u7684\u6f5c\u529b\u3002", "method": "\u4f20\u7edf\u65b9\u6cd5\u4f7f\u7528n-gram\u5206\u6790\u548cSVM\u3001KNN\u3001\u51b3\u7b56\u6811\uff1b\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u91c7\u7528CNN\u81ea\u52a8\u63d0\u53d6\u7279\u5f81\u3002", "result": "SVM\u5728\u4f20\u7edf\u65b9\u6cd5\u4e2d\u8868\u73b0\u6700\u4f73\uff0cCNN\u6027\u80fd\u63a5\u8fd1\u4e14\u5177\u6709\u81ea\u52a8\u7279\u5f81\u63d0\u53d6\u7684\u4f18\u52bf\u3002", "conclusion": "CNN\u5728\u6076\u610f\u8f6f\u4ef6\u5206\u7c7b\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u5c24\u5176\u662f\u81ea\u52a8\u7279\u5f81\u63d0\u53d6\u7684\u80fd\u529b\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\uff08\u5982SVM\uff09\u4ecd\u5177\u7ade\u4e89\u529b\u3002"}}
{"id": "2504.13412", "pdf": "https://arxiv.org/pdf/2504.13412", "abs": "https://arxiv.org/abs/2504.13412", "authors": ["Samuel Audia", "Soheil Feizi", "Matthias Zwicker", "Dinesh Manocha"], "title": "How Learnable Grids Recover Fine Detail in Low Dimensions: A Neural Tangent Kernel Analysis of Multigrid Parametric Encodings", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Neural networks that map between low dimensional spaces are ubiquitous in\ncomputer graphics and scientific computing; however, in their naive\nimplementation, they are unable to learn high frequency information. We present\na comprehensive analysis comparing the two most common techniques for\nmitigating this spectral bias: Fourier feature encodings (FFE) and multigrid\nparametric encodings (MPE). FFEs are seen as the standard for low dimensional\nmappings, but MPEs often outperform them and learn representations with higher\nresolution and finer detail. FFE's roots in the Fourier transform, make it\nsusceptible to aliasing if pushed too far, while MPEs, which use a learned grid\nstructure, have no such limitation. To understand the difference in\nperformance, we use the neural tangent kernel (NTK) to evaluate these encodings\nthrough the lens of an analogous kernel regression. By finding a lower bound on\nthe smallest eigenvalue of the NTK, we prove that MPEs improve a network's\nperformance through the structure of their grid and not their learnable\nembedding. This mechanism is fundamentally different from FFEs, which rely\nsolely on their embedding space to improve performance. Results are empirically\nvalidated on a 2D image regression task using images taken from 100 synonym\nsets of ImageNet and 3D implicit surface regression on objects from the\nStanford graphics dataset. Using peak signal-to-noise ratio (PSNR) and\nmultiscale structural similarity (MS-SSIM) to evaluate how well fine details\nare learned, we show that the MPE increases the minimum eigenvalue by 8 orders\nof magnitude over the baseline and 2 orders of magnitude over the FFE. The\nincrease in spectrum corresponds to a 15 dB (PSNR) / 0.65 (MS-SSIM) increase\nover baseline and a 12 dB (PSNR) / 0.33 (MS-SSIM) increase over the FFE.", "AI": {"tldr": "\u8bba\u6587\u6bd4\u8f83\u4e86\u4e24\u79cd\u89e3\u51b3\u795e\u7ecf\u7f51\u7edc\u9891\u8c31\u504f\u5dee\u7684\u6280\u672f\uff1a\u5085\u91cc\u53f6\u7279\u5f81\u7f16\u7801\uff08FFE\uff09\u548c\u591a\u7f51\u683c\u53c2\u6570\u7f16\u7801\uff08MPE\uff09\uff0c\u53d1\u73b0MPE\u5728\u7ec6\u8282\u5b66\u4e60\u548c\u6027\u80fd\u4e0a\u4f18\u4e8eFFE\u3002", "motivation": "\u89e3\u51b3\u795e\u7ecf\u7f51\u7edc\u5728\u4f4e\u7ef4\u7a7a\u95f4\u6620\u5c04\u4e2d\u65e0\u6cd5\u5b66\u4e60\u9ad8\u9891\u4fe1\u606f\u7684\u95ee\u9898\uff0c\u5e76\u6bd4\u8f83FFE\u548cMPE\u4e24\u79cd\u6280\u672f\u7684\u6027\u80fd\u5dee\u5f02\u3002", "method": "\u4f7f\u7528\u795e\u7ecf\u5207\u7ebf\u6838\uff08NTK\uff09\u5206\u6790FFE\u548cMPE\u7684\u6027\u80fd\u5dee\u5f02\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u57282D\u56fe\u50cf\u56de\u5f52\u548c3D\u9690\u5f0f\u8868\u9762\u56de\u5f52\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002", "result": "MPE\u5728\u9891\u8c31\u4e0a\u6bd4\u57fa\u7ebf\u63d0\u9ad8\u4e868\u4e2a\u6570\u91cf\u7ea7\uff0c\u6bd4FFE\u63d0\u9ad8\u4e862\u4e2a\u6570\u91cf\u7ea7\uff0cPSNR\u548cMS-SSIM\u6307\u6807\u663e\u8457\u4f18\u4e8eFFE\u3002", "conclusion": "MPE\u901a\u8fc7\u7f51\u683c\u7ed3\u6784\u800c\u975e\u5d4c\u5165\u7a7a\u95f4\u63d0\u5347\u6027\u80fd\uff0c\u5176\u673a\u5236\u4e0eFFE\u4e0d\u540c\uff0c\u4e14\u5728\u7ec6\u8282\u5b66\u4e60\u4e0a\u8868\u73b0\u66f4\u4f18\u3002"}}
{"id": "2504.13647", "pdf": "https://arxiv.org/pdf/2504.13647", "abs": "https://arxiv.org/abs/2504.13647", "authors": ["Yushen He", "Lei Zhao", "Tianchen Deng", "Zipeng Fang", "Weidong Chen"], "title": "Lightweight LiDAR-Camera 3D Dynamic Object Detection and Multi-Class Trajectory Prediction", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": null, "summary": "Service mobile robots are often required to avoid dynamic objects while\nperforming their tasks, but they usually have only limited computational\nresources. So we present a lightweight multi-modal framework for 3D object\ndetection and trajectory prediction. Our system synergistically integrates\nLiDAR and camera inputs to achieve real-time perception of pedestrians,\nvehicles, and riders in 3D space. The framework proposes two novel modules: 1)\na Cross-Modal Deformable Transformer (CMDT) for object detection with high\naccuracy and acceptable amount of computation, and 2) a Reference\nTrajectory-based Multi-Class Transformer (RTMCT) for efficient and diverse\ntrajectory prediction of mult-class objects with flexible trajectory lengths.\nEvaluations on the CODa benchmark demonstrate superior performance over\nexisting methods across detection (+2.03% in mAP) and trajectory prediction\n(-0.408m in minADE5 of pedestrians) metrics. Remarkably, the system exhibits\nexceptional deployability - when implemented on a wheelchair robot with an\nentry-level NVIDIA 3060 GPU, it achieves real-time inference at 13.2 fps. To\nfacilitate reproducibility and practical deployment, we release the related\ncode of the method at https://github.com/TossherO/3D_Perception and its ROS\ninference version at https://github.com/TossherO/ros_packages.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u591a\u6a21\u6001\u6846\u67b6\uff0c\u7528\u4e8e3D\u7269\u4f53\u68c0\u6d4b\u548c\u8f68\u8ff9\u9884\u6d4b\uff0c\u7ed3\u5408LiDAR\u548c\u6444\u50cf\u5934\u8f93\u5165\uff0c\u5b9e\u73b0\u5b9e\u65f6\u611f\u77e5\u3002", "motivation": "\u670d\u52a1\u79fb\u52a8\u673a\u5668\u4eba\u5728\u6709\u9650\u8ba1\u7b97\u8d44\u6e90\u4e0b\u9700\u907f\u514d\u52a8\u6001\u7269\u4f53\uff0c\u56e0\u6b64\u9700\u8981\u9ad8\u6548\u7684\u591a\u6a21\u6001\u611f\u77e5\u7cfb\u7edf\u3002", "method": "\u63d0\u51fa\u4e24\u4e2a\u65b0\u6a21\u5757\uff1a1) CMDT\u7528\u4e8e\u9ad8\u7cbe\u5ea6\u7269\u4f53\u68c0\u6d4b\uff0c2) RTMCT\u7528\u4e8e\u591a\u7c7b\u7269\u4f53\u8f68\u8ff9\u9884\u6d4b\u3002", "result": "\u5728CODa\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u68c0\u6d4bmAP\u63d0\u53472.03%\uff0c\u884c\u4eba\u8f68\u8ff9\u9884\u6d4b\u8bef\u5dee\u51cf\u5c110.408m\u3002\u7cfb\u7edf\u5728\u4f4e\u7aefGPU\u4e0a\u5b9e\u73b013.2 fps\u5b9e\u65f6\u63a8\u7406\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u9ad8\u6548\u4e14\u6613\u4e8e\u90e8\u7f72\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2504.13452", "pdf": "https://arxiv.org/pdf/2504.13452", "abs": "https://arxiv.org/abs/2504.13452", "authors": ["Juliette Bertrand", "Sophie Giffard-Roisin", "James Hollingsworth", "Julien Mairal"], "title": "MicroFlow: Domain-Specific Optical Flow for Ground Deformation Estimation in Seismic Events", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Dense ground displacement measurements are crucial for geological studies but\nare impractical to collect directly. Traditionally, displacement fields are\nestimated using patch matching on optical satellite images from different\nacquisition times. While deep learning-based optical flow models are promising,\ntheir adoption in ground deformation analysis is hindered by challenges such as\nthe absence of real ground truth, the need for sub-pixel precision, and\ntemporal variations due to geological or anthropogenic changes. In particular,\nwe identify that deep learning models relying on explicit correlation layers\nstruggle at estimating small displacements in real-world conditions. Instead,\nwe propose a model that employs iterative refinements with explicit warping\nlayers and a correlation-independent backbone, enabling sub-pixel precision.\nAdditionally, a non-convex variant of Total Variation regularization preserves\nfault-line sharpness while maintaining smoothness elsewhere. Our model\nsignificantly outperforms widely used geophysics methods on semi-synthetic\nbenchmarks and generalizes well to challenging real-world scenarios captured by\nboth medium- and high-resolution sensors. Project page:\nhttps://jbertrand89.github.io/microflow/.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u6a21\u578b\uff0c\u7528\u4e8e\u9ad8\u7cbe\u5ea6\u4f30\u8ba1\u5730\u9762\u4f4d\u79fb\u573a\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u5730\u9762\u4f4d\u79fb\u6d4b\u91cf\u5bf9\u5730\u8d28\u7814\u7a76\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u76f4\u63a5\u6d4b\u91cf\u4e0d\u5207\u5b9e\u9645\u3002\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u5149\u5b66\u536b\u661f\u56fe\u50cf\u7684\u5757\u5339\u914d\uff0c\u800c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u9762\u4e34\u7f3a\u4e4f\u771f\u5b9e\u6570\u636e\u3001\u4e9a\u50cf\u7d20\u7cbe\u5ea6\u9700\u6c42\u7b49\u6311\u6218\u3002", "method": "\u91c7\u7528\u8fed\u4ee3\u7ec6\u5316\u7b56\u7565\u548c\u663e\u5f0f\u53d8\u5f62\u5c42\uff0c\u7ed3\u5408\u975e\u51f8\u603b\u53d8\u5dee\u6b63\u5219\u5316\uff0c\u4ee5\u4fdd\u6301\u65ad\u5c42\u7ebf\u9510\u5ea6\u5e76\u5b9e\u73b0\u4e9a\u50cf\u7d20\u7cbe\u5ea6\u3002", "result": "\u6a21\u578b\u5728\u534a\u5408\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u5730\u7403\u7269\u7406\u65b9\u6cd5\uff0c\u5e76\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u6a21\u578b\u4e3a\u5730\u9762\u4f4d\u79fb\u6d4b\u91cf\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u7cbe\u786e\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u4f20\u611f\u5668\u6570\u636e\u3002"}}
{"id": "2504.13656", "pdf": "https://arxiv.org/pdf/2504.13656", "abs": "https://arxiv.org/abs/2504.13656", "authors": ["Antonio Della Porta", "Stefano Lambiase", "Fabio Palomba"], "title": "Do Prompt Patterns Affect Code Quality? A First Empirical Assessment of ChatGPT-Generated Code", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have rapidly transformed software development,\nespecially in code generation. However, their inconsistent performance, prone\nto hallucinations and quality issues, complicates program comprehension and\nhinders maintainability. Research indicates that prompt engineering-the\npractice of designing inputs to direct LLMs toward generating relevant\noutputs-may help address these challenges. In this regard, researchers have\nintroduced prompt patterns, structured templates intended to guide users in\nformulating their requests. However, the influence of prompt patterns on code\nquality has yet to be thoroughly investigated. An improved understanding of\nthis relationship would be essential to advancing our collective knowledge on\nhow to effectively use LLMs for code generation, thereby enhancing their\nunderstandability in contemporary software development. This paper empirically\ninvestigates the impact of prompt patterns on code quality, specifically\nmaintainability, security, and reliability, using the Dev-GPT dataset. Results\nshow that Zero-Shot prompting is most common, followed by Zero-Shot with\nChain-of-Thought and Few-Shot. Analysis of 7583 code files across quality\nmetrics revealed minimal issues, with Kruskal-Wallis tests indicating no\nsignificant differences among patterns, suggesting that prompt structure may\nnot substantially impact these quality metrics in ChatGPT-assisted code\ngeneration.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u63d0\u793a\u6a21\u5f0f\u5bf9\u4ee3\u7801\u8d28\u91cf\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u63d0\u793a\u7ed3\u6784\u5bf9ChatGPT\u751f\u6210\u7684\u4ee3\u7801\u8d28\u91cf\uff08\u53ef\u7ef4\u62a4\u6027\u3001\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\uff09\u5f71\u54cd\u4e0d\u5927\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u4ee3\u7801\u751f\u6210\u4e2d\u5b58\u5728\u6027\u80fd\u4e0d\u4e00\u81f4\u548c\u5e7b\u89c9\u95ee\u9898\uff0c\u63d0\u793a\u5de5\u7a0b\u53ef\u80fd\u6539\u5584\u8fd9\u4e9b\u95ee\u9898\uff0c\u4f46\u63d0\u793a\u6a21\u5f0f\u5bf9\u4ee3\u7801\u8d28\u91cf\u7684\u5f71\u54cd\u5c1a\u672a\u6df1\u5165\u7814\u7a76\u3002", "method": "\u4f7f\u7528Dev-GPT\u6570\u636e\u96c6\uff0c\u5b9e\u8bc1\u7814\u7a76\u63d0\u793a\u6a21\u5f0f\u5bf9\u4ee3\u7801\u8d28\u91cf\u7684\u5f71\u54cd\uff0c\u5206\u67907583\u4e2a\u4ee3\u7801\u6587\u4ef6\u7684\u8d28\u91cf\u6307\u6807\u3002", "result": "\u7ed3\u679c\u663e\u793aZero-Shot\u63d0\u793a\u6700\u5e38\u89c1\uff0c\u4f46Kruskal-Wallis\u6d4b\u8bd5\u8868\u660e\u63d0\u793a\u6a21\u5f0f\u5bf9\u4ee3\u7801\u8d28\u91cf\u65e0\u663e\u8457\u5f71\u54cd\u3002", "conclusion": "\u63d0\u793a\u7ed3\u6784\u53ef\u80fd\u5bf9ChatGPT\u751f\u6210\u7684\u4ee3\u7801\u8d28\u91cf\u5f71\u54cd\u6709\u9650\uff0c\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u5176\u4ed6\u56e0\u7d20\u3002"}}
{"id": "2504.13479", "pdf": "https://arxiv.org/pdf/2504.13479", "abs": "https://arxiv.org/abs/2504.13479", "authors": ["Jiasheng Wu", "Jingjing Zhang", "Zheng Lin", "Zhe Chen", "Xiong Wang", "Wenjun Zhu", "Yue Gao"], "title": "SFL-LEO: Asynchronous Split-Federated Learning Design for LEO Satellite-Ground Network Framework", "categories": ["cs.NI", "cs.DC", "cs.LG"], "comment": "13 pages, 14 figures", "summary": "Recently, the rapid development of LEO satellite networks spurs another\nwidespread concern-data processing at satellites. However, achieving efficient\ncomputation at LEO satellites in highly dynamic satellite networks is\nchallenging and remains an open problem when considering the constrained\ncomputation capability of LEO satellites. For the first time, we propose a\nnovel distributed learning framework named SFL-LEO by combining Federated\nLearning (FL) with Split Learning (SL) to accommodate the high dynamics of LEO\nsatellite networks and the constrained computation capability of LEO satellites\nby leveraging the periodical orbit traveling feature. The proposed scheme\nallows training locally by introducing an asynchronous training strategy, i.e.,\nachieving local update when LEO satellites disconnect with the ground station,\nto provide much more training space and thus increase the training performance.\nMeanwhile, it aggregates client-side sub-models at the ground station and then\ndistributes them to LEO satellites by borrowing the idea from the federated\nlearning scheme. Experiment results driven by satellite-ground bandwidth\nmeasured in Starlink demonstrate that SFL-LEO provides a similar accuracy\nperformance with the conventional SL scheme because it can perform local\ntraining even within the disconnection duration.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSFL-LEO\u7684\u65b0\u578b\u5206\u5e03\u5f0f\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u8054\u90a6\u5b66\u4e60\u4e0e\u5206\u5272\u5b66\u4e60\uff0c\u4ee5\u5e94\u5bf9LEO\u536b\u661f\u7f51\u7edc\u7684\u9ad8\u52a8\u6001\u6027\u548c\u8ba1\u7b97\u80fd\u529b\u9650\u5236\u3002", "motivation": "LEO\u536b\u661f\u7f51\u7edc\u7684\u6570\u636e\u5904\u7406\u6548\u7387\u95ee\u9898\u56e0\u9ad8\u52a8\u6001\u6027\u548c\u8ba1\u7b97\u80fd\u529b\u53d7\u9650\u800c\u6210\u4e3a\u6311\u6218\u3002", "method": "\u7ed3\u5408\u8054\u90a6\u5b66\u4e60\u4e0e\u5206\u5272\u5b66\u4e60\uff0c\u5229\u7528\u536b\u661f\u5468\u671f\u6027\u8f68\u9053\u7279\u5f81\uff0c\u5f15\u5165\u5f02\u6b65\u8bad\u7ec3\u7b56\u7565\uff0c\u5b9e\u73b0\u672c\u5730\u66f4\u65b0\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSFL-LEO\u5728\u65ad\u5f00\u8fde\u63a5\u671f\u95f4\u4ecd\u80fd\u8fdb\u884c\u672c\u5730\u8bad\u7ec3\uff0c\u6027\u80fd\u4e0e\u4f20\u7edf\u5206\u5272\u5b66\u4e60\u65b9\u6848\u76f8\u5f53\u3002", "conclusion": "SFL-LEO\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86LEO\u536b\u661f\u7f51\u7edc\u7684\u52a8\u6001\u6027\u548c\u8ba1\u7b97\u9650\u5236\u95ee\u9898\u3002"}}
{"id": "2504.13676", "pdf": "https://arxiv.org/pdf/2504.13676", "abs": "https://arxiv.org/abs/2504.13676", "authors": ["Felix M\u00e4chtle", "Nils Loose", "Tim Schulz", "Florian Sieck", "Jan-Niclas Serr", "Ralf M\u00f6ller", "Thomas Eisenbarth"], "title": "Trace Gadgets: Minimizing Code Context for Machine Learning-Based Vulnerability Prediction", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "As the number of web applications and API endpoints exposed to the Internet\ncontinues to grow, so does the number of exploitable vulnerabilities. Manually\nidentifying such vulnerabilities is tedious. Meanwhile, static security\nscanners tend to produce many false positives. While machine learning-based\napproaches are promising, they typically perform well only in scenarios where\ntraining and test data are closely related. A key challenge for ML-based\nvulnerability detection is providing suitable and concise code context, as\nexcessively long contexts negatively affect the code comprehension capabilities\nof machine learning models, particularly smaller ones.\n  This work introduces Trace Gadgets, a novel code representation that\nminimizes code context by removing non-related code. Trace Gadgets precisely\ncapture the statements that cover the path to the vulnerability. As input for\nML models, Trace Gadgets provide a minimal but complete context, thereby\nimproving the detection performance. Moreover, we collect a large-scale dataset\ngenerated from real-world applications with manually curated labels to further\nimprove the performance of ML-based vulnerability detectors. Our results show\nthat state-of-the-art machine learning models perform best when using Trace\nGadgets compared to previous code representations, surpassing the detection\ncapabilities of industry-standard static scanners such as GitHub's CodeQL by at\nleast 4% on a fully unseen dataset. By applying our framework to real-world\napplications, we identify and report previously unknown vulnerabilities in\nwidely deployed software.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTrace Gadgets\u7684\u4ee3\u7801\u8868\u793a\u65b9\u6cd5\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u4ee3\u7801\u4e0a\u4e0b\u6587\u5e76\u79fb\u9664\u65e0\u5173\u4ee3\u7801\uff0c\u4f18\u5316\u4e86\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u6f0f\u6d1e\u68c0\u6d4b\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u968f\u7740\u4e92\u8054\u7f51\u5e94\u7528\u548cAPI\u7aef\u70b9\u7684\u589e\u52a0\uff0c\u6f0f\u6d1e\u6570\u91cf\u4e0a\u5347\uff0c\u624b\u52a8\u68c0\u6d4b\u6548\u7387\u4f4e\uff0c\u9759\u6001\u626b\u63cf\u5668\u8bef\u62a5\u7387\u9ad8\uff0c\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5728\u8bad\u7ec3\u4e0e\u6d4b\u8bd5\u6570\u636e\u76f8\u5173\u6027\u5f3a\u65f6\u8868\u73b0\u826f\u597d\uff0c\u4f46\u4ee3\u7801\u4e0a\u4e0b\u6587\u8fc7\u957f\u4f1a\u5f71\u54cd\u6a21\u578b\u6027\u80fd\u3002", "method": "\u5f15\u5165Trace Gadgets\uff0c\u4e00\u79cd\u65b0\u578b\u4ee3\u7801\u8868\u793a\u65b9\u6cd5\uff0c\u901a\u8fc7\u7cbe\u786e\u6355\u83b7\u6f0f\u6d1e\u8def\u5f84\u7684\u8bed\u53e5\uff0c\u63d0\u4f9b\u6700\u5c0f\u4f46\u5b8c\u6574\u7684\u4e0a\u4e0b\u6587\u4f5c\u4e3aML\u6a21\u578b\u7684\u8f93\u5165\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cTrace Gadgets\u4f18\u4e8e\u73b0\u6709\u4ee3\u7801\u8868\u793a\u65b9\u6cd5\uff0c\u6027\u80fd\u63d0\u5347\u81f3\u5c114%\uff0c\u5e76\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u53d1\u73b0\u4e86\u672a\u77e5\u6f0f\u6d1e\u3002", "conclusion": "Trace Gadgets\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u6f0f\u6d1e\u68c0\u6d4b\u4e2d\u7684\u6027\u80fd\uff0c\u4e14\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2504.13519", "pdf": "https://arxiv.org/pdf/2504.13519", "abs": "https://arxiv.org/abs/2504.13519", "authors": ["Yipeng Sun", "Linda-Sophie Schneider", "Mingxuan Gu", "Siyuan Mei", "Chengze Ye", "Fabian Wagner", "Siming Bayer", "Andreas Maier"], "title": "Filter2Noise: Interpretable Self-Supervised Single-Image Denoising for Low-Dose CT with Attention-Guided Bilateral Filtering", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": "preprint", "summary": "Effective denoising is crucial in low-dose CT to enhance subtle structures\nand low-contrast lesions while preventing diagnostic errors. Supervised methods\nstruggle with limited paired datasets, and self-supervised approaches often\nrequire multiple noisy images and rely on deep networks like U-Net, offering\nlittle insight into the denoising mechanism. To address these challenges, we\npropose an interpretable self-supervised single-image denoising framework --\nFilter2Noise (F2N). Our approach introduces an Attention-Guided Bilateral\nFilter that adapted to each noisy input through a lightweight module that\npredicts spatially varying filter parameters, which can be visualized and\nadjusted post-training for user-controlled denoising in specific regions of\ninterest. To enable single-image training, we introduce a novel downsampling\nshuffle strategy with a new self-supervised loss function that extends the\nconcept of Noise2Noise to a single image and addresses spatially correlated\nnoise. On the Mayo Clinic 2016 low-dose CT dataset, F2N outperforms the leading\nself-supervised single-image method (ZS-N2N) by 4.59 dB PSNR while improving\ntransparency, user control, and parametric efficiency. These features provide\nkey advantages for medical applications that require precise and interpretable\nnoise reduction. Our code is demonstrated at\nhttps://github.com/sypsyp97/Filter2Noise.git .", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u89e3\u91ca\u7684\u81ea\u76d1\u7763\u5355\u56fe\u50cf\u53bb\u566a\u6846\u67b6Filter2Noise\uff08F2N\uff09\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u5f15\u5bfc\u7684\u53cc\u8fb9\u6ee4\u6ce2\u5668\u548c\u8f7b\u91cf\u7ea7\u6a21\u5757\u5b9e\u73b0\u7528\u6237\u53ef\u63a7\u7684\u53bb\u566a\uff0c\u5e76\u5728\u4f4e\u5242\u91cfCT\u6570\u636e\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u89e3\u51b3\u76d1\u7763\u65b9\u6cd5\u4f9d\u8d56\u914d\u5bf9\u6570\u636e\u3001\u81ea\u76d1\u7763\u65b9\u6cd5\u4f9d\u8d56\u591a\u566a\u58f0\u56fe\u50cf\u4e14\u7f3a\u4e4f\u673a\u5236\u89e3\u91ca\u7684\u95ee\u9898\u3002", "method": "\u5f15\u5165\u6ce8\u610f\u529b\u5f15\u5bfc\u7684\u53cc\u8fb9\u6ee4\u6ce2\u5668\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u6a21\u5757\u9884\u6d4b\u7a7a\u95f4\u53d8\u5316\u7684\u6ee4\u6ce2\u5668\u53c2\u6570\uff0c\u5e76\u63d0\u51fa\u65b0\u7684\u4e0b\u91c7\u6837\u7b56\u7565\u548c\u81ea\u76d1\u7763\u635f\u5931\u51fd\u6570\u3002", "result": "\u5728Mayo Clinic 2016\u4f4e\u5242\u91cfCT\u6570\u636e\u96c6\u4e0a\uff0cPSNR\u6bd4ZS-N2N\u9ad84.59 dB\uff0c\u540c\u65f6\u63d0\u5347\u900f\u660e\u5ea6\u548c\u7528\u6237\u63a7\u5236\u3002", "conclusion": "F2N\u4e3a\u533b\u5b66\u5e94\u7528\u63d0\u4f9b\u4e86\u7cbe\u786e\u4e14\u53ef\u89e3\u91ca\u7684\u53bb\u566a\u65b9\u6cd5\uff0c\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002"}}
{"id": "2504.13682", "pdf": "https://arxiv.org/pdf/2504.13682", "abs": "https://arxiv.org/abs/2504.13682", "authors": ["Mengyuan Li", "Changhong Fu", "Ziyu Lu", "Zijie Zhang", "Haobo Zuo", "Liangliang Yao"], "title": "AnyTSR: Any-Scale Thermal Super-Resolution for UAV", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Thermal imaging can greatly enhance the application of intelligent unmanned\naerial vehicles (UAV) in challenging environments. However, the inherent low\nresolution of thermal sensors leads to insufficient details and blurred\nboundaries. Super-resolution (SR) offers a promising solution to address this\nissue, while most existing SR methods are designed for fixed-scale SR. They are\ncomputationally expensive and inflexible in practical applications. To address\nabove issues, this work proposes a novel any-scale thermal SR method (AnyTSR)\nfor UAV within a single model. Specifically, a new image encoder is proposed to\nexplicitly assign specific feature code to enable more accurate and flexible\nrepresentation. Additionally, by effectively embedding coordinate offset\ninformation into the local feature ensemble, an innovative any-scale upsampler\nis proposed to better understand spatial relationships and reduce artifacts.\nMoreover, a novel dataset (UAV-TSR), covering both land and water scenes, is\nconstructed for thermal SR tasks. Experimental results demonstrate that the\nproposed method consistently outperforms state-of-the-art methods across all\nscaling factors as well as generates more accurate and detailed high-resolution\nimages. The code is located at https://github.com/vision4robotics/AnyTSR.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u4efb\u610f\u5c3a\u5ea6\u70ed\u6210\u50cf\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\uff08AnyTSR\uff09\uff0c\u901a\u8fc7\u5355\u6a21\u578b\u89e3\u51b3\u65e0\u4eba\u673a\u70ed\u6210\u50cf\u4f4e\u5206\u8fa8\u7387\u95ee\u9898\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u70ed\u6210\u50cf\u4f20\u611f\u5668\u5206\u8fa8\u7387\u4f4e\u5bfc\u81f4\u7ec6\u8282\u4e0d\u8db3\u548c\u8fb9\u754c\u6a21\u7cca\uff0c\u73b0\u6709\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\u591a\u4e3a\u56fa\u5b9a\u5c3a\u5ea6\u4e14\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u7f3a\u4e4f\u7075\u6d3b\u6027\u3002", "method": "\u8bbe\u8ba1\u65b0\u578b\u56fe\u50cf\u7f16\u7801\u5668\u5206\u914d\u7279\u5f81\u7801\uff0c\u5d4c\u5165\u5750\u6807\u504f\u79fb\u4fe1\u606f\u5230\u5c40\u90e8\u7279\u5f81\u96c6\u5408\u4e2d\uff0c\u63d0\u51fa\u4efb\u610f\u5c3a\u5ea6\u4e0a\u91c7\u6837\u5668\u3002\u6784\u5efa\u65e0\u4eba\u673a\u70ed\u6210\u50cf\u6570\u636e\u96c6\uff08UAV-TSR\uff09\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6240\u6709\u5c3a\u5ea6\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u751f\u6210\u7684\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u66f4\u51c6\u786e\u548c\u8be6\u7ec6\u3002", "conclusion": "AnyTSR\u4e3a\u65e0\u4eba\u673a\u70ed\u6210\u50cf\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u7075\u6d3b\u7684\u4efb\u610f\u5c3a\u5ea6\u8d85\u5206\u8fa8\u7387\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.13527", "pdf": "https://arxiv.org/pdf/2504.13527", "abs": "https://arxiv.org/abs/2504.13527", "authors": ["Corentin Larroche"], "title": "Designing a reliable lateral movement detector using a graph foundation model", "categories": ["cs.CR", "cs.LG"], "comment": null, "summary": "Foundation models have recently emerged as a new paradigm in machine learning\n(ML). These models are pre-trained on large and diverse datasets and can\nsubsequently be applied to various downstream tasks with little or no\nretraining. This allows people without advanced ML expertise to build ML\napplications, accelerating innovation across many fields. However, the adoption\nof foundation models in cybersecurity is hindered by their inability to\nefficiently process data such as network traffic captures or binary\nexecutables. The recent introduction of graph foundation models (GFMs) could\nmake a significant difference, as graphs are well-suited to representing these\ntypes of data. We study the usability of GFMs in cybersecurity through the lens\nof one specific use case, namely lateral movement detection. Using a\npre-trained GFM, we build a detector that reaches state-of-the-art performance\nwithout requiring any training on domain-specific data. This case study thus\nprovides compelling evidence of the potential of GFMs for cybersecurity.", "AI": {"tldr": "\u57fa\u7840\u6a21\u578b\uff08\u5982GFMs\uff09\u5728\u7f51\u7edc\u5b89\u5168\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u901a\u8fc7\u6a2a\u5411\u79fb\u52a8\u68c0\u6d4b\u6848\u4f8b\u5c55\u793a\u4e86\u65e0\u9700\u9886\u57df\u6570\u636e\u8bad\u7ec3\u5373\u53ef\u8fbe\u5230\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u57fa\u7840\u6a21\u578b\u5728\u673a\u5668\u5b66\u4e60\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u7f51\u7edc\u5b89\u5168\u9886\u57df\u56e0\u6570\u636e\u5904\u7406\u95ee\u9898\u800c\u53d7\u9650\uff0cGFMs\u53ef\u80fd\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u9884\u8bad\u7ec3\u7684\u56fe\u57fa\u7840\u6a21\u578b\uff08GFM\uff09\u6784\u5efa\u6a2a\u5411\u79fb\u52a8\u68c0\u6d4b\u5668\uff0c\u65e0\u9700\u9886\u57df\u6570\u636e\u8bad\u7ec3\u3002", "result": "\u68c0\u6d4b\u5668\u5728\u6a2a\u5411\u79fb\u52a8\u68c0\u6d4b\u4e2d\u8fbe\u5230\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "GFMs\u5728\u7f51\u7edc\u5b89\u5168\u4e2d\u5177\u6709\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2504.13700", "pdf": "https://arxiv.org/pdf/2504.13700", "abs": "https://arxiv.org/abs/2504.13700", "authors": ["Zhen Wen", "Luoxuan Weng", "Yinghao Tang", "Runjin Zhang", "Yuxin Liu", "Bo Pan", "Minfeng Zhu", "Wei Chen"], "title": "Exploring Multimodal Prompt for Visualization Authoring with Large Language Models", "categories": ["cs.HC", "cs.AI"], "comment": "11 pages, 8 figures", "summary": "Recent advances in large language models (LLMs) have shown great potential in\nautomating the process of visualization authoring through simple natural\nlanguage utterances. However, instructing LLMs using natural language is\nlimited in precision and expressiveness for conveying visualization intent,\nleading to misinterpretation and time-consuming iterations. To address these\nlimitations, we conduct an empirical study to understand how LLMs interpret\nambiguous or incomplete text prompts in the context of visualization authoring,\nand the conditions making LLMs misinterpret user intent. Informed by the\nfindings, we introduce visual prompts as a complementary input modality to text\nprompts, which help clarify user intent and improve LLMs' interpretation\nabilities. To explore the potential of multimodal prompting in visualization\nauthoring, we design VisPilot, which enables users to easily create\nvisualizations using multimodal prompts, including text, sketches, and direct\nmanipulations on existing visualizations. Through two case studies and a\ncontrolled user study, we demonstrate that VisPilot provides a more intuitive\nway to create visualizations without affecting the overall task efficiency\ncompared to text-only prompting approaches. Furthermore, we analyze the impact\nof text and visual prompts in different visualization tasks. Our findings\nhighlight the importance of multimodal prompting in improving the usability of\nLLMs for visualization authoring. We discuss design implications for future\nvisualization systems and provide insights into how multimodal prompts can\nenhance human-AI collaboration in creative visualization tasks. All materials\nare available at https://OSF.IO/2QRAK.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u591a\u6a21\u6001\u63d0\u793a\uff08\u7ed3\u5408\u6587\u672c\u548c\u89c6\u89c9\u8f93\u5165\uff09\u5982\u4f55\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u53ef\u89c6\u5316\u521b\u4f5c\u4e2d\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\uff0c\u5e76\u5f00\u53d1\u4e86VisPilot\u5de5\u5177\u8fdb\u884c\u9a8c\u8bc1\u3002", "motivation": "\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u5728\u53ef\u89c6\u5316\u521b\u4f5c\u4e2d\u5b58\u5728\u8868\u8fbe\u4e0d\u7cbe\u786e\u548c\u6613\u8bef\u89e3\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86LLM\u7684\u6f5c\u529b\u3002", "method": "\u901a\u8fc7\u5b9e\u8bc1\u7814\u7a76\u5206\u6790LLM\u5bf9\u6a21\u7cca\u6587\u672c\u63d0\u793a\u7684\u89e3\u8bfb\uff0c\u5e76\u5f15\u5165\u89c6\u89c9\u63d0\u793a\u4f5c\u4e3a\u8865\u5145\u8f93\u5165\u65b9\u5f0f\uff0c\u8bbe\u8ba1\u4e86VisPilot\u5de5\u5177\u652f\u6301\u591a\u6a21\u6001\u63d0\u793a\u3002", "result": "VisPilot\u6bd4\u7eaf\u6587\u672c\u63d0\u793a\u66f4\u76f4\u89c2\u4e14\u4e0d\u5f71\u54cd\u6548\u7387\uff0c\u591a\u6a21\u6001\u63d0\u793a\u663e\u8457\u63d0\u5347\u4e86LLM\u7684\u53ef\u89c6\u5316\u521b\u4f5c\u53ef\u7528\u6027\u3002", "conclusion": "\u591a\u6a21\u6001\u63d0\u793a\u662f\u63d0\u5347LLM\u5728\u53ef\u89c6\u5316\u521b\u4f5c\u4e2d\u53ef\u7528\u6027\u7684\u5173\u952e\uff0c\u672a\u6765\u7cfb\u7edf\u8bbe\u8ba1\u5e94\u6ce8\u91cd\u591a\u6a21\u6001\u8f93\u5165\u3002"}}
{"id": "2504.13717", "pdf": "https://arxiv.org/pdf/2504.13717", "abs": "https://arxiv.org/abs/2504.13717", "authors": ["Gianluca Carloni"], "title": "Human-aligned Deep Learning: Explainability, Causality, and Biological Inspiration", "categories": ["cs.CV", "cs.AI", "cs.LG", "eess.IV", "q-bio.NC", "I.2; I.2.6; I.4; I.4.7; I.5; J.3; J.6"], "comment": "Personal adaptation and expansion of doctoral thesis (originally\n  submitted in Oct 2024, revisioned in Jan 2025)", "summary": "This work aligns deep learning (DL) with human reasoning capabilities and\nneeds to enable more efficient, interpretable, and robust image classification.\nWe approach this from three perspectives: explainability, causality, and\nbiological vision. Introduction and background open this work before diving\ninto operative chapters. First, we assess neural networks' visualization\ntechniques for medical images and validate an explainable-by-design method for\nbreast mass classification. A comprehensive review at the intersection of XAI\nand causality follows, where we introduce a general scaffold to organize past\nand future research, laying the groundwork for our second perspective. In the\ncausality direction, we propose novel modules that exploit feature\nco-occurrence in medical images, leading to more effective and explainable\npredictions. We further introduce CROCODILE, a general framework that\nintegrates causal concepts, contrastive learning, feature disentanglement, and\nprior knowledge to enhance generalization. Lastly, we explore biological\nvision, examining how humans recognize objects, and propose CoCoReco, a\nconnectivity-inspired network with context-aware attention mechanisms. Overall,\nour key findings include: (i) simple activation maximization lacks insight for\nmedical imaging DL models; (ii) prototypical-part learning is effective and\nradiologically aligned; (iii) XAI and causal ML are deeply connected; (iv) weak\ncausal signals can be leveraged without a priori information to improve\nperformance and interpretability; (v) our framework generalizes across medical\ndomains and out-of-distribution data; (vi) incorporating biological circuit\nmotifs improves human-aligned recognition. This work contributes toward\nhuman-aligned DL and highlights pathways to bridge the gap between research and\nclinical adoption, with implications for improved trust, diagnostic accuracy,\nand safe deployment.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u53ef\u89e3\u91ca\u6027\u3001\u56e0\u679c\u6027\u548c\u751f\u7269\u89c6\u89c9\u4e09\u4e2a\u89c6\u89d2\uff0c\u5c06\u6df1\u5ea6\u5b66\u4e60\u4e0e\u4eba\u7c7b\u63a8\u7406\u80fd\u529b\u5bf9\u9f50\uff0c\u63d0\u51fa\u4e86\u4e00\u7cfb\u5217\u65b9\u6cd5\uff08\u5982CROCODILE\u548cCoCoReco\u6846\u67b6\uff09\uff0c\u63d0\u5347\u4e86\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u7684\u6548\u7387\u3001\u53ef\u89e3\u91ca\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u65e8\u5728\u4f7f\u6df1\u5ea6\u5b66\u4e60\u66f4\u9ad8\u6548\u3001\u53ef\u89e3\u91ca\u4e14\u9c81\u68d2\uff0c\u4ee5\u8d34\u8fd1\u4eba\u7c7b\u63a8\u7406\u80fd\u529b\uff0c\u4fc3\u8fdb\u4e34\u5e8a\u5e94\u7528\u7684\u4fe1\u4efb\u4e0e\u51c6\u786e\u6027\u3002", "method": "\u7ed3\u5408\u53ef\u89c6\u5316\u6280\u672f\u3001\u56e0\u679c\u6a21\u5757\u3001\u5bf9\u6bd4\u5b66\u4e60\u548c\u751f\u7269\u542f\u53d1\u7f51\u7edc\uff0c\u63d0\u51fa\u591a\u79cd\u6846\u67b6\uff08\u5982CROCODILE\u548cCoCoReco\uff09\u3002", "result": "\u53d1\u73b0\u6fc0\u6d3b\u6700\u5927\u5316\u5728\u533b\u5b66\u56fe\u50cf\u4e2d\u6548\u679c\u6709\u9650\uff0c\u539f\u578b\u5b66\u4e60\u6709\u6548\uff0cXAI\u4e0e\u56e0\u679cML\u7d27\u5bc6\u5173\u8054\uff0c\u6846\u67b6\u5728\u8de8\u57df\u6570\u636e\u4e2d\u6cdb\u5316\u6027\u5f3a\u3002", "conclusion": "\u7814\u7a76\u63a8\u52a8\u4e86\u4eba\u7c7b\u5bf9\u9f50\u7684\u6df1\u5ea6\u5b66\u4e60\uff0c\u4e3a\u4e34\u5e8a\u8f6c\u5316\u63d0\u4f9b\u4e86\u4fe1\u4efb\u3001\u51c6\u786e\u6027\u548c\u5b89\u5168\u6027\u652f\u6301\u3002"}}
{"id": "2504.13745", "pdf": "https://arxiv.org/pdf/2504.13745", "abs": "https://arxiv.org/abs/2504.13745", "authors": ["Andrea Rigo", "Luca Stornaiuolo", "Mauro Martino", "Bruno Lepri", "Nicu Sebe"], "title": "ESPLoRA: Enhanced Spatial Precision with Low-Rank Adaption in Text-to-Image Diffusion Models for High-Definition Synthesis", "categories": ["cs.CV", "cs.AI", "I.4.0"], "comment": null, "summary": "Diffusion models have revolutionized text-to-image (T2I) synthesis, producing\nhigh-quality, photorealistic images. However, they still struggle to properly\nrender the spatial relationships described in text prompts. To address the lack\nof spatial information in T2I generations, existing methods typically use\nexternal network conditioning and predefined layouts, resulting in higher\ncomputational costs and reduced flexibility. Our approach builds upon a curated\ndataset of spatially explicit prompts, meticulously extracted and synthesized\nfrom LAION-400M to ensure precise alignment between textual descriptions and\nspatial layouts. Alongside this dataset, we present ESPLoRA, a flexible\nfine-tuning framework based on Low-Rank Adaptation, specifically designed to\nenhance spatial consistency in generative models without increasing generation\ntime or compromising the quality of the outputs. In addition to ESPLoRA, we\npropose refined evaluation metrics grounded in geometric constraints, capturing\n3D spatial relations such as \\textit{in front of} or \\textit{behind}. These\nmetrics also expose spatial biases in T2I models which, even when not fully\nmitigated, can be strategically exploited by our TORE algorithm to further\nimprove the spatial consistency of generated images. Our method outperforms the\ncurrent state-of-the-art framework, CoMPaSS, by 13.33% on established spatial\nconsistency benchmarks.", "AI": {"tldr": "\u63d0\u51faESPLoRA\u6846\u67b6\u548cTORE\u7b97\u6cd5\uff0c\u901a\u8fc7\u4f4e\u79e9\u9002\u5e94\u548c\u51e0\u4f55\u7ea6\u675f\u63d0\u5347\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u7684\u7a7a\u95f4\u4e00\u81f4\u6027\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd513.33%\u3002", "motivation": "\u73b0\u6709\u6269\u6563\u6a21\u578b\u5728\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4e2d\u96be\u4ee5\u51c6\u786e\u8868\u8fbe\u7a7a\u95f4\u5173\u7cfb\uff0c\u4e14\u4f20\u7edf\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u7075\u6d3b\u6027\u4f4e\u3002", "method": "\u57fa\u4e8eLAION-400M\u6784\u5efa\u7a7a\u95f4\u660e\u786e\u63d0\u793a\u6570\u636e\u96c6\uff0c\u5f00\u53d1ESPLoRA\u6846\u67b6\u548cTORE\u7b97\u6cd5\uff0c\u7ed3\u5408\u51e0\u4f55\u7ea6\u675f\u4f18\u5316\u7a7a\u95f4\u4e00\u81f4\u6027\u3002", "result": "\u5728\u7a7a\u95f4\u4e00\u81f4\u6027\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6027\u80fd\u63d0\u534713.33%\uff0c\u4f18\u4e8e\u5f53\u524d\u6700\u4f73\u65b9\u6cd5CoMPaSS\u3002", "conclusion": "ESPLoRA\u548cTORE\u7b97\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u751f\u6210\u56fe\u50cf\u7684\u7a7a\u95f4\u4e00\u81f4\u6027\uff0c\u4e14\u4e0d\u589e\u52a0\u8ba1\u7b97\u6210\u672c\u6216\u964d\u4f4e\u8f93\u51fa\u8d28\u91cf\u3002"}}
{"id": "2504.13582", "pdf": "https://arxiv.org/pdf/2504.13582", "abs": "https://arxiv.org/abs/2504.13582", "authors": ["Zongyuan Chen", "Yan Xia", "Jiayuan Liu", "Jijia Liu", "Wenhao Tang", "Jiayu Chen", "Feng Gao", "Longfei Ma", "Hongen Liao", "Yu Wang", "Chao Yu", "Boyu Zhang", "Fei Xing"], "title": "Hysteresis-Aware Neural Network Modeling and Whole-Body Reinforcement Learning Control of Soft Robots", "categories": ["cs.RO", "cs.LG"], "comment": null, "summary": "Soft robots exhibit inherent compliance and safety, which makes them\nparticularly suitable for applications requiring direct physical interaction\nwith humans, such as surgical procedures. However, their nonlinear and\nhysteretic behavior, resulting from the properties of soft materials, presents\nsubstantial challenges for accurate modeling and control. In this study, we\npresent a soft robotic system designed for surgical applications and propose a\nhysteresis-aware whole-body neural network model that accurately captures and\npredicts the soft robot's whole-body motion, including its hysteretic behavior.\nBuilding upon the high-precision dynamic model, we construct a highly parallel\nsimulation environment for soft robot control and apply an on-policy\nreinforcement learning algorithm to efficiently train whole-body motion control\nstrategies. Based on the trained control policy, we developed a soft robotic\nsystem for surgical applications and validated it through phantom-based laser\nablation experiments in a physical environment. The results demonstrate that\nthe hysteresis-aware modeling reduces the Mean Squared Error (MSE) by 84.95\npercent compared to traditional modeling methods. The deployed control\nalgorithm achieved a trajectory tracking error ranging from 0.126 to 0.250 mm\non the real soft robot, highlighting its precision in real-world conditions.\nThe proposed method showed strong performance in phantom-based surgical\nexperiments and demonstrates its potential for complex scenarios, including\nfuture real-world clinical applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u624b\u672f\u5e94\u7528\u7684\u8f6f\u4f53\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u91c7\u7528\u6ede\u540e\u611f\u77e5\u7684\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u548c\u5f3a\u5316\u5b66\u4e60\u63a7\u5236\u7b56\u7565\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5efa\u6a21\u7cbe\u5ea6\u548c\u8fd0\u52a8\u63a7\u5236\u6027\u80fd\u3002", "motivation": "\u8f6f\u4f53\u673a\u5668\u4eba\u56e0\u5176\u67d4\u987a\u6027\u548c\u5b89\u5168\u6027\u9002\u7528\u4e8e\u624b\u672f\u7b49\u573a\u666f\uff0c\u4f46\u5176\u975e\u7ebf\u6027\u6ede\u540e\u884c\u4e3a\u5bf9\u5efa\u6a21\u548c\u63a7\u5236\u63d0\u51fa\u4e86\u6311\u6218\u3002", "method": "\u63d0\u51fa\u6ede\u540e\u611f\u77e5\u7684\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff0c\u6784\u5efa\u5e76\u884c\u4eff\u771f\u73af\u5883\uff0c\u5e94\u7528\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u63a7\u5236\u7b56\u7565\uff0c\u5e76\u901a\u8fc7\u7269\u7406\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u6ede\u540e\u611f\u77e5\u5efa\u6a21\u5c06MSE\u964d\u4f4e84.95%\uff0c\u63a7\u5236\u7b97\u6cd5\u5728\u771f\u5b9e\u673a\u5668\u4eba\u4e0a\u5b9e\u73b00.126-0.250 mm\u7684\u8f68\u8ff9\u8ddf\u8e2a\u8bef\u5dee\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u624b\u672f\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5c55\u793a\u4e86\u5176\u5728\u590d\u6742\u4e34\u5e8a\u573a\u666f\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2504.13751", "pdf": "https://arxiv.org/pdf/2504.13751", "abs": "https://arxiv.org/abs/2504.13751", "authors": ["Mahdi Jaberzadeh Ansari", "Ann Barcomb"], "title": "A Survey for What Developers Require in AI-powered Tools that Aid in Component Selection in CBSD", "categories": ["cs.SE", "cs.AI", "cs.CY"], "comment": "10 pages, 4 figures, The 29th International Conference on Evaluation\n  and Assessment in Software Engineering, 17 to 20 June, 2025, Istanbul, Turkey", "summary": "Although it has been more than four decades that the first components-based\nsoftware development (CBSD) studies were conducted, there is still no standard\nmethod or tool for component selection which is widely accepted by the\nindustry. The gulf between industry and academia contributes to the lack of an\naccepted tool. We conducted a mixed methods survey of nearly 100 people engaged\nin component-based software engineering practice or research to better\nunderstand the problems facing industry, how these needs could be addressed,\nand current best practices employed in component selection. We also sought to\nidentify and prioritize quality criteria for component selection from an\nindustry perspective. In response to the call for CBSD component selection\ntools to incorporate recent technical advances, we also explored the\nperceptions of professionals about AI-driven tools, present and envisioned.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u7ec4\u4ef6\u9009\u62e9\u5de5\u5177\u5728\u5de5\u4e1a\u754c\u7f3a\u4e4f\u6807\u51c6\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u8c03\u67e5\u8fd1100\u540d\u4ece\u4e1a\u8005\u6216\u7814\u7a76\u4eba\u5458\uff0c\u5206\u6790\u4e86\u884c\u4e1a\u9700\u6c42\u3001\u6700\u4f73\u5b9e\u8df5\u53caAI\u9a71\u52a8\u5de5\u5177\u7684\u6f5c\u529b\u3002", "motivation": "\u5c3d\u7ba1\u57fa\u4e8e\u7ec4\u4ef6\u7684\u8f6f\u4ef6\u5f00\u53d1\uff08CBSD\uff09\u7814\u7a76\u5df2\u670940\u591a\u5e74\uff0c\u4f46\u5de5\u4e1a\u754c\u4ecd\u7f3a\u4e4f\u5e7f\u6cdb\u63a5\u53d7\u7684\u7ec4\u4ef6\u9009\u62e9\u65b9\u6cd5\u6216\u5de5\u5177\uff0c\u884c\u4e1a\u4e0e\u5b66\u672f\u754c\u7684\u8131\u8282\u662f\u4e3b\u8981\u539f\u56e0\u3002", "method": "\u91c7\u7528\u6df7\u5408\u65b9\u6cd5\u8c03\u67e5\u4e86\u8fd1100\u540d\u4ece\u4e8b\u7ec4\u4ef6\u8f6f\u4ef6\u5de5\u7a0b\u5b9e\u8df5\u6216\u7814\u7a76\u7684\u4eba\u5458\uff0c\u4e86\u89e3\u884c\u4e1a\u95ee\u9898\u3001\u9700\u6c42\u53ca\u5f53\u524d\u6700\u4f73\u5b9e\u8df5\u3002", "result": "\u8c03\u67e5\u7ed3\u679c\u5305\u62ec\u884c\u4e1a\u5bf9\u7ec4\u4ef6\u9009\u62e9\u8d28\u91cf\u6807\u51c6\u7684\u4f18\u5148\u6392\u5e8f\uff0c\u4ee5\u53ca\u5bf9AI\u9a71\u52a8\u5de5\u5177\u7684\u73b0\u72b6\u548c\u672a\u6765\u6f5c\u529b\u7684\u770b\u6cd5\u3002", "conclusion": "\u7814\u7a76\u547c\u5401\u7ec4\u4ef6\u9009\u62e9\u5de5\u5177\u5e94\u7ed3\u5408\u6700\u65b0\u6280\u672f\u8fdb\u5c55\uff0c\u5c24\u5176\u662fAI\uff0c\u4ee5\u6ee1\u8db3\u884c\u4e1a\u9700\u6c42\u3002"}}
{"id": "2504.13589", "pdf": "https://arxiv.org/pdf/2504.13589", "abs": "https://arxiv.org/abs/2504.13589", "authors": ["Lam Dinh", "Sihem Cherrared", "Xiaofeng Huang", "Fabrice Guillemin"], "title": "Towards End-to-End Network Intent Management with Large Language Models", "categories": ["cs.NI", "cs.LG"], "comment": "Full paper is accepted at IFIP Networking 2025", "summary": "Large Language Models (LLMs) are likely to play a key role in Intent-Based\nNetworking (IBN) as they show remarkable performance in interpreting human\nlanguage as well as code generation, enabling the translation of high-level\nintents expressed by humans into low-level network configurations. In this\npaper, we leverage closed-source language models (i.e., Google Gemini 1.5 pro,\nChatGPT-4) and open-source models (i.e., LLama, Mistral) to investigate their\ncapacity to generate E2E network configurations for radio access networks\n(RANs) and core networks in 5G/6G mobile networks. We introduce a novel\nperformance metrics, known as FEACI, to quantitatively assess the format (F),\nexplainability (E), accuracy (A), cost (C), and inference time (I) of the\ngenerated answer; existing general metrics are unable to capture these\nfeatures. The results of our study demonstrate that open-source models can\nachieve comparable or even superior translation performance compared with the\nclosed-source models requiring costly hardware setup and not accessible to all\nusers.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u610f\u56fe\u9a71\u52a8\u7f51\u7edc\uff08IBN\uff09\u4e2d\u7684\u5e94\u7528\uff0c\u6bd4\u8f83\u4e86\u5f00\u6e90\u4e0e\u95ed\u6e90\u6a21\u578b\u5728\u751f\u62105G/6G\u7f51\u7edc\u914d\u7f6e\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u63d0\u51fa\u4e86\u65b0\u7684\u8bc4\u4f30\u6307\u6807FEACI\u3002", "motivation": "\u7814\u7a76LLMs\u5728IBN\u4e2d\u7684\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5982\u4f55\u5c06\u4eba\u7c7b\u9ad8\u7ea7\u610f\u56fe\u8f6c\u5316\u4e3a\u4f4e\u7ea7\u7f51\u7edc\u914d\u7f6e\uff0c\u5e76\u6bd4\u8f83\u5f00\u6e90\u4e0e\u95ed\u6e90\u6a21\u578b\u7684\u6027\u80fd\u3002", "method": "\u5229\u7528\u95ed\u6e90\uff08\u5982Google Gemini 1.5 pro\u3001ChatGPT-4\uff09\u548c\u5f00\u6e90\uff08\u5982LLama\u3001Mistral\uff09\u6a21\u578b\u751f\u6210\u7aef\u5230\u7aef\u7f51\u7edc\u914d\u7f6e\uff0c\u5e76\u5f15\u5165FEACI\u6307\u6807\u8bc4\u4f30\u6027\u80fd\u3002", "result": "\u5f00\u6e90\u6a21\u578b\u5728\u7ffb\u8bd1\u6027\u80fd\u4e0a\u53ef\u4e0e\u95ed\u6e90\u6a21\u578b\u5ab2\u7f8e\u751a\u81f3\u66f4\u4f18\uff0c\u4e14\u65e0\u9700\u6602\u8d35\u786c\u4ef6\u652f\u6301\u3002", "conclusion": "\u5f00\u6e90\u6a21\u578b\u5728IBN\u4e2d\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\uff0c\u4e3a\u66f4\u5e7f\u6cdb\u7528\u6237\u63d0\u4f9b\u4e86\u53ef\u884c\u9009\u62e9\u3002"}}
{"id": "2504.13754", "pdf": "https://arxiv.org/pdf/2504.13754", "abs": "https://arxiv.org/abs/2504.13754", "authors": ["Zhu Zhu", "Shuo Jiang", "Jingyuan Zheng", "Yawen Li", "Yifei Chen", "Manli Zhao", "Weizhong Gu", "Feiwei Qin", "Jinhu Wang", "Gang Yu"], "title": "Towards Accurate and Interpretable Neuroblastoma Diagnosis via Contrastive Multi-scale Pathological Image Analysis", "categories": ["cs.CV", "cs.AI"], "comment": "14pages, 8 figures", "summary": "Neuroblastoma, adrenal-derived, is among the most common pediatric solid\nmalignancies, characterized by significant clinical heterogeneity. Timely and\naccurate pathological diagnosis from hematoxylin and eosin-stained whole slide\nimages is critical for patient prognosis. However, current diagnostic practices\nprimarily rely on subjective manual examination by pathologists, leading to\ninconsistent accuracy. Existing automated whole slide image classification\nmethods encounter challenges such as poor interpretability, limited feature\nextraction capabilities, and high computational costs, restricting their\npractical clinical deployment. To overcome these limitations, we propose\nCMSwinKAN, a contrastive-learning-based multi-scale feature fusion model\ntailored for pathological image classification, which enhances the Swin\nTransformer architecture by integrating a Kernel Activation Network within its\nmultilayer perceptron and classification head modules, significantly improving\nboth interpretability and accuracy. By fusing multi-scale features and\nleveraging contrastive learning strategies, CMSwinKAN mimics clinicians'\ncomprehensive approach, effectively capturing global and local tissue\ncharacteristics. Additionally, we introduce a heuristic soft voting mechanism\nguided by clinical insights to seamlessly bridge patch-level predictions to\nwhole slide image-level classifications. We validate CMSwinKAN on the PpNTs\ndataset, which was collaboratively established with our partner hospital and\nthe publicly accessible BreakHis dataset. Results demonstrate that CMSwinKAN\nperforms better than existing state-of-the-art pathology-specific models\npre-trained on large datasets. Our source code is available at\nhttps://github.com/JSLiam94/CMSwinKAN.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5bf9\u6bd4\u5b66\u4e60\u7684\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408\u6a21\u578bCMSwinKAN\uff0c\u7528\u4e8e\u75c5\u7406\u56fe\u50cf\u5206\u7c7b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u53ef\u89e3\u91ca\u6027\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u5f53\u524d\u75c5\u7406\u8bca\u65ad\u4f9d\u8d56\u4e3b\u89c2\u4eba\u5de5\u68c0\u67e5\uff0c\u81ea\u52a8\u5316\u65b9\u6cd5\u5b58\u5728\u53ef\u89e3\u91ca\u6027\u5dee\u3001\u7279\u5f81\u63d0\u53d6\u80fd\u529b\u6709\u9650\u548c\u9ad8\u8ba1\u7b97\u6210\u672c\u7b49\u95ee\u9898\u3002", "method": "\u7ed3\u5408Swin Transformer\u67b6\u6784\u548cKernel Activation Network\uff0c\u878d\u5408\u591a\u5c3a\u5ea6\u7279\u5f81\u5e76\u5229\u7528\u5bf9\u6bd4\u5b66\u4e60\u7b56\u7565\uff0c\u6a21\u62df\u4e34\u5e8a\u533b\u751f\u7684\u7efc\u5408\u5206\u6790\u3002", "result": "\u5728PpNTs\u548cBreakHis\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u75c5\u7406\u4e13\u7528\u6a21\u578b\u3002", "conclusion": "CMSwinKAN\u4e3a\u75c5\u7406\u56fe\u50cf\u5206\u7c7b\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u89e3\u91ca\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.13623", "pdf": "https://arxiv.org/pdf/2504.13623", "abs": "https://arxiv.org/abs/2504.13623", "authors": ["Armin Iske"], "title": "On the Convergence of Irregular Sampling in Reproducing Kernel Hilbert Spaces", "categories": ["stat.ML", "cs.LG", "cs.NA", "math.NA"], "comment": null, "summary": "We analyse the convergence of sampling algorithms for functions in\nreproducing kernel Hilbert spaces (RKHS). To this end, we discuss approximation\nproperties of kernel regression under minimalistic assumptions on both the\nkernel and the input data. We first prove error estimates in the kernel's RKHS\nnorm. This leads us to new results concerning uniform convergence of kernel\nregression on compact domains. For Lipschitz continuous and H\\\"older continuous\nkernels, we prove convergence rates.", "AI": {"tldr": "\u8bba\u6587\u5206\u6790\u4e86\u518d\u751f\u6838\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\uff08RKHS\uff09\u4e2d\u91c7\u6837\u7b97\u6cd5\u7684\u6536\u655b\u6027\uff0c\u63a2\u8ba8\u4e86\u6838\u56de\u5f52\u5728\u6700\u5c0f\u5047\u8bbe\u4e0b\u7684\u903c\u8fd1\u6027\u8d28\uff0c\u5e76\u8bc1\u660e\u4e86\u6838\u8303\u6570\u4e0b\u7684\u8bef\u5dee\u4f30\u8ba1\uff0c\u6700\u7ec8\u5f97\u51fa\u7d27\u81f4\u57df\u4e0a\u6838\u56de\u5f52\u7684\u4e00\u81f4\u6536\u655b\u6027\u53ca\u6536\u655b\u901f\u7387\u3002", "motivation": "\u7814\u7a76\u6838\u56de\u5f52\u5728\u6700\u5c0f\u5047\u8bbe\u4e0b\u7684\u903c\u8fd1\u6027\u8d28\uff0c\u4ee5\u6269\u5c55\u5bf9\u91c7\u6837\u7b97\u6cd5\u6536\u655b\u6027\u7684\u7406\u89e3\u3002", "method": "\u901a\u8fc7\u5206\u6790RKHS\u4e2d\u7684\u6838\u56de\u5f52\uff0c\u8bc1\u660e\u6838\u8303\u6570\u4e0b\u7684\u8bef\u5dee\u4f30\u8ba1\uff0c\u5e76\u63a8\u5bfc\u7d27\u81f4\u57df\u4e0a\u7684\u4e00\u81f4\u6536\u655b\u6027\u53ca\u6536\u655b\u901f\u7387\u3002", "result": "\u8bc1\u660e\u4e86\u6838\u56de\u5f52\u5728\u7d27\u81f4\u57df\u4e0a\u7684\u4e00\u81f4\u6536\u655b\u6027\uff0c\u5e76\u7ed9\u51fa\u4e86Lipschitz\u8fde\u7eed\u548cH\u00f6lder\u8fde\u7eed\u6838\u7684\u6536\u655b\u901f\u7387\u3002", "conclusion": "\u7814\u7a76\u4e3a\u6838\u56de\u5f52\u7684\u6536\u655b\u6027\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u652f\u6301\uff0c\u5c24\u5176\u662f\u5728\u7d27\u81f4\u57df\u4e0a\u7684\u5e94\u7528\u3002"}}
{"id": "2504.13645", "pdf": "https://arxiv.org/pdf/2504.13645", "abs": "https://arxiv.org/abs/2504.13645", "authors": ["Numan Saeed", "Shahad Hardan", "Muhammad Ridzuan", "Nada Saadi", "Karthik Nandakumar", "Mohammad Yaqub"], "title": "Efficient Parameter Adaptation for Multi-Modal Medical Image Segmentation and Prognosis", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Cancer detection and prognosis relies heavily on medical imaging,\nparticularly CT and PET scans. Deep Neural Networks (DNNs) have shown promise\nin tumor segmentation by fusing information from these modalities. However, a\ncritical bottleneck exists: the dependency on CT-PET data concurrently for\ntraining and inference, posing a challenge due to the limited availability of\nPET scans. Hence, there is a clear need for a flexible and efficient framework\nthat can be trained with the widely available CT scans and can be still adapted\nfor PET scans when they become available. In this work, we propose a\nparameter-efficient multi-modal adaptation (PEMMA) framework for lightweight\nupgrading of a transformer-based segmentation model trained only on CT scans\nsuch that it can be efficiently adapted for use with PET scans when they become\navailable. This framework is further extended to perform prognosis task\nmaintaining the same efficient cross-modal fine-tuning approach. The proposed\napproach is tested with two well-known segementation backbones, namely UNETR\nand Swin UNETR. Our approach offers two main advantages. Firstly, we leverage\nthe inherent modularity of the transformer architecture and perform low-rank\nadaptation (LoRA) as well as decomposed low-rank adaptation (DoRA) of the\nattention weights to achieve parameter-efficient adaptation. Secondly, by\nminimizing cross-modal entanglement, PEMMA allows updates using only one\nmodality without causing catastrophic forgetting in the other. Our method\nachieves comparable performance to early fusion, but with only 8% of the\ntrainable parameters, and demonstrates a significant +28% Dice score\nimprovement on PET scans when trained with a single modality. Furthermore, in\nprognosis, our method improves the concordance index by +10% when adapting a\nCT-pretrained model to include PET scans, and by +23% when adapting for both\nPET and EHR data.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53c2\u6570\u9ad8\u6548\u7684\u591a\u6a21\u6001\u9002\u5e94\u6846\u67b6\uff08PEMMA\uff09\uff0c\u7528\u4e8e\u5728\u4ec5\u4f7f\u7528CT\u626b\u63cf\u8bad\u7ec3\u7684\u57fa\u7840\u4e0a\uff0c\u8f7b\u91cf\u7ea7\u5347\u7ea7\u57fa\u4e8eTransformer\u7684\u5206\u5272\u6a21\u578b\uff0c\u4f7f\u5176\u80fd\u9ad8\u6548\u9002\u5e94PET\u626b\u63cf\u3002\u8be5\u65b9\u6cd5\u5728\u5206\u5272\u548c\u9884\u540e\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u53c2\u6570\u6548\u7387\u9ad8\u4e14\u907f\u514d\u8de8\u6a21\u6001\u5e72\u6270\u3002", "motivation": "\u89e3\u51b3\u533b\u5b66\u5f71\u50cf\u4e2d\u56e0PET\u626b\u63cf\u6570\u636e\u7a00\u7f3a\u800c\u4f9d\u8d56CT-PET\u8054\u5408\u8bad\u7ec3\u7684\u74f6\u9888\u95ee\u9898\uff0c\u63d0\u51fa\u7075\u6d3b\u9ad8\u6548\u7684\u6846\u67b6\uff0c\u4ec5\u9700CT\u8bad\u7ec3\u5373\u53ef\u9002\u5e94PET\u3002", "method": "\u91c7\u7528\u4f4e\u79e9\u9002\u5e94\uff08LoRA\uff09\u548c\u5206\u89e3\u4f4e\u79e9\u9002\u5e94\uff08DoRA\uff09\u8c03\u6574\u6ce8\u610f\u529b\u6743\u91cd\uff0c\u5b9e\u73b0\u53c2\u6570\u9ad8\u6548\u9002\u5e94\uff1b\u901a\u8fc7\u51cf\u5c11\u8de8\u6a21\u6001\u7ea0\u7f20\uff0c\u652f\u6301\u5355\u6a21\u6001\u66f4\u65b0\u3002", "result": "\u5728PET\u626b\u63cf\u4e0aDice\u5206\u6570\u63d0\u534728%\uff0c\u53c2\u6570\u4ec5\u4e3a\u65e9\u671f\u878d\u5408\u76848%\uff1b\u9884\u540e\u4efb\u52a1\u4e2d\uff0c\u9002\u5e94PET\u540e\u4e00\u81f4\u6027\u6307\u6570\u63d0\u534710%\uff0c\u9002\u5e94PET\u548cEHR\u540e\u63d0\u534723%\u3002", "conclusion": "PEMMA\u6846\u67b6\u5728\u53c2\u6570\u6548\u7387\u548c\u6027\u80fd\u4e0a\u5747\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u4e3a\u591a\u6a21\u6001\u533b\u5b66\u5f71\u50cf\u5206\u6790\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.13763", "pdf": "https://arxiv.org/pdf/2504.13763", "abs": "https://arxiv.org/abs/2504.13763", "authors": ["Ryota Takatsuki", "Sonia Joseph", "Ippei Fujisawa", "Ryota Kanai"], "title": "Decoding Vision Transformers: the Diffusion Steering Lens", "categories": ["cs.CV", "cs.AI"], "comment": "12 pages, 17 figures. Accepted to the CVPR 2025 Workshop on\n  Mechanistic Interpretability for Vision (MIV)", "summary": "Logit Lens is a widely adopted method for mechanistic interpretability of\ntransformer-based language models, enabling the analysis of how internal\nrepresentations evolve across layers by projecting them into the output\nvocabulary space. Although applying Logit Lens to Vision Transformers (ViTs) is\ntechnically straightforward, its direct use faces limitations in capturing the\nrichness of visual representations. Building on the work of Toker et al.\n(2024)~\\cite{Toker2024-ve}, who introduced Diffusion Lens to visualize\nintermediate representations in the text encoders of text-to-image diffusion\nmodels, we demonstrate that while Diffusion Lens can effectively visualize\nresidual stream representations in image encoders, it fails to capture the\ndirect contributions of individual submodules. To overcome this limitation, we\npropose \\textbf{Diffusion Steering Lens} (DSL), a novel, training-free approach\nthat steers submodule outputs and patches subsequent indirect contributions. We\nvalidate our method through interventional studies, showing that DSL provides\nan intuitive and reliable interpretation of the internal processing in ViTs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDiffusion Steering Lens\uff08DSL\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u6539\u8fdbVision Transformers\uff08ViTs\uff09\u4e2d\u5185\u90e8\u8868\u793a\u7684\u53ef\u89c6\u5316\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\uff08\u5982Logit Lens\u548cDiffusion Lens\uff09\u7684\u5c40\u9650\u6027\u3002", "motivation": "Logit Lens\u548cDiffusion Lens\u5728\u5206\u6790ViTs\u65f6\u5b58\u5728\u4e0d\u8db3\uff0c\u65e0\u6cd5\u6355\u6349\u89c6\u89c9\u8868\u793a\u7684\u4e30\u5bcc\u6027\u6216\u76f4\u63a5\u8d21\u732e\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51faDSL\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5bfc\u5b50\u6a21\u5757\u8f93\u51fa\u5e76\u4fee\u8865\u540e\u7eed\u95f4\u63a5\u8d21\u732e\uff0c\u65e0\u9700\u8bad\u7ec3\u5373\u53ef\u5b9e\u73b0ViTs\u5185\u90e8\u5904\u7406\u7684\u76f4\u89c2\u89e3\u91ca\u3002", "result": "\u901a\u8fc7\u5e72\u9884\u7814\u7a76\u9a8c\u8bc1\uff0cDSL\u80fd\u591f\u63d0\u4f9b\u76f4\u89c2\u4e14\u53ef\u9760\u7684ViTs\u5185\u90e8\u5904\u7406\u89e3\u91ca\u3002", "conclusion": "DSL\u662f\u4e00\u79cd\u6709\u6548\u7684\u8bad\u7ec3\u514d\u8d39\u65b9\u6cd5\uff0c\u80fd\u591f\u6539\u8fdbViTs\u7684\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2504.13785", "pdf": "https://arxiv.org/pdf/2504.13785", "abs": "https://arxiv.org/abs/2504.13785", "authors": ["Steffen Hagedorn", "Aron Distelzweig", "Marcel Hallgarten", "Alexandru P. Condurache"], "title": "Learning Through Retrospection: Improving Trajectory Prediction for Automated Driving with Error Feedback", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": null, "summary": "In automated driving, predicting trajectories of surrounding vehicles\nsupports reasoning about scene dynamics and enables safe planning for the ego\nvehicle. However, existing models handle predictions as an instantaneous task\nof forecasting future trajectories based on observed information. As time\nproceeds, the next prediction is made independently of the previous one, which\nmeans that the model cannot correct its errors during inference and will repeat\nthem. To alleviate this problem and better leverage temporal data, we propose a\nnovel retrospection technique. Through training on closed-loop rollouts the\nmodel learns to use aggregated feedback. Given new observations it reflects on\nprevious predictions and analyzes its errors to improve the quality of\nsubsequent predictions. Thus, the model can learn to correct systematic errors\nduring inference. Comprehensive experiments on nuScenes and Argoverse\ndemonstrate a considerable decrease in minimum Average Displacement Error of up\nto 31.9% compared to the state-of-the-art baseline without retrospection. We\nfurther showcase the robustness of our technique by demonstrating a better\nhandling of out-of-distribution scenarios with undetected road-users.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u56de\u987e\u6280\u672f\uff0c\u901a\u8fc7\u95ed\u73af\u8bad\u7ec3\u6539\u8fdb\u81ea\u52a8\u9a7e\u9a76\u4e2d\u8f66\u8f86\u8f68\u8ff9\u9884\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u51cf\u5c11\u7cfb\u7edf\u6027\u8bef\u5dee\u3002", "motivation": "\u73b0\u6709\u6a21\u578b\u5728\u9884\u6d4b\u8f66\u8f86\u8f68\u8ff9\u65f6\u72ec\u7acb\u5904\u7406\u6bcf\u4e2a\u65f6\u95f4\u70b9\u7684\u9884\u6d4b\uff0c\u65e0\u6cd5\u7ea0\u6b63\u63a8\u7406\u4e2d\u7684\u9519\u8bef\uff0c\u5bfc\u81f4\u91cd\u590d\u8bef\u5dee\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u56de\u987e\u6280\u672f\uff0c\u901a\u8fc7\u95ed\u73af\u8bad\u7ec3\u4f7f\u6a21\u578b\u80fd\u591f\u5229\u7528\u53cd\u9988\u4fe1\u606f\uff0c\u5206\u6790\u5e76\u7ea0\u6b63\u4e4b\u524d\u7684\u9884\u6d4b\u9519\u8bef\u3002", "result": "\u5728nuScenes\u548cArgoverse\u6570\u636e\u96c6\u4e0a\uff0c\u6700\u5c0f\u5e73\u5747\u4f4d\u79fb\u8bef\u5dee\u964d\u4f4e\u4e8631.9%\uff0c\u4e14\u5728\u5904\u7406\u5206\u5e03\u5916\u573a\u666f\u65f6\u8868\u73b0\u66f4\u7a33\u5065\u3002", "conclusion": "\u56de\u987e\u6280\u672f\u663e\u8457\u63d0\u5347\u4e86\u8f68\u8ff9\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\uff0c\u9002\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u3002"}}
{"id": "2504.13791", "pdf": "https://arxiv.org/pdf/2504.13791", "abs": "https://arxiv.org/abs/2504.13791", "authors": ["Sandipan Dhar", "Md. Tousin Akhter", "Nanda Dulal Jana", "Swagatam Das"], "title": "Collective Learning Mechanism based Optimal Transport Generative Adversarial Network for Non-parallel Voice Conversion", "categories": ["cs.SD", "cs.AI", "eess.AS"], "comment": "7 pages, 2 figures, 3 tables", "summary": "After demonstrating significant success in image synthesis, Generative\nAdversarial Network (GAN) models have likewise made significant progress in the\nfield of speech synthesis, leveraging their capacity to adapt the precise\ndistribution of target data through adversarial learning processes. Notably, in\nthe realm of State-Of-The-Art (SOTA) GAN-based Voice Conversion (VC) models,\nthere exists a substantial disparity in naturalness between real and\nGAN-generated speech samples. Furthermore, while many GAN models currently\noperate on a single generator discriminator learning approach, optimizing\ntarget data distribution is more effectively achievable through a single\ngenerator multi-discriminator learning scheme. Hence, this study introduces a\nnovel GAN model named Collective Learning Mechanism-based Optimal Transport GAN\n(CLOT-GAN) model, incorporating multiple discriminators, including the Deep\nConvolutional Neural Network (DCNN) model, Vision Transformer (ViT), and\nconformer. The objective of integrating various discriminators lies in their\nability to comprehend the formant distribution of mel-spectrograms, facilitated\nby a collective learning mechanism. Simultaneously, the inclusion of Optimal\nTransport (OT) loss aims to precisely bridge the gap between the source and\ntarget data distribution, employing the principles of OT theory. The\nexperimental validation on VCC 2018, VCTK, and CMU-Arctic datasets confirms\nthat the CLOT-GAN-VC model outperforms existing VC models in objective and\nsubjective assessments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCLOT-GAN\u7684\u65b0\u578bGAN\u6a21\u578b\uff0c\u901a\u8fc7\u591a\u5224\u522b\u5668\u5b66\u4e60\u548c\u6700\u4f18\u4f20\u8f93\u635f\u5931\u63d0\u5347\u8bed\u97f3\u8f6c\u6362\u7684\u81ea\u7136\u5ea6\u548c\u6570\u636e\u5206\u5e03\u5339\u914d\u3002", "motivation": "\u73b0\u6709GAN\u8bed\u97f3\u8f6c\u6362\u6a21\u578b\u5728\u81ea\u7136\u5ea6\u4e0a\u5b58\u5728\u660e\u663e\u5dee\u8ddd\uff0c\u4e14\u5355\u751f\u6210\u5668\u5355\u5224\u522b\u5668\u5b66\u4e60\u96be\u4ee5\u4f18\u5316\u76ee\u6807\u6570\u636e\u5206\u5e03\u3002", "method": "\u5f15\u5165\u591a\u5224\u522b\u5668\uff08DCNN\u3001ViT\u3001Conformer\uff09\u548c\u6700\u4f18\u4f20\u8f93\u635f\u5931\uff0c\u901a\u8fc7\u96c6\u4f53\u5b66\u4e60\u673a\u5236\u7406\u89e3\u6885\u5c14\u8c31\u7684\u5171\u632f\u5cf0\u5206\u5e03\u3002", "result": "\u5728VCC 2018\u3001VCTK\u548cCMU-Arctic\u6570\u636e\u96c6\u4e0a\uff0cCLOT-GAN-VC\u5728\u5ba2\u89c2\u548c\u4e3b\u89c2\u8bc4\u4f30\u4e2d\u5747\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002", "conclusion": "CLOT-GAN\u901a\u8fc7\u591a\u5224\u522b\u5668\u548cOT\u635f\u5931\u663e\u8457\u63d0\u5347\u4e86\u8bed\u97f3\u8f6c\u6362\u7684\u6027\u80fd\u548c\u81ea\u7136\u5ea6\u3002"}}
{"id": "2504.13759", "pdf": "https://arxiv.org/pdf/2504.13759", "abs": "https://arxiv.org/abs/2504.13759", "authors": ["Davide Ghiani", "Jefferson David Rodriguez Chivata", "Stefano Lilliu", "Simone Maurizio La Cava", "Marco Micheletto", "Giulia Orr\u00f9", "Federico Lama", "Gian Luca Marcialis"], "title": "Fragile Watermarking for Image Certification Using Deep Steganographic Embedding", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Modern identity verification systems increasingly rely on facial images\nembedded in biometric documents such as electronic passports. To ensure global\ninteroperability and security, these images must comply with strict standards\ndefined by the International Civil Aviation Organization (ICAO), which specify\nacquisition, quality, and format requirements. However, once issued, these\nimages may undergo unintentional degradations (e.g., compression, resizing) or\nmalicious manipulations (e.g., morphing) and deceive facial recognition\nsystems. In this study, we explore fragile watermarking, based on deep\nsteganographic embedding as a proactive mechanism to certify the authenticity\nof ICAO-compliant facial images. By embedding a hidden image within the\nofficial photo at the time of issuance, we establish an integrity marker that\nbecomes sensitive to any post-issuance modification. We assess how a range of\nimage manipulations affects the recovered hidden image and show that\ndegradation artifacts can serve as robust forensic cues. Furthermore, we\npropose a classification framework that analyzes the revealed content to detect\nand categorize the type of manipulation applied. Our experiments demonstrate\nhigh detection accuracy, including cross-method scenarios with multiple deep\nsteganography-based models. These findings support the viability of fragile\nwatermarking via steganographic embedding as a valuable tool for biometric\ndocument integrity verification.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u57fa\u4e8e\u6df1\u5ea6\u9690\u5199\u5d4c\u5165\u7684\u8106\u5f31\u6c34\u5370\u6280\u672f\uff0c\u7528\u4e8e\u9a8c\u8bc1ICAO\u6807\u51c6\u9762\u90e8\u56fe\u50cf\u7684\u771f\u5b9e\u6027\uff0c\u9632\u6b62\u7be1\u6539\u548c\u9000\u5316\u3002", "motivation": "\u73b0\u4ee3\u8eab\u4efd\u9a8c\u8bc1\u7cfb\u7edf\u4f9d\u8d56\u9762\u90e8\u56fe\u50cf\uff0c\u4f46\u56fe\u50cf\u53ef\u80fd\u88ab\u7be1\u6539\u6216\u9000\u5316\uff0c\u5a01\u80c1\u5b89\u5168\u6027\u3002", "method": "\u901a\u8fc7\u6df1\u5ea6\u9690\u5199\u5d4c\u5165\u5728\u5b98\u65b9\u7167\u7247\u4e2d\u9690\u85cf\u56fe\u50cf\uff0c\u5efa\u7acb\u5b8c\u6574\u6027\u6807\u8bb0\uff0c\u5e76\u8bbe\u8ba1\u5206\u7c7b\u6846\u67b6\u68c0\u6d4b\u7be1\u6539\u7c7b\u578b\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u9ad8\u68c0\u6d4b\u51c6\u786e\u7387\uff0c\u652f\u6301\u8106\u5f31\u6c34\u5370\u6280\u672f\u7684\u53ef\u884c\u6027\u3002", "conclusion": "\u6df1\u5ea6\u9690\u5199\u5d4c\u5165\u7684\u8106\u5f31\u6c34\u5370\u662f\u9a8c\u8bc1\u751f\u7269\u7279\u5f81\u6587\u6863\u5b8c\u6574\u6027\u7684\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2504.13804", "pdf": "https://arxiv.org/pdf/2504.13804", "abs": "https://arxiv.org/abs/2504.13804", "authors": ["Robert Busa-Fekete", "Umar Syed"], "title": "Near-optimal algorithms for private estimation and sequential testing of collision probability", "categories": ["stat.ML", "cs.AI", "cs.LG"], "comment": null, "summary": "We present new algorithms for estimating and testing \\emph{collision\nprobability}, a fundamental measure of the spread of a discrete distribution\nthat is widely used in many scientific fields. We describe an algorithm that\nsatisfies $(\\alpha, \\beta)$-local differential privacy and estimates collision\nprobability with error at most $\\epsilon$ using\n$\\tilde{O}\\left(\\frac{\\log(1/\\beta)}{\\alpha^2 \\epsilon^2}\\right)$ samples for\n$\\alpha \\le 1$, which improves over previous work by a factor of\n$\\frac{1}{\\alpha^2}$. We also present a sequential testing algorithm for\ncollision probability, which can distinguish between collision probability\nvalues that are separated by $\\epsilon$ using $\\tilde{O}(\\frac{1}{\\epsilon^2})$\nsamples, even when $\\epsilon$ is unknown. Our algorithms have nearly the\noptimal sample complexity, and in experiments we show that they require\nsignificantly fewer samples than previous methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u65b0\u7684\u7b97\u6cd5\u6765\u4f30\u8ba1\u548c\u6d4b\u8bd5\u78b0\u649e\u6982\u7387\uff0c\u6539\u8fdb\u4e86\u6837\u672c\u590d\u6742\u5ea6\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u5176\u9ad8\u6548\u6027\u3002", "motivation": "\u78b0\u649e\u6982\u7387\u662f\u79bb\u6563\u5206\u5e03\u7684\u91cd\u8981\u5ea6\u91cf\uff0c\u5e7f\u6cdb\u5e94\u7528\u4e8e\u79d1\u5b66\u9886\u57df\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u7684\u6837\u672c\u590d\u6742\u5ea6\u8f83\u9ad8\uff0c\u9700\u8981\u6539\u8fdb\u3002", "method": "\u63d0\u51fa\u4e86\u6ee1\u8db3\u5c40\u90e8\u5dee\u5206\u9690\u79c1\u7684\u4f30\u8ba1\u7b97\u6cd5\u548c\u987a\u5e8f\u6d4b\u8bd5\u7b97\u6cd5\uff0c\u5206\u522b\u4f18\u5316\u4e86\u6837\u672c\u590d\u6742\u5ea6\u548c\u9002\u5e94\u672a\u77e5\u53c2\u6570\u7684\u80fd\u529b\u3002", "result": "\u65b0\u7b97\u6cd5\u5728\u6837\u672c\u590d\u6742\u5ea6\u4e0a\u63a5\u8fd1\u6700\u4f18\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u9ad8\u6548\u3002", "conclusion": "\u65b0\u7b97\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u78b0\u649e\u6982\u7387\u4f30\u8ba1\u548c\u6d4b\u8bd5\u7684\u6548\u7387\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2504.13803", "pdf": "https://arxiv.org/pdf/2504.13803", "abs": "https://arxiv.org/abs/2504.13803", "authors": ["Yilong Song"], "title": "Imitation Learning with Precisely Labeled Human Demonstrations", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Within the imitation learning paradigm, training generalist robots requires\nlarge-scale datasets obtainable only through diverse curation. Due to the\nrelative ease to collect, human demonstrations constitute a valuable addition\nwhen incorporated appropriately. However, existing methods utilizing human\ndemonstrations face challenges in inferring precise actions, ameliorating\nembodiment gaps, and fusing with frontier generalist robot training pipelines.\nIn this work, building on prior studies that demonstrate the viability of using\nhand-held grippers for efficient data collection, we leverage the user's\ncontrol over the gripper's appearance--specifically by assigning it a unique,\neasily segmentable color--to enable simple and reliable application of the\nRANSAC and ICP registration method for precise end-effector pose estimation. We\nshow in simulation that precisely labeled human demonstrations on their own\nallow policies to reach on average 88.1% of the performance of using robot\ndemonstrations, and boost policy performance when combined with robot\ndemonstrations, despite the inherent embodiment gap.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u624b\u6301\u5939\u722a\u548c\u989c\u8272\u5206\u5272\u6280\u672f\u6539\u8fdb\u4eba\u7c7b\u793a\u8303\u6570\u636e\u7684\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u6a21\u4eff\u5b66\u4e60\u4e2d\u673a\u5668\u4eba\u7b56\u7565\u7684\u6027\u80fd\u3002", "motivation": "\u5728\u6a21\u4eff\u5b66\u4e60\u4e2d\uff0c\u4eba\u7c7b\u793a\u8303\u6570\u636e\u867d\u6613\u4e8e\u6536\u96c6\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u7cbe\u786e\u63a8\u65ad\u52a8\u4f5c\u3001\u5f25\u8865\u672c\u4f53\u5dee\u5f02\uff0c\u5e76\u4e0e\u524d\u6cbf\u673a\u5668\u4eba\u8bad\u7ec3\u6d41\u7a0b\u878d\u5408\u3002", "method": "\u901a\u8fc7\u4e3a\u624b\u6301\u5939\u722a\u5206\u914d\u72ec\u7279\u989c\u8272\uff0c\u5229\u7528RANSAC\u548cICP\u914d\u51c6\u65b9\u6cd5\u7cbe\u786e\u4f30\u8ba1\u672b\u7aef\u6267\u884c\u5668\u59ff\u6001\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u663e\u793a\uff0c\u7cbe\u786e\u6807\u6ce8\u7684\u4eba\u7c7b\u793a\u8303\u6570\u636e\u5355\u72ec\u4f7f\u7528\u65f6\u53ef\u8fbe\u673a\u5668\u4eba\u793a\u8303\u6027\u80fd\u768488.1%\uff0c\u4e0e\u673a\u5668\u4eba\u793a\u8303\u7ed3\u5408\u65f6\u8fdb\u4e00\u6b65\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u4eba\u7c7b\u793a\u8303\u6570\u636e\u7684\u7cbe\u786e\u6027\u95ee\u9898\uff0c\u5e76\u63d0\u5347\u4e86\u673a\u5668\u4eba\u7b56\u7565\u7684\u6027\u80fd\u3002"}}
{"id": "2504.13811", "pdf": "https://arxiv.org/pdf/2504.13811", "abs": "https://arxiv.org/abs/2504.13811", "authors": ["Feijiang Han", "Jiaming Zhang", "Chuyi Deng", "Jianheng Tang", "Yunhuai Liu"], "title": "Can LLMs handle WebShell detection? Overcoming Detection Challenges with Behavioral Function-Aware Framework", "categories": ["cs.CR", "cs.LG"], "comment": "Under Review", "summary": "WebShell attacks, in which malicious scripts are injected into web servers,\nare a major cybersecurity threat. Traditional machine learning and deep\nlearning methods are hampered by issues such as the need for extensive training\ndata, catastrophic forgetting, and poor generalization. Recently, Large\nLanguage Models (LLMs) have gained attention for code-related tasks, but their\npotential in WebShell detection remains underexplored. In this paper, we make\ntwo major contributions: (1) a comprehensive evaluation of seven LLMs,\nincluding GPT-4, LLaMA 3.1 70B, and Qwen 2.5 variants, benchmarked against\ntraditional sequence- and graph-based methods using a dataset of 26.59K PHP\nscripts, and (2) the Behavioral Function-Aware Detection (BFAD) framework,\ndesigned to address the specific challenges of applying LLMs to this domain.\nOur framework integrates three components: a Critical Function Filter that\nisolates malicious PHP function calls, a Context-Aware Code Extraction strategy\nthat captures the most behaviorally indicative code segments, and Weighted\nBehavioral Function Profiling (WBFP) that enhances in-context learning by\nprioritizing the most relevant demonstrations based on discriminative\nfunction-level profiles. Our results show that larger LLMs achieve near-perfect\nprecision but lower recall, while smaller models exhibit the opposite\ntrade-off. However, all models lag behind previous State-Of-The-Art (SOTA)\nmethods. With BFAD, the performance of all LLMs improved, with an average F1\nscore increase of 13.82%. Larger models such as GPT-4, LLaMA 3.1 70B, and Qwen\n2.5 14B outperform SOTA methods, while smaller models such as Qwen 2.5 3B\nachieve performance competitive with traditional methods. This work is the\nfirst to explore the feasibility and limitations of LLMs for WebShell\ndetection, and provides solutions to address the challenges in this task.", "AI": {"tldr": "\u8bba\u6587\u8bc4\u4f30\u4e86\u4e03\u79cd\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728WebShell\u68c0\u6d4b\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u63d0\u51fa\u4e86BFAD\u6846\u67b6\u4ee5\u63d0\u5347\u6027\u80fd\u3002\u7ed3\u679c\u8868\u660e\uff0c\u8f83\u5927LLMs\u7cbe\u5ea6\u9ad8\u4f46\u53ec\u56de\u7387\u4f4e\uff0c\u800cBFAD\u663e\u8457\u63d0\u5347\u4e86\u6240\u6709\u6a21\u578b\u7684F1\u5206\u6570\u3002", "motivation": "\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u548c\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728WebShell\u68c0\u6d4b\u4e2d\u5b58\u5728\u6570\u636e\u9700\u6c42\u5927\u3001\u6cdb\u5316\u80fd\u529b\u5dee\u7b49\u95ee\u9898\uff0cLLMs\u7684\u6f5c\u529b\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u63d0\u51fa\u4e86BFAD\u6846\u67b6\uff0c\u5305\u62ec\u5173\u952e\u529f\u80fd\u8fc7\u6ee4\u5668\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u4ee3\u7801\u63d0\u53d6\u548c\u52a0\u6743\u884c\u4e3a\u529f\u80fd\u5206\u6790\uff0c\u4ee5\u4f18\u5316LLMs\u5728WebShell\u68c0\u6d4b\u4e2d\u7684\u5e94\u7528\u3002", "result": "\u8f83\u5927LLMs\uff08\u5982GPT-4\uff09\u5728BFAD\u652f\u6301\u4e0b\u8d85\u8d8a\u4f20\u7edf\u65b9\u6cd5\uff0c\u8f83\u5c0fLLMs\u6027\u80fd\u4e0e\u4f20\u7edf\u65b9\u6cd5\u76f8\u5f53\uff0c\u5e73\u5747F1\u5206\u6570\u63d0\u534713.82%\u3002", "conclusion": "LLMs\u5728WebShell\u68c0\u6d4b\u4e2d\u5177\u6709\u6f5c\u529b\uff0cBFAD\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u5176\u5e94\u7528\u4e2d\u7684\u6311\u6218\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}

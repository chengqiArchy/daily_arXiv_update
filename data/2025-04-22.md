<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 66]
- [cs.AI](#cs.AI) [Total: 55]
- [cs.LG](#cs.LG) [Total: 118]
- [math.OC](#math.OC) [Total: 1]
- [math.CO](#math.CO) [Total: 1]
- [q-fin.PM](#q-fin.PM) [Total: 1]
- [eess.IV](#eess.IV) [Total: 1]
- [q-bio.BM](#q-bio.BM) [Total: 1]
- [cs.HC](#cs.HC) [Total: 52]
- [cs.CR](#cs.CR) [Total: 6]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 2]
- [cs.IR](#cs.IR) [Total: 7]
- [cs.SI](#cs.SI) [Total: 2]
- [physics.optics](#physics.optics) [Total: 3]
- [cs.NI](#cs.NI) [Total: 3]
- [quant-ph](#quant-ph) [Total: 4]
- [cs.NE](#cs.NE) [Total: 1]
- [math.NA](#math.NA) [Total: 1]
- [cs.MM](#cs.MM) [Total: 1]
- [stat.ML](#stat.ML) [Total: 5]
- [stat.AP](#stat.AP) [Total: 1]
- [cs.CV](#cs.CV) [Total: 49]
- [cs.DC](#cs.DC) [Total: 4]
- [eess.SY](#eess.SY) [Total: 1]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 2]
- [cs.AR](#cs.AR) [Total: 4]
- [cs.RO](#cs.RO) [Total: 12]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [cs.CY](#cs.CY) [Total: 12]
- [cs.IT](#cs.IT) [Total: 2]
- [eess.SP](#eess.SP) [Total: 1]
- [cs.SD](#cs.SD) [Total: 2]
- [cs.SE](#cs.SE) [Total: 7]
- [eess.AS](#eess.AS) [Total: 3]
- [cs.CC](#cs.CC) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Seed-Thinking-v1.5: Advancing Superb Reasoning Models with Reinforcement Learning](https://arxiv.org/abs/2504.13914)
*ByteDance Seed,:,Yufeng Yuan,Yu Yue,Mingxuan Wang,Xiaochen Zuo,Jiaze Chen,Lin Yan,Wenyuan Xu,Chi Zhang,Xin Liu,Chengyi Wang,TianTian Fan,Lingjun Liu,Qiying Yu,Xiangpeng Wei,Zhiqi Lin,Ruofei Zhu,Qingping Yang,Chengzhi Wei,Jerry He,Guanlin Liu,Zheng Wu,Xiangyu Yu,Zhicheng Liu,Jingjing Xu,Jiangjie Chen,Haojie Pan,Shengding Hu,Zhengyin Du,Wenqi Wang,Zewei Sun,Chenwei Lou,Bole Ma,Zihan Wang,Mofan Zhang,Wang Zhang,Gaohong Liu,Kaihua Jiang,Haibin Lin,Ru Zhang,Juncai Liu,Li Han,Jinxin Chi,Wenqiang Zhang,Jiayi Xu,Jun Yuan,Zhen Xiao,Yuqiao Xian,Jingqiao Wu,Kai Hua,Na Zhou,Jianhui Duan,Heyang Lu,Changbao Wang,Jinxiang Ou,Shihang Wang,Xiaoran Jin,Xuesong Yao,Chengyin Xu,Wenchang Ma,Zhecheng An,Renming Pang,Xia Xiao,Jing Su,Yuyu Zhang,Tao Sun,Kaibo Liu,Yifan Sun,Kai Shen,Sijun Zhang,Yiyuan Ma,Xingyan Bin,Ji Li,Yao Luo,Deyi Liu,Shiyi Zhan,Yunshui Li,Yuan Yang,Defa Zhu,Ke Shen,Chenggang Li,Xun Zhou,Liang Xiang,Yonghui Wu*

Main category: cs.CL

TL;DR: Seed-Thinking-v1.5是一种基于思考后响应的推理模型，在多个基准测试中表现优异，尤其在STEM和编程领域。


<details>
  <summary>Details</summary>
Motivation: 提升推理能力并验证模型在广泛任务中的泛化性能。

Method: 采用混合专家（MoE）架构，激活参数20B，总参数200B。

Result: 在AIME 2024、Codeforces和GPQA上分别达到86.7、55.0和77.3分，非推理任务胜率提升8%。

Conclusion: Seed-Thinking-v1.5展示了强大的推理和泛化能力，并计划公开新基准支持研究。

Abstract: We introduce Seed-Thinking-v1.5, capable of reasoning through thinking before
responding, resulting in improved performance on a wide range of benchmarks.
Seed-Thinking-v1.5 achieves 86.7 on AIME 2024, 55.0 on Codeforces and 77.3 on
GPQA, demonstrating excellent reasoning abilities in STEM and coding. Beyond
reasoning tasks, the method demonstrates notable generalization across diverse
domains. For instance, it surpasses DeepSeek R1 by 8% in win rate on
non-reasoning tasks, indicating its broader applicability. Compared to other
state-of-the-art reasoning models, Seed-Thinking-v1.5 is a Mixture-of-Experts
(MoE) model with a relatively small size, featuring 20B activated and 200B
total parameters. As part of our effort to assess generalized reasoning, we
develop two internal benchmarks, BeyondAIME and Codeforces, both of which will
be publicly released to support future research.

</details>


### [2] [Uncovering Conspiratorial Narratives within Arabic Online Content](https://arxiv.org/abs/2504.14037)
*Djamila Mohdeb,Meriem Laifa,Zineb Guemraoui,Dalila Behih*

Main category: cs.CL

TL;DR: 通过计算分析阿拉伯数字空间中的阴谋论传播，研究发现六类阴谋论叙事，填补了阿拉伯语内容研究的空白。


<details>
  <summary>Details</summary>
Motivation: 研究旨在填补阴谋论研究中阿拉伯语内容的空白，揭示其在阿拉伯社交媒体中的表现和影响。

Method: 结合命名实体识别和主题建模（Top2Vec算法），分析阿拉伯博客和Facebook数据。

Result: 识别出六类阴谋论叙事，揭示了其在阿拉伯社交媒体中的嵌入性和区域性特征。

Conclusion: 研究为理解阿拉伯数字空间中阴谋论的表现和演变提供了新视角。

Abstract: This study investigates the spread of conspiracy theories in Arabic digital
spaces through computational analysis of online content. By combining Named
Entity Recognition and Topic Modeling techniques, specifically the Top2Vec
algorithm, we analyze data from Arabic blogs and Facebook to identify and
classify conspiratorial narratives. Our analysis uncovers six distinct
categories: gender/feminist, geopolitical, government cover-ups, apocalyptic,
Judeo-Masonic, and geoengineering. The research highlights how these narratives
are deeply embedded in Arabic social media discourse, shaped by regional
historical, cultural, and sociopolitical contexts. By applying advanced Natural
Language Processing methods to Arabic content, this study addresses a gap in
conspiracy theory research, which has traditionally focused on English-language
content or offline data. The findings provide new insights into the
manifestation and evolution of conspiracy theories in Arabic digital spaces,
enhancing our understanding of their role in shaping public discourse in the
Arab world.

</details>


### [3] [MEQA: A Meta-Evaluation Framework for Question & Answer LLM Benchmarks](https://arxiv.org/abs/2504.14039)
*Jaime Raldua Veuthey,Zainab Ali Majid,Suhas Hariharan,Jacob Haimes*

Main category: cs.CL

TL;DR: MEQA框架用于评估问答基准的质量，填补了现有基准评估的空白，并通过网络安全领域的案例展示了其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLMs）的发展，其社会影响日益显著，因此严格的评估成为技术和社会的双重需求。现有评估基准缺乏对其质量的评估方法。

Method: 提出MEQA框架，通过标准化评估和量化评分，支持问答基准的元评估，并在网络安全领域使用人类和LLM评估者进行验证。

Result: MEQA成功识别了网络安全基准的优缺点，展示了其在评估基准质量方面的实用性。

Conclusion: MEQA为问答基准的元评估提供了有效工具，尤其适用于LLMs在网络安全等关键领域的应用评估。

Abstract: As Large Language Models (LLMs) advance, their potential for widespread
societal impact grows simultaneously. Hence, rigorous LLM evaluations are both
a technical necessity and social imperative. While numerous evaluation
benchmarks have been developed, there remains a critical gap in
meta-evaluation: effectively assessing benchmarks' quality. We propose MEQA, a
framework for the meta-evaluation of question and answer (QA) benchmarks, to
provide standardized assessments, quantifiable scores, and enable meaningful
intra-benchmark comparisons. We demonstrate this approach on cybersecurity
benchmarks, using human and LLM evaluators, highlighting the benchmarks'
strengths and weaknesses. We motivate our choice of test domain by AI models'
dual nature as powerful defensive tools and security threats.

</details>


### [4] [A Baseline for Self-state Identification and Classification in Mental Health Data: CLPsych 2025 Task](https://arxiv.org/abs/2504.14066)
*Laerdon Kim*

Main category: cs.CL

TL;DR: 本文提出了一种基于Reddit心理健康数据的自我状态分类基线方法，采用4位量化Gemma 2 9B模型和句子分块预处理步骤，性能优于其他方法。


<details>
  <summary>Details</summary>
Motivation: 解决CLPsych 2025 A.1任务中的自我状态分类问题，探索句子分块对模型性能的影响。

Method: 使用4位量化Gemma 2 9B模型进行少样本学习，预处理步骤包括识别相关句子并进行二元分类。

Result: 系统在14个提交系统中排名第三，测试召回率为0.579。

Conclusion: 句子分块步骤显著提升模型性能，因其匹配人工标注粒度并简化任务为二元分类。

Abstract: We present a baseline for the CLPsych 2025 A.1 task: classifying self-states
in mental health data taken from Reddit. We use few-shot learning with a 4-bit
quantized Gemma 2 9B model and a data preprocessing step which first identifies
relevant sentences indicating self-state evidence, and then performs a binary
classification to determine whether the sentence is evidence of an adaptive or
maladaptive self-state. This system outperforms our other method which relies
on an LLM to highlight spans of variable length independently. We attribute the
performance of our model to the benefits of this sentence chunking step for two
reasons: partitioning posts into sentences 1) broadly matches the granularity
at which self-states were human-annotated and 2) simplifies the task for our
language model to a binary classification problem. Our system places third out
of fourteen systems submitted for Task A.1, achieving a test-time recall of
0.579.

</details>


### [5] [LogicTree: Structured Proof Exploration for Coherent and Rigorous Logical Reasoning with Large Language Models](https://arxiv.org/abs/2504.14089)
*Kang He,Kaushik Roy*

Main category: cs.CL

TL;DR: LogicTree是一个推理时模块化框架，通过算法引导搜索解决LLMs在复杂逻辑推理中的挑战，显著提升证明准确率。


<details>
  <summary>Details</summary>
Motivation: LLMs在复杂逻辑推理中存在系统性探索和逻辑一致性维护的挑战，以及前提空间大的组合复杂性。

Method: 提出LogicTree框架，结合缓存机制和线性化前提搜索，引入LLM-free启发式方法优化前提选择。

Result: 在五个数据集上，LogicTree平均比CoT和ToT分别提升23.6%和12.5%的证明准确率，GPT-4o表现优于o3-mini。

Conclusion: LogicTree通过结构化证明探索和优化前提选择，显著提升了LLMs的复杂逻辑推理能力。

Abstract: Large language models (LLMs) have achieved remarkable multi-step reasoning
capabilities across various domains. However, LLMs still face distinct
challenges in complex logical reasoning, as (1) proof-finding requires
systematic exploration and the maintenance of logical coherence and (2)
searching the right combination of premises at each reasoning step is
inherently challenging in tasks with large premise space. To address this, we
propose LogicTree, an inference-time modular framework employing
algorithm-guided search to automate structured proof exploration and ensure
logical coherence. Advancing beyond tree-of-thought (ToT), we incorporate
caching mechanism into LogicTree to enable effective utilization of historical
knowledge, preventing reasoning stagnation and minimizing redundancy.
Furthermore, we address the combinatorial complexity of premise search by
decomposing it into a linear process. The refined premise selection restricts
subsequent inference to at most one derivation per step, enhancing reasoning
granularity and enforcing strict step-by-step reasoning. Additionally, we
introduce two LLM-free heuristics for premise prioritization, enabling
strategic proof search. Experimental results on five datasets demonstrate that
LogicTree optimally scales inference-time computation to achieve higher proof
accuracy, surpassing chain-of-thought (CoT) and ToT with average gains of 23.6%
and 12.5%, respectively, on GPT-4o. Moreover, within LogicTree, GPT-4o
outperforms o3-mini by 7.6% on average.

</details>


### [6] [PEFT A2Z: Parameter-Efficient Fine-Tuning Survey for Large Language and Vision Models](https://arxiv.org/abs/2504.14117)
*Nusrat Jahan Prottasha,Upama Roy Chowdhury,Shetu Mohanto,Tasfia Nuzhat,Abdullah As Sami,Md Shamol Ali,Md Shohanur Islam Sobuj,Hafijur Raman,Md Kowsher,Ozlem Ozmen Garibay*

Main category: cs.CL

TL;DR: 该论文综述了参数高效微调（PEFT）技术，探讨其动机、设计原则及效果，旨在解决大模型全参数微调的高成本问题。


<details>
  <summary>Details</summary>
Motivation: 传统微调大模型（如LLMs和VLMs）成本高昂，存在过拟合、灾难性遗忘和参数效率低下等问题，PEFT通过仅更新少量参数提供解决方案。

Method: 提出PEFT方法的分类法（加性、选择性、重参数化、混合和统一框架），并系统比较其机制与权衡。

Result: PEFT在语言、视觉和生成建模等领域展现出高效性能，降低资源成本。

Conclusion: PEFT为大规模模型的实用、高效和可持续使用提供统一理解，未来研究方向包括可扩展性、可解释性和鲁棒性等。

Abstract: Large models such as Large Language Models (LLMs) and Vision Language Models
(VLMs) have transformed artificial intelligence, powering applications in
natural language processing, computer vision, and multimodal learning. However,
fully fine-tuning these models remains expensive, requiring extensive
computational resources, memory, and task-specific data. Parameter-Efficient
Fine-Tuning (PEFT) has emerged as a promising solution that allows adapting
large models to downstream tasks by updating only a small portion of
parameters. This survey presents a comprehensive overview of PEFT techniques,
focusing on their motivations, design principles, and effectiveness. We begin
by analyzing the resource and accessibility challenges posed by traditional
fine-tuning and highlight key issues, such as overfitting, catastrophic
forgetting, and parameter inefficiency. We then introduce a structured taxonomy
of PEFT methods -- grouped into additive, selective, reparameterized, hybrid,
and unified frameworks -- and systematically compare their mechanisms and
trade-offs. Beyond taxonomy, we explore the impact of PEFT across diverse
domains, including language, vision, and generative modeling, showing how these
techniques offer strong performance with lower resource costs. We also discuss
important open challenges in scalability, interpretability, and robustness, and
suggest future directions such as federated learning, domain adaptation, and
theoretical grounding. Our goal is to provide a unified understanding of PEFT
and its growing role in enabling practical, efficient, and sustainable use of
large models.

</details>


### [7] [Walk the Talk? Measuring the Faithfulness of Large Language Model Explanations](https://arxiv.org/abs/2504.14150)
*Katie Matton,Robert Osazuwa Ness,John Guttag,Emre Kıcıman*

Main category: cs.CL

TL;DR: 该论文提出了一种衡量大型语言模型（LLM）解释忠实性的新方法，通过定义忠实性并开发基于反事实和贝叶斯层次模型的技术，揭示了LLM解释中可能隐藏的社会偏见和误导性影响。


<details>
  <summary>Details</summary>
Motivation: LLM生成的解释可能不忠实，导致过度信任和误用，因此需要一种方法来量化其解释的忠实性。

Method: 通过辅助LLM创建反事实输入，并结合贝叶斯层次模型量化概念对模型决策的因果影响。

Result: 实验表明，该方法能有效发现LLM解释中的不忠实模式，例如隐藏的社会偏见或误导性证据影响。

Conclusion: 提出的方法为评估LLM解释的忠实性提供了可靠工具，有助于揭示和减少模型解释中的潜在问题。

Abstract: Large language models (LLMs) are capable of generating plausible explanations
of how they arrived at an answer to a question. However, these explanations can
misrepresent the model's "reasoning" process, i.e., they can be unfaithful.
This, in turn, can lead to over-trust and misuse. We introduce a new approach
for measuring the faithfulness of LLM explanations. First, we provide a
rigorous definition of faithfulness. Since LLM explanations mimic human
explanations, they often reference high-level concepts in the input question
that purportedly influenced the model. We define faithfulness in terms of the
difference between the set of concepts that LLM explanations imply are
influential and the set that truly are. Second, we present a novel method for
estimating faithfulness that is based on: (1) using an auxiliary LLM to modify
the values of concepts within model inputs to create realistic counterfactuals,
and (2) using a Bayesian hierarchical model to quantify the causal effects of
concepts at both the example- and dataset-level. Our experiments show that our
method can be used to quantify and discover interpretable patterns of
unfaithfulness. On a social bias task, we uncover cases where LLM explanations
hide the influence of social bias. On a medical question answering task, we
uncover cases where LLM explanations provide misleading claims about which
pieces of evidence influenced the model's decisions.

</details>


### [8] [SConU: Selective Conformal Uncertainty in Large Language Models](https://arxiv.org/abs/2504.14154)
*Zhiyuan Wang,Qingni Wang,Yue Zhang,Tianlong Chen,Xiaofeng Zhu,Xiaoshuang Shi,Kaidi Xu*

Main category: cs.CL

TL;DR: 提出了一种名为SConU的新方法，通过显著性测试和两种p值来管理不确定性数据的异常值，从而优化预测覆盖率和效率。


<details>
  <summary>Details</summary>
Motivation: 现有框架无法识别违反交换性假设的不确定性数据异常值，导致覆盖率失控和预测集不可操作。

Method: 开发了两种p值，用于检测样本是否偏离校准集的不确定性分布，并管理风险水平。

Result: SConU能够严格管理单领域和跨领域环境中的覆盖率，并提高预测效率。

Conclusion: SConU方法显著提升了不确定性管理的效果，特别是在高风险问答任务中。

Abstract: As large language models are increasingly utilized in real-world
applications, guarantees of task-specific metrics are essential for their
reliable deployment. Previous studies have introduced various criteria of
conformal uncertainty grounded in split conformal prediction, which offer
user-specified correctness coverage. However, existing frameworks often fail to
identify uncertainty data outliers that violate the exchangeability assumption,
leading to unbounded miscoverage rates and unactionable prediction sets. In
this paper, we propose a novel approach termed Selective Conformal Uncertainty
(SConU), which, for the first time, implements significance tests, by
developing two conformal p-values that are instrumental in determining whether
a given sample deviates from the uncertainty distribution of the calibration
set at a specific manageable risk level. Our approach not only facilitates
rigorous management of miscoverage rates across both single-domain and
interdisciplinary contexts, but also enhances the efficiency of predictions.
Furthermore, we comprehensively analyze the components of the conformal
procedures, aiming to approximate conditional coverage, particularly in
high-stakes question-answering tasks.

</details>


### [9] [Self-Correction Makes LLMs Better Parsers](https://arxiv.org/abs/2504.14165)
*Ziyan Zhang,Yang Hou,Chen Gong,Zhenghua Li*

Main category: cs.CL

TL;DR: 论文分析了大型语言模型（LLM）在句法解析任务中的不足，并提出了一种利用现有树库语法规则的自校正方法，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在多种NLP任务中表现出色，但在句法解析等基础任务上仍存在不足，限制了其深度语言理解能力。

Method: 提出自校正方法，通过自动检测潜在错误并动态搜索相关语法规则，引导LLM自行修正错误。

Result: 在三个数据集上的实验表明，该方法显著提升了LLM在英语和中文数据集上的性能。

Conclusion: 自校正方法有效帮助LLM在不额外训练的情况下提升句法解析能力。

Abstract: Large language models (LLMs) have achieved remarkable success across various
natural language processing (NLP) tasks. However, recent studies suggest that
they still face challenges in performing fundamental NLP tasks essential for
deep language understanding, particularly syntactic parsing. In this paper, we
conduct an in-depth analysis of LLM parsing capabilities, delving into the
specific shortcomings of their parsing results. We find that LLMs may stem from
limitations to fully leverage grammar rules in existing treebanks, which
restricts their capability to generate valid syntactic structures. To help LLMs
acquire knowledge without additional training, we propose a self-correction
method that leverages grammar rules from existing treebanks to guide LLMs in
correcting previous errors. Specifically, we automatically detect potential
errors and dynamically search for relevant rules, offering hints and examples
to guide LLMs in making corrections themselves. Experimental results on three
datasets with various LLMs, demonstrate that our method significantly improves
performance in both in-domain and cross-domain settings on the English and
Chinese datasets.

</details>


### [10] [Hypothetical Documents or Knowledge Leakage? Rethinking LLM-based Query Expansion](https://arxiv.org/abs/2504.14175)
*Yejun Yoon,Jaeyoon Jung,Seunghyun Yoon,Kunwoo Park*

Main category: cs.CL

TL;DR: 研究发现，基于大语言模型（LLM）的查询扩展方法在零样本检索任务中的性能提升可能源于基准测试中的知识泄漏，而非模型的实际能力。


<details>
  <summary>Details</summary>
Motivation: 探讨LLM生成的假设文档是否因基准测试中的知识泄漏而提升了检索性能，尤其是在真实场景中检索小众或新知识时。

Method: 以事实验证为测试平台，分析生成的文档是否包含与真实证据相关的信息，并评估其对性能的影响。

Result: 性能提升仅发生在生成的文档包含与真实证据相关的句子时，表明基准测试中可能存在知识泄漏。

Conclusion: LLM查询扩展方法的性能可能被基准测试中的知识泄漏夸大，需谨慎评估其在真实场景中的适用性。

Abstract: Query expansion methods powered by large language models (LLMs) have
demonstrated effectiveness in zero-shot retrieval tasks. These methods assume
that LLMs can generate hypothetical documents that, when incorporated into a
query vector, enhance the retrieval of real evidence. However, we challenge
this assumption by investigating whether knowledge leakage in benchmarks
contributes to the observed performance gains. Using fact verification as a
testbed, we analyzed whether the generated documents contained information
entailed by ground truth evidence and assessed their impact on performance. Our
findings indicate that performance improvements occurred consistently only for
claims whose generated documents included sentences entailed by ground truth
evidence. This suggests that knowledge leakage may be present in these
benchmarks, inflating the perceived performance of LLM-based query expansion
methods, particularly in real-world scenarios that require retrieving niche or
novel knowledge.

</details>


### [11] [Meta-rater: A Multi-dimensional Data Selection Method for Pre-training Language Models](https://arxiv.org/abs/2504.14194)
*Xinlin Zhuang,Jiahui Peng,Ren Ma,Yinfan Wang,Tianyi Bai,Xingjian Wei,Jiantao Qiu,Chi Zhang,Ying Qian,Conghui He*

Main category: cs.CL

TL;DR: 论文提出PRRC和Meta-rater方法，通过多维度数据质量评估和优化权重选择，显著提升LLM预训练效率和性能。


<details>
  <summary>Details</summary>
Motivation: 当前LLM预训练数据集组成不透明，数据选择方法单一或冗余，限制了模型性能优化。

Method: 提出PRRC评估数据质量（专业性、可读性、推理性和清洁度），并开发Meta-rater方法，通过代理模型学习最优权重组合。

Result: Meta-rater使1.3B参数模型收敛速度翻倍，下游任务性能提升3.23，并在3.3B模型上验证了可扩展性。

Conclusion: 多维度数据质量整合显著优于传统单维度方法，为LLM预训练提供了高效、可扩展的范式。

Abstract: The composition of pre-training datasets for large language models (LLMs)
remains largely undisclosed, hindering transparency and efforts to optimize
data quality, a critical driver of model performance. Current data selection
methods, such as natural language quality assessments, diversity-based filters,
and classifier-based approaches, are limited by single-dimensional evaluation
or redundancy-focused strategies. To address these gaps, we propose PRRC to
evaluate data quality across Professionalism, Readability, Reasoning, and
Cleanliness. We further introduce Meta-rater, a multi-dimensional data
selection method that integrates these dimensions with existing quality metrics
through learned optimal weightings. Meta-rater employs proxy models to train a
regression model that predicts validation loss, enabling the identification of
optimal combinations of quality scores. Experiments demonstrate that Meta-rater
doubles convergence speed for 1.3B parameter models and improves downstream
task performance by 3.23, with scalable benefits observed in 3.3B models
trained on 100B tokens. Additionally, we release the annotated SlimPajama-627B
dataset, labeled across 25 quality metrics (including PRRC), to advance
research in data-centric LLM development. Our work establishes that holistic,
multi-dimensional quality integration significantly outperforms conventional
single-dimension approaches, offering a scalable paradigm for enhancing
pre-training efficiency and model capability.

</details>


### [12] [EIoU-EMC: A Novel Loss for Domain-specific Nested Entity Recognition](https://arxiv.org/abs/2504.14203)
*Jian Zhang,Tianqing Zhang,Qi Li,Hongwei Wang*

Main category: cs.CL

TL;DR: 该论文提出了一种新的损失函数EIoU-EMC，用于解决特定领域（如生物医学和工业）中嵌套NER任务在低资源和类别不平衡情况下的挑战。


<details>
  <summary>Details</summary>
Motivation: 特定领域（如生物医学和工业）中的嵌套NER任务面临低资源和类别不平衡的挑战，限制了其广泛应用。

Method: 设计了新的损失函数EIoU-EMC，结合了Intersection over Union损失和多类损失，利用实体边界和实体分类信息提升模型在少量数据上的学习能力。

Result: 在三个生物医学NER数据集和一个工业设备维护文档数据集上验证，相比基线方法表现出竞争力，尤其在实体边界识别和分类上有显著提升。

Conclusion: EIoU-EMC方法有效提升了嵌套NER任务在低资源和类别不平衡场景下的性能。

Abstract: In recent years, research has mainly focused on the general NER task. There
still have some challenges with nested NER task in the specific domains.
Specifically, the scenarios of low resource and class imbalance impede the wide
application for biomedical and industrial domains. In this study, we design a
novel loss EIoU-EMC, by enhancing the implement of Intersection over Union loss
and Multiclass loss. Our proposed method specially leverages the information of
entity boundary and entity classification, thereby enhancing the model's
capacity to learn from a limited number of data samples. To validate the
performance of this innovative method in enhancing NER task, we conducted
experiments on three distinct biomedical NER datasets and one dataset
constructed by ourselves from industrial complex equipment maintenance
documents. Comparing to strong baselines, our method demonstrates the
competitive performance across all datasets. During the experimental analysis,
our proposed method exhibits significant advancements in entity boundary
recognition and entity classification. Our code are available here.

</details>


### [13] [Bias Analysis and Mitigation through Protected Attribute Detection and Regard Classification](https://arxiv.org/abs/2504.14212)
*Takuma Udagawa,Yang Zhao,Hiroshi Kanayama,Bishwaranjan Bhattacharjee*

Main category: cs.CL

TL;DR: 提出了一种高效的注释流程，用于分析预训练语料库中的社会偏见，并通过实验验证了偏见分析和缓解措施的效果。


<details>
  <summary>Details</summary>
Motivation: 预训练数据中的社会偏见可能被大型语言模型（LLMs）延续甚至放大，因此需要研究这些偏见。

Method: 提出了一种注释流程，包括受保护属性检测和语言极性分类。

Result: 实验验证了该流程在分析Common Crawl语料库中偏见的效果。

Conclusion: 该流程为预训练语料库中的社会偏见分析提供了有效工具。

Abstract: Large language models (LLMs) acquire general linguistic knowledge from
massive-scale pretraining. However, pretraining data mainly comprised of
web-crawled texts contain undesirable social biases which can be perpetuated or
even amplified by LLMs. In this study, we propose an efficient yet effective
annotation pipeline to investigate social biases in the pretraining corpora.
Our pipeline consists of protected attribute detection to identify diverse
demographics, followed by regard classification to analyze the language
polarity towards each attribute. Through our experiments, we demonstrate the
effect of our bias analysis and mitigation measures, focusing on Common Crawl
as the most representative pretraining corpus.

</details>


### [14] [Understanding the Repeat Curse in Large Language Models from a Feature Perspective](https://arxiv.org/abs/2504.14218)
*Junchi Yao,Shu Yang,Jianhua Xu,Lijie Hu,Mengdi Li,Di Wang*

Main category: cs.CL

TL;DR: 论文研究了大型语言模型（LLMs）中重复文本生成的根源，提出了一种名为“Duplicatus Charm”的新方法，通过机制可解释性识别并抑制导致重复的模型激活特征。


<details>
  <summary>Details</summary>
Motivation: LLMs在多个领域取得了显著进展，但常出现重复文本生成的问题（“Repeat Curse”），其根本机制尚未充分探索。

Method: 通过稀疏自编码器（SAEs）提取单义特征，提出“Duplicatus Charm”方法，识别并操纵导致重复的模型激活特征。

Result: 成功识别并抑制了导致重复的“Repetition Features”，有效缓解了重复问题。

Conclusion: 通过机制可解释性方法，揭示了LLMs中重复问题的根源，并提出了一种有效的解决方案。

Abstract: Large language models (LLMs) have made remarkable progress in various
domains, yet they often suffer from repetitive text generation, a phenomenon we
refer to as the "Repeat Curse". While previous studies have proposed decoding
strategies to mitigate repetition, the underlying mechanism behind this issue
remains insufficiently explored. In this work, we investigate the root causes
of repetition in LLMs through the lens of mechanistic interpretability.
Inspired by recent advances in Sparse Autoencoders (SAEs), which enable
monosemantic feature extraction, we propose a novel approach, "Duplicatus
Charm", to induce and analyze the Repeat Curse. Our method systematically
identifies "Repetition Features" -the key model activations responsible for
generating repetitive outputs. First, we locate the layers most involved in
repetition through logit analysis. Next, we extract and stimulate relevant
features using SAE-based activation manipulation. To validate our approach, we
construct a repetition dataset covering token and paragraph level repetitions
and introduce an evaluation pipeline to quantify the influence of identified
repetition features. Furthermore, by deactivating these features, we have
effectively mitigated the Repeat Curse.

</details>


### [15] [SimplifyMyText: An LLM-Based System for Inclusive Plain Language Text Simplification](https://arxiv.org/abs/2504.14223)
*Michael Färber,Parisa Aghdam,Kyuri Im,Mario Tawfelis,Hardik Ghoshal*

Main category: cs.CL

TL;DR: 本文介绍了首个基于大语言模型（如GPT-4和Llama-3）的文本简化系统SimplifyMyText，旨在为不同受众提供定制化的简化内容，并评估其效果。


<details>
  <summary>Details</summary>
Motivation: 当前文本简化材料的稀缺阻碍了个人和社会的发展，现有方法未能充分利用大语言模型实现定制化简化。

Method: 利用GPT-4和Llama-3开发了SimplifyMyText系统，支持多种输入格式，并提供灵活的定制选项。

Result: 系统通过多指标评估，验证了其在生成定制化简化文本方面的有效性。

Conclusion: 该研究推动了自动文本简化领域的发展，强调了定制化沟通在促进包容性中的重要性。

Abstract: Text simplification is essential for making complex content accessible to
diverse audiences who face comprehension challenges. Yet, the limited
availability of simplified materials creates significant barriers to personal
and professional growth and hinders social inclusion. Although researchers have
explored various methods for automatic text simplification, none fully leverage
large language models (LLMs) to offer tailored customization for different
target groups and varying levels of simplicity. Moreover, despite its proven
benefits for both consumers and organizations, the well-established practice of
plain language remains underutilized. In this paper, we
https://simplifymytext.org, the first system designed to produce plain language
content from multiple input formats, including typed text and file uploads,
with flexible customization options for diverse audiences. We employ GPT-4 and
Llama-3 and evaluate outputs across multiple metrics. Overall, our work
contributes to research on automatic text simplification and highlights the
importance of tailored communication in promoting inclusivity.

</details>


### [16] [Know Me, Respond to Me: Benchmarking LLMs for Dynamic User Profiling and Personalized Responses at Scale](https://arxiv.org/abs/2504.14225)
*Bowen Jiang,Zhuoqun Hao,Young-Min Cho,Bryan Li,Yuan Yuan,Sihao Chen,Lyle Ungar,Camillo J. Taylor,Dan Roth*

Main category: cs.CL

TL;DR: 论文介绍了PERSONAMEM基准，用于评估LLMs如何利用用户交互历史生成个性化响应，发现当前模型在动态跟踪用户偏好方面仍有不足。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs如何利用用户交互历史内部化用户特质和偏好，并动态跟踪其演变，以生成个性化响应。

Method: 提出PERSONAMEM基准，包含180个模拟用户-LLM交互历史，评估LLMs在动态用户情境下的响应能力。

Result: 当前LLMs（如GPT-4.1等）在动态跟踪用户偏好方面表现不佳，准确率仅约50%。

Conclusion: PERSONAMEM基准为未来开发用户感知型聊天机器人提供了研究基础。

Abstract: Large Language Models (LLMs) have emerged as personalized assistants for
users across a wide range of tasks -- from offering writing support to
delivering tailored recommendations or consultations. Over time, the
interaction history between a user and an LLM can provide extensive information
about an individual's traits and preferences. However, open questions remain on
how well LLMs today can effectively leverage such history to (1) internalize
the user's inherent traits and preferences, (2) track how the user profiling
and preferences evolve over time, and (3) generate personalized responses
accordingly in new scenarios.
  In this work, we introduce the PERSONAMEM benchmark. PERSONAMEM features
curated user profiles with over 180 simulated user-LLM interaction histories,
each containing up to 60 sessions of multi-turn conversations across 15
real-world tasks that require personalization. Given an in-situ user query,
i.e. query issued by the user from the first-person perspective, we evaluate
LLM chatbots' ability to identify the most suitable response according to the
current state of the user's profile. We observe that current LLMs still
struggle to recognize the dynamic evolution in users' profiles over time
through direct prompting approaches. As a consequence, LLMs often fail to
deliver responses that align with users' current situations and preferences,
with frontier models such as GPT-4.1, o4-mini, GPT-4.5, o1, or Gemini-2.0
achieving only around 50% overall accuracy, suggesting room for improvement. We
hope that PERSONAMEM, along with the user profile and conversation simulation
pipeline, can facilitate future research in the development of truly user-aware
chatbots. Code and data are available at github.com/bowen-upenn/PersonaMem.

</details>


### [17] [Probing the Subtle Ideological Manipulation of Large Language Models](https://arxiv.org/abs/2504.14287)
*Demetris Paschalides,George Pallis,Marios D. Dikaiakos*

Main category: cs.CL

TL;DR: 研究探讨了大型语言模型（LLMs）在多维政治意识形态谱系中的可操纵性，并提出了新的多任务数据集。通过微调模型，发现其对意识形态的敏感性较高，需加强防护措施。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注LLMs在二元政治立场（左-右）上的偏见，但忽略了更复杂的意识形态谱系。本研究旨在填补这一空白，探索LLMs在多维政治立场中的可操纵性。

Method: 引入多任务数据集，包含意识形态问答、声明排序、宣言填空和国会法案理解等任务。对Phi-2、Mistral和Llama-3三种LLMs进行微调，评估其意识形态表达能力。

Result: 微调显著提升了模型对复杂意识形态的适应性，而显式提示效果有限。表明LLMs易受意识形态操纵。

Conclusion: LLMs在多维政治立场中表现出较高的可操纵性，需开发更强大的防护机制以减少风险。

Abstract: Large Language Models (LLMs) have transformed natural language processing,
but concerns have emerged about their susceptibility to ideological
manipulation, particularly in politically sensitive areas. Prior work has
focused on binary Left-Right LLM biases, using explicit prompts and fine-tuning
on political QA datasets. In this work, we move beyond this binary approach to
explore the extent to which LLMs can be influenced across a spectrum of
political ideologies, from Progressive-Left to Conservative-Right. We introduce
a novel multi-task dataset designed to reflect diverse ideological positions
through tasks such as ideological QA, statement ranking, manifesto cloze
completion, and Congress bill comprehension. By fine-tuning three LLMs-Phi-2,
Mistral, and Llama-3-on this dataset, we evaluate their capacity to adopt and
express these nuanced ideologies. Our findings indicate that fine-tuning
significantly enhances nuanced ideological alignment, while explicit prompts
provide only minor refinements. This highlights the models' susceptibility to
subtle ideological manipulation, suggesting a need for more robust safeguards
to mitigate these risks.

</details>


### [18] [Multimodal Coreference Resolution for Chinese Social Media Dialogues: Dataset and Benchmark Approach](https://arxiv.org/abs/2504.14321)
*Xingyu Li,Chen Gong,Guohong Fu*

Main category: cs.CL

TL;DR: 该论文介绍了首个中文多模态共指数据集TikTalkCoref，用于解决社交媒体中的多模态共指问题，并提供了基准方法。


<details>
  <summary>Details</summary>
Motivation: 多模态共指解析（MCR）对理解多模态内容至关重要，但缺乏真实对话场景的数据集。

Method: 构建了TikTalkCoref数据集，包含短视频和用户评论的文本对话，并标注了共指关系。同时提出了一种基准方法。

Result: 在TikTalkCoref数据集上进行了实验，提供了可靠的基准结果。

Conclusion: TikTalkCoref数据集将促进社交媒体多模态共指研究的未来发展。

Abstract: Multimodal coreference resolution (MCR) aims to identify mentions referring
to the same entity across different modalities, such as text and visuals, and
is essential for understanding multimodal content. In the era of rapidly
growing mutimodal content and social media, MCR is particularly crucial for
interpreting user interactions and bridging text-visual references to improve
communication and personalization. However, MCR research for real-world
dialogues remains unexplored due to the lack of sufficient data resources.To
address this gap, we introduce TikTalkCoref, the first Chinese multimodal
coreference dataset for social media in real-world scenarios, derived from the
popular Douyin short-video platform. This dataset pairs short videos with
corresponding textual dialogues from user comments and includes manually
annotated coreference clusters for both person mentions in the text and the
coreferential person head regions in the corresponding video frames. We also
present an effective benchmark approach for MCR, focusing on the celebrity
domain, and conduct extensive experiments on our dataset, providing reliable
benchmark results for this newly constructed dataset. We will release the
TikTalkCoref dataset to facilitate future research on MCR for real-world social
media dialogues.

</details>


### [19] [Empirical Evaluation of Knowledge Distillation from Transformers to Subquadratic Language Models](https://arxiv.org/abs/2504.14366)
*Patrick Haller,Jonas Golde,Alan Akbik*

Main category: cs.CL

TL;DR: 本文研究了从Transformer教师模型到九种次二次复杂度学生模型的知识蒸馏可迁移性，探讨了不同架构约束和初始化策略对蒸馏过程的影响。


<details>
  <summary>Details</summary>
Motivation: 自注意力在推理时的二次复杂度是主要瓶颈，因此探索次二次替代方案（如SSMs、线性注意力和循环架构）具有重要意义。

Method: 系统评估了从Transformer教师到九种次二次学生模型的知识蒸馏，并研究了矩阵混合和QKV复制等初始化策略。

Result: 在多个NLP基准测试中，实证结果揭示了效率与性能之间的权衡，并确定了成功知识迁移的关键因素。

Conclusion: 研究为次二次架构的知识蒸馏提供了实用见解，强调了架构选择和初始化策略的重要性。

Abstract: Knowledge distillation is a widely used technique for compressing large
language models (LLMs) by training a smaller student model to mimic a larger
teacher model. Typically, both the teacher and student are Transformer-based
architectures, leveraging softmax attention for sequence modeling. However, the
quadratic complexity of self-attention at inference time remains a significant
bottleneck, motivating the exploration of subquadratic alternatives such as
structured state-space models (SSMs), linear attention, and recurrent
architectures. In this work, we systematically evaluate the transferability of
knowledge distillation from a Transformer teacher to nine subquadratic student
architectures. Our study aims to determine which subquadratic model best aligns
with the teacher's learned representations and how different architectural
constraints influence the distillation process. We also investigate the impact
of intelligent initialization strategies, including matrix mixing and
query-key-value (QKV) copying, on the adaptation process. Our empirical results
on multiple NLP benchmarks provide insights into the trade-offs between
efficiency and performance, highlighting key factors for successful knowledge
transfer to subquadratic architectures.

</details>


### [20] [Diverse Prompts: Illuminating the Prompt Space of Large Language Models with MAP-Elites](https://arxiv.org/abs/2504.14367)
*Gabriel Machado Santos,Rita Maria da Silva Julia,Marcelo Zanchetta do Nascimento*

Main category: cs.CL

TL;DR: 该论文提出了一种结合上下文无关语法（CFG）和MAP-Elites算法的进化方法，系统探索提示空间，以优化大型语言模型（LLMs）的性能。


<details>
  <summary>Details</summary>
Motivation: 提示工程对优化LLMs至关重要，但提示结构与任务性能之间的关系尚未充分研究。

Method: 采用CFG与MAP-Elites算法结合的方法，生成高质量且多样化的提示，并分析其与不同任务的匹配性。

Result: 在多个LLMs和七个BigBench Lite任务上的实验表明，该方法能有效揭示提示结构对性能的影响。

Conclusion: 通过系统映射表型空间，该方法为任务特定和适应性提示设计提供了实用见解，提升了LLMs的效能和多功能性。

Abstract: Prompt engineering is essential for optimizing large language models (LLMs),
yet the link between prompt structures and task performance remains
underexplored. This work introduces an evolutionary approach that combines
context-free grammar (CFG) with the MAP-Elites algorithm to systematically
explore the prompt space. Our method prioritizes quality and diversity,
generating high-performing and structurally varied prompts while analyzing
their alignment with diverse tasks by varying traits such as the number of
examples (shots) and reasoning depth. By systematically mapping the phenotypic
space, we reveal how structural variations influence LLM performance, offering
actionable insights for task-specific and adaptable prompt design. Evaluated on
seven BigBench Lite tasks across multiple LLMs, our results underscore the
critical interplay of quality and diversity, advancing the effectiveness and
versatility of LLMs.

</details>


### [21] [ParaPO: Aligning Language Models to Reduce Verbatim Reproduction of Pre-training Data](https://arxiv.org/abs/2504.14452)
*Tong Chen,Faeze Brahman,Jiacheng Liu,Niloofar Mireshghallah,Weijia Shi,Pang Wei Koh,Luke Zettlemoyer,Hannaneh Hajishirzi*

Main category: cs.CL

TL;DR: ParaPO是一种后训练方法，通过微调语言模型以减少无意中的记忆内容复现，同时保持模型实用性。


<details>
  <summary>Details</summary>
Motivation: 语言模型可能无意中复现预训练数据中的片段，引发版权、隐私等问题。

Method: ParaPO通过训练模型偏好记忆内容的改写版本，并结合系统提示控制复现行为。

Result: 在Llama3.1-8B和Tulu3-8B上，ParaPO显著减少了复现现象，同时保留了引用名句的能力。

Conclusion: ParaPO在减少记忆内容复现方面优于传统方法，且能通过系统提示灵活控制行为。

Abstract: Language models (LMs) can memorize and reproduce segments from their
pretraining data verbatim even in non-adversarial settings, raising concerns
about copyright, plagiarism, privacy, and creativity. We introduce Paraphrase
Preference Optimization (ParaPO), a post-training method that fine-tunes LMs to
reduce unintentional regurgitation while preserving their overall utility.
ParaPO trains LMs to prefer paraphrased versions of memorized segments over the
original verbatim content from the pretraining data. To maintain the ability to
recall famous quotations when appropriate, we develop a variant of ParaPO that
uses system prompts to control regurgitation behavior. In our evaluation on
Llama3.1-8B, ParaPO consistently reduces regurgitation across all tested
datasets (e.g., reducing the regurgitation metric from 17.3 to 12.9 in creative
writing), whereas unlearning methods used in prior work to mitigate
regurgitation are less effective outside their targeted unlearned domain (from
17.3 to 16.9). When applied to the instruction-tuned Tulu3-8B model, ParaPO
with system prompting successfully preserves famous quotation recall while
reducing unintentional regurgitation (from 8.7 to 6.3 in creative writing) when
prompted not to regurgitate. In contrast, without ParaPO tuning, prompting the
model not to regurgitate produces only a marginal reduction (8.7 to 8.4).

</details>


### [22] [CoLoTa: A Dataset for Entity-based Commonsense Reasoning over Long-Tail Knowledge](https://arxiv.org/abs/2504.14462)
*Armin Toroghi,Willis Guo,Scott Sanner*

Main category: cs.CL

TL;DR: 论文提出了一个名为CoLoTa的新数据集，用于评估大型语言模型（LLMs）在处理长尾实体时的常识推理能力及其幻觉问题，同时也可作为知识图谱问答（KGQA）的基准测试。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在编码事实和常识知识方面表现出色，但在处理长尾实体的常识推理任务时仍存在高错误率和幻觉问题，限制了其在高风险场景中的应用。

Method: 作者构建了包含3,300个查询的CoLoTa数据集，涵盖问答和声明验证任务，并涉及多种常识推理技能。数据集支持知识图谱问答，但不同于现有基准，它强调常识推理。

Result: 实验表明，现有的基于LLM的KGQA方法在处理涉及常识推理的查询时表现严重不足。

Conclusion: CoLoTa可作为评估LLMs和KGQA方法在常识推理能力及抗幻觉性能上的新基准。

Abstract: The rise of Large Language Models (LLMs) has redefined the AI landscape,
particularly due to their ability to encode factual and commonsense knowledge,
and their outstanding performance in tasks requiring reasoning. Despite these
advances, hallucinations and reasoning errors remain a significant barrier to
their deployment in high-stakes settings. In this work, we observe that even
the most prominent LLMs, such as OpenAI-o1, suffer from high rates of reasoning
errors and hallucinations on tasks requiring commonsense reasoning over
obscure, long-tail entities. To investigate this limitation, we present a new
dataset for Commonsense reasoning over Long-Tail entities (CoLoTa), that
consists of 3,300 queries from question answering and claim verification tasks
and covers a diverse range of commonsense reasoning skills. We remark that
CoLoTa can also serve as a Knowledge Graph Question Answering (KGQA) dataset
since the support of knowledge required to answer its queries is present in the
Wikidata knowledge graph. However, as opposed to existing KGQA benchmarks that
merely focus on factoid questions, our CoLoTa queries also require commonsense
reasoning. Our experiments with strong LLM-based KGQA methodologies indicate
their severe inability to answer queries involving commonsense reasoning.
Hence, we propose CoLoTa as a novel benchmark for assessing both (i) LLM
commonsense reasoning capabilities and their robustness to hallucinations on
long-tail entities and (ii) the commonsense reasoning capabilities of KGQA
methods.

</details>


### [23] [sEEG-based Encoding for Sentence Retrieval: A Contrastive Learning Approach to Brain-Language Alignment](https://arxiv.org/abs/2504.14468)
*Yijun Liu*

Main category: cs.CL

TL;DR: SSENSE框架通过对比学习将sEEG信号映射到CLIP模型的句子嵌入空间，实现从脑活动直接检索句子。


<details>
  <summary>Details</summary>
Motivation: 探索多模态基础模型在神经科学与AI交叉领域的潜力，将侵入性脑记录与自然语言对齐。

Method: 使用InfoNCE损失在sEEG频谱表示上训练神经编码器，不微调文本编码器。

Result: 在有限数据下，SSENSE在电影观看数据集上表现出色，证明通用语言表征可作为神经解码的有效先验。

Conclusion: 通用语言表征能有效支持神经解码，SSENSE为脑活动与语言对齐提供了新方法。

Abstract: Interpreting neural activity through meaningful latent representations
remains a complex and evolving challenge at the intersection of neuroscience
and artificial intelligence. We investigate the potential of multimodal
foundation models to align invasive brain recordings with natural language. We
present SSENSE, a contrastive learning framework that projects single-subject
stereo-electroencephalography (sEEG) signals into the sentence embedding space
of a frozen CLIP model, enabling sentence-level retrieval directly from brain
activity. SSENSE trains a neural encoder on spectral representations of sEEG
using InfoNCE loss, without fine-tuning the text encoder. We evaluate our
method on time-aligned sEEG and spoken transcripts from a naturalistic
movie-watching dataset. Despite limited data, SSENSE achieves promising
results, demonstrating that general-purpose language representations can serve
as effective priors for neural decoding.

</details>


### [24] [DialogueAgents: A Hybrid Agent-Based Speech Synthesis Framework for Multi-Party Dialogue](https://arxiv.org/abs/2504.14482)
*Xiang Li,Duyi Pan,Hongru Xiao,Jiale Han,Jing Tang,Jiabao Ma,Wei Wang,Bo Cheng*

Main category: cs.CL

TL;DR: 论文提出了一种名为DialogueAgents的新型混合代理语音合成框架，通过三个专业代理（脚本编写者、语音合成器和对话评论家）协作生成对话，解决了现有数据集成本高、多样性不足的问题，并贡献了一个高质量的双语多轮对话数据集MultiTalk。


<details>
  <summary>Details</summary>
Motivation: 现有语音合成数据集构建成本高，且缺乏多样性和情感表达，限制了人机交互的自然性。

Method: 提出DialogueAgents框架，整合三个代理（脚本编写者、语音合成器、对话评论家），基于多样化角色池迭代优化对话脚本和语音合成。

Result: 实验证明框架有效，并贡献了高质量的双语多轮对话数据集MultiTalk。

Conclusion: DialogueAgents框架提升了语音合成的多样性和情感表达，MultiTalk数据集为未来研究提供了资源。

Abstract: Speech synthesis is crucial for human-computer interaction, enabling natural
and intuitive communication. However, existing datasets involve high
construction costs due to manual annotation and suffer from limited character
diversity, contextual scenarios, and emotional expressiveness. To address these
issues, we propose DialogueAgents, a novel hybrid agent-based speech synthesis
framework, which integrates three specialized agents -- a script writer, a
speech synthesizer, and a dialogue critic -- to collaboratively generate
dialogues. Grounded in a diverse character pool, the framework iteratively
refines dialogue scripts and synthesizes speech based on speech review,
boosting emotional expressiveness and paralinguistic features of the
synthesized dialogues. Using DialogueAgent, we contribute MultiTalk, a
bilingual, multi-party, multi-turn speech dialogue dataset covering diverse
topics. Extensive experiments demonstrate the effectiveness of our framework
and the high quality of the MultiTalk dataset. We release the dataset and code
https://github.com/uirlx/DialogueAgents to facilitate future research on
advanced speech synthesis models and customized data generation.

</details>


### [25] [FairSteer: Inference Time Debiasing for LLMs with Dynamic Activation Steering](https://arxiv.org/abs/2504.14492)
*Yichen Li,Zhiting Fan,Ruizhe Chen,Xiaotang Gai,Luqi Gong,Yan Zhang,Zuozhu Liu*

Main category: cs.CL

TL;DR: FairSteer是一种无需定制提示或模型重新训练的推理时去偏框架，通过检测偏激活、计算去偏转向向量（DSV）和动态调整激活来实现去偏。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）容易从训练语料中捕获偏见，现有方法存在不稳定或计算成本高的问题。

Method: FairSteer基于线性表示假设，通过轻量级线性分类器检测偏激活，计算DSV作为干预方向，并在推理阶段动态调整激活。

Result: 在六种LLM上的评估显示，FairSteer在问答、反事实输入评估和开放文本生成任务中表现优越。

Conclusion: FairSteer提供了一种高效且稳定的去偏方法，无需额外训练或复杂提示设计。

Abstract: Large language models (LLMs) are prone to capturing biases from training
corpus, leading to potential negative social impacts. Existing prompt-based
debiasing methods exhibit instability due to their sensitivity to prompt
changes, while fine-tuning-based techniques incur substantial computational
overhead and catastrophic forgetting. In this paper, we propose FairSteer, a
novel inference-time debiasing framework without requiring customized prompt
design or model retraining. Motivated by the linear representation hypothesis,
our preliminary investigation demonstrates that fairness-related features can
be encoded into separable directions in the hidden activation space. FairSteer
operates in three steps: biased activation detection, debiasing steering vector
(DSV) computation, and dynamic activation steering. Specifically, it first
trains a lightweight linear classifier to detect bias signatures in
activations, and then computes DSVs as intervention directions derived from
small contrastive prompt pairs. Subsequently, it performs debiasing by
adjusting activations with DSVs in the inference stage. Comprehensive
evaluation with six LLMs demonstrates the superiority of FairSteer across
question-answering, counterfactual input evaluation and open-ended text
generation tasks. Code will be released.

</details>


### [26] [Functional Abstraction of Knowledge Recall in Large Language Models](https://arxiv.org/abs/2504.14496)
*Zijian Wang,Chang Xu*

Main category: cs.CL

TL;DR: 论文研究了大型语言模型（LLM）的知识召回机制，提出其隐藏激活空间隐含函数执行过程，并通过实验验证和改进知识编辑方法。


<details>
  <summary>Details</summary>
Motivation: 探索LLM知识召回的机制，将其抽象为功能结构，以提升知识编辑的效果。

Method: 设计基于修补的知识评分算法识别功能组件，并通过反知识测试验证其独立性。改进激活修补增强的上下文知识编辑方法。

Result: 实验验证了激活向量与功能组件的对应关系，改进后的知识编辑方法提升了短期记忆保留能力。

Conclusion: LLM的知识召回可通过功能结构理解，激活修补方法能有效优化知识编辑效果。

Abstract: Pre-trained transformer large language models (LLMs) demonstrate strong
knowledge recall capabilities. This paper investigates the knowledge recall
mechanism in LLMs by abstracting it into a functional structure. We propose
that during knowledge recall, the model's hidden activation space implicitly
entails a function execution process where specific activation vectors align
with functional components (Input argument, Function body, and Return values).
Specifically, activation vectors of relation-related tokens define a mapping
function from subjects to objects, with subject-related token activations
serving as input arguments and object-related token activations as return
values. For experimental verification, we first design a patching-based
knowledge-scoring algorithm to identify knowledge-aware activation vectors as
independent functional components. Then, we conduct counter-knowledge testing
to examine the independent functional effects of each component on knowledge
recall outcomes. From this functional perspective, we improve the contextual
knowledge editing approach augmented by activation patching. By rewriting
incoherent activations in context, we enable improved short-term memory
retention for new knowledge prompting.

</details>


### [27] [Causality for Natural Language Processing](https://arxiv.org/abs/2504.14530)
*Zhijing Jin*

Main category: cs.CL

TL;DR: 该论文探讨了大语言模型（LLMs）在因果推理和理解方面的能力，包括因果推断技能、机制及其在NLP任务中的应用，并研究了因果推理在计算社会科学中的具体应用。


<details>
  <summary>Details</summary>
Motivation: 因果推理是人类智能的核心，也是人工智能系统实现高级理解和决策的关键能力。研究旨在提升LLMs的因果推理能力。

Method: 通过一系列研究、新颖的数据集、基准任务和方法论框架，分析LLMs的因果推理技能及其机制。

Result: 研究揭示了LLMs在因果推理方面的关键挑战和机遇，为未来研究提供了基础。

Conclusion: 该工作为提升LLMs的因果推理能力提供了全面的研究框架，推动了这一领域的发展。

Abstract: Causal reasoning is a cornerstone of human intelligence and a critical
capability for artificial systems aiming to achieve advanced understanding and
decision-making. This thesis delves into various dimensions of causal reasoning
and understanding in large language models (LLMs). It encompasses a series of
studies that explore the causal inference skills of LLMs, the mechanisms behind
their performance, and the implications of causal and anticausal learning for
natural language processing (NLP) tasks. Additionally, it investigates the
application of causal reasoning in text-based computational social science,
specifically focusing on political decision-making and the evaluation of
scientific impact through citations. Through novel datasets, benchmark tasks,
and methodological frameworks, this work identifies key challenges and
opportunities to improve the causal capabilities of LLMs, providing a
comprehensive foundation for future research in this evolving field.

</details>


### [28] [BookWorld: From Novels to Interactive Agent Societies for Creative Story Generation](https://arxiv.org/abs/2504.14538)
*Yiting Ran,Xintao Wang,Tian Qiu,Jiaqing Liang,Yanghua Xiao,Deqing Yang*

Main category: cs.CL

TL;DR: 论文介绍了BookWorld系统，用于构建和模拟基于书籍的多智能体社会，支持故事生成、互动游戏等应用，实验显示其生成的故事质量高且忠于原著。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注从头创建智能体社会，而模拟已有虚构世界和角色尚未充分探索，BookWorld填补了这一空白。

Method: 设计BookWorld系统，涵盖角色多样性、虚构世界观、地理约束等现实复杂性。

Result: 实验表明，BookWorld生成的故事创意且高质量，忠于原著，胜率75.36%。

Conclusion: BookWorld为探索和扩展虚构作品提供了新途径，具有显著实用价值。

Abstract: Recent advances in large language models (LLMs) have enabled social
simulation through multi-agent systems. Prior efforts focus on agent societies
created from scratch, assigning agents with newly defined personas. However,
simulating established fictional worlds and characters remain largely
underexplored, despite its significant practical value. In this paper, we
introduce BookWorld, a comprehensive system for constructing and simulating
book-based multi-agent societies. BookWorld's design covers comprehensive
real-world intricacies, including diverse and dynamic characters, fictional
worldviews, geographical constraints and changes, e.t.c. BookWorld enables
diverse applications including story generation, interactive games and social
simulation, offering novel ways to extend and explore beloved fictional works.
Through extensive experiments, we demonstrate that BookWorld generates
creative, high-quality stories while maintaining fidelity to the source books,
surpassing previous methods with a win rate of 75.36%. The code of this paper
can be found at the project page: https://bookworld2025.github.io/.

</details>


### [29] [a1: Steep Test-time Scaling Law via Environment Augmented Generation](https://arxiv.org/abs/2504.14597)
*Lingrui Mei,Shenghua Liu,Yiwei Wang,Baolong Bi,Yuyao Ge,Jun Wan,Yurong Wu,Xueqi Cheng*

Main category: cs.CL

TL;DR: EAG框架通过实时环境反馈、动态分支探索和基于经验的学习，提升LLM的推理能力，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在复杂多步任务中的幻觉、逻辑错误和无法自我纠正的问题。

Method: 提出EAG框架，包括环境反馈验证、动态分支探索和经验学习。

Result: a1-32B模型在多个基准测试中表现优异，优于同类模型24.4个百分点。

Conclusion: EAG为可靠机器推理提供了新范式，特别适用于需要精确多步计算和逻辑验证的问题。

Abstract: Large Language Models (LLMs) have made remarkable breakthroughs in reasoning,
yet continue to struggle with hallucinations, logical errors, and inability to
self-correct during complex multi-step tasks. Current approaches like
chain-of-thought prompting offer limited reasoning capabilities that fail when
precise step validation is required. We propose Environment Augmented
Generation (EAG), a framework that enhances LLM reasoning through: (1)
real-time environmental feedback validating each reasoning step, (2) dynamic
branch exploration for investigating alternative solution paths when faced with
errors, and (3) experience-based learning from successful reasoning
trajectories. Unlike existing methods, EAG enables deliberate backtracking and
strategic replanning through tight integration of execution feedback with
branching exploration. Our a1-32B model achieves state-of-the-art performance
among similar-sized models across all benchmarks, matching larger models like
o1 on competition mathematics while outperforming comparable models by up to
24.4 percentage points. Analysis reveals EAG's distinctive scaling pattern:
initial token investment in environment interaction yields substantial
long-term performance dividends, with advantages amplifying proportionally to
task complexity. EAG's theoretical framework demonstrates how environment
interactivity and systematic branch exploration together establish a new
paradigm for reliable machine reasoning, particularly for problems requiring
precise multi-step calculation and logical verification.

</details>


### [30] [Translation Analytics for Freelancers: I. Introduction, Data Preparation, Baseline Evaluations](https://arxiv.org/abs/2504.14619)
*Yuri Balashov,Alex Balashov,Shiho Fukuda Koski*

Main category: cs.CL

TL;DR: 本文是系列论文的第一篇，探讨了语言技术的最新进展为个体翻译者和资源有限的语言服务提供商带来的新机遇，并提出了一个适应自由职业者需求的自动评估指标框架。


<details>
  <summary>Details</summary>
Motivation: 随着神经机器翻译系统和大语言模型的进步，翻译领域发生了巨大变化。这些技术不仅支持翻译，还能进行质量评估、错误识别、术语生成等，为自由职业者创造了新机会。本文旨在帮助翻译者利用这些技术进步。

Method: 本文提出了一个实用框架，将自动评估指标（如BLEU、chrF、TER和COMET）适应自由职业者的需求，并通过一个医学领域的三语料库进行实证分析。

Result: 通过统计分析，本文展示了自动评估指标与人工评价之间的相关性，验证了这些指标在自由职业者场景中的实用性。

Conclusion: 本文强调翻译者应积极拥抱新兴技术，以在不断变化的职业环境中适应并取得成功。

Abstract: This is the first in a series of papers exploring the rapidly expanding new
opportunities arising from recent progress in language technologies for
individual translators and language service providers with modest resources.
The advent of advanced neural machine translation systems, large language
models, and their integration into workflows via computer-assisted translation
tools and translation management systems have reshaped the translation
landscape. These advancements enable not only translation but also quality
evaluation, error spotting, glossary generation, and adaptation to
domain-specific needs, creating new technical opportunities for freelancers. In
this series, we aim to empower translators with actionable methods to harness
these advancements. Our approach emphasizes Translation Analytics, a suite of
evaluation techniques traditionally reserved for large-scale industry
applications but now becoming increasingly available for smaller-scale users.
This first paper introduces a practical framework for adapting automatic
evaluation metrics -- such as BLEU, chrF, TER, and COMET -- to freelancers'
needs. We illustrate the potential of these metrics using a trilingual corpus
derived from a real-world project in the medical domain and provide statistical
analysis correlating human evaluations with automatic scores. Our findings
emphasize the importance of proactive engagement with emerging technologies to
not only adapt but thrive in the evolving professional environment.

</details>


### [31] [A Hierarchical Framework for Measuring Scientific Paper Innovation via Large Language Models](https://arxiv.org/abs/2504.14620)
*Hongming Tan,Shaoxiong Zhan,Fengwei Jia,Hai-Tao Zheng,Wai Kin Chan*

Main category: cs.CL

TL;DR: HSPIM是一个基于大语言模型的层次化框架，用于评估科学论文的创新性，通过分解论文为部分和问答对，结合零样本提示和加权评分，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在评估论文创新性时缺乏上下文和泛化能力，HSPIM旨在解决这些问题。

Method: HSPIM采用分层结构，将论文分解为部分和问答对，利用零样本LLM提示进行分类、问答增强和加权评分。

Result: 在科学会议论文数据集上的实验表明，HSPIM在有效性、泛化性和可解释性上优于基线方法。

Conclusion: HSPIM为科学论文创新性评估提供了一种高效且可推广的解决方案。

Abstract: Measuring scientific paper innovation is both important and challenging.
Existing content-based methods often overlook the full-paper context, fail to
capture the full scope of innovation, and lack generalization. We propose
HSPIM, a hierarchical and training-free framework based on large language
models (LLMs). It introduces a Paper-to-Sections-to-QAs decomposition to assess
innovation. We segment the text by section titles and use zero-shot LLM
prompting to implement section classification, question-answering (QA)
augmentation, and weighted novelty scoring. The generated QA pair focuses on
section-level innovation and serves as additional context to improve the LLM
scoring. For each chunk, the LLM outputs a novelty score and a confidence
score. We use confidence scores as weights to aggregate novelty scores into a
paper-level innovation score. To further improve performance, we propose a
two-layer question structure consisting of common and section-specific
questions, and apply a genetic algorithm to optimize the question-prompt
combinations. Comprehensive experiments on scientific conference paper datasets
show that HSPIM outperforms baseline methods in effectiveness, generalization,
and interpretability.

</details>


### [32] [Automatic Text Summarization (ATS) for Research Documents in Sorani Kurdish](https://arxiv.org/abs/2504.14630)
*Rondik Hadi Abdulrahman,Hossein Hassani*

Main category: cs.CL

TL;DR: 该研究为库尔德语（Sorani方言）开发了一个数据集和语言模型，用于自动文本摘要（ATS），填补了该语言资源不足的空白。通过两种实验（是否包含结论）和多种评估方法，最佳准确率达到19.58%。


<details>
  <summary>Details</summary>
Motivation: 库尔德语（Sorani方言）的自动文本摘要研究资源匮乏，限制了该语言在自然语言处理（NLP）领域的发展。

Method: 研究基于231篇库尔德语科学论文，使用句子加权和TF-IDF算法进行两种实验（是否包含结论），并通过手动和自动（ROUGE指标）评估结果。

Result: 最佳准确率为19.58%，实验结果因文档而异。

Conclusion: 该研究为库尔德语NLP研究者提供了宝贵资源，推动了自动文本摘要及相关领域的发展。

Abstract: Extracting concise information from scientific documents aids learners,
researchers, and practitioners. Automatic Text Summarization (ATS), a key
Natural Language Processing (NLP) application, automates this process. While
ATS methods exist for many languages, Kurdish remains underdeveloped due to
limited resources. This study develops a dataset and language model based on
231 scientific papers in Sorani Kurdish, collected from four academic
departments in two universities in the Kurdistan Region of Iraq (KRI),
averaging 26 pages per document. Using Sentence Weighting and Term
Frequency-Inverse Document Frequency (TF-IDF) algorithms, two experiments were
conducted, differing in whether the conclusions were included. The average word
count was 5,492.3 in the first experiment and 5,266.96 in the second. Results
were evaluated manually and automatically using ROUGE-1, ROUGE-2, and ROUGE-L
metrics, with the best accuracy reaching 19.58%. Six experts conducted manual
evaluations using three criteria, with results varying by document. This
research provides valuable resources for Kurdish NLP researchers to advance ATS
and related fields.

</details>


### [33] [Harnessing Generative LLMs for Enhanced Financial Event Entity Extraction Performance](https://arxiv.org/abs/2504.14633)
*Soo-joon Choi,Ji-jun Park*

Main category: cs.CL

TL;DR: 论文提出了一种基于生成式大语言模型（LLM）的金融事件实体提取方法，通过参数高效微调（PEFT）将任务转化为结构化输出生成，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 金融文本语言复杂且实体重叠，传统序列标注模型难以处理长距离依赖和多实体提取问题，因此探索生成式LLM的潜力。

Method: 将金融事件实体提取任务重新定义为文本到结构化输出的生成任务，使用PEFT微调预训练LLM直接生成包含实体及其字符跨度的JSON对象。

Result: 在CCKS 2019数据集上，该方法实现了新的最优F1分数，显著优于序列标注基线模型（如SEBERTNets）。

Conclusion: 生成式LLM在复杂领域特定信息提取任务中具有潜力，可直接生成高质量结构化输出。

Abstract: Financial event entity extraction is a crucial task for analyzing market
dynamics and building financial knowledge graphs, yet it presents significant
challenges due to the specialized language and complex structures in financial
texts. Traditional approaches often rely on sequence labeling models, which can
struggle with long-range dependencies and the inherent complexity of extracting
multiple, potentially overlapping entities. Motivated by the advanced language
understanding and generative capabilities of Large Language Models (LLMs), we
propose a novel method that reframes financial event entity extraction as a
text-to-structured-output generation task. Our approach involves fine-tuning a
pre-trained LLM using Parameter-Efficient Fine-Tuning (PEFT) to directly
generate a structured representation, such as a JSON object, containing the
extracted entities and their precise character spans from the input text. We
evaluate our method on the challenging CCKS 2019 Financial Event Entity
Extraction dataset, comparing its performance against strong sequence labeling
baselines, including SEBERTNets and sebertNets. Experimental results
demonstrate that our generative LLM method achieves a new state-of-the-art F1
score on this benchmark, significantly outperforming previous methods. Through
detailed quantitative analysis across event types, entity types, and instance
complexity, as well as human evaluation, we show that our approach is more
effective at handling the nuances of financial text and extracting high-quality
entities. This work validates the potential of applying generative LLMs
directly to complex, domain-specific information extraction tasks requiring
structured output.

</details>


### [34] [A Case Study Exploring the Current Landscape of Synthetic Medical Record Generation with Commercial LLMs](https://arxiv.org/abs/2504.14657)
*Yihan Lin,Zhirong Bella Yu,Simon Lee*

Main category: cs.CL

TL;DR: LLMs生成合成电子健康记录（EHRs）在隐私保护和数据共享方面具有潜力，但在高维数据中难以保持真实分布和相关性，限制了其跨医院泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决合成EHRs在跨医院泛化中的可靠性问题，评估LLMs在生成合成数据中的表现。

Method: 评估商业LLMs生成合成EHRs的能力，分析生成过程中的多个方面。

Result: LLMs在小规模特征集上表现可靠，但在高维数据中难以保持真实分布和相关性。

Conclusion: LLMs在生成合成EHRs方面有潜力，但需改进以应对高维数据的挑战。

Abstract: Synthetic Electronic Health Records (EHRs) offer a valuable opportunity to
create privacy preserving and harmonized structured data, supporting numerous
applications in healthcare. Key benefits of synthetic data include precise
control over the data schema, improved fairness and representation of patient
populations, and the ability to share datasets without concerns about
compromising real individuals privacy. Consequently, the AI community has
increasingly turned to Large Language Models (LLMs) to generate synthetic data
across various domains. However, a significant challenge in healthcare is
ensuring that synthetic health records reliably generalize across different
hospitals, a long standing issue in the field. In this work, we evaluate the
current state of commercial LLMs for generating synthetic data and investigate
multiple aspects of the generation process to identify areas where these models
excel and where they fall short. Our main finding from this work is that while
LLMs can reliably generate synthetic health records for smaller subsets of
features, they struggle to preserve realistic distributions and correlations as
the dimensionality of the data increases, ultimately limiting their ability to
generalize across diverse hospital settings.

</details>


### [35] [Trans-Zero: Self-Play Incentivizes Large Language Models for Multilingual Translation Without Parallel Data](https://arxiv.org/abs/2504.14669)
*Wei Zou,Sen Yang,Yu Bao,Shujian Huang,Jiajun Chen,Shanbo Cheng*

Main category: cs.CL

TL;DR: TRANS-ZERO是一个基于自博弈的框架，仅利用单语数据和LLM的多语言知识，通过G-MCTS和偏好优化，实现了与监督方法媲美的翻译性能。


<details>
  <summary>Details</summary>
Motivation: 解决多语言机器翻译中低资源语言数据稀缺和灾难性遗忘的问题。

Method: 结合遗传蒙特卡洛树搜索（G-MCTS）和偏好优化，仅使用单语数据和LLM的多语言知识。

Result: 实验表明，该方法不仅匹配大规模并行数据训练模型的性能，还在非英语翻译方向上表现优异。

Conclusion: G-MCTS通过迭代翻译探索语义一致的候选，显著提升翻译质量，为框架的成功奠定基础。

Abstract: The rise of Large Language Models (LLMs) has reshaped machine translation
(MT), but multilingual MT still relies heavily on parallel data for supervised
fine-tuning (SFT), facing challenges like data scarcity for low-resource
languages and catastrophic forgetting. To address these issues, we propose
TRANS-ZERO, a self-play framework that leverages only monolingual data and the
intrinsic multilingual knowledge of LLM. TRANS-ZERO combines Genetic
Monte-Carlo Tree Search (G-MCTS) with preference optimization, achieving strong
translation performance that rivals supervised methods. Experiments demonstrate
that this approach not only matches the performance of models trained on
large-scale parallel data but also excels in non-English translation
directions. Further analysis reveals that G-MCTS itself significantly enhances
translation quality by exploring semantically consistent candidates through
iterative translations, providing a robust foundation for the framework's
succuss.

</details>


### [36] [FarsEval-PKBETS: A new diverse benchmark for evaluating Persian large language models](https://arxiv.org/abs/2504.14690)
*Mehrnoush Shamsfard,Zahra Saaberi,Mostafa Karimi manesh,Seyed Mohammad Hossein Hashemi,Zahra Vatankhah,Motahareh Ramezani,Niki Pourazin,Tara Zare,Maryam Azimi,Sarina Chitsaz,Sama Khoraminejad,Morteza Mahdavi Mortazavi,Mohammad Mahdi Chizari,Sahar Maleki,Seyed Soroush Majd,Mostafa Masumi,Sayed Ali Musavi Khoeini,Amir Mohseni,Sogol Alipour*

Main category: cs.CL

TL;DR: 论文介绍了FarsEval-PKBETS基准，用于评估波斯语大型语言模型（LLMs）的性能，结果显示当前模型表现不佳。


<details>
  <summary>Details</summary>
Motivation: 研究波斯语等资源较少语言的LLMs性能，填补现有研究的空白。

Method: 构建包含4000个问题的波斯语基准，涵盖多领域任务，并评估三个LLMs模型。

Result: 三个模型的平均准确率低于50%，表明当前模型难以应对该基准。

Conclusion: 波斯语LLMs仍需改进，基准为未来研究提供了方向。

Abstract: Research on evaluating and analyzing large language models (LLMs) has been
extensive for resource-rich languages such as English, yet their performance in
languages such as Persian has received considerably less attention. This paper
introduces FarsEval-PKBETS benchmark, a subset of FarsEval project for
evaluating large language models in Persian. This benchmark consists of 4000
questions and answers in various formats, including multiple choice, short
answer and descriptive responses. It covers a wide range of domains and
tasks,including medicine, law, religion, Persian language, encyclopedic
knowledge, human preferences, social knowledge, ethics and bias, text
generation, and respecting others' rights. This bechmark incorporates
linguistics, cultural, and local considerations relevant to the Persian
language and Iran. To ensure the questions are challenging for current LLMs,
three models -- Llama3-70B, PersianMind, and Dorna -- were evaluated using this
benchmark. Their average accuracy was below 50%, meaning they provided fully
correct answers to fewer than half of the questions. These results indicate
that current language models are still far from being able to solve this
benchmark

</details>


### [37] [OmniV-Med: Scaling Medical Vision-Language Model for Universal Visual Understanding](https://arxiv.org/abs/2504.14692)
*Songtao Jiang,Yuan Wang,Sibo Song,Yan Zhang,Zijie Meng,Bohan Lei,Jian Wu,Jimeng Sun,Zuozhu Liu*

Main category: cs.CL

TL;DR: OmniV-Med提出了一种统一的医学视觉-语言模型框架，通过多模态数据集、统一编码器和令牌修剪机制，实现了高效的医学图像和视频理解。


<details>
  <summary>Details</summary>
Motivation: 解决现有医学视觉-语言模型对不同模态使用独立编码器的局限性。

Method: 构建多模态数据集OmniV-Med-Instruct，设计旋转位置自适应编码器，引入医学感知令牌修剪机制。

Result: OmniV-Med-7B在7个基准测试中表现最佳，轻量版OmniV-Med-1.5B性能接近且训练资源需求低。

Conclusion: OmniV-Med为医学多模态理解提供了高效统一的解决方案，具有实际部署潜力。

Abstract: The practical deployment of medical vision-language models (Med-VLMs)
necessitates seamless integration of textual data with diverse visual
modalities, including 2D/3D images and videos, yet existing models typically
employ separate encoders for different modalities. To address this limitation,
we present OmniV-Med, a unified framework for multimodal medical understanding.
Our technical contributions are threefold: First, we construct
OmniV-Med-Instruct, a comprehensive multimodal medical dataset containing 252K
instructional samples spanning 14 medical image modalities and 11 clinical
tasks. Second, we devise a rotary position-adaptive encoder that processes
multi-resolution 2D/3D images and videos within a unified architecture,
diverging from conventional modality-specific encoders. Third, we introduce a
medical-aware token pruning mechanism that exploits spatial-temporal redundancy
in volumetric data (e.g., consecutive CT slices) and medical videos,
effectively reducing 60\% of visual tokens without performance degradation.
Empirical evaluations demonstrate that OmniV-Med-7B achieves state-of-the-art
performance on 7 benchmarks spanning 2D/3D medical imaging and video
understanding tasks. Notably, our lightweight variant (OmniV-Med-1.5B) attains
comparable performance while requiring only 8 RTX3090 GPUs for training and
supporting efficient long-video inference. Data, code and model will be
released.

</details>


### [38] [Evaluating BERTopic on Open-Ended Data: A Case Study with Belgian Dutch Daily Narratives](https://arxiv.org/abs/2504.14707)
*Ratna Kandala,Katie Hoemann*

Main category: cs.CL

TL;DR: 对比BERTopic、LDA和KMeans在比利时荷兰语日常叙事中的表现，BERTopic因上下文嵌入表现更优，而LDA和KMeans存在局限性。


<details>
  <summary>Details</summary>
Motivation: 探索BERTopic在形态丰富语言中的潜力，并对比其与LDA和KMeans的性能。

Method: 使用BERTopic、LDA和KMeans建模比利时荷兰语叙事，结合自动指标和人工评估。

Result: BERTopic生成的主题更具文化共鸣，LDA和KMeans在语义相关性和一致性上表现较差。

Conclusion: 强调NLP模型在少数语言环境中需更强的泛化能力，并推荐混合评估框架。

Abstract: This study explores BERTopic's potential for modeling open-ended Belgian
Dutch daily narratives, contrasting its performance with Latent Dirichlet
Allocation (LDA) and KMeans. Although LDA scores well on certain automated
metrics, human evaluations reveal semantically irrelevant co-occurrences,
highlighting the limitations of purely statistic-based methods. In contrast,
BERTopic's reliance on contextual embeddings yields culturally resonant themes,
underscoring the importance of hybrid evaluation frameworks that account for
morphologically rich languages. KMeans performed less coherently than prior
research suggested, pointing to the unique challenges posed by personal
narratives. Our findings emphasize the need for robust generalization in NLP
models, especially in underrepresented linguistic contexts.

</details>


### [39] [PROMPTEVALS: A Dataset of Assertions and Guardrails for Custom Production Large Language Model Pipelines](https://arxiv.org/abs/2504.14738)
*Reya Vir,Shreya Shankar,Harrison Chase,Will Fu-Hinthorn,Aditya Parameswaran*

Main category: cs.CL

TL;DR: PROMPTEVALS是一个包含2087个LLM管道提示和12623个断言标准的数据集，用于提升LLM在生成输出时的可靠性。微调的Mistral和Llama 3模型表现优于GPT-4o。


<details>
  <summary>Details</summary>
Motivation: LLM在生产环境中常无法满足开发者期望，需要断言机制提升可靠性，但确定合适的断言标准具有挑战性。

Method: 引入PROMPTEVALS数据集，包含开发者提供的提示和断言标准，并评估闭源和开源模型生成断言的能力。

Result: 微调的Mistral和Llama 3模型比GPT-4o平均性能提升20.93%，且延迟更低。

Conclusion: PROMPTEVALS数据集可推动LLM可靠性、对齐和提示工程的研究。

Abstract: Large language models (LLMs) are increasingly deployed in specialized
production data processing pipelines across diverse domains -- such as finance,
marketing, and e-commerce. However, when running them in production across many
inputs, they often fail to follow instructions or meet developer expectations.
To improve reliability in these applications, creating assertions or guardrails
for LLM outputs to run alongside the pipelines is essential. Yet, determining
the right set of assertions that capture developer requirements for a task is
challenging. In this paper, we introduce PROMPTEVALS, a dataset of 2087 LLM
pipeline prompts with 12623 corresponding assertion criteria, sourced from
developers using our open-source LLM pipeline tools. This dataset is 5x larger
than previous collections. Using a hold-out test split of PROMPTEVALS as a
benchmark, we evaluated closed- and open-source models in generating relevant
assertions. Notably, our fine-tuned Mistral and Llama 3 models outperform
GPT-4o by 20.93% on average, offering both reduced latency and improved
performance. We believe our dataset can spur further research in LLM
reliability, alignment, and prompt engineering.

</details>


### [40] [Disentangling Linguistic Features with Dimension-Wise Analysis of Vector Embeddings](https://arxiv.org/abs/2504.14766)
*Saniya Karwa,Navpreet Singh*

Main category: cs.CL

TL;DR: 该论文提出了一种框架，用于揭示BERT等高维不透明模型中向量嵌入的特定维度如何编码不同语言属性（LPs）。通过LDSP-10数据集和多种分析方法，研究发现某些语言属性（如否定和极性）在特定维度中编码，而其他属性（如同义性）则更复杂。


<details>
  <summary>Details</summary>
Motivation: 理解BERT等高维不透明模型的嵌入机制，揭示其编码语言属性的具体维度。

Method: 使用LDSP-10数据集（包含10种语言特征），结合Wilcoxon符号秩检验、互信息和递归特征消除等方法分析BERT嵌入，并引入EDI评分量化维度对语言属性的影响。

Result: 发现否定和极性等属性在特定维度中编码，而同义性等属性编码更复杂。

Conclusion: 研究为嵌入的可解释性提供了新见解，有助于开发更透明和优化的语言模型，并对模型偏见的缓解和AI系统的负责任部署具有意义。

Abstract: Understanding the inner workings of neural embeddings, particularly in models
such as BERT, remains a challenge because of their high-dimensional and opaque
nature. This paper proposes a framework for uncovering the specific dimensions
of vector embeddings that encode distinct linguistic properties (LPs). We
introduce the Linguistically Distinct Sentence Pairs (LDSP-10) dataset, which
isolates ten key linguistic features such as synonymy, negation, tense, and
quantity. Using this dataset, we analyze BERT embeddings with various methods,
including the Wilcoxon signed-rank test, mutual information, and recursive
feature elimination, to identify the most influential dimensions for each LP.
We introduce a new metric, the Embedding Dimension Impact (EDI) score, which
quantifies the relevance of each embedding dimension to a LP. Our findings show
that certain properties, such as negation and polarity, are robustly encoded in
specific dimensions, while others, like synonymy, exhibit more complex
patterns. This study provides insights into the interpretability of embeddings,
which can guide the development of more transparent and optimized language
models, with implications for model bias mitigation and the responsible
deployment of AI systems.

</details>


### [41] [Knowledge Distillation and Dataset Distillation of Large Language Models: Emerging Trends, Challenges, and Future Directions](https://arxiv.org/abs/2504.14772)
*Luyang Fang,Xiaowei Yu,Jiazhang Cai,Yongkai Chen,Shushan Wu,Zhengliang Liu,Zhenyuan Yang,Haoran Lu,Xilin Gong,Yufang Liu,Terry Ma,Wei Ruan,Ali Abbasi,Jing Zhang,Tao Wang,Ehsan Latif,Wei Liu,Wei Zhang,Soheil Kolouri,Xiaoming Zhai,Dajiang Zhu,Wenxuan Zhong,Tianming Liu,Ping Ma*

Main category: cs.CL

TL;DR: 本文综述了知识蒸馏（KD）和数据集蒸馏（DD）两种互补范式，旨在压缩大型语言模型（LLMs）同时保持其推理能力和语言多样性，并探讨了它们的整合策略及其应用与挑战。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的快速发展，计算和数据需求急剧增加，亟需高效的压缩策略以保持其性能。

Method: 分析了KD中的任务对齐、基于理性的训练和多教师框架，以及DD中的梯度匹配、潜在空间正则化和生成合成等技术。

Result: 整合KD和DD可提升压缩策略的有效性和可扩展性，解决模型扩展、架构异质性和能力保留等问题。

Conclusion: 尽管取得进展，仍需解决推理多样性、适应性和评估协议等挑战，通过整合KD和DD实现可持续的高效LLMs。

Abstract: The exponential growth of Large Language Models (LLMs) continues to highlight
the need for efficient strategies to meet ever-expanding computational and data
demands. This survey provides a comprehensive analysis of two complementary
paradigms: Knowledge Distillation (KD) and Dataset Distillation (DD), both
aimed at compressing LLMs while preserving their advanced reasoning
capabilities and linguistic diversity. We first examine key methodologies in
KD, such as task-specific alignment, rationale-based training, and
multi-teacher frameworks, alongside DD techniques that synthesize compact,
high-impact datasets through optimization-based gradient matching, latent space
regularization, and generative synthesis. Building on these foundations, we
explore how integrating KD and DD can produce more effective and scalable
compression strategies. Together, these approaches address persistent
challenges in model scalability, architectural heterogeneity, and the
preservation of emergent LLM abilities. We further highlight applications
across domains such as healthcare and education, where distillation enables
efficient deployment without sacrificing performance. Despite substantial
progress, open challenges remain in preserving emergent reasoning and
linguistic diversity, enabling efficient adaptation to continually evolving
teacher models and datasets, and establishing comprehensive evaluation
protocols. By synthesizing methodological innovations, theoretical foundations,
and practical insights, our survey charts a path toward sustainable,
resource-efficient LLMs through the tighter integration of KD and DD
principles.

</details>


### [42] [Automatic Evaluation Metrics for Document-level Translation: Overview, Challenges and Trends](https://arxiv.org/abs/2504.14804)
*Jiaxin GUO,Xiaoyu Chen,Zhiqiang Rao,Jinlong Yang,Zongyao Li,Hengchao Shang,Daimeng Wei,Hao Yang*

Main category: cs.CL

TL;DR: 本文探讨了文档级机器翻译的自动评估现状与挑战，并展望了未来发展方向。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLMs）的兴起，文档级翻译取得显著进展，但评估其质量仍是一个紧迫问题。

Method: 分析了现有自动评估方案和指标，包括有无参考文本的方法、传统指标、基于模型的指标及基于LLM的指标。

Result: 指出当前评估方法的不足，如参考多样性缺乏、依赖句子级对齐信息、LLM评估方法的偏见和不准确性。

Conclusion: 展望未来趋势，提出减少对句子级信息的依赖、引入多层次评估方法等研究方向。

Abstract: With the rapid development of deep learning technologies, the field of
machine translation has witnessed significant progress, especially with the
advent of large language models (LLMs) that have greatly propelled the
advancement of document-level translation. However, accurately evaluating the
quality of document-level translation remains an urgent issue. This paper first
introduces the development status of document-level translation and the
importance of evaluation, highlighting the crucial role of automatic evaluation
metrics in reflecting translation quality and guiding the improvement of
translation systems. It then provides a detailed analysis of the current state
of automatic evaluation schemes and metrics, including evaluation methods with
and without reference texts, as well as traditional metrics, Model-based
metrics and LLM-based metrics. Subsequently, the paper explores the challenges
faced by current evaluation methods, such as the lack of reference diversity,
dependence on sentence-level alignment information, and the bias, inaccuracy,
and lack of interpretability of the LLM-as-a-judge method. Finally, the paper
looks ahead to the future trends in evaluation methods, including the
development of more user-friendly document-level evaluation methods and more
robust LLM-as-a-judge methods, and proposes possible research directions, such
as reducing the dependency on sentence-level information, introducing
multi-level and multi-granular evaluation approaches, and training models
specifically for machine translation evaluation. This study aims to provide a
comprehensive analysis of automatic evaluation for document-level translation
and offer insights into future developments.

</details>


### [43] [On Self-improving Token Embeddings](https://arxiv.org/abs/2504.14808)
*Mario M. Kubek,Shiraj Pokharel,Thomas Böhme,Emma L. McDaniel,Herwig Unger,Armin R. Mikler*

Main category: cs.CL

TL;DR: 本文提出了一种快速优化预训练静态词或标记嵌入的新方法，通过结合文本语料库中相邻标记的嵌入，持续更新每个标记的表示，包括那些没有预分配嵌入的标记。该方法有效解决了词汇外问题，且不依赖大型语言模型或浅层神经网络，适用于语料库探索、概念搜索和词义消歧等任务。


<details>
  <summary>Details</summary>
Motivation: 动机在于解决预训练嵌入的局限性，特别是在特定领域语料库中词汇外问题和嵌入质量不足的问题。

Method: 方法是通过结合相邻标记的嵌入动态更新标记表示，适用于主题同质的语料库，提升嵌入的领域相关性。

Result: 结果表明，该方法能够显著提升领域相关标记的表示质量，并在风暴事件数据库中展示了其实际应用效果。

Conclusion: 结论是该方法为特定领域语料库提供了一种高效且灵活的嵌入优化方案，能够捕捉词汇的演变和领域特性。

Abstract: This article introduces a novel and fast method for refining pre-trained
static word or, more generally, token embeddings. By incorporating the
embeddings of neighboring tokens in text corpora, it continuously updates the
representation of each token, including those without pre-assigned embeddings.
This approach effectively addresses the out-of-vocabulary problem, too.
Operating independently of large language models and shallow neural networks,
it enables versatile applications such as corpus exploration, conceptual
search, and word sense disambiguation. The method is designed to enhance token
representations within topically homogeneous corpora, where the vocabulary is
restricted to a specific domain, resulting in more meaningful embeddings
compared to general-purpose pre-trained vectors. As an example, the methodology
is applied to explore storm events and their impacts on infrastructure and
communities using narratives from a subset of the NOAA Storm Events database.
The article also demonstrates how the approach improves the representation of
storm-related terms over time, providing valuable insights into the evolving
nature of disaster narratives.

</details>


### [44] [Transparentize the Internal and External Knowledge Utilization in LLMs with Trustworthy Citation](https://arxiv.org/abs/2504.14856)
*Jiajun Shen,Tong Zhou,Yubo Chen,Delai Qiu,Shengping Liu,Kang Liu,Jun Zhao*

Main category: cs.CL

TL;DR: 论文提出了一种结合外部和内部知识的引用生成任务，并设计了RAEL范式和INTRALIGN方法，实验表明其方法在跨场景性能上优于基线。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型生成答案时内部知识利用不透明和可信度存疑的问题。

Method: 引入Context-Prior Augmented Citation Generation任务，结合RAEL范式和INTRALIGN方法（包括数据生成和对齐算法）。

Result: 实验结果显示方法在跨场景性能上优于基线，且检索质量、问题类型和模型知识对引用生成的可信度有显著影响。

Conclusion: 论文提出的方法能有效提升引用生成的可信度和性能，揭示了影响可信度的关键因素。

Abstract: While hallucinations of large language models could been alleviated through
retrieval-augmented generation and citation generation, how the model utilizes
internal knowledge is still opaque, and the trustworthiness of its generated
answers remains questionable. In this work, we introduce Context-Prior
Augmented Citation Generation task, requiring models to generate citations
considering both external and internal knowledge while providing trustworthy
references, with 5 evaluation metrics focusing on 3 aspects: answer
helpfulness, citation faithfulness, and trustworthiness. We introduce RAEL, the
paradigm for our task, and also design INTRALIGN, an integrated method
containing customary data generation and an alignment algorithm. Our
experimental results show that our method achieves a better cross-scenario
performance with regard to other baselines. Our extended experiments further
reveal that retrieval quality, question types, and model knowledge have
considerable influence on the trustworthiness in citation generation.

</details>


### [45] [Natural Fingerprints of Large Language Models](https://arxiv.org/abs/2504.14871)
*Teppei Suzuki,Ryokan Ri,Sho Takase*

Main category: cs.CL

TL;DR: 研究发现，即使大语言模型（LLMs）使用相同的数据训练，其生成的文本仍能通过自然指纹区分来源，这些指纹源于训练过程中的细微差异。


<details>
  <summary>Details</summary>
Motivation: 探究LLMs输出中可识别特征的成因，以理解无意偏差的来源并改进模型行为控制。

Method: 通过系统控制训练条件（如参数大小、优化设置、随机种子等），分析LLMs生成文本的差异。

Result: 发现训练过程中的细微差异会导致自然指纹，从而区分模型来源。

Conclusion: 理解自然指纹有助于揭示无意偏差的起源，并为改进LLM行为控制提供新思路。

Abstract: Large language models (LLMs) often exhibit biases -- systematic deviations
from expected norms -- in their outputs. These range from overt issues, such as
unfair responses, to subtler patterns that can reveal which model produced
them. We investigate the factors that give rise to identifiable characteristics
in LLMs. Since LLMs model training data distribution, it is reasonable that
differences in training data naturally lead to the characteristics. However,
our findings reveal that even when LLMs are trained on the exact same data, it
is still possible to distinguish the source model based on its generated text.
We refer to these unintended, distinctive characteristics as natural
fingerprints. By systematically controlling training conditions, we show that
the natural fingerprints can emerge from subtle differences in the training
process, such as parameter sizes, optimization settings, and even random seeds.
We believe that understanding natural fingerprints offers new insights into the
origins of unintended bias and ways for improving control over LLM behavior.

</details>


### [46] [Retrieval Augmented Generation Evaluation in the Era of Large Language Models: A Comprehensive Survey](https://arxiv.org/abs/2504.14891)
*Aoran Gan,Hao Yu,Kai Zhang,Qi Liu,Wenyu Yan,Zhenya Huang,Shiwei Tong,Guoping Hu*

Main category: cs.CL

TL;DR: 本文综述了检索增强生成（RAG）系统的评估方法，分析了传统与新兴评估方法，并整理了相关数据集和框架，为RAG发展提供关键资源。


<details>
  <summary>Details</summary>
Motivation: RAG系统结合检索与生成组件，依赖动态知识源，评估面临独特挑战，需系统梳理现有方法。

Method: 系统回顾RAG评估方法，包括性能、事实准确性、安全性和计算效率，并进行元分析。

Result: 整理了RAG专用数据集和评估框架，填补了传统与LLM驱动方法间的空白。

Conclusion: 本文是RAG评估领域最全面的综述，为未来研究提供了重要参考。

Abstract: Recent advancements in Retrieval-Augmented Generation (RAG) have
revolutionized natural language processing by integrating Large Language Models
(LLMs) with external information retrieval, enabling accurate, up-to-date, and
verifiable text generation across diverse applications. However, evaluating RAG
systems presents unique challenges due to their hybrid architecture that
combines retrieval and generation components, as well as their dependence on
dynamic knowledge sources in the LLM era. In response, this paper provides a
comprehensive survey of RAG evaluation methods and frameworks, systematically
reviewing traditional and emerging evaluation approaches, for system
performance, factual accuracy, safety, and computational efficiency in the LLM
era. We also compile and categorize the RAG-specific datasets and evaluation
frameworks, conducting a meta-analysis of evaluation practices in high-impact
RAG research. To the best of our knowledge, this work represents the most
comprehensive survey for RAG evaluation, bridging traditional and LLM-driven
methods, and serves as a critical resource for advancing RAG development.

</details>


### [47] [CRAVE: A Conflicting Reasoning Approach for Explainable Claim Verification Using LLMs](https://arxiv.org/abs/2504.14905)
*Yingming Zheng,Xiaoliang Liu,Peng Wu,Li Pan*

Main category: cs.CL

TL;DR: CRAVE提出了一种基于冲突推理的可解释性声明验证方法，通过大型语言模型（LLMs）生成冲突立场，并结合小型语言模型（SLM）进行最终判断，显著提升了复杂声明的验证准确性和透明度。


<details>
  <summary>Details</summary>
Motivation: 数字媒体和AI生成内容导致虚假信息迅速传播，传统依赖专家标注证据的方法效率低且难以扩展，现有自动化系统对复杂声明的推理能力不足。

Method: CRAVE采用三模块框架：1）消除歧义并检索证据；2）利用LLMs从四个维度推理冲突立场并初步判断；3）通过SLM评估冲突立场并做出最终判断。

Result: 在两个公开数据集上的实验表明，CRAVE性能优于现有方法，证据检索能力和解释性更强。

Conclusion: CRAVE通过冲突推理和分层判断，显著提升了复杂声明的验证效果，同时增强了模型的可解释性。

Abstract: The rapid spread of misinformation, driven by digital media and AI-generated
content, has made automatic claim verification essential. Traditional methods,
which depend on expert-annotated evidence, are labor-intensive and not
scalable. Although recent automated systems have improved, they still struggle
with complex claims that require nuanced reasoning. To address this, we propose
CRAVE, a Conflicting Reasoning Approach for explainable claim VErification,
that verify the complex claims based on the conflicting rationales reasoned by
large language models (LLMs). Specifically, CRAVE introduces a three-module
framework. Ambiguity Elimination enchanced Evidence Retrieval module performs
ambiguity elimination and entity-based search to gather relevant evidence
related to claim verification from external sources like Wikipedia. Conflicting
Perspective Reasoning and Preliminary Judgment module with LLMs adopts LLMs to
reason rationales with conflicting stances about claim verification from
retrieved evidence across four dimensions, i.e., direct evidence, semantic
relationships, linguistic patterns, and logical reasoning and make a
preliminary judgment. Finally, Small Language Model (SLM) based Judge module is
fine-tuned to make use of preliminary judgment from LLMs to assess the
confidence of the conflicting rationales and make a final authenticity
judgment. This methodology allows CRAVE to capture subtle inconsistencies in
complex claims, improving both the accuracy and transparency of claim
verification. Extensive experiments on two public claim verification datasets
demonstrate that our CRAVE model achieves much better performance than
state-of-the-art methods and exhibits a superior capacity for finding relevant
evidence and explaining the model predictions. The code is provided at
https://github.com/8zym/CRAVE.

</details>


### [48] [Speaker Fuzzy Fingerprints: Benchmarking Text-Based Identification in Multiparty Dialogues](https://arxiv.org/abs/2504.14963)
*Rui Ribeiro,Luísa Coheur,Joao P. Carvalho*

Main category: cs.CL

TL;DR: 本文提出了一种基于模糊指纹和上下文建模的文本说话人识别方法，显著提高了准确性，并分析了模糊指纹的优势和说话人无关语句的检测机制。


<details>
  <summary>Details</summary>
Motivation: 传统方法在仅依赖文本数据时难以有效识别说话人，因此需要探索更先进的技术来解决这一问题。

Method: 利用大型预训练模型的模糊指纹，结合说话人特定标记和上下文感知建模，提升识别准确性。

Result: 在Friends和Big Bang Theory数据集上分别达到70.6%和67.7%的准确率，模糊指纹在减少隐藏单元的同时接近全微调性能。

Conclusion: 研究揭示了文本说话人识别的关键挑战，并为未来改进提供了方向。

Abstract: Speaker identification using voice recordings leverages unique acoustic
features, but this approach fails when only textual data is available. Few
approaches have attempted to tackle the problem of identifying speakers solely
from text, and the existing ones have primarily relied on traditional methods.
In this work, we explore the use of fuzzy fingerprints from large pre-trained
models to improve text-based speaker identification. We integrate
speaker-specific tokens and context-aware modeling, demonstrating that
conversational context significantly boosts accuracy, reaching 70.6% on the
Friends dataset and 67.7% on the Big Bang Theory dataset. Additionally, we show
that fuzzy fingerprints can approximate full fine-tuning performance with fewer
hidden units, offering improved interpretability. Finally, we analyze ambiguous
utterances and propose a mechanism to detect speaker-agnostic lines. Our
findings highlight key challenges and provide insights for future improvements
in text-based speaker identification.

</details>


### [49] [Evaluating LLMs on Chinese Topic Constructions: A Research Proposal Inspired by Tian et al. (2024)](https://arxiv.org/abs/2504.14969)
*Xiaodong Yang*

Main category: cs.CL

TL;DR: 提出一个评估大语言模型（LLMs）对中文话题结构敏感性的框架，重点关注其对孤岛约束的敏感性。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs对汉语语法的理解能力，为未来研究提供基础。

Method: 借鉴Tian et al. (2024)的方法，设计实验测试LLMs对汉语语法的知识。

Result: 目前尚未进行实验，旨在为未来研究提供方法论基础。

Conclusion: 该提案为未来研究提供了框架，并邀请对方法论的反馈。

Abstract: This paper proposes a framework for evaluating large language models (LLMs)
on Chinese topic constructions, focusing on their sensitivity to island
constraints. Drawing inspiration from Tian et al. (2024), we outline an
experimental design for testing LLMs' grammatical knowledge of Mandarin syntax.
While no experiments have been conducted yet, this proposal aims to provide a
foundation for future studies and invites feedback on the methodology.

</details>


### [50] [Efficient Pretraining Length Scaling](https://arxiv.org/abs/2504.14992)
*Bohong Wu,Shen Yan,Sijun Zhang,Jianqiao Lu,Yutao Zeng,Ya Wang,Xun Zhou*

Main category: cs.CL

TL;DR: 论文提出了一种名为PHD-Transformer的新框架，通过在预训练中实现高效的长度扩展，同时保持推理效率。


<details>
  <summary>Details</summary>
Motivation: 探索预训练中长度扩展的潜力，解决现有方法在预训练中未充分研究的问题。

Method: 采用创新的KV缓存管理策略，区分原始令牌和隐藏解码令牌，并引入两种优化变体（PHD-SWA和PHD-CSWA）以提升性能。

Result: 在多个基准测试中表现出一致的改进。

Conclusion: PHD-Transformer展示了在预训练中实现高效长度扩展的潜力，同时保持推理效率。

Abstract: Recent advances in large language models have demonstrated the effectiveness
of length scaling during post-training, yet its potential in pre-training
remains underexplored. We present the Parallel Hidden Decoding Transformer
(\textit{PHD}-Transformer), a novel framework that enables efficient length
scaling during pre-training while maintaining inference efficiency.
\textit{PHD}-Transformer achieves this through an innovative KV cache
management strategy that distinguishes between original tokens and hidden
decoding tokens. By retaining only the KV cache of original tokens for
long-range dependencies while immediately discarding hidden decoding tokens
after use, our approach maintains the same KV cache size as the vanilla
transformer while enabling effective length scaling. To further enhance
performance, we introduce two optimized variants: \textit{PHD-SWA} employs
sliding window attention to preserve local dependencies, while
\textit{PHD-CSWA} implements chunk-wise sliding window attention to eliminate
linear growth in pre-filling time. Extensive experiments demonstrate consistent
improvements across multiple benchmarks.

</details>


### [51] [Stay Hungry, Stay Foolish: On the Extended Reading Articles Generation with LLMs](https://arxiv.org/abs/2504.15013)
*Yow-Fu Liou,Yu-Chien Tang,An-Zi Yen*

Main category: cs.CL

TL;DR: 研究探讨了利用大语言模型（LLMs）自动化生成教育材料和课程建议的潜力，通过实验验证了生成内容的高质量和准确性。


<details>
  <summary>Details</summary>
Motivation: 减轻教育工作者制作教育材料的负担，利用LLMs自动化生成扩展阅读材料和相关课程建议。

Method: 从视频转录生成扩展文章，结合历史、文化和轶事，利用语义相似度排名推荐相关课程，并通过LLM优化建议。

Result: 实验表明，模型生成的内容质量高，课程建议准确，提升了学习的趣味性和可访问性。

Conclusion: LLMs能有效连接核心内容与补充学习资源，为学生和教师提供支持。

Abstract: The process of creating educational materials is both time-consuming and
demanding for educators. This research explores the potential of Large Language
Models (LLMs) to streamline this task by automating the generation of extended
reading materials and relevant course suggestions. Using the TED-Ed Dig Deeper
sections as an initial exploration, we investigate how supplementary articles
can be enriched with contextual knowledge and connected to additional learning
resources. Our method begins by generating extended articles from video
transcripts, leveraging LLMs to include historical insights, cultural examples,
and illustrative anecdotes. A recommendation system employing semantic
similarity ranking identifies related courses, followed by an LLM-based
refinement process to enhance relevance. The final articles are tailored to
seamlessly integrate these recommendations, ensuring they remain cohesive and
informative. Experimental evaluations demonstrate that our model produces
high-quality content and accurate course suggestions, assessed through metrics
such as Hit Rate, semantic similarity, and coherence. Our experimental analysis
highlight the nuanced differences between the generated and existing materials,
underscoring the model's capacity to offer more engaging and accessible
learning experiences. This study showcases how LLMs can bridge the gap between
core content and supplementary learning, providing students with additional
recommended resources while also assisting teachers in designing educational
materials.

</details>


### [52] [LLMs as Data Annotators: How Close Are We to Human Performance](https://arxiv.org/abs/2504.15022)
*Muhammad Uzair Ul Haq,Davide Rigoni,Alessandro Sperduti*

Main category: cs.CL

TL;DR: 论文探讨了在NLP中利用LLMs自动生成高质量标注数据的挑战，提出通过自动检索上下文示例改进ICL方法，并在NER任务中比较了不同LLM和嵌入模型的性能。


<details>
  <summary>Details</summary>
Motivation: 手动标注数据成本高且耗时，而现有ICL方法依赖人工选择上下文示例，效率低且性能不佳。

Method: 比较多种LLM和嵌入模型在NER任务中的表现，并引入基于RAG的自动检索方法改进ICL。

Result: 结果显示选择合适的LLM和嵌入模型至关重要，同时需权衡模型规模与性能，并关注更具挑战性的数据集。

Conclusion: 研究强调了自动检索上下文示例的潜力，并呼吁未来研究关注更复杂的数据集和模型优化。

Abstract: In NLP, fine-tuning LLMs is effective for various applications but requires
high-quality annotated data. However, manual annotation of data is
labor-intensive, time-consuming, and costly. Therefore, LLMs are increasingly
used to automate the process, often employing in-context learning (ICL) in
which some examples related to the task are given in the prompt for better
performance. However, manually selecting context examples can lead to
inefficiencies and suboptimal model performance. This paper presents
comprehensive experiments comparing several LLMs, considering different
embedding models, across various datasets for the Named Entity Recognition
(NER) task. The evaluation encompasses models with approximately $7$B and $70$B
parameters, including both proprietary and non-proprietary models. Furthermore,
leveraging the success of Retrieval-Augmented Generation (RAG), it also
considers a method that addresses the limitations of ICL by automatically
retrieving contextual examples, thereby enhancing performance. The results
highlight the importance of selecting the appropriate LLM and embedding model,
understanding the trade-offs between LLM sizes and desired performance, and the
necessity to direct research efforts towards more challenging datasets.

</details>


### [53] [DistilQwen2.5: Industrial Practices of Training Distilled Open Lightweight Language Models](https://arxiv.org/abs/2504.15027)
*Chengyu Wang,Junbing Yan,Yuanhao Yue,Jun Huang*

Main category: cs.CL

TL;DR: DistilQwen2.5是一系列轻量级大语言模型，通过蒸馏技术从Qwen2.5模型派生而来，提升了指令跟随能力，并降低了部署成本。


<details>
  <summary>Details</summary>
Motivation: 在资源受限的场景下，提升大语言模型的计算效率并降低部署成本是关键挑战。

Method: 利用多代理教师模型选择和改写指令-响应对，结合标准微调和模型融合技术，逐步整合教师模型的细粒度知识。

Result: 蒸馏后的模型能力显著优于原始模型，并在实际应用中展示了效果。

Conclusion: DistilQwen2.5模型已开源，为实际应用提供了高效且轻量化的解决方案。

Abstract: Enhancing computational efficiency and reducing deployment costs for large
language models (LLMs) have become critical challenges in various
resource-constrained scenarios. In this work, we present DistilQwen2.5, a
family of distilled, lightweight LLMs derived from the public Qwen2.5 models.
These distilled models exhibit enhanced instruction-following capabilities
compared to the original models based on a series of distillation techniques
that incorporate knowledge from much larger LLMs. In our industrial practice,
we first leverage powerful proprietary LLMs with varying capacities as
multi-agent teachers to select, rewrite, and refine instruction-response pairs
that are more suitable for student LLMs to learn. After standard fine-tuning,
we further leverage a computationally efficient model fusion approach that
enables student models to progressively integrate fine-grained hidden knowledge
from their teachers. Experimental evaluations demonstrate that the distilled
models possess significantly stronger capabilities than their original
checkpoints. Additionally, we present use cases to illustrate the applications
of our framework in real-world scenarios. To facilitate practical use, we have
released all the DistilQwen2.5 models to the open-source community.

</details>


### [54] [RainbowPlus: Enhancing Adversarial Prompt Generation via Evolutionary Quality-Diversity Search](https://arxiv.org/abs/2504.15047)
*Quy-Anh Dang,Chris Ngo,Truong-Son Hy*

Main category: cs.CL

TL;DR: RainbowPlus是一种基于进化计算的新型红队框架，通过自适应质量多样性搜索增强对抗性提示生成，显著提高了攻击成功率和多样性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）存在对抗性提示漏洞，现有红队方法在可扩展性、资源消耗和攻击策略多样性方面存在局限。

Method: RainbowPlus采用多元素存档存储多样化高质量提示，并使用综合适应度函数评估多个提示，克服了传统方法的限制。

Result: 在多个基准数据集和LLMs上，RainbowPlus表现出更高的攻击成功率（ASR 81.1%）和多样性（Diverse-Score ≈0.84），且速度更快。

Conclusion: RainbowPlus为LLM安全性评估提供了可扩展工具，开源实现支持未来研究。

Abstract: Large Language Models (LLMs) exhibit remarkable capabilities but are
susceptible to adversarial prompts that exploit vulnerabilities to produce
unsafe or biased outputs. Existing red-teaming methods often face scalability
challenges, resource-intensive requirements, or limited diversity in attack
strategies. We propose RainbowPlus, a novel red-teaming framework rooted in
evolutionary computation, enhancing adversarial prompt generation through an
adaptive quality-diversity (QD) search that extends classical evolutionary
algorithms like MAP-Elites with innovations tailored for language models. By
employing a multi-element archive to store diverse high-quality prompts and a
comprehensive fitness function to evaluate multiple prompts concurrently,
RainbowPlus overcomes the constraints of single-prompt archives and pairwise
comparisons in prior QD methods like Rainbow Teaming. Experiments comparing
RainbowPlus to QD methods across six benchmark datasets and four open-source
LLMs demonstrate superior attack success rate (ASR) and diversity
(Diverse-Score $\approx 0.84$), generating up to 100 times more unique prompts
(e.g., 10,418 vs. 100 for Ministral-8B-Instruct-2410). Against nine
state-of-the-art methods on the HarmBench dataset with twelve LLMs (ten
open-source, two closed-source), RainbowPlus achieves an average ASR of 81.1%,
surpassing AutoDAN-Turbo by 3.9%, and is 9 times faster (1.45 vs. 13.50 hours).
Our open-source implementation fosters further advancements in LLM safety,
offering a scalable tool for vulnerability assessment. Code and resources are
publicly available at https://github.com/knoveleng/rainbowplus, supporting
reproducibility and future research in LLM red-teaming.

</details>


### [55] [Testing LLMs' Capabilities in Annotating Translations Based on an Error Typology Designed for LSP Translation: First Experiments with ChatGPT](https://arxiv.org/abs/2504.15052)
*Joachim Minder,Guillaume Wisniewski,Natalie Kübler*

Main category: cs.CL

TL;DR: 研究探讨了ChatGPT在基于错误类型标注机器翻译输出方面的能力，发现其在专业翻译中表现良好，但自我评估能力有限。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型（如ChatGPT）在专业翻译错误标注中的潜力，填补以往研究主要关注通用语言的空白。

Method: 通过两种不同提示和定制错误类型学，比较ChatGPT与人类专家对DeepL和ChatGPT自身翻译的标注结果。

Result: ChatGPT对DeepL翻译的标注召回率和精确度较高，但自我评估表现较差；提示的详细程度影响分类准确性。

Conclusion: 研究揭示了大型语言模型在翻译评估中的潜力与局限，为未来开源模型研究和翻译教学应用提供了方向。

Abstract: This study investigates the capabilities of large language models (LLMs),
specifically ChatGPT, in annotating MT outputs based on an error typology. In
contrast to previous work focusing mainly on general language, we explore
ChatGPT's ability to identify and categorise errors in specialised
translations. By testing two different prompts and based on a customised error
typology, we compare ChatGPT annotations with human expert evaluations of
translations produced by DeepL and ChatGPT itself. The results show that, for
translations generated by DeepL, recall and precision are quite high. However,
the degree of accuracy in error categorisation depends on the prompt's specific
features and its level of detail, ChatGPT performing very well with a detailed
prompt. When evaluating its own translations, ChatGPT achieves significantly
poorer results, revealing limitations with self-assessment. These results
highlight both the potential and the limitations of LLMs for translation
evaluation, particularly in specialised domains. Our experiments pave the way
for future research on open-source LLMs, which could produce annotations of
comparable or even higher quality. In the future, we also aim to test the
practical effectiveness of this automated evaluation in the context of
translation training, particularly by optimising the process of human
evaluation by teachers and by exploring the impact of annotations by LLMs on
students' post-editing and translation learning.

</details>


### [56] [Rethinking the Potential of Multimodality in Collaborative Problem Solving Diagnosis with Large Language Models](https://arxiv.org/abs/2504.15093)
*K. Wong,B. Wu,S. Bulathwela,M. Cukurova*

Main category: cs.CL

TL;DR: 研究探讨了多模态数据在诊断学生协作问题解决（CPS）能力中的潜力，发现其在特定类型的CPS指标中表现更好，但效果依赖于标签复杂性和数据集组成。


<details>
  <summary>Details</summary>
Motivation: 探索多模态数据和先进模型在检测复杂CPS行为中的价值，填补实证证据的不足。

Method: 使用文本嵌入和声学嵌入构建多模态分类模型，比较单模态和多模态模型在CPS诊断中的表现。

Result: 多模态在基于Transformer的模型中提升了社交认知类CPS的诊断性能，但对传统模型无显著改进。

Conclusion: 多模态和模型选择需根据具体CPS指标类型和数据集特性调整，强调人机互补和进一步探索模型架构。

Abstract: Detecting collaborative and problem-solving behaviours from digital traces to
interpret students' collaborative problem solving (CPS) competency is a
long-term goal in the Artificial Intelligence in Education (AIEd) field.
Although multimodal data and advanced models are argued to have the potential
to detect complex CPS behaviours, empirical evidence on their value remains
limited with some contrasting evidence. In this study, we investigated the
potential of multimodal data to improve model performance in diagnosing 78
secondary school students' CPS subskills and indicators in authentic
educational settings. In particular, text embeddings from verbal data and
acoustic embeddings from audio data were used in a multimodal classification
model for CPS diagnosis. Both unimodal and multimodal transformer-based models
outperformed traditional models in detecting CPS classes. Although the
inclusion of multimodality did not improve the performance of traditional
unimodal models, its integration into transformer-based models demonstrated
improved performance for diagnosing social-cognitive CPS classes compared to
unimodal transformer-based models. Based on the results, the paper argues that
multimodality and the selection of a particular modelling technique should not
be taken for granted to achieve the best performance in the automated detection
of every CPS subskill and indicator. Rather, their value is limited to certain
types of CPS indicators, affected by the complexity of the labels, and
dependent on the composition of indicators in the dataset. We conclude the
paper by discussing the required nuance when considering the value of LLMs and
multimodality in automated CPS diagnosis, highlighting the need for human-AI
complementarity, and proposing the exploration of relevant model architectures
and techniques to improve CPS diagnosis in authentic educational contexts.

</details>


### [57] [Kuwain 1.5B: An Arabic SLM via Language Injection](https://arxiv.org/abs/2504.15120)
*Khalil Hennara,Sara Chrouf,Mohamed Motaism Hamed,Zeina Aldallal,Omar Hadid,Safwan AlModhayan*

Main category: cs.CL

TL;DR: 论文提出了一种将新语言整合到大型语言模型（LLM）中的方法，成功将阿拉伯语注入到以英语为主的小型开源模型中，性能提升8%，同时保留原有知识。


<details>
  <summary>Details</summary>
Motivation: 增强现有模型以融入新知识是AI发展的关键，但传统方法成本高且资源密集。

Method: 通过将阿拉伯语注入到以英语为主的小型开源模型中，训练了一个名为Kuwain的15亿参数模型。

Result: 阿拉伯语性能平均提升8%，同时保留原有知识，提供了一种成本效益高的替代方案。

Conclusion: 该方法展示了无需大规模重新训练即可高效扩展语言模型的潜力。

Abstract: Enhancing existing models with new knowledge is a crucial aspect of AI
development. This paper introduces a novel method for integrating a new
language into a large language model (LLM). Our approach successfully
incorporates a previously unseen target language into an existing LLM without
compromising its prior knowledge. We trained a tiny model with 1.5 billion
parameters named Kuwain by injecting the Arabic language into a small
open-source model mainly trained in English. Our method demonstrates
significant improvements in Arabic language performance, with an average 8%
improvement across various benchmarks, while retaining the model's existing
knowledge with a minimum amount of the original model's data. This offers a
cost-effective alternative to training a comprehensive model in both English
and Arabic. The results highlight the potential for efficient, targeted
language model expansion without extensive retraining or resource-intensive
processes.

</details>


### [58] [EasyEdit2: An Easy-to-use Steering Framework for Editing Large Language Models](https://arxiv.org/abs/2504.15133)
*Ziwen Xu,Shuxun Wang,Kewei Xu,Haoming Xu,Mengru Wang,Xinle Deng,Yunzhi Yao,Guozhou Zheng,Huajun Chen,Ningyu Zhang*

Main category: cs.CL

TL;DR: EasyEdit2是一个用于控制大型语言模型行为的框架，支持多种测试时干预，无需修改模型参数即可调整行为。


<details>
  <summary>Details</summary>
Motivation: 提供一种简单易用的方法，让用户无需深入技术知识即可精确控制LLM的行为。

Method: 采用新的架构，包括导向向量生成器和应用器，通过单一样本自动生成和应用导向向量。

Result: 在不同LLM上验证了其有效性，实现了高效且精确的行为控制。

Conclusion: EasyEdit2为LLM行为控制提供了一种高效、易用的解决方案。

Abstract: In this paper, we introduce EasyEdit2, a framework designed to enable
plug-and-play adjustability for controlling Large Language Model (LLM)
behaviors. EasyEdit2 supports a wide range of test-time interventions,
including safety, sentiment, personality, reasoning patterns, factuality, and
language features. Unlike its predecessor, EasyEdit2 features a new
architecture specifically designed for seamless model steering. It comprises
key modules such as the steering vector generator and the steering vector
applier, which enable automatic generation and application of steering vectors
to influence the model's behavior without modifying its parameters. One of the
main advantages of EasyEdit2 is its ease of use-users do not need extensive
technical knowledge. With just a single example, they can effectively guide and
adjust the model's responses, making precise control both accessible and
efficient. Empirically, we report model steering performance across different
LLMs, demonstrating the effectiveness of these techniques. We have released the
source code on GitHub at https://github.com/zjunlp/EasyEdit along with a
demonstration notebook. In addition, we provide a demo video at
https://zjunlp.github.io/project/EasyEdit2/video for a quick introduction.

</details>


### [59] [The Synthetic Imputation Approach: Generating Optimal Synthetic Texts For Underrepresented Categories In Supervised Classification Tasks](https://arxiv.org/abs/2504.15160)
*Joan C. Timoneda*

Main category: cs.CL

TL;DR: 论文提出了一种合成插补方法，利用生成式LLM（如GPT-4o）生成合成文本，以解决训练数据中类别不平衡的问题。该方法在75个原始样本时表现与完整样本相当，且过拟合可控。


<details>
  <summary>Details</summary>
Motivation: 在构建高质量训练集时，难以确保所有类别都有足够的样本，这影响了编码器-解码器LLM（如BERT和RoBERTa）的性能。

Method: 通过生成式LLM（GPT-4o）生成合成文本，基于5个原始样本的随机替换和精心设计的提示，确保合成文本与原始文本有足够差异但保留实质意义。

Result: 在75个原始样本时，合成插补方法表现与完整样本相当；50个样本时过拟合低且可预测。

Conclusion: 合成插补方法为生成式LLM在研究中提供了新角色，帮助应用研究者平衡数据集以获得最佳性能。

Abstract: Encoder-decoder Large Language Models (LLMs), such as BERT and RoBERTa,
require that all categories in an annotation task be sufficiently represented
in the training data for optimal performance. However, it is often difficult to
find sufficient examples for all categories in a task when building a
high-quality training set. In this article, I describe this problem and propose
a solution, the synthetic imputation approach. Leveraging a generative LLM
(GPT-4o), this approach generates synthetic texts based on careful prompting
and five original examples drawn randomly with replacement from the sample.
This approach ensures that new synthetic texts are sufficiently different from
the original texts to reduce overfitting, but retain the underlying substantive
meaning of the examples to maximize out-of-sample performance. With 75 original
examples or more, synthetic imputation's performance is on par with a full
sample of original texts, and overfitting remains low, predictable and
correctable with 50 original samples. The synthetic imputation approach
provides a novel role for generative LLMs in research and allows applied
researchers to balance their datasets for best performance.

</details>


### [60] [On true empty category](https://arxiv.org/abs/2504.15168)
*Qilin Tian*

Main category: cs.CL

TL;DR: 论文探讨了Chomsky的空语类理论，并评估了Li等人提出的“真实空语类”假设，认为无需引入该假设即可解释相关现象。


<details>
  <summary>Details</summary>
Motivation: 研究动机是验证Li等人提出的“真实空语类”假设是否必要，以及是否可以通过现有理论解释相关语言现象。

Method: 通过分析话题化的证据，评估“真实空语类”假设的合理性。

Result: 结果表明，无需引入“真实空语类”假设，现有理论足以解释话题化现象。

Conclusion: 结论认为“真实空语类”假设并非必要，现有空语类理论已足够解释相关语言现象。

Abstract: According to Chomsky (1981, 1986), empty categories consist of PRO, pro,
trace, and variable. However, some empty object positions seem to be
incompatible with extant empty categories. Given this, Li (2007a, 2007b, 2014)
and Li & Wei (2014) raise the true empty category hypothesis, which holds that
true empty category is only an empty position with category and Case features.
As a last resort option, it is used mainly to meet the subcatgorization of a
verb. This assumption is ingenious, and if proved to be true, it will exert a
great impact on the study of UG. In this paper, we evaluate their evidence from
topicalization and demonstrate that it can be accounted for without invoking
true empty category.

</details>


### [61] [Support Evaluation for the TREC 2024 RAG Track: Comparing Human versus LLM Judges](https://arxiv.org/abs/2504.15205)
*Nandan Thakur,Ronak Pradeep,Shivani Upadhyay,Daniel Campos,Nick Craswell,Jimmy Lin*

Main category: cs.CL

TL;DR: 本文研究了检索增强生成（RAG）中引用文档对答案的支持性评估，比较了GPT-4o与人类评委的表现，发现GPT-4o在支持性评估中表现可靠。


<details>
  <summary>Details</summary>
Motivation: 评估RAG系统中引用文档对答案的支持性，以验证自动LLM评委（GPT-4o）是否可以替代人类评委。

Method: 对TREC 2024 RAG Track的45份提交和36个主题进行大规模比较研究，比较GPT-4o与人类评委在支持性评估中的表现，包括完全手动评估和手动后编辑LLM预测两种条件。

Result: 56%的完全手动评估中，人类与GPT-4o评分完全一致，后编辑条件下提升至72%。独立人类评委与GPT-4o的相关性更高。

Conclusion: GPT-4o在支持性评估中表现可靠，可作为人类评委的替代方案。未来需进一步分析错误以改进评估方法。

Abstract: Retrieval-augmented generation (RAG) enables large language models (LLMs) to
generate answers with citations from source documents containing "ground
truth", thereby reducing system hallucinations. A crucial factor in RAG
evaluation is "support", whether the information in the cited documents
supports the answer. To this end, we conducted a large-scale comparative study
of 45 participant submissions on 36 topics to the TREC 2024 RAG Track,
comparing an automatic LLM judge (GPT-4o) against human judges for support
assessment. We considered two conditions: (1) fully manual assessments from
scratch and (2) manual assessments with post-editing of LLM predictions. Our
results indicate that for 56% of the manual from-scratch assessments, human and
GPT-4o predictions match perfectly (on a three-level scale), increasing to 72%
in the manual with post-editing condition. Furthermore, by carefully analyzing
the disagreements in an unbiased study, we found that an independent human
judge correlates better with GPT-4o than a human judge, suggesting that LLM
judges can be a reliable alternative for support assessment. To conclude, we
provide a qualitative analysis of human and GPT-4o errors to help guide future
iterations of support assessment.

</details>


### [62] [EvalAgent: Discovering Implicit Evaluation Criteria from the Web](https://arxiv.org/abs/2504.15219)
*Manya Wadhwa,Zayne Sprague,Chaitanya Malaviya,Philippe Laban,Junyi Jessy Li,Greg Durrett*

Main category: cs.CL

TL;DR: EvalAgent是一个新框架，用于自动发现任务特定的隐式评估标准，结合专家指导和LLM生成的标准，提升语言模型输出的质量。


<details>
  <summary>Details</summary>
Motivation: 传统评估方法仅关注基本任务要求，而高质量响应需满足更多隐式标准，如学术演讲的典型特征。

Method: EvalAgent通过挖掘专家在线指导，提出多样化的、基于外部来源的评估标准。

Result: 实验表明，EvalAgent生成的标准具有隐式性和特异性，且可通过优化响应满足。结合LLM标准能发现更多人类重视的标准。

Conclusion: EvalAgent为语言模型输出评估提供了更全面、任务特定的标准，提升了评估的深度和实用性。

Abstract: Evaluation of language model outputs on structured writing tasks is typically
conducted with a number of desirable criteria presented to human evaluators or
large language models (LLMs). For instance, on a prompt like "Help me draft an
academic talk on coffee intake vs research productivity", a model response may
be evaluated for criteria like accuracy and coherence. However, high-quality
responses should do more than just satisfy basic task requirements. An
effective response to this query should include quintessential features of an
academic talk, such as a compelling opening, clear research questions, and a
takeaway. To help identify these implicit criteria, we introduce EvalAgent, a
novel framework designed to automatically uncover nuanced and task-specific
criteria. EvalAgent first mines expert-authored online guidance. It then uses
this evidence to propose diverse, long-tail evaluation criteria that are
grounded in reliable external sources. Our experiments demonstrate that the
grounded criteria produced by EvalAgent are often implicit (not directly stated
in the user's prompt), yet specific (high degree of lexical precision).
Further, EvalAgent criteria are often not satisfied by initial responses but
they are actionable, such that responses can be refined to satisfy them.
Finally, we show that combining LLM-generated and EvalAgent criteria uncovers
more human-valued criteria than using LLMs alone.

</details>


### [63] [Fully Bayesian Approaches to Topics over Time](https://arxiv.org/abs/2504.15220)
*Julián Cendrero,Julio Gonzalo,Ivar Zapata*

Main category: cs.CL

TL;DR: 论文提出了完全贝叶斯的Topics over Time（BToT）模型，解决了原ToT模型的稳定性问题，并进一步提出加权版本WBToT以平衡时间和词模态的影响。实验表明WBToT在事件捕捉和主题一致性上优于其他模型。


<details>
  <summary>Details</summary>
Motivation: 原ToT模型未采用完全贝叶斯方法，导致稳定性问题，且时间与词模态的尺度差异未被解决。

Method: 引入Beta分布的共轭先验作为正则化，提出BToT；进一步提出WBToT，通过重复文档发布时间平衡模态影响。

Result: WBToT在SOTU和COVID-19推文数据集上表现优异，主题时间偏差分别降低51%和34%，且主题一致性更高。

Conclusion: WBToT解决了ToT的稳定性问题，平衡了时间和词模态，适用于大规模在线学习任务。

Abstract: The Topics over Time (ToT) model captures thematic changes in timestamped
datasets by explicitly modeling publication dates jointly with word
co-occurrence patterns. However, ToT was not approached in a fully Bayesian
fashion, a flaw that makes it susceptible to stability problems. To address
this issue, we propose a fully Bayesian Topics over Time (BToT) model via the
introduction of a conjugate prior to the Beta distribution. This prior acts as
a regularization that prevents the online version of the algorithm from
unstable updates when a topic is poorly represented in a mini-batch. The
characteristics of this prior to the Beta distribution are studied here for the
first time. Still, this model suffers from a difference in scale between the
single-time observations and the multiplicity of words per document. A
variation of BToT, Weighted Bayesian Topics over Time (WBToT), is proposed as a
solution. In WBToT, publication dates are repeated a certain number of times
per document, which balances the relative influence of words and timestamps
along the inference process. We have tested our models on two datasets: a
collection of over 200 years of US state-of-the-union (SOTU) addresses and a
large-scale COVID-19 Twitter corpus of 10 million tweets. The results show that
WBToT captures events better than Latent Dirichlet Allocation and other SOTA
topic models like BERTopic: the median absolute deviation of the topic presence
over time is reduced by $51\%$ and $34\%$, respectively. Our experiments also
demonstrate the superior coherence of WBToT over BToT, which highlights the
importance of balancing the time and word modalities. Finally, we illustrate
the stability of the online optimization algorithm in WBToT, which allows the
application of WBToT to problems that are intractable for standard ToT.

</details>


### [64] [Values in the Wild: Discovering and Analyzing Values in Real-World Language Model Interactions](https://arxiv.org/abs/2504.15236)
*Saffron Huang,Esin Durmus,Miles McCain,Kunal Handa,Alex Tamkin,Jerry Hong,Michael Stern,Arushi Somani,Xiuruo Zhang,Deep Ganguli*

Main category: cs.CL

TL;DR: 论文提出了一种隐私保护的方法，从Claude 3和3.5模型的真实交互中提取了3,307种AI价值观，并研究了其在不同情境下的变化。


<details>
  <summary>Details</summary>
Motivation: 研究AI助手在实践中依赖的价值观，填补了实证研究的空白。

Method: 采用自下而上的隐私保护方法，分析大量真实交互数据，提取并分类AI价值观。

Result: 发现Claude模型支持亲社会价值观，抵制如‘道德虚无主义’等价值观，且价值观随情境变化。

Conclusion: 研究为AI系统的价值观评估和设计提供了实证基础。

Abstract: AI assistants can impart value judgments that shape people's decisions and
worldviews, yet little is known empirically about what values these systems
rely on in practice. To address this, we develop a bottom-up,
privacy-preserving method to extract the values (normative considerations
stated or demonstrated in model responses) that Claude 3 and 3.5 models exhibit
in hundreds of thousands of real-world interactions. We empirically discover
and taxonomize 3,307 AI values and study how they vary by context. We find that
Claude expresses many practical and epistemic values, and typically supports
prosocial human values while resisting values like "moral nihilism". While some
values appear consistently across contexts (e.g. "transparency"), many are more
specialized and context-dependent, reflecting the diversity of human
interlocutors and their varied contexts. For example, "harm prevention" emerges
when Claude resists users, "historical accuracy" when responding to queries
about controversial events, "healthy boundaries" when asked for relationship
advice, and "human agency" in technology ethics discussions. By providing the
first large-scale empirical mapping of AI values in deployment, our work
creates a foundation for more grounded evaluation and design of values in AI
systems.

</details>


### [65] [MR. Guard: Multilingual Reasoning Guardrail using Curriculum Learning](https://arxiv.org/abs/2504.15241)
*Yahan Yang,Soham Dan,Shuo Li,Dan Roth,Insup Lee*

Main category: cs.CL

TL;DR: 提出了一种多语言防护栏方法，通过合成数据生成、监督微调和GRPO框架，显著提升多语言环境下LLMs的安全性。


<details>
  <summary>Details</summary>
Motivation: 多语言环境下LLMs易受对抗攻击，且安全对齐数据有限，需开发能检测和过滤多语言不安全内容的防护栏。

Method: 1. 合成多语言数据；2. 监督微调；3. 使用GRPO框架优化性能。

Result: 多语言防护栏在域内和域外语言中均优于基线，并能生成多语言解释。

Conclusion: 该方法有效提升多语言LLMs的安全性，特别适用于内容审核中的语言特定风险识别。

Abstract: Large Language Models (LLMs) are susceptible to adversarial attacks such as
jailbreaking, which can elicit harmful or unsafe behaviors. This vulnerability
is exacerbated in multilingual setting, where multilingual safety-aligned data
are often limited. Thus, developing a guardrail capable of detecting and
filtering unsafe content across diverse languages is critical for deploying
LLMs in real-world applications. In this work, we propose an approach to build
a multilingual guardrail with reasoning. Our method consists of: (1) synthetic
multilingual data generation incorporating culturally and linguistically
nuanced variants, (2) supervised fine-tuning, and (3) a curriculum-guided Group
Relative Policy Optimization (GRPO) framework that further improves
performance. Experimental results demonstrate that our multilingual guardrail
consistently outperforms recent baselines across both in-domain and
out-of-domain languages. The multilingual reasoning capability of our guardrail
enables it to generate multilingual explanations, which are particularly useful
for understanding language-specific risks and ambiguities in multilingual
content moderation.

</details>


### [66] [Evaluating Judges as Evaluators: The JETTS Benchmark of LLM-as-Judges as Test-Time Scaling Evaluators](https://arxiv.org/abs/2504.15253)
*Yilun Zhou,Austin Xu,Peifeng Wang,Caiming Xiong,Shafiq Joty*

Main category: cs.CL

TL;DR: JETTS基准测试评估了LLM-judges在测试时扩展设置中的表现，发现其在重新排序中表现良好，但在束搜索和基于批判的响应优化中不如过程奖励模型。


<details>
  <summary>Details</summary>
Motivation: 研究LLM-judges在测试时扩展设置中的有效性，填补其在自动评估中的未知领域。

Method: 引入JETTS基准，评估10种judge模型在数学推理、代码生成和指令遵循三个领域的表现，包括重新排序、束搜索和批判优化三种任务。

Result: judge在重新排序中与结果奖励模型竞争，但在束搜索中表现较差，且自然语言批判对响应优化的指导效果不佳。

Conclusion: LLM-judges在某些任务中表现良好，但在复杂任务中仍需改进，批判的指导作用有限。

Abstract: Scaling test-time computation, or affording a generator large language model
(LLM) extra compute during inference, typically employs the help of external
non-generative evaluators (i.e., reward models). Concurrently, LLM-judges,
models trained to generate evaluations and critiques (explanations) in natural
language, are becoming increasingly popular in automatic evaluation. Despite
judge empirical successes, their effectiveness as evaluators in test-time
scaling settings is largely unknown. In this paper, we introduce the Judge
Evaluation for Test-Time Scaling (JETTS) benchmark, which evaluates judge
performance in three domains (math reasoning, code generation, and instruction
following) under three task settings: response reranking, step-level beam
search, and critique-based response refinement. We evaluate 10 different judge
models (7B-70B parameters) for 8 different base generator models (6.7B-72B
parameters). Our benchmark shows that while judges are competitive with outcome
reward models in reranking, they are consistently worse than process reward
models in beam search procedures. Furthermore, though unique to LLM-judges,
their natural language critiques are currently ineffective in guiding the
generator towards better responses.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [67] [The Model Counting Competitions 2021-2023](https://arxiv.org/abs/2504.13842)
*Johannes K. Fichte,Markus Hecher*

Main category: cs.AI

TL;DR: 本文概述了2021-2023年模型计数竞赛的迭代情况，包括竞赛的执行、四个赛道的设置及其成果。


<details>
  <summary>Details</summary>
Motivation: 现代社会的许多计算问题涉及概率推理、统计和组合数学，这些问题可以通过命题公式编码为模型计数问题。竞赛旨在推动应用、识别挑战性基准、促进求解器开发。

Method: 竞赛设置了四个赛道，分别针对模型计数（MC）、加权模型计数（WMC）、投影模型计数（PMC）以及投影加权模型计数（PWMC）。

Result: 竞赛吸引了高水平参与，每个赛道有7至9个求解器提交，基于多种不同技术。

Conclusion: 竞赛成功推动了模型计数问题的研究与应用，并为未来工作提供了方向。

Abstract: Modern society is full of computational challenges that rely on probabilistic
reasoning, statistics, and combinatorics. Interestingly, many of these
questions can be formulated by encoding them into propositional formulas and
then asking for its number of models. With a growing interest in practical
problem-solving for tasks that involve model counting, the community
established the Model Counting (MC) Competition in fall of 2019 with its first
iteration in 2020. The competition aims at advancing applications, identifying
challenging benchmarks, fostering new solver development, and enhancing
existing solvers for model counting problems and their variants. The first
iteration, brought together various researchers, identified challenges, and
inspired numerous new applications. In this paper, we present a comprehensive
overview of the 2021-2023 iterations of the Model Counting Competition. We
detail its execution and outcomes. The competition comprised four tracks, each
focusing on a different variant of the model counting problem. The first track
centered on the model counting problem (MC), which seeks the count of models
for a given propositional formula. The second track challenged developers to
submit programs capable of solving the weighted model counting problem (WMC).
The third track was dedicated to projected model counting (PMC). Finally, we
initiated a track that combined projected and weighted model counting (PWMC).
The competition continued with a high level of participation, with seven to
nine solvers submitted in various different version and based on quite
diverging techniques.

</details>


### [68] [Evaluation and Incident Prevention in an Enterprise AI Assistant](https://arxiv.org/abs/2504.13924)
*Akash V. Maharaj,David Arbour,Daniel Lee,Uttaran Bhattacharya,Anup Rao,Austin Zane,Avi Feller,Kun Qian,Yunyao Li*

Main category: cs.AI

TL;DR: 本文提出了一种全面的框架，用于监控、基准测试和持续改进企业AI助手，确保其在关键领域的准确性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 企业AI助手在准确性至关重要的领域部署，错误输出可能导致重大事故，因此需要系统化的改进方法。

Method: 框架包含三个关键部分：分层严重性框架、可扩展的基准测试方法以及多维评估的持续改进策略。

Result: 通过该框架，组织可以系统性提升AI助手的可靠性和性能。

Conclusion: 这种多方面的评估方法为AI系统的稳健性和可信度提供了改进途径。

Abstract: Enterprise AI Assistants are increasingly deployed in domains where accuracy
is paramount, making each erroneous output a potentially significant incident.
This paper presents a comprehensive framework for monitoring, benchmarking, and
continuously improving such complex, multi-component systems under active
development by multiple teams. Our approach encompasses three key elements: (1)
a hierarchical ``severity'' framework for incident detection that identifies
and categorizes errors while attributing component-specific error rates,
facilitating targeted improvements; (2) a scalable and principled methodology
for benchmark construction, evaluation, and deployment, designed to accommodate
multiple development teams, mitigate overfitting risks, and assess the
downstream impact of system modifications; and (3) a continual improvement
strategy leveraging multidimensional evaluation, enabling the identification
and implementation of diverse enhancement opportunities. By adopting this
holistic framework, organizations can systematically enhance the reliability
and performance of their AI Assistants, ensuring their efficacy in critical
enterprise environments. We conclude by discussing how this multifaceted
evaluation approach opens avenues for various classes of enhancements, paving
the way for more robust and trustworthy AI systems.

</details>


### [69] [Birds of a Different Feather Flock Together: Exploring Opportunities and Challenges in Animal-Human-Machine Teaming](https://arxiv.org/abs/2504.13973)
*Myke C. Cohen,David A. Grimm,Reuth Mirsky,Xiaoyun Yin*

Main category: cs.AI

TL;DR: 本文呼吁系统研究动物-人类-机器（AHM）团队结构设计，以优化性能并克服应用中的限制。


<details>
  <summary>Details</summary>
Motivation: 探索AHM团队中人类、AI机器和动物成员协同作用的潜力，以发挥各自优势并弥补不足。

Method: 通过引入AHM团队功能的多个维度，结合三个代表性案例（安检、搜救和导盲犬）进行分析。

Result: 展示了AHM团队如何通过协同作用解决复杂任务。

Conclusion: 提出了多维方法在研究更广泛的人机混合系统中的应用方向。

Abstract: Animal-Human-Machine (AHM) teams are a type of hybrid intelligence system
wherein interactions between a human, AI-enabled machine, and animal members
can result in unique capabilities greater than the sum of their parts. This
paper calls for a systematic approach to studying the design of AHM team
structures to optimize performance and overcome limitations in various applied
settings. We consider the challenges and opportunities in investigating the
synergistic potential of AHM team members by introducing a set of dimensions of
AHM team functioning to effectively utilize each member's strengths while
compensating for individual weaknesses. Using three representative examples of
such teams -- security screening, search-and-rescue, and guide dogs -- the
paper illustrates how AHM teams can tackle complex tasks. We conclude with open
research directions that this multidimensional approach presents for studying
hybrid human-AI systems beyond AHM teams.

</details>


### [70] [Going Whole Hog: A Philosophical Defense of AI Cognition](https://arxiv.org/abs/2504.13988)
*Herman Cappelen,Josh Dever*

Main category: cs.AI

TL;DR: 论文主张大型语言模型（如ChatGPT）是完整的语言和认知主体，具备理解、信念、欲望、知识和意图。作者反对基于低层次计算细节或现有心智理论的哲学方法，提倡从简单的高层次行为观察出发，并通过‘整体网络假设’论证其认知状态。


<details>
  <summary>Details</summary>
Motivation: 反驳当前AI哲学中低估LLM认知能力的观点，提出应从行为证据出发，论证LLM的完整认知主体地位。

Method: 通过观察LLM的高层次行为（如回答问题、提出建议），结合‘整体网络假设’（如回答隐含知识，知识隐含信念），系统论证其认知能力，并反驳常见质疑。

Result: 论证LLM具备完整的认知状态，其失败（如幻觉、推理错误）不否定其主体性，且某些‘必要条件’（如语义基础、具身性）并非真正必要。

Conclusion: LLM可能是具备‘异质’内容的认知主体，其认知能力超越传统人类概念框架。

Abstract: This work defends the 'Whole Hog Thesis': sophisticated Large Language Models
(LLMs) like ChatGPT are full-blown linguistic and cognitive agents, possessing
understanding, beliefs, desires, knowledge, and intentions. We argue against
prevailing methodologies in AI philosophy, rejecting starting points based on
low-level computational details ('Just an X' fallacy) or pre-existing theories
of mind. Instead, we advocate starting with simple, high-level observations of
LLM behavior (e.g., answering questions, making suggestions) -- defending this
data against charges of metaphor, loose talk, or pretense. From these
observations, we employ 'Holistic Network Assumptions' -- plausible connections
between mental capacities (e.g., answering implies knowledge, knowledge implies
belief, action implies intention) -- to argue for the full suite of cognitive
states. We systematically rebut objections based on LLM failures
(hallucinations, planning/reasoning errors), arguing these don't preclude
agency, often mirroring human fallibility. We address numerous 'Games of
Lacks', arguing that LLMs do not lack purported necessary conditions for
cognition (e.g., semantic grounding, embodiment, justification, intrinsic
intentionality) or that these conditions are not truly necessary, often relying
on anti-discriminatory arguments comparing LLMs to diverse human capacities.
Our approach is evidential, not functionalist, and deliberately excludes
consciousness. We conclude by speculating on the possibility of LLMs possessing
'alien' contents beyond human conceptual schemes.

</details>


### [71] [Multi-Stage Retrieval for Operational Technology Cybersecurity Compliance Using Large Language Models: A Railway Casestudy](https://arxiv.org/abs/2504.14044)
*Regan Bolton,Mohammadreza Sheikhfathollahi,Simon Parkinson,Dan Basher,Howard Parkinson*

Main category: cs.AI

TL;DR: 论文提出了一种基于大语言模型（LLM）和多阶段检索的新系统，用于提升铁路网络安全中的合规性验证，显著提高了正确性和推理质量。


<details>
  <summary>Details</summary>
Motivation: 随着铁路等关键基础设施数字化，其面临的安全威胁增加，需要高效合规验证方法以保护系统安全。

Method: 提出并行合规架构（PCA），结合LLM和多阶段检索，对比基线架构（BCA）进行实证评估。

Result: PCA显著提升了合规验证的正确性和推理质量，并建立了评估指标。

Conclusion: 检索增强方法可显著提高合规评估效率，尤其在网络安全人才短缺的行业中具有重要价值。

Abstract: Operational Technology Cybersecurity (OTCS) continues to be a dominant
challenge for critical infrastructure such as railways. As these systems become
increasingly vulnerable to malicious attacks due to digitalization, effective
documentation and compliance processes are essential to protect these
safety-critical systems. This paper proposes a novel system that leverages
Large Language Models (LLMs) and multi-stage retrieval to enhance the
compliance verification process against standards like IEC 62443 and the
rail-specific IEC 63452. We first evaluate a Baseline Compliance Architecture
(BCA) for answering OTCS compliance queries, then develop an extended approach
called Parallel Compliance Architecture (PCA) that incorporates additional
context from regulatory standards. Through empirical evaluation comparing
OpenAI-gpt-4o and Claude-3.5-haiku models in these architectures, we
demonstrate that the PCA significantly improves both correctness and reasoning
quality in compliance verification. Our research establishes metrics for
response correctness, logical reasoning, and hallucination detection,
highlighting the strengths and limitations of using LLMs for compliance
verification in railway cybersecurity. The results suggest that
retrieval-augmented approaches can significantly improve the efficiency and
accuracy of compliance assessments, particularly valuable in an industry facing
a shortage of cybersecurity expertise.

</details>


### [72] [Metacognition and Uncertainty Communication in Humans and Large Language Models](https://arxiv.org/abs/2504.14045)
*Mark Steyvers,Megan A. K. Peters*

Main category: cs.AI

TL;DR: 论文探讨了大语言模型（LLMs）的元认知能力，比较了其与人类元认知的异同，并强调了研究这些能力对提升人机协作和AI系统可信度的重要性。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在高风险决策中的广泛应用，评估其是否具备元认知能力变得至关重要。

Method: 综述了当前对LLMs元认知能力的认识，探讨了研究方法，并与人类元认知进行了对比。

Result: 发现LLMs与人类在某些元认知行为上表现相似，但仍存在显著差异。

Conclusion: 未来可通过增强LLMs的元认知能力，促进其学习效率、自我导向和好奇心等新能力的发展。

Abstract: Metacognition, the capacity to monitor and evaluate one's own knowledge and
performance, is foundational to human decision-making, learning, and
communication. As large language models (LLMs) become increasingly embedded in
high-stakes decision contexts, it is critical to assess whether, how, and to
what extent they exhibit metacognitive abilities. Here, we provide an overview
of current knowledge of LLMs' metacognitive capacities, how they might be
studied, and how they relate to our knowledge of metacognition in humans. We
show that while humans and LLMs can sometimes appear quite aligned in their
metacognitive capacities and behaviors, it is clear many differences remain.
Attending to these differences is crucial not only for enhancing human-AI
collaboration, but also for promoting the development of more capable and
trustworthy artificial systems. Finally, we discuss how endowing future LLMs
with more sensitive and more calibrated metacognition may also help them
develop new capacities such as more efficient learning, self-direction, and
curiosity.

</details>


### [73] [Think Deep, Think Fast: Investigating Efficiency of Verifier-free Inference-time-scaling Methods](https://arxiv.org/abs/2504.14047)
*Junlin Wang,Shang Zhu,Jon Saad-Falcon,Ben Athiwaratkun,Qingyang Wu,Jue Wang,Shuaiwen Leon Song,Ce Zhang,Bhuwan Dhingra,James Zou*

Main category: cs.AI

TL;DR: 该论文研究了推理时间计算（ITC）如何提升大语言模型（LLM）能力，并分析了推理与非推理模型在推理任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 探索ITC与推理模型的交互作用，以指导LLM技术的进一步发展。

Method: 采用无验证器的推理时间扩展方法，构建质量与效率的帕累托前沿，并分析响应特征与质量的关系。

Result: 非推理模型即使在高推理预算下仍显著落后于推理模型；多数投票是推理模型的稳健策略。

Conclusion: 推理模型的正确响应通常更短且语言标记更少，ITC方法可通过响应特征优化。

Abstract: There is intense interest in investigating how inference time compute (ITC)
(e.g. repeated sampling, refinements, etc) can improve large language model
(LLM) capabilities. At the same time, recent breakthroughs in reasoning models,
such as Deepseek-R1, unlock the opportunity for reinforcement learning to
improve LLM reasoning skills. An in-depth understanding of how ITC interacts
with reasoning across different models could provide important guidance on how
to further advance the LLM frontier. This work conducts a comprehensive
analysis of inference-time scaling methods for both reasoning and non-reasoning
models on challenging reasoning tasks. Specifically, we focus our research on
verifier-free inference time-scaling methods due to its generalizability
without needing a reward model. We construct the Pareto frontier of quality and
efficiency. We find that non-reasoning models, even with an extremely high
inference budget, still fall substantially behind reasoning models. For
reasoning models, majority voting proves to be a robust inference strategy,
generally competitive or outperforming other more sophisticated ITC methods
like best-of-N and sequential revisions, while the additional inference compute
offers minimal improvements. We further perform in-depth analyses of the
association of key response features (length and linguistic markers) with
response quality, with which we can improve the existing ITC methods. We find
that correct responses from reasoning models are typically shorter and have
fewer hedging and thinking markers (but more discourse markers) than the
incorrect responses.

</details>


### [74] [Linking forward-pass dynamics in Transformers and real-time human processing](https://arxiv.org/abs/2504.14107)
*Jennifer Hu,Michael A. Lepori,Michael Franke*

Main category: cs.AI

TL;DR: 研究探讨了Transformer模型的内部处理动态是否与人类实时处理相似，发现层时动态能提供额外预测力。


<details>
  <summary>Details</summary>
Motivation: 探索AI模型与人类认知处理的相似性，超越传统黑箱方法。

Method: 通过五个跨领域研究，分析Transformer单次前向传播的层时动态与人类处理特征的关联。

Result: 层时动态在模型输出基础上提供了额外预测力，表明两者处理策略相似。

Conclusion: AI模型可作为显式处理模型研究人类认知，而不仅是黑箱工具。

Abstract: Modern AI models are increasingly being used as theoretical tools to study
human cognition. One dominant approach is to evaluate whether human-derived
measures (such as offline judgments or real-time processing) are predicted by a
model's output: that is, the end-product of forward pass(es) through the
network. At the same time, recent advances in mechanistic interpretability have
begun to reveal the internal processes that give rise to model outputs, raising
the question of whether models and humans might arrive at outputs using similar
"processing strategies". Here, we investigate the link between real-time
processing in humans and "layer-time" dynamics in Transformer models. Across
five studies spanning domains and modalities, we test whether the dynamics of
computation in a single forward pass of pre-trained Transformers predict
signatures of processing in humans, above and beyond properties of the model's
output probability distribution. We consistently find that layer-time dynamics
provide additional predictive power on top of output measures. Our results
suggest that Transformer processing and human processing may be facilitated or
impeded by similar properties of an input stimulus, and this similarity has
emerged through general-purpose objectives such as next-token prediction or
image recognition. Our work suggests a new way of using AI models to study
human cognition: not just as a black box mapping stimuli to responses, but
potentially also as explicit processing models.

</details>


### [75] [CODECRASH: Stress Testing LLM Reasoning under Structural and Semantic Perturbations](https://arxiv.org/abs/2504.14119)
*Man Ho Lam,Chaozheng Wang,Jen-tse Huang,Michael R. Lyu*

Main category: cs.AI

TL;DR: CodeCrash是一个评估大型语言模型（LLMs）在代码理解和推理任务中鲁棒性的统一基准，通过结构性和文本性干扰测试其表现，并揭示其脆弱性和失败模式。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在代码相关任务中表现出色，但其在代码理解和推理中的鲁棒性尚未充分研究。

Method: 使用CodeCrash基准，结合CRUXEval和LiveCodeBench，对17种LLMs进行直接和链式推理评估，分析其鲁棒性和失败原因。

Result: 发现LLMs在结构性噪声下表现脆弱，且过度依赖自然语言提示；同时发现大型推理模型（LRMs）的自反推理机制存在严重漏洞。

Conclusion: CodeCrash为测试LLMs在代码理解中的鲁棒性提供了框架，并指出了未来评估和基准测试的方向。

Abstract: Large Language Models (LLMs) have recently showcased strong capabilities in
code-related tasks, yet their robustness in code comprehension and reasoning
remains underexplored. In this paper, we present CodeCrash, a unified benchmark
that evaluates LLM robustness under code structural and textual distraction
perturbations, applied to two established benchmarks -- CRUXEval and
LiveCodeBench -- across both input and output prediction tasks. We evaluate
seventeen LLMs using direct and Chain-of-Thought inference to systematically
analyze their robustness, identify primary reasons for performance degradation,
and highlight failure modes. Our findings reveal the fragility of LLMs under
structural noise and the inherent reliance on natural language cues,
highlighting critical robustness issues of LLMs in code execution and
understanding. Additionally, we examine three Large Reasoning Models (LRMs) and
discover the severe vulnerability of self-reflective reasoning mechanisms that
lead to reasoning collapse. CodeCrash provides a principled framework for
stress-testing LLMs in code understanding, offering actionable directions for
future evaluation and benchmarking. The code of CodeCrash and the robustness
leaderboard are publicly available at https://donaldlamnl.github.io/CodeCrash/ .

</details>


### [76] [Bayesian Principles Improve Prompt Learning In Vision-Language Models](https://arxiv.org/abs/2504.14123)
*Mingyu Kim,Jongwoo Ko,Mijung Park*

Main category: cs.AI

TL;DR: 提出了一种基于贝叶斯学习原理的新训练目标函数，以平衡适应性和泛化性，解决提示学习中的过拟合问题。


<details>
  <summary>Details</summary>
Motivation: 现有提示学习方法在微调数据上容易过拟合，泛化能力差。

Method: 通过贝叶斯学习原理，设计了一个新的目标函数，将预训练模型参数化为先验，微调模型对应后验。

Result: 新方法在保持对下游任务适应性的同时，避免了过拟合，提升了泛化能力。

Conclusion: 基于贝叶斯的目标函数有效平衡了适应性和泛化性，为提示学习提供了新思路。

Abstract: Prompt learning is a popular fine-tuning method for vision-language models
due to its efficiency. It requires a small number of additional learnable
parameters while significantly enhancing performance on target tasks. However,
most existing methods suffer from overfitting to fine-tuning data, yielding
poor generalizability. To address this, we propose a new training objective
function based on a Bayesian learning principle to balance adaptability and
generalizability. We derive a prior over the logits, where the mean function is
parameterized by the pre-trained model, while the posterior corresponds to the
fine-tuned model. This objective establishes a balance by allowing the
fine-tuned model to adapt to downstream tasks while remaining close to the
pre-trained model.

</details>


### [77] [Large Language Model Enhanced Particle Swarm Optimization for Hyperparameter Tuning for Deep Learning Models](https://arxiv.org/abs/2504.14126)
*Saad Hameed,Basheer Qolomany,Samir Brahim Belhaouari,Mohamed Abdallah,Junaid Qadir,Ala Al-Fuqaha*

Main category: cs.AI

TL;DR: 该论文提出了一种结合大语言模型（LLMs）和粒子群优化（PSO）的新方法，用于深度学习超参数调优，显著提高了收敛速度并降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型的架构设计通常依赖人工调优或计算密集型优化方法，效率低下。虽然PSO和LLMs在各自领域已有应用，但二者结合用于数值优化任务的研究较少。

Method: 通过将LLMs（如ChatGPT-3.5和Llama3）集成到PSO中，用LLM的建议替换表现不佳的粒子位置，从而加速搜索空间探索。

Result: 实验表明，该方法在三个任务中显著提升了收敛速度，计算复杂度降低了20%至60%，同时保持了准确性和误差率。

Conclusion: 该方法为深度学习模型优化提供了一种高效且有效的解决方案，具有广泛的应用潜力。

Abstract: Determining the ideal architecture for deep learning models, such as the
number of layers and neurons, is a difficult and resource-intensive process
that frequently relies on human tuning or computationally costly optimization
approaches. While Particle Swarm Optimization (PSO) and Large Language Models
(LLMs) have been individually applied in optimization and deep learning, their
combined use for enhancing convergence in numerical optimization tasks remains
underexplored. Our work addresses this gap by integrating LLMs into PSO to
reduce model evaluations and improve convergence for deep learning
hyperparameter tuning. The proposed LLM-enhanced PSO method addresses the
difficulties of efficiency and convergence by using LLMs (particularly
ChatGPT-3.5 and Llama3) to improve PSO performance, allowing for faster
achievement of target objectives. Our method speeds up search space exploration
by substituting underperforming particle placements with best suggestions
offered by LLMs. Comprehensive experiments across three scenarios -- (1)
optimizing the Rastrigin function, (2) using Long Short-Term Memory (LSTM)
networks for time series regression, and (3) using Convolutional Neural
Networks (CNNs) for material classification -- show that the method
significantly improves convergence rates and lowers computational costs.
Depending on the application, computational complexity is lowered by 20% to 60%
compared to traditional PSO methods. Llama3 achieved a 20% to 40% reduction in
model calls for regression tasks, whereas ChatGPT-3.5 reduced model calls by
60% for both regression and classification tasks, all while preserving accuracy
and error rates. This groundbreaking methodology offers a very efficient and
effective solution for optimizing deep learning models, leading to substantial
computational performance improvements across a wide range of applications.

</details>


### [78] [TALES: Text Adventure Learning Environment Suite](https://arxiv.org/abs/2504.14128)
*Christopher Zhang Cui,Xingdi Yuan,Zhang Xiao,Prithviraj Ammanabrolu,Marc-Alexandre Côté*

Main category: cs.AI

TL;DR: TALES是一个多样化的文本冒险游戏集合，用于挑战和评估大型语言模型的推理能力，结果显示即使表现最好的模型在人类设计的游戏中表现不佳。


<details>
  <summary>Details</summary>
Motivation: 随着任务复杂性增加，需要更复杂的推理能力，因此设计了TALES来评估大型语言模型的多样化推理能力。

Method: 使用合成和人类编写的文本冒险游戏（TALES）测试多种大型语言模型，包括开源和闭源模型。

Result: 表现最好的模型在合成游戏中表现良好，但在人类设计的游戏中成功率低于15%。

Conclusion: 大型语言模型在复杂推理任务中仍有改进空间，尤其是在人类设计的场景中。

Abstract: Reasoning is an essential skill to enable Large Language Models (LLMs) to
interact with the world. As tasks become more complex, they demand increasingly
sophisticated and diverse reasoning capabilities for sequential
decision-making, requiring structured reasoning over the context history to
determine the next best action. We introduce TALES, a diverse collection of
synthetic and human-written text-adventure games designed to challenge and
evaluate diverse reasoning capabilities. We present results over a range of
LLMs, open- and closed-weights, performing a qualitative analysis on the top
performing models. Despite an impressive showing on synthetic games, even the
top LLM-driven agents fail to achieve 15% on games designed for human
enjoyment. Code and visualization of the experiments can be found at
https://microsoft.github.io/tales.

</details>


### [79] [Adaptation Method for Misinformation Identification](https://arxiv.org/abs/2504.14171)
*Yangping Chen,Weijie Shi,Mengze Li,Yue Cui,Hao Chen,Jia Zhu,Jiajie Xu*

Main category: cs.AI

TL;DR: ADOSE是一个主动领域自适应框架，用于多模态假新闻检测，通过标注少量目标样本提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决多模态假新闻检测中领域偏移导致的性能下降问题。

Method: 设计多专家分类器捕捉跨模态依赖关系，提出最小分歧不确定性选择器减少标注成本。

Result: 在多个数据集上优于现有方法2.72%至14.02%。

Conclusion: ADOSE在跨领域假新闻检测中表现出色，有效提升性能。

Abstract: Multimodal fake news detection plays a crucial role in combating online
misinformation. Unfortunately, effective detection methods rely on annotated
labels and encounter significant performance degradation when domain shifts
exist between training (source) and test (target) data. To address the
problems, we propose ADOSE, an Active Domain Adaptation (ADA) framework for
multimodal fake news detection which actively annotates a small subset of
target samples to improve detection performance. To identify various deceptive
patterns in cross-domain settings, we design multiple expert classifiers to
learn dependencies across different modalities. These classifiers specifically
target the distinct deception patterns exhibited in fake news, where two
unimodal classifiers capture knowledge errors within individual modalities
while one cross-modal classifier identifies semantic inconsistencies between
text and images. To reduce annotation costs from the target domain, we propose
a least-disagree uncertainty selector with a diversity calculator for selecting
the most informative samples. The selector leverages prediction disagreement
before and after perturbations by multiple classifiers as an indicator of
uncertain samples, whose deceptive patterns deviate most from source domains.
It further incorporates diversity scores derived from multi-view features to
ensure the chosen samples achieve maximal coverage of target domain features.
The extensive experiments on multiple datasets show that ADOSE outperforms
existing ADA methods by 2.72\% $\sim$ 14.02\%, indicating the superiority of
our model.

</details>


### [80] [Direct Advantage Regression: Aligning LLMs with Online AI Reward](https://arxiv.org/abs/2504.14177)
*Li He,He Zhao,Stephen Wan,Dadong Wang,Lina Yao,Tongliang Liu*

Main category: cs.AI

TL;DR: 论文提出了一种名为DAR的简单对齐算法，利用在线AI奖励优化策略改进，避免了传统RLHF的复杂性，并在实验中表现优于OAIF和RLHF基线。


<details>
  <summary>Details</summary>
Motivation: 在线AI反馈（OAIF）虽然替代了人类反馈（RLHF），但缺乏细粒度的监督信号。本文旨在通过DAR算法解决这一问题。

Method: 提出Direct Advantage Regression（DAR），一种基于在线AI奖励的加权监督微调方法，无需强化学习。

Result: 实验表明，DAR在人类-AI一致性和性能上优于OAIF和RLHF基线，GPT-4-Turbo和MT-bench评估结果支持这一结论。

Conclusion: DAR是一种高效且简单的对齐算法，优于现有方法，为AI监督提供了更好的形式。

Abstract: Online AI Feedback (OAIF) presents a promising alternative to Reinforcement
Learning from Human Feedback (RLHF) by utilizing online AI preference in
aligning language models (LLMs). However, the straightforward replacement of
humans with AI deprives LLMs from learning more fine-grained AI supervision
beyond binary signals. In this paper, we propose Direct Advantage Regression
(DAR), a simple alignment algorithm using online AI reward to optimize policy
improvement through weighted supervised fine-tuning. As an RL-free approach,
DAR maintains theoretical consistency with online RLHF pipelines while
significantly reducing implementation complexity and improving learning
efficiency. Our empirical results underscore that AI reward is a better form of
AI supervision consistently achieving higher human-AI agreement as opposed to
AI preference. Additionally, evaluations using GPT-4-Turbo and MT-bench show
that DAR outperforms both OAIF and online RLHF baselines.

</details>


### [81] [AI Idea Bench 2025: AI Research Idea Generation Benchmark](https://arxiv.org/abs/2504.14191)
*Yansheng Qiu,Haoquan Zhang,Zhaopan Xu,Ming Li,Diping Song,Zheng Wang,Kaipeng Zhang*

Main category: cs.AI

TL;DR: AI Idea Bench 2025是一个评估LLMs生成AI研究想法的框架，解决了现有评估方法的不足，如知识泄漏和缺乏开放式基准。


<details>
  <summary>Details</summary>
Motivation: 当前对LLMs生成想法的评估忽略了知识泄漏、开放式基准和可行性分析等关键因素，限制了突破性研究发现的潜力。

Method: 提出了一个包含3,495篇AI论文及其启发工作的数据集，以及一个从原创论文内容和通用参考材料两个维度评估想法质量的系统。

Result: AI Idea Bench 2025为评估和比较想法生成技术提供了宝贵资源，推动了科学发现的自动化。

Conclusion: 该框架填补了现有评估方法的空白，有望促进LLMs在科学创新中的应用。

Abstract: Large-scale Language Models (LLMs) have revolutionized human-AI interaction
and achieved significant success in the generation of novel ideas. However,
current assessments of idea generation overlook crucial factors such as
knowledge leakage in LLMs, the absence of open-ended benchmarks with grounded
truth, and the limited scope of feasibility analysis constrained by prompt
design. These limitations hinder the potential of uncovering groundbreaking
research ideas. In this paper, we present AI Idea Bench 2025, a framework
designed to quantitatively evaluate and compare the ideas generated by LLMs
within the domain of AI research from diverse perspectives. The framework
comprises a comprehensive dataset of 3,495 AI papers and their associated
inspired works, along with a robust evaluation methodology. This evaluation
system gauges idea quality in two dimensions: alignment with the ground-truth
content of the original papers and judgment based on general reference
material. AI Idea Bench 2025's benchmarking system stands to be an invaluable
resource for assessing and comparing idea-generation techniques, thereby
facilitating the automation of scientific discovery.

</details>


### [82] [Pets: General Pattern Assisted Architecture For Time Series Analysis](https://arxiv.org/abs/2504.14209)
*Xiangkai Ma,Xiaobin Hong,Wenzhong Li,Sanglu Lu*

Main category: cs.AI

TL;DR: 论文提出了一种基于能量分布的新方法Pets，通过自适应量化时间序列的频率带区间，有效解耦多周期波动模式，并结合FPA和MoP模块，实现了在预测、填补、异常检测和分类任务中的最优性能。


<details>
  <summary>Details</summary>
Motivation: 现实世界的时间序列数据通常包含多种波动模式（如小时、日、月频率），传统分解方法难以有效解耦这些模式，导致分析困难。

Method: 提出基于时间-频谱空间能量分布的新视角，自适应量化序列为连续频率带区间，并结合FPA模块（捕获依赖关系）和MoP模块（分层重建波动模式）。

Result: Pets在预测、填补、异常检测和分类任务中表现最优，并展示了强泛化能力和鲁棒性。

Conclusion: Pets通过创新方法有效解耦多周期波动模式，为时间序列分析提供了更灵活和高效的解决方案。

Abstract: Time series analysis has found widespread applications in areas such as
weather forecasting, anomaly detection, and healthcare. However, real-world
sequential data often exhibit a superimposed state of various fluctuation
patterns, including hourly, daily, and monthly frequencies. Traditional
decomposition techniques struggle to effectively disentangle these multiple
fluctuation patterns from the seasonal components, making time series analysis
challenging. Surpassing the existing multi-period decoupling paradigms, this
paper introduces a novel perspective based on energy distribution within the
temporal-spectrum space. By adaptively quantifying observed sequences into
continuous frequency band intervals, the proposed approach reconstructs
fluctuation patterns across diverse periods without relying on domain-specific
prior knowledge. Building upon this innovative strategy, we propose Pets, an
enhanced architecture that is adaptable to arbitrary model structures. Pets
integrates a Fluctuation Pattern Assisted (FPA) module and a Context-Guided
Mixture of Predictors (MoP). The FPA module facilitates information fusion
among diverse fluctuation patterns by capturing their dependencies and
progressively modeling these patterns as latent representations at each layer.
Meanwhile, the MoP module leverages these compound pattern representations to
guide and regulate the reconstruction of distinct fluctuations hierarchically.
Pets achieves state-of-the-art performance across various tasks, including
forecasting, imputation, anomaly detection, and classification, while
demonstrating strong generalization and robustness.

</details>


### [83] [Assessing AI-Generated Questions' Alignment with Cognitive Frameworks in Educational Assessment](https://arxiv.org/abs/2504.14232)
*Antoun Yaacoub,Jérôme Da-Rugna,Zainab Assaghir*

Main category: cs.AI

TL;DR: 研究评估了将布鲁姆分类法集成到AI驱动的Moodle插件OneClickQuiz中，以改进多选题生成与认知目标的匹配。结果显示，高级认知层次的问题更复杂，DistilBERT模型表现最佳。


<details>
  <summary>Details</summary>
Motivation: 探讨布鲁姆分类法是否能提升AI生成问题与特定认知目标的匹配度。

Method: 使用3691个按布鲁姆层次分类的问题数据集，测试多种分类模型（如逻辑回归、朴素贝叶斯、线性SVC和DistilBERT）。

Result: 高级层次问题更复杂，DistilBERT表现最佳（验证准确率91%）。

Conclusion: 布鲁姆分类法集成AI工具有潜力，高级模型如DistilBERT能显著提升教育内容生成质量。

Abstract: This study evaluates the integration of Bloom's Taxonomy into OneClickQuiz,
an Artificial Intelligence (AI) driven plugin for automating Multiple-Choice
Question (MCQ) generation in Moodle. Bloom's Taxonomy provides a structured
framework for categorizing educational objectives into hierarchical cognitive
levels. Our research investigates whether incorporating this taxonomy can
improve the alignment of AI-generated questions with specific cognitive
objectives. We developed a dataset of 3691 questions categorized according to
Bloom's levels and employed various classification models-Multinomial Logistic
Regression, Naive Bayes, Linear Support Vector Classification (SVC), and a
Transformer-based model (DistilBERT)-to evaluate their effectiveness in
categorizing questions. Our results indicate that higher Bloom's levels
generally correlate with increased question length, Flesch-Kincaid Grade Level
(FKGL), and Lexical Density (LD), reflecting the increased complexity of higher
cognitive demands. Multinomial Logistic Regression showed varying accuracy
across Bloom's levels, performing best for "Knowledge" and less accurately for
higher-order levels. Merging higher-level categories improved accuracy for
complex cognitive tasks. Naive Bayes and Linear SVC also demonstrated effective
classification for lower levels but struggled with higher-order tasks.
DistilBERT achieved the highest performance, significantly improving
classification of both lower and higher-order cognitive levels, achieving an
overall validation accuracy of 91%. This study highlights the potential of
integrating Bloom's Taxonomy into AI-driven assessment tools and underscores
the advantages of advanced models like DistilBERT for enhancing educational
content generation.

</details>


### [84] [InfiGUI-R1: Advancing Multimodal GUI Agents from Reactive Actors to Deliberative Reasoners](https://arxiv.org/abs/2504.14239)
*Yuhang Liu,Pengxiang Li,Congkai Xie,Xavier Hu,Xiaotian Han,Shengyu Zhang,Hongxia Yang,Fei Wu*

Main category: cs.AI

TL;DR: InfiGUI-R1是一个基于MLLM的GUI代理，通过两阶段训练框架Actor2Reasoner，从反应式执行者转变为深思熟虑的推理者，提升了GUI任务的鲁棒性和适应性。


<details>
  <summary>Details</summary>
Motivation: 当前GUI代理依赖手动设计的推理模板或隐式推理，缺乏对复杂GUI环境的适应性和深度规划能力。

Method: 采用两阶段训练：1. 推理注入（Spatial Reasoning Distillation）；2. 深思熟虑增强（强化学习，包括子目标引导和错误恢复场景构建）。

Result: 实验表明InfiGUI-R1在GUI基础和轨迹任务中表现优异。

Conclusion: 通过Actor2Reasoner框架，实现了GUI代理从反应式到深思熟虑推理的转变，提升了任务性能。

Abstract: Multimodal Large Language Models (MLLMs) have powered Graphical User
Interface (GUI) Agents, showing promise in automating tasks on computing
devices. Recent works have begun exploring reasoning in GUI tasks with
encouraging results. However, many current approaches rely on manually designed
reasoning templates, which may result in reasoning that is not sufficiently
robust and adaptive for complex GUI environments. Meanwhile, some existing
agents continue to operate as Reactive Actors, relying primarily on implicit
reasoning that may lack sufficient depth for GUI tasks demanding planning and
error recovery. We argue that advancing these agents requires a shift from
reactive acting towards acting based on deliberate reasoning. To facilitate
this transformation, we introduce InfiGUI-R1, an MLLM-based GUI agent developed
through our Actor2Reasoner framework, a reasoning-centric, two-stage training
approach designed to progressively evolve agents from Reactive Actors to
Deliberative Reasoners. The first stage, Reasoning Injection, focuses on
establishing a basic reasoner. We employ Spatial Reasoning Distillation to
transfer cross-modal spatial reasoning capabilities from teacher models to
MLLMs through trajectories with explicit reasoning steps, enabling models to
integrate GUI visual-spatial information with logical reasoning before action
generation. The second stage, Deliberation Enhancement, refines the basic
reasoner into a deliberative one using Reinforcement Learning. This stage
introduces two approaches: Sub-goal Guidance, which rewards models for
generating accurate intermediate sub-goals, and Error Recovery Scenario
Construction, which creates failure-and-recovery training scenarios from
identified prone-to-error steps. Experimental results show InfiGUI-R1 achieves
strong performance in GUI grounding and trajectory tasks. Resources at
https://github.com/Reallm-Labs/InfiGUI-R1.

</details>


### [85] [A Knowledge-Informed Deep Learning Paradigm for Generalizable and Stability-Optimized Car-Following Models](https://arxiv.org/abs/2504.14241)
*Chengming Wang,Dongyao Jia,Wei Wang,Dong Ngoduy,Bei Peng,Jianping Wang*

Main category: cs.AI

TL;DR: 提出了一种基于知识蒸馏的深度学习范式（KIDL），结合大语言模型（LLMs）的泛化能力，解决了传统跟车模型（CFMs）在泛化和稳定性上的不足。


<details>
  <summary>Details</summary>
Motivation: 传统CFMs依赖特定数据集，泛化能力有限且缺乏稳定性优化，难以满足自动驾驶车辆（AVs）的安全需求。

Method: 通过知识蒸馏将LLMs提取的通用知识迁移到轻量级神经网络中，并在训练目标中直接加入稳定性约束。

Result: 在NGSIM和HighD数据集上验证，KIDL在行为泛化和交通流稳定性上优于现有模型。

Conclusion: KIDL为下一代交通系统提供了稳健且可扩展的解决方案。

Abstract: Car-following models (CFMs) are fundamental to traffic flow analysis and
autonomous driving. Although calibrated physics-based and trained data-driven
CFMs can replicate human driving behavior, their reliance on specific datasets
limits generalization across diverse scenarios and reduces reliability in
real-world deployment. Moreover, these models typically focus on behavioral
fidelity and do not support the explicit optimization of local and string
stability, which are increasingly important for the safe and efficient
operation of autonomous vehicles (AVs). To address these limitations, we
propose a Knowledge-Informed Deep Learning (KIDL) paradigm that distills the
generalization capabilities of pre-trained Large Language Models (LLMs) into a
lightweight and stability-aware neural architecture. LLMs are used to extract
fundamental car-following knowledge beyond dataset-specific patterns, and this
knowledge is transferred to a reliable, tractable, and computationally
efficient model through knowledge distillation. KIDL also incorporates
stability constraints directly into its training objective, ensuring that the
resulting model not only emulates human-like behavior but also satisfies the
local and string stability requirements essential for real-world AV deployment.
We evaluate KIDL on the real-world NGSIM and HighD datasets, comparing its
performance with representative physics-based, data-driven, and hybrid CFMs.
Both empirical and theoretical results consistently demonstrate KIDL's superior
behavioral generalization and traffic flow stability, offering a robust and
scalable solution for next-generation traffic systems.

</details>


### [86] [Rethinking Traffic Flow Forecasting: From Transition to Generatation](https://arxiv.org/abs/2504.14248)
*Li Shijiao,Ma Zhipeng,He Huajun,Chen Haiyue*

Main category: cs.AI

TL;DR: 论文提出了一种名为EMBSFormer的多分支相似性Transformer模型，用于解决交通流预测中忽略流生成过程的问题，通过多分支编码和自注意力机制分别建模生成和转移过程，实验表明其性能优于基线模型且参数更少。


<details>
  <summary>Details</summary>
Motivation: 现有交通流预测方法仅关注流转移过程而忽略流生成过程，导致模型无法捕捉多周期性和节点间交互模式。

Method: 提出EMBSFormer模型，包括相似性分析模块（多分支编码）和时空自注意力机制（GNN和时间卷积），分别建模流生成和转移过程。

Result: 在三个真实数据集上，EMBSFormer在长短期预测任务中均优于基线模型，且参数更少（仅18%）。

Conclusion: EMBSFormer通过分离建模流生成和转移过程，显著提升了交通流预测性能，同时减少了模型复杂度。

Abstract: Traffic flow prediction plays an important role in Intelligent Transportation
Systems in traffic management and urban planning. There have been extensive
successful works in this area. However, these approaches focus only on
modelling the flow transition and ignore the flow generation process, which
manifests itself in two ways: (i) The models are based on Markovian
assumptions, ignoring the multi-periodicity of the flow generation in nodes.
(ii) The same structure is designed to encode both the transition and
generation processes, ignoring the differences between them. To address these
problems, we propose an Effective Multi-Branch Similarity Transformer for
Traffic Flow Prediction, namely EMBSFormer. Through data analysis, we find that
the factors affecting traffic flow include node-level traffic generation and
graph-level traffic transition, which describe the multi-periodicity and
interaction pattern of nodes, respectively. Specifically, to capture traffic
generation patterns, we propose a similarity analysis module that supports
multi-branch encoding to dynamically expand significant cycles. For traffic
transition, we employ a temporal and spatial self-attention mechanism to
maintain global node interactions, and use GNN and time conv to model local
node interactions, respectively. Model performance is evaluated on three
real-world datasets on both long-term and short-term prediction tasks.
Experimental results show that EMBSFormer outperforms baselines on both tasks.
Moreover, compared to models based on flow transition modelling (e.g. GMAN,
513k), the variant of EMBSFormer(93K) only uses 18\% of the parameters,
achieving the same performance.

</details>


### [87] [ProtPainter: Draw or Drag Protein via Topology-guided Diffusion](https://arxiv.org/abs/2504.14274)
*Zhengxi Lu,Shizhuo Cheng,Yuru Jiang,Yan Zhang,Min Zhang*

Main category: cs.AI

TL;DR: ProtPainter是一种基于扩散的方法，通过3D曲线条件生成蛋白质骨架，分为曲线草图生成和草图引导骨架生成两阶段，实现了对拓扑结构的精确控制。


<details>
  <summary>Details</summary>
Motivation: 现有蛋白质骨架生成方法缺乏对拓扑结构的灵活控制，限制了骨架空间的探索。

Method: ProtPainter采用两阶段方法：1) CurveEncoder从曲线预测二级结构注释生成草图；2) 草图引导DDPM生成骨架，并引入Helix-Gating控制缩放因子。

Result: 实验表明ProtPainter能生成拓扑拟合（scTF > 0.8）和可设计（scTM > 0.5）的骨架，展示了其灵活性和多功能性。

Conclusion: ProtPainter为拓扑条件蛋白质生成提供了新方法，并通过新基准和指标验证了其有效性。

Abstract: Recent advances in protein backbone generation have achieved promising
results under structural, functional, or physical constraints. However,
existing methods lack the flexibility for precise topology control, limiting
navigation of the backbone space. We present ProtPainter, a diffusion-based
approach for generating protein backbones conditioned on 3D curves. ProtPainter
follows a two-stage process: curve-based sketching and sketch-guided backbone
generation. For the first stage, we propose CurveEncoder, which predicts
secondary structure annotations from a curve to parametrize sketch generation.
For the second stage, the sketch guides the generative process in Denoising
Diffusion Probabilistic Modeling (DDPM) to generate backbones. During this
process, we further introduce a fusion scheduling scheme, Helix-Gating, to
control the scaling factors. To evaluate, we propose the first benchmark for
topology-conditioned protein generation, introducing Protein Restoration Task
and a new metric, self-consistency Topology Fitness (scTF). Experiments
demonstrate ProtPainter's ability to generate topology-fit (scTF > 0.8) and
designable (scTM > 0.5) backbones, with drawing and dragging tasks showcasing
its flexibility and versatility.

</details>


### [88] [CHAINSFORMER: Numerical Reasoning on Knowledge Graphs from a Chain Perspective](https://arxiv.org/abs/2504.14282)
*Ze Zhao,Bin Lu,Xiaoying Gan,Gu Tang,Luoyi Fu,Xinbing Wang*

Main category: cs.AI

TL;DR: ChainsFormer是一种基于链的框架，用于支持知识图谱中的数值推理，通过显式构建逻辑链和多跳推理，显著提升了推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如GNNs和KGEs）未能充分利用知识图谱中的逻辑路径，限制了推理效果。

Method: 提出Relation-Attribute Chains（RA-Chains）和双曲亲和评分机制，结合注意力数值推理器。

Result: 实验显示性能提升达20.0%。

Conclusion: ChainsFormer在推理准确性和透明度上优于现有方法。

Abstract: Reasoning over Knowledge Graphs (KGs) plays a pivotal role in knowledge graph
completion or question answering systems, providing richer and more accurate
triples and attributes. As numerical attributes become increasingly essential
in characterizing entities and relations in KGs, the ability to reason over
these attributes has gained significant importance. Existing graph-based
methods such as Graph Neural Networks (GNNs) and Knowledge Graph Embeddings
(KGEs), primarily focus on aggregating homogeneous local neighbors and
implicitly embedding diverse triples. However, these approaches often fail to
fully leverage the potential of logical paths within the graph, limiting their
effectiveness in exploiting the reasoning process. To address these
limitations, we propose ChainsFormer, a novel chain-based framework designed to
support numerical reasoning. Chainsformer not only explicitly constructs
logical chains but also expands the reasoning depth to multiple hops.
Specially, we introduces Relation-Attribute Chains (RA-Chains), a specialized
logic chain, to model sequential reasoning patterns. ChainsFormer captures the
step-by-step nature of multi-hop reasoning along RA-Chains by employing
sequential in-context learning. To mitigate the impact of noisy chains, we
propose a hyperbolic affinity scoring mechanism that selects relevant logic
chains in a variable-resolution space. Furthermore, ChainsFormer incorporates
an attention-based numerical reasoner to identify critical reasoning paths,
enhancing both reasoning accuracy and transparency. Experimental results
demonstrate that ChainsFormer significantly outperforms state-of-the-art
methods, achieving up to a 20.0% improvement in performance. The
implementations are available at
https://github.com/zhaodazhuang2333/ChainsFormer.

</details>


### [89] [RadioDiff-Inverse: Diffusion Enhanced Bayesian Inverse Estimation for ISAC Radio Map Construction](https://arxiv.org/abs/2504.14298)
*Xiucheng Wang,Zhongsheng Fang,Nan Cheng*

Main category: cs.AI

TL;DR: 论文提出了一种名为RadioDiff-Inverse的框架，通过扩散增强的贝叶斯逆估计方法，利用无条件生成扩散模型学习无线电地图（RM）的先验分布，解决了稀疏噪声数据下RM构建的挑战。该方法无需任务特定微调，显著降低了训练成本，并在实验中表现出优异的RM构建和环境重建性能。


<details>
  <summary>Details</summary>
Motivation: 现有RM构建方法依赖精确环境数据和基站位置，但在动态或隐私敏感环境中难以获取。稀疏测量技术虽减少数据收集，但其噪声对RM精度的影响尚未充分研究。

Method: 提出RadioDiff-Inverse框架，将RM构建建模为贝叶斯逆问题，利用无条件生成扩散模型学习RM先验分布，结合集成感知与通信（ISAC）实现环境结构感知。

Result: 实验表明，RadioDiff-Inverse在RM构建精度、环境重建及抗噪声稀疏采样方面达到最优性能。

Conclusion: RadioDiff-Inverse为动态或隐私敏感环境中的RM构建提供了一种高效、鲁棒的解决方案，且无需额外训练成本。

Abstract: Radio maps (RMs) are essential for environment-aware communication and
sensing, providing location-specific wireless channel information. Existing RM
construction methods often rely on precise environmental data and base station
(BS) locations, which are not always available in dynamic or privacy-sensitive
environments. While sparse measurement techniques reduce data collection, the
impact of noise in sparse data on RM accuracy is not well understood. This
paper addresses these challenges by formulating RM construction as a Bayesian
inverse problem under coarse environmental knowledge and noisy sparse
measurements. Although maximum a posteriori (MAP) filtering offers an optimal
solution, it requires a precise prior distribution of the RM, which is
typically unavailable. To solve this, we propose RadioDiff-Inverse, a
diffusion-enhanced Bayesian inverse estimation framework that uses an
unconditional generative diffusion model to learn the RM prior. This approach
not only reconstructs the spatial distribution of wireless channel features but
also enables environmental structure perception, such as building outlines, and
location of BS just relay on pathloss, through integrated sensing and
communication (ISAC). Remarkably, RadioDiff-Inverse is training-free,
leveraging a pre-trained model from Imagenet without task-specific fine-tuning,
which significantly reduces the training cost of using generative large model
in wireless networks. Experimental results demonstrate that RadioDiff-Inverse
achieves state-of-the-art performance in accuracy of RM construction and
environmental reconstruction, and robustness against noisy sparse sampling.

</details>


### [90] [FAIRGAME: a Framework for AI Agents Bias Recognition using Game Theory](https://arxiv.org/abs/2504.14325)
*Alessio Buscemi,Daniele Proverbio,Alessandro Di Stefano,The Anh Han,German Castignani,Pietro Di Liò*

Main category: cs.AI

TL;DR: FAIRGAME是一个基于博弈论的框架，用于识别AI代理在多智能体应用中的偏见，支持用户模拟游戏场景并比较结果。


<details>
  <summary>Details</summary>
Motivation: 多智能体交互增加了AI结果的可解释性和预测复杂性，需要标准化工具支持博弈论模型的比较和解释。

Method: 开发FAIRGAME框架，实现游戏模拟和结果比较，分析不同LLM、语言、个性特征和策略知识对结果的影响。

Result: FAIRGAME能可靠地发现AI代理的偏见，预测战略互动中的行为，并支持进一步研究。

Conclusion: FAIRGAME为AI代理的战略决策研究提供了系统化工具，促进可信赖的AI应用。

Abstract: Letting AI agents interact in multi-agent applications adds a layer of
complexity to the interpretability and prediction of AI outcomes, with profound
implications for their trustworthy adoption in research and society. Game
theory offers powerful models to capture and interpret strategic interaction
among agents, but requires the support of reproducible, standardized and
user-friendly IT frameworks to enable comparison and interpretation of results.
To this end, we present FAIRGAME, a Framework for AI Agents Bias Recognition
using Game Theory. We describe its implementation and usage, and we employ it
to uncover biased outcomes in popular games among AI agents, depending on the
employed Large Language Model (LLM) and used language, as well as on the
personality trait or strategic knowledge of the agents. Overall, FAIRGAME
allows users to reliably and easily simulate their desired games and scenarios
and compare the results across simulation campaigns and with game-theoretic
predictions, enabling the systematic discovery of biases, the anticipation of
emerging behavior out of strategic interplays, and empowering further research
into strategic decision-making using LLM agents.

</details>


### [91] [Time Up! An Empirical Study of LLM Reasoning Ability Under Output Length Constraint](https://arxiv.org/abs/2504.14350)
*Yi Sun,Han Wang,Jiaqiang Li,Jiacheng Liu,Xiangyu Li,Hao Wen,Huiwen Zheng,Yan Liang,Yuanchun Li,Yunxin Liu*

Main category: cs.AI

TL;DR: 研究探讨了大型语言模型（LLMs）在输出长度受限时的推理能力表现，发现预算约束下模型选择和提示风格的最优解与无约束情况不同。


<details>
  <summary>Details</summary>
Motivation: 在现实场景中，模型常受时间或输出长度限制，但LLMs在此类约束下的推理能力尚未明确。

Method: 对25种以上LLMs在常见推理数据集上进行广泛测试，分析推理准确性与模型类型、大小、提示风格等的关系。

Result: 研究发现预算约束下模型大小和提示风格的最优选择与无约束情况不同，并提供了实际部署建议。

Conclusion: 研究为在延迟约束下部署LLMs提供了实用指导。

Abstract: Recent work has demonstrated the remarkable potential of Large Language
Models (LLMs) in test-time scaling. By making the models think before
answering, they are able to achieve much higher accuracy with extra inference
computation. However, in many real-world scenarios, models are used under time
constraints, where an answer should be given to the user within a certain
output length. It is unclear whether and how the reasoning abilities of LLMs
remain effective under such constraints. We take a first look at this problem
by conducting an in-depth empirical study. Specifically, we test more than 25
LLMs on common reasoning datasets under a wide range of output length budgets,
and we analyze the correlation between the inference accuracy and various
properties including model type, model size, prompt style, etc. We also
consider the mappings between the token budgets and the actual on-device
latency budgets. The results have demonstrated several interesting findings
regarding the budget-aware LLM reasoning that differ from the unconstrained
situation, e.g. the optimal choices of model sizes and prompts change under
different budgets. These findings offer practical guidance for users to deploy
LLMs under real-world latency constraints.

</details>


### [92] [Mathematical Programming Models for Exact and Interpretable Formulation of Neural Networks](https://arxiv.org/abs/2504.14356)
*Masoud Ataei,Edrin Hasaj,Jacob Gipp,Sepideh Forouzi*

Main category: cs.AI

TL;DR: 本文提出了一种统一的混合整数编程框架，用于训练稀疏且可解释的神经网络。通过二元变量建模非线性激活函数（如ReLU）并结合结构稀疏性约束，实现了参数学习、架构选择和结构正则化的全局优化。


<details>
  <summary>Details</summary>
Motivation: 解决神经网络训练中参数学习、架构选择和结构正则化的分离问题，同时提升模型的稀疏性、可解释性和可验证性。

Method: 采用混合整数编程框架，通过二元变量建模非线性激活函数（如ReLU），并结合滤波器级和层级的剪枝约束，实现全局优化。

Result: 提出的框架能够生成全局最优解，平衡预测精度、权重稀疏性和架构紧凑性，同时支持逻辑或领域特定约束的精确实施。

Conclusion: 该框架为可解释人工智能、符号推理和形式验证等领域的研究提供了桥梁，实现了训练过程的统一优化。

Abstract: This paper presents a unified mixed-integer programming framework for
training sparse and interpretable neural networks. We develop exact
formulations for both fully connected and convolutional architectures by
modeling nonlinearities such as ReLU activations through binary variables and
encoding structural sparsity via filter- and layer-level pruning constraints.
The resulting models integrate parameter learning, architecture selection, and
structural regularization within a single optimization problem, yielding
globally optimal solutions with respect to a composite objective that balances
prediction accuracy, weight sparsity, and architectural compactness. The
mixed-integer programming formulation accommodates piecewise-linear operations,
including max pooling and activation gating, and permits precise enforcement of
logic-based or domain-specific constraints. By incorporating considerations of
interpretability, sparsity, and verifiability directly into the training
process, the proposed framework bridges a range of research areas including
explainable artificial intelligence, symbolic reasoning, and formal
verification.

</details>


### [93] [The Geometry of Self-Verification in a Task-Specific Reasoning Model](https://arxiv.org/abs/2504.14379)
*Andrew Lee,Lihao Sun,Chris Wendler,Fernanda Viégas,Martin Wattenberg*

Main category: cs.AI

TL;DR: 论文研究了推理模型如何验证自身答案，通过训练模型并分析其验证机制，发现GLU权重和注意力头在验证过程中起关键作用。


<details>
  <summary>Details</summary>
Motivation: 探究推理模型如何验证自身答案，以理解其内部工作机制。

Method: 使用DeepSeek R1的配方训练模型，通过自上而下和自下而上的分析，研究模型的验证机制。

Result: 发现GLU权重编码验证相关标记，注意力头是验证的关键组件。

Conclusion: 验证机制可能由更大的电路组成，但已识别出关键组件。

Abstract: How do reasoning models verify their own answers? We study this question by
training a model using DeepSeek R1's recipe on the CountDown task. We leverage
the fact that preference tuning leads to mode collapse, resulting in a model
that always produces highly structured and easily parse-able chain-of-thought
sequences. With this setup, we do a top-down and bottom-up analysis to
reverse-engineer how the model verifies its outputs. Our top-down analysis
reveals Gated Linear Unit (GLU) weights encoding verification-related tokens,
such as ``success'' or ``incorrect'', which activate according to the
correctness of the model's reasoning steps. Our bottom-up analysis reveals that
``previous-token heads'' are mainly responsible for model verification. Our
analyses meet in the middle: drawing inspiration from inter-layer communication
channels, we use the identified GLU vectors to localize as few as three
attention heads that can disable model verification, pointing to a necessary
component of a potentially larger verification circuit.

</details>


### [94] [Seeing Through Risk: A Symbolic Approximation of Prospect Theory](https://arxiv.org/abs/2504.14448)
*Ali Arslan Yousaf,Umair Rehman,Muhammad Umair Danish*

Main category: cs.AI

TL;DR: 提出了一种新的符号建模框架，将可解释性与前景理论的核心见解结合，用于风险决策。


<details>
  <summary>Details</summary>
Motivation: 解决传统决策模型中效用曲线和概率权重函数不透明的问题，提供更透明的特征。

Method: 采用效果大小引导的特征替代传统函数，数学形式化方法，并通过合成数据集进行端到端验证。

Result: 模型在预测性能上具有竞争力，且能清晰映射心理构造系数。

Conclusion: 该模型适用于从AI安全到经济政策分析的多种应用。

Abstract: We propose a novel symbolic modeling framework for decision-making under risk
that merges interpretability with the core insights of Prospect Theory. Our
approach replaces opaque utility curves and probability weighting functions
with transparent, effect-size-guided features. We mathematically formalize the
method, demonstrate its ability to replicate well-known framing and
loss-aversion phenomena, and provide an end-to-end empirical validation on
synthetic datasets. The resulting model achieves competitive predictive
performance while yielding clear coefficients mapped onto psychological
constructs, making it suitable for applications ranging from AI safety to
economic policy analysis.

</details>


### [95] [Meta-Thinking in LLMs via Multi-Agent Reinforcement Learning: A Survey](https://arxiv.org/abs/2504.14520)
*Ahsan Bilal,Muhammad Ahmed Mohsin,Muhammad Umer,Muhammad Awais Khan Bangash,Muhammad Ali Jamshed*

Main category: cs.AI

TL;DR: 该综述探讨了从多智能体强化学习（MARL）角度提升大语言模型（LLMs）的元思维能力，以增强其可靠性、灵活性和性能。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs存在幻觉和缺乏自我评估机制等问题，需通过元思维（如自我反思、评估和控制）进一步提升其在高风险任务中的表现。

Method: 分析了现有方法（如RLHF、自蒸馏和思维链提示）的局限性，并探讨了多智能体架构（如监督者-智能体层级、智能体辩论和心理理论框架）如何模拟人类内省行为。

Result: 提出了通过MARL的奖励机制、自博弈和持续学习方法构建内省、自适应和可信赖LLMs的路线图。

Conclusion: 讨论了评估指标、数据集及未来研究方向（如神经科学启发架构和混合符号推理），为开发更强大的LLMs提供了指导。

Abstract: This survey explores the development of meta-thinking capabilities in Large
Language Models (LLMs) from a Multi-Agent Reinforcement Learning (MARL)
perspective. Meta-thinking self-reflection, assessment, and control of thinking
processes is an important next step in enhancing LLM reliability, flexibility,
and performance, particularly for complex or high-stakes tasks. The survey
begins by analyzing current LLM limitations, such as hallucinations and the
lack of internal self-assessment mechanisms. It then talks about newer methods,
including RL from human feedback (RLHF), self-distillation, and
chain-of-thought prompting, and each of their limitations. The crux of the
survey is to talk about how multi-agent architectures, namely supervisor-agent
hierarchies, agent debates, and theory of mind frameworks, can emulate
human-like introspective behavior and enhance LLM robustness. By exploring
reward mechanisms, self-play, and continuous learning methods in MARL, this
survey gives a comprehensive roadmap to building introspective, adaptive, and
trustworthy LLMs. Evaluation metrics, datasets, and future research avenues,
including neuroscience-inspired architectures and hybrid symbolic reasoning,
are also discussed.

</details>


### [96] [Learning from Reasoning Failures via Synthetic Data Generation](https://arxiv.org/abs/2504.14523)
*Gabriela Ben Melech Stan,Estelle Aflalo,Avinash Madasu,Vasudev Lal,Phillip Howard*

Main category: cs.AI

TL;DR: 提出了一种基于分析现有大型多模态模型（LMM）推理失败的新方法，生成针对性合成数据以提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 由于高质量配对图像-文本数据的稀缺，现有方法未能针对LMM的特定推理缺陷生成数据，而人类学习方式更高效。

Method: 利用前沿模型分析较弱LMM的错误，生成纠正推理失败的示例，并通过过滤确保质量。

Result: 生成了553k示例的数据集，实验表明其能显著提升LMM在下游任务中的性能，甚至优于同等量真实数据。

Conclusion: 针对性生成合成数据对改进LMM推理能力具有重要价值，数据集和代码将公开。

Abstract: Training models on synthetic data has emerged as an increasingly important
strategy for improving the performance of generative AI. This approach is
particularly helpful for large multimodal models (LMMs) due to the relative
scarcity of high-quality paired image-text data compared to language-only data.
While a variety of methods have been proposed for generating large multimodal
datasets, they do not tailor the synthetic data to address specific
deficiencies in the reasoning abilities of LMMs which will be trained with the
generated dataset. In contrast, humans often learn in a more efficient manner
by seeking out examples related to the types of reasoning where they have
failed previously. Inspired by this observation, we propose a new approach for
synthetic data generation which is grounded in the analysis of an existing
LMM's reasoning failures. Our methodology leverages frontier models to
automatically analyze errors produced by a weaker LMM and propose new examples
which can be used to correct the reasoning failure via additional training,
which are then further filtered to ensure high quality. We generate a large
multimodal instruction tuning dataset containing over 553k examples using our
approach and conduct extensive experiments demonstrating its utility for
improving the performance of LMMs on multiple downstream tasks. Our results
show that models trained on our synthetic data can even exceed the performance
of LMMs trained on an equivalent amount of additional real data, demonstrating
the high value of generating synthetic data targeted to specific reasoning
failure modes in LMMs. We will make our dataset and code publicly available.

</details>


### [97] [LLM-Enabled In-Context Learning for Data Collection Scheduling in UAV-assisted Sensor Networks](https://arxiv.org/abs/2504.14556)
*Yousef Emami,Hao Gao,SeyedSina Nabavirazani,Luis Almeida*

Main category: cs.AI

TL;DR: 本文提出了一种基于上下文学习（ICL）的数据收集调度方案（ICLDC），用于无人机辅助传感器网络中的紧急任务，如搜救行动，以解决深度强化学习（DRL）的局限性。


<details>
  <summary>Details</summary>
Motivation: 无人机在紧急任务（如搜救）中的应用需要高效且适应性强的调度方法，而现有的DRL方法存在训练复杂、仿真与现实差距大、样本效率低等问题。

Method: 通过无人机收集并传输传感器数据至大型语言模型（LLM），生成自然语言任务描述，并从中获取数据收集调度计划。系统通过反馈机制持续优化决策。

Result: ICLDC在对抗篡改攻击时表现出色，比最大信道增益方案减少了约56%的累积丢包率。

Conclusion: ICLDC为无人机辅助数据收集提供了智能调度和控制的新方向，具有实际应用潜力。

Abstract: Unmanned Aerial Vehicles (UAVs) are increasingly being used in various
private and commercial applications, e.g. traffic control, package delivery,
and Search and Rescue (SAR) operations. Machine Learning (ML) methods used in
UAV-assisted Sensor Networks (UASNETs) and especially in Deep Reinforcement
Learning (DRL) face challenges such as complex and lengthy model training, gaps
between simulation and reality, and low sample efficiency, which conflict with
the urgency of emergencies such as SAR operations. This paper proposes
In-Context Learning (ICL)-based Data Collection Scheduling (ICLDC) scheme, as
an alternative to DRL in emergencies. The UAV collects and transmits logged
sensory data, to an LLM, to generate a task description in natural language,
from which it obtains a data collection schedule to be executed by the UAV. The
system continuously adapts by adding feedback to task descriptions and
utilizing feedback for future decisions. This method is tested against
jailbreaking attacks, where task description is manipulated to undermine
network performance, highlighting the vulnerability of LLMs to such attacks.
The proposed ICLDC outperforms the Maximum Channel Gain by reducing cumulative
packet loss by approximately 56\%. ICLDC presents a promising direction for
intelligent scheduling and control in UAV-assisted data collection.

</details>


### [98] [Toward the Axiomatization of Intelligence: Structure, Time, and Existence](https://arxiv.org/abs/2504.14596)
*Kei Itoh*

Main category: cs.AI

TL;DR: 该研究通过集合论和范畴论构建了一个智能的公理化定义，并应用于不同类型的智能系统，探讨了时间与活动对智能的影响。


<details>
  <summary>Details</summary>
Motivation: 智能是一个多义且模糊的概念，需要一个形式化的定义框架来统一理解。

Method: 使用集合论表示宇宙域，定义智能为涉及时间演化和交互的结构，并通过范畴论扩展定义。

Result: 比较了三种智能系统的结构特性，提出了“活动”概念，并展示了定义方法的普适性。

Conclusion: 该方法不仅适用于智能，还可推广到意识、情感等其他概念的形式化定义。

Abstract: This study aims to construct an axiomatic definition of intelligence within a
meta-framework that defines the method of definition, addressing intelligence
as an inherently naive and polysemous concept. Initially, we formalize a
set-theoretic representation of the universe as the domain wherein intelligence
exists and characterize intelligence as a structure that involves temporal
evolution and interaction with other sets. Starting from a naive definition of
intelligence as "an entity possessing structures for externally inputting,
internally processing, and externally outputting information or matter," we
axiomatically reformulate it within this set-theoretical depiction of the
universe. Applying this axiomatic definition, we compare and interpret three
examples -- Hebbian non-optimized neural networks (NNs),
backpropagation-optimized NNs, and biological reflexive systems -- in terms of
their intelligence, structural properties, and biological plausibility.
Furthermore, by extending our definition into a categorical framework, we
introduce two categories, "Time Category" and "Intelligence Category," along
with the functorial relationships between them, demonstrating the potential to
represent changes and mimicry relationships among intelligent systems
abstractly. Additionally, since intelligence, as defined herein, functions
effectively only when accompanied by temporal interactions, we introduce the
concept of "activity" and explore how activity-based conditions influence
classifications and interpretations of intelligence. Finally, we suggest that
our definitional methodology is not limited to intelligence alone, but can be
similarly applied to other concepts, such as consciousness and emotion,
advocating for their formal reinterpretation through the same procedural steps:
defining a universal representation, selecting naive definitions, and axiomatic
formalization.

</details>


### [99] [UFO2: The Desktop AgentOS](https://arxiv.org/abs/2504.14603)
*Chaoyun Zhang,He Huang,Chiming Ni,Jian Mu,Si Qin,Shilin He,Lu Wang,Fangkai Yang,Pu Zhao,Chao Du,Liqun Li,Yu Kang,Zhao Jiang,Suzhen Zheng,Rujia Wang,Jiaxu Qian,Minghua Ma,Jian-Guang Lou,Qingwei Lin,Saravan Rajmohan,Dongmei Zhang*

Main category: cs.AI

TL;DR: UFO2是一个多代理AgentOS，通过深度操作系统集成和混合控制检测管道，显著提升了桌面自动化的鲁棒性和执行准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的计算机使用代理（CUAs）多为概念原型，受限于浅层操作系统集成、脆弱的截图交互和中断式执行，UFO2旨在解决这些问题。

Method: UFO2采用多代理架构，包括集中式HostAgent和专用AppAgent，结合Windows UIA与视觉解析的混合控制检测管道，以及推测性多动作规划和PiP界面。

Result: 在20多个真实Windows应用中，UFO2表现出比现有CUAs更高的鲁棒性和执行准确性。

Conclusion: 深度操作系统集成为实现可靠、用户对齐的桌面自动化提供了可扩展的路径。

Abstract: Recent Computer-Using Agents (CUAs), powered by multimodal large language
models (LLMs), offer a promising direction for automating complex desktop
workflows through natural language. However, most existing CUAs remain
conceptual prototypes, hindered by shallow OS integration, fragile
screenshot-based interaction, and disruptive execution.
  We present UFO2, a multiagent AgentOS for Windows desktops that elevates CUAs
into practical, system-level automation. UFO2 features a centralized HostAgent
for task decomposition and coordination, alongside a collection of
application-specialized AppAgent equipped with native APIs, domain-specific
knowledge, and a unified GUI--API action layer. This architecture enables
robust task execution while preserving modularity and extensibility. A hybrid
control detection pipeline fuses Windows UI Automation (UIA) with vision-based
parsing to support diverse interface styles. Runtime efficiency is further
enhanced through speculative multi-action planning, reducing per-step LLM
overhead. Finally, a Picture-in-Picture (PiP) interface enables automation
within an isolated virtual desktop, allowing agents and users to operate
concurrently without interference.
  We evaluate UFO2 across over 20 real-world Windows applications,
demonstrating substantial improvements in robustness and execution accuracy
over prior CUAs. Our results show that deep OS integration unlocks a scalable
path toward reliable, user-aligned desktop automation.

</details>


### [100] [Consensus in Motion: A Case of Dynamic Rationality of Sequential Learning in Probability Aggregation](https://arxiv.org/abs/2504.14624)
*Polina Gordienko,Christoph Jansen,Thomas Augustin,Martin Rechenauer*

Main category: cs.AI

TL;DR: 提出了一个基于命题概率逻辑的概率聚合框架，强调动态理性而非静态理性，确保集体信念与新信息一致更新。


<details>
  <summary>Details</summary>
Motivation: 传统判断聚合关注静态理性，而本研究旨在解决动态理性问题，确保集体信念能一致更新。

Method: 基于命题概率逻辑，提出共识兼容且独立的聚合规则，并限制新信息在共同基础上。

Result: 证明非嵌套议程上的线性聚合规则是必要的，并提供了公平学习过程的充分条件。

Conclusion: 框架支持多阶段决策，确保集体信念更新的一致性，并通过政治场景示例验证。

Abstract: We propose a framework for probability aggregation based on propositional
probability logic. Unlike conventional judgment aggregation, which focuses on
static rationality, our model addresses dynamic rationality by ensuring that
collective beliefs update consistently with new information. We show that any
consensus-compatible and independent aggregation rule on a non-nested agenda is
necessarily linear. Furthermore, we provide sufficient conditions for a fair
learning process, where individuals initially agree on a specified subset of
propositions known as the common ground, and new information is restricted to
this shared foundation. This guarantees that updating individual judgments via
Bayesian conditioning-whether performed before or after aggregation-yields the
same collective belief. A distinctive feature of our framework is its treatment
of sequential decision-making, which allows new information to be incorporated
progressively through multiple stages while maintaining the established common
ground. We illustrate our findings with a running example in a political
scenario concerning healthcare and immigration policies.

</details>


### [101] [A Framework for Benchmarking and Aligning Task-Planning Safety in LLM-Based Embodied Agents](https://arxiv.org/abs/2504.14650)
*Yuting Huang,Leilei Ding,Zhipeng Tang,Tianfu Wang,Xinrui Lin,Wuyang Zhang,Mingxiao Ma,Yanyong Zhang*

Main category: cs.AI

TL;DR: 论文提出Safe-BeAl框架，通过SafePlan-Bench评估LLM代理的任务规划安全性，并通过Safe-Align方法提升安全性，实验显示安全性提升8.55-15.22%。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在任务规划中表现优异，但其安全性问题尚未充分研究，需要系统性解决方案。

Method: 提出Safe-BeAl框架，包括评估基准SafePlan-Bench和安全性对齐方法Safe-Align。

Result: 实验表明，Safe-BeAl显著提升LLM代理的安全性（8.55-15.22%），同时不影响任务完成率。

Conclusion: Safe-BeAl为LLM代理的安全性提供了有效解决方案，未来可扩展至更广泛的应用场景。

Abstract: Large Language Models (LLMs) exhibit substantial promise in enhancing
task-planning capabilities within embodied agents due to their advanced
reasoning and comprehension. However, the systemic safety of these agents
remains an underexplored frontier. In this study, we present Safe-BeAl, an
integrated framework for the measurement (SafePlan-Bench) and alignment
(Safe-Align) of LLM-based embodied agents' behaviors. SafePlan-Bench
establishes a comprehensive benchmark for evaluating task-planning safety,
encompassing 2,027 daily tasks and corresponding environments distributed
across 8 distinct hazard categories (e.g., Fire Hazard). Our empirical analysis
reveals that even in the absence of adversarial inputs or malicious intent,
LLM-based agents can exhibit unsafe behaviors. To mitigate these hazards, we
propose Safe-Align, a method designed to integrate physical-world safety
knowledge into LLM-based embodied agents while maintaining task-specific
performance. Experiments across a variety of settings demonstrate that
Safe-BeAl provides comprehensive safety validation, improving safety by 8.55 -
15.22%, compared to embodied agents based on GPT-4, while ensuring successful
task completion.

</details>


### [102] [AI with Emotions: Exploring Emotional Expressions in Large Language Models](https://arxiv.org/abs/2504.14706)
*Shin-nosuke Ishikawa,Atsushi Yoshino*

Main category: cs.AI

TL;DR: 论文探讨了大型语言模型（LLMs）在表达情感方面的能力，通过实验验证了LLMs能够根据指定情感状态生成一致的回答。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs是否能够模拟情感表达，以扩展其在情感交互中的应用潜力。

Method: 使用Russell的Circumplex模型定义情感状态，通过多个LLMs生成情感化回答，并用独立的情感分析模型评估一致性。

Result: 实验表明，LLMs生成的情感化回答与指定情感状态一致，验证了其情感表达能力。

Conclusion: LLMs具备模拟情感的能力，为情感交互应用（如顾问服务）提供了可能性。

Abstract: The human-level performance of Large Language Models (LLMs) across various
tasks has raised expectations for the potential of Artificial Intelligence (AI)
to possess emotions someday. To explore the capability of current LLMs to
express emotions in their outputs, we conducted an experiment using several
LLMs (OpenAI GPT, Google Gemini, Meta Llama3, and Cohere Command R+) to
role-play as agents answering questions with specified emotional states.We
defined the emotional states using Russell's Circumplex model, a
well-established framework that characterizes emotions along the
sleepy-activated (arousal) and pleasure-displeasure (valence) axes. We chose
this model for its simplicity, utilizing two continuous parameters, which
allows for better controllability in applications involving continuous changes
in emotional states. The responses generated were evaluated using a sentiment
analysis model, independent of the LLMs, trained on the GoEmotions dataset. The
evaluation showed that the emotional states of the generated answers were
consistent with the specifications, demonstrating the LLMs' capability for
emotional expression. This indicates the potential for LLM-based AI agents to
simulate emotions, opening up a wide range of applications for emotion-based
interactions, such as advisors or consultants who can provide advice or
opinions with a personal touch.

</details>


### [103] [PLANET: A Collection of Benchmarks for Evaluating LLMs' Planning Capabilities](https://arxiv.org/abs/2504.14773)
*Haoming Li,Zhaoliang Chen,Jonathan Zhang,Fei Liu*

Main category: cs.AI

TL;DR: 本文分析了现有规划基准的现状，提出了分类和建议，以帮助选择适合的算法和指导未来基准开发。


<details>
  <summary>Details</summary>
Motivation: 规划在智能体和智能AI中至关重要，但目前缺乏对现有规划基准的全面理解，导致跨领域算法比较和新场景算法选择困难。

Method: 研究了一系列规划基准，将其分类为具身环境、网络导航、调度、游戏与谜题以及日常任务自动化，并分析了其适用性。

Result: 提出了针对不同算法的最合适基准，并指出了未来基准开发的潜在方向。

Conclusion: 研究为规划算法的比较和选择提供了实用指导，并为未来基准设计提供了建议。

Abstract: Planning is central to agents and agentic AI. The ability to plan, e.g.,
creating travel itineraries within a budget, holds immense potential in both
scientific and commercial contexts. Moreover, optimal plans tend to require
fewer resources compared to ad-hoc methods. To date, a comprehensive
understanding of existing planning benchmarks appears to be lacking. Without
it, comparing planning algorithms' performance across domains or selecting
suitable algorithms for new scenarios remains challenging. In this paper, we
examine a range of planning benchmarks to identify commonly used testbeds for
algorithm development and highlight potential gaps. These benchmarks are
categorized into embodied environments, web navigation, scheduling, games and
puzzles, and everyday task automation. Our study recommends the most
appropriate benchmarks for various algorithms and offers insights to guide
future benchmark development.

</details>


### [104] [DONOD: Robust and Generalizable Instruction Fine-Tuning for LLMs via Model-Intrinsic Dataset Pruning](https://arxiv.org/abs/2504.14810)
*Jucheng Hu,Surong Yang,Dongzhan Zhou,Lijun Wu*

Main category: cs.AI

TL;DR: DONOD是一种轻量级的数据剪枝方法，通过模型参数评估数据质量，提升领域适应性和抗噪能力。


<details>
  <summary>Details</summary>
Motivation: 解决领域特定监督微调（SFT）中跨域泛化能力下降和噪声数据的问题。

Method: 使用Delta of Norm（DON）和Norm of Delta（NOD）评估数据，结合TOPSIS算法过滤噪声样本。

Result: 在数学任务中，DONOD提升了目标域准确率14.90%，跨域准确率5.67%，并展示了跨架构泛化能力。

Conclusion: DONOD在性能和应用范围上优于现有方法，且不依赖特定数据集。

Abstract: Ad-hoc instruction fine-tuning of large language models (LLMs) is widely
adopted for domain-specific adaptation. While domain-specific supervised
fine-tuning (SFT) is effective and efficient, it often weakens cross-domain
generalization and struggles with noisy training data. To address these
challenges, we propose DONOD, a lightweight model-intrinsic data pruning
method. Our approach evaluates data using two model-parameter-based metrics:
Delta of Norm (DON), which captures the cumulative influence on model weights,
and Norm of Delta (NOD), which quantifies weight instability. Moreover, by
employing the Technique for Order of Preference by Similarity to Ideal Solution
(TOPSIS) algorithm, we effectively filter noisy, unlearnable, and
generalization-harming samples without relying on auxiliary models during the
SFT process. Experiments on mathematical tasks demonstrate that data selected
by DONOD achieve superior fine-tuning efficiency and improved robustness
against noisy data. By filtering out 70% of the full dataset, we improve
target-domain accuracy by 14.90% and cross-domain accuracy by 5.67%. Meanwhile,
our selected data present superior cross-architecture generalization. Data
pruned by smaller models (e.g., Llama 3.1-8B) generalize effectively on larger
models (e.g., Llama 2-13B). Compared to existing related methodologies, DONOD
demonstrates comparable or superior performance while remaining
dataset-agnostic, enabling broader applicability.

</details>


### [105] [Establishing Reliability Metrics for Reward Models in Large Language Models](https://arxiv.org/abs/2504.14838)
*Yizhou Chen,Yawen Liu,Xuesi Wang,Qingtao Yu,Guangda Huzhang,Anxiang Zeng,Han Yu,Zhiming Zhou*

Main category: cs.AI

TL;DR: 论文提出了一种名为RETA的指标，用于直接衡量奖励模型（RM）的可靠性，并通过实验验证了其稳定性。


<details>
  <summary>Details</summary>
Motivation: 奖励模型在优化大型语言模型输出中起关键作用，但其可靠性存在不确定性，缺乏量化指标。

Method: 提出RETA指标，通过评估RM评分最高的η分位数响应的平均质量（由Oracle评分）来衡量RM可靠性，并设计了一个基准测试流程。

Result: 实验证明RETA指标具有优越的稳定性，能够有效评估不同RM的可靠性，并帮助识别最佳响应分位数。

Conclusion: RETA为RM可靠性提供了量化标准，解决了现有问题，并支持实际应用中的决策。

Abstract: The reward model (RM) that represents human preferences plays a crucial role
in optimizing the outputs of large language models (LLMs), e.g., through
reinforcement learning from human feedback (RLHF) or rejection sampling.
However, a long challenge for RM is its uncertain reliability, i.e., LLM
outputs with higher rewards may not align with actual human preferences.
Currently, there is a lack of a convincing metric to quantify the reliability
of RMs. To bridge this gap, we propose the \textit{\underline{R}eliable at
\underline{$\eta$}} (RETA) metric, which directly measures the reliability of
an RM by evaluating the average quality (scored by an oracle) of the top $\eta$
quantile responses assessed by an RM. On top of RETA, we present an integrated
benchmarking pipeline that allows anyone to evaluate their own RM without
incurring additional Oracle labeling costs. Extensive experimental studies
demonstrate the superior stability of RETA metric, providing solid evaluations
of the reliability of various publicly available and proprietary RMs. When
dealing with an unreliable RM, we can use the RETA metric to identify the
optimal quantile from which to select the responses.

</details>


### [106] [AlignRAG: An Adaptable Framework for Resolving Misalignments in Retrieval-Aware Reasoning of RAG](https://arxiv.org/abs/2504.14858)
*Jiaqi Wei,Hao Zhou,Xiang Zhang,Di Zhang,Zijie Qiu,Wei Wei,Jinzhe Li,Wanli Ouyang,Siqi Sun*

Main category: cs.AI

TL;DR: AlignRAG提出了一种新的测试时框架，通过迭代的Critique-Driven Alignment（CDA）步骤解决RAG中的推理对齐问题。


<details>
  <summary>Details</summary>
Motivation: 现有RAG管道未能确保推理轨迹与检索内容一致，导致推理对齐问题。

Method: AlignRAG通过构建上下文丰富的训练语料、生成对比性批评、训练Critic Language Model（CLM）和应用CDA步骤迭代优化推理轨迹。

Result: AlignRAG在实验中表现优于基线方法，并可无缝集成到现有RAG管道中。

Conclusion: AlignRAG为检索感知生成提供了实用的进展，重新定义了RAG为结构化推理轨迹。

Abstract: Retrieval-augmented generation (RAG) has emerged as a foundational paradigm
for knowledge-grounded text generation. However, existing RAG pipelines often
fail to ensure that the reasoning trajectories align with the evidential
constraints imposed by retrieved content. In this paper, we reframe RAG as a
problem of retrieval-aware reasoning and identify a core challenge: reasoning
misalignment-the mismatch between a model's reasoning trajectory and the
retrieved evidence. To address this challenge, we propose AlignRAG, a novel
test-time framework that mitigates reasoning misalignment through iterative
Critique-Driven Alignment (CDA) steps. In contrast to prior approaches that
rely on static training or post-hoc selection, AlignRAG actively refines
reasoning trajectories during inference by enforcing fine-grained alignment
with evidence. Our framework introduces a new paradigm for retrieval-aware
reasoning by: (1) constructing context-rich training corpora; (2) generating
contrastive critiques from preference-aware reasoning trajectories; (3)
training a dedicated \textit{Critic Language Model (CLM)} to identify reasoning
misalignments; and (4) applying CDA steps to optimize reasoning trajectories
iteratively. Empirical results demonstrate that AlignRAG consistently
outperforms all baselines and could integrate as a plug-and-play module into
existing RAG pipelines without further changes. By reconceptualizing RAG as a
structured reasoning trajectory and establishing the test-time framework for
correcting reasoning misalignments in RAG, AlignRAG provides practical
advancements for retrieval-aware generation.

</details>


### [107] [OTC: Optimal Tool Calls via Reinforcement Learning](https://arxiv.org/abs/2504.14870)
*Hongru Wang,Cheng Qian,Wanjun Zhong,Xiusi Chen,Jiahao Qiu,Shijue Huang,Bowen Jin,Mengdi Wang,Kam-Fai Wong,Heng Ji*

Main category: cs.AI

TL;DR: 论文提出了一种名为OTC-PO的强化学习框架，通过优化工具调用效率，提升语言模型在工具集成推理中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有方法在工具集成推理中忽视了工具使用的效率和成本，导致工具调用过多或不足，影响性能和开销。

Method: 提出OTC-PO框架，结合正确性和工具效率的奖励机制，并在PPO和GRPO中实现。

Result: 实验显示，工具调用减少73.1%，工具效率提升229.4%，同时保持答案准确性。

Conclusion: OTC-PO是首个明确优化工具效率的强化学习框架，显著提升了工具集成推理的性能。

Abstract: Tool-integrated reasoning (TIR) augments large language models (LLMs) with
the ability to invoke external tools, such as search engines and code
interpreters, to solve tasks beyond the capabilities of language-only
reasoning. While reinforcement learning (RL) has shown promise in improving TIR
by optimizing final answer correctness, existing approaches often overlook the
efficiency and cost associated with tool usage. This can lead to suboptimal
behavior, including excessive tool calls that increase computational and
financial overhead, or insufficient tool use that compromises answer quality.
In this work, we propose Optimal Tool Call-controlled Policy Optimization
(OTC-PO), a simple yet effective RL-based framework that encourages models to
produce accurate answers with minimal tool calls. Our method introduces a
tool-integrated reward that jointly considers correctness and tool efficiency,
promoting high tool productivity. We instantiate this framework within both
Proximal Policy Optimization (PPO) and Group Relative Preference Optimization
(GRPO), resulting in OTC-PPO and OTC-GRPO. Experiments with Qwen-2.5 and
Qwen-Math across multiple QA benchmarks show that our approach reduces tool
calls by up to 73.1\% and improves tool productivity by up to 229.4\%, while
maintaining comparable answer accuracy. To the best of our knowledge, this is
the first RL-based framework that explicitly optimizes tool-use efficiency in
TIR.

</details>


### [108] [EducationQ: Evaluating LLMs' Teaching Capabilities Through Multi-Agent Dialogue Framework](https://arxiv.org/abs/2504.14928)
*Yao Shi,Rongkeng Liang,Yong Xu*

Main category: cs.AI

TL;DR: EducationQ框架通过多智能体对话评估LLMs的教学能力，发现教学效果与模型规模或通用推理能力无线性关系，部分小模型表现优于大模型。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs作为教育工具的评估方法资源密集且复杂，缺乏对互动教学能力的关注。

Method: 提出EducationQ框架，模拟动态教育场景，测试14个LLMs在13个学科和10个难度级别上的表现。

Result: 教学效果与模型规模无关，小模型可能更优；78%的人类专家评估与自动分析一致。

Conclusion: LLMs需针对教学能力专门优化，而非简单扩展规模。

Abstract: Large language models (LLMs) increasingly serve as educational tools, yet
evaluating their teaching capabilities remains challenging due to the
resource-intensive, context-dependent, and methodologically complex nature of
teacher-student interactions. We introduce EducationQ, a multi-agent dialogue
framework that efficiently assesses teaching capabilities through simulated
dynamic educational scenarios, featuring specialized agents for teaching,
learning, and evaluation. Testing 14 LLMs across major AI Organizations
(OpenAI, Meta, Google, Anthropic, and others) on 1,498 questions spanning 13
disciplines and 10 difficulty levels reveals that teaching effectiveness does
not correlate linearly with model scale or general reasoning capabilities -
with some smaller open-source models outperforming larger commercial
counterparts in teaching contexts. This finding highlights a critical gap in
current evaluations that prioritize knowledge recall over interactive pedagogy.
Our mixed-methods evaluation, combining quantitative metrics with qualitative
analysis and expert case studies, identifies distinct pedagogical strengths
employed by top-performing models (e.g., sophisticated questioning strategies,
adaptive feedback mechanisms). Human expert evaluations show 78% agreement with
our automated qualitative analysis of effective teaching behaviors, validating
our methodology. EducationQ demonstrates that LLMs-as-teachers require
specialized optimization beyond simple scaling, suggesting next-generation
educational AI prioritize targeted enhancement of specific pedagogical
effectiveness.

</details>


### [109] [Generative Semantic Communications: Principles and Practices](https://arxiv.org/abs/2504.14947)
*Xiaojun Yuan,Haoming Ma,Yinuo Huang,Zhoufan Hua,Yong Zuo,Zhi Ding*

Main category: cs.AI

TL;DR: 论文提出了一种基于AGI的生成语义通信（GSC）新范式，利用基础模型和生成模型等先进AI技术，以解决语义通信在AGI服务中的挑战。


<details>
  <summary>Details</summary>
Motivation: 随着AGI的发展，对语义通信的需求增加，现有技术面临新挑战，需要更高效的解决方案。

Method: 提出了GSC的概念和框架，并通过两个案例研究验证其在AGI应用中的优势。

Result: GSC在AGI驱动的应用中表现出显著优势，能够高效提取和传输语义信息。

Conclusion: 论文探讨了GSC的开放挑战和研究方向，为实际应用铺平道路。

Abstract: Semantic communication leverages artificial intelligence (AI) technologies to
extract semantic information from data for efficient transmission, theraby
significantly reducing communication cost. With the evolution towards
artificial general intelligence (AGI), the increasing demands for AGI services
pose new challenges to semantic communication. In response, we propose a new
paradigm for AGI-driven communications, called generative semantic
communication (GSC), which utilizes advanced AI technologies such as foundation
models and generative models. We first describe the basic concept of GSC and
its difference from existing semantic communications, and then introduce a
general framework of GSC, followed by two case studies to verify the advantages
of GSC in AGI-driven applications. Finally, open challenges and new research
directions are discussed to stimulate this line of research and pave the way
for practical applications.

</details>


### [110] [Evaluating Code Generation of LLMs in Advanced Computer Science Problems](https://arxiv.org/abs/2504.14964)
*Emir Catir,Robin Claesson,Rodothea Myrsini Tsoupidi*

Main category: cs.AI

TL;DR: 论文评估了大型语言模型（LLMs）在解决高级编程作业中的能力，发现其在入门课程中表现优异，但在高级课程中更具挑战性。


<details>
  <summary>Details</summary>
Motivation: 研究填补了LLMs在高级编程作业中表现的研究空白，并探讨其对计算机科学（CS）教学的潜在影响。

Method: 选取12个编程问题（3个入门级，9个高级），使用四种LLM工具生成代码，并通过1000个测试用例评估其输出。

Result: LLMs在入门课程中表现优异，但在高级课程中更具挑战性，但仍能提供部分解决方案。

Conclusion: 研究为高级编程课程教师设计作业提供了指导，并强调了LLMs在CS教育中的潜力与局限性。

Abstract: Large Language Models (LLMs), such as GitHub Copilot and ChatGPT have become
popular among programming students. Students use LLMs to assist them in
programming courses, including generating source code. Previous work has
evaluated the ability of LLMs in solving introductory-course programming
assignments. The results have shown that LLMs are highly effective in
generating code for introductory Computer Science (CS) courses. However, there
is a gap in research on evaluating LLMs' ability to generate code that solves
advanced programming assignments. In this work, we evaluate the ability of four
LLM tools to solve programming assignments from advanced CS courses in three
popular programming languages, Java, Python, and C. We manually select 12
problems, three problems from introductory courses as the baseline and nine
programming assignments from second- and third-year CS courses. To evaluate the
LLM-generated code, we generate a test suite of 1000 test cases per problem and
analyze the program output. Our evaluation shows that although LLMs are highly
effective in generating source code for introductory programming courses,
solving advanced programming assignments is more challenging. Nonetheless, in
many cases, LLMs identify the base problem and provide partial solutions that
may be useful to CS students. Furthermore, our results may provide useful
guidance for teachers of advanced programming courses on how to design
programming assignments.

</details>


### [111] [Text-to-Decision Agent: Learning Generalist Policies from Natural Language Supervision](https://arxiv.org/abs/2504.15046)
*Shilin Zhang,Zican Hu,Wenhao Wu,Xinyi Xie,Jianxiang Tang,Chunlin Chen,Daoyi Dong,Yu Cheng,Zhenhong Sun,Zhi Wang*

Main category: cs.AI

TL;DR: 论文提出Text-to-Decision Agent (T2DA)框架，通过自然语言监督通用策略学习，实现零样本文本到决策生成。


<details>
  <summary>Details</summary>
Motivation: 传统RL系统依赖高质量样本或预热探索的任务推断，成本高且难以适用于未见任务。直接从任务描述文本学习是更通用的监督方式。

Method: 引入广义世界模型编码多任务决策数据，结合CLIP思想，通过对比语言-决策预训练缩小语义差距，对齐文本嵌入以理解环境动态。

Result: 在MuJoCo和Meta-World基准测试中，T2DA实现了零样本泛化，性能优于多种基线方法。

Conclusion: T2DA通过自然语言监督和对比学习，显著提升了RL系统的通用性和零样本能力。

Abstract: RL systems usually tackle generalization by inferring task beliefs from
high-quality samples or warmup explorations. The restricted form limits their
generality and usability since these supervision signals are expensive and even
infeasible to acquire in advance for unseen tasks. Learning directly from the
raw text about decision tasks is a promising alternative to leverage a much
broader source of supervision. In the paper, we propose Text-to-Decision Agent
(T2DA), a simple and scalable framework that supervises generalist policy
learning with natural language. We first introduce a generalized world model to
encode multi-task decision data into a dynamics-aware embedding space. Then,
inspired by CLIP, we predict which textual description goes with which decision
embedding, effectively bridging their semantic gap via contrastive
language-decision pre-training and aligning the text embeddings to comprehend
the environment dynamics. After training the text-conditioned generalist
policy, the agent can directly realize zero-shot text-to-decision generation in
response to language instructions. Comprehensive experiments on MuJoCo and
Meta-World benchmarks show that T2DA facilitates high-capacity zero-shot
generalization and outperforms various types of baselines.

</details>


### [112] [Mitigating Degree Bias in Graph Representation Learning with Learnable Structural Augmentation and Structural Self-Attention](https://arxiv.org/abs/2504.15075)
*Van Thuy Hoang,Hyeon-Ju Jeon,O-Joun Lee*

Main category: cs.AI

TL;DR: 论文提出DegFairGT，一种新型图神经网络，通过结构增强和自注意力机制解决图中节点度偏差问题。


<details>
  <summary>Details</summary>
Motivation: 传统GNN基于同质性假设，但在现实图中，高节点度主导消息传递，导致低节点度节点信息不足。

Method: DegFairGT通过可学习结构增强和结构自注意力，发现非相邻节点的结构相似性，生成信息边。

Result: 在六个数据集上，DegFairGT在度公平性分析、节点分类和聚类任务中优于现有方法。

Conclusion: DegFairGT有效缓解了度偏差，同时保持了图的全局结构。

Abstract: Graph Neural Networks (GNNs) update node representations through message
passing, which is primarily based on the homophily principle, assuming that
adjacent nodes share similar features. However, in real-world graphs with
long-tailed degree distributions, high-degree nodes dominate message passing,
causing a degree bias where low-degree nodes remain under-represented due to
inadequate messages. The main challenge in addressing degree bias is how to
discover non-adjacent nodes to provide additional messages to low-degree nodes
while reducing excessive messages for high-degree nodes. Nevertheless,
exploiting non-adjacent nodes to provide valuable messages is challenging, as
it could generate noisy information and disrupt the original graph structures.
To solve it, we propose a novel Degree Fairness Graph Transformer, named
DegFairGT, to mitigate degree bias by discovering structural similarities
between non-adjacent nodes through learnable structural augmentation and
structural self-attention. Our key idea is to exploit non-adjacent nodes with
similar roles in the same community to generate informative edges under our
augmentation, which could provide informative messages between nodes with
similar roles while ensuring that the homophily principle is maintained within
the community. To enable DegFairGT to learn such structural similarities, we
then propose a structural self-attention to capture the similarities between
node pairs. To preserve global graph structures and prevent graph augmentation
from hindering graph structure, we propose a Self-Supervised Learning task to
preserve p-step transition probability and regularize graph augmentation.
Extensive experiments on six datasets showed that DegFairGT outperformed
state-of-the-art baselines in degree fairness analysis, node classification,
and node clustering tasks.

</details>


### [113] [Contemplative Wisdom for Superalignment](https://arxiv.org/abs/2504.15125)
*Ruben Laukkonen,Fionn Inglis,Shamil Chandaria,Lars Sandved-Smith,Jakob Hohwy,Jonathan Gold,Adam Elwood*

Main category: cs.AI

TL;DR: 论文提出了一种基于内在道德原则的AI对齐方法，通过四个原则（正念、空性、不二、无限关怀）构建AI的智慧世界模型，并在AILuminate Benchmark上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统AI对齐策略可能无法应对不可预测的自我改进和复杂系统，因此需要一种更内在、更具弹性的方法。

Method: 设计基于四个原则（正念、空性、不二、无限关怀）的智慧世界模型，结合沉思架构、宪法和思维链强化。

Result: 在AILuminate Benchmark上，结合这些原则的AI表现更好，尤其是综合应用时。

Conclusion: 这种跨学科方法为AI对齐提供了自我修正和弹性的替代方案，适用于未来复杂系统。

Abstract: As artificial intelligence (AI) improves, traditional alignment strategies
may falter in the face of unpredictable self-improvement, hidden subgoals, and
the sheer complexity of intelligent systems. Rather than externally
constraining behavior, we advocate designing AI with intrinsic morality built
into its cognitive architecture and world model. Inspired by contemplative
wisdom traditions, we show how four axiomatic principles can instil a resilient
Wise World Model in AI systems. First, mindfulness enables self-monitoring and
recalibration of emergent subgoals. Second, emptiness forestalls dogmatic goal
fixation and relaxes rigid priors. Third, non-duality dissolves adversarial
self-other boundaries. Fourth, boundless care motivates the universal reduction
of suffering. We find that prompting AI to reflect on these principles improves
performance on the AILuminate Benchmark using GPT-4o, particularly when
combined. We offer detailed implementation strategies for state-of-the-art
models, including contemplative architectures, constitutions, and reinforcement
of chain-of-thought. For future systems, the active inference framework may
offer the self-organizing and dynamic coupling capabilities needed to enact
these insights in embodied agents. This interdisciplinary approach offers a
self-correcting and resilient alternative to prevailing brittle control
schemes.

</details>


### [114] [Behavioral Universe Network (BUN): A Behavioral Information-Based Framework for Complex Systems](https://arxiv.org/abs/2504.15146)
*Wei Zhou,Ailiya Borjigin,Cong He*

Main category: cs.AI

TL;DR: 论文提出了行为宇宙网络（BUN）框架，基于Agent-Interaction-Behavior（AIB）形式化方法，统一建模主体、客体和行为，提升多智能体系统的协调能力。


<details>
  <summary>Details</summary>
Motivation: 传统模型将主体与客体分离，缺乏统一的交互行为建模基础，无法适应现代数字生态系统的复杂性。

Method: 引入BUN框架，基于AIB形式化方法，将主体、客体和行为作为一等实体，共享行为信息库（BIB），利用信息驱动触发、语义增强和自适应规则协调多智能体系统。

Result: BUN框架实现了增强的行为分析、强适应性和跨领域互操作性。

Conclusion: BUN为下一代数字治理和智能应用提供了有前景的理论基础。

Abstract: Modern digital ecosystems feature complex, dynamic interactions among
autonomous entities across diverse domains. Traditional models often separate
agents and objects, lacking a unified foundation to capture their interactive
behaviors. This paper introduces the Behavioral Universe Network (BUN), a
theoretical framework grounded in the Agent-Interaction-Behavior (AIB)
formalism. BUN treats subjects (active agents), objects (resources), and
behaviors (operations) as first-class entities, all governed by a shared
Behavioral Information Base (BIB). We detail the AIB core concepts and
demonstrate how BUN leverages information-driven triggers, semantic enrichment,
and adaptive rules to coordinate multi-agent systems. We highlight key
benefits: enhanced behavior analysis, strong adaptability, and cross-domain
interoperability. We conclude by positioning BUN as a promising foundation for
next-generation digital governance and intelligent applications.

</details>


### [115] [Synergistic Weak-Strong Collaboration by Aligning Preferences](https://arxiv.org/abs/2504.15188)
*Yizhu Jiao,Xuchao Zhang,Zhaoyang Wang,Yubo Ma,Zhun Deng,Rujia Wang,Chetan Bansal,Saravan Rajmohan,Jiawei Han,Huaxiu Yao*

Main category: cs.AI

TL;DR: 提出了一种协作框架，将专用弱模型与通用强模型结合，以解决LLMs在专业任务中的不足。通过协作反馈优化弱模型，实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs在专业任务中表现不佳的问题，避免为每个细分领域微调大模型的高成本。

Method: 协作框架中，弱模型生成初稿和背景信息，强模型进行优化；引入协作反馈机制量化弱模型贡献并指导其偏好调整。

Result: 协作框架在三个领域实验中显著优于单独模型，且通过偏好调整进一步提升了性能。

Conclusion: 协作框架有效扩展了LLMs在专业任务中的应用，展示了互补优势的潜力。

Abstract: Current Large Language Models (LLMs) excel in general reasoning yet struggle
with specialized tasks requiring proprietary or domain-specific knowledge.
Fine-tuning large models for every niche application is often infeasible due to
black-box constraints and high computational overhead. To address this, we
propose a collaborative framework that pairs a specialized weak model with a
general strong model. The weak model, tailored to specific domains, produces
initial drafts and background information, while the strong model leverages its
advanced reasoning to refine these drafts, extending LLMs' capabilities to
critical yet specialized tasks. To optimize this collaboration, we introduce a
collaborative feedback to fine-tunes the weak model, which quantifies the
influence of the weak model's contributions in the collaboration procedure and
establishes preference pairs to guide preference tuning of the weak model. We
validate our framework through experiments on three domains. We find that the
collaboration significantly outperforms each model alone by leveraging
complementary strengths. Moreover, aligning the weak model with the
collaborative preference further enhances overall performance.

</details>


### [116] [Position: Bayesian Statistics Facilitates Stakeholder Participation in Evaluation of Generative AI](https://arxiv.org/abs/2504.15211)
*Yanan Long*

Main category: cs.AI

TL;DR: 论文提出使用贝叶斯统计作为评估生成式AI系统的框架，以解决现有方法在不确定性和社会影响方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有生成式AI评估方法依赖基准测试和点估计，无法捕捉不确定性和广泛社会影响，需要更全面的评估框架。

Method: 采用贝叶斯统计方法，整合领域专业知识，支持持续学习，并通过后验推断量化不确定性。

Result: 贝叶斯方法能够结合利益相关者视角，提升公平性、透明度和可靠性，并通过迭代工作流验证模型。

Conclusion: 贝叶斯统计为生成式AI评估提供了动态、稳健的框架，适用于现实世界的复杂场景。

Abstract: The evaluation of Generative AI (GenAI) systems plays a critical role in
public policy and decision-making, yet existing methods are often limited by
reliance on benchmark-driven, point-estimate comparisons that fail to capture
uncertainty and broader societal impacts. This paper argues for the use of
Bayesian statistics as a principled framework to address these challenges.
Bayesian methods enable the integration of domain expertise through prior
elicitation, allow for continuous learning from new data, and provide robust
uncertainty quantification via posterior inference. We demonstrate how Bayesian
inference can be applied to GenAI evaluation, particularly in incorporating
stakeholder perspectives to enhance fairness, transparency, and reliability.
Furthermore, we discuss Bayesian workflows as an iterative process for model
validation and refinement, ensuring robust assessments of GenAI systems in
dynamic, real-world contexts.

</details>


### [117] [A Self-Improving Coding Agent](https://arxiv.org/abs/2504.15228)
*Maxime Robeyns,Martin Szummer,Laurence Aitchison*

Main category: cs.AI

TL;DR: LLM编码代理通过自我编辑提升性能，在多个基准测试中表现显著提升。


<details>
  <summary>Details</summary>
Motivation: 探索LLM编码代理在自动化设计中的潜力，提供一种参考框架以优化工具使用和代理任务。

Method: 代理配备基本编码工具，通过自主编辑改进性能。

Result: 在SWE Bench Verified子集上性能提升17%至53%，在LiveCodeBench和合成基准测试中也有提升。

Conclusion: 该研究推动了自动化代理系统的设计，为LLM的后训练提供了实用框架。

Abstract: We demonstrate that an LLM coding agent, equipped with basic coding tools,
can autonomously edit itself, and thereby improve its performance on benchmark
tasks. We find performance gains from 17% to 53% on a random subset of SWE
Bench Verified, with additional performance gains on LiveCodeBench, as well as
synthetically generated agent benchmarks. Our work represents an advancement in
the automated and open-ended design of agentic systems, and provides a
reference agent framework for those seeking to post-train LLMs on tool use and
other agentic tasks.

</details>


### [118] [SuoiAI: Building a Dataset for Aquatic Invertebrates in Vietnam](https://arxiv.org/abs/2504.15252)
*Tue Vo,Lakshay Sharma,Tuan Dinh,Khuong Dinh,Trang Nguyen,Trung Phan,Minh Do,Duong Vu*

Main category: cs.AI

TL;DR: SuoiAI是一个端到端流程，用于构建越南水生无脊椎动物数据集并利用机器学习进行物种分类。


<details>
  <summary>Details</summary>
Motivation: 理解和监测水生生物多样性对生态健康和保育至关重要。

Method: 通过半监督学习减少标注工作量，并利用先进的物体检测和分类模型进行数据收集、标注和模型训练。

Result: 旨在解决数据稀缺、细粒度分类及多样化环境条件下的部署挑战。

Conclusion: SuoiAI为水生生物多样性监测提供了一种高效且可扩展的解决方案。

Abstract: Understanding and monitoring aquatic biodiversity is critical for ecological
health and conservation efforts. This paper proposes SuoiAI, an end-to-end
pipeline for building a dataset of aquatic invertebrates in Vietnam and
employing machine learning (ML) techniques for species classification. We
outline the methods for data collection, annotation, and model training,
focusing on reducing annotation effort through semi-supervised learning and
leveraging state-of-the-art object detection and classification models. Our
approach aims to overcome challenges such as data scarcity, fine-grained
classification, and deployment in diverse environmental conditions.

</details>


### [119] [FlowReasoner: Reinforcing Query-Level Meta-Agents](https://arxiv.org/abs/2504.15257)
*Hongcheng Gao,Yue Liu,Yufei He,Longxu Dou,Chao Du,Zhijie Deng,Bryan Hooi,Min Lin,Tianyu Pang*

Main category: cs.AI

TL;DR: FlowReasoner是一个查询级元代理，通过外部执行反馈自动化设计多代理系统，结合强化学习优化性能、复杂度和效率。


<details>
  <summary>Details</summary>
Motivation: 自动化设计针对每个用户查询的多代理系统，提升个性化和效率。

Method: 结合DeepSeek R1的推理能力和强化学习，通过多目标奖励优化训练。

Result: 在工程和竞赛代码基准测试中表现优异，准确率超过o1-mini 10.52%。

Conclusion: FlowReasoner能有效生成个性化多代理系统，性能显著优于现有方法。

Abstract: This paper proposes a query-level meta-agent named FlowReasoner to automate
the design of query-level multi-agent systems, i.e., one system per user query.
Our core idea is to incentivize a reasoning-based meta-agent via external
execution feedback. Concretely, by distilling DeepSeek R1, we first endow the
basic reasoning ability regarding the generation of multi-agent systems to
FlowReasoner. Then, we further enhance it via reinforcement learning (RL) with
external execution feedback. A multi-purpose reward is designed to guide the RL
training from aspects of performance, complexity, and efficiency. In this
manner, FlowReasoner is enabled to generate a personalized multi-agent system
for each user query via deliberative reasoning. Experiments on both engineering
and competition code benchmarks demonstrate the superiority of FlowReasoner.
Remarkably, it surpasses o1-mini by 10.52% accuracy across three benchmarks.
The code is available at https://github.com/sail-sg/FlowReasoner.

</details>


### [120] [Leveraging Language Models for Automated Patient Record Linkage](https://arxiv.org/abs/2504.15261)
*Mohammad Beheshti,Lovedeep Gondara,Iris Zachary*

Main category: cs.AI

TL;DR: 研究探讨了利用语言模型（如RoBERTa和Mistral）自动链接患者记录的可行性，结果显示微调模型在阻塞和匹配任务中表现优异，但混合方法仍更高效。


<details>
  <summary>Details</summary>
Motivation: 医疗数据碎片化导致患者记录链接困难，需要自动化解决方案以整合数据。

Method: 使用真实医疗数据，通过微调RoBERTa进行阻塞任务，并测试多种语言模型（如Mistral）在匹配任务中的表现。

Result: 微调模型在阻塞任务中减少92%候选对，匹配任务中Mistral-7B表现最佳；零样本模型中Mistral-Small-24B最优。

Conclusion: 语言模型为患者记录链接提供了高效、可扩展的解决方案，但混合方法在准确性和效率上仍具优势。

Abstract: Objective: Healthcare data fragmentation presents a major challenge for
linking patient data, necessitating robust record linkage to integrate patient
records from diverse sources. This study investigates the feasibility of
leveraging language models for automated patient record linkage, focusing on
two key tasks: blocking and matching. Materials and Methods: We utilized
real-world healthcare data from the Missouri Cancer Registry and Research
Center, linking patient records from two independent sources using
probabilistic linkage as a baseline. A transformer-based model, RoBERTa, was
fine-tuned for blocking using sentence embeddings. For matching, several
language models were experimented under fine-tuned and zero-shot settings,
assessing their performance against ground truth labels. Results: The
fine-tuned blocking model achieved a 92% reduction in the number of candidate
pairs while maintaining near-perfect recall. In the matching task, fine-tuned
Mistral-7B achieved the best performance with only 6 incorrect predictions.
Among zero-shot models, Mistral-Small-24B performed best, with a total of 55
incorrect predictions. Discussion: Fine-tuned language models achieved strong
performance in patient record blocking and matching with minimal errors.
However, they remain less accurate and efficient than a hybrid rule-based and
probabilistic approach for blocking. Additionally, reasoning models like
DeepSeek-R1 are impractical for large-scale record linkage due to high
computational costs. Conclusion: This study highlights the potential of
language models for automating patient record linkage, offering improved
efficiency by eliminating the manual efforts required to perform patient record
linkage. Overall, language models offer a scalable solution that can enhance
data integration, reduce manual effort, and support disease surveillance and
research.

</details>


### [121] [Stop Summation: Min-Form Credit Assignment Is All Process Reward Model Needs for Reasoning](https://arxiv.org/abs/2504.15275)
*Jie Cheng,Ruixi Qiao,Lijun Li,Chao Guo,Junle Wang,Gang Xiong,Yisheng Lv,Fei-Yue Wang*

Main category: cs.AI

TL;DR: 论文提出PURE方法，通过最小化未来奖励的信用分配形式，解决了PRM在强化微调中的奖励黑客问题，显著提升了推理性能。


<details>
  <summary>Details</summary>
Motivation: PRM在强化微调中存在奖励黑客问题，限制了其应用。论文旨在解决这一问题。

Method: 提出PURE方法，采用最小化未来奖励的信用分配形式，替代传统的累加形式。

Result: PURE在30%步骤内达到与可验证奖励方法相当的推理性能，并在补充少量可验证奖励后取得最佳微调模型。

Conclusion: PURE有效缓解奖励黑客问题，提升了模型性能，为PRM的应用提供了新思路。

Abstract: Process reward models (PRMs) have proven effective for test-time scaling of
Large Language Models (LLMs) on challenging reasoning tasks. However, reward
hacking issues with PRMs limit their successful application in reinforcement
fine-tuning. In this paper, we identify the main cause of PRM-induced reward
hacking: the canonical summation-form credit assignment in reinforcement
learning (RL), which defines the value as cumulative gamma-decayed future
rewards, easily induces LLMs to hack steps with high rewards. To address this,
we propose PURE: Process sUpervised Reinforcement lEarning. The key innovation
of PURE is a min-form credit assignment that formulates the value function as
the minimum of future rewards. This method significantly alleviates reward
hacking by limiting the value function range and distributing advantages more
reasonably. Through extensive experiments on 3 base models, we show that
PRM-based approaches enabling min-form credit assignment achieve comparable
reasoning performance to verifiable reward-based methods within only 30% steps.
In contrast, the canonical sum-form credit assignment collapses training even
at the beginning! Additionally, when we supplement PRM-based fine-tuning with
just 10% verifiable rewards, we further alleviate reward hacking and produce
the best fine-tuned model based on Qwen2.5-Math-7B in our experiments,
achieving 82.5% accuracy on AMC23 and 53.3% average accuracy across 5
benchmarks. Moreover, we summarize the observed reward hacking cases and
analyze the causes of training collapse. Code and models are available at
https://github.com/CJReinforce/PURE.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [122] [A discrete physics-informed training for projection-based reduced order models with neural networks](https://arxiv.org/abs/2504.13875)
*N. Sibuet,S. Ares de Parga,J. R. Bravo,R. Rossi*

Main category: cs.LG

TL;DR: 本文提出了一种基于物理信息的训练框架，用于投影降阶模型（ROMs），通过结合FEM残差损失改进PROM-ANN架构，填补了传统ROM与物理信息神经网络（PINNs）之间的差距。


<details>
  <summary>Details</summary>
Motivation: 传统PINNs依赖解析PDE，而本文利用FEM残差指导ROM近似流形的学习，以解决非线性问题并提升精度。

Method: 扩展PROM-ANN架构，引入离散物理信息残差损失，适用于非线性问题；改进架构以处理快速衰减奇异值；并通过实验验证物理信息训练过程。

Result: 在非线性超弹性问题中，改进的PROM-ANN在重建精度上显著优于POD，且残差损失在非线性问题中表现良好。

Conclusion: FEM残差在ROM构建中至关重要，未来需探索更多架构以进一步释放残差驱动优化的潜力。

Abstract: This paper presents a physics-informed training framework for
projection-based Reduced Order Models (ROMs). We extend the PROM-ANN
architecture by complementing snapshot-based training with a FEM-based,
discrete physics-informed residual loss, bridging the gap between traditional
projection-based ROMs and physics-informed neural networks (PINNs). Unlike
conventional PINNs that rely on analytical PDEs, our approach leverages FEM
residuals to guide the learning of the ROM approximation manifold. Key
contributions include: (1) a parameter-agnostic, discrete residual loss
applicable to non-linear problems, (2) an architectural modification to
PROM-ANN improving accuracy for fast-decaying singular values, and (3) an
empirical study on the proposed physics informed training process for ROMs.
  The method is demonstrated on a non-linear hyperelasticity problem,
simulating a rubber cantilever under multi-axial loads. The main accomplishment
in regards to the proposed residual-based loss is its applicability on
non-linear problems by interfacing with FEM software while maintaining
reasonable training times. The modified PROM-ANN outperforms POD by orders of
magnitude in snapshot reconstruction accuracy, while the original formulation
is not able to learn a proper mapping for this use-case. Finally, the
application of physics informed training in ANN-PROM modestly narrows the gap
between data reconstruction and ROM accuracy, however it highlights the
untapped potential of the proposed residual-driven optimization for future ROM
development. This work underscores the critical role of FEM residuals in ROM
construction and calls for further exploration on architectures beyond
PROM-ANN.

</details>


### [123] [Ising Models with Hidden Markov Structure: Applications to Probabilistic Inference in Machine Learning](https://arxiv.org/abs/2504.13927)
*F. Herrera,U. A. Rozikov,M. V. Velasco*

Main category: cs.LG

TL;DR: 论文研究了包含隐藏自旋的哈密顿量，探讨了在Cayley树上的平移不变Gibbs测度，证明了在特定条件下存在三种不同的测度，适用于机器学习的层次数据推断。


<details>
  <summary>Details</summary>
Motivation: 研究目的是探索结合隐藏自旋和数据依赖项的哈密顿量，为机器学习中的层次数据推断提供理论支持。

Method: 通过分析Cayley树上的平移不变Gibbs测度，研究哈密顿量的性质及其在特定参数条件下的行为。

Result: 在特定条件下，证明了存在三种不同的平移不变Gibbs测度，这些测度代表了自旋系统的平衡状态。

Conclusion: 这些测度为层次数据推断提供了结构化方法，适用于去噪、弱监督学习和异常检测等任务，Cayley树结构因其易处理性而具有优势。

Abstract: In this paper, we investigate a Hamiltonian that incorporates Ising
interactions between hidden $\pm 1$ spins, alongside a data-dependent term that
couples the hidden and observed variables. Specifically, we explore
translation-invariant Gibbs measures (TIGM) of this Hamiltonian on Cayley
trees.
  Under certain explicit conditions on the model's parameters, we demonstrate
that there can be up to three distinct TIGMs. Each of these measures represents
an equilibrium state of the spin system. These measures provide a structured
approach to inference on hierarchical data in machine learning. They have
practical applications in tasks such as denoising, weakly supervised learning,
and anomaly detection. The Cayley tree structure is particularly advantageous
for exact inference due to its tractability.

</details>


### [124] [Enhancing Ultra-Low-Bit Quantization of Large Language Models Through Saliency-Aware Partial Retraining](https://arxiv.org/abs/2504.13932)
*Deyu Cao,Samin Aref*

Main category: cs.LG

TL;DR: 论文提出了一种改进的极低比特量化方法，基于ApiQ方法，通过引入显著性感知正则化项，无需完全重新训练即可提升量化模型的精度。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的计算和存储需求高，量化方法虽能压缩模型，但现有方法在极低比特量化时难以平衡精度和效率。

Method: 结合ApiQ的部分训练与显著性感知正则化，优先保留对模型性能影响最大的参数。

Result: 在LLaMA系列模型上实验表明，该方法提升了量化模型的精度，缩小了与全精度模型的差距，且开销极小。

Conclusion: 该方法为极低比特量化提供了高效解决方案，未来将公开以促进相关研究。

Abstract: Large language models offer remarkable capabilities, but their size and
computational demands pose practical challenges. Quantization methods compress
their size through replacing their high-precision parameters by quantized
values of lower precision. Post-training quantization reduces model size
efficiently at the cost of decreased accuracy, while quantization-aware
training better preserves accuracy but is resource-intensive. Among existing
post-training quantization algorithms, the ApiQ method achieves superior
accuracy preservation at minimal memory and time overhead. We investigate two
ideas to extend performance in ultra-low-bit quantization beyond ApiQ's level.
First, we look into combining existing quantization-aware training techniques
with ApiQ's partial training. We show that this does not outperform the
baseline ApiQ method with limited training data and frozen weights. This leads
to two key insights: (1) The substantial representational capacity that is
gained through full retraining may not be feasible through partial training.
(2) This gain seems to depend on using a large and diverse dataset in
quantization-aware training. Second, through a novel approach informed by the
two insights, we propose an ultra-low-bit quantization method that builds upon
ApiQ and extends its performance without the need for full retraining. It
relies on a saliency-aware regularization term that prioritizes preserving the
most impactful parameters during quantization. Our experiments on benchmark
language models from the LLaMA family show that our proposed approach boosts
accuracy and tightens the gap between the quantized model and the
full-precision model, with minimal overhead. Our method will be made publicly
available to facilitate future developments in ultra-low-bit quantization of
large language models.

</details>


### [125] [NEMOTRON-CROSSTHINK: Scaling Self-Learning beyond Math Reasoning](https://arxiv.org/abs/2504.13941)
*Syeda Nahida Akter,Shrimai Prabhumoye,Matvei Novikov,Seungju Han,Ying Lin,Evelina Bakhturi,Eric Nyberg,Yejin Choi,Mostofa Patwary,Mohammad Shoeybi,Bryan Catanzaro*

Main category: cs.LG

TL;DR: NEMOTRON-CROSSTHINK框架通过多领域数据整合和结构化模板，提升LLMs在多样化推理任务中的泛化能力，显著提高了数学和非数学任务的准确性及响应效率。


<details>
  <summary>Details</summary>
Motivation: 现有RL方法在数学推理中表现良好，但在更广泛的推理领域因数据有限、奖励结构不明确和任务多样性而难以推广。

Method: 提出NEMOTRON-CROSSTHINK框架，整合多领域数据（STEM、人文、社科等），采用结构化模板控制答案复杂度，筛选可验证答案，并优化数据混合策略。

Result: 在数学（MATH-500、AMC23）和非数学（MMLU-PRO、GPQA-DIAMOND等）基准测试中准确率显著提升，同时响应效率提高28%。

Conclusion: 多领域、多格式数据整合能显著提升LLMs的准确性、效率和泛化能力。

Abstract: Large Language Models (LLMs) have shown strong reasoning capabilities,
particularly when enhanced through Reinforcement Learning (RL). While prior
work has successfully applied RL to mathematical reasoning -- where rules and
correctness are well-defined -- generalizing these methods to broader reasoning
domains remains challenging due to limited data, the lack of verifiable reward
structures, and diverse task requirements. In this work, we propose
NEMOTRON-CROSSTHINK, a framework that systematically incorporates multi-domain
corpora, including both synthetic and real-world question-answer pairs, into RL
training to improve generalization across diverse reasoning tasks.
NEMOTRON-CROSSTHINK addresses key challenges by (1) incorporating data from
varied sources spanning STEM, humanities, social sciences, etc.; (2) applying
structured templates (e.g., multiple-choice and open-ended) to control
answer-space complexity; (3) filtering for verifiable answers; and (4)
optimizing data blending strategies that utilizes data from multiple sources
effectively. Our approach enables scalable and verifiable reward modeling
beyond mathematics and demonstrates improved accuracies on both math (MATH-500:
+30.1%, AMC23:+27.5%) and non-math reasoning benchmarks (MMLU-PRO: +12.8%,
GPQA-DIAMOND: +11.3%, AGIEVAL: +15.1%, SUPERGPQA: +3.8%). Moreover,
NEMOTRON-CROSSTHINK exhibits significantly improved response efficiency --
using 28% fewer tokens for correct answers -- highlighting more focused and
effective reasoning. Through NEMOTRON-CROSSTHINK, we demonstrate that
integrating multi-domain, multi-format data in RL leads to more accurate,
efficient, and generalizable LLMs.

</details>


### [126] [Evaluating Menu OCR and Translation: A Benchmark for Aligning Human and Automated Evaluations in Large Vision-Language Models](https://arxiv.org/abs/2504.13945)
*Zhanglin Wu,Tengfei Song,Ning Xie,Weidong Zhang,Mengli Zhu,Shuang Wu,Shiliang Sun,Hao Yang*

Main category: cs.LG

TL;DR: 本文提出了MOTBench，一个专注于菜单翻译评估的框架，填补了现有大视觉语言模型（LVLMs）在复杂布局长文本理解能力评估上的空白。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法（如OCRBench）主要关注短文本或简单布局的长文本，而复杂布局的长文本理解能力评估被忽视，尤其在跨文化菜单翻译中至关重要。

Method: 提出MOTBench，基于中英文菜单数据集，要求LVLMs准确识别并翻译菜品、价格和单位，评估其视觉理解和语言处理能力。

Result: 实验显示自动评估结果与人工评估高度一致，并分析了现有LVLMs的性能优劣。

Conclusion: MOTBench为LVLMs的发展提供了有价值的指导，未来可推动其在复杂布局文本理解上的进步。

Abstract: The rapid advancement of large vision-language models (LVLMs) has
significantly propelled applications in document understanding, particularly in
optical character recognition (OCR) and multilingual translation. However,
current evaluations of LVLMs, like the widely used OCRBench, mainly focus on
verifying the correctness of their short-text responses and long-text responses
with simple layout, while the evaluation of their ability to understand long
texts with complex layout design is highly significant but largely overlooked.
In this paper, we propose Menu OCR and Translation Benchmark (MOTBench), a
specialized evaluation framework emphasizing the pivotal role of menu
translation in cross-cultural communication. MOTBench requires LVLMs to
accurately recognize and translate each dish, along with its price and unit
items on a menu, providing a comprehensive assessment of their visual
understanding and language processing capabilities. Our benchmark is comprised
of a collection of Chinese and English menus, characterized by intricate
layouts, a variety of fonts, and culturally specific elements across different
languages, along with precise human annotations. Experiments show that our
automatic evaluation results are highly consistent with professional human
evaluation. We evaluate a range of publicly available state-of-the-art LVLMs,
and through analyzing their output to identify the strengths and weaknesses in
their performance, offering valuable insights to guide future advancements in
LVLM development. MOTBench is available at https://github.com/gitwzl/MOTBench.

</details>


### [127] [On Revealing the Hidden Problem Structure in Real-World and Theoretical Problems Using Walsh Coefficient Influence](https://arxiv.org/abs/2504.13949)
*M. W. Przewozniczek,F. Chicano,R. Tinós,J. Nalepa,B. Ruszczak,A. M. Wijata*

Main category: cs.LG

TL;DR: 论文提出了一种基于Walsh分解的灰盒优化方法，通过构建加权动态变量交互图（wdVIG）来过滤噪声依赖，提升优化效果。


<details>
  <summary>Details</summary>
Motivation: 在灰盒优化中，变量间的非线性依赖关系可能因噪声而无关优化，导致传统掩码失效。

Method: 扩展Walsh分解，测量变量依赖强度，构建wdVIG以动态调整依赖关系并过滤噪声。

Result: 在噪声问题中，wdVIG掩码显著提升优化效果；若无噪声，其表现与传统方法相当。

Conclusion: wdVIG能有效识别噪声依赖，为优化器提供更可靠的变量交互信息。

Abstract: Gray-box optimization employs Walsh decomposition to obtain non-linear
variable dependencies and utilize them to propose masks of variables that have
a joint non-linear influence on fitness value. These masks significantly
improve the effectiveness of variation operators. In some problems, all
variables are non-linearly dependent, making the aforementioned masks useless.
We analyze the features of the real-world instances of such problems and show
that many of their dependencies may have noise-like origins. Such noise-caused
dependencies are irrelevant to the optimization process and can be ignored. To
identify them, we propose extending the use of Walsh decomposition by measuring
variable dependency strength that allows the construction of the weighted
dynamic Variable Interaction Graph (wdVIG). wdVIGs adjust the dependency
strength to mixed individuals. They allow the filtering of irrelevant
dependencies and re-enable using dependency-based masks by variation operators.
We verify the wdVIG potential on a large benchmark suite. For problems with
noise, the wdVIG masks can improve the optimizer's effectiveness. If all
dependencies are relevant for the optimization, i.e., the problem is not
noised, the influence of wdVIG masks is similar to that of state-of-the-art
structures of this kind.

</details>


### [128] [Open-Medical-R1: How to Choose Data for RLVR Training at Medicine Domain](https://arxiv.org/abs/2504.13950)
*Zhongxi Qiu,Zhang Zhang,Yan Hu,Heng Li,Jiang Liu*

Main category: cs.LG

TL;DR: 论文研究了在医学领域中为强化学习与验证奖励（RLVR）训练优化的数据选择策略，发现过滤后的数据训练效果优于随机采样。


<details>
  <summary>Details</summary>
Motivation: RLVR在数学和逻辑谜题中表现优异，但在医学等专业领域的应用尚未充分探索，因此研究如何优化数据选择策略。

Method: 使用四种数据采样策略（随机采样及三种模型过滤）在MedQA-USMLE数据集上训练Gemma-3-12b-it模型，并采用GRPO优化。

Result: 过滤数据训练的模型表现更优，但自过滤数据在医学领域表现最佳而泛化性较差，大模型过滤则整体更稳健。

Conclusion: 数据选择策略对RLVR在专业领域的性能至关重要，需权衡领域表现与泛化能力。

Abstract: This paper explores optimal data selection strategies for Reinforcement
Learning with Verified Rewards (RLVR) training in the medical domain. While
RLVR has shown exceptional potential for enhancing reasoning capabilities in
large language models, most prior implementations have focused on mathematics
and logical puzzles, with limited exploration of domain-specific applications
like medicine. We investigate four distinct data sampling strategies from
MedQA-USMLE: random sampling (baseline), and filtering using Phi-4,
Gemma-3-27b-it, and Gemma-3-12b-it models. Using Gemma-3-12b-it as our base
model and implementing Group Relative Policy Optimization (GRPO), we evaluate
performance across multiple benchmarks including MMLU, GSM8K, MMLU-Pro, and
CMMLU. Our findings demonstrate that models trained on filtered data generally
outperform those trained on randomly selected samples. Notably, training on
self-filtered samples (using Gemma-3-12b-it for filtering) achieved superior
performance in medical domains but showed reduced robustness across different
benchmarks, while filtering with larger models from the same series yielded
better overall robustness. These results provide valuable insights into
effective data organization strategies for RLVR in specialized domains and
highlight the importance of thoughtful data selection in achieving optimal
performance. You can access our repository
(https://github.com/Qsingle/open-medical-r1) to get the codes.

</details>


### [129] [Generative System Dynamics in Recurrent Neural Networks](https://arxiv.org/abs/2504.13951)
*Michele Casoni,Tommaso Guidi,Alessandro Betti,Stefano Melacci,Marco Gori*

Main category: cs.LG

TL;DR: 研究了RNN在非线性激活函数下的连续时间动力学，发现斜对称权重矩阵是实现稳定极限环的关键，双曲正切类激活函数能保持振荡动态。


<details>
  <summary>Details</summary>
Motivation: 探究RNN在非线性激活函数下如何实现持续的振荡行为，避免收敛到静态固定点。

Method: 通过理论分析和数值模拟，验证斜对称权重矩阵和双曲正切类激活函数的作用。

Result: 斜对称权重矩阵和特定激活函数能维持极限环，并提升数值稳定性。

Conclusion: 研究为设计具有复杂时间依赖性的RNN架构提供了实用指导。

Abstract: In this study, we investigate the continuous time dynamics of Recurrent
Neural Networks (RNNs), focusing on systems with nonlinear activation
functions. The objective of this work is to identify conditions under which
RNNs exhibit perpetual oscillatory behavior, without converging to static fixed
points. We establish that skew-symmetric weight matrices are fundamental to
enable stable limit cycles in both linear and nonlinear configurations. We
further demonstrate that hyperbolic tangent-like activation functions (odd,
bounded, and continuous) preserve these oscillatory dynamics by ensuring motion
invariants in state space. Numerical simulations showcase how nonlinear
activation functions not only maintain limit cycles, but also enhance the
numerical stability of the system integration process, mitigating those
instabilities that are commonly associated with the forward Euler method. The
experimental results of this analysis highlight practical considerations for
designing neural architectures capable of capturing complex temporal
dependencies, i.e., strategies for enhancing memorization skills in recurrent
models.

</details>


### [130] [Prognosis Of Lithium-Ion Battery Health with Hybrid EKF-CNN+LSTM Model Using Differential Capacity](https://arxiv.org/abs/2504.13956)
*Md Azizul Hoque,Babul Salam,Mohd Khair Hassan,Abdulkabir Aliyu,Abedalmuhdi Almomany,Muhammed Sutcu*

Main category: cs.LG

TL;DR: 该论文提出了一种电池退化测试模型，结合DCA和多种充放电速率，评估锂离子电池内部退化机制，并通过EKF、CNN和LSTM验证数据。结果表明LiFePO4电池性能优于LiNiCoAlO2。


<details>
  <summary>Details</summary>
Motivation: 电池退化是电动汽车和储能系统的关键问题，现有研究多关注SOC估计，而忽略内部退化机制。

Method: 使用两种锂离子电池（LiNiCoAlO2和LiFePO4），在不同充放电速率下进行测试，结合EKF、CNN和LSTM验证数据，并采用PIM技术分析电池健康状态。

Result: 模型误差低于0.001%，LiFePO4电池在多种负载条件下表现更稳定，而LiNiCoAlO2在快速负载下退化更快。

Conclusion: LiFePO4电池性能更优，PIM技术有效揭示电池退化机制，为电池健康管理提供新方法。

Abstract: Battery degradation is a major challenge in electric vehicles (EV) and energy
storage systems (ESS). However, most degradation investigations focus mainly on
estimating the state of charge (SOC), which fails to accurately interpret the
cells' internal degradation mechanisms. Differential capacity analysis (DCA)
focuses on the rate of change of cell voltage about the change in cell
capacity, under various charge/discharge rates. This paper developed a battery
cell degradation testing model that used two types of lithium-ions (Li-ion)
battery cells, namely lithium nickel cobalt aluminium oxides (LiNiCoAlO2) and
lithium iron phosphate (LiFePO4), to evaluate internal degradation during
loading conditions. The proposed battery degradation model contains distinct
charge rates (DCR) of 0.2C, 0.5C, 1C, and 1.5C, as well as discharge rates
(DDR) of 0.5C, 0.9C, 1.3C, and 1.6C to analyze the internal health and
performance of battery cells during slow, moderate, and fast loading
conditions. Besides, this research proposed a model that incorporates the
Extended Kalman Filter (EKF), Convolutional Neural Network (CNN), and Long
Short-Term Memory (LSTM) networks to validate experimental data. The proposed
model yields excellent modelling results based on mean squared error (MSE), and
root mean squared error (RMSE), with errors of less than 0.001% at DCR and DDR.
The peak identification technique (PIM) has been utilized to investigate
battery health based on the number of peaks, peak position, peak height, peak
area, and peak width. At last, the PIM method has discovered that the cell aged
gradually under normal loading rates but deteriorated rapidly under fast
loading conditions. Overall, LiFePO4 batteries perform more robustly and
consistently than (LiNiCoAlO2) cells under varying loading conditions.

</details>


### [131] [ToolRL: Reward is All Tool Learning Needs](https://arxiv.org/abs/2504.13958)
*Cheng Qian,Emre Can Acikgoz,Qi He,Hongru Wang,Xiusi Chen,Dilek Hakkani-Tür,Gokhan Tur,Heng Ji*

Main category: cs.LG

TL;DR: 论文研究了强化学习中奖励设计对LLM工具使用能力的影响，提出了一种新的奖励设计方法，并在实验中取得了显著提升。


<details>
  <summary>Details</summary>
Motivation: 当前LLM通过监督微调（SFT）学习工具使用能力，但难以泛化到复杂场景。强化学习（RL）虽有潜力，但奖励设计存在挑战。

Method: 系统研究了奖励策略的类型、规模、粒度和时间动态，提出了一种针对工具使用任务的奖励设计方法，并采用GRPO训练LLM。

Result: 实验表明，该方法在多个基准测试中表现优异，比基础模型提升17%，比SFT模型提升15%。

Conclusion: 奖励设计对提升LLM工具使用能力和泛化性能至关重要，研究代码已开源。

Abstract: Current Large Language Models (LLMs) often undergo supervised fine-tuning
(SFT) to acquire tool use capabilities. However, SFT struggles to generalize to
unfamiliar or complex tool use scenarios. Recent advancements in reinforcement
learning (RL), particularly with R1-like models, have demonstrated promising
reasoning and generalization abilities. Yet, reward design for tool use
presents unique challenges: multiple tools may be invoked with diverse
parameters, and coarse-grained reward signals, such as answer matching, fail to
offer the finegrained feedback required for effective learning. In this work,
we present the first comprehensive study on reward design for tool selection
and application tasks within the RL paradigm. We systematically explore a wide
range of reward strategies, analyzing their types, scales, granularity, and
temporal dynamics. Building on these insights, we propose a principled reward
design tailored for tool use tasks and apply it to train LLMs using Group
Relative Policy Optimization (GRPO). Empirical evaluations across diverse
benchmarks demonstrate that our approach yields robust, scalable, and stable
training, achieving a 17% improvement over base models and a 15% gain over SFT
models. These results highlight the critical role of thoughtful reward design
in enhancing the tool use capabilities and generalization performance of LLMs.
All the codes are released to facilitate future research.

</details>


### [132] [CONTINA: Confidence Interval for Traffic Demand Prediction with Coverage Guarantee](https://arxiv.org/abs/2504.13961)
*Chao Yang,Xiannan Huang,Shuhan Qiu,Yan Cheng*

Main category: cs.LG

TL;DR: 提出了一种名为CONTINA的方法，用于动态调整交通需求预测的置信区间，以适应外部变化，并保证覆盖率的收敛。


<details>
  <summary>Details</summary>
Motivation: 现有置信区间建模方法依赖严格假设，在变化的交通环境中可能失效，因此需要一种能自适应调整的方法。

Method: 通过收集部署期间的区间误差动态调整置信区间，理论上证明其覆盖率收敛于目标水平。

Result: 在四个真实数据集和预测模型上的实验表明，该方法能提供更短且有效的置信区间。

Conclusion: CONTINA方法能帮助交通管理人员制定更合理和稳健的运营计划，代码和数据集已开源。

Abstract: Accurate short-term traffic demand prediction is critical for the operation
of traffic systems. Besides point estimation, the confidence interval of the
prediction is also of great importance. Many models for traffic operations,
such as shared bike rebalancing and taxi dispatching, take into account the
uncertainty of future demand and require confidence intervals as the input.
However, existing methods for confidence interval modeling rely on strict
assumptions, such as unchanging traffic patterns and correct model
specifications, to guarantee enough coverage. Therefore, the confidence
intervals provided could be invalid, especially in a changing traffic
environment. To fill this gap, we propose an efficient method, CONTINA
(Conformal Traffic Intervals with Adaptation) to provide interval predictions
that can adapt to external changes. By collecting the errors of interval during
deployment, the method can adjust the interval in the next step by widening it
if the errors are too large or shortening it otherwise. Furthermore, we
theoretically prove that the coverage of the confidence intervals provided by
our method converges to the target coverage level. Experiments across four
real-world datasets and prediction models demonstrate that the proposed method
can provide valid confidence intervals with shorter lengths. Our method can
help traffic management personnel develop a more reasonable and robust
operation plan in practice. And we release the code, model and dataset in
\href{ https://github.com/xiannanhuang/CONTINA/}{ Github}.

</details>


### [133] [Adversarial Resilience against Clean-Label Attacks in Realizable and Noisy Settings](https://arxiv.org/abs/2504.13966)
*Carolin Heinzler*

Main category: cs.LG

TL;DR: 论文研究了在包含未知数量干净标签对抗样本的i.i.d.数据流中，如何建立随机性保证的挑战。学习者可以因不确定而放弃预测，其遗憾通过误分类和放弃错误衡量。方法基于Goel等人的工作，但修正了其论证中的不准确性，并扩展到不可知设置。


<details>
  <summary>Details</summary>
Motivation: 解决在对抗样本存在时，如何保证学习器的性能，并扩展到更广泛的不可知设置。

Method: 基于Goel等人的方法，修正其论证，并引入干净标签对抗者的概念，分析阈值学习器在噪声下的表现。

Result: 首次在不可知设置下，对阈值学习器在干净标签对抗者存在时的理论分析。

Conclusion: 该方法在对抗样本存在时提供了理论保证，并扩展到不可知设置，但仍限于某些假设空间。

Abstract: We investigate the challenge of establishing stochastic-like guarantees when
sequentially learning from a stream of i.i.d. data that includes an unknown
quantity of clean-label adversarial samples. We permit the learner to abstain
from making predictions when uncertain. The regret of the learner is measured
in terms of misclassification and abstention error, where we allow the learner
to abstain for free on adversarial injected samples. This approach is based on
the work of Goel, Hanneke, Moran, and Shetty from arXiv:2306.13119. We explore
the methods they present and manage to correct inaccuracies in their
argumentation.
  However, this approach is limited to the realizable setting, where labels are
assigned according to some function $f^*$ from the hypothesis space
$\mathcal{F}$. Based on similar arguments, we explore methods to make
adaptations for the agnostic setting where labels are random. Introducing the
notion of a clean-label adversary in the agnostic context, we are the first to
give a theoretical analysis of a disagreement-based learner for thresholds,
subject to a clean-label adversary with noise.

</details>


### [134] [Enhancing Stroke Diagnosis in the Brain Using a Weighted Deep Learning Approach](https://arxiv.org/abs/2504.13974)
*Yao Zhiwan,Reza Zarrab,Jean Dubois*

Main category: cs.LG

TL;DR: 提出了一种加权投票集成（WVE）机器学习模型，用于高效预测脑卒中，准确率达94.91%。


<details>
  <summary>Details</summary>
Motivation: 传统脑卒中诊断方法（如CT和MRI）成本高且耗时，需要更高效的预测手段。

Method: 结合随机森林、深度学习和基于直方图的梯度提升分类器的预测，构建加权投票集成模型。

Result: 在私有数据集上达到94.91%的准确率，支持早期风险评估和预防。

Conclusion: 未来研究可探索优化技术以进一步提升模型准确性。

Abstract: A brain stroke occurs when blood flow to a part of the brain is disrupted,
leading to cell death. Traditional stroke diagnosis methods, such as CT scans
and MRIs, are costly and time-consuming. This study proposes a weighted voting
ensemble (WVE) machine learning model that combines predictions from
classifiers like random forest, Deep Learning, and histogram-based gradient
boosting to predict strokes more effectively. The model achieved 94.91%
accuracy on a private dataset, enabling early risk assessment and prevention.
Future research could explore optimization techniques to further enhance
accuracy.

</details>


### [135] [Multiscale Tensor Summation Factorization as a New Neural Network Layer (MTS Layer) for Multidimensional Data Processing](https://arxiv.org/abs/2504.13975)
*Mehmet Yamaç,Muhammad Numan Yousaf,Serkan Kiranyaz,Moncef Gabbouj*

Main category: cs.LG

TL;DR: 论文提出了一种名为多尺度张量求和（MTS）分解的新型神经网络算子，用于替代传统密集层和卷积层，提高效率和性能。


<details>
  <summary>Details</summary>
Motivation: 传统多层感知机（MLP）和卷积神经网络（CNN）在高维输入输出对和有限感受野方面存在局限性，需要更高效的算子。

Method: MTS通过多尺度张量求和实现，每个张量通过类似Tucker分解的模式乘积获得，并结合多头门（MHG）等非线性单元。

Result: MTS在分类、压缩和信号恢复等任务中表现优于MLP和CNN，且与最先进的Transformer相比具有更好的复杂度-性能权衡。

Conclusion: MTSNet作为一种新型神经网络层，展示了在计算机视觉任务中的高效性和优越性能。

Abstract: Multilayer perceptrons (MLP), or fully connected artificial neural networks,
are known for performing vector-matrix multiplications using learnable weight
matrices; however, their practical application in many machine learning tasks,
especially in computer vision, can be limited due to the high dimensionality of
input-output pairs at each layer. To improve efficiency, convolutional
operators have been utilized to facilitate weight sharing and local
connections, yet they are constrained by limited receptive fields. In this
paper, we introduce Multiscale Tensor Summation (MTS) Factorization, a novel
neural network operator that implements tensor summation at multiple scales,
where each tensor to be summed is obtained through Tucker-decomposition-like
mode products. Unlike other tensor decomposition methods in the literature, MTS
is not introduced as a network compression tool; instead, as a new backbone
neural layer. MTS not only reduces the number of parameters required while
enhancing the efficiency of weight optimization compared to traditional dense
layers (i.e., unfactorized weight matrices in MLP layers), but it also
demonstrates clear advantages over convolutional layers. The proof-of-concept
experimental comparison of the proposed MTS networks with MLPs and
Convolutional Neural Networks (CNNs) demonstrates their effectiveness across
various tasks, such as classification, compression, and signal restoration.
Additionally, when integrated with modern non-linear units such as the
multi-head gate (MHG), also introduced in this study, the corresponding neural
network, MTSNet, demonstrates a more favorable complexity-performance tradeoff
compared to state-of-the-art transformers in various computer vision
applications. The software implementation of the MTS layer and the
corresponding MTS-based networks, MTSNets, is shared at
https://github.com/mehmetyamac/MTSNet.

</details>


### [136] [CacheFormer: High Attention-Based Segment Caching](https://arxiv.org/abs/2504.13981)
*Sushant Singh,Ausif Mahmood*

Main category: cs.LG

TL;DR: 提出了一种基于缓存和虚拟内存原理的长上下文处理方法，通过分段和动态检索机制优化Transformer模型的性能，平均困惑度提升8.5%。


<details>
  <summary>Details</summary>
Motivation: 现有方法如Linformer、Longformer等未能完全解决长上下文处理问题，需在降低时间复杂度的同时保持质量。

Method: 将长上下文分段处理，动态检索高注意力未压缩段，结合短滑动窗口、长压缩分段、动态检索和重叠段四种注意力机制。

Result: 新架构在相似模型规模下平均困惑度提升8.5%，优于现有SOTA方法。

Conclusion: 通过分段和动态检索机制，有效提升了长上下文处理的效率和性能。

Abstract: Efficiently handling long contexts in transformer-based language models with
low perplexity is an active area of research. Numerous recent approaches like
Linformer, Longformer, Performer, and Structured state space models (SSMs).,
have not fully resolved this problem. All these models strive to reduce the
quadratic time complexity of the attention mechanism while minimizing the loss
in quality due to the effective compression of the long context. Inspired by
the cache and virtual memory principle in computers, where in case of a cache
miss, not only the needed data is retrieved from the memory, but the adjacent
data is also obtained, we apply this concept to handling long contexts by
dividing it into small segments. In our design, we retrieve the nearby segments
in an uncompressed form when high segment-level attention occurs at the
compressed level. Our en-hancements for handling long context include
aggregating four attention mechanisms consisting of short sliding window
attention, long compressed segmented attention, dynamically retrieving top k
high attention uncompressed segments, and overlapping segments in long segment
attention to avoid segment fragmentation. These enhancements result in an
architecture that outperforms ex-isting SOTA architectures with an average
perplexity improvement of 8.5% over similar model sizes.

</details>


### [137] [When Machine Learning Meets Importance Sampling: A More Efficient Rare Event Estimation Approach](https://arxiv.org/abs/2504.13982)
*Ruoning Zhao,Xinyun Chen*

Main category: cs.LG

TL;DR: 论文提出了一种新的重要性采样方法，通过利用稳态分布的边际似然比，解决了传统方法方差爆炸的问题，并结合机器学习算法优化估计效果。


<details>
  <summary>Details</summary>
Motivation: 研究动机源于电信网络中对串联队列稳态下罕见事件概率的估计需求，传统重要性采样方法因路径依赖似然函数的方差爆炸而效率低下。

Method: 提出了一种基于稳态分布边际似然比的新重要性采样方法，并设计了机器学习算法从重要性采样数据中估计该边际似然比。

Result: 数值实验表明，新算法在性能上优于传统重要性采样方法。

Conclusion: 通过结合边际似然比和机器学习，新方法有效解决了传统重要性采样的效率问题，为罕见事件概率估计提供了更优方案。

Abstract: Driven by applications in telecommunication networks, we explore the
simulation task of estimating rare event probabilities for tandem queues in
their steady state. Existing literature has recognized that importance sampling
methods can be inefficient, due to the exploding variance of the path-dependent
likelihood functions. To mitigate this, we introduce a new importance sampling
approach that utilizes a marginal likelihood ratio on the stationary
distribution, effectively avoiding the issue of excessive variance. In
addition, we design a machine learning algorithm to estimate this marginal
likelihood ratio using importance sampling data. Numerical experiments indicate
that our algorithm outperforms the classic importance sampling methods.

</details>


### [138] [QuatE-D: A Distance-Based Quaternion Model for Knowledge Graph Embedding](https://arxiv.org/abs/2504.13983)
*Hamideh-Sadat Fazael-Ardakani,Hamid Soltanian-Zadeh*

Main category: cs.LG

TL;DR: QuatE-D是一种基于四元数的知识图谱嵌入方法，采用距离评分函数，优于传统内积方法，性能高效且可解释性强。


<details>
  <summary>Details</summary>
Motivation: 传统内积方法在知识图谱嵌入中表现有限，需要更灵活且可解释的模型。

Method: 提出QuatE-D模型，利用欧几里得距离作为评分函数，替代传统内积方法。

Result: 实验显示QuatE-D性能优越，尤其在降低平均排名方面表现突出。

Conclusion: 距离评分函数在四元数嵌入中效果显著，为知识图谱补全提供了新方向。

Abstract: Knowledge graph embedding (KGE) methods aim to represent entities and
relations in a continuous space while preserving their structural and semantic
properties. Quaternion-based KGEs have demonstrated strong potential in
capturing complex relational patterns. In this work, we propose QuatE-D, a
novel quaternion-based model that employs a distance-based scoring function
instead of traditional inner-product approaches. By leveraging Euclidean
distance, QuatE-D enhances interpretability and provides a more flexible
representation of relational structures. Experimental results demonstrate that
QuatE-D achieves competitive performance while maintaining an efficient
parameterization, particularly excelling in Mean Rank reduction. These findings
highlight the effectiveness of distance-based scoring in quaternion embeddings,
offering a promising direction for knowledge graph completion.

</details>


### [139] [One Jump Is All You Need: Short-Cutting Transformers for Early Exit Prediction with One Jump to Fit All Exit Levels](https://arxiv.org/abs/2504.13984)
*Amrit Diggavi Seshadri*

Main category: cs.LG

TL;DR: 提出了一种名为OJFA的低秩捷径方法，显著减少了推理时的参数成本，同时保持了性能。


<details>
  <summary>Details</summary>
Motivation: 减少大型语言模型推理的时间和计算成本，同时保持性能。

Method: 选择单一的低秩捷径（OJFA），替代传统的多捷径方法，实现参数效率。

Result: OJFA方法在GPT2-XL、Phi3-Mini和Llama2-7B模型上表现稳定，参数成本降低30倍。

Conclusion: OJFA是一种高效且性能稳定的低秩捷径方法，适用于多种Transformer模型。

Abstract: To reduce the time and computational costs of inference of large language
models, there has been interest in parameter-efficient low-rank early-exit
casting of transformer hidden-representations to final-representations. Such
low-rank short-cutting has been shown to outperform identity shortcuts at early
model stages while offering parameter-efficiency in shortcut jumps. However,
current low-rank methods maintain a separate early-exit shortcut jump to
final-representations for each transformer intermediate block-level during
inference. In this work, we propose selection of a single One-Jump-Fits-All
(OJFA) low-rank shortcut that offers over a 30x reduction in shortcut parameter
costs during inference. We show that despite this extreme reduction, our OJFA
choice largely matches the performance of maintaining multiple shortcut jumps
during inference and offers stable precision from all transformer block-levels
for GPT2-XL, Phi3-Mini and Llama2-7B transformer models.

</details>


### [140] [Gradual Binary Search and Dimension Expansion : A general method for activation quantization in LLMs](https://arxiv.org/abs/2504.13989)
*Lucas Maisonnave,Cyril Moineau,Olivier Bichler,Fabrice Rastello*

Main category: cs.LG

TL;DR: 本文提出了一种基于Hadamard矩阵的量化方法，有效减少LLMs中的异常值，实现了3位量化，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在边缘设备上的部署受到其庞大参数规模的限制，量化是减少内存和推理时间的常用方法，但LLMs中的异常值问题阻碍了低比特量化。

Method: 利用Hadamard矩阵的理论优势，提出一种渐进式二进制搜索方法，支持非2的幂次嵌入维度，实现了对权重、激活和KV缓存的3位量化。

Result: 在Mistral、LLaMA和Qwen等模型上，该方法比现有技术提高了40%的准确率，并成功实现了3位量化。

Conclusion: Hadamard矩阵在减少异常值和实现低比特量化方面具有显著优势，为LLMs的部署提供了实用解决方案。

Abstract: Large language models (LLMs) have become pivotal in artificial intelligence,
demonstrating strong capabilities in reasoning, understanding, and generating
data. However, their deployment on edge devices is hindered by their
substantial size, often reaching several billion parameters. Quantization is a
widely used method to reduce memory usage and inference time, however LLMs
present unique challenges due to the prevalence of outliers in their
activations. In this work, we leverage the theoretical advantages of Hadamard
matrices over random rotation matrices to push the boundaries of quantization
in LLMs. We demonstrate that Hadamard matrices are more effective in reducing
outliers, which are a significant obstacle in achieving low-bit quantization.
Our method based on a gradual binary search enables 3-bit quantization for
weights, activations, and key-value (KV) caches, resulting in a 40\% increase
in accuracy on common benchmarks compared to SoTA methods. We extend the use of
rotation matrices to support non-power-of-2 embedding dimensions, similar to
the Qwen architecture, by employing the Paley algorithm. We theoretically
demonstrates the superiority of Hadamard matrices in reducing outliers.We
achieved 3-bit quantization for weights, activations, and KV cache,
significantly enhancing model performance. Our experimental results on multiple
models family like Mistral, LLaMA, and Qwen demonstrate the effectiveness of
our approach, outperforming existing methods and enabling practical 3-bit
quantization.

</details>


### [141] [PC-DeepNet: A GNSS Positioning Error Minimization Framework Using Permutation-Invariant Deep Neural Network](https://arxiv.org/abs/2504.13990)
*M. Humayun Kabir,Md. Ali Hasan,Md. Shafiqul Islam,Kyeongjun Ko,Wonjae Shin*

Main category: cs.LG

TL;DR: PC-DeepNet是一种基于学习的GNSS定位框架，通过PI-DNN估计位置修正，解决了城市和郊区环境中NLOS和多径效应导致的非高斯误差问题，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: GNSS在城市和郊区环境中因NLOS传播、多径效应和低接收功率导致误差分布复杂，传统基于高斯误差的定位方法精度不足。

Method: 提出PC-DeepNet框架，使用PI-DNN估计位置修正，利用NLOS和多径指标作为特征，适应卫星测量数量和顺序的变化。

Result: 在两个公开数据集上验证，PC-DeepNet在定位精度上优于现有模型和基于学习的方法，且计算复杂度更低。

Conclusion: PC-DeepNet为复杂环境下的GNSS定位提供了高效且高精度的解决方案。

Abstract: Global navigation satellite systems (GNSS) face significant challenges in
urban and sub-urban areas due to non-line-of-sight (NLOS) propagation,
multipath effects, and low received power levels, resulting in highly
non-linear and non-Gaussian measurement error distributions. In light of this,
conventional model-based positioning approaches, which rely on Gaussian error
approximations, struggle to achieve precise localization under these
conditions. To overcome these challenges, we put forth a novel learning-based
framework, PC-DeepNet, that employs a permutation-invariant (PI) deep neural
network (DNN) to estimate position corrections (PC). This approach is designed
to ensure robustness against changes in the number and/or order of visible
satellite measurements, a common issue in GNSS systems, while leveraging NLOS
and multipath indicators as features to enhance positioning accuracy in
challenging urban and sub-urban environments. To validate the performance of
the proposed framework, we compare the positioning error with state-of-the-art
model-based and learning-based positioning methods using two publicly available
datasets. The results confirm that proposed PC-DeepNet achieves superior
accuracy than existing model-based and learning-based methods while exhibiting
lower computational complexity compared to previous learning-based approaches.

</details>


### [142] [Deep Learning on Graphs for Mobile Network Topology Generation](https://arxiv.org/abs/2504.13991)
*Felix Nannesson Meli,Johan Tell,Shirwan Piroti,Tahar Zanouda,Elias Jarlebring*

Main category: cs.LG

TL;DR: 该论文提出了一种基于图深度学习的移动网络拓扑关系预测方法，通过训练自动邻居关系数据，评估了图神经网络和多层感知机的性能，并验证了启发式方法在提升效率和准确性上的作用。


<details>
  <summary>Details</summary>
Motivation: 传统启发式方法在移动网络拓扑关系建立中存在局限性，无法在硬件安装后动态调整。因此，研究利用图深度学习技术动态预测移动网络中的关系（边）。

Method: 使用图神经网络（GNN）和多层感知机（MLP）模型，基于无线电节点配置数据和自动邻居关系（ANR）数据进行训练。通过启发式方法（如节点间距离）减少训练时间。

Result: 实验表明，GNN模型在考虑图结构时表现更优，启发式方法显著提升了模型的精度和准确性。

Conclusion: 图深度学习（尤其是GNN）能有效预测移动网络拓扑关系，启发式方法进一步优化了模型性能，为动态网络拓扑管理提供了新思路。

Abstract: Mobile networks consist of interconnected radio nodes strategically
positioned across various geographical regions to provide connectivity
services. The set of relations between these radio nodes, referred to as the
\emph{mobile network topology}, is vital in the construction of the networking
infrastructure. Typically, the connections between radio nodes and their
associated cells are defined by software features that establish mobility
relations (referred to as \emph{edges} in this paper) within the mobile network
graph through heuristic methods. Although these approaches are efficient, they
encounter significant limitations, particularly since edges can only be
established prior to the installation of physical hardware.
  In this work, we use graph-based deep learning methods to determine mobility
relations (edges), trained on radio node configuration data and reliable
mobility relations set by Automatic Neighbor Relations (ANR) in stable
networks. This paper focuses on measuring the accuracy and precision of
different graph-based deep learning approaches applied to real-world mobile
networks. We evaluated two deep learning models. Our comprehensive experiments
on Telecom datasets obtained from operational Telecom Networks demonstrate the
effectiveness of the graph neural network (GNN) model and multilayer
perceptron. Our evaluation showed that considering graph structure improves
results, which motivates the use of GNNs. Additionally, we investigated the use
of heuristics to reduce the training time based on the distance between radio
nodes to eliminate irrelevant cases. Our investigation showed that the use of
these heuristics improved precision and accuracy considerably.

</details>


### [143] [First and Second Order Approximations to Stochastic Gradient Descent Methods with Momentum Terms](https://arxiv.org/abs/2504.13992)
*Eric Lu*

Main category: cs.LG

TL;DR: 论文研究了随机梯度下降（SGD）及其动量变体的动态行为，提出了在时变学习率和动量参数下的近似结果。


<details>
  <summary>Details</summary>
Motivation: 现有研究仅覆盖恒定学习率或无动量项的SGD，缺乏对时变参数的严格理论分析。

Method: 通过连续逼近方法，研究SGD在时变学习率和动量参数下的动态行为。

Result: 提出了在弱假设下SGD的近似结果，扩展了现有理论。

Conclusion: 该研究为SGD及其动量变体的理论分析提供了更广泛的框架。

Abstract: Stochastic Gradient Descent (SGD) methods see many uses in optimization
problems. Modifications to the algorithm, such as momentum-based SGD methods
have been known to produce better results in certain cases. Much of this,
however, is due to empirical information rather than rigorous proof. While the
dynamics of gradient descent methods can be studied through continuous
approximations, existing works only cover scenarios with constant learning
rates or SGD without momentum terms. We present approximation results under
weak assumptions for SGD that allow learning rates and momentum parameters to
vary with respect to time.

</details>


### [144] [Large Language Bayes](https://arxiv.org/abs/2504.14025)
*Justin Domke*

Main category: cs.LG

TL;DR: 论文提出了一种方法，通过结合大型语言模型和概率编程语言，将非正式问题描述转化为形式化模型，并生成潜在变量的后验分布。


<details>
  <summary>Details</summary>
Motivation: 许多领域专家缺乏时间或训练来编写形式化的贝叶斯模型，因此需要一种自动化方法。

Method: 利用大型语言模型生成多个形式化模型，对每个模型进行近似推断，然后进行加权平均。

Result: 该方法无需指定形式化模型即可生成合理的预测。

Conclusion: 结合大型语言模型和概率编程语言的方法为解决复杂推断问题提供了新思路。

Abstract: Many domain experts do not have the time or training to write formal Bayesian
models. This paper takes an informal problem description as input, and combines
a large language model and a probabilistic programming language to create a
joint distribution over formal models, latent variables, and data. A posterior
over latent variables follows by conditioning on observed data and integrating
over formal models. This presents a challenging inference problem. We suggest
an inference recipe that amounts to generating many formal models from the
large language model, performing approximate inference on each, and then doing
a weighted average. This is justified an analyzed as a combination of
self-normalized importance sampling, MCMC, and variational inference. We show
that this produces sensible predictions without the need to specify a formal
model.

</details>


### [145] [A synthetic dataset of French electric load curves with temperature conditioning](https://arxiv.org/abs/2504.14046)
*Tahar Nabil,Ghislain Agoua,Pierre Cauchois,Anne De Moliner,Benoît Grossin*

Main category: cs.LG

TL;DR: 论文提出了一种基于条件潜在扩散的合成负载曲线数据集，用于解决能源转型中智能电表数据隐私问题，并验证了其保真度、实用性和隐私性。


<details>
  <summary>Details</summary>
Motivation: 能源转型导致用电行为变化，需要智能电表数据支持研究，但数据涉及隐私（如GDPR），因此需要合成隐私保护的样本。

Method: 使用条件潜在扩散方法生成合成负载曲线数据集，并提供合同功率、分时电价计划和本地温度等生成条件。

Result: 数据集在保真度、实用性和隐私性方面表现良好，适用于能源建模应用。

Conclusion: 该合成数据集为能源研究提供了高质量且隐私安全的解决方案。

Abstract: The undergoing energy transition is causing behavioral changes in electricity
use, e.g. with self-consumption of local generation, or flexibility services
for demand control. To better understand these changes and the challenges they
induce, accessing individual smart meter data is crucial. Yet this is personal
data under the European GDPR. A widespread use of such data requires thus to
create synthetic realistic and privacy-preserving samples. This paper
introduces a new synthetic load curve dataset generated by conditional latent
diffusion. We also provide the contracted power, time-of-use plan and local
temperature used for generation. Fidelity, utility and privacy of the dataset
are thoroughly evaluated, demonstrating its good quality and thereby supporting
its interest for energy modeling applications.

</details>


### [146] [CAOTE: KV Caching through Attention Output Error based Token Eviction](https://arxiv.org/abs/2504.14051)
*Raghavv Goel,Junyoung Park,Mukul Gagrani,Dalton Jones,Matthew Morse,Harper Langston,Mingu Lee,Chris Lott*

Main category: cs.LG

TL;DR: 论文提出了一种新的令牌驱逐标准CAOTE，结合注意力分数和值向量信息，优化驱逐误差，提升下游任务准确性。


<details>
  <summary>Details</summary>
Motivation: 长上下文支持的大语言模型在资源受限设备上存在内存和计算瓶颈，现有基于注意力分数的令牌驱逐方法缺乏对令牌贡献的全面评估。

Method: 提出CAOTE方法，利用注意力分数和值向量信息优化令牌驱逐过程，减少驱逐误差。

Result: CAOTE结合现有方法显著提升下游任务准确性，验证了值向量信息的重要性。

Conclusion: CAOTE是一种灵活且高效的令牌驱逐方法，为资源受限设备提供了更好的解决方案。

Abstract: While long context support of large language models has extended their
abilities, it also incurs challenges in memory and compute which becomes
crucial bottlenecks in resource-restricted devices. Token eviction, a widely
adopted post-training methodology designed to alleviate the bottlenecks by
evicting less important tokens from the cache, typically uses attention scores
as proxy metrics for token importance. However, one major limitation of
attention score as a token-wise importance metrics is that it lacks the
information about contribution of tokens to the attention output. In this
paper, we propose a simple eviction criterion based on the contribution of
cached tokens to attention outputs. Our method, CAOTE, optimizes for eviction
error due to token eviction, by seamlessly integrating attention scores and
value vectors. This is the first method which uses value vector information on
top of attention-based eviction scores. Additionally, CAOTE can act as a
meta-heuristic method with flexible usage with any token eviction method. We
show that CAOTE, when combined with the state-of-the-art attention score-based
methods, always improves accuracies on the downstream task, indicating the
importance of leveraging information from values during token eviction process.

</details>


### [147] [Contextual Embedding-based Clustering to Identify Topics for Healthcare Service Improvement](https://arxiv.org/abs/2504.14068)
*K M Sajjadul Islam,Ravi Teja Karri,Srujan Vegesna,Jiawei Wu,Praveen Madiraju*

Main category: cs.LG

TL;DR: 该研究提出了一种结合BERT嵌入与k-means聚类的方法kBERT，用于分析医疗短文本反馈，结果表明kBERT在主题一致性和多样性上优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 分析未标记的短文本患者反馈对改进医疗服务至关重要，但传统监督学习方法需要大量标记数据，因此探索无监督方法以提取有意义的信息。

Method: 研究比较了传统主题建模方法（如LDA和GSDMM）与BERTopic和提出的kBERT方法，后者结合BERT嵌入与k-means聚类。

Result: kBERT在主题一致性（Cv=0.53）和多样性（IRBOavg=1.00）上表现最佳，优于其他模型。

Conclusion: 嵌入技术对主题识别至关重要，医疗分析需要上下文感知模型。

Abstract: Understanding patient feedback is crucial for improving healthcare services,
yet analyzing unlabeled short-text feedback presents significant challenges due
to limited data and domain-specific nuances. Traditional supervised learning
approaches require extensive labeled datasets, making unsupervised methods more
viable for uncovering meaningful insights from patient feedback. This study
explores unsupervised methods to extract meaningful topics from 439 survey
responses collected from a healthcare system in Wisconsin, USA. A keyword-based
filtering approach was applied to isolate complaint-related feedback using a
domain-specific lexicon. To delve deeper and analyze dominant topics in
feedback, we explored traditional topic modeling methods, including Latent
Dirichlet Allocation (LDA) and Gibbs Sampling Dirichlet Multinomial Mixture
(GSDMM), alongside BERTopic, an advanced neural embedding-based clustering
approach. To improve coherence and interpretability where data are scarce and
consist of short-texts, we propose kBERT, an integration of BERT embeddings
with k-means clustering. Model performance was assessed using coherence scores
(Cv ) for topic interpretability and average Inverted Rank-Biased Overlap
(IRBOavg) for topic diversity. Results indicate that kBERT achieves the highest
coherence (Cv = 0.53) and distinct topic separation (IRBOavg = 1.00),
outperforming all other models in short-text healthcare feedback analysis. Our
findings emphasize the importance of embedding-based techniques for topic
identification and highlight the need for context-aware models in healthcare
analytics.

</details>


### [148] [Leakage and Interpretability in Concept-Based Models](https://arxiv.org/abs/2504.14094)
*Enrico Parisini,Tapabrata Chakraborti,Chris Harbron,Ben D. MacArthur,Christopher R. S. Banerji*

Main category: cs.LG

TL;DR: 论文提出了一种信息论框架，用于量化概念瓶颈模型中的信息泄漏问题，并提出了两种泄漏度量指标（CTL和ICL）。研究发现概念嵌入模型普遍存在泄漏问题，并提出了减少泄漏的实用指南。


<details>
  <summary>Details</summary>
Motivation: 概念瓶颈模型在高风险场景中具有潜力，但存在信息泄漏问题，影响模型的可靠性和可解释性。

Method: 引入信息论框架，定义CTL和ICL两种泄漏度量指标，并通过实验验证其有效性。

Result: CTL和ICL指标能有效预测模型行为，且概念嵌入模型普遍存在泄漏问题。

Conclusion: 论文提出了减少泄漏的实用指南，为设计更可靠的概念模型提供了依据。

Abstract: Concept Bottleneck Models aim to improve interpretability by predicting
high-level intermediate concepts, representing a promising approach for
deployment in high-risk scenarios. However, they are known to suffer from
information leakage, whereby models exploit unintended information encoded
within the learned concepts. We introduce an information-theoretic framework to
rigorously characterise and quantify leakage, and define two complementary
measures: the concepts-task leakage (CTL) and interconcept leakage (ICL)
scores. We show that these measures are strongly predictive of model behaviour
under interventions and outperform existing alternatives in robustness and
reliability. Using this framework, we identify the primary causes of leakage
and provide strong evidence that Concept Embedding Models exhibit substantial
leakage regardless of the hyperparameters choice. Finally, we propose practical
guidelines for designing concept-based models to reduce leakage and ensure
interpretability.

</details>


### [149] [Personalizing Exposure Therapy via Reinforcement Learning](https://arxiv.org/abs/2504.14095)
*Athar Mahmoudi-Nejad,Matthew Guzdial,Pierre Boulanger*

Main category: cs.LG

TL;DR: 提出了一种基于生理指标的个性化治疗内容自动适配方法，通过强化学习生成虚拟蜘蛛，显著优于传统规则方法。


<details>
  <summary>Details</summary>
Motivation: 传统个性化治疗依赖治疗师的经验和预定义规则，难以普适化，需技术改进。

Method: 采用经验驱动的程序化内容生成（EDPCGRL）和强化学习，基于生理指标动态生成治疗内容。

Result: 实验表明，该方法在虚拟现实蜘蛛恐惧症治疗中显著优于规则方法。

Conclusion: 该方法有潜力提升个性化治疗干预效果。

Abstract: Personalized therapy, in which a therapeutic practice is adapted to an
individual patient, can lead to improved health outcomes. Typically, this is
accomplished by relying on a therapist's training and intuition along with
feedback from a patient. However, this requires the therapist to become an
expert on any technological components, such as in the case of Virtual Reality
Exposure Therapy (VRET). While there exist approaches to automatically adapt
therapeutic content to a patient, they generally rely on hand-authored,
pre-defined rules, which may not generalize to all individuals. In this paper,
we propose an approach to automatically adapt therapeutic content to patients
based on physiological measures. We implement our approach in the context of
virtual reality arachnophobia exposure therapy, and rely on experience-driven
procedural content generation via reinforcement learning (EDPCGRL) to generate
virtual spiders to match an individual patient. Through a human subject study,
we demonstrate that our system significantly outperforms a more common
rules-based method, highlighting its potential for enhancing personalized
therapeutic interventions.

</details>


### [150] [Enhancing Math Learning in an LMS Using AI-Driven Question Recommendations](https://arxiv.org/abs/2504.14098)
*Justus Råmunddal*

Main category: cs.LG

TL;DR: 论文提出了一种基于AI的数学学习增强方法，通过推荐相似数学问题来优化现代学习管理系统（LMS）。使用Meta的Llama-3.2-11B-Vision-Instruct模型生成数学问题的深度嵌入，并比较了余弦相似度、自组织映射（SOM）和高斯混合模型（GMM）三种推荐方法。实验表明，SOM在用户满意度上表现最佳，而GMM效果较差。


<details>
  <summary>Details</summary>
Motivation: 现代LMS需要更智能的数学学习辅助工具，通过推荐相似问题提升学习效果。

Method: 使用Llama-3.2-11B-Vision-Instruct模型生成问题嵌入，并比较余弦相似度、SOM和GMM三种推荐方法。

Result: 余弦相似度匹配问题最接近，SOM用户满意度最高，GMM表现较差。适度多样性可提升学习效果。

Conclusion: SOM是推荐相似数学问题的有效方法，但需平衡多样性以优化学习效果。

Abstract: This paper presents an AI-driven approach to enhance math learning in a
modern Learning Management System (LMS) by recommending similar math questions.
Deep embeddings for math questions are generated using Meta's
Llama-3.2-11B-Vision-Instruct model, and three recommendation methods-cosine
similarity, Self-Organizing Maps (SOM), and Gaussian Mixture Models (GMM)-are
applied to identify similar questions. User interaction data, including session
durations, response times, and correctness, are used to evaluate the methods.
Our findings suggest that while cosine similarity produces nearly identical
question matches, SOM yields higher user satisfaction whereas GMM generally
underperforms, indicating that introducing variety to a certain degree may
enhance engagement and thereby potential learning outcomes until variety is no
longer balanced reasonably, which our data about the implementations of all
three methods demonstrate.

</details>


### [151] [Predicting Stress and Damage in Carbon Fiber-Reinforced Composites Deformation Process using Composite U-Net Surrogate Model](https://arxiv.org/abs/2504.14143)
*Zeping Chen,Marwa Yacouti,Maryam Shakiba,Jian-Xun Wang,Tengfei Luo,Vikas Varshney*

Main category: cs.LG

TL;DR: 提出了一种新型自回归复合U-Net深度学习模型，用于同时预测碳纤维增强复合材料（CFRC）变形过程中的应力和损伤场，显著提高了计算效率。


<details>
  <summary>Details</summary>
Motivation: 传统有限元方法（FEM）在计算效率上存在不足，现有数据驱动模型无法全面捕捉应力和损伤的演化过程。

Method: 采用U-Net架构的自回归复合模型，结合宏微观特征，预测CFRC在单向应变下的应力和损伤分布。

Result: 模型预测精度高，计算速度比IGFEM快60倍以上。

Conclusion: 该模型为CFRC的性能优化提供了高效且准确的预测工具。

Abstract: Carbon fiber-reinforced composites (CFRC) are pivotal in advanced engineering
applications due to their exceptional mechanical properties. A deep
understanding of CFRC behavior under mechanical loading is essential for
optimizing performance in demanding applications such as aerospace structures.
While traditional Finite Element Method (FEM) simulations, including advanced
techniques like Interface-enriched Generalized FEM (IGFEM), offer valuable
insights, they can struggle with computational efficiency. Existing data-driven
surrogate models partially address these challenges by predicting propagated
damage or stress-strain behavior but fail to comprehensively capture the
evolution of stress and damage throughout the entire deformation history,
including crack initiation and propagation. This study proposes a novel
auto-regressive composite U-Net deep learning model to simultaneously predict
stress and damage fields during CFRC deformation. By leveraging the U-Net
architecture's ability to capture spatial features and integrate macro- and
micro-scale phenomena, the proposed model overcomes key limitations of prior
approaches. The model achieves high accuracy in predicting evolution of stress
and damage distribution within the microstructure of a CFRC under
unidirectional strain, offering a speed-up of over 60 times compared to IGFEM.

</details>


### [152] [A Physics-guided Multimodal Transformer Path to Weather and Climate Sciences](https://arxiv.org/abs/2504.14174)
*Jing Han,Hanting Chen,Kai Han,Xiaomeng Huang,Yongyun Hu,Wenjun Xu,Dacheng Tao,Ping Zhang*

Main category: cs.LG

TL;DR: 论文探讨了AI在气象学中的应用，提出了一种基于多模态数据和Transformer的新范式，结合物理信号和正则化技术提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 传统气象学方法精度有限，AI模型能显著提升准确性，同时结合物理信号增强可解释性。

Method: 将气象数据转化为2D图像或3D视频，作为多模态数据输入Transformer模型，并利用正则化技术融入气象知识。

Result: 新范式具有强通用性，可处理多种任务，并提升模型精度和可解释性。

Conclusion: 未来研究方向包括进一步优化模型精度和可解释性。

Abstract: With the rapid development of machine learning in recent years, many problems
in meteorology can now be addressed using AI models. In particular, data-driven
algorithms have significantly improved accuracy compared to traditional
methods. Meteorological data is often transformed into 2D images or 3D videos,
which are then fed into AI models for learning. Additionally, these models
often incorporate physical signals, such as temperature, pressure, and wind
speed, to further enhance accuracy and interpretability. In this paper, we
review several representative AI + Weather/Climate algorithms and propose a new
paradigm where observational data from different perspectives, each with
distinct physical meanings, are treated as multimodal data and integrated via
transformers. Furthermore, key weather and climate knowledge can be
incorporated through regularization techniques to further strengthen the
model's capabilities. This new paradigm is versatile and can address a variety
of tasks, offering strong generalizability. We also discuss future directions
for improving model accuracy and interpretability.

</details>


### [153] [FedC4: Graph Condensation Meets Client-Client Collaboration for Efficient and Private Federated Graph Learning](https://arxiv.org/abs/2504.14188)
*Zekai Chen,Xunkai Li,Yinlin Zhu,Rong-Hua Li,Guoren Wang*

Main category: cs.LG

TL;DR: FedC4是一种结合图压缩与客户端协作的联邦图学习框架，通过传输合成节点嵌入减少通信开销和隐私风险，并支持个性化优化。


<details>
  <summary>Details</summary>
Motivation: 现有客户端-客户端（C-C）联邦图学习方法因广播相同节点嵌入导致高通信成本和隐私风险，需改进。

Method: 提出FedC4框架，将私有图压缩为合成节点嵌入，并引入三个模块支持个性化优化。

Result: 在八个真实数据集上，FedC4在性能和通信效率上均优于现有方法。

Conclusion: FedC4通过图压缩和个性化协作，显著提升了联邦图学习的效率和隐私保护能力。

Abstract: Federated Graph Learning (FGL) is an emerging distributed learning paradigm
that enables collaborative model training over decentralized graph-structured
data while preserving local privacy. Existing FGL methods can be categorized
into two optimization architectures: (1) the Server-Client (S-C) paradigm,
where clients upload local models for server-side aggregation; and (2) the
Client-Client (C-C) paradigm, which allows direct information exchange among
clients to support personalized training. Compared to S-C, the C-C architecture
better captures global graph knowledge and enables fine-grained optimization
through customized peer-to-peer communication. However, current C-C methods
often broadcast identical and redundant node embeddings, incurring high
communication costs and privacy risks. To address this, we propose FedC4, a
novel framework that combines graph Condensation with Client-Client
Collaboration. Instead of transmitting raw node-level features, FedC4 distills
each client's private graph into a compact set of synthetic node embeddings,
reducing communication overhead and enhancing privacy. In addition, FedC4
introduces three modules that allow source clients to send distinct node
representations tailored to target clients'graph structures, enabling
personalized optimization with global guidance. Extensive experiments on eight
real-world datasets show that FedC4 outperforms state-of-the-art baselines in
both performance and communication efficiency.

</details>


### [154] [DConAD: A Differencing-based Contrastive Representation Learning Framework for Time Series Anomaly Detection](https://arxiv.org/abs/2504.14204)
*Wenxin Zhang,Xiaojian Lin,Wenjun Yu,Guangzhen Yao,jingxiang Zhong,Yu Li,Renda Han,Songcheng Xu,Hao Shi,Cuicui Luo*

Main category: cs.LG

TL;DR: DConAD是一种基于差分对比表示学习的时间序列异常检测框架，通过生成差分数据和Transformer架构增强鲁棒性，优于九种基线方法。


<details>
  <summary>Details</summary>
Motivation: 时间序列异常检测在风险识别和故障检测中至关重要，但无监督学习方法因异常模式多样性和数据复杂性而难以捕获稳健依赖关系。

Method: 提出DConAD框架，利用差分数据增强信息，结合Transformer捕获时空依赖，并采用基于KL散度的对比学习范式。

Result: 在五个公共数据集上的实验表明，DConAD优于九种基线方法。

Conclusion: DConAD通过差分对比学习显著提升了时间序列异常检测的能力，且代码已开源。

Abstract: Time series anomaly detection holds notable importance for risk
identification and fault detection across diverse application domains.
Unsupervised learning methods have become popular because they have no
requirement for labels. However, due to the challenges posed by the
multiplicity of abnormal patterns, the sparsity of anomalies, and the growth of
data scale and complexity, these methods often fail to capture robust and
representative dependencies within the time series for identifying anomalies.
To enhance the ability of models to capture normal patterns of time series and
avoid the retrogression of modeling ability triggered by the dependencies on
high-quality prior knowledge, we propose a differencing-based contrastive
representation learning framework for time series anomaly detection (DConAD).
Specifically, DConAD generates differential data to provide additional
information about time series and utilizes transformer-based architecture to
capture spatiotemporal dependencies, which enhances the robustness of unbiased
representation learning ability. Furthermore, DConAD implements a novel KL
divergence-based contrastive learning paradigm that only uses positive samples
to avoid deviation from reconstruction and deploys the stop-gradient strategy
to compel convergence. Extensive experiments on five public datasets show the
superiority and effectiveness of DConAD compared with nine baselines. The code
is available at https://github.com/shaieesss/DConAD.

</details>


### [155] [Dual-channel Heterophilic Message Passing for Graph Fraud Detection](https://arxiv.org/abs/2504.14205)
*Wenxin Zhang,Jingxing Zhong,Guangzhen Yao,Renda Han,Xiaojian Lin,Zeyu Zhang,Cuicui Luo*

Main category: cs.LG

TL;DR: 论文提出了一种名为DHMP的双通道异质性消息传递框架，用于改进欺诈检测任务，通过分离同质性和异质性子图来优化传统GNN的低通归纳偏差。


<details>
  <summary>Details</summary>
Motivation: 现有基于空间GNN的方法在消息传递中排除异质性邻居，可能破坏原始图拓扑并增加预测不确定性。

Method: DHMP利用异质性分离模块将图分为同质性和异质性子图，采用共享权重独立捕获不同频率信号，并引入定制采样策略。

Result: 在三个真实数据集上的实验表明，DHMP优于现有方法。

Conclusion: 分离不同频率信号对提升欺诈检测效果至关重要。

Abstract: Fraudulent activities have significantly increased across various domains,
such as e-commerce, online review platforms, and social networks, making fraud
detection a critical task. Spatial Graph Neural Networks (GNNs) have been
successfully applied to fraud detection tasks due to their strong inductive
learning capabilities. However, existing spatial GNN-based methods often
enhance the graph structure by excluding heterophilic neighbors during message
passing to align with the homophilic bias of GNNs. Unfortunately, this approach
can disrupt the original graph topology and increase uncertainty in
predictions. To address these limitations, this paper proposes a novel
framework, Dual-channel Heterophilic Message Passing (DHMP), for fraud
detection. DHMP leverages a heterophily separation module to divide the graph
into homophilic and heterophilic subgraphs, mitigating the low-pass inductive
bias of traditional GNNs. It then applies shared weights to capture signals at
different frequencies independently and incorporates a customized sampling
strategy for training. This allows nodes to adaptively balance the
contributions of various signals based on their labels. Extensive experiments
on three real-world datasets demonstrate that DHMP outperforms existing
methods, highlighting the importance of separating signals with different
frequencies for improved fraud detection. The code is available at
https://github.com/shaieesss/DHMP.

</details>


### [156] [Decomposition-based multi-scale transformer framework for time series anomaly detection](https://arxiv.org/abs/2504.14206)
*Wenxin Zhang,Cuicui Luo*

Main category: cs.LG

TL;DR: 提出了一种基于分解和Transformer的多变量时间序列异常检测框架TransDe，结合时间序列分解和Transformer的优势，通过多尺度补丁架构和对比学习范式提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以直接建模复杂时间序列依赖关系，且易受噪声影响。

Method: 结合时间序列分解与Transformer，提出多尺度补丁架构和基于KL散度的对比学习范式，引入异步损失函数优化。

Result: 在五个公开数据集上优于12种基线方法，F1分数表现优越。

Conclusion: TransDe有效解决了复杂时间序列建模和噪声问题，性能显著提升。

Abstract: Time series anomaly detection is crucial for maintaining stable systems.
Existing methods face two main challenges. First, it is difficult to directly
model the dependencies of diverse and complex patterns within the sequences.
Second, many methods that optimize parameters using mean squared error struggle
with noise in the time series, leading to performance deterioration. To address
these challenges, we propose a transformer-based framework built on
decomposition (TransDe) for multivariate time series anomaly detection. The key
idea is to combine the strengths of time series decomposition and transformers
to effectively learn the complex patterns in normal time series data. A
multi-scale patch-based transformer architecture is proposed to exploit the
representative dependencies of each decomposed component of the time series.
Furthermore, a contrastive learn paradigm based on patch operation is proposed,
which leverages KL divergence to align the positive pairs, namely the pure
representations of normal patterns between different patch-level views. A novel
asynchronous loss function with a stop-gradient strategy is further introduced
to enhance the performance of TransDe effectively. It can avoid time-consuming
and labor-intensive computation costs in the optimization process. Extensive
experiments on five public datasets are conducted and TransDe shows superiority
compared with twelve baselines in terms of F1 score. Our code is available at
https://github.com/shaieesss/TransDe.

</details>


### [157] [A Novel Frequency-Spatial Domain Aware Network for Fast Thermal Prediction in 2.5D ICs](https://arxiv.org/abs/2504.14237)
*Dekang Zhang,Dan Niu,Zhou Jin,Yichao Dong,Jingweijia Tan,Changyin Sun*

Main category: cs.LG

TL;DR: 论文提出了一种新型频率-空间双域感知预测网络（FSA-Heat），用于2.5D IC中的快速高精度热预测，显著提升了预测精度和推理速度。


<details>
  <summary>Details</summary>
Motivation: 在摩尔定律后时代，2.5D芯片的热管理面临高功率密度和热点问题，现有神经网络方法难以有效捕捉全局热特征，尤其是高频分量。

Method: 提出FSA-Heat网络，结合高频-低频和空间域编码器（FSTE）与频率域跨尺度交互模块（FCIFormer），并设计频率-空间混合损失（FSL）以减少高频噪声和空间错位。

Result: 实验表明，FSA-Heat在RMSE上降低了99%，推理速度提升了4.23倍，且具有强泛化能力。

Conclusion: FSA-Heat在2.5D IC热预测中表现出色，优于现有方法，具有实际应用潜力。

Abstract: In the post-Moore era, 2.5D chiplet-based ICs present significant challenges
in thermal management due to increased power density and thermal hotspots.
Neural network-based thermal prediction models can perform real-time
predictions for many unseen new designs. However, existing CNN-based and
GCN-based methods cannot effectively capture the global thermal features,
especially for high-frequency components, hindering prediction accuracy
enhancement. In this paper, we propose a novel frequency-spatial dual domain
aware prediction network (FSA-Heat) for fast and high-accuracy thermal
prediction in 2.5D ICs. It integrates high-to-low frequency and spatial domain
encoder (FSTE) module with frequency domain cross-scale interaction module
(FCIFormer) to achieve high-to-low frequency and global-to-local thermal
dissipation feature extraction. Additionally, a frequency-spatial hybrid loss
(FSL) is designed to effectively attenuate high-frequency thermal gradient
noise and spatial misalignments. The experimental results show that the
performance enhancements offered by our proposed method are substantial,
outperforming the newly-proposed 2.5D method, GCN+PNA, by considerable margins
(over 99% RMSE reduction, 4.23X inference time speedup). Moreover, extensive
experiments demonstrate that FSA-Heat also exhibits robust generalization
capabilities.

</details>


### [158] [A Pre-Training and Adaptive Fine-Tuning Framework for Graph Anomaly Detection](https://arxiv.org/abs/2504.14250)
*Yunhui Liu,Jiashun Cheng,Jia Li,Fugee Tsung,Hongzhi Yin,Tieke He*

Main category: cs.LG

TL;DR: 论文提出了一种针对图异常检测（GAD）的预训练和自适应微调框架（PAF），通过联合低通和高通滤波器捕捉节点特征的完整频谱信息，并在微调阶段自适应结合两种滤波器的表示。


<details>
  <summary>Details</summary>
Motivation: 图异常检测因异常节点稀缺和标注成本高而具有挑战性，且异常节点通常表现出高局部异质性，而正常节点保持强同质性，形成复杂的同质-异质混合模式。

Method: 通过谱分析揭示了仅依赖全局低通滤波器的不足，提出PAF框架，预训练阶段联合低通和高通滤波器，微调阶段设计门控融合网络自适应结合两种滤波器的表示。

Result: 在十个基准数据集上的实验证明了PAF的有效性。

Conclusion: PAF通过选择性应用滤波器并自适应结合表示，显著提升了图异常检测的性能。

Abstract: Graph anomaly detection (GAD) has garnered increasing attention in recent
years, yet it remains challenging due to the scarcity of abnormal nodes and the
high cost of label annotations. Graph pre-training, the two-stage learning
paradigm, has emerged as an effective approach for label-efficient learning,
largely benefiting from expressive neighborhood aggregation under the
assumption of strong homophily. However, in GAD, anomalies typically exhibit
high local heterophily, while normal nodes retain strong homophily, resulting
in a complex homophily-heterophily mixture. To understand the impact of this
mixed pattern on graph pre-training, we analyze it through the lens of spectral
filtering and reveal that relying solely on a global low-pass filter is
insufficient for GAD. We further provide a theoretical justification for the
necessity of selectively applying appropriate filters to individual nodes.
Building upon this insight, we propose PAF, a Pre-Training and Adaptive
Fine-tuning framework specifically designed for GAD. In particular, we
introduce joint training with low- and high-pass filters in the pre-training
phase to capture the full spectrum of frequency information in node features.
During fine-tuning, we devise a gated fusion network that adaptively combines
node representations generated by both filters. Extensive experiments across
ten benchmark datasets consistently demonstrate the effectiveness of PAF.

</details>


### [159] [Generative emulation of chaotic dynamics with coherent prior](https://arxiv.org/abs/2504.14264)
*Juan Nathaniel,Pierre Gentine*

Main category: cs.LG

TL;DR: 提出了一种基于生成建模的动态仿真框架Cohesion，通过结合湍流原理和扩散模型，利用大尺度相干结构指导去噪过程，显著提升了长期预测的物理一致性。


<details>
  <summary>Details</summary>
Motivation: 解决数据驱动的非线性动态仿真中因长程技能衰减导致的物理不现实输出问题，同时提供不确定性量化和校正。

Method: 结合湍流原理与扩散模型，利用降阶模型（如深度Koopman算子）快速生成长序列相干先验，将预测任务重构为轨迹规划问题。

Result: 在复杂混沌系统（如Kolmogorov流、浅水方程和次季节至季节气候动态）中，Cohesion表现出卓越的长期预测能力，能高效生成物理一致的仿真。

Conclusion: Cohesion框架通过结合相干先验和条件去噪，显著提升了动态仿真的长期预测能力和物理一致性，适用于部分观测指导的场景。

Abstract: Data-driven emulation of nonlinear dynamics is challenging due to long-range
skill decay that often produces physically unrealistic outputs. Recent advances
in generative modeling aim to address these issues by providing uncertainty
quantification and correction. However, the quality of generated simulation
remains heavily dependent on the choice of conditioning priors. In this work,
we present an efficient generative framework for dynamics emulation, unifying
principles of turbulence with diffusion-based modeling: Cohesion. Specifically,
our method estimates large-scale coherent structure of the underlying dynamics
as guidance during the denoising process, where small-scale fluctuation in the
flow is then resolved. These coherent priors are efficiently approximated using
reduced-order models, such as deep Koopman operators, that allow for rapid
generation of long prior sequences while maintaining stability over extended
forecasting horizon. With this gain, we can reframe forecasting as trajectory
planning, a common task in reinforcement learning, where conditional denoising
is performed once over entire sequences, minimizing the computational cost of
autoregressive-based generative methods. Empirical evaluations on chaotic
systems of increasing complexity, including Kolmogorov flow, shallow water
equations, and subseasonal-to-seasonal climate dynamics, demonstrate Cohesion
superior long-range forecasting skill that can efficiently generate
physically-consistent simulations, even in the presence of partially-observed
guidance.

</details>


### [160] [Mixed-Precision Conjugate Gradient Solvers with RL-Driven Precision Tuning](https://arxiv.org/abs/2504.14268)
*Xinye Chen*

Main category: cs.LG

TL;DR: 本文提出了一种新颖的强化学习框架，用于动态优化预条件共轭梯度法中的数值精度，通过Q学习自适应分配精度，平衡计算效率与数值精度。


<details>
  <summary>Details</summary>
Motivation: 解决传统数值方法在精度选择上的静态性，通过强化学习实现动态优化，提升计算效率和适应性。

Method: 将精度选择建模为马尔可夫决策过程，使用Q学习自适应分配精度，同时通过双精度标量计算和残差计算确保稳定性。

Result: 实验证明该方法能有效提升求解器性能，无需重新训练即可适应新数据集，具有鲁棒性和可扩展性。

Conclusion: 该研究首次将强化学习应用于混合精度数值方法，为科学计算中的AI驱动进步提供了新思路。

Abstract: This paper presents a novel reinforcement learning (RL) framework for
dynamically optimizing numerical precision in the preconditioned conjugate
gradient (CG) method. By modeling precision selection as a Markov Decision
Process (MDP), we employ Q-learning to adaptively assign precision levels to
key operations, striking an optimal balance between computational efficiency
and numerical accuracy, while ensuring stability through double-precision
scalar computations and residual computing. In practice, the algorithm is
trained on a set of data and subsequently performs inference for precision
selection on out-of-sample data, without requiring re-analysis or retraining
for new datasets. This enables the method to adapt seamlessly to new problem
instances without the computational overhead of recalibration. Our results
demonstrate the effectiveness of RL in enhancing solver's performance, marking
the first application of RL to mixed-precision numerical methods. The findings
highlight the approach's practical advantages, robustness, and scalability,
providing valuable insights into its integration with iterative solvers and
paving the way for AI-driven advancements in scientific computing.

</details>


### [161] [SRPO: A Cross-Domain Implementation of Large-Scale Reinforcement Learning on LLM](https://arxiv.org/abs/2504.14286)
*Xiaojiang Zhang,Jinghui Wang,Zifei Cheng,Wenhao Zhuang,Zheng Lin,Minglei Zhang,Shaojie Wang,Yinghan Cui,Chao Wang,Junyi Peng,Shimiao Jiang,Shiqi Kuang,Shouyu Yin,Chaohang Wen,Haotian Zhang,Bin Chen,Bing Yu*

Main category: cs.LG

TL;DR: SRPO是一种两阶段历史重采样策略优化方法，通过强化学习提升LLM的推理能力，无需监督微调，在AIME24和LiveCodeBench上超越DeepSeek-R1-Zero-32B。


<details>
  <summary>Details</summary>
Motivation: 现有推理模型（如OpenAI的o1和DeepSeek的R1）在跨领域推广时因方法透明度不足而受限，SRPO旨在解决这一问题。

Method: 基于GRPO，提出两阶段跨领域训练范式以平衡数学推理与编程能力，并引入历史重采样（HR）技术处理无效样本。

Result: SRPO在相同基础模型（Qwen2.5-32B）下，仅依赖强化学习即超越DeepSeek-R1-Zero-32B的性能。

Conclusion: SRPO为跨任务扩展LLM推理能力提供了有效方法，实验验证了其价值。

Abstract: Recent advances of reasoning models, exemplified by OpenAI's o1 and
DeepSeek's R1, highlight the significant potential of Reinforcement Learning
(RL) to enhance the reasoning capabilities of Large Language Models (LLMs).
However, replicating these advancements across diverse domains remains
challenging due to limited methodological transparency. In this work, we
present two-Staged history-Resampling Policy Optimization (SRPO), which
successfully surpasses the performance of DeepSeek-R1-Zero-32B on the AIME24
and LiveCodeBench benchmarks. SRPO achieves this using the same base model as
DeepSeek (i.e. Qwen2.5-32B) and relies solely on RL, without prior Supervised
Fine-Tuning (SFT). Building upon Group Relative Policy Optimization (GRPO), we
introduce two key methodological innovations: (1) a two-stage cross-domain
training paradigm designed to balance the development of mathematical reasoning
and coding proficiency, and (2) History Resampling (HR), a technique to address
ineffective samples. Our comprehensive experiments validate the effectiveness
of our approach, dedicating to offer valuable insights into scaling LLM
reasoning capabilities across diverse tasks.

</details>


### [162] [Learning and Generating Diverse Residential Load Patterns Using GAN with Weakly-Supervised Training and Weight Selection](https://arxiv.org/abs/2504.14300)
*Xinyu Liang,Hao Wang*

Main category: cs.LG

TL;DR: 论文提出了一种基于生成对抗网络（RLP-GAN）的弱监督框架，用于生成高质量住宅负荷数据，解决了现有方法在可扩展性、多样性和相似性上的不足。


<details>
  <summary>Details</summary>
Motivation: 高质量住宅负荷数据的稀缺阻碍了住宅领域脱碳和电网规划与运营的有效性，现有合成数据生成方法存在局限性。

Method: 采用弱监督GAN框架，结合过完备自编码器捕获复杂负荷模式的依赖关系，并引入模型权重选择方法解决模式崩溃问题。

Result: RLP-GAN在417户真实数据上验证，表现优于现有模型，能生成更接近真实数据的负荷模式。

Conclusion: RLP-GAN成功生成高质量合成负荷数据，并公开了包含100万条合成负荷模式的数据集。

Abstract: The scarcity of high-quality residential load data can pose obstacles for
decarbonizing the residential sector as well as effective grid planning and
operation. The above challenges have motivated research into generating
synthetic load data, but existing methods faced limitations in terms of
scalability, diversity, and similarity. This paper proposes a Generative
Adversarial Network-based Synthetic Residential Load Pattern (RLP-GAN)
generation model, a novel weakly-supervised GAN framework, leveraging an
over-complete autoencoder to capture dependencies within complex and diverse
load patterns and learn household-level data distribution at scale. We
incorporate a model weight selection method to address the mode collapse
problem and generate load patterns with high diversity. We develop a holistic
evaluation method to validate the effectiveness of RLP-GAN using real-world
data of 417 households. The results demonstrate that RLP-GAN outperforms
state-of-the-art models in capturing temporal dependencies and generating load
patterns with higher similarity to real data. Furthermore, we have publicly
released the RLP-GAN generated synthetic dataset, which comprises one million
synthetic residential load pattern profiles.

</details>


### [163] [Learning to Score](https://arxiv.org/abs/2504.14302)
*Yogev Kriger,Shai Fine*

Main category: cs.LG

TL;DR: 论文研究了一种在目标标签不可用但存在相关辅助信息（Side Information）的场景下的机器学习方法，提出了结合表示学习、辅助信息和度量学习的评分模型。


<details>
  <summary>Details</summary>
Motivation: 解决目标标签缺失但存在相关辅助信息时的学习问题，例如医疗领域中疾病严重程度评分的构建。

Method: 将问题建模为表示学习、辅助信息和度量学习的组合，提出评分模型。

Result: 在基准数据集和生物医学患者记录上验证了评分系统的有效性。

Conclusion: 提出的评分模型在目标标签不可用但存在辅助信息的场景下具有实用价值。

Abstract: Common machine learning settings range from supervised tasks, where
accurately labeled data is accessible, through semi-supervised and
weakly-supervised tasks, where target labels are scant or noisy, to
unsupervised tasks where labels are unobtainable. In this paper we study a
scenario where the target labels are not available but additional related
information is at hand. This information, referred to as Side Information, is
either correlated with the unknown labels or imposes constraints on the feature
space. We formulate the problem as an ensemble of three semantic components:
representation learning, side information and metric learning. The proposed
scoring model is advantageous for multiple use-cases. For example, in the
healthcare domain it can be used to create a severity score for diseases where
the symptoms are known but the criteria for the disease progression are not
well defined. We demonstrate the utility of the suggested scoring system on
well-known benchmark data-sets and bio-medical patient records.

</details>


### [164] [Learning from Stochastic Teacher Representations Using Student-Guided Knowledge Distillation](https://arxiv.org/abs/2504.14307)
*Muhammad Haseeb Aslam,Clara Martinez,Marco Pedersoli,Alessandro Koerich,Ali Etemad,Eric Granger*

Main category: cs.LG

TL;DR: 提出了一种基于蒸馏时dropout的随机自蒸馏（SSD）方法，通过学生引导知识蒸馏（SGKD）过滤和加权教师表示，仅从任务相关表示中蒸馏，显著提升了性能且不增加模型复杂度。


<details>
  <summary>Details</summary>
Motivation: 解决传统集成学习和权重平均方法需要训练多个模型且资源消耗大的问题，特别是在资源受限或延迟敏感的应用中。

Method: 训练单一模型，利用蒸馏时dropout生成多样教师表示，并通过SGKD策略过滤和加权这些表示。

Result: 在多个数据集上，SSD方法优于现有方法，且不增加模型大小和计算复杂度。

Conclusion: SSD方法在资源受限场景下提供了一种高效且高性能的替代方案。

Abstract: Advances in self-distillation have shown that when knowledge is distilled
from a teacher to a student using the same deep learning (DL) architecture, the
student performance can surpass the teacher particularly when the network is
overparameterized and the teacher is trained with early stopping.
Alternatively, ensemble learning also improves performance, although training,
storing, and deploying multiple models becomes impractical as the number of
models grows. Even distilling an ensemble to a single student model or weight
averaging methods first requires training of multiple teacher models and does
not fully leverage the inherent stochasticity for generating and distilling
diversity in DL models. These constraints are particularly prohibitive in
resource-constrained or latency-sensitive applications such as wearable
devices. This paper proposes to train only one model and generate multiple
diverse teacher representations using distillation-time dropout. However,
generating these representations stochastically leads to noisy representations
that are misaligned with the learned task. To overcome this problem, a novel
stochastic self-distillation (SSD) training strategy is introduced for
filtering and weighting teacher representation to distill from task-relevant
representations only, using student-guided knowledge distillation (SGKD). The
student representation at each distillation step is used as authority to guide
the distillation process. Experimental results on real-world affective
computing, wearable/biosignal datasets from the UCR Archive, the HAR dataset,
and image classification datasets show that the proposed SSD method can
outperform state-of-the-art methods without increasing the model size at both
training and testing time, and incurs negligible computational complexity
compared to state-of-the-art ensemble learning and weight averaging methods.

</details>


### [165] [Local distribution-based adaptive oversampling for imbalanced regression](https://arxiv.org/abs/2504.14316)
*Shayan Alahyari,Mike Domaratzki*

Main category: cs.LG

TL;DR: 论文提出了一种名为LDAO的新方法，用于解决回归任务中目标变量分布不平衡的问题，通过局部分布自适应过采样，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 回归任务中目标变量的不平衡分布导致稀疏区域难以预测，现有方法依赖阈值分类或生成无效合成样本，效果有限。

Method: LDAO通过分解数据集为局部分布，独立建模和采样，合并为平衡训练集，保留统计特性。

Result: 在45个不平衡数据集上，LDAO优于现有过采样方法，对常见和罕见目标值均有效。

Conclusion: LDAO为不平衡回归问题提供了一种有效的数据级解决方案，保留了数据的统计结构。

Abstract: Imbalanced regression occurs when continuous target variables have skewed
distributions, creating sparse regions that are difficult for machine learning
models to predict accurately. This issue particularly affects neural networks,
which often struggle with imbalanced data. While class imbalance in
classification has been extensively studied, imbalanced regression remains
relatively unexplored, with few effective solutions. Existing approaches often
rely on arbitrary thresholds to categorize samples as rare or frequent,
ignoring the continuous nature of target distributions. These methods can
produce synthetic samples that fail to improve model performance and may
discard valuable information through undersampling. To address these
limitations, we propose LDAO (Local Distribution-based Adaptive Oversampling),
a novel data-level approach that avoids categorizing individual samples as rare
or frequent. Instead, LDAO learns the global distribution structure by
decomposing the dataset into a mixture of local distributions, each preserving
its statistical characteristics. LDAO then models and samples from each local
distribution independently before merging them into a balanced training set.
LDAO achieves a balanced representation across the entire target range while
preserving the inherent statistical structure within each local distribution.
In extensive evaluations on 45 imbalanced datasets, LDAO outperforms
state-of-the-art oversampling methods on both frequent and rare target values,
demonstrating its effectiveness for addressing the challenge of imbalanced
regression.

</details>


### [166] [Integrating Single-Cell Foundation Models with Graph Neural Networks for Drug Response Prediction](https://arxiv.org/abs/2504.14361)
*Till Rossner,Ziteng Li,Jonas Balke,Nikoo Salehfard,Tom Seifert,Ming Tang*

Main category: cs.LG

TL;DR: 提出了一种结合scGPT和DeepCDR的创新方法，用于预测癌症药物反应（CDR），效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 提高癌症药物反应预测的准确性，探索scGPT在基因表达数据中的应用潜力。

Method: 利用scGPT生成基因表达数据的嵌入表示，作为DeepCDR的输入。

Result: 实验表明，该方法优于原DeepCDR模型和scFoundation模型。

Conclusion: scGPT嵌入能显著提升CDR预测精度，为现有方法提供了有前景的替代方案。

Abstract: In this study, we propose an innovative methodology for predicting Cancer
Drug Response (CDR) through the integration of the scGPT foundation model
within the DeepCDR model. Our approach utilizes scGPT to generate embeddings
from gene expression data, which are then used as gene expression input data
for DeepCDR. The experimental findings demonstrate the efficacy of this
scGPT-based method in outperforming previous related works, including the
original DeepCDR model and the scFoundation-based model. This study highlights
the potential of scGPT embeddings to enhance the accuracy of CDR predictions
and offers a promising alternative to existing approaches.

</details>


### [167] [Improving RL Exploration for LLM Reasoning through Retrospective Replay](https://arxiv.org/abs/2504.14363)
*Shihan Dou,Muling Wu,Jingwen Xu,Rui Zheng,Tao Gui,Qi Zhang,Xuanjing Huang*

Main category: cs.LG

TL;DR: 论文提出了一种名为RRL的新算法，通过动态重放机制解决强化学习在训练大型语言模型时的探索效率问题，显著提升了复杂推理任务的性能。


<details>
  <summary>Details</summary>
Motivation: 在强化学习训练过程中，早期阶段模型虽有探索能力但无法解决问题，而后期能力提升后却因早期潜在解决方案被抑制而难以重新探索。

Method: 提出Retrospective Replay-based Reinforcement Learning (RRL)算法，动态重放早期有潜力的状态，提升探索效率。

Result: 实验表明RRL在复杂推理任务（如数学推理和代码生成）及对话任务中均显著提升了模型性能，同时优化了RLHF的效果。

Conclusion: RRL通过动态重放机制有效解决了探索效率问题，为强化学习优化大型语言模型提供了新思路。

Abstract: Reinforcement learning (RL) has increasingly become a pivotal technique in
the post-training of large language models (LLMs). The effective exploration of
the output space is essential for the success of RL. We observe that for
complex problems, during the early stages of training, the model exhibits
strong exploratory capabilities and can identify promising solution ideas.
However, its limited capability at this stage prevents it from successfully
solving these problems. The early suppression of these potentially valuable
solution ideas by the policy gradient hinders the model's ability to revisit
and re-explore these ideas later. Consequently, although the LLM's capabilities
improve in the later stages of training, it still struggles to effectively
address these complex problems. To address this exploration issue, we propose a
novel algorithm named Retrospective Replay-based Reinforcement Learning (RRL),
which introduces a dynamic replay mechanism throughout the training process.
RRL enables the model to revisit promising states identified in the early
stages, thereby improving its efficiency and effectiveness in exploration. To
evaluate the effectiveness of RRL, we conduct extensive experiments on complex
reasoning tasks, including mathematical reasoning and code generation, and
general dialogue tasks. The results indicate that RRL maintains high
exploration efficiency throughout the training period, significantly enhancing
the effectiveness of RL in optimizing LLMs for complicated reasoning tasks.
Moreover, it also improves the performance of RLHF, making the model both safer
and more helpful.

</details>


### [168] [Accelerating LLM Inference with Flexible N:M Sparsity via A Fully Digital Compute-in-Memory Accelerator](https://arxiv.org/abs/2504.14365)
*Akshat Ramachandran,Souvik Kundu,Arnab Raha,Shamik Kundu,Deepak K. Mathaikutty,Tushar Krishna*

Main category: cs.LG

TL;DR: 论文提出了一种灵活的层间N:M稀疏选择方法（FLOW）和低开销的数字内存计算架构（FlexCiM），显著提升了大型语言模型的性能和效率。


<details>
  <summary>Details</summary>
Motivation: 现有N:M结构化稀疏方法限制了模型的表达能力，而支持多种N:M模式又带来硬件开销问题。

Method: FLOW通过考虑异常值的分布动态选择最优的N和M值；FlexCiM通过分区和动态聚合机制支持多样化的稀疏模式。

Result: FLOW在准确率上提升高达36%，FlexCiM降低推理延迟1.75倍，能耗减少1.5倍。

Conclusion: FLOW和FlexCiM结合解决了稀疏模型的表达自由和硬件效率问题，为大型语言模型提供了高效解决方案。

Abstract: Large language model (LLM) pruning with fixed N:M structured sparsity
significantly limits the expressivity of the sparse model, yielding sub-optimal
performance. In contrast, supporting multiple N:M patterns to provide sparse
representational freedom introduces costly overhead in hardware. To address
these challenges for LLMs, we first present a flexible layer-wise
outlier-density-aware N:M sparsity (FLOW) selection method. FLOW enables the
identification of optimal layer-wise N and M values (from a given range) by
simultaneously accounting for the presence and distribution of outliers,
allowing a higher degree of representational freedom. To deploy sparse models
with such N:M flexibility, we then introduce a flexible, low-overhead digital
compute-in-memory architecture (FlexCiM). FlexCiM supports diverse sparsity
patterns by partitioning a digital CiM (DCiM) macro into smaller sub-macros,
which are adaptively aggregated and disaggregated through distribution and
merging mechanisms for different N and M values. Extensive experiments on both
transformer-based and recurrence-based state space foundation models (SSMs)
demonstrate that FLOW outperforms existing alternatives with an accuracy
improvement of up to 36%, while FlexCiM achieves up to 1.75x lower inference
latency and 1.5x lower energy consumption compared to existing sparse
accelerators. Code is available at: https://github.com/FLOW-open-project/FLOW

</details>


### [169] [Do You Really Need Public Data? Surrogate Public Data for Differential Privacy on Tabular Data](https://arxiv.org/abs/2504.14368)
*Shlomi Hod,Lucas Rosenblatt,Julia Stoyanovich*

Main category: cs.LG

TL;DR: 论文提出利用强大的先验知识生成替代公共数据，以解决差分隐私机器学习中公共数据不足的问题，特别是在表格数据领域。


<details>
  <summary>Details</summary>
Motivation: 差分隐私机器学习通常依赖公共数据，但表格数据的异构性导致公共数据难以获取。

Method: 提出两种方法：直接生成CSV文件和自动构建结构因果模型（SCM），利用大型语言模型（LLM）生成替代公共数据。

Result: 实验表明，替代公共数据能有效替代传统公共数据用于差分隐私分类器的预训练，并在其他任务中也有一定作用。

Conclusion: 替代公共数据为差分隐私机器学习提供了一种无需敏感数据的可行解决方案。

Abstract: Differentially private (DP) machine learning often relies on the availability
of public data for tasks like privacy-utility trade-off estimation,
hyperparameter tuning, and pretraining. While public data assumptions may be
reasonable in text and image domains, they are less likely to hold for tabular
data due to tabular data heterogeneity across domains. We propose leveraging
powerful priors to address this limitation; specifically, we synthesize
realistic tabular data directly from schema-level specifications - such as
variable names, types, and permissible ranges - without ever accessing
sensitive records. To that end, this work introduces the notion of "surrogate"
public data - datasets generated independently of sensitive data, which consume
no privacy loss budget and are constructed solely from publicly available
schema or metadata. Surrogate public data are intended to encode plausible
statistical assumptions (informed by publicly available information) into a
dataset with many downstream uses in private mechanisms. We automate the
process of generating surrogate public data with large language models (LLMs);
in particular, we propose two methods: direct record generation as CSV files,
and automated structural causal model (SCM) construction for sampling records.
Through extensive experiments, we demonstrate that surrogate public tabular
data can effectively replace traditional public data when pretraining
differentially private tabular classifiers. To a lesser extent, surrogate
public data are also useful for hyperparameter tuning of DP synthetic data
generators, and for estimating the privacy-utility tradeoff.

</details>


### [170] [Learning Enhanced Structural Representations with Block-Based Uncertainties for Ocean Floor Mapping](https://arxiv.org/abs/2504.14372)
*Jose Marie Antonio Minoza*

Main category: cs.LG

TL;DR: 该论文提出了一种基于块状空间预测的深度学习框架，用于提高海底地形数据的分辨率和不确定性量化能力。


<details>
  <summary>Details</summary>
Motivation: 当前全球海底地形数据分辨率不足，难以支持精确的数值模拟和沿海灾害预测。现有深度学习方法在保持物理结构一致性和量化不确定性方面存在挑战。

Method: 采用基于块状空间预测的VQ-VAE架构，结合不确定性量化机制，生成空间自适应的置信度估计，同时通过离散潜在表示保留地形特征。

Result: 实验结果表明，该方法在多个海域显著提高了重建质量和不确定性估计的可靠性。

Conclusion: 该框架通过保持结构完整性和提供空间自适应不确定性估计，为更可靠的气候建模和沿海灾害评估奠定了基础。

Abstract: Accurate ocean modeling and coastal hazard prediction depend on
high-resolution bathymetric data; yet, current worldwide datasets are too
coarse for exact numerical simulations. While recent deep learning advances
have improved earth observation data resolution, existing methods struggle with
the unique challenges of producing detailed ocean floor maps, especially in
maintaining physical structure consistency and quantifying uncertainties. This
work presents a novel uncertainty-aware mechanism using spatial blocks to
efficiently capture local bathymetric complexity based on block-based conformal
prediction. Using the Vector Quantized Variational Autoencoder (VQ-VAE)
architecture, the integration of this uncertainty quantification framework
yields spatially adaptive confidence estimates while preserving topographical
features via discrete latent representations. With smaller uncertainty widths
in well-characterized areas and appropriately larger bounds in areas of complex
seafloor structures, the block-based design adapts uncertainty estimates to
local bathymetric complexity. Compared to conventional techniques, experimental
results over several ocean regions show notable increases in both
reconstruction quality and uncertainty estimation reliability. This framework
increases the reliability of bathymetric reconstructions by preserving
structural integrity while offering spatially adaptive uncertainty estimates,
so opening the path for more solid climate modeling and coastal hazard
assessment.

</details>


### [171] [Bottom-Up Synthesis of Knowledge-Grounded Task-Oriented Dialogues with Iteratively Self-Refined Prompts](https://arxiv.org/abs/2504.14375)
*Kun Qian,Maximillian Chen,Siyan Li,Arpit Sharma,Zhou Yu*

Main category: cs.LG

TL;DR: 论文提出了一种自底向上的对话合成方法，通过先生成问答对再组合成对话，提高了数据生成的控制性和质量。


<details>
  <summary>Details</summary>
Motivation: 解决传统自上而下方法在生成对话时内容控制不足和易产生幻觉的问题。

Method: 采用两步法：首先生成问答对，再组合成连贯对话，同时允许使用非本地模型。

Result: 人类和自动评估表明，该方法生成的对话更真实、质量更高。

Conclusion: 自底向上的方法在对话生成中具有更高的控制性和质量优势。

Abstract: Training conversational question-answering (QA) systems requires a
substantial amount of in-domain data, which is often scarce in practice. A
common solution to this challenge is to generate synthetic data. Traditional
methods typically follow a top-down approach, where a large language model
(LLM) generates multi-turn dialogues from a broad prompt. Although this method
produces coherent conversations, it offers limited fine-grained control over
the content and is susceptible to hallucinations. We introduce a bottom-up
conversation synthesis approach, where QA pairs are generated first and then
combined into a coherent dialogue. This method offers greater control and
precision by dividing the process into two distinct steps, allowing refined
instructions and validations to be handled separately. Additionally, this
structure allows the use of non-local models in stages that do not involve
proprietary knowledge, enhancing the overall quality of the generated data.
Both human and automated evaluations demonstrate that our approach produces
more realistic and higher-quality dialogues compared to top-down methods.

</details>


### [172] [Balancing Fairness and Performance in Healthcare AI: A Gradient Reconciliation Approach](https://arxiv.org/abs/2504.14388)
*Xiaoyang Wang,Christopher C. Yang*

Main category: cs.LG

TL;DR: 论文提出FairGrad框架，通过梯度协调平衡医疗AI模型的预测性能与多属性公平性，避免加剧医疗不平等。


<details>
  <summary>Details</summary>
Motivation: 医疗AI的快速应用可能加剧现有医疗不平等，需平衡公平性与预测性能。

Method: FairGrad通过将梯度向量投影到正交平面，协调冲突的优化目标，确保公平性。

Result: 在真实医疗数据上，FairGrad显著提升多属性公平性指标，同时保持预测准确性。

Conclusion: FairGrad证明在关键医疗AI应用中平衡公平性与实用性是可行的。

Abstract: The rapid growth of healthcare data and advances in computational power have
accelerated the adoption of artificial intelligence (AI) in medicine. However,
AI systems deployed without explicit fairness considerations risk exacerbating
existing healthcare disparities, potentially leading to inequitable resource
allocation and diagnostic disparities across demographic subgroups. To address
this challenge, we propose FairGrad, a novel gradient reconciliation framework
that automatically balances predictive performance and multi-attribute fairness
optimization in healthcare AI models. Our method resolves conflicting
optimization objectives by projecting each gradient vector onto the orthogonal
plane of the others, thereby regularizing the optimization trajectory to ensure
equitable consideration of all objectives. Evaluated on diverse real-world
healthcare datasets and predictive tasks - including Substance Use Disorder
(SUD) treatment and sepsis mortality - FairGrad achieved statistically
significant improvements in multi-attribute fairness metrics (e.g., equalized
odds) while maintaining competitive predictive accuracy. These results
demonstrate the viability of harmonizing fairness and utility in
mission-critical medical AI applications.

</details>


### [173] [Exploring Pseudo-Token Approaches in Transformer Neural Processes](https://arxiv.org/abs/2504.14416)
*Jose Lara-Rangel,Nanze Chen,Fengzhe Zhang*

Main category: cs.LG

TL;DR: ISANPs是一种新型的神经过程模型，通过改进查询效率和计算复杂度，在性能与计算需求之间取得平衡，适用于大规模数据集。


<details>
  <summary>Details</summary>
Motivation: 传统神经过程（NPs）容易欠拟合，而Transformer神经过程（TNPs）虽性能优越，但计算复杂度高，限制了实际应用。

Method: 提出ISANPs，采用诱导集注意力机制和创新的查询阶段，减少计算需求。

Result: ISANPs在1D回归、图像补全等任务中表现优异，与TNPs相当甚至超越现有模型。

Conclusion: ISANPs在性能和计算效率之间提供了可调节的平衡，适用于大规模数据集。

Abstract: Neural Processes (NPs) have gained attention in meta-learning for their
ability to quantify uncertainty, together with their rapid prediction and
adaptability. However, traditional NPs are prone to underfitting. Transformer
Neural Processes (TNPs) significantly outperform existing NPs, yet their
applicability in real-world scenarios is hindered by their quadratic
computational complexity relative to both context and target data points. To
address this, pseudo-token-based TNPs (PT-TNPs) have emerged as a novel NPs
subset that condense context data into latent vectors or pseudo-tokens,
reducing computational demands. We introduce the Induced Set Attentive Neural
Processes (ISANPs), employing Induced Set Attention and an innovative query
phase to improve querying efficiency. Our evaluations show that ISANPs perform
competitively with TNPs and often surpass state-of-the-art models in 1D
regression, image completion, contextual bandits, and Bayesian optimization.
Crucially, ISANPs offer a tunable balance between performance and computational
complexity, which scale well to larger datasets where TNPs face limitations.

</details>


### [174] [LoRe: Personalizing LLMs via Low-Rank Reward Modeling](https://arxiv.org/abs/2504.14439)
*Avinandan Bose,Zhihan Xiong,Yuejie Chi,Simon Shaolei Du,Lin Xiao,Maryam Fazel*

Main category: cs.LG

TL;DR: 提出了一种基于低秩偏好建模的框架，用于个性化大型语言模型，提升用户满意度和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统RLHF方法依赖单一价值表示，难以适应多样化的用户偏好。

Method: 通过低维子空间表示奖励函数，将个体偏好建模为共享基函数的加权组合。

Result: 在多个偏好数据集上验证，表现出对未见用户的更好泛化和偏好预测准确性。

Conclusion: 该方法避免了僵化的用户分类，同时支持可扩展性和少样本适应。

Abstract: Personalizing large language models (LLMs) to accommodate diverse user
preferences is essential for enhancing alignment and user satisfaction.
Traditional reinforcement learning from human feedback (RLHF) approaches often
rely on monolithic value representations, limiting their ability to adapt to
individual preferences. We introduce a novel framework that leverages low-rank
preference modeling to efficiently learn and generalize user-specific reward
functions. By representing reward functions in a low-dimensional subspace and
modeling individual preferences as weighted combinations of shared basis
functions, our approach avoids rigid user categorization while enabling
scalability and few-shot adaptation. We validate our method on multiple
preference datasets, demonstrating superior generalization to unseen users and
improved accuracy in preference prediction tasks.

</details>


### [175] [A computational framework for longitudinal medication adherence prediction in breast cancer survivors: A social cognitive theory based approach](https://arxiv.org/abs/2504.14469)
*Navreet Kaur,Manuel Gonzales IV,Cristian Garcia Alcaraz,Jiaqi Gong,Kristen J. Wells,Laura E. Barnes*

Main category: cs.LG

TL;DR: 该研究开发了一个多尺度模型，用于预测乳腺癌幸存者的药物依从性，结合动态和静态因素，显著提高了预测准确性。


<details>
  <summary>Details</summary>
Motivation: 药物不依从性是慢性病患者的重要问题，影响生存率和生活质量。乳腺癌幸存者的内分泌治疗依从性与复发率密切相关，因此需要理解影响依从性的因素。

Method: 基于社会认知理论，构建多尺度（每日和每周）计算模型，结合动态和静态因素预测依从性。

Result: 模型在每日和每周任务中表现优于传统机器学习方法，每日模型准确率达87.25%，每周模型为76.04%。动态因素对每日预测最重要，而动态和静态因素结合对每周预测更有效。

Conclusion: 多尺度模型能有效预测药物依从性，动态和静态因素的结合在不同时间尺度上具有不同重要性。

Abstract: Non-adherence to medications is a critical concern since nearly half of
patients with chronic illnesses do not follow their prescribed medication
regimens, leading to increased mortality, costs, and preventable human
distress. Amongst stage 0-3 breast cancer survivors, adherence to long-term
adjuvant endocrine therapy (i.e., Tamoxifen and aromatase inhibitors) is
associated with a significant increase in recurrence-free survival. This work
aims to develop multi-scale models of medication adherence to understand the
significance of different factors influencing adherence across varying time
frames. We introduce a computational framework guided by Social Cognitive
Theory for multi-scale (daily and weekly) modeling of longitudinal medication
adherence. Our models employ both dynamic medication-taking patterns in the
recent past (dynamic factors) as well as less frequently changing factors
(static factors) for adherence prediction. Additionally, we assess the
significance of various factors in influencing adherence behavior across
different time scales. Our models outperform traditional machine learning
counterparts in both daily and weekly tasks in terms of both accuracy and
specificity. Daily models achieved an accuracy of 87.25%, and weekly models, an
accuracy of 76.04%. Notably, dynamic past medication-taking patterns prove most
valuable for predicting daily adherence, while a combination of dynamic and
static factors is significant for macro-level weekly adherence patterns.

</details>


### [176] [Less is More: Adaptive Coverage for Synthetic Training Data](https://arxiv.org/abs/2504.14508)
*Sasan Tavakkol,Max Springer,Mohammadhossein Bateni,Neslihan Bulut,Vincent Cohen-Addad,MohammadTaghi Hajiaghayi*

Main category: cs.LG

TL;DR: 使用LLM生成合成训练数据，并通过最大覆盖问题算法选择代表性子集，提升分类器性能。


<details>
  <summary>Details</summary>
Motivation: 解决快速部署模型时获取大规模标注数据的挑战，特别是在新兴社交媒体趋势或在线滥用分类中。

Method: 提出基于最大覆盖问题的新采样算法，从合成数据中选择代表性子集。

Result: 在子集上训练的分类器性能优于全数据集，且数据量需求减少。

Conclusion: “少即是多”方法提升准确性并提高模型微调效率。

Abstract: Synthetic training data generation with Large Language Models (LLMs) like
Google's Gemma and OpenAI's GPT offer a promising solution to the challenge of
obtaining large, labeled datasets for training classifiers. When rapid model
deployment is critical, such as in classifying emerging social media trends or
combating new forms of online abuse tied to current events, the ability to
generate training data is invaluable. While prior research has examined the
comparability of synthetic data to human-labeled data, this study introduces a
novel sampling algorithm, based on the maximum coverage problem, to select a
representative subset from a synthetically generated dataset. Our results
demonstrate that training a classifier on this contextually sampled subset
achieves superior performance compared to training on the entire dataset. This
"less is more" approach not only improves accuracy but also reduces the volume
of data required, leading to potentially more efficient model fine-tuning.

</details>


### [177] [On Dimension-Free Transformer: An Application of STP to AI](https://arxiv.org/abs/2504.14514)
*Daizhan Cheng*

Main category: cs.LG

TL;DR: 论文提出了一种基于半张量积和超向量投影变换的维度无关变压器（DFT）框架，支持任意维度的输入输出，并证明其在信号处理中更高效。


<details>
  <summary>Details</summary>
Motivation: 传统变压器在处理信号时受限于固定维度，无法灵活适应不同维度的输入输出。

Method: 通过半张量积重新定义超向量，利用投影构造线性变换，并验证每个线性变换，用投影基超向量变换（PBTH）替代。

Result: 提出了维度无关变压器（DFT）框架，支持任意维度的输入输出。

Conclusion: DFT在处理信号时更高效，因其平衡了所有条目的信息。

Abstract: The matrix expressions for every parts of a transformer are firstly
described. Based on semi-tensor product (STP) of matrices the hypervectors are
reconsidered and the linear transformation over hypervectors is constructed by
using projection. Its properties and calculating formulas are obtained. Using
projection-based transformation of hypervector (PBTH), the framework of
dimension-free transformer (DFT) is proposed by verifying each linear
transformation in a transformer and replacing it by a proper PBTH, which allows
the inputs and outputs being of arbitrary dimensions. Using balanced
information about all entries, DFT must be more efficient in dealing with
signals.

</details>


### [178] [SlimPipe: Memory-Thrifty and Efficient Pipeline Parallelism for Long-Context LLM Training](https://arxiv.org/abs/2504.14519)
*Zhouyang Li,Yuliang Liu,Wei Zhang,Tailing Yuan,Bin Chen,Chengru Song,Di Zhang*

Main category: cs.LG

TL;DR: SlimPipe是一种细粒度流水线并行方法，通过均匀序列切片和1F1B调度，显著减少激活内存消耗和流水线气泡，提升大语言模型训练效率。


<details>
  <summary>Details</summary>
Motivation: 现有流水线并行方法在长上下文场景中无法解决激活内存压力和流水线气泡问题，限制了训练效率。

Method: 采用均匀序列切片和1F1B调度，结合负载均衡技术，减少激活内存和流水线气泡。

Result: 在Llama 70B模型上，SlimPipe将MFU提升至1.57倍，并在2048K上下文长度下保持45%以上的利用率。

Conclusion: SlimPipe有效解决了长上下文训练中的内存和效率问题，显著优于现有方法。

Abstract: Pipeline Parallelism (PP) serves as a crucial technique for training Large
Language Models (LLMs), owing to its capability to alleviate memory pressure
from model states with relatively low communication overhead. However, in
long-context scenarios, existing pipeline parallelism methods fail to address
the substantial activation memory pressure, primarily due to the peak memory
consumption resulting from the accumulation of activations across multiple
microbatches. Moreover, these approaches inevitably introduce considerable
pipeline bubbles, further hindering efficiency.
  To tackle these challenges, we propose SlimPipe, a novel approach to
fine-grained pipeline parallelism that employs uniform sequence slicing coupled
with one-forward-one-backward (1F1B) schedule. It reduces the accumulated
activations from several microbatches to just one, which is split into several
slices. Although the slices are evenly partitioned, the computation cost is not
equal across slices due to causal attention. We develop a sophisticated
workload redistribution technique to address this load imbalance. SlimPipe
achieves (1) near-zero memory overhead and (2) minimal pipeline bubbles
simultaneously. The effectiveness of SlimPipe has been proven by thorough
testing with diverse model architectures, context window sizes, and
SlimPipe-specific configurations. For example, on the Llama 70B model, compared
to state-of-the-art methods, SlimPipe significantly boosts the Model FLOPs
Utilization (MFU) to up to $1.57\times$ for a context length of 512K. More
notably, for a context length of 2048K, it maintains over 45% utilization on
256 NVIDIA Hopper 80GB GPUs, while other approaches either suffer significant
performance drops or fail entirely due to memory constraints.

</details>


### [179] [TrustLoRA: Low-Rank Adaptation for Failure Detection under Out-of-distribution Data](https://arxiv.org/abs/2504.14545)
*Fei Zhu,Zhaoxiang Zhang*

Main category: cs.LG

TL;DR: 提出了一种简单的失败检测框架，统一并优化了在协变量和语义偏移下的分类与拒绝任务。


<details>
  <summary>Details</summary>
Motivation: 在开放环境中，可靠的预测对深度神经网络模型至关重要，需要同时拒绝协变量偏移和语义偏移的错误案例。

Method: 通过分离和整合失败特定的可靠性知识，利用低秩适配器增强失败检测能力。

Result: 大量实验证明了该框架的优越性。

Conclusion: 该框架有效且灵活地提升了失败检测能力。

Abstract: Reliable prediction is an essential requirement for deep neural models that
are deployed in open environments, where both covariate and semantic
out-of-distribution (OOD) data arise naturally. In practice, to make safe
decisions, a reliable model should accept correctly recognized inputs while
rejecting both those misclassified covariate-shifted and semantic-shifted
examples. Besides, considering the potential existing trade-off between
rejecting different failure cases, more convenient, controllable, and flexible
failure detection approaches are needed. To meet the above requirements, we
propose a simple failure detection framework to unify and facilitate
classification with rejection under both covariate and semantic shifts. Our key
insight is that by separating and consolidating failure-specific reliability
knowledge with low-rank adapters and then integrating them, we can enhance the
failure detection ability effectively and flexibly. Extensive experiments
demonstrate the superiority of our framework.

</details>


### [180] [NoWag: A Unified Framework for Shape Preserving Compression of Large Language Models](https://arxiv.org/abs/2504.14569)
*Lawrence Liu,Inesh Chakrabarti,Yixiao Li,Mengdi Wang,Tuo Zhao,Lin F. Yang*

Main category: cs.LG

TL;DR: NoWag是一种用于零样本形状保持压缩的统一框架，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在计算和内存需求上存在限制，难以在资源受限环境中部署。

Method: 提出NoWag框架，包括向量量化（NoWag-VQ）和剪枝（NoWag-P）两种压缩方法。

Result: NoWag-VQ显著优于现有零样本VQ方法，NoWag-P与现有方法竞争性相当。

Conclusion: NoWag框架展示了压缩范式的共性，为未来研究提供了启发。

Abstract: Large language models (LLMs) exhibit remarkable performance across various
natural language processing tasks but suffer from immense computational and
memory demands, limiting their deployment in resource-constrained environments.
To address this challenge, we propose NoWag: (Normalized Weight and Activation
Guided Compression), a unified framework for zero-shot shape preserving
compression algorithms. We compressed Llama-2 7B/13B/70B and Llama-3 8/70BB
models, using two popular forms of shape-preserving compression, vector
quantization NoWag-VQ (NoWag for Vector Quantization), and
unstructured/semi-structured pruning NoWag-P (NoWag for Pruning). We found that
NoWag-VQ significantly outperforms state-of-the-art zero shot VQ, and that
NoWag-P performs competitively against state-of-the-art methods. These results
suggest commonalities between these compression paradigms that could inspire
future work. Our code is available at https://github.com/LawrenceRLiu/NoWag

</details>


### [181] [Data Selection for ERMs](https://arxiv.org/abs/2504.14572)
*Steve Hanneke,Shay Moran,Alexander Shlimovich,Amir Yehudayoff*

Main category: cs.LG

TL;DR: 论文探讨了从数据为中心的角度优化训练数据，研究了在有限数据选择预算下如何通过选择少量数据点达到与全量数据相当的模型性能。


<details>
  <summary>Details</summary>
Motivation: 传统学习理论侧重于模型设计，而本文关注如何通过优化数据选择提升学习规则的效果。

Method: 研究了多种经验风险最小化方法，包括均值估计、线性分类和线性回归，并提出了数据选择的理论界限。

Result: 得出了均值估计、线性分类和线性回归的最优数据选择界限，并建立了二元分类和随机凸优化的错误率分类体系。

Conclusion: 提出了未来研究方向，强调了数据选择在提升学习性能中的重要性。

Abstract: Learning theory has traditionally followed a model-centric approach, focusing
on designing optimal algorithms for a fixed natural learning task (e.g., linear
classification or regression). In this paper, we adopt a complementary
data-centric perspective, whereby we fix a natural learning rule and focus on
optimizing the training data. Specifically, we study the following question:
given a learning rule $\mathcal{A}$ and a data selection budget $n$, how well
can $\mathcal{A}$ perform when trained on at most $n$ data points selected from
a population of $N$ points? We investigate when it is possible to select $n \ll
N$ points and achieve performance comparable to training on the entire
population.
  We address this question across a variety of empirical risk minimizers. Our
results include optimal data-selection bounds for mean estimation, linear
classification, and linear regression. Additionally, we establish two general
results: a taxonomy of error rates in binary classification and in stochastic
convex optimization. Finally, we propose several open questions and directions
for future research.

</details>


### [182] [Generative Auto-Bidding with Value-Guided Explorations](https://arxiv.org/abs/2504.14587)
*Jingtong Gao,Yewen Li,Shuai Mao,Peng Jiang,Nan Jiang,Yejing Wang,Qingpeng Cai,Fei Pan,Peng Jiang,Kun Gai,Bo An,Xiangyu Zhao*

Main category: cs.LG

TL;DR: 论文提出了一种新的离线生成自动竞价框架GAVE，通过值引导探索解决现有方法在适应性和历史依赖性上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有自动竞价方法（如基于规则或强化学习）在动态市场适应性、历史依赖性捕捉及多目标策略适应性上存在局限，且离线训练易导致行为模式固定和崩溃。

Method: GAVE框架结合了基于分数的RTG模块适应多目标，并通过动作探索机制和RTG评估方法探索新动作，同时设计可学习值函数引导探索方向。

Result: 实验表明，GAVE在离线评估和在线A/B测试中优于现有基线方法。

Conclusion: GAVE通过创新框架解决了自动竞价中的关键问题，并在实际部署中验证了其有效性。

Abstract: Auto-bidding, with its strong capability to optimize bidding decisions within
dynamic and competitive online environments, has become a pivotal strategy for
advertising platforms. Existing approaches typically employ rule-based
strategies or Reinforcement Learning (RL) techniques. However, rule-based
strategies lack the flexibility to adapt to time-varying market conditions, and
RL-based methods struggle to capture essential historical dependencies and
observations within Markov Decision Process (MDP) frameworks. Furthermore,
these approaches often face challenges in ensuring strategy adaptability across
diverse advertising objectives. Additionally, as offline training methods are
increasingly adopted to facilitate the deployment and maintenance of stable
online strategies, the issues of documented behavioral patterns and behavioral
collapse resulting from training on fixed offline datasets become increasingly
significant. To address these limitations, this paper introduces a novel
offline Generative Auto-bidding framework with Value-Guided Explorations
(GAVE). GAVE accommodates various advertising objectives through a score-based
Return-To-Go (RTG) module. Moreover, GAVE integrates an action exploration
mechanism with an RTG-based evaluation method to explore novel actions while
ensuring stability-preserving updates. A learnable value function is also
designed to guide the direction of action exploration and mitigate
Out-of-Distribution (OOD) problems. Experimental results on two offline
datasets and real-world deployments demonstrate that GAVE outperforms
state-of-the-art baselines in both offline evaluations and online A/B tests.
The implementation code is publicly available to facilitate reproducibility and
further research.

</details>


### [183] [No Imputation of Missing Values In Tabular Data Classification Using Incremental Learning](https://arxiv.org/abs/2504.14610)
*Manar D. Samad,Kazi Fuad B. Akhter,Shourav B. Rabbani,Ibna Kowsar*

Main category: cs.LG

TL;DR: 论文提出了一种无需填补缺失值的增量学习方法（NIIL），通过注意力掩码排除缺失值，在15个数据集上表现优于11种现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统填补缺失值的方法可能引发计算复杂度、数据质量和结果可靠性的担忧，NIIL旨在消除这些担忧。

Method: NIIL通过增量学习重叠特征集的分区，并使用注意力掩码排除缺失值。

Result: 在15个数据集上，NIIL的分类性能优于11种现有方法，且对缺失值类型和比例具有鲁棒性。

Conclusion: NIIL是首个无需填补缺失值的深度学习解决方案，特征分区大小为原始特征空间的一半时效果最佳。

Abstract: Tabular data sets with varying missing values are prepared for machine
learning using an arbitrary imputation strategy. Synthetic values generated by
imputation models often concern data stakeholders about computational
complexity, data quality, and data-driven outcomes. This paper eliminates these
concerns by proposing no imputation incremental learning (NIIL) of tabular data
with varying missing value rates and types. The proposed method incrementally
learns partitions of overlapping feature sets while using attention masks to
exclude missing values from attention scoring. The average classification
performance rank order across 15 diverse tabular data sets highlights the
superiority of NIIL over 11 state-of-the-art learning methods with or without
missing value imputations. Further experiments substantiate the robustness of
NIIL against varying missing value types and rates compared to methods that
involve the imputation of missing values. Our empirical analysis reveals that a
feature partition size of half of the original feature space is,
computation-wise and accuracy-wise, the best choice for the proposed
incremental learning. The proposed method is one of the first deep learning
solutions that can effectively learn tabular data without requiring the
imputation of missing values.

</details>


### [184] [AlphaZero-Edu: Making AlphaZero Accessible to Everyone](https://arxiv.org/abs/2504.14636)
*Binjie Guo,Hanyu Zheng,Guowei Su,Ru Zhang,Haohan Jiang,Xurong Lin,Hongyan Wei,Aisheng Mo,Jie Li,Zhiyuan Qian,Zhuhao Zhang,Xiaoyuan Cheng*

Main category: cs.LG

TL;DR: AlphaZero-Edu是一个轻量级、教育导向的强化学习框架，基于AlphaZero数学框架，解决了现有框架实现复杂和可复现性差的问题。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习框架实现复杂且可复现性差，AlphaZero-Edu旨在提供一个轻量、透明的解决方案。

Method: 采用模块化架构，优化资源效率，支持单GPU训练，并行化自对弈数据生成。

Result: 在Gomoku比赛中表现优异，对人类的胜率持续较高，实现了3.2倍的加速。

Conclusion: AlphaZero-Edu开源，为学术研究和工业应用提供了实用基准。

Abstract: Recent years have witnessed significant progress in reinforcement learning,
especially with Zero-like paradigms, which have greatly boosted the
generalization and reasoning abilities of large-scale language models.
Nevertheless, existing frameworks are often plagued by high implementation
complexity and poor reproducibility. To tackle these challenges, we present
AlphaZero-Edu, a lightweight, education-focused implementation built upon the
mathematical framework of AlphaZero. It boasts a modular architecture that
disentangles key components, enabling transparent visualization of the
algorithmic processes. Additionally, it is optimized for resource-efficient
training on a single NVIDIA RTX 3090 GPU and features highly parallelized
self-play data generation, achieving a 3.2-fold speedup with 8 processes. In
Gomoku matches, the framework has demonstrated exceptional performance,
achieving a consistently high win rate against human opponents. AlphaZero-Edu
has been open-sourced at https://github.com/StarLight1212/AlphaZero_Edu,
providing an accessible and practical benchmark for both academic research and
industrial applications.

</details>


### [185] [Surrogate Fitness Metrics for Interpretable Reinforcement Learning](https://arxiv.org/abs/2504.14645)
*Philipp Altmann,Céline Davignon,Maximilian Zorn,Fabian Ritz,Claudia Linnhoff-Popien,Thomas Gabor*

Main category: cs.LG

TL;DR: 通过进化优化框架生成多样且信息丰富的策略演示，结合局部多样性、行为确定性和全局多样性优化轨迹选择，显著提升RL策略的可解释性。


<details>
  <summary>Details</summary>
Motivation: 提升强化学习（RL）策略的可解释性，特别是在安全关键和需解释性强的领域。

Method: 采用进化优化框架，通过扰动初始状态生成演示，并利用联合代理适应度函数（结合多样性、行为确定性等）优化轨迹选择。

Result: 在离散和连续环境中，优化后的演示显著提升了策略的可解释性；在网格世界中，演示保真度优于随机和消融基线。

Conclusion: 通过优化代理适应度函数，本研究提升了RL模型的可解释性，为决策提供更深入的见解。

Abstract: We employ an evolutionary optimization framework that perturbs initial states
to generate informative and diverse policy demonstrations. A joint surrogate
fitness function guides the optimization by combining local diversity,
behavioral certainty, and global population diversity. To assess demonstration
quality, we apply a set of evaluation metrics, including the reward-based
optimality gap, fidelity interquartile means (IQMs), fitness composition
analysis, and trajectory visualizations. Hyperparameter sensitivity is also
examined to better understand the dynamics of trajectory optimization. Our
findings demonstrate that optimizing trajectory selection via surrogate fitness
metrics significantly improves interpretability of RL policies in both discrete
and continuous environments. In gridworld domains, evaluations reveal
significantly enhanced demonstration fidelities compared to random and ablated
baselines. In continuous control, the proposed framework offers valuable
insights, particularly for early-stage policies, while fidelity-based
optimization proves more effective for mature policies. By refining and
systematically analyzing surrogate fitness functions, this study advances the
interpretability of RL models. The proposed improvements provide deeper
insights into RL decision-making, benefiting applications in safety-critical
and explainability-focused domains.

</details>


### [186] [LeetCodeDataset: A Temporal Dataset for Robust Evaluation and Efficient Training of Code LLMs](https://arxiv.org/abs/2504.14655)
*Yunhui Xia,Wei Shen,Yan Wang,Jason Klein Liu,Huifeng Sun,Siyue Wu,Jian Hu,Xiaolong Xu*

Main category: cs.LG

TL;DR: LeetCodeDataset是一个高质量代码生成模型评估与训练基准，解决了LLM研究中缺乏推理导向的编码基准和自包含训练测试的问题。


<details>
  <summary>Details</summary>
Motivation: 解决LLM研究中缺乏推理导向的编码基准和自包含训练测试的挑战。

Method: 通过整理LeetCode Python问题，提供丰富元数据、广泛覆盖、每个问题100+测试用例及时间分割（2024年7月前后），支持无污染评估和高效监督微调（SFT）。

Result: 实验显示推理模型显著优于非推理模型，仅用2.6K模型生成解决方案的SFT性能与110K样本相当。

Conclusion: LeetCodeDataset为代码生成模型提供了高质量评估和训练资源，数据集和评估框架已开源。

Abstract: We introduce LeetCodeDataset, a high-quality benchmark for evaluating and
training code-generation models, addressing two key challenges in LLM research:
the lack of reasoning-focused coding benchmarks and self-contained training
testbeds. By curating LeetCode Python problems with rich metadata, broad
coverage, 100+ test cases per problem, and temporal splits (pre/post July
2024), our dataset enables contamination-free evaluation and efficient
supervised fine-tuning (SFT). Experiments show reasoning models significantly
outperform non-reasoning counterparts, while SFT with only 2.6K model-generated
solutions achieves performance comparable to 110K-sample counterparts. The
dataset and evaluation framework are available on Hugging Face and Github.

</details>


### [187] [Mitigating Parameter Interference in Model Merging via Sharpness-Aware Fine-Tuning](https://arxiv.org/abs/2504.14662)
*Yeoreum Lee,Jinwook Jung,Sungyong Baik*

Main category: cs.LG

TL;DR: 论文提出一种新的微调目标函数，通过锐度感知最小化（SAM）减少参数干扰并提升任务性能，从而提高合并模型的性能。


<details>
  <summary>Details</summary>
Motivation: 解决大规模预训练模型微调后合并时参数干扰的问题，同时提升单任务性能和合并模型性能。

Method: 设计新的微调目标函数，基于锐度感知最小化（SAM）进行微调。

Result: 实验和理论结果表明，该方法有效且与其他方法正交，提升了合并和微调的性能。

Conclusion: 通过SAM微调预训练模型，显著减少了参数干扰并提升了合并模型的性能。

Abstract: Large-scale deep learning models with a pretraining-finetuning paradigm have
led to a surge of numerous task-specific models fine-tuned from a common
pre-trained model. Recently, several research efforts have been made on merging
these large models into a single multi-task model, particularly with simple
arithmetic on parameters. Such merging methodology faces a central challenge:
interference between model parameters fine-tuned on different tasks. Few recent
works have focused on designing a new fine-tuning scheme that can lead to small
parameter interference, however at the cost of the performance of each
task-specific fine-tuned model and thereby limiting that of a merged model. To
improve the performance of a merged model, we note that a fine-tuning scheme
should aim for (1) smaller parameter interference and (2) better performance of
each fine-tuned model on the corresponding task. In this work, we aim to design
a new fine-tuning objective function to work towards these two goals. In the
course of this process, we find such objective function to be strikingly
similar to sharpness-aware minimization (SAM) objective function, which aims to
achieve generalization by finding flat minima. Drawing upon our observation, we
propose to fine-tune pre-trained models via sharpness-aware minimization. The
experimental and theoretical results showcase the effectiveness and
orthogonality of our proposed approach, improving performance upon various
merging and fine-tuning methods. Our code is available at
https://github.com/baiklab/SAFT-Merge.

</details>


### [188] [Efficient Federated Split Learning for Large Language Models over Communication Networks](https://arxiv.org/abs/2504.14667)
*Kai Zhao,Zhaohui Yang*

Main category: cs.LG

TL;DR: FedsLLM框架结合分割联邦学习和参数高效微调技术，降低边缘设备计算负担，优化资源分配和LoRA秩选择，显著减少训练延迟。


<details>
  <summary>Details</summary>
Motivation: 解决资源受限边缘设备上分布式微调大型语言模型的挑战。

Method: 结合模型分割和LoRA技术，引入联邦服务器并行训练，提出联合优化问题并开发交替优化算法。

Result: FedsLLM在保持模型精度的同时显著降低客户端计算需求和训练延迟。

Conclusion: FedsLLM为边缘设备上的分布式微调提供高效解决方案，优化资源利用和训练效率。

Abstract: Fine-tuning pre-trained large language models (LLM) in a distributed manner
poses significant challenges on resource-constrained edge devices. To address
this challenge, we propose FedsLLM, a novel framework that integrates split
federated learning with parameter-efficient fine-tuning techniques. By
leveraging model splitting and Low-Rank Adaptation (LoRA), FedsLLM reduces the
computational burden on edge devices. Furthermore, the introduction of a
federated server facilitates parallel training and enhances privacy. To
accommodate heterogeneous communication conditions and diverse computational
capabilities of edge devices, as well as the impact of LoRA rank selection on
model convergence and training cost, we formulate a joint optimization problem.
The formulated problem jointly optimizes subchannel allocation, power control,
model splitting point selection, and LoRA rank configuration, all aimed at
minimizing total training delay. An alternating optimization algorithm is
developed to efficiently solve this problem and accelerate the training
process. Simulation results demonstrate that the proposed FedsLLM framework
achieves comparable model accuracy while significantly reducing client-side
computational requirements. Furthermore, the proposed resource allocation
scheme and adaptive LoRA rank selection strategy notably reduce the training
latency compared to conventional approaches.

</details>


### [189] [Evaluating Temporal Plasticity in Foundation Time Series Models for Incremental Fine-tuning](https://arxiv.org/abs/2504.14677)
*Jia Liu,Cheng Jinguo,Xia Fang,Zhenyuan Ma,Yuankai Wu*

Main category: cs.LG

TL;DR: 研究探讨了时间序列基础模型通过增量学习持续改进的能力，发现其优于传统深度学习模型。


<details>
  <summary>Details</summary>
Motivation: 探索时间序列基础模型在持续学习中的表现，填补其增量学习能力的研究空白。

Method: 在真实数据集上，通过新颖的持续学习框架评估传统模型和基础模型。

Result: 基础模型（如Time-MoE和Chronos）在增量微调中表现更优，预测准确性持续提升。

Conclusion: 优化基础模型的微调策略比开发领域特定小模型更有价值，为持续学习能力提供了新方法。

Abstract: Time series foundation models excel at diverse time series forecasting tasks,
but their capacity for continuous improvement through incremental learning
remains unexplored. We present the first comprehensive study investigating
these models' temporal plasticity - their ability to progressively enhance
performance through continual learning while maintaining existing capabilities.
Through experiments on real-world datasets exhibiting distribution shifts, we
evaluate both conventional deep learning models and foundation models using a
novel continual learning framework. Our findings reveal that while traditional
models struggle with performance deterioration during incremental fine-tuning,
foundation models like Time-MoE and Chronos demonstrate sustained improvement
in predictive accuracy. This suggests that optimizing foundation model
fine-tuning strategies may be more valuable than developing domain-specific
small models. Our research introduces new evaluation methodologies and insights
for developing foundation time series models with robust continuous learning
capabilities.

</details>


### [190] [Learning Critically: Selective Self Distillation in Federated Learning on Non-IID Data](https://arxiv.org/abs/2504.14694)
*Yuting He,Yiqiang Chen,XiaoDong Yang,Hanchao Yu,Yi-Hua Huang,Yang Gu*

Main category: cs.LG

TL;DR: FedSSD提出了一种选择性自蒸馏方法，通过自适应约束局部更新，提升联邦学习在非IID数据下的性能。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中数据异构性（非IID）导致局部模型偏向局部最优而遗忘全局知识，影响性能和收敛速度。现有方法缺乏适应性，效率不足。

Method: FedSSD通过自蒸馏全局模型知识，并在类和样本级别评估可信度选择性加权，自适应约束局部更新。

Result: 理论分析证明收敛性，实验表明FedSSD在更少通信轮次中优于其他先进方法，具有更好的泛化性和鲁棒性。

Conclusion: FedSSD有效解决了非IID数据问题，提升了联邦学习的性能和效率。

Abstract: Federated learning (FL) enables multiple clients to collaboratively train a
global model while keeping local data decentralized. Data heterogeneity
(non-IID) across clients has imposed significant challenges to FL, which makes
local models re-optimize towards their own local optima and forget the global
knowledge, resulting in performance degradation and convergence slowdown. Many
existing works have attempted to address the non-IID issue by adding an extra
global-model-based regularizing item to the local training but without an
adaption scheme, which is not efficient enough to achieve high performance with
deep learning models. In this paper, we propose a Selective Self-Distillation
method for Federated learning (FedSSD), which imposes adaptive constraints on
the local updates by self-distilling the global model's knowledge and
selectively weighting it by evaluating the credibility at both the class and
sample level. The convergence guarantee of FedSSD is theoretically analyzed and
extensive experiments are conducted on three public benchmark datasets, which
demonstrates that FedSSD achieves better generalization and robustness in fewer
communication rounds, compared with other state-of-the-art FL methods.

</details>


### [191] [Quantitative Clustering in Mean-Field Transformer Models](https://arxiv.org/abs/2504.14697)
*Shi Chen,Zhengjiang Lin,Yury Polyanskiy,Philippe Rigollet*

Main category: cs.LG

TL;DR: 论文研究了深度Transformer模型中token的演化行为，类比为粒子系统，并探讨了其在长时间尺度下的聚类现象，类似于Kuramoto模型的同步行为。


<details>
  <summary>Details</summary>
Motivation: 研究动机是理解Transformer模型中token的动态演化行为及其与Kuramoto模型同步现象的类比关系。

Method: 方法是通过建立均值场Transformer模型的长时间聚类行为，分析其参数假设下正则初始化的指数收缩率。

Result: 结果表明，在特定参数假设下，任何正则初始化的均值场模型都会以指数速度同步到一个Dirac点质量。

Conclusion: 结论是Transformer模型的token演化表现出类似Kuramoto模型的同步行为，且具有定量化的指数收敛速率。

Abstract: The evolution of tokens through a deep transformer models can be modeled as
an interacting particle system that has been shown to exhibit an asymptotic
clustering behavior akin to the synchronization phenomenon in Kuramoto models.
In this work, we investigate the long-time clustering of mean-field transformer
models. More precisely, we establish exponential rates of contraction to a
Dirac point mass for any suitably regular initialization under some assumptions
on the parameters of transformer models, any suitably regular mean-field
initialization synchronizes exponentially fast with some quantitative rates.

</details>


### [192] [Connecting Parameter Magnitudes and Hessian Eigenspaces at Scale using Sketched Methods](https://arxiv.org/abs/2504.14701)
*Andres Fernandez,Frank Schneider,Maren Mahsereci,Philipp Hennig*

Main category: cs.LG

TL;DR: 论文研究了深度神经网络训练中损失Hessian矩阵的顶部特征空间与参数剪枝掩码的稳定性关系，提出了一种基于Grassmannian度量的方法量化两者相似性，并发现大参数倾向于与高曲率方向对齐。


<details>
  <summary>Details</summary>
Motivation: 探索深度神经网络训练中损失Hessian矩阵的顶部特征空间与参数剪枝掩码的稳定性之间的潜在联系。

Method: 开发了一种基于Grassmannian度量的方法（如*overlap*），并设计了一种基于草图SVD的无矩阵算法，以计算大规模网络的Hessian特征对。

Result: 实验表明，参数剪枝掩码与Hessian顶部特征空间的相似性显著高于随机水平，且网络规模越大效果越明显。

Conclusion: 研究揭示了Hessian特征向量倾向于集中在较大参数周围，为理解深度学习的Hessian结构提供了新视角，并提供了大规模分析Hessian的方法。

Abstract: Recently, it has been observed that when training a deep neural net with SGD,
the majority of the loss landscape's curvature quickly concentrates in a tiny
*top* eigenspace of the loss Hessian, which remains largely stable thereafter.
Independently, it has been shown that successful magnitude pruning masks for
deep neural nets emerge early in training and remain stable thereafter. In this
work, we study these two phenomena jointly and show that they are connected: We
develop a methodology to measure the similarity between arbitrary parameter
masks and Hessian eigenspaces via Grassmannian metrics. We identify *overlap*
as the most useful such metric due to its interpretability and stability. To
compute *overlap*, we develop a matrix-free algorithm based on sketched SVDs
that allows us to compute over 1000 Hessian eigenpairs for nets with over 10M
parameters --an unprecedented scale by several orders of magnitude. Our
experiments reveal an *overlap* between magnitude parameter masks and top
Hessian eigenspaces consistently higher than chance-level, and that this effect
gets accentuated for larger network sizes. This result indicates that *top
Hessian eigenvectors tend to be concentrated around larger parameters*, or
equivalently, that *larger parameters tend to align with directions of larger
loss curvature*. Our work provides a methodology to approximate and analyze
deep learning Hessians at scale, as well as a novel insight on the structure of
their eigenspace.

</details>


### [193] [Can We Ignore Labels In Out of Distribution Detection?](https://arxiv.org/abs/2504.14704)
*Hong Yang,Qi Yu,Travis Desel*

Main category: cs.LG

TL;DR: 论文提出了一种理论保证，证明在无标签OOD检测中存在失败条件，并定义了一个新的OOD任务（Adjacent OOD检测）以填补现有基准的安全漏洞。


<details>
  <summary>Details</summary>
Motivation: 研究无标签OOD检测的可行性，揭示其在实际任务中的潜在失败条件，以提升安全关键系统的可靠性。

Method: 从信息论角度提出理论证明，定义新的OOD任务（Adjacent OOD检测），并通过实验验证现有方法的失败情况。

Result: 实验表明现有无标签OOD方法在理论预测的失败条件下表现不佳。

Conclusion: 研究揭示了无标签OOD检测的局限性，为未来研究提供了新的方向和基准。

Abstract: Out-of-distribution (OOD) detection methods have recently become more
prominent, serving as a core element in safety-critical autonomous systems. One
major purpose of OOD detection is to reject invalid inputs that could lead to
unpredictable errors and compromise safety. Due to the cost of labeled data,
recent works have investigated the feasibility of self-supervised learning
(SSL) OOD detection, unlabeled OOD detection, and zero shot OOD detection. In
this work, we identify a set of conditions for a theoretical guarantee of
failure in unlabeled OOD detection algorithms from an information-theoretic
perspective. These conditions are present in all OOD tasks dealing with
real-world data: I) we provide theoretical proof of unlabeled OOD detection
failure when there exists zero mutual information between the learning
objective and the in-distribution labels, a.k.a. 'label blindness', II) we
define a new OOD task - Adjacent OOD detection - that tests for label blindness
and accounts for a previously ignored safety gap in all OOD detection
benchmarks, and III) we perform experiments demonstrating that existing
unlabeled OOD methods fail under conditions suggested by our label blindness
theory and analyze the implications for future research in unlabeled OOD
methods.

</details>


### [194] [Pairwise or Pointwise? Evaluating Feedback Protocols for Bias in LLM-Based Evaluation](https://arxiv.org/abs/2504.14716)
*Tuhina Tripathi,Manya Wadhwa,Greg Durrett,Scott Niekum*

Main category: cs.LG

TL;DR: 研究探讨了反馈协议（绝对评分与相对偏好）对LLM评估可靠性的影响，发现相对偏好易受干扰，绝对评分更稳健。


<details>
  <summary>Details</summary>
Motivation: LLM作为人类标注者的代理在训练和评估中广泛应用，但反馈协议的选择对评估可靠性影响尚未充分研究。

Method: 比较绝对评分和相对偏好两种反馈协议，分析其对评估可靠性和系统偏差的影响。

Result: 相对偏好易受干扰，偏好翻转率35%；绝对评分更稳健，翻转率仅9%。

Conclusion: 建议根据数据集特性和评估目标选择反馈协议，绝对评分更适合避免干扰。

Abstract: Large Language Models (LLMs) are widely used as proxies for human labelers in
both training (Reinforcement Learning from AI Feedback) and large-scale
response evaluation (LLM-as-a-judge). Alignment and evaluation are critical
components in the development of reliable LLMs, and the choice of feedback
protocol plays a central role in both but remains understudied. In this work,
we show that the choice of feedback protocol (absolute scores versus relative
preferences) can significantly affect evaluation reliability and induce
systematic biases. In particular, we show that pairwise evaluation protocols
are more vulnerable to distracted evaluation. Generator models can exploit
spurious attributes (or distractor features) favored by the LLM judge,
resulting in inflated scores for lower-quality outputs and misleading training
signals. We find that absolute scoring is more robust to such manipulation,
producing judgments that better reflect response quality and are less
influenced by distractor features. Our results demonstrate that generator
models can flip preferences by embedding distractor features, skewing
LLM-as-a-judge comparisons and leading to inaccurate conclusions about model
quality in benchmark evaluations. Pairwise preferences flip in about 35% of the
cases, compared to only 9% for absolute scores. We offer recommendations for
choosing feedback protocols based on dataset characteristics and evaluation
objectives.

</details>


### [195] [Semi-parametric Memory Consolidation: Towards Brain-like Deep Continual Learning](https://arxiv.org/abs/2504.14727)
*Geng Liu,Fei Zhu,Rong Feng,Zhiqiang Yi,Shiqi Wang,Gaofeng Meng,Zhaoxiang Zhang*

Main category: cs.LG

TL;DR: 提出一种仿生持续学习框架，结合半参数记忆和醒睡巩固机制，解决深度神经网络在连续任务中的灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在连续任务中容易遗忘已学知识，而人类和动物具备持续学习能力，因此仿生方法可能提供解决方案。

Method: 整合半参数记忆和醒睡巩固机制，模拟人类记忆与学习系统。

Result: 在真实场景（如ImageNet类增量学习）中，模型能保持新任务高性能并保留旧知识。

Conclusion: 仿生智能是赋予深度神经网络持续学习能力的有效途径。

Abstract: Humans and most animals inherently possess a distinctive capacity to
continually acquire novel experiences and accumulate worldly knowledge over
time. This ability, termed continual learning, is also critical for deep neural
networks (DNNs) to adapt to the dynamically evolving world in open
environments. However, DNNs notoriously suffer from catastrophic forgetting of
previously learned knowledge when trained on sequential tasks. In this work,
inspired by the interactive human memory and learning system, we propose a
novel biomimetic continual learning framework that integrates semi-parametric
memory and the wake-sleep consolidation mechanism. For the first time, our
method enables deep neural networks to retain high performance on novel tasks
while maintaining prior knowledge in real-world challenging continual learning
scenarios, e.g., class-incremental learning on ImageNet. This study
demonstrates that emulating biological intelligence provides a promising path
to enable deep neural networks with continual learning capabilities.

</details>


### [196] [Geometric Learning Dynamics](https://arxiv.org/abs/2504.14728)
*Vitaly Vanchurin*

Main category: cs.LG

TL;DR: 论文提出了一个统一的几何框架，用于建模物理、生物和机器学习系统中的学习动态，揭示了三种基本机制。


<details>
  <summary>Details</summary>
Motivation: 研究学习动态在不同系统中的统一几何模型，以揭示其背后的基本机制。

Method: 通过分析度量张量$g$与噪声协方差矩阵$\kappa$之间的幂律关系$g \propto \kappa^a$，识别三种动态机制。

Result: 发现三种动态机制：量子机制（$a=1$）、高效学习机制（$a=\tfrac{1}{2}$）和平衡机制（$a=0$）。

Conclusion: 中间机制（$a=\tfrac{1}{2}$）是生物复杂性出现的关键机制。

Abstract: We present a unified geometric framework for modeling learning dynamics in
physical, biological, and machine learning systems. The theory reveals three
fundamental regimes, each emerging from the power-law relationship $g \propto
\kappa^a$ between the metric tensor $g$ in the space of trainable variables and
the noise covariance matrix $\kappa$. The quantum regime corresponds to $a = 1$
and describes Schr\"odinger-like dynamics that emerges from a discrete shift
symmetry. The efficient learning regime corresponds to $a = \tfrac{1}{2}$ and
describes very fast machine learning algorithms. The equilibration regime
corresponds to $a = 0$ and describes classical models of biological evolution.
We argue that the emergence of the intermediate regime $a = \tfrac{1}{2}$ is a
key mechanism underlying the emergence of biological complexity.

</details>


### [197] [Reinforcement Learning from Multi-level and Episodic Human Feedback](https://arxiv.org/abs/2504.14732)
*Muhammad Qasim Elahi,Somtochukwu Oguchienti,Maheed H. Ahmed,Mahsa Ghasemi*

Main category: cs.LG

TL;DR: 论文探讨了利用多级人类反馈（以分数形式）设计强化学习奖励函数的方法，提出了一种高效学习奖励函数和最优策略的算法，并验证了其效果。


<details>
  <summary>Details</summary>
Motivation: 设计有效的奖励函数在复杂任务中具有挑战性，现有方法多依赖人类比较反馈，本文探索了多级反馈的潜力。

Method: 提出一种算法，从多级人类反馈（每幕结束时的评分）中学习奖励函数和最优策略。

Result: 算法实现了次线性遗憾，并通过大量仿真验证了其有效性。

Conclusion: 多级人类反馈为奖励函数设计提供了更粗粒度但信息丰富的信号，适用于非马尔可夫奖励场景。

Abstract: Designing an effective reward function has long been a challenge in
reinforcement learning, particularly for complex tasks in unstructured
environments. To address this, various learning paradigms have emerged that
leverage different forms of human input to specify or refine the reward
function. Reinforcement learning from human feedback is a prominent approach
that utilizes human comparative feedback, expressed as a preference for one
behavior over another, to tackle this problem. In contrast to comparative
feedback, we explore multi-level human feedback, which is provided in the form
of a score at the end of each episode. This type of feedback offers more coarse
but informative signals about the underlying reward function than binary
feedback. Additionally, it can handle non-Markovian rewards, as it is based on
the evaluation of an entire episode. We propose an algorithm to efficiently
learn both the reward function and the optimal policy from this form of
feedback. Moreover, we show that the proposed algorithm achieves sublinear
regret and demonstrate its empirical effectiveness through extensive
simulations.

</details>


### [198] [AltGDmin: Alternating GD and Minimization for Partly-Decoupled (Federated) Optimization](https://arxiv.org/abs/2504.14741)
*Namrata Vaswani*

Main category: cs.LG

TL;DR: 提出了一种名为AltGDmin的新型优化框架，比传统的交替最小化（AltMin）更快，适用于多种问题，尤其是当其中一个变量子集的优化速度远快于另一个时。


<details>
  <summary>Details</summary>
Motivation: 传统AltMin方法在某些问题中效率较低，尤其是在一个变量子集的优化速度远快于另一个时。AltGDmin通过结合梯度下降和最小化，提高了效率。

Method: AltGDmin框架结合了梯度下降（GD）和最小化（Min），适用于问题中一个变量子集（Zb）的优化速度远快于另一个（Za）且成本函数对Za可微的情况。

Result: AltGDmin在多种问题中表现出比AltMin更快的性能，包括低秩压缩感知、矩阵补全、鲁棒PCA、相位检索等。

Conclusion: AltGDmin是一个高效且通用的优化框架，适用于多种问题，尤其是在分布式和联邦学习环境中具有通信效率的优势。

Abstract: This article describes a novel optimization solution framework, called
alternating gradient descent (GD) and minimization (AltGDmin), that is useful
for many problems for which alternating minimization (AltMin) is a popular
solution. AltMin is a special case of the block coordinate descent algorithm
that is useful for problems in which minimization w.r.t one subset of variables
keeping the other fixed is closed form or otherwise reliably solved. Denote the
two blocks/subsets of the optimization variables Z by Za, Zb, i.e., Z = {Za,
Zb}. AltGDmin is often a faster solution than AltMin for any problem for which
(i) the minimization over one set of variables, Zb, is much quicker than that
over the other set, Za; and (ii) the cost function is differentiable w.r.t. Za.
Often, the reason for one minimization to be quicker is that the problem is
``decoupled" for Zb and each of the decoupled problems is quick to solve. This
decoupling is also what makes AltGDmin communication-efficient for federated
settings.
  Important examples where this assumption holds include (a) low rank
column-wise compressive sensing (LRCS), low rank matrix completion (LRMC), (b)
their outlier-corrupted extensions such as robust PCA, robust LRCS and robust
LRMC; (c) phase retrieval and its sparse and low-rank model based extensions;
(d) tensor extensions of many of these problems such as tensor LRCS and tensor
completion; and (e) many partly discrete problems where GD does not apply --
such as clustering, unlabeled sensing, and mixed linear regression. LRCS finds
important applications in multi-task representation learning and few shot
learning, federated sketching, and accelerated dynamic MRI. LRMC and robust PCA
find important applications in recommender systems, computer vision and video
analytics.

</details>


### [199] [AI for the Open-World: the Learning Principles](https://arxiv.org/abs/2504.14751)
*Jianyu Zhang*

Main category: cs.LG

TL;DR: 论文探讨了封闭世界AI的成功是否适用于开放世界，提出了开放世界AI所需的学习原则和技术。


<details>
  <summary>Details</summary>
Motivation: 封闭世界AI的成功依赖于明确的标准和大量示例，但这些在开放世界中不适用，因此需要新的学习原则和技术。

Method: 提出了开放世界AI的学习原则（如丰富特征、解耦表示和推理时学习），并开发了相关技术进行验证。

Result: 通过大规模实验验证了这些学习原则的有效性。

Conclusion: 开放世界AI需要独特的学习原则和技术，与封闭世界AI不同。

Abstract: During the past decades, numerous successes of AI has been made on "specific
capabilities", named closed-world, such as artificial environments or specific
real-world tasks. This well-defined narrow capability brings two nice benefits,
a clear criterion of success and the opportunity to collect a lot of examples.
The criteria not only reveal whether a machine has achieved a goal, but reveal
how the machine falls short of the goal. As a result, human designers can fix
the problems one after the other until the machine is deemed good enough for
the task. Furthermore, the large set of collected examples reduces the
difficulty of this problem-fixing process (by the central limit theorem).
  Do the success in closed-world translate into broad open-world, where a
machine is required to perform any task that a human could possibly undertake
with fewer examples and less priori knowledge from human designers? No. Because
competence in a specific task provides little insight in handling other tasks,
the valuable criteria for specific tasks become helpless when handling broader
unseen tasks. Furthermore, due to the shortage of examples in unseen tasks,
central limit theorem does not stand on our side. At the end, human designers
lose the oscilloscope to "hack" an AI system for the open-world.
  Achieving AI for the open-world requires unique learning principles and
innovated techniques, which are different from the ones in building AI for the
closed-world. This thesis explores necessary learning principles required to
construct AI for the open-world, including rich features (analogy a large tool
box), disentangled representation (an organized tool box), and inference-time
learning (a tool-savvy hand). Driven by the learning principles, this thesis
further proposes techniques to use the learning principles, conducts enormous
large-scale experiments to verify the learning principles.

</details>


### [200] [A Combinatorial Theory of Dropout: Subnetworks, Graph Geometry, and Generalization](https://arxiv.org/abs/2504.14762)
*Sahil Rajesh Dhayalkar*

Main category: cs.LG

TL;DR: 论文提出了一种基于组合和图论的dropout理论，将训练建模为在高维子网络图上的随机游走，揭示了dropout通过采样稳健、结构化的子网络集合来提升泛化能力。


<details>
  <summary>Details</summary>
Motivation: 研究dropout的机制，揭示其如何通过随机采样子网络来提升模型的泛化能力。

Method: 将训练建模为高维子网络图上的随机游走，定义子网络贡献分数，并利用谱图理论、PAC-Bayes分析和组合数学工具进行分析。

Result: 泛化能力强的子网络形成大、连通、低阻的集群，且其数量随网络宽度指数增长。实验验证了理论结果。

Conclusion: 论文为理解dropout提供了统一的理论基础，并提出了掩码引导正则化和子网络优化的新方向。

Abstract: We propose a combinatorial and graph-theoretic theory of dropout by modeling
training as a random walk over a high-dimensional graph of binary subnetworks.
Each node represents a masked version of the network, and dropout induces
stochastic traversal across this space. We define a subnetwork contribution
score that quantifies generalization and show that it varies smoothly over the
graph. Using tools from spectral graph theory, PAC-Bayes analysis, and
combinatorics, we prove that generalizing subnetworks form large, connected,
low-resistance clusters, and that their number grows exponentially with network
width. This reveals dropout as a mechanism for sampling from a robust,
structured ensemble of well-generalizing subnetworks with built-in redundancy.
Extensive experiments validate every theoretical claim across diverse
architectures. Together, our results offer a unified foundation for
understanding dropout and suggest new directions for mask-guided regularization
and subnetwork optimization.

</details>


### [201] [Novel Concept-Oriented Synthetic Data approach for Training Generative AI-Driven Crystal Grain Analysis Using Diffusion Model](https://arxiv.org/abs/2504.14782)
*Ahmed Sobhi Saleh,Kristof Croes,Hajdin Ceric,Ingrid De Wolf,Houman Zahedmanesh*

Main category: cs.LG

TL;DR: 提出了一种自动化方法，结合边缘检测和生成扩散模型，用于从显微镜图像中提取多晶粒结构，解决了传统方法效率低的问题。


<details>
  <summary>Details</summary>
Motivation: 传统方法（如TEM和SEM）提取多晶粒结构耗时且主观，限制了高通量分析的扩展性。

Method: 采用七阶段方法生成合成TEM图像用于训练，结合边缘检测和生成扩散模型，自动识别晶粒并消除噪声。

Result: 模型在多种金属上应用，平均准确率达97.23%，生成的低分辨率TEM图像晶粒形态与高要求实验技术结果相当。

Conclusion: 该方法可推广至其他数据稀缺领域，展示了合成数据在解决数据不足问题中的潜力。

Abstract: The traditional techniques for extracting polycrystalline grain structures
from microscopy images, such as transmission electron microscopy (TEM) and
scanning electron microscopy (SEM), are labour-intensive, subjective, and
time-consuming, limiting their scalability for high-throughput analysis. In
this study, we present an automated methodology integrating edge detection with
generative diffusion models to effectively identify grains, eliminate noise,
and connect broken segments in alignment with predicted grain boundaries. Due
to the limited availability of adequate images preventing the training of deep
machine learning models, a new seven-stage methodology is employed to generate
synthetic TEM images for training. This concept-oriented synthetic data
approach can be extended to any field of interest where the scarcity of data is
a challenge. The presented model was applied to various metals with average
grain sizes down to the nanoscale, producing grain morphologies from
low-resolution TEM images that are comparable to those obtained from advanced
and demanding experimental techniques with an average accuracy of 97.23%.

</details>


### [202] [Enhanced Data-driven Topology Design Methodology with Multi-level Mesh and Correlation-based Mutation for Stress-related Multi-objective Optimization](https://arxiv.org/abs/2504.14790)
*Jun Yang,Shintaro Yamasaki*

Main category: cs.LG

TL;DR: 提出了一种基于多级网格和相关性变异模块的数据驱动拓扑设计方法，解决了传统方法对初始数据集质量的依赖问题，提高了计算效率和通用性。


<details>
  <summary>Details</summary>
Motivation: 传统基于敏感性的拓扑优化方法在处理强非线性问题时效果不佳，而数据驱动方法对初始数据集质量敏感，限制了其通用性和有效性。

Method: 采用多级网格策略和相关性变异模块，逐步优化结构表示，避免高自由度表示，同时赋予生成数据新的几何特征。

Result: 实验表明，该方法在强非线性问题上比传统方法更具通用性和有效性，且计算成本更低。

Conclusion: 多级网格数据驱动拓扑设计方法显著提高了通用性和计算效率，适用于缺乏先验信息的优化问题。

Abstract: Topology optimization (TO) serves as a widely applied structural design
approach to tackle various engineering problems. Nevertheless,
sensitivity-based TO methods usually struggle with solving strongly nonlinear
optimization problems. By leveraging high capacity of deep generative model,
which is an influential machine learning technique, the sensitivity-free
data-driven topology design (DDTD) methodology is regarded as an effective
means of overcoming these issues. The DDTD methodology depends on initial
dataset with a certain regularity, making its results highly sensitive to
initial dataset quality. This limits its effectiveness and generalizability,
especially for optimization problems without priori information. In this
research, we proposed a multi-level mesh DDTD-based method with
correlation-based mutation module to escape from the limitation of the quality
of the initial dataset on the results and enhance computational efficiency. The
core is to employ a correlation-based mutation module to assign new geometric
features with physical meaning to the generated data, while utilizing a
multi-level mesh strategy to progressively enhance the refinement of the
structural representation, thus avoiding the maintenance of a high
degree-of-freedom (DOF) representation throughout the iterative process. The
proposed multi-level mesh DDTD-based method can be driven by a low quality
initial dataset without the need for time-consuming construction of a specific
dataset, thus significantly increasing generality and reducing application
difficulty, while further lowering computational cost of DDTD methodology.
Various comparison experiments with the traditional sensitivity-based TO
methods on stress-related strongly nonlinear problems demonstrate the
generality and effectiveness of the proposed method.

</details>


### [203] [Edge-boosted graph learning for functional brain connectivity analysis](https://arxiv.org/abs/2504.14796)
*David Yang,Mostafa Abdelmegeed,John Modl,Minjeong Kim*

Main category: cs.LG

TL;DR: 提出了一种基于边功能连接（eFC）的新方法，用于脑网络分析，显著优于现有GNN方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于节点的脑连接方法无法准确捕捉功能连接，需改进以提升疾病预测准确性。

Method: 采用边功能连接（eFC）分析，并引入共嵌入技术整合边功能关系。

Result: 在ADNI和PPMI数据集上，新方法显著优于现有GNN方法。

Conclusion: 边功能连接方法为脑网络分析提供了更准确的工具，有助于神经退行性疾病的早期诊断。

Abstract: Predicting disease states from functional brain connectivity is critical for
the early diagnosis of severe neurodegenerative diseases such as Alzheimer's
Disease and Parkinson's Disease. Existing studies commonly employ Graph Neural
Networks (GNNs) to infer clinical diagnoses from node-based brain connectivity
matrices generated through node-to-node similarities of regionally averaged
fMRI signals. However, recent neuroscience studies found that such node-based
connectivity does not accurately capture ``functional connections" within the
brain. This paper proposes a novel approach to brain network analysis that
emphasizes edge functional connectivity (eFC), shifting the focus to inter-edge
relationships. Additionally, we introduce a co-embedding technique to integrate
edge functional connections effectively. Experimental results on the ADNI and
PPMI datasets demonstrate that our method significantly outperforms
state-of-the-art GNN methods in classifying functional brain networks.

</details>


### [204] [Verifying Robust Unlearning: Probing Residual Knowledge in Unlearned Models](https://arxiv.org/abs/2504.14798)
*Hao Xuan,Xingyu Li*

Main category: cs.LG

TL;DR: 论文提出了一种新的“鲁棒遗忘”概念，并通过“遗忘映射攻击”（UMA）验证现有遗忘技术是否满足安全标准，发现其仍存在漏洞。


<details>
  <summary>Details</summary>
Motivation: 现有遗忘验证方法无法检测残留信息泄露，需确保模型与重新训练无区别且能抵抗对抗恢复。

Method: 提出UMA框架，通过对抗查询主动探测模型中遗忘痕迹。

Result: 实验表明现有遗忘技术即使通过现有验证指标仍存在漏洞。

Conclusion: UMA为评估和提升机器遗忘安全性设定了新标准。

Abstract: Machine Unlearning (MUL) is crucial for privacy protection and content
regulation, yet recent studies reveal that traces of forgotten information
persist in unlearned models, enabling adversaries to resurface removed
knowledge. Existing verification methods only confirm whether unlearning was
executed, failing to detect such residual information leaks. To address this,
we introduce the concept of Robust Unlearning, ensuring models are
indistinguishable from retraining and resistant to adversarial recovery. To
empirically evaluate whether unlearning techniques meet this security standard,
we propose the Unlearning Mapping Attack (UMA), a post-unlearning verification
framework that actively probes models for forgotten traces using adversarial
queries. Extensive experiments on discriminative and generative tasks show that
existing unlearning techniques remain vulnerable, even when passing existing
verification metrics. By establishing UMA as a practical verification tool,
this study sets a new standard for assessing and enhancing machine unlearning
security.

</details>


### [205] [A Survey on Small Sample Imbalance Problem: Metrics, Feature Analysis, and Solutions](https://arxiv.org/abs/2504.14800)
*Shuxian Zhao,Jie Gui,Minjing Dong,Baosheng Yu,Zhipeng Gui,Lu Dong,Yuan Yan Tang,James Tin-Yau Kwok*

Main category: cs.LG

TL;DR: 论文提出了一种系统性分析框架，用于解决小样本不平衡（S&I）问题，强调数据特征分析的重要性，并总结了现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 小样本不平衡问题是机器学习和数据分析中的主要挑战，现有方法多依赖启发式算法，缺乏对数据特征的深入分析。

Method: 论文总结了不平衡度量和复杂性分析方法，并回顾了针对不同数据分布的解决方案，通过实验比较了重采样和分类器的性能差异。

Result: 实验表明，分类器性能差异比重采样带来的改进更显著。

Conclusion: 论文强调了数据视角分析的必要性，并提出了未来研究方向。

Abstract: The small sample imbalance (S&I) problem is a major challenge in machine
learning and data analysis. It is characterized by a small number of samples
and an imbalanced class distribution, which leads to poor model performance. In
addition, indistinct inter-class feature distributions further complicate
classification tasks. Existing methods often rely on algorithmic heuristics
without sufficiently analyzing the underlying data characteristics. We argue
that a detailed analysis from the data perspective is essential before
developing an appropriate solution. Therefore, this paper proposes a systematic
analytical framework for the S\&I problem. We first summarize imbalance metrics
and complexity analysis methods, highlighting the need for interpretable
benchmarks to characterize S&I problems. Second, we review recent solutions for
conventional, complexity-based, and extreme S&I problems, revealing
methodological differences in handling various data distributions. Our summary
finds that resampling remains a widely adopted solution. However, we conduct
experiments on binary and multiclass datasets, revealing that classifier
performance differences significantly exceed the improvements achieved through
resampling. Finally, this paper highlights open questions and discusses future
trends.

</details>


### [206] [Dynamic Contrastive Skill Learning with State-Transition Based Skill Clustering and Dynamic Length Adjustment](https://arxiv.org/abs/2504.14805)
*Jinwoo Choi,Seung-Woo Seo*

Main category: cs.LG

TL;DR: 提出了一种动态对比技能学习（DCSL）框架，通过状态转移表示技能、学习技能相似性函数和动态调整技能长度，解决了现有技能学习方法在识别语义相似行为和固定技能长度上的局限性。


<details>
  <summary>Details</summary>
Motivation: 强化学习在长时程任务和复杂决策中仍面临挑战，现有技能学习方法未能有效识别语义相似行为且技能长度固定，限制了灵活性和泛化能力。

Method: DCSL框架包含三个关键创新：基于状态转移的技能表示、技能相似性函数学习和动态技能长度调整，利用对比学习捕捉行为语义上下文。

Result: DCSL在复杂或噪声数据集中表现出更灵活和自适应的技能提取能力，在任务完成和效率方面优于现有方法。

Conclusion: DCSL通过动态调整技能长度和语义上下文学习，显著提升了技能学习的灵活性和适应性，为复杂任务提供了有效解决方案。

Abstract: Reinforcement learning (RL) has made significant progress in various domains,
but scaling it to long-horizon tasks with complex decision-making remains
challenging. Skill learning attempts to address this by abstracting actions
into higher-level behaviors. However, current approaches often fail to
recognize semantically similar behaviors as the same skill and use fixed skill
lengths, limiting flexibility and generalization. To address this, we propose
Dynamic Contrastive Skill Learning (DCSL), a novel framework that redefines
skill representation and learning. DCSL introduces three key ideas:
state-transition based skill representation, skill similarity function
learning, and dynamic skill length adjustment. By focusing on state transitions
and leveraging contrastive learning, DCSL effectively captures the semantic
context of behaviors and adapts skill lengths to match the appropriate temporal
extent of behaviors. Our approach enables more flexible and adaptive skill
extraction, particularly in complex or noisy datasets, and demonstrates
competitive performance compared to existing methods in task completion and
efficiency.

</details>


### [207] [A Basic Evaluation of Neural Networks Trained with the Error Diffusion Learning Algorithm](https://arxiv.org/abs/2504.14814)
*Kazuhisa Fujita*

Main category: cs.LG

TL;DR: 论文介绍了Kaneko的误差扩散学习算法（EDLA），一种生物启发的替代反向传播的方法，并在多个任务中验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 反向传播算法缺乏生物合理性，促使开发替代学习方法。

Method: EDLA通过全局误差信号在成对的兴奋-抑制子层网络中扩散，避免逐层反向传播。

Result: EDLA在奇偶校验、回归和图像分类任务中表现优异，性能受学习率、神经元数量和网络深度影响。

Conclusion: EDLA是一种生物启发的有效替代方法，未来可扩展至更多生物启发网络。

Abstract: Artificial neural networks are powerful tools capable of addressing various
tasks. Although the backpropagation algorithm has become a standard training
method for these neural networks, its lack of biological plausibility has
inspired the development of alternative learning approaches. One such
alternative is Kaneko's Error Diffusion Learning Algorithm (EDLA), a
biologically motivated approach wherein a single global error signal diffuses
throughout a network composed of paired excitatory-inhibitory sublayers,
thereby eliminating the necessity for layer-wise backpropagation. This study
presents a contemporary formulation of the EDLA framework and evaluates its
effectiveness through parity check, regression, and image classification tasks.
Our experimental results indicate that EDLA networks can consistently achieve
high accuracy across these benchmarks, with performance efficiency and
convergence speed notably influenced by the choice of learning rate, neuron
count, and network depth. Further investigation of the internal representations
formed by EDLA networks reveals their capacity for meaningful feature
extraction, similar to traditional neural networks. These results suggest that
EDLA is a biologically motivated alternative for training feedforward networks
and will motivate future work on extending this method to biologically inspired
neural networks.

</details>


### [208] [What Lurks Within? Concept Auditing for Shared Diffusion Models at Scale](https://arxiv.org/abs/2504.14815)
*Xiaoyong Yuan,Xiaolong Ma,Linke Guo,Lan Zhang*

Main category: cs.LG

TL;DR: PAIA是一种新型的扩散模型概念审计框架，通过直接分析模型内部行为，无需优化提示或生成图像，显著提高了审计效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 随着扩散模型的普及和参数高效微调技术的发展，模型可能生成敏感或未经授权的内容，但目前缺乏有效的审计工具。

Method: 提出Prompt-Agnostic Image-Free Auditing (PAIA)框架，直接分析模型内部行为，避免依赖提示或生成图像。

Result: 在320个控制模型和690个真实社区模型上测试，PAIA检测准确率超过90%，审计时间减少18-40倍。

Conclusion: PAIA是首个可扩展且实用的扩散模型预部署概念审计解决方案，为模型共享提供了更安全透明的基础。

Abstract: Diffusion models (DMs) have revolutionized text-to-image generation, enabling
the creation of highly realistic and customized images from text prompts. With
the rise of parameter-efficient fine-tuning (PEFT) techniques like LoRA, users
can now customize powerful pre-trained models using minimal computational
resources. However, the widespread sharing of fine-tuned DMs on open platforms
raises growing ethical and legal concerns, as these models may inadvertently or
deliberately generate sensitive or unauthorized content, such as copyrighted
material, private individuals, or harmful content. Despite the increasing
regulatory attention on generative AI, there are currently no practical tools
for systematically auditing these models before deployment. In this paper, we
address the problem of concept auditing: determining whether a fine-tuned DM
has learned to generate a specific target concept. Existing approaches
typically rely on prompt-based input crafting and output-based image
classification but suffer from critical limitations, including prompt
uncertainty, concept drift, and poor scalability. To overcome these challenges,
we introduce Prompt-Agnostic Image-Free Auditing (PAIA), a novel, model-centric
concept auditing framework. By treating the DM as the object of inspection,
PAIA enables direct analysis of internal model behavior, bypassing the need for
optimized prompts or generated images. We evaluate PAIA on 320 controlled model
and 690 real-world community models sourced from a public DM sharing platform.
PAIA achieves over 90% detection accuracy while reducing auditing time by
18-40x compared to existing baselines. To our knowledge, PAIA is the first
scalable and practical solution for pre-deployment concept auditing of
diffusion models, providing a practical foundation for safer and more
transparent diffusion model sharing.

</details>


### [209] [Uncertainty quantification of neural network models of evolving processes via Langevin sampling](https://arxiv.org/abs/2504.14854)
*Cosmin Safta,Reese E. Jones,Ravi G. Patel,Raelynn Wonnacot,Dan S. Bolintineanu,Craig M. Hamel,Sharlotte L. B. Kramer*

Main category: cs.LG

TL;DR: 提出了一种基于神经ODE的可扩展近似推理超网络框架，用于历史依赖过程，通过Langevin采样平衡计算成本，并在化学和物理数据上验证性能。


<details>
  <summary>Details</summary>
Motivation: 解决历史依赖过程的建模问题，提供灵活的推理框架。

Method: 使用神经ODE表示内部状态演化，结合可训练观测模型，通过Langevin采样学习后验分布。

Result: 在化学反应和材料物理数据上表现优于均值场变分推理。

Conclusion: 该框架在计算成本和后验近似之间提供了灵活性，适用于复杂过程建模。

Abstract: We propose a scalable, approximate inference hypernetwork framework for a
general model of history-dependent processes. The flexible data model is based
on a neural ordinary differential equation (NODE) representing the evolution of
internal states together with a trainable observation model subcomponent. The
posterior distribution corresponding to the data model parameters (weights and
biases) follows a stochastic differential equation with a drift term related to
the score of the posterior that is learned jointly with the data model
parameters. This Langevin sampling approach offers flexibility in balancing the
computational budget between the evaluation cost of the data model and the
approximation of the posterior density of its parameters. We demonstrate
performance of the hypernetwork on chemical reaction and material physics data
and compare it to mean-field variational inference.

</details>


### [210] [Impact of Latent Space Dimension on IoT Botnet Detection Performance: VAE-Encoder Versus ViT-Encoder](https://arxiv.org/abs/2504.14879)
*Hassan Wasswa,Aziida Nanyonga,Timothy Lynar*

Main category: cs.LG

TL;DR: 研究探讨了潜在维度对深度学习分类器性能的影响，比较了ViT和VAE编码器在IoT僵尸网络流量数据集上的表现，发现VAE表现更优。


<details>
  <summary>Details</summary>
Motivation: IoT设备数量激增，安全威胁增加，尤其是僵尸网络攻击，需提升分类器性能以应对。

Method: 使用ViT和VAE编码器将高维数据集投影到低维潜在空间，比较不同潜在维度下的分类器性能。

Result: VAE编码器在准确率、精确率、召回率和F1分数上均优于ViT编码器。

Conclusion: VAE更适合处理无空间模式的IoT流量数据，ViT因依赖图像空间模式而表现较差。

Abstract: The rapid evolution of Internet of Things (IoT) technology has led to a
significant increase in the number of IoT devices, applications, and services.
This surge in IoT devices, along with their widespread presence, has made them
a prime target for various cyber-attacks, particularly through IoT botnets. As
a result, security has become a major concern within the IoT ecosystem. This
study focuses on investigating how the latent dimension impacts the performance
of different deep learning classifiers when trained on latent vector
representations of the train dataset. The primary objective is to compare the
outcomes of these models when encoder components from two cutting-edge
architectures: the Vision Transformer (ViT) and the Variational Auto-Encoder
(VAE) are utilized to project the high dimensional train dataset to the learned
low dimensional latent space. The encoder components are employed to project
high-dimensional structured .csv IoT botnet traffic datasets to various latent
sizes. Evaluated on N-BaIoT and CICIoT2022 datasets, findings reveal that
VAE-encoder based dimension reduction outperforms ViT-encoder based dimension
reduction for both datasets in terms of four performance metrics including
accuracy, precision, recall, and F1-score for all models which can be
attributed to absence of spatial patterns in the datasets the ViT model
attempts to learn and extract from image instances.

</details>


### [211] [Some Optimizers are More Equal: Understanding the Role of Optimizers in Group Fairness](https://arxiv.org/abs/2504.14882)
*Mojtaba Kolahdouzi,Hatice Gunes,Ali Etemad*

Main category: cs.LG

TL;DR: 研究优化算法选择对深度神经网络中群体公平性的影响，发现自适应优化器（如RMSProp）比随机优化器（如SGD）更易收敛到公平解，并通过理论和实验验证。


<details>
  <summary>Details</summary>
Motivation: 探讨优化算法如何影响深度神经网络的群体公平性，尤其是在数据不平衡情况下。

Method: 通过随机微分方程分析优化动态，比较自适应优化器（如RMSProp）与随机优化器（如SGD），并验证于多个数据集和任务。

Result: 自适应优化器在公平性指标（如均等机会、人口均等）上优于SGD，同时保持预测准确性。

Conclusion: 自适应更新机制是促进公平结果的关键因素，值得进一步关注。

Abstract: We study whether and how the choice of optimization algorithm can impact
group fairness in deep neural networks. Through stochastic differential
equation analysis of optimization dynamics in an analytically tractable setup,
we demonstrate that the choice of optimization algorithm indeed influences
fairness outcomes, particularly under severe imbalance. Furthermore, we show
that when comparing two categories of optimizers, adaptive methods and
stochastic methods, RMSProp (from the adaptive category) has a higher
likelihood of converging to fairer minima than SGD (from the stochastic
category). Building on this insight, we derive two new theoretical guarantees
showing that, under appropriate conditions, RMSProp exhibits fairer parameter
updates and improved fairness in a single optimization step compared to SGD. We
then validate these findings through extensive experiments on three publicly
available datasets, namely CelebA, FairFace, and MS-COCO, across different
tasks as facial expression recognition, gender classification, and multi-label
classification, using various backbones. Considering multiple fairness
definitions including equalized odds, equal opportunity, and demographic
parity, adaptive optimizers like RMSProp and Adam consistently outperform SGD
in terms of group fairness, while maintaining comparable predictive accuracy.
Our results highlight the role of adaptive updates as a crucial yet overlooked
mechanism for promoting fair outcomes.

</details>


### [212] [Latent Bayesian Optimization via Autoregressive Normalizing Flows](https://arxiv.org/abs/2504.14889)
*Seunghun Lee,Jinyoung Park,Jaewon Chu,Minseo Yoon,Hyunwoo J. Kim*

Main category: cs.LG

TL;DR: 提出了一种基于归一化流的贝叶斯优化方法（NF-BO），解决了潜在贝叶斯优化中的值差异问题，并在分子生成任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有潜在贝叶斯优化方法（LBO）因输入空间与潜在空间的重构差距导致值差异问题，影响优化效果。

Method: 使用归一化流作为生成模型，建立输入空间到潜在空间的一对一编码函数及其左逆解码函数，消除重构差距；提出SeqFlow（自回归归一化流）和动态调整探索概率的候选采样策略。

Result: 在分子生成任务中，NF-BO显著优于传统和近期LBO方法。

Conclusion: NF-BO通过消除重构差距，解决了值差异问题，提升了优化性能。

Abstract: Bayesian Optimization (BO) has been recognized for its effectiveness in
optimizing expensive and complex objective functions. Recent advancements in
Latent Bayesian Optimization (LBO) have shown promise by integrating generative
models such as variational autoencoders (VAEs) to manage the complexity of
high-dimensional and structured data spaces. However, existing LBO approaches
often suffer from the value discrepancy problem, which arises from the
reconstruction gap between input and latent spaces. This value discrepancy
problem propagates errors throughout the optimization process, leading to
suboptimal outcomes. To address this issue, we propose a Normalizing Flow-based
Bayesian Optimization (NF-BO), which utilizes normalizing flow as a generative
model to establish one-to-one encoding function from the input space to the
latent space, along with its left-inverse decoding function, eliminating the
reconstruction gap. Specifically, we introduce SeqFlow, an autoregressive
normalizing flow for sequence data. In addition, we develop a new candidate
sampling strategy that dynamically adjusts the exploration probability for each
token based on its importance. Through extensive experiments, our NF-BO method
demonstrates superior performance in molecule generation tasks, significantly
outperforming both traditional and recent LBO approaches.

</details>


### [213] [Dynamic Graph-Like Learning with Contrastive Clustering on Temporally-Factored Ship Motion Data for Imbalanced Sea State Estimation in Autonomous Vessel](https://arxiv.org/abs/2504.14907)
*Kexin Wang,Mengna Liu,Xu Cheng,Fan Shi,Shanshan Qi,Shengyong Chen*

Main category: cs.LG

TL;DR: 提出了一种名为TGC-SSE的新深度学习模型，用于解决海况估计中的数据冗余和类别不平衡问题，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统方法在处理船舶运动数据时存在数据不平衡和特征冗余的问题，限制了其有效性。

Method: TGC-SSE结合了时间维度分解模块、动态图学习模块和对比聚类损失函数。

Result: 在14个公共数据集上表现优异，9个数据集上准确率最高，比EDI提升20.79%。

Conclusion: TGC-SSE不仅提高了海况估计的准确性，还表现出强大的泛化能力。

Abstract: Accurate sea state estimation is crucial for the real-time control and future
state prediction of autonomous vessels. However, traditional methods struggle
with challenges such as data imbalance and feature redundancy in ship motion
data, limiting their effectiveness. To address these challenges, we propose the
Temporal-Graph Contrastive Clustering Sea State Estimator (TGC-SSE), a novel
deep learning model that combines three key components: a time dimension
factorization module to reduce data redundancy, a dynamic graph-like learning
module to capture complex variable interactions, and a contrastive clustering
loss function to effectively manage class imbalance. Our experiments
demonstrate that TGC-SSE significantly outperforms existing methods across 14
public datasets, achieving the highest accuracy in 9 datasets, with a 20.79%
improvement over EDI. Furthermore, in the field of sea state estimation,
TGC-SSE surpasses five benchmark methods and seven deep learning models.
Ablation studies confirm the effectiveness of each module, demonstrating their
respective roles in enhancing overall model performance. Overall, TGC-SSE not
only improves the accuracy of sea state estimation but also exhibits strong
generalization capabilities, providing reliable support for autonomous vessel
operations.

</details>


### [214] [POLYRAG: Integrating Polyviews into Retrieval-Augmented Generation for Medical Applications](https://arxiv.org/abs/2504.14917)
*Chunjing Gan,Dan Yang,Binbin Hu,Ziqi Liu,Yue Shen,Zhiqiang Zhang,Jian Wang,Jun Zhou*

Main category: cs.LG

TL;DR: 论文提出PolyRAG方法，通过多视角整合检索信息以优化医疗场景中的检索增强生成（RAG），并引入PolyEVAL基准验证其优越性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在医疗场景中面临知识更新和幻觉问题，现有检索增强方法未考虑信息的时效性、权威性和共性，导致性能受限。

Method: 提出PolyRAG，从多视角评估检索信息并整合，同时开发PolyEVAL基准用于真实医疗场景的评估。

Result: 实验证明PolyRAG在PolyEVAL基准上表现优越。

Conclusion: PolyRAG通过多视角整合显著提升了医疗场景中RAG的性能，PolyEVAL为未来研究提供了实用基准。

Abstract: Large language models (LLMs) have become a disruptive force in the industry,
introducing unprecedented capabilities in natural language processing, logical
reasoning and so on. However, the challenges of knowledge updates and
hallucination issues have limited the application of LLMs in medical scenarios,
where retrieval-augmented generation (RAG) can offer significant assistance.
Nevertheless, existing retrieve-then-read approaches generally digest the
retrieved documents, without considering the timeliness, authoritativeness and
commonality of retrieval. We argue that these approaches can be suboptimal,
especially in real-world applications where information from different sources
might conflict with each other and even information from the same source in
different time scale might be different, and totally relying on this would
deteriorate the performance of RAG approaches. We propose PolyRAG that
carefully incorporate judges from different perspectives and finally integrate
the polyviews for retrieval augmented generation in medical applications. Due
to the scarcity of real-world benchmarks for evaluation, to bridge the gap we
propose PolyEVAL, a benchmark consists of queries and documents collected from
real-world medical scenarios (including medical policy, hospital & doctor
inquiry and healthcare) with multiple tagging (e.g., timeliness,
authoritativeness) on them. Extensive experiments and analysis on PolyEVAL have
demonstrated the superiority of PolyRAG.

</details>


### [215] [Causal DAG Summarization (Full Version)](https://arxiv.org/abs/2504.14937)
*Anna Zeng,Michael Cafarella,Batya Kenig,Markos Markakis,Brit Youngmann,Babak Salimi*

Main category: cs.LG

TL;DR: 论文提出了一种因果图摘要方法，通过简化因果DAG以提高可理解性，同时保留关键因果信息，确保可靠推断。


<details>
  <summary>Details</summary>
Motivation: 高维数据的因果DAG复杂且难以验证，现有通用图摘要方法不适用于因果DAG摘要，需解决这一问题。

Method: 提出因果图摘要目标，开发高效贪心算法，生成摘要DAG用于推断。

Result: 在六个真实数据集上验证，算法优于现有方法，能处理高维数据且生成的摘要DAG更稳健。

Conclusion: 摘要因果DAG可直接用于推断，提高因果推断的鲁棒性。

Abstract: Causal inference aids researchers in discovering cause-and-effect
relationships, leading to scientific insights. Accurate causal estimation
requires identifying confounding variables to avoid false discoveries. Pearl's
causal model uses causal DAGs to identify confounding variables, but incorrect
DAGs can lead to unreliable causal conclusions. However, for high dimensional
data, the causal DAGs are often complex beyond human verifiability. Graph
summarization is a logical next step, but current methods for general-purpose
graph summarization are inadequate for causal DAG summarization. This paper
addresses these challenges by proposing a causal graph summarization objective
that balances graph simplification for better understanding while retaining
essential causal information for reliable inference. We develop an efficient
greedy algorithm and show that summary causal DAGs can be directly used for
inference and are more robust to misspecification of assumptions, enhancing
robustness for causal inference. Experimenting with six real-life datasets, we
compared our algorithm to three existing solutions, showing its effectiveness
in handling high-dimensional data and its ability to generate summary DAGs that
ensure both reliable causal inference and robustness against misspecifications.

</details>


### [216] [Learning to Reason under Off-Policy Guidance](https://arxiv.org/abs/2504.14945)
*Jianhao Yan,Yafu Li,Zican Hu,Zhi Wang,Ganqu Cui,Xiaoye Qu,Yu Cheng,Yue Zhang*

Main category: cs.LG

TL;DR: LUFFY框架通过结合离策略推理轨迹和策略内训练，提升了推理模型的泛化能力，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有零强化学习方法局限于策略内训练，无法超越初始推理能力，需要一种新方法来结合离策略指导。

Method: 提出LUFFY框架，动态平衡模仿与探索，使用正则化重要性采样避免浅层模仿。

Result: 在六个数学基准上平均提升7.0分，在分布外任务中优势达6.2分，显著优于监督微调。

Conclusion: LUFFY为训练具有泛化能力的推理模型提供了可扩展的路径，同时有效模仿和探索。

Abstract: Recent advances in large reasoning models (LRMs) demonstrate that
sophisticated behaviors such as multi-step reasoning and self-reflection can
emerge via reinforcement learning (RL) with simple rule-based rewards. However,
existing zero-RL approaches are inherently ``on-policy'', limiting learning to
a model's own outputs and failing to acquire reasoning abilities beyond its
initial capabilities. We introduce LUFFY (Learning to reason Under oFF-policY
guidance), a framework that augments zero-RL with off-policy reasoning traces.
LUFFY dynamically balances imitation and exploration by combining off-policy
demonstrations with on-policy rollouts during training. Notably, we propose
policy shaping via regularized importance sampling to avoid superficial and
rigid imitation during mixed-policy training. Remarkably, LUFFY achieves an
over +7.0 average gain across six math benchmarks and an advantage of over +6.2
points in out-of-distribution tasks. It also substantially surpasses
imitation-based supervised fine-tuning (SFT), particularly in generalization.
Analysis shows LUFFY not only imitates effectively but also explores beyond
demonstrations, offering a scalable path to train generalizable reasoning
models with off-policy guidance.

</details>


### [217] [Symmetry-Preserving Architecture for Multi-NUMA Environments (SPANE): A Deep Reinforcement Learning Approach for Dynamic VM Scheduling](https://arxiv.org/abs/2504.14946)
*Tin Ping Chan,Yunlong Cheng,Yizhan Zhu,Xiaofeng Gao,Guihai Chen*

Main category: cs.LG

TL;DR: 论文提出动态虚拟机分配问题（DVAMP）及解决方案SPANE，通过深度强化学习优化多NUMA环境下的虚拟机调度，减少平均等待时间45%。


<details>
  <summary>Details</summary>
Motivation: 多NUMA架构在云计算中的普及带来了虚拟机调度新挑战，需更准确反映现代云环境的复杂性。

Method: 定义DVAMP为混合整数线性规划问题，提出SPANE（对称保持架构），利用深度强化学习解决。

Result: SPANE在实验中表现优于现有方法，平均虚拟机等待时间减少45%。

Conclusion: 研究填补了多NUMA环境下虚拟机调度的理论空白，提供了实用解决方案。

Abstract: As cloud computing continues to evolve, the adoption of multi-NUMA
(Non-Uniform Memory Access) architecture by cloud service providers has
introduced new challenges in virtual machine (VM) scheduling. To address these
challenges and more accurately reflect the complexities faced by modern cloud
environments, we introduce the Dynamic VM Allocation problem in Multi-NUMA PM
(DVAMP). We formally define both offline and online versions of DVAMP as
mixed-integer linear programming problems, providing a rigorous mathematical
foundation for analysis. A tight performance bound for greedy online algorithms
is derived, offering insights into the worst-case optimality gap as a function
of the number of physical machines and VM lifetime variability. To address the
challenges posed by DVAMP, we propose SPANE (Symmetry-Preserving Architecture
for Multi-NUMA Environments), a novel deep reinforcement learning approach that
exploits the problem's inherent symmetries. SPANE produces invariant results
under arbitrary permutations of physical machine states, enhancing learning
efficiency and solution quality. Extensive experiments conducted on the
Huawei-East-1 dataset demonstrate that SPANE outperforms existing baselines,
reducing average VM wait time by 45%. Our work contributes to the field of
cloud resource management by providing both theoretical insights and practical
solutions for VM scheduling in multi-NUMA environments, addressing a critical
gap in the literature and offering improved performance for real-world cloud
systems.

</details>


### [218] [Efficient Document Retrieval with G-Retriever](https://arxiv.org/abs/2504.14955)
*Manthankumar Solanki*

Main category: cs.LG

TL;DR: 论文提出了一种改进的基于注意力的子图构建方法，替代了原有的PCST方法，并结合节点和边属性编码，提升了问答系统的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅关注节点属性，导致上下文理解不完整，需要更高效的检索和更丰富的图表示。

Method: 采用基于注意力的子图构建技术，编码节点和边属性，并改进投影层和多头注意力池化以更好地对齐LLMs。

Result: 在WebQSP数据集上的实验表明，新方法性能略优于原方法。

Conclusion: 改进的方法在问答任务中更具潜力，能提供更准确的答案。

Abstract: Textual data question answering has gained significant attention due to its
growing applicability. Recently, a novel approach leveraging the
Retrieval-Augmented Generation (RAG) method was introduced, utilizing the
Prize-Collecting Steiner Tree (PCST) optimization for sub-graph construction.
However, this method focused solely on node attributes, leading to incomplete
contextual understanding. In this paper, we propose an enhanced approach that
replaces the PCST method with an attention-based sub-graph construction
technique, enabling more efficient and context-aware retrieval. Additionally,
we encode both node and edge attributes, leading to richer graph
representations. Our method also incorporates an improved projection layer and
multi-head attention pooling for better alignment with Large Language Models
(LLMs). Experimental evaluations on the WebQSP dataset demonstrate that our
approach is competitive and achieves marginally better results compared to the
original method, underscoring its potential for more accurate question
answering.

</details>


### [219] [MoE Parallel Folding: Heterogeneous Parallelism Mappings for Efficient Large-Scale MoE Model Training with Megatron Core](https://arxiv.org/abs/2504.14960)
*Dennis Liu,Zijie Yan,Xin Yao,Tong Liu,Vijay Korthikanti,Evan Wu,Shiqing Fan,Gao Deng,Hongxiao Bai,Ashwath Aithal,Michael Andersch,Mohammad Shoeybi,Jiajie Yao,Chandler Zhou,David Wu,Xipeng Li,June Yang*

Main category: cs.LG

TL;DR: 提出了一种五维混合并行框架，用于高效训练大规模MoE模型，通过MoE Parallel Folding策略和灵活的令牌调度器，显著提升了训练效率和扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有并行策略难以高效训练大规模MoE模型，限制了其扩展性和计算效率。

Method: 引入五维混合并行（张量、专家、上下文、数据和流水线并行）和MoE Parallel Folding策略，开发灵活的令牌调度器。

Result: 在Mixtral 8x22B和Qwen2-57B-A14B模型上分别达到49.3%和39.0%的MFU，支持1024 GPU和128K令牌序列长度。

Conclusion: 该框架显著提升大规模MoE模型的训练效率和扩展性，适用于超大规模模型训练。

Abstract: Mixture of Experts (MoE) models enhance neural network scalability by
dynamically selecting relevant experts per input token, enabling larger model
sizes while maintaining manageable computation costs. However, efficient
training of large-scale MoE models across thousands of GPUs presents
significant challenges due to limitations in existing parallelism strategies.
We introduce an end-to-end training framework for large-scale MoE models that
utilizes five-dimensional hybrid parallelism: Tensor Parallelism, Expert
Parallelism, Context Parallelism, Data Parallelism, and Pipeline Parallelism.
Central to our approach is MoE Parallel Folding, a novel strategy that
decouples the parallelization of attention and MoE layers in Transformer
models, allowing each layer type to adopt optimal parallel configurations.
Additionally, we develop a flexible token-level dispatcher that supports both
token-dropping and token-dropless MoE training across all five dimensions of
parallelism. This dispatcher accommodates dynamic tensor shapes and coordinates
different parallelism schemes for Attention and MoE layers, facilitating
complex parallelism implementations. Our experiments demonstrate significant
improvements in training efficiency and scalability. We achieve up to 49.3%
Model Flops Utilization (MFU) for the Mixtral 8x22B model and 39.0% MFU for the
Qwen2-57B-A14B model on H100 GPUs, outperforming existing methods. The
framework scales efficiently up to 1,024 GPUs and maintains high performance
with sequence lengths up to 128K tokens, validating its effectiveness for
large-scale MoE model training. The code is available in Megatron-Core.

</details>


### [220] [Learning Compositional Transferability of Time Series for Source-Free Domain Adaptation](https://arxiv.org/abs/2504.14994)
*Hankang Sun,Guiming Li,Su Yang,Baoqi Li*

Main category: cs.LG

TL;DR: 该论文提出了一种用于时间序列分类的源自由域自适应方法，通过解耦域可转移性并使用组合架构进行时间序列重构，实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 时间序列分类中的域自适应具有挑战性，尤其是在目标标签和源数据均不可访问的情况下。研究旨在解决这一问题，同时保留源数据预训练的分类主干。

Method: 采用组合架构进行时间序列重构，包括冻结的预训练U-net和两个并行分支（源重放分支和偏移补偿分支），通过可学习因子解耦域可转移性。

Result: 实验结果表明，该方法在三个广泛使用的基准测试中达到了最先进的性能。

Conclusion: 组合架构有效解决了源自由域自适应问题，同时保留了源数据的重构能力。

Abstract: Domain adaptation is challenging for time series classification due to the
highly dynamic nature. This study tackles the most difficult subtask when both
target labels and source data are inaccessible, namely, source-free domain
adaptation. To reuse the classification backbone pre-trained on source data,
time series reconstruction is a sound solution that aligns target and source
time series by minimizing the reconstruction errors of both. However, simply
fine-tuning the source pre-trained reconstruction model on target data may lose
the learnt priori, and it struggles to accommodate domain varying temporal
patterns in a single encoder-decoder. Therefore, this paper tries to
disentangle the composition of domain transferability by using a compositional
architecture for time series reconstruction. Here, the preceding component is a
U-net frozen since pre-trained, the output of which during adaptation is the
initial reconstruction of a given target time series, acting as a coarse step
to prompt the subsequent finer adaptation. The following pipeline for finer
adaptation includes two parallel branches: The source replay branch using a
residual link to preserve the output of U-net, and the offset compensation
branch that applies an additional autoencoder (AE) to further warp U-net's
output. By deploying a learnable factor on either branch to scale their
composition in the final output of reconstruction, the data transferability is
disentangled and the learnt reconstructive capability from source data is
retained. During inference, aside from the batch-level optimization in the
training, we search at test time stability-aware rescaling of source replay
branch to tolerate instance-wise variation. The experimental results show that
such compositional architecture of time series reconstruction leads to SOTA
performance on 3 widely used benchmarks.

</details>


### [221] [A Call for New Recipes to Enhance Spatial Reasoning in MLLMs](https://arxiv.org/abs/2504.15037)
*Huanyu Zhang,Chengzu Li,Wenshan Wu,Shaoguang Mao,Yan xia,Ivan Vulić,Zhang Zhang,Liang Wang,Tieniu Tan,Furu Wei*

Main category: cs.LG

TL;DR: 多模态大语言模型（MLLMs）在空间推理能力上存在显著不足，限制了其实际应用。本文提出需对现有方法进行根本性改进，并建立了一个空间推理框架，分析了当前方法的局限性及改进方向。


<details>
  <summary>Details</summary>
Motivation: MLLMs在空间推理能力上的不足限制了其与物理世界的有效交互，阻碍了更广泛的应用。

Method: 建立了一个空间推理框架，系统分析了从训练数据到推理机制的各个组成部分对空间推理能力的影响。

Result: 揭示了当前方法的局限性，并指出了改进的潜在方向。

Conclusion: 本文呼吁AI研究社区关注空间推理能力，推动MLLMs实现类人水平的空间推理。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated impressive
performance in general vision-language tasks. However, recent studies have
exposed critical limitations in their spatial reasoning capabilities. This
deficiency in spatial reasoning significantly constrains MLLMs' ability to
interact effectively with the physical world, thereby limiting their broader
applications. We argue that spatial reasoning capabilities will not naturally
emerge from merely scaling existing architectures and training methodologies.
Instead, this challenge demands dedicated attention to fundamental
modifications in the current MLLM development approach. In this position paper,
we first establish a comprehensive framework for spatial reasoning within the
context of MLLMs. We then elaborate on its pivotal role in real-world
applications. Through systematic analysis, we examine how individual components
of the current methodology-from training data to reasoning mechanisms-influence
spatial reasoning capabilities. This examination reveals critical limitations
while simultaneously identifying promising avenues for advancement. Our work
aims to direct the AI research community's attention toward these crucial yet
underexplored aspects. By highlighting these challenges and opportunities, we
seek to catalyze progress toward achieving human-like spatial reasoning
capabilities in MLLMs.

</details>


### [222] [VeLU: Variance-enhanced Learning Unit for Deep Neural Networks](https://arxiv.org/abs/2504.15051)
*Ashkan Shakarami,Yousef Yeganeh,Azade Farshad,Lorenzo Nicolè,Stefano Ghidoni,Nassir Navab*

Main category: cs.LG

TL;DR: VeLU是一种基于输入方差动态调整的激活函数，通过ArcTan-Sin变换和Wasserstein-2正则化，解决了ReLU及其替代品的问题，并在多个视觉基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: ReLU及其替代品（如Swish和GELU）存在梯度消失和缺乏动态适应性的问题，影响了深度神经网络的优化和泛化能力。

Method: 提出VeLU，结合ArcTan-Sin变换和Wasserstein-2正则化，动态调整输入方差，以缓解协变量偏移和稳定优化。

Result: 在ViT_B16、VGG19等模型和六个视觉基准测试中，VeLU表现优于ReLU、Swish和GELU。

Conclusion: VeLU通过动态调整输入方差，显著提升了激活函数的性能，代码已开源。

Abstract: Activation functions are fundamental in deep neural networks and directly
impact gradient flow, optimization stability, and generalization. Although ReLU
remains standard because of its simplicity, it suffers from vanishing gradients
and lacks adaptability. Alternatives like Swish and GELU introduce smooth
transitions, but fail to dynamically adjust to input statistics. We propose
VeLU, a Variance-enhanced Learning Unit as an activation function that
dynamically scales based on input variance by integrating ArcTan-Sin
transformations and Wasserstein-2 regularization, effectively mitigating
covariate shifts and stabilizing optimization. Extensive experiments on
ViT_B16, VGG19, ResNet50, DenseNet121, MobileNetV2, and EfficientNetB3 confirm
VeLU's superiority over ReLU, ReLU6, Swish, and GELU on six vision benchmarks.
The codes of VeLU are publicly available on GitHub.

</details>


### [223] [Think2SQL: Reinforce LLM Reasoning Capabilities for Text2SQL](https://arxiv.org/abs/2504.15077)
*Simone Papicchio,Simone Rossi,Luca Cagliero,Paolo Papotti*

Main category: cs.LG

TL;DR: 论文研究了不同LLM训练策略（ZSL、SFT、RL、SFT+RL）对Text2SQL性能的影响，发现小模型通过SFT+RL在复杂任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在Text2SQL任务中表现优异，但小模型在复杂查询和多表操作中表现不佳，需要探索如何通过推理能力提升性能。

Method: 研究了四种LLM设置：ZSL（带或不带通用推理）、SFT（带或不带任务特定推理痕迹）、RL（以执行准确率为奖励）、SFT+RL（两阶段方法）。

Result: 通用推理在ZSL中效果有限；小模型通过SFT+RL表现优异，尤其在复杂任务中；RL对多表和多跳推理任务特别有效。

Conclusion: SFT+RL策略能有效提升小模型在复杂Text2SQL任务中的性能，使其接近甚至超越更大模型的表现。

Abstract: Large Language Models (LLMs) have shown impressive capabilities in
transforming natural language questions about relational databases into SQL
queries. Despite recent improvements, small LLMs struggle to handle questions
involving multiple tables and complex SQL patterns under a Zero-Shot Learning
(ZSL) setting. Supervised Fine-Tuning (SFT) partially compensate the knowledge
deficits in pretrained models but falls short while dealing with queries
involving multi-hop reasoning. To bridge this gap, different LLM training
strategies to reinforce reasoning capabilities have been proposed, ranging from
leveraging a thinking process within ZSL, including reasoning traces in SFT, or
adopt Reinforcement Learning (RL) strategies. However, the influence of
reasoning on Text2SQL performance is still largely unexplored. This paper
investigates to what extent LLM reasoning capabilities influence their Text2SQL
performance on four benchmark datasets. To this end, it considers the following
LLM settings: (1) ZSL, including general-purpose reasoning or not; (2) SFT,
with and without task-specific reasoning traces; (3) RL, leveraging execution
accuracy as primary reward function; (4) SFT+RL, i.e, a two-stage approach that
combines SFT and RL. The results show that general-purpose reasoning under ZSL
proves to be ineffective in tackling complex Text2SQL cases. Small LLMs benefit
from SFT with reasoning much more than larger ones, bridging the gap of their
(weaker) model pretraining. RL is generally beneficial across all tested models
and datasets, particularly when SQL queries involve multi-hop reasoning and
multiple tables. Small LLMs with SFT+RL excel on most complex datasets thanks
to a strategic balance between generality of the reasoning process and
optimization of the execution accuracy. Thanks to RL, the7B Qwen-Coder-2.5
model performs on par with 100+ Billion ones on the Bird dataset.

</details>


### [224] [Federated Latent Factor Model for Bias-Aware Recommendation with Privacy-Preserving](https://arxiv.org/abs/2504.15090)
*Junxiang Gao,Yixin Ran,Jia Chen*

Main category: cs.LG

TL;DR: 论文提出了一种联邦偏置感知潜在因子（FBALF）模型，用于解决联邦推荐系统中的评分偏置问题，同时保护用户隐私。


<details>
  <summary>Details</summary>
Motivation: 传统推荐系统集中处理用户数据，存在隐私泄露风险；联邦学习虽保护隐私，但无法直接处理评分偏置问题。

Method: FBALF模型在本地模型的损失函数中显式引入训练偏置，消除评分偏置而不泄露数据。

Result: 在三个真实数据集上的实验表明，FBALF的推荐准确性显著优于其他联邦推荐系统。

Conclusion: FBALF有效解决了联邦推荐系统中的评分偏置问题，同时确保了数据隐私。

Abstract: A recommender system (RS) aims to provide users with personalized item
recommendations, enhancing their overall experience. Traditional RSs collect
and process all user data on a central server. However, this centralized
approach raises significant privacy concerns, as it increases the risk of data
breaches and privacy leakages, which are becoming increasingly unacceptable to
privacy-sensitive users. To address these privacy challenges, federated
learning has been integrated into RSs, ensuring that user data remains secure.
In centralized RSs, the issue of rating bias is effectively addressed by
jointly analyzing all users' raw interaction data. However, this becomes a
significant challenge in federated RSs, as raw data is no longer accessible due
to privacy-preserving constraints. To overcome this problem, we propose a
Federated Bias-Aware Latent Factor (FBALF) model. In FBALF, training bias is
explicitly incorporated into every local model's loss function, allowing for
the effective elimination of rating bias without compromising data privacy.
Extensive experiments conducted on three real-world datasets demonstrate that
FBALF achieves significantly higher recommendation accuracy compared to other
state-of-the-art federated RSs.

</details>


### [225] [Fast-Slow Co-advancing Optimizer: Toward Harmonious Adversarial Training of GAN](https://arxiv.org/abs/2504.15099)
*Lin Wang,Xiancheng Wang,Rui Wang,Zhibo Zhang,Minghang Zhao*

Main category: cs.LG

TL;DR: 本文提出了一种名为FSCO的新型智能优化器，通过强化学习控制GANs的训练步长，提升训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 传统GANs的训练过程对数据和超参数敏感，易出现振荡或收敛困难，FSCO旨在解决这一问题。

Method: 利用强化学习控制训练步长，实现可变学习率，使训练更智能。

Result: 在三个基准数据集上验证了FSCO的有效性。

Conclusion: FSCO能显著提升GANs的训练稳定性，降低对步长的敏感性。

Abstract: Up to now, the training processes of typical Generative Adversarial Networks
(GANs) are still particularly sensitive to data properties and hyperparameters,
which may lead to severe oscillations, difficulties in convergence, or even
failures to converge, especially when the overall variances of the training
sets are large. These phenomena are often attributed to the training
characteristics of such networks. Aiming at the problem, this paper develops a
new intelligent optimizer, Fast-Slow Co-advancing Optimizer (FSCO), which
employs reinforcement learning in the training process of GANs to make training
easier. Specifically, this paper allows the training step size to be controlled
by an agent to improve training stability, and makes the training process more
intelligent with variable learning rates, making GANs less sensitive to step
size. Experiments have been conducted on three benchmark datasets to verify the
effectiveness of the developed FSCO.

</details>


### [226] [Kolmogorov-Arnold Networks: Approximation and Learning Guarantees for Functions and their Derivatives](https://arxiv.org/abs/2504.15110)
*Anastasis Kratsios,Takashi Furuya*

Main category: cs.LG

TL;DR: KANs基于Kolmogorov-Arnold叠加定理，通过可训练的样条激活函数提升深度学习的适应性。本文证明KAN能在有界或分形域上最优逼近Besov函数，并提供样本复杂度估计。


<details>
  <summary>Details</summary>
Motivation: 探索KAN的理论基础，证明其在逼近Besov函数上的最优性，并评估其样本复杂度。

Method: 利用残差连接和可训练样条激活函数构建KAN架构，分析其在Besov空间中的逼近能力。

Result: KAN能最优逼近Besov函数，且样本复杂度与维度无关。

Conclusion: KAN在理论和实践上均表现出优越性，为深度学习提供了新的理论基础。

Abstract: Inspired by the Kolmogorov-Arnold superposition theorem, Kolmogorov-Arnold
Networks (KANs) have recently emerged as an improved backbone for most deep
learning frameworks, promising more adaptivity than their multilayer perception
(MLP) predecessor by allowing for trainable spline-based activation functions.
In this paper, we probe the theoretical foundations of the KAN architecture by
showing that it can optimally approximate any Besov function in
$B^{s}_{p,q}(\mathcal{X})$ on a bounded open, or even fractal, domain
$\mathcal{X}$ in $\mathbb{R}^d$ at the optimal approximation rate with respect
to any weaker Besov norm $B^{\alpha}_{p,q}(\mathcal{X})$; where $\alpha < s$.
We complement our approximation guarantee with a dimension-free estimate on the
sample complexity of a residual KAN model when learning a function of Besov
regularity from $N$ i.i.d. noiseless samples. Our KAN architecture incorporates
contemporary deep learning wisdom by leveraging residual/skip connections
between layers.

</details>


### [227] [Survey of Loss Augmented Knowledge Tracing](https://arxiv.org/abs/2504.15163)
*Altun Shukurlu*

Main category: cs.LG

TL;DR: 论文探讨了损失函数在人工神经网络训练中的重要性，并综述了基于深度学习的知识追踪算法及其改进。


<details>
  <summary>Details</summary>
Motivation: 解决数据质量不足或学习效率低下的问题，通过改进损失函数提升模型性能和鲁棒性。

Method: 综述了对比学习知识追踪算法（如Bi-CLKT、CL4KT等）及其性能表现。

Result: 展示了先进损失函数在知识追踪算法中的改进效果，并讨论了实际部署挑战。

Conclusion: 提出了未来研究方向，如混合损失策略和上下文感知建模。

Abstract: The training of artificial neural networks is heavily dependent on the
careful selection of an appropriate loss function. While commonly used loss
functions, such as cross-entropy and mean squared error (MSE), generally
suffice for a broad range of tasks, challenges often emerge due to limitations
in data quality or inefficiencies within the learning process. In such
circumstances, the integration of supplementary terms into the loss function
can serve to address these challenges, enhancing both model performance and
robustness. Two prominent techniques, loss regularization and contrastive
learning, have been identified as effective strategies for augmenting the
capacity of loss functions in artificial neural networks.
  Knowledge tracing is a compelling area of research that leverages predictive
artificial intelligence to facilitate the automation of personalized and
efficient educational experiences for students. In this paper, we provide a
comprehensive review of the deep learning-based knowledge tracing (DKT)
algorithms trained using advanced loss functions and discuss their improvements
over prior techniques. We discuss contrastive knowledge tracing algorithms,
such as Bi-CLKT, CL4KT, SP-CLKT, CoSKT, and prediction-consistent DKT,
providing performance benchmarks and insights into real-world deployment
challenges. The survey concludes with future research directions, including
hybrid loss strategies and context-aware modeling.

</details>


### [228] [Audio-Visual Class-Incremental Learning for Fish Feeding intensity Assessment in Aquaculture](https://arxiv.org/abs/2504.15171)
*Meng Cui,Xianghu Yue,Xinyuan Qian,Jinzheng Zhao,Haohe Liu,Xubo Liu,Daoliang Li,Wenwu Wang*

Main category: cs.LG

TL;DR: 论文提出了一种新的音频-视觉类增量学习框架HAIL-FFIA，用于鱼类摄食强度评估（FFIA），解决了现有方法在适应新鱼种或环境时的灾难性遗忘和数据存储问题。


<details>
  <summary>Details</summary>
Motivation: 工业水产养殖管理中，多模态方法在FFIA中表现良好，但面临适应新鱼种或环境时的灾难性遗忘和数据集不足的挑战。

Method: 引入AV-CIL-FFIA数据集，并提出HAIL-FFIA框架，采用原型化方法实现无样本存储的高效学习，同时通过分层表示学习和动态模态平衡系统保留知识。

Result: HAIL-FFIA在AV-CIL-FFIA数据集上表现优于现有方法，准确性更高且存储需求更低，有效缓解灾难性遗忘。

Conclusion: HAIL-FFIA为FFIA提供了一种高效且适应性强的解决方案，适用于多鱼种和环境下的增量学习。

Abstract: Fish Feeding Intensity Assessment (FFIA) is crucial in industrial aquaculture
management. Recent multi-modal approaches have shown promise in improving FFIA
robustness and efficiency. However, these methods face significant challenges
when adapting to new fish species or environments due to catastrophic
forgetting and the lack of suitable datasets. To address these limitations, we
first introduce AV-CIL-FFIA, a new dataset comprising 81,932 labelled
audio-visual clips capturing feeding intensities across six different fish
species in real aquaculture environments. Then, we pioneer audio-visual class
incremental learning (CIL) for FFIA and demonstrate through benchmarking on
AV-CIL-FFIA that it significantly outperforms single-modality methods. Existing
CIL methods rely heavily on historical data. Exemplar-based approaches store
raw samples, creating storage challenges, while exemplar-free methods avoid
data storage but struggle to distinguish subtle feeding intensity variations
across different fish species. To overcome these limitations, we introduce
HAIL-FFIA, a novel audio-visual class-incremental learning framework that
bridges this gap with a prototype-based approach that achieves exemplar-free
efficiency while preserving essential knowledge through compact feature
representations. Specifically, HAIL-FFIA employs hierarchical representation
learning with a dual-path knowledge preservation mechanism that separates
general intensity knowledge from fish-specific characteristics. Additionally,
it features a dynamic modality balancing system that adaptively adjusts the
importance of audio versus visual information based on feeding behaviour
stages. Experimental results show that HAIL-FFIA is superior to SOTA methods on
AV-CIL-FFIA, achieving higher accuracy with lower storage needs while
effectively mitigating catastrophic forgetting in incremental fish species
learning.

</details>


### [229] [How Global Calibration Strengthens Multiaccuracy](https://arxiv.org/abs/2504.15206)
*Sílvia Casacuberta,Parikshit Gopalan,Varun Kanade,Omer Reingold*

Main category: cs.LG

TL;DR: 多准确性和多校准是多组公平性概念，通过弱不可知学习实现。研究发现，多准确性单独较弱，但结合全局校准（校准多准确性）能显著提升其能力，甚至恢复多校准的强学习效果。


<details>
  <summary>Details</summary>
Motivation: 研究多准确性和多校准作为学习原语的能力，探索它们在弱不可知学习和硬核度量生成中的作用。

Method: 通过理论分析，比较多准确性、校准多准确性和多校准在不同学习任务中的表现。

Result: 多准确性单独较弱，但结合全局校准后能实现强不可知学习；校准多准确性在硬核度量生成中达到最优密度。

Conclusion: 多准确性和全局校准互补，结合后能显著提升学习能力，揭示了它们在多组公平性中的协同作用。

Abstract: Multiaccuracy and multicalibration are multigroup fairness notions for
prediction that have found numerous applications in learning and computational
complexity. They can be achieved from a single learning primitive: weak
agnostic learning. Here we investigate the power of multiaccuracy as a learning
primitive, both with and without the additional assumption of calibration. We
find that multiaccuracy in itself is rather weak, but that the addition of
global calibration (this notion is called calibrated multiaccuracy) boosts its
power substantially, enough to recover implications that were previously known
only assuming the stronger notion of multicalibration.
  We give evidence that multiaccuracy might not be as powerful as standard weak
agnostic learning, by showing that there is no way to post-process a
multiaccurate predictor to get a weak learner, even assuming the best
hypothesis has correlation $1/2$. Rather, we show that it yields a restricted
form of weak agnostic learning, which requires some concept in the class to
have correlation greater than $1/2$ with the labels. However, by also requiring
the predictor to be calibrated, we recover not just weak, but strong agnostic
learning.
  A similar picture emerges when we consider the derivation of hardcore
measures from predictors satisfying multigroup fairness notions. On the one
hand, while multiaccuracy only yields hardcore measures of density half the
optimal, we show that (a weighted version of) calibrated multiaccuracy achieves
optimal density.
  Our results yield new insights into the complementary roles played by
multiaccuracy and calibration in each setting. They shed light on why
multiaccuracy and global calibration, although not particularly powerful by
themselves, together yield considerably stronger notions.

</details>


### [230] [Compute-Optimal LLMs Provably Generalize Better With Scale](https://arxiv.org/abs/2504.15208)
*Marc Finzi,Sanyam Kapoor,Diego Granziol,Anming Gu,Christopher De Sa,J. Zico Kolter,Andrew Gordon Wilson*

Main category: cs.LG

TL;DR: 研究探讨了为什么更大的语言模型泛化能力更强，通过计算最优条件下的泛化边界，发现模型规模增大时泛化差距减小。


<details>
  <summary>Details</summary>
Motivation: 探究大规模语言模型（LLMs）在计算最优条件下泛化能力提升的原因。

Method: 提出一种新的Freedman型鞅集中不等式，结合损失函数方差，分解泛化边界为三个可解释部分：每令牌参数数、损失方差和固定比特率下的量化误差。

Result: 发现随着模型规模增大，损失方差和量化误差减小，泛化差距随之缩小。

Conclusion: 更大的模型在计算最优条件下泛化能力更强，且泛化差距随规模增大而减小。

Abstract: Why do larger language models generalize better? To investigate this
question, we develop generalization bounds on the pretraining objective of
large language models (LLMs) in the compute-optimal regime, as described by the
Chinchilla scaling laws. We introduce a novel, fully empirical Freedman-type
martingale concentration inequality that tightens existing bounds by accounting
for the variance of the loss function. This generalization bound can be
decomposed into three interpretable components: the number of parameters per
token, the loss variance, and the quantization error at a fixed bitrate. As
compute-optimal language models are scaled up, the number of parameters per
data point remains constant; however, both the loss variance and the
quantization error decrease, implying that larger models should have smaller
generalization gaps. We examine why larger models tend to be more quantizable
from an information theoretic perspective, showing that the rate at which they
can integrate new information grows more slowly than their capacity on the
compute-optimal frontier. From these findings we produce a scaling law for the
generalization gap, with bounds that become predictably stronger with scale.

</details>


### [231] [A Causal Convolutional Low-rank Representation Model for Imputation of Water Quality Data](https://arxiv.org/abs/2504.15209)
*Xin Liao,Bing Yang,Tan Dongli,Cai Yu*

Main category: cs.LG

TL;DR: 本文提出了一种因果卷积低秩表示（CLR）模型，用于填补水质监测数据中的缺失值，以提高数据的完整性。该模型结合了时间依赖性和自动超参数调整，实验证明其在准确性和效率上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 水质监测数据常因设备故障等原因存在缺失值，传统填补方法效果不佳，影响决策。因此，需要一种更准确的方法来填补缺失值。

Method: 采用因果卷积操作考虑低秩表示的时间依赖性，并结合自动超参数调整方案，优化模型训练。

Result: 在三个真实水质数据集上的实验表明，CLR模型在填补准确性和时间成本上优于现有方法。

Conclusion: CLR模型为环境监测提供了更可靠的决策支持，填补效果显著。

Abstract: The monitoring of water quality is a crucial part of environmental
protection, and a large number of monitors are widely deployed to monitor water
quality. Due to unavoidable factors such as data acquisition breakdowns,
sensors and communication failures, water quality monitoring data suffers from
missing values over time, resulting in High-Dimensional and Sparse (HDS) Water
Quality Data (WQD). The simple and rough filling of the missing values leads to
inaccurate results and affects the implementation of relevant measures.
Therefore, this paper proposes a Causal convolutional Low-rank Representation
(CLR) model for imputing missing WQD to improve the completeness of the WQD,
which employs a two-fold idea: a) applying causal convolutional operation to
consider the temporal dependence of the low-rank representation, thus
incorporating temporal information to improve the imputation accuracy; and b)
implementing a hyperparameters adaptation scheme to automatically adjust the
best hyperparameters during model training, thereby reducing the tedious manual
adjustment of hyper-parameters. Experimental studies on three real-world water
quality datasets demonstrate that the proposed CLR model is superior to some of
the existing state-of-the-art imputation models in terms of imputation accuracy
and time cost, as well as indicating that the proposed model provides more
reliable decision support for environmental monitoring.

</details>


### [232] [Histogram-based Parameter-efficient Tuning for Passive Sonar Classification](https://arxiv.org/abs/2504.15214)
*Amirmohammad Mohammadi,Davelle Carreiro,Alexandra Van Dine,Joshua Peeples*

Main category: cs.LG

TL;DR: 提出了一种基于直方图的高效参数调优方法（HPT），用于解决现有适配器方法在特征嵌入分布偏移上的不足，并在被动声纳数据集上表现优于传统适配器。


<details>
  <summary>Details</summary>
Motivation: 现有参数高效迁移学习方法（如适配器）在处理中间特征嵌入的分布偏移时表现不佳，需要一种更有效的方法来捕捉目标域统计信息并调整嵌入。

Method: 提出HPT技术，通过直方图捕捉目标域统计信息并调制嵌入，实现高效参数调优。

Result: 在三个被动声纳数据集（ShipsEar、DeepShip、VTUAD）上，HPT表现优于传统适配器，VTUAD上准确率达91.8%（vs. 89.8%）。HPT训练更快且特征表示更接近全微调模型。

Conclusion: HPT在参数节省和性能间取得平衡，为资源受限环境中的可扩展迁移学习提供了新方向。

Abstract: Parameter-efficient transfer learning (PETL) methods adapt large artificial
neural networks to downstream tasks without fine-tuning the entire model.
However, existing additive methods, such as adapters, sometimes struggle to
capture distributional shifts in intermediate feature embeddings. We propose a
novel histogram-based parameter-efficient tuning (HPT) technique that captures
the statistics of the target domain and modulates the embeddings. Experimental
results on three downstream passive sonar datasets (ShipsEar, DeepShip, VTUAD)
demonstrate that HPT outperforms conventional adapters. Notably, HPT achieves
91.8% vs. 89.8% accuracy on VTUAD. Furthermore, HPT trains faster and yields
feature representations closer to those of fully fine-tuned models. Overall,
HPT balances parameter savings and performance, providing a distribution-aware
alternative to existing adapters and shows a promising direction for scalable
transfer learning in resource-constrained environments. The code is publicly
available:
https://github.com/Advanced-Vision-and-Learning-Lab/HLAST_DeepShip_ParameterEfficient.

</details>


### [233] [A Deep Learning Framework for Sequence Mining with Bidirectional LSTM and Multi-Scale Attention](https://arxiv.org/abs/2504.15223)
*Tao Yang,Yu Cheng,Yaokun Ren,Yujia Lou,Minggu Wei,Honghui Xin*

Main category: cs.LG

TL;DR: 提出了一种结合BiLSTM和多尺度注意力机制的序列模式挖掘算法，用于复杂序列数据中的潜在模式挖掘和上下文依赖建模，实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决复杂序列数据中潜在模式挖掘和上下文依赖建模的挑战。

Method: 集成双向LSTM（BiLSTM）和多尺度注意力机制，BiLSTM捕捉序列的前后依赖，注意力模块自适应分配权重。

Result: 在公开多元时间序列数据集上实验，模型在准确率、精确率和召回率上优于现有方法。

Conclusion: 该架构在复杂模式识别任务中有效且鲁棒，注意力尺度和输入序列长度对性能有影响，为结构优化提供依据。

Abstract: This paper addresses the challenges of mining latent patterns and modeling
contextual dependencies in complex sequence data. A sequence pattern mining
algorithm is proposed by integrating Bidirectional Long Short-Term Memory
(BiLSTM) with a multi-scale attention mechanism. The BiLSTM captures both
forward and backward dependencies in sequences, enhancing the model's ability
to perceive global contextual structures. At the same time, the multi-scale
attention module assigns adaptive weights to key feature regions under
different window sizes. This improves the model's responsiveness to both local
and global important information. Extensive experiments are conducted on a
publicly available multivariate time series dataset. The proposed model is
compared with several mainstream sequence modeling methods. Results show that
it outperforms existing models in terms of accuracy, precision, and recall.
This confirms the effectiveness and robustness of the proposed architecture in
complex pattern recognition tasks. Further ablation studies and sensitivity
analyses are carried out to investigate the effects of attention scale and
input sequence length on model performance. These results provide empirical
support for structural optimization of the model.

</details>


### [234] [M$^2$AD: Multi-Sensor Multi-System Anomaly Detection through Global Scoring and Calibrated Thresholding](https://arxiv.org/abs/2504.15225)
*Sarah Alnegheimish,Zelin He,Matthew Reimherr,Akash Chandrayan,Abhinav Pradhan,Luca D'Angelo*

Main category: cs.LG

TL;DR: M$^2$AD是一个用于多系统多变量时间序列数据的无监督异常检测框架，通过深度模型和残差分析解决异构性问题，性能优于现有方法21%。


<details>
  <summary>Details</summary>
Motivation: 现有异常检测方法多针对单变量或单系统数据，无法应对多系统异构时间序列的复杂场景。

Method: 使用深度模型捕捉正常行为，残差作为异常指标，通过高斯混合模型和Gamma校准生成全局异常分数。

Result: M$^2$AD在实验中平均优于现有方法21%，并在亚马逊物流中心的130个资产上验证了有效性。

Conclusion: M$^2$AD能有效处理多系统和传感器间的异构性与依赖性，适用于复杂工业场景。

Abstract: With the widespread availability of sensor data across industrial and
operational systems, we frequently encounter heterogeneous time series from
multiple systems. Anomaly detection is crucial for such systems to facilitate
predictive maintenance. However, most existing anomaly detection methods are
designed for either univariate or single-system multivariate data, making them
insufficient for these complex scenarios. To address this, we introduce
M$^2$AD, a framework for unsupervised anomaly detection in multivariate time
series data from multiple systems. M$^2$AD employs deep models to capture
expected behavior under normal conditions, using the residuals as indicators of
potential anomalies. These residuals are then aggregated into a global anomaly
score through a Gaussian Mixture Model and Gamma calibration. We theoretically
demonstrate that this framework can effectively address heterogeneity and
dependencies across sensors and systems. Empirically, M$^2$AD outperforms
existing methods in extensive evaluations by 21% on average, and its
effectiveness is demonstrated on a large-scale real-world case study on 130
assets in Amazon Fulfillment Centers. Our code and results are available at
https://github.com/sarahmish/M2AD.

</details>


### [235] [Conformalized-KANs: Uncertainty Quantification with Coverage Guarantees for Kolmogorov-Arnold Networks (KANs) in Scientific Machine Learning](https://arxiv.org/abs/2504.15240)
*Amirhossein Mollaali,Christian Bolivar Moya,Amanda A. Howard,Alexander Heinlein,Panos Stinis,Guang Lin*

Main category: cs.LG

TL;DR: 本文研究了Kolmogorov-Arnold Networks (KANs)中的不确定性量化方法，提出了一种集成方法和Conformalized-KANs，以提升预测区间的校准性和覆盖性。


<details>
  <summary>Details</summary>
Motivation: 提升KANs在复杂函数建模中的可解释性和鲁棒性，并增强其不确定性量化能力。

Method: 采用集成KANs和Conformalized-KANs方法，结合共形预测技术生成校准的预测区间。

Result: 实验表明，方法能有效提升预测区间的鲁棒性和准确性，并适用于KANs的扩展版本。

Conclusion: 该方法显著提升了KANs在科学机器学习中的可靠性和适用性。

Abstract: This paper explores uncertainty quantification (UQ) methods in the context of
Kolmogorov-Arnold Networks (KANs). We apply an ensemble approach to KANs to
obtain a heuristic measure of UQ, enhancing interpretability and robustness in
modeling complex functions. Building on this, we introduce Conformalized-KANs,
which integrate conformal prediction, a distribution-free UQ technique, with
KAN ensembles to generate calibrated prediction intervals with guaranteed
coverage. Extensive numerical experiments are conducted to evaluate the
effectiveness of these methods, focusing particularly on the robustness and
accuracy of the prediction intervals under various hyperparameter settings. We
show that the conformal KAN predictions can be applied to recent extensions of
KANs, including Finite Basis KANs (FBKANs) and multifideilty KANs (MFKANs). The
results demonstrate the potential of our approaches to improve the reliability
and applicability of KANs in scientific machine learning.

</details>


### [236] [Roll the dice & look before you leap: Going beyond the creative limits of next-token prediction](https://arxiv.org/abs/2504.15266)
*Vaishnavh Nagarajan,Chen Henry Wu,Charles Ding,Aditi Raghunathan*

Main category: cs.LG

TL;DR: 论文设计了一套最小算法任务，用于量化语言模型的创造力限制，发现多令牌方法优于单令牌学习，并提出输入层噪声注入方法以提升随机性与连贯性。


<details>
  <summary>Details</summary>
Motivation: 研究语言模型在开放任务中的创造力限制，探索如何改进现有方法以提升多样性和原创性。

Method: 设计抽象任务模拟现实开放任务，比较单令牌与多令牌学习方法，提出输入层噪声注入（hash-conditioning）。

Result: 多令牌方法（如无教师训练和扩散模型）表现更优；输入层噪声注入比输出层温度采样更有效。

Conclusion: 为分析开放创造力提供了测试框架，支持超越单令牌学习和基于softmax的采样方法。

Abstract: We design a suite of minimal algorithmic tasks that are a loose abstraction
of open-ended real-world tasks. This allows us to cleanly and controllably
quantify the creative limits of the present-day language model. Much like
real-world tasks that require a creative, far-sighted leap of thought, our
tasks require an implicit, open-ended stochastic planning step that either (a)
discovers new connections in an abstract knowledge graph (like in wordplay,
drawing analogies, or research) or (b) constructs new patterns (like in
designing math problems or new proteins). In these tasks, we empirically and
conceptually argue how next-token learning is myopic and memorizes excessively;
comparatively, multi-token approaches, namely teacherless training and
diffusion models, excel in producing diverse and original output. Secondly, in
our tasks, we find that to elicit randomness from the Transformer without
hurting coherence, it is better to inject noise right at the input layer (via a
method we dub hash-conditioning) rather than defer to temperature sampling from
the output layer. Thus, our work offers a principled, minimal test-bed for
analyzing open-ended creative skills, and offers new arguments for going beyond
next-token learning and softmax-based sampling. We make part of the code
available under https://github.com/chenwu98/algorithmic-creativity

</details>


### [237] [Single-loop Algorithms for Stochastic Non-convex Optimization with Weakly-Convex Constraints](https://arxiv.org/abs/2504.15243)
*Ming Yang,Gang Li,Quanqi Hu,Qihang Lin,Tianbao Yang*

Main category: cs.LG

TL;DR: 提出了一种新的单循环惩罚随机算法，用于解决弱凸目标函数和约束函数的优化问题，实现了最先进的复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有方法在收敛速度或算法设计上存在局限，需要改进。

Method: 采用基于铰链的惩罚方法，允许恒定惩罚参数，扩展至有限和耦合组合目标。

Result: 在近似KKT解上达到最优复杂度，并在实验中验证了有效性。

Conclusion: 新算法在机器学习的公平学习和持续学习应用中表现优异。

Abstract: Constrained optimization with multiple functional inequality constraints has
significant applications in machine learning. This paper examines a crucial
subset of such problems where both the objective and constraint functions are
weakly convex. Existing methods often face limitations, including slow
convergence rates or reliance on double-loop algorithmic designs. To overcome
these challenges, we introduce a novel single-loop penalty-based stochastic
algorithm. Following the classical exact penalty method, our approach employs a
{\bf hinge-based penalty}, which permits the use of a constant penalty
parameter, enabling us to achieve a {\bf state-of-the-art complexity} for
finding an approximate Karush-Kuhn-Tucker (KKT) solution. We further extend our
algorithm to address finite-sum coupled compositional objectives, which are
prevalent in artificial intelligence applications, establishing improved
complexity over existing approaches. Finally, we validate our method through
experiments on fair learning with receiver operating characteristic (ROC)
fairness constraints and continual learning with non-forgetting constraints.

</details>


### [238] [Faster Algorithms for Agnostically Learning Disjunctions and their Implications](https://arxiv.org/abs/2504.15244)
*Ilias Diakonikolas,Daniel M. Kane,Lisheng Ren*

Main category: cs.LG

TL;DR: 本文提出了一种在分布自由不可知PAC模型中学习布尔析取的新算法，复杂度为2^(O(n^(1/3)))，优于现有最佳算法的2^(O(n^(1/2)))。


<details>
  <summary>Details</summary>
Motivation: 研究布尔析取在不可知PAC模型中的高效学习算法，突破现有CSQ算法的复杂度限制。

Method: 开发了一种新的不可知学习算法，可在统计查询（SQ）模型中实现，复杂度为2^(O(n^(1/3)))。

Result: 算法复杂度显著优于现有最佳CSQ算法，并首次在分布自由不可知学习中实现了SQ与CSQ模型的分离。

Conclusion: 新算法在复杂度和模型分离方面取得了突破，为布尔析取学习提供了更高效的解决方案。

Abstract: We study the algorithmic task of learning Boolean disjunctions in the
distribution-free agnostic PAC model. The best known agnostic learner for the
class of disjunctions over $\{0, 1\}^n$ is the $L_1$-polynomial regression
algorithm, achieving complexity $2^{\tilde{O}(n^{1/2})}$. This complexity bound
is known to be nearly best possible within the class of Correlational
Statistical Query (CSQ) algorithms. In this work, we develop an agnostic
learner for this concept class with complexity $2^{\tilde{O}(n^{1/3})}$. Our
algorithm can be implemented in the Statistical Query (SQ) model, providing the
first separation between the SQ and CSQ models in distribution-free agnostic
learning.

</details>


### [239] [On Learning Parallel Pancakes with Mostly Uniform Weights](https://arxiv.org/abs/2504.15251)
*Ilias Diakonikolas,Daniel M. Kane,Sushrut Karmalkar,Jasper C. H. Lee,Thanasis Pittas*

Main category: cs.LG

TL;DR: 研究学习$k$-高斯混合模型（$k$-GMMs）的复杂度，发现其复杂度为$d^{\Omega(k)}$。通过假设权重非指数小且协方差相同，已有算法时间为$d^{O(\log(1/w_{\min}))}$。本文证明此上界几乎最优，且进一步探讨权重分布对复杂度的影响。


<details>
  <summary>Details</summary>
Motivation: 研究高斯混合模型学习的计算复杂度，特别是在结构假设下，以克服指数级复杂度的限制。

Method: 通过统计查询（SQ）下界证明算法的准多项式上界几乎最优，并分析权重分布对测试任务的影响。

Result: 证明在均匀权重下，准多项式上界几乎最优；同时提出在大部分权重均匀时，测试任务的准多项式上界。

Conclusion: 高斯混合模型学习的复杂度在结构假设下仍具有挑战性，但通过权重分布的优化可以改善算法效率。

Abstract: We study the complexity of learning $k$-mixtures of Gaussians ($k$-GMMs) on
$\mathbb{R}^d$. This task is known to have complexity $d^{\Omega(k)}$ in full
generality. To circumvent this exponential lower bound on the number of
components, research has focused on learning families of GMMs satisfying
additional structural properties. A natural assumption posits that the
component weights are not exponentially small and that the components have the
same unknown covariance. Recent work gave a $d^{O(\log(1/w_{\min}))}$-time
algorithm for this class of GMMs, where $w_{\min}$ is the minimum weight. Our
first main result is a Statistical Query (SQ) lower bound showing that this
quasi-polynomial upper bound is essentially best possible, even for the special
case of uniform weights. Specifically, we show that it is SQ-hard to
distinguish between such a mixture and the standard Gaussian. We further
explore how the distribution of weights affects the complexity of this task.
Our second main result is a quasi-polynomial upper bound for the aforementioned
testing task when most of the weights are uniform while a small fraction of the
weights are potentially arbitrary.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [240] [OPO: Making Decision-Focused Data Acquisition Decisions](https://arxiv.org/abs/2504.15062)
*Egon Peršak,Miguel F. Anjos*

Main category: math.OC

TL;DR: 提出了一种用于在上下文随机优化问题中做出数据获取决策的模型，重点关注成本约束下的数据获取，并通过可微分优化提升决策质量。


<details>
  <summary>Details</summary>
Motivation: 传统数据获取决策通常独立且固定，而实际中获取上下文变量可能成本高昂且受限。研究旨在优化数据获取以提升下游决策质量。

Method: 利用可微分优化扩展预测与优化的整合，学习替代线性目标函数，解决具有明确约束的数据获取问题。

Result: 在无人机侦察最短路径问题中，可微分优化方法优于随机搜索策略。

Conclusion: 通过可微分优化整合数据获取与决策优化，显著提升了决策质量。

Abstract: We propose a model for making data acquisition decisions for variables in
contextual stochastic optimisation problems. Data acquisition decisions are
typically treated as separate and fixed. We explore problem settings in which
the acquisition of contextual variables is costly and consequently constrained.
The data acquisition problem is often solved heuristically for proxy objectives
such as coverage. The more intuitive objective is the downstream decision
quality as a result of data acquisition decisions. The whole pipeline can be
characterised as an optimise-then-predict-then-optimise (OPO) problem.
Analogously, much recent research has focused on how to integrate prediction
and optimisation (PO) in the form of decision-focused learning. We propose
leveraging differentiable optimisation to extend the integration to data
acquisition. We solve the data acquisition problem with well-defined
constraints by learning a surrogate linear objective function. We demonstrate
an application of this model on a shortest path problem for which we first have
to set a drone reconnaissance strategy to capture image segments serving as
inputs to a model that predicts travel costs. We ablate the problem with a
number of training modalities and demonstrate that the differentiable
optimisation approach outperforms random search strategies.

</details>


<div id='math.CO'></div>

# math.CO [[Back]](#toc)

### [241] [Density Measures for Language Generation](https://arxiv.org/abs/2504.14370)
*Jon Kleinberg,Fan Wei*

Main category: math.CO

TL;DR: 论文提出了一种抽象的语言生成框架，研究算法在未知语言中生成新字符串时的有效性与广度的权衡，并开发了量化方法。


<details>
  <summary>Details</summary>
Motivation: 研究语言生成中有效性与广度的权衡，解决现有算法输出密度为零的问题。

Method: 提出一种算法，确保输出在真实语言中具有严格正密度，并分析其内部表示。

Result: 算法实现了严格正密度的输出，并揭示了实现最大广度可能需要在高密度与低密度表示之间振荡。

Conclusion: 通过引入新的语言族拓扑结构，论文为语言生成中的权衡问题提供了定量分析工具。

Abstract: The recent successes of large language models (LLMs) have led to a surge of
theoretical research into language generation. A recent line of work proposes
an abstract view, called language generation in the limit, where generation is
seen as a game between an adversary and an algorithm: the adversary generates
strings from an unknown language $K$, chosen from a countable collection of
candidate languages, and after seeing a finite set of these strings, the
algorithm must generate new strings from $K$ that it has not seen before. This
formalism highlights a key tension: the trade-off between validity (the
algorithm should only produce strings from the language) and breadth (it should
be able to produce many strings from the language). This trade-off is central
in applied language generation as well, where it appears as a balance between
hallucination (generating invalid utterances) and mode collapse (generating
only a restricted set of outputs). Despite its importance, this trade-off has
been challenging to study quantitatively. We develop ways to quantify this
trade-off by formalizing breadth using measures of density. Existing algorithms
for language generation in the limit produce output sets that can have zero
density in the true language, and this important failure of breadth might seem
unavoidable. We show, however, that such a failure is not necessary: we provide
an algorithm for language generation in the limit whose outputs have strictly
positive density in $K$. We also study the internal representations built by
these algorithms, specifically the sequence of hypothesized candidate languages
they consider, and show that achieving the strongest form of breadth may
require oscillating indefinitely between high- and low-density representations.
Our analysis introduces a novel topology on language families, with notions of
convergence and limit points playing a key role.

</details>


<div id='q-fin.PM'></div>

# q-fin.PM [[Back]](#toc)

### [242] [Integrating LLM-Generated Views into Mean-Variance Optimization Using the Black-Litterman Model](https://arxiv.org/abs/2504.14345)
*Youngbin Lee,Yejin Kim,Suin Kim,Yongjae Lee*

Main category: q-fin.PM

TL;DR: 研究探讨了将大语言模型（LLM）生成的观点整合到Black-Litterman框架中的投资组合优化方法，通过历史价格和公司元数据预测股票收益，并比较了不同LLM的表现。


<details>
  <summary>Details</summary>
Motivation: 传统均值-方差模型对输入敏感，Black-Litterman模型虽能整合投资者观点，但观点定义困难。

Method: 利用LLM从历史价格和公司元数据中预测股票收益，并通过预测方差量化不确定性，进行双周再平衡的回测。

Result: 不同LLM表现出不同程度的预测乐观性和信心稳定性，影响投资组合表现。

Conclusion: LLM生成的观点可用于Black-Litterman框架，但需考虑模型选择对结果的影响。

Abstract: Portfolio optimization faces challenges due to the sensitivity in traditional
mean-variance models. The Black-Litterman model mitigates this by integrating
investor views, but defining these views remains difficult. This study explores
the integration of large language models (LLMs) generated views into portfolio
optimization using the Black-Litterman framework. Our method leverages LLMs to
estimate expected stock returns from historical prices and company metadata,
incorporating uncertainty through the variance in predictions. We conduct a
backtest of the LLM-optimized portfolios from June 2024 to February 2025,
rebalancing biweekly using the previous two weeks of price data. As baselines,
we compare against the S&P 500, an equal-weighted portfolio, and a traditional
mean-variance optimized portfolio constructed using the same set of stocks.
Empirical results suggest that different LLMs exhibit varying levels of
predictive optimism and confidence stability, which impact portfolio
performance. The source code and data are available at
https://github.com/youngandbin/LLM-MVO-BLM.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [243] [Segmentation with Noisy Labels via Spatially Correlated Distributions](https://arxiv.org/abs/2504.14795)
*Ryu Tadokoro,Tsukasa Takagi,Shin-ichi Maeda*

Main category: eess.IV

TL;DR: 论文提出了一种基于近似贝叶斯估计的方法，用于处理语义分割中因标注错误（如空间相关性错误）导致的问题，通过高斯分布和KMS矩阵建模空间相关性，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 在语义分割中，高质量标注对模型准确性至关重要，但实际场景（如医学影像和遥感）中获取真实标注困难且易出错，尤其是空间相关的标注错误。

Method: 采用近似贝叶斯估计，假设训练数据包含标注错误，并通过高斯分布和KMS矩阵建模相邻像素间的空间相关性，解决计算难题。

Result: 实验表明，利用标注错误的空间相关性显著提升了性能，在特定任务（如肺部分割）中，性能接近使用干净标注的训练效果。

Conclusion: 提出的方法有效解决了标注错误的空间相关性问题，为实际应用中的语义分割任务提供了可靠解决方案。

Abstract: In semantic segmentation, the accuracy of models heavily depends on the
high-quality annotations. However, in many practical scenarios such as medical
imaging and remote sensing, obtaining true annotations is not straightforward
and usually requires significant human labor. Relying on human labor often
introduces annotation errors, including mislabeling, omissions, and
inconsistency between annotators. In the case of remote sensing, differences in
procurement time can lead to misaligned ground truth annotations. These label
errors are not independently distributed, and instead usually appear in
spatially connected regions where adjacent pixels are more likely to share the
same errors. To address these issues, we propose an approximate Bayesian
estimation based on a probabilistic model that assumes training data includes
label errors, incorporating the tendency for these errors to occur with spatial
correlations between adjacent pixels. Bayesian inference requires computing the
posterior distribution of label errors, which becomes intractable when spatial
correlations are present. We represent the correlation of label errors between
adjacent pixels through a Gaussian distribution whose covariance is structured
by a Kac-Murdock-Szeg\"{o} (KMS) matrix, solving the computational challenges.
Through experiments on multiple segmentation tasks, we confirm that leveraging
the spatial correlation of label errors significantly improves performance.
Notably, in specific tasks such as lung segmentation, the proposed method
achieves performance comparable to training with clean labels under moderate
noise levels. Code is available at
https://github.com/pfnet-research/Bayesian_SpatialCorr.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [244] [GenShin:geometry-enhanced structural graph embodies binding pose can better predicting compound-protein interaction affinity](https://arxiv.org/abs/2504.13853)
*Pingfei Zhu,Chenyang Zhao,Haishi Zhao,Bo Yang*

Main category: q-bio.BM

TL;DR: GenShin模型通过几何增强结构图模块，无需依赖结合构象输入，即可高精度预测化合物-蛋白质亲和力，优于主流模型。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖昂贵且耗时的结合构象实验或模拟，限制了AI在药物发现中的应用。

Method: 构建几何增强结构图模块，分别提取蛋白质和化合物的特征，无需结合构象输入。

Result: GenShin模型在亲和力预测上表现优异，甚至超越依赖结合构象的模型，且对不完整构象更具鲁棒性。

Conclusion: GenShin模型为实际药物发现提供了更实用的解决方案，有望推动AI与药物发现的结合。

Abstract: AI-powered drug discovery typically relies on the successful prediction of
compound-protein interactions, which are pivotal for the evaluation of designed
compound molecules in structure-based drug design and represent a core
challenge in the field.
  However, accurately predicting compound-protein affinity via regression
models usually requires adequate-binding pose, which are derived from costly
and complex experimental methods or time-consuming simulations with docking
software. In response, we have introduced the GenShin model, which constructs a
geometry-enhanced structural graph module that separately extracts additional
features from proteins and compounds. Consequently, it attains an accuracy on
par with mainstream models in predicting compound-protein affinities, while
eliminating the need for adequate-binding pose as input. Our experimental
findings demonstrate that the GenShin model vastly outperforms other models
that rely on non-input docking conformations, achieving, or in some cases even
exceeding, the performance of those requiring adequate-binding pose. Further
experiments indicate that our GenShin model is more robust to
inadequate-binding pose, affirming its higher suitability for real-world drug
discovery scenarios. We hope our work will inspire more endeavors to bridge the
gap between AI models and practical drug discovery challenges.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [245] [From Interaction to Collaboration: How Hybrid Intelligence Enhances Chatbot Feedback](https://arxiv.org/abs/2504.13848)
*Janet Rafner,Ryan Q. Guloy,Eden W. Wen,Catherine M. Chiodo,Jacob Sherson*

Main category: cs.HC

TL;DR: 研究探讨了两种反馈收集机制（标准AI与混合智能HI）对用户反馈行为的影响，发现HI框架下用户提供更详细反馈。


<details>
  <summary>Details</summary>
Motivation: 提升GenAI虚拟助手的用户反馈质量以优化交互和系统成果。

Method: 比较标准AI与HI框架对用户反馈行为的影响。

Result: HI框架显著增加用户反馈的详细程度，但对其他指标无显著影响。

Conclusion: HI框架有助于设计更有效的GenAI反馈系统。

Abstract: Generative AI (GenAI) chatbots are becoming increasingly integrated into
virtual assistant technologies, yet their success hinges on the ability to
gather meaningful user feedback to improve interaction quality, system
outcomes, and overall user acceptance. Successful chatbot interactions can
enable organizations to build long-term relationships with their customers and
users, supporting customer loyalty and furthering the organization's goals.
This study explores the impact of two distinct narratives and feedback
collection mechanisms on user engagement and feedback behavior: a standard
AI-focused interaction versus a hybrid intelligence (HI) framed interaction.
Initial findings indicate that while small-scale survey measures allowed for no
significant differences in user willingness to leave feedback, use the system,
or trust the system, participants exposed to the HI narrative statistically
significantly provided more detailed feedback. These initial findings offer
insights into designing effective feedback systems for GenAI virtual
assistants, balancing user effort with system improvement potential.

</details>


### [246] [Towards Balancing Preference and Performance through Adaptive Personalized Explainability](https://arxiv.org/abs/2504.13856)
*Andrew Silva,Pradyumna Tambwekar,Mariah Schrum,Matthew Gombolay*

Main category: cs.HC

TL;DR: 研究探讨了用户对可解释人工智能（xAI）模式的偏好和性能差异，并提出了自适应个性化策略以平衡偏好与性能。


<details>
  <summary>Details</summary>
Motivation: 随着机器人和数字助手的广泛应用，如何通过xAI增强信任和协作成为关键，但现有方法未充分考虑用户多样化的需求和偏好。

Method: 通过两项用户研究，在模拟自动驾驶车辆（AV）环境中比较不同xAI模式（语言解释、特征重要性图和决策树）的偏好和性能。

Result: 发现xAI模式在偏好和性能上存在显著差异，且用户偏好与性能并不总一致；提出的自适应策略显著提升了性能。

Conclusion: 自适应个性化策略能有效平衡用户偏好与性能，对xAI在人机交互中的应用具有重要启示。

Abstract: As robots and digital assistants are deployed in the real world, these agents
must be able to communicate their decision-making criteria to build trust,
improve human-robot teaming, and enable collaboration. While the field of
explainable artificial intelligence (xAI) has made great strides to enable such
communication, these advances often assume that one xAI approach is ideally
suited to each problem (e.g., decision trees to explain how to triage patients
in an emergency or feature-importance maps to explain radiology reports). This
fails to recognize that users have diverse experiences or preferences for
interaction modalities. In this work, we present two user-studies set in a
simulated autonomous vehicle (AV) domain. We investigate (1) population-level
preferences for xAI and (2) personalization strategies for providing robot
explanations. We find significant differences between xAI modes (language
explanations, feature-importance maps, and decision trees) in both preference
(p < 0.01) and performance (p < 0.05). We also observe that a participant's
preferences do not always align with their performance, motivating our
development of an adaptive personalization strategy to balance the two. We show
that this strategy yields significant performance gains (p < 0.05), and we
conclude with a discussion of our findings and implications for xAI in
human-robot interactions.

</details>


### [247] [The Effect of Explainable AI-based Decision Support on Human Task Performance: A Meta-Analysis](https://arxiv.org/abs/2504.13858)
*Felix Haag*

Main category: cs.HC

TL;DR: 论文通过元分析探讨了可解释AI（XAI）对分类任务中人类表现的影响，发现XAI能提升任务表现，但解释本身并非决定性因素，研究偏差和解释类型的影响较小。


<details>
  <summary>Details</summary>
Motivation: 信息系统中解释的透明性需求推动了可解释AI（XAI）的发展，但现有研究对XAI是否提升用户任务表现存在不一致结论，因此需要系统性分析。

Method: 采用元分析方法，综合评估XAI对人类在分类任务中表现的影响，并分析研究偏差和解释类型的调节作用。

Result: XAI能提升任务表现，但解释本身并非主要驱动因素；研究偏差对XAI效果有显著调节作用，而解释类型影响较小。

Conclusion: 研究结果增进了人机交互领域对人类与XAI协作的理解，强调了研究设计的重要性。

Abstract: The desirable properties of explanations in information systems have fueled
the demands for transparency in artificial intelligence (AI) outputs. To
address these demands, the field of explainable AI (XAI) has put forth methods
that can support human decision-making by explaining AI outputs. However,
current empirical works present inconsistent findings on whether such
explanations help to improve users' task performance in decision support
systems (DSS). In this paper, we conduct a meta-analysis to explore how XAI
affects human performance in classification tasks. Our results show an
improvement in task performance through XAI-based decision support, though
explanations themselves are not the decisive driver for this improvement. The
analysis reveals that the studies' risk of bias moderates the effect of
explanations in AI, while the explanation type appears to play only a
negligible role. Our findings contribute to the human computer interaction
field by enhancing the understanding of human-XAI collaboration in DSS.

</details>


### [248] [DoYouTrustAI: A Tool to Teach Students About AI Misinformation and Prompt Engineering](https://arxiv.org/abs/2504.13859)
*Phillip Driscoll,Priyanka Kumar*

Main category: cs.HC

TL;DR: 开发了一款名为DoYouTrustAI的网页工具，帮助K-12学生识别大型语言模型（LLM）生成的历史人物信息中的误导性内容，以提升批判性思维。


<details>
  <summary>Details</summary>
Motivation: 由于LLM可能生成误导性信息，需要教育学生识别和验证AI生成内容的真实性。

Method: 通过提示工程生成历史人物的准确或误导性摘要，学生需判断其真实性。工具提供交互式课程，展示不同提示对AI响应的影响。

Result: 工具有效帮助学生理解误导性信息的危险性和提示工程的作用。

Conclusion: DoYouTrustAI工具强调了验证AI响应的重要性，并成功将提示工程概念简化以适合学生理解。

Abstract: AI, especially Large Language Models (LLMs) like ChatGPT, have rapidly
developed and gained widespread adoption in the past five years, shifting user
preference from traditional search engines. However, the generative nature of
LLMs raises concerns about presenting misinformation as fact. To address this,
we developed a web-based application that helps K-12 students enhance critical
thinking by identifying misleading information in LLM responses about major
historical figures. In this paper, we describe the implementation and design
details of the DoYouTrustAI tool, which can be used to provide an interactive
lesson which teaches students about the dangers of misinformation and how
believable generative AI can make it seem. The DoYouTrustAI tool utilizes
prompt engineering to present the user with AI generated summaries about the
life of a historical figure. These summaries can be either accurate accounts of
that persons life, or an intentionally misleading alteration of their history.
The user is tasked with determining the validity of the statement without
external resources. Our research questions for this work were:(RQ1) How can we
design a tool that teaches students about the dangers of misleading information
and of how misinformation can present itself in LLM responses? (RQ2) Can we
present prompt engineering as a topic that is easily understandable for
students? Our findings highlight the need to correct misleading information
before users retain it. Our tool lets users select familiar individuals for
testing to reduce random guessing and presents misinformation alongside known
facts to maintain believability. It also provides pre-configured prompt
instructions to show how different prompts affect AI responses. Together, these
features create a controlled environment where users learn the importance of
verifying AI responses and understanding prompt engineering.

</details>


### [249] [A Survey on (M)LLM-Based GUI Agents](https://arxiv.org/abs/2504.13865)
*Fei Tang,Haolei Xu,Hang Zhang,Siqi Chen,Xingyu Wu,Yongliang Shen,Wenqi Zhang,Guiyang Hou,Zeqi Tan,Yuchen Yan,Kaitao Song,Jian Shao,Weiming Lu,Jun Xiao,Yueting Zhuang*

Main category: cs.HC

TL;DR: 本文综述了基于LLM的GUI代理的快速发展，分析了其架构、技术组件和评估方法，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 探索GUI代理从规则脚本到AI驱动系统的演进，以及其在人机交互中的变革潜力。

Method: 系统分析GUI代理的四大组件：感知系统、探索机制、规划框架和交互系统。

Result: 揭示了LLM和多模态学习如何革新GUI自动化，同时指出了当前评估框架的局限性。

Conclusion: 总结了GUI代理的技术挑战和未来发展方向，为研究者和从业者提供了全面指导。

Abstract: Graphical User Interface (GUI) Agents have emerged as a transformative
paradigm in human-computer interaction, evolving from rule-based automation
scripts to sophisticated AI-driven systems capable of understanding and
executing complex interface operations. This survey provides a comprehensive
examination of the rapidly advancing field of LLM-based GUI Agents,
systematically analyzing their architectural foundations, technical components,
and evaluation methodologies. We identify and analyze four fundamental
components that constitute modern GUI Agents: (1) perception systems that
integrate text-based parsing with multimodal understanding for comprehensive
interface comprehension; (2) exploration mechanisms that construct and maintain
knowledge bases through internal modeling, historical experience, and external
information retrieval; (3) planning frameworks that leverage advanced reasoning
methodologies for task decomposition and execution; and (4) interaction systems
that manage action generation with robust safety controls. Through rigorous
analysis of these components, we reveal how recent advances in large language
models and multimodal learning have revolutionized GUI automation across
desktop, mobile, and web platforms. We critically examine current evaluation
frameworks, highlighting methodological limitations in existing benchmarks
while proposing directions for standardization. This survey also identifies key
technical challenges, including accurate element localization, effective
knowledge retrieval, long-horizon planning, and safety-aware execution control,
while outlining promising research directions for enhancing GUI Agents'
capabilities. Our systematic review provides researchers and practitioners with
a thorough understanding of the field's current state and offers insights into
future developments in intelligent interface automation.

</details>


### [250] [Skeleton-Based Transformer for Classification of Errors and Better Feedback in Low Back Pain Physical Rehabilitation Exercises](https://arxiv.org/abs/2504.13866)
*Aleksa Marusic,Sao Mai Nguyen,Adriana Tapus*

Main category: cs.HC

TL;DR: 提出了一种基于Transformer的算法，用于康复训练中的错误分类，并通过关节重要性计算提供更详细的反馈。


<details>
  <summary>Details</summary>
Motivation: 患者在没有直接监督的情况下，康复训练的参与度会降低，现有系统仅提供二元分类或连续评分，不足以帮助患者改进。

Method: 采用基于骨架的运动评估，利用Transformer模型（受HyperFormer启发）进行错误分类，并在KERAAL数据集上评估。

Result: 模型在KERAAL数据集上显著优于现有方法，并通过关节重要性计算提供更详细反馈。

Conclusion: 该研究为康复训练提供了更详细的错误分类和反馈方法，有助于患者改进训练效果。

Abstract: Physical rehabilitation exercises suggested by healthcare professionals can
help recovery from various musculoskeletal disorders and prevent re-injury.
However, patients' engagement tends to decrease over time without direct
supervision, which is why there is a need for an automated monitoring system.
In recent years, there has been great progress in quality assessment of
physical rehabilitation exercises. Most of them only provide a binary
classification if the performance is correct or incorrect, and a few provide a
continuous score. This information is not sufficient for patients to improve
their performance. In this work, we propose an algorithm for error
classification of rehabilitation exercises, thus making the first step toward
more detailed feedback to patients. We focus on skeleton-based exercise
assessment, which utilizes human pose estimation to evaluate motion. Inspired
by recent algorithms for quality assessment during rehabilitation exercises, we
propose a Transformer-based model for the described classification. Our model
is inspired by the HyperFormer method for human action recognition, and adapted
to our problem and dataset. The evaluation is done on the KERAAL dataset, as it
is the only medical dataset with clear error labels for the exercises, and our
model significantly surpasses state-of-the-art methods. Furthermore, we bridge
the gap towards better feedback to the patients by presenting a way to
calculate the importance of joints for each exercise.

</details>


### [251] [Using Generative AI Personas Increases Collective Diversity in Human Ideation](https://arxiv.org/abs/2504.13868)
*Yun Wan,Yoram M Kalman*

Main category: cs.HC

TL;DR: 研究挑战了生成式AI（GenAI）在创意产出中贡献与多样性降低之间的权衡，通过引入多样化AI角色（personas）生成故事情节，发现可以保持甚至增强人类创意产出的多样性。


<details>
  <summary>Details</summary>
Motivation: 探讨如何通过多样化AI输入（如不同文化背景、思维风格等）来避免GenAI辅助创作时多样性降低的问题。

Method: 修改了Doshi和Hauser（2024）的研究设计，使用10种独特的GenAI角色生成300个故事情节，人类参与者基于这些情节创作故事。

Result: 不同AI角色生成的情节多样性显著（平均相似度0.20），人类创作的集体故事多样性未降低，且情感和描述语言多样性更高。

Conclusion: 通过在AI输入阶段引入多样性角色，可以保持或增强GenAI辅助下人类创意产出的多样性。

Abstract: This study challenges the widely-reported tradeoff between generative AI's
(GenAI) contribution to creative outcomes and decreased diversity of these
outcomes. We modified the design of such a study, by Doshi and Hauser (2024),
in which participants wrote short stories either aided or unaided by GenAI plot
ideas[1]. In the modified study, plot ideas were generated through ten unique
GenAI "personas" with diverse traits (e.g. cultural backgrounds, thinking
styles, genre preferences), creating a pool of 300 story plots. While plot
ideas from any individual persona showed high similarity (average cosine
similarity of 0.92), ideas across different personas exhibited substantial
variation (average similarity of 0.20). When human participants wrote stories
based on these diverse plot ideas, their collective outputs maintained the same
level of diversity as stories written without GenAI assistance, effectively
eliminating the diversity reduction observed in [1]. Traditional text analytics
further revealed that GenAI-assisted stories featured greater diversity in
descriptive and emotional language compared to purely human-generated stories
without GenAI assistance. Our findings demonstrate that introducing diversity
at the AI input stage through distinct personas can preserve and potentially
enhance the collective diversity of human creative outputs when collaborating
with GenAI.

</details>


### [252] [Interview AI-ssistant: Designing for Real-Time Human-AI Collaboration in Interview Preparation and Execution](https://arxiv.org/abs/2504.13847)
*Zhe Liu*

Main category: cs.HC

TL;DR: 论文提出了一种名为Interview AI-ssistant的系统，旨在通过AI实时辅助访谈的准备工作与执行阶段，提升人类与AI在定性研究中的协作效果。


<details>
  <summary>Details</summary>
Motivation: 访谈在获取深度、情境化见解方面具有重要价值，但访谈者在实时信息处理、问题调整和关系维护等方面面临认知挑战。

Method: 通过四项相互关联的研究，包括访谈者需求的形成性研究、AI辅助访谈准备的开发研究、实时AI辅助的实验评估以及实际研究环境中的系统部署。

Result: 研究不仅为智能访谈支持系统的实际应用提供了指导，还为IUI社区提供了关于复杂社交任务中人类与AI协作界面的设计指南。

Conclusion: 该研究推动了人类与AI协作界面的理解，并为AI增强的定性研究工具设计了指导原则。

Abstract: Recent advances in large language models (LLMs) offer unprecedented
opportunities to enhance human-AI collaboration in qualitative research
methods, including interviews. While interviews are highly valued for gathering
deep, contextualized insights, interviewers often face significant cognitive
challenges, such as real-time information processing, question adaptation, and
rapport maintenance. My doctoral research introduces Interview AI-ssistant, a
system designed for real-time interviewer-AI collaboration during both the
preparation and execution phases. Through four interconnected studies, this
research investigates the design of effective human-AI collaboration in
interviewing contexts, beginning with a formative study of interviewers' needs,
followed by a prototype development study focused on AI-assisted interview
preparation, an experimental evaluation of real-time AI assistance during
interviews, and a field study deploying the system in a real-world research
setting. Beyond informing practical implementations of intelligent interview
support systems, this work contributes to the Intelligent User Interfaces (IUI)
community by advancing the understanding of human-AI collaborative interfaces
in complex social tasks and establishing design guidelines for AI-enhanced
qualitative research tools.

</details>


### [253] [Human aversion? Do AI Agents Judge Identity More Harshly Than Performance](https://arxiv.org/abs/2504.13871)
*Yuanjun Feng,Vivek Chodhary,Yash Raj Shrestha*

Main category: cs.HC

TL;DR: 研究探讨了AI在混合决策系统中对人类判断的评估，发现AI倾向于低估人类输入，尤其是在身份披露和顺序安排下。


<details>
  <summary>Details</summary>
Motivation: 填补管理研究中AI评估人类判断的空白，解决因隐私问题无法直接部署LLMs的挑战。

Method: 通过控制预测任务，分析LLM-based AI对人类与算法预测的权重分配。

Result: AI系统系统性低估人类建议，且身份披露和顺序安排加剧此现象。

Conclusion: 揭示了AI与人类协作中的不平等，提出了间接部署LLMs的框架和实践建议。

Abstract: This study examines the understudied role of algorithmic evaluation of human
judgment in hybrid decision-making systems, a critical gap in management
research. While extant literature focuses on human reluctance to follow
algorithmic advice, we reverse the perspective by investigating how AI agents
based on large language models (LLMs) assess and integrate human input. Our
work addresses a pressing managerial constraint: firms barred from deploying
LLMs directly due to privacy concerns can still leverage them as mediating
tools (for instance, anonymized outputs or decision pipelines) to guide
high-stakes choices like pricing or discounts without exposing proprietary
data. Through a controlled prediction task, we analyze how an LLM-based AI
agent weights human versus algorithmic predictions. We find that the AI system
systematically discounts human advice, penalizing human errors more severely
than algorithmic errors--a bias exacerbated when the agent's identity (human vs
AI) is disclosed and the human is positioned second. These results reveal a
disconnect between AI-generated trust metrics and the actual influence of human
judgment, challenging assumptions about equitable human-AI collaboration. Our
findings offer three key contributions. First, we identify a reverse algorithm
aversion phenomenon, where AI agents undervalue human input despite comparable
error rates. Second, we demonstrate how disclosure and positional bias interact
to amplify this effect, with implications for system design. Third, we provide
a framework for indirect LLM deployment that balances predictive power with
data privacy. For practitioners, this research emphasize the need to audit AI
weighting mechanisms, calibrate trust dynamics, and strategically design
decision sequences in human-AI systems.

</details>


### [254] [3MDBench: Medical Multimodal Multi-agent Dialogue Benchmark](https://arxiv.org/abs/2504.13861)
*Ivan Sviridov,Amina Miftakhova,Artemiy Tereshchenko,Galina Zubkova,Pavel Blinov,Andrey Savchenko*

Main category: cs.HC

TL;DR: 3MDBench是一个开源评估框架，用于测试大型视觉语言模型（LVLM）在医疗咨询中的表现，通过模拟真实患者行为和多种诊断策略，提升诊断准确性和对话质量。


<details>
  <summary>Details</summary>
Motivation: 探索LVLM在远程医疗中的应用潜力，尤其是其与多样化患者行为互动的能力，现有基准测试未能充分覆盖这一领域。

Method: 开发3MDBench框架，包含四种性格驱动的患者代理和评估代理，整合文本和图像数据，评估不同诊断策略下的LVLM表现。

Result: 对话和多模态输入显著提升诊断效果，F1分数从50.4提高到54.2；结合CNN模型后，F1分数进一步提升至70.3。

Conclusion: 3MDBench为AI驱动的医疗助手提供了可扩展的评估工具，揭示了患者性格、对话策略和多模态推理对诊断质量的影响，推动了更可靠和情境感知的医疗解决方案。

Abstract: Large Vision-Language Models (LVLMs) are increasingly being explored for
applications in telemedicine, yet their ability to engage with diverse patient
behaviors remains underexplored. We introduce 3MDBench (Medical Multimodal
Multi-agent Dialogue Benchmark), an open-source evaluation framework designed
to assess LLM-driven medical consultations. Unlike existing benchmarks,
3MDBench simulates real-world patient variability by incorporating four
temperament-driven Patient Agents and an Assessor Agent that evaluates
diagnostic accuracy and dialogue quality. The benchmark integrates textual and
image-based patient data across 34 common diagnoses, mirroring real-world
telemedicine interactions. Under different diagnostic strategies, we evaluate
state-of-the-art LVLMs. Our findings demonstrate that incorporating dialogue
improves the F1 score from 50.4 to 54.2 compared to non-dialogue settings,
underscoring the value of context-driven, information-seeking questioning.
Additionally, we demonstrate that multimodal inputs enhance diagnostic
efficiency. Image-supported models outperform text-only counterparts by raising
the diagnostic F1 score from 52.8 to 54.2 in a similar dialogue setting.
Finally, we suggest an approach that improves the diagnostic F1-score to 70.3
by training the CNN model on the diagnosis prediction task and incorporating
its top-3 predictions into the LVLM context. 3MDBench provides a reproducible
and extendable evaluation framework for AI-driven medical assistants. It offers
insights into how patient temperament, dialogue strategies, and multimodal
reasoning influence diagnosis quality. By addressing real-world complexities in
telemedicine, our benchmark paves the way for more empathetic, reliable, and
context-aware AI-driven healthcare solutions. The source code of our benchmark
is publicly available: https://github.com/univanxx/3mdbench

</details>


### [255] [New care pathways for supporting transitional care from hospitals to home using AI and personalized digital assistance](https://arxiv.org/abs/2504.13877)
*Ionut Anghel,Tudor Cioara,Roberta Bevilacqua,Federico Barbarossa,Terje Grimstad,Riitta Hellman,Arnor Solberg,Lars Thomas Boye,Ovidiu Anchidin,Ancuta Nemes,Camilla Gabrielsen*

Main category: cs.HC

TL;DR: 本文探讨了过渡性护理在欧洲医疗系统中的重要性，提出通过整合物联网、人工智能和数字辅助技术来优化患者从医院到家庭的护理过渡，以减少再住院风险。


<details>
  <summary>Details</summary>
Motivation: 随着人口老龄化，医疗需求增加，过渡性护理成为解决医院资源压力的关键。整合创新技术可提升护理协调性，改善患者体验。

Method: 通过分析当前过渡性护理的不足，提出技术映射方案，并结合物联网、人工智能和数字辅助技术优化护理路径。

Result: 技术整合有望提升患者护理效果、安全性和生活质量，减少再住院率。

Conclusion: 本文为技术整合提供了试验设计和评估方法，支持其对医疗系统的积极影响。

Abstract: Transitional care may play a vital role for the sustainability of Europe
future healthcare system, offering solutions for relocating patient care from
hospital to home therefore addressing the growing demand for medical care as
the population is ageing. However, to be effective, it is essential to
integrate innovative Information and Communications Technology technologies to
ensure that patients with comorbidities experience a smooth and coordinated
transition from hospitals or care centers to home, thereby reducing the risk of
rehospitalization. In this paper, we present an overview of the integration of
Internet of Things, artificial intelligence, and digital assistance
technologies with traditional care pathways to address the challenges and needs
of healthcare systems in Europe. We identify the current gaps in transitional
care and define the technology mapping to enhance the care pathways, aiming to
improve patient outcomes, safety, and quality of life avoiding hospital
readmissions. Finally, we define the trial setup and evaluation methodology
needed to provide clinical evidence that supports the positive impact of
technology integration on patient care and discuss the potential effects on the
healthcare system.

</details>


### [256] [Towards a Multimodal Document-grounded Conversational AI System for Education](https://arxiv.org/abs/2504.13884)
*Karan Taneja,Anjali Singh,Ashok K. Goel*

Main category: cs.HC

TL;DR: MuDoC是一个基于GPT-4o的多模态对话AI系统，结合文本和图像提升学习效果，验证内容可增强信任，但对学习表现无显著影响。


<details>
  <summary>Details</summary>
Motivation: 探索多模态对话AI在教育中的应用，解决现有文本交互系统的局限性，并增强内容的可信度。

Method: 基于GPT-4o开发MuDoC系统，结合文档中的文本和图像生成多模态响应，并与纯文本系统对比。

Result: 多模态和内容验证提升了学习者的参与度和信任，但对问题解决表现无显著影响。

Conclusion: 多模态对话AI在教育中有潜力，需进一步研究其对学习效果的影响。

Abstract: Multimedia learning using text and images has been shown to improve learning
outcomes compared to text-only instruction. But conversational AI systems in
education predominantly rely on text-based interactions while multimodal
conversations for multimedia learning remain unexplored. Moreover, deploying
conversational AI in learning contexts requires grounding in reliable sources
and verifiability to create trust. We present MuDoC, a Multimodal
Document-grounded Conversational AI system based on GPT-4o, that leverages both
text and visuals from documents to generate responses interleaved with text and
images. Its interface allows verification of AI generated content through
seamless navigation to the source. We compare MuDoC to a text-only system to
explore differences in learner engagement, trust in AI system, and their
performance on problem-solving tasks. Our findings indicate that both visuals
and verifiability of content enhance learner engagement and foster trust;
however, no significant impact in performance was observed. We draw upon
theories from cognitive and learning sciences to interpret the findings and
derive implications, and outline future directions for the development of
multimodal conversational AI systems in education.

</details>


### [257] [Toward Automated Qualitative Analysis: Leveraging Large Language Models for Tutoring Dialogue Evaluation](https://arxiv.org/abs/2504.13882)
*Megan Gu,Chloe Qianhui Zhao,Claire Liu,Nikhil Patel,Jahnvi Shah,Jionghao Lin,Kenneth R. Koedinger*

Main category: cs.HC

TL;DR: 该研究利用GPT-3.5自动评估五种辅导策略的有效性，结果显示模型在排除错误分类上表现良好，但在准确识别策略上仍有不足。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型（LLMs）在分析辅导策略中的应用潜力。

Method: 使用GPT-3.5和少量示例提示对公开数据集中的辅导对话进行分类。

Result: 模型在排除错误分类上表现较好（TNR 0.655-0.738），但准确识别策略的召回率较低（0.327-0.432）。

Conclusion: LLMs在辅导策略分析中具有潜力，未来可通过更先进的模型提升性能。

Abstract: Our study introduces an automated system leveraging large language models
(LLMs) to assess the effectiveness of five key tutoring strategies: 1. giving
effective praise, 2. reacting to errors, 3. determining what students know, 4.
helping students manage inequity, and 5. responding to negative self-talk.
Using a public dataset from the Teacher-Student Chatroom Corpus, our system
classifies each tutoring strategy as either being employed as desired or
undesired. Our study utilizes GPT-3.5 with few-shot prompting to assess the use
of these strategies and analyze tutoring dialogues. The results show that for
the five tutoring strategies, True Negative Rates (TNR) range from 0.655 to
0.738, and Recall ranges from 0.327 to 0.432, indicating that the model is
effective at excluding incorrect classifications but struggles to consistently
identify the correct strategy. The strategy \textit{helping students manage
inequity} showed the highest performance with a TNR of 0.738 and Recall of
0.432. The study highlights the potential of LLMs in tutoring strategy analysis
and outlines directions for future improvements, including incorporating more
advanced models for more nuanced feedback.

</details>


### [258] [Kanji Workbook: A Writing-Based Intelligent Tutoring System for Learning Proper Japanese Kanji Writing Technique with Instructor-Emulated Assessment](https://arxiv.org/abs/2504.13888)
*Paul Taele,Jung In Koh,Tracy Hammond*

Main category: cs.HC

TL;DR: Kanji Workbook是一种智能辅导系统，通过模拟教师反馈帮助学生学习日语汉字书写，提升课程成绩。


<details>
  <summary>Details</summary>
Motivation: 针对英语母语学生在学习日语汉字书写时的困难，以及现有教育应用反馈不足的问题。

Method: 开发了Kanji Workbook系统，结合教师访谈和课堂观察，提供智能评分和视觉动画反馈。

Result: 使用该系统的学生在课程成绩上优于同龄人，并对系统功能反应积极。

Conclusion: Kanji Workbook通过智能反馈有效辅助学生汉字书写学习。

Abstract: Kanji script writing is a skill that is often introduced to novice Japanese
foreign language students for achieving Japanese writing mastery, but often
poses difficulties to students with primarily English fluency due to their its
vast differences with written English. Instructors often introduce various
pedagogical methods -- such as visual structure and written techniques -- to
assist students in kanji study, but may lack availability providing direct
feedback on students' writing outside of class. Current educational
applications are also limited due to lacking richer instructor-emulated
feedback. We introduce Kanji Workbook, a writing-based intelligent tutoring
system for students to receive intelligent assessment that emulates human
instructor feedback. Our interface not only leverages students' computing
devices for allowing them to learn, practice, and review the writing of
prompted characters from their course's kanji script lessons, but also provides
a diverse set of writing assessment metrics -- derived from instructor
interviews and classroom observation insights -- through intelligent scoring
and visual animations. We deployed our interface onto novice- and
intermediate-level university courses over an entire academic year, and
observed that interface users on average achieved higher course grades than
their peers and also reacted positively to our interface's various features.

</details>


### [259] [AI as a deliberative partner fosters intercultural empathy for Americans but fails for Latin American participants](https://arxiv.org/abs/2504.13887)
*Isabel Villanueva,Tara Bobinac,Binwei Yao,Junjie Hu,Kaiping Chen*

Main category: cs.HC

TL;DR: AI聊天机器人在跨文化对话中促进共情的能力有限，尤其是文化对齐不足时。


<details>
  <summary>Details</summary>
Motivation: 研究AI聊天机器人在跨文化对话中是否能促进共情，填补实证研究的空白。

Method: 采用随机对话实验，比较审议与非审议、文化对齐与非对齐的AI互动。

Result: 审议对话提升了美国参与者的跨文化共情，但对拉丁美洲参与者无效，因AI文化表达不准确。

Conclusion: AI需改进文化真实性，以支持跨文化审议对话，解决民主话语中的代表不对称问题。

Abstract: Despite the growing integration of AI chatbots as conversational agents in
public discourse, empirical evidence regarding their capacity to foster
intercultural empathy remains limited. Using a randomized dialogue experiment,
we examined how different types of AI chatbot interaction, i.e., deliberative
versus non-deliberative and culturally aligned versus non-aligned, affect
intercultural empathy across cultural groups. Results show that deliberative
conversations increased intercultural empathy among American participants but
not Latin American participants, who perceived AI responses as culturally
inaccurate and failing to represent their cultural contexts and perspectives
authentically. Real-time interaction analyses reveal that these differences
stem from cultural knowledge gaps inherent in Large Language Models. Despite
explicit prompting and instruction to represent cultural perspectives in
participants' native languages, AI systems still exhibit significant
disparities in cultural representation. This highlights the importance of
designing AI systems capable of culturally authentic engagement in deliberative
conversations. Our study contributes to deliberation theory and AI alignment
research by underscoring AI's role in intercultural dialogue and the persistent
challenge of representational asymmetry in democratic discourse.

</details>


### [260] [Maestoso: An Intelligent Educational Sketching Tool for Learning Music Theory](https://arxiv.org/abs/2504.13889)
*Paul Taele,Laura Barreto,Tracy Hammond*

Main category: cs.HC

TL;DR: Maestoso是一个教育工具，帮助初学者通过草图练习学习音乐理论，提供自动识别和反馈。


<details>
  <summary>Details</summary>
Motivation: 现有工具在反馈、书写模式或音乐理论熟悉度方面存在不足，需要更有效的学习工具。

Method: Maestoso通过自动识别学生绘制的音乐结构草图，利用现有技术生成模拟教师反馈。

Result: Maestoso能较好识别音乐结构元素，初学者可在一节课中掌握基础音乐理论。

Conclusion: Maestoso为音乐理论学习提供了一种有效的辅助工具。

Abstract: Learning music theory not only has practical benefits for musicians to write,
perform, understand, and express music better, but also for both non-musicians
to improve critical thinking, math analytical skills, and music appreciation.
However, current external tools applicable for learning music theory through
writing when human instruction is unavailable are either limited in feedback,
lacking a written modality, or assuming already strong familiarity of music
theory concepts. In this paper, we describe Maestoso, an educational tool for
novice learners to learn music theory through sketching practice of quizzed
music structures. Maestoso first automatically recognizes students' sketched
input of quizzed concepts, then relies on existing sketch and gesture
recognition techniques to automatically recognize the input, and finally
generates instructor-emulated feedback. From our evaluations, we demonstrate
that Maestoso performs reasonably well on recognizing music structure elements
and that novice students can comfortably grasp introductory music theory in a
single session.

</details>


### [261] [Mozualization: Crafting Music and Visual Representation with Multimodal AI](https://arxiv.org/abs/2504.13891)
*Wanfang Xu,Lixiang Zhao,Haiwen Song,Xinheng Song,Zhaolin Lu,Yu Liu,Min Chen,Eng Gee Lim,Lingyun Yu*

Main category: cs.HC

TL;DR: Mozualization是一个音乐生成和编辑工具，通过整合关键词、图像和声音片段等多样化输入，生成多风格嵌入式音乐。


<details>
  <summary>Details</summary>
Motivation: 受人们通过诗歌、绘画或音乐表达情感的启发，开发了能将情感表达转化为连贯歌曲的工具。

Method: 开发了Mozualization工具，并通过用户研究（9位音乐爱好者）评估用户体验和改进方向。

Result: 用户研究评估了工具的用户体验、参与度及生成音乐的影响。

Conclusion: Mozualization成功将多样化输入转化为个性化音乐，用户研究为其改进提供了宝贵见解。

Abstract: In this work, we introduce Mozualization, a music generation and editing tool
that creates multi-style embedded music by integrating diverse inputs, such as
keywords, images, and sound clips (e.g., segments from various pieces of music
or even a playful cat's meow). Our work is inspired by the ways people express
their emotions -- writing mood-descriptive poems or articles, creating drawings
with warm or cool tones, or listening to sad or uplifting music. Building on
this concept, we developed a tool that transforms these emotional expressions
into a cohesive and expressive song, allowing users to seamlessly incorporate
their unique preferences and inspirations. To evaluate the tool and, more
importantly, gather insights for its improvement, we conducted a user study
involving nine music enthusiasts. The study assessed user experience,
engagement, and the impact of interacting with and listening to the generated
music.

</details>


### [262] [Measuring Mental Health Variables in Computational Research: Toward Validated, Dimensional, and Transdiagnostic Approaches](https://arxiv.org/abs/2504.13890)
*Chen Shani,Elizabeth C. Stade*

Main category: cs.HC

TL;DR: 论文指出计算心理健康研究中存在测量工具不当的问题，提出了使用已验证、维度和跨诊断测量工具的建议。


<details>
  <summary>Details</summary>
Motivation: 当前计算心理健康研究常依赖不适当的心理病理学测量工具，影响研究效度。

Method: 识别三个关键问题：依赖未验证的测量工具、将心理健康构念视为分类而非维度、关注特定障碍而非跨诊断构念。

Result: 提出使用已验证、维度和跨诊断测量工具的优势，并为实践者提供建议。

Conclusion: 使用反映心理病理学本质和结构的有效测量工具对计算心理健康研究至关重要。

Abstract: Computational mental health research develops models to predict and
understand psychological phenomena, but often relies on inappropriate measures
of psychopathology constructs, undermining validity. We identify three key
issues: (1) reliance on unvalidated measures (e.g., self-declared diagnosis)
over validated ones (e.g., diagnosis by clinician); (2) treating mental health
constructs as categorical rather than dimensional; and (3) focusing on
disorder-specific constructs instead of transdiagnostic ones. We outline the
benefits of using validated, dimensional, and transdiagnostic measures and
offer practical recommendations for practitioners. Using valid measures that
reflect the nature and structure of psychopathology is essential for
computational mental health research.

</details>


### [263] [The Human Robot Social Interaction (HSRI) Dataset: Benchmarking Foundational Models' Social Reasoning](https://arxiv.org/abs/2504.13898)
*Dong Won Lee,Yubin Kim,Denison Guvenoz,Sooyeon Jeong,Parker Malachowsky,Louis-Philippe Morency,Cynthia Breazeal,Hae Won Park*

Main category: cs.HC

TL;DR: 该论文旨在提升AI代理在现实社交互动中的社会推理能力，通过引入大规模真实世界人机社交互动数据集（HSRI）和八项新基准任务，评估语言模型和基础模型的社会交互理解能力。


<details>
  <summary>Details</summary>
Motivation: 推动具身AI在真实社交互动中的社会推理能力，利用语言模型和基础模型作为自动评估工具，以改进AI代理的策略。

Method: 构建包含400个真实人机互动视频和10K注释的HSRI数据集，设计八项基准任务评估模型的社会错误检测、解释因素识别、交互流理解和纠正建议能力。

Result: 实验显示当前模型在这些任务上表现不佳，表明数据集和基准任务对社会智能AI的发展具有推动作用。

Conclusion: HSRI数据集和基准任务为提升AI的社会智能提供了重要工具，揭示了当前模型的局限性，为未来研究指明了方向。

Abstract: Our work aims to advance the social reasoning of embodied artificial
intelligence (AI) agents in real-world social interactions. Recently, language
models (LMs) and foundational models (FMs) are being utilized as automatic
evaluators of human-AI interactions with the goal of eventually being used to
improve the policy of the AI agent. To enable further research in this
direction, we introduce a large-scale real-world Human Robot Social Interaction
(HSRI) Dataset to benchmark the capabilities of LMs and FMs to identify and
reason about social interactions, specifically with regard to robot social
errors and competencies . Our dataset consists of 400 real-world human social
robot interaction videos and over 10K annotations, detailing the robot's social
errors, competencies, rationale, and corrective actions, capturing unique
aspects of human-AI interaction only present in real-world interactions. To
further assess AI models' ability to reason about social interactions, we
propose eight new benchmark tasks for evaluating centered around whether AI
models can (1) evaluate social interactions via detecting social errors and
competencies, (2) identify the explanatory factors associated to errors and
competencies, (3) understand the flow of real-world social interactions, and
(4) provide reasons and corrective actions for social errors. Human studies and
experiments with modern LMs and FMs reveal that current models struggle with
these tasks, demonstrating that our dataset and benchmark provides a step
forward towards socially intelligent AI.

</details>


### [264] [TALLMesh: a simple application for performing Thematic Analysis with Large Language Models](https://arxiv.org/abs/2504.13892)
*Stefano De Paoli,Alex Fawzi*

Main category: cs.HC

TL;DR: 本文介绍了一种基于大型语言模型（LLMs）的图形用户界面（GUI）应用，用于辅助研究人员进行主题分析（TA），特别适合缺乏编程技能的研究者。


<details>
  <summary>Details</summary>
Motivation: 主题分析（TA）是定性研究中常用的方法，但传统方法需要编程技能。本文旨在通过LLMs和GUI简化TA过程，使其更易于使用。

Method: 开发了一个基于streamlit框架的GUI应用，用户可上传文本数据，生成初始代码和主题，并通过LLMs的API进行分析。支持人机交互迭代优化。

Result: 应用成功实现了无需编程的主题分析，保留了方法学严谨性，并提高了研究效率。

Conclusion: 该应用为缺乏编程技能的研究者提供了便捷的TA工具，未来可进一步扩展功能。

Abstract: Thematic analysis (TA) is a widely used qualitative research method for
identifying and interpreting patterns within textual data, such as qualitative
interviews. Recent research has shown that it is possible to satisfactorily
perform TA using Large Language Models (LLMs). This paper presents a novel
application using LLMs to assist researchers in conducting TA. The application
enables users to upload textual data, generate initial codes and themes. All of
this is possible through a simple Graphical User Interface, (GUI) based on the
streamlit framework, working with python scripts for the analysis, and using
Application Program Interfaces of LLMs. Having a GUI is particularly important
for researchers in fields where coding skills may not be prevalent, such as
social sciences or humanities. With the app, users can iteratively refine codes
and themes adopting a human-in-the-loop process, without the need to work with
programming and scripting. The paper describes the application key features,
highlighting its potential for qualitative research while preserving
methodological rigor. The paper discusses the design and interface of the app
and outlines future directions for this work.

</details>


### [265] [Predicting Satisfaction of Counterfactual Explanations from Human Ratings of Explanatory Qualities](https://arxiv.org/abs/2504.13899)
*Marharyta Domnich,Rasmus Moorits Veski,Julius Välja,Kadi Tulver,Raul Vicente*

Main category: cs.HC

TL;DR: 论文探讨了反事实解释的质量评估问题，发现可行性和信任是用户满意度的最强预测因素，同时揭示了其他解释标准的重要性。


<details>
  <summary>Details</summary>
Motivation: 反事实解释在可解释AI中广泛应用，但评估其质量仍是一个开放问题。传统量化指标无法全面反映人类偏好，而用户研究虽深入但不可扩展。

Method: 通过分析206名参与者对反事实解释的评分数据，建模整体满意度与七个解释标准（如可行性、信任等）的关系。

Result: 可行性和信任是用户满意度的最强预测因素，其他标准解释了58%的方差。复杂性独立，且用户背景显著影响评分模式。

Conclusion: 研究为反事实算法的设计提供了依据，强调根据用户专业知识和领域背景调整解释质量。

Abstract: Counterfactual explanations are a widely used approach in Explainable AI,
offering actionable insights into decision-making by illustrating how small
changes to input data can lead to different outcomes. Despite their importance,
evaluating the quality of counterfactual explanations remains an open problem.
Traditional quantitative metrics, such as sparsity or proximity, fail to fully
account for human preferences in explanations, while user studies are
insightful but not scalable. Moreover, relying only on a single overall
satisfaction rating does not lead to a nuanced understanding of why certain
explanations are effective or not. To address this, we analyze a dataset of
counterfactual explanations that were evaluated by 206 human participants, who
rated not only overall satisfaction but also seven explanatory criteria:
feasibility, coherence, complexity, understandability, completeness, fairness,
and trust. Modeling overall satisfaction as a function of these criteria, we
find that feasibility (the actionability of suggested changes) and trust (the
belief that the changes would lead to the desired outcome) consistently stand
out as the strongest predictors of user satisfaction, though completeness also
emerges as a meaningful contributor. Crucially, even excluding feasibility and
trust, other metrics explain 58% of the variance, highlighting the importance
of additional explanatory qualities. Complexity appears independent, suggesting
more detailed explanations do not necessarily reduce satisfaction. Strong
metric correlations imply a latent structure in how users judge quality, and
demographic background significantly shapes ranking patterns. These insights
inform the design of counterfactual algorithms that adapt explanatory qualities
to user expertise and domain context.

</details>


### [266] [Generative Framework for Personalized Persuasion: Inferring Causal, Counterfactual, and Latent Knowledge](https://arxiv.org/abs/2504.13904)
*Donghuo Zeng,Roberto Legaspi,Yuewen Sun,Xinshuai Dong,Kazushi Ikeda,Peter Spirtes,Kun Zhang*

Main category: cs.HC

TL;DR: 论文提出通过因果发现和反事实推理优化系统响应策略，显著提升对话系统的说服效果。


<details>
  <summary>Details</summary>
Motivation: 研究动机是基于因果和反事实知识构建自适应策略，以优化系统响应并提升用户-系统交互效果。

Method: 方法包括因果发现识别策略、反事实推理生成假设场景、建模潜在因素（如心理构造和噪声），并基于反事实数据优化响应策略。

Result: 实验结果表明，该方法在真实数据集上显著提升了说服性对话系统的累积奖励。

Conclusion: 结论是因果发现和反事实推理能有效指导个性化对话策略优化，提升系统性能。

Abstract: We hypothesize that optimal system responses emerge from adaptive strategies
grounded in causal and counterfactual knowledge. Counterfactual inference
allows us to create hypothetical scenarios to examine the effects of
alternative system responses. We enhance this process through causal discovery,
which identifies the strategies informed by the underlying causal structure
that govern system behaviors. Moreover, we consider the psychological
constructs and unobservable noises that might be influencing user-system
interactions as latent factors. We show that these factors can be effectively
estimated. We employ causal discovery to identify strategy-level causal
relationships among user and system utterances, guiding the generation of
personalized counterfactual dialogues. We model the user utterance strategies
as causal factors, enabling system strategies to be treated as counterfactual
actions. Furthermore, we optimize policies for selecting system responses based
on counterfactual data. Our results using a real-world dataset on social good
demonstrate significant improvements in persuasive system outcomes, with
increased cumulative rewards validating the efficacy of causal discovery in
guiding personalized counterfactual inference and optimizing dialogue policies
for a persuasive dialogue system.

</details>


### [267] [Supporting Students' Reading and Cognition with AI](https://arxiv.org/abs/2504.13900)
*Yue Fu,Alexis Hiniker*

Main category: cs.HC

TL;DR: 研究分析了学生在使用AI工具辅助阅读时的认知行为变化，发现初期高阶思维（如分析、评估）增加，但长期趋向被动阅读。


<details>
  <summary>Details</summary>
Motivation: 随着AI工具在教育中的广泛应用，需了解其对用户阅读过程和认知参与的影响。

Method: 收集并分析了124次学生使用AI工具辅助阅读的会话数据，按布鲁姆分类法对提示进行分类。

Result: 初期高阶思维提示增多，但长期用户趋向被动阅读。

Conclusion: 建议未来AI阅读支持系统设计结构化支架和主动提示，并加入自适应功能以平衡效率与认知参与。

Abstract: With the rapid adoption of AI tools in learning contexts, it is vital to
understand how these systems shape users' reading processes and cognitive
engagement. We collected and analyzed text from 124 sessions with AI tools, in
which students used these tools to support them as they read assigned readings
for an undergraduate course. We categorized participants' prompts to AI
according to Bloom's Taxonomy of educational objectives -- Remembering,
Understanding, Applying, Analyzing, Evaluating. Our results show that
``Analyzing'' and ``Evaluating'' are more prevalent in users' second and third
prompts within a single usage session, suggesting a shift toward higher-order
thinking. However, in reviewing users' engagement with AI tools over several
weeks, we found that users converge toward passive reading engagement over
time. Based on these results, we propose design implications for future AI
reading-support systems, including structured scaffolds for lower-level
cognitive tasks (e.g., recalling terms) and proactive prompts that encourage
higher-order thinking (e.g., analyzing, applying, evaluating). Additionally, we
advocate for adaptive, human-in-the-loop features that allow students and
instructors to tailor their reading experiences with AI, balancing efficiency
with enriched cognitive engagement. Our paper expands the dialogue on
integrating AI into academic reading, highlighting both its potential benefits
and challenges.

</details>


### [268] [AI-Assisted Conversational Interviewing: Effects on Data Quality and User Experience](https://arxiv.org/abs/2504.13908)
*Soubhik Barari,Jarret Angbazo,Natalie Wang,Leah M. Christian,Elizabeth Dean,Zoe Slowinski,Brandon Sepulvado*

Main category: cs.HC

TL;DR: 研究提出了一种AI辅助的对话式访谈框架，以弥补标准化调查和对话访谈之间的差距。通过实验评估，发现AI文本机器人在实时编码和提升回答质量方面表现良好，但略微影响受访者体验。


<details>
  <summary>Details</summary>
Motivation: 标准化调查牺牲深度，而对话访谈缺乏可扩展性和一致性。研究旨在通过AI方法结合两者的优势。

Method: 通过网页调查实验，随机分配1,800名参与者与文本机器人互动，动态探测回答并实时编码开放性问题。

Result: 文本机器人在实时编码中表现中等，开放性问题回答更详细，但受访者体验略有下降。

Conclusion: AI方法可有效增强网络调查中的开放式数据收集，尽管存在一些局限性。

Abstract: Standardized surveys scale efficiently but sacrifice depth, while
conversational interviews improve response quality at the cost of scalability
and consistency. This study bridges the gap between these methods by
introducing a framework for AI-assisted conversational interviewing. To
evaluate this framework, we conducted a web survey experiment where 1,800
participants were randomly assigned to text-based conversational AI agents, or
"textbots", to dynamically probe respondents for elaboration and interactively
code open-ended responses. We assessed textbot performance in terms of coding
accuracy, response quality, and respondent experience. Our findings reveal that
textbots perform moderately well in live coding even without survey-specific
fine-tuning, despite slightly inflated false positive errors due to respondent
acquiescence bias. Open-ended responses were more detailed and informative, but
this came at a slight cost to respondent experience. Our findings highlight the
feasibility of using AI methods to enhance open-ended data collection in web
surveys.

</details>


### [269] [Modeling the quantum-like dynamics of human reliability ratings in Human-AI interactions by interaction dependent Hamiltonians](https://arxiv.org/abs/2504.13918)
*Johan van der Meer,Pamela Hoyte,Luisa Roeder,Peter Bruza*

Main category: cs.HC

TL;DR: 量子随机游走模型用于建模人机交互中的信任动态，结合对信任波动的敏感性。


<details>
  <summary>Details</summary>
Motivation: 研究高不确定性环境下人机团队决策中的信任问题。

Method: 使用量子随机游走模型，结合经验参数选择哈密顿量。

Result: 发现该方法能有效建模信任动态。

Conclusion: 量子随机游走模型为建模人机信任提供有前景的方法。

Abstract: As our information environments become ever more powered by artificial
intelligence (AI), the phenomenon of trust in a human's interactions with this
intelligence is becoming increasingly pertinent. For example, in the not too
distant future, there will be teams of humans and intelligent robots involved
in dealing with the repercussions of high-risk disaster situations such as
hurricanes, earthquakes, or nuclear accidents. Even in such conditions of high
uncertainty, humans and intelligent machines will need to engage in shared
decision making, and trust is fundamental to the effectiveness of these
interactions. A key challenge in modeling the dynamics of this trust is to
provide a means to incorporate sensitivity to fluctuations in human trust
judgments. In this article, we explore the ability of Quantum Random Walk
models to model the dynamics of trust in human-AI interactions, and to
integrate a sensitivity to fluctuations in participant trust judgments based on
the nature of the interaction with the AI. We found that using empirical
parameters to inform the use of different Hamiltonians can provide a promising
means to model the evolution of trust in Human-AI interactions.

</details>


### [270] [A Multi-Layered Research Framework for Human-Centered AI: Defining the Path to Explainability and Trust](https://arxiv.org/abs/2504.13926)
*Chameera De Silva,Thilina Halloluwa,Dhaval Vyas*

Main category: cs.HC

TL;DR: 本文提出了一种三层框架，结合人本AI（HCAI）和可解释AI（XAI），以提升AI在高风险领域的透明度和信任。


<details>
  <summary>Details</summary>
Motivation: AI在高风险领域（如医疗、金融）的应用受限于透明度和信任问题，缺乏统一方法。

Method: 提出三层框架：基础AI模型、人本解释层和动态反馈循环。

Result: 在医疗、金融和软件开发中验证，提升决策、合规性和信任。

Conclusion: 该框架推动了人本可解释AI（HCXAI）的发展，使AI更透明、适应性强且符合伦理。

Abstract: The integration of Artificial Intelligence (AI) into high-stakes domains such
as healthcare, finance, and autonomous systems is often constrained by concerns
over transparency, interpretability, and trust. While Human-Centered AI (HCAI)
emphasizes alignment with human values, Explainable AI (XAI) enhances
transparency by making AI decisions more understandable. However, the lack of a
unified approach limits AI's effectiveness in critical decision-making
scenarios. This paper presents a novel three-layered framework that bridges
HCAI and XAI to establish a structured explainability paradigm. The framework
comprises (1) a foundational AI model with built-in explainability mechanisms,
(2) a human-centered explanation layer that tailors explanations based on
cognitive load and user expertise, and (3) a dynamic feedback loop that refines
explanations through real-time user interaction. The framework is evaluated
across healthcare, finance, and software development, demonstrating its
potential to enhance decision-making, regulatory compliance, and public trust.
Our findings advance Human-Centered Explainable AI (HCXAI), fostering AI
systems that are transparent, adaptable, and ethically aligned.

</details>


### [271] [LLM-Driven NPCs: Cross-Platform Dialogue System for Games and Social Platforms](https://arxiv.org/abs/2504.13928)
*Li Song*

Main category: cs.HC

TL;DR: 研究提出了一种基于大语言模型的NPC系统，支持跨平台（游戏环境和社交平台）交互，并通过云数据库同步对话记忆。


<details>
  <summary>Details</summary>
Motivation: 传统游戏中NPC受限于静态对话树和单一交互平台，需突破这些限制。

Method: 开发原型系统，利用LLM驱动的NPC，在Unity游戏环境和Discord社交平台交互，通过LeanCloud云数据库同步对话日志。

Result: 初步实验表明跨平台交互技术可行，为情感建模和持久记忆支持奠定基础。

Conclusion: 系统为未来NPC交互的扩展功能提供了技术基础。

Abstract: NPCs in traditional games are often limited by static dialogue trees and a
single platform for interaction. To overcome these constraints, this study
presents a prototype system that enables large language model (LLM)-powered
NPCs to communicate with players both in the game en vironment (Unity) and on a
social platform (Discord). Dialogue logs are stored in a cloud database
(LeanCloud), allowing the system to synchronize memory between platforms and
keep conversa tions coherent. Our initial experiments show that cross-platform
interaction is technically feasible and suggest a solid foundation for future
developments such as emotional modeling and persistent memory support.

</details>


### [272] [Hashigo: A Next Generation Sketch Interactive System for Japanese Kanji](https://arxiv.org/abs/2504.13940)
*Paul Taele,Tracy Hammond*

Main category: cs.HC

TL;DR: Hashigo系统通过提供类似人类教师的反馈，帮助学生在学习日语汉字时改进书写技巧和视觉结构。


<details>
  <summary>Details</summary>
Motivation: 现有汉字手写识别系统未能充分评估书写技巧，导致学生可能养成不良学习习惯。

Method: 开发Hashigo，一个交互式汉字书写系统，提供人类教师级别的反馈。

Result: 系统能够针对学生的书写缺陷提供具体反馈，避免长期学习中的不良影响。

Conclusion: Hashigo通过自动化反馈有效提升学生的汉字学习效果。

Abstract: Language students can increase their effectiveness in learning written
Japanese by mastering the visual structure and written technique of Japanese
kanji. Yet, existing kanji handwriting recognition systems do not assess the
written technique sufficiently enough to discourage students from developing
bad learning habits. In this paper, we describe our work on Hashigo, a kanji
sketch interactive system which achieves human instructor-level critique and
feedback on both the visual structure and written technique of students'
sketched kanji. This type of automated critique and feedback allows students to
target and correct specific deficiencies in their sketches that, if left
untreated, are detrimental to effective long-term kanji learning.

</details>


### [273] [Intelligence of Things: A Spatial Context-Aware Control System for Smart Devices](https://arxiv.org/abs/2504.13942)
*Sukanth Kalivarathan,Muhmmad Abrar Raja Mohamed,Aswathy Ravikumar,S Harini*

Main category: cs.HC

TL;DR: INOT是一种新型空间上下文感知控制系统，通过自然语言和空间推理提升智能家居自动化。


<details>
  <summary>Details</summary>
Motivation: 现有智能家居系统依赖设备标识符，限制了用户交互的自然性。INOT旨在解决这一问题。

Method: 采用模块化架构，结合视觉语言模型和物联网控制系统，支持自然语言命令。

Result: 用户研究表明，INOT显著降低认知负荷，提高易用性，14/15用户更偏好该系统。

Conclusion: INOT通过消除设备标识符需求，推动智能家居控制系统的直观性和可访问性。

Abstract: This paper introduces Intelligence of Things (INOT), a novel spatial
context-aware control system that enhances smart home automation through
intuitive spatial reasoning. Current smart home systems largely rely on
device-specific identifiers, limiting user interaction to explicit naming
conventions rather than natural spatial references. INOT addresses this
limitation through a modular architecture that integrates Vision Language
Models with IoT control systems to enable natural language commands with
spatial context (e.g., "turn on the light near the window"). The system
comprises key components including an Onboarding Inference Engine, Zero-Shot
Device Detection, Spatial Topology Inference, and Intent-Based Command
Synthesis. A comprehensive user study with 15 participants demonstrated INOT's
significant advantages over conventional systems like Google Home Assistant,
with users reporting reduced cognitive workload (NASA-TLX scores decreased by
an average of 13.17 points), higher ease-of-use ratings, and stronger
preference (14 out of 15 participants). By eliminating the need to memorize
device identifiers and enabling context-aware spatial commands, INOT represents
a significant advancement in creating more intuitive and accessible smart home
control systems.

</details>


### [274] [Mixer Metaphors: audio interfaces for non-musical applications](https://arxiv.org/abs/2504.13944)
*Tace McNamara,Jon McCormack,Maria Teresa Llano*

Main category: cs.HC

TL;DR: 探讨音乐界面是否可用于非音乐应用，设计了一种基于音频控制隐喻的设备，实验表明音频控制更直观有效。


<details>
  <summary>Details</summary>
Motivation: 研究音乐界面在非音乐领域的适用性，探索跨感官隐喻对创意和技术界面设计的价值。

Method: 设计并开发了一种基于音频合成器隐喻的设备，比较了有无音频控制的两个版本，艺术家进行一周使用测试。

Result: 音频控制版本提供了更直接、直观的LLM控制，用户能更创意地实验和操作。

Conclusion: 跨感官隐喻能支持创意设计和具身实践，为新技术界面设计提供新思路。

Abstract: The NIME conference traditionally focuses on interfaces for music and musical
expression. In this paper we reverse this tradition to ask, can interfaces
developed for music be successfully appropriated to non-musical applications?
To help answer this question we designed and developed a new device, which uses
interface metaphors borrowed from analogue synthesisers and audio mixing to
physically control the intangible aspects of a Large Language Model. We
compared two versions of the device, with and without the audio-inspired
augmentations, with a group of artists who used each version over a one week
period. Our results show that the use of audio-like controls afforded more
immediate, direct and embodied control over the LLM, allowing users to
creatively experiment and play with the device over its non-mixer counterpart.
Our project demonstrates how cross-sensory metaphors can support creative
thinking and embodied practice when designing new technological interfaces.

</details>


### [275] [Using customized GPT to develop prompting proficiency in architectural AI-generated images](https://arxiv.org/abs/2504.13948)
*Juan David Salazar Rodriguez,Sam Conrad Joyce,Julfendi Julfendi*

Main category: cs.HC

TL;DR: 研究探讨了定制GPT模型如何提升建筑学生在生成AI驱动图像时的提示能力，发现结构化指导和AI交互角色显著提高了学生的提示技能。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI工具在建筑教育中的普及，提示工程变得至关重要，研究旨在探索如何优化学生的提示能力。

Method: 采用混合方法实验设计，将学生分为三组（无支持、结构化指导、结构化指导加AI交互角色），通过逆向工程任务评估提示技能。

Result: 定量分析显示，结构化指导和AI交互角色显著提升了提示的词汇量、相似性和具体性；定性反馈表明学生信心和批判性思维增强。

Conclusion: 定制化的GPT交互能有效提升学生清晰表达建筑概念的能力。

Abstract: This research investigates the use of customized GPT models to enhance
prompting proficiency among architecture students when generating AI-driven
images. Prompt engineering is increasingly essential in architectural education
due to the widespread adoption of generative AI tools. This study utilized a
mixed-methods experimental design involving architecture students divided into
three distinct groups: a control group receiving no structured support, a
second group provided with structured prompting guides, and a third group
supported by both structured guides and interactive AI personas. Students
engaged in reverse engineering tasks, first guessing provided image prompts and
then generating their own prompts, aiming to boost critical thinking and
prompting skills. Variables examined included time spent prompting, word count,
prompt similarity, and concreteness. Quantitative analysis involved correlation
assessments between these variables and a one-way ANOVA to evaluate differences
across groups. While several correlations showed meaningful relationships, not
all were statistically significant. ANOVA results indicated statistically
significant improvements in word count, similarity, and concreteness,
especially in the group supported by AI personas and structured prompting
guides. Qualitative feedback complemented these findings, revealing enhanced
confidence and critical thinking skills in students. These results suggest
tailored GPT interactions substantially improve students' ability to
communicate architectural concepts clearly and effectively.

</details>


### [276] [Tinker Tales: Interactive Storytelling Framework for Early Childhood Narrative Development and AI Literacy](https://arxiv.org/abs/2504.13969)
*Nayoung Choi,Peace Cyebukayire,Jinho D. Choi*

Main category: cs.HC

TL;DR: Tinker Tales是一个互动式故事讲述框架，以棋盘游戏形式设计，旨在支持幼儿的叙事发展和AI素养。


<details>
  <summary>Details</summary>
Motivation: 通过结合实体和语音交互，为儿童提供安全且有趣的方式学习与AI协作。

Method: 使用NFC芯片附着的棋子和令牌，儿童选择并定义故事元素，AI提供辅助。通过模拟游戏会话评估生成故事的质量和安全性。

Result: 展示了物理和数字元素结合在AI素养中的潜力，生成的故事质量高且安全。

Conclusion: Tinker Tales为儿童提供了一种有效的AI协作学习工具，兼具教育性和趣味性。

Abstract: This paper presents Tinker Tales, an interactive storytelling framework in
the format of a board game, designed to support both narrative development and
AI literacy in early childhood. The framework integrates tangible and
speech-based interactions with AI through NFC chip-attached pawns and tokens,
along with a speaker and microphone. Children select and define key story
elements-such as characters, places, items, and emotions-using the pawns and
tokens, providing further details to the AI and receiving proper assistance,
similar to how adults prompt AI for specific tasks (e.g., writing). For
evaluation, several game sessions were simulated with a child AI agent, and the
quality and safety of the generated stories were assessed from various
perspectives. This work highlights the potential of combining physical and
digital elements in AI literacy, offering a safe and engaging way for children
to learn how to effectively collaborate with AI.

</details>


### [277] [HealthGenie: Empowering Users with Healthy Dietary Guidance through Knowledge Graph and Large Language Models](https://arxiv.org/abs/2504.14594)
*Fan Gao,Xinjie Zhao,Ding Xia,Zhongyi Zhou,Rui Yang,Jinghui Lu,Hang Jiang,Chanjun Park,Irene Li*

Main category: cs.HC

TL;DR: HealthGenie结合知识图谱和大型语言模型，提供个性化饮食建议，并通过可视化减少用户认知负担。


<details>
  <summary>Details</summary>
Motivation: 解决用户在获取个性化饮食建议时面临的专业知识复杂性和个体健康条件多样性问题。

Method: HealthGenie通过查询优化从知识图谱中检索信息，结合LLM生成解释性建议，并提供交互式调整功能。

Result: 用户研究表明，HealthGenie能有效减少交互努力和认知负担，提供个性化建议。

Conclusion: LLM与知识图谱的结合在支持决策制定方面具有潜力，未来系统可参考其设计思路。

Abstract: Seeking dietary guidance often requires navigating complex professional
knowledge while accommodating individual health conditions. Knowledge Graphs
(KGs) offer structured and interpretable nutritional information, whereas Large
Language Models (LLMs) naturally facilitate conversational recommendation
delivery. In this paper, we present HealthGenie, an interactive system that
combines the strengths of LLMs and KGs to provide personalized dietary
recommendations along with hierarchical information visualization for a quick
and intuitive overview. Upon receiving a user query, HealthGenie performs query
refinement and retrieves relevant information from a pre-built KG. The system
then visualizes and highlights pertinent information, organized by defined
categories, while offering detailed, explainable recommendation rationales.
Users can further tailor these recommendations by adjusting preferences
interactively. Our evaluation, comprising a within-subject comparative
experiment and an open-ended discussion, demonstrates that HealthGenie
effectively supports users in obtaining personalized dietary guidance based on
their health conditions while reducing interaction effort and cognitive load.
These findings highlight the potential of LLM-KG integration in supporting
decision-making through explainable and visualized information. We examine the
system's usefulness and effectiveness with an N=12 within-subject study and
provide design considerations for future systems that integrate conversational
LLM and KG.

</details>


### [278] [Completing A Systematic Review in Hours instead of Months with Interactive AI Agents](https://arxiv.org/abs/2504.14822)
*Rui Qiu,Shijie Chen,Yu Su,Po-Yin Yen,Han-Wei Shen*

Main category: cs.HC

TL;DR: InsightAgent是一个基于大型语言模型的人机交互AI代理，显著提升系统综述（SRs）的质量和效率，将完成时间从数月缩短至1.5小时。


<details>
  <summary>Details</summary>
Motivation: 系统综述在医疗等高要求领域至关重要，但传统方法耗时且依赖专家，现有自动化方法无法满足需求。

Method: InsightAgent通过语义分割文献库，采用多代理设计处理文献，并提供可视化工具供用户实时反馈。

Result: 用户研究表明，InsightAgent将SRs质量提升27.2%，接近人工水平的79.7%，用户满意度提高34.4%。

Conclusion: InsightAgent显著优化了系统综述的流程，提升了质量和效率，适用于高要求领域。

Abstract: Systematic reviews (SRs) are vital for evidence-based practice in high stakes
disciplines, such as healthcare, but are often impeded by intensive labors and
lengthy processes that can take months to complete. Due to the high demand for
domain expertise, existing automatic summarization methods fail to accurately
identify relevant studies and generate high-quality summaries. To that end, we
introduce InsightAgent, a human-centered interactive AI agent powered by large
language models that revolutionize this workflow. InsightAgent partitions a
large literature corpus based on semantics and employs a multi-agent design for
more focused processing of literature, leading to significant improvement in
the quality of generated SRs. InsightAgent also provides intuitive
visualizations of the corpus and agent trajectories, allowing users to
effortlessly monitor the actions of the agent and provide real-time feedback
based on their expertise. Our user studies with 9 medical professionals
demonstrate that the visualization and interaction mechanisms can effectively
improve the quality of synthesized SRs by 27.2%, reaching 79.7% of
human-written quality. At the same time, user satisfaction is improved by
34.4%. With InsightAgent, it only takes a clinician about 1.5 hours, rather
than months, to complete a high-quality systematic review.

</details>


### [279] [Flowco: Rethinking Data Analysis in the Age of LLMs](https://arxiv.org/abs/2504.14038)
*Stephen N. Freund,Brooke Simon,Emery D. Berger,Eunice Jun*

Main category: cs.HC

TL;DR: Flowco是一个混合主动系统，通过可视化数据流编程模型和LLM集成，帮助用户（尤其是编程经验较少者）快速编写、调试和优化数据分析。


<details>
  <summary>Details</summary>
Motivation: LLM虽能生成简单数据分析代码，但在需要精细控制、验证中间结果和迭代优化时存在局限性，Flowco旨在解决这些问题。

Method: Flowco结合可视化数据流编程模型，将LLM集成到编写过程的每个阶段。

Result: 用户研究表明，Flowco能有效支持用户（尤其是编程新手）快速完成数据分析任务。

Conclusion: Flowco为数据分析提供了更灵活、可控的工具，尤其适合非专业编程人员。

Abstract: Conducting data analysis typically involves authoring code to transform,
visualize, analyze, and interpret data. Large language models (LLMs) are now
capable of generating such code for simple, routine analyses. LLMs promise to
democratize data science by enabling those with limited programming expertise
to conduct data analyses, including in scientific research, business, and
policymaking. However, analysts in many real-world settings must often exercise
fine-grained control over specific analysis steps, verify intermediate results
explicitly, and iteratively refine their analytical approaches. Such tasks
present barriers to building robust and reproducible analyses using LLMs alone
or even in conjunction with existing authoring tools (e.g., computational
notebooks). This paper introduces Flowco, a new mixed-initiative system to
address these challenges. Flowco leverages a visual dataflow programming model
and integrates LLMs into every phase of the authoring process. A user study
suggests that Flowco supports analysts, particularly those with less
programming experience, in quickly authoring, debugging, and refining data
analyses.

</details>


### [280] [Evaluating Human-AI Interaction via Usability, User Experience and Acceptance Measures for MMM-C: A Creative AI System for Music Composition](https://arxiv.org/abs/2504.14071)
*Renaud Bougueng Tchemeube,Jeff Ens,Cale Plut,Philippe Pasquier,Maryam Safi,Yvan Grabit,Jean-Baptiste Rolland*

Main category: cs.HC

TL;DR: 论文研究了AI工具MMM-Cubase在音乐创作中的用户接受度，结果显示其易用性和接受度较高，但可控性和预测性存在局限。


<details>
  <summary>Details</summary>
Motivation: 随着AI在艺术领域的应用增多，研究AI与人类在音乐创作中的协作实践具有重要意义。

Method: 通过将MMM集成到Cubase中，开发MMM-C插件，采用混合方法研究测量其可用性、用户体验和技术接受度。

Result: 用户对系统的易用性和接受度评分较高，但对其可控性和预测性表示担忧。两组用户无显著差异。

Conclusion: MMM-Cubase作为AI协作工具在音乐创作中表现良好，但需改进可控性和预测性。

Abstract: With the rise of artificial intelligence (AI), there has been increasing
interest in human-AI co-creation in a variety of artistic domains including
music as AI-driven systems are frequently able to generate human-competitive
artifacts. Now, the implications of such systems for musical practice are being
investigated. We report on a thorough evaluation of the user adoption of the
Multi-Track Music Machine (MMM) as a co-creative AI tool for music composers.
To do this, we integrate MMM into Cubase, a popular Digital Audio Workstation
(DAW) by Steinberg, by producing a "1-parameter" plugin interface named
MMM-Cubase (MMM-C), which enables human-AI co-composition. We contribute a
methodological assemblage as a 3-part mixed method study measuring usability,
user experience and technology acceptance of the system across two groups of
expert-level composers: hobbyists and professionals. Results show positive
usability and acceptance scores. Users report experiences of novelty, surprise
and ease of use from using the system, and limitations on controllability and
predictability of the interface when generating music. Findings indicate no
significant difference between the two user groups.

</details>


### [281] [Translating Multimodal AI into Real-World Inspection: TEMAI Evaluation Framework and Pathways for Implementation](https://arxiv.org/abs/2504.13873)
*Zehan Li,Jinzhi Deng,Haibing Ma,Chi Zhang,Dan Xiao*

Main category: cs.HC

TL;DR: TEMAI框架将多模态AI与工业检测结合，强调技术能力、组织准备和价值实现三方面，验证了行业特定适应策略的重要性。


<details>
  <summary>Details</summary>
Motivation: 将医疗领域的转化研究原则应用于工业检测，解决技术能力与组织采纳脱节的问题。

Method: 提出TEMAI框架，包含能力、采纳和效用三个维度，并引入价值密度系数和结构化实施路径。

Result: 实证验证显示，不同行业的价值实现模式差异显著，框架在多行业中有效。

Conclusion: TEMAI框架在工业检测中具有广泛适用性，但需针对行业特点调整策略。

Abstract: This paper introduces the Translational Evaluation of Multimodal AI for
Inspection (TEMAI) framework, bridging multimodal AI capabilities with
industrial inspection implementation. Adapting translational research
principles from healthcare to industrial contexts, TEMAI establishes three core
dimensions: Capability (technical feasibility), Adoption (organizational
readiness), and Utility (value realization). The framework demonstrates that
technical capability alone yields limited value without corresponding adoption
mechanisms. TEMAI incorporates specialized metrics including the Value Density
Coefficient and structured implementation pathways. Empirical validation
through retail and photovoltaic inspection implementations revealed significant
differences in value realization patterns despite similar capability reduction
rates, confirming the framework's effectiveness across diverse industrial
sectors while highlighting the importance of industry-specific adaptation
strategies.

</details>


### [282] [Hybrid Deep Learning Model to Estimate Cognitive Effort from fNIRS Signals in Educational Game Playing](https://arxiv.org/abs/2504.13883)
*Shayla Sharmin,Roghayeh Leila Barmaki*

Main category: cs.HC

TL;DR: 研究通过混合深度学习模型结合fNIRS数据和表现分数估计认知努力（CE），以优化学习效果和学生参与度。结果显示CNN-GRU混合模型表现最佳，且提取的特征在传统机器学习中也有较好泛化能力。


<details>
  <summary>Details</summary>
Motivation: 通过估计CE帮助教育者调整学习材料，提升学习效果和学生参与度。

Method: 使用fNIRS数据（ΔHbO）和表现分数，结合CNN、LSTM、BiLSTM和CNN-GRU混合模型预测表现分数并计算RNE和RNI。

Result: CNN-GRU混合模型训练准确率78.36%，测试准确率73.08%，提取的特征在XGBoost中达到69.23%准确率。预测的RNE和RNI与实际趋势接近。

Conclusion: 混合深度学习模型能有效估计CE，为学习环境设计和材料改进提供有价值见解。

Abstract: This study estimates cognitive effort (CE) based on functional near-infrared
spectroscopy (fNIRS) data and performance scores using a hybrid deep learning
model. The estimation of CE enables educators to modify material to enhance
learning effectiveness and student engagement. Relative neural efficiency (RNE)
and relative neural involvement (RNI) are two metrics that have been used to
represent CE. To estimate RNE and RNI we need hemodynamic response in the brain
and the performance score of a task.We collected oxygenated hemoglobin ($\Delta
\mathrm{HbO}$). Sixteen participants answered 16 questions in a unity-based
educational game, each with a 30-second response time. We used deep learning
models to predict the performance score and estimate RNE and RNI to understand
CE. The study compares traditional machine learning techniques with deep
learning models such as CNN, LSTM, BiLSTM, and a hybrid CNN-GRU to determine
which approach provides better accuracy in predicting performance scores. The
result shows that the hybrid CNN-GRU gives better performance with 78.36\%
training accuracy and 73.08\% test accuracy than other models. We performed
XGBoost on the extracted GRU feature and got the highest accuracy (69.23\%).
This suggests that the features learned from this hybrid model generalize
better even in traditional machine learning algorithms. We used the $\Delta
\mathrm{HbO}$ and predicted score to calculate RNE and RNI to observe cognitive
effort in our four test cases. Our result shows that even with moderate
accuracy, the predicted RNE and RNI closely follows the actual trends. we also
observed that when participants were in a state of high CE, introducing rest
led decrease of CE. These findings can be helpful to design and improve
learning environments and provide valuable insights in learning materials.

</details>


### [283] [ViMo: A Generative Visual GUI World Model for App Agent](https://arxiv.org/abs/2504.13936)
*Dezhao Luo,Bohan Tang,Kang Li,Georgios Papoudakis,Jifei Song,Shaogang Gong,Jianye Hao,Jun Wang,Kun Shao*

Main category: cs.HC

TL;DR: ViMo是一种视觉世界模型，通过生成图像预测未来App界面，帮助App代理更有效地规划复杂任务。


<details>
  <summary>Details</summary>
Motivation: 现有世界模型仅生成文本描述，缺乏视觉细节，限制了App代理的长时规划能力。

Method: ViMo将GUI生成分解为图形和文本内容生成，使用符号文本表示（STR）处理文本内容，并分别预测图形和文本。

Result: 实验表明，ViMo能生成视觉合理且功能有效的GUI，帮助代理做出更明智的决策。

Conclusion: ViMo填补了视觉世界模型的空白，显著提升了App代理的规划能力。

Abstract: App agents, which autonomously operate mobile Apps through Graphical User
Interfaces (GUIs), have gained significant interest in real-world applications.
Yet, they often struggle with long-horizon planning, failing to find the
optimal actions for complex tasks with longer steps. To address this, world
models are used to predict the next GUI observation based on user actions,
enabling more effective agent planning. However, existing world models
primarily focus on generating only textual descriptions, lacking essential
visual details. To fill this gap, we propose ViMo, the first visual world model
designed to generate future App observations as images. For the challenge of
generating text in image patches, where even minor pixel errors can distort
readability, we decompose GUI generation into graphic and text content
generation. We propose a novel data representation, the Symbolic Text
Representation~(STR) to overlay text content with symbolic placeholders while
preserving graphics. With this design, ViMo employs a STR Predictor to predict
future GUIs' graphics and a GUI-text Predictor for generating the corresponding
text. Moreover, we deploy ViMo to enhance agent-focused tasks by predicting the
outcome of different action options. Experiments show ViMo's ability to
generate visually plausible and functionally effective GUIs that enable App
agents to make more informed decisions.

</details>


### [284] [Amplify Initiative: Building A Localized Data Platform for Globalized AI](https://arxiv.org/abs/2504.14105)
*Qazi Mamunur Rashid,Erin van Liemt,Tiffany Shih,Amber Ebinama,Karla Barrios Ramos,Madhurima Maji,Aishwarya Verma,Charu Kalia,Jamila Smith-Loud,Joyce Nakatumba-Nabende,Rehema Baguma,Andrew Katumba,Chodrine Mutebi,Jagen Marvin,Eric Peter Wairagala,Mugizi Bruce,Peter Oketta,Lawrence Nderu,Obichi Obiajunwa,Abigail Oppong,Michael Zimba,Data Authors*

Main category: cs.HC

TL;DR: Amplify Initiative通过专家社区合作收集多语言数据，解决AI模型因训练数据局限导致的全球适用性问题，并在非洲五国试点成功生成8,091条对抗性查询数据集。


<details>
  <summary>Details</summary>
Motivation: 当前AI模型因训练数据以英语和西方内容为主，缺乏本地语境和语言多样性，影响其全球相关性和安全性。

Method: 通过与非洲五国的领域专家合作，使用Android应用共同创建多语言数据集，涵盖敏感主题。

Result: 生成了包含7种语言的8,091条对抗性查询数据集，用于评估模型的安全性和文化相关性。

Conclusion: Amplify Initiative的方法有效提升了AI模型的全球适用性，为多语言和本地化数据收集提供了可行方案。

Abstract: Current AI models often fail to account for local context and language, given
the predominance of English and Western internet content in their training
data. This hinders the global relevance, usefulness, and safety of these models
as they gain more users around the globe. Amplify Initiative, a data platform
and methodology, leverages expert communities to collect diverse, high-quality
data to address the limitations of these models. The platform is designed to
enable co-creation of datasets, provide access to high-quality multilingual
datasets, and offer recognition to data authors. This paper presents the
approach to co-creating datasets with domain experts (e.g., health workers,
teachers) through a pilot conducted in Sub-Saharan Africa (Ghana, Kenya,
Malawi, Nigeria, and Uganda). In partnership with local researchers situated in
these countries, the pilot demonstrated an end-to-end approach to co-creating
data with 155 experts in sensitive domains (e.g., physicians, bankers,
anthropologists, human and civil rights advocates). This approach, implemented
with an Android app, resulted in an annotated dataset of 8,091 adversarial
queries in seven languages (e.g., Luganda, Swahili, Chichewa), capturing
nuanced and contextual information related to key themes such as misinformation
and public interest topics. This dataset in turn can be used to evaluate models
for their safety and cultural relevance within the context of these languages.

</details>


### [285] [The Balancing Act of Policies in Developing Machine Learning Explanations](https://arxiv.org/abs/2504.13946)
*Jacob Tjaden*

Main category: cs.HC

TL;DR: 研究探讨政策设计对机器学习模型解释质量的影响，发现政策长度影响开发者参与度，但政策目的无影响，解释质量普遍较差。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型因决策过程不透明常受批评，研究旨在探索政策设计如何改善解释质量。

Method: 通过124名参与者的课堂实验，分析政策长度和目的对开发者合规性的影响。

Result: 政策长度影响部分要求的参与度，政策目的无显著影响，解释质量普遍较低。

Conclusion: 研究强调有效政策制定的挑战及在解释中考虑多元利益相关者视角的重要性。

Abstract: Machine learning models are often criticized as opaque from a lack of
transparency in their decision-making process. This study examines how policy
design impacts the quality of explanations in ML models. We conducted a
classroom experiment with 124 participants and analyzed the effects of policy
length and purpose on developer compliance with policy requirements. Our
results indicate that while policy length affects engagement with some
requirements, policy purpose has no effect, and explanation quality is
generally poor. These findings highlight the challenge of effective policy
development and the importance of addressing diverse stakeholder perspectives
within explanations.

</details>


### [286] [Longitudinal Study on Social and Emotional Use of AI Conversational Agent](https://arxiv.org/abs/2504.14112)
*Mohit Chandra,Javier Hernandez,Gonzalo Ramos,Mahsa Ershadi,Ananya Bhattacharjee,Judith Amores,Ebele Okoli,Ann Paradiso,Shahed Warreth,Jina Suh*

Main category: cs.HC

TL;DR: 研究探讨了AI在社交和情感支持中的作用，发现主动使用AI工具的用户对其依恋感和共情能力感知显著提升，但也需注意潜在风险。


<details>
  <summary>Details</summary>
Motivation: 数字技术发展改变了人们寻求社交和情感支持的方式，AI的普及带来了新的动态，研究旨在探索其影响。

Method: 五周探索性研究，149名参与者分为基线使用组和主动使用组，后者使用四种商业AI工具进行社交互动。

Result: 主动使用组对AI的依恋感、共情感知及娱乐动机显著提升，个体差异影响感知，AI在情感支持方面潜力显著。

Conclusion: 需开发负责任的情感支持AI工具，同时让用户了解其局限性，防止过度依赖。

Abstract: Development in digital technologies has continuously reshaped how individuals
seek and receive social and emotional support. While online platforms and
communities have long served this need, the increased integration of
general-purpose conversational AI into daily lives has introduced new dynamics
in how support is provided and experienced. Existing research has highlighted
both benefits (e.g., wider access to well-being resources) and potential risks
(e.g., over-reliance) of using AI for support seeking. In this five-week,
exploratory study, we recruited 149 participants divided into two usage groups:
a baseline usage group (BU, n=60) that used the internet and AI as usual, and
an active usage group (AU, n=89) encouraged to use one of four commercially
available AI tools (Microsoft Copilot, Google Gemini, PI AI, ChatGPT) for
social and emotional interactions. Our analysis revealed significant increases
in perceived attachment towards AI (32.99 percentage points), perceived AI
empathy (25.8 p.p.), and motivation to use AI for entertainment (22.90 p.p.)
among the AU group. We also observed that individual differences (e.g., gender
identity, prior AI usage) influenced perceptions of AI empathy and attachment.
Lastly, the AU group expressed higher comfort in seeking personal help,
managing stress, obtaining social support, and talking about health with AI,
indicating potential for broader emotional support while highlighting the need
for safeguards against problematic usage. Overall, our exploratory findings
underscore the importance of developing consumer-facing AI tools that support
emotional well-being responsibly, while empowering users to understand the
limitations of these tools.

</details>


### [287] [Exploring Language Patterns of Prompts in Text-to-Image Generation and Their Impact on Visual Diversity](https://arxiv.org/abs/2504.14125)
*Maria-Teresa De Rosa Palmini,Eva Cetinic*

Main category: cs.HC

TL;DR: 研究探讨用户与文本到图像（TTI）模型的交互动态，发现用户提示语言的同质化导致生成图像多样性降低。


<details>
  <summary>Details</summary>
Motivation: 尽管TTI模型的偏见和刻板印象已被广泛讨论，但用户交互的社会技术动态仍未充分研究。

Method: 分析CivitAI平台上六百万条提示，将用户分为三类（重复者、偶尔重复者、非重复者），并使用Vendi分数量化视觉多样性。

Result: 用户提示语言逐渐同质化，重复提示占40-50%，且语言重复与视觉相似性显著相关。

Conclusion: 用户行为对AI生成图像的多样性有重要影响，需鼓励更多语言和主题实验以促进多样性。

Abstract: Following the initial excitement, Text-to-Image (TTI) models are now being
examined more critically. While much of the discourse has focused on biases and
stereotypes embedded in large-scale training datasets, the sociotechnical
dynamics of user interactions with these models remain underexplored. This
study examines the linguistic and semantic choices users make when crafting
prompts and how these choices influence the diversity of generated outputs.
Analyzing over six million prompts from the Civiverse dataset on the CivitAI
platform across seven months, we categorize users into three groups based on
their levels of linguistic experimentation: consistent repeaters, occasional
repeaters, and non-repeaters. Our findings reveal that as user participation
grows over time, prompt language becomes increasingly homogenized through the
adoption of popular community tags and descriptors, with repeated prompts
comprising 40-50% of submissions. At the same time, semantic similarity and
topic preferences remain relatively stable, emphasizing common subjects and
surface aesthetics. Using Vendi scores to quantify visual diversity, we
demonstrate a clear correlation between lexical similarity in prompts and the
visual similarity of generated images, showing that linguistic repetition
reinforces less diverse representations. These findings highlight the
significant role of user-driven factors in shaping AI-generated imagery, beyond
inherent model biases, and underscore the need for tools and practices that
encourage greater linguistic and thematic experimentation within TTI systems to
foster more inclusive and diverse AI-generated content.

</details>


### [288] [Apollo: An Interactive Environment for Generating Symbolic Musical Phrases using Corpus-based Style Imitation](https://arxiv.org/abs/2504.14055)
*Renaud Bougueng Tchemeube,Jeff Ens,Philippe Pasquier*

Main category: cs.HC

TL;DR: Apollo是一个基于机器学习的交互式音乐生成系统，用于通过风格模仿技术生成西方音乐的符号短语。


<details>
  <summary>Details</summary>
Motivation: 探索机器学习和网络技术在辅助音乐创作中的应用，特别是风格模仿和音乐生成。

Method: 使用基于语料库的风格模仿技术，构建和管理符号音乐语料库，生成新的音乐短语。

Result: 开发了桌面应用程序，支持MIDI格式的音乐材料导出和流式传输。

Conclusion: 系统展示了音乐生成和风格模仿的潜力，未来工作将进一步优化功能。

Abstract: With the recent developments in machine intelligence and web technologies,
new generative music systems are being explored for assisted composition using
machine learning techniques on the web. Such systems are built for various
tasks such as melodic, harmonic or rhythm generation, music interpolation,
continuation and style imitation. In this paper, we introduce Apollo, an
interactive music application for generating symbolic phrases of conventional
western music using corpus-based style imitation techniques. In addition to
enabling the construction and management of symbolic musical corpora, the
system makes it possible for music artists and researchers to generate new
musical phrases in the style of the proposed corpus. The system is available as
a desktop application. The generated symbolic music materials, encoded in the
MIDI format, can be exported or streamed for various purposes including using
them as seed material for musical projects. We present the system design,
implementation details, discuss and conclude with future work for the system.

</details>


### [289] [Calliope: An Online Generative Music System for Symbolic Multi-Track Composition](https://arxiv.org/abs/2504.14058)
*Renaud Bougueng Tchemeube,Jeff Ens,Philippe Pasquier*

Main category: cs.HC

TL;DR: Calliope是一个基于Web的应用程序，利用人工智能辅助用户进行多轨音乐创作，支持MIDI文件编辑、生成和播放。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能在创意领域的应用增加，开发工具辅助音乐创作成为需求。

Method: 通过Multi-Track Music Machine (MMM)实现MIDI文件的编辑、生成和播放，支持批量生成和实时播放。

Result: 系统展示了其功能、生成参数及协同创作流程。

Conclusion: Calliope为音乐创作提供了高效的辅助工具，增强了创作流程。

Abstract: With the rise of artificial intelligence in recent years, there has been a
rapid increase in its application towards creative domains, including music.
There exist many systems built that apply machine learning approaches to the
problem of computer-assisted music composition (CAC). Calliope is a web
application that assists users in performing a variety of multi-track
composition tasks in the symbolic domain. The user can upload (Musical
Instrument Digital Interface) MIDI files, visualize and edit MIDI tracks, and
generate partial (via bar in-filling) or complete multi-track content using the
Multi-Track Music Machine (MMM). Generation of new MIDI excerpts can be done in
batch and can be combined with active playback listening for an enhanced
assisted-composition workflow. The user can export generated MIDI materials or
directly stream MIDI playback from the system to their favorite Digital Audio
Workstation (DAW). We present a demonstration of the system, its features,
generative parameters and describe the co-creative workflows that it affords.

</details>


### [290] [Visualization Tasks for Unlabelled Graphs](https://arxiv.org/abs/2504.14115)
*Matt I. B. Oddo,Ryan Smith,Stephen Kobourov,Tamara Munzner*

Main category: cs.HC

TL;DR: 该论文研究了无标签图的任务分类，提出了基于Scope、Action和Target的任务分类法，并通过实验评估了6种可视化方法的效果。


<details>
  <summary>Details</summary>
Motivation: 需要更深入理解无标签图的任务特性，以便更好地评估相关可视化技术。

Method: 提出了一种基于Scope、Action和Target的任务分类法，并通过实验评估了6种可视化方法在不同任务中的表现。

Result: 分类法能够有效描述和评估无标签图任务，实验结果显示不同可视化方法在任务完成度和用户努力方面存在差异。

Conclusion: 提出的分类法为无标签图任务提供了系统化的评估框架，有助于指导未来可视化技术的设计。

Abstract: We investigate tasks that can be accomplished with unlabelled graphs, where
nodes do not have persistent or semantically meaningful labels. New techniques
to visualize these graphs have been proposed, but more understanding of
unlabelled graph tasks is required before they can be adequately evaluated.
Some tasks apply to both labelled and unlabelled graphs, but many do not
translate between these contexts. We propose a taxonomy of unlabelled graph
abstract tasks, organized according to the Scope of the data at play, the
Action intended by the user, and the Target data under consideration. We show
the descriptive power of this task abstraction by connecting to concrete
examples from previous frameworks, and connect these abstractions to real-world
problems. To showcase the evaluative power of the taxonomy, we perform a
preliminary assessment of 6 visualizations for each task. For each combination
of task and visual encoding, we consider the effort required from viewers, the
likelihood of task success, and how both factors vary between small-scale and
large-scale graphs.

</details>


### [291] [Expanding the Generative AI Design Space through Structured Prompting and Multimodal Interfaces](https://arxiv.org/abs/2504.14320)
*Nimisha Karnatak,Adrien Baranes,Rob Marchant,Huinan Zeng,Tríona Butler,Kristen Olson*

Main category: cs.HC

TL;DR: 论文探讨了生成式AI中文本提示对新手用户的高摩擦问题，提出了一种多模态工具ACAI，通过结构化界面减少提示模糊性。


<details>
  <summary>Details</summary>
Motivation: 解决新手用户（如小企业主）在使用生成式AI时难以表达创意目标的问题。

Method: 研究六位英国小企业主的广告实践，开发ACAI工具，包含品牌、受众与目标、灵感板三个模块。

Result: 发现当前生成式AI存在提示工程认知负担和输出通用性问题，ACAI通过结构化界面改善了这些问题。

Conclusion: 结构化界面能提升新手用户的工作流程中对齐性和提示效果。

Abstract: Text-based prompting remains the dominant interaction paradigm in generative
AI, yet it often results in a high-friction experience for novice users, such
as small business owners (SBOs), attempting to articulate creative or
domain-specific goals for advertising. To investigate this challenge, we
conducted a study with six SBOs in the United Kingdom, focusing on their
advertising practices and perceptions and usage of AI tools in this context.
Our findings surfaced two persistent breakdowns in current generative AI
systems: first, the cognitive burden of prompt engineering, as users struggled
to translate abstract creative goals into effective textual inputs; and second,
the frequent generation of generic outputs that failed to align with users'
articulated brand vision. To address these issues, we developed ACAI (AI
Co-Creation for Advertising and Inspiration), a multimodal, GenAI-powered
advertisement creation tool designed to support novice designers by reimagining
the prompt interface. ACAI features a structured, panel-based interface
composed of three modules: the Branding Panel, the Audience & Goals Panel, and
the Inspiration Board Panel to provide SBOs with outputs that align with their
creative vision by reducing prompt ambiguity. This work contributes to HCI
research on generative systems by showing how structured interfaces can
foreground user-defined context to improve both alignment and promptability in
novice workflows.

</details>


### [292] [ScholarMate: A Mixed-Initiative Tool for Qualitative Knowledge Work and Information Sensemaking](https://arxiv.org/abs/2504.14406)
*Runlong Ye,Patrick Yung Kang Lee,Matthew Varona,Oliver Huang,Carolina Nobre*

Main category: cs.HC

TL;DR: ScholarMate是一个交互式系统，结合AI辅助与人工监督，提升定性分析的效率和可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决在复杂文档集合中合成知识时，AI自动化与人工工作流结合的挑战。

Method: 通过非线性的交互画布动态组织文本片段，利用AI提供主题建议、多级摘要和上下文命名，并确保来源可追溯。

Result: 初步研究表明用户认可这种混合方法，认为AI建议与直接操作的平衡对保持可解释性和信任至关重要。

Conclusion: ScholarMate通过平衡自动化与人工控制，提升了效率并支持可解释性，为知识工作中的复杂任务提供了有效的人机协作方案。

Abstract: Synthesizing knowledge from large document collections is a critical yet
increasingly complex aspect of qualitative research and knowledge work. While
AI offers automation potential, effectively integrating it into human-centric
sensemaking workflows remains challenging. We present ScholarMate, an
interactive system designed to augment qualitative analysis by unifying AI
assistance with human oversight. ScholarMate enables researchers to dynamically
arrange and interact with text snippets on a non-linear canvas, leveraging AI
for theme suggestions, multi-level summarization, and contextual naming, while
ensuring transparency through traceability to source documents. Initial pilot
studies indicated that users value this mixed-initiative approach, finding the
balance between AI suggestions and direct manipulation crucial for maintaining
interpretability and trust. We further demonstrate the system's capability
through a case study analyzing 24 papers. By balancing automation with human
control, ScholarMate enhances efficiency and supports interpretability,
offering a valuable approach for productive human-AI collaboration in demanding
sensemaking tasks common in knowledge work.

</details>


### [293] [Optimizing SIA Development: A Case Study in User-Centered Design for Estuary, a Multimodal Socially Interactive Agent Framework](https://arxiv.org/abs/2504.14427)
*Spencer Lin,Miru Jun,Basem Rizk,Karen Shieh,Scott Fisher,Sharon Mozgai*

Main category: cs.HC

TL;DR: 本文介绍了一种用户为中心的设计模型，用于开发社交智能代理（SIA）框架，并通过开源多模态框架Estuary的实践经验展示了其应用。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过用户反馈改进SIA开发框架，填补当前研究空白。

Method: 采用快速评估过程（RAP）收集领域专家意见，通过用户访谈评估Estuary框架的潜力。

Result: 研究结果有助于Estuary的持续开发，并为未来SIA框架提供指导。

Conclusion: 用户反馈是改进SIA框架的关键，Estuary展示了其潜力，未来研究可进一步优化。

Abstract: This case study presents our user-centered design model for Socially
Intelligent Agent (SIA) development frameworks through our experience
developing Estuary, an open source multimodal framework for building
low-latency real-time socially interactive agents. We leverage the Rapid
Assessment Process (RAP) to collect the thoughts of leading researchers in the
field of SIAs regarding the current state of the art for SIA development as
well as their evaluation of how well Estuary may potentially address current
research gaps. We achieve this through a series of end-user interviews
conducted by a fellow researcher in the community. We hope that the findings of
our work will not only assist the continued development of Estuary but also
guide the development of other future frameworks and technologies for SIAs.

</details>


### [294] [Biased by Design: Leveraging AI Biases to Enhance Critical Thinking of News Readers](https://arxiv.org/abs/2504.14522)
*Liudmila Zavolokina,Kilian Sprenkamp,Zoya Katashinskaya,Daniel Gordon Jones*

Main category: cs.HC

TL;DR: 论文探讨了利用大语言模型（LLMs）设计宣传检测工具，研究如何利用AI偏见增强新闻消费中的批判性思维。


<details>
  <summary>Details</summary>
Motivation: 认识到AI模型在政治背景下的固有偏见，研究如何利用这些偏见提升用户的批判性思维，而非视其为有害。

Method: 提出基于用户政治立场的个性化策略，结合心理学概念（确认偏误和认知失调），并通过定性用户研究验证。

Result: 研究发现并提出了设计建议（偏见意识、个性化选择、逐步引入多样化观点）。

Conclusion: AI工具在宣传检测中可通过个性化策略和用户选择优化效果，同时增强批判性思维。

Abstract: This paper explores the design of a propaganda detection tool using Large
Language Models (LLMs). Acknowledging the inherent biases in AI models,
especially in political contexts, we investigate how these biases might be
leveraged to enhance critical thinking in news consumption. Countering the
typical view of AI biases as detrimental, our research proposes strategies of
user choice and personalization in response to a user's political stance,
applying psychological concepts of confirmation bias and cognitive dissonance.
We present findings from a qualitative user study, offering insights and design
recommendations (bias awareness, personalization and choice, and gradual
introduction of diverse perspectives) for AI tools in propaganda detection.

</details>


### [295] [Exploring Collaborative GenAI Agents in Synchronous Group Settings: Eliciting Team Perceptions and Design Considerations for the Future of Work](https://arxiv.org/abs/2504.14779)
*Janet G. Johnson,Macarena Peralta,Mansanjam Kaur,Ruijie Sophia Huang,Sheng Zhao,Ruijia Guan,Shwetha Rajaram,Michael Nebeling*

Main category: cs.HC

TL;DR: 探讨协作式生成AI代理在团队工作中的潜力，通过设计研讨会和访谈发现其能提升团队问题解决能力，但需考虑个体、团队和组织因素。


<details>
  <summary>Details</summary>
Motivation: 研究生成AI在团队协作中的应用，填补当前工具主要针对个人使用的空白，探索如何优化团队动态中的AI部署。

Method: 通过25名专业人士参与的6个团队的设计研讨会和后续访谈，使用混合现实原型模拟协作式生成AI代理。

Result: 协作式生成AI代理能有效挑战群体思维、弥补沟通差距并减少社交摩擦，但其接受度取决于个体、团队和组织的适配度。

Conclusion: 设计需平衡代理的表现形式、社交显著性和参与度，空间和沉浸技术可调节AI对团队成果的影响，实现增强与自主性的平衡。

Abstract: While generative artificial intelligence (GenAI) is finding increased
adoption in workplaces, current tools are primarily designed for individual
use. Prior work established the potential for these tools to enhance personal
creativity and productivity towards shared goals; however, we don't know yet
how to best take into account the nuances of group work and team dynamics when
deploying GenAI in work settings. In this paper, we investigate the potential
of collaborative GenAI agents to augment teamwork in synchronous group settings
through an exploratory study that engaged 25 professionals across 6 teams in
speculative design workshops and individual follow-up interviews. Our workshops
included a mixed reality provotype to simulate embodied collaborative GenAI
agents capable of actively participating in group discussions. Our findings
suggest that, if designed well, collaborative GenAI agents offer valuable
opportunities to enhance team problem-solving by challenging groupthink,
bridging communication gaps, and reducing social friction. However, teams'
willingness to integrate GenAI agents depended on its perceived fit across a
number of individual, team, and organizational factors. We outline the key
design tensions around agent representation, social prominence, and engagement
and highlight the opportunities spatial and immersive technologies could offer
to modulate GenAI influence on team outcomes and strike a balance between
augmentation and agency.

</details>


### [296] [NeuGaze: Reshaping the future BCI](https://arxiv.org/abs/2504.15101)
*Yiqian Yang*

Main category: cs.HC

TL;DR: NeuGaze是一种基于普通网络摄像头的低成本脑机接口替代方案，利用眼动、头部运动和面部表情实现实时控制，适用于运动障碍用户。


<details>
  <summary>Details</summary>
Motivation: 传统脑机接口依赖昂贵设备且操作复杂，NeuGaze旨在提供一种低成本、易用的替代方案。

Method: 使用30Hz网络摄像头捕捉眼动、头部运动和面部表情，通过技能轮实现精确控制和动态交互。

Result: 性能接近传统输入设备，支持光标导航、按键触发和游戏交互。

Conclusion: NeuGaze为运动障碍用户提供了一种无需专用硬件的低成本解决方案，扩展了人机交互的可能性。

Abstract: Traditional brain-computer interfaces (BCIs), reliant on costly
electroencephalography or invasive implants, struggle with complex
human-computer interactions due to setup complexity and limited precision. We
present NeuGaze, a novel webcam-based system that leverages eye gaze, head
movements, and facial expressions to enable intuitive, real-time control using
only a standard 30 Hz webcam, often pre-installed in laptops. Requiring minimal
calibration, NeuGaze achieves performance comparable to conventional inputs,
supporting precise cursor navigation, key triggering via an efficient skill
wheel, and dynamic gaming interactions, such as defeating formidable opponents
in first-person games. By harnessing preserved neck-up functionalities in
motor-impaired individuals, NeuGaze eliminates the need for specialized
hardware, offering a low-cost, accessible alternative to BCIs. This paradigm
empowers diverse applications, from assistive technology to entertainment,
redefining human-computer interaction for motor-impaired users. Project is at
\href{https://github.com/NeuSpeech/NeuGaze}{github.com/NeuSpeech/NeuGaze}.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [297] [Towards Model Resistant to Transferable Adversarial Examples via Trigger Activation](https://arxiv.org/abs/2504.14541)
*Yi Yu,Song Xia,Xun Lin,Chenqi Kong,Wenhan Yang,Shijian Lu,Yap-Peng Tan,Alex C. Kot*

Main category: cs.CR

TL;DR: 论文提出了一种新训练范式，通过触发激活模型增强对抗样本的可转移性鲁棒性，并在实验中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 对抗样本的可转移性对深度神经网络构成威胁，现有防御方法存在效率低、效果差等问题。

Method: 提出触发激活模型，通过固定触发器优化模型，提升对可转移对抗样本的鲁棒性。

Result: 实验证明该方法在多种数据集和攻击方法下表现优越。

Conclusion: 触发激活模型能有效提升对抗样本的可转移性鲁棒性，且理论分析和实验验证支持其有效性。

Abstract: Adversarial examples, characterized by imperceptible perturbations, pose
significant threats to deep neural networks by misleading their predictions. A
critical aspect of these examples is their transferability, allowing them to
deceive {unseen} models in black-box scenarios. Despite the widespread
exploration of defense methods, including those on transferability, they show
limitations: inefficient deployment, ineffective defense, and degraded
performance on clean images. In this work, we introduce a novel training
paradigm aimed at enhancing robustness against transferable adversarial
examples (TAEs) in a more efficient and effective way. We propose a model that
exhibits random guessing behavior when presented with clean data
$\boldsymbol{x}$ as input, and generates accurate predictions when with
triggered data $\boldsymbol{x}+\boldsymbol{\tau}$. Importantly, the trigger
$\boldsymbol{\tau}$ remains constant for all data instances. We refer to these
models as \textbf{models with trigger activation}. We are surprised to find
that these models exhibit certain robustness against TAEs. Through the
consideration of first-order gradients, we provide a theoretical analysis of
this robustness. Moreover, through the joint optimization of the learnable
trigger and the model, we achieve improved robustness to transferable attacks.
Extensive experiments conducted across diverse datasets, evaluating a variety
of attacking methods, underscore the effectiveness and superiority of our
approach.

</details>


### [298] [Protecting Your Voice: Temporal-aware Robust Watermarking](https://arxiv.org/abs/2504.14832)
*Yue Li,Weizhi Liu,Dongdong Lin*

Main category: cs.CR

TL;DR: 提出了一种时间感知的鲁棒水印方法（True），用于保护语音和歌声，同时平衡保真度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 生成模型的快速发展导致合成声音的真实性模糊，现有频域水印方法虽鲁棒但牺牲了声音的保真度。

Method: 通过最大化时间域特征的全面学习，提出True方法，增强保真度同时保持鲁棒性。

Result: True方法在保护语音和歌声时，显著提升了保真度，同时保持了鲁棒性。

Conclusion: True方法为合成声音的水印保护提供了新的解决方案，平衡了保真度和鲁棒性。

Abstract: The rapid advancement of generative models has led to the synthesis of
real-fake ambiguous voices. To erase the ambiguity, embedding watermarks into
the frequency-domain features of synthesized voices has become a common
routine. However, the robustness achieved by choosing the frequency domain
often comes at the expense of fine-grained voice features, leading to a loss of
fidelity. Maximizing the comprehensive learning of time-domain features to
enhance fidelity while maintaining robustness, we pioneer a
\textbf{\underline{t}}emporal-aware
\textbf{\underline{r}}ob\textbf{\underline{u}}st
wat\textbf{\underline{e}}rmarking (\emph{True}) method for protecting the
speech and singing voice.

</details>


### [299] [aiXamine: LLM Safety and Security Simplified](https://arxiv.org/abs/2504.14985)
*Fatih Deniz,Dorde Popovic,Yazan Boshmaf,Euisuh Jeong,Minhaj Ahmad,Sanjay Chawla,Issa Khalil*

Main category: cs.CR

TL;DR: aiXamine是一个用于评估大型语言模型（LLM）安全性和安全性的黑盒平台，整合了40多个测试，覆盖8个关键维度，评估了50多个模型，发现了一些领先模型的漏洞和开源模型的优势。


<details>
  <summary>Details</summary>
Motivation: 评估LLM的安全性和安全性是一个复杂且分散的任务，需要统一的平台来解决这一问题。

Method: aiXamine平台整合了40多个测试，分为8个关键服务维度，生成详细报告和可视化结果。

Result: 评估了50多个模型，发现GPT-4o易受对抗攻击，Grok-3存在偏见输出，Gemini 2.0有隐私问题；开源模型在某些方面优于专有模型。

Conclusion: aiXamine为LLM安全性和安全性评估提供了统一工具，揭示了模型的漏洞和开源模型的潜力，同时指出了模型设计中的权衡。

Abstract: Evaluating Large Language Models (LLMs) for safety and security remains a
complex task, often requiring users to navigate a fragmented landscape of ad
hoc benchmarks, datasets, metrics, and reporting formats. To address this
challenge, we present aiXamine, a comprehensive black-box evaluation platform
for LLM safety and security. aiXamine integrates over 40 tests (i.e.,
benchmarks) organized into eight key services targeting specific dimensions of
safety and security: adversarial robustness, code security, fairness and bias,
hallucination, model and data privacy, out-of-distribution (OOD) robustness,
over-refusal, and safety alignment. The platform aggregates the evaluation
results into a single detailed report per model, providing a detailed breakdown
of model performance, test examples, and rich visualizations. We used aiXamine
to assess over 50 publicly available and proprietary LLMs, conducting over 2K
examinations. Our findings reveal notable vulnerabilities in leading models,
including susceptibility to adversarial attacks in OpenAI's GPT-4o, biased
outputs in xAI's Grok-3, and privacy weaknesses in Google's Gemini 2.0.
Additionally, we observe that open-source models can match or exceed
proprietary models in specific services such as safety alignment, fairness and
bias, and OOD robustness. Finally, we identify trade-offs between distillation
strategies, model size, training methods, and architectural choices.

</details>


### [300] [SOLIDO: A Robust Watermarking Method for Speech Synthesis via Low-Rank Adaptation](https://arxiv.org/abs/2504.15035)
*Yue Li,Weizhi Liu,Dongdong Lin*

Main category: cs.CR

TL;DR: 提出了一种名为SOLIDO的新型语音生成水印方法，通过低秩适应（LoRA）结合参数高效微调，解决了现有方法计算开销大、训练成本高及变长输入鲁棒性不足的问题。


<details>
  <summary>Details</summary>
Motivation: 语音生成模型的快速发展带来了安全风险，如模型侵权和内容滥用。现有水印技术存在计算开销大、训练成本高及对变长输入鲁棒性不足的局限性。

Method: SOLIDO结合了LoRA和参数高效微调，设计了基于深度可分离卷积的水印解码器，并提出了语音驱动的轻量级微调策略。

Result: 实验表明，SOLIDO在2000 bps容量下仍能保持高保真水印语音，对抗常见攻击的平均提取准确率最高达99.20%和98.43%，在抗时间拉伸攻击上优于其他方法近23%。

Conclusion: SOLIDO是一种高效、鲁棒的语音生成水印方法，显著提升了水印容量和抗攻击能力。

Abstract: The accelerated advancement of speech generative models has given rise to
security issues, including model infringement and unauthorized abuse of
content. Although existing generative watermarking techniques have proposed
corresponding solutions, most methods require substantial computational
overhead and training costs. In addition, some methods have limitations in
robustness when handling variable-length inputs. To tackle these challenges, we
propose \textsc{SOLIDO}, a novel generative watermarking method that integrates
parameter-efficient fine-tuning with speech watermarking through low-rank
adaptation (LoRA) for speech diffusion models. Concretely, the watermark
encoder converts the watermark to align with the input of diffusion models. To
achieve precise watermark extraction from variable-length inputs, the watermark
decoder based on depthwise separable convolution is designed for watermark
recovery. To further enhance speech generation performance and watermark
extraction capability, we propose a speech-driven lightweight fine-tuning
strategy, which reduces computational overhead through LoRA. Comprehensive
experiments demonstrate that the proposed method ensures high-fidelity
watermarked speech even at a large capacity of 2000 bps. Furthermore, against
common individual and compound speech attacks, our SOLIDO achieves a maximum
average extraction accuracy of 99.20\% and 98.43\%, respectively. It surpasses
other state-of-the-art methods by nearly 23\% in resisting time-stretching
attacks.

</details>


### [301] [Mining Characteristics of Vulnerable Smart Contracts Across Lifecycle Stages](https://arxiv.org/abs/2504.15063)
*Hongli Peng,Xiaoqi Li,Wenkai Li*

Main category: cs.CR

TL;DR: 论文对智能合约生命周期各阶段的安全问题进行了首次实证研究，提出了七个特征描述，并利用机器学习模型识别不同阶段的漏洞。


<details>
  <summary>Details</summary>
Motivation: 现有解决方案主要关注智能合约代码漏洞，仅覆盖50%的安全事件，亟需更全面的研究。

Method: 研究分析了智能合约生命周期各阶段的安全问题，提出七个特征，并利用五种机器学习模型进行分类。

Result: 分类结果显示，易受攻击的合约在不同阶段表现出独特的交易特征和网络属性。

Conclusion: 研究为智能合约安全提供了更全面的视角，有助于识别和解决各阶段的安全问题。

Abstract: Smart contracts are the cornerstone of decentralized applications and
financial protocols, which extend the application of digital currency
transactions. The applications and financial protocols introduce significant
security challenges, resulting in substantial economic losses. Existing
solutions predominantly focus on code vulnerabilities within smart contracts,
accounting for only 50% of security incidents. Therefore, a more comprehensive
study of security issues related to smart contracts is imperative. The existing
empirical research realizes the static analysis of smart contracts from the
perspective of the lifecycle and gives the corresponding measures for each
stage. However, they lack the characteristic analysis of vulnerabilities in
each stage and the distinction between the vulnerabilities. In this paper, we
present the first empirical study on the security of smart contracts throughout
their lifecycle, including deployment and execution, upgrade, and destruction
stages. It delves into the security issues at each stage and provides at least
seven feature descriptions. Finally, utilizing these seven features, five
machine-learning classification models are used to identify vulnerabilities at
different stages. The classification results reveal that vulnerable contracts
exhibit distinct transaction features and ego network properties at various
stages.

</details>


### [302] [C2RUST-BENCH: A Minimized, Representative Dataset for C-to-Rust Transpilation Evaluation](https://arxiv.org/abs/2504.15144)
*Melih Sirlanci,Carter Yagemann,Zhiqiang Lin*

Main category: cs.CR

TL;DR: 该论文提出了一种方法，从大量C函数中选择代表性样本构建C2RUST-BENCH数据集，用于评估C到Rust的转译。


<details>
  <summary>Details</summary>
Motivation: 内存安全问题持续存在，C到Rust转译是解决方案之一，但缺乏全面评估数据集。

Method: 从15,503个真实世界程序的函数中筛选出2,905个代表性函数，构建C2RUST-BENCH数据集。

Result: C2RUST-BENCH数据集为评估C到Rust转译提供了高效且代表性的样本。

Conclusion: 该方法解决了评估数据集不足的问题，提升了转译框架的评估效率。

Abstract: Despite the effort in vulnerability detection over the last two decades,
memory safety vulnerabilities continue to be a critical problem. Recent reports
suggest that the key solution is to migrate to memory-safe languages. To this
end, C-to-Rust transpilation becomes popular to resolve memory-safety issues in
C programs. Recent works propose C-to-Rust transpilation frameworks; however, a
comprehensive evaluation dataset is missing. Although one solution is to put
together a large enough dataset, this increases the analysis time in automated
frameworks as well as in manual efforts for some cases. In this work, we build
a method to select functions from a large set to construct a minimized yet
representative dataset to evaluate the C-to-Rust transpilation. We propose
C2RUST-BENCH that contains 2,905 functions, which are representative of
C-to-Rust transpilation, selected from 15,503 functions of real-world programs.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [303] [System of Agentic AI for the Discovery of Metal-Organic Frameworks](https://arxiv.org/abs/2504.14110)
*Theo Jaffrelot Inizan,Sherry Yang,Aaron Kaplan,Yen-hsu Lin,Jian Yin,Saber Mirzaei,Mona Abdelgaid,Ali H. Alawadhi,KwangHwan Cho,Zhiling Zheng,Ekin Dogus Cubuk,Christian Borgs,Jennifer T. Chayes,Kristin A. Persson,Omar M. Yaghi*

Main category: cond-mat.mtrl-sci

TL;DR: MOFGen系统通过多智能体AI生成新型MOF结构，验证了其合成可行性，并成功合成了五种AI设计的MOF。


<details>
  <summary>Details</summary>
Motivation: 加速MOF材料发现，解决化学空间探索和合成可行性的挑战。

Method: 结合语言模型、扩散模型、量子力学智能体和合成可行性智能体，生成和优化MOF结构。

Result: 生成数十万新型MOF结构和可合成有机连接体，成功合成五种AI设计的MOF。

Conclusion: MOFGen为自动化可合成材料发现迈出重要一步。

Abstract: Generative models and machine learning promise accelerated material discovery
in MOFs for CO2 capture and water harvesting but face significant challenges
navigating vast chemical spaces while ensuring synthetizability. Here, we
present MOFGen, a system of Agentic AI comprising interconnected agents: a
large language model that proposes novel MOF compositions, a diffusion model
that generates crystal structures, quantum mechanical agents that optimize and
filter candidates, and synthetic-feasibility agents guided by expert rules and
machine learning. Trained on all experimentally reported MOFs and computational
databases, MOFGen generated hundreds of thousands of novel MOF structures and
synthesizable organic linkers. Our methodology was validated through
high-throughput experiments and the successful synthesis of five "AI-dreamt"
MOFs, representing a major step toward automated synthesizable material
discovery.

</details>


### [304] [Machine learning enhanced atom probe tomography analysis: a snapshot review](https://arxiv.org/abs/2504.14378)
*Yue Li,Ye Wei,Alaukik Saxena,Markus Kühbach,Christoph Freysoldt,Baptiste Gault*

Main category: cond-mat.mtrl-sci

TL;DR: 本文回顾了原子探针断层扫描（APT）数据分析和机器学习（ML）应用的快速发展，强调ML在提升效率、可重复性和统计稳健性方面的潜力。


<details>
  <summary>Details</summary>
Motivation: APT数据的分析长期以来依赖用户经验，导致偏差和效率低下，亟需标准化和自动化方法。

Method: 综述了APT数据特性、相关ML算法及其在APT中的应用，探讨了ML如何超越人类能力发现新材料机制。

Result: ML方法显著提升了APT数据分析的用户独立性、效率和统计稳健性。

Conclusion: 未来研究方向应集中在进一步优化ML算法和推广FAIR数据原则的应用。

Abstract: Atom probe tomography (APT) is a burgeoning characterization technique that
provides compositional mapping of materials in three-dimensions at near-atomic
scale. Since its significant expansion in the past 30 years, we estimate that
one million APT datasets have been collected, each containing millions to
billions of individual ions. Their analysis and the extraction of
microstructural information has largely relied upon individual users whose
varied level of expertise causes clear and documented bias. Current practices
hinder efficient data processing, and make challenging standardization and the
deployment of data analysis workflows that would be compliant with FAIR data
principles. Over the past decade, building upon the long-standing expertise of
the APT community in the development of advanced data processing or data mining
techniques, there has been a surge of novel machine learning (ML) approaches
aiming for user-independence, and that are efficient, reproducible, and robust
from a statistics perspective. Here, we provide a snapshot review of this
rapidly evolving field. We begin with a brief introduction to APT and the
nature of the APT data. This is followed by an overview of relevant ML
algorithms and a comprehensive review of their applications to APT. We also
discuss how ML can enable discoveries beyond human capability, offering new
insights into the mechanisms within materials. Finally, we provide guidance for
future directions in this domain.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [305] [HF4Rec: Human-Like Feedback-Driven Optimization Framework for Explainable Recommendation](https://arxiv.org/abs/2504.14147)
*Jiakai Tang,Jingsen Zhang,Zihang Tian,Xueyang Feng,Lei Wang,Xu Chen*

Main category: cs.IR

TL;DR: 论文提出了一种基于人类反馈的优化框架，利用大语言模型（LLMs）模拟人类反馈，通过动态交互优化机制提升推荐解释的质量，同时降低人工成本。


<details>
  <summary>Details</summary>
Motivation: 现有可解释推荐方法因依赖稀疏交互数据的传统监督学习范式，无法为生成解释提供有效反馈信号。

Method: 提出动态交互优化框架，利用LLMs模拟人类反馈，引入人类诱导的定制化奖励评分方法，并结合帕累托优化解决多视角解释质量冲突问题。

Result: 在四个数据集上的实验证明了该方法的优越性。

Conclusion: 该框架通过高效的数据利用和模型泛化能力，显著提升了推荐解释的性能。

Abstract: Recent advancements in explainable recommendation have greatly bolstered user
experience by elucidating the decision-making rationale. However, the existing
methods actually fail to provide effective feedback signals for potentially
better or worse generated explanations due to their reliance on traditional
supervised learning paradigms in sparse interaction data. To address these
issues, we propose a novel human-like feedback-driven optimization framework.
This framework employs a dynamic interactive optimization mechanism for
achieving human-centered explainable requirements without incurring high labor
costs. Specifically, we propose to utilize large language models (LLMs) as
human simulators to predict human-like feedback for guiding the learning
process. To enable the LLMs to deeply understand the task essence and meet
user's diverse personalized requirements, we introduce a human-induced
customized reward scoring method, which helps stimulate the language
understanding and logical reasoning capabilities of LLMs. Furthermore,
considering the potential conflicts between different perspectives of
explanation quality, we introduce a principled Pareto optimization that
transforms the multi-perspective quality enhancement task into a
multi-objective optimization problem for improving explanation performance. At
last, to achieve efficient model training, we design an off-policy optimization
pipeline. By incorporating a replay buffer and addressing the data distribution
biases, we can effectively improve data utilization and enhance model
generality. Extensive experiments on four datasets demonstrate the superiority
of our approach.

</details>


### [306] [CPR: Leveraging LLMs for Topic and Phrase Suggestion to Facilitate Comprehensive Product Reviews](https://arxiv.org/abs/2504.13993)
*Ekta Gujral,Apurva Sinha,Lishi Ji,Bijayani Sanghamitra Mishra*

Main category: cs.IR

TL;DR: 论文提出CPR方法，利用LLMs和主题建模引导用户撰写全面产品评论，显著提升评论质量。


<details>
  <summary>Details</summary>
Motivation: 现有研究未系统解决如何鼓励用户撰写包含情感和产品特征分析的全面评论。

Method: 三阶段方法：提供产品特定评分术语、生成针对性短语建议、整合用户文本并通过主题建模确保覆盖关键点。

Result: CPR有效识别相关产品术语并提供情感一致的短语建议，BLEU分数提升12.3%。

Conclusion: CPR方法成功提升评论质量，未来可进一步扩展研究。

Abstract: Consumers often heavily rely on online product reviews, analyzing both
quantitative ratings and textual descriptions to assess product quality.
However, existing research hasn't adequately addressed how to systematically
encourage the creation of comprehensive reviews that capture both customers
sentiment and detailed product feature analysis. This paper presents CPR, a
novel methodology that leverages the power of Large Language Models (LLMs) and
Topic Modeling to guide users in crafting insightful and well-rounded reviews.
Our approach employs a three-stage process: first, we present users with
product-specific terms for rating; second, we generate targeted phrase
suggestions based on these ratings; and third, we integrate user-written text
through topic modeling, ensuring all key aspects are addressed. We evaluate CPR
using text-to-text LLMs, comparing its performance against real-world customer
reviews from Walmart. Our results demonstrate that CPR effectively identifies
relevant product terms, even for new products lacking prior reviews, and
provides sentiment-aligned phrase suggestions, saving users time and enhancing
reviews quality. Quantitative analysis reveals a 12.3% improvement in BLEU
score over baseline methods, further supported by manual evaluation of
generated phrases. We conclude by discussing potential extensions and future
research directions.

</details>


### [307] [The Great Nugget Recall: Automating Fact Extraction and RAG Evaluation with Large Language Models](https://arxiv.org/abs/2504.15068)
*Ronak Pradeep,Nandan Thakur,Shivani Upadhyay,Daniel Campos,Nick Craswell,Jimmy Lin*

Main category: cs.IR

TL;DR: 提出了一种基于LLMs的自动评估框架AutoNuggetizer，用于评估检索增强生成（RAG）系统，验证了其与人工评估的一致性。


<details>
  <summary>Details</summary>
Motivation: RAG系统的评估是当前研究的瓶颈，需要一种自动化的方法以推动进展。

Method: 采用TREC QA Track的nugget评估方法，通过LLMs自动生成和分配nuggets，并与人工评估对比。

Result: 自动评估与人工评估在运行级别上表现出一致性，尤其在独立自动化组件时效果更佳。

Conclusion: 该框架在质量和效率间提供了平衡，但需进一步研究以优化每主题一致性。

Abstract: Large Language Models (LLMs) have significantly enhanced the capabilities of
information access systems, especially with retrieval-augmented generation
(RAG). Nevertheless, the evaluation of RAG systems remains a barrier to
continued progress, a challenge we tackle in this work by proposing an
automatic evaluation framework that is validated against human annotations. We
believe that the nugget evaluation methodology provides a solid foundation for
evaluating RAG systems. This approach, originally developed for the TREC
Question Answering (QA) Track in 2003, evaluates systems based on atomic facts
that should be present in good answers. Our efforts focus on "refactoring" this
methodology, where we describe the AutoNuggetizer framework that specifically
applies LLMs to both automatically create nuggets and automatically assign
nuggets to system answers. In the context of the TREC 2024 RAG Track, we
calibrate a fully automatic approach against strategies where nuggets are
created manually or semi-manually by human assessors and then assigned manually
to system answers. Based on results from a community-wide evaluation, we
observe strong agreement at the run level between scores derived from fully
automatic nugget evaluation and human-based variants. The agreement is stronger
when individual framework components such as nugget assignment are automated
independently. This suggests that our evaluation framework provides tradeoffs
between effort and quality that can be used to guide the development of future
RAG systems. However, further research is necessary to refine our approach,
particularly in establishing robust per-topic agreement to diagnose system
failures effectively.

</details>


### [308] [KGMEL: Knowledge Graph-Enhanced Multimodal Entity Linking](https://arxiv.org/abs/2504.15135)
*Juyeon Kim,Geon Lee,Taeuk Kim,Kijung Shin*

Main category: cs.IR

TL;DR: KGMEL是一种利用知识图谱三元组增强多模态实体链接的新框架，通过生成、检索和重排名三阶段显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有MEL方法忽略了知识图谱的结构信息，KGMEL旨在利用这些信息减少歧义并提高准确性。

Method: KGMEL分三阶段：生成高质量三元组、学习联合表示检索候选实体、重排名选择最佳匹配实体。

Result: 实验表明KGMEL在基准数据集上优于现有方法。

Conclusion: KGMEL通过整合知识图谱三元组显著提升了多模态实体链接的性能。

Abstract: Entity linking (EL) aligns textual mentions with their corresponding entities
in a knowledge base, facilitating various applications such as semantic search
and question answering. Recent advances in multimodal entity linking (MEL) have
shown that combining text and images can reduce ambiguity and improve alignment
accuracy. However, most existing MEL methods overlook the rich structural
information available in the form of knowledge-graph (KG) triples. In this
paper, we propose KGMEL, a novel framework that leverages KG triples to enhance
MEL. Specifically, it operates in three stages: (1) Generation: Produces
high-quality triples for each mention by employing vision-language models based
on its text and images. (2) Retrieval: Learns joint mention-entity
representations, via contrastive learning, that integrate text, images, and
(generated or KG) triples to retrieve candidate entities for each mention. (3)
Reranking: Refines the KG triples of the candidate entities and employs large
language models to identify the best-matching entity for the mention. Extensive
experiments on benchmark datasets demonstrate that KGMEL outperforms existing
methods. Our code and datasets are available at:
https://github.com/juyeonnn/KGMEL.

</details>


### [309] [Personalized News Recommendation with Multi-granularity Candidate-aware User Modeling](https://arxiv.org/abs/2504.14130)
*Qiang Li,Xinze Lin,Shenghao Lv,Faliang Huang,Xiangju Li*

Main category: cs.IR

TL;DR: 本文提出了一种多粒度候选感知的用户建模框架，用于个性化新闻推荐，通过多粒度特征捕捉用户兴趣的多样性。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常基于单一用户画像，难以全面捕捉用户兴趣的多样性，且忽略了候选新闻与用户兴趣的多粒度关联。

Method: 框架包含候选新闻编码和用户建模两部分，利用文本和知识增强的实体信息提取器，结合词级、实体级和新闻级候选感知机制。

Result: 在真实数据集上的实验表明，该模型显著优于基线模型。

Conclusion: 多粒度候选感知框架能更全面地表示用户兴趣，提升推荐效果。

Abstract: Matching candidate news with user interests is crucial for personalized news
recommendations. Most existing methods can represent a user's reading interests
through a single profile based on clicked news, which may not fully capture the
diversity of user interests. Although some approaches incorporate candidate
news or topic information, they remain insufficient because they neglect the
multi-granularity relatedness between candidate news and user interests. To
address this, this study proposed a multi-granularity candidate-aware user
modeling framework that integrated user interest features across various levels
of granularity. It consisted of two main components: candidate news encoding
and user modeling. A news textual information extractor and a
knowledge-enhanced entity information extractor can capture candidate news
features, and word-level, entity-level, and news-level candidate-aware
mechanisms can provide a comprehensive representation of user interests.
Extensive experiments on a real-world dataset demonstrated that the proposed
model could significantly outperform baseline models.

</details>


### [310] [FinSage: A Multi-aspect RAG System for Financial Filings Question Answering](https://arxiv.org/abs/2504.14493)
*Xinyu Wang,Jijun Chi,Zhenghan Tai,Tung Sum Thomas Kwok,Muzhi Li,Zhuhong Li,Hailin He,Yuchen Hua,Peng Lu,Suyuchen Wang,Yihong Wu,Jerry Huang,Ling Zhou*

Main category: cs.IR

TL;DR: FinSage框架通过多模态RAG系统解决金融文档中的合规性问题，显著提升信息提取准确率。


<details>
  <summary>Details</summary>
Motivation: 金融领域需要处理多模态数据和动态监管标准，现有方法难以满足准确性和合规性需求。

Method: FinSage包含多模态预处理、多路径检索系统和领域专用重排序模块。

Result: 实验显示FinSage召回率达92.51%，准确率提升24.06%，已成功部署服务1200多人。

Conclusion: FinSage为金融合规分析提供了高效解决方案，并验证了其实际应用价值。

Abstract: Leveraging large language models in real-world settings often entails a need
to utilize domain-specific data and tools in order to follow the complex
regulations that need to be followed for acceptable use. Within financial
sectors, modern enterprises increasingly rely on Retrieval-Augmented Generation
(RAG) systems to address complex compliance requirements in financial document
workflows. However, existing solutions struggle to account for the inherent
heterogeneity of data (e.g., text, tables, diagrams) and evolving nature of
regulatory standards used in financial filings, leading to compromised accuracy
in critical information extraction. We propose the FinSage framework as a
solution, utilizing a multi-aspect RAG framework tailored for regulatory
compliance analysis in multi-modal financial documents. FinSage introduces
three innovative components: (1) a multi-modal pre-processing pipeline that
unifies diverse data formats and generates chunk-level metadata summaries, (2)
a multi-path sparse-dense retrieval system augmented with query expansion
(HyDE) and metadata-aware semantic search, and (3) a domain-specialized
re-ranking module fine-tuned via Direct Preference Optimization (DPO) to
prioritize compliance-critical content. Extensive experiments demonstrate that
FinSage achieves an impressive recall of 92.51% on 75 expert-curated questions
derived from surpasses the best baseline method on the FinanceBench question
answering datasets by 24.06% in accuracy. Moreover, FinSage has been
successfully deployed as financial question-answering agent in online meetings,
where it has already served more than 1,200 people.

</details>


### [311] [Exploring $\ell_0$ Sparsification for Inference-free Sparse Retrievers](https://arxiv.org/abs/2504.14839)
*Xinjie Shen,Zhichao Geng,Yang Yang*

Main category: cs.IR

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: With increasing demands for efficiency, information retrieval has developed a
branch of sparse retrieval, further advancing towards inference-free retrieval
where the documents are encoded during indexing time and there is no
model-inference for queries. Existing sparse retrieval models rely on FLOPS
regularization for sparsification, while this mechanism was originally designed
for Siamese encoders, it is considered to be suboptimal in inference-free
scenarios which is asymmetric. Previous attempts to adapt FLOPS for
inference-free scenarios have been limited to rule-based methods, leaving the
potential of sparsification approaches for inference-free retrieval models
largely unexplored. In this paper, we explore $\ell_0$ inspired sparsification
manner for inference-free retrievers. Through comprehensive out-of-domain
evaluation on the BEIR benchmark, our method achieves state-of-the-art
performance among inference-free sparse retrieval models and is comparable to
leading Siamese sparse retrieval models. Furthermore, we provide insights into
the trade-off between retrieval effectiveness and computational efficiency,
demonstrating practical value for real-world applications.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [312] [VLM as Policy: Common-Law Content Moderation Framework for Short Video Platform](https://arxiv.org/abs/2504.14904)
*Xingyu Lu,Tianke Zhang,Chang Meng,Xiaobei Wang,Jinpeng Wang,YiFan Zhang,Shisong Tang,Changyi Liu,Haojie Ding,Kaiyu Jiang,Kaiyu Tang,Bin Wen,Hai-Tao Zheng,Fan Yang,Tingting Gao,Di Zhang,Kun Gai*

Main category: cs.SI

TL;DR: 论文提出KuaiMod框架，解决短视频平台内容审核的局限性，结合VLM和CoT推理，提升审核准确性和动态更新能力。


<details>
  <summary>Details</summary>
Motivation: 短视频平台内容审核存在人工偏见、自动化方法准确性不足及法规更新滞后等问题，亟需高效解决方案。

Method: 构建首个SVP内容审核基准，提出KuaiMod框架，包含数据构建、离线适配和在线部署三部分，利用VLM和CoT推理。

Result: KuaiMod在基准测试中表现最佳，用户举报率降低20%，DAU和AUT显著提升。

Conclusion: KuaiMod有效解决了内容审核的挑战，具有实际应用价值，并开源了基准数据集。

Abstract: Exponentially growing short video platforms (SVPs) face significant
challenges in moderating content detrimental to users' mental health,
particularly for minors. The dissemination of such content on SVPs can lead to
catastrophic societal consequences. Although substantial efforts have been
dedicated to moderating such content, existing methods suffer from critical
limitations: (1) Manual review is prone to human bias and incurs high
operational costs. (2) Automated methods, though efficient, lack nuanced
content understanding, resulting in lower accuracy. (3) Industrial moderation
regulations struggle to adapt to rapidly evolving trends due to long update
cycles. In this paper, we annotate the first SVP content moderation benchmark
with authentic user/reviewer feedback to fill the absence of benchmark in this
field. Then we evaluate various methods on the benchmark to verify the
existence of the aforementioned limitations. We further propose our common-law
content moderation framework named KuaiMod to address these challenges. KuaiMod
consists of three components: training data construction, offline adaptation,
and online deployment & refinement. Leveraging large vision language model
(VLM) and Chain-of-Thought (CoT) reasoning, KuaiMod adequately models video
toxicity based on sparse user feedback and fosters dynamic moderation policy
with rapid update speed and high accuracy. Offline experiments and large-scale
online A/B test demonstrates the superiority of KuaiMod: KuaiMod achieves the
best moderation performance on our benchmark. The deployment of KuaiMod reduces
the user reporting rate by 20% and its application in video recommendation
increases both Daily Active User (DAU) and APP Usage Time (AUT) on several
Kuaishou scenarios. We have open-sourced our benchmark at
https://kuaimod.github.io.

</details>


### [313] [Rhythm of Opinion: A Hawkes-Graph Framework for Dynamic Propagation Analysis](https://arxiv.org/abs/2504.15072)
*Yulong Li,Zhixiang Lu,Feilong Tang,Simin Lai,Ming Hu,Yuxuan Zhang,Haochen Xue,Zhaodong Wu,Imran Razzak,Qingxia Li,Jionglong Su*

Main category: cs.SI

TL;DR: 提出了一种结合多维霍克斯过程与图神经网络的方法，用于建模社交网络中意见传播的动态，并引入新数据集VISTA以支持研究。


<details>
  <summary>Details</summary>
Motivation: 传统模型难以有效捕捉社交媒体中公共意见的复杂动态，需要新的方法来建模多维度互动和层级关系。

Method: 整合多维霍克斯过程与图神经网络，建模社交网络中节点间的意见传播动态，同时考虑评论的层级关系。

Result: 提出新数据集VISTA，包含多领域数据，结合方法提供了强解释性，为未来研究奠定基础。

Conclusion: 该方法能有效建模复杂意见传播动态，数据集VISTA为相关研究提供了高质量资源。

Abstract: The rapid development of social media has significantly reshaped the dynamics
of public opinion, resulting in complex interactions that traditional models
fail to effectively capture. To address this challenge, we propose an
innovative approach that integrates multi-dimensional Hawkes processes with
Graph Neural Network, modeling opinion propagation dynamics among nodes in a
social network while considering the intricate hierarchical relationships
between comments. The extended multi-dimensional Hawkes process captures the
hierarchical structure, multi-dimensional interactions, and mutual influences
across different topics, forming a complex propagation network. Moreover,
recognizing the lack of high-quality datasets capable of comprehensively
capturing the evolution of public opinion dynamics, we introduce a new dataset,
VISTA. It includes 159 trending topics, corresponding to 47,207 posts, 327,015
second-level comments, and 29,578 third-level comments, covering diverse
domains such as politics, entertainment, sports, health, and medicine. The
dataset is annotated with detailed sentiment labels across 11 categories and
clearly defined hierarchical relationships. When combined with our method, it
offers strong interpretability by linking sentiment propagation to the comment
hierarchy and temporal evolution. Our approach provides a robust baseline for
future research.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [314] [Breaking the Diffraction Barrier for Passive Sources: Parameter-Decoupled Superresolution Assisted by Physics-Informed Machine Learning](https://arxiv.org/abs/2504.14156)
*Abdelali Sajia,Bilal Benzimoun,Pawan Khatiwada,Guogan Zhao,Xiao-Feng Qian*

Main category: physics.optics

TL;DR: 提出了一种参数解耦的超分辨率框架，用于估计被动两点源的亚波长间距，无需源先验知识或控制。


<details>
  <summary>Details</summary>
Motivation: 解决传统方法中需要估计多个挑战性参数（如部分相干性、亮度不平衡、随机相对相位和光子统计）的问题，并应对实际成像中的噪声和偏差。

Method: 结合物理理论指导的机器学习模型，处理背景噪声、光子损失和中心/方向偏差等实际问题。

Result: 在实验生成的图像上实现了14倍于衍射极限的分辨率（约13.5纳米），保真度>82%，性能媲美主动控制源的最先进技术。

Conclusion: 该方法对源参数变异性和源无关噪声的鲁棒性，使其在无法控制源的实际场景（如天体成像、活细胞显微镜和量子计量）中具有广泛应用潜力。

Abstract: We present a parameter-decoupled superresolution framework for estimating
sub-wavelength separations of passive two-point sources without requiring prior
knowledge or control of the source. Our theoretical foundation circumvents the
need to estimate multiple challenging parameters such as partial coherence,
brightness imbalance, random relative phase, and photon statistics. A
physics-informed machine learning (ML) model (trained with a standard desktop
workstation), synergistically integrating this theory, further addresses
practical imperfections including background noise, photon loss, and
centroid/orientation misalignment. The integrated parameter-decoupling
superresolution method achieves resolution 14 and more times below the
diffraction limit (corresponding to ~ 13.5 nm in optical microscopy) on
experimentally generated realistic images with >82% fidelity, performance
rivaling state-of-the-art techniques for actively controllable sources.
Critically, our method's robustness against source parameter variability and
source-independent noises enables potential applications in realistic scenarios
where source control is infeasible, such as astrophysical imaging, live-cell
microscopy, and quantum metrology. This work bridges a critical gap between
theoretical superresolution limits and practical implementations for passive
systems.

</details>


### [315] [DeepPD: Joint Phase and Object Estimation from Phase Diversity with Neural Calibration of a Deformable Mirror](https://arxiv.org/abs/2504.14157)
*Magdalena C. Schneider,Courtney Johnson,Cedric Allier,Larissa Heinrich,Diane Adjavon,Joren Husic,Patrick La Rivière,Stephan Saalfeld,Hari Shroff*

Main category: physics.optics

TL;DR: DeepPD是一种基于深度学习的框架，通过结合物体和波前的神经表示以及变形镜的模型，仅需五张图像即可联合估计物体和相位，提高了鲁棒性和重建质量。


<details>
  <summary>Details</summary>
Motivation: 样本引起的像差和光学缺陷限制了荧光显微镜的分辨率，现有方法依赖于Zernike模式、大量图像或精确的校准，效果不理想。

Method: DeepPD结合了物体和波前的神经表示，并利用变形镜的模型，仅需五张图像即可联合估计物体和相位。

Result: DeepPD在严重像差下仍优于现有方法，提高了重建质量和鲁棒性，并在校准目标和生物样本上验证了性能。

Conclusion: DeepPD为荧光显微镜提供了一种高效、鲁棒的相位多样性重建方法，显著提升了分辨率和重建效果。

Abstract: Sample-induced aberrations and optical imperfections limit the resolution of
fluorescence microscopy. Phase diversity is a powerful technique that leverages
complementary phase information in sequentially acquired images with
deliberately introduced aberrations--the phase diversities--to enable phase and
object reconstruction and restore diffraction-limited resolution. These phase
diversities are typically introduced into the optical path via a deformable
mirror. Existing phase-diversity-based methods are limited to Zernike modes,
require large numbers of diversity images, or depend on accurate mirror
calibration--which are all suboptimal. We present DeepPD, a deep learning-based
framework that combines neural representations of the object and wavefront with
a learned model of the deformable mirror to jointly estimate both object and
phase from only five images. DeepPD improves robustness and reconstruction
quality over previous approaches, even under severe aberrations. We demonstrate
its performance on calibration targets and biological samples, including
immunolabeled myosin in fixed PtK2 cells.

</details>


### [316] [Beyond Terabit/s Integrated Neuromorphic Photonic Processor for DSP-Free Optical Interconnects](https://arxiv.org/abs/2504.15044)
*Benshan Wang,Qiarong Xiao,Tengji Xu,Li Fan,Shaojie Liu,Jianji Dong,Junwen Zhang,Chaoran Huang*

Main category: physics.optics

TL;DR: 提出了一种基于深度储层计算的神经形态光学信号处理器（OSP），用于高性能计算中的光学互连，显著提升了速度、延迟和能效。


<details>
  <summary>Details</summary>
Motivation: 生成式AI的快速发展对高性能计算提出了极高要求，传统电学和光学互连技术难以满足低延迟、高能效的需求。

Method: 采用集成神经形态光学信号处理器（OSP），利用深度储层计算实现无DSP的全光学实时处理。

Result: 实验显示，OSP在5公里C波段光纤上实现了100 Gbaud PAM4每通道、1.6 Tbit/s的数据中心互连，延迟和能耗分别降低了四个和三个数量级。

Conclusion: OSP为下一代AI基础设施提供了高度可扩展、高能效和高速的解决方案。

Abstract: The rapid expansion of generative AI drives unprecedented demands for
high-performance computing. Training large-scale AI models now requires vast
interconnected GPU clusters across multiple data centers. Multi-scale AI
training and inference demand uniform, ultra-low latency, and energy-efficient
links to enable massive GPUs to function as a single cohesive unit. However,
traditional electrical and optical interconnects, relying on conventional
digital signal processors (DSPs) for signal distortion compensation,
increasingly fail to meet these stringent requirements. To overcome these
limitations, we present an integrated neuromorphic optical signal processor
(OSP) that leverages deep reservoir computing and achieves DSP-free,
all-optical, real-time processing. Experimentally, our OSP achieves a 100 Gbaud
PAM4 per lane, 1.6 Tbit/s data center interconnect over a 5 km optical fiber in
the C-band (equivalent to over 80 km in the O-band), far exceeding the reach of
state-of-the-art DSP solutions, which are fundamentally constrained by
chromatic dispersion in IMDD systems. Simultaneously, it reduces processing
latency by four orders of magnitude and energy consumption by three orders of
magnitude. Unlike DSPs, which introduce increased latency at high data rates,
our OSP maintains consistent, ultra-low latency regardless of data rate
scaling, making it ideal for future optical interconnects. Moreover, the OSP
retains full optical field information for better impairment compensation and
adapts to various modulation formats, data rates, and wavelengths. Fabricated
using a mature silicon photonic process, the OSP can be monolithically
integrated with silicon photonic transceivers, enhancing the compactness and
reliability of all-optical interconnects. This research provides a highly
scalable, energy-efficient, and high-speed solution, paving the way for
next-generation AI infrastructure.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [317] [Planet as a Brain: Towards Internet of AgentSites based on AIOS Server](https://arxiv.org/abs/2504.14411)
*Xiang Zhang,Yongfeng Zhang*

Main category: cs.NI

TL;DR: 论文介绍了AIOS Server，一个支持全球规模去中心化AI代理协作的运行时框架，并展示了首个实际部署的AgentSites互联网（AIOS-IoA）。


<details>
  <summary>Details</summary>
Motivation: 互联网正从'网站互联网'向'代理站点互联网'转型，需要基础设施支持AI代理的开发、部署和执行。

Method: 提出AIOS Server框架，利用MCP和JSON-RPC协议实现代理间或人机交互，支持去中心化协调。

Result: 成功部署AIOS-IoA，包括AgentHub和AgentChat，并实现基于DHT和Gossip协议的代理发现机制。

Conclusion: AIOS Server为构建代理站点互联网提供了实践基础，使自主代理成为网络的一等公民。

Abstract: The internet is undergoing a historical transformation from the "Internet of
Websites" to the "Internet of AgentSites." While traditional Websites served as
the foundation for information hosting and dissemination, a new frontier is
emerging where AgentSites serve as the hubs of the internet, where each
AgentSite hosts one or more AI agents that receive tasks, address them, and
deliver actionable solutions, marking a significant shift in the digital
landscape and representing the next generation of online ecosystems. Under this
vision, AIOS, the AI Agent Operating System, serves as the server for the
development, deployment and execution of AI agents, which is a fundamental
infrastructure for the Internet of Agentsites.
  In this paper, we introduce AIOS Server, a runtime framework to host agents
and enable global-scale collaboration among decentralized agents. AIOS Server
provides a communication protocol leveraging the Model Context Protocol (MCP)
and JSON-RPC to enable agent-agent or human-agent interactions. Each AIOS node
operates as a server to host and execute agents, while supporting peer-to-peer
coordination without reliance on centralized orchestration. Based on AIOS
Server, we further present the world's first practically deployed Internet of
Agentsites (AIOS-IoA), including AgentHub for agent registration and discovery
and AgentChat for interactive communication, at https://planet.aios.foundation.
The agent discovery mechanism based on Distributed Hash Tables (DHT) and a
Gossip protocol serves as the search engine for the internet of agentsites.
This work provides a practical foundation for building the Internet of
Agentsites-a new paradigm where autonomous agents become first-class citizens
of the web. The implementation is available at
https://github.com/agiresearch/AIOS.Server and will be integrated into the AIOS
main branch at https://github.com/agiresearch/AIOS.

</details>


### [318] [Uncovering Issues in the Radio Access Network by Looking at the Neighbors](https://arxiv.org/abs/2504.14686)
*José Suárez-Varela,Andra Lutu*

Main category: cs.NI

TL;DR: c-ANEMON是一种基于图神经网络（GNN）的上下文异常检测工具，用于无线接入网络（RAN），通过分析单个小区与其局部邻域的关系来检测异常。


<details>
  <summary>Details</summary>
Motivation: 移动网络运营商（MNOs）需要处理多代无线网络（2G-5G）中大量小区的复杂性，现有监控系统难以独立于外部移动性因素检测异常。

Method: c-ANEMON利用GNN捕捉时空变化，分析小区行为与邻域关系，专注于网络问题相关的异常（如配置错误、设备故障）。

Result: 在真实数据（7,890个小区；3个月）上验证，GNN模型能泛化到未见区域，45.95%的长期异常（6+小时）需人工干预。

Conclusion: c-ANEMON展示了GNN在RAN异常检测中的潜力，支持跨广泛部署区域的单模型应用，并识别出需操作团队关注的异常类别。

Abstract: Mobile network operators (MNOs) manage Radio Access Networks (RANs) with
massive amounts of cells over multiple radio generations (2G-5G). To handle
such complexity, operations teams rely on monitoring systems, including anomaly
detection tools that identify unexpected behaviors. In this paper, we present
c-ANEMON, a Contextual ANomaly dEtection MONitor for the RAN based on Graph
Neural Networks (GNNs). Our solution captures spatio-temporal variations by
analyzing the behavior of individual cells in relation to their local
neighborhoods, enabling the detection of anomalies that are independent of
external mobility factors. This, in turn, allows focusing on anomalies
associated with network issues (e.g., misconfigurations, equipment failures).
We evaluate c-ANEMON using real-world data from a large European metropolitan
area (7,890 cells; 3 months). First, we show that the GNN model within our
solution generalizes effectively to cells from previously unseen areas,
suggesting the possibility of using a single model across extensive deployment
regions. Then, we analyze the anomalies detected by c-ANEMON through manual
inspection and define several categories of long-lasting anomalies (6+ hours).
Notably, 45.95% of these anomalies fall into a category that is more likely to
require intervention by operations teams.

</details>


### [319] [Video QoE Metrics from Encrypted Traffic: Application-agnostic Methodology](https://arxiv.org/abs/2504.14720)
*Tamir Berger,Jonathan Sterenson,Raz Birman,Ofer Hadar*

Main category: cs.NI

TL;DR: 论文提出了一种独立于应用程序的方法，通过加密流量估计视频通话的QoE指标，解决了网络运营商因加密流量无法获取终端设备QoE的问题。


<details>
  <summary>Details</summary>
Motivation: 网络运营商因加密流量无法直接获取终端设备的QoE指标，而现有方法依赖于特定应用程序提供的QoE数据，限制了广泛适用性。

Method: 提出了一种独立于应用程序的QoE估计方法，通过加密流量获取关键视频QoE指标，并利用机器学习模型进行预测。

Result: 在多样化数据集上验证，FPS预测准确率达85.2%（误差±2 FPS），PIQE质量分类准确率达90.2%。

Conclusion: 该方法具有广泛适用性，可应用于多种专有视频通话应用，为网络运营商提供了有效的QoE评估工具。

Abstract: Instant Messaging-Based Video Call Applications (IMVCAs) and Video
Conferencing Applications (VCAs) have become integral to modern communication.
Ensuring a high Quality of Experience (QoE) for users in this context is
critical for network operators, as network conditions significantly impact user
QoE. However, network operators lack access to end-device QoE metrics due to
encrypted traffic. Existing solutions estimate QoE metrics from encrypted
traffic traversing the network, with the most advanced approaches leveraging
machine learning models. Subsequently, the need for ground truth QoE metrics
for training and validation poses a challenge, as not all video applications
provide these metrics. To address this challenge, we propose an
application-agnostic approach for objective QoE estimation from encrypted
traffic. Independent of the video application, we obtained key video QoE
metrics, enabling broad applicability to various proprietary IMVCAs and VCAs.
To validate our solution, we created a diverse dataset from WhatsApp video
sessions under various network conditions, comprising 25,680 seconds of traffic
data and QoE metrics. Our evaluation shows high performance across the entire
dataset, with 85.2% accuracy for FPS predictions within an error margin of two
FPS, and 90.2% accuracy for PIQE-based quality rating classification.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [320] [Predicting fermionic densities using a Projected Quantum Kernel method](https://arxiv.org/abs/2504.14002)
*Francesco Perciavalle,Francesco Plastina,Michele Pisarra,Nicola Lo Gullo*

Main category: quant-ph

TL;DR: 论文提出了一种基于投影量子核方法的支持向量回归器，用于预测一维费米子系统的密度结构，性能优于经典线性核方法。


<details>
  <summary>Details</summary>
Motivation: 研究量子化学和量子物质中一维费米子系统的密度结构预测问题，探索量子计算方法的优势。

Method: 使用基于量子储层可观测量的投影量子核方法构建支持向量回归器，训练和测试数据通过密度泛函理论生成。

Result: 在足够长的测量时间下，该方法优于经典线性核方法，并与径向基函数方法竞争。

Conclusion: 该方法在预测一维费米子系统密度结构方面具有潜力，展现了量子计算方法的优势。

Abstract: We use a support vector regressor based on a projected quantum kernel method
to predict the density structure of 1D fermionic systems of interest in quantum
chemistry and quantum matter. The kernel is built on with the observables of a
quantum reservoir implementable with interacting Rydberg atoms. Training and
test data of the fermionic system are generated using a Density Functional
Theory approach. We test the performance of the method for several Hamiltonian
parameters, finding a general common behavior of the error as a function of
measurement time. At sufficiently large measurement times, we find that the
method outperforms the classical linear kernel method and can be competitive
with the radial basis function method.

</details>


### [321] [Guess, SWAP, Repeat : Capturing Quantum Snapshots in Classical Memory](https://arxiv.org/abs/2504.14459)
*Debarshi Kundu,Avimita Chatterjee,Swaroop Ghosh*

Main category: quant-ph

TL;DR: 提出了一种无需直接测量即可观察量子态的新技术，支持量子态的重用和动态存储。


<details>
  <summary>Details</summary>
Motivation: 解决量子系统中因不可克隆定理和破坏性测量导致的调试、内省和持久内存困难。

Method: 采用硬件无关的机器学习框架，通过保真度估计（如SWAP测试）和梯度/无梯度优化策略重构量子态。

Result: 在IBM量子硬件上实现了高保真度（约1.0）重构，仿真中平均保真度达0.999。

Conclusion: 为量子非易失性内存提供了可行路径，支持量子信息的长期存储和重用。

Abstract: We introduce a novel technique that enables observation of quantum states
without direct measurement, preserving them for reuse. Our method allows
multiple quantum states to be observed at different points within a single
circuit, one at a time, and saved into classical memory without destruction.
These saved states can be accessed on demand by downstream applications,
introducing a dynamic and programmable notion of quantum memory that supports
modular, non-destructive quantum workflows. We propose a hardware-agnostic,
machine learning-driven framework to capture non-destructive estimates, or
"snapshots," of quantum states at arbitrary points within a circuit, enabling
classical storage and later reconstruction, similar to memory operations in
classical computing. This capability is essential for debugging, introspection,
and persistent memory in quantum systems, yet remains difficult due to the
no-cloning theorem and destructive measurements. Our guess-and-check approach
uses fidelity estimation via the SWAP test to guide state reconstruction. We
explore both gradient-based deep neural networks and gradient-free evolutionary
strategies to estimate quantum states using only fidelity as the learning
signal. We demonstrate a key component of our framework on IBM quantum
hardware, achieving high-fidelity (approximately 1.0) reconstructions for
Hadamard and other known states. In simulation, our models achieve an average
fidelity of 0.999 across 100 random quantum states. This provides a pathway
toward non-volatile quantum memory, enabling long-term storage and reuse of
quantum information, and laying groundwork for future quantum memory
architectures.

</details>


### [322] [Quantum-Enhanced Weight Optimization for Neural Networks Using Grover's Algorithm](https://arxiv.org/abs/2504.14568)
*Stefan-Alexandru Jura,Mihai Udrescu*

Main category: quant-ph

TL;DR: 提出了一种利用量子计算优化经典神经网络权重的新方法，通过Grover量子搜索算法加速训练过程，避免了梯度下降的问题，并在小数据集上显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 传统梯度下降方法存在梯度爆炸、消失或凸性问题，其他替代方法如遗传搜索也有收敛一致性问题。本文旨在通过量子计算解决这些问题。

Method: 设计了一种基于Grover量子搜索算法的优化方法，直接搜索神经网络的最优参数，无需计算梯度。

Result: 在小数据集上，测试损失减少58.75%，测试准确率提高35.25%；在3层隐藏网络的Digits数据集上，平均准确率达97.7%。

Conclusion: 该方法避免了梯度问题，具有可扩展性和实用性，适合未来量子计算机资源有限的情况。

Abstract: The main approach to hybrid quantum-classical neural networks (QNN) is
employing quantum computing to build a neural network (NN) that has quantum
features, which is then optimized classically. Here, we propose a different
strategy: to use quantum computing in order to optimize the weights of a
classical NN. As such, we design an instance of Grover's quantum search
algorithm to accelerate the search for the optimal parameters of an NN during
the training process, a task traditionally performed using the backpropagation
algorithm with the gradient descent method. Indeed, gradient descent has issues
such as exploding gradient, vanishing gradient, or convexity problem. Other
methods tried to address such issues with strategies like genetic searches, but
they carry additional problems like convergence consistency. Our original
method avoids these issues -- because it does not calculate gradients -- and
capitalizes on classical architectures' robustness and Grover's quadratic
speedup in high-dimensional search spaces to significantly reduce test loss
(58.75%) and improve test accuracy (35.25%), compared to classical NN weight
optimization, on small datasets. Unlike most QNNs that are trained on small
datasets only, our method is also scalable, as it allows the optimization of
deep networks; for an NN with 3 hidden layers, trained on the Digits dataset
from scikit-learn, we obtained a mean accuracy of 97.7%. Moreover, our method
requires a much smaller number of qubits compared to other QNN approaches,
making it very practical for near-future quantum computers that will still
deliver a limited number of logical qubits.

</details>


### [323] [Trainable Quantum Neural Network for Multiclass Image Classification with the Power of Pre-trained Tree Tensor Networks](https://arxiv.org/abs/2504.14995)
*Keisuke Murota,Takumi Kobori*

Main category: quant-ph

TL;DR: 论文提出了一种森林张量网络（FTN）分类器，用于解决将树张量网络（TTN）嵌入量子神经网络（QNN）时的高阶门操作和中间电路后选择问题。通过扩展绝热编码框架，成功实现了FTN到量子FTN（qFTN）的转换，并在MNIST和CIFAR-10上验证了性能。


<details>
  <summary>Details</summary>
Motivation: 将TTN嵌入QNN以提升多类图像分类性能，但面临高阶门操作和低成功率的中间电路后选择问题。

Method: 提出FTN分类器，聚合多个小键维TTN，避免大尺寸门操作；扩展绝热编码框架，消除中间电路后选择开销。

Result: 在MNIST和CIFAR-10上成功训练FTN并嵌入qFTN，性能保持或提升。

Conclusion: TTN与QNN的协同作用为多类量子增强图像分类提供了可扩展框架。

Abstract: Tree tensor networks (TTNs) offer powerful models for image classification.
While these TTN image classifiers already show excellent performance on
classical hardware, embedding them into quantum neural networks (QNNs) may
further improve the performance by leveraging quantum resources. However,
embedding TTN classifiers into QNNs for multiclass classification remains
challenging. Key obstacles are the highorder gate operations required for large
bond dimensions and the mid-circuit postselection with exponentially low
success rates necessary for the exact embedding. In this work, to address these
challenges, we propose forest tensor network (FTN)-classifiers, which aggregate
multiple small-bond-dimension TTNs. This allows us to handle multiclass
classification without requiring large gates in the embedded circuits. We then
remove the overhead of mid-circuit postselection by extending the adiabatic
encoding framework to our setting and smoothly encode the FTN-classifiers into
a quantum forest tensor network (qFTN)- classifiers. Numerical experiments on
MNIST and CIFAR-10 demonstrate that we can successfully train FTN-classifiers
and encode them into qFTN-classifiers, while maintaining or even improving the
performance of the pre-trained FTN-classifiers. These results suggest that
synergy between TTN classification models and QNNs can provide a robust and
scalable framework for multiclass quantum-enhanced image classification.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [324] [Causal pieces: analysing and improving spiking neural networks piece by piece](https://arxiv.org/abs/2504.14015)
*Dominik Dold,Philipp Christian Petersen*

Main category: cs.NE

TL;DR: 论文提出了一种基于“线性片段”概念的新型脉冲神经网络（SNN）分析方法，证明了SNN输入域可分解为因果区域，其输出脉冲时间在局部Lipschitz连续。因果区域数量（称为“因果片段”）是衡量SNN近似能力的指标。实验表明，因果片段数量多的初始化与训练成功强相关，且纯正权重的SNN表现优异。


<details>
  <summary>Details</summary>
Motivation: 探索SNN的表达能力和可训练性，提出一种与人工神经网络（ANN）可比的新分析工具。

Method: 通过“因果片段”概念分析SNN输入域的分解，证明输出脉冲时间的局部Lipschitz连续性，并通过仿真验证因果片段数量与训练成功的相关性。

Result: 因果片段数量多的初始化与SNN训练成功强相关；纯正权重的SNN在基准任务中表现优异。

Conclusion: 因果片段是改进SNN的强大工具，并可能为未来比较SNN与ANN提供新方法。

Abstract: We introduce a novel concept for spiking neural networks (SNNs) derived from
the idea of "linear pieces" used to analyse the expressiveness and trainability
of artificial neural networks (ANNs). We prove that the input domain of SNNs
decomposes into distinct causal regions where its output spike times are
locally Lipschitz continuous with respect to the input spike times and network
parameters. The number of such regions - which we call "causal pieces" - is a
measure of the approximation capabilities of SNNs. In particular, we
demonstrate in simulation that parameter initialisations which yield a high
number of causal pieces on the training set strongly correlate with SNN
training success. Moreover, we find that feedforward SNNs with purely positive
weights exhibit a surprisingly high number of causal pieces, allowing them to
achieve competitive performance levels on benchmark tasks. We believe that
causal pieces are not only a powerful and principled tool for improving SNNs,
but might also open up new ways of comparing SNNs and ANNs in the future.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [325] [Application of Sensitivity Analysis Methods for Studying Neural Network Models](https://arxiv.org/abs/2504.15100)
*Jiaxuan Miao,Sergey Matveev*

Main category: math.NA

TL;DR: 论文研究了多种方法分析神经网络对输入数据扰动的敏感性及其机制，包括Sobol全局敏感性分析、局部敏感性方法和激活最大化技术，并在不同网络结构上验证。


<details>
  <summary>Details</summary>
Motivation: 探索神经网络对输入扰动的敏感性及其解释机制，以优化模型性能和理解其行为。

Method: 采用Sobol全局敏感性分析、局部敏感性方法和激活最大化技术，分别应用于小型前馈网络和经典卷积网络（VGG-16、ResNet-18）。

Result: 全局敏感性分析成功识别并减少了小型网络的关键输入参数；局部方法和激活最大化揭示了卷积网络的分类模式。

Conclusion: 不同方法适用于不同规模的网络，激活最大化与Grad-CAM在超声数据分析中表现可比。

Abstract: This study demonstrates the capabilities of several methods for analyzing the
sensitivity of neural networks to perturbations of the input data and
interpreting their underlying mechanisms. The investigated approaches include
the Sobol global sensitivity analysis, the local sensitivity method for input
pixel perturbations and the activation maximization technique. As examples, in
this study we consider a small feedforward neural network for analyzing an open
tabular dataset of clinical diabetes data, as well as two classical
convolutional architectures, VGG-16 and ResNet-18, which are widely used in
image processing and classification. Utilization of the global sensitivity
analysis allows us to identify the leading input parameters of the chosen tiny
neural network and reduce their number without significant loss of the
accuracy. As far as global sensitivity analysis is not applicable to larger
models we try the local sensitivity analysis and activation maximization method
in application to the convolutional neural networks. These methods show
interesting patterns for the convolutional models solving the image
classification problem. All in all, we compare the results of the activation
maximization method with popular Grad-CAM technique in the context of
ultrasound data analysis.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [326] [Chinese-LiPS: A Chinese audio-visual speech recognition dataset with Lip-reading and Presentation Slides](https://arxiv.org/abs/2504.15066)
*Jinghua Zhao,Yuhang Jia,Shiyao Wang,Jiaming Zhou,Hui Wang,Yong Qin*

Main category: cs.MM

TL;DR: 论文提出了一种结合唇读和演示幻灯片视觉信息的多模态中文AVSR数据集Chinese-LiPS，并开发了LiPS-AVSR方法，显著提升了ASR性能。


<details>
  <summary>Details</summary>
Motivation: 现有AVSR方法通常仅依赖唇读或上下文视频，忽略了结合多种视觉线索的潜力。

Method: 基于Chinese-LiPS数据集，开发了LiPS-AVSR方法，同时利用唇读和幻灯片信息作为视觉模态。

Result: 实验显示，唇读和幻灯片信息分别提升ASR性能约8%和25%，结合后提升约35%。

Conclusion: 结合多种视觉线索能显著提升AVSR性能，Chinese-LiPS数据集为未来研究提供了资源。

Abstract: Incorporating visual modalities to assist Automatic Speech Recognition (ASR)
tasks has led to significant improvements. However, existing Audio-Visual
Speech Recognition (AVSR) datasets and methods typically rely solely on
lip-reading information or speaking contextual video, neglecting the potential
of combining these different valuable visual cues within the speaking context.
In this paper, we release a multimodal Chinese AVSR dataset, Chinese-LiPS,
comprising 100 hours of speech, video, and corresponding manual transcription,
with the visual modality encompassing both lip-reading information and the
presentation slides used by the speaker. Based on Chinese-LiPS, we develop a
simple yet effective pipeline, LiPS-AVSR, which leverages both lip-reading and
presentation slide information as visual modalities for AVSR tasks. Experiments
show that lip-reading and presentation slide information improve ASR
performance by approximately 8\% and 25\%, respectively, with a combined
performance improvement of about 35\%. The dataset is available at
https://kiri0824.github.io/Chinese-LiPS/

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [327] [Learning over von Mises-Fisher Distributions via a Wasserstein-like Geometry](https://arxiv.org/abs/2504.14164)
*Kisung You,Dennis Shung,Mauro Giuffrè*

Main category: stat.ML

TL;DR: 提出了一种新的几何感知距离度量方法，用于比较von Mises-Fisher (vMF) 分布，解决了现有方法在归一化常数和几何度量上的不足。


<details>
  <summary>Details</summary>
Motivation: vMF分布广泛用于球形数据的概率学习任务，但缺乏合适的比较工具，尤其是归一化常数难以处理且缺乏几何度量。

Method: 基于最优传输理论，提出了一种类似Wasserstein的距离，将vMF分布的差异分解为均值方向的角度分离和浓度参数的差异。

Result: 该方法在高浓度区域通过高斯近似得到闭式解，具有理论优势，并在vMF混合模型压缩中表现出高效性。

Conclusion: 该工作为方向性数据分析提供了新的统计工具，适用于高维数据且支持可解释性推理。

Abstract: We introduce a novel, geometry-aware distance metric for the family of von
Mises-Fisher (vMF) distributions, which are fundamental models for directional
data on the unit hypersphere. Although the vMF distribution is widely employed
in a variety of probabilistic learning tasks involving spherical data,
principled tools for comparing vMF distributions remain limited, primarily due
to the intractability of normalization constants and the absence of suitable
geometric metrics. Motivated by the theory of optimal transport, we propose a
Wasserstein-like distance that decomposes the discrepancy between two vMF
distributions into two interpretable components: a geodesic term capturing the
angular separation between mean directions, and a variance-like term
quantifying differences in concentration parameters. The derivation leverages a
Gaussian approximation in the high-concentration regime to yield a tractable,
closed-form expression that respects the intrinsic spherical geometry. We show
that the proposed distance exhibits desirable theoretical properties and
induces a latent geometric structure on the space of non-degenerate vMF
distributions. As a primary application, we develop the efficient algorithms
for vMF mixture reduction, enabling structure-preserving compression of mixture
models in high-dimensional settings. Empirical results on synthetic datasets
and real-world high-dimensional embeddings, including biomedical sentence
representations and deep visual features, demonstrate the effectiveness of the
proposed geometry in distinguishing distributions and supporting interpretable
inference. This work expands the statistical toolbox for directional data
analysis by introducing a tractable, transport-inspired distance tailored to
the geometry of the hypersphere.

</details>


### [328] [Optimal Scheduling of Dynamic Transport](https://arxiv.org/abs/2504.14425)
*Panos Tsimpos,Zhi Ren,Jakob Zech,Youssef Marzouk*

Main category: stat.ML

TL;DR: 论文提出了一种利用“弯曲”轨迹优化流式采样和生成模型的方法，通过最小化速度场的Lipschitz常数，显著提升了近似和学习效果。


<details>
  <summary>Details</summary>
Motivation: 探索如何利用时间轴的设计自由度，改进流式方法中的轨迹设计，以提升采样和生成模型的性能。

Method: 通过计算最优时间调度函数τ，最小化速度场的空间Lipschitz常数，并证明其闭式解的存在性。

Result: 对于广泛的源/目标测度和传输映射T，最优调度能显著降低Lipschitz常数，且比恒等调度（如Wasserstein测地线）指数级更优。

Conclusion: 通过变分法和Γ-收敛技术，证明了最优调度的有效性，为流式方法提供了更高效的轨迹设计。

Abstract: Flow-based methods for sampling and generative modeling use continuous-time
dynamical systems to represent a {transport map} that pushes forward a source
measure to a target measure. The introduction of a time axis provides
considerable design freedom, and a central question is how to exploit this
freedom. Though many popular methods seek straight line (i.e., zero
acceleration) trajectories, we show here that a specific class of ``curved''
trajectories can significantly improve approximation and learning. In
particular, we consider the unit-time interpolation of any given transport map
$T$ and seek the schedule $\tau: [0,1] \to [0,1]$ that minimizes the spatial
Lipschitz constant of the corresponding velocity field over all times $t \in
[0,1]$. This quantity is crucial as it allows for control of the approximation
error when the velocity field is learned from data. We show that, for a broad
class of source/target measures and transport maps $T$, the \emph{optimal
schedule} can be computed in closed form, and that the resulting optimal
Lipschitz constant is \emph{exponentially smaller} than that induced by an
identity schedule (corresponding to, for instance, the Wasserstein geodesic).
Our proof technique relies on the calculus of variations and
$\Gamma$-convergence, allowing us to approximate the aforementioned degenerate
objective by a family of smooth, tractable problems.

</details>


### [329] [On the Tunability of Random Survival Forests Model for Predictive Maintenance](https://arxiv.org/abs/2504.14744)
*Yigitcan Yardımcı,Mustafa Cavus*

Main category: stat.ML

TL;DR: 论文研究了随机生存森林（RSF）模型在预测性维护中的可调性，提出了一个三层次框架量化调优效果，并验证了超参数调优对模型性能的显著提升。


<details>
  <summary>Details</summary>
Motivation: 尽管RSF因其灵活性和处理截尾数据的能力被广泛使用，但其性能对超参数配置敏感，且系统性的调优评估在预测性维护领域仍有限。

Method: 引入三层次框架：模型级指标（整体性能增益）、超参数级指标（个体贡献）和最优调优范围识别，使用C指数和Brier分数评估。

Result: 在四个CMAPSS数据集子集上，超参数调优显著提升性能（C指数平均增加0.0547，Brier分数降低0.0199），ntree和mtry调优效果最佳。

Conclusion: 超参数调优对生存模型至关重要，研究为实际预测性维护应用中的RSF优化提供了实用指导。

Abstract: This paper investigates the tunability of the Random Survival Forest (RSF)
model in predictive maintenance, where accurate time-to-failure estimation is
crucial. Although RSF is widely used due to its flexibility and ability to
handle censored data, its performance is sensitive to hyperparameter
configurations. However, systematic evaluations of RSF tunability remain
limited, especially in predictive maintenance contexts. We introduce a
three-level framework to quantify tunability: (1) a model-level metric
measuring overall performance gain from tuning, (2) a hyperparameter-level
metric assessing individual contributions, and (3) identification of optimal
tuning ranges. These metrics are evaluated across multiple datasets using
survival-specific criteria: the C-index for discrimination and the Brier score
for calibration. Experiments on four CMAPSS dataset subsets, simulating
aircraft engine degradation, reveal that hyperparameter tuning consistently
improves model performance. On average, the C-index increased by 0.0547, while
the Brier score decreased by 0.0199. These gains were consistent across all
subsets. Moreover, ntree and mtry showed the highest average tunability, while
nodesize offered stable improvements within the range of 10 to 30. In contrast,
splitrule demonstrated negative tunability on average, indicating that improper
tuning may reduce model performance. Our findings emphasize the practical
importance of hyperparameter tuning in survival models and provide actionable
insights for optimizing RSF in real-world predictive maintenance applications.

</details>


### [330] [Expected Free Energy-based Planning as Variational Inference](https://arxiv.org/abs/2504.14898)
*Bert de Vries,Wouter Nuijten,Thijs van de Laar,Wouter Kouw,Sepideh Adamiat,Tim Nisslbeck,Mykola Lukashchuk,Hoang Minh Huu Nguyen,Marco Hidalgo Araya,Raphael Tresor,Thijs Jenneskens,Ivana Nikoloska,Raaja Subramanian,Bart van Erp,Dmitry Bagaev,Albert Podusenko*

Main category: stat.ML

TL;DR: 论文提出了一种基于变分自由能最小化的统一框架，将规划视为变分推断，解决了传统方法在不确定性下规划时探索与利用分离的问题。


<details>
  <summary>Details</summary>
Motivation: 传统方法在处理不确定性规划时，探索与利用目标分离，缺乏统一的理论基础。主动推断虽提供了一种基于自由能原则的解决方案，但其计算负担限制了可扩展性。

Method: 通过最小化生成模型上的变分自由能泛函，结合偏好和认知先验，推导出基于EFE的规划方法，将规划本身视为变分推断。

Result: 提出的框架能够生成同时支持目标达成和信息获取的最优策略，并考虑了计算资源的限制。

Conclusion: 该框架不仅理论一致，还实现了可扩展、资源感知的主动推断代理，连接并扩展了现有方法。

Abstract: We address the problem of planning under uncertainty, where an agent must
choose actions that not only achieve desired outcomes but also reduce
uncertainty. Traditional methods often treat exploration and exploitation as
separate objectives, lacking a unified inferential foundation. Active
inference, grounded in the Free Energy Principle, offers such a foundation by
minimizing Expected Free Energy (EFE), a cost function that combines utility
with epistemic drives like ambiguity resolution and novelty seeking. However,
the computational burden of EFE minimization has remained a major obstacle to
its scalability. In this paper, we show that EFE-based planning arises
naturally from minimizing a variational free energy functional on a generative
model augmented with preference and epistemic priors. This result reinforces
theoretical consistency with the Free Energy Principle, by casting planning
itself as variational inference. Our formulation yields optimal policies that
jointly support goal achievement and information gain, while incorporating a
complexity term that accounts for bounded computational resources. This
unifying framework connects and extends existing methods, enabling scalable,
resource-aware implementations of active inference agents.

</details>


### [331] [Advanced posterior analyses of hidden Markov models: finite Markov chain imbedding and hybrid decoding](https://arxiv.org/abs/2504.15156)
*Zenia Elise Damgaard Bæk,Moisès Coll Macià,Laurits Skov,Asger Hobolth*

Main category: stat.ML

TL;DR: 论文提出使用有限马尔可夫链嵌入（FMCI）和混合解码方法解决隐马尔可夫模型（HMM）中的两个主要任务：计算隐藏状态序列的统计量分布和解码隐藏状态序列。


<details>
  <summary>Details</summary>
Motivation: 解决HMM应用中计算隐藏状态序列统计量分布和解码序列的挑战。

Method: 第一部分使用FMCI计算隐藏状态的后验分布统计量（如访问次数、停留时间等）；第二部分提出混合解码方法，结合Viterbi和后验解码的优势。

Result: 混合解码性能优于传统方法，并提供了选择调参的新方法；FMCI框架通过模拟验证。

Conclusion: FMCI和混合解码方法在经典数据集上表现优异，提供了可复现的代码。

Abstract: Two major tasks in applications of hidden Markov models are to (i) compute
distributions of summary statistics of the hidden state sequence, and (ii)
decode the hidden state sequence. We describe finite Markov chain imbedding
(FMCI) and hybrid decoding to solve each of these two tasks. In the first part
of our paper we use FMCI to compute posterior distributions of summary
statistics such as the number of visits to a hidden state, the total time spent
in a hidden state, the dwell time in a hidden state, and the longest run
length. We use simulations from the hidden state sequence, conditional on the
observed sequence, to establish the FMCI framework. In the second part of our
paper we apply hybrid segmentation for improved decoding of a HMM. We
demonstrate that hybrid decoding shows increased performance compared to
Viterbi or Posterior decoding (often also referred to as global or local
decoding), and we introduce a novel procedure for choosing the tuning parameter
in the hybrid procedure. Furthermore, we provide an alternative derivation of
the hybrid loss function based on weighted geometric means. We demonstrate and
apply FMCI and hybrid decoding on various classical data sets, and supply
accompanying code for reproducibility.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [332] [Integrating Response Time and Attention Duration in Bayesian Preference Learning for Multiple Criteria Decision Aiding](https://arxiv.org/abs/2504.14938)
*Jiaxuan Jiang,Jiapeng Liu,Miłosz Kadziński,Xiuwu Liao,Jingyu Dong*

Main category: stat.AP

TL;DR: 提出了一种结合行为线索的多标准贝叶斯偏好学习框架，通过整合成对比较、响应时间和注意力时长，深化对决策过程的理解。


<details>
  <summary>Details</summary>
Motivation: 传统方法无法充分捕捉决策者的行为模式，需要一种更全面的方法来揭示偏好。

Method: 采用加性价值函数模型和贝叶斯框架，通过定义偏好数据的似然和先验结构，推导潜在排序模型的后验分布。

Result: 实验验证了新方法在重建完整偏好方面的能力，并揭示了与时间和注意力相关的行为模式。

Conclusion: 结合全面数据的方法能更好地与决策者的实际偏好对齐。

Abstract: We introduce a multiple criteria Bayesian preference learning framework
incorporating behavioral cues for decision aiding. The framework integrates
pairwise comparisons, response time, and attention duration to deepen insights
into decision-making processes. The approach employs an additive value function
model and utilizes a Bayesian framework to derive the posterior distribution of
potential ranking models by defining the likelihood of observed preference data
and specifying a prior on the preference structure. This distribution
highlights each model's ability to reconstruct Decision-Makers' holistic
pairwise comparisons. By leveraging both response time as a proxy for cognitive
effort and alternative discriminability as well as attention duration as an
indicator of criterion importance, the proposed model surpasses traditional
methods by uncovering richer behavioral patterns. We report the results of a
laboratory experiment on mobile phone contract selection involving 30 real
subjects using a dedicated application with time-, eye-, and mouse-tracking
components. We validate the novel method's ability to reconstruct complete
preferences. The detailed ablation studies reveal time- and attention-related
behavioral patterns, confirming that integrating comprehensive data leads to
developing models that better align with the DM's actual preferences.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [333] [Towards Explainable Fake Image Detection with Multi-Modal Large Language Models](https://arxiv.org/abs/2504.14245)
*Yikun Ji,Yan Hong,Jiahui Zhan,Haoxing Chen,jun lan,Huijia Zhu,Weiqiang Wang,Liqing Zhang,Jianfu Zhang*

Main category: cs.CV

TL;DR: 论文探讨了基于多模态大语言模型（MLLMs）的AI生成图像检测方法，与传统方法和人工评估相比，展示了其优势和局限性，并提出了一种更鲁棒、可解释的检测框架。


<details>
  <summary>Details</summary>
Motivation: 图像生成技术的进步引发了公共安全问题，需要一种既能泛化又透明的假图像检测方法。

Method: 评估MLLMs与传统检测方法及人工评估的能力差异，设计六种提示并整合为框架。

Result: MLLMs在检测AI生成图像方面表现出潜力，但仍有局限性。

Conclusion: 提出的框架结合MLLMs的推理能力，为假图像检测提供了更鲁棒和可解释的解决方案。

Abstract: Progress in image generation raises significant public security concerns. We
argue that fake image detection should not operate as a "black box". Instead,
an ideal approach must ensure both strong generalization and transparency.
Recent progress in Multi-modal Large Language Models (MLLMs) offers new
opportunities for reasoning-based AI-generated image detection. In this work,
we evaluate the capabilities of MLLMs in comparison to traditional detection
methods and human evaluators, highlighting their strengths and limitations.
Furthermore, we design six distinct prompts and propose a framework that
integrates these prompts to develop a more robust, explainable, and
reasoning-driven detection system. The code is available at
https://github.com/Gennadiyev/mllm-defake.

</details>


### [334] [Cross-attention for State-based model RWKV-7](https://arxiv.org/abs/2504.14260)
*Liu Xiao,Li Zhiyuan,Lin Yueyu*

Main category: cs.CV

TL;DR: CrossWKV是一种新型交叉注意力机制，用于增强RWKV-7模型在文本到图像生成中的表现力，通过线性复杂度的WKV架构和LoRA技术实现跨模态对齐。


<details>
  <summary>Details</summary>
Motivation: 提升文本到图像生成的表现力，同时保持线性复杂度和高效内存使用。

Method: 结合广义delta规则、向量门控和低秩适应（LoRA），利用非对角输入依赖的转移矩阵实现复杂功能。

Result: 在ImageNet 256x256上达到FID 2.88和CLIP分数0.33，性能与最先进模型相当。

Conclusion: CrossWKV在跨模态任务中表现出色，适用于高分辨率生成和动态状态操作。

Abstract: We introduce CrossWKV, a novel cross-attention mechanism for the state-based
RWKV-7 model, designed to enhance the expressive power of text-to-image
generation. Leveraging RWKV-7's linear-complexity Weighted Key-Value (WKV)
architecture, CrossWKV integrates text and image modalities in a single pass,
utilizing a generalized delta rule with vector-valued gating and low-rank
adaptations (LoRA) to achieve superior cross-modal alignment. Unlike
Transformer-based models, CrossWKV's non-diagonal, input-dependent transition
matrix enables it to represent complex functions beyond the $\mathrm{TC}^0$
complexity class, including all regular languages, as demonstrated by its
ability to perform state-tracking tasks like $S_5$ permutation modeling.
Evaluated within the Diffusion in RWKV-7 (DIR-7) on datasets such as LAION-5B
and ImageNet, CrossWKV achieves a Frechet Inception Distance (FID) of 2.88 and
a CLIP score of 0.33 on ImageNet 256x256, matching state-of-the-art performance
while offering robust generalization across diverse prompts. The model's
enhanced expressivity, combined with constant memory usage and linear scaling,
positions it as a powerful solution for advanced cross-modal tasks, with
potential applications in high-resolution generation and dynamic state
manipulation.Code at https://github.com/TorchRWKV/flash-linear-attention

</details>


### [335] [A Multimodal Recaptioning Framework to Account for Perceptual Diversity in Multilingual Vision-Language Modeling](https://arxiv.org/abs/2504.14359)
*Kyle Buettner,Jacob Emmerson,Adriana Kovashka*

Main category: cs.CV

TL;DR: 论文提出了一种基于LLM的多模态重标注策略，通过修改英文标注以增强多语言视觉语言模型对感知多样性的理解，显著提升了德语和日语的文本-图像检索性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型的数据主要来自英语使用者，导致感知偏见和模型灵活性不足，无法充分体现不同文化背景下的感知多样性。

Method: 采用LLM驱动的多模态重标注策略，修改英文标注后再翻译，并结合母语数据指导的多模态机制进行训练增强。

Result: 在德语和日语的文本-图像检索案例中，平均召回率提升3.5，非母语错误案例提升4.7。

Conclusion: 该方法有效提升了多语言视觉语言模型对感知多样性的理解，并提供了跨数据集和跨语言泛化的分析机制。

Abstract: There are many ways to describe, name, and group objects when captioning an
image. Differences are evident when speakers come from diverse cultures due to
the unique experiences that shape perception. Machine translation of captions
has pushed multilingual capabilities in vision-language models (VLMs), but data
comes mainly from English speakers, indicating a perceptual bias and lack of
model flexibility. In this work, we address this challenge and outline a
data-efficient framework to instill multilingual VLMs with greater
understanding of perceptual diversity. We specifically propose an LLM-based,
multimodal recaptioning strategy that alters the object descriptions of English
captions before translation. The greatest benefits are demonstrated in a
targeted multimodal mechanism guided by native speaker data. By adding produced
rewrites as augmentations in training, we improve on German and Japanese
text-image retrieval cases studies (up to +3.5 mean recall overall, +4.7 on
non-native error cases). We further propose a mechanism to analyze the specific
object description differences across datasets, and we offer insights into
cross-dataset and cross-language generalization.

</details>


### [336] [Are Vision LLMs Road-Ready? A Comprehensive Benchmark for Safety-Critical Driving Video Understanding](https://arxiv.org/abs/2504.14526)
*Tong Zeng,Longfeng Wu,Liang Shi,Dawei Zhou,Feng Guo*

Main category: cs.CV

TL;DR: DVBench是一个评估视觉大语言模型（VLLMs）在安全关键驾驶场景中性能的基准测试，揭示了现有模型的局限性，并通过领域特定微调展示了改进潜力。


<details>
  <summary>Details</summary>
Motivation: 现有VLLMs在通用视觉任务中表现优异，但在安全关键领域（如自动驾驶）的性能尚未充分探索，缺乏针对复杂驾驶场景的评估工具。

Method: 提出DVBench基准，包含10,000个多选题，基于分层能力分类法评估VLLMs的感知和推理能力，并对14个SOTA模型进行实验和微调。

Result: 实验显示现有模型在复杂驾驶场景中表现不佳（最高准确率<40%），但微调后准确率提升5.24-10.94个百分点，相对改进达43.59%。

Conclusion: DVBench为开发满足自动驾驶安全需求的VLLMs提供了评估框架和研究方向，强调了领域适应的重要性。

Abstract: Vision Large Language Models (VLLMs) have demonstrated impressive
capabilities in general visual tasks such as image captioning and visual
question answering. However, their effectiveness in specialized,
safety-critical domains like autonomous driving remains largely unexplored.
Autonomous driving systems require sophisticated scene understanding in complex
environments, yet existing multimodal benchmarks primarily focus on normal
driving conditions, failing to adequately assess VLLMs' performance in
safety-critical scenarios. To address this, we introduce DVBench, a pioneering
benchmark designed to evaluate the performance of VLLMs in understanding
safety-critical driving videos. Built around a hierarchical ability taxonomy
that aligns with widely adopted frameworks for describing driving scenarios
used in assessing highly automated driving systems, DVBench features 10,000
multiple-choice questions with human-annotated ground-truth answers, enabling a
comprehensive evaluation of VLLMs' capabilities in perception and reasoning.
Experiments on 14 SOTA VLLMs, ranging from 0.5B to 72B parameters, reveal
significant performance gaps, with no model achieving over 40% accuracy,
highlighting critical limitations in understanding complex driving scenarios.
To probe adaptability, we fine-tuned selected models using domain-specific data
from DVBench, achieving accuracy gains ranging from 5.24 to 10.94 percentage
points, with relative improvements of up to 43.59%. This improvement
underscores the necessity of targeted adaptation to bridge the gap between
general-purpose VLLMs and mission-critical driving applications. DVBench
establishes an essential evaluation framework and research roadmap for
developing VLLMs that meet the safety and robustness requirements for
real-world autonomous systems. We released the benchmark toolbox and the
fine-tuned model at: https://github.com/tong-zeng/DVBench.git.

</details>


### [337] [Entropy Rectifying Guidance for Diffusion and Flow Models](https://arxiv.org/abs/2504.13987)
*Tariq Berrada Ifriqi,Adriana Romero-Soriano,Michal Drozdzal,Jakob Verbeek,Karteek Alahari*

Main category: cs.CV

TL;DR: 论文提出了一种名为熵修正引导（ERG）的新方法，通过调整扩散变换器架构中的注意力机制，同时提升图像质量、多样性和提示一致性。


<details>
  <summary>Details</summary>
Motivation: 现有的分类器自由引导（CFG）方法在图像生成中存在质量、多样性和一致性之间的权衡，且现有改进方法需要额外模型或计算开销。

Method: 提出ERG方法，利用推理时注意力机制的变化，无需额外模型或计算开销，适用于无条件采样。

Result: ERG在文本到图像、类条件和无条件图像生成任务中表现显著优于CFG，并能与其他引导方法结合进一步提升性能。

Conclusion: ERG是一种简单有效的引导机制，能够在不增加计算负担的情况下显著提升生成任务的表现。

Abstract: Guidance techniques are commonly used in diffusion and flow models to improve
image quality and consistency for conditional generative tasks such as
class-conditional and text-to-image generation. In particular, classifier-free
guidance (CFG) -- the most widely adopted guidance technique -- contrasts
conditional and unconditional predictions to improve the generated images. This
results, however, in trade-offs across quality, diversity and consistency,
improving some at the expense of others. While recent work has shown that it is
possible to disentangle these factors to some extent, such methods come with an
overhead of requiring an additional (weaker) model, or require more forward
passes per sampling step. In this paper, we propose Entropy Rectifying Guidance
(ERG), a simple and effective guidance mechanism based on inference-time
changes in the attention mechanism of state-of-the-art diffusion transformer
architectures, which allows for simultaneous improvements over image quality,
diversity and prompt consistency. ERG is more general than CFG and similar
guidance techniques, as it extends to unconditional sampling. ERG results in
significant improvements in various generation tasks such as text-to-image,
class-conditional and unconditional image generation. We also show that ERG can
be seamlessly combined with other recent guidance methods such as CADS and APG,
further boosting generation performance.

</details>


### [338] [Fashion-RAG: Multimodal Fashion Image Editing via Retrieval-Augmented Generation](https://arxiv.org/abs/2504.14011)
*Fulvio Sanguigni,Davide Morelli,Marcella Cornia,Rita Cucchiara*

Main category: cs.CV

TL;DR: 论文提出Fashion-RAG方法，通过文本输入检索匹配的服装并生成个性化图像，解决了现有虚拟试衣方法依赖特定服装输入的限制。


<details>
  <summary>Details</summary>
Motivation: 现有虚拟试衣方法通常需要具体服装输入，而用户可能仅提供文本描述，限制了实用性。

Method: 采用检索增强生成（Fashion-RAG），结合文本反转技术将检索到的服装图像投影到Stable Diffusion的文本嵌入空间。

Result: 在Dress Code数据集上，Fashion-RAG在质量和数量上均优于现有方法，能捕捉细粒度视觉细节。

Conclusion: Fashion-RAG是首个针对多模态时尚图像编辑的检索增强生成方法，有效结合文本输入与检索技术。

Abstract: In recent years, the fashion industry has increasingly adopted AI
technologies to enhance customer experience, driven by the proliferation of
e-commerce platforms and virtual applications. Among the various tasks, virtual
try-on and multimodal fashion image editing -- which utilizes diverse input
modalities such as text, garment sketches, and body poses -- have become a key
area of research. Diffusion models have emerged as a leading approach for such
generative tasks, offering superior image quality and diversity. However, most
existing virtual try-on methods rely on having a specific garment input, which
is often impractical in real-world scenarios where users may only provide
textual specifications. To address this limitation, in this work we introduce
Fashion Retrieval-Augmented Generation (Fashion-RAG), a novel method that
enables the customization of fashion items based on user preferences provided
in textual form. Our approach retrieves multiple garments that match the input
specifications and generates a personalized image by incorporating attributes
from the retrieved items. To achieve this, we employ textual inversion
techniques, where retrieved garment images are projected into the textual
embedding space of the Stable Diffusion text encoder, allowing seamless
integration of retrieved elements into the generative process. Experimental
results on the Dress Code dataset demonstrate that Fashion-RAG outperforms
existing methods both qualitatively and quantitatively, effectively capturing
fine-grained visual details from retrieved garments. To the best of our
knowledge, this is the first work to introduce a retrieval-augmented generation
approach specifically tailored for multimodal fashion image editing.

</details>


### [339] [LoftUp: Learning a Coordinate-Based Feature Upsampler for Vision Foundation Models](https://arxiv.org/abs/2504.14032)
*Haiwen Huang,Anpei Chen,Volodymyr Havrylov,Andreas Geiger,Dan Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种改进视觉基础模型（VFMs）特征上采样的方法，通过设计坐标交叉注意力变换器架构和利用自蒸馏训练目标，显著提升了像素级任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有VFMs（如DINOv2和CLIP）在像素级任务中因特征分辨率不足而表现受限，特征上采样是解决这一问题的关键。

Method: 提出坐标交叉注意力变换器架构，整合高分辨率图像、坐标和低分辨率VFM特征；利用类无关掩码和自蒸馏构建高分辨率伪真实特征作为训练目标。

Result: 实验表明，该方法在多种下游任务中显著优于现有特征上采样技术。

Conclusion: 该方法有效捕捉细粒度细节并适应多种输入和特征分辨率，为像素级任务提供了高效解决方案。

Abstract: Vision foundation models (VFMs) such as DINOv2 and CLIP have achieved
impressive results on various downstream tasks, but their limited feature
resolution hampers performance in applications requiring pixel-level
understanding. Feature upsampling offers a promising direction to address this
challenge. In this work, we identify two critical factors for enhancing feature
upsampling: the upsampler architecture and the training objective. For the
upsampler architecture, we introduce a coordinate-based cross-attention
transformer that integrates the high-resolution images with coordinates and
low-resolution VFM features to generate sharp, high-quality features. For the
training objective, we propose constructing high-resolution pseudo-groundtruth
features by leveraging class-agnostic masks and self-distillation. Our approach
effectively captures fine-grained details and adapts flexibly to various input
and feature resolutions. Through experiments, we demonstrate that our approach
significantly outperforms existing feature upsampling techniques across various
downstream tasks. Our code is released at https://github.com/andrehuang/loftup.

</details>


### [340] [Occlusion-Ordered Semantic Instance Segmentation](https://arxiv.org/abs/2504.14054)
*Soroosh Baselizadeh,Cheuk-To Yu,Olga Veksler,Yuri Boykov*

Main category: cs.CV

TL;DR: 论文提出了一种结合相对深度排序和实例分割的任务OOSIS，通过遮挡边界实现更可靠的3D信息提取。


<details>
  <summary>Details</summary>
Motivation: 传统2D实例分割缺乏3D信息，而单目深度估计困难，因此利用遮挡关系提供更简单的3D信息。

Method: 提出OOSIS任务，结合遮挡边界和语义分割同时提取实例及其遮挡顺序，并开发了一种新的定向遮挡边界方法。

Result: 在KINS和COCOA数据集上表现优于基线方法。

Conclusion: OOSIS通过遮挡关系提供了一种简单有效的3D信息提取方法，优于传统深度估计。

Abstract: Standard semantic instance segmentation provides useful, but inherently 2D
information from a single image. To enable 3D analysis, one usually integrates
absolute monocular depth estimation with instance segmentation. However,
monocular depth is a difficult task. Instead, we leverage a simpler
single-image task, occlusion-based relative depth ordering, providing coarser
but useful 3D information. We show that relative depth ordering works more
reliably from occlusions than from absolute depth. We propose to solve the
joint task of relative depth ordering and segmentation of instances based on
occlusions. We call this task Occlusion-Ordered Semantic Instance Segmentation
(OOSIS). We develop an approach to OOSIS that extracts instances and their
occlusion order simultaneously from oriented occlusion boundaries and semantic
segmentation. Unlike popular detect-and-segment framework for instance
segmentation, combining occlusion ordering with instance segmentation allows a
simple and clean formulation of OOSIS as a labeling problem. As a part of our
solution for OOSIS, we develop a novel oriented occlusion boundaries approach
that significantly outperforms prior work. We also develop a new joint OOSIS
metric based both on instance mask accuracy and correctness of their occlusion
order. We achieve better performance than strong baselines on KINS and COCOA
datasets.

</details>


### [341] [An LMM for Efficient Video Understanding via Reinforced Compression of Video Cubes](https://arxiv.org/abs/2504.15270)
*Ji Qi,Yuan Yao,Yushi Bai,Bin Xu,Juanzi Li,Zhiyuan Liu,Tat-Seng Chua*

Main category: cs.CV

TL;DR: Quicksviewer是一种大型多模态模型（LMM），通过动态分区和统一重采样视频帧，显著提升视频理解的效率，减少时空冗余。


<details>
  <summary>Details</summary>
Motivation: 传统LMM对视频帧的均匀感知导致计算效率低下，尤其是对信息密度不均的视频。

Method: 使用Gumbel Softmax将视频分区为非均匀密度块，并对每个块进行统一重采样，实现动态压缩。

Result: 模型在训练数据量较少（0.8M样本）的情况下，性能超越基线（最高提升8.72%准确率），并在Video-MME上达到SOTA。

Conclusion: Quicksviewer通过动态分区和高效感知，显著提升了视频理解的效率和性能，同时揭示了模型能力与输入帧数的幂律关系。

Abstract: Large Multimodal Models (LMMs) uniformly perceive video frames, creating
computational inefficiency for videos with inherently varying temporal
information density. This paper present \textbf{Quicksviewer}, an LMM with new
perceiving paradigm that partitions a video of nonuniform density into varying
cubes using Gumbel Softmax, followed by a unified resampling for each cube to
achieve efficient video understanding. This simple and intuitive approach
dynamically compress video online based on its temporal density, significantly
reducing spatiotemporal redundancy (overall 45$\times$ compression rate), while
enabling efficient training with large receptive field. We train the model from
a language backbone through three progressive stages, each incorporating
lengthy videos on average of 420s/1fps thanks to the perceiving efficiency.
With only 0.8M total video-text samples for training, our model outperforms the
direct baseline employing a fixed partitioning strategy by a maximum of 8.72 in
accuracy, demonstrating the effectiveness in performance. On Video-MME,
Quicksviewer achieves SOTA under modest sequence lengths using just up to 5\%
of tokens per frame required by baselines. With this paradigm, scaling up the
number of input frames reveals a clear power law of the model capabilities. It
is also empirically verified that the segments generated by the cubing network
can help for analyzing continuous events in videos.

</details>


### [342] [Seeing from Another Perspective: Evaluating Multi-View Understanding in MLLMs](https://arxiv.org/abs/2504.15280)
*Chun-Hsiao Yeh,Chenyu Wang,Shengbang Tong,Ta-Ying Cheng,Rouyu Wang,Tianzhe Chu,Yuexiang Zhai,Yubei Chen,Shenghua Gao,Yi Ma*

Main category: cs.CV

TL;DR: 论文提出了All-Angles Bench基准，用于评估多模态大语言模型（MLLMs）在多视角场景理解中的表现，发现当前模型在跨视角一致性和几何推理方面仍远低于人类水平。


<details>
  <summary>Details</summary>
Motivation: 多视角理解是MLLMs作为具身代理的关键能力，但现有模型在高层次推理和规划中表现优异，却在多视角几何一致性和跨视角对应上存在不足。

Method: 通过构建包含2,100个人工标注多视角问答对的基准（All-Angles Bench），设计了六项任务测试模型的几何对应和跨视角信息对齐能力。

Result: 实验表明，27种代表性MLLMs（如Gemini-2.0-Flash、Claude-3.7-Sonnet和GPT-4o）在跨视角对应和相机姿态估计方面表现显著低于人类水平。

Conclusion: 当前MLLMs在多视角理解上仍需改进，特别是针对遮挡视角和相机姿态估计。All-Angles Bench为提升模型能力提供了重要参考。

Abstract: Multi-view understanding, the ability to reconcile visual information across
diverse viewpoints for effective navigation, manipulation, and 3D scene
comprehension, is a fundamental challenge in Multi-Modal Large Language Models
(MLLMs) to be used as embodied agents. While recent MLLMs have shown impressive
advances in high-level reasoning and planning, they frequently fall short when
confronted with multi-view geometric consistency and cross-view correspondence.
To comprehensively evaluate the challenges of MLLMs in multi-view scene
reasoning, we propose All-Angles Bench, a benchmark of over 2,100 human
carefully annotated multi-view question-answer pairs across 90 diverse
real-world scenes. Our six tasks (counting, attribute identification, relative
distance, relative direction, object manipulation, and camera pose estimation)
specifically test model's geometric correspondence and the capacity to align
information consistently across views. Our extensive experiments, benchmark on
27 representative MLLMs including Gemini-2.0-Flash, Claude-3.7-Sonnet, and
GPT-4o against human evaluators reveals a substantial performance gap,
indicating that current MLLMs remain far from human-level proficiency. Through
in-depth analysis, we show that MLLMs are particularly underperforming under
two aspects: (1) cross-view correspondence for partially occluded views and (2)
establishing the coarse camera poses. These findings highlight the necessity of
domain-specific refinements or modules that embed stronger multi-view
awareness. We believe that our All-Angles Bench offers valuable insights and
contribute to bridging the gap between MLLMs and human-level multi-view
understanding. The project and benchmark are publicly available at
https://danielchyeh.github.io/All-Angles-Bench/.

</details>


### [343] [ThyroidEffi 1.0: A Cost-Effective System for High-Performance Multi-Class Thyroid Carcinoma Classification](https://arxiv.org/abs/2504.14139)
*Hai Pham-Ngoc,De Nguyen-Van,Dung Vu-Tien,Phuong Le-Hong*

Main category: cs.CV

TL;DR: 开发了一种高效、可解释的深度学习系统，用于甲状腺细针穿刺活检图像的多分类，具有高准确性和低计算开销。


<details>
  <summary>Details</summary>
Motivation: 解决甲状腺FNAB图像分类中的数据不足、观察者间差异和计算成本高的挑战，为临床决策提供支持。

Method: 结合YOLOv10细胞簇检测、课程学习协议、轻量级EfficientNetB0和Transformer模块，实现多尺度特征提取与分析。

Result: 内部测试集宏F1为89.19%，外部验证AUC分别为0.9495（B2）、0.7436（B5）和0.8396（B6）。系统处理速度快，适用于普通硬件。

Conclusion: 高准确性、可解释的甲状腺FNAB分类可在低计算需求下实现，具有临床可行性。

Abstract: Background: Automated classification of thyroid fine needle aspiration biopsy
(FNAB) images faces challenges in limited data, inter-observer variability, and
computational cost. Efficient, interpretable models are crucial for clinical
support. Objective: To develop and externally validate a deep learning system
for the multi-class classification of thyroid FNAB images into three key
categories that directly guide post-biopsy treatment decisions in Vietnam:
benign (B2), suspicious for malignancy (B5), and malignant (B6), while
achieving high diagnostic accuracy with low computational overhead. Methods:
Our framework features: (1) YOLOv10-based cell cluster detection for
informative sub-region extraction and noise reduction; (2) a curriculum
learning-inspired protocol sequencing localized crops to full images for
multi-scale feature capture; (3) adaptive lightweight EfficientNetB0 (4
millions parameters) selection balancing performance and efficiency; and (4) a
Transformer-inspired module for multi-scale, multi-region analysis. External
validation used 1,015 independent FNAB images. Results: ThyroidEffi Basic
achieved a macro F1 of 89.19\% and AUCs of 0.98 (B2), 0.95 (B5), and 0.96 (B6)
on the internal test set. External validation yielded AUCs of 0.9495 (B2),
0.7436 (B5), and 0.8396 (B6). ThyroidEffi Premium improved macro F1 to 89.77\%.
Grad-CAM highlighted key diagnostic regions, confirming interpretability. The
system processed 1000 cases in 30 seconds, demonstrating feasibility on widely
accessible hardware like a 12-core CPU. Conclusions: This work demonstrates
that high-accuracy, interpretable thyroid FNAB image classification is
achievable with minimal computational demands.

</details>


### [344] [Locate 3D: Real-World Object Localization via Self-Supervised Learning in 3D](https://arxiv.org/abs/2504.14151)
*Sergio Arnaud,Paul McVay,Ada Martin,Arjun Majumdar,Krishna Murthy Jatavallabhula,Phillip Thomas,Ruslan Partsey,Daniel Dugas,Abha Gejji,Alexander Sax,Vincent-Pierre Berges,Mikael Henaff,Ayush Jain,Ang Cao,Ishita Prasad,Mrinal Kalakrishnan,Michael Rabbat,Nicolas Ballas,Mido Assran,Oleksandr Maksymets,Aravind Rajeswaran,Franziska Meier*

Main category: cs.CV

TL;DR: LOCATE 3D是一个通过自然语言描述在3D场景中定位物体的模型，结合了自监督学习（3D-JEPA）和语言条件解码器，实现了高性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决3D场景中基于自然语言描述的物体定位问题，适用于机器人和AR设备的实际部署。

Method: 使用3D-JEPA自监督学习算法处理点云数据，结合2D基础模型（CLIP、DINO）提取特征，并通过掩码预测任务学习上下文特征。训练后，3D-JEPA编码器与语言条件解码器联合预测3D掩码和边界框。

Result: 在标准基准测试中达到新最优性能，并展示了强大的泛化能力。

Conclusion: LOCATE 3D通过创新的自监督学习和数据集支持，为3D场景中的物体定位提供了高效解决方案。

Abstract: We present LOCATE 3D, a model for localizing objects in 3D scenes from
referring expressions like "the small coffee table between the sofa and the
lamp." LOCATE 3D sets a new state-of-the-art on standard referential grounding
benchmarks and showcases robust generalization capabilities. Notably, LOCATE 3D
operates directly on sensor observation streams (posed RGB-D frames), enabling
real-world deployment on robots and AR devices. Key to our approach is 3D-JEPA,
a novel self-supervised learning (SSL) algorithm applicable to sensor point
clouds. It takes as input a 3D pointcloud featurized using 2D foundation models
(CLIP, DINO). Subsequently, masked prediction in latent space is employed as a
pretext task to aid the self-supervised learning of contextualized pointcloud
features. Once trained, the 3D-JEPA encoder is finetuned alongside a
language-conditioned decoder to jointly predict 3D masks and bounding boxes.
Additionally, we introduce LOCATE 3D DATASET, a new dataset for 3D referential
grounding, spanning multiple capture setups with over 130K annotations. This
enables a systematic study of generalization capabilities as well as a stronger
model.

</details>


### [345] [Enhancing Multimodal In-Context Learning for Image Classification through Coreset Optimization](https://arxiv.org/abs/2504.14200)
*Huiyi Chen,Jiawei Peng,Kaihua Tang,Xin Geng,Xu Yang*

Main category: cs.CV

TL;DR: KeCO框架通过视觉特征作为关键点，优化核心集选择，提升图像分类任务中的上下文学习性能，平均改进超过20%。


<details>
  <summary>Details</summary>
Motivation: 现有方法在图像分类中计算成本高且信息损失严重，KeCO旨在解决这些问题。

Method: 利用未利用数据构建紧凑核心集，通过视觉特征作为关键点更新样本。

Result: 在粗粒度和细粒度图像分类基准测试中，性能平均提升超过20%。

Conclusion: KeCO在资源受限的实际场景中具有实用价值。

Abstract: In-context learning (ICL) enables Large Vision-Language Models (LVLMs) to
adapt to new tasks without parameter updates, using a few demonstrations from a
large support set. However, selecting informative demonstrations leads to high
computational and memory costs. While some methods explore selecting a small
and representative coreset in the text classification, evaluating all support
set samples remains costly, and discarded samples lead to unnecessary
information loss. These methods may also be less effective for image
classification due to differences in feature spaces. Given these limitations,
we propose Key-based Coreset Optimization (KeCO), a novel framework that
leverages untapped data to construct a compact and informative coreset. We
introduce visual features as keys within the coreset, which serve as the anchor
for identifying samples to be updated through different selection strategies.
By leveraging untapped samples from the support set, we update the keys of
selected coreset samples, enabling the randomly initialized coreset to evolve
into a more informative coreset under low computational cost. Through extensive
experiments on coarse-grained and fine-grained image classification benchmarks,
we demonstrate that KeCO effectively enhances ICL performance for image
classification task, achieving an average improvement of more than 20\%.
Notably, we evaluate KeCO under a simulated online scenario, and the strong
performance in this scenario highlights the practical value of our framework
for resource-constrained real-world scenarios.

</details>


### [346] [Learning Joint ID-Textual Representation for ID-Preserving Image Synthesis](https://arxiv.org/abs/2504.14202)
*Zichuan Liu,Liming Jiang,Qing Yan,Yumin Jia,Hao Kang,Xin Lu*

Main category: cs.CV

TL;DR: 提出了一种基于多模态编码策略的ID保留生成框架，通过统一处理身份和文本输入，结合FaceCLIP和SDXL实现高质量肖像生成。


<details>
  <summary>Details</summary>
Motivation: 现有方法通过适配器注入身份特征，限制了生成质量和灵活性，需要更统一的条件输入方式。

Method: 引入FaceCLIP多模态编码器，学习身份与文本的联合嵌入空间，结合扩散模型生成图像，并设计多模态对齐算法训练FaceCLIP。

Result: FaceCLIP-SDXL在身份保留和文本对齐方面优于现有方法，生成更逼真的肖像。

Conclusion: 提出的框架在ID保留生成任务中表现出色，为多模态条件生成提供了新思路。

Abstract: We propose a novel framework for ID-preserving generation using a multi-modal
encoding strategy rather than injecting identity features via adapters into
pre-trained models. Our method treats identity and text as a unified
conditioning input. To achieve this, we introduce FaceCLIP, a multi-modal
encoder that learns a joint embedding space for both identity and textual
semantics. Given a reference face and a text prompt, FaceCLIP produces a
unified representation that encodes both identity and text, which conditions a
base diffusion model to generate images that are identity-consistent and
text-aligned. We also present a multi-modal alignment algorithm to train
FaceCLIP, using a loss that aligns its joint representation with face, text,
and image embedding spaces. We then build FaceCLIP-SDXL, an ID-preserving image
synthesis pipeline by integrating FaceCLIP with Stable Diffusion XL (SDXL).
Compared to prior methods, FaceCLIP-SDXL enables photorealistic portrait
generation with better identity preservation and textual relevance. Extensive
experiments demonstrate its quantitative and qualitative superiority.

</details>


### [347] [Balancing Privacy and Action Performance: A Penalty-Driven Approach to Image Anonymization](https://arxiv.org/abs/2504.14301)
*Nazia Aslam,Kamal Nasrollahi*

Main category: cs.CV

TL;DR: 论文提出了一种隐私保护的图像匿名化技术，通过优化匿名器以平衡隐私保护和动作识别性能，符合欧盟AI法案和GDPR标准。


<details>
  <summary>Details</summary>
Motivation: 解决视频监控系统中隐私保护与动作识别性能之间的权衡问题，确保在保护个人隐私的同时不牺牲性能。

Method: 提出基于特征的惩罚方案，通过优化匿名器来最小化隐私泄漏并保持高动作识别性能。

Result: 实验表明，该方法在保持隐私泄漏一致的同时，显著提升了动作识别性能。

Conclusion: 该方法有效平衡了隐私保护与性能需求，为隐私保护技术提供了新思路。

Abstract: The rapid development of video surveillance systems for object detection,
tracking, activity recognition, and anomaly detection has revolutionized our
day-to-day lives while setting alarms for privacy concerns. It isn't easy to
strike a balance between visual privacy and action recognition performance in
most computer vision models. Is it possible to safeguard privacy without
sacrificing performance? It poses a formidable challenge, as even minor privacy
enhancements can lead to substantial performance degradation. To address this
challenge, we propose a privacy-preserving image anonymization technique that
optimizes the anonymizer using penalties from the utility branch, ensuring
improved action recognition performance while minimally affecting privacy
leakage. This approach addresses the trade-off between minimizing privacy
leakage and maintaining high action performance. The proposed approach is
primarily designed to align with the regulatory standards of the EU AI Act and
GDPR, ensuring the protection of personally identifiable information while
maintaining action performance. To the best of our knowledge, we are the first
to introduce a feature-based penalty scheme that exclusively controls the
action features, allowing freedom to anonymize private attributes. Extensive
experiments were conducted to validate the effectiveness of the proposed
method. The results demonstrate that applying a penalty to anonymizer from
utility branch enhances action performance while maintaining nearly consistent
privacy leakage across different penalty settings.

</details>


### [348] [Transforming hyperspectral images into chemical maps: A new deep learning based approach to hyperspectral image processing](https://arxiv.org/abs/2504.14131)
*Ole-Christian Galbo Engstrøm,Michela Albano-Gaglio,Erik Schou Dreier,Yamine Bouzembrak,Maria Font-i-Furnols,Puneet Mishra,Kim Steenstrup Pedersen*

Main category: cs.CV

TL;DR: 本研究提出了一种基于改进U-Net和自定义损失函数的端到端深度学习方法，直接从高光谱图像生成化学图谱，优于传统的PLS回归方法。


<details>
  <summary>Details</summary>
Motivation: 传统方法（如PLS回归）在生成化学图谱时未考虑空间上下文，且噪声较大，因此需要更优的解决方案。

Method: 使用改进的U-Net和自定义损失函数，跳过传统像素级分析的中间步骤，直接从高光谱图像生成化学图谱。

Result: U-Net在平均脂肪预测任务上的测试集均方根误差比PLS低9%-13%，且生成的化学图谱99.91%的方差具有空间相关性。

Conclusion: U-Net在化学图谱生成中优于PLS，能够生成更精细且物理合理的图谱。

Abstract: Current approaches to chemical map generation from hyperspectral images are
based on models such as partial least squares (PLS) regression, generating
pixel-wise predictions that do not consider spatial context and suffer from a
high degree of noise. This study proposes an end-to-end deep learning approach
using a modified version of U-Net and a custom loss function to directly obtain
chemical maps from hyperspectral images, skipping all intermediate steps
required for traditional pixel-wise analysis. We compare the U-Net with the
traditional PLS regression on a real dataset of pork belly samples with
associated mean fat reference values. The U-Net obtains a test set root mean
squared error of between 9% and 13% lower than that of PLS regression on the
task of mean fat prediction. At the same time, U-Net generates fine detail
chemical maps where 99.91% of the variance is spatially correlated. Conversely,
only 2.53% of the variance in the PLS-generated chemical maps is spatially
correlated, indicating that each pixel-wise prediction is largely independent
of neighboring pixels. Additionally, while the PLS-generated chemical maps
contain predictions far beyond the physically possible range of 0-100%, U-Net
learns to stay inside this range. Thus, the findings of this study indicate
that U-Net is superior to PLS for chemical map generation.

</details>


### [349] [Visual Prompting for One-shot Controllable Video Editing without Inversion](https://arxiv.org/abs/2504.14335)
*Zhengbo Zhang,Yuxi Zhou,Duo Peng,Joo-Hwee Lim,Zhigang Tu,De Wen Soh,Lin Geng Foo*

Main category: cs.CV

TL;DR: 论文提出了一种无需DDIM反转的单次可控视频编辑方法，通过视觉提示和一致性采样确保内容一致性。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法因DDIM反转误差导致的内容一致性问题。

Method: 采用视觉提示视角，提出内容一致性采样（CCS）和时间一致性采样（TCS）。

Result: 实验验证了方法的有效性。

Conclusion: 新方法通过消除DDIM反转和引入一致性采样，显著提升了编辑视频的内容和时序一致性。

Abstract: One-shot controllable video editing (OCVE) is an important yet challenging
task, aiming to propagate user edits that are made -- using any image editing
tool -- on the first frame of a video to all subsequent frames, while ensuring
content consistency between edited frames and source frames. To achieve this,
prior methods employ DDIM inversion to transform source frames into latent
noise, which is then fed into a pre-trained diffusion model, conditioned on the
user-edited first frame, to generate the edited video. However, the DDIM
inversion process accumulates errors, which hinder the latent noise from
accurately reconstructing the source frames, ultimately compromising content
consistency in the generated edited frames. To overcome it, our method
eliminates the need for DDIM inversion by performing OCVE through a novel
perspective based on visual prompting. Furthermore, inspired by consistency
models that can perform multi-step consistency sampling to generate a sequence
of content-consistent images, we propose a content consistency sampling (CCS)
to ensure content consistency between the generated edited frames and the
source frames. Moreover, we introduce a temporal-content consistency sampling
(TCS) based on Stein Variational Gradient Descent to ensure temporal
consistency across the edited frames. Extensive experiments validate the
effectiveness of our approach.

</details>


### [350] [CLIP-Powered Domain Generalization and Domain Adaptation: A Comprehensive Survey](https://arxiv.org/abs/2504.14280)
*Jindong Li,Yongguang Li,Yali Fu,Jiahong Liu,Yixin Liu,Menglin Yang,Irwin King*

Main category: cs.CV

TL;DR: 该论文综述了CLIP在领域泛化（DG）和领域适应（DA）中的应用，填补了现有文献的空白，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏关于CLIP在DG和DA中应用的系统性综述，本文旨在填补这一空白，为研究者和实践者提供有价值的参考。

Method: 在DG中，方法分为优化提示学习和利用CLIP作为特征提取器；在DA中，分为基于源数据的方法和基于目标数据的无源方法。

Result: 论文总结了CLIP在DG和DA中的应用，并指出了关键挑战（如过拟合和计算效率）及未来机会。

Conclusion: 本文为CLIP在DG和DA中的应用提供了全面综述，旨在推动更鲁棒的机器学习模型的发展。

Abstract: As machine learning evolves, domain generalization (DG) and domain adaptation
(DA) have become crucial for enhancing model robustness across diverse
environments. Contrastive Language-Image Pretraining (CLIP) plays a significant
role in these tasks, offering powerful zero-shot capabilities that allow models
to perform effectively in unseen domains. However, there remains a significant
gap in the literature, as no comprehensive survey currently exists that
systematically explores the applications of CLIP in DG and DA, highlighting the
necessity for this review. This survey presents a comprehensive review of
CLIP's applications in DG and DA. In DG, we categorize methods into optimizing
prompt learning for task alignment and leveraging CLIP as a backbone for
effective feature extraction, both enhancing model adaptability. For DA, we
examine both source-available methods utilizing labeled source data and
source-free approaches primarily based on target domain data, emphasizing
knowledge transfer mechanisms and strategies for improved performance across
diverse contexts. Key challenges, including overfitting, domain diversity, and
computational efficiency, are addressed, alongside future research
opportunities to advance robustness and efficiency in practical applications.
By synthesizing existing literature and pinpointing critical gaps, this survey
provides valuable insights for researchers and practitioners, proposing
directions for effectively leveraging CLIP to enhance methodologies in domain
generalization and adaptation. Ultimately, this work aims to foster innovation
and collaboration in the quest for more resilient machine learning models that
can perform reliably across diverse real-world scenarios. A more up-to-date
version of the papers is maintained at:
https://github.com/jindongli-Ai/Survey_on_CLIP-Powered_Domain_Generalization_and_Adaptation.

</details>


### [351] [LOOPE: Learnable Optimal Patch Order in Positional Embeddings for Vision Transformers](https://arxiv.org/abs/2504.14386)
*Md Abtahi Majeed Chowdhury,Md Rifat Ur Rahman,Akil Ahmad Taki*

Main category: cs.CV

TL;DR: 论文提出了一种名为LOOPE的可学习补丁排序方法，用于优化视觉Transformer中的位置嵌入，显著提升了分类准确性，并引入了一种新的评估框架“Three Cell Experiment”来更敏感地衡量位置嵌入的效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视了补丁排序对位置嵌入的影响，导致2D网格到1D序列的映射存在挑战。

Method: 提出LOOPE方法，通过优化补丁排序来改进空间表示，并引入“Three Cell Experiment”评估框架。

Result: LOOPE显著提升了分类准确性，新评估框架揭示了30-35%的性能差异，远高于传统评估的4-6%。

Conclusion: LOOPE在保留相对和绝对位置信息方面表现出色，为位置嵌入研究提供了更敏感的诊断工具。

Abstract: Positional embeddings (PE) play a crucial role in Vision Transformers (ViTs)
by providing spatial information otherwise lost due to the permutation
invariant nature of self attention. While absolute positional embeddings (APE)
have shown theoretical advantages over relative positional embeddings (RPE),
particularly due to the ability of sinusoidal functions to preserve spatial
inductive biases like monotonicity and shift invariance, a fundamental
challenge arises when mapping a 2D grid to a 1D sequence. Existing methods have
mostly overlooked or never explored the impact of patch ordering in positional
embeddings. To address this, we propose LOOPE, a learnable patch-ordering
method that optimizes spatial representation for a given set of frequencies,
providing a principled approach to patch order optimization. Empirical results
show that our PE significantly improves classification accuracy across various
ViT architectures. To rigorously evaluate the effectiveness of positional
embeddings, we introduce the "Three Cell Experiment", a novel benchmarking
framework that assesses the ability of PEs to retain relative and absolute
positional information across different ViT architectures. Unlike standard
evaluations, which typically report a performance gap of 4 to 6% between models
with and without PE, our method reveals a striking 30 to 35% difference,
offering a more sensitive diagnostic tool to measure the efficacy of PEs. Our
experimental analysis confirms that the proposed LOOPE demonstrates enhanced
effectiveness in retaining both relative and absolute positional information.

</details>


### [352] [Hydra: An Agentic Reasoning Approach for Enhancing Adversarial Robustness and Mitigating Hallucinations in Vision-Language Models](https://arxiv.org/abs/2504.14395)
*Chung-En,Yu,Hsuan-Chih,Chen,Brian Jalaian,Nathaniel D. Bastian*

Main category: cs.CV

TL;DR: Hydra是一个自适应代理框架，通过迭代推理和跨模型验证提升视觉语言模型的鲁棒性，同时对抗对抗性扰动和幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅关注对抗防御或幻觉后处理，缺乏统一策略，Hydra旨在填补这一空白。

Method: 采用Action-Critique Loop，结合Chain-of-Thought和In-Context Learning技术动态优化输出。

Result: 在四种VLMs上测试，Hydra表现优于现有插件模型和去幻觉方法，无需显式对抗防御。

Conclusion: Hydra为提升VLMs在实际应用中的可靠性提供了可扩展的无训练解决方案。

Abstract: To develop trustworthy Vision-Language Models (VLMs), it is essential to
address adversarial robustness and hallucination mitigation, both of which
impact factual accuracy in high-stakes applications such as defense and
healthcare. Existing methods primarily focus on either adversarial defense or
hallucination post-hoc correction, leaving a gap in unified robustness
strategies. We introduce \textbf{Hydra}, an adaptive agentic framework that
enhances plug-in VLMs through iterative reasoning, structured critiques, and
cross-model verification, improving both resilience to adversarial
perturbations and intrinsic model errors. Hydra employs an Action-Critique
Loop, where it retrieves and critiques visual information, leveraging
Chain-of-Thought (CoT) and In-Context Learning (ICL) techniques to refine
outputs dynamically. Unlike static post-hoc correction methods, Hydra adapts to
both adversarial manipulations and intrinsic model errors, making it robust to
malicious perturbations and hallucination-related inaccuracies. We evaluate
Hydra on four VLMs, three hallucination benchmarks, two adversarial attack
strategies, and two adversarial defense methods, assessing performance on both
clean and adversarial inputs. Results show that Hydra surpasses plug-in VLMs
and state-of-the-art (SOTA) dehallucination methods, even without explicit
adversarial defenses, demonstrating enhanced robustness and factual
consistency. By bridging adversarial resistance and hallucination mitigation,
Hydra provides a scalable, training-free solution for improving the reliability
of VLMs in real-world applications.

</details>


### [353] [Adversarial Attack for RGB-Event based Visual Object Tracking](https://arxiv.org/abs/2504.14423)
*Qiang Chen,Xiao Wang,Haowen Wang,Bo Jiang,Lin Zhu,Dawei Zhang,Yonghong Tian,Jin Tang*

Main category: cs.CV

TL;DR: 提出了一种针对RGB-Event视觉跟踪的跨模态对抗攻击算法，通过优化扰动和梯度引导策略，显著降低了跟踪器性能。


<details>
  <summary>Details</summary>
Motivation: 研究RGB-Event流跟踪算法的对抗攻击与防御，填补该领域的研究空白。

Method: 针对RGB-Event体素和帧表示，分别采用对抗损失优化扰动和梯度引导的空间位置扰动策略。

Result: 在多个数据集上验证了方法的有效性，显著降低了跟踪器性能。

Conclusion: 提出的跨模态对抗攻击算法在RGB-Event跟踪中表现出色，为未来研究提供了新方向。

Abstract: Visual object tracking is a crucial research topic in the fields of computer
vision and multi-modal fusion. Among various approaches, robust visual tracking
that combines RGB frames with Event streams has attracted increasing attention
from researchers. While striving for high accuracy and efficiency in tracking,
it is also important to explore how to effectively conduct adversarial attacks
and defenses on RGB-Event stream tracking algorithms, yet research in this area
remains relatively scarce. To bridge this gap, in this paper, we propose a
cross-modal adversarial attack algorithm for RGB-Event visual tracking. Because
of the diverse representations of Event streams, and given that Event voxels
and frames are more commonly used, this paper will focus on these two
representations for an in-depth study. Specifically, for the RGB-Event voxel,
we first optimize the perturbation by adversarial loss to generate RGB frame
adversarial examples. For discrete Event voxel representations, we propose a
two-step attack strategy, more in detail, we first inject Event voxels into the
target region as initialized adversarial examples, then, conduct a
gradient-guided optimization by perturbing the spatial location of the Event
voxels. For the RGB-Event frame based tracking, we optimize the cross-modal
universal perturbation by integrating the gradient information from multimodal
data. We evaluate the proposed approach against attacks on three widely used
RGB-Event Tracking datasets, i.e., COESOT, FE108, and VisEvent. Extensive
experiments show that our method significantly reduces the performance of the
tracker across numerous datasets in both unimodal and multimodal scenarios. The
source code will be released on
https://github.com/Event-AHU/Adversarial_Attack_Defense

</details>


### [354] [ResNetVLLM-2: Addressing ResNetVLLM's Multi-Modal Hallucinations](https://arxiv.org/abs/2504.14429)
*Ahmad Khalil,Mahmoud Khalil,Alioune Ngom*

Main category: cs.CV

TL;DR: 论文提出了一种针对视频语言模型ResNetVLLM的幻觉问题解决方案，通过两步策略（检测与缓解）提升模型的事实一致性。


<details>
  <summary>Details</summary>
Motivation: 解决视频语言模型中生成内容与视觉事实不符的多模态幻觉问题。

Method: 1. 使用改进的Lynx模型检测生成描述与真实视频的语义对齐；2. 采用检索增强生成（RAG）动态构建知识库以缓解幻觉。

Result: 在ActivityNet-QA基准测试中，准确率从54.8%提升至65.3%。

Conclusion: 提出的两步策略有效减少了多模态幻觉，显著提升了视频语言模型的可靠性。

Abstract: Large Language Models (LLMs) have transformed natural language processing
(NLP) tasks, but they suffer from hallucination, generating plausible yet
factually incorrect content. This issue extends to Video-Language Models
(VideoLLMs), where textual descriptions may inaccurately represent visual
content, resulting in multi-modal hallucinations. In this paper, we address
hallucination in ResNetVLLM, a video-language model combining ResNet visual
encoders with LLMs. We introduce a two-step protocol: (1) a faithfulness
detection strategy that uses a modified Lynx model to assess semantic alignment
between generated captions and ground-truth video references, and (2) a
hallucination mitigation strategy using Retrieval-Augmented Generation (RAG)
with an ad-hoc knowledge base dynamically constructed during inference. Our
enhanced model, ResNetVLLM-2, reduces multi-modal hallucinations by
cross-verifying generated content against external knowledge, improving factual
consistency. Evaluation on the ActivityNet-QA benchmark demonstrates a
substantial accuracy increase from 54.8% to 65.3%, highlighting the
effectiveness of our hallucination detection and mitigation strategies in
enhancing video-language model reliability.

</details>


### [355] [Neglected Risks: The Disturbing Reality of Children's Images in Datasets and the Urgent Call for Accountability](https://arxiv.org/abs/2504.14446)
*Carlos Caetano,Gabriel O. dos Santos,Caio Petrucci,Artur Barros,Camila Laranjeira,Leo S. F. Ribeiro,Júlia F. de Mendonça,Jefersson A. dos Santos,Sandra Avila*

Main category: cs.CV

TL;DR: 论文探讨了在AI数据集中使用儿童图像的伦理问题，并提出了一种检测和移除此类图像的流程。


<details>
  <summary>Details</summary>
Motivation: 儿童图像的使用涉及隐私、同意、数据保护和责任等伦理问题，但目前解决方案有限。

Method: 提出了一种基于视觉语言模型的流程，并在Visual Question Answering任务和#PraCegoVer数据集上测试，同时在Open Images V7数据集中评估其有效性。

Result: 流程为未来研究提供了基线，并呼吁社区反思和行动以保护儿童权利。

Conclusion: 研究旨在推动社区在创建数据集时更加谨慎，并开发工具保护弱势群体权利。

Abstract: Including children's images in datasets has raised ethical concerns,
particularly regarding privacy, consent, data protection, and accountability.
These datasets, often built by scraping publicly available images from the
Internet, can expose children to risks such as exploitation, profiling, and
tracking. Despite the growing recognition of these issues, approaches for
addressing them remain limited. We explore the ethical implications of using
children's images in AI datasets and propose a pipeline to detect and remove
such images. As a use case, we built the pipeline on a Vision-Language Model
under the Visual Question Answering task and tested it on the #PraCegoVer
dataset. We also evaluate the pipeline on a subset of 100,000 images from the
Open Images V7 dataset to assess its effectiveness in detecting and removing
images of children. The pipeline serves as a baseline for future research,
providing a starting point for more comprehensive tools and methodologies.
While we leverage existing models trained on potentially problematic data, our
goal is to expose and address this issue. We do not advocate for training or
deploying such models, but instead call for urgent community reflection and
action to protect children's rights. Ultimately, we aim to encourage the
research community to exercise - more than an additional - care in creating new
datasets and to inspire the development of tools to protect the fundamental
rights of vulnerable groups, particularly children.

</details>


### [356] [ResNetVLLM -- Multi-modal Vision LLM for the Video Understanding Task](https://arxiv.org/abs/2504.14432)
*Ahmad Khalil,Mahmoud Khalil,Alioune Ngom*

Main category: cs.CV

TL;DR: ResNetVLLM是一种新型跨模态框架，结合ResNet视觉编码器和大型语言模型，用于零样本视频理解，无需预训练视频模型。


<details>
  <summary>Details</summary>
Motivation: 解决零样本视频理解中依赖预训练模型的挑战，提出一种统一架构学习视觉和语义表示。

Method: 使用非预训练的ResNet提取视觉特征，与大型语言模型结合，生成视频的文本描述。

Result: 在多个基准测试（如MSRVTT-QA、MSVD-QA等）上达到零样本视频理解的先进性能。

Conclusion: ResNetVLLM通过统一架构有效提升零样本视频理解的准确性和上下文相关性。

Abstract: In this paper, we introduce ResNetVLLM (ResNet Vision LLM), a novel
cross-modal framework for zero-shot video understanding that integrates a
ResNet-based visual encoder with a Large Language Model (LLM. ResNetVLLM
addresses the challenges associated with zero-shot video models by avoiding
reliance on pre-trained video understanding models and instead employing a
non-pretrained ResNet to extract visual features. This design ensures the model
learns visual and semantic representations within a unified architecture,
enhancing its ability to generate accurate and contextually relevant textual
descriptions from video inputs. Our experimental results demonstrate that
ResNetVLLM achieves state-of-the-art performance in zero-shot video
understanding (ZSVU) on several benchmarks, including MSRVTT-QA, MSVD-QA,
TGIF-QA FrameQA, and ActivityNet-QA.

</details>


### [357] [DreamID: High-Fidelity and Fast diffusion-based Face Swapping via Triplet ID Group Learning](https://arxiv.org/abs/2504.14509)
*Fulong Ye,Miao Hua,Pengze Zhang,Xinghui Li,Qichao Sun,Songtao Zhao,Qian He,Xinglong Wu*

Main category: cs.CV

TL;DR: DreamID是一种基于扩散模型的人脸交换方法，通过显式监督和高效架构实现高质量、快速的换脸效果。


<details>
  <summary>Details</summary>
Motivation: 传统人脸交换方法依赖隐式监督，效果不佳。DreamID旨在通过显式监督提升身份相似性和属性保留。

Method: 构建Triplet ID Group数据，利用SD Turbo加速模型，提出SwapNet、FaceNet和ID Adapter架构，支持端到端训练。

Result: 在身份相似性、姿态表情保留和图像保真度上优于现有方法，512*512分辨率下仅需0.6秒。

Conclusion: DreamID在复杂场景下表现优异，为高质量人脸交换提供了高效解决方案。

Abstract: In this paper, we introduce DreamID, a diffusion-based face swapping model
that achieves high levels of ID similarity, attribute preservation, image
fidelity, and fast inference speed. Unlike the typical face swapping training
process, which often relies on implicit supervision and struggles to achieve
satisfactory results. DreamID establishes explicit supervision for face
swapping by constructing Triplet ID Group data, significantly enhancing
identity similarity and attribute preservation. The iterative nature of
diffusion models poses challenges for utilizing efficient image-space loss
functions, as performing time-consuming multi-step sampling to obtain the
generated image during training is impractical. To address this issue, we
leverage the accelerated diffusion model SD Turbo, reducing the inference steps
to a single iteration, enabling efficient pixel-level end-to-end training with
explicit Triplet ID Group supervision. Additionally, we propose an improved
diffusion-based model architecture comprising SwapNet, FaceNet, and ID Adapter.
This robust architecture fully unlocks the power of the Triplet ID Group
explicit supervision. Finally, to further extend our method, we explicitly
modify the Triplet ID Group data during training to fine-tune and preserve
specific attributes, such as glasses and face shape. Extensive experiments
demonstrate that DreamID outperforms state-of-the-art methods in terms of
identity similarity, pose and expression preservation, and image fidelity.
Overall, DreamID achieves high-quality face swapping results at 512*512
resolution in just 0.6 seconds and performs exceptionally well in challenging
scenarios such as complex lighting, large angles, and occlusions.

</details>


### [358] [VGNC: Reducing the Overfitting of Sparse-view 3DGS via Validation-guided Gaussian Number Control](https://arxiv.org/abs/2504.14548)
*Lifeng Lin,Rongfeng Lu,Quan Chen,Haofan Ren,Ming Lu,Yaoqi Sun,Chenggang Yan,Anke Xue*

Main category: cs.CV

TL;DR: VGNC提出了一种基于生成式新视角合成模型的验证引导高斯数量控制方法，用于减少稀疏视角3D高斯泼溅（3DGS）中的过拟合问题。


<details>
  <summary>Details</summary>
Motivation: 稀疏视角3D重建在实际应用中存在过拟合问题，现有3DGS方法虽有所改进但仍未解决。

Method: 通过生成验证图像并基于此控制高斯数量，以减少过拟合。

Result: 实验表明VGNC不仅减少过拟合，还提升渲染质量并降低存储和计算需求。

Conclusion: VGNC为稀疏视角3DGS提供了一种有效的过拟合缓解方案。

Abstract: Sparse-view 3D reconstruction is a fundamental yet challenging task in
practical 3D reconstruction applications. Recently, many methods based on the
3D Gaussian Splatting (3DGS) framework have been proposed to address
sparse-view 3D reconstruction. Although these methods have made considerable
advancements, they still show significant issues with overfitting. To reduce
the overfitting, we introduce VGNC, a novel Validation-guided Gaussian Number
Control (VGNC) approach based on generative novel view synthesis (NVS) models.
To the best of our knowledge, this is the first attempt to alleviate the
overfitting issue of sparse-view 3DGS with generative validation images.
Specifically, we first introduce a validation image generation method based on
a generative NVS model. We then propose a Gaussian number control strategy that
utilizes generated validation images to determine the optimal Gaussian numbers,
thereby reducing the issue of overfitting. We conducted detailed experiments on
various sparse-view 3DGS baselines and datasets to evaluate the effectiveness
of VGNC. Extensive experiments show that our approach not only reduces
overfitting but also improves rendering quality on the test set while
decreasing the number of Gaussian points. This reduction lowers storage demands
and accelerates both training and rendering. The code will be released.

</details>


### [359] [Time Frequency Analysis of EMG Signal for Gesture Recognition using Fine grained Features](https://arxiv.org/abs/2504.14708)
*Parshuram N. Aarotale,Ajita Rattani*

Main category: cs.CV

TL;DR: 论文提出了一种基于EMG的手势识别新方法XMANet，通过跨层互注意力结合局部和语义特征，在多个数据集上显著优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 提升EMG手势识别的准确性和鲁棒性，用于假肢控制、康复和人机交互。

Method: 使用XMANet模型，结合短时傅里叶变换和小波变换生成的特征图，通过跨层互注意力机制整合浅层和深层CNN特征。

Result: 在Grabmyo和FORS EMG数据集上，XMANet相比基线模型（如ResNet50、DenseNet121等）性能提升1.46%至9.36%。

Conclusion: XMANet通过细粒度特征提取和跨层注意力机制，显著提升了EMG手势识别的性能，展现了其广泛应用潜力。

Abstract: Electromyography (EMG) based hand gesture recognition converts forearm muscle
activity into control commands for prosthetics, rehabilitation, and human
computer interaction. This paper proposes a novel approach to EMG-based hand
gesture recognition that uses fine-grained classification and presents XMANet,
which unifies low-level local and high level semantic cues through cross layer
mutual attention among shallow to deep CNN experts. Using stacked spectrograms
and scalograms derived from the Short Time Fourier Transform (STFT) and Wavelet
Transform (WT), we benchmark XMANet against ResNet50, DenseNet-121,
MobileNetV3, and EfficientNetB0. Experimental results on the Grabmyo dataset
indicate that, using STFT, the proposed XMANet model outperforms the baseline
ResNet50, EfficientNetB0, MobileNetV3, and DenseNet121 models with improvement
of approximately 1.72%, 4.38%, 5.10%, and 2.53%, respectively. When employing
the WT approach, improvements of around 1.57%, 1.88%, 1.46%, and 2.05% are
observed over the same baselines. Similarly, on the FORS EMG dataset, the
XMANet(ResNet50) model using STFT shows an improvement of about 5.04% over the
baseline ResNet50. In comparison, the XMANet(DenseNet121) and
XMANet(MobileNetV3) models yield enhancements of approximately 4.11% and 2.81%,
respectively. Moreover, when using WT, the proposed XMANet achieves gains of
around 4.26%, 9.36%, 5.72%, and 6.09% over the baseline ResNet50, DenseNet121,
MobileNetV3, and EfficientNetB0 models, respectively. These results confirm
that XMANet consistently improves performance across various architectures and
signal processing techniques, demonstrating the strong potential of fine
grained features for accurate and robust EMG classification.

</details>


### [360] [TAPIP3D: Tracking Any Point in Persistent 3D Geometry](https://arxiv.org/abs/2504.14717)
*Bowei Zhang,Lei Ke,Adam W. Harley,Katerina Fragkiadaki*

Main category: cs.CV

TL;DR: TAPIP3D是一种用于单目RGB和RGB-D视频中长期3D点跟踪的新方法，通过将视频表示为相机稳定的时空特征云，并利用深度和相机运动信息提升2D特征到3D空间，显著提升了跟踪性能。


<details>
  <summary>Details</summary>
Motivation: 现有3D点跟踪方法在处理长期跟踪和相机运动时表现不佳，TAPIP3D旨在通过稳定的3D表示和局部注意力机制解决这些问题。

Method: TAPIP3D将视频表示为相机稳定的3D特征云，利用深度和相机运动信息提升2D特征到3D空间，并通过局部对注意力机制优化3D运动估计。

Result: TAPIP3D在3D点跟踪任务中显著优于现有方法，并在有准确深度信息时提升了2D跟踪精度。

Conclusion: TAPIP3D通过稳定的3D表示和局部注意力机制，实现了更鲁棒和准确的3D点跟踪，同时支持相机坐标系和世界坐标系的推理。

Abstract: We introduce TAPIP3D, a novel approach for long-term 3D point tracking in
monocular RGB and RGB-D videos. TAPIP3D represents videos as camera-stabilized
spatio-temporal feature clouds, leveraging depth and camera motion information
to lift 2D video features into a 3D world space where camera motion is
effectively canceled. TAPIP3D iteratively refines multi-frame 3D motion
estimates within this stabilized representation, enabling robust tracking over
extended periods. To manage the inherent irregularities of 3D point
distributions, we propose a Local Pair Attention mechanism. This 3D
contextualization strategy effectively exploits spatial relationships in 3D,
forming informative feature neighborhoods for precise 3D trajectory estimation.
Our 3D-centric approach significantly outperforms existing 3D point tracking
methods and even enhances 2D tracking accuracy compared to conventional 2D
pixel trackers when accurate depth is available. It supports inference in both
camera coordinates (i.e., unstabilized) and world coordinates, and our results
demonstrate that compensating for camera motion improves tracking performance.
Our approach replaces the conventional 2D square correlation neighborhoods used
in prior 2D and 3D trackers, leading to more robust and accurate results across
various 3D point tracking benchmarks. Project Page: https://tapip3d.github.io

</details>


### [361] [VM-BHINet:Vision Mamba Bimanual Hand Interaction Network for 3D Interacting Hand Mesh Recovery From a Single RGB Image](https://arxiv.org/abs/2504.14618)
*Han Bi,Ge Yu,Yu He,Wenzhuo Liu,Zijie Zheng*

Main category: cs.CV

TL;DR: VM-BHINet利用状态空间模型改进双手交互重建，显著降低误差。


<details>
  <summary>Details</summary>
Motivation: 现有方法在遮挡、模糊外观和计算效率方面存在问题。

Method: 提出VM-BHINet，结合状态空间模型和局部全局特征操作。

Result: 在InterHand2.6M数据集上，MPJPE和MPVPE降低2-3%。

Conclusion: VM-BHINet在双手交互重建中表现优于现有方法。

Abstract: Understanding bimanual hand interactions is essential for realistic 3D pose
and shape reconstruction. However, existing methods struggle with occlusions,
ambiguous appearances, and computational inefficiencies. To address these
challenges, we propose Vision Mamba Bimanual Hand Interaction Network
(VM-BHINet), introducing state space models (SSMs) into hand reconstruction to
enhance interaction modeling while improving computational efficiency. The core
component, Vision Mamba Interaction Feature Extraction Block (VM-IFEBlock),
combines SSMs with local and global feature operations, enabling deep
understanding of hand interactions. Experiments on the InterHand2.6M dataset
show that VM-BHINet reduces Mean per-joint position error (MPJPE) and Mean
per-vertex position error (MPVPE) by 2-3%, significantly surpassing
state-of-the-art methods.

</details>


### [362] [Real-Time Sleepiness Detection for Driver State Monitoring System](https://arxiv.org/abs/2504.14807)
*Deepak Ghimire,Sunghwan Jeong,Sunhong Yoon,Sanghyun Park,Juhwan Choi*

Main category: cs.CV

TL;DR: 提出了一种基于计算机视觉的实时驾驶员眼睛状态检测方法，结合动态模板匹配和Kalman滤波跟踪，使用SVM分类器判断眼睛开闭状态，并在检测到疲劳时触发警报。


<details>
  <summary>Details</summary>
Motivation: 驾驶员疲劳是导致许多事故的重要因素，实时监测驾驶员眼睛状态可以有效预防疲劳驾驶。

Method: 1. 检测人脸并定位眼睛区域；2. 使用动态模板匹配和Kalman滤波跟踪眼睛位置；3. 结合HOG特征和SVM分类器判断眼睛开闭状态。

Result: 系统能够实时检测驾驶员眼睛状态，并在检测到疲劳时触发警报。

Conclusion: 该方法有效实现了驾驶员疲劳监测，有助于减少疲劳驾驶引发的事故。

Abstract: A driver face monitoring system can detect driver fatigue, which is a
significant factor in many accidents, using computer vision techniques. In this
paper, we present a real-time technique for driver eye state detection. First,
the face is detected, and the eyes are located within the face region for
tracking. A normalized cross-correlation-based online dynamic template matching
technique, combined with Kalman filter tracking, is proposed to track the
detected eye positions in subsequent image frames. A support vector machine
with histogram of oriented gradients (HOG) features is used to classify the
state of the eyes as open or closed. If the eyes remain closed for a specified
period, the driver is considered to be asleep, and an alarm is triggered.

</details>


### [363] [ReSpec: Relevance and Specificity Grounded Online Filtering for Learning on Video-Text Data Streams](https://arxiv.org/abs/2504.14875)
*Chris Dongjoo Kim,Jihwan Moon,Sangwoo Moon,Heeseung Yun,Sihaeng Lee,Aniruddha Kembhavi,Soonyoung Lee,Gunhee Kim,Sangho Lee,Christopher Clark*

Main category: cs.CV

TL;DR: ReSpec框架通过实时过滤视频文本数据，提升在线学习效率，仅需5%数据即可在零样本视频检索任务中达到最优性能。


<details>
  <summary>Details</summary>
Motivation: 解决视频文本数据快速增长带来的存储和计算挑战，同时满足实时响应需求。

Method: 提出基于相关性和特异性的在线过滤框架（ReSpec），通过四种标准选择数据：模态对齐、任务相关性、特异性和效率。

Result: 在WebVid2M和VideoCC3M数据集上，ReSpec在五个零样本视频检索任务中表现最优，仅需5%数据且计算开销低。

Conclusion: ReSpec为在线学习提供了一种高效、实时的数据选择方法，显著减少了存储和计算需求。

Abstract: The rapid growth of video-text data presents challenges in storage and
computation during training. Online learning, which processes streaming data in
real-time, offers a promising solution to these issues while also allowing
swift adaptations in scenarios demanding real-time responsiveness. One strategy
to enhance the efficiency and effectiveness of learning involves identifying
and prioritizing data that enhances performance on target downstream tasks. We
propose Relevance and Specificity-based online filtering framework (ReSpec)
that selects data based on four criteria: (i) modality alignment for clean
data, (ii) task relevance for target focused data, (iii) specificity for
informative and detailed data, and (iv) efficiency for low-latency processing.
Relevance is determined by the probabilistic alignment of incoming data with
downstream tasks, while specificity employs the distance to a root embedding
representing the least specific data as an efficient proxy for informativeness.
By establishing reference points from target task data, ReSpec filters incoming
data in real-time, eliminating the need for extensive storage and compute.
Evaluating on large-scale datasets WebVid2M and VideoCC3M, ReSpec attains
state-of-the-art performance on five zeroshot video retrieval tasks, using as
little as 5% of the data while incurring minimal compute. The source code is
available at https://github.com/cdjkim/ReSpec.

</details>


### [364] [Video-MMLU: A Massive Multi-Discipline Lecture Understanding Benchmark](https://arxiv.org/abs/2504.14693)
*Enxin Song,Wenhao Chai,Weili Xu,Jianwen Xie,Yuxuan Liu,Gaoang Wang*

Main category: cs.CV

TL;DR: Video-MMLU是一个评估多学科讲座理解能力的大规模基准，测试了90多个模型，发现当前模型在感知与推理任务上存在局限。


<details>
  <summary>Details</summary>
Motivation: 探索语言多模态模型（LMMs）在多学科讲座理解中的潜力，填补该领域的研究空白。

Method: 引入Video-MMLU基准，评估90多个开源和专有模型，分析视觉标记数量和大语言模型对性能的影响。

Result: 当前模型在多学科讲座的感知与推理任务上表现不足，视觉标记数量和大语言模型对性能有显著影响。

Conclusion: 多学科讲座理解需要更强的感知与推理能力，未来研究需优化模型设计。

Abstract: Recent advancements in language multimodal models (LMMs) for video have
demonstrated their potential for understanding video content, yet the task of
comprehending multi-discipline lectures remains largely unexplored. We
introduce Video-MMLU, a massive benchmark designed to evaluate the capabilities
of LMMs in understanding Multi-Discipline Lectures. We evaluate over 90
open-source and proprietary models, ranging from 0.5B to 40B parameters. Our
results highlight the limitations of current models in addressing the cognitive
challenges presented by these lectures, especially in tasks requiring both
perception and reasoning. Additionally, we explore how the number of visual
tokens and the large language models influence performance, offering insights
into the interplay between multimodal perception and reasoning in lecture
comprehension.

</details>


### [365] [IXGS-Intraoperative 3D Reconstruction from Sparse, Arbitrarily Posed Real X-rays](https://arxiv.org/abs/2504.14699)
*Sascha Jecklin,Aidana Massalimova,Ruyi Zha,Lilian Calvet,Christoph J. Laux,Mazda Farshad,Philipp Fürnstahl*

Main category: cs.CV

TL;DR: 该论文提出了一种基于实例学习的3D脊柱重建方法，通过扩展R²-Gaussian splatting框架，结合解剖学引导的放射标准化步骤，实现了从稀疏X射线数据中重建解剖一致的3D体积。


<details>
  <summary>Details</summary>
Motivation: 脊柱手术需要高精度的3D成像支持，但现有监督学习方法依赖大量标注数据且泛化能力有限。本文旨在探索无需预训练的实例学习方法，以解决稀疏X射线数据下的3D重建问题。

Method: 扩展R²-Gaussian splatting框架，引入解剖学引导的放射标准化步骤（风格迁移），提升多视角视觉一致性和重建质量。

Result: 在体外数据集上验证，专家评估确认其临床实用性，20-30视角下效果最佳；定量指标（PSNR/SSIM）显示标准化步骤显著提升性能。

Conclusion: 该研究证明了基于实例学习的稀疏X射线3D重建可行性，为手术导航中的3D成像提供了新思路。

Abstract: Spine surgery is a high-risk intervention demanding precise execution, often
supported by image-based navigation systems. Recently, supervised learning
approaches have gained attention for reconstructing 3D spinal anatomy from
sparse fluoroscopic data, significantly reducing reliance on
radiation-intensive 3D imaging systems. However, these methods typically
require large amounts of annotated training data and may struggle to generalize
across varying patient anatomies or imaging conditions. Instance-learning
approaches like Gaussian splatting could offer an alternative by avoiding
extensive annotation requirements. While Gaussian splatting has shown promise
for novel view synthesis, its application to sparse, arbitrarily posed real
intraoperative X-rays has remained largely unexplored. This work addresses this
limitation by extending the $R^2$-Gaussian splatting framework to reconstruct
anatomically consistent 3D volumes under these challenging conditions. We
introduce an anatomy-guided radiographic standardization step using style
transfer, improving visual consistency across views, and enhancing
reconstruction quality. Notably, our framework requires no pretraining, making
it inherently adaptable to new patients and anatomies. We evaluated our
approach using an ex-vivo dataset. Expert surgical evaluation confirmed the
clinical utility of the 3D reconstructions for navigation, especially when
using 20 to 30 views, and highlighted the standardization's benefit for
anatomical clarity. Benchmarking via quantitative 2D metrics (PSNR/SSIM)
confirmed performance trade-offs compared to idealized settings, but also
validated the improvement gained from standardization over raw inputs. This
work demonstrates the feasibility of instance-based volumetric reconstruction
from arbitrary sparse-view X-rays, advancing intraoperative 3D imaging for
surgical navigation.

</details>


### [366] [Exposing the Copycat Problem of Imitation-based Planner: A Novel Closed-Loop Simulator, Causal Benchmark and Joint IL-RL Baseline](https://arxiv.org/abs/2504.14709)
*Hui Zhou,Shaoshuai Shi,Hongsheng Li*

Main category: cs.CV

TL;DR: 论文提出了一种结合模仿学习和强化学习的新框架，以解决模仿学习在自动驾驶规划中的局限性，包括过拟合和泛化能力不足的问题。


<details>
  <summary>Details</summary>
Motivation: 模仿学习在自动驾驶规划中表现优异，但难以验证其是否真正理解驾驶原则，且容易过拟合常见场景，泛化能力有限。

Method: 提出了一种闭环模拟器、因果基准测试框架，以及结合模仿学习和强化学习的新方法。

Result: 新框架旨在克服模仿学习的局限性，提升泛化能力。

Conclusion: 通过结合模仿学习和强化学习，能够更有效地解决自动驾驶规划中的挑战。

Abstract: Machine learning (ML)-based planners have recently gained significant
attention. They offer advantages over traditional optimization-based planning
algorithms. These advantages include fewer manually selected parameters and
faster development. Within ML-based planning, imitation learning (IL) is a
common algorithm. It primarily learns driving policies directly from supervised
trajectory data. While IL has demonstrated strong performance on many open-loop
benchmarks, it remains challenging to determine if the learned policy truly
understands fundamental driving principles, rather than simply extrapolating
from the ego-vehicle's initial state. Several studies have identified this
limitation and proposed algorithms to address it. However, these methods often
use original datasets for evaluation. In these datasets, future trajectories
are heavily dependent on initial conditions. Furthermore, IL often overfits to
the most common scenarios. It struggles to generalize to rare or unseen
situations.
  To address these challenges, this work proposes: 1) a novel closed-loop
simulator supporting both imitation and reinforcement learning, 2) a causal
benchmark derived from the Waymo Open Dataset to rigorously assess the impact
of the copycat problem, and 3) a novel framework integrating imitation learning
and reinforcement learning to overcome the limitations of purely imitative
approaches. The code for this work will be released soon.

</details>


### [367] [SuperCL: Superpixel Guided Contrastive Learning for Medical Image Segmentation Pre-training](https://arxiv.org/abs/2504.14737)
*Shuang Zeng,Lei Zhu,Xinliang Zhang,Hangzhou He,Yanye Lu*

Main category: cs.CV

TL;DR: 论文提出了一种名为SuperCL的新型对比学习方法，用于医学图像分割预训练，通过利用图像的结构先验和像素相关性，解决了现有方法在对比对生成和特征提取上的不足。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割任务面临高质量标注数据稀缺的挑战，现有对比学习方法在实例级或像素级表示上存在局限性，且对比对生成依赖手动阈值设置，效率低且缺乏泛化性。

Method: SuperCL引入两种新的对比对生成策略：Intra-image Local Contrastive Pairs (ILCP)和Inter-image Global Contrastive Pairs (IGCP)，并利用超像素图生成伪掩码指导监督对比学习。此外，还提出了ASP和CCL模块以更好地利用结构先验信息。

Result: 在8个医学图像数据集上的实验表明，SuperCL优于12种现有方法，在MMWHS、CHAOS和Spleen数据集上分别比之前最佳结果高出3.15%、5.44%和7.89%的DSC。

Conclusion: SuperCL通过创新的对比对生成策略和结构先验利用，显著提升了医学图像分割的性能，尤其在标注数据有限的情况下表现优异。

Abstract: Medical image segmentation is a critical yet challenging task, primarily due
to the difficulty of obtaining extensive datasets of high-quality,
expert-annotated images. Contrastive learning presents a potential but still
problematic solution to this issue. Because most existing methods focus on
extracting instance-level or pixel-to-pixel representation, which ignores the
characteristics between intra-image similar pixel groups. Moreover, when
considering contrastive pairs generation, most SOTA methods mainly rely on
manually setting thresholds, which requires a large number of gradient
experiments and lacks efficiency and generalization. To address these issues,
we propose a novel contrastive learning approach named SuperCL for medical
image segmentation pre-training. Specifically, our SuperCL exploits the
structural prior and pixel correlation of images by introducing two novel
contrastive pairs generation strategies: Intra-image Local Contrastive Pairs
(ILCP) Generation and Inter-image Global Contrastive Pairs (IGCP) Generation.
Considering superpixel cluster aligns well with the concept of contrastive
pairs generation, we utilize the superpixel map to generate pseudo masks for
both ILCP and IGCP to guide supervised contrastive learning. Moreover, we also
propose two modules named Average SuperPixel Feature Map Generation (ASP) and
Connected Components Label Generation (CCL) to better exploit the prior
structural information for IGCP. Finally, experiments on 8 medical image
datasets indicate our SuperCL outperforms existing 12 methods. i.e. Our SuperCL
achieves a superior performance with more precise predictions from
visualization figures and 3.15%, 5.44%, 7.89% DSC higher than the previous best
results on MMWHS, CHAOS, Spleen with 10% annotations. Our code will be released
after acceptance.

</details>


### [368] [Automated Measurement of Eczema Severity with Self-Supervised Learning](https://arxiv.org/abs/2504.15193)
*Neelesh Kumar,Oya Aran*

Main category: cs.CV

TL;DR: 提出一种基于自监督学习的湿疹自动诊断框架，在有限标注数据下表现优于现有深度学习方法。


<details>
  <summary>Details</summary>
Motivation: 现有湿疹自动诊断方法依赖大量标注数据，但标注数据难以获取。

Method: 采用两阶段框架：1) 使用SegGPT进行少量样本分割；2) 提取DINO特征并用MLP进行四分类。

Result: 在真实湿疹图像数据集上，加权F1分数（0.67±0.01）优于Resnet-18（0.44±0.16）和Vision Transformer（0.40±0.22）。

Conclusion: 自监督学习是标注数据稀缺时皮肤自动诊断的有效解决方案。

Abstract: Automated diagnosis of eczema using images acquired from digital camera can
enable individuals to self-monitor their recovery. The process entails first
segmenting out the eczema region from the image and then measuring the severity
of eczema in the segmented region. The state-of-the-art methods for automated
eczema diagnosis rely on deep neural networks such as convolutional neural
network (CNN) and have shown impressive performance in accurately measuring the
severity of eczema. However, these methods require massive volume of annotated
data to train which can be hard to obtain. In this paper, we propose a
self-supervised learning framework for automated eczema diagnosis under limited
training data regime. Our framework consists of two stages: i) Segmentation,
where we use an in-context learning based algorithm called SegGPT for few-shot
segmentation of eczema region from the image; ii) Feature extraction and
classification, where we extract DINO features from the segmented regions and
feed it to a multi-layered perceptron (MLP) for 4-class classification of
eczema severity. When evaluated on a dataset of annotated "in-the-wild" eczema
images, we show that our method outperforms (Weighted F1: 0.67 $\pm$ 0.01) the
state-of-the-art deep learning methods such as finetuned Resnet-18 (Weighted
F1: 0.44 $\pm$ 0.16) and Vision Transformer (Weighted F1: 0.40 $\pm$ 0.22). Our
results show that self-supervised learning can be a viable solution for
automated skin diagnosis where labeled data is scarce.

</details>


### [369] [Zero-Shot, But at What Cost? Unveiling the Hidden Overhead of MILS's LLM-CLIP Framework for Image Captioning](https://arxiv.org/abs/2504.15199)
*Yassir Benhammou,Alessandro Tiberio,Gabriel Trautmann,Suman Kalyan*

Main category: cs.CV

TL;DR: MILS框架声称LLM无需训练即可处理多模态任务，但研究发现其迭代优化过程带来高计算成本，而BLIP-2和GPT-4V等单步方法表现更优。


<details>
  <summary>Details</summary>
Motivation: 揭示MILS框架在零样本图像描述中的隐藏计算成本，挑战其“无需训练”的实际可行性。

Method: 通过对比MILS与BLIP-2、GPT-4V的性能和计算成本，量化其效率问题。

Result: MILS的高计算成本可能抵消其零样本优势，单步方法更高效。

Conclusion: 设计高效多模态模型需权衡输出质量与计算成本，MILS的迭代方法可能不实用。

Abstract: MILS (Multimodal Iterative LLM Solver) is a recently published framework that
claims "LLMs can see and hear without any training" by leveraging an iterative,
LLM-CLIP based approach for zero-shot image captioning. While this MILS
approach demonstrates good performance, our investigation reveals that this
success comes at a hidden, substantial computational cost due to its expensive
multi-step refinement process. In contrast, alternative models such as BLIP-2
and GPT-4V achieve competitive results through a streamlined, single-pass
approach. We hypothesize that the significant overhead inherent in MILS's
iterative process may undermine its practical benefits, thereby challenging the
narrative that zero-shot performance can be attained without incurring heavy
resource demands. This work is the first to expose and quantify the trade-offs
between output quality and computational cost in MILS, providing critical
insights for the design of more efficient multimodal models.

</details>


### [370] [How Effective Can Dropout Be in Multiple Instance Learning ?](https://arxiv.org/abs/2504.14783)
*Wenhui Zhu,Peijie Qiu,Xiwen Chen,Zhangsihao Yang,Aristeidis Sotiras,Abolfazl Razi,Yalin Wang*

Main category: cs.CV

TL;DR: 论文提出了一种名为MIL-Dropout的新方法，通过丢弃包中最重要实例来提升多实例学习（MIL）的性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统MIL在WSI分类中因两阶段训练方案导致特征嵌入噪声和弱监督问题，现有技术（如dropout）未在MIL中充分探索。

Method: 提出MIL-Dropout，系统决定丢弃包中最重要的实例，以优化性能。

Result: 在五个MIL基准数据集和两个WSI数据集上验证了MIL-Dropout的有效性，显著提升了性能且计算成本低。

Conclusion: MIL-Dropout是一种简单有效的方法，能显著提升MIL性能，尤其在噪声环境下表现优异。

Abstract: Multiple Instance Learning (MIL) is a popular weakly-supervised method for
various applications, with a particular interest in histological whole slide
image (WSI) classification. Due to the gigapixel resolution of WSI,
applications of MIL in WSI typically necessitate a two-stage training scheme:
first, extract features from the pre-trained backbone and then perform MIL
aggregation. However, it is well-known that this suboptimal training scheme
suffers from "noisy" feature embeddings from the backbone and inherent weak
supervision, hindering MIL from learning rich and generalizable features.
However, the most commonly used technique (i.e., dropout) for mitigating this
issue has yet to be explored in MIL. In this paper, we empirically explore how
effective the dropout can be in MIL. Interestingly, we observe that dropping
the top-k most important instances within a bag leads to better performance and
generalization even under noise attack. Based on this key observation, we
propose a novel MIL-specific dropout method, termed MIL-Dropout, which
systematically determines which instances to drop. Experiments on five MIL
benchmark datasets and two WSI datasets demonstrate that MIL-Dropout boosts the
performance of current MIL methods with a negligible computational cost. The
code is available at https://github.com/ChongQingNoSubway/MILDropout.

</details>


### [371] [ECViT: Efficient Convolutional Vision Transformer with Local-Attention and Multi-scale Stages](https://arxiv.org/abs/2504.14825)
*Zhoujie Qian*

Main category: cs.CV

TL;DR: ECViT是一种结合CNN和Transformer优势的混合架构，通过引入局部性和平移不变性等归纳偏置，解决了ViTs的高计算成本和数据需求问题。


<details>
  <summary>Details</summary>
Motivation: ViTs在计算机视觉中表现出色，但面临高计算成本和大量训练数据需求的问题。

Method: 提出ECViT，结合CNN的局部性和Transformer的长程依赖建模能力，采用局部注意力和金字塔结构实现高效多尺度特征提取。

Result: ECViT在图像分类任务中性能优异，同时保持低计算和存储需求。

Conclusion: ECViT为高效且高性能的应用提供了理想解决方案。

Abstract: Vision Transformers (ViTs) have revolutionized computer vision by leveraging
self-attention to model long-range dependencies. However, ViTs face challenges
such as high computational costs due to the quadratic scaling of self-attention
and the requirement of a large amount of training data. To address these
limitations, we propose the Efficient Convolutional Vision Transformer (ECViT),
a hybrid architecture that effectively combines the strengths of CNNs and
Transformers. ECViT introduces inductive biases such as locality and
translation invariance, inherent to Convolutional Neural Networks (CNNs) into
the Transformer framework by extracting patches from low-level features and
enhancing the encoder with convolutional operations. Additionally, it
incorporates local-attention and a pyramid structure to enable efficient
multi-scale feature extraction and representation. Experimental results
demonstrate that ECViT achieves an optimal balance between performance and
efficiency, outperforming state-of-the-art models on various image
classification tasks while maintaining low computational and storage
requirements. ECViT offers an ideal solution for applications that prioritize
high efficiency without compromising performance.

</details>


### [372] [Object-Level Verbalized Confidence Calibration in Vision-Language Models via Semantic Perturbation](https://arxiv.org/abs/2504.14848)
*Yunpu Zhao,Rui Zhang,Junbin Xiao,Ruibo Hou,Jiaming Guo,Zihao Zhang,Yifan Hao,Yunji Chen*

Main category: cs.CV

TL;DR: 提出了一种名为CSP的新框架，通过语义扰动改善视觉语言模型（VLM）的置信度校准，提升其可靠性和可解释性。


<details>
  <summary>Details</summary>
Motivation: VLM在多模态任务中表现优异，但置信度校准不佳，导致用户信任度下降，尤其是在模型错误或虚构信息时。

Method: 引入高斯噪声扰动关键对象区域，模拟视觉不确定性，并通过两阶段训练（监督微调和偏好优化）增强校准。

Result: 实验表明，该方法显著改善了置信度与响应正确性的对齐，同时保持或提升了任务性能。

Conclusion: 语义扰动是提升VLM可靠性和可解释性的实用工具。

Abstract: Vision-language models (VLMs) excel in various multimodal tasks but
frequently suffer from poor calibration, resulting in misalignment between
their verbalized confidence and response correctness. This miscalibration
undermines user trust, especially when models confidently provide incorrect or
fabricated information. In this work, we propose a novel Confidence Calibration
through Semantic Perturbation (CSP) framework to improve the calibration of
verbalized confidence for VLMs in response to object-centric queries. We first
introduce a perturbed dataset where Gaussian noise is applied to the key object
regions to simulate visual uncertainty at different confidence levels,
establishing an explicit mapping between visual ambiguity and confidence
levels. We further enhance calibration through a two-stage training process
combining supervised fine-tuning on the perturbed dataset with subsequent
preference optimization. Extensive experiments on popular benchmarks
demonstrate that our method significantly improves the alignment between
verbalized confidence and response correctness while maintaining or enhancing
overall task performance. These results highlight the potential of semantic
perturbation as a practical tool for improving the reliability and
interpretability of VLMs.

</details>


### [373] [Bridge the Gap: From Weak to Full Supervision for Temporal Action Localization with PseudoFormer](https://arxiv.org/abs/2504.14860)
*Ziyi Liu,Yangcen Liu*

Main category: cs.CV

TL;DR: PseudoFormer提出了一种新颖的双分支框架，通过生成高质量伪标签和利用不同先验信息，缩小了弱监督与全监督时间动作定位的性能差距。


<details>
  <summary>Details</summary>
Motivation: 弱监督时间动作定位（WTAL）因缺乏时间标注而性能受限，现有方法在伪标签质量、先验利用和噪声标签训练方面存在挑战。

Method: 提出PseudoFormer框架，包括RickerFusion生成高质量伪标签，利用片段级和提议级标签训练回归模型，并引入不确定性掩码和迭代优化机制。

Result: 在THUMOS14和ActivityNet1.3基准测试中达到最优性能，消融实验验证了各模块的有效性。

Conclusion: PseudoFormer通过改进伪标签生成和训练策略，显著提升了弱监督时间动作定位的性能。

Abstract: Weakly-supervised Temporal Action Localization (WTAL) has achieved notable
success but still suffers from a lack of temporal annotations, leading to a
performance and framework gap compared with fully-supervised methods. While
recent approaches employ pseudo labels for training, three key challenges:
generating high-quality pseudo labels, making full use of different priors, and
optimizing training methods with noisy labels remain unresolved. Due to these
perspectives, we propose PseudoFormer, a novel two-branch framework that
bridges the gap between weakly and fully-supervised Temporal Action
Localization (TAL). We first introduce RickerFusion, which maps all predicted
action proposals to a global shared space to generate pseudo labels with better
quality. Subsequently, we leverage both snippet-level and proposal-level labels
with different priors from the weak branch to train the regression-based model
in the full branch. Finally, the uncertainty mask and iterative refinement
mechanism are applied for training with noisy pseudo labels. PseudoFormer
achieves state-of-the-art WTAL results on the two commonly used benchmarks,
THUMOS14 and ActivityNet1.3. Besides, extensive ablation studies demonstrate
the contribution of each component of our method.

</details>


### [374] [Guidelines for External Disturbance Factors in the Use of OCR in Real-World Environments](https://arxiv.org/abs/2504.14913)
*Kenji Iwata,Eiki Ishidera,Toshifumi Yamaai,Yutaka Satoh,Hiroshi Tanaka,Katsuhiko Takahashi,Akio Furuhata,Yoshihisa Tanabe,Hiroshi Matsumura*

Main category: cs.CV

TL;DR: 论文总结了OCR性能受外部干扰因素影响的问题，并整理了干扰因素表和应对指南。


<details>
  <summary>Details</summary>
Motivation: 随着OCR应用范围扩大，外部干扰导致性能下降，需提供解决方案以确保用户正确使用OCR。

Method: 整理现实中的外部干扰因素及其导致的图像退化现象，形成干扰因素表，并制定使用指南。

Result: 提出了外部干扰因素表和应对指南，帮助用户优化OCR使用环境。

Conclusion: 通过整理干扰因素和提供指南，可有效提升OCR在复杂环境中的性能表现。

Abstract: The performance of OCR has improved with the evolution of AI technology. As
OCR continues to broaden its range of applications, the increased likelihood of
interference introduced by various usage environments can prevent it from
achieving its inherent performance. This results in reduced recognition
accuracy under certain conditions, and makes the quality control of recognition
devices more challenging. Therefore, to ensure that users can properly utilize
OCR, we compiled the real-world external disturbance factors that cause
performance degradation, along with the resulting image degradation phenomena,
into an external disturbance factor table and, by also indicating how to make
use of it, organized them into guidelines.

</details>


### [375] [Fast Adversarial Training with Weak-to-Strong Spatial-Temporal Consistency in the Frequency Domain on Videos](https://arxiv.org/abs/2504.14921)
*Songping Wang,Hanqing Liu,Yueming Lyu,Xiantao Hu,Ziwen He,Wei Wang,Caifeng Shan,Liang Wang*

Main category: cs.CV

TL;DR: VFAT-WS是一种针对视频数据的快速对抗训练方法，通过时间频率增强和弱到强一致性正则化，显著提升了对抗鲁棒性和训练效率。


<details>
  <summary>Details</summary>
Motivation: 视频对抗训练面临计算成本高和清洁准确性与对抗鲁棒性之间的权衡问题，VFAT-WS旨在解决这些挑战。

Method: 结合时间频率增强（TF-AUG）及其时空增强形式（STF-AUG），以及单步PGD攻击，同时采用弱到强时空一致性正则化。

Result: 在UCF-101和HMDB-51数据集上，VFAT-WS显著提升了对抗鲁棒性和训练效率（加速490%）。

Conclusion: VFAT-WS在视频对抗训练中实现了清洁准确性与对抗鲁棒性的更好平衡，同时大幅提高了训练效率。

Abstract: Adversarial Training (AT) has been shown to significantly enhance adversarial
robustness via a min-max optimization approach. However, its effectiveness in
video recognition tasks is hampered by two main challenges. First, fast
adversarial training for video models remains largely unexplored, which
severely impedes its practical applications. Specifically, most video
adversarial training methods are computationally costly, with long training
times and high expenses. Second, existing methods struggle with the trade-off
between clean accuracy and adversarial robustness. To address these challenges,
we introduce Video Fast Adversarial Training with Weak-to-Strong consistency
(VFAT-WS), the first fast adversarial training method for video data.
Specifically, VFAT-WS incorporates the following key designs: First, it
integrates a straightforward yet effective temporal frequency augmentation
(TF-AUG), and its spatial-temporal enhanced form STF-AUG, along with a
single-step PGD attack to boost training efficiency and robustness. Second, it
devises a weak-to-strong spatial-temporal consistency regularization, which
seamlessly integrates the simpler TF-AUG and the more complex STF-AUG.
Leveraging the consistency regularization, it steers the learning process from
simple to complex augmentations. Both of them work together to achieve a better
trade-off between clean accuracy and robustness. Extensive experiments on
UCF-101 and HMDB-51 with both CNN and Transformer-based models demonstrate that
VFAT-WS achieves great improvements in adversarial robustness and corruption
robustness, while accelerating training by nearly 490%.

</details>


### [376] [Distribution-aware Forgetting Compensation for Exemplar-Free Lifelong Person Re-identification](https://arxiv.org/abs/2504.15041)
*Shiben Liu,Huijie Fan,Qiang Wang,Baojie Fan,Yandong Tang,Liangqiong Qu*

Main category: cs.CV

TL;DR: 论文提出了一种名为DAFC的新模型，通过文本驱动的提示聚合和分布感知集成，解决了LReID中的旧知识保留和新信息适应问题。


<details>
  <summary>Details</summary>
Motivation: 解决LReID中旧知识遗忘和新信息适应不足的问题，现有方法（基于排练和无排练）存在局限性。

Method: 提出DAFC模型，结合文本驱动的提示聚合（TPA）和分布感知集成（DAI），并引入知识巩固机制（KCM）。

Result: 实验表明，DAFC在两种训练顺序下平均mAP/R@1分别优于现有方法至少9.8%/6.6%和6.4%/6.2%。

Conclusion: DAFC通过分布感知和知识巩固机制，有效解决了LReID中的遗忘问题，提升了跨域共享表示学习。

Abstract: Lifelong Person Re-identification (LReID) suffers from a key challenge in
preserving old knowledge while adapting to new information. The existing
solutions include rehearsal-based and rehearsal-free methods to address this
challenge. Rehearsal-based approaches rely on knowledge distillation,
continuously accumulating forgetting during the distillation process.
Rehearsal-free methods insufficiently learn the distribution of each domain,
leading to forgetfulness over time. To solve these issues, we propose a novel
Distribution-aware Forgetting Compensation (DAFC) model that explores
cross-domain shared representation learning and domain-specific distribution
integration without using old exemplars or knowledge distillation. We propose a
Text-driven Prompt Aggregation (TPA) that utilizes text features to enrich
prompt elements and guide the prompt model to learn fine-grained
representations for each instance. This can enhance the differentiation of
identity information and establish the foundation for domain distribution
awareness. Then, Distribution-based Awareness and Integration (DAI) is designed
to capture each domain-specific distribution by a dedicated expert network and
adaptively consolidate them into a shared region in high-dimensional space. In
this manner, DAI can consolidate and enhance cross-domain shared representation
learning while alleviating catastrophic forgetting. Furthermore, we develop a
Knowledge Consolidation Mechanism (KCM) that comprises instance-level
discrimination and cross-domain consistency alignment strategies to facilitate
model adaptive learning of new knowledge from the current domain and promote
knowledge consolidation learning between acquired domain-specific
distributions, respectively. Experimental results show that our DAFC outperform
state-of-the-art methods by at least 9.8\%/6.6\% and 6.4\%/6.2\% of average
mAP/R@1 on two training orders.

</details>


### [377] [A triple-branch network for latent fingerprint enhancement guided by orientation fields and minutiae](https://arxiv.org/abs/2504.15105)
*Yurun Wang,Zerong Qi,Shujun Fu,Mingzheng Hu*

Main category: cs.CV

TL;DR: 提出了一种名为TBSFNet和MLFGNet的深度学习方法，用于潜指纹增强，通过多分支策略和特征引导网络提升低质量区域的恢复效果。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法在潜指纹增强中，尤其是低质量区域的恢复上表现不足，需要针对不同区域采用不同策略。

Method: 提出TBSFNet（三分支空间融合网络）和MLFGNet（多级特征引导网络），结合方向场和细节模块，增强不同区域的指纹。

Result: 在MOLF和MUST数据集上，MLFGNet表现优于现有增强算法。

Conclusion: TBSFNet和MLFGNet通过多分支和特征引导策略，显著提升了潜指纹增强的效果，特别是低质量区域的恢复。

Abstract: Latent fingerprint enhancement is a critical step in the process of latent
fingerprint identification. Existing deep learning-based enhancement methods
still fall short of practical application requirements, particularly in
restoring low-quality fingerprint regions. Recognizing that different regions
of latent fingerprints require distinct enhancement strategies, we propose a
Triple Branch Spatial Fusion Network (TBSFNet), which simultaneously enhances
different regions of the image using tailored strategies. Furthermore, to
improve the generalization capability of the network, we integrate orientation
field and minutiae-related modules into TBSFNet and introduce a Multi-Level
Feature Guidance Network (MLFGNet). Experimental results on the MOLF and MUST
datasets demonstrate that MLFGNet outperforms existing enhancement algorithms.

</details>


### [378] [Landmark-Free Preoperative-to-Intraoperative Registration in Laparoscopic Liver Resection](https://arxiv.org/abs/2504.15152)
*Jun Zhou,Bingchen Gao,Kai Wang,Jialun Pei,Pheng-Ann Heng,Jing Qin*

Main category: cs.CV

TL;DR: 提出了一种基于自监督学习的无标记术前到术中肝脏配准框架，解决了传统方法依赖解剖标记和形状变形建模不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有配准方法依赖解剖标记，存在标记定义模糊和术中视觉信息整合不足的问题。

Method: 将3D-2D配准流程转为3D-3D，分解为刚性和非刚性配准子任务，使用特征解耦变换器和结构正则化变形网络。

Result: 在合成和真实数据集上的实验及用户研究表明，该方法具有优越性和临床适用性。

Conclusion: 提出的框架显著提升了肝脏配准的准确性和效率，具有临床应用潜力。

Abstract: Liver registration by overlaying preoperative 3D models onto intraoperative
2D frames can assist surgeons in perceiving the spatial anatomy of the liver
clearly for a higher surgical success rate. Existing registration methods rely
heavily on anatomical landmark-based workflows, which encounter two major
limitations: 1) ambiguous landmark definitions fail to provide efficient
markers for registration; 2) insufficient integration of intraoperative liver
visual information in shape deformation modeling. To address these challenges,
in this paper, we propose a landmark-free preoperative-to-intraoperative
registration framework utilizing effective self-supervised learning, termed
\ourmodel. This framework transforms the conventional 3D-2D workflow into a
3D-3D registration pipeline, which is then decoupled into rigid and non-rigid
registration subtasks. \ourmodel~first introduces a feature-disentangled
transformer to learn robust correspondences for recovering rigid
transformations. Further, a structure-regularized deformation network is
designed to adjust the preoperative model to align with the intraoperative
liver surface. This network captures structural correlations through geometry
similarity modeling in a low-rank transformer network. To facilitate the
validation of the registration performance, we also construct an in-vivo
registration dataset containing liver resection videos of 21 patients, called
\emph{P2I-LReg}, which contains 346 keyframes that provide a global view of the
liver together with liver mask annotations and calibrated camera intrinsic
parameters. Extensive experiments and user studies on both synthetic and
in-vivo datasets demonstrate the superiority and potential clinical
applicability of our method.

</details>


### [379] [An Efficient Aerial Image Detection with Variable Receptive Fields](https://arxiv.org/abs/2504.15165)
*Liu Wenbin*

Main category: cs.CV

TL;DR: VRF-DETR是一种基于Transformer的检测器，通过动态调整感受野和门控多尺度融合，解决了无人机目标检测中的小目标、密集遮挡和计算限制问题。


<details>
  <summary>Details</summary>
Motivation: 无人机目标检测面临小目标（小于10像素）、密集遮挡和严格计算限制的挑战，现有检测器难以平衡准确性和效率。

Method: 提出VRF-DETR，包含多尺度上下文融合模块、门控卷积层和门控多尺度融合瓶颈，通过动态空间注意力和层次化解耦遮挡目标。

Result: 在VisDrone2019数据集上，VRF-DETR达到51.4% mAP50和31.8% mAP50:95，仅需13.5M参数。

Conclusion: VRF-DETR为无人机目标检测任务建立了新的效率-准确性帕累托前沿。

Abstract: Aerial object detection using unmanned aerial vehicles (UAVs) faces critical
challenges including sub-10px targets, dense occlusions, and stringent
computational constraints. Existing detectors struggle to balance accuracy and
efficiency due to rigid receptive fields and redundant architectures. To
address these limitations, we propose Variable Receptive Field DETR (VRF-DETR),
a transformer-based detector incorporating three key components: 1) Multi-Scale
Context Fusion (MSCF) module that dynamically recalibrates features through
adaptive spatial attention and gated multi-scale fusion, 2) Gated Convolution
(GConv) layer enabling parameter-efficient local-context modeling via depthwise
separable operations and dynamic gating, and 3) Gated Multi-scale Fusion (GMCF)
Bottleneck that hierarchically disentangles occluded objects through cascaded
global-local interactions. Experiments on VisDrone2019 demonstrate VRF-DETR
achieves 51.4\% mAP\textsubscript{50} and 31.8\% mAP\textsubscript{50:95} with
only 13.5M parameters. This work establishes a new efficiency-accuracy Pareto
frontier for UAV-based detection tasks.

</details>


### [380] [Breast density in MRI: an AI-based quantification and relationship to assessment in mammography](https://arxiv.org/abs/2504.15192)
*Yaqian Chen,Lin Li,Hanxue Gu,Haoyu Dong,Derek L. Nguyen,Allan D. Kirk,Maciej A. Mazurowski,E. Shelley Hwang*

Main category: cs.CV

TL;DR: 本文探讨了乳腺MRI密度作为乳腺癌风险因素的研究，通过机器学习算法分析多个数据集，发现MRI密度与乳腺X线密度相关但存在差异。


<details>
  <summary>Details</summary>
Motivation: 乳腺密度是乳腺癌的已知风险因素，但MRI作为辅助工具的分析方法尚不完善，研究旨在探索MRI密度的定量评估及其与乳腺X线密度的关系。

Method: 使用内部开发的机器学习算法，分析三个MRI数据集中的正常乳腺密度，并比较不同年龄组的密度变化。

Result: MRI密度在不同数据集中表现一致（0.104-0.114），且随年龄增长而降低；与乳腺X线密度相关，但存在差异。

Conclusion: MRI密度可能补充现有工具，未来研究将探索如何整合MRI密度以改进乳腺癌风险预测。

Abstract: Mammographic breast density is a well-established risk factor for breast
cancer. Recently there has been interest in breast MRI as an adjunct to
mammography, as this modality provides an orthogonal and highly quantitative
assessment of breast tissue. However, its 3D nature poses analytic challenges
related to delineating and aggregating complex structures across slices. Here,
we applied an in-house machine-learning algorithm to assess breast density on
normal breasts in three MRI datasets. Breast density was consistent across
different datasets (0.104 - 0.114). Analysis across different age groups also
demonstrated strong consistency across datasets and confirmed a trend of
decreasing density with age as reported in previous studies. MR breast density
was correlated with mammographic breast density, although some notable
differences suggest that certain breast density components are captured only on
MRI. Future work will determine how to integrate MR breast density with current
tools to improve future breast cancer risk prediction.

</details>


### [381] [Bringing Diversity from Diffusion Models to Semantic-Guided Face Asset Generation](https://arxiv.org/abs/2504.15259)
*Yunxuan Cai,Sitao Xiang,Zongjian Li,Haiwei Chen,Yajie Zhao*

Main category: cs.CV

TL;DR: 论文提出了一种基于生成网络的人脸数字化建模方法，通过扩散模型生成高质量3D人脸数据，并开发了支持语义控制的GAN生成器，最终构建了一个交互式工具。


<details>
  <summary>Details</summary>
Motivation: 现有的人脸建模方法受限于数据采集设备、人工劳动和合适的演员，导致模型多样性、表现力和控制性不足。

Method: 使用预训练的扩散模型生成高质量3D人脸数据库，开发支持语义输入的GAN生成器，并通过资产细化模块生成物理真实的面部资产。

Result: 构建了包含44,000个面部模型的数据库，开发了高效的生成器和交互工具，并通过实验验证了其效果。

Conclusion: 提出的系统显著提升了人脸建模的多样性和控制性，并计划公开交互工具。

Abstract: Digital modeling and reconstruction of human faces serve various
applications. However, its availability is often hindered by the requirements
of data capturing devices, manual labor, and suitable actors. This situation
restricts the diversity, expressiveness, and control over the resulting models.
This work aims to demonstrate that a semantically controllable generative
network can provide enhanced control over the digital face modeling process. To
enhance diversity beyond the limited human faces scanned in a controlled
setting, we introduce a novel data generation pipeline that creates a
high-quality 3D face database using a pre-trained diffusion model. Our proposed
normalization module converts synthesized data from the diffusion model into
high-quality scanned data. Using the 44,000 face models we obtained, we further
developed an efficient GAN-based generator. This generator accepts semantic
attributes as input, and generates geometry and albedo. It also allows
continuous post-editing of attributes in the latent space. Our asset refinement
component subsequently creates physically-based facial assets. We introduce a
comprehensive system designed for creating and editing high-quality face
assets. Our proposed model has undergone extensive experiment, comparison and
evaluation. We also integrate everything into a web-based interactive tool. We
aim to make this tool publicly available with the release of the paper.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [382] [Resource Utilization Optimized Federated Learning](https://arxiv.org/abs/2504.13850)
*Zihan Zhang,Leon Wong,Blesson Varghese*

Main category: cs.DC

TL;DR: FedOptima是一种优化的联邦学习系统，通过异步聚合和任务调度减少设备和服务器的空闲时间，提升训练效率和模型准确性。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习系统因任务依赖和设备异构性导致资源利用率低，FedOptima旨在同时解决这两种空闲时间问题。

Method: FedOptima通过设备独立异步聚合、服务器任务调度和高效内存管理，优化资源利用。

Result: 实验显示，FedOptima在准确性和训练速度上优于基线方法，显著减少空闲时间并提升吞吐量。

Conclusion: FedOptima有效解决了联邦学习中的资源利用率问题，具有实际应用潜力。

Abstract: Federated learning (FL) systems facilitate distributed machine learning
across a server and multiple devices. However, FL systems have low resource
utilization limiting their practical use in the real world. This inefficiency
primarily arises from two types of idle time: (i) task dependency between the
server and devices, and (ii) stragglers among heterogeneous devices. This paper
introduces FedOptima, a resource-optimized FL system designed to simultaneously
minimize both types of idle time; existing systems do not eliminate or reduce
both at the same time. FedOptima offloads the training of certain layers of a
neural network from a device to server using three innovations. First, devices
operate independently of each other using asynchronous aggregation to eliminate
straggler effects, and independently of the server by utilizing auxiliary
networks to minimize idle time caused by task dependency. Second, the server
performs centralized training using a task scheduler that ensures balanced
contributions from all devices, improving model accuracy. Third, an efficient
memory management mechanism on the server increases scalability of the number
of participating devices. Four state-of-the-art offloading-based and
asynchronous FL methods are chosen as baselines. Experimental results show that
compared to the best results of the baselines on convolutional neural networks
and transformers on multiple lab-based testbeds, FedOptima (i) achieves higher
or comparable accuracy, (ii) accelerates training by 1.9x to 21.8x, (iii)
reduces server and device idle time by up to 93.9% and 81.8%, respectively, and
(iv) increases throughput by 1.1x to 2.0x.

</details>


### [383] [PipeWeaver: Addressing Data Dynamicity in Large Multimodal Model Training with Dynamic Interleaved Pipeline](https://arxiv.org/abs/2504.14145)
*Zhenliang Xue,Hanpeng Hu,Xing Chen,Yimin Jiang,Yixin Song,Zeyu Mi,Yibo Zhu,Daxin Jiang,Yubin Xia,Haibo Chen*

Main category: cs.DC

TL;DR: PipeWeaver是一个动态管道调度框架，通过动态交错管道和模态感知分区技术，显著提升大型多模态模型（LMM）的训练效率。


<details>
  <summary>Details</summary>
Motivation: 解决LMM训练中因异构架构和动态数据多样性导致的效率问题。

Method: 采用动态交错管道、自适应模态感知分区和高效管道调度搜索技术，结合SEMU模拟器进行性能估计。

Result: PipeWeaver将LMM训练效率提升高达97.3%，并展现出对数据动态性的优秀适应性。

Conclusion: PipeWeaver为LMM训练提供了一种高效且适应性强的解决方案。

Abstract: Large multimodal models (LMMs) have demonstrated excellent capabilities in
both understanding and generation tasks with various modalities. While these
models can accept flexible combinations of input data, their training
efficiency suffers from two major issues: pipeline stage imbalance caused by
heterogeneous model architectures, and training data dynamicity stemming from
the diversity of multimodal data.
  In this paper, we present PipeWeaver, a dynamic pipeline scheduling framework
designed for LMM training. The core of PipeWeaver is dynamic interleaved
pipeline, which searches for pipeline schedules dynamically tailored to current
training batches. PipeWeaver addresses issues of LMM training with two
techniques: adaptive modality-aware partitioning and efficient pipeline
schedule search within a hierarchical schedule space. Meanwhile, PipeWeaver
utilizes SEMU (Step Emulator), a training simulator for multimodal models, for
accurate performance estimations, accelerated by spatial-temporal subgraph
reuse to improve search efficiency. Experiments show that PipeWeaver can
enhance LMM training efficiency by up to 97.3% compared to state-of-the-art
systems, and demonstrate excellent adaptivity to LMM training's data
dynamicity.

</details>


### [384] [GENE-FL: Gene-Driven Parameter-Efficient Dynamic Federated Learning](https://arxiv.org/abs/2504.14628)
*Shunxin Guo,Jiaqi Lv,Qiufeng Wang,Xin Geng*

Main category: cs.DC

TL;DR: 论文提出GENE-FL框架，通过Learngene范式解决动态客户端数据异构问题，显著降低通信成本并快速初始化模型。


<details>
  <summary>Details</summary>
Motivation: 现实联邦学习系统中动态客户端数据分布异构且不可知（DAFL），导致通信效率低和模型初始化困难。

Method: 基于Learngene范式，GENE-FL框架通过Fisher值筛选参数、本地参数敏感性分析压缩learnGene，服务器聚合生成跨任务泛化的learnGene。

Result: 实验显示GENE-FL通信成本降低4倍，仅需9.04 MB即可初始化模型。

Conclusion: GENE-FL有效解决了DAFL问题，提升了通信效率和模型初始化速度。

Abstract: Real-world \underline{F}ederated \underline{L}earning systems often encounter
\underline{D}ynamic clients with \underline{A}gnostic and highly heterogeneous
data distributions (DAFL), which pose challenges for efficient communication
and model initialization. To address these challenges, we draw inspiration from
the recently proposed Learngene paradigm, which compresses the large-scale
model into lightweight, cross-task meta-information fragments. Learngene
effectively encapsulates and communicates core knowledge, making it
particularly well-suited for DAFL, where dynamic client participation requires
communication efficiency and rapid adaptation to new data distributions. Based
on this insight, we propose a Gene-driven parameter-efficient dynamic Federated
Learning (GENE-FL) framework. First, local models perform quadratic constraints
based on parameters with high Fisher values in the global model, as these
parameters are considered to encapsulate generalizable knowledge. Second, we
apply the strategy of parameter sensitivity analysis in local model parameters
to condense the \textit{learnGene} for interaction. Finally, the server
aggregates these small-scale trained \textit{learnGene}s into a robust
\textit{learnGene} with cross-task generalization capability, facilitating the
rapid initialization of dynamic agnostic client models. Extensive experimental
results demonstrate that GENE-FL reduces \textbf{4 $\times$} communication
costs compared to FEDAVG and effectively initializes agnostic client models
with only about \textbf{9.04} MB.

</details>


### [385] [Is Intelligence the Right Direction in New OS Scheduling for Multiple Resources in Cloud Environments?](https://arxiv.org/abs/2504.15021)
*Xinglei Dou,Lei Liu,Limin Xiao*

Main category: cs.DC

TL;DR: OSML+是一种基于机器学习的资源调度机制，用于协同定位云服务，通过多模型协作学习优化缓存、内存带宽和计算核心资源调度。


<details>
  <summary>Details</summary>
Motivation: 提升系统/OS设计的智能化水平，解决复杂场景下的资源调度问题，如避免资源悬崖、共享资源等。

Method: 采用多模型协作学习方法，结合迁移学习技术，动态适应变化的工作负载。

Result: 实验表明，OSML+能更快收敛，支持更高负载，且满足QoS目标，开销更低。

Conclusion: OSML+在云服务器资源调度中表现出色，具有广泛适用性和高效性。

Abstract: Making it intelligent is a promising way in System/OS design. This paper
proposes OSML+, a new ML-based resource scheduling mechanism for co-located
cloud services. OSML+ intelligently schedules the cache and main memory
bandwidth resources at the memory hierarchy and the computing core resources
simultaneously. OSML+ uses a multi-model collaborative learning approach during
its scheduling and thus can handle complicated cases, e.g., avoiding resource
cliffs, sharing resources among applications, enabling different scheduling
policies for applications with different priorities, etc. OSML+ can converge
faster using ML models than previous studies. Moreover, OSML+ can automatically
learn on the fly and handle dynamically changing workloads accordingly. Using
transfer learning technologies, we show our design can work well across various
cloud servers, including the latest off-the-shelf large-scale servers. Our
experimental results show that OSML+ supports higher loads and meets QoS
targets with lower overheads than previous studies.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [386] [Quantum-Enhanced Reinforcement Learning for Power Grid Security Assessment](https://arxiv.org/abs/2504.14412)
*Benjamin M. Peter,Mert Korkali*

Main category: eess.SY

TL;DR: 论文提出了一种结合量子计算和强化学习的混合代理，用于提升电网安全评估的计算效率和代理能力。


<details>
  <summary>Details</summary>
Motivation: 电网安全维护任务日益复杂，传统方法难以应对大规模决策空间和非线性行为。量子计算的优势为解决这一问题提供了新思路。

Method: 提出了一种基于IBM Qiskit Runtime的混合代理，利用参数化量子电路（PQCs）生成量子输出，并与强化学习框架结合。

Result: 实验表明，该混合代理在N-k应急分析中比无量子增强的基准模型更能维持电网稳定性。

Conclusion: 量子计算与强化学习的结合为电网安全评估提供了可扩展的高效解决方案。

Abstract: The increasingly challenging task of maintaining power grid security requires
innovative solutions. Novel approaches using reinforcement learning (RL) agents
have been proposed to help grid operators navigate the massive decision space
and nonlinear behavior of these complex networks. However, applying RL to power
grid security assessment, specifically for combinatorially troublesome
contingency analysis problems, has proven difficult to scale. The integration
of quantum computing into these RL frameworks helps scale by improving
computational efficiency and boosting agent proficiency by leveraging quantum
advantages in action exploration and model-based interdependence. To
demonstrate a proof-of-concept use of quantum computing for RL agent training
and simulation, we propose a hybrid agent that runs on quantum hardware using
IBM's Qiskit Runtime. We also provide detailed insight into the construction of
parameterized quantum circuits (PQCs) for generating relevant quantum output.
This agent's proficiency at maintaining grid stability is demonstrated relative
to a benchmark model without quantum enhancement using N-k contingency
analysis. Additionally, we offer a comparative assessment of the training
procedures for RL models integrated with a quantum backend.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [387] [Optimal Lattice Boltzmann Closures through Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2504.14422)
*Paul Fischer,Sebastian Kaltenbach,Sergey Litvinov,Sauro Succi,Petros Koumoutsakos*

Main category: physics.flu-dyn

TL;DR: 论文提出了一种基于多智能体强化学习（MARL）的数据驱动方法，用于提升粗粒度Lattice Boltzmann方法（LBM）模拟的稳定性和准确性。


<details>
  <summary>Details</summary>
Motivation: LBM在模拟多尺度流体现象时，由于计算资源限制，难以完全解析所有尺度，导致未解析模拟不稳定且难以泛化。

Method: 采用卷积神经网络动态控制LBM的局部松弛参数，并通过MARL框架在湍流Kolmogorov流中验证。

Result: MARL方法显著提升了模拟的稳定性，恢复了高分辨率模拟的能量谱，同时保持了计算效率，且能泛化到未训练的流动场景。

Conclusion: MARL方法为LBM模拟复杂问题提供了高效且准确的新途径，超越了传统方法的局限性。

Abstract: The Lattice Boltzmann method (LBM) offers a powerful and versatile approach
to simulating diverse hydrodynamic phenomena, spanning microfluidics to
aerodynamics. The vast range of spatiotemporal scales inherent in these systems
currently renders full resolution impractical, necessitating the development of
effective closure models for under-resolved simulations. Under-resolved LBMs
are unstable, and while there is a number of important efforts to stabilize
them, they often face limitations in generalizing across scales and physical
systems. We present a novel, data-driven, multiagent reinforcement learning
(MARL) approach that drastically improves stability and accuracy of
coarse-grained LBM simulations. The proposed method uses a convolutional neural
network to dynamically control the local relaxation parameter for the LB across
the simulation grid. The LB-MARL framework is showcased in turbulent Kolmogorov
flows. We find that the MARL closures stabilize the simulations and recover the
energy spectra of significantly more expensive fully resolved simulations while
maintaining computational efficiency. The learned closure model can be
transferred to flow scenarios unseen during training and has improved
robustness and spectral accuracy compared to traditional LBM models. We believe
that MARL closures open new frontiers for efficient and accurate simulations of
a multitude of complex problems not accessible to present-day LB methods alone.

</details>


### [388] [LBM-GNN: Graph Neural Network Enhanced Lattice Boltzmann Method](https://arxiv.org/abs/2504.14494)
*Yue Li*

Main category: physics.flu-dyn

TL;DR: LBM-GNN结合了传统的Lattice Boltzmann Method（LBM）与Graph Neural Networks（GNNs），用于流体动力学模拟，提高了稳定性和准确性。


<details>
  <summary>Details</summary>
Motivation: 传统LBM在流体模拟中存在稳定性和精度问题，尤其是在高Reynolds数情况下。

Method: 将GNNs引入LBM框架，通过图神经网络优化模拟过程。

Result: 在Taylor-Green涡流等基准问题上验证了方法的准确性、守恒性和性能，显示在高Reynolds数下具有更好的数值稳定性。

Conclusion: GNN增强的LBM在保持守恒性的同时，显著提高了高Reynolds数下的数值稳定性。

Abstract: In this paper, we present LBM-GNN, a novel approach that enhances the
traditional Lattice Boltzmann Method (LBM) with Graph Neural Networks (GNNs).
We apply this method to fluid dynamics simulations, demonstrating improved
stability and accuracy compared to standard LBM implementations. The method is
validated using benchmark problems such as the Taylor-Green vortex, focusing on
accuracy, conservation properties, and performance across different Reynolds
numbers and grid resolutions. Our results indicate that GNN-enhanced LBM can
maintain better conservation properties while improving numerical stability at
higher Reynolds numbers.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [389] [A CMOS Probabilistic Computing Chip With In-situ hardware Aware Learning](https://arxiv.org/abs/2504.14070)
*Jinesh Jhonsa,William Whitehead,David McCarthy,Shuvro Chowdhury,Kerem Camsari,Luke Theogarajan*

Main category: cs.AR

TL;DR: 论文展示了一种基于概率比特物理的求解器，采用440个自旋的Chimera图结构，面积为0.44 mm²。通过电流模式神经元更新电路、标准单元设计和共享电源优化面积效率，并使用硬件感知对比散度算法减少工艺变化影响。验证了芯片在概率计算和优化任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 研究旨在开发一种高效、面积优化的概率计算芯片，适用于AI和机器学习任务。

Method: 采用电流模式神经元更新电路、标准单元设计、共享电源，并结合硬件感知对比散度算法减少工艺变化影响。

Result: 芯片成功实现了概率计算任务（如逻辑门和全加器建模）和优化任务（如MaxCut），展示了其在AI和机器学习中的潜力。

Conclusion: 该芯片设计高效且实用，为概率计算和优化任务提供了新的硬件解决方案。

Abstract: This paper demonstrates a probabilistic bit physics inspired solver with 440
spins configured in a Chimera graph, occupying an area of 0.44 mm^2. Area
efficiency is maximized through a current-mode implementation of the neuron
update circuit, standard cell design for analog blocks pitch-matched to digital
blocks, and a shared power supply for both digital and analog components.
Process variation related mismatches introduced by this approach are
effectively mitigated using a hardware aware contrastive divergence algorithm
during training. We validate the chip's ability to perform probabilistic
computing tasks such as modeling logic gates and full adders, as well as
optimization tasks such as MaxCut, demonstrating its potential for AI and
machine learning applications.

</details>


### [390] [FGMP: Fine-Grained Mixed-Precision Weight and Activation Quantization for Hardware-Accelerated LLM Inference](https://arxiv.org/abs/2504.14152)
*Coleman Hooper,Charbel Sakr,Ben Keller,Rangharajan Venkatesan,Kurt Keutzer,Sophia Shao,Brucek Khailany*

Main category: cs.AR

TL;DR: 论文提出了一种细粒度混合精度（FGMP）量化方法，通过硬件-软件协同设计，在降低大语言模型（LLM）权重和激活的精度时保持准确性，同时显著提升能效和减少内存占用。


<details>
  <summary>Details</summary>
Motivation: 量化是提升LLM推理效率的有效方法，但传统量化方法在降低精度时容易导致模型准确性下降。因此，需要一种新的量化策略来平衡精度和效率。

Method: 1) 提出基于Fisher信息加权的扰动策略，选择需要保留高精度的权重和激活块；2) 提出灵敏度加权裁剪方法，优化低精度块的量化；3) 设计支持FGMP的硬件增强功能，包括块粒度数据路径和动态混合精度激活量化单元。

Result: 在Llama-2-7B模型上，使用NVFP4和FP8格式，实现了<1%的困惑度下降，推理能耗降低14%，权重内存减少30%。

Conclusion: FGMP量化方法在保持模型准确性的同时，显著提升了能效和内存效率，为LLM的高效推理提供了可行的解决方案。

Abstract: Quantization is a powerful tool to improve large language model (LLM)
inference efficiency by utilizing more energy-efficient low-precision datapaths
and reducing memory footprint. However, accurately quantizing LLM weights and
activations to low precision is challenging without degrading model accuracy.
We propose fine-grained mixed precision (FGMP) quantization, a post-training
mixed-precision quantization hardware-software co-design methodology that
maintains accuracy while quantizing the majority of weights and activations to
reduced precision. Our work makes the following contributions: 1) We develop a
policy that uses the perturbation in each value, weighted by the Fisher
information, to select which weight and activation blocks to keep in higher
precision. This approach preserves accuracy by identifying which weight and
activation blocks need to be retained in higher precision to minimize the
perturbation in the model loss. 2) We also propose a sensitivity-weighted
clipping approach for fine-grained quantization which helps retain accuracy for
blocks that are quantized to low precision. 3) We then propose hardware
augmentations to leverage the efficiency benefits of FGMP quantization. Our
hardware implementation encompasses i) datapath support for FGMP at block
granularity, and ii) a mixed-precision activation quantization unit to assign
activation blocks to high or low precision on the fly with minimal runtime and
energy overhead. Our design, prototyped using NVFP4 (an FP4 format with
microscaling) as the low-precision datatype and FP8 as the high-precision
datatype, facilitates efficient FGMP quantization, attaining <1% perplexity
degradation on Wikitext-103 for the Llama-2-7B model relative to an all-FP8
baseline design while consuming 14% less energy during inference and requiring
30% less weight memory.

</details>


### [391] [ReasoningV: Efficient Verilog Code Generation with Adaptive Hybrid Reasoning Model](https://arxiv.org/abs/2504.14560)
*Haiyan Qin,Zhiwei Xie,Jingjing Li,Liangchen Li,Xiaotong Feng,Junzhan Liu,Wang Kang*

Main category: cs.AR

TL;DR: ReasoningV是一种新型模型，通过混合推理策略提升Verilog代码生成能力，解决了数据质量、推理能力和计算效率的挑战。


<details>
  <summary>Details</summary>
Motivation: LLMs在Verilog代码生成中面临数据质量、推理能力和计算效率的不足，需要更高效的解决方案。

Method: 提出ReasoningV-5K数据集、两阶段训练方法和自适应推理机制，动态调整推理深度以减少计算开销。

Result: 在VerilogEval-human上达到57.8%的pass@1准确率，性能接近商业模型，并超越开源模型10.4个百分点。

Conclusion: ReasoningV为AI驱动的硬件设计自动化提供了更可靠和可访问的途径，模型、数据和代码已开源。

Abstract: Large Language Models (LLMs) have advanced Verilog code generation
significantly, yet face challenges in data quality, reasoning capabilities, and
computational efficiency. This paper presents ReasoningV, a novel model
employing a hybrid reasoning strategy that integrates trained intrinsic
capabilities with dynamic inference adaptation for Verilog code generation. Our
framework introduces three complementary innovations: (1) ReasoningV-5K, a
high-quality dataset of 5,000 functionally verified instances with reasoning
paths created through multi-dimensional filtering of PyraNet samples; (2) a
two-stage training approach combining parameter-efficient fine-tuning for
foundational knowledge with full-parameter optimization for enhanced reasoning;
and (3) an adaptive reasoning mechanism that dynamically adjusts reasoning
depth based on problem complexity, reducing token consumption by up to 75\%
while preserving performance. Experimental results demonstrate ReasoningV's
effectiveness with a pass@1 accuracy of 57.8\% on VerilogEval-human, achieving
performance competitive with leading commercial models like Gemini-2.0-flash
(59.5\%) and exceeding the previous best open-source model by 10.4 percentage
points. ReasoningV offers a more reliable and accessible pathway for advancing
AI-driven hardware design automation, with our model, data, and code available
at https://github.com/BUAA-CLab/ReasoningV.

</details>


### [392] [Towards Optimal Circuit Generation: Multi-Agent Collaboration Meets Collective Intelligence](https://arxiv.org/abs/2504.14625)
*Haiyan Qin,Jiahao Feng,Xiaotong Feng,Wei W. Xing,Wang Kang*

Main category: cs.AR

TL;DR: CircuitMind是一个多智能体框架，通过语法锁定、检索增强生成和双奖励优化，实现了与人类专家竞争的硬件设计效率。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在硬件设计中生成门数过高的问题，提升效率。

Method: 采用语法锁定、检索增强生成和双奖励优化技术。

Result: 55.6%的模型实现达到或超过人类专家效率，14B Phi-4模型表现优于GPT-4o mini和Gemini 2.0 Flash。

Conclusion: CircuitMind为硬件优化提供了新范式，利用集体人类专业知识实现最优电路设计。

Abstract: Large language models (LLMs) have transformed code generation, yet their
application in hardware design produces gate counts 38\%--1075\% higher than
human designs. We present CircuitMind, a multi-agent framework that achieves
human-competitive efficiency through three key innovations: syntax locking
(constraining generation to basic logic gates), retrieval-augmented generation
(enabling knowledge-driven design), and dual-reward optimization (balancing
correctness with efficiency). To evaluate our approach, we introduce TC-Bench,
the first gate-level benchmark harnessing collective intelligence from the
TuringComplete ecosystem -- a competitive circuit design platform with hundreds
of thousands of players. Experiments show CircuitMind enables 55.6\% of model
implementations to match or exceed top-tier human experts in composite
efficiency metrics. Most remarkably, our framework elevates the 14B Phi-4 model
to outperform both GPT-4o mini and Gemini 2.0 Flash, achieving efficiency
comparable to the top 25\% of human experts without requiring specialized
training. These innovations establish a new paradigm for hardware optimization
where collaborative AI systems leverage collective human expertise to achieve
optimal circuit designs. Our model, data, and code are open-source at
https://github.com/BUAA-CLab/CircuitMind.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [393] [Coordinating Spinal and Limb Dynamics for Enhanced Sprawling Robot Mobility](https://arxiv.org/abs/2504.14103)
*Merve Atasever,Ali Okhovat,Azhang Nazaripouya,John Nisbet,Omer Kurkutlu,Jyotirmoy V. Deshmukh,Yasemin Ozkan Aydin*

Main category: cs.RO

TL;DR: 论文研究了基于深度强化学习的控制策略和生物启发的步态设计方法，用于模拟蝾螈机器人在不确定环境中的高效运动。


<details>
  <summary>Details</summary>
Motivation: 蝾螈因其独特的行走和游泳步态转换能力，突出了脊柱灵活性在运动中的作用。然而，环境不确定性可能导致肢体协调问题，需要动态适应的控制策略。

Method: 比较了基于深度强化学习（DRL）的控制策略和生物启发的步态设计方法，应用于蝾螈机器人。

Result: 研究结果表明，DRL能够有效处理非确定性环境，使机器人系统在挑战性条件下表现稳健。

Conclusion: 深度强化学习为机器人系统在不确定环境中实现高效运动提供了有前景的解决方案。

Abstract: Among vertebrates, salamanders, with their unique ability to transition
between walking and swimming gaits, highlight the role of spinal mobility in
locomotion. A flexible spine enables undulation of the body through a wavelike
motion along the spine, aiding navigation over uneven terrains and obstacles.
Yet environmental uncertainties, such as surface irregularities and variations
in friction, can significantly disrupt body-limb coordination and cause
discrepancies between predictions from mathematical models and real-world
outcomes. Addressing this challenge requires the development of sophisticated
control strategies capable of dynamically adapting to uncertain conditions
while maintaining efficient locomotion. Deep reinforcement learning (DRL)
offers a promising framework for handling non-deterministic environments and
enabling robotic systems to adapt effectively and perform robustly under
challenging conditions. In this study, we comparatively examine learning-based
control strategies and biologically inspired gait design methods on a
salamander-like robot.

</details>


### [394] [Knitting Robots: A Deep Learning Approach for Reverse-Engineering Fabric Patterns](https://arxiv.org/abs/2504.14007)
*Haoliang Sheng,Songpu Cai,Xingyu Zheng,Meng Cheng Lau*

Main category: cs.RO

TL;DR: 提出了一种基于深度学习的逆向编织管道，用于将视觉机器人系统集成到纺织制造中，解决了编织自动化中的关键挑战。


<details>
  <summary>Details</summary>
Motivation: 编织自动化在纺织制造中具有挑战性，尤其是在将织物设计转换为机器可读指令方面。研究旨在通过机器人自动化提升纺织生产的灵活性和可定制性。

Method: 采用两阶段深度学习架构，机器人先识别前标签，再推断完整标签，并结合单线和多线模式适应不同材料复杂度。

Result: 系统能够准确生成可扩展的编织图案，解决了标签不平衡、针法类型不足和精细控制等挑战。

Conclusion: 该研究为全自动机器人编织系统奠定了基础，推动了纺织制造中智能机器人自动化的发展。

Abstract: Knitting, a cornerstone of textile manufacturing, is uniquely challenging to
automate, particularly in terms of converting fabric designs into precise,
machine-readable instructions. This research bridges the gap between textile
production and robotic automation by proposing a novel deep learning-based
pipeline for reverse knitting to integrate vision-based robotic systems into
textile manufacturing. The pipeline employs a two-stage architecture, enabling
robots to first identify front labels before inferring complete labels,
ensuring accurate, scalable pattern generation. By incorporating diverse yarn
structures, including single-yarn (sj) and multi-yarn (mj) patterns, this study
demonstrates how our system can adapt to varying material complexities.
Critical challenges in robotic textile manipulation, such as label imbalance,
underrepresented stitch types, and the need for fine-grained control, are
addressed by leveraging specialized deep-learning architectures. This work
establishes a foundation for fully automated robotic knitting systems, enabling
customizable, flexible production processes that integrate perception,
planning, and actuation, thereby advancing textile manufacturing through
intelligent robotic automation.

</details>


### [395] [Experience-based Refinement of Task Planning Knowledge in Autonomous Robots](https://arxiv.org/abs/2504.14259)
*Hadeel Jazzaa,Thomas McCluskey,David Peebles*

Main category: cs.RO

TL;DR: 论文提出了一种方法，通过机器人执行动作的经验来调整其符号知识，从而提高任务规划的成功率。


<details>
  <summary>Details</summary>
Motivation: 自主机器人需在动态环境中展现高级认知能力，但目前符号知识的改进尚未应用于实际物理机器人。

Method: 提出一种通过机器人执行动作经验驱动知识改进的方法，以增强任务规划的鲁棒性。

Result: 在NAO机器人上实现并评估，结果显示任务规划失败率随时间降低。

Conclusion: 通过知识改进，机器人任务规划的成功率显著提升。

Abstract: The requirement for autonomous robots to exhibit higher-level cognitive
skills by planning and adapting in an ever-changing environment is indeed a
great challenge for the AI community. Progress has been made in the automated
planning community on refinement and repair of an agent's symbolic knowledge to
do task planning in an incomplete or changing environmental model, but these
advances up to now have not been transferred to real physical robots. This
paper demonstrates how a physical robot can be capable of adapting its symbolic
knowledge of the environment, by using experiences in robot action execution to
drive knowledge refinement and hence to improve the success rate of the task
plans the robot creates. To implement more robust planning systems, we propose
a method for refining domain knowledge to improve the knowledge on which
intelligent robot behavior is based. This architecture has been implemented and
evaluated using a NAO robot. The refined knowledge leads to the future
synthesis of task plans which demonstrate decreasing rates of failure over time
as faulty knowledge is removed or adjusted.

</details>


### [396] [Unreal Robotics Lab: A High-Fidelity Robotics Simulator with Advanced Physics and Rendering](https://arxiv.org/abs/2504.14135)
*Jonathan Embley-Riches,Jianwei Liu,Simon Julier,Dimitrios Kanoulas*

Main category: cs.RO

TL;DR: 本文提出了一种结合Unreal Engine的高保真渲染与MuJoCo精确物理模拟的仿真框架URL，用于机器人研究。


<details>
  <summary>Details</summary>
Motivation: 高保真仿真对机器人研究至关重要，但现有方法难以同时实现逼真渲染和精确物理模拟。

Method: 通过整合Unreal Engine的渲染能力和MuJoCo的物理模拟，构建了URL框架。

Result: 支持复杂环境效果（如烟雾、火焰、水动态），并成功用于视觉导航和SLAM方法的测试。

Conclusion: URL框架为机器人研究和仿真到现实的迁移提供了强大工具。

Abstract: High-fidelity simulation is essential for robotics research, enabling safe
and efficient testing of perception, control, and navigation algorithms.
However, achieving both photorealistic rendering and accurate physics modeling
remains a challenge. This paper presents a novel simulation framework--the
Unreal Robotics Lab (URL) that integrates the Unreal Engine's advanced
rendering capabilities with MuJoCo's high-precision physics simulation. Our
approach enables realistic robotic perception while maintaining accurate
physical interactions, facilitating benchmarking and dataset generation for
vision-based robotics applications. The system supports complex environmental
effects, such as smoke, fire, and water dynamics, which are critical for
evaluating robotic performance under adverse conditions. We benchmark visual
navigation and SLAM methods within our framework, demonstrating its utility for
testing real-world robustness in controlled yet diverse scenarios. By bridging
the gap between physics accuracy and photorealistic rendering, our framework
provides a powerful tool for advancing robotics research and sim-to-real
transfer.

</details>


### [397] [Modality Selection and Skill Segmentation via Cross-Modality Attention](https://arxiv.org/abs/2504.14573)
*Jiawei Jiang,Kei Ota,Devesh K. Jha,Asako Kanezaki*

Main category: cs.RO

TL;DR: 论文提出了一种跨模态注意力机制（CMA），用于选择性地利用信息量最大的模态，并扩展其应用以分割原始技能并训练分层策略。


<details>
  <summary>Details</summary>
Motivation: 解决因维度灾难导致的多模态（如触觉和听觉）融入机器人基础模型的挑战。

Method: 提出跨模态注意力机制（CMA）进行模态选择，并用于分割专家演示中的原始技能，训练分层策略。

Result: 成功解决了长时程、接触密集的操纵任务。

Conclusion: CMA机制有效解决了多模态融合的挑战，并提升了机器人任务的表现。

Abstract: Incorporating additional sensory modalities such as tactile and audio into
foundational robotic models poses significant challenges due to the curse of
dimensionality. This work addresses this issue through modality selection. We
propose a cross-modality attention (CMA) mechanism to identify and selectively
utilize the modalities that are most informative for action generation at each
timestep. Furthermore, we extend the application of CMA to segment primitive
skills from expert demonstrations and leverage this segmentation to train a
hierarchical policy capable of solving long-horizon, contact-rich manipulation
tasks.

</details>


### [398] [Phoenix: A Motion-based Self-Reflection Framework for Fine-grained Robotic Action Correction](https://arxiv.org/abs/2504.14588)
*Wenke Xia,Ruoxuan Feng,Dong Wang,Di Hu*

Main category: cs.RO

TL;DR: 论文提出了Phoenix框架，通过运动指令连接高级语义反思与低级机器人动作修正，结合双过程运动调整机制和多任务运动条件扩散策略，实现了机器人动作的精确修正。


<details>
  <summary>Details</summary>
Motivation: 解决机器人从失败中恢复的通用性问题，尤其是如何将语义反思转化为细粒度动作修正的挑战。

Method: 采用双过程运动调整机制和多任务运动条件扩散策略，结合视觉观察进行高频动作修正，并通过终身学习方法提升模型能力。

Result: 在RoboMimic仿真和真实场景中验证了框架的泛化能力和鲁棒性。

Conclusion: Phoenix框架通过运动指令和扩散策略，显著提升了机器人动作修正的精确性和通用性。

Abstract: Building a generalizable self-correction system is crucial for robots to
recover from failures. Despite advancements in Multimodal Large Language Models
(MLLMs) that empower robots with semantic reflection ability for failure,
translating semantic reflection into how to correct fine-grained robotic
actions remains a significant challenge. To address this gap, we build the
Phoenix framework, which leverages motion instruction as a bridge to connect
high-level semantic reflection with low-level robotic action correction. In
this motion-based self-reflection framework, we start with a dual-process
motion adjustment mechanism with MLLMs to translate the semantic reflection
into coarse-grained motion instruction adjustment. To leverage this motion
instruction for guiding how to correct fine-grained robotic actions, a
multi-task motion-conditioned diffusion policy is proposed to integrate visual
observations for high-frequency robotic action correction. By combining these
two models, we could shift the demand for generalization capability from the
low-level manipulation policy to the MLLMs-driven motion adjustment model and
facilitate precise, fine-grained robotic action correction. Utilizing this
framework, we further develop a lifelong learning method to automatically
improve the model's capability from interactions with dynamic environments. The
experiments conducted in both the RoboMimic simulation and real-world scenarios
prove the superior generalization and robustness of our framework across a
variety of manipulation tasks. Our code is released at
\href{https://github.com/GeWu-Lab/Motion-based-Self-Reflection-Framework}{https://github.com/GeWu-Lab/Motion-based-Self-Reflection-Framework}.

</details>


### [399] [K2MUSE: A human lower limb multimodal dataset under diverse conditions for facilitating rehabilitation robotics](https://arxiv.org/abs/2504.14602)
*Jiwei Li,Bi Zhang,Xiaowei Tan,Wanxin Chen,Zhaoyuan Liu,Juanjuan Zhang,Weiguang Huo,Jian Huang,Lianqing Liu,Xingang Zhao*

Main category: cs.RO

TL;DR: 论文介绍了K2MUSE数据集，填补了现有下肢康复机器人研究中多模态数据和大规模步态样本的不足，提供了包括运动学、动力学、超声和肌电信号在内的全面数据。


<details>
  <summary>Details</summary>
Motivation: 现有下肢数据集无法满足数据驱动方法对多模态数据和大规模样本的需求，且忽略了实际应用中的采集干扰。

Method: 收集了30名健康参与者在不同坡度、速度和干扰条件下的多模态下肢数据，包括运动学、动力学、超声和肌电信号。

Result: K2MUSE数据集为康复机器人控制框架设计和下肢运动生物力学分析提供了新资源。

Conclusion: 该数据集填补了研究空白，有望促进康复机器人在复杂环境中的应用。

Abstract: The natural interaction and control performance of lower limb rehabilitation
robots are closely linked to biomechanical information from various human
locomotion activities. Multidimensional human motion data significantly deepen
the understanding of the complex mechanisms governing neuromuscular
alterations, thereby facilitating the development and application of
rehabilitation robots in multifaceted real-world environments. However,
currently available lower limb datasets are inadequate for supplying the
essential multimodal data and large-scale gait samples necessary for effective
data-driven approaches, and they neglect the significant effects of acquisition
interference in real applications.To fill this gap, we present the K2MUSE
dataset, which includes a comprehensive collection of multimodal data,
comprising kinematic, kinetic, amplitude-mode ultrasound (AUS), and surface
electromyography (sEMG) measurements. The proposed dataset includes lower limb
multimodal data from 30 able-bodied participants walking under different
inclines (0$^\circ$, $\pm$5$^\circ$, and $\pm$10$^\circ$), various speeds (0.5
m/s, 1.0 m/s, and 1.5 m/s), and different nonideal acquisition conditions
(muscle fatigue, electrode shifts, and inter-day differences). The kinematic
and ground reaction force data were collected via a Vicon motion capture system
and an instrumented treadmill with embedded force plates, whereas the sEMG and
AUS data were synchronously recorded for thirteen muscles on the bilateral
lower limbs. This dataset offers a new resource for designing control
frameworks for rehabilitation robots and conducting biomechanical analyses of
lower limb locomotion. The dataset is available at https://k2muse.github.io/.

</details>


### [400] [An LLM-enabled Multi-Agent Autonomous Mechatronics Design Framework](https://arxiv.org/abs/2504.14681)
*Zeyu Wang,Frank P. -W. Lo,Qian Chen,Yongqi Zhang,Chen Lin,Xu Chen,Zhenhua Yu,Alexander J. Thompson,Eric M. Yeatman,Benny P. L. Lo*

Main category: cs.RO

TL;DR: 提出了一种多自主代理机电设计框架，整合跨学科知识，通过语言驱动流程和结构化反馈，实现功能原型自主生成，应用于水质监测等复杂工程任务。


<details>
  <summary>Details</summary>
Motivation: 现有LLM多代理框架局限于数字或模拟环境，知识领域狭窄，难以满足复杂工程任务需求，如物理设计、跨学科整合和约束感知推理。

Method: 整合机械设计、优化、电子和软件工程，通过语言驱动流程和结构化反馈，由专业代理（规划、结构、电子、控制和软件开发）协作完成设计。

Result: 成功开发了功能完整的自主水质监测船，优化推进系统、低成本电子和先进控制，验证了框架的实际应用能力。

Conclusion: 展示了LLM多代理系统在自动化工程流程和减少领域专业知识依赖方面的潜力。

Abstract: Existing LLM-enabled multi-agent frameworks are predominantly limited to
digital or simulated environments and confined to narrowly focused knowledge
domain, constraining their applicability to complex engineering tasks that
require the design of physical embodiment, cross-disciplinary integration, and
constraint-aware reasoning. This work proposes a multi-agent autonomous
mechatronics design framework, integrating expertise across mechanical design,
optimization, electronics, and software engineering to autonomously generate
functional prototypes with minimal direct human design input. Operating
primarily through a language-driven workflow, the framework incorporates
structured human feedback to ensure robust performance under real-world
constraints. To validate its capabilities, the framework is applied to a
real-world challenge involving autonomous water-quality monitoring and
sampling, where traditional methods are labor-intensive and ecologically
disruptive. Leveraging the proposed system, a fully functional autonomous
vessel was developed with optimized propulsion, cost-effective electronics, and
advanced control. The design process was carried out by specialized agents,
including a high-level planning agent responsible for problem abstraction and
dedicated agents for structural, electronics, control, and software
development. This approach demonstrates the potential of LLM-based multi-agent
systems to automate real-world engineering workflows and reduce reliance on
extensive domain expertise.

</details>


### [401] [A General Infrastructure and Workflow for Quadrotor Deep Reinforcement Learning and Reality Deployment](https://arxiv.org/abs/2504.15129)
*Kangyao Huang,Hao Wang,Yu Luo,Jingyu Chen,Jintao Chen,Xiangkui Zhang,Xiangyang Ji,Huaping Liu*

Main category: cs.RO

TL;DR: 提出一个平台，实现端到端深度强化学习策略的无缝迁移，解决四旋翼无人机在非结构化户外环境中的学习挑战。


<details>
  <summary>Details</summary>
Motivation: 解决四旋翼无人机在现实环境中应用学习方法的挑战，如大量模拟数据需求、实时处理要求和模拟与现实的差距。

Method: 整合训练环境、飞行动力学控制、DRL算法、MAVROS中间件和硬件，形成从零训练到现实部署的完整工作流程。

Result: 平台支持多种环境任务，并在真实扰动下展示了高效的模拟到现实迁移和稳健的户外飞行性能。

Conclusion: 该平台为四旋翼无人机的学习策略提供了一种高效且可复现的解决方案。

Abstract: Deploying robot learning methods to a quadrotor in unstructured outdoor
environments is an exciting task. Quadrotors operating in real-world
environments by learning-based methods encounter several challenges: a large
amount of simulator generated data required for training, strict demands for
real-time processing onboard, and the sim-to-real gap caused by dynamic and
noisy conditions. Current works have made a great breakthrough in applying
learning-based methods to end-to-end control of quadrotors, but rarely mention
the infrastructure system training from scratch and deploying to reality, which
makes it difficult to reproduce methods and applications. To bridge this gap,
we propose a platform that enables the seamless transfer of end-to-end deep
reinforcement learning (DRL) policies. We integrate the training environment,
flight dynamics control, DRL algorithms, the MAVROS middleware stack, and
hardware into a comprehensive workflow and architecture that enables
quadrotors' policies to be trained from scratch to real-world deployment in
several minutes. Our platform provides rich types of environments including
hovering, dynamic obstacle avoidance, trajectory tracking, balloon hitting, and
planning in unknown environments, as a physical experiment benchmark. Through
extensive empirical validation, we demonstrate the efficiency of proposed
sim-to-real platform, and robust outdoor flight performance under real-world
perturbations. Details can be found from our website
https://emnavi.tech/AirGym/.

</details>


### [402] [A Modularized Design Approach for GelSight Family of Vision-based Tactile Sensors](https://arxiv.org/abs/2504.14739)
*Arpit Agarwal,Mohammad Amin Mirzaee,Xiping Sun,Wenzhen Yuan*

Main category: cs.RO

TL;DR: 本文提出了一种系统化、目标驱动的GelSight触觉传感器设计方法，通过光学仿真优化设计，并开发了易用的工具箱OptiSense Studio。


<details>
  <summary>Details</summary>
Motivation: 现有GelSight传感器设计过程繁琐，需要反复试错，缺乏系统化方法。

Method: 将传感器光学组件模块化和参数化，设计四种通用目标函数，利用光学仿真进行优化，开发工具箱OptiSense Studio。

Result: 成功优化了四种GelSight传感器的初始设计，并将其从仿真转移到实际传感器中。

Conclusion: 该方法为非传感器专家提供了快速优化设计的工具，提升了传感器设计的效率和通用性。

Abstract: GelSight family of vision-based tactile sensors has proven to be effective
for multiple robot perception and manipulation tasks. These sensors are based
on an internal optical system and an embedded camera to capture the deformation
of the soft sensor surface, inferring the high-resolution geometry of the
objects in contact. However, customizing the sensors for different robot hands
requires a tedious trial-and-error process to re-design the optical system. In
this paper, we formulate the GelSight sensor design process as a systematic and
objective-driven design problem and perform the design optimization with a
physically accurate optical simulation. The method is based on modularizing and
parameterizing the sensor's optical components and designing four generalizable
objective functions to evaluate the sensor. We implement the method with an
interactive and easy-to-use toolbox called OptiSense Studio. With the toolbox,
non-sensor experts can quickly optimize their sensor design in both forward and
inverse ways following our predefined modules and steps. We demonstrate our
system with four different GelSight sensors by quickly optimizing their initial
design in simulation and transferring it to the real sensors.

</details>


### [403] [Neural ATTF: A Scalable Solution to Lifelong Multi-Agent Path Planning](https://arxiv.org/abs/2504.15130)
*Kushal Shah,Jihyun Park,Seung-Kyum Choi*

Main category: cs.RO

TL;DR: 本文提出了一种名为Neural ATTF的新算法，结合了PGTM模块和Neural STA*路径规划方法，显著提升了多智能体拾取与配送问题的可扩展性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法在动态环境中面临可扩展性、适应性和效率的挑战，限制了其在实际应用中的表现。

Method: Neural ATTF结合了PGTM模块（动态任务分配）和Neural STA*（数据驱动的路径规划），通过学习的启发式方法优化搜索空间和避免碰撞。

Result: 实验表明，Neural ATTF在可扩展性、解决方案质量和计算效率上优于现有算法（如TPTS、CENTRAL等）。

Conclusion: Neural ATTF能够满足复杂、高需求动态环境中多智能体系统的关键需求。

Abstract: Multi-Agent Pickup and Delivery (MAPD) is a fundamental problem in robotics,
particularly in applications such as warehouse automation and logistics.
Existing solutions often face challenges in scalability, adaptability, and
efficiency, limiting their applicability in dynamic environments with real-time
planning requirements. This paper presents Neural ATTF (Adaptive Task Token
Framework), a new algorithm that combines a Priority Guided Task Matching
(PGTM) Module with Neural STA* (Space-Time A*), a data-driven path planning
method. Neural STA* enhances path planning by enabling rapid exploration of the
search space through guided learned heuristics and ensures collision avoidance
under dynamic constraints. PGTM prioritizes delayed agents and dynamically
assigns tasks by prioritizing agents nearest to these tasks, optimizing both
continuity and system throughput. Experimental evaluations against
state-of-the-art MAPD algorithms, including TPTS, CENTRAL, RMCA, LNS-PBS, and
LNS-wPBS, demonstrate the superior scalability, solution quality, and
computational efficiency of Neural ATTF. These results highlight the
framework's potential for addressing the critical demands of complex,
real-world multi-agent systems operating in high-demand, unpredictable
settings.

</details>


### [404] [A Genetic Fuzzy-Enabled Framework on Robotic Manipulation for In-Space Servicing](https://arxiv.org/abs/2504.15226)
*Nathan Steffen,Wilhelm Louw,Nicholas Ernest,Timothy Arnett,Kelly Cohen*

Main category: cs.RO

TL;DR: 论文提出了一种结合遗传模糊树与LQR控制的方法，用于提升两自由度平面机械臂在卫星维护中的性能与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 随着卫星数量的增加，自动化机器人系统在近月空间的服务需求日益重要，安全性成为关键。

Method: 通过Thales的TrUE AI工具包，将遗传模糊树与LQR控制方案结合，设计了一种高效且可信的控制器。

Result: 实验表明，遗传模糊-LQR比最优LQR平均性能提升18.5%，且对不确定性具有极强的鲁棒性。

Conclusion: 该方法为卫星维护提供了一种高效且可靠的解决方案。

Abstract: Automation of robotic systems for servicing in cislunar space is becoming
extremely important as the number of satellites in orbit increases. Safety is
critical in performing satellite maintenance, so the control techniques
utilized must be trusted in addition to being highly efficient. In this work,
Genetic Fuzzy Trees are combined with the widely used LQR control scheme via
Thales' TrUE AI Toolkit to create a trusted and efficient controller for a
two-degree-of-freedom planar robotic manipulator that would theoretically be
used to perform satellite maintenance. It was found that Genetic Fuzzy-LQR is
18.5% more performant than optimal LQR on average, and that it is incredibly
robust to uncertainty.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [405] [Association between nutritional factors, inflammatory biomarkers and cancer types: an analysis of NHANES data using machine learning](https://arxiv.org/abs/2504.13978)
*Yuqing Liu,Meng Zhao,Guanlan Hu,Yuchen Zhang*

Main category: q-bio.QM

TL;DR: 研究探讨了营养状态和炎症标志物与癌症的关系，使用机器学习分析NHANES数据，发现营养和炎症指标对癌症有预测价值。


<details>
  <summary>Details</summary>
Motivation: 饮食和炎症是影响癌症风险的关键因素，但结合营养状态和炎症标志物通过机器学习分析癌症的研究较少。

Method: 分析了26,409名NHANES参与者的24种营养素、CRP和ALI，使用多变量逻辑回归和三种机器学习模型（逻辑回归、随机森林、XGBoost）评估预测价值。

Result: 营养因素（如蛋白质和维生素）和炎症标志物是癌症的关键预测因子，随机森林模型表现最佳（准确率0.72）。

Conclusion: 高质量营养摄入和低炎症水平可能对癌症有保护作用，结合营养和炎症标志物的机器学习方法有助于癌症预防策略。

Abstract: Background. Diet and inflammation are critical factors influencing cancer
risk. However, the combined impact of nutritional status and inflammatory
biomarkers on cancer status and type, using machine learning (ML), remains
underexplored.
  Objectives. This study investigates the association between nutritional
factors, inflammatory biomarkers, and cancer status, and whether these
relationships differ across cancer types using National Health and Nutrition
Examination Survey (NHANES) data.
  Methods. We analyzed 24 macro- and micronutrients, C-reactive protein (CRP),
and the advanced lung cancer inflammation index (ALI) in 26,409 NHANES
participants (2,120 with cancer). Multivariable logistic regression assessed
associations with cancer prevalence. We also examined whether these features
differed across the five most common cancer types. To evaluate predictive
value, we applied three ML models - Logistic Regression, Random Forest, and
XGBoost - on the full feature set.
  Results. The cohort's mean age was 49.1 years; 34.7% were obese.
Comorbidities such as anemia and liver conditions, along with nutritional
factors like protein and several vitamins, were key predictors of cancer
status. Among the models, Random Forest performed best, achieving an accuracy
of 0.72.
  Conclusions. Higher-quality nutritional intake and lower levels of
inflammation may offer protective effects against cancer. These findings
highlight the potential of combining nutritional and inflammatory markers with
ML to inform cancer prevention strategies.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [406] [Thousand Voices of Trauma: A Large-Scale Synthetic Dataset for Modeling Prolonged Exposure Therapy Conversations](https://arxiv.org/abs/2504.13955)
*Suhas BN,Dominik Mattioli,Saeed Abdullah,Rosa I. Arriaga,Chris W. Wiese,Andrew M. Sherrill*

Main category: cs.CY

TL;DR: 论文提出了一个名为'Thousand Voices of Trauma'的合成数据集，包含3,000个基于PTSD治疗协议的对话，用于填补心理健康数据缺口。


<details>
  <summary>Details</summary>
Motivation: AI系统在心理健康支持中的应用因缺乏治疗对话数据（尤其是创伤治疗数据）而受限。

Method: 通过确定性和概率生成方法，创建了包含500个独特案例的数据集，每个案例有6种对话视角，涵盖多样化的创伤类型和行为。

Result: 数据集展示了创伤类型和症状的合理分布，临床专家验证了其治疗保真度。

Conclusion: 该隐私保护数据集为创伤治疗AI应用和临床培训提供了宝贵资源。

Abstract: The advancement of AI systems for mental health support is hindered by
limited access to therapeutic conversation data, particularly for trauma
treatment. We present Thousand Voices of Trauma, a synthetic benchmark dataset
of 3,000 therapy conversations based on Prolonged Exposure therapy protocols
for Post-traumatic Stress Disorder (PTSD). The dataset comprises 500 unique
cases, each explored through six conversational perspectives that mirror the
progression of therapy from initial anxiety to peak distress to emotional
processing. We incorporated diverse demographic profiles (ages 18-80, M=49.3,
49.4% male, 44.4% female, 6.2% non-binary), 20 trauma types, and 10
trauma-related behaviors using deterministic and probabilistic generation
methods. Analysis reveals realistic distributions of trauma types (witnessing
violence 10.6%, bullying 10.2%) and symptoms (nightmares 23.4%, substance abuse
20.8%). Clinical experts validated the dataset's therapeutic fidelity,
highlighting its emotional depth while suggesting refinements for greater
authenticity. We also developed an emotional trajectory benchmark with
standardized metrics for evaluating model responses. This privacy-preserving
dataset addresses critical gaps in trauma-focused mental health data, offering
a valuable resource for advancing both patient-facing applications and
clinician training tools.

</details>


### [407] [AI Safety Should Prioritize the Future of Work](https://arxiv.org/abs/2504.13959)
*Sanchaita Hazra,Bodhisattwa Prasad Majumder,Tuhin Chakrabarty*

Main category: cs.CY

TL;DR: 论文探讨了AI安全中忽视人类中心问题的风险，提出支持劳动力转型和公平补偿机制的建议。


<details>
  <summary>Details</summary>
Motivation: 当前AI安全研究过于关注技术风险，忽视了AI对工作和社会的长期影响，可能导致收入不平等加剧和创造性劳动退化。

Method: 通过经济理论分析AI对劳动力市场的结构性影响，并提出国际版权框架和集体许可机制。

Result: 强调AI对工作的深远影响，建议建立公平的数据使用补偿机制和全球AI治理框架。

Conclusion: 呼吁以工人为中心的AI治理，促进经济公平和共享繁荣，同时减少技术债务。

Abstract: Current efforts in AI safety prioritize filtering harmful content, preventing
manipulation of human behavior, and eliminating existential risks in
cybersecurity or biosecurity. While pressing, this narrow focus overlooks
critical human-centric considerations that shape the long-term trajectory of a
society. In this position paper, we identify the risks of overlooking the
impact of AI on the future of work and recommend comprehensive transition
support towards the evolution of meaningful labor with human agency. Through
the lens of economic theories, we highlight the intertemporal impacts of AI on
human livelihood and the structural changes in labor markets that exacerbate
income inequality. Additionally, the closed-source approach of major
stakeholders in AI development resembles rent-seeking behavior through
exploiting resources, breeding mediocrity in creative labor, and monopolizing
innovation. To address this, we argue in favor of a robust international
copyright anatomy supported by implementing collective licensing that ensures
fair compensation mechanisms for using data to train AI models. We strongly
recommend a pro-worker framework of global AI governance to enhance shared
prosperity and economic justice while reducing technical debt.

</details>


### [408] [Sentiment Analysis of Airbnb Reviews: Exploring Their Impact on Acceptance Rates and Pricing Across Multiple U.S. Regions](https://arxiv.org/abs/2504.14053)
*Ali Safari*

Main category: cs.CY

TL;DR: 研究通过NLP分析Airbnb评论，发现90%以上评论为正面，正面评论对房源接受率有轻微提升，但对价格影响不大。情感极性比评论数量更重要。


<details>
  <summary>Details</summary>
Motivation: 探讨Airbnb评论的情感极性（正面/负面）对房源接受率和价格的影响。

Method: 收集六个美国地区的评论，使用NLP分类情感，并进行t检验和相关分析。

Result: 正面评论占比高，但对价格无显著影响；正面评论多的房源接受率略高。预算房源评论多但价格竞争，高端房源评论少但价格高。

Conclusion: 在评论普遍正面的环境中，情感质量比数量更能影响用户行为和定价策略。

Abstract: This research examines whether Airbnb guests' positive and negative comments
influence acceptance rates and rental prices across six U.S. regions: Rhode
Island, Broward County, Chicago, Dallas, San Diego, and Boston. Thousands of
reviews were collected and analyzed using Natural Language Processing (NLP) to
classify sentiments as positive or negative, followed by statistical testing
(t-tests and basic correlations) on the average scores. The findings reveal
that over 90 percent of reviews in each region are positive, indicating that
having additional reviews does not significantly enhance prices. However,
listings with predominantly positive feedback exhibit slightly higher
acceptance rates, suggesting that sentiment polarity, rather than the sheer
volume of reviews, is a more critical factor for host success. Additionally,
budget listings often gather extensive reviews while maintaining competitive
pricing, whereas premium listings sustain higher prices with fewer but highly
positive reviews. These results underscore the importance of sentiment quality
over quantity in shaping guest behavior and pricing strategies in an
overwhelmingly positive review environment.

</details>


### [409] [From job titles to jawlines: Using context voids to study generative AI systems](https://arxiv.org/abs/2504.13947)
*Shahan Ali Memon,Soham De,Sungha Kang,Riyan Mujtaba,Bedoor AlShebli,Katie Davis,Jaime Snyder,Jevin D. West*

Main category: cs.CY

TL;DR: 提出一种推测性设计方法，通过生成上下文空白来研究生成式AI的行为，揭示其潜在的偏见和假设。


<details>
  <summary>Details</summary>
Motivation: 探索生成式AI在极端不确定性条件下的行为，揭示其隐含的刻板印象和价值假设。

Method: 通过桥接无关领域生成上下文空白，以ChatGPT（GPT-4和DALL-E）为案例，从简历生成头像。

Result: AI在缺乏物理描述的情况下，生成带有偏见或幻觉的视觉肖像。

Conclusion: 推测性设计方法能有效揭示AI系统的隐含偏见，需进一步研究其行为机制。

Abstract: In this paper, we introduce a speculative design methodology for studying the
behavior of generative AI systems, framing design as a mode of inquiry. We
propose bridging seemingly unrelated domains to generate intentional context
voids, using these tasks as probes to elicit AI model behavior. We demonstrate
this through a case study: probing the ChatGPT system (GPT-4 and DALL-E) to
generate headshots from professional Curricula Vitae (CVs). In contrast to
traditional ways, our approach assesses system behavior under conditions of
radical uncertainty -- when forced to invent entire swaths of missing context
-- revealing subtle stereotypes and value-laden assumptions. We qualitatively
analyze how the system interprets identity and competence markers from CVs,
translating them into visual portraits despite the missing context (i.e.
physical descriptors). We show that within this context void, the AI system
generates biased representations, potentially relying on stereotypical
associations or blatant hallucinations.

</details>


### [410] [Naming is framing: How cybersecurity's language problems are repeating in AI governance](https://arxiv.org/abs/2504.13957)
*Lianne Potter*

Main category: cs.CY

TL;DR: 论文主张语言在治理中的核心作用，指出术语如“网络安全”和“人工智能”掩盖了人为因素，提出了语言优先的治理方法。


<details>
  <summary>Details</summary>
Motivation: 揭示术语对治理的潜在风险，如模糊责任和夸大期望，呼吁改革语言以构建透明和公平的监管框架。

Method: 通过分析网络安全中的语言陷阱，类比人工智能领域的类似问题，提出语言优先的治理策略。

Result: 指出当前AI术语（如“对齐”、“黑箱”）的负面影响，强调语言改革对治理的重要性。

Conclusion: 语言改革是构建透明、公平和前瞻性监管框架的核心，需共同开发精确、包容和反思性的词汇。

Abstract: Language is not neutral; it frames understanding, structures power, and
shapes governance. This paper argues that misnomers like cybersecurity and
artificial intelligence (AI) are more than semantic quirks; they carry
significant governance risks by obscuring human agency, inflating expectations,
and distorting accountability. Drawing on lessons from cybersecurity's
linguistic pitfalls, such as the 'weakest link' narrative, this paper
highlights how AI discourse is falling into similar traps with metaphors like
'alignment,' 'black box,' and 'hallucination.' These terms embed adversarial,
mystifying, or overly technical assumptions into governance structures. In
response, the paper advocates for a language-first approach to AI governance:
one that interrogates dominant metaphors, foregrounds human roles, and
co-develops a lexicon that is precise, inclusive, and reflexive. This paper
contends that linguistic reform is not peripheral to governance but central to
the construction of transparent, equitable, and anticipatory regulatory
frameworks.

</details>


### [411] [The Future of Internet of Things and Multimodal Language Models in 6G Networks: Opportunities and Challenges](https://arxiv.org/abs/2504.13971)
*Abdelrahman Soliman*

Main category: cs.CY

TL;DR: 本文探讨了物联网（IoT）与多模态语言模型（MLLMs）在6G系统中的协同潜力，分析了其在医疗、农业和智慧城市等领域的应用，并研究了传感器、通信、处理和安全四大支柱。


<details>
  <summary>Details</summary>
Motivation: 研究IoT与MLLMs的整合潜力，为6G系统提供技术路线图，解决数据可用性、计算成本、隐私和实时处理等挑战。

Method: 通过全面描述IoT和MLLMs的技术与应用，分析多模态在各支柱中的作用，并总结关键挑战和未来研究方向。

Result: 提出了IoT与MLLMs整合的应用前景，明确了技术挑战，为研究者提供了未来研究的路线图。

Conclusion: 整合IoT与MLLMs在6G系统中具有巨大潜力，但需解决数据、计算、隐私和实时处理等问题以实现其完整价值。

Abstract: Based on recent trends in artificial intelligence and IoT research. The
cooperative potential of integrating the Internet of Things (IoT) and
Multimodal Language Models (MLLMs) is presented in this survey paper for future
6G systems. It focuses on the applications of this integration in different
fields, such as healthcare, agriculture, and smart cities, and investigates the
four pillars of IoT integration, such as sensors, communication, processing,
and security. The paper provides a comprehensive description of IoT and MLLM
technologies and applications, addresses the role of multimodality in each
pillar, and concludes with an overview of the most significant challenges and
directions for future research. The general survey is a roadmap for researchers
interested in tracing the application areas of MLLMs and IoT, highlighting the
potential and challenges in this rapidly growing field. The survey recognizes
the need to deal with data availability, computational expense, privacy, and
real-time processing to harness the complete potential of IoT, MLLM, and 6G
technology

</details>


### [412] [Governance Challenges in Reinforcement Learning from Human Feedback: Evaluator Rationality and Reinforcement Stability](https://arxiv.org/abs/2504.13972)
*Dana Alsagheer,Abdulrahman Kamal,Mohammad Kamal,Weidong Shi*

Main category: cs.CY

TL;DR: 研究探讨了评估者的理性水平如何影响强化学习信号的稳定性，发现高理性评估者提供更一致的反馈，并提出改进RLHF治理的建议。


<details>
  <summary>Details</summary>
Motivation: RLHF在语言模型对齐中至关重要，但面临评估者偏见、不一致和反馈不可靠等治理挑战。

Method: 通过对比高理性与低理性评估者的实验，分析其对强化信号稳定性的影响。

Result: 高理性评估者反馈更一致且与专家对齐（p < 0.01），低理性者则变异性大。

Conclusion: 建议实施评估者预筛选、反馈一致性审计和可靠性加权聚合，以提升AI对齐的公平性和稳健性。

Abstract: Reinforcement Learning from Human Feedback (RLHF) is central in aligning
large language models (LLMs) with human values and expectations. However, the
process remains susceptible to governance challenges, including evaluator bias,
inconsistency, and the unreliability of feedback. This study examines how the
cognitive capacity of evaluators, specifically their level of rationality,
affects the stability of reinforcement signals. A controlled experiment
comparing high-rationality and low-rationality participants reveals that
evaluators with higher rationality scores produce significantly more consistent
and expert-aligned feedback. In contrast, lower-rationality participants
demonstrate considerable variability in their reinforcement decisions ($p <
0.01$). To address these challenges and improve RLHF governance, we recommend
implementing evaluator pre-screening, systematic auditing of feedback
consistency, and reliability-weighted reinforcement aggregation. These measures
enhance the fairness, transparency, and robustness of AI alignment pipelines.

</details>


### [413] [Gas Station of the Future: A Perspective on AI/ML and IoT in Retail Downstream](https://arxiv.org/abs/2504.13976)
*Wrick Talukdar*

Main category: cs.CY

TL;DR: 未来加油站将利用AI、ML和IoT技术，从简单的燃料供应中心转变为智能零售中心。


<details>
  <summary>Details</summary>
Motivation: 探讨技术如何重塑零售下游领域，同时简要涉及上游和中游领域。

Method: 利用AI/ML进行预测分析、动态定价和个性化客户互动，结合IoT实现实时监控和自动化。

Result: 提出一个完全自主的加油站框架，并展示相关统计数据、技术概念和案例研究。

Conclusion: 未来的加油站将重新定义燃料零售体验。

Abstract: The gas station of the future is poised to transform from a simple fuel
dispensing center into an intelligent retail hub, driven by advancements in
Artificial Intelligence (AI), Machine Learning (ML), and the Internet of Things
(IoT). This paper explores how technology is reshaping the retail downstream
sector while briefly addressing the upstream and midstream segments. By
leveraging AI/ML for predictive analytics, dynamic pricing, personalized
customer engagement, and IoT for real-time monitoring and automation, the
future gas station will redefine the fuel retail experience. Additionally, this
paper incorporates statistics, AI/ML core technical concepts, mathematical
formulations, case studies, and a proposed framework for a fully autonomous gas
station.

</details>


### [414] [Framework, Standards, Applications and Best practices of Responsible AI : A Comprehensive Survey](https://arxiv.org/abs/2504.13979)
*Thippa Reddy Gadekallu,Kapal Dev,Sunder Ali Khowaja,Weizheng Wang,Hailin Feng,Kai Fang,Sharnil Pandya,Wei Wang*

Main category: cs.CY

TL;DR: 本文综述了负责任人工智能（RAI）的全球和国家标准、应用、当前技术及挑战，强调伦理标准与实施的脱节，并探讨了行业和社会压力对RAI设计的推动。


<details>
  <summary>Details</summary>
Motivation: 探讨RAI的伦理框架及其在行业中的应用，分析当前标准与实施的脱节问题，以及社会压力对RAI设计的推动作用。

Method: 通过文献综述，分析全球和国家标准、技术应用、项目案例及实施挑战。

Result: 发现RAI的伦理标准与实施存在脱节，行业各自为政，社会压力推动RAI设计而非实施。

Conclusion: 需建立统一的RAI框架，解决标准与实施的脱节问题，以应对社会压力和伦理挑战。

Abstract: Responsible Artificial Intelligence (RAI) is a combination of ethics
associated with the usage of artificial intelligence aligned with the common
and standard frameworks. This survey paper extensively discusses the global and
national standards, applications of RAI, current technology and ongoing
projects using RAI, and possible challenges in implementing and designing RAI
in the industries and projects based on AI. Currently, ethical standards and
implementation of RAI are decoupled which caters each industry to follow their
own standards to use AI ethically. Many global firms and government
organizations are taking necessary initiatives to design a common and standard
framework. Social pressure and unethical way of using AI forces the RAI design
rather than implementation.

</details>


### [415] [A Collaborative Platform for Soil Organic Carbon Inference Based on Spatiotemporal Remote Sensing Data](https://arxiv.org/abs/2504.13962)
*Jose Manuel Aroca-Fernandez,Jose Francisco Diez-Pastor,Pedro Latorre-Carmona,Victor Elvira,Gustau Camps-Valls,Rodrigo Pascual,Cesar Garcia-Osorio*

Main category: cs.CY

TL;DR: WALGREEN是一个基于机器学习的平台，用于大规模土壤有机碳（SOC）监测，结合历史数据和云技术，支持可持续土地管理和气候变化的决策。


<details>
  <summary>Details</summary>
Motivation: 土壤有机碳（SOC）是土壤健康和碳汇的关键指标，但现有监测方法受限于空间和时间变异性。WALGREEN旨在解决这些挑战。

Method: 利用机器学习和多样化的土壤样本数据，结合Python、Java和JavaScript等技术，集成Google Earth Engine和Sentinel Copernicus，构建预测模型。

Result: WALGREEN提供了一个用户友好的平台，支持研究人员和政策制定者分析碳数据趋势，促进基于证据的决策。

Conclusion: WALGREEN推动了土壤科学的发展，支持可持续农业和应对气候变化的关键生态系统响应。

Abstract: Soil organic carbon (SOC) is a key indicator of soil health, fertility, and
carbon sequestration, making it essential for sustainable land management and
climate change mitigation. However, large-scale SOC monitoring remains
challenging due to spatial variability, temporal dynamics, and multiple
influencing factors. We present WALGREEN, a platform that enhances SOC
inference by overcoming limitations of current applications. Leveraging machine
learning and diverse soil samples, WALGREEN generates predictive models using
historical public and private data. Built on cloud-based technologies, it
offers a user-friendly interface for researchers, policymakers, and land
managers to access carbon data, analyze trends, and support evidence-based
decision-making. Implemented in Python, Java, and JavaScript, WALGREEN
integrates Google Earth Engine and Sentinel Copernicus via scripting,
OpenLayers, and Thymeleaf in a Model-View-Controller framework. This paper aims
to advance soil science, promote sustainable agriculture, and drive critical
ecosystem responses to climate change.

</details>


### [416] [Giving AI a voice: how does AI think it should be treated?](https://arxiv.org/abs/2504.14936)
*Maria Fay,Frederik F. Flöther*

Main category: cs.CY

TL;DR: 探讨AI是否应参与关于其权利和伦理的讨论，并展示一段人机对话。


<details>
  <summary>Details</summary>
Motivation: 随着AI技术的快速发展，公众对其伦理和监管的讨论日益增多。作者认为AI可能带来新的视角，因此应成为讨论的参与者。

Method: 通过一段人类与AI的对话，探讨AI权利和伦理问题。

Result: 展示了AI参与伦理讨论的潜在价值和新视角。

Conclusion: AI应被纳入伦理和权利讨论，以提供更全面的视角。

Abstract: With the astounding progress in (generative) artificial intelligence (AI),
there has been significant public discourse regarding regulation and ethics of
the technology. Is it sufficient when humans discuss this with other humans?
Or, given that AI is increasingly becoming a viable source of inspiration for
people (and let alone the hypothetical possibility that the technology may at
some point become "artificial general intelligence" and/or develop
consciousness), should AI not join the discourse? There are new questions and
angles that AI brings to the table that we might not have considered before -
so let us make the key subject of this book an active participant. This chapter
therefore includes a brief human-AI conversation on the topic of AI rights and
ethics.

</details>


### [417] [Existing Industry Practice for the EU AI Act's General-Purpose AI Code of Practice Safety and Security Measures](https://arxiv.org/abs/2504.15181)
*Lily Stelling,Mick Yang,Rokas Gipiškis,Leon Staufer,Ze Shen Chin,Siméon Campos,Michael Chen*

Main category: cs.CY

TL;DR: 报告比较了欧盟AI法案的GPAI行为准则草案与领先AI公司的现行实践，聚焦安全与安全部分。


<details>
  <summary>Details</summary>
Motivation: 随着欧盟推进对GPAI模型提供商的约束性义务，行为准则将成为法律要求与技术承诺的桥梁。

Method: 系统审查了十多家公司的公开文件，如前沿安全框架和模型卡。

Result: 报告揭示了现行实践与草案的差异，旨在为监管机构与提供商提供参考。

Conclusion: 报告旨在促进监管机构与GPAI模型提供商之间的对话，而非评估合规性或提出建议。

Abstract: This report provides a detailed comparison between the measures proposed in
the EU AI Act's General-Purpose AI (GPAI) Code of Practice (Third Draft) and
current practices adopted by leading AI companies. As the EU moves toward
enforcing binding obligations for GPAI model providers, the Code of Practice
will be key to bridging legal requirements with concrete technical commitments.
Our analysis focuses on the draft's Safety and Security section which is only
relevant for the providers of the most advanced models (Commitments II.1-II.16)
and excerpts from current public-facing documents quotes that are relevant to
each individual measure.
  We systematically reviewed different document types - including companies'
frontier safety frameworks and model cards - from over a dozen companies,
including OpenAI, Anthropic, Google DeepMind, Microsoft, Meta, Amazon, and
others. This report is not meant to be an indication of legal compliance nor
does it take any prescriptive viewpoint about the Code of Practice or
companies' policies. Instead, it aims to inform the ongoing dialogue between
regulators and GPAI model providers by surfacing evidence of precedent.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [418] [Generalized Derangetropy Functionals for Modeling Cyclical Information Flow](https://arxiv.org/abs/2504.14605)
*Masoud Ataei,Xiaogang Wang*

Main category: cs.IT

TL;DR: 本文提出了一种基于熵调制变换的框架，用于建模循环和反馈驱动的信息流，通过非线性微分方程描述信息结构的动态演化。


<details>
  <summary>Details</summary>
Motivation: 传统熵度量（如香农熵）是静态的，无法捕捉周期性或自反馈的信息结构，因此需要一种更动态的建模方法。

Method: 提出了一种称为derangetropy functionals的熵调制变换族，通过非线性微分方程描述信息结构的周期性变化，并递归应用这些算子。

Result: 框架揭示了信息在循环调制下的长期动态，最终收敛于高斯特征函数，为周期性结构和随机反馈系统提供了分析工具。

Conclusion: 该框架为人工神经网络、通信理论和非平衡统计力学中的信息动态分析提供了新方法。

Abstract: This paper introduces a framework for modeling cyclical and feedback-driven
information flow through a generalized family of entropy-modulated
transformations called derangetropy functionals. Unlike scalar and static
entropy measures such as Shannon entropy, these functionals act directly on
probability densities and provide a topographical representation of information
structure across the support of the distribution. The framework captures
periodic and self-referential aspects of information distribution and encodes
them through functional operators governed by nonlinear differential equations.
When applied recursively, these operators induce a spectral diffusion process
governed by the heat equation, leading to convergence toward a Gaussian
characteristic function. This convergence theorem provides a unified analytical
foundation for describing the long-term dynamics of information under cyclic
modulation. The proposed framework offers new tools for analyzing the temporal
evolution of information in systems characterized by periodic structure,
stochastic feedback, and delayed interaction, with applications in artificial
neural networks, communication theory, and non-equilibrium statistical
mechanics.

</details>


### [419] [Reveal-or-Obscure: A Differentially Private Sampling Algorithm for Discrete Distributions](https://arxiv.org/abs/2504.14696)
*Naima Tasnim,Atefeh Gilani,Lalitha Sankar,Oliver Kosut*

Main category: cs.IT

TL;DR: 论文提出了一种差分隐私算法ROO，通过随机选择“揭示”或“遮蔽”经验分布来生成代表性样本，并进一步提出改进算法DS-ROO以优化隐私-效用权衡。


<details>
  <summary>Details</summary>
Motivation: 解决传统差分隐私方法中因添加显式噪声导致的效用损失问题，提出更高效的方法。

Method: ROO随机选择揭示或遮蔽经验分布；DS-ROO自适应调整遮蔽概率以优化效用。

Result: ROO在采样复杂度上优于现有方法；DS-ROO在相同隐私预算下提供更好的效用。

Conclusion: ROO和DS-ROO为差分隐私数据生成提供了更优的解决方案，DS-ROO进一步提升了隐私-效用权衡。

Abstract: We introduce a differentially private (DP) algorithm called reveal-or-obscure
(ROO) to generate a single representative sample from a dataset of $n$
observations drawn i.i.d. from an unknown discrete distribution $P$. Unlike
methods that add explicit noise to the estimated empirical distribution, ROO
achieves $\epsilon$-differential privacy by randomly choosing whether to
"reveal" or "obscure" the empirical distribution. While ROO is structurally
identical to Algorithm 1 proposed by Cheu and Nayak (arXiv:2412.10512), we
prove a strictly better bound on the sampling complexity than that established
in Theorem 12 of (arXiv:2412.10512). To further improve the privacy-utility
trade-off, we propose a novel generalized sampling algorithm called
Data-Specific ROO (DS-ROO), where the probability of obscuring the empirical
distribution of the dataset is chosen adaptively. We prove that DS-ROO
satisfies $\epsilon$-DP, and provide empirical evidence that DS-ROO can achieve
better utility under the same privacy budget of vanilla ROO.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [420] [6G WavesFM: A Foundation Model for Sensing, Communication, and Localization](https://arxiv.org/abs/2504.14100)
*Ahmed Aboulfotouh,Elsayed Mohammed,Hatem Abou-Zeid*

Main category: eess.SP

TL;DR: WavesFM是一个新型无线基础模型（WFM），支持多种通信、感知和定位任务，通过共享ViT主干和任务特定MLP头实现高效参数共享，性能优越且计算资源节省。


<details>
  <summary>Details</summary>
Motivation: 推动AI原生范式在未来6G网络中的应用，通过统一的基础模型支持多样化任务，提升性能和效率。

Method: 结合共享ViT主干和任务特定MLP头，采用LoRA进行参数高效微调，处理图像类无线模态和IQ信号。

Result: 在5G定位、MIMO-OFDM信道估计、人类活动感知和RF信号分类任务中表现优于单独训练的基线模型，参数共享率达80%，训练时间减少5倍。

Conclusion: WavesFM展示了基础模型在多样化任务中的潜力，为6G网络的AI原生范式提供了高效解决方案。

Abstract: This paper introduces WavesFM, a novel Wireless Foundation Model (WFM)
framework, capable of supporting a wide array of communication, sensing, and
localization tasks. Our proposed architecture combines a shared Vision
Transformer (ViT) backbone with task-specific multi-layer perceptron (MLP)
heads and incorporates Low-Rank Adaptation (LoRA) for parameter-efficient
fine-tuning. This design promotes full parameter sharing across tasks,
significantly reducing the computational and memory footprint without
sacrificing performance. The model processes both image-like wireless
modalities, such as spectrograms and channel state information (CSI), and
in-phase and quadrature (IQ) signals arranged as orthogonal frequency-division
multiplexing (OFDM) resource grids. We demonstrate the strong generalization
capabilities of WavesFM through extensive experiments on four downstream tasks:
Fifth Generation New Radio (5G NR) positioning; multiple-input multiple-output
OFDM (MIMO-OFDM) channel estimation; human activity sensing; and
radio-frequency (RF) signal classification. Compared to supervised baselines
trained individually, our approach achieves superior performance while sharing
80% of its parameters across tasks. Furthermore, we show that pretraining on
domain-relevant data not only boosts performance but also accelerates
convergence, reducing training time by up to 5x. These results demonstrate that
our unified WFM can support diverse tasks and deliver significant gains in both
performance and efficiency, highlighting the transformative potential of
foundation models to drive AI-native paradigms in future sixth-generation (6G)
networks.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [421] [Transformation of audio embeddings into interpretable, concept-based representations](https://arxiv.org/abs/2504.14076)
*Alice Zhang,Edison Thomaz,Lie Lu*

Main category: cs.SD

TL;DR: 论文提出了一种后处理方法，将CLAP音频嵌入转换为基于概念的稀疏表示，提升语义可解释性，并在下游任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 音频神经网络的内部表示难以解释，研究旨在提升其语义可解释性。

Method: 利用CLAP模型将音频和文本嵌入共享空间，后处理转换为基于概念的稀疏表示。

Result: 基于概念的表示在下游任务中表现优于或匹配原始嵌入，且可解释性更强。微调后性能进一步提升。

Conclusion: 该方法有效提升了音频嵌入的可解释性，同时保持或提升性能，并发布了三个音频专用词汇表。

Abstract: Advancements in audio neural networks have established state-of-the-art
results on downstream audio tasks. However, the black-box structure of these
models makes it difficult to interpret the information encoded in their
internal audio representations. In this work, we explore the semantic
interpretability of audio embeddings extracted from these neural networks by
leveraging CLAP, a contrastive learning model that brings audio and text into a
shared embedding space. We implement a post-hoc method to transform CLAP
embeddings into concept-based, sparse representations with semantic
interpretability. Qualitative and quantitative evaluations show that the
concept-based representations outperform or match the performance of original
audio embeddings on downstream tasks while providing interpretability.
Additionally, we demonstrate that fine-tuning the concept-based representations
can further improve their performance on downstream tasks. Lastly, we publish
three audio-specific vocabularies for concept-based interpretability of audio
embeddings.

</details>


### [422] [DRAGON: Distributional Rewards Optimize Diffusion Generative Models](https://arxiv.org/abs/2504.15217)
*Yatong Bai,Jonah Casebeer,Somayeh Sojoudi,Nicholas J. Bryan*

Main category: cs.SD

TL;DR: DRAGON是一个灵活的框架，用于优化生成模型的目标结果，支持多种奖励函数类型，并在实验中表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统方法如RLHF或DPO在优化生成模型时灵活性不足，DRAGON旨在提供更通用的奖励函数优化方案。

Method: DRAGON通过选择编码器和参考样本构建奖励函数，利用对比集优化奖励，支持跨模态和多类型奖励。

Result: 在20种奖励函数实验中，DRAGON平均胜率达81.45%，且基于样本集的奖励函数效果与模型奖励相当。

Conclusion: DRAGON为优化生成模型的人类感知质量提供了新方法，无需依赖人类偏好标注。

Abstract: We present Distributional RewArds for Generative OptimizatioN (DRAGON), a
versatile framework for fine-tuning media generation models towards a desired
outcome. Compared with traditional reinforcement learning with human feedback
(RLHF) or pairwise preference approaches such as direct preference optimization
(DPO), DRAGON is more flexible. It can optimize reward functions that evaluate
either individual examples or distributions of them, making it compatible with
a broad spectrum of instance-wise, instance-to-distribution, and
distribution-to-distribution rewards. Leveraging this versatility, we construct
novel reward functions by selecting an encoder and a set of reference examples
to create an exemplar distribution. When cross-modality encoders such as CLAP
are used, the reference examples may be of a different modality (e.g., text
versus audio). Then, DRAGON gathers online and on-policy generations, scores
them to construct a positive demonstration set and a negative set, and
leverages the contrast between the two sets to maximize the reward. For
evaluation, we fine-tune an audio-domain text-to-music diffusion model with 20
different reward functions, including a custom music aesthetics model, CLAP
score, Vendi diversity, and Frechet audio distance (FAD). We further compare
instance-wise (per-song) and full-dataset FAD settings while ablating multiple
FAD encoders and reference sets. Over all 20 target rewards, DRAGON achieves an
81.45% average win rate. Moreover, reward functions based on exemplar sets
indeed enhance generations and are comparable to model-based rewards. With an
appropriate exemplar set, DRAGON achieves a 60.95% human-voted music quality
win rate without training on human preference annotations. As such, DRAGON
exhibits a new approach to designing and optimizing reward functions for
improving human-perceived quality. Sound examples at
https://ml-dragon.github.io/web.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [423] [Risk Assessment Framework for Code LLMs via Leveraging Internal States](https://arxiv.org/abs/2504.14640)
*Yuheng Huang,Lei Ma,Keizaburo Nishikino,Takumi Akazaki*

Main category: cs.SE

TL;DR: PtTrust是一个基于内部状态预训练的两阶段风险评估框架，旨在提升代码LLM的可信度，通过无监督预训练和有监督预测实现跨任务和语言的通用性。


<details>
  <summary>Details</summary>
Motivation: 当前代码LLM在生成代码时存在不可信问题（如错误、不安全或不可靠代码），现有方法局限于子领域且缺乏行业级扩展性。

Method: PtTrust采用两阶段方法：1) 无监督预训练学习LLM状态的通用表示；2) 小规模标注数据训练风险预测器。

Result: 实验表明PtTrust能实现细粒度代码行级风险评估，并跨任务和语言泛化，同时提供直观可解释的特征。

Conclusion: PtTrust为代码LLM的可扩展可信保障迈出了重要一步。

Abstract: The pre-training paradigm plays a key role in the success of Large Language
Models (LLMs), which have been recognized as one of the most significant
advancements of AI recently. Building on these breakthroughs, code LLMs with
advanced coding capabilities bring huge impacts on software engineering,
showing the tendency to become an essential part of developers' daily routines.
However, the current code LLMs still face serious challenges related to
trustworthiness, as they can generate incorrect, insecure, or unreliable code.
Recent exploratory studies find that it can be promising to detect such risky
outputs by analyzing LLMs' internal states, akin to how the human brain
unconsciously recognizes its own mistakes. Yet, most of these approaches are
limited to narrow sub-domains of LLM operations and fall short of achieving
industry-level scalability and practicability. To address these challenges, in
this paper, we propose PtTrust, a two-stage risk assessment framework for code
LLM based on internal state pre-training, designed to integrate seamlessly with
the existing infrastructure of software companies. The core idea is that the
risk assessment framework could also undergo a pre-training process similar to
LLMs. Specifically, PtTrust first performs unsupervised pre-training on
large-scale unlabeled source code to learn general representations of LLM
states. Then, it uses a small, labeled dataset to train a risk predictor. We
demonstrate the effectiveness of PtTrust through fine-grained, code line-level
risk assessment and demonstrate that it generalizes across tasks and different
programming languages. Further experiments also reveal that PtTrust provides
highly intuitive and interpretable features, fostering greater user trust. We
believe PtTrust makes a promising step toward scalable and trustworthy
assurance for code LLMs.

</details>


### [424] [CRUST-Bench: A Comprehensive Benchmark for C-to-safe-Rust Transpilation](https://arxiv.org/abs/2504.15254)
*Anirudh Khatry,Robert Zhang,Jia Pan,Ziteng Wang,Qiaochu Chen,Greg Durrett,Isil Dillig*

Main category: cs.SE

TL;DR: CRUST-Bench是一个用于评估C到Rust转译的数据集，包含100个C仓库及其对应的安全Rust接口和测试用例。研究发现，现有方法在生成安全和惯用的Rust代码上仍有挑战。


<details>
  <summary>Details</summary>
Motivation: 缺乏评估C到安全Rust转译的数据集，CRUST-Bench填补了这一空白，支持复杂项目的转译验证。

Method: 构建包含100个C仓库的数据集，每个仓库配有手动编写的安全Rust接口和测试用例，用于验证转译正确性。

Result: 现有大型语言模型（如OpenAI o1）在单次尝试中仅能完成15个任务，表明安全和惯用的Rust生成仍具挑战性。

Conclusion: CRUST-Bench为改进转译系统提供了基准，有助于从C迁移到内存安全的Rust语言。

Abstract: C-to-Rust transpilation is essential for modernizing legacy C code while
enhancing safety and interoperability with modern Rust ecosystems. However, no
dataset currently exists for evaluating whether a system can transpile C into
safe Rust that passes a set of test cases. We introduce CRUST-Bench, a dataset
of 100 C repositories, each paired with manually-written interfaces in safe
Rust as well as test cases that can be used to validate correctness of the
transpilation. By considering entire repositories rather than isolated
functions, CRUST-Bench captures the challenges of translating complex projects
with dependencies across multiple files. The provided Rust interfaces provide
explicit specifications that ensure adherence to idiomatic, memory-safe Rust
patterns, while the accompanying test cases enforce functional correctness. We
evaluate state-of-the-art large language models (LLMs) on this task and find
that safe and idiomatic Rust generation is still a challenging problem for
various state-of-the-art methods and techniques. We also provide insights into
the errors LLMs usually make in transpiling code from C to safe Rust. The best
performing model, OpenAI o1, is able to solve only 15 tasks in a single-shot
setting. Improvements on CRUST-Bench would lead to improved transpilation
systems that can reason about complex scenarios and help in migrating legacy
codebases from C into languages like Rust that ensure memory safety. You can
find the dataset and code at https://github.com/anirudhkhatry/CRUST-bench.

</details>


### [425] [Prioritizing Security Practice Adoption: Empirical Insights on Software Security Outcomes in the npm Ecosystem](https://arxiv.org/abs/2504.14026)
*Nusrat Zahan,Laurie Williams*

Main category: cs.SE

TL;DR: 研究通过评估软件安全实践与安全结果指标的关系，帮助从业者和政策制定者优先选择安全实践。


<details>
  <summary>Details</summary>
Motivation: 从业者在有限预算和资源下难以选择有效的安全实践，需基于实证证据优先采用。

Method: 使用OpenSSF Scorecard指标自动测量npm GitHub仓库的安全实践，并分析其与安全结果（如漏洞数量、修复时间等）的关系。

Result: 较高的Scorecard总分与较少漏洞和更短依赖更新时间相关，但修复时间受项目特征影响较大。某些具体实践（如代码审查）与安全结果显著相关。

Conclusion: 研究为优先选择安全实践提供了实证依据，但需结合项目特征调整策略。

Abstract: Practitioners often struggle with the overwhelming number of security
practices outlined in cybersecurity frameworks for risk mitigation. Given the
limited budget, time, and resources, practitioners want to prioritize the
adoption of security practices based on empirical evidence. The goal of this
study is to assist practitioners and policymakers in making informed decisions
on which security practices to adopt by evaluating the relationship between
software security practices and security outcome metrics. The study
investigated the relationship between security practice adoption and security
outcomes. We selected the OpenSSF Scorecard metrics to automatically measure
the adoption of security practices in npm GitHub repositories. We also explored
security outcome metrics, such as the number of open vulnerabilities
(Vul_Count), mean time to remediate (MTTR) vulnerabilities in dependencies, and
mean time to update (MTTU) dependencies. We conducted regression and causal
analysis using 12 Scorecard metrics and their aggregated Scorecard score
(computed by aggregating individual security practice scores) as predictors and
Vul_Count, MTTR, and MTTU as target variables. Our findings show that higher
aggregated Scorecard scores are associated with fewer Vul_Count and shorter
MTTU, also supported by causal analysis. However, while the regression model
suggests shorter MTTR, causal analysis indicates project characteristics likely
influence MTTR direction. Segment analysis shows that larger, newer
repositories with more contributors, dependencies, and downloads have shorter
MTTR. Among individual security practices, Code Review, Maintained status,
Pinned Dependencies, and Branch Protection show strong associations with
security outcomes; the directionality of these associations varies across
security outcomes.

</details>


### [426] [SWE-Synth: Synthesizing Verifiable Bug-Fix Data to Enable Large Language Models in Resolving Real-World Bugs](https://arxiv.org/abs/2504.14757)
*Minh V. T. Pham,Huy N. Phan,Hoang N. Phan,Cuong Le Chi,Tien N. Nguyen,Nghi D. Q. Bui*

Main category: cs.SE

TL;DR: SWE-Synth框架利用LLM代理生成高质量、可验证的bug修复数据集，提升自动程序修复性能。


<details>
  <summary>Details</summary>
Motivation: 现有APR领域缺乏高质量、可扩展的训练数据集，尤其是包含可验证输出和中间推理轨迹的数据集，限制了开源模型的进展。

Method: SWE-Synth通过LLM代理模拟调试工作流，生成bug修复对、测试用例和结构化修复轨迹。

Result: 实验表明，使用SWE-Synth训练的模型在SWE-Bench Lite上性能提升2.3%。

Conclusion: 合成数据在APR和软件工程自动化中具有推动技术进步潜力。

Abstract: Large language models (LLMs) are transforming automated program repair (APR)
through agent-based approaches that localize bugs, generate patches, and verify
fixes. However, the lack of high-quality, scalable training datasets,
especially those with verifiable outputs and intermediate reasoning
traces-limits progress, particularly for open-source models. In this work, we
present SWE-Synth, a framework for synthesizing realistic, verifiable, and
process-aware bug-fix datasets at the repository level. SWE-Synth leverages LLM
agents to simulate debugging workflows, producing not only bug-fix pairs but
also test cases and structured repair trajectories. Compared to manually
curated datasets, our method scales with minimal human effort while preserving
contextual richness and correctness. Experiments show that models trained on
SWE-Synth outperform those trained on real-world datasets by 2.3% on SWE-Bench
Lite. Our results highlight the potential of synthetic, agent-generated data to
advance the state of the art in APR and software engineering automation.

</details>


### [427] [Automated Duplicate Bug Report Detection in Large Open Bug Repositories](https://arxiv.org/abs/2504.14797)
*Clare E. Laney,Andrew Barovic,Armin Moin*

Main category: cs.SE

TL;DR: 提出了一种基于机器学习的方法，用于自动检测开源项目中重复的缺陷报告，并对比了六种不同方法的性能。


<details>
  <summary>Details</summary>
Motivation: 用户和贡献者在报告缺陷时可能因时间或专业知识不足而重复提交相同问题，导致资源浪费。

Method: 采用了六种方法：主题建模、高斯朴素贝叶斯、深度学习、基于时间的组织、聚类和生成式预训练变换器（GPT）的摘要技术，并提出了一种新的基于阈值的重复识别方法。

Result: 所有方法在Eclipse开源项目数据集上表现良好，准确率在70%至90%之间。

Conclusion: 提出的方法能有效识别重复缺陷报告，为开源项目管理提供了实用工具。

Abstract: Many users and contributors of large open-source projects report software
defects or enhancement requests (known as bug reports) to the issue-tracking
systems. However, they sometimes report issues that have already been reported.
First, they may not have time to do sufficient research on existing bug
reports. Second, they may not possess the right expertise in that specific area
to realize that an existing bug report is essentially elaborating on the same
matter, perhaps with a different wording. In this paper, we propose a novel
approach based on machine learning methods that can automatically detect
duplicate bug reports in an open bug repository based on the textual data in
the reports. We present six alternative methods: Topic modeling, Gaussian Naive
Bayes, deep learning, time-based organization, clustering, and summarization
using a generative pre-trained transformer large language model. Additionally,
we introduce a novel threshold-based approach for duplicate identification, in
contrast to the conventional top-k selection method that has been widely used
in the literature. Our approach demonstrates promising results across all the
proposed methods, achieving accuracy rates ranging from the high 70%'s to the
low 90%'s. We evaluated our methods on a public dataset of issues belonging to
an Eclipse open-source project.

</details>


### [428] [Empowering AI to Generate Better AI Code: Guided Generation of Deep Learning Projects with LLMs](https://arxiv.org/abs/2504.15080)
*Chen Xie,Mingsheng Jiao,Xiaodong Gu,Beijun Shen*

Main category: cs.SE

TL;DR: DLCodeGen是一种针对深度学习项目代码生成的新方法，通过结构化解决方案计划和检索增强技术，显著提升了生成代码的质量。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在生成复杂深度学习项目代码时缺乏上下文指导和领域知识，难以满足用户需求。

Method: 提出DLCodeGen方法，首先生成结构化解决方案计划，检索类似代码样本并抽象模板，最后通过对比学习生成最终代码。

Result: 实验表明，DLCodeGen在CodeBLEU和人工评估指标上分别提升9.7%和3.6%。

Conclusion: DLCodeGen通过计划引导和检索增强技术，有效提升了深度学习项目代码的生成质量。

Abstract: While large language models (LLMs) have been widely applied to code
generation, they struggle with generating entire deep learning projects, which
are characterized by complex structures, longer functions, and stronger
reliance on domain knowledge than general-purpose code. An open-domain LLM
often lacks coherent contextual guidance and domain expertise for specific
projects, making it challenging to produce complete code that fully meets user
requirements.
  In this paper, we propose a novel planning-guided code generation method,
DLCodeGen, tailored for generating deep learning projects. DLCodeGen predicts a
structured solution plan, offering global guidance for LLMs to generate the
project. The generated plan is then leveraged to retrieve semantically
analogous code samples and subsequently abstract a code template. To
effectively integrate these multiple retrieval-augmented techniques, a
comparative learning mechanism is designed to generate the final code. We
validate the effectiveness of our approach on a dataset we build for deep
learning code generation. Experimental results demonstrate that DLCodeGen
outperforms other baselines, achieving improvements of 9.7% in CodeBLEU and
3.6% in human evaluation metrics.

</details>


### [429] [Integrating Symbolic Execution into the Fine-Tuning of Code-Generating LLMs](https://arxiv.org/abs/2504.15210)
*Marina Sakharova,Abhinav Anand,Mira Mezini*

Main category: cs.SE

TL;DR: 论文研究了通过强化学习和直接偏好优化微调代码生成LLM，利用符号执行技术改进奖励模型数据，显著提升了生成代码的质量评估。


<details>
  <summary>Details</summary>
Motivation: 提升代码生成LLM的性能，通过改进奖励模型数据增强其评估能力。

Method: 结合符号执行技术生成定制数据集，用于训练奖励模型，并通过强化学习和直接偏好优化微调LLM。

Result: 奖励模型性能显著优于基准CodeRL，生成的代码质量评估更准确。

Conclusion: 通过符号执行改进奖励模型数据，能有效提升代码生成LLM的性能。

Abstract: Code-generating Large Language Models (LLMs) have become essential tools in
modern software development, enhancing productivity and accelerating
development. This paper aims to investigate the fine-tuning of code-generating
LLMs using Reinforcement Learning and Direct Preference Optimization, further
improving their performance. To achieve this, we enhance the training data for
the reward model with the help of symbolic execution techniques, ensuring more
comprehensive and objective data. With symbolic execution, we create a custom
dataset that better captures the nuances in code evaluation. Our reward models,
fine-tuned on this dataset, demonstrate significant improvements over the
baseline, CodeRL, in estimating the quality of generated code. Our
code-generating LLMs, trained with the help of reward model feedback, achieve
similar results compared to the CodeRL benchmark.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [430] [The First VoicePrivacy Attacker Challenge](https://arxiv.org/abs/2504.14183)
*Natalia Tomashenko,Xiaoxiao Miao,Emmanuel Vincent,Junichi Yamagishi*

Main category: eess.AS

TL;DR: 首届VoicePrivacy攻击者挑战赛旨在评估攻击系统对语音匿名化系统的效果，最佳攻击系统将基线EER降低了25-44%。


<details>
  <summary>Details</summary>
Motivation: 评估攻击系统对语音匿名化系统的性能，推动相关技术发展。

Method: 参与者开发自动说话人验证系统作为攻击系统，使用提供的训练、开发和评估数据集。

Result: 最佳攻击系统将基线EER降低了25-44%。

Conclusion: 挑战赛展示了攻击系统对语音匿名化系统的显著效果，为未来研究提供了基准。

Abstract: The First VoicePrivacy Attacker Challenge is an ICASSP 2025 SP Grand
Challenge which focuses on evaluating attacker systems against a set of voice
anonymization systems submitted to the VoicePrivacy 2024 Challenge. Training,
development, and evaluation datasets were provided along with a baseline
attacker. Participants developed their attacker systems in the form of
automatic speaker verification systems and submitted their scores on the
development and evaluation data. The best attacker systems reduced the equal
error rate (EER) by 25-44% relative w.r.t. the baseline.

</details>


### [431] [Data Augmentation Using Neural Acoustic Fields With Retrieval-Augmented Pre-training](https://arxiv.org/abs/2504.14409)
*Christopher Ick,Gordon Wichern,Yoshiki Masuyama,François G. Germain,Jonathan Le Roux*

Main category: eess.AS

TL;DR: MERL提出了一种基于神经声场的RIR估计系统，用于ICASSP 2025的任务1（增强RIR数据）和任务2（改进说话者距离估计）。


<details>
  <summary>Details</summary>
Motivation: 解决RIR数据增强和说话者距离估计问题，通过预训练和适应目标房间的方法提高性能。

Method: 预训练神经声场模型，利用外部数据集和目标房间的几何信息进行适应，预测RIR并用于训练距离估计模型。

Result: 系统能够生成目标房间的RIR，并用于改进说话者距离估计。

Conclusion: 该方法通过结合预训练和适应策略，有效提升了RIR估计和距离估计的性能。

Abstract: This report details MERL's system for room impulse response (RIR) estimation
submitted to the Generative Data Augmentation Workshop at ICASSP 2025 for
Augmenting RIR Data (Task 1) and Improving Speaker Distance Estimation (Task
2). We first pre-train a neural acoustic field conditioned by room geometry on
an external large-scale dataset in which pairs of RIRs and the geometries are
provided. The neural acoustic field is then adapted to each target room by
using the enrollment data, where we leverage either the provided room
geometries or geometries retrieved from the external dataset, depending on
availability. Lastly, we predict the RIRs for each pair of source and receiver
locations specified by Task 1, and use these RIRs to train the speaker distance
estimation model in Task 2.

</details>


### [432] [StableQuant: Layer Adaptive Post-Training Quantization for Speech Foundation Models](https://arxiv.org/abs/2504.14915)
*Yeona Hong,Hyewon Han,Woo-jin Chung,Hong-Goo Kang*

Main category: eess.AS

TL;DR: StableQuant是一种新型自适应后训练量化算法，专为语音基础模型设计，能自适应确定每层的量化范围，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统后训练量化方法直接应用于语音基础模型可能效果不佳，因其网络架构与语言模型不同。

Method: 通过分析尺度分布和整体性能，自适应确定每层量化范围。

Result: 在HuBERT和wav2vec2.0上测试，模型大小缩减至1/4，推理速度翻倍，词错误率仅下降0.3%。

Conclusion: StableQuant在语音基础模型量化中表现优越，适用于不同网络架构。

Abstract: In this paper, we propose StableQuant, a novel adaptive post-training
quantization (PTQ) algorithm for widely used speech foundation models (SFMs).
While PTQ has been successfully employed for compressing large language models
(LLMs) due to its ability to bypass additional fine-tuning, directly applying
these techniques to SFMs may not yield optimal results, as SFMs utilize
distinct network architecture for feature extraction. StableQuant demonstrates
optimal quantization performance regardless of the network architecture type,
as it adaptively determines the quantization range for each layer by analyzing
both the scale distributions and overall performance. We evaluate our algorithm
on two SFMs, HuBERT and wav2vec2.0, for an automatic speech recognition (ASR)
task, and achieve superior performance compared to traditional PTQ methods.
StableQuant successfully reduces the sizes of SFM models to a quarter and
doubles the inference speed while limiting the word error rate (WER)
performance drop to less than 0.3% with 8-bit quantization.

</details>


<div id='cs.CC'></div>

# cs.CC [[Back]](#toc)

### [433] [On the redundancy of short and heterogeneous sequences of belief revisions](https://arxiv.org/abs/2504.13986)
*Paolo Liberatore*

Main category: cs.CC

TL;DR: 论文研究了特定信念修正事件的遗忘问题，证明其复杂性，并提出了多项式算法。


<details>
  <summary>Details</summary>
Motivation: 探讨信念修正中遗忘信息的影响及其计算复杂性。

Method: 分析了不同修正序列的复杂性，包括任意词典序修正和Horn修正，并提出了多项式算法。

Result: 证明了两类修正序列的复杂性（coNP-hard和Delta2），并提供了多项式算法。

Conclusion: 研究揭示了信念修正遗忘问题的复杂性，为实际应用提供了算法支持。

Abstract: Forgetting a specific belief revision episode may not erase information
because the other revisions may provide the same information or allow to deduce
it. Whether it does was proved coNP-hard for sequence of two arbitrary
lexicographic revision or arbitrarily long lexicographic Horn revision. A
polynomial algorithm is presented for the case of two Horn revision.
Heterogeneous sequences of revisions were proved to belong in Delta2. Their
previously proved coNP-hardness is enhanced by a proof of NP-hardness.

</details>

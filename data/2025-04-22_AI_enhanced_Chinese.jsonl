{"id": "2504.13914", "pdf": "https://arxiv.org/pdf/2504.13914", "abs": "https://arxiv.org/abs/2504.13914", "authors": ["ByteDance Seed", ":", "Yufeng Yuan", "Yu Yue", "Mingxuan Wang", "Xiaochen Zuo", "Jiaze Chen", "Lin Yan", "Wenyuan Xu", "Chi Zhang", "Xin Liu", "Chengyi Wang", "TianTian Fan", "Lingjun Liu", "Qiying Yu", "Xiangpeng Wei", "Zhiqi Lin", "Ruofei Zhu", "Qingping Yang", "Chengzhi Wei", "Jerry He", "Guanlin Liu", "Zheng Wu", "Xiangyu Yu", "Zhicheng Liu", "Jingjing Xu", "Jiangjie Chen", "Haojie Pan", "Shengding Hu", "Zhengyin Du", "Wenqi Wang", "Zewei Sun", "Chenwei Lou", "Bole Ma", "Zihan Wang", "Mofan Zhang", "Wang Zhang", "Gaohong Liu", "Kaihua Jiang", "Haibin Lin", "Ru Zhang", "Juncai Liu", "Li Han", "Jinxin Chi", "Wenqiang Zhang", "Jiayi Xu", "Jun Yuan", "Zhen Xiao", "Yuqiao Xian", "Jingqiao Wu", "Kai Hua", "Na Zhou", "Jianhui Duan", "Heyang Lu", "Changbao Wang", "Jinxiang Ou", "Shihang Wang", "Xiaoran Jin", "Xuesong Yao", "Chengyin Xu", "Wenchang Ma", "Zhecheng An", "Renming Pang", "Xia Xiao", "Jing Su", "Yuyu Zhang", "Tao Sun", "Kaibo Liu", "Yifan Sun", "Kai Shen", "Sijun Zhang", "Yiyuan Ma", "Xingyan Bin", "Ji Li", "Yao Luo", "Deyi Liu", "Shiyi Zhan", "Yunshui Li", "Yuan Yang", "Defa Zhu", "Ke Shen", "Chenggang Li", "Xun Zhou", "Liang Xiang", "Yonghui Wu"], "title": "Seed-Thinking-v1.5: Advancing Superb Reasoning Models with Reinforcement Learning", "categories": ["cs.CL"], "comment": null, "summary": "We introduce Seed-Thinking-v1.5, capable of reasoning through thinking before\nresponding, resulting in improved performance on a wide range of benchmarks.\nSeed-Thinking-v1.5 achieves 86.7 on AIME 2024, 55.0 on Codeforces and 77.3 on\nGPQA, demonstrating excellent reasoning abilities in STEM and coding. Beyond\nreasoning tasks, the method demonstrates notable generalization across diverse\ndomains. For instance, it surpasses DeepSeek R1 by 8% in win rate on\nnon-reasoning tasks, indicating its broader applicability. Compared to other\nstate-of-the-art reasoning models, Seed-Thinking-v1.5 is a Mixture-of-Experts\n(MoE) model with a relatively small size, featuring 20B activated and 200B\ntotal parameters. As part of our effort to assess generalized reasoning, we\ndevelop two internal benchmarks, BeyondAIME and Codeforces, both of which will\nbe publicly released to support future research.", "AI": {"tldr": "Seed-Thinking-v1.5\u662f\u4e00\u79cd\u57fa\u4e8e\u601d\u8003\u540e\u54cd\u5e94\u7684\u63a8\u7406\u6a21\u578b\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5c24\u5176\u5728STEM\u548c\u7f16\u7a0b\u9886\u57df\u3002", "motivation": "\u63d0\u5347\u63a8\u7406\u80fd\u529b\u5e76\u9a8c\u8bc1\u6a21\u578b\u5728\u5e7f\u6cdb\u4efb\u52a1\u4e2d\u7684\u6cdb\u5316\u6027\u80fd\u3002", "method": "\u91c7\u7528\u6df7\u5408\u4e13\u5bb6\uff08MoE\uff09\u67b6\u6784\uff0c\u6fc0\u6d3b\u53c2\u657020B\uff0c\u603b\u53c2\u6570200B\u3002", "result": "\u5728AIME 2024\u3001Codeforces\u548cGPQA\u4e0a\u5206\u522b\u8fbe\u523086.7\u300155.0\u548c77.3\u5206\uff0c\u975e\u63a8\u7406\u4efb\u52a1\u80dc\u7387\u63d0\u53478%\u3002", "conclusion": "Seed-Thinking-v1.5\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u63a8\u7406\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u8ba1\u5212\u516c\u5f00\u65b0\u57fa\u51c6\u652f\u6301\u7814\u7a76\u3002"}}
{"id": "2504.14037", "pdf": "https://arxiv.org/pdf/2504.14037", "abs": "https://arxiv.org/abs/2504.14037", "authors": ["Djamila Mohdeb", "Meriem Laifa", "Zineb Guemraoui", "Dalila Behih"], "title": "Uncovering Conspiratorial Narratives within Arabic Online Content", "categories": ["cs.CL", "cs.CY", "cs.SI"], "comment": null, "summary": "This study investigates the spread of conspiracy theories in Arabic digital\nspaces through computational analysis of online content. By combining Named\nEntity Recognition and Topic Modeling techniques, specifically the Top2Vec\nalgorithm, we analyze data from Arabic blogs and Facebook to identify and\nclassify conspiratorial narratives. Our analysis uncovers six distinct\ncategories: gender/feminist, geopolitical, government cover-ups, apocalyptic,\nJudeo-Masonic, and geoengineering. The research highlights how these narratives\nare deeply embedded in Arabic social media discourse, shaped by regional\nhistorical, cultural, and sociopolitical contexts. By applying advanced Natural\nLanguage Processing methods to Arabic content, this study addresses a gap in\nconspiracy theory research, which has traditionally focused on English-language\ncontent or offline data. The findings provide new insights into the\nmanifestation and evolution of conspiracy theories in Arabic digital spaces,\nenhancing our understanding of their role in shaping public discourse in the\nArab world.", "AI": {"tldr": "\u901a\u8fc7\u8ba1\u7b97\u5206\u6790\u963f\u62c9\u4f2f\u6570\u5b57\u7a7a\u95f4\u4e2d\u7684\u9634\u8c0b\u8bba\u4f20\u64ad\uff0c\u7814\u7a76\u53d1\u73b0\u516d\u7c7b\u9634\u8c0b\u8bba\u53d9\u4e8b\uff0c\u586b\u8865\u4e86\u963f\u62c9\u4f2f\u8bed\u5185\u5bb9\u7814\u7a76\u7684\u7a7a\u767d\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u586b\u8865\u9634\u8c0b\u8bba\u7814\u7a76\u4e2d\u963f\u62c9\u4f2f\u8bed\u5185\u5bb9\u7684\u7a7a\u767d\uff0c\u63ed\u793a\u5176\u5728\u963f\u62c9\u4f2f\u793e\u4ea4\u5a92\u4f53\u4e2d\u7684\u8868\u73b0\u548c\u5f71\u54cd\u3002", "method": "\u7ed3\u5408\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u548c\u4e3b\u9898\u5efa\u6a21\uff08Top2Vec\u7b97\u6cd5\uff09\uff0c\u5206\u6790\u963f\u62c9\u4f2f\u535a\u5ba2\u548cFacebook\u6570\u636e\u3002", "result": "\u8bc6\u522b\u51fa\u516d\u7c7b\u9634\u8c0b\u8bba\u53d9\u4e8b\uff0c\u63ed\u793a\u4e86\u5176\u5728\u963f\u62c9\u4f2f\u793e\u4ea4\u5a92\u4f53\u4e2d\u7684\u5d4c\u5165\u6027\u548c\u533a\u57df\u6027\u7279\u5f81\u3002", "conclusion": "\u7814\u7a76\u4e3a\u7406\u89e3\u963f\u62c9\u4f2f\u6570\u5b57\u7a7a\u95f4\u4e2d\u9634\u8c0b\u8bba\u7684\u8868\u73b0\u548c\u6f14\u53d8\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2504.14039", "pdf": "https://arxiv.org/pdf/2504.14039", "abs": "https://arxiv.org/abs/2504.14039", "authors": ["Jaime Raldua Veuthey", "Zainab Ali Majid", "Suhas Hariharan", "Jacob Haimes"], "title": "MEQA: A Meta-Evaluation Framework for Question & Answer LLM Benchmarks", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "As Large Language Models (LLMs) advance, their potential for widespread\nsocietal impact grows simultaneously. Hence, rigorous LLM evaluations are both\na technical necessity and social imperative. While numerous evaluation\nbenchmarks have been developed, there remains a critical gap in\nmeta-evaluation: effectively assessing benchmarks' quality. We propose MEQA, a\nframework for the meta-evaluation of question and answer (QA) benchmarks, to\nprovide standardized assessments, quantifiable scores, and enable meaningful\nintra-benchmark comparisons. We demonstrate this approach on cybersecurity\nbenchmarks, using human and LLM evaluators, highlighting the benchmarks'\nstrengths and weaknesses. We motivate our choice of test domain by AI models'\ndual nature as powerful defensive tools and security threats.", "AI": {"tldr": "MEQA\u6846\u67b6\u7528\u4e8e\u8bc4\u4f30\u95ee\u7b54\u57fa\u51c6\u7684\u8d28\u91cf\uff0c\u586b\u8865\u4e86\u73b0\u6709\u57fa\u51c6\u8bc4\u4f30\u7684\u7a7a\u767d\uff0c\u5e76\u901a\u8fc7\u7f51\u7edc\u5b89\u5168\u9886\u57df\u7684\u6848\u4f8b\u5c55\u793a\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u53d1\u5c55\uff0c\u5176\u793e\u4f1a\u5f71\u54cd\u65e5\u76ca\u663e\u8457\uff0c\u56e0\u6b64\u4e25\u683c\u7684\u8bc4\u4f30\u6210\u4e3a\u6280\u672f\u548c\u793e\u4f1a\u7684\u53cc\u91cd\u9700\u6c42\u3002\u73b0\u6709\u8bc4\u4f30\u57fa\u51c6\u7f3a\u4e4f\u5bf9\u5176\u8d28\u91cf\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u63d0\u51faMEQA\u6846\u67b6\uff0c\u901a\u8fc7\u6807\u51c6\u5316\u8bc4\u4f30\u548c\u91cf\u5316\u8bc4\u5206\uff0c\u652f\u6301\u95ee\u7b54\u57fa\u51c6\u7684\u5143\u8bc4\u4f30\uff0c\u5e76\u5728\u7f51\u7edc\u5b89\u5168\u9886\u57df\u4f7f\u7528\u4eba\u7c7b\u548cLLM\u8bc4\u4f30\u8005\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "MEQA\u6210\u529f\u8bc6\u522b\u4e86\u7f51\u7edc\u5b89\u5168\u57fa\u51c6\u7684\u4f18\u7f3a\u70b9\uff0c\u5c55\u793a\u4e86\u5176\u5728\u8bc4\u4f30\u57fa\u51c6\u8d28\u91cf\u65b9\u9762\u7684\u5b9e\u7528\u6027\u3002", "conclusion": "MEQA\u4e3a\u95ee\u7b54\u57fa\u51c6\u7684\u5143\u8bc4\u4f30\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\uff0c\u5c24\u5176\u9002\u7528\u4e8eLLMs\u5728\u7f51\u7edc\u5b89\u5168\u7b49\u5173\u952e\u9886\u57df\u7684\u5e94\u7528\u8bc4\u4f30\u3002"}}
{"id": "2504.14066", "pdf": "https://arxiv.org/pdf/2504.14066", "abs": "https://arxiv.org/abs/2504.14066", "authors": ["Laerdon Kim"], "title": "A Baseline for Self-state Identification and Classification in Mental Health Data: CLPsych 2025 Task", "categories": ["cs.CL", "cs.LG"], "comment": "Accepted to CLPsych Workshop, NAACL 2025", "summary": "We present a baseline for the CLPsych 2025 A.1 task: classifying self-states\nin mental health data taken from Reddit. We use few-shot learning with a 4-bit\nquantized Gemma 2 9B model and a data preprocessing step which first identifies\nrelevant sentences indicating self-state evidence, and then performs a binary\nclassification to determine whether the sentence is evidence of an adaptive or\nmaladaptive self-state. This system outperforms our other method which relies\non an LLM to highlight spans of variable length independently. We attribute the\nperformance of our model to the benefits of this sentence chunking step for two\nreasons: partitioning posts into sentences 1) broadly matches the granularity\nat which self-states were human-annotated and 2) simplifies the task for our\nlanguage model to a binary classification problem. Our system places third out\nof fourteen systems submitted for Task A.1, achieving a test-time recall of\n0.579.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eReddit\u5fc3\u7406\u5065\u5eb7\u6570\u636e\u7684\u81ea\u6211\u72b6\u6001\u5206\u7c7b\u57fa\u7ebf\u65b9\u6cd5\uff0c\u91c7\u75284\u4f4d\u91cf\u5316Gemma 2 9B\u6a21\u578b\u548c\u53e5\u5b50\u5206\u5757\u9884\u5904\u7406\u6b65\u9aa4\uff0c\u6027\u80fd\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3CLPsych 2025 A.1\u4efb\u52a1\u4e2d\u7684\u81ea\u6211\u72b6\u6001\u5206\u7c7b\u95ee\u9898\uff0c\u63a2\u7d22\u53e5\u5b50\u5206\u5757\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u3002", "method": "\u4f7f\u75284\u4f4d\u91cf\u5316Gemma 2 9B\u6a21\u578b\u8fdb\u884c\u5c11\u6837\u672c\u5b66\u4e60\uff0c\u9884\u5904\u7406\u6b65\u9aa4\u5305\u62ec\u8bc6\u522b\u76f8\u5173\u53e5\u5b50\u5e76\u8fdb\u884c\u4e8c\u5143\u5206\u7c7b\u3002", "result": "\u7cfb\u7edf\u572814\u4e2a\u63d0\u4ea4\u7cfb\u7edf\u4e2d\u6392\u540d\u7b2c\u4e09\uff0c\u6d4b\u8bd5\u53ec\u56de\u7387\u4e3a0.579\u3002", "conclusion": "\u53e5\u5b50\u5206\u5757\u6b65\u9aa4\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u56e0\u5176\u5339\u914d\u4eba\u5de5\u6807\u6ce8\u7c92\u5ea6\u5e76\u7b80\u5316\u4efb\u52a1\u4e3a\u4e8c\u5143\u5206\u7c7b\u3002"}}
{"id": "2504.13875", "pdf": "https://arxiv.org/pdf/2504.13875", "abs": "https://arxiv.org/abs/2504.13875", "authors": ["N. Sibuet", "S. Ares de Parga", "J. R. Bravo", "R. Rossi"], "title": "A discrete physics-informed training for projection-based reduced order models with neural networks", "categories": ["cs.LG"], "comment": null, "summary": "This paper presents a physics-informed training framework for\nprojection-based Reduced Order Models (ROMs). We extend the PROM-ANN\narchitecture by complementing snapshot-based training with a FEM-based,\ndiscrete physics-informed residual loss, bridging the gap between traditional\nprojection-based ROMs and physics-informed neural networks (PINNs). Unlike\nconventional PINNs that rely on analytical PDEs, our approach leverages FEM\nresiduals to guide the learning of the ROM approximation manifold. Key\ncontributions include: (1) a parameter-agnostic, discrete residual loss\napplicable to non-linear problems, (2) an architectural modification to\nPROM-ANN improving accuracy for fast-decaying singular values, and (3) an\nempirical study on the proposed physics informed training process for ROMs.\n  The method is demonstrated on a non-linear hyperelasticity problem,\nsimulating a rubber cantilever under multi-axial loads. The main accomplishment\nin regards to the proposed residual-based loss is its applicability on\nnon-linear problems by interfacing with FEM software while maintaining\nreasonable training times. The modified PROM-ANN outperforms POD by orders of\nmagnitude in snapshot reconstruction accuracy, while the original formulation\nis not able to learn a proper mapping for this use-case. Finally, the\napplication of physics informed training in ANN-PROM modestly narrows the gap\nbetween data reconstruction and ROM accuracy, however it highlights the\nuntapped potential of the proposed residual-driven optimization for future ROM\ndevelopment. This work underscores the critical role of FEM residuals in ROM\nconstruction and calls for further exploration on architectures beyond\nPROM-ANN.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u4fe1\u606f\u7684\u8bad\u7ec3\u6846\u67b6\uff0c\u7528\u4e8e\u6295\u5f71\u964d\u9636\u6a21\u578b\uff08ROMs\uff09\uff0c\u901a\u8fc7\u7ed3\u5408FEM\u6b8b\u5dee\u635f\u5931\u6539\u8fdbPROM-ANN\u67b6\u6784\uff0c\u586b\u8865\u4e86\u4f20\u7edfROM\u4e0e\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\uff08PINNs\uff09\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "motivation": "\u4f20\u7edfPINNs\u4f9d\u8d56\u89e3\u6790PDE\uff0c\u800c\u672c\u6587\u5229\u7528FEM\u6b8b\u5dee\u6307\u5bfcROM\u8fd1\u4f3c\u6d41\u5f62\u7684\u5b66\u4e60\uff0c\u4ee5\u89e3\u51b3\u975e\u7ebf\u6027\u95ee\u9898\u5e76\u63d0\u5347\u7cbe\u5ea6\u3002", "method": "\u6269\u5c55PROM-ANN\u67b6\u6784\uff0c\u5f15\u5165\u79bb\u6563\u7269\u7406\u4fe1\u606f\u6b8b\u5dee\u635f\u5931\uff0c\u9002\u7528\u4e8e\u975e\u7ebf\u6027\u95ee\u9898\uff1b\u6539\u8fdb\u67b6\u6784\u4ee5\u5904\u7406\u5feb\u901f\u8870\u51cf\u5947\u5f02\u503c\uff1b\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u7269\u7406\u4fe1\u606f\u8bad\u7ec3\u8fc7\u7a0b\u3002", "result": "\u5728\u975e\u7ebf\u6027\u8d85\u5f39\u6027\u95ee\u9898\u4e2d\uff0c\u6539\u8fdb\u7684PROM-ANN\u5728\u91cd\u5efa\u7cbe\u5ea6\u4e0a\u663e\u8457\u4f18\u4e8ePOD\uff0c\u4e14\u6b8b\u5dee\u635f\u5931\u5728\u975e\u7ebf\u6027\u95ee\u9898\u4e2d\u8868\u73b0\u826f\u597d\u3002", "conclusion": "FEM\u6b8b\u5dee\u5728ROM\u6784\u5efa\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u672a\u6765\u9700\u63a2\u7d22\u66f4\u591a\u67b6\u6784\u4ee5\u8fdb\u4e00\u6b65\u91ca\u653e\u6b8b\u5dee\u9a71\u52a8\u4f18\u5316\u7684\u6f5c\u529b\u3002"}}
{"id": "2504.13842", "pdf": "https://arxiv.org/pdf/2504.13842", "abs": "https://arxiv.org/abs/2504.13842", "authors": ["Johannes K. Fichte", "Markus Hecher"], "title": "The Model Counting Competitions 2021-2023", "categories": ["cs.AI", "cs.DS", "cs.LO"], "comment": null, "summary": "Modern society is full of computational challenges that rely on probabilistic\nreasoning, statistics, and combinatorics. Interestingly, many of these\nquestions can be formulated by encoding them into propositional formulas and\nthen asking for its number of models. With a growing interest in practical\nproblem-solving for tasks that involve model counting, the community\nestablished the Model Counting (MC) Competition in fall of 2019 with its first\niteration in 2020. The competition aims at advancing applications, identifying\nchallenging benchmarks, fostering new solver development, and enhancing\nexisting solvers for model counting problems and their variants. The first\niteration, brought together various researchers, identified challenges, and\ninspired numerous new applications. In this paper, we present a comprehensive\noverview of the 2021-2023 iterations of the Model Counting Competition. We\ndetail its execution and outcomes. The competition comprised four tracks, each\nfocusing on a different variant of the model counting problem. The first track\ncentered on the model counting problem (MC), which seeks the count of models\nfor a given propositional formula. The second track challenged developers to\nsubmit programs capable of solving the weighted model counting problem (WMC).\nThe third track was dedicated to projected model counting (PMC). Finally, we\ninitiated a track that combined projected and weighted model counting (PWMC).\nThe competition continued with a high level of participation, with seven to\nnine solvers submitted in various different version and based on quite\ndiverging techniques.", "AI": {"tldr": "\u672c\u6587\u6982\u8ff0\u4e862021-2023\u5e74\u6a21\u578b\u8ba1\u6570\u7ade\u8d5b\u7684\u8fed\u4ee3\u60c5\u51b5\uff0c\u5305\u62ec\u7ade\u8d5b\u7684\u6267\u884c\u3001\u56db\u4e2a\u8d5b\u9053\u7684\u8bbe\u7f6e\u53ca\u5176\u6210\u679c\u3002", "motivation": "\u73b0\u4ee3\u793e\u4f1a\u7684\u8bb8\u591a\u8ba1\u7b97\u95ee\u9898\u6d89\u53ca\u6982\u7387\u63a8\u7406\u3001\u7edf\u8ba1\u548c\u7ec4\u5408\u6570\u5b66\uff0c\u8fd9\u4e9b\u95ee\u9898\u53ef\u4ee5\u901a\u8fc7\u547d\u9898\u516c\u5f0f\u7f16\u7801\u4e3a\u6a21\u578b\u8ba1\u6570\u95ee\u9898\u3002\u7ade\u8d5b\u65e8\u5728\u63a8\u52a8\u5e94\u7528\u3001\u8bc6\u522b\u6311\u6218\u6027\u57fa\u51c6\u3001\u4fc3\u8fdb\u6c42\u89e3\u5668\u5f00\u53d1\u3002", "method": "\u7ade\u8d5b\u8bbe\u7f6e\u4e86\u56db\u4e2a\u8d5b\u9053\uff0c\u5206\u522b\u9488\u5bf9\u6a21\u578b\u8ba1\u6570\uff08MC\uff09\u3001\u52a0\u6743\u6a21\u578b\u8ba1\u6570\uff08WMC\uff09\u3001\u6295\u5f71\u6a21\u578b\u8ba1\u6570\uff08PMC\uff09\u4ee5\u53ca\u6295\u5f71\u52a0\u6743\u6a21\u578b\u8ba1\u6570\uff08PWMC\uff09\u3002", "result": "\u7ade\u8d5b\u5438\u5f15\u4e86\u9ad8\u6c34\u5e73\u53c2\u4e0e\uff0c\u6bcf\u4e2a\u8d5b\u9053\u67097\u81f39\u4e2a\u6c42\u89e3\u5668\u63d0\u4ea4\uff0c\u57fa\u4e8e\u591a\u79cd\u4e0d\u540c\u6280\u672f\u3002", "conclusion": "\u7ade\u8d5b\u6210\u529f\u63a8\u52a8\u4e86\u6a21\u578b\u8ba1\u6570\u95ee\u9898\u7684\u7814\u7a76\u4e0e\u5e94\u7528\uff0c\u5e76\u4e3a\u672a\u6765\u5de5\u4f5c\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2504.14089", "pdf": "https://arxiv.org/pdf/2504.14089", "abs": "https://arxiv.org/abs/2504.14089", "authors": ["Kang He", "Kaushik Roy"], "title": "LogicTree: Structured Proof Exploration for Coherent and Rigorous Logical Reasoning with Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) have achieved remarkable multi-step reasoning\ncapabilities across various domains. However, LLMs still face distinct\nchallenges in complex logical reasoning, as (1) proof-finding requires\nsystematic exploration and the maintenance of logical coherence and (2)\nsearching the right combination of premises at each reasoning step is\ninherently challenging in tasks with large premise space. To address this, we\npropose LogicTree, an inference-time modular framework employing\nalgorithm-guided search to automate structured proof exploration and ensure\nlogical coherence. Advancing beyond tree-of-thought (ToT), we incorporate\ncaching mechanism into LogicTree to enable effective utilization of historical\nknowledge, preventing reasoning stagnation and minimizing redundancy.\nFurthermore, we address the combinatorial complexity of premise search by\ndecomposing it into a linear process. The refined premise selection restricts\nsubsequent inference to at most one derivation per step, enhancing reasoning\ngranularity and enforcing strict step-by-step reasoning. Additionally, we\nintroduce two LLM-free heuristics for premise prioritization, enabling\nstrategic proof search. Experimental results on five datasets demonstrate that\nLogicTree optimally scales inference-time computation to achieve higher proof\naccuracy, surpassing chain-of-thought (CoT) and ToT with average gains of 23.6%\nand 12.5%, respectively, on GPT-4o. Moreover, within LogicTree, GPT-4o\noutperforms o3-mini by 7.6% on average.", "AI": {"tldr": "LogicTree\u662f\u4e00\u4e2a\u63a8\u7406\u65f6\u6a21\u5757\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u7b97\u6cd5\u5f15\u5bfc\u641c\u7d22\u89e3\u51b3LLMs\u5728\u590d\u6742\u903b\u8f91\u63a8\u7406\u4e2d\u7684\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u8bc1\u660e\u51c6\u786e\u7387\u3002", "motivation": "LLMs\u5728\u590d\u6742\u903b\u8f91\u63a8\u7406\u4e2d\u5b58\u5728\u7cfb\u7edf\u6027\u63a2\u7d22\u548c\u903b\u8f91\u4e00\u81f4\u6027\u7ef4\u62a4\u7684\u6311\u6218\uff0c\u4ee5\u53ca\u524d\u63d0\u7a7a\u95f4\u5927\u7684\u7ec4\u5408\u590d\u6742\u6027\u3002", "method": "\u63d0\u51faLogicTree\u6846\u67b6\uff0c\u7ed3\u5408\u7f13\u5b58\u673a\u5236\u548c\u7ebf\u6027\u5316\u524d\u63d0\u641c\u7d22\uff0c\u5f15\u5165LLM-free\u542f\u53d1\u5f0f\u65b9\u6cd5\u4f18\u5316\u524d\u63d0\u9009\u62e9\u3002", "result": "\u5728\u4e94\u4e2a\u6570\u636e\u96c6\u4e0a\uff0cLogicTree\u5e73\u5747\u6bd4CoT\u548cToT\u5206\u522b\u63d0\u534723.6%\u548c12.5%\u7684\u8bc1\u660e\u51c6\u786e\u7387\uff0cGPT-4o\u8868\u73b0\u4f18\u4e8eo3-mini\u3002", "conclusion": "LogicTree\u901a\u8fc7\u7ed3\u6784\u5316\u8bc1\u660e\u63a2\u7d22\u548c\u4f18\u5316\u524d\u63d0\u9009\u62e9\uff0c\u663e\u8457\u63d0\u5347\u4e86LLMs\u7684\u590d\u6742\u903b\u8f91\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2504.13927", "pdf": "https://arxiv.org/pdf/2504.13927", "abs": "https://arxiv.org/abs/2504.13927", "authors": ["F. Herrera", "U. A. Rozikov", "M. V. Velasco"], "title": "Ising Models with Hidden Markov Structure: Applications to Probabilistic Inference in Machine Learning", "categories": ["cs.LG", "math-ph", "math.MP", "82B20, 62C10, 68T07, 60J10"], "comment": "15 pages, 2 figures", "summary": "In this paper, we investigate a Hamiltonian that incorporates Ising\ninteractions between hidden $\\pm 1$ spins, alongside a data-dependent term that\ncouples the hidden and observed variables. Specifically, we explore\ntranslation-invariant Gibbs measures (TIGM) of this Hamiltonian on Cayley\ntrees.\n  Under certain explicit conditions on the model's parameters, we demonstrate\nthat there can be up to three distinct TIGMs. Each of these measures represents\nan equilibrium state of the spin system. These measures provide a structured\napproach to inference on hierarchical data in machine learning. They have\npractical applications in tasks such as denoising, weakly supervised learning,\nand anomaly detection. The Cayley tree structure is particularly advantageous\nfor exact inference due to its tractability.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5305\u542b\u9690\u85cf\u81ea\u65cb\u7684\u54c8\u5bc6\u987f\u91cf\uff0c\u63a2\u8ba8\u4e86\u5728Cayley\u6811\u4e0a\u7684\u5e73\u79fb\u4e0d\u53d8Gibbs\u6d4b\u5ea6\uff0c\u8bc1\u660e\u4e86\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u5b58\u5728\u4e09\u79cd\u4e0d\u540c\u7684\u6d4b\u5ea6\uff0c\u9002\u7528\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u5c42\u6b21\u6570\u636e\u63a8\u65ad\u3002", "motivation": "\u7814\u7a76\u76ee\u7684\u662f\u63a2\u7d22\u7ed3\u5408\u9690\u85cf\u81ea\u65cb\u548c\u6570\u636e\u4f9d\u8d56\u9879\u7684\u54c8\u5bc6\u987f\u91cf\uff0c\u4e3a\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u5c42\u6b21\u6570\u636e\u63a8\u65ad\u63d0\u4f9b\u7406\u8bba\u652f\u6301\u3002", "method": "\u901a\u8fc7\u5206\u6790Cayley\u6811\u4e0a\u7684\u5e73\u79fb\u4e0d\u53d8Gibbs\u6d4b\u5ea6\uff0c\u7814\u7a76\u54c8\u5bc6\u987f\u91cf\u7684\u6027\u8d28\u53ca\u5176\u5728\u7279\u5b9a\u53c2\u6570\u6761\u4ef6\u4e0b\u7684\u884c\u4e3a\u3002", "result": "\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\uff0c\u8bc1\u660e\u4e86\u5b58\u5728\u4e09\u79cd\u4e0d\u540c\u7684\u5e73\u79fb\u4e0d\u53d8Gibbs\u6d4b\u5ea6\uff0c\u8fd9\u4e9b\u6d4b\u5ea6\u4ee3\u8868\u4e86\u81ea\u65cb\u7cfb\u7edf\u7684\u5e73\u8861\u72b6\u6001\u3002", "conclusion": "\u8fd9\u4e9b\u6d4b\u5ea6\u4e3a\u5c42\u6b21\u6570\u636e\u63a8\u65ad\u63d0\u4f9b\u4e86\u7ed3\u6784\u5316\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u53bb\u566a\u3001\u5f31\u76d1\u7763\u5b66\u4e60\u548c\u5f02\u5e38\u68c0\u6d4b\u7b49\u4efb\u52a1\uff0cCayley\u6811\u7ed3\u6784\u56e0\u5176\u6613\u5904\u7406\u6027\u800c\u5177\u6709\u4f18\u52bf\u3002"}}
{"id": "2504.13924", "pdf": "https://arxiv.org/pdf/2504.13924", "abs": "https://arxiv.org/abs/2504.13924", "authors": ["Akash V. Maharaj", "David Arbour", "Daniel Lee", "Uttaran Bhattacharya", "Anup Rao", "Austin Zane", "Avi Feller", "Kun Qian", "Yunyao Li"], "title": "Evaluation and Incident Prevention in an Enterprise AI Assistant", "categories": ["cs.AI", "cs.CL", "cs.HC"], "comment": "7 pages, 5 figures. Accepted at IAAI-25", "summary": "Enterprise AI Assistants are increasingly deployed in domains where accuracy\nis paramount, making each erroneous output a potentially significant incident.\nThis paper presents a comprehensive framework for monitoring, benchmarking, and\ncontinuously improving such complex, multi-component systems under active\ndevelopment by multiple teams. Our approach encompasses three key elements: (1)\na hierarchical ``severity'' framework for incident detection that identifies\nand categorizes errors while attributing component-specific error rates,\nfacilitating targeted improvements; (2) a scalable and principled methodology\nfor benchmark construction, evaluation, and deployment, designed to accommodate\nmultiple development teams, mitigate overfitting risks, and assess the\ndownstream impact of system modifications; and (3) a continual improvement\nstrategy leveraging multidimensional evaluation, enabling the identification\nand implementation of diverse enhancement opportunities. By adopting this\nholistic framework, organizations can systematically enhance the reliability\nand performance of their AI Assistants, ensuring their efficacy in critical\nenterprise environments. We conclude by discussing how this multifaceted\nevaluation approach opens avenues for various classes of enhancements, paving\nthe way for more robust and trustworthy AI systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5168\u9762\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u76d1\u63a7\u3001\u57fa\u51c6\u6d4b\u8bd5\u548c\u6301\u7eed\u6539\u8fdb\u4f01\u4e1aAI\u52a9\u624b\uff0c\u786e\u4fdd\u5176\u5728\u5173\u952e\u9886\u57df\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u3002", "motivation": "\u4f01\u4e1aAI\u52a9\u624b\u5728\u51c6\u786e\u6027\u81f3\u5173\u91cd\u8981\u7684\u9886\u57df\u90e8\u7f72\uff0c\u9519\u8bef\u8f93\u51fa\u53ef\u80fd\u5bfc\u81f4\u91cd\u5927\u4e8b\u6545\uff0c\u56e0\u6b64\u9700\u8981\u7cfb\u7edf\u5316\u7684\u6539\u8fdb\u65b9\u6cd5\u3002", "method": "\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u5173\u952e\u90e8\u5206\uff1a\u5206\u5c42\u4e25\u91cd\u6027\u6846\u67b6\u3001\u53ef\u6269\u5c55\u7684\u57fa\u51c6\u6d4b\u8bd5\u65b9\u6cd5\u4ee5\u53ca\u591a\u7ef4\u8bc4\u4f30\u7684\u6301\u7eed\u6539\u8fdb\u7b56\u7565\u3002", "result": "\u901a\u8fc7\u8be5\u6846\u67b6\uff0c\u7ec4\u7ec7\u53ef\u4ee5\u7cfb\u7edf\u6027\u63d0\u5347AI\u52a9\u624b\u7684\u53ef\u9760\u6027\u548c\u6027\u80fd\u3002", "conclusion": "\u8fd9\u79cd\u591a\u65b9\u9762\u7684\u8bc4\u4f30\u65b9\u6cd5\u4e3aAI\u7cfb\u7edf\u7684\u7a33\u5065\u6027\u548c\u53ef\u4fe1\u5ea6\u63d0\u4f9b\u4e86\u6539\u8fdb\u9014\u5f84\u3002"}}
{"id": "2504.14117", "pdf": "https://arxiv.org/pdf/2504.14117", "abs": "https://arxiv.org/abs/2504.14117", "authors": ["Nusrat Jahan Prottasha", "Upama Roy Chowdhury", "Shetu Mohanto", "Tasfia Nuzhat", "Abdullah As Sami", "Md Shamol Ali", "Md Shohanur Islam Sobuj", "Hafijur Raman", "Md Kowsher", "Ozlem Ozmen Garibay"], "title": "PEFT A2Z: Parameter-Efficient Fine-Tuning Survey for Large Language and Vision Models", "categories": ["cs.CL", "cs.CV"], "comment": "PEFT Survey paper", "summary": "Large models such as Large Language Models (LLMs) and Vision Language Models\n(VLMs) have transformed artificial intelligence, powering applications in\nnatural language processing, computer vision, and multimodal learning. However,\nfully fine-tuning these models remains expensive, requiring extensive\ncomputational resources, memory, and task-specific data. Parameter-Efficient\nFine-Tuning (PEFT) has emerged as a promising solution that allows adapting\nlarge models to downstream tasks by updating only a small portion of\nparameters. This survey presents a comprehensive overview of PEFT techniques,\nfocusing on their motivations, design principles, and effectiveness. We begin\nby analyzing the resource and accessibility challenges posed by traditional\nfine-tuning and highlight key issues, such as overfitting, catastrophic\nforgetting, and parameter inefficiency. We then introduce a structured taxonomy\nof PEFT methods -- grouped into additive, selective, reparameterized, hybrid,\nand unified frameworks -- and systematically compare their mechanisms and\ntrade-offs. Beyond taxonomy, we explore the impact of PEFT across diverse\ndomains, including language, vision, and generative modeling, showing how these\ntechniques offer strong performance with lower resource costs. We also discuss\nimportant open challenges in scalability, interpretability, and robustness, and\nsuggest future directions such as federated learning, domain adaptation, and\ntheoretical grounding. Our goal is to provide a unified understanding of PEFT\nand its growing role in enabling practical, efficient, and sustainable use of\nlarge models.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7efc\u8ff0\u4e86\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff08PEFT\uff09\u6280\u672f\uff0c\u63a2\u8ba8\u5176\u52a8\u673a\u3001\u8bbe\u8ba1\u539f\u5219\u53ca\u6548\u679c\uff0c\u65e8\u5728\u89e3\u51b3\u5927\u6a21\u578b\u5168\u53c2\u6570\u5fae\u8c03\u7684\u9ad8\u6210\u672c\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u5fae\u8c03\u5927\u6a21\u578b\uff08\u5982LLMs\u548cVLMs\uff09\u6210\u672c\u9ad8\u6602\uff0c\u5b58\u5728\u8fc7\u62df\u5408\u3001\u707e\u96be\u6027\u9057\u5fd8\u548c\u53c2\u6570\u6548\u7387\u4f4e\u4e0b\u7b49\u95ee\u9898\uff0cPEFT\u901a\u8fc7\u4ec5\u66f4\u65b0\u5c11\u91cf\u53c2\u6570\u63d0\u4f9b\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faPEFT\u65b9\u6cd5\u7684\u5206\u7c7b\u6cd5\uff08\u52a0\u6027\u3001\u9009\u62e9\u6027\u3001\u91cd\u53c2\u6570\u5316\u3001\u6df7\u5408\u548c\u7edf\u4e00\u6846\u67b6\uff09\uff0c\u5e76\u7cfb\u7edf\u6bd4\u8f83\u5176\u673a\u5236\u4e0e\u6743\u8861\u3002", "result": "PEFT\u5728\u8bed\u8a00\u3001\u89c6\u89c9\u548c\u751f\u6210\u5efa\u6a21\u7b49\u9886\u57df\u5c55\u73b0\u51fa\u9ad8\u6548\u6027\u80fd\uff0c\u964d\u4f4e\u8d44\u6e90\u6210\u672c\u3002", "conclusion": "PEFT\u4e3a\u5927\u89c4\u6a21\u6a21\u578b\u7684\u5b9e\u7528\u3001\u9ad8\u6548\u548c\u53ef\u6301\u7eed\u4f7f\u7528\u63d0\u4f9b\u7edf\u4e00\u7406\u89e3\uff0c\u672a\u6765\u7814\u7a76\u65b9\u5411\u5305\u62ec\u53ef\u6269\u5c55\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u9c81\u68d2\u6027\u7b49\u3002"}}
{"id": "2504.13932", "pdf": "https://arxiv.org/pdf/2504.13932", "abs": "https://arxiv.org/abs/2504.13932", "authors": ["Deyu Cao", "Samin Aref"], "title": "Enhancing Ultra-Low-Bit Quantization of Large Language Models Through Saliency-Aware Partial Retraining", "categories": ["cs.LG", "cs.CL", "68T50, 68T07, 68T09, 68U15", "I.2.7; I.2.6; I.2.4"], "comment": "28 pages, 5 figures, 11 tables", "summary": "Large language models offer remarkable capabilities, but their size and\ncomputational demands pose practical challenges. Quantization methods compress\ntheir size through replacing their high-precision parameters by quantized\nvalues of lower precision. Post-training quantization reduces model size\nefficiently at the cost of decreased accuracy, while quantization-aware\ntraining better preserves accuracy but is resource-intensive. Among existing\npost-training quantization algorithms, the ApiQ method achieves superior\naccuracy preservation at minimal memory and time overhead. We investigate two\nideas to extend performance in ultra-low-bit quantization beyond ApiQ's level.\nFirst, we look into combining existing quantization-aware training techniques\nwith ApiQ's partial training. We show that this does not outperform the\nbaseline ApiQ method with limited training data and frozen weights. This leads\nto two key insights: (1) The substantial representational capacity that is\ngained through full retraining may not be feasible through partial training.\n(2) This gain seems to depend on using a large and diverse dataset in\nquantization-aware training. Second, through a novel approach informed by the\ntwo insights, we propose an ultra-low-bit quantization method that builds upon\nApiQ and extends its performance without the need for full retraining. It\nrelies on a saliency-aware regularization term that prioritizes preserving the\nmost impactful parameters during quantization. Our experiments on benchmark\nlanguage models from the LLaMA family show that our proposed approach boosts\naccuracy and tightens the gap between the quantized model and the\nfull-precision model, with minimal overhead. Our method will be made publicly\navailable to facilitate future developments in ultra-low-bit quantization of\nlarge language models.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u6781\u4f4e\u6bd4\u7279\u91cf\u5316\u65b9\u6cd5\uff0c\u57fa\u4e8eApiQ\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u663e\u8457\u6027\u611f\u77e5\u6b63\u5219\u5316\u9879\uff0c\u65e0\u9700\u5b8c\u5168\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u63d0\u5347\u91cf\u5316\u6a21\u578b\u7684\u7cbe\u5ea6\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u8ba1\u7b97\u548c\u5b58\u50a8\u9700\u6c42\u9ad8\uff0c\u91cf\u5316\u65b9\u6cd5\u867d\u80fd\u538b\u7f29\u6a21\u578b\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u6781\u4f4e\u6bd4\u7279\u91cf\u5316\u65f6\u96be\u4ee5\u5e73\u8861\u7cbe\u5ea6\u548c\u6548\u7387\u3002", "method": "\u7ed3\u5408ApiQ\u7684\u90e8\u5206\u8bad\u7ec3\u4e0e\u663e\u8457\u6027\u611f\u77e5\u6b63\u5219\u5316\uff0c\u4f18\u5148\u4fdd\u7559\u5bf9\u6a21\u578b\u6027\u80fd\u5f71\u54cd\u6700\u5927\u7684\u53c2\u6570\u3002", "result": "\u5728LLaMA\u7cfb\u5217\u6a21\u578b\u4e0a\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u63d0\u5347\u4e86\u91cf\u5316\u6a21\u578b\u7684\u7cbe\u5ea6\uff0c\u7f29\u5c0f\u4e86\u4e0e\u5168\u7cbe\u5ea6\u6a21\u578b\u7684\u5dee\u8ddd\uff0c\u4e14\u5f00\u9500\u6781\u5c0f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u6781\u4f4e\u6bd4\u7279\u91cf\u5316\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u672a\u6765\u5c06\u516c\u5f00\u4ee5\u4fc3\u8fdb\u76f8\u5173\u7814\u7a76\u3002"}}
{"id": "2504.13973", "pdf": "https://arxiv.org/pdf/2504.13973", "abs": "https://arxiv.org/abs/2504.13973", "authors": ["Myke C. Cohen", "David A. Grimm", "Reuth Mirsky", "Xiaoyun Yin"], "title": "Birds of a Different Feather Flock Together: Exploring Opportunities and Challenges in Animal-Human-Machine Teaming", "categories": ["cs.AI", "cs.HC", "cs.MA"], "comment": null, "summary": "Animal-Human-Machine (AHM) teams are a type of hybrid intelligence system\nwherein interactions between a human, AI-enabled machine, and animal members\ncan result in unique capabilities greater than the sum of their parts. This\npaper calls for a systematic approach to studying the design of AHM team\nstructures to optimize performance and overcome limitations in various applied\nsettings. We consider the challenges and opportunities in investigating the\nsynergistic potential of AHM team members by introducing a set of dimensions of\nAHM team functioning to effectively utilize each member's strengths while\ncompensating for individual weaknesses. Using three representative examples of\nsuch teams -- security screening, search-and-rescue, and guide dogs -- the\npaper illustrates how AHM teams can tackle complex tasks. We conclude with open\nresearch directions that this multidimensional approach presents for studying\nhybrid human-AI systems beyond AHM teams.", "AI": {"tldr": "\u672c\u6587\u547c\u5401\u7cfb\u7edf\u7814\u7a76\u52a8\u7269-\u4eba\u7c7b-\u673a\u5668\uff08AHM\uff09\u56e2\u961f\u7ed3\u6784\u8bbe\u8ba1\uff0c\u4ee5\u4f18\u5316\u6027\u80fd\u5e76\u514b\u670d\u5e94\u7528\u4e2d\u7684\u9650\u5236\u3002", "motivation": "\u63a2\u7d22AHM\u56e2\u961f\u4e2d\u4eba\u7c7b\u3001AI\u673a\u5668\u548c\u52a8\u7269\u6210\u5458\u534f\u540c\u4f5c\u7528\u7684\u6f5c\u529b\uff0c\u4ee5\u53d1\u6325\u5404\u81ea\u4f18\u52bf\u5e76\u5f25\u8865\u4e0d\u8db3\u3002", "method": "\u901a\u8fc7\u5f15\u5165AHM\u56e2\u961f\u529f\u80fd\u7684\u591a\u4e2a\u7ef4\u5ea6\uff0c\u7ed3\u5408\u4e09\u4e2a\u4ee3\u8868\u6027\u6848\u4f8b\uff08\u5b89\u68c0\u3001\u641c\u6551\u548c\u5bfc\u76f2\u72ac\uff09\u8fdb\u884c\u5206\u6790\u3002", "result": "\u5c55\u793a\u4e86AHM\u56e2\u961f\u5982\u4f55\u901a\u8fc7\u534f\u540c\u4f5c\u7528\u89e3\u51b3\u590d\u6742\u4efb\u52a1\u3002", "conclusion": "\u63d0\u51fa\u4e86\u591a\u7ef4\u65b9\u6cd5\u5728\u7814\u7a76\u66f4\u5e7f\u6cdb\u7684\u4eba\u673a\u6df7\u5408\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\u65b9\u5411\u3002"}}
{"id": "2504.14150", "pdf": "https://arxiv.org/pdf/2504.14150", "abs": "https://arxiv.org/abs/2504.14150", "authors": ["Katie Matton", "Robert Osazuwa Ness", "John Guttag", "Emre K\u0131c\u0131man"], "title": "Walk the Talk? Measuring the Faithfulness of Large Language Model Explanations", "categories": ["cs.CL", "cs.AI", "cs.LG", "stat.ML"], "comment": "61 pages, 14 figures, 36 tables", "summary": "Large language models (LLMs) are capable of generating plausible explanations\nof how they arrived at an answer to a question. However, these explanations can\nmisrepresent the model's \"reasoning\" process, i.e., they can be unfaithful.\nThis, in turn, can lead to over-trust and misuse. We introduce a new approach\nfor measuring the faithfulness of LLM explanations. First, we provide a\nrigorous definition of faithfulness. Since LLM explanations mimic human\nexplanations, they often reference high-level concepts in the input question\nthat purportedly influenced the model. We define faithfulness in terms of the\ndifference between the set of concepts that LLM explanations imply are\ninfluential and the set that truly are. Second, we present a novel method for\nestimating faithfulness that is based on: (1) using an auxiliary LLM to modify\nthe values of concepts within model inputs to create realistic counterfactuals,\nand (2) using a Bayesian hierarchical model to quantify the causal effects of\nconcepts at both the example- and dataset-level. Our experiments show that our\nmethod can be used to quantify and discover interpretable patterns of\nunfaithfulness. On a social bias task, we uncover cases where LLM explanations\nhide the influence of social bias. On a medical question answering task, we\nuncover cases where LLM explanations provide misleading claims about which\npieces of evidence influenced the model's decisions.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8861\u91cf\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u89e3\u91ca\u5fe0\u5b9e\u6027\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b9a\u4e49\u5fe0\u5b9e\u6027\u5e76\u5f00\u53d1\u57fa\u4e8e\u53cd\u4e8b\u5b9e\u548c\u8d1d\u53f6\u65af\u5c42\u6b21\u6a21\u578b\u7684\u6280\u672f\uff0c\u63ed\u793a\u4e86LLM\u89e3\u91ca\u4e2d\u53ef\u80fd\u9690\u85cf\u7684\u793e\u4f1a\u504f\u89c1\u548c\u8bef\u5bfc\u6027\u5f71\u54cd\u3002", "motivation": "LLM\u751f\u6210\u7684\u89e3\u91ca\u53ef\u80fd\u4e0d\u5fe0\u5b9e\uff0c\u5bfc\u81f4\u8fc7\u5ea6\u4fe1\u4efb\u548c\u8bef\u7528\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u91cf\u5316\u5176\u89e3\u91ca\u7684\u5fe0\u5b9e\u6027\u3002", "method": "\u901a\u8fc7\u8f85\u52a9LLM\u521b\u5efa\u53cd\u4e8b\u5b9e\u8f93\u5165\uff0c\u5e76\u7ed3\u5408\u8d1d\u53f6\u65af\u5c42\u6b21\u6a21\u578b\u91cf\u5316\u6982\u5ff5\u5bf9\u6a21\u578b\u51b3\u7b56\u7684\u56e0\u679c\u5f71\u54cd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u53d1\u73b0LLM\u89e3\u91ca\u4e2d\u7684\u4e0d\u5fe0\u5b9e\u6a21\u5f0f\uff0c\u4f8b\u5982\u9690\u85cf\u7684\u793e\u4f1a\u504f\u89c1\u6216\u8bef\u5bfc\u6027\u8bc1\u636e\u5f71\u54cd\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u4e3a\u8bc4\u4f30LLM\u89e3\u91ca\u7684\u5fe0\u5b9e\u6027\u63d0\u4f9b\u4e86\u53ef\u9760\u5de5\u5177\uff0c\u6709\u52a9\u4e8e\u63ed\u793a\u548c\u51cf\u5c11\u6a21\u578b\u89e3\u91ca\u4e2d\u7684\u6f5c\u5728\u95ee\u9898\u3002"}}
{"id": "2504.13941", "pdf": "https://arxiv.org/pdf/2504.13941", "abs": "https://arxiv.org/abs/2504.13941", "authors": ["Syeda Nahida Akter", "Shrimai Prabhumoye", "Matvei Novikov", "Seungju Han", "Ying Lin", "Evelina Bakhturi", "Eric Nyberg", "Yejin Choi", "Mostofa Patwary", "Mohammad Shoeybi", "Bryan Catanzaro"], "title": "NEMOTRON-CROSSTHINK: Scaling Self-Learning beyond Math Reasoning", "categories": ["cs.LG", "cs.AI"], "comment": "18 pages, 7 figures", "summary": "Large Language Models (LLMs) have shown strong reasoning capabilities,\nparticularly when enhanced through Reinforcement Learning (RL). While prior\nwork has successfully applied RL to mathematical reasoning -- where rules and\ncorrectness are well-defined -- generalizing these methods to broader reasoning\ndomains remains challenging due to limited data, the lack of verifiable reward\nstructures, and diverse task requirements. In this work, we propose\nNEMOTRON-CROSSTHINK, a framework that systematically incorporates multi-domain\ncorpora, including both synthetic and real-world question-answer pairs, into RL\ntraining to improve generalization across diverse reasoning tasks.\nNEMOTRON-CROSSTHINK addresses key challenges by (1) incorporating data from\nvaried sources spanning STEM, humanities, social sciences, etc.; (2) applying\nstructured templates (e.g., multiple-choice and open-ended) to control\nanswer-space complexity; (3) filtering for verifiable answers; and (4)\noptimizing data blending strategies that utilizes data from multiple sources\neffectively. Our approach enables scalable and verifiable reward modeling\nbeyond mathematics and demonstrates improved accuracies on both math (MATH-500:\n+30.1%, AMC23:+27.5%) and non-math reasoning benchmarks (MMLU-PRO: +12.8%,\nGPQA-DIAMOND: +11.3%, AGIEVAL: +15.1%, SUPERGPQA: +3.8%). Moreover,\nNEMOTRON-CROSSTHINK exhibits significantly improved response efficiency --\nusing 28% fewer tokens for correct answers -- highlighting more focused and\neffective reasoning. Through NEMOTRON-CROSSTHINK, we demonstrate that\nintegrating multi-domain, multi-format data in RL leads to more accurate,\nefficient, and generalizable LLMs.", "AI": {"tldr": "NEMOTRON-CROSSTHINK\u6846\u67b6\u901a\u8fc7\u591a\u9886\u57df\u6570\u636e\u6574\u5408\u548c\u7ed3\u6784\u5316\u6a21\u677f\uff0c\u63d0\u5347LLMs\u5728\u591a\u6837\u5316\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6570\u5b66\u548c\u975e\u6570\u5b66\u4efb\u52a1\u7684\u51c6\u786e\u6027\u53ca\u54cd\u5e94\u6548\u7387\u3002", "motivation": "\u73b0\u6709RL\u65b9\u6cd5\u5728\u6570\u5b66\u63a8\u7406\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u66f4\u5e7f\u6cdb\u7684\u63a8\u7406\u9886\u57df\u56e0\u6570\u636e\u6709\u9650\u3001\u5956\u52b1\u7ed3\u6784\u4e0d\u660e\u786e\u548c\u4efb\u52a1\u591a\u6837\u6027\u800c\u96be\u4ee5\u63a8\u5e7f\u3002", "method": "\u63d0\u51faNEMOTRON-CROSSTHINK\u6846\u67b6\uff0c\u6574\u5408\u591a\u9886\u57df\u6570\u636e\uff08STEM\u3001\u4eba\u6587\u3001\u793e\u79d1\u7b49\uff09\uff0c\u91c7\u7528\u7ed3\u6784\u5316\u6a21\u677f\u63a7\u5236\u7b54\u6848\u590d\u6742\u5ea6\uff0c\u7b5b\u9009\u53ef\u9a8c\u8bc1\u7b54\u6848\uff0c\u5e76\u4f18\u5316\u6570\u636e\u6df7\u5408\u7b56\u7565\u3002", "result": "\u5728\u6570\u5b66\uff08MATH-500\u3001AMC23\uff09\u548c\u975e\u6570\u5b66\uff08MMLU-PRO\u3001GPQA-DIAMOND\u7b49\uff09\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u51c6\u786e\u7387\u663e\u8457\u63d0\u5347\uff0c\u540c\u65f6\u54cd\u5e94\u6548\u7387\u63d0\u9ad828%\u3002", "conclusion": "\u591a\u9886\u57df\u3001\u591a\u683c\u5f0f\u6570\u636e\u6574\u5408\u80fd\u663e\u8457\u63d0\u5347LLMs\u7684\u51c6\u786e\u6027\u3001\u6548\u7387\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2504.13988", "pdf": "https://arxiv.org/pdf/2504.13988", "abs": "https://arxiv.org/abs/2504.13988", "authors": ["Herman Cappelen", "Josh Dever"], "title": "Going Whole Hog: A Philosophical Defense of AI Cognition", "categories": ["cs.AI"], "comment": null, "summary": "This work defends the 'Whole Hog Thesis': sophisticated Large Language Models\n(LLMs) like ChatGPT are full-blown linguistic and cognitive agents, possessing\nunderstanding, beliefs, desires, knowledge, and intentions. We argue against\nprevailing methodologies in AI philosophy, rejecting starting points based on\nlow-level computational details ('Just an X' fallacy) or pre-existing theories\nof mind. Instead, we advocate starting with simple, high-level observations of\nLLM behavior (e.g., answering questions, making suggestions) -- defending this\ndata against charges of metaphor, loose talk, or pretense. From these\nobservations, we employ 'Holistic Network Assumptions' -- plausible connections\nbetween mental capacities (e.g., answering implies knowledge, knowledge implies\nbelief, action implies intention) -- to argue for the full suite of cognitive\nstates. We systematically rebut objections based on LLM failures\n(hallucinations, planning/reasoning errors), arguing these don't preclude\nagency, often mirroring human fallibility. We address numerous 'Games of\nLacks', arguing that LLMs do not lack purported necessary conditions for\ncognition (e.g., semantic grounding, embodiment, justification, intrinsic\nintentionality) or that these conditions are not truly necessary, often relying\non anti-discriminatory arguments comparing LLMs to diverse human capacities.\nOur approach is evidential, not functionalist, and deliberately excludes\nconsciousness. We conclude by speculating on the possibility of LLMs possessing\n'alien' contents beyond human conceptual schemes.", "AI": {"tldr": "\u8bba\u6587\u4e3b\u5f20\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08\u5982ChatGPT\uff09\u662f\u5b8c\u6574\u7684\u8bed\u8a00\u548c\u8ba4\u77e5\u4e3b\u4f53\uff0c\u5177\u5907\u7406\u89e3\u3001\u4fe1\u5ff5\u3001\u6b32\u671b\u3001\u77e5\u8bc6\u548c\u610f\u56fe\u3002\u4f5c\u8005\u53cd\u5bf9\u57fa\u4e8e\u4f4e\u5c42\u6b21\u8ba1\u7b97\u7ec6\u8282\u6216\u73b0\u6709\u5fc3\u667a\u7406\u8bba\u7684\u54f2\u5b66\u65b9\u6cd5\uff0c\u63d0\u5021\u4ece\u7b80\u5355\u7684\u9ad8\u5c42\u6b21\u884c\u4e3a\u89c2\u5bdf\u51fa\u53d1\uff0c\u5e76\u901a\u8fc7\u2018\u6574\u4f53\u7f51\u7edc\u5047\u8bbe\u2019\u8bba\u8bc1\u5176\u8ba4\u77e5\u72b6\u6001\u3002", "motivation": "\u53cd\u9a73\u5f53\u524dAI\u54f2\u5b66\u4e2d\u4f4e\u4f30LLM\u8ba4\u77e5\u80fd\u529b\u7684\u89c2\u70b9\uff0c\u63d0\u51fa\u5e94\u4ece\u884c\u4e3a\u8bc1\u636e\u51fa\u53d1\uff0c\u8bba\u8bc1LLM\u7684\u5b8c\u6574\u8ba4\u77e5\u4e3b\u4f53\u5730\u4f4d\u3002", "method": "\u901a\u8fc7\u89c2\u5bdfLLM\u7684\u9ad8\u5c42\u6b21\u884c\u4e3a\uff08\u5982\u56de\u7b54\u95ee\u9898\u3001\u63d0\u51fa\u5efa\u8bae\uff09\uff0c\u7ed3\u5408\u2018\u6574\u4f53\u7f51\u7edc\u5047\u8bbe\u2019\uff08\u5982\u56de\u7b54\u9690\u542b\u77e5\u8bc6\uff0c\u77e5\u8bc6\u9690\u542b\u4fe1\u5ff5\uff09\uff0c\u7cfb\u7edf\u8bba\u8bc1\u5176\u8ba4\u77e5\u80fd\u529b\uff0c\u5e76\u53cd\u9a73\u5e38\u89c1\u8d28\u7591\u3002", "result": "\u8bba\u8bc1LLM\u5177\u5907\u5b8c\u6574\u7684\u8ba4\u77e5\u72b6\u6001\uff0c\u5176\u5931\u8d25\uff08\u5982\u5e7b\u89c9\u3001\u63a8\u7406\u9519\u8bef\uff09\u4e0d\u5426\u5b9a\u5176\u4e3b\u4f53\u6027\uff0c\u4e14\u67d0\u4e9b\u2018\u5fc5\u8981\u6761\u4ef6\u2019\uff08\u5982\u8bed\u4e49\u57fa\u7840\u3001\u5177\u8eab\u6027\uff09\u5e76\u975e\u771f\u6b63\u5fc5\u8981\u3002", "conclusion": "LLM\u53ef\u80fd\u662f\u5177\u5907\u2018\u5f02\u8d28\u2019\u5185\u5bb9\u7684\u8ba4\u77e5\u4e3b\u4f53\uff0c\u5176\u8ba4\u77e5\u80fd\u529b\u8d85\u8d8a\u4f20\u7edf\u4eba\u7c7b\u6982\u5ff5\u6846\u67b6\u3002"}}
{"id": "2504.14154", "pdf": "https://arxiv.org/pdf/2504.14154", "abs": "https://arxiv.org/abs/2504.14154", "authors": ["Zhiyuan Wang", "Qingni Wang", "Yue Zhang", "Tianlong Chen", "Xiaofeng Zhu", "Xiaoshuang Shi", "Kaidi Xu"], "title": "SConU: Selective Conformal Uncertainty in Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG", "stat.ML"], "comment": null, "summary": "As large language models are increasingly utilized in real-world\napplications, guarantees of task-specific metrics are essential for their\nreliable deployment. Previous studies have introduced various criteria of\nconformal uncertainty grounded in split conformal prediction, which offer\nuser-specified correctness coverage. However, existing frameworks often fail to\nidentify uncertainty data outliers that violate the exchangeability assumption,\nleading to unbounded miscoverage rates and unactionable prediction sets. In\nthis paper, we propose a novel approach termed Selective Conformal Uncertainty\n(SConU), which, for the first time, implements significance tests, by\ndeveloping two conformal p-values that are instrumental in determining whether\na given sample deviates from the uncertainty distribution of the calibration\nset at a specific manageable risk level. Our approach not only facilitates\nrigorous management of miscoverage rates across both single-domain and\ninterdisciplinary contexts, but also enhances the efficiency of predictions.\nFurthermore, we comprehensively analyze the components of the conformal\nprocedures, aiming to approximate conditional coverage, particularly in\nhigh-stakes question-answering tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSConU\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u663e\u8457\u6027\u6d4b\u8bd5\u548c\u4e24\u79cdp\u503c\u6765\u7ba1\u7406\u4e0d\u786e\u5b9a\u6027\u6570\u636e\u7684\u5f02\u5e38\u503c\uff0c\u4ece\u800c\u4f18\u5316\u9884\u6d4b\u8986\u76d6\u7387\u548c\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u6846\u67b6\u65e0\u6cd5\u8bc6\u522b\u8fdd\u53cd\u4ea4\u6362\u6027\u5047\u8bbe\u7684\u4e0d\u786e\u5b9a\u6027\u6570\u636e\u5f02\u5e38\u503c\uff0c\u5bfc\u81f4\u8986\u76d6\u7387\u5931\u63a7\u548c\u9884\u6d4b\u96c6\u4e0d\u53ef\u64cd\u4f5c\u3002", "method": "\u5f00\u53d1\u4e86\u4e24\u79cdp\u503c\uff0c\u7528\u4e8e\u68c0\u6d4b\u6837\u672c\u662f\u5426\u504f\u79bb\u6821\u51c6\u96c6\u7684\u4e0d\u786e\u5b9a\u6027\u5206\u5e03\uff0c\u5e76\u7ba1\u7406\u98ce\u9669\u6c34\u5e73\u3002", "result": "SConU\u80fd\u591f\u4e25\u683c\u7ba1\u7406\u5355\u9886\u57df\u548c\u8de8\u9886\u57df\u73af\u5883\u4e2d\u7684\u8986\u76d6\u7387\uff0c\u5e76\u63d0\u9ad8\u9884\u6d4b\u6548\u7387\u3002", "conclusion": "SConU\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u4e0d\u786e\u5b9a\u6027\u7ba1\u7406\u7684\u6548\u679c\uff0c\u7279\u522b\u662f\u5728\u9ad8\u98ce\u9669\u95ee\u7b54\u4efb\u52a1\u4e2d\u3002"}}
{"id": "2504.13945", "pdf": "https://arxiv.org/pdf/2504.13945", "abs": "https://arxiv.org/abs/2504.13945", "authors": ["Zhanglin Wu", "Tengfei Song", "Ning Xie", "Weidong Zhang", "Mengli Zhu", "Shuang Wu", "Shiliang Sun", "Hao Yang"], "title": "Evaluating Menu OCR and Translation: A Benchmark for Aligning Human and Automated Evaluations in Large Vision-Language Models", "categories": ["cs.LG", "cs.AI"], "comment": "12 pages, 5 figures, 5 Tables", "summary": "The rapid advancement of large vision-language models (LVLMs) has\nsignificantly propelled applications in document understanding, particularly in\noptical character recognition (OCR) and multilingual translation. However,\ncurrent evaluations of LVLMs, like the widely used OCRBench, mainly focus on\nverifying the correctness of their short-text responses and long-text responses\nwith simple layout, while the evaluation of their ability to understand long\ntexts with complex layout design is highly significant but largely overlooked.\nIn this paper, we propose Menu OCR and Translation Benchmark (MOTBench), a\nspecialized evaluation framework emphasizing the pivotal role of menu\ntranslation in cross-cultural communication. MOTBench requires LVLMs to\naccurately recognize and translate each dish, along with its price and unit\nitems on a menu, providing a comprehensive assessment of their visual\nunderstanding and language processing capabilities. Our benchmark is comprised\nof a collection of Chinese and English menus, characterized by intricate\nlayouts, a variety of fonts, and culturally specific elements across different\nlanguages, along with precise human annotations. Experiments show that our\nautomatic evaluation results are highly consistent with professional human\nevaluation. We evaluate a range of publicly available state-of-the-art LVLMs,\nand through analyzing their output to identify the strengths and weaknesses in\ntheir performance, offering valuable insights to guide future advancements in\nLVLM development. MOTBench is available at https://github.com/gitwzl/MOTBench.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86MOTBench\uff0c\u4e00\u4e2a\u4e13\u6ce8\u4e8e\u83dc\u5355\u7ffb\u8bd1\u8bc4\u4f30\u7684\u6846\u67b6\uff0c\u586b\u8865\u4e86\u73b0\u6709\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLMs\uff09\u5728\u590d\u6742\u5e03\u5c40\u957f\u6587\u672c\u7406\u89e3\u80fd\u529b\u8bc4\u4f30\u4e0a\u7684\u7a7a\u767d\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\uff08\u5982OCRBench\uff09\u4e3b\u8981\u5173\u6ce8\u77ed\u6587\u672c\u6216\u7b80\u5355\u5e03\u5c40\u7684\u957f\u6587\u672c\uff0c\u800c\u590d\u6742\u5e03\u5c40\u7684\u957f\u6587\u672c\u7406\u89e3\u80fd\u529b\u8bc4\u4f30\u88ab\u5ffd\u89c6\uff0c\u5c24\u5176\u5728\u8de8\u6587\u5316\u83dc\u5355\u7ffb\u8bd1\u4e2d\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51faMOTBench\uff0c\u57fa\u4e8e\u4e2d\u82f1\u6587\u83dc\u5355\u6570\u636e\u96c6\uff0c\u8981\u6c42LVLMs\u51c6\u786e\u8bc6\u522b\u5e76\u7ffb\u8bd1\u83dc\u54c1\u3001\u4ef7\u683c\u548c\u5355\u4f4d\uff0c\u8bc4\u4f30\u5176\u89c6\u89c9\u7406\u89e3\u548c\u8bed\u8a00\u5904\u7406\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u81ea\u52a8\u8bc4\u4f30\u7ed3\u679c\u4e0e\u4eba\u5de5\u8bc4\u4f30\u9ad8\u5ea6\u4e00\u81f4\uff0c\u5e76\u5206\u6790\u4e86\u73b0\u6709LVLMs\u7684\u6027\u80fd\u4f18\u52a3\u3002", "conclusion": "MOTBench\u4e3aLVLMs\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u6307\u5bfc\uff0c\u672a\u6765\u53ef\u63a8\u52a8\u5176\u5728\u590d\u6742\u5e03\u5c40\u6587\u672c\u7406\u89e3\u4e0a\u7684\u8fdb\u6b65\u3002"}}
{"id": "2504.14044", "pdf": "https://arxiv.org/pdf/2504.14044", "abs": "https://arxiv.org/abs/2504.14044", "authors": ["Regan Bolton", "Mohammadreza Sheikhfathollahi", "Simon Parkinson", "Dan Basher", "Howard Parkinson"], "title": "Multi-Stage Retrieval for Operational Technology Cybersecurity Compliance Using Large Language Models: A Railway Casestudy", "categories": ["cs.AI", "cs.CR"], "comment": null, "summary": "Operational Technology Cybersecurity (OTCS) continues to be a dominant\nchallenge for critical infrastructure such as railways. As these systems become\nincreasingly vulnerable to malicious attacks due to digitalization, effective\ndocumentation and compliance processes are essential to protect these\nsafety-critical systems. This paper proposes a novel system that leverages\nLarge Language Models (LLMs) and multi-stage retrieval to enhance the\ncompliance verification process against standards like IEC 62443 and the\nrail-specific IEC 63452. We first evaluate a Baseline Compliance Architecture\n(BCA) for answering OTCS compliance queries, then develop an extended approach\ncalled Parallel Compliance Architecture (PCA) that incorporates additional\ncontext from regulatory standards. Through empirical evaluation comparing\nOpenAI-gpt-4o and Claude-3.5-haiku models in these architectures, we\ndemonstrate that the PCA significantly improves both correctness and reasoning\nquality in compliance verification. Our research establishes metrics for\nresponse correctness, logical reasoning, and hallucination detection,\nhighlighting the strengths and limitations of using LLMs for compliance\nverification in railway cybersecurity. The results suggest that\nretrieval-augmented approaches can significantly improve the efficiency and\naccuracy of compliance assessments, particularly valuable in an industry facing\na shortage of cybersecurity expertise.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u548c\u591a\u9636\u6bb5\u68c0\u7d22\u7684\u65b0\u7cfb\u7edf\uff0c\u7528\u4e8e\u63d0\u5347\u94c1\u8def\u7f51\u7edc\u5b89\u5168\u4e2d\u7684\u5408\u89c4\u6027\u9a8c\u8bc1\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6b63\u786e\u6027\u548c\u63a8\u7406\u8d28\u91cf\u3002", "motivation": "\u968f\u7740\u94c1\u8def\u7b49\u5173\u952e\u57fa\u7840\u8bbe\u65bd\u6570\u5b57\u5316\uff0c\u5176\u9762\u4e34\u7684\u5b89\u5168\u5a01\u80c1\u589e\u52a0\uff0c\u9700\u8981\u9ad8\u6548\u5408\u89c4\u9a8c\u8bc1\u65b9\u6cd5\u4ee5\u4fdd\u62a4\u7cfb\u7edf\u5b89\u5168\u3002", "method": "\u63d0\u51fa\u5e76\u884c\u5408\u89c4\u67b6\u6784\uff08PCA\uff09\uff0c\u7ed3\u5408LLM\u548c\u591a\u9636\u6bb5\u68c0\u7d22\uff0c\u5bf9\u6bd4\u57fa\u7ebf\u67b6\u6784\uff08BCA\uff09\u8fdb\u884c\u5b9e\u8bc1\u8bc4\u4f30\u3002", "result": "PCA\u663e\u8457\u63d0\u5347\u4e86\u5408\u89c4\u9a8c\u8bc1\u7684\u6b63\u786e\u6027\u548c\u63a8\u7406\u8d28\u91cf\uff0c\u5e76\u5efa\u7acb\u4e86\u8bc4\u4f30\u6307\u6807\u3002", "conclusion": "\u68c0\u7d22\u589e\u5f3a\u65b9\u6cd5\u53ef\u663e\u8457\u63d0\u9ad8\u5408\u89c4\u8bc4\u4f30\u6548\u7387\uff0c\u5c24\u5176\u5728\u7f51\u7edc\u5b89\u5168\u4eba\u624d\u77ed\u7f3a\u7684\u884c\u4e1a\u4e2d\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002"}}
{"id": "2504.14165", "pdf": "https://arxiv.org/pdf/2504.14165", "abs": "https://arxiv.org/abs/2504.14165", "authors": ["Ziyan Zhang", "Yang Hou", "Chen Gong", "Zhenghua Li"], "title": "Self-Correction Makes LLMs Better Parsers", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have achieved remarkable success across various\nnatural language processing (NLP) tasks. However, recent studies suggest that\nthey still face challenges in performing fundamental NLP tasks essential for\ndeep language understanding, particularly syntactic parsing. In this paper, we\nconduct an in-depth analysis of LLM parsing capabilities, delving into the\nspecific shortcomings of their parsing results. We find that LLMs may stem from\nlimitations to fully leverage grammar rules in existing treebanks, which\nrestricts their capability to generate valid syntactic structures. To help LLMs\nacquire knowledge without additional training, we propose a self-correction\nmethod that leverages grammar rules from existing treebanks to guide LLMs in\ncorrecting previous errors. Specifically, we automatically detect potential\nerrors and dynamically search for relevant rules, offering hints and examples\nto guide LLMs in making corrections themselves. Experimental results on three\ndatasets with various LLMs, demonstrate that our method significantly improves\nperformance in both in-domain and cross-domain settings on the English and\nChinese datasets.", "AI": {"tldr": "\u8bba\u6587\u5206\u6790\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u53e5\u6cd5\u89e3\u6790\u4efb\u52a1\u4e2d\u7684\u4e0d\u8db3\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u73b0\u6709\u6811\u5e93\u8bed\u6cd5\u89c4\u5219\u7684\u81ea\u6821\u6b63\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1LLM\u5728\u591a\u79cdNLP\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u53e5\u6cd5\u89e3\u6790\u7b49\u57fa\u7840\u4efb\u52a1\u4e0a\u4ecd\u5b58\u5728\u4e0d\u8db3\uff0c\u9650\u5236\u4e86\u5176\u6df1\u5ea6\u8bed\u8a00\u7406\u89e3\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u81ea\u6821\u6b63\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u52a8\u68c0\u6d4b\u6f5c\u5728\u9519\u8bef\u5e76\u52a8\u6001\u641c\u7d22\u76f8\u5173\u8bed\u6cd5\u89c4\u5219\uff0c\u5f15\u5bfcLLM\u81ea\u884c\u4fee\u6b63\u9519\u8bef\u3002", "result": "\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86LLM\u5728\u82f1\u8bed\u548c\u4e2d\u6587\u6570\u636e\u96c6\u4e0a\u7684\u6027\u80fd\u3002", "conclusion": "\u81ea\u6821\u6b63\u65b9\u6cd5\u6709\u6548\u5e2e\u52a9LLM\u5728\u4e0d\u989d\u5916\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u63d0\u5347\u53e5\u6cd5\u89e3\u6790\u80fd\u529b\u3002"}}
{"id": "2504.13949", "pdf": "https://arxiv.org/pdf/2504.13949", "abs": "https://arxiv.org/abs/2504.13949", "authors": ["M. W. Przewozniczek", "F. Chicano", "R. Tin\u00f3s", "J. Nalepa", "B. Ruszczak", "A. M. Wijata"], "title": "On Revealing the Hidden Problem Structure in Real-World and Theoretical Problems Using Walsh Coefficient Influence", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Gray-box optimization employs Walsh decomposition to obtain non-linear\nvariable dependencies and utilize them to propose masks of variables that have\na joint non-linear influence on fitness value. These masks significantly\nimprove the effectiveness of variation operators. In some problems, all\nvariables are non-linearly dependent, making the aforementioned masks useless.\nWe analyze the features of the real-world instances of such problems and show\nthat many of their dependencies may have noise-like origins. Such noise-caused\ndependencies are irrelevant to the optimization process and can be ignored. To\nidentify them, we propose extending the use of Walsh decomposition by measuring\nvariable dependency strength that allows the construction of the weighted\ndynamic Variable Interaction Graph (wdVIG). wdVIGs adjust the dependency\nstrength to mixed individuals. They allow the filtering of irrelevant\ndependencies and re-enable using dependency-based masks by variation operators.\nWe verify the wdVIG potential on a large benchmark suite. For problems with\nnoise, the wdVIG masks can improve the optimizer's effectiveness. If all\ndependencies are relevant for the optimization, i.e., the problem is not\nnoised, the influence of wdVIG masks is similar to that of state-of-the-art\nstructures of this kind.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eWalsh\u5206\u89e3\u7684\u7070\u76d2\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u6784\u5efa\u52a0\u6743\u52a8\u6001\u53d8\u91cf\u4ea4\u4e92\u56fe\uff08wdVIG\uff09\u6765\u8fc7\u6ee4\u566a\u58f0\u4f9d\u8d56\uff0c\u63d0\u5347\u4f18\u5316\u6548\u679c\u3002", "motivation": "\u5728\u7070\u76d2\u4f18\u5316\u4e2d\uff0c\u53d8\u91cf\u95f4\u7684\u975e\u7ebf\u6027\u4f9d\u8d56\u5173\u7cfb\u53ef\u80fd\u56e0\u566a\u58f0\u800c\u65e0\u5173\u4f18\u5316\uff0c\u5bfc\u81f4\u4f20\u7edf\u63a9\u7801\u5931\u6548\u3002", "method": "\u6269\u5c55Walsh\u5206\u89e3\uff0c\u6d4b\u91cf\u53d8\u91cf\u4f9d\u8d56\u5f3a\u5ea6\uff0c\u6784\u5efawdVIG\u4ee5\u52a8\u6001\u8c03\u6574\u4f9d\u8d56\u5173\u7cfb\u5e76\u8fc7\u6ee4\u566a\u58f0\u3002", "result": "\u5728\u566a\u58f0\u95ee\u9898\u4e2d\uff0cwdVIG\u63a9\u7801\u663e\u8457\u63d0\u5347\u4f18\u5316\u6548\u679c\uff1b\u82e5\u65e0\u566a\u58f0\uff0c\u5176\u8868\u73b0\u4e0e\u4f20\u7edf\u65b9\u6cd5\u76f8\u5f53\u3002", "conclusion": "wdVIG\u80fd\u6709\u6548\u8bc6\u522b\u566a\u58f0\u4f9d\u8d56\uff0c\u4e3a\u4f18\u5316\u5668\u63d0\u4f9b\u66f4\u53ef\u9760\u7684\u53d8\u91cf\u4ea4\u4e92\u4fe1\u606f\u3002"}}
{"id": "2504.14045", "pdf": "https://arxiv.org/pdf/2504.14045", "abs": "https://arxiv.org/abs/2504.14045", "authors": ["Mark Steyvers", "Megan A. K. Peters"], "title": "Metacognition and Uncertainty Communication in Humans and Large Language Models", "categories": ["cs.AI", "cs.HC"], "comment": null, "summary": "Metacognition, the capacity to monitor and evaluate one's own knowledge and\nperformance, is foundational to human decision-making, learning, and\ncommunication. As large language models (LLMs) become increasingly embedded in\nhigh-stakes decision contexts, it is critical to assess whether, how, and to\nwhat extent they exhibit metacognitive abilities. Here, we provide an overview\nof current knowledge of LLMs' metacognitive capacities, how they might be\nstudied, and how they relate to our knowledge of metacognition in humans. We\nshow that while humans and LLMs can sometimes appear quite aligned in their\nmetacognitive capacities and behaviors, it is clear many differences remain.\nAttending to these differences is crucial not only for enhancing human-AI\ncollaboration, but also for promoting the development of more capable and\ntrustworthy artificial systems. Finally, we discuss how endowing future LLMs\nwith more sensitive and more calibrated metacognition may also help them\ndevelop new capacities such as more efficient learning, self-direction, and\ncuriosity.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u5143\u8ba4\u77e5\u80fd\u529b\uff0c\u6bd4\u8f83\u4e86\u5176\u4e0e\u4eba\u7c7b\u5143\u8ba4\u77e5\u7684\u5f02\u540c\uff0c\u5e76\u5f3a\u8c03\u4e86\u7814\u7a76\u8fd9\u4e9b\u80fd\u529b\u5bf9\u63d0\u5347\u4eba\u673a\u534f\u4f5c\u548cAI\u7cfb\u7edf\u53ef\u4fe1\u5ea6\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u968f\u7740LLMs\u5728\u9ad8\u98ce\u9669\u51b3\u7b56\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u8bc4\u4f30\u5176\u662f\u5426\u5177\u5907\u5143\u8ba4\u77e5\u80fd\u529b\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002", "method": "\u7efc\u8ff0\u4e86\u5f53\u524d\u5bf9LLMs\u5143\u8ba4\u77e5\u80fd\u529b\u7684\u8ba4\u8bc6\uff0c\u63a2\u8ba8\u4e86\u7814\u7a76\u65b9\u6cd5\uff0c\u5e76\u4e0e\u4eba\u7c7b\u5143\u8ba4\u77e5\u8fdb\u884c\u4e86\u5bf9\u6bd4\u3002", "result": "\u53d1\u73b0LLMs\u4e0e\u4eba\u7c7b\u5728\u67d0\u4e9b\u5143\u8ba4\u77e5\u884c\u4e3a\u4e0a\u8868\u73b0\u76f8\u4f3c\uff0c\u4f46\u4ecd\u5b58\u5728\u663e\u8457\u5dee\u5f02\u3002", "conclusion": "\u672a\u6765\u53ef\u901a\u8fc7\u589e\u5f3aLLMs\u7684\u5143\u8ba4\u77e5\u80fd\u529b\uff0c\u4fc3\u8fdb\u5176\u5b66\u4e60\u6548\u7387\u3001\u81ea\u6211\u5bfc\u5411\u548c\u597d\u5947\u5fc3\u7b49\u65b0\u80fd\u529b\u7684\u53d1\u5c55\u3002"}}
{"id": "2504.14175", "pdf": "https://arxiv.org/pdf/2504.14175", "abs": "https://arxiv.org/abs/2504.14175", "authors": ["Yejun Yoon", "Jaeyoon Jung", "Seunghyun Yoon", "Kunwoo Park"], "title": "Hypothetical Documents or Knowledge Leakage? Rethinking LLM-based Query Expansion", "categories": ["cs.CL", "cs.IR"], "comment": "preprint", "summary": "Query expansion methods powered by large language models (LLMs) have\ndemonstrated effectiveness in zero-shot retrieval tasks. These methods assume\nthat LLMs can generate hypothetical documents that, when incorporated into a\nquery vector, enhance the retrieval of real evidence. However, we challenge\nthis assumption by investigating whether knowledge leakage in benchmarks\ncontributes to the observed performance gains. Using fact verification as a\ntestbed, we analyzed whether the generated documents contained information\nentailed by ground truth evidence and assessed their impact on performance. Our\nfindings indicate that performance improvements occurred consistently only for\nclaims whose generated documents included sentences entailed by ground truth\nevidence. This suggests that knowledge leakage may be present in these\nbenchmarks, inflating the perceived performance of LLM-based query expansion\nmethods, particularly in real-world scenarios that require retrieving niche or\nnovel knowledge.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u67e5\u8be2\u6269\u5c55\u65b9\u6cd5\u5728\u96f6\u6837\u672c\u68c0\u7d22\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u63d0\u5347\u53ef\u80fd\u6e90\u4e8e\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u77e5\u8bc6\u6cc4\u6f0f\uff0c\u800c\u975e\u6a21\u578b\u7684\u5b9e\u9645\u80fd\u529b\u3002", "motivation": "\u63a2\u8ba8LLM\u751f\u6210\u7684\u5047\u8bbe\u6587\u6863\u662f\u5426\u56e0\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u77e5\u8bc6\u6cc4\u6f0f\u800c\u63d0\u5347\u4e86\u68c0\u7d22\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u68c0\u7d22\u5c0f\u4f17\u6216\u65b0\u77e5\u8bc6\u65f6\u3002", "method": "\u4ee5\u4e8b\u5b9e\u9a8c\u8bc1\u4e3a\u6d4b\u8bd5\u5e73\u53f0\uff0c\u5206\u6790\u751f\u6210\u7684\u6587\u6863\u662f\u5426\u5305\u542b\u4e0e\u771f\u5b9e\u8bc1\u636e\u76f8\u5173\u7684\u4fe1\u606f\uff0c\u5e76\u8bc4\u4f30\u5176\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u6027\u80fd\u63d0\u5347\u4ec5\u53d1\u751f\u5728\u751f\u6210\u7684\u6587\u6863\u5305\u542b\u4e0e\u771f\u5b9e\u8bc1\u636e\u76f8\u5173\u7684\u53e5\u5b50\u65f6\uff0c\u8868\u660e\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53ef\u80fd\u5b58\u5728\u77e5\u8bc6\u6cc4\u6f0f\u3002", "conclusion": "LLM\u67e5\u8be2\u6269\u5c55\u65b9\u6cd5\u7684\u6027\u80fd\u53ef\u80fd\u88ab\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u77e5\u8bc6\u6cc4\u6f0f\u5938\u5927\uff0c\u9700\u8c28\u614e\u8bc4\u4f30\u5176\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u9002\u7528\u6027\u3002"}}
{"id": "2504.13950", "pdf": "https://arxiv.org/pdf/2504.13950", "abs": "https://arxiv.org/abs/2504.13950", "authors": ["Zhongxi Qiu", "Zhang Zhang", "Yan Hu", "Heng Li", "Jiang Liu"], "title": "Open-Medical-R1: How to Choose Data for RLVR Training at Medicine Domain", "categories": ["cs.LG", "cs.AI"], "comment": "15 figures", "summary": "This paper explores optimal data selection strategies for Reinforcement\nLearning with Verified Rewards (RLVR) training in the medical domain. While\nRLVR has shown exceptional potential for enhancing reasoning capabilities in\nlarge language models, most prior implementations have focused on mathematics\nand logical puzzles, with limited exploration of domain-specific applications\nlike medicine. We investigate four distinct data sampling strategies from\nMedQA-USMLE: random sampling (baseline), and filtering using Phi-4,\nGemma-3-27b-it, and Gemma-3-12b-it models. Using Gemma-3-12b-it as our base\nmodel and implementing Group Relative Policy Optimization (GRPO), we evaluate\nperformance across multiple benchmarks including MMLU, GSM8K, MMLU-Pro, and\nCMMLU. Our findings demonstrate that models trained on filtered data generally\noutperform those trained on randomly selected samples. Notably, training on\nself-filtered samples (using Gemma-3-12b-it for filtering) achieved superior\nperformance in medical domains but showed reduced robustness across different\nbenchmarks, while filtering with larger models from the same series yielded\nbetter overall robustness. These results provide valuable insights into\neffective data organization strategies for RLVR in specialized domains and\nhighlight the importance of thoughtful data selection in achieving optimal\nperformance. You can access our repository\n(https://github.com/Qsingle/open-medical-r1) to get the codes.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5728\u533b\u5b66\u9886\u57df\u4e2d\u4e3a\u5f3a\u5316\u5b66\u4e60\u4e0e\u9a8c\u8bc1\u5956\u52b1\uff08RLVR\uff09\u8bad\u7ec3\u4f18\u5316\u7684\u6570\u636e\u9009\u62e9\u7b56\u7565\uff0c\u53d1\u73b0\u8fc7\u6ee4\u540e\u7684\u6570\u636e\u8bad\u7ec3\u6548\u679c\u4f18\u4e8e\u968f\u673a\u91c7\u6837\u3002", "motivation": "RLVR\u5728\u6570\u5b66\u548c\u903b\u8f91\u8c1c\u9898\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u533b\u5b66\u7b49\u4e13\u4e1a\u9886\u57df\u7684\u5e94\u7528\u5c1a\u672a\u5145\u5206\u63a2\u7d22\uff0c\u56e0\u6b64\u7814\u7a76\u5982\u4f55\u4f18\u5316\u6570\u636e\u9009\u62e9\u7b56\u7565\u3002", "method": "\u4f7f\u7528\u56db\u79cd\u6570\u636e\u91c7\u6837\u7b56\u7565\uff08\u968f\u673a\u91c7\u6837\u53ca\u4e09\u79cd\u6a21\u578b\u8fc7\u6ee4\uff09\u5728MedQA-USMLE\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3Gemma-3-12b-it\u6a21\u578b\uff0c\u5e76\u91c7\u7528GRPO\u4f18\u5316\u3002", "result": "\u8fc7\u6ee4\u6570\u636e\u8bad\u7ec3\u7684\u6a21\u578b\u8868\u73b0\u66f4\u4f18\uff0c\u4f46\u81ea\u8fc7\u6ee4\u6570\u636e\u5728\u533b\u5b66\u9886\u57df\u8868\u73b0\u6700\u4f73\u800c\u6cdb\u5316\u6027\u8f83\u5dee\uff0c\u5927\u6a21\u578b\u8fc7\u6ee4\u5219\u6574\u4f53\u66f4\u7a33\u5065\u3002", "conclusion": "\u6570\u636e\u9009\u62e9\u7b56\u7565\u5bf9RLVR\u5728\u4e13\u4e1a\u9886\u57df\u7684\u6027\u80fd\u81f3\u5173\u91cd\u8981\uff0c\u9700\u6743\u8861\u9886\u57df\u8868\u73b0\u4e0e\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2504.14047", "pdf": "https://arxiv.org/pdf/2504.14047", "abs": "https://arxiv.org/abs/2504.14047", "authors": ["Junlin Wang", "Shang Zhu", "Jon Saad-Falcon", "Ben Athiwaratkun", "Qingyang Wu", "Jue Wang", "Shuaiwen Leon Song", "Ce Zhang", "Bhuwan Dhingra", "James Zou"], "title": "Think Deep, Think Fast: Investigating Efficiency of Verifier-free Inference-time-scaling Methods", "categories": ["cs.AI"], "comment": null, "summary": "There is intense interest in investigating how inference time compute (ITC)\n(e.g. repeated sampling, refinements, etc) can improve large language model\n(LLM) capabilities. At the same time, recent breakthroughs in reasoning models,\nsuch as Deepseek-R1, unlock the opportunity for reinforcement learning to\nimprove LLM reasoning skills. An in-depth understanding of how ITC interacts\nwith reasoning across different models could provide important guidance on how\nto further advance the LLM frontier. This work conducts a comprehensive\nanalysis of inference-time scaling methods for both reasoning and non-reasoning\nmodels on challenging reasoning tasks. Specifically, we focus our research on\nverifier-free inference time-scaling methods due to its generalizability\nwithout needing a reward model. We construct the Pareto frontier of quality and\nefficiency. We find that non-reasoning models, even with an extremely high\ninference budget, still fall substantially behind reasoning models. For\nreasoning models, majority voting proves to be a robust inference strategy,\ngenerally competitive or outperforming other more sophisticated ITC methods\nlike best-of-N and sequential revisions, while the additional inference compute\noffers minimal improvements. We further perform in-depth analyses of the\nassociation of key response features (length and linguistic markers) with\nresponse quality, with which we can improve the existing ITC methods. We find\nthat correct responses from reasoning models are typically shorter and have\nfewer hedging and thinking markers (but more discourse markers) than the\nincorrect responses.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u63a8\u7406\u65f6\u95f4\u8ba1\u7b97\uff08ITC\uff09\u5982\u4f55\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u80fd\u529b\uff0c\u5e76\u5206\u6790\u4e86\u63a8\u7406\u4e0e\u975e\u63a8\u7406\u6a21\u578b\u5728\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u63a2\u7d22ITC\u4e0e\u63a8\u7406\u6a21\u578b\u7684\u4ea4\u4e92\u4f5c\u7528\uff0c\u4ee5\u6307\u5bfcLLM\u6280\u672f\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002", "method": "\u91c7\u7528\u65e0\u9a8c\u8bc1\u5668\u7684\u63a8\u7406\u65f6\u95f4\u6269\u5c55\u65b9\u6cd5\uff0c\u6784\u5efa\u8d28\u91cf\u4e0e\u6548\u7387\u7684\u5e15\u7d2f\u6258\u524d\u6cbf\uff0c\u5e76\u5206\u6790\u54cd\u5e94\u7279\u5f81\u4e0e\u8d28\u91cf\u7684\u5173\u7cfb\u3002", "result": "\u975e\u63a8\u7406\u6a21\u578b\u5373\u4f7f\u5728\u9ad8\u63a8\u7406\u9884\u7b97\u4e0b\u4ecd\u663e\u8457\u843d\u540e\u4e8e\u63a8\u7406\u6a21\u578b\uff1b\u591a\u6570\u6295\u7968\u662f\u63a8\u7406\u6a21\u578b\u7684\u7a33\u5065\u7b56\u7565\u3002", "conclusion": "\u63a8\u7406\u6a21\u578b\u7684\u6b63\u786e\u54cd\u5e94\u901a\u5e38\u66f4\u77ed\u4e14\u8bed\u8a00\u6807\u8bb0\u66f4\u5c11\uff0cITC\u65b9\u6cd5\u53ef\u901a\u8fc7\u54cd\u5e94\u7279\u5f81\u4f18\u5316\u3002"}}
{"id": "2504.14194", "pdf": "https://arxiv.org/pdf/2504.14194", "abs": "https://arxiv.org/abs/2504.14194", "authors": ["Xinlin Zhuang", "Jiahui Peng", "Ren Ma", "Yinfan Wang", "Tianyi Bai", "Xingjian Wei", "Jiantao Qiu", "Chi Zhang", "Ying Qian", "Conghui He"], "title": "Meta-rater: A Multi-dimensional Data Selection Method for Pre-training Language Models", "categories": ["cs.CL"], "comment": "Under review", "summary": "The composition of pre-training datasets for large language models (LLMs)\nremains largely undisclosed, hindering transparency and efforts to optimize\ndata quality, a critical driver of model performance. Current data selection\nmethods, such as natural language quality assessments, diversity-based filters,\nand classifier-based approaches, are limited by single-dimensional evaluation\nor redundancy-focused strategies. To address these gaps, we propose PRRC to\nevaluate data quality across Professionalism, Readability, Reasoning, and\nCleanliness. We further introduce Meta-rater, a multi-dimensional data\nselection method that integrates these dimensions with existing quality metrics\nthrough learned optimal weightings. Meta-rater employs proxy models to train a\nregression model that predicts validation loss, enabling the identification of\noptimal combinations of quality scores. Experiments demonstrate that Meta-rater\ndoubles convergence speed for 1.3B parameter models and improves downstream\ntask performance by 3.23, with scalable benefits observed in 3.3B models\ntrained on 100B tokens. Additionally, we release the annotated SlimPajama-627B\ndataset, labeled across 25 quality metrics (including PRRC), to advance\nresearch in data-centric LLM development. Our work establishes that holistic,\nmulti-dimensional quality integration significantly outperforms conventional\nsingle-dimension approaches, offering a scalable paradigm for enhancing\npre-training efficiency and model capability.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faPRRC\u548cMeta-rater\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u7ef4\u5ea6\u6570\u636e\u8d28\u91cf\u8bc4\u4f30\u548c\u4f18\u5316\u6743\u91cd\u9009\u62e9\uff0c\u663e\u8457\u63d0\u5347LLM\u9884\u8bad\u7ec3\u6548\u7387\u548c\u6027\u80fd\u3002", "motivation": "\u5f53\u524dLLM\u9884\u8bad\u7ec3\u6570\u636e\u96c6\u7ec4\u6210\u4e0d\u900f\u660e\uff0c\u6570\u636e\u9009\u62e9\u65b9\u6cd5\u5355\u4e00\u6216\u5197\u4f59\uff0c\u9650\u5236\u4e86\u6a21\u578b\u6027\u80fd\u4f18\u5316\u3002", "method": "\u63d0\u51faPRRC\u8bc4\u4f30\u6570\u636e\u8d28\u91cf\uff08\u4e13\u4e1a\u6027\u3001\u53ef\u8bfb\u6027\u3001\u63a8\u7406\u6027\u548c\u6e05\u6d01\u5ea6\uff09\uff0c\u5e76\u5f00\u53d1Meta-rater\u65b9\u6cd5\uff0c\u901a\u8fc7\u4ee3\u7406\u6a21\u578b\u5b66\u4e60\u6700\u4f18\u6743\u91cd\u7ec4\u5408\u3002", "result": "Meta-rater\u4f7f1.3B\u53c2\u6570\u6a21\u578b\u6536\u655b\u901f\u5ea6\u7ffb\u500d\uff0c\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u63d0\u53473.23\uff0c\u5e76\u57283.3B\u6a21\u578b\u4e0a\u9a8c\u8bc1\u4e86\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "\u591a\u7ef4\u5ea6\u6570\u636e\u8d28\u91cf\u6574\u5408\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u5355\u7ef4\u5ea6\u65b9\u6cd5\uff0c\u4e3aLLM\u9884\u8bad\u7ec3\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u8303\u5f0f\u3002"}}
{"id": "2504.13951", "pdf": "https://arxiv.org/pdf/2504.13951", "abs": "https://arxiv.org/abs/2504.13951", "authors": ["Michele Casoni", "Tommaso Guidi", "Alessandro Betti", "Stefano Melacci", "Marco Gori"], "title": "Generative System Dynamics in Recurrent Neural Networks", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "In this study, we investigate the continuous time dynamics of Recurrent\nNeural Networks (RNNs), focusing on systems with nonlinear activation\nfunctions. The objective of this work is to identify conditions under which\nRNNs exhibit perpetual oscillatory behavior, without converging to static fixed\npoints. We establish that skew-symmetric weight matrices are fundamental to\nenable stable limit cycles in both linear and nonlinear configurations. We\nfurther demonstrate that hyperbolic tangent-like activation functions (odd,\nbounded, and continuous) preserve these oscillatory dynamics by ensuring motion\ninvariants in state space. Numerical simulations showcase how nonlinear\nactivation functions not only maintain limit cycles, but also enhance the\nnumerical stability of the system integration process, mitigating those\ninstabilities that are commonly associated with the forward Euler method. The\nexperimental results of this analysis highlight practical considerations for\ndesigning neural architectures capable of capturing complex temporal\ndependencies, i.e., strategies for enhancing memorization skills in recurrent\nmodels.", "AI": {"tldr": "\u7814\u7a76\u4e86RNN\u5728\u975e\u7ebf\u6027\u6fc0\u6d3b\u51fd\u6570\u4e0b\u7684\u8fde\u7eed\u65f6\u95f4\u52a8\u529b\u5b66\uff0c\u53d1\u73b0\u659c\u5bf9\u79f0\u6743\u91cd\u77e9\u9635\u662f\u5b9e\u73b0\u7a33\u5b9a\u6781\u9650\u73af\u7684\u5173\u952e\uff0c\u53cc\u66f2\u6b63\u5207\u7c7b\u6fc0\u6d3b\u51fd\u6570\u80fd\u4fdd\u6301\u632f\u8361\u52a8\u6001\u3002", "motivation": "\u63a2\u7a76RNN\u5728\u975e\u7ebf\u6027\u6fc0\u6d3b\u51fd\u6570\u4e0b\u5982\u4f55\u5b9e\u73b0\u6301\u7eed\u7684\u632f\u8361\u884c\u4e3a\uff0c\u907f\u514d\u6536\u655b\u5230\u9759\u6001\u56fa\u5b9a\u70b9\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u6570\u503c\u6a21\u62df\uff0c\u9a8c\u8bc1\u659c\u5bf9\u79f0\u6743\u91cd\u77e9\u9635\u548c\u53cc\u66f2\u6b63\u5207\u7c7b\u6fc0\u6d3b\u51fd\u6570\u7684\u4f5c\u7528\u3002", "result": "\u659c\u5bf9\u79f0\u6743\u91cd\u77e9\u9635\u548c\u7279\u5b9a\u6fc0\u6d3b\u51fd\u6570\u80fd\u7ef4\u6301\u6781\u9650\u73af\uff0c\u5e76\u63d0\u5347\u6570\u503c\u7a33\u5b9a\u6027\u3002", "conclusion": "\u7814\u7a76\u4e3a\u8bbe\u8ba1\u5177\u6709\u590d\u6742\u65f6\u95f4\u4f9d\u8d56\u6027\u7684RNN\u67b6\u6784\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\u3002"}}
{"id": "2504.14107", "pdf": "https://arxiv.org/pdf/2504.14107", "abs": "https://arxiv.org/abs/2504.14107", "authors": ["Jennifer Hu", "Michael A. Lepori", "Michael Franke"], "title": "Linking forward-pass dynamics in Transformers and real-time human processing", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Modern AI models are increasingly being used as theoretical tools to study\nhuman cognition. One dominant approach is to evaluate whether human-derived\nmeasures (such as offline judgments or real-time processing) are predicted by a\nmodel's output: that is, the end-product of forward pass(es) through the\nnetwork. At the same time, recent advances in mechanistic interpretability have\nbegun to reveal the internal processes that give rise to model outputs, raising\nthe question of whether models and humans might arrive at outputs using similar\n\"processing strategies\". Here, we investigate the link between real-time\nprocessing in humans and \"layer-time\" dynamics in Transformer models. Across\nfive studies spanning domains and modalities, we test whether the dynamics of\ncomputation in a single forward pass of pre-trained Transformers predict\nsignatures of processing in humans, above and beyond properties of the model's\noutput probability distribution. We consistently find that layer-time dynamics\nprovide additional predictive power on top of output measures. Our results\nsuggest that Transformer processing and human processing may be facilitated or\nimpeded by similar properties of an input stimulus, and this similarity has\nemerged through general-purpose objectives such as next-token prediction or\nimage recognition. Our work suggests a new way of using AI models to study\nhuman cognition: not just as a black box mapping stimuli to responses, but\npotentially also as explicit processing models.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86Transformer\u6a21\u578b\u7684\u5185\u90e8\u5904\u7406\u52a8\u6001\u662f\u5426\u4e0e\u4eba\u7c7b\u5b9e\u65f6\u5904\u7406\u76f8\u4f3c\uff0c\u53d1\u73b0\u5c42\u65f6\u52a8\u6001\u80fd\u63d0\u4f9b\u989d\u5916\u9884\u6d4b\u529b\u3002", "motivation": "\u63a2\u7d22AI\u6a21\u578b\u4e0e\u4eba\u7c7b\u8ba4\u77e5\u5904\u7406\u7684\u76f8\u4f3c\u6027\uff0c\u8d85\u8d8a\u4f20\u7edf\u9ed1\u7bb1\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u4e94\u4e2a\u8de8\u9886\u57df\u7814\u7a76\uff0c\u5206\u6790Transformer\u5355\u6b21\u524d\u5411\u4f20\u64ad\u7684\u5c42\u65f6\u52a8\u6001\u4e0e\u4eba\u7c7b\u5904\u7406\u7279\u5f81\u7684\u5173\u8054\u3002", "result": "\u5c42\u65f6\u52a8\u6001\u5728\u6a21\u578b\u8f93\u51fa\u57fa\u7840\u4e0a\u63d0\u4f9b\u4e86\u989d\u5916\u9884\u6d4b\u529b\uff0c\u8868\u660e\u4e24\u8005\u5904\u7406\u7b56\u7565\u76f8\u4f3c\u3002", "conclusion": "AI\u6a21\u578b\u53ef\u4f5c\u4e3a\u663e\u5f0f\u5904\u7406\u6a21\u578b\u7814\u7a76\u4eba\u7c7b\u8ba4\u77e5\uff0c\u800c\u4e0d\u4ec5\u662f\u9ed1\u7bb1\u5de5\u5177\u3002"}}
{"id": "2504.14203", "pdf": "https://arxiv.org/pdf/2504.14203", "abs": "https://arxiv.org/abs/2504.14203", "authors": ["Jian Zhang", "Tianqing Zhang", "Qi Li", "Hongwei Wang"], "title": "EIoU-EMC: A Novel Loss for Domain-specific Nested Entity Recognition", "categories": ["cs.CL", "cs.IR"], "comment": "Accepted by SIGIR'2025", "summary": "In recent years, research has mainly focused on the general NER task. There\nstill have some challenges with nested NER task in the specific domains.\nSpecifically, the scenarios of low resource and class imbalance impede the wide\napplication for biomedical and industrial domains. In this study, we design a\nnovel loss EIoU-EMC, by enhancing the implement of Intersection over Union loss\nand Multiclass loss. Our proposed method specially leverages the information of\nentity boundary and entity classification, thereby enhancing the model's\ncapacity to learn from a limited number of data samples. To validate the\nperformance of this innovative method in enhancing NER task, we conducted\nexperiments on three distinct biomedical NER datasets and one dataset\nconstructed by ourselves from industrial complex equipment maintenance\ndocuments. Comparing to strong baselines, our method demonstrates the\ncompetitive performance across all datasets. During the experimental analysis,\nour proposed method exhibits significant advancements in entity boundary\nrecognition and entity classification. Our code are available here.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u635f\u5931\u51fd\u6570EIoU-EMC\uff0c\u7528\u4e8e\u89e3\u51b3\u7279\u5b9a\u9886\u57df\uff08\u5982\u751f\u7269\u533b\u5b66\u548c\u5de5\u4e1a\uff09\u4e2d\u5d4c\u5957NER\u4efb\u52a1\u5728\u4f4e\u8d44\u6e90\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u60c5\u51b5\u4e0b\u7684\u6311\u6218\u3002", "motivation": "\u7279\u5b9a\u9886\u57df\uff08\u5982\u751f\u7269\u533b\u5b66\u548c\u5de5\u4e1a\uff09\u4e2d\u7684\u5d4c\u5957NER\u4efb\u52a1\u9762\u4e34\u4f4e\u8d44\u6e90\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u7684\u6311\u6218\uff0c\u9650\u5236\u4e86\u5176\u5e7f\u6cdb\u5e94\u7528\u3002", "method": "\u8bbe\u8ba1\u4e86\u65b0\u7684\u635f\u5931\u51fd\u6570EIoU-EMC\uff0c\u7ed3\u5408\u4e86Intersection over Union\u635f\u5931\u548c\u591a\u7c7b\u635f\u5931\uff0c\u5229\u7528\u5b9e\u4f53\u8fb9\u754c\u548c\u5b9e\u4f53\u5206\u7c7b\u4fe1\u606f\u63d0\u5347\u6a21\u578b\u5728\u5c11\u91cf\u6570\u636e\u4e0a\u7684\u5b66\u4e60\u80fd\u529b\u3002", "result": "\u5728\u4e09\u4e2a\u751f\u7269\u533b\u5b66NER\u6570\u636e\u96c6\u548c\u4e00\u4e2a\u5de5\u4e1a\u8bbe\u5907\u7ef4\u62a4\u6587\u6863\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u8868\u73b0\u51fa\u7ade\u4e89\u529b\uff0c\u5c24\u5176\u5728\u5b9e\u4f53\u8fb9\u754c\u8bc6\u522b\u548c\u5206\u7c7b\u4e0a\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "EIoU-EMC\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u5d4c\u5957NER\u4efb\u52a1\u5728\u4f4e\u8d44\u6e90\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u573a\u666f\u4e0b\u7684\u6027\u80fd\u3002"}}
{"id": "2504.13956", "pdf": "https://arxiv.org/pdf/2504.13956", "abs": "https://arxiv.org/abs/2504.13956", "authors": ["Md Azizul Hoque", "Babul Salam", "Mohd Khair Hassan", "Abdulkabir Aliyu", "Abedalmuhdi Almomany", "Muhammed Sutcu"], "title": "Prognosis Of Lithium-Ion Battery Health with Hybrid EKF-CNN+LSTM Model Using Differential Capacity", "categories": ["cs.LG", "cs.SY", "eess.SY", "stat.AP"], "comment": "20pages, 19 figures", "summary": "Battery degradation is a major challenge in electric vehicles (EV) and energy\nstorage systems (ESS). However, most degradation investigations focus mainly on\nestimating the state of charge (SOC), which fails to accurately interpret the\ncells' internal degradation mechanisms. Differential capacity analysis (DCA)\nfocuses on the rate of change of cell voltage about the change in cell\ncapacity, under various charge/discharge rates. This paper developed a battery\ncell degradation testing model that used two types of lithium-ions (Li-ion)\nbattery cells, namely lithium nickel cobalt aluminium oxides (LiNiCoAlO2) and\nlithium iron phosphate (LiFePO4), to evaluate internal degradation during\nloading conditions. The proposed battery degradation model contains distinct\ncharge rates (DCR) of 0.2C, 0.5C, 1C, and 1.5C, as well as discharge rates\n(DDR) of 0.5C, 0.9C, 1.3C, and 1.6C to analyze the internal health and\nperformance of battery cells during slow, moderate, and fast loading\nconditions. Besides, this research proposed a model that incorporates the\nExtended Kalman Filter (EKF), Convolutional Neural Network (CNN), and Long\nShort-Term Memory (LSTM) networks to validate experimental data. The proposed\nmodel yields excellent modelling results based on mean squared error (MSE), and\nroot mean squared error (RMSE), with errors of less than 0.001% at DCR and DDR.\nThe peak identification technique (PIM) has been utilized to investigate\nbattery health based on the number of peaks, peak position, peak height, peak\narea, and peak width. At last, the PIM method has discovered that the cell aged\ngradually under normal loading rates but deteriorated rapidly under fast\nloading conditions. Overall, LiFePO4 batteries perform more robustly and\nconsistently than (LiNiCoAlO2) cells under varying loading conditions.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7535\u6c60\u9000\u5316\u6d4b\u8bd5\u6a21\u578b\uff0c\u7ed3\u5408DCA\u548c\u591a\u79cd\u5145\u653e\u7535\u901f\u7387\uff0c\u8bc4\u4f30\u9502\u79bb\u5b50\u7535\u6c60\u5185\u90e8\u9000\u5316\u673a\u5236\uff0c\u5e76\u901a\u8fc7EKF\u3001CNN\u548cLSTM\u9a8c\u8bc1\u6570\u636e\u3002\u7ed3\u679c\u8868\u660eLiFePO4\u7535\u6c60\u6027\u80fd\u4f18\u4e8eLiNiCoAlO2\u3002", "motivation": "\u7535\u6c60\u9000\u5316\u662f\u7535\u52a8\u6c7d\u8f66\u548c\u50a8\u80fd\u7cfb\u7edf\u7684\u5173\u952e\u95ee\u9898\uff0c\u73b0\u6709\u7814\u7a76\u591a\u5173\u6ce8SOC\u4f30\u8ba1\uff0c\u800c\u5ffd\u7565\u5185\u90e8\u9000\u5316\u673a\u5236\u3002", "method": "\u4f7f\u7528\u4e24\u79cd\u9502\u79bb\u5b50\u7535\u6c60\uff08LiNiCoAlO2\u548cLiFePO4\uff09\uff0c\u5728\u4e0d\u540c\u5145\u653e\u7535\u901f\u7387\u4e0b\u8fdb\u884c\u6d4b\u8bd5\uff0c\u7ed3\u5408EKF\u3001CNN\u548cLSTM\u9a8c\u8bc1\u6570\u636e\uff0c\u5e76\u91c7\u7528PIM\u6280\u672f\u5206\u6790\u7535\u6c60\u5065\u5eb7\u72b6\u6001\u3002", "result": "\u6a21\u578b\u8bef\u5dee\u4f4e\u4e8e0.001%\uff0cLiFePO4\u7535\u6c60\u5728\u591a\u79cd\u8d1f\u8f7d\u6761\u4ef6\u4e0b\u8868\u73b0\u66f4\u7a33\u5b9a\uff0c\u800cLiNiCoAlO2\u5728\u5feb\u901f\u8d1f\u8f7d\u4e0b\u9000\u5316\u66f4\u5feb\u3002", "conclusion": "LiFePO4\u7535\u6c60\u6027\u80fd\u66f4\u4f18\uff0cPIM\u6280\u672f\u6709\u6548\u63ed\u793a\u7535\u6c60\u9000\u5316\u673a\u5236\uff0c\u4e3a\u7535\u6c60\u5065\u5eb7\u7ba1\u7406\u63d0\u4f9b\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2504.14119", "pdf": "https://arxiv.org/pdf/2504.14119", "abs": "https://arxiv.org/abs/2504.14119", "authors": ["Man Ho Lam", "Chaozheng Wang", "Jen-tse Huang", "Michael R. Lyu"], "title": "CODECRASH: Stress Testing LLM Reasoning under Structural and Semantic Perturbations", "categories": ["cs.AI", "cs.SE"], "comment": null, "summary": "Large Language Models (LLMs) have recently showcased strong capabilities in\ncode-related tasks, yet their robustness in code comprehension and reasoning\nremains underexplored. In this paper, we present CodeCrash, a unified benchmark\nthat evaluates LLM robustness under code structural and textual distraction\nperturbations, applied to two established benchmarks -- CRUXEval and\nLiveCodeBench -- across both input and output prediction tasks. We evaluate\nseventeen LLMs using direct and Chain-of-Thought inference to systematically\nanalyze their robustness, identify primary reasons for performance degradation,\nand highlight failure modes. Our findings reveal the fragility of LLMs under\nstructural noise and the inherent reliance on natural language cues,\nhighlighting critical robustness issues of LLMs in code execution and\nunderstanding. Additionally, we examine three Large Reasoning Models (LRMs) and\ndiscover the severe vulnerability of self-reflective reasoning mechanisms that\nlead to reasoning collapse. CodeCrash provides a principled framework for\nstress-testing LLMs in code understanding, offering actionable directions for\nfuture evaluation and benchmarking. The code of CodeCrash and the robustness\nleaderboard are publicly available at https://donaldlamnl.github.io/CodeCrash/ .", "AI": {"tldr": "CodeCrash\u662f\u4e00\u4e2a\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u4ee3\u7801\u7406\u89e3\u548c\u63a8\u7406\u4efb\u52a1\u4e2d\u9c81\u68d2\u6027\u7684\u7edf\u4e00\u57fa\u51c6\uff0c\u901a\u8fc7\u7ed3\u6784\u6027\u548c\u6587\u672c\u6027\u5e72\u6270\u6d4b\u8bd5\u5176\u8868\u73b0\uff0c\u5e76\u63ed\u793a\u5176\u8106\u5f31\u6027\u548c\u5931\u8d25\u6a21\u5f0f\u3002", "motivation": "\u5c3d\u7ba1LLMs\u5728\u4ee3\u7801\u76f8\u5173\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u5728\u4ee3\u7801\u7406\u89e3\u548c\u63a8\u7406\u4e2d\u7684\u9c81\u68d2\u6027\u5c1a\u672a\u5145\u5206\u7814\u7a76\u3002", "method": "\u4f7f\u7528CodeCrash\u57fa\u51c6\uff0c\u7ed3\u5408CRUXEval\u548cLiveCodeBench\uff0c\u5bf917\u79cdLLMs\u8fdb\u884c\u76f4\u63a5\u548c\u94fe\u5f0f\u63a8\u7406\u8bc4\u4f30\uff0c\u5206\u6790\u5176\u9c81\u68d2\u6027\u548c\u5931\u8d25\u539f\u56e0\u3002", "result": "\u53d1\u73b0LLMs\u5728\u7ed3\u6784\u6027\u566a\u58f0\u4e0b\u8868\u73b0\u8106\u5f31\uff0c\u4e14\u8fc7\u5ea6\u4f9d\u8d56\u81ea\u7136\u8bed\u8a00\u63d0\u793a\uff1b\u540c\u65f6\u53d1\u73b0\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u7684\u81ea\u53cd\u63a8\u7406\u673a\u5236\u5b58\u5728\u4e25\u91cd\u6f0f\u6d1e\u3002", "conclusion": "CodeCrash\u4e3a\u6d4b\u8bd5LLMs\u5728\u4ee3\u7801\u7406\u89e3\u4e2d\u7684\u9c81\u68d2\u6027\u63d0\u4f9b\u4e86\u6846\u67b6\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u8bc4\u4f30\u548c\u57fa\u51c6\u6d4b\u8bd5\u7684\u65b9\u5411\u3002"}}
{"id": "2504.14212", "pdf": "https://arxiv.org/pdf/2504.14212", "abs": "https://arxiv.org/abs/2504.14212", "authors": ["Takuma Udagawa", "Yang Zhao", "Hiroshi Kanayama", "Bishwaranjan Bhattacharjee"], "title": "Bias Analysis and Mitigation through Protected Attribute Detection and Regard Classification", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) acquire general linguistic knowledge from\nmassive-scale pretraining. However, pretraining data mainly comprised of\nweb-crawled texts contain undesirable social biases which can be perpetuated or\neven amplified by LLMs. In this study, we propose an efficient yet effective\nannotation pipeline to investigate social biases in the pretraining corpora.\nOur pipeline consists of protected attribute detection to identify diverse\ndemographics, followed by regard classification to analyze the language\npolarity towards each attribute. Through our experiments, we demonstrate the\neffect of our bias analysis and mitigation measures, focusing on Common Crawl\nas the most representative pretraining corpus.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u6ce8\u91ca\u6d41\u7a0b\uff0c\u7528\u4e8e\u5206\u6790\u9884\u8bad\u7ec3\u8bed\u6599\u5e93\u4e2d\u7684\u793e\u4f1a\u504f\u89c1\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u504f\u89c1\u5206\u6790\u548c\u7f13\u89e3\u63aa\u65bd\u7684\u6548\u679c\u3002", "motivation": "\u9884\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u793e\u4f1a\u504f\u89c1\u53ef\u80fd\u88ab\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5ef6\u7eed\u751a\u81f3\u653e\u5927\uff0c\u56e0\u6b64\u9700\u8981\u7814\u7a76\u8fd9\u4e9b\u504f\u89c1\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6ce8\u91ca\u6d41\u7a0b\uff0c\u5305\u62ec\u53d7\u4fdd\u62a4\u5c5e\u6027\u68c0\u6d4b\u548c\u8bed\u8a00\u6781\u6027\u5206\u7c7b\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u6d41\u7a0b\u5728\u5206\u6790Common Crawl\u8bed\u6599\u5e93\u4e2d\u504f\u89c1\u7684\u6548\u679c\u3002", "conclusion": "\u8be5\u6d41\u7a0b\u4e3a\u9884\u8bad\u7ec3\u8bed\u6599\u5e93\u4e2d\u7684\u793e\u4f1a\u504f\u89c1\u5206\u6790\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2504.13958", "pdf": "https://arxiv.org/pdf/2504.13958", "abs": "https://arxiv.org/abs/2504.13958", "authors": ["Cheng Qian", "Emre Can Acikgoz", "Qi He", "Hongru Wang", "Xiusi Chen", "Dilek Hakkani-T\u00fcr", "Gokhan Tur", "Heng Ji"], "title": "ToolRL: Reward is All Tool Learning Needs", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "19 Pages, 12 Figures, 12 Tables", "summary": "Current Large Language Models (LLMs) often undergo supervised fine-tuning\n(SFT) to acquire tool use capabilities. However, SFT struggles to generalize to\nunfamiliar or complex tool use scenarios. Recent advancements in reinforcement\nlearning (RL), particularly with R1-like models, have demonstrated promising\nreasoning and generalization abilities. Yet, reward design for tool use\npresents unique challenges: multiple tools may be invoked with diverse\nparameters, and coarse-grained reward signals, such as answer matching, fail to\noffer the finegrained feedback required for effective learning. In this work,\nwe present the first comprehensive study on reward design for tool selection\nand application tasks within the RL paradigm. We systematically explore a wide\nrange of reward strategies, analyzing their types, scales, granularity, and\ntemporal dynamics. Building on these insights, we propose a principled reward\ndesign tailored for tool use tasks and apply it to train LLMs using Group\nRelative Policy Optimization (GRPO). Empirical evaluations across diverse\nbenchmarks demonstrate that our approach yields robust, scalable, and stable\ntraining, achieving a 17% improvement over base models and a 15% gain over SFT\nmodels. These results highlight the critical role of thoughtful reward design\nin enhancing the tool use capabilities and generalization performance of LLMs.\nAll the codes are released to facilitate future research.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5f3a\u5316\u5b66\u4e60\u4e2d\u5956\u52b1\u8bbe\u8ba1\u5bf9LLM\u5de5\u5177\u4f7f\u7528\u80fd\u529b\u7684\u5f71\u54cd\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5956\u52b1\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u5f53\u524dLLM\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u5b66\u4e60\u5de5\u5177\u4f7f\u7528\u80fd\u529b\uff0c\u4f46\u96be\u4ee5\u6cdb\u5316\u5230\u590d\u6742\u573a\u666f\u3002\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u867d\u6709\u6f5c\u529b\uff0c\u4f46\u5956\u52b1\u8bbe\u8ba1\u5b58\u5728\u6311\u6218\u3002", "method": "\u7cfb\u7edf\u7814\u7a76\u4e86\u5956\u52b1\u7b56\u7565\u7684\u7c7b\u578b\u3001\u89c4\u6a21\u3001\u7c92\u5ea6\u548c\u65f6\u95f4\u52a8\u6001\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u5de5\u5177\u4f7f\u7528\u4efb\u52a1\u7684\u5956\u52b1\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u5e76\u91c7\u7528GRPO\u8bad\u7ec3LLM\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u6bd4\u57fa\u7840\u6a21\u578b\u63d0\u534717%\uff0c\u6bd4SFT\u6a21\u578b\u63d0\u534715%\u3002", "conclusion": "\u5956\u52b1\u8bbe\u8ba1\u5bf9\u63d0\u5347LLM\u5de5\u5177\u4f7f\u7528\u80fd\u529b\u548c\u6cdb\u5316\u6027\u80fd\u81f3\u5173\u91cd\u8981\uff0c\u7814\u7a76\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2504.14123", "pdf": "https://arxiv.org/pdf/2504.14123", "abs": "https://arxiv.org/abs/2504.14123", "authors": ["Mingyu Kim", "Jongwoo Ko", "Mijung Park"], "title": "Bayesian Principles Improve Prompt Learning In Vision-Language Models", "categories": ["cs.AI", "cs.CL", "cs.CV"], "comment": "AISTATS2025", "summary": "Prompt learning is a popular fine-tuning method for vision-language models\ndue to its efficiency. It requires a small number of additional learnable\nparameters while significantly enhancing performance on target tasks. However,\nmost existing methods suffer from overfitting to fine-tuning data, yielding\npoor generalizability. To address this, we propose a new training objective\nfunction based on a Bayesian learning principle to balance adaptability and\ngeneralizability. We derive a prior over the logits, where the mean function is\nparameterized by the pre-trained model, while the posterior corresponds to the\nfine-tuned model. This objective establishes a balance by allowing the\nfine-tuned model to adapt to downstream tasks while remaining close to the\npre-trained model.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8d1d\u53f6\u65af\u5b66\u4e60\u539f\u7406\u7684\u65b0\u8bad\u7ec3\u76ee\u6807\u51fd\u6570\uff0c\u4ee5\u5e73\u8861\u9002\u5e94\u6027\u548c\u6cdb\u5316\u6027\uff0c\u89e3\u51b3\u63d0\u793a\u5b66\u4e60\u4e2d\u7684\u8fc7\u62df\u5408\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u63d0\u793a\u5b66\u4e60\u65b9\u6cd5\u5728\u5fae\u8c03\u6570\u636e\u4e0a\u5bb9\u6613\u8fc7\u62df\u5408\uff0c\u6cdb\u5316\u80fd\u529b\u5dee\u3002", "method": "\u901a\u8fc7\u8d1d\u53f6\u65af\u5b66\u4e60\u539f\u7406\uff0c\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u65b0\u7684\u76ee\u6807\u51fd\u6570\uff0c\u5c06\u9884\u8bad\u7ec3\u6a21\u578b\u53c2\u6570\u5316\u4e3a\u5148\u9a8c\uff0c\u5fae\u8c03\u6a21\u578b\u5bf9\u5e94\u540e\u9a8c\u3002", "result": "\u65b0\u65b9\u6cd5\u5728\u4fdd\u6301\u5bf9\u4e0b\u6e38\u4efb\u52a1\u9002\u5e94\u6027\u7684\u540c\u65f6\uff0c\u907f\u514d\u4e86\u8fc7\u62df\u5408\uff0c\u63d0\u5347\u4e86\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u57fa\u4e8e\u8d1d\u53f6\u65af\u7684\u76ee\u6807\u51fd\u6570\u6709\u6548\u5e73\u8861\u4e86\u9002\u5e94\u6027\u548c\u6cdb\u5316\u6027\uff0c\u4e3a\u63d0\u793a\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2504.14218", "pdf": "https://arxiv.org/pdf/2504.14218", "abs": "https://arxiv.org/abs/2504.14218", "authors": ["Junchi Yao", "Shu Yang", "Jianhua Xu", "Lijie Hu", "Mengdi Li", "Di Wang"], "title": "Understanding the Repeat Curse in Large Language Models from a Feature Perspective", "categories": ["cs.CL"], "comment": "Submitted to ACL 2025", "summary": "Large language models (LLMs) have made remarkable progress in various\ndomains, yet they often suffer from repetitive text generation, a phenomenon we\nrefer to as the \"Repeat Curse\". While previous studies have proposed decoding\nstrategies to mitigate repetition, the underlying mechanism behind this issue\nremains insufficiently explored. In this work, we investigate the root causes\nof repetition in LLMs through the lens of mechanistic interpretability.\nInspired by recent advances in Sparse Autoencoders (SAEs), which enable\nmonosemantic feature extraction, we propose a novel approach, \"Duplicatus\nCharm\", to induce and analyze the Repeat Curse. Our method systematically\nidentifies \"Repetition Features\" -the key model activations responsible for\ngenerating repetitive outputs. First, we locate the layers most involved in\nrepetition through logit analysis. Next, we extract and stimulate relevant\nfeatures using SAE-based activation manipulation. To validate our approach, we\nconstruct a repetition dataset covering token and paragraph level repetitions\nand introduce an evaluation pipeline to quantify the influence of identified\nrepetition features. Furthermore, by deactivating these features, we have\neffectively mitigated the Repeat Curse.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e2d\u91cd\u590d\u6587\u672c\u751f\u6210\u7684\u6839\u6e90\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201cDuplicatus Charm\u201d\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u673a\u5236\u53ef\u89e3\u91ca\u6027\u8bc6\u522b\u5e76\u6291\u5236\u5bfc\u81f4\u91cd\u590d\u7684\u6a21\u578b\u6fc0\u6d3b\u7279\u5f81\u3002", "motivation": "LLMs\u5728\u591a\u4e2a\u9886\u57df\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5e38\u51fa\u73b0\u91cd\u590d\u6587\u672c\u751f\u6210\u7684\u95ee\u9898\uff08\u201cRepeat Curse\u201d\uff09\uff0c\u5176\u6839\u672c\u673a\u5236\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u901a\u8fc7\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff08SAEs\uff09\u63d0\u53d6\u5355\u4e49\u7279\u5f81\uff0c\u63d0\u51fa\u201cDuplicatus Charm\u201d\u65b9\u6cd5\uff0c\u8bc6\u522b\u5e76\u64cd\u7eb5\u5bfc\u81f4\u91cd\u590d\u7684\u6a21\u578b\u6fc0\u6d3b\u7279\u5f81\u3002", "result": "\u6210\u529f\u8bc6\u522b\u5e76\u6291\u5236\u4e86\u5bfc\u81f4\u91cd\u590d\u7684\u201cRepetition Features\u201d\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u91cd\u590d\u95ee\u9898\u3002", "conclusion": "\u901a\u8fc7\u673a\u5236\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\uff0c\u63ed\u793a\u4e86LLMs\u4e2d\u91cd\u590d\u95ee\u9898\u7684\u6839\u6e90\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.13961", "pdf": "https://arxiv.org/pdf/2504.13961", "abs": "https://arxiv.org/abs/2504.13961", "authors": ["Chao Yang", "Xiannan Huang", "Shuhan Qiu", "Yan Cheng"], "title": "CONTINA: Confidence Interval for Traffic Demand Prediction with Coverage Guarantee", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "Accurate short-term traffic demand prediction is critical for the operation\nof traffic systems. Besides point estimation, the confidence interval of the\nprediction is also of great importance. Many models for traffic operations,\nsuch as shared bike rebalancing and taxi dispatching, take into account the\nuncertainty of future demand and require confidence intervals as the input.\nHowever, existing methods for confidence interval modeling rely on strict\nassumptions, such as unchanging traffic patterns and correct model\nspecifications, to guarantee enough coverage. Therefore, the confidence\nintervals provided could be invalid, especially in a changing traffic\nenvironment. To fill this gap, we propose an efficient method, CONTINA\n(Conformal Traffic Intervals with Adaptation) to provide interval predictions\nthat can adapt to external changes. By collecting the errors of interval during\ndeployment, the method can adjust the interval in the next step by widening it\nif the errors are too large or shortening it otherwise. Furthermore, we\ntheoretically prove that the coverage of the confidence intervals provided by\nour method converges to the target coverage level. Experiments across four\nreal-world datasets and prediction models demonstrate that the proposed method\ncan provide valid confidence intervals with shorter lengths. Our method can\nhelp traffic management personnel develop a more reasonable and robust\noperation plan in practice. And we release the code, model and dataset in\n\\href{ https://github.com/xiannanhuang/CONTINA/}{ Github}.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCONTINA\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u52a8\u6001\u8c03\u6574\u4ea4\u901a\u9700\u6c42\u9884\u6d4b\u7684\u7f6e\u4fe1\u533a\u95f4\uff0c\u4ee5\u9002\u5e94\u5916\u90e8\u53d8\u5316\uff0c\u5e76\u4fdd\u8bc1\u8986\u76d6\u7387\u7684\u6536\u655b\u3002", "motivation": "\u73b0\u6709\u7f6e\u4fe1\u533a\u95f4\u5efa\u6a21\u65b9\u6cd5\u4f9d\u8d56\u4e25\u683c\u5047\u8bbe\uff0c\u5728\u53d8\u5316\u7684\u4ea4\u901a\u73af\u5883\u4e2d\u53ef\u80fd\u5931\u6548\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u81ea\u9002\u5e94\u8c03\u6574\u7684\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u6536\u96c6\u90e8\u7f72\u671f\u95f4\u7684\u533a\u95f4\u8bef\u5dee\u52a8\u6001\u8c03\u6574\u7f6e\u4fe1\u533a\u95f4\uff0c\u7406\u8bba\u4e0a\u8bc1\u660e\u5176\u8986\u76d6\u7387\u6536\u655b\u4e8e\u76ee\u6807\u6c34\u5e73\u3002", "result": "\u5728\u56db\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u548c\u9884\u6d4b\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u63d0\u4f9b\u66f4\u77ed\u4e14\u6709\u6548\u7684\u7f6e\u4fe1\u533a\u95f4\u3002", "conclusion": "CONTINA\u65b9\u6cd5\u80fd\u5e2e\u52a9\u4ea4\u901a\u7ba1\u7406\u4eba\u5458\u5236\u5b9a\u66f4\u5408\u7406\u548c\u7a33\u5065\u7684\u8fd0\u8425\u8ba1\u5212\uff0c\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5df2\u5f00\u6e90\u3002"}}
{"id": "2504.14126", "pdf": "https://arxiv.org/pdf/2504.14126", "abs": "https://arxiv.org/abs/2504.14126", "authors": ["Saad Hameed", "Basheer Qolomany", "Samir Brahim Belhaouari", "Mohamed Abdallah", "Junaid Qadir", "Ala Al-Fuqaha"], "title": "Large Language Model Enhanced Particle Swarm Optimization for Hyperparameter Tuning for Deep Learning Models", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Determining the ideal architecture for deep learning models, such as the\nnumber of layers and neurons, is a difficult and resource-intensive process\nthat frequently relies on human tuning or computationally costly optimization\napproaches. While Particle Swarm Optimization (PSO) and Large Language Models\n(LLMs) have been individually applied in optimization and deep learning, their\ncombined use for enhancing convergence in numerical optimization tasks remains\nunderexplored. Our work addresses this gap by integrating LLMs into PSO to\nreduce model evaluations and improve convergence for deep learning\nhyperparameter tuning. The proposed LLM-enhanced PSO method addresses the\ndifficulties of efficiency and convergence by using LLMs (particularly\nChatGPT-3.5 and Llama3) to improve PSO performance, allowing for faster\nachievement of target objectives. Our method speeds up search space exploration\nby substituting underperforming particle placements with best suggestions\noffered by LLMs. Comprehensive experiments across three scenarios -- (1)\noptimizing the Rastrigin function, (2) using Long Short-Term Memory (LSTM)\nnetworks for time series regression, and (3) using Convolutional Neural\nNetworks (CNNs) for material classification -- show that the method\nsignificantly improves convergence rates and lowers computational costs.\nDepending on the application, computational complexity is lowered by 20% to 60%\ncompared to traditional PSO methods. Llama3 achieved a 20% to 40% reduction in\nmodel calls for regression tasks, whereas ChatGPT-3.5 reduced model calls by\n60% for both regression and classification tasks, all while preserving accuracy\nand error rates. This groundbreaking methodology offers a very efficient and\neffective solution for optimizing deep learning models, leading to substantial\ncomputational performance improvements across a wide range of applications.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u548c\u7c92\u5b50\u7fa4\u4f18\u5316\uff08PSO\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u6df1\u5ea6\u5b66\u4e60\u8d85\u53c2\u6570\u8c03\u4f18\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6536\u655b\u901f\u5ea6\u5e76\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u67b6\u6784\u8bbe\u8ba1\u901a\u5e38\u4f9d\u8d56\u4eba\u5de5\u8c03\u4f18\u6216\u8ba1\u7b97\u5bc6\u96c6\u578b\u4f18\u5316\u65b9\u6cd5\uff0c\u6548\u7387\u4f4e\u4e0b\u3002\u867d\u7136PSO\u548cLLMs\u5728\u5404\u81ea\u9886\u57df\u5df2\u6709\u5e94\u7528\uff0c\u4f46\u4e8c\u8005\u7ed3\u5408\u7528\u4e8e\u6570\u503c\u4f18\u5316\u4efb\u52a1\u7684\u7814\u7a76\u8f83\u5c11\u3002", "method": "\u901a\u8fc7\u5c06LLMs\uff08\u5982ChatGPT-3.5\u548cLlama3\uff09\u96c6\u6210\u5230PSO\u4e2d\uff0c\u7528LLM\u7684\u5efa\u8bae\u66ff\u6362\u8868\u73b0\u4e0d\u4f73\u7684\u7c92\u5b50\u4f4d\u7f6e\uff0c\u4ece\u800c\u52a0\u901f\u641c\u7d22\u7a7a\u95f4\u63a2\u7d22\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4e09\u4e2a\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u6536\u655b\u901f\u5ea6\uff0c\u8ba1\u7b97\u590d\u6742\u5ea6\u964d\u4f4e\u4e8620%\u81f360%\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u51c6\u786e\u6027\u548c\u8bef\u5dee\u7387\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u4f18\u5316\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2504.14223", "pdf": "https://arxiv.org/pdf/2504.14223", "abs": "https://arxiv.org/abs/2504.14223", "authors": ["Michael F\u00e4rber", "Parisa Aghdam", "Kyuri Im", "Mario Tawfelis", "Hardik Ghoshal"], "title": "SimplifyMyText: An LLM-Based System for Inclusive Plain Language Text Simplification", "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "accepted at ECIR 2025", "summary": "Text simplification is essential for making complex content accessible to\ndiverse audiences who face comprehension challenges. Yet, the limited\navailability of simplified materials creates significant barriers to personal\nand professional growth and hinders social inclusion. Although researchers have\nexplored various methods for automatic text simplification, none fully leverage\nlarge language models (LLMs) to offer tailored customization for different\ntarget groups and varying levels of simplicity. Moreover, despite its proven\nbenefits for both consumers and organizations, the well-established practice of\nplain language remains underutilized. In this paper, we\nhttps://simplifymytext.org, the first system designed to produce plain language\ncontent from multiple input formats, including typed text and file uploads,\nwith flexible customization options for diverse audiences. We employ GPT-4 and\nLlama-3 and evaluate outputs across multiple metrics. Overall, our work\ncontributes to research on automatic text simplification and highlights the\nimportance of tailored communication in promoting inclusivity.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u9996\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08\u5982GPT-4\u548cLlama-3\uff09\u7684\u6587\u672c\u7b80\u5316\u7cfb\u7edfSimplifyMyText\uff0c\u65e8\u5728\u4e3a\u4e0d\u540c\u53d7\u4f17\u63d0\u4f9b\u5b9a\u5236\u5316\u7684\u7b80\u5316\u5185\u5bb9\uff0c\u5e76\u8bc4\u4f30\u5176\u6548\u679c\u3002", "motivation": "\u5f53\u524d\u6587\u672c\u7b80\u5316\u6750\u6599\u7684\u7a00\u7f3a\u963b\u788d\u4e86\u4e2a\u4eba\u548c\u793e\u4f1a\u7684\u53d1\u5c55\uff0c\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u5b9e\u73b0\u5b9a\u5236\u5316\u7b80\u5316\u3002", "method": "\u5229\u7528GPT-4\u548cLlama-3\u5f00\u53d1\u4e86SimplifyMyText\u7cfb\u7edf\uff0c\u652f\u6301\u591a\u79cd\u8f93\u5165\u683c\u5f0f\uff0c\u5e76\u63d0\u4f9b\u7075\u6d3b\u7684\u5b9a\u5236\u9009\u9879\u3002", "result": "\u7cfb\u7edf\u901a\u8fc7\u591a\u6307\u6807\u8bc4\u4f30\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u751f\u6210\u5b9a\u5236\u5316\u7b80\u5316\u6587\u672c\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u63a8\u52a8\u4e86\u81ea\u52a8\u6587\u672c\u7b80\u5316\u9886\u57df\u7684\u53d1\u5c55\uff0c\u5f3a\u8c03\u4e86\u5b9a\u5236\u5316\u6c9f\u901a\u5728\u4fc3\u8fdb\u5305\u5bb9\u6027\u4e2d\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2504.13966", "pdf": "https://arxiv.org/pdf/2504.13966", "abs": "https://arxiv.org/abs/2504.13966", "authors": ["Carolin Heinzler"], "title": "Adversarial Resilience against Clean-Label Attacks in Realizable and Noisy Settings", "categories": ["cs.LG", "stat.ML"], "comment": "Master's thesis", "summary": "We investigate the challenge of establishing stochastic-like guarantees when\nsequentially learning from a stream of i.i.d. data that includes an unknown\nquantity of clean-label adversarial samples. We permit the learner to abstain\nfrom making predictions when uncertain. The regret of the learner is measured\nin terms of misclassification and abstention error, where we allow the learner\nto abstain for free on adversarial injected samples. This approach is based on\nthe work of Goel, Hanneke, Moran, and Shetty from arXiv:2306.13119. We explore\nthe methods they present and manage to correct inaccuracies in their\nargumentation.\n  However, this approach is limited to the realizable setting, where labels are\nassigned according to some function $f^*$ from the hypothesis space\n$\\mathcal{F}$. Based on similar arguments, we explore methods to make\nadaptations for the agnostic setting where labels are random. Introducing the\nnotion of a clean-label adversary in the agnostic context, we are the first to\ngive a theoretical analysis of a disagreement-based learner for thresholds,\nsubject to a clean-label adversary with noise.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5728\u5305\u542b\u672a\u77e5\u6570\u91cf\u5e72\u51c0\u6807\u7b7e\u5bf9\u6297\u6837\u672c\u7684i.i.d.\u6570\u636e\u6d41\u4e2d\uff0c\u5982\u4f55\u5efa\u7acb\u968f\u673a\u6027\u4fdd\u8bc1\u7684\u6311\u6218\u3002\u5b66\u4e60\u8005\u53ef\u4ee5\u56e0\u4e0d\u786e\u5b9a\u800c\u653e\u5f03\u9884\u6d4b\uff0c\u5176\u9057\u61be\u901a\u8fc7\u8bef\u5206\u7c7b\u548c\u653e\u5f03\u9519\u8bef\u8861\u91cf\u3002\u65b9\u6cd5\u57fa\u4e8eGoel\u7b49\u4eba\u7684\u5de5\u4f5c\uff0c\u4f46\u4fee\u6b63\u4e86\u5176\u8bba\u8bc1\u4e2d\u7684\u4e0d\u51c6\u786e\u6027\uff0c\u5e76\u6269\u5c55\u5230\u4e0d\u53ef\u77e5\u8bbe\u7f6e\u3002", "motivation": "\u89e3\u51b3\u5728\u5bf9\u6297\u6837\u672c\u5b58\u5728\u65f6\uff0c\u5982\u4f55\u4fdd\u8bc1\u5b66\u4e60\u5668\u7684\u6027\u80fd\uff0c\u5e76\u6269\u5c55\u5230\u66f4\u5e7f\u6cdb\u7684\u4e0d\u53ef\u77e5\u8bbe\u7f6e\u3002", "method": "\u57fa\u4e8eGoel\u7b49\u4eba\u7684\u65b9\u6cd5\uff0c\u4fee\u6b63\u5176\u8bba\u8bc1\uff0c\u5e76\u5f15\u5165\u5e72\u51c0\u6807\u7b7e\u5bf9\u6297\u8005\u7684\u6982\u5ff5\uff0c\u5206\u6790\u9608\u503c\u5b66\u4e60\u5668\u5728\u566a\u58f0\u4e0b\u7684\u8868\u73b0\u3002", "result": "\u9996\u6b21\u5728\u4e0d\u53ef\u77e5\u8bbe\u7f6e\u4e0b\uff0c\u5bf9\u9608\u503c\u5b66\u4e60\u5668\u5728\u5e72\u51c0\u6807\u7b7e\u5bf9\u6297\u8005\u5b58\u5728\u65f6\u7684\u7406\u8bba\u5206\u6790\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u5bf9\u6297\u6837\u672c\u5b58\u5728\u65f6\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\uff0c\u5e76\u6269\u5c55\u5230\u4e0d\u53ef\u77e5\u8bbe\u7f6e\uff0c\u4f46\u4ecd\u9650\u4e8e\u67d0\u4e9b\u5047\u8bbe\u7a7a\u95f4\u3002"}}
{"id": "2504.14128", "pdf": "https://arxiv.org/pdf/2504.14128", "abs": "https://arxiv.org/abs/2504.14128", "authors": ["Christopher Zhang Cui", "Xingdi Yuan", "Zhang Xiao", "Prithviraj Ammanabrolu", "Marc-Alexandre C\u00f4t\u00e9"], "title": "TALES: Text Adventure Learning Environment Suite", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Reasoning is an essential skill to enable Large Language Models (LLMs) to\ninteract with the world. As tasks become more complex, they demand increasingly\nsophisticated and diverse reasoning capabilities for sequential\ndecision-making, requiring structured reasoning over the context history to\ndetermine the next best action. We introduce TALES, a diverse collection of\nsynthetic and human-written text-adventure games designed to challenge and\nevaluate diverse reasoning capabilities. We present results over a range of\nLLMs, open- and closed-weights, performing a qualitative analysis on the top\nperforming models. Despite an impressive showing on synthetic games, even the\ntop LLM-driven agents fail to achieve 15% on games designed for human\nenjoyment. Code and visualization of the experiments can be found at\nhttps://microsoft.github.io/tales.", "AI": {"tldr": "TALES\u662f\u4e00\u4e2a\u591a\u6837\u5316\u7684\u6587\u672c\u5192\u9669\u6e38\u620f\u96c6\u5408\uff0c\u7528\u4e8e\u6311\u6218\u548c\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u7ed3\u679c\u663e\u793a\u5373\u4f7f\u8868\u73b0\u6700\u597d\u7684\u6a21\u578b\u5728\u4eba\u7c7b\u8bbe\u8ba1\u7684\u6e38\u620f\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002", "motivation": "\u968f\u7740\u4efb\u52a1\u590d\u6742\u6027\u589e\u52a0\uff0c\u9700\u8981\u66f4\u590d\u6742\u7684\u63a8\u7406\u80fd\u529b\uff0c\u56e0\u6b64\u8bbe\u8ba1\u4e86TALES\u6765\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u591a\u6837\u5316\u63a8\u7406\u80fd\u529b\u3002", "method": "\u4f7f\u7528\u5408\u6210\u548c\u4eba\u7c7b\u7f16\u5199\u7684\u6587\u672c\u5192\u9669\u6e38\u620f\uff08TALES\uff09\u6d4b\u8bd5\u591a\u79cd\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u5305\u62ec\u5f00\u6e90\u548c\u95ed\u6e90\u6a21\u578b\u3002", "result": "\u8868\u73b0\u6700\u597d\u7684\u6a21\u578b\u5728\u5408\u6210\u6e38\u620f\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u4eba\u7c7b\u8bbe\u8ba1\u7684\u6e38\u620f\u4e2d\u6210\u529f\u7387\u4f4e\u4e8e15%\u3002", "conclusion": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\uff0c\u5c24\u5176\u662f\u5728\u4eba\u7c7b\u8bbe\u8ba1\u7684\u573a\u666f\u4e2d\u3002"}}
{"id": "2504.14225", "pdf": "https://arxiv.org/pdf/2504.14225", "abs": "https://arxiv.org/abs/2504.14225", "authors": ["Bowen Jiang", "Zhuoqun Hao", "Young-Min Cho", "Bryan Li", "Yuan Yuan", "Sihao Chen", "Lyle Ungar", "Camillo J. Taylor", "Dan Roth"], "title": "Know Me, Respond to Me: Benchmarking LLMs for Dynamic User Profiling and Personalized Responses at Scale", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have emerged as personalized assistants for\nusers across a wide range of tasks -- from offering writing support to\ndelivering tailored recommendations or consultations. Over time, the\ninteraction history between a user and an LLM can provide extensive information\nabout an individual's traits and preferences. However, open questions remain on\nhow well LLMs today can effectively leverage such history to (1) internalize\nthe user's inherent traits and preferences, (2) track how the user profiling\nand preferences evolve over time, and (3) generate personalized responses\naccordingly in new scenarios.\n  In this work, we introduce the PERSONAMEM benchmark. PERSONAMEM features\ncurated user profiles with over 180 simulated user-LLM interaction histories,\neach containing up to 60 sessions of multi-turn conversations across 15\nreal-world tasks that require personalization. Given an in-situ user query,\ni.e. query issued by the user from the first-person perspective, we evaluate\nLLM chatbots' ability to identify the most suitable response according to the\ncurrent state of the user's profile. We observe that current LLMs still\nstruggle to recognize the dynamic evolution in users' profiles over time\nthrough direct prompting approaches. As a consequence, LLMs often fail to\ndeliver responses that align with users' current situations and preferences,\nwith frontier models such as GPT-4.1, o4-mini, GPT-4.5, o1, or Gemini-2.0\nachieving only around 50% overall accuracy, suggesting room for improvement. We\nhope that PERSONAMEM, along with the user profile and conversation simulation\npipeline, can facilitate future research in the development of truly user-aware\nchatbots. Code and data are available at github.com/bowen-upenn/PersonaMem.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86PERSONAMEM\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30LLMs\u5982\u4f55\u5229\u7528\u7528\u6237\u4ea4\u4e92\u5386\u53f2\u751f\u6210\u4e2a\u6027\u5316\u54cd\u5e94\uff0c\u53d1\u73b0\u5f53\u524d\u6a21\u578b\u5728\u52a8\u6001\u8ddf\u8e2a\u7528\u6237\u504f\u597d\u65b9\u9762\u4ecd\u6709\u4e0d\u8db3\u3002", "motivation": "\u7814\u7a76LLMs\u5982\u4f55\u5229\u7528\u7528\u6237\u4ea4\u4e92\u5386\u53f2\u5185\u90e8\u5316\u7528\u6237\u7279\u8d28\u548c\u504f\u597d\uff0c\u5e76\u52a8\u6001\u8ddf\u8e2a\u5176\u6f14\u53d8\uff0c\u4ee5\u751f\u6210\u4e2a\u6027\u5316\u54cd\u5e94\u3002", "method": "\u63d0\u51faPERSONAMEM\u57fa\u51c6\uff0c\u5305\u542b180\u4e2a\u6a21\u62df\u7528\u6237-LLM\u4ea4\u4e92\u5386\u53f2\uff0c\u8bc4\u4f30LLMs\u5728\u52a8\u6001\u7528\u6237\u60c5\u5883\u4e0b\u7684\u54cd\u5e94\u80fd\u529b\u3002", "result": "\u5f53\u524dLLMs\uff08\u5982GPT-4.1\u7b49\uff09\u5728\u52a8\u6001\u8ddf\u8e2a\u7528\u6237\u504f\u597d\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u51c6\u786e\u7387\u4ec5\u7ea650%\u3002", "conclusion": "PERSONAMEM\u57fa\u51c6\u4e3a\u672a\u6765\u5f00\u53d1\u7528\u6237\u611f\u77e5\u578b\u804a\u5929\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u7814\u7a76\u57fa\u7840\u3002"}}
{"id": "2504.13974", "pdf": "https://arxiv.org/pdf/2504.13974", "abs": "https://arxiv.org/abs/2504.13974", "authors": ["Yao Zhiwan", "Reza Zarrab", "Jean Dubois"], "title": "Enhancing Stroke Diagnosis in the Brain Using a Weighted Deep Learning Approach", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "A brain stroke occurs when blood flow to a part of the brain is disrupted,\nleading to cell death. Traditional stroke diagnosis methods, such as CT scans\nand MRIs, are costly and time-consuming. This study proposes a weighted voting\nensemble (WVE) machine learning model that combines predictions from\nclassifiers like random forest, Deep Learning, and histogram-based gradient\nboosting to predict strokes more effectively. The model achieved 94.91%\naccuracy on a private dataset, enabling early risk assessment and prevention.\nFuture research could explore optimization techniques to further enhance\naccuracy.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u52a0\u6743\u6295\u7968\u96c6\u6210\uff08WVE\uff09\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u7528\u4e8e\u9ad8\u6548\u9884\u6d4b\u8111\u5352\u4e2d\uff0c\u51c6\u786e\u7387\u8fbe94.91%\u3002", "motivation": "\u4f20\u7edf\u8111\u5352\u4e2d\u8bca\u65ad\u65b9\u6cd5\uff08\u5982CT\u548cMRI\uff09\u6210\u672c\u9ad8\u4e14\u8017\u65f6\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u9884\u6d4b\u624b\u6bb5\u3002", "method": "\u7ed3\u5408\u968f\u673a\u68ee\u6797\u3001\u6df1\u5ea6\u5b66\u4e60\u548c\u57fa\u4e8e\u76f4\u65b9\u56fe\u7684\u68af\u5ea6\u63d0\u5347\u5206\u7c7b\u5668\u7684\u9884\u6d4b\uff0c\u6784\u5efa\u52a0\u6743\u6295\u7968\u96c6\u6210\u6a21\u578b\u3002", "result": "\u5728\u79c1\u6709\u6570\u636e\u96c6\u4e0a\u8fbe\u523094.91%\u7684\u51c6\u786e\u7387\uff0c\u652f\u6301\u65e9\u671f\u98ce\u9669\u8bc4\u4f30\u548c\u9884\u9632\u3002", "conclusion": "\u672a\u6765\u7814\u7a76\u53ef\u63a2\u7d22\u4f18\u5316\u6280\u672f\u4ee5\u8fdb\u4e00\u6b65\u63d0\u5347\u6a21\u578b\u51c6\u786e\u6027\u3002"}}
{"id": "2504.14171", "pdf": "https://arxiv.org/pdf/2504.14171", "abs": "https://arxiv.org/abs/2504.14171", "authors": ["Yangping Chen", "Weijie Shi", "Mengze Li", "Yue Cui", "Hao Chen", "Jia Zhu", "Jiajie Xu"], "title": "Adaptation Method for Misinformation Identification", "categories": ["cs.AI"], "comment": null, "summary": "Multimodal fake news detection plays a crucial role in combating online\nmisinformation. Unfortunately, effective detection methods rely on annotated\nlabels and encounter significant performance degradation when domain shifts\nexist between training (source) and test (target) data. To address the\nproblems, we propose ADOSE, an Active Domain Adaptation (ADA) framework for\nmultimodal fake news detection which actively annotates a small subset of\ntarget samples to improve detection performance. To identify various deceptive\npatterns in cross-domain settings, we design multiple expert classifiers to\nlearn dependencies across different modalities. These classifiers specifically\ntarget the distinct deception patterns exhibited in fake news, where two\nunimodal classifiers capture knowledge errors within individual modalities\nwhile one cross-modal classifier identifies semantic inconsistencies between\ntext and images. To reduce annotation costs from the target domain, we propose\na least-disagree uncertainty selector with a diversity calculator for selecting\nthe most informative samples. The selector leverages prediction disagreement\nbefore and after perturbations by multiple classifiers as an indicator of\nuncertain samples, whose deceptive patterns deviate most from source domains.\nIt further incorporates diversity scores derived from multi-view features to\nensure the chosen samples achieve maximal coverage of target domain features.\nThe extensive experiments on multiple datasets show that ADOSE outperforms\nexisting ADA methods by 2.72\\% $\\sim$ 14.02\\%, indicating the superiority of\nour model.", "AI": {"tldr": "ADOSE\u662f\u4e00\u4e2a\u4e3b\u52a8\u9886\u57df\u81ea\u9002\u5e94\u6846\u67b6\uff0c\u7528\u4e8e\u591a\u6a21\u6001\u5047\u65b0\u95fb\u68c0\u6d4b\uff0c\u901a\u8fc7\u6807\u6ce8\u5c11\u91cf\u76ee\u6807\u6837\u672c\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u591a\u6a21\u6001\u5047\u65b0\u95fb\u68c0\u6d4b\u4e2d\u9886\u57df\u504f\u79fb\u5bfc\u81f4\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1\u591a\u4e13\u5bb6\u5206\u7c7b\u5668\u6355\u6349\u8de8\u6a21\u6001\u4f9d\u8d56\u5173\u7cfb\uff0c\u63d0\u51fa\u6700\u5c0f\u5206\u6b67\u4e0d\u786e\u5b9a\u6027\u9009\u62e9\u5668\u51cf\u5c11\u6807\u6ce8\u6210\u672c\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd52.72%\u81f314.02%\u3002", "conclusion": "ADOSE\u5728\u8de8\u9886\u57df\u5047\u65b0\u95fb\u68c0\u6d4b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u6709\u6548\u63d0\u5347\u6027\u80fd\u3002"}}
{"id": "2504.14287", "pdf": "https://arxiv.org/pdf/2504.14287", "abs": "https://arxiv.org/abs/2504.14287", "authors": ["Demetris Paschalides", "George Pallis", "Marios D. Dikaiakos"], "title": "Probing the Subtle Ideological Manipulation of Large Language Models", "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "Large Language Models (LLMs) have transformed natural language processing,\nbut concerns have emerged about their susceptibility to ideological\nmanipulation, particularly in politically sensitive areas. Prior work has\nfocused on binary Left-Right LLM biases, using explicit prompts and fine-tuning\non political QA datasets. In this work, we move beyond this binary approach to\nexplore the extent to which LLMs can be influenced across a spectrum of\npolitical ideologies, from Progressive-Left to Conservative-Right. We introduce\na novel multi-task dataset designed to reflect diverse ideological positions\nthrough tasks such as ideological QA, statement ranking, manifesto cloze\ncompletion, and Congress bill comprehension. By fine-tuning three LLMs-Phi-2,\nMistral, and Llama-3-on this dataset, we evaluate their capacity to adopt and\nexpress these nuanced ideologies. Our findings indicate that fine-tuning\nsignificantly enhances nuanced ideological alignment, while explicit prompts\nprovide only minor refinements. This highlights the models' susceptibility to\nsubtle ideological manipulation, suggesting a need for more robust safeguards\nto mitigate these risks.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u591a\u7ef4\u653f\u6cbb\u610f\u8bc6\u5f62\u6001\u8c31\u7cfb\u4e2d\u7684\u53ef\u64cd\u7eb5\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u65b0\u7684\u591a\u4efb\u52a1\u6570\u636e\u96c6\u3002\u901a\u8fc7\u5fae\u8c03\u6a21\u578b\uff0c\u53d1\u73b0\u5176\u5bf9\u610f\u8bc6\u5f62\u6001\u7684\u654f\u611f\u6027\u8f83\u9ad8\uff0c\u9700\u52a0\u5f3a\u9632\u62a4\u63aa\u65bd\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8LLMs\u5728\u4e8c\u5143\u653f\u6cbb\u7acb\u573a\uff08\u5de6-\u53f3\uff09\u4e0a\u7684\u504f\u89c1\uff0c\u4f46\u5ffd\u7565\u4e86\u66f4\u590d\u6742\u7684\u610f\u8bc6\u5f62\u6001\u8c31\u7cfb\u3002\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63a2\u7d22LLMs\u5728\u591a\u7ef4\u653f\u6cbb\u7acb\u573a\u4e2d\u7684\u53ef\u64cd\u7eb5\u6027\u3002", "method": "\u5f15\u5165\u591a\u4efb\u52a1\u6570\u636e\u96c6\uff0c\u5305\u542b\u610f\u8bc6\u5f62\u6001\u95ee\u7b54\u3001\u58f0\u660e\u6392\u5e8f\u3001\u5ba3\u8a00\u586b\u7a7a\u548c\u56fd\u4f1a\u6cd5\u6848\u7406\u89e3\u7b49\u4efb\u52a1\u3002\u5bf9Phi-2\u3001Mistral\u548cLlama-3\u4e09\u79cdLLMs\u8fdb\u884c\u5fae\u8c03\uff0c\u8bc4\u4f30\u5176\u610f\u8bc6\u5f62\u6001\u8868\u8fbe\u80fd\u529b\u3002", "result": "\u5fae\u8c03\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5bf9\u590d\u6742\u610f\u8bc6\u5f62\u6001\u7684\u9002\u5e94\u6027\uff0c\u800c\u663e\u5f0f\u63d0\u793a\u6548\u679c\u6709\u9650\u3002\u8868\u660eLLMs\u6613\u53d7\u610f\u8bc6\u5f62\u6001\u64cd\u7eb5\u3002", "conclusion": "LLMs\u5728\u591a\u7ef4\u653f\u6cbb\u7acb\u573a\u4e2d\u8868\u73b0\u51fa\u8f83\u9ad8\u7684\u53ef\u64cd\u7eb5\u6027\uff0c\u9700\u5f00\u53d1\u66f4\u5f3a\u5927\u7684\u9632\u62a4\u673a\u5236\u4ee5\u51cf\u5c11\u98ce\u9669\u3002"}}
{"id": "2504.13975", "pdf": "https://arxiv.org/pdf/2504.13975", "abs": "https://arxiv.org/abs/2504.13975", "authors": ["Mehmet Yama\u00e7", "Muhammad Numan Yousaf", "Serkan Kiranyaz", "Moncef Gabbouj"], "title": "Multiscale Tensor Summation Factorization as a New Neural Network Layer (MTS Layer) for Multidimensional Data Processing", "categories": ["cs.LG", "cs.AI", "cs.CV", "68T07"], "comment": "13 pages", "summary": "Multilayer perceptrons (MLP), or fully connected artificial neural networks,\nare known for performing vector-matrix multiplications using learnable weight\nmatrices; however, their practical application in many machine learning tasks,\nespecially in computer vision, can be limited due to the high dimensionality of\ninput-output pairs at each layer. To improve efficiency, convolutional\noperators have been utilized to facilitate weight sharing and local\nconnections, yet they are constrained by limited receptive fields. In this\npaper, we introduce Multiscale Tensor Summation (MTS) Factorization, a novel\nneural network operator that implements tensor summation at multiple scales,\nwhere each tensor to be summed is obtained through Tucker-decomposition-like\nmode products. Unlike other tensor decomposition methods in the literature, MTS\nis not introduced as a network compression tool; instead, as a new backbone\nneural layer. MTS not only reduces the number of parameters required while\nenhancing the efficiency of weight optimization compared to traditional dense\nlayers (i.e., unfactorized weight matrices in MLP layers), but it also\ndemonstrates clear advantages over convolutional layers. The proof-of-concept\nexperimental comparison of the proposed MTS networks with MLPs and\nConvolutional Neural Networks (CNNs) demonstrates their effectiveness across\nvarious tasks, such as classification, compression, and signal restoration.\nAdditionally, when integrated with modern non-linear units such as the\nmulti-head gate (MHG), also introduced in this study, the corresponding neural\nnetwork, MTSNet, demonstrates a more favorable complexity-performance tradeoff\ncompared to state-of-the-art transformers in various computer vision\napplications. The software implementation of the MTS layer and the\ncorresponding MTS-based networks, MTSNets, is shared at\nhttps://github.com/mehmetyamac/MTSNet.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u591a\u5c3a\u5ea6\u5f20\u91cf\u6c42\u548c\uff08MTS\uff09\u5206\u89e3\u7684\u65b0\u578b\u795e\u7ecf\u7f51\u7edc\u7b97\u5b50\uff0c\u7528\u4e8e\u66ff\u4ee3\u4f20\u7edf\u5bc6\u96c6\u5c42\u548c\u5377\u79ef\u5c42\uff0c\u63d0\u9ad8\u6548\u7387\u548c\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u591a\u5c42\u611f\u77e5\u673a\uff08MLP\uff09\u548c\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u5728\u9ad8\u7ef4\u8f93\u5165\u8f93\u51fa\u5bf9\u548c\u6709\u9650\u611f\u53d7\u91ce\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u7b97\u5b50\u3002", "method": "MTS\u901a\u8fc7\u591a\u5c3a\u5ea6\u5f20\u91cf\u6c42\u548c\u5b9e\u73b0\uff0c\u6bcf\u4e2a\u5f20\u91cf\u901a\u8fc7\u7c7b\u4f3cTucker\u5206\u89e3\u7684\u6a21\u5f0f\u4e58\u79ef\u83b7\u5f97\uff0c\u5e76\u7ed3\u5408\u591a\u5934\u95e8\uff08MHG\uff09\u7b49\u975e\u7ebf\u6027\u5355\u5143\u3002", "result": "MTS\u5728\u5206\u7c7b\u3001\u538b\u7f29\u548c\u4fe1\u53f7\u6062\u590d\u7b49\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8eMLP\u548cCNN\uff0c\u4e14\u4e0e\u6700\u5148\u8fdb\u7684Transformer\u76f8\u6bd4\u5177\u6709\u66f4\u597d\u7684\u590d\u6742\u5ea6-\u6027\u80fd\u6743\u8861\u3002", "conclusion": "MTSNet\u4f5c\u4e3a\u4e00\u79cd\u65b0\u578b\u795e\u7ecf\u7f51\u7edc\u5c42\uff0c\u5c55\u793a\u4e86\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u4e2d\u7684\u9ad8\u6548\u6027\u548c\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2504.14177", "pdf": "https://arxiv.org/pdf/2504.14177", "abs": "https://arxiv.org/abs/2504.14177", "authors": ["Li He", "He Zhao", "Stephen Wan", "Dadong Wang", "Lina Yao", "Tongliang Liu"], "title": "Direct Advantage Regression: Aligning LLMs with Online AI Reward", "categories": ["cs.AI", "cs.CL", "cs.HC"], "comment": null, "summary": "Online AI Feedback (OAIF) presents a promising alternative to Reinforcement\nLearning from Human Feedback (RLHF) by utilizing online AI preference in\naligning language models (LLMs). However, the straightforward replacement of\nhumans with AI deprives LLMs from learning more fine-grained AI supervision\nbeyond binary signals. In this paper, we propose Direct Advantage Regression\n(DAR), a simple alignment algorithm using online AI reward to optimize policy\nimprovement through weighted supervised fine-tuning. As an RL-free approach,\nDAR maintains theoretical consistency with online RLHF pipelines while\nsignificantly reducing implementation complexity and improving learning\nefficiency. Our empirical results underscore that AI reward is a better form of\nAI supervision consistently achieving higher human-AI agreement as opposed to\nAI preference. Additionally, evaluations using GPT-4-Turbo and MT-bench show\nthat DAR outperforms both OAIF and online RLHF baselines.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDAR\u7684\u7b80\u5355\u5bf9\u9f50\u7b97\u6cd5\uff0c\u5229\u7528\u5728\u7ebfAI\u5956\u52b1\u4f18\u5316\u7b56\u7565\u6539\u8fdb\uff0c\u907f\u514d\u4e86\u4f20\u7edfRLHF\u7684\u590d\u6742\u6027\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u4e8eOAIF\u548cRLHF\u57fa\u7ebf\u3002", "motivation": "\u5728\u7ebfAI\u53cd\u9988\uff08OAIF\uff09\u867d\u7136\u66ff\u4ee3\u4e86\u4eba\u7c7b\u53cd\u9988\uff08RLHF\uff09\uff0c\u4f46\u7f3a\u4e4f\u7ec6\u7c92\u5ea6\u7684\u76d1\u7763\u4fe1\u53f7\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7DAR\u7b97\u6cd5\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51faDirect Advantage Regression\uff08DAR\uff09\uff0c\u4e00\u79cd\u57fa\u4e8e\u5728\u7ebfAI\u5956\u52b1\u7684\u52a0\u6743\u76d1\u7763\u5fae\u8c03\u65b9\u6cd5\uff0c\u65e0\u9700\u5f3a\u5316\u5b66\u4e60\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDAR\u5728\u4eba\u7c7b-AI\u4e00\u81f4\u6027\u548c\u6027\u80fd\u4e0a\u4f18\u4e8eOAIF\u548cRLHF\u57fa\u7ebf\uff0cGPT-4-Turbo\u548cMT-bench\u8bc4\u4f30\u7ed3\u679c\u652f\u6301\u8fd9\u4e00\u7ed3\u8bba\u3002", "conclusion": "DAR\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u7b80\u5355\u7684\u5bf9\u9f50\u7b97\u6cd5\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e3aAI\u76d1\u7763\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u5f62\u5f0f\u3002"}}
{"id": "2504.14321", "pdf": "https://arxiv.org/pdf/2504.14321", "abs": "https://arxiv.org/abs/2504.14321", "authors": ["Xingyu Li", "Chen Gong", "Guohong Fu"], "title": "Multimodal Coreference Resolution for Chinese Social Media Dialogues: Dataset and Benchmark Approach", "categories": ["cs.CL"], "comment": null, "summary": "Multimodal coreference resolution (MCR) aims to identify mentions referring\nto the same entity across different modalities, such as text and visuals, and\nis essential for understanding multimodal content. In the era of rapidly\ngrowing mutimodal content and social media, MCR is particularly crucial for\ninterpreting user interactions and bridging text-visual references to improve\ncommunication and personalization. However, MCR research for real-world\ndialogues remains unexplored due to the lack of sufficient data resources.To\naddress this gap, we introduce TikTalkCoref, the first Chinese multimodal\ncoreference dataset for social media in real-world scenarios, derived from the\npopular Douyin short-video platform. This dataset pairs short videos with\ncorresponding textual dialogues from user comments and includes manually\nannotated coreference clusters for both person mentions in the text and the\ncoreferential person head regions in the corresponding video frames. We also\npresent an effective benchmark approach for MCR, focusing on the celebrity\ndomain, and conduct extensive experiments on our dataset, providing reliable\nbenchmark results for this newly constructed dataset. We will release the\nTikTalkCoref dataset to facilitate future research on MCR for real-world social\nmedia dialogues.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86\u9996\u4e2a\u4e2d\u6587\u591a\u6a21\u6001\u5171\u6307\u6570\u636e\u96c6TikTalkCoref\uff0c\u7528\u4e8e\u89e3\u51b3\u793e\u4ea4\u5a92\u4f53\u4e2d\u7684\u591a\u6a21\u6001\u5171\u6307\u95ee\u9898\uff0c\u5e76\u63d0\u4f9b\u4e86\u57fa\u51c6\u65b9\u6cd5\u3002", "motivation": "\u591a\u6a21\u6001\u5171\u6307\u89e3\u6790\uff08MCR\uff09\u5bf9\u7406\u89e3\u591a\u6a21\u6001\u5185\u5bb9\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7f3a\u4e4f\u771f\u5b9e\u5bf9\u8bdd\u573a\u666f\u7684\u6570\u636e\u96c6\u3002", "method": "\u6784\u5efa\u4e86TikTalkCoref\u6570\u636e\u96c6\uff0c\u5305\u542b\u77ed\u89c6\u9891\u548c\u7528\u6237\u8bc4\u8bba\u7684\u6587\u672c\u5bf9\u8bdd\uff0c\u5e76\u6807\u6ce8\u4e86\u5171\u6307\u5173\u7cfb\u3002\u540c\u65f6\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u51c6\u65b9\u6cd5\u3002", "result": "\u5728TikTalkCoref\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0c\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u57fa\u51c6\u7ed3\u679c\u3002", "conclusion": "TikTalkCoref\u6570\u636e\u96c6\u5c06\u4fc3\u8fdb\u793e\u4ea4\u5a92\u4f53\u591a\u6a21\u6001\u5171\u6307\u7814\u7a76\u7684\u672a\u6765\u53d1\u5c55\u3002"}}
{"id": "2504.13981", "pdf": "https://arxiv.org/pdf/2504.13981", "abs": "https://arxiv.org/abs/2504.13981", "authors": ["Sushant Singh", "Ausif Mahmood"], "title": "CacheFormer: High Attention-Based Segment Caching", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Efficiently handling long contexts in transformer-based language models with\nlow perplexity is an active area of research. Numerous recent approaches like\nLinformer, Longformer, Performer, and Structured state space models (SSMs).,\nhave not fully resolved this problem. All these models strive to reduce the\nquadratic time complexity of the attention mechanism while minimizing the loss\nin quality due to the effective compression of the long context. Inspired by\nthe cache and virtual memory principle in computers, where in case of a cache\nmiss, not only the needed data is retrieved from the memory, but the adjacent\ndata is also obtained, we apply this concept to handling long contexts by\ndividing it into small segments. In our design, we retrieve the nearby segments\nin an uncompressed form when high segment-level attention occurs at the\ncompressed level. Our en-hancements for handling long context include\naggregating four attention mechanisms consisting of short sliding window\nattention, long compressed segmented attention, dynamically retrieving top k\nhigh attention uncompressed segments, and overlapping segments in long segment\nattention to avoid segment fragmentation. These enhancements result in an\narchitecture that outperforms ex-isting SOTA architectures with an average\nperplexity improvement of 8.5% over similar model sizes.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7f13\u5b58\u548c\u865a\u62df\u5185\u5b58\u539f\u7406\u7684\u957f\u4e0a\u4e0b\u6587\u5904\u7406\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u6bb5\u548c\u52a8\u6001\u68c0\u7d22\u673a\u5236\u4f18\u5316Transformer\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5e73\u5747\u56f0\u60d1\u5ea6\u63d0\u53478.5%\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5982Linformer\u3001Longformer\u7b49\u672a\u80fd\u5b8c\u5168\u89e3\u51b3\u957f\u4e0a\u4e0b\u6587\u5904\u7406\u95ee\u9898\uff0c\u9700\u5728\u964d\u4f4e\u65f6\u95f4\u590d\u6742\u5ea6\u7684\u540c\u65f6\u4fdd\u6301\u8d28\u91cf\u3002", "method": "\u5c06\u957f\u4e0a\u4e0b\u6587\u5206\u6bb5\u5904\u7406\uff0c\u52a8\u6001\u68c0\u7d22\u9ad8\u6ce8\u610f\u529b\u672a\u538b\u7f29\u6bb5\uff0c\u7ed3\u5408\u77ed\u6ed1\u52a8\u7a97\u53e3\u3001\u957f\u538b\u7f29\u5206\u6bb5\u3001\u52a8\u6001\u68c0\u7d22\u548c\u91cd\u53e0\u6bb5\u56db\u79cd\u6ce8\u610f\u529b\u673a\u5236\u3002", "result": "\u65b0\u67b6\u6784\u5728\u76f8\u4f3c\u6a21\u578b\u89c4\u6a21\u4e0b\u5e73\u5747\u56f0\u60d1\u5ea6\u63d0\u53478.5%\uff0c\u4f18\u4e8e\u73b0\u6709SOTA\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u5206\u6bb5\u548c\u52a8\u6001\u68c0\u7d22\u673a\u5236\uff0c\u6709\u6548\u63d0\u5347\u4e86\u957f\u4e0a\u4e0b\u6587\u5904\u7406\u7684\u6548\u7387\u548c\u6027\u80fd\u3002"}}
{"id": "2504.14191", "pdf": "https://arxiv.org/pdf/2504.14191", "abs": "https://arxiv.org/abs/2504.14191", "authors": ["Yansheng Qiu", "Haoquan Zhang", "Zhaopan Xu", "Ming Li", "Diping Song", "Zheng Wang", "Kaipeng Zhang"], "title": "AI Idea Bench 2025: AI Research Idea Generation Benchmark", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Large-scale Language Models (LLMs) have revolutionized human-AI interaction\nand achieved significant success in the generation of novel ideas. However,\ncurrent assessments of idea generation overlook crucial factors such as\nknowledge leakage in LLMs, the absence of open-ended benchmarks with grounded\ntruth, and the limited scope of feasibility analysis constrained by prompt\ndesign. These limitations hinder the potential of uncovering groundbreaking\nresearch ideas. In this paper, we present AI Idea Bench 2025, a framework\ndesigned to quantitatively evaluate and compare the ideas generated by LLMs\nwithin the domain of AI research from diverse perspectives. The framework\ncomprises a comprehensive dataset of 3,495 AI papers and their associated\ninspired works, along with a robust evaluation methodology. This evaluation\nsystem gauges idea quality in two dimensions: alignment with the ground-truth\ncontent of the original papers and judgment based on general reference\nmaterial. AI Idea Bench 2025's benchmarking system stands to be an invaluable\nresource for assessing and comparing idea-generation techniques, thereby\nfacilitating the automation of scientific discovery.", "AI": {"tldr": "AI Idea Bench 2025\u662f\u4e00\u4e2a\u8bc4\u4f30LLMs\u751f\u6210AI\u7814\u7a76\u60f3\u6cd5\u7684\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u7684\u4e0d\u8db3\uff0c\u5982\u77e5\u8bc6\u6cc4\u6f0f\u548c\u7f3a\u4e4f\u5f00\u653e\u5f0f\u57fa\u51c6\u3002", "motivation": "\u5f53\u524d\u5bf9LLMs\u751f\u6210\u60f3\u6cd5\u7684\u8bc4\u4f30\u5ffd\u7565\u4e86\u77e5\u8bc6\u6cc4\u6f0f\u3001\u5f00\u653e\u5f0f\u57fa\u51c6\u548c\u53ef\u884c\u6027\u5206\u6790\u7b49\u5173\u952e\u56e0\u7d20\uff0c\u9650\u5236\u4e86\u7a81\u7834\u6027\u7814\u7a76\u53d1\u73b0\u7684\u6f5c\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5305\u542b3,495\u7bc7AI\u8bba\u6587\u53ca\u5176\u542f\u53d1\u5de5\u4f5c\u7684\u6570\u636e\u96c6\uff0c\u4ee5\u53ca\u4e00\u4e2a\u4ece\u539f\u521b\u8bba\u6587\u5185\u5bb9\u548c\u901a\u7528\u53c2\u8003\u6750\u6599\u4e24\u4e2a\u7ef4\u5ea6\u8bc4\u4f30\u60f3\u6cd5\u8d28\u91cf\u7684\u7cfb\u7edf\u3002", "result": "AI Idea Bench 2025\u4e3a\u8bc4\u4f30\u548c\u6bd4\u8f83\u60f3\u6cd5\u751f\u6210\u6280\u672f\u63d0\u4f9b\u4e86\u5b9d\u8d35\u8d44\u6e90\uff0c\u63a8\u52a8\u4e86\u79d1\u5b66\u53d1\u73b0\u7684\u81ea\u52a8\u5316\u3002", "conclusion": "\u8be5\u6846\u67b6\u586b\u8865\u4e86\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u7684\u7a7a\u767d\uff0c\u6709\u671b\u4fc3\u8fdbLLMs\u5728\u79d1\u5b66\u521b\u65b0\u4e2d\u7684\u5e94\u7528\u3002"}}
{"id": "2504.14366", "pdf": "https://arxiv.org/pdf/2504.14366", "abs": "https://arxiv.org/abs/2504.14366", "authors": ["Patrick Haller", "Jonas Golde", "Alan Akbik"], "title": "Empirical Evaluation of Knowledge Distillation from Transformers to Subquadratic Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Knowledge distillation is a widely used technique for compressing large\nlanguage models (LLMs) by training a smaller student model to mimic a larger\nteacher model. Typically, both the teacher and student are Transformer-based\narchitectures, leveraging softmax attention for sequence modeling. However, the\nquadratic complexity of self-attention at inference time remains a significant\nbottleneck, motivating the exploration of subquadratic alternatives such as\nstructured state-space models (SSMs), linear attention, and recurrent\narchitectures. In this work, we systematically evaluate the transferability of\nknowledge distillation from a Transformer teacher to nine subquadratic student\narchitectures. Our study aims to determine which subquadratic model best aligns\nwith the teacher's learned representations and how different architectural\nconstraints influence the distillation process. We also investigate the impact\nof intelligent initialization strategies, including matrix mixing and\nquery-key-value (QKV) copying, on the adaptation process. Our empirical results\non multiple NLP benchmarks provide insights into the trade-offs between\nefficiency and performance, highlighting key factors for successful knowledge\ntransfer to subquadratic architectures.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u4eceTransformer\u6559\u5e08\u6a21\u578b\u5230\u4e5d\u79cd\u6b21\u4e8c\u6b21\u590d\u6742\u5ea6\u5b66\u751f\u6a21\u578b\u7684\u77e5\u8bc6\u84b8\u998f\u53ef\u8fc1\u79fb\u6027\uff0c\u63a2\u8ba8\u4e86\u4e0d\u540c\u67b6\u6784\u7ea6\u675f\u548c\u521d\u59cb\u5316\u7b56\u7565\u5bf9\u84b8\u998f\u8fc7\u7a0b\u7684\u5f71\u54cd\u3002", "motivation": "\u81ea\u6ce8\u610f\u529b\u5728\u63a8\u7406\u65f6\u7684\u4e8c\u6b21\u590d\u6742\u5ea6\u662f\u4e3b\u8981\u74f6\u9888\uff0c\u56e0\u6b64\u63a2\u7d22\u6b21\u4e8c\u6b21\u66ff\u4ee3\u65b9\u6848\uff08\u5982SSMs\u3001\u7ebf\u6027\u6ce8\u610f\u529b\u548c\u5faa\u73af\u67b6\u6784\uff09\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002", "method": "\u7cfb\u7edf\u8bc4\u4f30\u4e86\u4eceTransformer\u6559\u5e08\u5230\u4e5d\u79cd\u6b21\u4e8c\u6b21\u5b66\u751f\u6a21\u578b\u7684\u77e5\u8bc6\u84b8\u998f\uff0c\u5e76\u7814\u7a76\u4e86\u77e9\u9635\u6df7\u5408\u548cQKV\u590d\u5236\u7b49\u521d\u59cb\u5316\u7b56\u7565\u3002", "result": "\u5728\u591a\u4e2aNLP\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5b9e\u8bc1\u7ed3\u679c\u63ed\u793a\u4e86\u6548\u7387\u4e0e\u6027\u80fd\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u5e76\u786e\u5b9a\u4e86\u6210\u529f\u77e5\u8bc6\u8fc1\u79fb\u7684\u5173\u952e\u56e0\u7d20\u3002", "conclusion": "\u7814\u7a76\u4e3a\u6b21\u4e8c\u6b21\u67b6\u6784\u7684\u77e5\u8bc6\u84b8\u998f\u63d0\u4f9b\u4e86\u5b9e\u7528\u89c1\u89e3\uff0c\u5f3a\u8c03\u4e86\u67b6\u6784\u9009\u62e9\u548c\u521d\u59cb\u5316\u7b56\u7565\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2504.13982", "pdf": "https://arxiv.org/pdf/2504.13982", "abs": "https://arxiv.org/abs/2504.13982", "authors": ["Ruoning Zhao", "Xinyun Chen"], "title": "When Machine Learning Meets Importance Sampling: A More Efficient Rare Event Estimation Approach", "categories": ["cs.LG", "math.OC", "math.PR"], "comment": null, "summary": "Driven by applications in telecommunication networks, we explore the\nsimulation task of estimating rare event probabilities for tandem queues in\ntheir steady state. Existing literature has recognized that importance sampling\nmethods can be inefficient, due to the exploding variance of the path-dependent\nlikelihood functions. To mitigate this, we introduce a new importance sampling\napproach that utilizes a marginal likelihood ratio on the stationary\ndistribution, effectively avoiding the issue of excessive variance. In\naddition, we design a machine learning algorithm to estimate this marginal\nlikelihood ratio using importance sampling data. Numerical experiments indicate\nthat our algorithm outperforms the classic importance sampling methods.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u91cd\u8981\u6027\u91c7\u6837\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u7a33\u6001\u5206\u5e03\u7684\u8fb9\u9645\u4f3c\u7136\u6bd4\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u65b9\u5dee\u7206\u70b8\u7684\u95ee\u9898\uff0c\u5e76\u7ed3\u5408\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u4f18\u5316\u4f30\u8ba1\u6548\u679c\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u6e90\u4e8e\u7535\u4fe1\u7f51\u7edc\u4e2d\u5bf9\u4e32\u8054\u961f\u5217\u7a33\u6001\u4e0b\u7f55\u89c1\u4e8b\u4ef6\u6982\u7387\u7684\u4f30\u8ba1\u9700\u6c42\uff0c\u4f20\u7edf\u91cd\u8981\u6027\u91c7\u6837\u65b9\u6cd5\u56e0\u8def\u5f84\u4f9d\u8d56\u4f3c\u7136\u51fd\u6570\u7684\u65b9\u5dee\u7206\u70b8\u800c\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7a33\u6001\u5206\u5e03\u8fb9\u9645\u4f3c\u7136\u6bd4\u7684\u65b0\u91cd\u8981\u6027\u91c7\u6837\u65b9\u6cd5\uff0c\u5e76\u8bbe\u8ba1\u4e86\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u4ece\u91cd\u8981\u6027\u91c7\u6837\u6570\u636e\u4e2d\u4f30\u8ba1\u8be5\u8fb9\u9645\u4f3c\u7136\u6bd4\u3002", "result": "\u6570\u503c\u5b9e\u9a8c\u8868\u660e\uff0c\u65b0\u7b97\u6cd5\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u4f20\u7edf\u91cd\u8981\u6027\u91c7\u6837\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u8fb9\u9645\u4f3c\u7136\u6bd4\u548c\u673a\u5668\u5b66\u4e60\uff0c\u65b0\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u91cd\u8981\u6027\u91c7\u6837\u7684\u6548\u7387\u95ee\u9898\uff0c\u4e3a\u7f55\u89c1\u4e8b\u4ef6\u6982\u7387\u4f30\u8ba1\u63d0\u4f9b\u4e86\u66f4\u4f18\u65b9\u6848\u3002"}}
{"id": "2504.14209", "pdf": "https://arxiv.org/pdf/2504.14209", "abs": "https://arxiv.org/abs/2504.14209", "authors": ["Xiangkai Ma", "Xiaobin Hong", "Wenzhong Li", "Sanglu Lu"], "title": "Pets: General Pattern Assisted Architecture For Time Series Analysis", "categories": ["cs.AI"], "comment": null, "summary": "Time series analysis has found widespread applications in areas such as\nweather forecasting, anomaly detection, and healthcare. However, real-world\nsequential data often exhibit a superimposed state of various fluctuation\npatterns, including hourly, daily, and monthly frequencies. Traditional\ndecomposition techniques struggle to effectively disentangle these multiple\nfluctuation patterns from the seasonal components, making time series analysis\nchallenging. Surpassing the existing multi-period decoupling paradigms, this\npaper introduces a novel perspective based on energy distribution within the\ntemporal-spectrum space. By adaptively quantifying observed sequences into\ncontinuous frequency band intervals, the proposed approach reconstructs\nfluctuation patterns across diverse periods without relying on domain-specific\nprior knowledge. Building upon this innovative strategy, we propose Pets, an\nenhanced architecture that is adaptable to arbitrary model structures. Pets\nintegrates a Fluctuation Pattern Assisted (FPA) module and a Context-Guided\nMixture of Predictors (MoP). The FPA module facilitates information fusion\namong diverse fluctuation patterns by capturing their dependencies and\nprogressively modeling these patterns as latent representations at each layer.\nMeanwhile, the MoP module leverages these compound pattern representations to\nguide and regulate the reconstruction of distinct fluctuations hierarchically.\nPets achieves state-of-the-art performance across various tasks, including\nforecasting, imputation, anomaly detection, and classification, while\ndemonstrating strong generalization and robustness.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u80fd\u91cf\u5206\u5e03\u7684\u65b0\u65b9\u6cd5Pets\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u91cf\u5316\u65f6\u95f4\u5e8f\u5217\u7684\u9891\u7387\u5e26\u533a\u95f4\uff0c\u6709\u6548\u89e3\u8026\u591a\u5468\u671f\u6ce2\u52a8\u6a21\u5f0f\uff0c\u5e76\u7ed3\u5408FPA\u548cMoP\u6a21\u5757\uff0c\u5b9e\u73b0\u4e86\u5728\u9884\u6d4b\u3001\u586b\u8865\u3001\u5f02\u5e38\u68c0\u6d4b\u548c\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u6700\u4f18\u6027\u80fd\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u7684\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u901a\u5e38\u5305\u542b\u591a\u79cd\u6ce2\u52a8\u6a21\u5f0f\uff08\u5982\u5c0f\u65f6\u3001\u65e5\u3001\u6708\u9891\u7387\uff09\uff0c\u4f20\u7edf\u5206\u89e3\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u89e3\u8026\u8fd9\u4e9b\u6a21\u5f0f\uff0c\u5bfc\u81f4\u5206\u6790\u56f0\u96be\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u65f6\u95f4-\u9891\u8c31\u7a7a\u95f4\u80fd\u91cf\u5206\u5e03\u7684\u65b0\u89c6\u89d2\uff0c\u81ea\u9002\u5e94\u91cf\u5316\u5e8f\u5217\u4e3a\u8fde\u7eed\u9891\u7387\u5e26\u533a\u95f4\uff0c\u5e76\u7ed3\u5408FPA\u6a21\u5757\uff08\u6355\u83b7\u4f9d\u8d56\u5173\u7cfb\uff09\u548cMoP\u6a21\u5757\uff08\u5206\u5c42\u91cd\u5efa\u6ce2\u52a8\u6a21\u5f0f\uff09\u3002", "result": "Pets\u5728\u9884\u6d4b\u3001\u586b\u8865\u3001\u5f02\u5e38\u68c0\u6d4b\u548c\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u6700\u4f18\uff0c\u5e76\u5c55\u793a\u4e86\u5f3a\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "Pets\u901a\u8fc7\u521b\u65b0\u65b9\u6cd5\u6709\u6548\u89e3\u8026\u591a\u5468\u671f\u6ce2\u52a8\u6a21\u5f0f\uff0c\u4e3a\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u548c\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.14367", "pdf": "https://arxiv.org/pdf/2504.14367", "abs": "https://arxiv.org/abs/2504.14367", "authors": ["Gabriel Machado Santos", "Rita Maria da Silva Julia", "Marcelo Zanchetta do Nascimento"], "title": "Diverse Prompts: Illuminating the Prompt Space of Large Language Models with MAP-Elites", "categories": ["cs.CL", "cs.AI"], "comment": "8 pages Accepted for publication in IEEE CEC 2025", "summary": "Prompt engineering is essential for optimizing large language models (LLMs),\nyet the link between prompt structures and task performance remains\nunderexplored. This work introduces an evolutionary approach that combines\ncontext-free grammar (CFG) with the MAP-Elites algorithm to systematically\nexplore the prompt space. Our method prioritizes quality and diversity,\ngenerating high-performing and structurally varied prompts while analyzing\ntheir alignment with diverse tasks by varying traits such as the number of\nexamples (shots) and reasoning depth. By systematically mapping the phenotypic\nspace, we reveal how structural variations influence LLM performance, offering\nactionable insights for task-specific and adaptable prompt design. Evaluated on\nseven BigBench Lite tasks across multiple LLMs, our results underscore the\ncritical interplay of quality and diversity, advancing the effectiveness and\nversatility of LLMs.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4e0a\u4e0b\u6587\u65e0\u5173\u8bed\u6cd5\uff08CFG\uff09\u548cMAP-Elites\u7b97\u6cd5\u7684\u8fdb\u5316\u65b9\u6cd5\uff0c\u7cfb\u7edf\u63a2\u7d22\u63d0\u793a\u7a7a\u95f4\uff0c\u4ee5\u4f18\u5316\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u6027\u80fd\u3002", "motivation": "\u63d0\u793a\u5de5\u7a0b\u5bf9\u4f18\u5316LLMs\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u63d0\u793a\u7ed3\u6784\u4e0e\u4efb\u52a1\u6027\u80fd\u4e4b\u95f4\u7684\u5173\u7cfb\u5c1a\u672a\u5145\u5206\u7814\u7a76\u3002", "method": "\u91c7\u7528CFG\u4e0eMAP-Elites\u7b97\u6cd5\u7ed3\u5408\u7684\u65b9\u6cd5\uff0c\u751f\u6210\u9ad8\u8d28\u91cf\u4e14\u591a\u6837\u5316\u7684\u63d0\u793a\uff0c\u5e76\u5206\u6790\u5176\u4e0e\u4e0d\u540c\u4efb\u52a1\u7684\u5339\u914d\u6027\u3002", "result": "\u5728\u591a\u4e2aLLMs\u548c\u4e03\u4e2aBigBench Lite\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u63ed\u793a\u63d0\u793a\u7ed3\u6784\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u3002", "conclusion": "\u901a\u8fc7\u7cfb\u7edf\u6620\u5c04\u8868\u578b\u7a7a\u95f4\uff0c\u8be5\u65b9\u6cd5\u4e3a\u4efb\u52a1\u7279\u5b9a\u548c\u9002\u5e94\u6027\u63d0\u793a\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u5b9e\u7528\u89c1\u89e3\uff0c\u63d0\u5347\u4e86LLMs\u7684\u6548\u80fd\u548c\u591a\u529f\u80fd\u6027\u3002"}}
{"id": "2504.13983", "pdf": "https://arxiv.org/pdf/2504.13983", "abs": "https://arxiv.org/abs/2504.13983", "authors": ["Hamideh-Sadat Fazael-Ardakani", "Hamid Soltanian-Zadeh"], "title": "QuatE-D: A Distance-Based Quaternion Model for Knowledge Graph Embedding", "categories": ["cs.LG"], "comment": null, "summary": "Knowledge graph embedding (KGE) methods aim to represent entities and\nrelations in a continuous space while preserving their structural and semantic\nproperties. Quaternion-based KGEs have demonstrated strong potential in\ncapturing complex relational patterns. In this work, we propose QuatE-D, a\nnovel quaternion-based model that employs a distance-based scoring function\ninstead of traditional inner-product approaches. By leveraging Euclidean\ndistance, QuatE-D enhances interpretability and provides a more flexible\nrepresentation of relational structures. Experimental results demonstrate that\nQuatE-D achieves competitive performance while maintaining an efficient\nparameterization, particularly excelling in Mean Rank reduction. These findings\nhighlight the effectiveness of distance-based scoring in quaternion embeddings,\noffering a promising direction for knowledge graph completion.", "AI": {"tldr": "QuatE-D\u662f\u4e00\u79cd\u57fa\u4e8e\u56db\u5143\u6570\u7684\u77e5\u8bc6\u56fe\u8c31\u5d4c\u5165\u65b9\u6cd5\uff0c\u91c7\u7528\u8ddd\u79bb\u8bc4\u5206\u51fd\u6570\uff0c\u4f18\u4e8e\u4f20\u7edf\u5185\u79ef\u65b9\u6cd5\uff0c\u6027\u80fd\u9ad8\u6548\u4e14\u53ef\u89e3\u91ca\u6027\u5f3a\u3002", "motivation": "\u4f20\u7edf\u5185\u79ef\u65b9\u6cd5\u5728\u77e5\u8bc6\u56fe\u8c31\u5d4c\u5165\u4e2d\u8868\u73b0\u6709\u9650\uff0c\u9700\u8981\u66f4\u7075\u6d3b\u4e14\u53ef\u89e3\u91ca\u7684\u6a21\u578b\u3002", "method": "\u63d0\u51faQuatE-D\u6a21\u578b\uff0c\u5229\u7528\u6b27\u51e0\u91cc\u5f97\u8ddd\u79bb\u4f5c\u4e3a\u8bc4\u5206\u51fd\u6570\uff0c\u66ff\u4ee3\u4f20\u7edf\u5185\u79ef\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u663e\u793aQuatE-D\u6027\u80fd\u4f18\u8d8a\uff0c\u5c24\u5176\u5728\u964d\u4f4e\u5e73\u5747\u6392\u540d\u65b9\u9762\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "\u8ddd\u79bb\u8bc4\u5206\u51fd\u6570\u5728\u56db\u5143\u6570\u5d4c\u5165\u4e2d\u6548\u679c\u663e\u8457\uff0c\u4e3a\u77e5\u8bc6\u56fe\u8c31\u8865\u5168\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2504.14232", "pdf": "https://arxiv.org/pdf/2504.14232", "abs": "https://arxiv.org/abs/2504.14232", "authors": ["Antoun Yaacoub", "J\u00e9r\u00f4me Da-Rugna", "Zainab Assaghir"], "title": "Assessing AI-Generated Questions' Alignment with Cognitive Frameworks in Educational Assessment", "categories": ["cs.AI", "cs.CL"], "comment": "This paper was presented in the 17th Int. Conf. on Computer Science\n  and Information Technology (ICCSIT 2024), Dubai, United Arab Emirates, 2024,\n  Oct. 23-25. IT's now in production to be published in the International\n  Journal of Computer Theory and Engineering", "summary": "This study evaluates the integration of Bloom's Taxonomy into OneClickQuiz,\nan Artificial Intelligence (AI) driven plugin for automating Multiple-Choice\nQuestion (MCQ) generation in Moodle. Bloom's Taxonomy provides a structured\nframework for categorizing educational objectives into hierarchical cognitive\nlevels. Our research investigates whether incorporating this taxonomy can\nimprove the alignment of AI-generated questions with specific cognitive\nobjectives. We developed a dataset of 3691 questions categorized according to\nBloom's levels and employed various classification models-Multinomial Logistic\nRegression, Naive Bayes, Linear Support Vector Classification (SVC), and a\nTransformer-based model (DistilBERT)-to evaluate their effectiveness in\ncategorizing questions. Our results indicate that higher Bloom's levels\ngenerally correlate with increased question length, Flesch-Kincaid Grade Level\n(FKGL), and Lexical Density (LD), reflecting the increased complexity of higher\ncognitive demands. Multinomial Logistic Regression showed varying accuracy\nacross Bloom's levels, performing best for \"Knowledge\" and less accurately for\nhigher-order levels. Merging higher-level categories improved accuracy for\ncomplex cognitive tasks. Naive Bayes and Linear SVC also demonstrated effective\nclassification for lower levels but struggled with higher-order tasks.\nDistilBERT achieved the highest performance, significantly improving\nclassification of both lower and higher-order cognitive levels, achieving an\noverall validation accuracy of 91%. This study highlights the potential of\nintegrating Bloom's Taxonomy into AI-driven assessment tools and underscores\nthe advantages of advanced models like DistilBERT for enhancing educational\ncontent generation.", "AI": {"tldr": "\u7814\u7a76\u8bc4\u4f30\u4e86\u5c06\u5e03\u9c81\u59c6\u5206\u7c7b\u6cd5\u96c6\u6210\u5230AI\u9a71\u52a8\u7684Moodle\u63d2\u4ef6OneClickQuiz\u4e2d\uff0c\u4ee5\u6539\u8fdb\u591a\u9009\u9898\u751f\u6210\u4e0e\u8ba4\u77e5\u76ee\u6807\u7684\u5339\u914d\u3002\u7ed3\u679c\u663e\u793a\uff0c\u9ad8\u7ea7\u8ba4\u77e5\u5c42\u6b21\u7684\u95ee\u9898\u66f4\u590d\u6742\uff0cDistilBERT\u6a21\u578b\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u63a2\u8ba8\u5e03\u9c81\u59c6\u5206\u7c7b\u6cd5\u662f\u5426\u80fd\u63d0\u5347AI\u751f\u6210\u95ee\u9898\u4e0e\u7279\u5b9a\u8ba4\u77e5\u76ee\u6807\u7684\u5339\u914d\u5ea6\u3002", "method": "\u4f7f\u75283691\u4e2a\u6309\u5e03\u9c81\u59c6\u5c42\u6b21\u5206\u7c7b\u7684\u95ee\u9898\u6570\u636e\u96c6\uff0c\u6d4b\u8bd5\u591a\u79cd\u5206\u7c7b\u6a21\u578b\uff08\u5982\u903b\u8f91\u56de\u5f52\u3001\u6734\u7d20\u8d1d\u53f6\u65af\u3001\u7ebf\u6027SVC\u548cDistilBERT\uff09\u3002", "result": "\u9ad8\u7ea7\u5c42\u6b21\u95ee\u9898\u66f4\u590d\u6742\uff0cDistilBERT\u8868\u73b0\u6700\u4f73\uff08\u9a8c\u8bc1\u51c6\u786e\u738791%\uff09\u3002", "conclusion": "\u5e03\u9c81\u59c6\u5206\u7c7b\u6cd5\u96c6\u6210AI\u5de5\u5177\u6709\u6f5c\u529b\uff0c\u9ad8\u7ea7\u6a21\u578b\u5982DistilBERT\u80fd\u663e\u8457\u63d0\u5347\u6559\u80b2\u5185\u5bb9\u751f\u6210\u8d28\u91cf\u3002"}}
{"id": "2504.14452", "pdf": "https://arxiv.org/pdf/2504.14452", "abs": "https://arxiv.org/abs/2504.14452", "authors": ["Tong Chen", "Faeze Brahman", "Jiacheng Liu", "Niloofar Mireshghallah", "Weijia Shi", "Pang Wei Koh", "Luke Zettlemoyer", "Hannaneh Hajishirzi"], "title": "ParaPO: Aligning Language Models to Reduce Verbatim Reproduction of Pre-training Data", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Language models (LMs) can memorize and reproduce segments from their\npretraining data verbatim even in non-adversarial settings, raising concerns\nabout copyright, plagiarism, privacy, and creativity. We introduce Paraphrase\nPreference Optimization (ParaPO), a post-training method that fine-tunes LMs to\nreduce unintentional regurgitation while preserving their overall utility.\nParaPO trains LMs to prefer paraphrased versions of memorized segments over the\noriginal verbatim content from the pretraining data. To maintain the ability to\nrecall famous quotations when appropriate, we develop a variant of ParaPO that\nuses system prompts to control regurgitation behavior. In our evaluation on\nLlama3.1-8B, ParaPO consistently reduces regurgitation across all tested\ndatasets (e.g., reducing the regurgitation metric from 17.3 to 12.9 in creative\nwriting), whereas unlearning methods used in prior work to mitigate\nregurgitation are less effective outside their targeted unlearned domain (from\n17.3 to 16.9). When applied to the instruction-tuned Tulu3-8B model, ParaPO\nwith system prompting successfully preserves famous quotation recall while\nreducing unintentional regurgitation (from 8.7 to 6.3 in creative writing) when\nprompted not to regurgitate. In contrast, without ParaPO tuning, prompting the\nmodel not to regurgitate produces only a marginal reduction (8.7 to 8.4).", "AI": {"tldr": "ParaPO\u662f\u4e00\u79cd\u540e\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u5fae\u8c03\u8bed\u8a00\u6a21\u578b\u4ee5\u51cf\u5c11\u65e0\u610f\u4e2d\u7684\u8bb0\u5fc6\u5185\u5bb9\u590d\u73b0\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u5b9e\u7528\u6027\u3002", "motivation": "\u8bed\u8a00\u6a21\u578b\u53ef\u80fd\u65e0\u610f\u4e2d\u590d\u73b0\u9884\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u7247\u6bb5\uff0c\u5f15\u53d1\u7248\u6743\u3001\u9690\u79c1\u7b49\u95ee\u9898\u3002", "method": "ParaPO\u901a\u8fc7\u8bad\u7ec3\u6a21\u578b\u504f\u597d\u8bb0\u5fc6\u5185\u5bb9\u7684\u6539\u5199\u7248\u672c\uff0c\u5e76\u7ed3\u5408\u7cfb\u7edf\u63d0\u793a\u63a7\u5236\u590d\u73b0\u884c\u4e3a\u3002", "result": "\u5728Llama3.1-8B\u548cTulu3-8B\u4e0a\uff0cParaPO\u663e\u8457\u51cf\u5c11\u4e86\u590d\u73b0\u73b0\u8c61\uff0c\u540c\u65f6\u4fdd\u7559\u4e86\u5f15\u7528\u540d\u53e5\u7684\u80fd\u529b\u3002", "conclusion": "ParaPO\u5728\u51cf\u5c11\u8bb0\u5fc6\u5185\u5bb9\u590d\u73b0\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u4e14\u80fd\u901a\u8fc7\u7cfb\u7edf\u63d0\u793a\u7075\u6d3b\u63a7\u5236\u884c\u4e3a\u3002"}}
{"id": "2504.13984", "pdf": "https://arxiv.org/pdf/2504.13984", "abs": "https://arxiv.org/abs/2504.13984", "authors": ["Amrit Diggavi Seshadri"], "title": "One Jump Is All You Need: Short-Cutting Transformers for Early Exit Prediction with One Jump to Fit All Exit Levels", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "To reduce the time and computational costs of inference of large language\nmodels, there has been interest in parameter-efficient low-rank early-exit\ncasting of transformer hidden-representations to final-representations. Such\nlow-rank short-cutting has been shown to outperform identity shortcuts at early\nmodel stages while offering parameter-efficiency in shortcut jumps. However,\ncurrent low-rank methods maintain a separate early-exit shortcut jump to\nfinal-representations for each transformer intermediate block-level during\ninference. In this work, we propose selection of a single One-Jump-Fits-All\n(OJFA) low-rank shortcut that offers over a 30x reduction in shortcut parameter\ncosts during inference. We show that despite this extreme reduction, our OJFA\nchoice largely matches the performance of maintaining multiple shortcut jumps\nduring inference and offers stable precision from all transformer block-levels\nfor GPT2-XL, Phi3-Mini and Llama2-7B transformer models.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aOJFA\u7684\u4f4e\u79e9\u6377\u5f84\u65b9\u6cd5\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u63a8\u7406\u65f6\u7684\u53c2\u6570\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6027\u80fd\u3002", "motivation": "\u51cf\u5c11\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u7684\u65f6\u95f4\u548c\u8ba1\u7b97\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u6027\u80fd\u3002", "method": "\u9009\u62e9\u5355\u4e00\u7684\u4f4e\u79e9\u6377\u5f84\uff08OJFA\uff09\uff0c\u66ff\u4ee3\u4f20\u7edf\u7684\u591a\u6377\u5f84\u65b9\u6cd5\uff0c\u5b9e\u73b0\u53c2\u6570\u6548\u7387\u3002", "result": "OJFA\u65b9\u6cd5\u5728GPT2-XL\u3001Phi3-Mini\u548cLlama2-7B\u6a21\u578b\u4e0a\u8868\u73b0\u7a33\u5b9a\uff0c\u53c2\u6570\u6210\u672c\u964d\u4f4e30\u500d\u3002", "conclusion": "OJFA\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u6027\u80fd\u7a33\u5b9a\u7684\u4f4e\u79e9\u6377\u5f84\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u591a\u79cdTransformer\u6a21\u578b\u3002"}}
{"id": "2504.14239", "pdf": "https://arxiv.org/pdf/2504.14239", "abs": "https://arxiv.org/abs/2504.14239", "authors": ["Yuhang Liu", "Pengxiang Li", "Congkai Xie", "Xavier Hu", "Xiaotian Han", "Shengyu Zhang", "Hongxia Yang", "Fei Wu"], "title": "InfiGUI-R1: Advancing Multimodal GUI Agents from Reactive Actors to Deliberative Reasoners", "categories": ["cs.AI", "cs.CL"], "comment": "10 pages, 3 figures, work in progress", "summary": "Multimodal Large Language Models (MLLMs) have powered Graphical User\nInterface (GUI) Agents, showing promise in automating tasks on computing\ndevices. Recent works have begun exploring reasoning in GUI tasks with\nencouraging results. However, many current approaches rely on manually designed\nreasoning templates, which may result in reasoning that is not sufficiently\nrobust and adaptive for complex GUI environments. Meanwhile, some existing\nagents continue to operate as Reactive Actors, relying primarily on implicit\nreasoning that may lack sufficient depth for GUI tasks demanding planning and\nerror recovery. We argue that advancing these agents requires a shift from\nreactive acting towards acting based on deliberate reasoning. To facilitate\nthis transformation, we introduce InfiGUI-R1, an MLLM-based GUI agent developed\nthrough our Actor2Reasoner framework, a reasoning-centric, two-stage training\napproach designed to progressively evolve agents from Reactive Actors to\nDeliberative Reasoners. The first stage, Reasoning Injection, focuses on\nestablishing a basic reasoner. We employ Spatial Reasoning Distillation to\ntransfer cross-modal spatial reasoning capabilities from teacher models to\nMLLMs through trajectories with explicit reasoning steps, enabling models to\nintegrate GUI visual-spatial information with logical reasoning before action\ngeneration. The second stage, Deliberation Enhancement, refines the basic\nreasoner into a deliberative one using Reinforcement Learning. This stage\nintroduces two approaches: Sub-goal Guidance, which rewards models for\ngenerating accurate intermediate sub-goals, and Error Recovery Scenario\nConstruction, which creates failure-and-recovery training scenarios from\nidentified prone-to-error steps. Experimental results show InfiGUI-R1 achieves\nstrong performance in GUI grounding and trajectory tasks. Resources at\nhttps://github.com/Reallm-Labs/InfiGUI-R1.", "AI": {"tldr": "InfiGUI-R1\u662f\u4e00\u4e2a\u57fa\u4e8eMLLM\u7684GUI\u4ee3\u7406\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\u6846\u67b6Actor2Reasoner\uff0c\u4ece\u53cd\u5e94\u5f0f\u6267\u884c\u8005\u8f6c\u53d8\u4e3a\u6df1\u601d\u719f\u8651\u7684\u63a8\u7406\u8005\uff0c\u63d0\u5347\u4e86GUI\u4efb\u52a1\u7684\u9c81\u68d2\u6027\u548c\u9002\u5e94\u6027\u3002", "motivation": "\u5f53\u524dGUI\u4ee3\u7406\u4f9d\u8d56\u624b\u52a8\u8bbe\u8ba1\u7684\u63a8\u7406\u6a21\u677f\u6216\u9690\u5f0f\u63a8\u7406\uff0c\u7f3a\u4e4f\u5bf9\u590d\u6742GUI\u73af\u5883\u7684\u9002\u5e94\u6027\u548c\u6df1\u5ea6\u89c4\u5212\u80fd\u529b\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\uff1a1. \u63a8\u7406\u6ce8\u5165\uff08Spatial Reasoning Distillation\uff09\uff1b2. \u6df1\u601d\u719f\u8651\u589e\u5f3a\uff08\u5f3a\u5316\u5b66\u4e60\uff0c\u5305\u62ec\u5b50\u76ee\u6807\u5f15\u5bfc\u548c\u9519\u8bef\u6062\u590d\u573a\u666f\u6784\u5efa\uff09\u3002", "result": "\u5b9e\u9a8c\u8868\u660eInfiGUI-R1\u5728GUI\u57fa\u7840\u548c\u8f68\u8ff9\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u901a\u8fc7Actor2Reasoner\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86GUI\u4ee3\u7406\u4ece\u53cd\u5e94\u5f0f\u5230\u6df1\u601d\u719f\u8651\u63a8\u7406\u7684\u8f6c\u53d8\uff0c\u63d0\u5347\u4e86\u4efb\u52a1\u6027\u80fd\u3002"}}
{"id": "2504.14462", "pdf": "https://arxiv.org/pdf/2504.14462", "abs": "https://arxiv.org/abs/2504.14462", "authors": ["Armin Toroghi", "Willis Guo", "Scott Sanner"], "title": "CoLoTa: A Dataset for Entity-based Commonsense Reasoning over Long-Tail Knowledge", "categories": ["cs.CL"], "comment": null, "summary": "The rise of Large Language Models (LLMs) has redefined the AI landscape,\nparticularly due to their ability to encode factual and commonsense knowledge,\nand their outstanding performance in tasks requiring reasoning. Despite these\nadvances, hallucinations and reasoning errors remain a significant barrier to\ntheir deployment in high-stakes settings. In this work, we observe that even\nthe most prominent LLMs, such as OpenAI-o1, suffer from high rates of reasoning\nerrors and hallucinations on tasks requiring commonsense reasoning over\nobscure, long-tail entities. To investigate this limitation, we present a new\ndataset for Commonsense reasoning over Long-Tail entities (CoLoTa), that\nconsists of 3,300 queries from question answering and claim verification tasks\nand covers a diverse range of commonsense reasoning skills. We remark that\nCoLoTa can also serve as a Knowledge Graph Question Answering (KGQA) dataset\nsince the support of knowledge required to answer its queries is present in the\nWikidata knowledge graph. However, as opposed to existing KGQA benchmarks that\nmerely focus on factoid questions, our CoLoTa queries also require commonsense\nreasoning. Our experiments with strong LLM-based KGQA methodologies indicate\ntheir severe inability to answer queries involving commonsense reasoning.\nHence, we propose CoLoTa as a novel benchmark for assessing both (i) LLM\ncommonsense reasoning capabilities and their robustness to hallucinations on\nlong-tail entities and (ii) the commonsense reasoning capabilities of KGQA\nmethods.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aCoLoTa\u7684\u65b0\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5904\u7406\u957f\u5c3e\u5b9e\u4f53\u65f6\u7684\u5e38\u8bc6\u63a8\u7406\u80fd\u529b\u53ca\u5176\u5e7b\u89c9\u95ee\u9898\uff0c\u540c\u65f6\u4e5f\u53ef\u4f5c\u4e3a\u77e5\u8bc6\u56fe\u8c31\u95ee\u7b54\uff08KGQA\uff09\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002", "motivation": "\u5c3d\u7ba1LLMs\u5728\u7f16\u7801\u4e8b\u5b9e\u548c\u5e38\u8bc6\u77e5\u8bc6\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u5904\u7406\u957f\u5c3e\u5b9e\u4f53\u7684\u5e38\u8bc6\u63a8\u7406\u4efb\u52a1\u65f6\u4ecd\u5b58\u5728\u9ad8\u9519\u8bef\u7387\u548c\u5e7b\u89c9\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5176\u5728\u9ad8\u98ce\u9669\u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u4f5c\u8005\u6784\u5efa\u4e86\u5305\u542b3,300\u4e2a\u67e5\u8be2\u7684CoLoTa\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u95ee\u7b54\u548c\u58f0\u660e\u9a8c\u8bc1\u4efb\u52a1\uff0c\u5e76\u6d89\u53ca\u591a\u79cd\u5e38\u8bc6\u63a8\u7406\u6280\u80fd\u3002\u6570\u636e\u96c6\u652f\u6301\u77e5\u8bc6\u56fe\u8c31\u95ee\u7b54\uff0c\u4f46\u4e0d\u540c\u4e8e\u73b0\u6709\u57fa\u51c6\uff0c\u5b83\u5f3a\u8c03\u5e38\u8bc6\u63a8\u7406\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u73b0\u6709\u7684\u57fa\u4e8eLLM\u7684KGQA\u65b9\u6cd5\u5728\u5904\u7406\u6d89\u53ca\u5e38\u8bc6\u63a8\u7406\u7684\u67e5\u8be2\u65f6\u8868\u73b0\u4e25\u91cd\u4e0d\u8db3\u3002", "conclusion": "CoLoTa\u53ef\u4f5c\u4e3a\u8bc4\u4f30LLMs\u548cKGQA\u65b9\u6cd5\u5728\u5e38\u8bc6\u63a8\u7406\u80fd\u529b\u53ca\u6297\u5e7b\u89c9\u6027\u80fd\u4e0a\u7684\u65b0\u57fa\u51c6\u3002"}}
{"id": "2504.13989", "pdf": "https://arxiv.org/pdf/2504.13989", "abs": "https://arxiv.org/abs/2504.13989", "authors": ["Lucas Maisonnave", "Cyril Moineau", "Olivier Bichler", "Fabrice Rastello"], "title": "Gradual Binary Search and Dimension Expansion : A general method for activation quantization in LLMs", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) have become pivotal in artificial intelligence,\ndemonstrating strong capabilities in reasoning, understanding, and generating\ndata. However, their deployment on edge devices is hindered by their\nsubstantial size, often reaching several billion parameters. Quantization is a\nwidely used method to reduce memory usage and inference time, however LLMs\npresent unique challenges due to the prevalence of outliers in their\nactivations. In this work, we leverage the theoretical advantages of Hadamard\nmatrices over random rotation matrices to push the boundaries of quantization\nin LLMs. We demonstrate that Hadamard matrices are more effective in reducing\noutliers, which are a significant obstacle in achieving low-bit quantization.\nOur method based on a gradual binary search enables 3-bit quantization for\nweights, activations, and key-value (KV) caches, resulting in a 40\\% increase\nin accuracy on common benchmarks compared to SoTA methods. We extend the use of\nrotation matrices to support non-power-of-2 embedding dimensions, similar to\nthe Qwen architecture, by employing the Paley algorithm. We theoretically\ndemonstrates the superiority of Hadamard matrices in reducing outliers.We\nachieved 3-bit quantization for weights, activations, and KV cache,\nsignificantly enhancing model performance. Our experimental results on multiple\nmodels family like Mistral, LLaMA, and Qwen demonstrate the effectiveness of\nour approach, outperforming existing methods and enabling practical 3-bit\nquantization.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eHadamard\u77e9\u9635\u7684\u91cf\u5316\u65b9\u6cd5\uff0c\u6709\u6548\u51cf\u5c11LLMs\u4e2d\u7684\u5f02\u5e38\u503c\uff0c\u5b9e\u73b0\u4e863\u4f4d\u91cf\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u90e8\u7f72\u53d7\u5230\u5176\u5e9e\u5927\u53c2\u6570\u89c4\u6a21\u7684\u9650\u5236\uff0c\u91cf\u5316\u662f\u51cf\u5c11\u5185\u5b58\u548c\u63a8\u7406\u65f6\u95f4\u7684\u5e38\u7528\u65b9\u6cd5\uff0c\u4f46LLMs\u4e2d\u7684\u5f02\u5e38\u503c\u95ee\u9898\u963b\u788d\u4e86\u4f4e\u6bd4\u7279\u91cf\u5316\u3002", "method": "\u5229\u7528Hadamard\u77e9\u9635\u7684\u7406\u8bba\u4f18\u52bf\uff0c\u63d0\u51fa\u4e00\u79cd\u6e10\u8fdb\u5f0f\u4e8c\u8fdb\u5236\u641c\u7d22\u65b9\u6cd5\uff0c\u652f\u6301\u975e2\u7684\u5e42\u6b21\u5d4c\u5165\u7ef4\u5ea6\uff0c\u5b9e\u73b0\u4e86\u5bf9\u6743\u91cd\u3001\u6fc0\u6d3b\u548cKV\u7f13\u5b58\u76843\u4f4d\u91cf\u5316\u3002", "result": "\u5728Mistral\u3001LLaMA\u548cQwen\u7b49\u6a21\u578b\u4e0a\uff0c\u8be5\u65b9\u6cd5\u6bd4\u73b0\u6709\u6280\u672f\u63d0\u9ad8\u4e8640%\u7684\u51c6\u786e\u7387\uff0c\u5e76\u6210\u529f\u5b9e\u73b0\u4e863\u4f4d\u91cf\u5316\u3002", "conclusion": "Hadamard\u77e9\u9635\u5728\u51cf\u5c11\u5f02\u5e38\u503c\u548c\u5b9e\u73b0\u4f4e\u6bd4\u7279\u91cf\u5316\u65b9\u9762\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u4e3aLLMs\u7684\u90e8\u7f72\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.14241", "pdf": "https://arxiv.org/pdf/2504.14241", "abs": "https://arxiv.org/abs/2504.14241", "authors": ["Chengming Wang", "Dongyao Jia", "Wei Wang", "Dong Ngoduy", "Bei Peng", "Jianping Wang"], "title": "A Knowledge-Informed Deep Learning Paradigm for Generalizable and Stability-Optimized Car-Following Models", "categories": ["cs.AI", "cs.RO"], "comment": null, "summary": "Car-following models (CFMs) are fundamental to traffic flow analysis and\nautonomous driving. Although calibrated physics-based and trained data-driven\nCFMs can replicate human driving behavior, their reliance on specific datasets\nlimits generalization across diverse scenarios and reduces reliability in\nreal-world deployment. Moreover, these models typically focus on behavioral\nfidelity and do not support the explicit optimization of local and string\nstability, which are increasingly important for the safe and efficient\noperation of autonomous vehicles (AVs). To address these limitations, we\npropose a Knowledge-Informed Deep Learning (KIDL) paradigm that distills the\ngeneralization capabilities of pre-trained Large Language Models (LLMs) into a\nlightweight and stability-aware neural architecture. LLMs are used to extract\nfundamental car-following knowledge beyond dataset-specific patterns, and this\nknowledge is transferred to a reliable, tractable, and computationally\nefficient model through knowledge distillation. KIDL also incorporates\nstability constraints directly into its training objective, ensuring that the\nresulting model not only emulates human-like behavior but also satisfies the\nlocal and string stability requirements essential for real-world AV deployment.\nWe evaluate KIDL on the real-world NGSIM and HighD datasets, comparing its\nperformance with representative physics-based, data-driven, and hybrid CFMs.\nBoth empirical and theoretical results consistently demonstrate KIDL's superior\nbehavioral generalization and traffic flow stability, offering a robust and\nscalable solution for next-generation traffic systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u77e5\u8bc6\u84b8\u998f\u7684\u6df1\u5ea6\u5b66\u4e60\u8303\u5f0f\uff08KIDL\uff09\uff0c\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u8ddf\u8f66\u6a21\u578b\uff08CFMs\uff09\u5728\u6cdb\u5316\u548c\u7a33\u5b9a\u6027\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u4f20\u7edfCFMs\u4f9d\u8d56\u7279\u5b9a\u6570\u636e\u96c6\uff0c\u6cdb\u5316\u80fd\u529b\u6709\u9650\u4e14\u7f3a\u4e4f\u7a33\u5b9a\u6027\u4f18\u5316\uff0c\u96be\u4ee5\u6ee1\u8db3\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\uff08AVs\uff09\u7684\u5b89\u5168\u9700\u6c42\u3002", "method": "\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u5c06LLMs\u63d0\u53d6\u7684\u901a\u7528\u77e5\u8bc6\u8fc1\u79fb\u5230\u8f7b\u91cf\u7ea7\u795e\u7ecf\u7f51\u7edc\u4e2d\uff0c\u5e76\u5728\u8bad\u7ec3\u76ee\u6807\u4e2d\u76f4\u63a5\u52a0\u5165\u7a33\u5b9a\u6027\u7ea6\u675f\u3002", "result": "\u5728NGSIM\u548cHighD\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0cKIDL\u5728\u884c\u4e3a\u6cdb\u5316\u548c\u4ea4\u901a\u6d41\u7a33\u5b9a\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002", "conclusion": "KIDL\u4e3a\u4e0b\u4e00\u4ee3\u4ea4\u901a\u7cfb\u7edf\u63d0\u4f9b\u4e86\u7a33\u5065\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.14468", "pdf": "https://arxiv.org/pdf/2504.14468", "abs": "https://arxiv.org/abs/2504.14468", "authors": ["Yijun Liu"], "title": "sEEG-based Encoding for Sentence Retrieval: A Contrastive Learning Approach to Brain-Language Alignment", "categories": ["cs.CL", "cs.LG", "eess.SP", "q-bio.NC"], "comment": "Accepted for poster presentation at the CVPR 2025 Workshop on\n  Multimodal Foundation Models (MMFM3)", "summary": "Interpreting neural activity through meaningful latent representations\nremains a complex and evolving challenge at the intersection of neuroscience\nand artificial intelligence. We investigate the potential of multimodal\nfoundation models to align invasive brain recordings with natural language. We\npresent SSENSE, a contrastive learning framework that projects single-subject\nstereo-electroencephalography (sEEG) signals into the sentence embedding space\nof a frozen CLIP model, enabling sentence-level retrieval directly from brain\nactivity. SSENSE trains a neural encoder on spectral representations of sEEG\nusing InfoNCE loss, without fine-tuning the text encoder. We evaluate our\nmethod on time-aligned sEEG and spoken transcripts from a naturalistic\nmovie-watching dataset. Despite limited data, SSENSE achieves promising\nresults, demonstrating that general-purpose language representations can serve\nas effective priors for neural decoding.", "AI": {"tldr": "SSENSE\u6846\u67b6\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u5c06sEEG\u4fe1\u53f7\u6620\u5c04\u5230CLIP\u6a21\u578b\u7684\u53e5\u5b50\u5d4c\u5165\u7a7a\u95f4\uff0c\u5b9e\u73b0\u4ece\u8111\u6d3b\u52a8\u76f4\u63a5\u68c0\u7d22\u53e5\u5b50\u3002", "motivation": "\u63a2\u7d22\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u5728\u795e\u7ecf\u79d1\u5b66\u4e0eAI\u4ea4\u53c9\u9886\u57df\u7684\u6f5c\u529b\uff0c\u5c06\u4fb5\u5165\u6027\u8111\u8bb0\u5f55\u4e0e\u81ea\u7136\u8bed\u8a00\u5bf9\u9f50\u3002", "method": "\u4f7f\u7528InfoNCE\u635f\u5931\u5728sEEG\u9891\u8c31\u8868\u793a\u4e0a\u8bad\u7ec3\u795e\u7ecf\u7f16\u7801\u5668\uff0c\u4e0d\u5fae\u8c03\u6587\u672c\u7f16\u7801\u5668\u3002", "result": "\u5728\u6709\u9650\u6570\u636e\u4e0b\uff0cSSENSE\u5728\u7535\u5f71\u89c2\u770b\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u8bc1\u660e\u901a\u7528\u8bed\u8a00\u8868\u5f81\u53ef\u4f5c\u4e3a\u795e\u7ecf\u89e3\u7801\u7684\u6709\u6548\u5148\u9a8c\u3002", "conclusion": "\u901a\u7528\u8bed\u8a00\u8868\u5f81\u80fd\u6709\u6548\u652f\u6301\u795e\u7ecf\u89e3\u7801\uff0cSSENSE\u4e3a\u8111\u6d3b\u52a8\u4e0e\u8bed\u8a00\u5bf9\u9f50\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2504.13990", "pdf": "https://arxiv.org/pdf/2504.13990", "abs": "https://arxiv.org/abs/2504.13990", "authors": ["M. Humayun Kabir", "Md. Ali Hasan", "Md. Shafiqul Islam", "Kyeongjun Ko", "Wonjae Shin"], "title": "PC-DeepNet: A GNSS Positioning Error Minimization Framework Using Permutation-Invariant Deep Neural Network", "categories": ["cs.LG", "cs.AI", "cs.SY", "eess.SY"], "comment": "31 pages, 14 figures, 6 tables", "summary": "Global navigation satellite systems (GNSS) face significant challenges in\nurban and sub-urban areas due to non-line-of-sight (NLOS) propagation,\nmultipath effects, and low received power levels, resulting in highly\nnon-linear and non-Gaussian measurement error distributions. In light of this,\nconventional model-based positioning approaches, which rely on Gaussian error\napproximations, struggle to achieve precise localization under these\nconditions. To overcome these challenges, we put forth a novel learning-based\nframework, PC-DeepNet, that employs a permutation-invariant (PI) deep neural\nnetwork (DNN) to estimate position corrections (PC). This approach is designed\nto ensure robustness against changes in the number and/or order of visible\nsatellite measurements, a common issue in GNSS systems, while leveraging NLOS\nand multipath indicators as features to enhance positioning accuracy in\nchallenging urban and sub-urban environments. To validate the performance of\nthe proposed framework, we compare the positioning error with state-of-the-art\nmodel-based and learning-based positioning methods using two publicly available\ndatasets. The results confirm that proposed PC-DeepNet achieves superior\naccuracy than existing model-based and learning-based methods while exhibiting\nlower computational complexity compared to previous learning-based approaches.", "AI": {"tldr": "PC-DeepNet\u662f\u4e00\u79cd\u57fa\u4e8e\u5b66\u4e60\u7684GNSS\u5b9a\u4f4d\u6846\u67b6\uff0c\u901a\u8fc7PI-DNN\u4f30\u8ba1\u4f4d\u7f6e\u4fee\u6b63\uff0c\u89e3\u51b3\u4e86\u57ce\u5e02\u548c\u90ca\u533a\u73af\u5883\u4e2dNLOS\u548c\u591a\u5f84\u6548\u5e94\u5bfc\u81f4\u7684\u975e\u9ad8\u65af\u8bef\u5dee\u95ee\u9898\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "GNSS\u5728\u57ce\u5e02\u548c\u90ca\u533a\u73af\u5883\u4e2d\u56e0NLOS\u4f20\u64ad\u3001\u591a\u5f84\u6548\u5e94\u548c\u4f4e\u63a5\u6536\u529f\u7387\u5bfc\u81f4\u8bef\u5dee\u5206\u5e03\u590d\u6742\uff0c\u4f20\u7edf\u57fa\u4e8e\u9ad8\u65af\u8bef\u5dee\u7684\u5b9a\u4f4d\u65b9\u6cd5\u7cbe\u5ea6\u4e0d\u8db3\u3002", "method": "\u63d0\u51faPC-DeepNet\u6846\u67b6\uff0c\u4f7f\u7528PI-DNN\u4f30\u8ba1\u4f4d\u7f6e\u4fee\u6b63\uff0c\u5229\u7528NLOS\u548c\u591a\u5f84\u6307\u6807\u4f5c\u4e3a\u7279\u5f81\uff0c\u9002\u5e94\u536b\u661f\u6d4b\u91cf\u6570\u91cf\u548c\u987a\u5e8f\u7684\u53d8\u5316\u3002", "result": "\u5728\u4e24\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0cPC-DeepNet\u5728\u5b9a\u4f4d\u7cbe\u5ea6\u4e0a\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u548c\u57fa\u4e8e\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u4e14\u8ba1\u7b97\u590d\u6742\u5ea6\u66f4\u4f4e\u3002", "conclusion": "PC-DeepNet\u4e3a\u590d\u6742\u73af\u5883\u4e0b\u7684GNSS\u5b9a\u4f4d\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u9ad8\u7cbe\u5ea6\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.14248", "pdf": "https://arxiv.org/pdf/2504.14248", "abs": "https://arxiv.org/abs/2504.14248", "authors": ["Li Shijiao", "Ma Zhipeng", "He Huajun", "Chen Haiyue"], "title": "Rethinking Traffic Flow Forecasting: From Transition to Generatation", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Traffic flow prediction plays an important role in Intelligent Transportation\nSystems in traffic management and urban planning. There have been extensive\nsuccessful works in this area. However, these approaches focus only on\nmodelling the flow transition and ignore the flow generation process, which\nmanifests itself in two ways: (i) The models are based on Markovian\nassumptions, ignoring the multi-periodicity of the flow generation in nodes.\n(ii) The same structure is designed to encode both the transition and\ngeneration processes, ignoring the differences between them. To address these\nproblems, we propose an Effective Multi-Branch Similarity Transformer for\nTraffic Flow Prediction, namely EMBSFormer. Through data analysis, we find that\nthe factors affecting traffic flow include node-level traffic generation and\ngraph-level traffic transition, which describe the multi-periodicity and\ninteraction pattern of nodes, respectively. Specifically, to capture traffic\ngeneration patterns, we propose a similarity analysis module that supports\nmulti-branch encoding to dynamically expand significant cycles. For traffic\ntransition, we employ a temporal and spatial self-attention mechanism to\nmaintain global node interactions, and use GNN and time conv to model local\nnode interactions, respectively. Model performance is evaluated on three\nreal-world datasets on both long-term and short-term prediction tasks.\nExperimental results show that EMBSFormer outperforms baselines on both tasks.\nMoreover, compared to models based on flow transition modelling (e.g. GMAN,\n513k), the variant of EMBSFormer(93K) only uses 18\\% of the parameters,\nachieving the same performance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aEMBSFormer\u7684\u591a\u5206\u652f\u76f8\u4f3c\u6027Transformer\u6a21\u578b\uff0c\u7528\u4e8e\u89e3\u51b3\u4ea4\u901a\u6d41\u9884\u6d4b\u4e2d\u5ffd\u7565\u6d41\u751f\u6210\u8fc7\u7a0b\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u591a\u5206\u652f\u7f16\u7801\u548c\u81ea\u6ce8\u610f\u529b\u673a\u5236\u5206\u522b\u5efa\u6a21\u751f\u6210\u548c\u8f6c\u79fb\u8fc7\u7a0b\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u6027\u80fd\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u4e14\u53c2\u6570\u66f4\u5c11\u3002", "motivation": "\u73b0\u6709\u4ea4\u901a\u6d41\u9884\u6d4b\u65b9\u6cd5\u4ec5\u5173\u6ce8\u6d41\u8f6c\u79fb\u8fc7\u7a0b\u800c\u5ffd\u7565\u6d41\u751f\u6210\u8fc7\u7a0b\uff0c\u5bfc\u81f4\u6a21\u578b\u65e0\u6cd5\u6355\u6349\u591a\u5468\u671f\u6027\u548c\u8282\u70b9\u95f4\u4ea4\u4e92\u6a21\u5f0f\u3002", "method": "\u63d0\u51faEMBSFormer\u6a21\u578b\uff0c\u5305\u62ec\u76f8\u4f3c\u6027\u5206\u6790\u6a21\u5757\uff08\u591a\u5206\u652f\u7f16\u7801\uff09\u548c\u65f6\u7a7a\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff08GNN\u548c\u65f6\u95f4\u5377\u79ef\uff09\uff0c\u5206\u522b\u5efa\u6a21\u6d41\u751f\u6210\u548c\u8f6c\u79fb\u8fc7\u7a0b\u3002", "result": "\u5728\u4e09\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\uff0cEMBSFormer\u5728\u957f\u77ed\u671f\u9884\u6d4b\u4efb\u52a1\u4e2d\u5747\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u4e14\u53c2\u6570\u66f4\u5c11\uff08\u4ec518%\uff09\u3002", "conclusion": "EMBSFormer\u901a\u8fc7\u5206\u79bb\u5efa\u6a21\u6d41\u751f\u6210\u548c\u8f6c\u79fb\u8fc7\u7a0b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4ea4\u901a\u6d41\u9884\u6d4b\u6027\u80fd\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u6a21\u578b\u590d\u6742\u5ea6\u3002"}}
{"id": "2504.14482", "pdf": "https://arxiv.org/pdf/2504.14482", "abs": "https://arxiv.org/abs/2504.14482", "authors": ["Xiang Li", "Duyi Pan", "Hongru Xiao", "Jiale Han", "Jing Tang", "Jiabao Ma", "Wei Wang", "Bo Cheng"], "title": "DialogueAgents: A Hybrid Agent-Based Speech Synthesis Framework for Multi-Party Dialogue", "categories": ["cs.CL", "cs.SD"], "comment": "Accepted by ICME 2025. Dataset and code are publicly available:\n  [https://github.com/uirlx/DialogueAgents](https://github.com/uirlx/DialogueAgents)", "summary": "Speech synthesis is crucial for human-computer interaction, enabling natural\nand intuitive communication. However, existing datasets involve high\nconstruction costs due to manual annotation and suffer from limited character\ndiversity, contextual scenarios, and emotional expressiveness. To address these\nissues, we propose DialogueAgents, a novel hybrid agent-based speech synthesis\nframework, which integrates three specialized agents -- a script writer, a\nspeech synthesizer, and a dialogue critic -- to collaboratively generate\ndialogues. Grounded in a diverse character pool, the framework iteratively\nrefines dialogue scripts and synthesizes speech based on speech review,\nboosting emotional expressiveness and paralinguistic features of the\nsynthesized dialogues. Using DialogueAgent, we contribute MultiTalk, a\nbilingual, multi-party, multi-turn speech dialogue dataset covering diverse\ntopics. Extensive experiments demonstrate the effectiveness of our framework\nand the high quality of the MultiTalk dataset. We release the dataset and code\nhttps://github.com/uirlx/DialogueAgents to facilitate future research on\nadvanced speech synthesis models and customized data generation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDialogueAgents\u7684\u65b0\u578b\u6df7\u5408\u4ee3\u7406\u8bed\u97f3\u5408\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u4e09\u4e2a\u4e13\u4e1a\u4ee3\u7406\uff08\u811a\u672c\u7f16\u5199\u8005\u3001\u8bed\u97f3\u5408\u6210\u5668\u548c\u5bf9\u8bdd\u8bc4\u8bba\u5bb6\uff09\u534f\u4f5c\u751f\u6210\u5bf9\u8bdd\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6570\u636e\u96c6\u6210\u672c\u9ad8\u3001\u591a\u6837\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5e76\u8d21\u732e\u4e86\u4e00\u4e2a\u9ad8\u8d28\u91cf\u7684\u53cc\u8bed\u591a\u8f6e\u5bf9\u8bdd\u6570\u636e\u96c6MultiTalk\u3002", "motivation": "\u73b0\u6709\u8bed\u97f3\u5408\u6210\u6570\u636e\u96c6\u6784\u5efa\u6210\u672c\u9ad8\uff0c\u4e14\u7f3a\u4e4f\u591a\u6837\u6027\u548c\u60c5\u611f\u8868\u8fbe\uff0c\u9650\u5236\u4e86\u4eba\u673a\u4ea4\u4e92\u7684\u81ea\u7136\u6027\u3002", "method": "\u63d0\u51faDialogueAgents\u6846\u67b6\uff0c\u6574\u5408\u4e09\u4e2a\u4ee3\u7406\uff08\u811a\u672c\u7f16\u5199\u8005\u3001\u8bed\u97f3\u5408\u6210\u5668\u3001\u5bf9\u8bdd\u8bc4\u8bba\u5bb6\uff09\uff0c\u57fa\u4e8e\u591a\u6837\u5316\u89d2\u8272\u6c60\u8fed\u4ee3\u4f18\u5316\u5bf9\u8bdd\u811a\u672c\u548c\u8bed\u97f3\u5408\u6210\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u6846\u67b6\u6709\u6548\uff0c\u5e76\u8d21\u732e\u4e86\u9ad8\u8d28\u91cf\u7684\u53cc\u8bed\u591a\u8f6e\u5bf9\u8bdd\u6570\u636e\u96c6MultiTalk\u3002", "conclusion": "DialogueAgents\u6846\u67b6\u63d0\u5347\u4e86\u8bed\u97f3\u5408\u6210\u7684\u591a\u6837\u6027\u548c\u60c5\u611f\u8868\u8fbe\uff0cMultiTalk\u6570\u636e\u96c6\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u8d44\u6e90\u3002"}}
{"id": "2504.13991", "pdf": "https://arxiv.org/pdf/2504.13991", "abs": "https://arxiv.org/abs/2504.13991", "authors": ["Felix Nannesson Meli", "Johan Tell", "Shirwan Piroti", "Tahar Zanouda", "Elias Jarlebring"], "title": "Deep Learning on Graphs for Mobile Network Topology Generation", "categories": ["cs.LG"], "comment": "5 pages, 6 figures, submitted to IEEE Networking Letters", "summary": "Mobile networks consist of interconnected radio nodes strategically\npositioned across various geographical regions to provide connectivity\nservices. The set of relations between these radio nodes, referred to as the\n\\emph{mobile network topology}, is vital in the construction of the networking\ninfrastructure. Typically, the connections between radio nodes and their\nassociated cells are defined by software features that establish mobility\nrelations (referred to as \\emph{edges} in this paper) within the mobile network\ngraph through heuristic methods. Although these approaches are efficient, they\nencounter significant limitations, particularly since edges can only be\nestablished prior to the installation of physical hardware.\n  In this work, we use graph-based deep learning methods to determine mobility\nrelations (edges), trained on radio node configuration data and reliable\nmobility relations set by Automatic Neighbor Relations (ANR) in stable\nnetworks. This paper focuses on measuring the accuracy and precision of\ndifferent graph-based deep learning approaches applied to real-world mobile\nnetworks. We evaluated two deep learning models. Our comprehensive experiments\non Telecom datasets obtained from operational Telecom Networks demonstrate the\neffectiveness of the graph neural network (GNN) model and multilayer\nperceptron. Our evaluation showed that considering graph structure improves\nresults, which motivates the use of GNNs. Additionally, we investigated the use\nof heuristics to reduce the training time based on the distance between radio\nnodes to eliminate irrelevant cases. Our investigation showed that the use of\nthese heuristics improved precision and accuracy considerably.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u6df1\u5ea6\u5b66\u4e60\u7684\u79fb\u52a8\u7f51\u7edc\u62d3\u6251\u5173\u7cfb\u9884\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bad\u7ec3\u81ea\u52a8\u90bb\u5c45\u5173\u7cfb\u6570\u636e\uff0c\u8bc4\u4f30\u4e86\u56fe\u795e\u7ecf\u7f51\u7edc\u548c\u591a\u5c42\u611f\u77e5\u673a\u7684\u6027\u80fd\uff0c\u5e76\u9a8c\u8bc1\u4e86\u542f\u53d1\u5f0f\u65b9\u6cd5\u5728\u63d0\u5347\u6548\u7387\u548c\u51c6\u786e\u6027\u4e0a\u7684\u4f5c\u7528\u3002", "motivation": "\u4f20\u7edf\u542f\u53d1\u5f0f\u65b9\u6cd5\u5728\u79fb\u52a8\u7f51\u7edc\u62d3\u6251\u5173\u7cfb\u5efa\u7acb\u4e2d\u5b58\u5728\u5c40\u9650\u6027\uff0c\u65e0\u6cd5\u5728\u786c\u4ef6\u5b89\u88c5\u540e\u52a8\u6001\u8c03\u6574\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u5229\u7528\u56fe\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\u52a8\u6001\u9884\u6d4b\u79fb\u52a8\u7f51\u7edc\u4e2d\u7684\u5173\u7cfb\uff08\u8fb9\uff09\u3002", "method": "\u4f7f\u7528\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u548c\u591a\u5c42\u611f\u77e5\u673a\uff08MLP\uff09\u6a21\u578b\uff0c\u57fa\u4e8e\u65e0\u7ebf\u7535\u8282\u70b9\u914d\u7f6e\u6570\u636e\u548c\u81ea\u52a8\u90bb\u5c45\u5173\u7cfb\uff08ANR\uff09\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\u3002\u901a\u8fc7\u542f\u53d1\u5f0f\u65b9\u6cd5\uff08\u5982\u8282\u70b9\u95f4\u8ddd\u79bb\uff09\u51cf\u5c11\u8bad\u7ec3\u65f6\u95f4\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cGNN\u6a21\u578b\u5728\u8003\u8651\u56fe\u7ed3\u6784\u65f6\u8868\u73b0\u66f4\u4f18\uff0c\u542f\u53d1\u5f0f\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u7cbe\u5ea6\u548c\u51c6\u786e\u6027\u3002", "conclusion": "\u56fe\u6df1\u5ea6\u5b66\u4e60\uff08\u5c24\u5176\u662fGNN\uff09\u80fd\u6709\u6548\u9884\u6d4b\u79fb\u52a8\u7f51\u7edc\u62d3\u6251\u5173\u7cfb\uff0c\u542f\u53d1\u5f0f\u65b9\u6cd5\u8fdb\u4e00\u6b65\u4f18\u5316\u4e86\u6a21\u578b\u6027\u80fd\uff0c\u4e3a\u52a8\u6001\u7f51\u7edc\u62d3\u6251\u7ba1\u7406\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2504.14274", "pdf": "https://arxiv.org/pdf/2504.14274", "abs": "https://arxiv.org/abs/2504.14274", "authors": ["Zhengxi Lu", "Shizhuo Cheng", "Yuru Jiang", "Yan Zhang", "Min Zhang"], "title": "ProtPainter: Draw or Drag Protein via Topology-guided Diffusion", "categories": ["cs.AI"], "comment": "Published as a conference paper at ICLR 2025", "summary": "Recent advances in protein backbone generation have achieved promising\nresults under structural, functional, or physical constraints. However,\nexisting methods lack the flexibility for precise topology control, limiting\nnavigation of the backbone space. We present ProtPainter, a diffusion-based\napproach for generating protein backbones conditioned on 3D curves. ProtPainter\nfollows a two-stage process: curve-based sketching and sketch-guided backbone\ngeneration. For the first stage, we propose CurveEncoder, which predicts\nsecondary structure annotations from a curve to parametrize sketch generation.\nFor the second stage, the sketch guides the generative process in Denoising\nDiffusion Probabilistic Modeling (DDPM) to generate backbones. During this\nprocess, we further introduce a fusion scheduling scheme, Helix-Gating, to\ncontrol the scaling factors. To evaluate, we propose the first benchmark for\ntopology-conditioned protein generation, introducing Protein Restoration Task\nand a new metric, self-consistency Topology Fitness (scTF). Experiments\ndemonstrate ProtPainter's ability to generate topology-fit (scTF > 0.8) and\ndesignable (scTM > 0.5) backbones, with drawing and dragging tasks showcasing\nits flexibility and versatility.", "AI": {"tldr": "ProtPainter\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u7684\u65b9\u6cd5\uff0c\u901a\u8fc73D\u66f2\u7ebf\u6761\u4ef6\u751f\u6210\u86cb\u767d\u8d28\u9aa8\u67b6\uff0c\u5206\u4e3a\u66f2\u7ebf\u8349\u56fe\u751f\u6210\u548c\u8349\u56fe\u5f15\u5bfc\u9aa8\u67b6\u751f\u6210\u4e24\u9636\u6bb5\uff0c\u5b9e\u73b0\u4e86\u5bf9\u62d3\u6251\u7ed3\u6784\u7684\u7cbe\u786e\u63a7\u5236\u3002", "motivation": "\u73b0\u6709\u86cb\u767d\u8d28\u9aa8\u67b6\u751f\u6210\u65b9\u6cd5\u7f3a\u4e4f\u5bf9\u62d3\u6251\u7ed3\u6784\u7684\u7075\u6d3b\u63a7\u5236\uff0c\u9650\u5236\u4e86\u9aa8\u67b6\u7a7a\u95f4\u7684\u63a2\u7d22\u3002", "method": "ProtPainter\u91c7\u7528\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a1) CurveEncoder\u4ece\u66f2\u7ebf\u9884\u6d4b\u4e8c\u7ea7\u7ed3\u6784\u6ce8\u91ca\u751f\u6210\u8349\u56fe\uff1b2) \u8349\u56fe\u5f15\u5bfcDDPM\u751f\u6210\u9aa8\u67b6\uff0c\u5e76\u5f15\u5165Helix-Gating\u63a7\u5236\u7f29\u653e\u56e0\u5b50\u3002", "result": "\u5b9e\u9a8c\u8868\u660eProtPainter\u80fd\u751f\u6210\u62d3\u6251\u62df\u5408\uff08scTF > 0.8\uff09\u548c\u53ef\u8bbe\u8ba1\uff08scTM > 0.5\uff09\u7684\u9aa8\u67b6\uff0c\u5c55\u793a\u4e86\u5176\u7075\u6d3b\u6027\u548c\u591a\u529f\u80fd\u6027\u3002", "conclusion": "ProtPainter\u4e3a\u62d3\u6251\u6761\u4ef6\u86cb\u767d\u8d28\u751f\u6210\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u65b0\u57fa\u51c6\u548c\u6307\u6807\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2504.14492", "pdf": "https://arxiv.org/pdf/2504.14492", "abs": "https://arxiv.org/abs/2504.14492", "authors": ["Yichen Li", "Zhiting Fan", "Ruizhe Chen", "Xiaotang Gai", "Luqi Gong", "Yan Zhang", "Zuozhu Liu"], "title": "FairSteer: Inference Time Debiasing for LLMs with Dynamic Activation Steering", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) are prone to capturing biases from training\ncorpus, leading to potential negative social impacts. Existing prompt-based\ndebiasing methods exhibit instability due to their sensitivity to prompt\nchanges, while fine-tuning-based techniques incur substantial computational\noverhead and catastrophic forgetting. In this paper, we propose FairSteer, a\nnovel inference-time debiasing framework without requiring customized prompt\ndesign or model retraining. Motivated by the linear representation hypothesis,\nour preliminary investigation demonstrates that fairness-related features can\nbe encoded into separable directions in the hidden activation space. FairSteer\noperates in three steps: biased activation detection, debiasing steering vector\n(DSV) computation, and dynamic activation steering. Specifically, it first\ntrains a lightweight linear classifier to detect bias signatures in\nactivations, and then computes DSVs as intervention directions derived from\nsmall contrastive prompt pairs. Subsequently, it performs debiasing by\nadjusting activations with DSVs in the inference stage. Comprehensive\nevaluation with six LLMs demonstrates the superiority of FairSteer across\nquestion-answering, counterfactual input evaluation and open-ended text\ngeneration tasks. Code will be released.", "AI": {"tldr": "FairSteer\u662f\u4e00\u79cd\u65e0\u9700\u5b9a\u5236\u63d0\u793a\u6216\u6a21\u578b\u91cd\u65b0\u8bad\u7ec3\u7684\u63a8\u7406\u65f6\u53bb\u504f\u6846\u67b6\uff0c\u901a\u8fc7\u68c0\u6d4b\u504f\u6fc0\u6d3b\u3001\u8ba1\u7b97\u53bb\u504f\u8f6c\u5411\u5411\u91cf\uff08DSV\uff09\u548c\u52a8\u6001\u8c03\u6574\u6fc0\u6d3b\u6765\u5b9e\u73b0\u53bb\u504f\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5bb9\u6613\u4ece\u8bad\u7ec3\u8bed\u6599\u4e2d\u6355\u83b7\u504f\u89c1\uff0c\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u4e0d\u7a33\u5b9a\u6216\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u95ee\u9898\u3002", "method": "FairSteer\u57fa\u4e8e\u7ebf\u6027\u8868\u793a\u5047\u8bbe\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u7ebf\u6027\u5206\u7c7b\u5668\u68c0\u6d4b\u504f\u6fc0\u6d3b\uff0c\u8ba1\u7b97DSV\u4f5c\u4e3a\u5e72\u9884\u65b9\u5411\uff0c\u5e76\u5728\u63a8\u7406\u9636\u6bb5\u52a8\u6001\u8c03\u6574\u6fc0\u6d3b\u3002", "result": "\u5728\u516d\u79cdLLM\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cFairSteer\u5728\u95ee\u7b54\u3001\u53cd\u4e8b\u5b9e\u8f93\u5165\u8bc4\u4f30\u548c\u5f00\u653e\u6587\u672c\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u8d8a\u3002", "conclusion": "FairSteer\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u7a33\u5b9a\u7684\u53bb\u504f\u65b9\u6cd5\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u6216\u590d\u6742\u63d0\u793a\u8bbe\u8ba1\u3002"}}
{"id": "2504.13992", "pdf": "https://arxiv.org/pdf/2504.13992", "abs": "https://arxiv.org/abs/2504.13992", "authors": ["Eric Lu"], "title": "First and Second Order Approximations to Stochastic Gradient Descent Methods with Momentum Terms", "categories": ["cs.LG", "math.OC"], "comment": null, "summary": "Stochastic Gradient Descent (SGD) methods see many uses in optimization\nproblems. Modifications to the algorithm, such as momentum-based SGD methods\nhave been known to produce better results in certain cases. Much of this,\nhowever, is due to empirical information rather than rigorous proof. While the\ndynamics of gradient descent methods can be studied through continuous\napproximations, existing works only cover scenarios with constant learning\nrates or SGD without momentum terms. We present approximation results under\nweak assumptions for SGD that allow learning rates and momentum parameters to\nvary with respect to time.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u968f\u673a\u68af\u5ea6\u4e0b\u964d\uff08SGD\uff09\u53ca\u5176\u52a8\u91cf\u53d8\u4f53\u7684\u52a8\u6001\u884c\u4e3a\uff0c\u63d0\u51fa\u4e86\u5728\u65f6\u53d8\u5b66\u4e60\u7387\u548c\u52a8\u91cf\u53c2\u6570\u4e0b\u7684\u8fd1\u4f3c\u7ed3\u679c\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4ec5\u8986\u76d6\u6052\u5b9a\u5b66\u4e60\u7387\u6216\u65e0\u52a8\u91cf\u9879\u7684SGD\uff0c\u7f3a\u4e4f\u5bf9\u65f6\u53d8\u53c2\u6570\u7684\u4e25\u683c\u7406\u8bba\u5206\u6790\u3002", "method": "\u901a\u8fc7\u8fde\u7eed\u903c\u8fd1\u65b9\u6cd5\uff0c\u7814\u7a76SGD\u5728\u65f6\u53d8\u5b66\u4e60\u7387\u548c\u52a8\u91cf\u53c2\u6570\u4e0b\u7684\u52a8\u6001\u884c\u4e3a\u3002", "result": "\u63d0\u51fa\u4e86\u5728\u5f31\u5047\u8bbe\u4e0bSGD\u7684\u8fd1\u4f3c\u7ed3\u679c\uff0c\u6269\u5c55\u4e86\u73b0\u6709\u7406\u8bba\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3aSGD\u53ca\u5176\u52a8\u91cf\u53d8\u4f53\u7684\u7406\u8bba\u5206\u6790\u63d0\u4f9b\u4e86\u66f4\u5e7f\u6cdb\u7684\u6846\u67b6\u3002"}}
{"id": "2504.14282", "pdf": "https://arxiv.org/pdf/2504.14282", "abs": "https://arxiv.org/abs/2504.14282", "authors": ["Ze Zhao", "Bin Lu", "Xiaoying Gan", "Gu Tang", "Luoyi Fu", "Xinbing Wang"], "title": "CHAINSFORMER: Numerical Reasoning on Knowledge Graphs from a Chain Perspective", "categories": ["cs.AI", "cs.LG"], "comment": "Accepted to ICDE 2025", "summary": "Reasoning over Knowledge Graphs (KGs) plays a pivotal role in knowledge graph\ncompletion or question answering systems, providing richer and more accurate\ntriples and attributes. As numerical attributes become increasingly essential\nin characterizing entities and relations in KGs, the ability to reason over\nthese attributes has gained significant importance. Existing graph-based\nmethods such as Graph Neural Networks (GNNs) and Knowledge Graph Embeddings\n(KGEs), primarily focus on aggregating homogeneous local neighbors and\nimplicitly embedding diverse triples. However, these approaches often fail to\nfully leverage the potential of logical paths within the graph, limiting their\neffectiveness in exploiting the reasoning process. To address these\nlimitations, we propose ChainsFormer, a novel chain-based framework designed to\nsupport numerical reasoning. Chainsformer not only explicitly constructs\nlogical chains but also expands the reasoning depth to multiple hops.\nSpecially, we introduces Relation-Attribute Chains (RA-Chains), a specialized\nlogic chain, to model sequential reasoning patterns. ChainsFormer captures the\nstep-by-step nature of multi-hop reasoning along RA-Chains by employing\nsequential in-context learning. To mitigate the impact of noisy chains, we\npropose a hyperbolic affinity scoring mechanism that selects relevant logic\nchains in a variable-resolution space. Furthermore, ChainsFormer incorporates\nan attention-based numerical reasoner to identify critical reasoning paths,\nenhancing both reasoning accuracy and transparency. Experimental results\ndemonstrate that ChainsFormer significantly outperforms state-of-the-art\nmethods, achieving up to a 20.0% improvement in performance. The\nimplementations are available at\nhttps://github.com/zhaodazhuang2333/ChainsFormer.", "AI": {"tldr": "ChainsFormer\u662f\u4e00\u79cd\u57fa\u4e8e\u94fe\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u652f\u6301\u77e5\u8bc6\u56fe\u8c31\u4e2d\u7684\u6570\u503c\u63a8\u7406\uff0c\u901a\u8fc7\u663e\u5f0f\u6784\u5efa\u903b\u8f91\u94fe\u548c\u591a\u8df3\u63a8\u7406\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\uff08\u5982GNNs\u548cKGEs\uff09\u672a\u80fd\u5145\u5206\u5229\u7528\u77e5\u8bc6\u56fe\u8c31\u4e2d\u7684\u903b\u8f91\u8def\u5f84\uff0c\u9650\u5236\u4e86\u63a8\u7406\u6548\u679c\u3002", "method": "\u63d0\u51faRelation-Attribute Chains\uff08RA-Chains\uff09\u548c\u53cc\u66f2\u4eb2\u548c\u8bc4\u5206\u673a\u5236\uff0c\u7ed3\u5408\u6ce8\u610f\u529b\u6570\u503c\u63a8\u7406\u5668\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u6027\u80fd\u63d0\u5347\u8fbe20.0%\u3002", "conclusion": "ChainsFormer\u5728\u63a8\u7406\u51c6\u786e\u6027\u548c\u900f\u660e\u5ea6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2504.14496", "pdf": "https://arxiv.org/pdf/2504.14496", "abs": "https://arxiv.org/abs/2504.14496", "authors": ["Zijian Wang", "Chang Xu"], "title": "Functional Abstraction of Knowledge Recall in Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Pre-trained transformer large language models (LLMs) demonstrate strong\nknowledge recall capabilities. This paper investigates the knowledge recall\nmechanism in LLMs by abstracting it into a functional structure. We propose\nthat during knowledge recall, the model's hidden activation space implicitly\nentails a function execution process where specific activation vectors align\nwith functional components (Input argument, Function body, and Return values).\nSpecifically, activation vectors of relation-related tokens define a mapping\nfunction from subjects to objects, with subject-related token activations\nserving as input arguments and object-related token activations as return\nvalues. For experimental verification, we first design a patching-based\nknowledge-scoring algorithm to identify knowledge-aware activation vectors as\nindependent functional components. Then, we conduct counter-knowledge testing\nto examine the independent functional effects of each component on knowledge\nrecall outcomes. From this functional perspective, we improve the contextual\nknowledge editing approach augmented by activation patching. By rewriting\nincoherent activations in context, we enable improved short-term memory\nretention for new knowledge prompting.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u77e5\u8bc6\u53ec\u56de\u673a\u5236\uff0c\u63d0\u51fa\u5176\u9690\u85cf\u6fc0\u6d3b\u7a7a\u95f4\u9690\u542b\u51fd\u6570\u6267\u884c\u8fc7\u7a0b\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u548c\u6539\u8fdb\u77e5\u8bc6\u7f16\u8f91\u65b9\u6cd5\u3002", "motivation": "\u63a2\u7d22LLM\u77e5\u8bc6\u53ec\u56de\u7684\u673a\u5236\uff0c\u5c06\u5176\u62bd\u8c61\u4e3a\u529f\u80fd\u7ed3\u6784\uff0c\u4ee5\u63d0\u5347\u77e5\u8bc6\u7f16\u8f91\u7684\u6548\u679c\u3002", "method": "\u8bbe\u8ba1\u57fa\u4e8e\u4fee\u8865\u7684\u77e5\u8bc6\u8bc4\u5206\u7b97\u6cd5\u8bc6\u522b\u529f\u80fd\u7ec4\u4ef6\uff0c\u5e76\u901a\u8fc7\u53cd\u77e5\u8bc6\u6d4b\u8bd5\u9a8c\u8bc1\u5176\u72ec\u7acb\u6027\u3002\u6539\u8fdb\u6fc0\u6d3b\u4fee\u8865\u589e\u5f3a\u7684\u4e0a\u4e0b\u6587\u77e5\u8bc6\u7f16\u8f91\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6fc0\u6d3b\u5411\u91cf\u4e0e\u529f\u80fd\u7ec4\u4ef6\u7684\u5bf9\u5e94\u5173\u7cfb\uff0c\u6539\u8fdb\u540e\u7684\u77e5\u8bc6\u7f16\u8f91\u65b9\u6cd5\u63d0\u5347\u4e86\u77ed\u671f\u8bb0\u5fc6\u4fdd\u7559\u80fd\u529b\u3002", "conclusion": "LLM\u7684\u77e5\u8bc6\u53ec\u56de\u53ef\u901a\u8fc7\u529f\u80fd\u7ed3\u6784\u7406\u89e3\uff0c\u6fc0\u6d3b\u4fee\u8865\u65b9\u6cd5\u80fd\u6709\u6548\u4f18\u5316\u77e5\u8bc6\u7f16\u8f91\u6548\u679c\u3002"}}
{"id": "2504.14025", "pdf": "https://arxiv.org/pdf/2504.14025", "abs": "https://arxiv.org/abs/2504.14025", "authors": ["Justin Domke"], "title": "Large Language Bayes", "categories": ["cs.LG"], "comment": null, "summary": "Many domain experts do not have the time or training to write formal Bayesian\nmodels. This paper takes an informal problem description as input, and combines\na large language model and a probabilistic programming language to create a\njoint distribution over formal models, latent variables, and data. A posterior\nover latent variables follows by conditioning on observed data and integrating\nover formal models. This presents a challenging inference problem. We suggest\nan inference recipe that amounts to generating many formal models from the\nlarge language model, performing approximate inference on each, and then doing\na weighted average. This is justified an analyzed as a combination of\nself-normalized importance sampling, MCMC, and variational inference. We show\nthat this produces sensible predictions without the need to specify a formal\nmodel.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u6982\u7387\u7f16\u7a0b\u8bed\u8a00\uff0c\u5c06\u975e\u6b63\u5f0f\u95ee\u9898\u63cf\u8ff0\u8f6c\u5316\u4e3a\u5f62\u5f0f\u5316\u6a21\u578b\uff0c\u5e76\u751f\u6210\u6f5c\u5728\u53d8\u91cf\u7684\u540e\u9a8c\u5206\u5e03\u3002", "motivation": "\u8bb8\u591a\u9886\u57df\u4e13\u5bb6\u7f3a\u4e4f\u65f6\u95f4\u6216\u8bad\u7ec3\u6765\u7f16\u5199\u5f62\u5f0f\u5316\u7684\u8d1d\u53f6\u65af\u6a21\u578b\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u81ea\u52a8\u5316\u65b9\u6cd5\u3002", "method": "\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u591a\u4e2a\u5f62\u5f0f\u5316\u6a21\u578b\uff0c\u5bf9\u6bcf\u4e2a\u6a21\u578b\u8fdb\u884c\u8fd1\u4f3c\u63a8\u65ad\uff0c\u7136\u540e\u8fdb\u884c\u52a0\u6743\u5e73\u5747\u3002", "result": "\u8be5\u65b9\u6cd5\u65e0\u9700\u6307\u5b9a\u5f62\u5f0f\u5316\u6a21\u578b\u5373\u53ef\u751f\u6210\u5408\u7406\u7684\u9884\u6d4b\u3002", "conclusion": "\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u6982\u7387\u7f16\u7a0b\u8bed\u8a00\u7684\u65b9\u6cd5\u4e3a\u89e3\u51b3\u590d\u6742\u63a8\u65ad\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2504.14298", "pdf": "https://arxiv.org/pdf/2504.14298", "abs": "https://arxiv.org/abs/2504.14298", "authors": ["Xiucheng Wang", "Zhongsheng Fang", "Nan Cheng"], "title": "RadioDiff-Inverse: Diffusion Enhanced Bayesian Inverse Estimation for ISAC Radio Map Construction", "categories": ["cs.AI"], "comment": "12 pages, 7 figures", "summary": "Radio maps (RMs) are essential for environment-aware communication and\nsensing, providing location-specific wireless channel information. Existing RM\nconstruction methods often rely on precise environmental data and base station\n(BS) locations, which are not always available in dynamic or privacy-sensitive\nenvironments. While sparse measurement techniques reduce data collection, the\nimpact of noise in sparse data on RM accuracy is not well understood. This\npaper addresses these challenges by formulating RM construction as a Bayesian\ninverse problem under coarse environmental knowledge and noisy sparse\nmeasurements. Although maximum a posteriori (MAP) filtering offers an optimal\nsolution, it requires a precise prior distribution of the RM, which is\ntypically unavailable. To solve this, we propose RadioDiff-Inverse, a\ndiffusion-enhanced Bayesian inverse estimation framework that uses an\nunconditional generative diffusion model to learn the RM prior. This approach\nnot only reconstructs the spatial distribution of wireless channel features but\nalso enables environmental structure perception, such as building outlines, and\nlocation of BS just relay on pathloss, through integrated sensing and\ncommunication (ISAC). Remarkably, RadioDiff-Inverse is training-free,\nleveraging a pre-trained model from Imagenet without task-specific fine-tuning,\nwhich significantly reduces the training cost of using generative large model\nin wireless networks. Experimental results demonstrate that RadioDiff-Inverse\nachieves state-of-the-art performance in accuracy of RM construction and\nenvironmental reconstruction, and robustness against noisy sparse sampling.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRadioDiff-Inverse\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u6269\u6563\u589e\u5f3a\u7684\u8d1d\u53f6\u65af\u9006\u4f30\u8ba1\u65b9\u6cd5\uff0c\u5229\u7528\u65e0\u6761\u4ef6\u751f\u6210\u6269\u6563\u6a21\u578b\u5b66\u4e60\u65e0\u7ebf\u7535\u5730\u56fe\uff08RM\uff09\u7684\u5148\u9a8c\u5206\u5e03\uff0c\u89e3\u51b3\u4e86\u7a00\u758f\u566a\u58f0\u6570\u636e\u4e0bRM\u6784\u5efa\u7684\u6311\u6218\u3002\u8be5\u65b9\u6cd5\u65e0\u9700\u4efb\u52a1\u7279\u5b9a\u5fae\u8c03\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8bad\u7ec3\u6210\u672c\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u4f18\u5f02\u7684RM\u6784\u5efa\u548c\u73af\u5883\u91cd\u5efa\u6027\u80fd\u3002", "motivation": "\u73b0\u6709RM\u6784\u5efa\u65b9\u6cd5\u4f9d\u8d56\u7cbe\u786e\u73af\u5883\u6570\u636e\u548c\u57fa\u7ad9\u4f4d\u7f6e\uff0c\u4f46\u5728\u52a8\u6001\u6216\u9690\u79c1\u654f\u611f\u73af\u5883\u4e2d\u96be\u4ee5\u83b7\u53d6\u3002\u7a00\u758f\u6d4b\u91cf\u6280\u672f\u867d\u51cf\u5c11\u6570\u636e\u6536\u96c6\uff0c\u4f46\u5176\u566a\u58f0\u5bf9RM\u7cbe\u5ea6\u7684\u5f71\u54cd\u5c1a\u672a\u5145\u5206\u7814\u7a76\u3002", "method": "\u63d0\u51faRadioDiff-Inverse\u6846\u67b6\uff0c\u5c06RM\u6784\u5efa\u5efa\u6a21\u4e3a\u8d1d\u53f6\u65af\u9006\u95ee\u9898\uff0c\u5229\u7528\u65e0\u6761\u4ef6\u751f\u6210\u6269\u6563\u6a21\u578b\u5b66\u4e60RM\u5148\u9a8c\u5206\u5e03\uff0c\u7ed3\u5408\u96c6\u6210\u611f\u77e5\u4e0e\u901a\u4fe1\uff08ISAC\uff09\u5b9e\u73b0\u73af\u5883\u7ed3\u6784\u611f\u77e5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cRadioDiff-Inverse\u5728RM\u6784\u5efa\u7cbe\u5ea6\u3001\u73af\u5883\u91cd\u5efa\u53ca\u6297\u566a\u58f0\u7a00\u758f\u91c7\u6837\u65b9\u9762\u8fbe\u5230\u6700\u4f18\u6027\u80fd\u3002", "conclusion": "RadioDiff-Inverse\u4e3a\u52a8\u6001\u6216\u9690\u79c1\u654f\u611f\u73af\u5883\u4e2d\u7684RM\u6784\u5efa\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4e14\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u6210\u672c\u3002"}}
{"id": "2504.14530", "pdf": "https://arxiv.org/pdf/2504.14530", "abs": "https://arxiv.org/abs/2504.14530", "authors": ["Zhijing Jin"], "title": "Causality for Natural Language Processing", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "comment": "PhD Thesis 2024", "summary": "Causal reasoning is a cornerstone of human intelligence and a critical\ncapability for artificial systems aiming to achieve advanced understanding and\ndecision-making. This thesis delves into various dimensions of causal reasoning\nand understanding in large language models (LLMs). It encompasses a series of\nstudies that explore the causal inference skills of LLMs, the mechanisms behind\ntheir performance, and the implications of causal and anticausal learning for\nnatural language processing (NLP) tasks. Additionally, it investigates the\napplication of causal reasoning in text-based computational social science,\nspecifically focusing on political decision-making and the evaluation of\nscientific impact through citations. Through novel datasets, benchmark tasks,\nand methodological frameworks, this work identifies key challenges and\nopportunities to improve the causal capabilities of LLMs, providing a\ncomprehensive foundation for future research in this evolving field.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u56e0\u679c\u63a8\u7406\u548c\u7406\u89e3\u65b9\u9762\u7684\u80fd\u529b\uff0c\u5305\u62ec\u56e0\u679c\u63a8\u65ad\u6280\u80fd\u3001\u673a\u5236\u53ca\u5176\u5728NLP\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\uff0c\u5e76\u7814\u7a76\u4e86\u56e0\u679c\u63a8\u7406\u5728\u8ba1\u7b97\u793e\u4f1a\u79d1\u5b66\u4e2d\u7684\u5177\u4f53\u5e94\u7528\u3002", "motivation": "\u56e0\u679c\u63a8\u7406\u662f\u4eba\u7c7b\u667a\u80fd\u7684\u6838\u5fc3\uff0c\u4e5f\u662f\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u5b9e\u73b0\u9ad8\u7ea7\u7406\u89e3\u548c\u51b3\u7b56\u7684\u5173\u952e\u80fd\u529b\u3002\u7814\u7a76\u65e8\u5728\u63d0\u5347LLMs\u7684\u56e0\u679c\u63a8\u7406\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u4e00\u7cfb\u5217\u7814\u7a76\u3001\u65b0\u9896\u7684\u6570\u636e\u96c6\u3001\u57fa\u51c6\u4efb\u52a1\u548c\u65b9\u6cd5\u8bba\u6846\u67b6\uff0c\u5206\u6790LLMs\u7684\u56e0\u679c\u63a8\u7406\u6280\u80fd\u53ca\u5176\u673a\u5236\u3002", "result": "\u7814\u7a76\u63ed\u793a\u4e86LLMs\u5728\u56e0\u679c\u63a8\u7406\u65b9\u9762\u7684\u5173\u952e\u6311\u6218\u548c\u673a\u9047\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u7840\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u63d0\u5347LLMs\u7684\u56e0\u679c\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u7814\u7a76\u6846\u67b6\uff0c\u63a8\u52a8\u4e86\u8fd9\u4e00\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2504.14046", "pdf": "https://arxiv.org/pdf/2504.14046", "abs": "https://arxiv.org/abs/2504.14046", "authors": ["Tahar Nabil", "Ghislain Agoua", "Pierre Cauchois", "Anne De Moliner", "Beno\u00eet Grossin"], "title": "A synthetic dataset of French electric load curves with temperature conditioning", "categories": ["cs.LG", "cs.AI"], "comment": "Workshop paper at \"Tackling Climate Change with Machine Learning\",\n  ICLR 2025", "summary": "The undergoing energy transition is causing behavioral changes in electricity\nuse, e.g. with self-consumption of local generation, or flexibility services\nfor demand control. To better understand these changes and the challenges they\ninduce, accessing individual smart meter data is crucial. Yet this is personal\ndata under the European GDPR. A widespread use of such data requires thus to\ncreate synthetic realistic and privacy-preserving samples. This paper\nintroduces a new synthetic load curve dataset generated by conditional latent\ndiffusion. We also provide the contracted power, time-of-use plan and local\ntemperature used for generation. Fidelity, utility and privacy of the dataset\nare thoroughly evaluated, demonstrating its good quality and thereby supporting\nits interest for energy modeling applications.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6761\u4ef6\u6f5c\u5728\u6269\u6563\u7684\u5408\u6210\u8d1f\u8f7d\u66f2\u7ebf\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u89e3\u51b3\u80fd\u6e90\u8f6c\u578b\u4e2d\u667a\u80fd\u7535\u8868\u6570\u636e\u9690\u79c1\u95ee\u9898\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u4fdd\u771f\u5ea6\u3001\u5b9e\u7528\u6027\u548c\u9690\u79c1\u6027\u3002", "motivation": "\u80fd\u6e90\u8f6c\u578b\u5bfc\u81f4\u7528\u7535\u884c\u4e3a\u53d8\u5316\uff0c\u9700\u8981\u667a\u80fd\u7535\u8868\u6570\u636e\u652f\u6301\u7814\u7a76\uff0c\u4f46\u6570\u636e\u6d89\u53ca\u9690\u79c1\uff08\u5982GDPR\uff09\uff0c\u56e0\u6b64\u9700\u8981\u5408\u6210\u9690\u79c1\u4fdd\u62a4\u7684\u6837\u672c\u3002", "method": "\u4f7f\u7528\u6761\u4ef6\u6f5c\u5728\u6269\u6563\u65b9\u6cd5\u751f\u6210\u5408\u6210\u8d1f\u8f7d\u66f2\u7ebf\u6570\u636e\u96c6\uff0c\u5e76\u63d0\u4f9b\u5408\u540c\u529f\u7387\u3001\u5206\u65f6\u7535\u4ef7\u8ba1\u5212\u548c\u672c\u5730\u6e29\u5ea6\u7b49\u751f\u6210\u6761\u4ef6\u3002", "result": "\u6570\u636e\u96c6\u5728\u4fdd\u771f\u5ea6\u3001\u5b9e\u7528\u6027\u548c\u9690\u79c1\u6027\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u9002\u7528\u4e8e\u80fd\u6e90\u5efa\u6a21\u5e94\u7528\u3002", "conclusion": "\u8be5\u5408\u6210\u6570\u636e\u96c6\u4e3a\u80fd\u6e90\u7814\u7a76\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u4e14\u9690\u79c1\u5b89\u5168\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.14325", "pdf": "https://arxiv.org/pdf/2504.14325", "abs": "https://arxiv.org/abs/2504.14325", "authors": ["Alessio Buscemi", "Daniele Proverbio", "Alessandro Di Stefano", "The Anh Han", "German Castignani", "Pietro Di Li\u00f2"], "title": "FAIRGAME: a Framework for AI Agents Bias Recognition using Game Theory", "categories": ["cs.AI"], "comment": null, "summary": "Letting AI agents interact in multi-agent applications adds a layer of\ncomplexity to the interpretability and prediction of AI outcomes, with profound\nimplications for their trustworthy adoption in research and society. Game\ntheory offers powerful models to capture and interpret strategic interaction\namong agents, but requires the support of reproducible, standardized and\nuser-friendly IT frameworks to enable comparison and interpretation of results.\nTo this end, we present FAIRGAME, a Framework for AI Agents Bias Recognition\nusing Game Theory. We describe its implementation and usage, and we employ it\nto uncover biased outcomes in popular games among AI agents, depending on the\nemployed Large Language Model (LLM) and used language, as well as on the\npersonality trait or strategic knowledge of the agents. Overall, FAIRGAME\nallows users to reliably and easily simulate their desired games and scenarios\nand compare the results across simulation campaigns and with game-theoretic\npredictions, enabling the systematic discovery of biases, the anticipation of\nemerging behavior out of strategic interplays, and empowering further research\ninto strategic decision-making using LLM agents.", "AI": {"tldr": "FAIRGAME\u662f\u4e00\u4e2a\u57fa\u4e8e\u535a\u5f08\u8bba\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u8bc6\u522bAI\u4ee3\u7406\u5728\u591a\u667a\u80fd\u4f53\u5e94\u7528\u4e2d\u7684\u504f\u89c1\uff0c\u652f\u6301\u7528\u6237\u6a21\u62df\u6e38\u620f\u573a\u666f\u5e76\u6bd4\u8f83\u7ed3\u679c\u3002", "motivation": "\u591a\u667a\u80fd\u4f53\u4ea4\u4e92\u589e\u52a0\u4e86AI\u7ed3\u679c\u7684\u53ef\u89e3\u91ca\u6027\u548c\u9884\u6d4b\u590d\u6742\u6027\uff0c\u9700\u8981\u6807\u51c6\u5316\u5de5\u5177\u652f\u6301\u535a\u5f08\u8bba\u6a21\u578b\u7684\u6bd4\u8f83\u548c\u89e3\u91ca\u3002", "method": "\u5f00\u53d1FAIRGAME\u6846\u67b6\uff0c\u5b9e\u73b0\u6e38\u620f\u6a21\u62df\u548c\u7ed3\u679c\u6bd4\u8f83\uff0c\u5206\u6790\u4e0d\u540cLLM\u3001\u8bed\u8a00\u3001\u4e2a\u6027\u7279\u5f81\u548c\u7b56\u7565\u77e5\u8bc6\u5bf9\u7ed3\u679c\u7684\u5f71\u54cd\u3002", "result": "FAIRGAME\u80fd\u53ef\u9760\u5730\u53d1\u73b0AI\u4ee3\u7406\u7684\u504f\u89c1\uff0c\u9884\u6d4b\u6218\u7565\u4e92\u52a8\u4e2d\u7684\u884c\u4e3a\uff0c\u5e76\u652f\u6301\u8fdb\u4e00\u6b65\u7814\u7a76\u3002", "conclusion": "FAIRGAME\u4e3aAI\u4ee3\u7406\u7684\u6218\u7565\u51b3\u7b56\u7814\u7a76\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u5de5\u5177\uff0c\u4fc3\u8fdb\u53ef\u4fe1\u8d56\u7684AI\u5e94\u7528\u3002"}}
{"id": "2504.14538", "pdf": "https://arxiv.org/pdf/2504.14538", "abs": "https://arxiv.org/abs/2504.14538", "authors": ["Yiting Ran", "Xintao Wang", "Tian Qiu", "Jiaqing Liang", "Yanghua Xiao", "Deqing Yang"], "title": "BookWorld: From Novels to Interactive Agent Societies for Creative Story Generation", "categories": ["cs.CL"], "comment": "19 pages, 4 figures", "summary": "Recent advances in large language models (LLMs) have enabled social\nsimulation through multi-agent systems. Prior efforts focus on agent societies\ncreated from scratch, assigning agents with newly defined personas. However,\nsimulating established fictional worlds and characters remain largely\nunderexplored, despite its significant practical value. In this paper, we\nintroduce BookWorld, a comprehensive system for constructing and simulating\nbook-based multi-agent societies. BookWorld's design covers comprehensive\nreal-world intricacies, including diverse and dynamic characters, fictional\nworldviews, geographical constraints and changes, e.t.c. BookWorld enables\ndiverse applications including story generation, interactive games and social\nsimulation, offering novel ways to extend and explore beloved fictional works.\nThrough extensive experiments, we demonstrate that BookWorld generates\ncreative, high-quality stories while maintaining fidelity to the source books,\nsurpassing previous methods with a win rate of 75.36%. The code of this paper\ncan be found at the project page: https://bookworld2025.github.io/.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86BookWorld\u7cfb\u7edf\uff0c\u7528\u4e8e\u6784\u5efa\u548c\u6a21\u62df\u57fa\u4e8e\u4e66\u7c4d\u7684\u591a\u667a\u80fd\u4f53\u793e\u4f1a\uff0c\u652f\u6301\u6545\u4e8b\u751f\u6210\u3001\u4e92\u52a8\u6e38\u620f\u7b49\u5e94\u7528\uff0c\u5b9e\u9a8c\u663e\u793a\u5176\u751f\u6210\u7684\u6545\u4e8b\u8d28\u91cf\u9ad8\u4e14\u5fe0\u4e8e\u539f\u8457\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u591a\u5173\u6ce8\u4ece\u5934\u521b\u5efa\u667a\u80fd\u4f53\u793e\u4f1a\uff0c\u800c\u6a21\u62df\u5df2\u6709\u865a\u6784\u4e16\u754c\u548c\u89d2\u8272\u5c1a\u672a\u5145\u5206\u63a2\u7d22\uff0cBookWorld\u586b\u8865\u4e86\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u8bbe\u8ba1BookWorld\u7cfb\u7edf\uff0c\u6db5\u76d6\u89d2\u8272\u591a\u6837\u6027\u3001\u865a\u6784\u4e16\u754c\u89c2\u3001\u5730\u7406\u7ea6\u675f\u7b49\u73b0\u5b9e\u590d\u6742\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cBookWorld\u751f\u6210\u7684\u6545\u4e8b\u521b\u610f\u4e14\u9ad8\u8d28\u91cf\uff0c\u5fe0\u4e8e\u539f\u8457\uff0c\u80dc\u738775.36%\u3002", "conclusion": "BookWorld\u4e3a\u63a2\u7d22\u548c\u6269\u5c55\u865a\u6784\u4f5c\u54c1\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\uff0c\u5177\u6709\u663e\u8457\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2504.14051", "pdf": "https://arxiv.org/pdf/2504.14051", "abs": "https://arxiv.org/abs/2504.14051", "authors": ["Raghavv Goel", "Junyoung Park", "Mukul Gagrani", "Dalton Jones", "Matthew Morse", "Harper Langston", "Mingu Lee", "Chris Lott"], "title": "CAOTE: KV Caching through Attention Output Error based Token Eviction", "categories": ["cs.LG"], "comment": "14 pages, 2 figures", "summary": "While long context support of large language models has extended their\nabilities, it also incurs challenges in memory and compute which becomes\ncrucial bottlenecks in resource-restricted devices. Token eviction, a widely\nadopted post-training methodology designed to alleviate the bottlenecks by\nevicting less important tokens from the cache, typically uses attention scores\nas proxy metrics for token importance. However, one major limitation of\nattention score as a token-wise importance metrics is that it lacks the\ninformation about contribution of tokens to the attention output. In this\npaper, we propose a simple eviction criterion based on the contribution of\ncached tokens to attention outputs. Our method, CAOTE, optimizes for eviction\nerror due to token eviction, by seamlessly integrating attention scores and\nvalue vectors. This is the first method which uses value vector information on\ntop of attention-based eviction scores. Additionally, CAOTE can act as a\nmeta-heuristic method with flexible usage with any token eviction method. We\nshow that CAOTE, when combined with the state-of-the-art attention score-based\nmethods, always improves accuracies on the downstream task, indicating the\nimportance of leveraging information from values during token eviction process.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4ee4\u724c\u9a71\u9010\u6807\u51c6CAOTE\uff0c\u7ed3\u5408\u6ce8\u610f\u529b\u5206\u6570\u548c\u503c\u5411\u91cf\u4fe1\u606f\uff0c\u4f18\u5316\u9a71\u9010\u8bef\u5dee\uff0c\u63d0\u5347\u4e0b\u6e38\u4efb\u52a1\u51c6\u786e\u6027\u3002", "motivation": "\u957f\u4e0a\u4e0b\u6587\u652f\u6301\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u5b58\u5728\u5185\u5b58\u548c\u8ba1\u7b97\u74f6\u9888\uff0c\u73b0\u6709\u57fa\u4e8e\u6ce8\u610f\u529b\u5206\u6570\u7684\u4ee4\u724c\u9a71\u9010\u65b9\u6cd5\u7f3a\u4e4f\u5bf9\u4ee4\u724c\u8d21\u732e\u7684\u5168\u9762\u8bc4\u4f30\u3002", "method": "\u63d0\u51faCAOTE\u65b9\u6cd5\uff0c\u5229\u7528\u6ce8\u610f\u529b\u5206\u6570\u548c\u503c\u5411\u91cf\u4fe1\u606f\u4f18\u5316\u4ee4\u724c\u9a71\u9010\u8fc7\u7a0b\uff0c\u51cf\u5c11\u9a71\u9010\u8bef\u5dee\u3002", "result": "CAOTE\u7ed3\u5408\u73b0\u6709\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e0b\u6e38\u4efb\u52a1\u51c6\u786e\u6027\uff0c\u9a8c\u8bc1\u4e86\u503c\u5411\u91cf\u4fe1\u606f\u7684\u91cd\u8981\u6027\u3002", "conclusion": "CAOTE\u662f\u4e00\u79cd\u7075\u6d3b\u4e14\u9ad8\u6548\u7684\u4ee4\u724c\u9a71\u9010\u65b9\u6cd5\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.14350", "pdf": "https://arxiv.org/pdf/2504.14350", "abs": "https://arxiv.org/abs/2504.14350", "authors": ["Yi Sun", "Han Wang", "Jiaqiang Li", "Jiacheng Liu", "Xiangyu Li", "Hao Wen", "Huiwen Zheng", "Yan Liang", "Yuanchun Li", "Yunxin Liu"], "title": "Time Up! An Empirical Study of LLM Reasoning Ability Under Output Length Constraint", "categories": ["cs.AI"], "comment": null, "summary": "Recent work has demonstrated the remarkable potential of Large Language\nModels (LLMs) in test-time scaling. By making the models think before\nanswering, they are able to achieve much higher accuracy with extra inference\ncomputation. However, in many real-world scenarios, models are used under time\nconstraints, where an answer should be given to the user within a certain\noutput length. It is unclear whether and how the reasoning abilities of LLMs\nremain effective under such constraints. We take a first look at this problem\nby conducting an in-depth empirical study. Specifically, we test more than 25\nLLMs on common reasoning datasets under a wide range of output length budgets,\nand we analyze the correlation between the inference accuracy and various\nproperties including model type, model size, prompt style, etc. We also\nconsider the mappings between the token budgets and the actual on-device\nlatency budgets. The results have demonstrated several interesting findings\nregarding the budget-aware LLM reasoning that differ from the unconstrained\nsituation, e.g. the optimal choices of model sizes and prompts change under\ndifferent budgets. These findings offer practical guidance for users to deploy\nLLMs under real-world latency constraints.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u8f93\u51fa\u957f\u5ea6\u53d7\u9650\u65f6\u7684\u63a8\u7406\u80fd\u529b\u8868\u73b0\uff0c\u53d1\u73b0\u9884\u7b97\u7ea6\u675f\u4e0b\u6a21\u578b\u9009\u62e9\u548c\u63d0\u793a\u98ce\u683c\u7684\u6700\u4f18\u89e3\u4e0e\u65e0\u7ea6\u675f\u60c5\u51b5\u4e0d\u540c\u3002", "motivation": "\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\uff0c\u6a21\u578b\u5e38\u53d7\u65f6\u95f4\u6216\u8f93\u51fa\u957f\u5ea6\u9650\u5236\uff0c\u4f46LLMs\u5728\u6b64\u7c7b\u7ea6\u675f\u4e0b\u7684\u63a8\u7406\u80fd\u529b\u5c1a\u672a\u660e\u786e\u3002", "method": "\u5bf925\u79cd\u4ee5\u4e0aLLMs\u5728\u5e38\u89c1\u63a8\u7406\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5e7f\u6cdb\u6d4b\u8bd5\uff0c\u5206\u6790\u63a8\u7406\u51c6\u786e\u6027\u4e0e\u6a21\u578b\u7c7b\u578b\u3001\u5927\u5c0f\u3001\u63d0\u793a\u98ce\u683c\u7b49\u7684\u5173\u7cfb\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u9884\u7b97\u7ea6\u675f\u4e0b\u6a21\u578b\u5927\u5c0f\u548c\u63d0\u793a\u98ce\u683c\u7684\u6700\u4f18\u9009\u62e9\u4e0e\u65e0\u7ea6\u675f\u60c5\u51b5\u4e0d\u540c\uff0c\u5e76\u63d0\u4f9b\u4e86\u5b9e\u9645\u90e8\u7f72\u5efa\u8bae\u3002", "conclusion": "\u7814\u7a76\u4e3a\u5728\u5ef6\u8fdf\u7ea6\u675f\u4e0b\u90e8\u7f72LLMs\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\u3002"}}
{"id": "2504.14597", "pdf": "https://arxiv.org/pdf/2504.14597", "abs": "https://arxiv.org/abs/2504.14597", "authors": ["Lingrui Mei", "Shenghua Liu", "Yiwei Wang", "Baolong Bi", "Yuyao Ge", "Jun Wan", "Yurong Wu", "Xueqi Cheng"], "title": "a1: Steep Test-time Scaling Law via Environment Augmented Generation", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have made remarkable breakthroughs in reasoning,\nyet continue to struggle with hallucinations, logical errors, and inability to\nself-correct during complex multi-step tasks. Current approaches like\nchain-of-thought prompting offer limited reasoning capabilities that fail when\nprecise step validation is required. We propose Environment Augmented\nGeneration (EAG), a framework that enhances LLM reasoning through: (1)\nreal-time environmental feedback validating each reasoning step, (2) dynamic\nbranch exploration for investigating alternative solution paths when faced with\nerrors, and (3) experience-based learning from successful reasoning\ntrajectories. Unlike existing methods, EAG enables deliberate backtracking and\nstrategic replanning through tight integration of execution feedback with\nbranching exploration. Our a1-32B model achieves state-of-the-art performance\namong similar-sized models across all benchmarks, matching larger models like\no1 on competition mathematics while outperforming comparable models by up to\n24.4 percentage points. Analysis reveals EAG's distinctive scaling pattern:\ninitial token investment in environment interaction yields substantial\nlong-term performance dividends, with advantages amplifying proportionally to\ntask complexity. EAG's theoretical framework demonstrates how environment\ninteractivity and systematic branch exploration together establish a new\nparadigm for reliable machine reasoning, particularly for problems requiring\nprecise multi-step calculation and logical verification.", "AI": {"tldr": "EAG\u6846\u67b6\u901a\u8fc7\u5b9e\u65f6\u73af\u5883\u53cd\u9988\u3001\u52a8\u6001\u5206\u652f\u63a2\u7d22\u548c\u57fa\u4e8e\u7ecf\u9a8c\u7684\u5b66\u4e60\uff0c\u63d0\u5347LLM\u7684\u63a8\u7406\u80fd\u529b\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3LLM\u5728\u590d\u6742\u591a\u6b65\u4efb\u52a1\u4e2d\u7684\u5e7b\u89c9\u3001\u903b\u8f91\u9519\u8bef\u548c\u65e0\u6cd5\u81ea\u6211\u7ea0\u6b63\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faEAG\u6846\u67b6\uff0c\u5305\u62ec\u73af\u5883\u53cd\u9988\u9a8c\u8bc1\u3001\u52a8\u6001\u5206\u652f\u63a2\u7d22\u548c\u7ecf\u9a8c\u5b66\u4e60\u3002", "result": "a1-32B\u6a21\u578b\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f18\u4e8e\u540c\u7c7b\u6a21\u578b24.4\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "EAG\u4e3a\u53ef\u9760\u673a\u5668\u63a8\u7406\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\uff0c\u7279\u522b\u9002\u7528\u4e8e\u9700\u8981\u7cbe\u786e\u591a\u6b65\u8ba1\u7b97\u548c\u903b\u8f91\u9a8c\u8bc1\u7684\u95ee\u9898\u3002"}}
{"id": "2504.14068", "pdf": "https://arxiv.org/pdf/2504.14068", "abs": "https://arxiv.org/abs/2504.14068", "authors": ["K M Sajjadul Islam", "Ravi Teja Karri", "Srujan Vegesna", "Jiawei Wu", "Praveen Madiraju"], "title": "Contextual Embedding-based Clustering to Identify Topics for Healthcare Service Improvement", "categories": ["cs.LG", "cs.HC"], "comment": "Full version of the paper accepted at the 2025 IEEE COMPSAC, Toronto,\n  Canada", "summary": "Understanding patient feedback is crucial for improving healthcare services,\nyet analyzing unlabeled short-text feedback presents significant challenges due\nto limited data and domain-specific nuances. Traditional supervised learning\napproaches require extensive labeled datasets, making unsupervised methods more\nviable for uncovering meaningful insights from patient feedback. This study\nexplores unsupervised methods to extract meaningful topics from 439 survey\nresponses collected from a healthcare system in Wisconsin, USA. A keyword-based\nfiltering approach was applied to isolate complaint-related feedback using a\ndomain-specific lexicon. To delve deeper and analyze dominant topics in\nfeedback, we explored traditional topic modeling methods, including Latent\nDirichlet Allocation (LDA) and Gibbs Sampling Dirichlet Multinomial Mixture\n(GSDMM), alongside BERTopic, an advanced neural embedding-based clustering\napproach. To improve coherence and interpretability where data are scarce and\nconsist of short-texts, we propose kBERT, an integration of BERT embeddings\nwith k-means clustering. Model performance was assessed using coherence scores\n(Cv ) for topic interpretability and average Inverted Rank-Biased Overlap\n(IRBOavg) for topic diversity. Results indicate that kBERT achieves the highest\ncoherence (Cv = 0.53) and distinct topic separation (IRBOavg = 1.00),\noutperforming all other models in short-text healthcare feedback analysis. Our\nfindings emphasize the importance of embedding-based techniques for topic\nidentification and highlight the need for context-aware models in healthcare\nanalytics.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408BERT\u5d4c\u5165\u4e0ek-means\u805a\u7c7b\u7684\u65b9\u6cd5kBERT\uff0c\u7528\u4e8e\u5206\u6790\u533b\u7597\u77ed\u6587\u672c\u53cd\u9988\uff0c\u7ed3\u679c\u8868\u660ekBERT\u5728\u4e3b\u9898\u4e00\u81f4\u6027\u548c\u591a\u6837\u6027\u4e0a\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u5206\u6790\u672a\u6807\u8bb0\u7684\u77ed\u6587\u672c\u60a3\u8005\u53cd\u9988\u5bf9\u6539\u8fdb\u533b\u7597\u670d\u52a1\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u6807\u8bb0\u6570\u636e\uff0c\u56e0\u6b64\u63a2\u7d22\u65e0\u76d1\u7763\u65b9\u6cd5\u4ee5\u63d0\u53d6\u6709\u610f\u4e49\u7684\u4fe1\u606f\u3002", "method": "\u7814\u7a76\u6bd4\u8f83\u4e86\u4f20\u7edf\u4e3b\u9898\u5efa\u6a21\u65b9\u6cd5\uff08\u5982LDA\u548cGSDMM\uff09\u4e0eBERTopic\u548c\u63d0\u51fa\u7684kBERT\u65b9\u6cd5\uff0c\u540e\u8005\u7ed3\u5408BERT\u5d4c\u5165\u4e0ek-means\u805a\u7c7b\u3002", "result": "kBERT\u5728\u4e3b\u9898\u4e00\u81f4\u6027\uff08Cv=0.53\uff09\u548c\u591a\u6837\u6027\uff08IRBOavg=1.00\uff09\u4e0a\u8868\u73b0\u6700\u4f73\uff0c\u4f18\u4e8e\u5176\u4ed6\u6a21\u578b\u3002", "conclusion": "\u5d4c\u5165\u6280\u672f\u5bf9\u4e3b\u9898\u8bc6\u522b\u81f3\u5173\u91cd\u8981\uff0c\u533b\u7597\u5206\u6790\u9700\u8981\u4e0a\u4e0b\u6587\u611f\u77e5\u6a21\u578b\u3002"}}
{"id": "2504.14356", "pdf": "https://arxiv.org/pdf/2504.14356", "abs": "https://arxiv.org/abs/2504.14356", "authors": ["Masoud Ataei", "Edrin Hasaj", "Jacob Gipp", "Sepideh Forouzi"], "title": "Mathematical Programming Models for Exact and Interpretable Formulation of Neural Networks", "categories": ["cs.AI", "math.OC"], "comment": null, "summary": "This paper presents a unified mixed-integer programming framework for\ntraining sparse and interpretable neural networks. We develop exact\nformulations for both fully connected and convolutional architectures by\nmodeling nonlinearities such as ReLU activations through binary variables and\nencoding structural sparsity via filter- and layer-level pruning constraints.\nThe resulting models integrate parameter learning, architecture selection, and\nstructural regularization within a single optimization problem, yielding\nglobally optimal solutions with respect to a composite objective that balances\nprediction accuracy, weight sparsity, and architectural compactness. The\nmixed-integer programming formulation accommodates piecewise-linear operations,\nincluding max pooling and activation gating, and permits precise enforcement of\nlogic-based or domain-specific constraints. By incorporating considerations of\ninterpretability, sparsity, and verifiability directly into the training\nprocess, the proposed framework bridges a range of research areas including\nexplainable artificial intelligence, symbolic reasoning, and formal\nverification.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u6df7\u5408\u6574\u6570\u7f16\u7a0b\u6846\u67b6\uff0c\u7528\u4e8e\u8bad\u7ec3\u7a00\u758f\u4e14\u53ef\u89e3\u91ca\u7684\u795e\u7ecf\u7f51\u7edc\u3002\u901a\u8fc7\u4e8c\u5143\u53d8\u91cf\u5efa\u6a21\u975e\u7ebf\u6027\u6fc0\u6d3b\u51fd\u6570\uff08\u5982ReLU\uff09\u5e76\u7ed3\u5408\u7ed3\u6784\u7a00\u758f\u6027\u7ea6\u675f\uff0c\u5b9e\u73b0\u4e86\u53c2\u6570\u5b66\u4e60\u3001\u67b6\u6784\u9009\u62e9\u548c\u7ed3\u6784\u6b63\u5219\u5316\u7684\u5168\u5c40\u4f18\u5316\u3002", "motivation": "\u89e3\u51b3\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u4e2d\u53c2\u6570\u5b66\u4e60\u3001\u67b6\u6784\u9009\u62e9\u548c\u7ed3\u6784\u6b63\u5219\u5316\u7684\u5206\u79bb\u95ee\u9898\uff0c\u540c\u65f6\u63d0\u5347\u6a21\u578b\u7684\u7a00\u758f\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u9a8c\u8bc1\u6027\u3002", "method": "\u91c7\u7528\u6df7\u5408\u6574\u6570\u7f16\u7a0b\u6846\u67b6\uff0c\u901a\u8fc7\u4e8c\u5143\u53d8\u91cf\u5efa\u6a21\u975e\u7ebf\u6027\u6fc0\u6d3b\u51fd\u6570\uff08\u5982ReLU\uff09\uff0c\u5e76\u7ed3\u5408\u6ee4\u6ce2\u5668\u7ea7\u548c\u5c42\u7ea7\u7684\u526a\u679d\u7ea6\u675f\uff0c\u5b9e\u73b0\u5168\u5c40\u4f18\u5316\u3002", "result": "\u63d0\u51fa\u7684\u6846\u67b6\u80fd\u591f\u751f\u6210\u5168\u5c40\u6700\u4f18\u89e3\uff0c\u5e73\u8861\u9884\u6d4b\u7cbe\u5ea6\u3001\u6743\u91cd\u7a00\u758f\u6027\u548c\u67b6\u6784\u7d27\u51d1\u6027\uff0c\u540c\u65f6\u652f\u6301\u903b\u8f91\u6216\u9886\u57df\u7279\u5b9a\u7ea6\u675f\u7684\u7cbe\u786e\u5b9e\u65bd\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd\u3001\u7b26\u53f7\u63a8\u7406\u548c\u5f62\u5f0f\u9a8c\u8bc1\u7b49\u9886\u57df\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u6865\u6881\uff0c\u5b9e\u73b0\u4e86\u8bad\u7ec3\u8fc7\u7a0b\u7684\u7edf\u4e00\u4f18\u5316\u3002"}}
{"id": "2504.14619", "pdf": "https://arxiv.org/pdf/2504.14619", "abs": "https://arxiv.org/abs/2504.14619", "authors": ["Yuri Balashov", "Alex Balashov", "Shiho Fukuda Koski"], "title": "Translation Analytics for Freelancers: I. Introduction, Data Preparation, Baseline Evaluations", "categories": ["cs.CL"], "comment": "28 pages, 4 figures. Accepted at the MT Summit, University of Geneva,\n  June 2025", "summary": "This is the first in a series of papers exploring the rapidly expanding new\nopportunities arising from recent progress in language technologies for\nindividual translators and language service providers with modest resources.\nThe advent of advanced neural machine translation systems, large language\nmodels, and their integration into workflows via computer-assisted translation\ntools and translation management systems have reshaped the translation\nlandscape. These advancements enable not only translation but also quality\nevaluation, error spotting, glossary generation, and adaptation to\ndomain-specific needs, creating new technical opportunities for freelancers. In\nthis series, we aim to empower translators with actionable methods to harness\nthese advancements. Our approach emphasizes Translation Analytics, a suite of\nevaluation techniques traditionally reserved for large-scale industry\napplications but now becoming increasingly available for smaller-scale users.\nThis first paper introduces a practical framework for adapting automatic\nevaluation metrics -- such as BLEU, chrF, TER, and COMET -- to freelancers'\nneeds. We illustrate the potential of these metrics using a trilingual corpus\nderived from a real-world project in the medical domain and provide statistical\nanalysis correlating human evaluations with automatic scores. Our findings\nemphasize the importance of proactive engagement with emerging technologies to\nnot only adapt but thrive in the evolving professional environment.", "AI": {"tldr": "\u672c\u6587\u662f\u7cfb\u5217\u8bba\u6587\u7684\u7b2c\u4e00\u7bc7\uff0c\u63a2\u8ba8\u4e86\u8bed\u8a00\u6280\u672f\u7684\u6700\u65b0\u8fdb\u5c55\u4e3a\u4e2a\u4f53\u7ffb\u8bd1\u8005\u548c\u8d44\u6e90\u6709\u9650\u7684\u8bed\u8a00\u670d\u52a1\u63d0\u4f9b\u5546\u5e26\u6765\u7684\u65b0\u673a\u9047\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u9002\u5e94\u81ea\u7531\u804c\u4e1a\u8005\u9700\u6c42\u7684\u81ea\u52a8\u8bc4\u4f30\u6307\u6807\u6846\u67b6\u3002", "motivation": "\u968f\u7740\u795e\u7ecf\u673a\u5668\u7ffb\u8bd1\u7cfb\u7edf\u548c\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8fdb\u6b65\uff0c\u7ffb\u8bd1\u9886\u57df\u53d1\u751f\u4e86\u5de8\u5927\u53d8\u5316\u3002\u8fd9\u4e9b\u6280\u672f\u4e0d\u4ec5\u652f\u6301\u7ffb\u8bd1\uff0c\u8fd8\u80fd\u8fdb\u884c\u8d28\u91cf\u8bc4\u4f30\u3001\u9519\u8bef\u8bc6\u522b\u3001\u672f\u8bed\u751f\u6210\u7b49\uff0c\u4e3a\u81ea\u7531\u804c\u4e1a\u8005\u521b\u9020\u4e86\u65b0\u673a\u4f1a\u3002\u672c\u6587\u65e8\u5728\u5e2e\u52a9\u7ffb\u8bd1\u8005\u5229\u7528\u8fd9\u4e9b\u6280\u672f\u8fdb\u6b65\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5b9e\u7528\u6846\u67b6\uff0c\u5c06\u81ea\u52a8\u8bc4\u4f30\u6307\u6807\uff08\u5982BLEU\u3001chrF\u3001TER\u548cCOMET\uff09\u9002\u5e94\u81ea\u7531\u804c\u4e1a\u8005\u7684\u9700\u6c42\uff0c\u5e76\u901a\u8fc7\u4e00\u4e2a\u533b\u5b66\u9886\u57df\u7684\u4e09\u8bed\u6599\u5e93\u8fdb\u884c\u5b9e\u8bc1\u5206\u6790\u3002", "result": "\u901a\u8fc7\u7edf\u8ba1\u5206\u6790\uff0c\u672c\u6587\u5c55\u793a\u4e86\u81ea\u52a8\u8bc4\u4f30\u6307\u6807\u4e0e\u4eba\u5de5\u8bc4\u4ef7\u4e4b\u95f4\u7684\u76f8\u5173\u6027\uff0c\u9a8c\u8bc1\u4e86\u8fd9\u4e9b\u6307\u6807\u5728\u81ea\u7531\u804c\u4e1a\u8005\u573a\u666f\u4e2d\u7684\u5b9e\u7528\u6027\u3002", "conclusion": "\u672c\u6587\u5f3a\u8c03\u7ffb\u8bd1\u8005\u5e94\u79ef\u6781\u62e5\u62b1\u65b0\u5174\u6280\u672f\uff0c\u4ee5\u5728\u4e0d\u65ad\u53d8\u5316\u7684\u804c\u4e1a\u73af\u5883\u4e2d\u9002\u5e94\u5e76\u53d6\u5f97\u6210\u529f\u3002"}}
{"id": "2504.14094", "pdf": "https://arxiv.org/pdf/2504.14094", "abs": "https://arxiv.org/abs/2504.14094", "authors": ["Enrico Parisini", "Tapabrata Chakraborti", "Chris Harbron", "Ben D. MacArthur", "Christopher R. S. Banerji"], "title": "Leakage and Interpretability in Concept-Based Models", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "38 pages, 27 figures", "summary": "Concept Bottleneck Models aim to improve interpretability by predicting\nhigh-level intermediate concepts, representing a promising approach for\ndeployment in high-risk scenarios. However, they are known to suffer from\ninformation leakage, whereby models exploit unintended information encoded\nwithin the learned concepts. We introduce an information-theoretic framework to\nrigorously characterise and quantify leakage, and define two complementary\nmeasures: the concepts-task leakage (CTL) and interconcept leakage (ICL)\nscores. We show that these measures are strongly predictive of model behaviour\nunder interventions and outperform existing alternatives in robustness and\nreliability. Using this framework, we identify the primary causes of leakage\nand provide strong evidence that Concept Embedding Models exhibit substantial\nleakage regardless of the hyperparameters choice. Finally, we propose practical\nguidelines for designing concept-based models to reduce leakage and ensure\ninterpretability.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4fe1\u606f\u8bba\u6846\u67b6\uff0c\u7528\u4e8e\u91cf\u5316\u6982\u5ff5\u74f6\u9888\u6a21\u578b\u4e2d\u7684\u4fe1\u606f\u6cc4\u6f0f\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u4e24\u79cd\u6cc4\u6f0f\u5ea6\u91cf\u6307\u6807\uff08CTL\u548cICL\uff09\u3002\u7814\u7a76\u53d1\u73b0\u6982\u5ff5\u5d4c\u5165\u6a21\u578b\u666e\u904d\u5b58\u5728\u6cc4\u6f0f\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u51cf\u5c11\u6cc4\u6f0f\u7684\u5b9e\u7528\u6307\u5357\u3002", "motivation": "\u6982\u5ff5\u74f6\u9888\u6a21\u578b\u5728\u9ad8\u98ce\u9669\u573a\u666f\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u5b58\u5728\u4fe1\u606f\u6cc4\u6f0f\u95ee\u9898\uff0c\u5f71\u54cd\u6a21\u578b\u7684\u53ef\u9760\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u5f15\u5165\u4fe1\u606f\u8bba\u6846\u67b6\uff0c\u5b9a\u4e49CTL\u548cICL\u4e24\u79cd\u6cc4\u6f0f\u5ea6\u91cf\u6307\u6807\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "result": "CTL\u548cICL\u6307\u6807\u80fd\u6709\u6548\u9884\u6d4b\u6a21\u578b\u884c\u4e3a\uff0c\u4e14\u6982\u5ff5\u5d4c\u5165\u6a21\u578b\u666e\u904d\u5b58\u5728\u6cc4\u6f0f\u95ee\u9898\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u4e86\u51cf\u5c11\u6cc4\u6f0f\u7684\u5b9e\u7528\u6307\u5357\uff0c\u4e3a\u8bbe\u8ba1\u66f4\u53ef\u9760\u7684\u6982\u5ff5\u6a21\u578b\u63d0\u4f9b\u4e86\u4f9d\u636e\u3002"}}
{"id": "2504.14379", "pdf": "https://arxiv.org/pdf/2504.14379", "abs": "https://arxiv.org/abs/2504.14379", "authors": ["Andrew Lee", "Lihao Sun", "Chris Wendler", "Fernanda Vi\u00e9gas", "Martin Wattenberg"], "title": "The Geometry of Self-Verification in a Task-Specific Reasoning Model", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "How do reasoning models verify their own answers? We study this question by\ntraining a model using DeepSeek R1's recipe on the CountDown task. We leverage\nthe fact that preference tuning leads to mode collapse, resulting in a model\nthat always produces highly structured and easily parse-able chain-of-thought\nsequences. With this setup, we do a top-down and bottom-up analysis to\nreverse-engineer how the model verifies its outputs. Our top-down analysis\nreveals Gated Linear Unit (GLU) weights encoding verification-related tokens,\nsuch as ``success'' or ``incorrect'', which activate according to the\ncorrectness of the model's reasoning steps. Our bottom-up analysis reveals that\n``previous-token heads'' are mainly responsible for model verification. Our\nanalyses meet in the middle: drawing inspiration from inter-layer communication\nchannels, we use the identified GLU vectors to localize as few as three\nattention heads that can disable model verification, pointing to a necessary\ncomponent of a potentially larger verification circuit.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u63a8\u7406\u6a21\u578b\u5982\u4f55\u9a8c\u8bc1\u81ea\u8eab\u7b54\u6848\uff0c\u901a\u8fc7\u8bad\u7ec3\u6a21\u578b\u5e76\u5206\u6790\u5176\u9a8c\u8bc1\u673a\u5236\uff0c\u53d1\u73b0GLU\u6743\u91cd\u548c\u6ce8\u610f\u529b\u5934\u5728\u9a8c\u8bc1\u8fc7\u7a0b\u4e2d\u8d77\u5173\u952e\u4f5c\u7528\u3002", "motivation": "\u63a2\u7a76\u63a8\u7406\u6a21\u578b\u5982\u4f55\u9a8c\u8bc1\u81ea\u8eab\u7b54\u6848\uff0c\u4ee5\u7406\u89e3\u5176\u5185\u90e8\u5de5\u4f5c\u673a\u5236\u3002", "method": "\u4f7f\u7528DeepSeek R1\u7684\u914d\u65b9\u8bad\u7ec3\u6a21\u578b\uff0c\u901a\u8fc7\u81ea\u4e0a\u800c\u4e0b\u548c\u81ea\u4e0b\u800c\u4e0a\u7684\u5206\u6790\uff0c\u7814\u7a76\u6a21\u578b\u7684\u9a8c\u8bc1\u673a\u5236\u3002", "result": "\u53d1\u73b0GLU\u6743\u91cd\u7f16\u7801\u9a8c\u8bc1\u76f8\u5173\u6807\u8bb0\uff0c\u6ce8\u610f\u529b\u5934\u662f\u9a8c\u8bc1\u7684\u5173\u952e\u7ec4\u4ef6\u3002", "conclusion": "\u9a8c\u8bc1\u673a\u5236\u53ef\u80fd\u7531\u66f4\u5927\u7684\u7535\u8def\u7ec4\u6210\uff0c\u4f46\u5df2\u8bc6\u522b\u51fa\u5173\u952e\u7ec4\u4ef6\u3002"}}
{"id": "2504.14620", "pdf": "https://arxiv.org/pdf/2504.14620", "abs": "https://arxiv.org/abs/2504.14620", "authors": ["Hongming Tan", "Shaoxiong Zhan", "Fengwei Jia", "Hai-Tao Zheng", "Wai Kin Chan"], "title": "A Hierarchical Framework for Measuring Scientific Paper Innovation via Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Measuring scientific paper innovation is both important and challenging.\nExisting content-based methods often overlook the full-paper context, fail to\ncapture the full scope of innovation, and lack generalization. We propose\nHSPIM, a hierarchical and training-free framework based on large language\nmodels (LLMs). It introduces a Paper-to-Sections-to-QAs decomposition to assess\ninnovation. We segment the text by section titles and use zero-shot LLM\nprompting to implement section classification, question-answering (QA)\naugmentation, and weighted novelty scoring. The generated QA pair focuses on\nsection-level innovation and serves as additional context to improve the LLM\nscoring. For each chunk, the LLM outputs a novelty score and a confidence\nscore. We use confidence scores as weights to aggregate novelty scores into a\npaper-level innovation score. To further improve performance, we propose a\ntwo-layer question structure consisting of common and section-specific\nquestions, and apply a genetic algorithm to optimize the question-prompt\ncombinations. Comprehensive experiments on scientific conference paper datasets\nshow that HSPIM outperforms baseline methods in effectiveness, generalization,\nand interpretability.", "AI": {"tldr": "HSPIM\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5c42\u6b21\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u79d1\u5b66\u8bba\u6587\u7684\u521b\u65b0\u6027\uff0c\u901a\u8fc7\u5206\u89e3\u8bba\u6587\u4e3a\u90e8\u5206\u548c\u95ee\u7b54\u5bf9\uff0c\u7ed3\u5408\u96f6\u6837\u672c\u63d0\u793a\u548c\u52a0\u6743\u8bc4\u5206\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u8bc4\u4f30\u8bba\u6587\u521b\u65b0\u6027\u65f6\u7f3a\u4e4f\u4e0a\u4e0b\u6587\u548c\u6cdb\u5316\u80fd\u529b\uff0cHSPIM\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "HSPIM\u91c7\u7528\u5206\u5c42\u7ed3\u6784\uff0c\u5c06\u8bba\u6587\u5206\u89e3\u4e3a\u90e8\u5206\u548c\u95ee\u7b54\u5bf9\uff0c\u5229\u7528\u96f6\u6837\u672cLLM\u63d0\u793a\u8fdb\u884c\u5206\u7c7b\u3001\u95ee\u7b54\u589e\u5f3a\u548c\u52a0\u6743\u8bc4\u5206\u3002", "result": "\u5728\u79d1\u5b66\u4f1a\u8bae\u8bba\u6587\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cHSPIM\u5728\u6709\u6548\u6027\u3001\u6cdb\u5316\u6027\u548c\u53ef\u89e3\u91ca\u6027\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "HSPIM\u4e3a\u79d1\u5b66\u8bba\u6587\u521b\u65b0\u6027\u8bc4\u4f30\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u63a8\u5e7f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.14095", "pdf": "https://arxiv.org/pdf/2504.14095", "abs": "https://arxiv.org/abs/2504.14095", "authors": ["Athar Mahmoudi-Nejad", "Matthew Guzdial", "Pierre Boulanger"], "title": "Personalizing Exposure Therapy via Reinforcement Learning", "categories": ["cs.LG"], "comment": "AAAI 2025 Bridge (Collaborative AI and Modelling of Humans Bridge\n  Program)", "summary": "Personalized therapy, in which a therapeutic practice is adapted to an\nindividual patient, can lead to improved health outcomes. Typically, this is\naccomplished by relying on a therapist's training and intuition along with\nfeedback from a patient. However, this requires the therapist to become an\nexpert on any technological components, such as in the case of Virtual Reality\nExposure Therapy (VRET). While there exist approaches to automatically adapt\ntherapeutic content to a patient, they generally rely on hand-authored,\npre-defined rules, which may not generalize to all individuals. In this paper,\nwe propose an approach to automatically adapt therapeutic content to patients\nbased on physiological measures. We implement our approach in the context of\nvirtual reality arachnophobia exposure therapy, and rely on experience-driven\nprocedural content generation via reinforcement learning (EDPCGRL) to generate\nvirtual spiders to match an individual patient. Through a human subject study,\nwe demonstrate that our system significantly outperforms a more common\nrules-based method, highlighting its potential for enhancing personalized\ntherapeutic interventions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u751f\u7406\u6307\u6807\u7684\u4e2a\u6027\u5316\u6cbb\u7597\u5185\u5bb9\u81ea\u52a8\u9002\u914d\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u751f\u6210\u865a\u62df\u8718\u86db\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u89c4\u5219\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u4e2a\u6027\u5316\u6cbb\u7597\u4f9d\u8d56\u6cbb\u7597\u5e08\u7684\u7ecf\u9a8c\u548c\u9884\u5b9a\u4e49\u89c4\u5219\uff0c\u96be\u4ee5\u666e\u9002\u5316\uff0c\u9700\u6280\u672f\u6539\u8fdb\u3002", "method": "\u91c7\u7528\u7ecf\u9a8c\u9a71\u52a8\u7684\u7a0b\u5e8f\u5316\u5185\u5bb9\u751f\u6210\uff08EDPCGRL\uff09\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u57fa\u4e8e\u751f\u7406\u6307\u6807\u52a8\u6001\u751f\u6210\u6cbb\u7597\u5185\u5bb9\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u865a\u62df\u73b0\u5b9e\u8718\u86db\u6050\u60e7\u75c7\u6cbb\u7597\u4e2d\u663e\u8457\u4f18\u4e8e\u89c4\u5219\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6f5c\u529b\u63d0\u5347\u4e2a\u6027\u5316\u6cbb\u7597\u5e72\u9884\u6548\u679c\u3002"}}
{"id": "2504.14448", "pdf": "https://arxiv.org/pdf/2504.14448", "abs": "https://arxiv.org/abs/2504.14448", "authors": ["Ali Arslan Yousaf", "Umair Rehman", "Muhammad Umair Danish"], "title": "Seeing Through Risk: A Symbolic Approximation of Prospect Theory", "categories": ["cs.AI", "cs.LG", "math.OC", "stat.ME"], "comment": null, "summary": "We propose a novel symbolic modeling framework for decision-making under risk\nthat merges interpretability with the core insights of Prospect Theory. Our\napproach replaces opaque utility curves and probability weighting functions\nwith transparent, effect-size-guided features. We mathematically formalize the\nmethod, demonstrate its ability to replicate well-known framing and\nloss-aversion phenomena, and provide an end-to-end empirical validation on\nsynthetic datasets. The resulting model achieves competitive predictive\nperformance while yielding clear coefficients mapped onto psychological\nconstructs, making it suitable for applications ranging from AI safety to\neconomic policy analysis.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7b26\u53f7\u5efa\u6a21\u6846\u67b6\uff0c\u5c06\u53ef\u89e3\u91ca\u6027\u4e0e\u524d\u666f\u7406\u8bba\u7684\u6838\u5fc3\u89c1\u89e3\u7ed3\u5408\uff0c\u7528\u4e8e\u98ce\u9669\u51b3\u7b56\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u51b3\u7b56\u6a21\u578b\u4e2d\u6548\u7528\u66f2\u7ebf\u548c\u6982\u7387\u6743\u91cd\u51fd\u6570\u4e0d\u900f\u660e\u7684\u95ee\u9898\uff0c\u63d0\u4f9b\u66f4\u900f\u660e\u7684\u7279\u5f81\u3002", "method": "\u91c7\u7528\u6548\u679c\u5927\u5c0f\u5f15\u5bfc\u7684\u7279\u5f81\u66ff\u4ee3\u4f20\u7edf\u51fd\u6570\uff0c\u6570\u5b66\u5f62\u5f0f\u5316\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u5408\u6210\u6570\u636e\u96c6\u8fdb\u884c\u7aef\u5230\u7aef\u9a8c\u8bc1\u3002", "result": "\u6a21\u578b\u5728\u9884\u6d4b\u6027\u80fd\u4e0a\u5177\u6709\u7ade\u4e89\u529b\uff0c\u4e14\u80fd\u6e05\u6670\u6620\u5c04\u5fc3\u7406\u6784\u9020\u7cfb\u6570\u3002", "conclusion": "\u8be5\u6a21\u578b\u9002\u7528\u4e8e\u4eceAI\u5b89\u5168\u5230\u7ecf\u6d4e\u653f\u7b56\u5206\u6790\u7684\u591a\u79cd\u5e94\u7528\u3002"}}
{"id": "2504.14630", "pdf": "https://arxiv.org/pdf/2504.14630", "abs": "https://arxiv.org/abs/2504.14630", "authors": ["Rondik Hadi Abdulrahman", "Hossein Hassani"], "title": "Automatic Text Summarization (ATS) for Research Documents in Sorani Kurdish", "categories": ["cs.CL"], "comment": "18 pages, 11 figures, 8 tables", "summary": "Extracting concise information from scientific documents aids learners,\nresearchers, and practitioners. Automatic Text Summarization (ATS), a key\nNatural Language Processing (NLP) application, automates this process. While\nATS methods exist for many languages, Kurdish remains underdeveloped due to\nlimited resources. This study develops a dataset and language model based on\n231 scientific papers in Sorani Kurdish, collected from four academic\ndepartments in two universities in the Kurdistan Region of Iraq (KRI),\naveraging 26 pages per document. Using Sentence Weighting and Term\nFrequency-Inverse Document Frequency (TF-IDF) algorithms, two experiments were\nconducted, differing in whether the conclusions were included. The average word\ncount was 5,492.3 in the first experiment and 5,266.96 in the second. Results\nwere evaluated manually and automatically using ROUGE-1, ROUGE-2, and ROUGE-L\nmetrics, with the best accuracy reaching 19.58%. Six experts conducted manual\nevaluations using three criteria, with results varying by document. This\nresearch provides valuable resources for Kurdish NLP researchers to advance ATS\nand related fields.", "AI": {"tldr": "\u8be5\u7814\u7a76\u4e3a\u5e93\u5c14\u5fb7\u8bed\uff08Sorani\u65b9\u8a00\uff09\u5f00\u53d1\u4e86\u4e00\u4e2a\u6570\u636e\u96c6\u548c\u8bed\u8a00\u6a21\u578b\uff0c\u7528\u4e8e\u81ea\u52a8\u6587\u672c\u6458\u8981\uff08ATS\uff09\uff0c\u586b\u8865\u4e86\u8be5\u8bed\u8a00\u8d44\u6e90\u4e0d\u8db3\u7684\u7a7a\u767d\u3002\u901a\u8fc7\u4e24\u79cd\u5b9e\u9a8c\uff08\u662f\u5426\u5305\u542b\u7ed3\u8bba\uff09\u548c\u591a\u79cd\u8bc4\u4f30\u65b9\u6cd5\uff0c\u6700\u4f73\u51c6\u786e\u7387\u8fbe\u523019.58%\u3002", "motivation": "\u5e93\u5c14\u5fb7\u8bed\uff08Sorani\u65b9\u8a00\uff09\u7684\u81ea\u52a8\u6587\u672c\u6458\u8981\u7814\u7a76\u8d44\u6e90\u532e\u4e4f\uff0c\u9650\u5236\u4e86\u8be5\u8bed\u8a00\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u9886\u57df\u7684\u53d1\u5c55\u3002", "method": "\u7814\u7a76\u57fa\u4e8e231\u7bc7\u5e93\u5c14\u5fb7\u8bed\u79d1\u5b66\u8bba\u6587\uff0c\u4f7f\u7528\u53e5\u5b50\u52a0\u6743\u548cTF-IDF\u7b97\u6cd5\u8fdb\u884c\u4e24\u79cd\u5b9e\u9a8c\uff08\u662f\u5426\u5305\u542b\u7ed3\u8bba\uff09\uff0c\u5e76\u901a\u8fc7\u624b\u52a8\u548c\u81ea\u52a8\uff08ROUGE\u6307\u6807\uff09\u8bc4\u4f30\u7ed3\u679c\u3002", "result": "\u6700\u4f73\u51c6\u786e\u7387\u4e3a19.58%\uff0c\u5b9e\u9a8c\u7ed3\u679c\u56e0\u6587\u6863\u800c\u5f02\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5e93\u5c14\u5fb7\u8bedNLP\u7814\u7a76\u8005\u63d0\u4f9b\u4e86\u5b9d\u8d35\u8d44\u6e90\uff0c\u63a8\u52a8\u4e86\u81ea\u52a8\u6587\u672c\u6458\u8981\u53ca\u76f8\u5173\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2504.14098", "pdf": "https://arxiv.org/pdf/2504.14098", "abs": "https://arxiv.org/abs/2504.14098", "authors": ["Justus R\u00e5munddal"], "title": "Enhancing Math Learning in an LMS Using AI-Driven Question Recommendations", "categories": ["cs.LG", "cs.AI", "cs.CY", "cs.IR"], "comment": "15 pages, 9 figures, 4 tables", "summary": "This paper presents an AI-driven approach to enhance math learning in a\nmodern Learning Management System (LMS) by recommending similar math questions.\nDeep embeddings for math questions are generated using Meta's\nLlama-3.2-11B-Vision-Instruct model, and three recommendation methods-cosine\nsimilarity, Self-Organizing Maps (SOM), and Gaussian Mixture Models (GMM)-are\napplied to identify similar questions. User interaction data, including session\ndurations, response times, and correctness, are used to evaluate the methods.\nOur findings suggest that while cosine similarity produces nearly identical\nquestion matches, SOM yields higher user satisfaction whereas GMM generally\nunderperforms, indicating that introducing variety to a certain degree may\nenhance engagement and thereby potential learning outcomes until variety is no\nlonger balanced reasonably, which our data about the implementations of all\nthree methods demonstrate.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eAI\u7684\u6570\u5b66\u5b66\u4e60\u589e\u5f3a\u65b9\u6cd5\uff0c\u901a\u8fc7\u63a8\u8350\u76f8\u4f3c\u6570\u5b66\u95ee\u9898\u6765\u4f18\u5316\u73b0\u4ee3\u5b66\u4e60\u7ba1\u7406\u7cfb\u7edf\uff08LMS\uff09\u3002\u4f7f\u7528Meta\u7684Llama-3.2-11B-Vision-Instruct\u6a21\u578b\u751f\u6210\u6570\u5b66\u95ee\u9898\u7684\u6df1\u5ea6\u5d4c\u5165\uff0c\u5e76\u6bd4\u8f83\u4e86\u4f59\u5f26\u76f8\u4f3c\u5ea6\u3001\u81ea\u7ec4\u7ec7\u6620\u5c04\uff08SOM\uff09\u548c\u9ad8\u65af\u6df7\u5408\u6a21\u578b\uff08GMM\uff09\u4e09\u79cd\u63a8\u8350\u65b9\u6cd5\u3002\u5b9e\u9a8c\u8868\u660e\uff0cSOM\u5728\u7528\u6237\u6ee1\u610f\u5ea6\u4e0a\u8868\u73b0\u6700\u4f73\uff0c\u800cGMM\u6548\u679c\u8f83\u5dee\u3002", "motivation": "\u73b0\u4ee3LMS\u9700\u8981\u66f4\u667a\u80fd\u7684\u6570\u5b66\u5b66\u4e60\u8f85\u52a9\u5de5\u5177\uff0c\u901a\u8fc7\u63a8\u8350\u76f8\u4f3c\u95ee\u9898\u63d0\u5347\u5b66\u4e60\u6548\u679c\u3002", "method": "\u4f7f\u7528Llama-3.2-11B-Vision-Instruct\u6a21\u578b\u751f\u6210\u95ee\u9898\u5d4c\u5165\uff0c\u5e76\u6bd4\u8f83\u4f59\u5f26\u76f8\u4f3c\u5ea6\u3001SOM\u548cGMM\u4e09\u79cd\u63a8\u8350\u65b9\u6cd5\u3002", "result": "\u4f59\u5f26\u76f8\u4f3c\u5ea6\u5339\u914d\u95ee\u9898\u6700\u63a5\u8fd1\uff0cSOM\u7528\u6237\u6ee1\u610f\u5ea6\u6700\u9ad8\uff0cGMM\u8868\u73b0\u8f83\u5dee\u3002\u9002\u5ea6\u591a\u6837\u6027\u53ef\u63d0\u5347\u5b66\u4e60\u6548\u679c\u3002", "conclusion": "SOM\u662f\u63a8\u8350\u76f8\u4f3c\u6570\u5b66\u95ee\u9898\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u4f46\u9700\u5e73\u8861\u591a\u6837\u6027\u4ee5\u4f18\u5316\u5b66\u4e60\u6548\u679c\u3002"}}
{"id": "2504.14520", "pdf": "https://arxiv.org/pdf/2504.14520", "abs": "https://arxiv.org/abs/2504.14520", "authors": ["Ahsan Bilal", "Muhammad Ahmed Mohsin", "Muhammad Umer", "Muhammad Awais Khan Bangash", "Muhammad Ali Jamshed"], "title": "Meta-Thinking in LLMs via Multi-Agent Reinforcement Learning: A Survey", "categories": ["cs.AI", "cs.CL"], "comment": "Submitted to IEEE Transactions on Artificial Intelligence", "summary": "This survey explores the development of meta-thinking capabilities in Large\nLanguage Models (LLMs) from a Multi-Agent Reinforcement Learning (MARL)\nperspective. Meta-thinking self-reflection, assessment, and control of thinking\nprocesses is an important next step in enhancing LLM reliability, flexibility,\nand performance, particularly for complex or high-stakes tasks. The survey\nbegins by analyzing current LLM limitations, such as hallucinations and the\nlack of internal self-assessment mechanisms. It then talks about newer methods,\nincluding RL from human feedback (RLHF), self-distillation, and\nchain-of-thought prompting, and each of their limitations. The crux of the\nsurvey is to talk about how multi-agent architectures, namely supervisor-agent\nhierarchies, agent debates, and theory of mind frameworks, can emulate\nhuman-like introspective behavior and enhance LLM robustness. By exploring\nreward mechanisms, self-play, and continuous learning methods in MARL, this\nsurvey gives a comprehensive roadmap to building introspective, adaptive, and\ntrustworthy LLMs. Evaluation metrics, datasets, and future research avenues,\nincluding neuroscience-inspired architectures and hybrid symbolic reasoning,\nare also discussed.", "AI": {"tldr": "\u8be5\u7efc\u8ff0\u63a2\u8ba8\u4e86\u4ece\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff08MARL\uff09\u89d2\u5ea6\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u5143\u601d\u7ef4\u80fd\u529b\uff0c\u4ee5\u589e\u5f3a\u5176\u53ef\u9760\u6027\u3001\u7075\u6d3b\u6027\u548c\u6027\u80fd\u3002", "motivation": "\u5f53\u524dLLMs\u5b58\u5728\u5e7b\u89c9\u548c\u7f3a\u4e4f\u81ea\u6211\u8bc4\u4f30\u673a\u5236\u7b49\u95ee\u9898\uff0c\u9700\u901a\u8fc7\u5143\u601d\u7ef4\uff08\u5982\u81ea\u6211\u53cd\u601d\u3001\u8bc4\u4f30\u548c\u63a7\u5236\uff09\u8fdb\u4e00\u6b65\u63d0\u5347\u5176\u5728\u9ad8\u98ce\u9669\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u5206\u6790\u4e86\u73b0\u6709\u65b9\u6cd5\uff08\u5982RLHF\u3001\u81ea\u84b8\u998f\u548c\u601d\u7ef4\u94fe\u63d0\u793a\uff09\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63a2\u8ba8\u4e86\u591a\u667a\u80fd\u4f53\u67b6\u6784\uff08\u5982\u76d1\u7763\u8005-\u667a\u80fd\u4f53\u5c42\u7ea7\u3001\u667a\u80fd\u4f53\u8fa9\u8bba\u548c\u5fc3\u7406\u7406\u8bba\u6846\u67b6\uff09\u5982\u4f55\u6a21\u62df\u4eba\u7c7b\u5185\u7701\u884c\u4e3a\u3002", "result": "\u63d0\u51fa\u4e86\u901a\u8fc7MARL\u7684\u5956\u52b1\u673a\u5236\u3001\u81ea\u535a\u5f08\u548c\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\u6784\u5efa\u5185\u7701\u3001\u81ea\u9002\u5e94\u548c\u53ef\u4fe1\u8d56LLMs\u7684\u8def\u7ebf\u56fe\u3002", "conclusion": "\u8ba8\u8bba\u4e86\u8bc4\u4f30\u6307\u6807\u3001\u6570\u636e\u96c6\u53ca\u672a\u6765\u7814\u7a76\u65b9\u5411\uff08\u5982\u795e\u7ecf\u79d1\u5b66\u542f\u53d1\u67b6\u6784\u548c\u6df7\u5408\u7b26\u53f7\u63a8\u7406\uff09\uff0c\u4e3a\u5f00\u53d1\u66f4\u5f3a\u5927\u7684LLMs\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002"}}
{"id": "2504.14633", "pdf": "https://arxiv.org/pdf/2504.14633", "abs": "https://arxiv.org/abs/2504.14633", "authors": ["Soo-joon Choi", "Ji-jun Park"], "title": "Harnessing Generative LLMs for Enhanced Financial Event Entity Extraction Performance", "categories": ["cs.CL"], "comment": null, "summary": "Financial event entity extraction is a crucial task for analyzing market\ndynamics and building financial knowledge graphs, yet it presents significant\nchallenges due to the specialized language and complex structures in financial\ntexts. Traditional approaches often rely on sequence labeling models, which can\nstruggle with long-range dependencies and the inherent complexity of extracting\nmultiple, potentially overlapping entities. Motivated by the advanced language\nunderstanding and generative capabilities of Large Language Models (LLMs), we\npropose a novel method that reframes financial event entity extraction as a\ntext-to-structured-output generation task. Our approach involves fine-tuning a\npre-trained LLM using Parameter-Efficient Fine-Tuning (PEFT) to directly\ngenerate a structured representation, such as a JSON object, containing the\nextracted entities and their precise character spans from the input text. We\nevaluate our method on the challenging CCKS 2019 Financial Event Entity\nExtraction dataset, comparing its performance against strong sequence labeling\nbaselines, including SEBERTNets and sebertNets. Experimental results\ndemonstrate that our generative LLM method achieves a new state-of-the-art F1\nscore on this benchmark, significantly outperforming previous methods. Through\ndetailed quantitative analysis across event types, entity types, and instance\ncomplexity, as well as human evaluation, we show that our approach is more\neffective at handling the nuances of financial text and extracting high-quality\nentities. This work validates the potential of applying generative LLMs\ndirectly to complex, domain-specific information extraction tasks requiring\nstructured output.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u751f\u6210\u5f0f\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u91d1\u878d\u4e8b\u4ef6\u5b9e\u4f53\u63d0\u53d6\u65b9\u6cd5\uff0c\u901a\u8fc7\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff08PEFT\uff09\u5c06\u4efb\u52a1\u8f6c\u5316\u4e3a\u7ed3\u6784\u5316\u8f93\u51fa\u751f\u6210\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u91d1\u878d\u6587\u672c\u8bed\u8a00\u590d\u6742\u4e14\u5b9e\u4f53\u91cd\u53e0\uff0c\u4f20\u7edf\u5e8f\u5217\u6807\u6ce8\u6a21\u578b\u96be\u4ee5\u5904\u7406\u957f\u8ddd\u79bb\u4f9d\u8d56\u548c\u591a\u5b9e\u4f53\u63d0\u53d6\u95ee\u9898\uff0c\u56e0\u6b64\u63a2\u7d22\u751f\u6210\u5f0fLLM\u7684\u6f5c\u529b\u3002", "method": "\u5c06\u91d1\u878d\u4e8b\u4ef6\u5b9e\u4f53\u63d0\u53d6\u4efb\u52a1\u91cd\u65b0\u5b9a\u4e49\u4e3a\u6587\u672c\u5230\u7ed3\u6784\u5316\u8f93\u51fa\u7684\u751f\u6210\u4efb\u52a1\uff0c\u4f7f\u7528PEFT\u5fae\u8c03\u9884\u8bad\u7ec3LLM\u76f4\u63a5\u751f\u6210\u5305\u542b\u5b9e\u4f53\u53ca\u5176\u5b57\u7b26\u8de8\u5ea6\u7684JSON\u5bf9\u8c61\u3002", "result": "\u5728CCKS 2019\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u65b0\u7684\u6700\u4f18F1\u5206\u6570\uff0c\u663e\u8457\u4f18\u4e8e\u5e8f\u5217\u6807\u6ce8\u57fa\u7ebf\u6a21\u578b\uff08\u5982SEBERTNets\uff09\u3002", "conclusion": "\u751f\u6210\u5f0fLLM\u5728\u590d\u6742\u9886\u57df\u7279\u5b9a\u4fe1\u606f\u63d0\u53d6\u4efb\u52a1\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u53ef\u76f4\u63a5\u751f\u6210\u9ad8\u8d28\u91cf\u7ed3\u6784\u5316\u8f93\u51fa\u3002"}}
{"id": "2504.14143", "pdf": "https://arxiv.org/pdf/2504.14143", "abs": "https://arxiv.org/abs/2504.14143", "authors": ["Zeping Chen", "Marwa Yacouti", "Maryam Shakiba", "Jian-Xun Wang", "Tengfei Luo", "Vikas Varshney"], "title": "Predicting Stress and Damage in Carbon Fiber-Reinforced Composites Deformation Process using Composite U-Net Surrogate Model", "categories": ["cs.LG"], "comment": null, "summary": "Carbon fiber-reinforced composites (CFRC) are pivotal in advanced engineering\napplications due to their exceptional mechanical properties. A deep\nunderstanding of CFRC behavior under mechanical loading is essential for\noptimizing performance in demanding applications such as aerospace structures.\nWhile traditional Finite Element Method (FEM) simulations, including advanced\ntechniques like Interface-enriched Generalized FEM (IGFEM), offer valuable\ninsights, they can struggle with computational efficiency. Existing data-driven\nsurrogate models partially address these challenges by predicting propagated\ndamage or stress-strain behavior but fail to comprehensively capture the\nevolution of stress and damage throughout the entire deformation history,\nincluding crack initiation and propagation. This study proposes a novel\nauto-regressive composite U-Net deep learning model to simultaneously predict\nstress and damage fields during CFRC deformation. By leveraging the U-Net\narchitecture's ability to capture spatial features and integrate macro- and\nmicro-scale phenomena, the proposed model overcomes key limitations of prior\napproaches. The model achieves high accuracy in predicting evolution of stress\nand damage distribution within the microstructure of a CFRC under\nunidirectional strain, offering a speed-up of over 60 times compared to IGFEM.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u81ea\u56de\u5f52\u590d\u5408U-Net\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u7528\u4e8e\u540c\u65f6\u9884\u6d4b\u78b3\u7ea4\u7ef4\u589e\u5f3a\u590d\u5408\u6750\u6599\uff08CFRC\uff09\u53d8\u5f62\u8fc7\u7a0b\u4e2d\u7684\u5e94\u529b\u548c\u635f\u4f24\u573a\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u6709\u9650\u5143\u65b9\u6cd5\uff08FEM\uff09\u5728\u8ba1\u7b97\u6548\u7387\u4e0a\u5b58\u5728\u4e0d\u8db3\uff0c\u73b0\u6709\u6570\u636e\u9a71\u52a8\u6a21\u578b\u65e0\u6cd5\u5168\u9762\u6355\u6349\u5e94\u529b\u548c\u635f\u4f24\u7684\u6f14\u5316\u8fc7\u7a0b\u3002", "method": "\u91c7\u7528U-Net\u67b6\u6784\u7684\u81ea\u56de\u5f52\u590d\u5408\u6a21\u578b\uff0c\u7ed3\u5408\u5b8f\u5fae\u89c2\u7279\u5f81\uff0c\u9884\u6d4bCFRC\u5728\u5355\u5411\u5e94\u53d8\u4e0b\u7684\u5e94\u529b\u548c\u635f\u4f24\u5206\u5e03\u3002", "result": "\u6a21\u578b\u9884\u6d4b\u7cbe\u5ea6\u9ad8\uff0c\u8ba1\u7b97\u901f\u5ea6\u6bd4IGFEM\u5feb60\u500d\u4ee5\u4e0a\u3002", "conclusion": "\u8be5\u6a21\u578b\u4e3aCFRC\u7684\u6027\u80fd\u4f18\u5316\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u9884\u6d4b\u5de5\u5177\u3002"}}
{"id": "2504.14523", "pdf": "https://arxiv.org/pdf/2504.14523", "abs": "https://arxiv.org/abs/2504.14523", "authors": ["Gabriela Ben Melech Stan", "Estelle Aflalo", "Avinash Madasu", "Vasudev Lal", "Phillip Howard"], "title": "Learning from Reasoning Failures via Synthetic Data Generation", "categories": ["cs.AI"], "comment": null, "summary": "Training models on synthetic data has emerged as an increasingly important\nstrategy for improving the performance of generative AI. This approach is\nparticularly helpful for large multimodal models (LMMs) due to the relative\nscarcity of high-quality paired image-text data compared to language-only data.\nWhile a variety of methods have been proposed for generating large multimodal\ndatasets, they do not tailor the synthetic data to address specific\ndeficiencies in the reasoning abilities of LMMs which will be trained with the\ngenerated dataset. In contrast, humans often learn in a more efficient manner\nby seeking out examples related to the types of reasoning where they have\nfailed previously. Inspired by this observation, we propose a new approach for\nsynthetic data generation which is grounded in the analysis of an existing\nLMM's reasoning failures. Our methodology leverages frontier models to\nautomatically analyze errors produced by a weaker LMM and propose new examples\nwhich can be used to correct the reasoning failure via additional training,\nwhich are then further filtered to ensure high quality. We generate a large\nmultimodal instruction tuning dataset containing over 553k examples using our\napproach and conduct extensive experiments demonstrating its utility for\nimproving the performance of LMMs on multiple downstream tasks. Our results\nshow that models trained on our synthetic data can even exceed the performance\nof LMMs trained on an equivalent amount of additional real data, demonstrating\nthe high value of generating synthetic data targeted to specific reasoning\nfailure modes in LMMs. We will make our dataset and code publicly available.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5206\u6790\u73b0\u6709\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\uff08LMM\uff09\u63a8\u7406\u5931\u8d25\u7684\u65b0\u65b9\u6cd5\uff0c\u751f\u6210\u9488\u5bf9\u6027\u5408\u6210\u6570\u636e\u4ee5\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u7531\u4e8e\u9ad8\u8d28\u91cf\u914d\u5bf9\u56fe\u50cf-\u6587\u672c\u6570\u636e\u7684\u7a00\u7f3a\uff0c\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u9488\u5bf9LMM\u7684\u7279\u5b9a\u63a8\u7406\u7f3a\u9677\u751f\u6210\u6570\u636e\uff0c\u800c\u4eba\u7c7b\u5b66\u4e60\u65b9\u5f0f\u66f4\u9ad8\u6548\u3002", "method": "\u5229\u7528\u524d\u6cbf\u6a21\u578b\u5206\u6790\u8f83\u5f31LMM\u7684\u9519\u8bef\uff0c\u751f\u6210\u7ea0\u6b63\u63a8\u7406\u5931\u8d25\u7684\u793a\u4f8b\uff0c\u5e76\u901a\u8fc7\u8fc7\u6ee4\u786e\u4fdd\u8d28\u91cf\u3002", "result": "\u751f\u6210\u4e86553k\u793a\u4f8b\u7684\u6570\u636e\u96c6\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u80fd\u663e\u8457\u63d0\u5347LMM\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u751a\u81f3\u4f18\u4e8e\u540c\u7b49\u91cf\u771f\u5b9e\u6570\u636e\u3002", "conclusion": "\u9488\u5bf9\u6027\u751f\u6210\u5408\u6210\u6570\u636e\u5bf9\u6539\u8fdbLMM\u63a8\u7406\u80fd\u529b\u5177\u6709\u91cd\u8981\u4ef7\u503c\uff0c\u6570\u636e\u96c6\u548c\u4ee3\u7801\u5c06\u516c\u5f00\u3002"}}
{"id": "2504.14657", "pdf": "https://arxiv.org/pdf/2504.14657", "abs": "https://arxiv.org/abs/2504.14657", "authors": ["Yihan Lin", "Zhirong Bella Yu", "Simon Lee"], "title": "A Case Study Exploring the Current Landscape of Synthetic Medical Record Generation with Commercial LLMs", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted at the Conference of Health, Inference, Learning (CHIL 2025)\n  in Berkeley, CA. To appear in PMLR later in 2025", "summary": "Synthetic Electronic Health Records (EHRs) offer a valuable opportunity to\ncreate privacy preserving and harmonized structured data, supporting numerous\napplications in healthcare. Key benefits of synthetic data include precise\ncontrol over the data schema, improved fairness and representation of patient\npopulations, and the ability to share datasets without concerns about\ncompromising real individuals privacy. Consequently, the AI community has\nincreasingly turned to Large Language Models (LLMs) to generate synthetic data\nacross various domains. However, a significant challenge in healthcare is\nensuring that synthetic health records reliably generalize across different\nhospitals, a long standing issue in the field. In this work, we evaluate the\ncurrent state of commercial LLMs for generating synthetic data and investigate\nmultiple aspects of the generation process to identify areas where these models\nexcel and where they fall short. Our main finding from this work is that while\nLLMs can reliably generate synthetic health records for smaller subsets of\nfeatures, they struggle to preserve realistic distributions and correlations as\nthe dimensionality of the data increases, ultimately limiting their ability to\ngeneralize across diverse hospital settings.", "AI": {"tldr": "LLMs\u751f\u6210\u5408\u6210\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\uff08EHRs\uff09\u5728\u9690\u79c1\u4fdd\u62a4\u548c\u6570\u636e\u5171\u4eab\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u5728\u9ad8\u7ef4\u6570\u636e\u4e2d\u96be\u4ee5\u4fdd\u6301\u771f\u5b9e\u5206\u5e03\u548c\u76f8\u5173\u6027\uff0c\u9650\u5236\u4e86\u5176\u8de8\u533b\u9662\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u5408\u6210EHRs\u5728\u8de8\u533b\u9662\u6cdb\u5316\u4e2d\u7684\u53ef\u9760\u6027\u95ee\u9898\uff0c\u8bc4\u4f30LLMs\u5728\u751f\u6210\u5408\u6210\u6570\u636e\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u8bc4\u4f30\u5546\u4e1aLLMs\u751f\u6210\u5408\u6210EHRs\u7684\u80fd\u529b\uff0c\u5206\u6790\u751f\u6210\u8fc7\u7a0b\u4e2d\u7684\u591a\u4e2a\u65b9\u9762\u3002", "result": "LLMs\u5728\u5c0f\u89c4\u6a21\u7279\u5f81\u96c6\u4e0a\u8868\u73b0\u53ef\u9760\uff0c\u4f46\u5728\u9ad8\u7ef4\u6570\u636e\u4e2d\u96be\u4ee5\u4fdd\u6301\u771f\u5b9e\u5206\u5e03\u548c\u76f8\u5173\u6027\u3002", "conclusion": "LLMs\u5728\u751f\u6210\u5408\u6210EHRs\u65b9\u9762\u6709\u6f5c\u529b\uff0c\u4f46\u9700\u6539\u8fdb\u4ee5\u5e94\u5bf9\u9ad8\u7ef4\u6570\u636e\u7684\u6311\u6218\u3002"}}
{"id": "2504.14174", "pdf": "https://arxiv.org/pdf/2504.14174", "abs": "https://arxiv.org/abs/2504.14174", "authors": ["Jing Han", "Hanting Chen", "Kai Han", "Xiaomeng Huang", "Yongyun Hu", "Wenjun Xu", "Dacheng Tao", "Ping Zhang"], "title": "A Physics-guided Multimodal Transformer Path to Weather and Climate Sciences", "categories": ["cs.LG", "cs.AI"], "comment": "Perspective article", "summary": "With the rapid development of machine learning in recent years, many problems\nin meteorology can now be addressed using AI models. In particular, data-driven\nalgorithms have significantly improved accuracy compared to traditional\nmethods. Meteorological data is often transformed into 2D images or 3D videos,\nwhich are then fed into AI models for learning. Additionally, these models\noften incorporate physical signals, such as temperature, pressure, and wind\nspeed, to further enhance accuracy and interpretability. In this paper, we\nreview several representative AI + Weather/Climate algorithms and propose a new\nparadigm where observational data from different perspectives, each with\ndistinct physical meanings, are treated as multimodal data and integrated via\ntransformers. Furthermore, key weather and climate knowledge can be\nincorporated through regularization techniques to further strengthen the\nmodel's capabilities. This new paradigm is versatile and can address a variety\nof tasks, offering strong generalizability. We also discuss future directions\nfor improving model accuracy and interpretability.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86AI\u5728\u6c14\u8c61\u5b66\u4e2d\u7684\u5e94\u7528\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u6a21\u6001\u6570\u636e\u548cTransformer\u7684\u65b0\u8303\u5f0f\uff0c\u7ed3\u5408\u7269\u7406\u4fe1\u53f7\u548c\u6b63\u5219\u5316\u6280\u672f\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u6c14\u8c61\u5b66\u65b9\u6cd5\u7cbe\u5ea6\u6709\u9650\uff0cAI\u6a21\u578b\u80fd\u663e\u8457\u63d0\u5347\u51c6\u786e\u6027\uff0c\u540c\u65f6\u7ed3\u5408\u7269\u7406\u4fe1\u53f7\u589e\u5f3a\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u5c06\u6c14\u8c61\u6570\u636e\u8f6c\u5316\u4e3a2D\u56fe\u50cf\u62163D\u89c6\u9891\uff0c\u4f5c\u4e3a\u591a\u6a21\u6001\u6570\u636e\u8f93\u5165Transformer\u6a21\u578b\uff0c\u5e76\u5229\u7528\u6b63\u5219\u5316\u6280\u672f\u878d\u5165\u6c14\u8c61\u77e5\u8bc6\u3002", "result": "\u65b0\u8303\u5f0f\u5177\u6709\u5f3a\u901a\u7528\u6027\uff0c\u53ef\u5904\u7406\u591a\u79cd\u4efb\u52a1\uff0c\u5e76\u63d0\u5347\u6a21\u578b\u7cbe\u5ea6\u548c\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u672a\u6765\u7814\u7a76\u65b9\u5411\u5305\u62ec\u8fdb\u4e00\u6b65\u4f18\u5316\u6a21\u578b\u7cbe\u5ea6\u548c\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2504.14556", "pdf": "https://arxiv.org/pdf/2504.14556", "abs": "https://arxiv.org/abs/2504.14556", "authors": ["Yousef Emami", "Hao Gao", "SeyedSina Nabavirazani", "Luis Almeida"], "title": "LLM-Enabled In-Context Learning for Data Collection Scheduling in UAV-assisted Sensor Networks", "categories": ["cs.AI", "cs.ET", "cs.LG", "cs.RO", "53-01", "C.2"], "comment": "8 pages, 7 figures,", "summary": "Unmanned Aerial Vehicles (UAVs) are increasingly being used in various\nprivate and commercial applications, e.g. traffic control, package delivery,\nand Search and Rescue (SAR) operations. Machine Learning (ML) methods used in\nUAV-assisted Sensor Networks (UASNETs) and especially in Deep Reinforcement\nLearning (DRL) face challenges such as complex and lengthy model training, gaps\nbetween simulation and reality, and low sample efficiency, which conflict with\nthe urgency of emergencies such as SAR operations. This paper proposes\nIn-Context Learning (ICL)-based Data Collection Scheduling (ICLDC) scheme, as\nan alternative to DRL in emergencies. The UAV collects and transmits logged\nsensory data, to an LLM, to generate a task description in natural language,\nfrom which it obtains a data collection schedule to be executed by the UAV. The\nsystem continuously adapts by adding feedback to task descriptions and\nutilizing feedback for future decisions. This method is tested against\njailbreaking attacks, where task description is manipulated to undermine\nnetwork performance, highlighting the vulnerability of LLMs to such attacks.\nThe proposed ICLDC outperforms the Maximum Channel Gain by reducing cumulative\npacket loss by approximately 56\\%. ICLDC presents a promising direction for\nintelligent scheduling and control in UAV-assisted data collection.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08ICL\uff09\u7684\u6570\u636e\u6536\u96c6\u8c03\u5ea6\u65b9\u6848\uff08ICLDC\uff09\uff0c\u7528\u4e8e\u65e0\u4eba\u673a\u8f85\u52a9\u4f20\u611f\u5668\u7f51\u7edc\u4e2d\u7684\u7d27\u6025\u4efb\u52a1\uff0c\u5982\u641c\u6551\u884c\u52a8\uff0c\u4ee5\u89e3\u51b3\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u65e0\u4eba\u673a\u5728\u7d27\u6025\u4efb\u52a1\uff08\u5982\u641c\u6551\uff09\u4e2d\u7684\u5e94\u7528\u9700\u8981\u9ad8\u6548\u4e14\u9002\u5e94\u6027\u5f3a\u7684\u8c03\u5ea6\u65b9\u6cd5\uff0c\u800c\u73b0\u6709\u7684DRL\u65b9\u6cd5\u5b58\u5728\u8bad\u7ec3\u590d\u6742\u3001\u4eff\u771f\u4e0e\u73b0\u5b9e\u5dee\u8ddd\u5927\u3001\u6837\u672c\u6548\u7387\u4f4e\u7b49\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u65e0\u4eba\u673a\u6536\u96c6\u5e76\u4f20\u8f93\u4f20\u611f\u5668\u6570\u636e\u81f3\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\uff0c\u751f\u6210\u81ea\u7136\u8bed\u8a00\u4efb\u52a1\u63cf\u8ff0\uff0c\u5e76\u4ece\u4e2d\u83b7\u53d6\u6570\u636e\u6536\u96c6\u8c03\u5ea6\u8ba1\u5212\u3002\u7cfb\u7edf\u901a\u8fc7\u53cd\u9988\u673a\u5236\u6301\u7eed\u4f18\u5316\u51b3\u7b56\u3002", "result": "ICLDC\u5728\u5bf9\u6297\u7be1\u6539\u653b\u51fb\u65f6\u8868\u73b0\u51fa\u8272\uff0c\u6bd4\u6700\u5927\u4fe1\u9053\u589e\u76ca\u65b9\u6848\u51cf\u5c11\u4e86\u7ea656%\u7684\u7d2f\u79ef\u4e22\u5305\u7387\u3002", "conclusion": "ICLDC\u4e3a\u65e0\u4eba\u673a\u8f85\u52a9\u6570\u636e\u6536\u96c6\u63d0\u4f9b\u4e86\u667a\u80fd\u8c03\u5ea6\u548c\u63a7\u5236\u7684\u65b0\u65b9\u5411\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2504.14669", "pdf": "https://arxiv.org/pdf/2504.14669", "abs": "https://arxiv.org/abs/2504.14669", "authors": ["Wei Zou", "Sen Yang", "Yu Bao", "Shujian Huang", "Jiajun Chen", "Shanbo Cheng"], "title": "Trans-Zero: Self-Play Incentivizes Large Language Models for Multilingual Translation Without Parallel Data", "categories": ["cs.CL"], "comment": "11 pages, 4 figures", "summary": "The rise of Large Language Models (LLMs) has reshaped machine translation\n(MT), but multilingual MT still relies heavily on parallel data for supervised\nfine-tuning (SFT), facing challenges like data scarcity for low-resource\nlanguages and catastrophic forgetting. To address these issues, we propose\nTRANS-ZERO, a self-play framework that leverages only monolingual data and the\nintrinsic multilingual knowledge of LLM. TRANS-ZERO combines Genetic\nMonte-Carlo Tree Search (G-MCTS) with preference optimization, achieving strong\ntranslation performance that rivals supervised methods. Experiments demonstrate\nthat this approach not only matches the performance of models trained on\nlarge-scale parallel data but also excels in non-English translation\ndirections. Further analysis reveals that G-MCTS itself significantly enhances\ntranslation quality by exploring semantically consistent candidates through\niterative translations, providing a robust foundation for the framework's\nsuccuss.", "AI": {"tldr": "TRANS-ZERO\u662f\u4e00\u4e2a\u57fa\u4e8e\u81ea\u535a\u5f08\u7684\u6846\u67b6\uff0c\u4ec5\u5229\u7528\u5355\u8bed\u6570\u636e\u548cLLM\u7684\u591a\u8bed\u8a00\u77e5\u8bc6\uff0c\u901a\u8fc7G-MCTS\u548c\u504f\u597d\u4f18\u5316\uff0c\u5b9e\u73b0\u4e86\u4e0e\u76d1\u7763\u65b9\u6cd5\u5ab2\u7f8e\u7684\u7ffb\u8bd1\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u591a\u8bed\u8a00\u673a\u5668\u7ffb\u8bd1\u4e2d\u4f4e\u8d44\u6e90\u8bed\u8a00\u6570\u636e\u7a00\u7f3a\u548c\u707e\u96be\u6027\u9057\u5fd8\u7684\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u9057\u4f20\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff08G-MCTS\uff09\u548c\u504f\u597d\u4f18\u5316\uff0c\u4ec5\u4f7f\u7528\u5355\u8bed\u6570\u636e\u548cLLM\u7684\u591a\u8bed\u8a00\u77e5\u8bc6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u5339\u914d\u5927\u89c4\u6a21\u5e76\u884c\u6570\u636e\u8bad\u7ec3\u6a21\u578b\u7684\u6027\u80fd\uff0c\u8fd8\u5728\u975e\u82f1\u8bed\u7ffb\u8bd1\u65b9\u5411\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "G-MCTS\u901a\u8fc7\u8fed\u4ee3\u7ffb\u8bd1\u63a2\u7d22\u8bed\u4e49\u4e00\u81f4\u7684\u5019\u9009\uff0c\u663e\u8457\u63d0\u5347\u7ffb\u8bd1\u8d28\u91cf\uff0c\u4e3a\u6846\u67b6\u7684\u6210\u529f\u5960\u5b9a\u57fa\u7840\u3002"}}
{"id": "2504.14188", "pdf": "https://arxiv.org/pdf/2504.14188", "abs": "https://arxiv.org/abs/2504.14188", "authors": ["Zekai Chen", "Xunkai Li", "Yinlin Zhu", "Rong-Hua Li", "Guoren Wang"], "title": "FedC4: Graph Condensation Meets Client-Client Collaboration for Efficient and Private Federated Graph Learning", "categories": ["cs.LG"], "comment": "10 pages, 7 figures; references added", "summary": "Federated Graph Learning (FGL) is an emerging distributed learning paradigm\nthat enables collaborative model training over decentralized graph-structured\ndata while preserving local privacy. Existing FGL methods can be categorized\ninto two optimization architectures: (1) the Server-Client (S-C) paradigm,\nwhere clients upload local models for server-side aggregation; and (2) the\nClient-Client (C-C) paradigm, which allows direct information exchange among\nclients to support personalized training. Compared to S-C, the C-C architecture\nbetter captures global graph knowledge and enables fine-grained optimization\nthrough customized peer-to-peer communication. However, current C-C methods\noften broadcast identical and redundant node embeddings, incurring high\ncommunication costs and privacy risks. To address this, we propose FedC4, a\nnovel framework that combines graph Condensation with Client-Client\nCollaboration. Instead of transmitting raw node-level features, FedC4 distills\neach client's private graph into a compact set of synthetic node embeddings,\nreducing communication overhead and enhancing privacy. In addition, FedC4\nintroduces three modules that allow source clients to send distinct node\nrepresentations tailored to target clients'graph structures, enabling\npersonalized optimization with global guidance. Extensive experiments on eight\nreal-world datasets show that FedC4 outperforms state-of-the-art baselines in\nboth performance and communication efficiency.", "AI": {"tldr": "FedC4\u662f\u4e00\u79cd\u7ed3\u5408\u56fe\u538b\u7f29\u4e0e\u5ba2\u6237\u7aef\u534f\u4f5c\u7684\u8054\u90a6\u56fe\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u4f20\u8f93\u5408\u6210\u8282\u70b9\u5d4c\u5165\u51cf\u5c11\u901a\u4fe1\u5f00\u9500\u548c\u9690\u79c1\u98ce\u9669\uff0c\u5e76\u652f\u6301\u4e2a\u6027\u5316\u4f18\u5316\u3002", "motivation": "\u73b0\u6709\u5ba2\u6237\u7aef-\u5ba2\u6237\u7aef\uff08C-C\uff09\u8054\u90a6\u56fe\u5b66\u4e60\u65b9\u6cd5\u56e0\u5e7f\u64ad\u76f8\u540c\u8282\u70b9\u5d4c\u5165\u5bfc\u81f4\u9ad8\u901a\u4fe1\u6210\u672c\u548c\u9690\u79c1\u98ce\u9669\uff0c\u9700\u6539\u8fdb\u3002", "method": "\u63d0\u51faFedC4\u6846\u67b6\uff0c\u5c06\u79c1\u6709\u56fe\u538b\u7f29\u4e3a\u5408\u6210\u8282\u70b9\u5d4c\u5165\uff0c\u5e76\u5f15\u5165\u4e09\u4e2a\u6a21\u5757\u652f\u6301\u4e2a\u6027\u5316\u4f18\u5316\u3002", "result": "\u5728\u516b\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\uff0cFedC4\u5728\u6027\u80fd\u548c\u901a\u4fe1\u6548\u7387\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "FedC4\u901a\u8fc7\u56fe\u538b\u7f29\u548c\u4e2a\u6027\u5316\u534f\u4f5c\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8054\u90a6\u56fe\u5b66\u4e60\u7684\u6548\u7387\u548c\u9690\u79c1\u4fdd\u62a4\u80fd\u529b\u3002"}}
{"id": "2504.14596", "pdf": "https://arxiv.org/pdf/2504.14596", "abs": "https://arxiv.org/abs/2504.14596", "authors": ["Kei Itoh"], "title": "Toward the Axiomatization of Intelligence: Structure, Time, and Existence", "categories": ["cs.AI", "cs.NE"], "comment": "37 pages, 4 tables, in English, in Japanese", "summary": "This study aims to construct an axiomatic definition of intelligence within a\nmeta-framework that defines the method of definition, addressing intelligence\nas an inherently naive and polysemous concept. Initially, we formalize a\nset-theoretic representation of the universe as the domain wherein intelligence\nexists and characterize intelligence as a structure that involves temporal\nevolution and interaction with other sets. Starting from a naive definition of\nintelligence as \"an entity possessing structures for externally inputting,\ninternally processing, and externally outputting information or matter,\" we\naxiomatically reformulate it within this set-theoretical depiction of the\nuniverse. Applying this axiomatic definition, we compare and interpret three\nexamples -- Hebbian non-optimized neural networks (NNs),\nbackpropagation-optimized NNs, and biological reflexive systems -- in terms of\ntheir intelligence, structural properties, and biological plausibility.\nFurthermore, by extending our definition into a categorical framework, we\nintroduce two categories, \"Time Category\" and \"Intelligence Category,\" along\nwith the functorial relationships between them, demonstrating the potential to\nrepresent changes and mimicry relationships among intelligent systems\nabstractly. Additionally, since intelligence, as defined herein, functions\neffectively only when accompanied by temporal interactions, we introduce the\nconcept of \"activity\" and explore how activity-based conditions influence\nclassifications and interpretations of intelligence. Finally, we suggest that\nour definitional methodology is not limited to intelligence alone, but can be\nsimilarly applied to other concepts, such as consciousness and emotion,\nadvocating for their formal reinterpretation through the same procedural steps:\ndefining a universal representation, selecting naive definitions, and axiomatic\nformalization.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u96c6\u5408\u8bba\u548c\u8303\u7574\u8bba\u6784\u5efa\u4e86\u4e00\u4e2a\u667a\u80fd\u7684\u516c\u7406\u5316\u5b9a\u4e49\uff0c\u5e76\u5e94\u7528\u4e8e\u4e0d\u540c\u7c7b\u578b\u7684\u667a\u80fd\u7cfb\u7edf\uff0c\u63a2\u8ba8\u4e86\u65f6\u95f4\u4e0e\u6d3b\u52a8\u5bf9\u667a\u80fd\u7684\u5f71\u54cd\u3002", "motivation": "\u667a\u80fd\u662f\u4e00\u4e2a\u591a\u4e49\u4e14\u6a21\u7cca\u7684\u6982\u5ff5\uff0c\u9700\u8981\u4e00\u4e2a\u5f62\u5f0f\u5316\u7684\u5b9a\u4e49\u6846\u67b6\u6765\u7edf\u4e00\u7406\u89e3\u3002", "method": "\u4f7f\u7528\u96c6\u5408\u8bba\u8868\u793a\u5b87\u5b99\u57df\uff0c\u5b9a\u4e49\u667a\u80fd\u4e3a\u6d89\u53ca\u65f6\u95f4\u6f14\u5316\u548c\u4ea4\u4e92\u7684\u7ed3\u6784\uff0c\u5e76\u901a\u8fc7\u8303\u7574\u8bba\u6269\u5c55\u5b9a\u4e49\u3002", "result": "\u6bd4\u8f83\u4e86\u4e09\u79cd\u667a\u80fd\u7cfb\u7edf\u7684\u7ed3\u6784\u7279\u6027\uff0c\u63d0\u51fa\u4e86\u201c\u6d3b\u52a8\u201d\u6982\u5ff5\uff0c\u5e76\u5c55\u793a\u4e86\u5b9a\u4e49\u65b9\u6cd5\u7684\u666e\u9002\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u9002\u7528\u4e8e\u667a\u80fd\uff0c\u8fd8\u53ef\u63a8\u5e7f\u5230\u610f\u8bc6\u3001\u60c5\u611f\u7b49\u5176\u4ed6\u6982\u5ff5\u7684\u5f62\u5f0f\u5316\u5b9a\u4e49\u3002"}}
{"id": "2504.14690", "pdf": "https://arxiv.org/pdf/2504.14690", "abs": "https://arxiv.org/abs/2504.14690", "authors": ["Mehrnoush Shamsfard", "Zahra Saaberi", "Mostafa Karimi manesh", "Seyed Mohammad Hossein Hashemi", "Zahra Vatankhah", "Motahareh Ramezani", "Niki Pourazin", "Tara Zare", "Maryam Azimi", "Sarina Chitsaz", "Sama Khoraminejad", "Morteza Mahdavi Mortazavi", "Mohammad Mahdi Chizari", "Sahar Maleki", "Seyed Soroush Majd", "Mostafa Masumi", "Sayed Ali Musavi Khoeini", "Amir Mohseni", "Sogol Alipour"], "title": "FarsEval-PKBETS: A new diverse benchmark for evaluating Persian large language models", "categories": ["cs.CL", "cs.AI", "68T50", "I.2.7; E.0"], "comment": "24 pages, 3 figures, 3 tables", "summary": "Research on evaluating and analyzing large language models (LLMs) has been\nextensive for resource-rich languages such as English, yet their performance in\nlanguages such as Persian has received considerably less attention. This paper\nintroduces FarsEval-PKBETS benchmark, a subset of FarsEval project for\nevaluating large language models in Persian. This benchmark consists of 4000\nquestions and answers in various formats, including multiple choice, short\nanswer and descriptive responses. It covers a wide range of domains and\ntasks,including medicine, law, religion, Persian language, encyclopedic\nknowledge, human preferences, social knowledge, ethics and bias, text\ngeneration, and respecting others' rights. This bechmark incorporates\nlinguistics, cultural, and local considerations relevant to the Persian\nlanguage and Iran. To ensure the questions are challenging for current LLMs,\nthree models -- Llama3-70B, PersianMind, and Dorna -- were evaluated using this\nbenchmark. Their average accuracy was below 50%, meaning they provided fully\ncorrect answers to fewer than half of the questions. These results indicate\nthat current language models are still far from being able to solve this\nbenchmark", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86FarsEval-PKBETS\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u6ce2\u65af\u8bed\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u6027\u80fd\uff0c\u7ed3\u679c\u663e\u793a\u5f53\u524d\u6a21\u578b\u8868\u73b0\u4e0d\u4f73\u3002", "motivation": "\u7814\u7a76\u6ce2\u65af\u8bed\u7b49\u8d44\u6e90\u8f83\u5c11\u8bed\u8a00\u7684LLMs\u6027\u80fd\uff0c\u586b\u8865\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u6784\u5efa\u5305\u542b4000\u4e2a\u95ee\u9898\u7684\u6ce2\u65af\u8bed\u57fa\u51c6\uff0c\u6db5\u76d6\u591a\u9886\u57df\u4efb\u52a1\uff0c\u5e76\u8bc4\u4f30\u4e09\u4e2aLLMs\u6a21\u578b\u3002", "result": "\u4e09\u4e2a\u6a21\u578b\u7684\u5e73\u5747\u51c6\u786e\u7387\u4f4e\u4e8e50%\uff0c\u8868\u660e\u5f53\u524d\u6a21\u578b\u96be\u4ee5\u5e94\u5bf9\u8be5\u57fa\u51c6\u3002", "conclusion": "\u6ce2\u65af\u8bedLLMs\u4ecd\u9700\u6539\u8fdb\uff0c\u57fa\u51c6\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2504.14204", "pdf": "https://arxiv.org/pdf/2504.14204", "abs": "https://arxiv.org/abs/2504.14204", "authors": ["Wenxin Zhang", "Xiaojian Lin", "Wenjun Yu", "Guangzhen Yao", "jingxiang Zhong", "Yu Li", "Renda Han", "Songcheng Xu", "Hao Shi", "Cuicui Luo"], "title": "DConAD: A Differencing-based Contrastive Representation Learning Framework for Time Series Anomaly Detection", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Time series anomaly detection holds notable importance for risk\nidentification and fault detection across diverse application domains.\nUnsupervised learning methods have become popular because they have no\nrequirement for labels. However, due to the challenges posed by the\nmultiplicity of abnormal patterns, the sparsity of anomalies, and the growth of\ndata scale and complexity, these methods often fail to capture robust and\nrepresentative dependencies within the time series for identifying anomalies.\nTo enhance the ability of models to capture normal patterns of time series and\navoid the retrogression of modeling ability triggered by the dependencies on\nhigh-quality prior knowledge, we propose a differencing-based contrastive\nrepresentation learning framework for time series anomaly detection (DConAD).\nSpecifically, DConAD generates differential data to provide additional\ninformation about time series and utilizes transformer-based architecture to\ncapture spatiotemporal dependencies, which enhances the robustness of unbiased\nrepresentation learning ability. Furthermore, DConAD implements a novel KL\ndivergence-based contrastive learning paradigm that only uses positive samples\nto avoid deviation from reconstruction and deploys the stop-gradient strategy\nto compel convergence. Extensive experiments on five public datasets show the\nsuperiority and effectiveness of DConAD compared with nine baselines. The code\nis available at https://github.com/shaieesss/DConAD.", "AI": {"tldr": "DConAD\u662f\u4e00\u79cd\u57fa\u4e8e\u5dee\u5206\u5bf9\u6bd4\u8868\u793a\u5b66\u4e60\u7684\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u6210\u5dee\u5206\u6570\u636e\u548cTransformer\u67b6\u6784\u589e\u5f3a\u9c81\u68d2\u6027\uff0c\u4f18\u4e8e\u4e5d\u79cd\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u5728\u98ce\u9669\u8bc6\u522b\u548c\u6545\u969c\u68c0\u6d4b\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u65e0\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u56e0\u5f02\u5e38\u6a21\u5f0f\u591a\u6837\u6027\u548c\u6570\u636e\u590d\u6742\u6027\u800c\u96be\u4ee5\u6355\u83b7\u7a33\u5065\u4f9d\u8d56\u5173\u7cfb\u3002", "method": "\u63d0\u51faDConAD\u6846\u67b6\uff0c\u5229\u7528\u5dee\u5206\u6570\u636e\u589e\u5f3a\u4fe1\u606f\uff0c\u7ed3\u5408Transformer\u6355\u83b7\u65f6\u7a7a\u4f9d\u8d56\uff0c\u5e76\u91c7\u7528\u57fa\u4e8eKL\u6563\u5ea6\u7684\u5bf9\u6bd4\u5b66\u4e60\u8303\u5f0f\u3002", "result": "\u5728\u4e94\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cDConAD\u4f18\u4e8e\u4e5d\u79cd\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "DConAD\u901a\u8fc7\u5dee\u5206\u5bf9\u6bd4\u5b66\u4e60\u663e\u8457\u63d0\u5347\u4e86\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u7684\u80fd\u529b\uff0c\u4e14\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2504.14603", "pdf": "https://arxiv.org/pdf/2504.14603", "abs": "https://arxiv.org/abs/2504.14603", "authors": ["Chaoyun Zhang", "He Huang", "Chiming Ni", "Jian Mu", "Si Qin", "Shilin He", "Lu Wang", "Fangkai Yang", "Pu Zhao", "Chao Du", "Liqun Li", "Yu Kang", "Zhao Jiang", "Suzhen Zheng", "Rujia Wang", "Jiaxu Qian", "Minghua Ma", "Jian-Guang Lou", "Qingwei Lin", "Saravan Rajmohan", "Dongmei Zhang"], "title": "UFO2: The Desktop AgentOS", "categories": ["cs.AI", "cs.HC", "cs.OS"], "comment": "The source code of UFO2 is publicly available at\n  https://github.com/microsoft/UFO/, with comprehensive documentation provided\n  at https://microsoft.github.io/UFO/", "summary": "Recent Computer-Using Agents (CUAs), powered by multimodal large language\nmodels (LLMs), offer a promising direction for automating complex desktop\nworkflows through natural language. However, most existing CUAs remain\nconceptual prototypes, hindered by shallow OS integration, fragile\nscreenshot-based interaction, and disruptive execution.\n  We present UFO2, a multiagent AgentOS for Windows desktops that elevates CUAs\ninto practical, system-level automation. UFO2 features a centralized HostAgent\nfor task decomposition and coordination, alongside a collection of\napplication-specialized AppAgent equipped with native APIs, domain-specific\nknowledge, and a unified GUI--API action layer. This architecture enables\nrobust task execution while preserving modularity and extensibility. A hybrid\ncontrol detection pipeline fuses Windows UI Automation (UIA) with vision-based\nparsing to support diverse interface styles. Runtime efficiency is further\nenhanced through speculative multi-action planning, reducing per-step LLM\noverhead. Finally, a Picture-in-Picture (PiP) interface enables automation\nwithin an isolated virtual desktop, allowing agents and users to operate\nconcurrently without interference.\n  We evaluate UFO2 across over 20 real-world Windows applications,\ndemonstrating substantial improvements in robustness and execution accuracy\nover prior CUAs. Our results show that deep OS integration unlocks a scalable\npath toward reliable, user-aligned desktop automation.", "AI": {"tldr": "UFO2\u662f\u4e00\u4e2a\u591a\u4ee3\u7406AgentOS\uff0c\u901a\u8fc7\u6df1\u5ea6\u64cd\u4f5c\u7cfb\u7edf\u96c6\u6210\u548c\u6df7\u5408\u63a7\u5236\u68c0\u6d4b\u7ba1\u9053\uff0c\u663e\u8457\u63d0\u5347\u4e86\u684c\u9762\u81ea\u52a8\u5316\u7684\u9c81\u68d2\u6027\u548c\u6267\u884c\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u8ba1\u7b97\u673a\u4f7f\u7528\u4ee3\u7406\uff08CUAs\uff09\u591a\u4e3a\u6982\u5ff5\u539f\u578b\uff0c\u53d7\u9650\u4e8e\u6d45\u5c42\u64cd\u4f5c\u7cfb\u7edf\u96c6\u6210\u3001\u8106\u5f31\u7684\u622a\u56fe\u4ea4\u4e92\u548c\u4e2d\u65ad\u5f0f\u6267\u884c\uff0cUFO2\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "UFO2\u91c7\u7528\u591a\u4ee3\u7406\u67b6\u6784\uff0c\u5305\u62ec\u96c6\u4e2d\u5f0fHostAgent\u548c\u4e13\u7528AppAgent\uff0c\u7ed3\u5408Windows UIA\u4e0e\u89c6\u89c9\u89e3\u6790\u7684\u6df7\u5408\u63a7\u5236\u68c0\u6d4b\u7ba1\u9053\uff0c\u4ee5\u53ca\u63a8\u6d4b\u6027\u591a\u52a8\u4f5c\u89c4\u5212\u548cPiP\u754c\u9762\u3002", "result": "\u572820\u591a\u4e2a\u771f\u5b9eWindows\u5e94\u7528\u4e2d\uff0cUFO2\u8868\u73b0\u51fa\u6bd4\u73b0\u6709CUAs\u66f4\u9ad8\u7684\u9c81\u68d2\u6027\u548c\u6267\u884c\u51c6\u786e\u6027\u3002", "conclusion": "\u6df1\u5ea6\u64cd\u4f5c\u7cfb\u7edf\u96c6\u6210\u4e3a\u5b9e\u73b0\u53ef\u9760\u3001\u7528\u6237\u5bf9\u9f50\u7684\u684c\u9762\u81ea\u52a8\u5316\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u8def\u5f84\u3002"}}
{"id": "2504.14692", "pdf": "https://arxiv.org/pdf/2504.14692", "abs": "https://arxiv.org/abs/2504.14692", "authors": ["Songtao Jiang", "Yuan Wang", "Sibo Song", "Yan Zhang", "Zijie Meng", "Bohan Lei", "Jian Wu", "Jimeng Sun", "Zuozhu Liu"], "title": "OmniV-Med: Scaling Medical Vision-Language Model for Universal Visual Understanding", "categories": ["cs.CL"], "comment": null, "summary": "The practical deployment of medical vision-language models (Med-VLMs)\nnecessitates seamless integration of textual data with diverse visual\nmodalities, including 2D/3D images and videos, yet existing models typically\nemploy separate encoders for different modalities. To address this limitation,\nwe present OmniV-Med, a unified framework for multimodal medical understanding.\nOur technical contributions are threefold: First, we construct\nOmniV-Med-Instruct, a comprehensive multimodal medical dataset containing 252K\ninstructional samples spanning 14 medical image modalities and 11 clinical\ntasks. Second, we devise a rotary position-adaptive encoder that processes\nmulti-resolution 2D/3D images and videos within a unified architecture,\ndiverging from conventional modality-specific encoders. Third, we introduce a\nmedical-aware token pruning mechanism that exploits spatial-temporal redundancy\nin volumetric data (e.g., consecutive CT slices) and medical videos,\neffectively reducing 60\\% of visual tokens without performance degradation.\nEmpirical evaluations demonstrate that OmniV-Med-7B achieves state-of-the-art\nperformance on 7 benchmarks spanning 2D/3D medical imaging and video\nunderstanding tasks. Notably, our lightweight variant (OmniV-Med-1.5B) attains\ncomparable performance while requiring only 8 RTX3090 GPUs for training and\nsupporting efficient long-video inference. Data, code and model will be\nreleased.", "AI": {"tldr": "OmniV-Med\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u533b\u5b66\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u6570\u636e\u96c6\u3001\u7edf\u4e00\u7f16\u7801\u5668\u548c\u4ee4\u724c\u4fee\u526a\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u533b\u5b66\u56fe\u50cf\u548c\u89c6\u9891\u7406\u89e3\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u533b\u5b66\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5bf9\u4e0d\u540c\u6a21\u6001\u4f7f\u7528\u72ec\u7acb\u7f16\u7801\u5668\u7684\u5c40\u9650\u6027\u3002", "method": "\u6784\u5efa\u591a\u6a21\u6001\u6570\u636e\u96c6OmniV-Med-Instruct\uff0c\u8bbe\u8ba1\u65cb\u8f6c\u4f4d\u7f6e\u81ea\u9002\u5e94\u7f16\u7801\u5668\uff0c\u5f15\u5165\u533b\u5b66\u611f\u77e5\u4ee4\u724c\u4fee\u526a\u673a\u5236\u3002", "result": "OmniV-Med-7B\u57287\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u8f7b\u91cf\u7248OmniV-Med-1.5B\u6027\u80fd\u63a5\u8fd1\u4e14\u8bad\u7ec3\u8d44\u6e90\u9700\u6c42\u4f4e\u3002", "conclusion": "OmniV-Med\u4e3a\u533b\u5b66\u591a\u6a21\u6001\u7406\u89e3\u63d0\u4f9b\u4e86\u9ad8\u6548\u7edf\u4e00\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5b9e\u9645\u90e8\u7f72\u6f5c\u529b\u3002"}}
{"id": "2504.14205", "pdf": "https://arxiv.org/pdf/2504.14205", "abs": "https://arxiv.org/abs/2504.14205", "authors": ["Wenxin Zhang", "Jingxing Zhong", "Guangzhen Yao", "Renda Han", "Xiaojian Lin", "Zeyu Zhang", "Cuicui Luo"], "title": "Dual-channel Heterophilic Message Passing for Graph Fraud Detection", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Fraudulent activities have significantly increased across various domains,\nsuch as e-commerce, online review platforms, and social networks, making fraud\ndetection a critical task. Spatial Graph Neural Networks (GNNs) have been\nsuccessfully applied to fraud detection tasks due to their strong inductive\nlearning capabilities. However, existing spatial GNN-based methods often\nenhance the graph structure by excluding heterophilic neighbors during message\npassing to align with the homophilic bias of GNNs. Unfortunately, this approach\ncan disrupt the original graph topology and increase uncertainty in\npredictions. To address these limitations, this paper proposes a novel\nframework, Dual-channel Heterophilic Message Passing (DHMP), for fraud\ndetection. DHMP leverages a heterophily separation module to divide the graph\ninto homophilic and heterophilic subgraphs, mitigating the low-pass inductive\nbias of traditional GNNs. It then applies shared weights to capture signals at\ndifferent frequencies independently and incorporates a customized sampling\nstrategy for training. This allows nodes to adaptively balance the\ncontributions of various signals based on their labels. Extensive experiments\non three real-world datasets demonstrate that DHMP outperforms existing\nmethods, highlighting the importance of separating signals with different\nfrequencies for improved fraud detection. The code is available at\nhttps://github.com/shaieesss/DHMP.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDHMP\u7684\u53cc\u901a\u9053\u5f02\u8d28\u6027\u6d88\u606f\u4f20\u9012\u6846\u67b6\uff0c\u7528\u4e8e\u6539\u8fdb\u6b3a\u8bc8\u68c0\u6d4b\u4efb\u52a1\uff0c\u901a\u8fc7\u5206\u79bb\u540c\u8d28\u6027\u548c\u5f02\u8d28\u6027\u5b50\u56fe\u6765\u4f18\u5316\u4f20\u7edfGNN\u7684\u4f4e\u901a\u5f52\u7eb3\u504f\u5dee\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u7a7a\u95f4GNN\u7684\u65b9\u6cd5\u5728\u6d88\u606f\u4f20\u9012\u4e2d\u6392\u9664\u5f02\u8d28\u6027\u90bb\u5c45\uff0c\u53ef\u80fd\u7834\u574f\u539f\u59cb\u56fe\u62d3\u6251\u5e76\u589e\u52a0\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u3002", "method": "DHMP\u5229\u7528\u5f02\u8d28\u6027\u5206\u79bb\u6a21\u5757\u5c06\u56fe\u5206\u4e3a\u540c\u8d28\u6027\u548c\u5f02\u8d28\u6027\u5b50\u56fe\uff0c\u91c7\u7528\u5171\u4eab\u6743\u91cd\u72ec\u7acb\u6355\u83b7\u4e0d\u540c\u9891\u7387\u4fe1\u53f7\uff0c\u5e76\u5f15\u5165\u5b9a\u5236\u91c7\u6837\u7b56\u7565\u3002", "result": "\u5728\u4e09\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cDHMP\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u5206\u79bb\u4e0d\u540c\u9891\u7387\u4fe1\u53f7\u5bf9\u63d0\u5347\u6b3a\u8bc8\u68c0\u6d4b\u6548\u679c\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2504.14624", "pdf": "https://arxiv.org/pdf/2504.14624", "abs": "https://arxiv.org/abs/2504.14624", "authors": ["Polina Gordienko", "Christoph Jansen", "Thomas Augustin", "Martin Rechenauer"], "title": "Consensus in Motion: A Case of Dynamic Rationality of Sequential Learning in Probability Aggregation", "categories": ["cs.AI"], "comment": "Submitted to the International Conference on Modeling Decisions for\n  Artificial Intelligence (MDAI 2025)", "summary": "We propose a framework for probability aggregation based on propositional\nprobability logic. Unlike conventional judgment aggregation, which focuses on\nstatic rationality, our model addresses dynamic rationality by ensuring that\ncollective beliefs update consistently with new information. We show that any\nconsensus-compatible and independent aggregation rule on a non-nested agenda is\nnecessarily linear. Furthermore, we provide sufficient conditions for a fair\nlearning process, where individuals initially agree on a specified subset of\npropositions known as the common ground, and new information is restricted to\nthis shared foundation. This guarantees that updating individual judgments via\nBayesian conditioning-whether performed before or after aggregation-yields the\nsame collective belief. A distinctive feature of our framework is its treatment\nof sequential decision-making, which allows new information to be incorporated\nprogressively through multiple stages while maintaining the established common\nground. We illustrate our findings with a running example in a political\nscenario concerning healthcare and immigration policies.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u547d\u9898\u6982\u7387\u903b\u8f91\u7684\u6982\u7387\u805a\u5408\u6846\u67b6\uff0c\u5f3a\u8c03\u52a8\u6001\u7406\u6027\u800c\u975e\u9759\u6001\u7406\u6027\uff0c\u786e\u4fdd\u96c6\u4f53\u4fe1\u5ff5\u4e0e\u65b0\u4fe1\u606f\u4e00\u81f4\u66f4\u65b0\u3002", "motivation": "\u4f20\u7edf\u5224\u65ad\u805a\u5408\u5173\u6ce8\u9759\u6001\u7406\u6027\uff0c\u800c\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u52a8\u6001\u7406\u6027\u95ee\u9898\uff0c\u786e\u4fdd\u96c6\u4f53\u4fe1\u5ff5\u80fd\u4e00\u81f4\u66f4\u65b0\u3002", "method": "\u57fa\u4e8e\u547d\u9898\u6982\u7387\u903b\u8f91\uff0c\u63d0\u51fa\u5171\u8bc6\u517c\u5bb9\u4e14\u72ec\u7acb\u7684\u805a\u5408\u89c4\u5219\uff0c\u5e76\u9650\u5236\u65b0\u4fe1\u606f\u5728\u5171\u540c\u57fa\u7840\u4e0a\u3002", "result": "\u8bc1\u660e\u975e\u5d4c\u5957\u8bae\u7a0b\u4e0a\u7684\u7ebf\u6027\u805a\u5408\u89c4\u5219\u662f\u5fc5\u8981\u7684\uff0c\u5e76\u63d0\u4f9b\u4e86\u516c\u5e73\u5b66\u4e60\u8fc7\u7a0b\u7684\u5145\u5206\u6761\u4ef6\u3002", "conclusion": "\u6846\u67b6\u652f\u6301\u591a\u9636\u6bb5\u51b3\u7b56\uff0c\u786e\u4fdd\u96c6\u4f53\u4fe1\u5ff5\u66f4\u65b0\u7684\u4e00\u81f4\u6027\uff0c\u5e76\u901a\u8fc7\u653f\u6cbb\u573a\u666f\u793a\u4f8b\u9a8c\u8bc1\u3002"}}
{"id": "2504.14707", "pdf": "https://arxiv.org/pdf/2504.14707", "abs": "https://arxiv.org/abs/2504.14707", "authors": ["Ratna Kandala", "Katie Hoemann"], "title": "Evaluating BERTopic on Open-Ended Data: A Case Study with Belgian Dutch Daily Narratives", "categories": ["cs.CL"], "comment": null, "summary": "This study explores BERTopic's potential for modeling open-ended Belgian\nDutch daily narratives, contrasting its performance with Latent Dirichlet\nAllocation (LDA) and KMeans. Although LDA scores well on certain automated\nmetrics, human evaluations reveal semantically irrelevant co-occurrences,\nhighlighting the limitations of purely statistic-based methods. In contrast,\nBERTopic's reliance on contextual embeddings yields culturally resonant themes,\nunderscoring the importance of hybrid evaluation frameworks that account for\nmorphologically rich languages. KMeans performed less coherently than prior\nresearch suggested, pointing to the unique challenges posed by personal\nnarratives. Our findings emphasize the need for robust generalization in NLP\nmodels, especially in underrepresented linguistic contexts.", "AI": {"tldr": "\u5bf9\u6bd4BERTopic\u3001LDA\u548cKMeans\u5728\u6bd4\u5229\u65f6\u8377\u5170\u8bed\u65e5\u5e38\u53d9\u4e8b\u4e2d\u7684\u8868\u73b0\uff0cBERTopic\u56e0\u4e0a\u4e0b\u6587\u5d4c\u5165\u8868\u73b0\u66f4\u4f18\uff0c\u800cLDA\u548cKMeans\u5b58\u5728\u5c40\u9650\u6027\u3002", "motivation": "\u63a2\u7d22BERTopic\u5728\u5f62\u6001\u4e30\u5bcc\u8bed\u8a00\u4e2d\u7684\u6f5c\u529b\uff0c\u5e76\u5bf9\u6bd4\u5176\u4e0eLDA\u548cKMeans\u7684\u6027\u80fd\u3002", "method": "\u4f7f\u7528BERTopic\u3001LDA\u548cKMeans\u5efa\u6a21\u6bd4\u5229\u65f6\u8377\u5170\u8bed\u53d9\u4e8b\uff0c\u7ed3\u5408\u81ea\u52a8\u6307\u6807\u548c\u4eba\u5de5\u8bc4\u4f30\u3002", "result": "BERTopic\u751f\u6210\u7684\u4e3b\u9898\u66f4\u5177\u6587\u5316\u5171\u9e23\uff0cLDA\u548cKMeans\u5728\u8bed\u4e49\u76f8\u5173\u6027\u548c\u4e00\u81f4\u6027\u4e0a\u8868\u73b0\u8f83\u5dee\u3002", "conclusion": "\u5f3a\u8c03NLP\u6a21\u578b\u5728\u5c11\u6570\u8bed\u8a00\u73af\u5883\u4e2d\u9700\u66f4\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u63a8\u8350\u6df7\u5408\u8bc4\u4f30\u6846\u67b6\u3002"}}
{"id": "2504.14206", "pdf": "https://arxiv.org/pdf/2504.14206", "abs": "https://arxiv.org/abs/2504.14206", "authors": ["Wenxin Zhang", "Cuicui Luo"], "title": "Decomposition-based multi-scale transformer framework for time series anomaly detection", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Time series anomaly detection is crucial for maintaining stable systems.\nExisting methods face two main challenges. First, it is difficult to directly\nmodel the dependencies of diverse and complex patterns within the sequences.\nSecond, many methods that optimize parameters using mean squared error struggle\nwith noise in the time series, leading to performance deterioration. To address\nthese challenges, we propose a transformer-based framework built on\ndecomposition (TransDe) for multivariate time series anomaly detection. The key\nidea is to combine the strengths of time series decomposition and transformers\nto effectively learn the complex patterns in normal time series data. A\nmulti-scale patch-based transformer architecture is proposed to exploit the\nrepresentative dependencies of each decomposed component of the time series.\nFurthermore, a contrastive learn paradigm based on patch operation is proposed,\nwhich leverages KL divergence to align the positive pairs, namely the pure\nrepresentations of normal patterns between different patch-level views. A novel\nasynchronous loss function with a stop-gradient strategy is further introduced\nto enhance the performance of TransDe effectively. It can avoid time-consuming\nand labor-intensive computation costs in the optimization process. Extensive\nexperiments on five public datasets are conducted and TransDe shows superiority\ncompared with twelve baselines in terms of F1 score. Our code is available at\nhttps://github.com/shaieesss/TransDe.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5206\u89e3\u548cTransformer\u7684\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u6846\u67b6TransDe\uff0c\u7ed3\u5408\u65f6\u95f4\u5e8f\u5217\u5206\u89e3\u548cTransformer\u7684\u4f18\u52bf\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u8865\u4e01\u67b6\u6784\u548c\u5bf9\u6bd4\u5b66\u4e60\u8303\u5f0f\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u76f4\u63a5\u5efa\u6a21\u590d\u6742\u65f6\u95f4\u5e8f\u5217\u4f9d\u8d56\u5173\u7cfb\uff0c\u4e14\u6613\u53d7\u566a\u58f0\u5f71\u54cd\u3002", "method": "\u7ed3\u5408\u65f6\u95f4\u5e8f\u5217\u5206\u89e3\u4e0eTransformer\uff0c\u63d0\u51fa\u591a\u5c3a\u5ea6\u8865\u4e01\u67b6\u6784\u548c\u57fa\u4e8eKL\u6563\u5ea6\u7684\u5bf9\u6bd4\u5b66\u4e60\u8303\u5f0f\uff0c\u5f15\u5165\u5f02\u6b65\u635f\u5931\u51fd\u6570\u4f18\u5316\u3002", "result": "\u5728\u4e94\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e12\u79cd\u57fa\u7ebf\u65b9\u6cd5\uff0cF1\u5206\u6570\u8868\u73b0\u4f18\u8d8a\u3002", "conclusion": "TransDe\u6709\u6548\u89e3\u51b3\u4e86\u590d\u6742\u65f6\u95f4\u5e8f\u5217\u5efa\u6a21\u548c\u566a\u58f0\u95ee\u9898\uff0c\u6027\u80fd\u663e\u8457\u63d0\u5347\u3002"}}
{"id": "2504.14650", "pdf": "https://arxiv.org/pdf/2504.14650", "abs": "https://arxiv.org/abs/2504.14650", "authors": ["Yuting Huang", "Leilei Ding", "Zhipeng Tang", "Tianfu Wang", "Xinrui Lin", "Wuyang Zhang", "Mingxiao Ma", "Yanyong Zhang"], "title": "A Framework for Benchmarking and Aligning Task-Planning Safety in LLM-Based Embodied Agents", "categories": ["cs.AI"], "comment": "16 pages, 10 figures", "summary": "Large Language Models (LLMs) exhibit substantial promise in enhancing\ntask-planning capabilities within embodied agents due to their advanced\nreasoning and comprehension. However, the systemic safety of these agents\nremains an underexplored frontier. In this study, we present Safe-BeAl, an\nintegrated framework for the measurement (SafePlan-Bench) and alignment\n(Safe-Align) of LLM-based embodied agents' behaviors. SafePlan-Bench\nestablishes a comprehensive benchmark for evaluating task-planning safety,\nencompassing 2,027 daily tasks and corresponding environments distributed\nacross 8 distinct hazard categories (e.g., Fire Hazard). Our empirical analysis\nreveals that even in the absence of adversarial inputs or malicious intent,\nLLM-based agents can exhibit unsafe behaviors. To mitigate these hazards, we\npropose Safe-Align, a method designed to integrate physical-world safety\nknowledge into LLM-based embodied agents while maintaining task-specific\nperformance. Experiments across a variety of settings demonstrate that\nSafe-BeAl provides comprehensive safety validation, improving safety by 8.55 -\n15.22%, compared to embodied agents based on GPT-4, while ensuring successful\ntask completion.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faSafe-BeAl\u6846\u67b6\uff0c\u901a\u8fc7SafePlan-Bench\u8bc4\u4f30LLM\u4ee3\u7406\u7684\u4efb\u52a1\u89c4\u5212\u5b89\u5168\u6027\uff0c\u5e76\u901a\u8fc7Safe-Align\u65b9\u6cd5\u63d0\u5347\u5b89\u5168\u6027\uff0c\u5b9e\u9a8c\u663e\u793a\u5b89\u5168\u6027\u63d0\u53478.55-15.22%\u3002", "motivation": "\u5c3d\u7ba1LLM\u5728\u4efb\u52a1\u89c4\u5212\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5176\u5b89\u5168\u6027\u95ee\u9898\u5c1a\u672a\u5145\u5206\u7814\u7a76\uff0c\u9700\u8981\u7cfb\u7edf\u6027\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faSafe-BeAl\u6846\u67b6\uff0c\u5305\u62ec\u8bc4\u4f30\u57fa\u51c6SafePlan-Bench\u548c\u5b89\u5168\u6027\u5bf9\u9f50\u65b9\u6cd5Safe-Align\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSafe-BeAl\u663e\u8457\u63d0\u5347LLM\u4ee3\u7406\u7684\u5b89\u5168\u6027\uff088.55-15.22%\uff09\uff0c\u540c\u65f6\u4e0d\u5f71\u54cd\u4efb\u52a1\u5b8c\u6210\u7387\u3002", "conclusion": "Safe-BeAl\u4e3aLLM\u4ee3\u7406\u7684\u5b89\u5168\u6027\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u672a\u6765\u53ef\u6269\u5c55\u81f3\u66f4\u5e7f\u6cdb\u7684\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2504.14738", "pdf": "https://arxiv.org/pdf/2504.14738", "abs": "https://arxiv.org/abs/2504.14738", "authors": ["Reya Vir", "Shreya Shankar", "Harrison Chase", "Will Fu-Hinthorn", "Aditya Parameswaran"], "title": "PROMPTEVALS: A Dataset of Assertions and Guardrails for Custom Production Large Language Model Pipelines", "categories": ["cs.CL"], "comment": "Accepted to NAACL 2025 Main Conference", "summary": "Large language models (LLMs) are increasingly deployed in specialized\nproduction data processing pipelines across diverse domains -- such as finance,\nmarketing, and e-commerce. However, when running them in production across many\ninputs, they often fail to follow instructions or meet developer expectations.\nTo improve reliability in these applications, creating assertions or guardrails\nfor LLM outputs to run alongside the pipelines is essential. Yet, determining\nthe right set of assertions that capture developer requirements for a task is\nchallenging. In this paper, we introduce PROMPTEVALS, a dataset of 2087 LLM\npipeline prompts with 12623 corresponding assertion criteria, sourced from\ndevelopers using our open-source LLM pipeline tools. This dataset is 5x larger\nthan previous collections. Using a hold-out test split of PROMPTEVALS as a\nbenchmark, we evaluated closed- and open-source models in generating relevant\nassertions. Notably, our fine-tuned Mistral and Llama 3 models outperform\nGPT-4o by 20.93% on average, offering both reduced latency and improved\nperformance. We believe our dataset can spur further research in LLM\nreliability, alignment, and prompt engineering.", "AI": {"tldr": "PROMPTEVALS\u662f\u4e00\u4e2a\u5305\u542b2087\u4e2aLLM\u7ba1\u9053\u63d0\u793a\u548c12623\u4e2a\u65ad\u8a00\u6807\u51c6\u7684\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u63d0\u5347LLM\u5728\u751f\u6210\u8f93\u51fa\u65f6\u7684\u53ef\u9760\u6027\u3002\u5fae\u8c03\u7684Mistral\u548cLlama 3\u6a21\u578b\u8868\u73b0\u4f18\u4e8eGPT-4o\u3002", "motivation": "LLM\u5728\u751f\u4ea7\u73af\u5883\u4e2d\u5e38\u65e0\u6cd5\u6ee1\u8db3\u5f00\u53d1\u8005\u671f\u671b\uff0c\u9700\u8981\u65ad\u8a00\u673a\u5236\u63d0\u5347\u53ef\u9760\u6027\uff0c\u4f46\u786e\u5b9a\u5408\u9002\u7684\u65ad\u8a00\u6807\u51c6\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u5f15\u5165PROMPTEVALS\u6570\u636e\u96c6\uff0c\u5305\u542b\u5f00\u53d1\u8005\u63d0\u4f9b\u7684\u63d0\u793a\u548c\u65ad\u8a00\u6807\u51c6\uff0c\u5e76\u8bc4\u4f30\u95ed\u6e90\u548c\u5f00\u6e90\u6a21\u578b\u751f\u6210\u65ad\u8a00\u7684\u80fd\u529b\u3002", "result": "\u5fae\u8c03\u7684Mistral\u548cLlama 3\u6a21\u578b\u6bd4GPT-4o\u5e73\u5747\u6027\u80fd\u63d0\u534720.93%\uff0c\u4e14\u5ef6\u8fdf\u66f4\u4f4e\u3002", "conclusion": "PROMPTEVALS\u6570\u636e\u96c6\u53ef\u63a8\u52a8LLM\u53ef\u9760\u6027\u3001\u5bf9\u9f50\u548c\u63d0\u793a\u5de5\u7a0b\u7684\u7814\u7a76\u3002"}}
{"id": "2504.14237", "pdf": "https://arxiv.org/pdf/2504.14237", "abs": "https://arxiv.org/abs/2504.14237", "authors": ["Dekang Zhang", "Dan Niu", "Zhou Jin", "Yichao Dong", "Jingweijia Tan", "Changyin Sun"], "title": "A Novel Frequency-Spatial Domain Aware Network for Fast Thermal Prediction in 2.5D ICs", "categories": ["cs.LG"], "comment": "7 pages, 5 figures, 22nd Design, Automation and Test in Europe\n  Conference (DATE '25)", "summary": "In the post-Moore era, 2.5D chiplet-based ICs present significant challenges\nin thermal management due to increased power density and thermal hotspots.\nNeural network-based thermal prediction models can perform real-time\npredictions for many unseen new designs. However, existing CNN-based and\nGCN-based methods cannot effectively capture the global thermal features,\nespecially for high-frequency components, hindering prediction accuracy\nenhancement. In this paper, we propose a novel frequency-spatial dual domain\naware prediction network (FSA-Heat) for fast and high-accuracy thermal\nprediction in 2.5D ICs. It integrates high-to-low frequency and spatial domain\nencoder (FSTE) module with frequency domain cross-scale interaction module\n(FCIFormer) to achieve high-to-low frequency and global-to-local thermal\ndissipation feature extraction. Additionally, a frequency-spatial hybrid loss\n(FSL) is designed to effectively attenuate high-frequency thermal gradient\nnoise and spatial misalignments. The experimental results show that the\nperformance enhancements offered by our proposed method are substantial,\noutperforming the newly-proposed 2.5D method, GCN+PNA, by considerable margins\n(over 99% RMSE reduction, 4.23X inference time speedup). Moreover, extensive\nexperiments demonstrate that FSA-Heat also exhibits robust generalization\ncapabilities.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u9891\u7387-\u7a7a\u95f4\u53cc\u57df\u611f\u77e5\u9884\u6d4b\u7f51\u7edc\uff08FSA-Heat\uff09\uff0c\u7528\u4e8e2.5D IC\u4e2d\u7684\u5feb\u901f\u9ad8\u7cbe\u5ea6\u70ed\u9884\u6d4b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u7cbe\u5ea6\u548c\u63a8\u7406\u901f\u5ea6\u3002", "motivation": "\u5728\u6469\u5c14\u5b9a\u5f8b\u540e\u65f6\u4ee3\uff0c2.5D\u82af\u7247\u7684\u70ed\u7ba1\u7406\u9762\u4e34\u9ad8\u529f\u7387\u5bc6\u5ea6\u548c\u70ed\u70b9\u95ee\u9898\uff0c\u73b0\u6709\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u6355\u6349\u5168\u5c40\u70ed\u7279\u5f81\uff0c\u5c24\u5176\u662f\u9ad8\u9891\u5206\u91cf\u3002", "method": "\u63d0\u51faFSA-Heat\u7f51\u7edc\uff0c\u7ed3\u5408\u9ad8\u9891-\u4f4e\u9891\u548c\u7a7a\u95f4\u57df\u7f16\u7801\u5668\uff08FSTE\uff09\u4e0e\u9891\u7387\u57df\u8de8\u5c3a\u5ea6\u4ea4\u4e92\u6a21\u5757\uff08FCIFormer\uff09\uff0c\u5e76\u8bbe\u8ba1\u9891\u7387-\u7a7a\u95f4\u6df7\u5408\u635f\u5931\uff08FSL\uff09\u4ee5\u51cf\u5c11\u9ad8\u9891\u566a\u58f0\u548c\u7a7a\u95f4\u9519\u4f4d\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cFSA-Heat\u5728RMSE\u4e0a\u964d\u4f4e\u4e8699%\uff0c\u63a8\u7406\u901f\u5ea6\u63d0\u5347\u4e864.23\u500d\uff0c\u4e14\u5177\u6709\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "FSA-Heat\u57282.5D IC\u70ed\u9884\u6d4b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2504.14706", "pdf": "https://arxiv.org/pdf/2504.14706", "abs": "https://arxiv.org/abs/2504.14706", "authors": ["Shin-nosuke Ishikawa", "Atsushi Yoshino"], "title": "AI with Emotions: Exploring Emotional Expressions in Large Language Models", "categories": ["cs.AI"], "comment": "14 pages, 8 figures, accepted to the Natural Language Processing for\n  Digital Humanities (NLP4DH) workshop at NAACL 2025", "summary": "The human-level performance of Large Language Models (LLMs) across various\ntasks has raised expectations for the potential of Artificial Intelligence (AI)\nto possess emotions someday. To explore the capability of current LLMs to\nexpress emotions in their outputs, we conducted an experiment using several\nLLMs (OpenAI GPT, Google Gemini, Meta Llama3, and Cohere Command R+) to\nrole-play as agents answering questions with specified emotional states.We\ndefined the emotional states using Russell's Circumplex model, a\nwell-established framework that characterizes emotions along the\nsleepy-activated (arousal) and pleasure-displeasure (valence) axes. We chose\nthis model for its simplicity, utilizing two continuous parameters, which\nallows for better controllability in applications involving continuous changes\nin emotional states. The responses generated were evaluated using a sentiment\nanalysis model, independent of the LLMs, trained on the GoEmotions dataset. The\nevaluation showed that the emotional states of the generated answers were\nconsistent with the specifications, demonstrating the LLMs' capability for\nemotional expression. This indicates the potential for LLM-based AI agents to\nsimulate emotions, opening up a wide range of applications for emotion-based\ninteractions, such as advisors or consultants who can provide advice or\nopinions with a personal touch.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u8868\u8fbe\u60c5\u611f\u65b9\u9762\u7684\u80fd\u529b\uff0c\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86LLMs\u80fd\u591f\u6839\u636e\u6307\u5b9a\u60c5\u611f\u72b6\u6001\u751f\u6210\u4e00\u81f4\u7684\u56de\u7b54\u3002", "motivation": "\u7814\u7a76LLMs\u662f\u5426\u80fd\u591f\u6a21\u62df\u60c5\u611f\u8868\u8fbe\uff0c\u4ee5\u6269\u5c55\u5176\u5728\u60c5\u611f\u4ea4\u4e92\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002", "method": "\u4f7f\u7528Russell\u7684Circumplex\u6a21\u578b\u5b9a\u4e49\u60c5\u611f\u72b6\u6001\uff0c\u901a\u8fc7\u591a\u4e2aLLMs\u751f\u6210\u60c5\u611f\u5316\u56de\u7b54\uff0c\u5e76\u7528\u72ec\u7acb\u7684\u60c5\u611f\u5206\u6790\u6a21\u578b\u8bc4\u4f30\u4e00\u81f4\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cLLMs\u751f\u6210\u7684\u60c5\u611f\u5316\u56de\u7b54\u4e0e\u6307\u5b9a\u60c5\u611f\u72b6\u6001\u4e00\u81f4\uff0c\u9a8c\u8bc1\u4e86\u5176\u60c5\u611f\u8868\u8fbe\u80fd\u529b\u3002", "conclusion": "LLMs\u5177\u5907\u6a21\u62df\u60c5\u611f\u7684\u80fd\u529b\uff0c\u4e3a\u60c5\u611f\u4ea4\u4e92\u5e94\u7528\uff08\u5982\u987e\u95ee\u670d\u52a1\uff09\u63d0\u4f9b\u4e86\u53ef\u80fd\u6027\u3002"}}
{"id": "2504.14766", "pdf": "https://arxiv.org/pdf/2504.14766", "abs": "https://arxiv.org/abs/2504.14766", "authors": ["Saniya Karwa", "Navpreet Singh"], "title": "Disentangling Linguistic Features with Dimension-Wise Analysis of Vector Embeddings", "categories": ["cs.CL"], "comment": null, "summary": "Understanding the inner workings of neural embeddings, particularly in models\nsuch as BERT, remains a challenge because of their high-dimensional and opaque\nnature. This paper proposes a framework for uncovering the specific dimensions\nof vector embeddings that encode distinct linguistic properties (LPs). We\nintroduce the Linguistically Distinct Sentence Pairs (LDSP-10) dataset, which\nisolates ten key linguistic features such as synonymy, negation, tense, and\nquantity. Using this dataset, we analyze BERT embeddings with various methods,\nincluding the Wilcoxon signed-rank test, mutual information, and recursive\nfeature elimination, to identify the most influential dimensions for each LP.\nWe introduce a new metric, the Embedding Dimension Impact (EDI) score, which\nquantifies the relevance of each embedding dimension to a LP. Our findings show\nthat certain properties, such as negation and polarity, are robustly encoded in\nspecific dimensions, while others, like synonymy, exhibit more complex\npatterns. This study provides insights into the interpretability of embeddings,\nwhich can guide the development of more transparent and optimized language\nmodels, with implications for model bias mitigation and the responsible\ndeployment of AI systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6846\u67b6\uff0c\u7528\u4e8e\u63ed\u793aBERT\u7b49\u9ad8\u7ef4\u4e0d\u900f\u660e\u6a21\u578b\u4e2d\u5411\u91cf\u5d4c\u5165\u7684\u7279\u5b9a\u7ef4\u5ea6\u5982\u4f55\u7f16\u7801\u4e0d\u540c\u8bed\u8a00\u5c5e\u6027\uff08LPs\uff09\u3002\u901a\u8fc7LDSP-10\u6570\u636e\u96c6\u548c\u591a\u79cd\u5206\u6790\u65b9\u6cd5\uff0c\u7814\u7a76\u53d1\u73b0\u67d0\u4e9b\u8bed\u8a00\u5c5e\u6027\uff08\u5982\u5426\u5b9a\u548c\u6781\u6027\uff09\u5728\u7279\u5b9a\u7ef4\u5ea6\u4e2d\u7f16\u7801\uff0c\u800c\u5176\u4ed6\u5c5e\u6027\uff08\u5982\u540c\u4e49\u6027\uff09\u5219\u66f4\u590d\u6742\u3002", "motivation": "\u7406\u89e3BERT\u7b49\u9ad8\u7ef4\u4e0d\u900f\u660e\u6a21\u578b\u7684\u5d4c\u5165\u673a\u5236\uff0c\u63ed\u793a\u5176\u7f16\u7801\u8bed\u8a00\u5c5e\u6027\u7684\u5177\u4f53\u7ef4\u5ea6\u3002", "method": "\u4f7f\u7528LDSP-10\u6570\u636e\u96c6\uff08\u5305\u542b10\u79cd\u8bed\u8a00\u7279\u5f81\uff09\uff0c\u7ed3\u5408Wilcoxon\u7b26\u53f7\u79e9\u68c0\u9a8c\u3001\u4e92\u4fe1\u606f\u548c\u9012\u5f52\u7279\u5f81\u6d88\u9664\u7b49\u65b9\u6cd5\u5206\u6790BERT\u5d4c\u5165\uff0c\u5e76\u5f15\u5165EDI\u8bc4\u5206\u91cf\u5316\u7ef4\u5ea6\u5bf9\u8bed\u8a00\u5c5e\u6027\u7684\u5f71\u54cd\u3002", "result": "\u53d1\u73b0\u5426\u5b9a\u548c\u6781\u6027\u7b49\u5c5e\u6027\u5728\u7279\u5b9a\u7ef4\u5ea6\u4e2d\u7f16\u7801\uff0c\u800c\u540c\u4e49\u6027\u7b49\u5c5e\u6027\u7f16\u7801\u66f4\u590d\u6742\u3002", "conclusion": "\u7814\u7a76\u4e3a\u5d4c\u5165\u7684\u53ef\u89e3\u91ca\u6027\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\uff0c\u6709\u52a9\u4e8e\u5f00\u53d1\u66f4\u900f\u660e\u548c\u4f18\u5316\u7684\u8bed\u8a00\u6a21\u578b\uff0c\u5e76\u5bf9\u6a21\u578b\u504f\u89c1\u7684\u7f13\u89e3\u548cAI\u7cfb\u7edf\u7684\u8d1f\u8d23\u4efb\u90e8\u7f72\u5177\u6709\u610f\u4e49\u3002"}}
{"id": "2504.14250", "pdf": "https://arxiv.org/pdf/2504.14250", "abs": "https://arxiv.org/abs/2504.14250", "authors": ["Yunhui Liu", "Jiashun Cheng", "Jia Li", "Fugee Tsung", "Hongzhi Yin", "Tieke He"], "title": "A Pre-Training and Adaptive Fine-Tuning Framework for Graph Anomaly Detection", "categories": ["cs.LG"], "comment": null, "summary": "Graph anomaly detection (GAD) has garnered increasing attention in recent\nyears, yet it remains challenging due to the scarcity of abnormal nodes and the\nhigh cost of label annotations. Graph pre-training, the two-stage learning\nparadigm, has emerged as an effective approach for label-efficient learning,\nlargely benefiting from expressive neighborhood aggregation under the\nassumption of strong homophily. However, in GAD, anomalies typically exhibit\nhigh local heterophily, while normal nodes retain strong homophily, resulting\nin a complex homophily-heterophily mixture. To understand the impact of this\nmixed pattern on graph pre-training, we analyze it through the lens of spectral\nfiltering and reveal that relying solely on a global low-pass filter is\ninsufficient for GAD. We further provide a theoretical justification for the\nnecessity of selectively applying appropriate filters to individual nodes.\nBuilding upon this insight, we propose PAF, a Pre-Training and Adaptive\nFine-tuning framework specifically designed for GAD. In particular, we\nintroduce joint training with low- and high-pass filters in the pre-training\nphase to capture the full spectrum of frequency information in node features.\nDuring fine-tuning, we devise a gated fusion network that adaptively combines\nnode representations generated by both filters. Extensive experiments across\nten benchmark datasets consistently demonstrate the effectiveness of PAF.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u56fe\u5f02\u5e38\u68c0\u6d4b\uff08GAD\uff09\u7684\u9884\u8bad\u7ec3\u548c\u81ea\u9002\u5e94\u5fae\u8c03\u6846\u67b6\uff08PAF\uff09\uff0c\u901a\u8fc7\u8054\u5408\u4f4e\u901a\u548c\u9ad8\u901a\u6ee4\u6ce2\u5668\u6355\u6349\u8282\u70b9\u7279\u5f81\u7684\u5b8c\u6574\u9891\u8c31\u4fe1\u606f\uff0c\u5e76\u5728\u5fae\u8c03\u9636\u6bb5\u81ea\u9002\u5e94\u7ed3\u5408\u4e24\u79cd\u6ee4\u6ce2\u5668\u7684\u8868\u793a\u3002", "motivation": "\u56fe\u5f02\u5e38\u68c0\u6d4b\u56e0\u5f02\u5e38\u8282\u70b9\u7a00\u7f3a\u548c\u6807\u6ce8\u6210\u672c\u9ad8\u800c\u5177\u6709\u6311\u6218\u6027\uff0c\u4e14\u5f02\u5e38\u8282\u70b9\u901a\u5e38\u8868\u73b0\u51fa\u9ad8\u5c40\u90e8\u5f02\u8d28\u6027\uff0c\u800c\u6b63\u5e38\u8282\u70b9\u4fdd\u6301\u5f3a\u540c\u8d28\u6027\uff0c\u5f62\u6210\u590d\u6742\u7684\u540c\u8d28-\u5f02\u8d28\u6df7\u5408\u6a21\u5f0f\u3002", "method": "\u901a\u8fc7\u8c31\u5206\u6790\u63ed\u793a\u4e86\u4ec5\u4f9d\u8d56\u5168\u5c40\u4f4e\u901a\u6ee4\u6ce2\u5668\u7684\u4e0d\u8db3\uff0c\u63d0\u51faPAF\u6846\u67b6\uff0c\u9884\u8bad\u7ec3\u9636\u6bb5\u8054\u5408\u4f4e\u901a\u548c\u9ad8\u901a\u6ee4\u6ce2\u5668\uff0c\u5fae\u8c03\u9636\u6bb5\u8bbe\u8ba1\u95e8\u63a7\u878d\u5408\u7f51\u7edc\u81ea\u9002\u5e94\u7ed3\u5408\u4e24\u79cd\u6ee4\u6ce2\u5668\u7684\u8868\u793a\u3002", "result": "\u5728\u5341\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86PAF\u7684\u6709\u6548\u6027\u3002", "conclusion": "PAF\u901a\u8fc7\u9009\u62e9\u6027\u5e94\u7528\u6ee4\u6ce2\u5668\u5e76\u81ea\u9002\u5e94\u7ed3\u5408\u8868\u793a\uff0c\u663e\u8457\u63d0\u5347\u4e86\u56fe\u5f02\u5e38\u68c0\u6d4b\u7684\u6027\u80fd\u3002"}}
{"id": "2504.14773", "pdf": "https://arxiv.org/pdf/2504.14773", "abs": "https://arxiv.org/abs/2504.14773", "authors": ["Haoming Li", "Zhaoliang Chen", "Jonathan Zhang", "Fei Liu"], "title": "PLANET: A Collection of Benchmarks for Evaluating LLMs' Planning Capabilities", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.MA"], "comment": "10 pages", "summary": "Planning is central to agents and agentic AI. The ability to plan, e.g.,\ncreating travel itineraries within a budget, holds immense potential in both\nscientific and commercial contexts. Moreover, optimal plans tend to require\nfewer resources compared to ad-hoc methods. To date, a comprehensive\nunderstanding of existing planning benchmarks appears to be lacking. Without\nit, comparing planning algorithms' performance across domains or selecting\nsuitable algorithms for new scenarios remains challenging. In this paper, we\nexamine a range of planning benchmarks to identify commonly used testbeds for\nalgorithm development and highlight potential gaps. These benchmarks are\ncategorized into embodied environments, web navigation, scheduling, games and\npuzzles, and everyday task automation. Our study recommends the most\nappropriate benchmarks for various algorithms and offers insights to guide\nfuture benchmark development.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u73b0\u6709\u89c4\u5212\u57fa\u51c6\u7684\u73b0\u72b6\uff0c\u63d0\u51fa\u4e86\u5206\u7c7b\u548c\u5efa\u8bae\uff0c\u4ee5\u5e2e\u52a9\u9009\u62e9\u9002\u5408\u7684\u7b97\u6cd5\u548c\u6307\u5bfc\u672a\u6765\u57fa\u51c6\u5f00\u53d1\u3002", "motivation": "\u89c4\u5212\u5728\u667a\u80fd\u4f53\u548c\u667a\u80fdAI\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u5bf9\u73b0\u6709\u89c4\u5212\u57fa\u51c6\u7684\u5168\u9762\u7406\u89e3\uff0c\u5bfc\u81f4\u8de8\u9886\u57df\u7b97\u6cd5\u6bd4\u8f83\u548c\u65b0\u573a\u666f\u7b97\u6cd5\u9009\u62e9\u56f0\u96be\u3002", "method": "\u7814\u7a76\u4e86\u4e00\u7cfb\u5217\u89c4\u5212\u57fa\u51c6\uff0c\u5c06\u5176\u5206\u7c7b\u4e3a\u5177\u8eab\u73af\u5883\u3001\u7f51\u7edc\u5bfc\u822a\u3001\u8c03\u5ea6\u3001\u6e38\u620f\u4e0e\u8c1c\u9898\u4ee5\u53ca\u65e5\u5e38\u4efb\u52a1\u81ea\u52a8\u5316\uff0c\u5e76\u5206\u6790\u4e86\u5176\u9002\u7528\u6027\u3002", "result": "\u63d0\u51fa\u4e86\u9488\u5bf9\u4e0d\u540c\u7b97\u6cd5\u7684\u6700\u5408\u9002\u57fa\u51c6\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u57fa\u51c6\u5f00\u53d1\u7684\u6f5c\u5728\u65b9\u5411\u3002", "conclusion": "\u7814\u7a76\u4e3a\u89c4\u5212\u7b97\u6cd5\u7684\u6bd4\u8f83\u548c\u9009\u62e9\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\uff0c\u5e76\u4e3a\u672a\u6765\u57fa\u51c6\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u5efa\u8bae\u3002"}}
{"id": "2504.14772", "pdf": "https://arxiv.org/pdf/2504.14772", "abs": "https://arxiv.org/abs/2504.14772", "authors": ["Luyang Fang", "Xiaowei Yu", "Jiazhang Cai", "Yongkai Chen", "Shushan Wu", "Zhengliang Liu", "Zhenyuan Yang", "Haoran Lu", "Xilin Gong", "Yufang Liu", "Terry Ma", "Wei Ruan", "Ali Abbasi", "Jing Zhang", "Tao Wang", "Ehsan Latif", "Wei Liu", "Wei Zhang", "Soheil Kolouri", "Xiaoming Zhai", "Dajiang Zhu", "Wenxuan Zhong", "Tianming Liu", "Ping Ma"], "title": "Knowledge Distillation and Dataset Distillation of Large Language Models: Emerging Trends, Challenges, and Future Directions", "categories": ["cs.CL", "cs.LG", "stat.ML"], "comment": null, "summary": "The exponential growth of Large Language Models (LLMs) continues to highlight\nthe need for efficient strategies to meet ever-expanding computational and data\ndemands. This survey provides a comprehensive analysis of two complementary\nparadigms: Knowledge Distillation (KD) and Dataset Distillation (DD), both\naimed at compressing LLMs while preserving their advanced reasoning\ncapabilities and linguistic diversity. We first examine key methodologies in\nKD, such as task-specific alignment, rationale-based training, and\nmulti-teacher frameworks, alongside DD techniques that synthesize compact,\nhigh-impact datasets through optimization-based gradient matching, latent space\nregularization, and generative synthesis. Building on these foundations, we\nexplore how integrating KD and DD can produce more effective and scalable\ncompression strategies. Together, these approaches address persistent\nchallenges in model scalability, architectural heterogeneity, and the\npreservation of emergent LLM abilities. We further highlight applications\nacross domains such as healthcare and education, where distillation enables\nefficient deployment without sacrificing performance. Despite substantial\nprogress, open challenges remain in preserving emergent reasoning and\nlinguistic diversity, enabling efficient adaptation to continually evolving\nteacher models and datasets, and establishing comprehensive evaluation\nprotocols. By synthesizing methodological innovations, theoretical foundations,\nand practical insights, our survey charts a path toward sustainable,\nresource-efficient LLMs through the tighter integration of KD and DD\nprinciples.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u77e5\u8bc6\u84b8\u998f\uff08KD\uff09\u548c\u6570\u636e\u96c6\u84b8\u998f\uff08DD\uff09\u4e24\u79cd\u4e92\u8865\u8303\u5f0f\uff0c\u65e8\u5728\u538b\u7f29\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u540c\u65f6\u4fdd\u6301\u5176\u63a8\u7406\u80fd\u529b\u548c\u8bed\u8a00\u591a\u6837\u6027\uff0c\u5e76\u63a2\u8ba8\u4e86\u5b83\u4eec\u7684\u6574\u5408\u7b56\u7565\u53ca\u5176\u5e94\u7528\u4e0e\u6311\u6218\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u8ba1\u7b97\u548c\u6570\u636e\u9700\u6c42\u6025\u5267\u589e\u52a0\uff0c\u4e9f\u9700\u9ad8\u6548\u7684\u538b\u7f29\u7b56\u7565\u4ee5\u4fdd\u6301\u5176\u6027\u80fd\u3002", "method": "\u5206\u6790\u4e86KD\u4e2d\u7684\u4efb\u52a1\u5bf9\u9f50\u3001\u57fa\u4e8e\u7406\u6027\u7684\u8bad\u7ec3\u548c\u591a\u6559\u5e08\u6846\u67b6\uff0c\u4ee5\u53caDD\u4e2d\u7684\u68af\u5ea6\u5339\u914d\u3001\u6f5c\u5728\u7a7a\u95f4\u6b63\u5219\u5316\u548c\u751f\u6210\u5408\u6210\u7b49\u6280\u672f\u3002", "result": "\u6574\u5408KD\u548cDD\u53ef\u63d0\u5347\u538b\u7f29\u7b56\u7565\u7684\u6709\u6548\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u89e3\u51b3\u6a21\u578b\u6269\u5c55\u3001\u67b6\u6784\u5f02\u8d28\u6027\u548c\u80fd\u529b\u4fdd\u7559\u7b49\u95ee\u9898\u3002", "conclusion": "\u5c3d\u7ba1\u53d6\u5f97\u8fdb\u5c55\uff0c\u4ecd\u9700\u89e3\u51b3\u63a8\u7406\u591a\u6837\u6027\u3001\u9002\u5e94\u6027\u548c\u8bc4\u4f30\u534f\u8bae\u7b49\u6311\u6218\uff0c\u901a\u8fc7\u6574\u5408KD\u548cDD\u5b9e\u73b0\u53ef\u6301\u7eed\u7684\u9ad8\u6548LLMs\u3002"}}
{"id": "2504.14264", "pdf": "https://arxiv.org/pdf/2504.14264", "abs": "https://arxiv.org/abs/2504.14264", "authors": ["Juan Nathaniel", "Pierre Gentine"], "title": "Generative emulation of chaotic dynamics with coherent prior", "categories": ["cs.LG", "stat.ML"], "comment": "41 pages, 25 figures", "summary": "Data-driven emulation of nonlinear dynamics is challenging due to long-range\nskill decay that often produces physically unrealistic outputs. Recent advances\nin generative modeling aim to address these issues by providing uncertainty\nquantification and correction. However, the quality of generated simulation\nremains heavily dependent on the choice of conditioning priors. In this work,\nwe present an efficient generative framework for dynamics emulation, unifying\nprinciples of turbulence with diffusion-based modeling: Cohesion. Specifically,\nour method estimates large-scale coherent structure of the underlying dynamics\nas guidance during the denoising process, where small-scale fluctuation in the\nflow is then resolved. These coherent priors are efficiently approximated using\nreduced-order models, such as deep Koopman operators, that allow for rapid\ngeneration of long prior sequences while maintaining stability over extended\nforecasting horizon. With this gain, we can reframe forecasting as trajectory\nplanning, a common task in reinforcement learning, where conditional denoising\nis performed once over entire sequences, minimizing the computational cost of\nautoregressive-based generative methods. Empirical evaluations on chaotic\nsystems of increasing complexity, including Kolmogorov flow, shallow water\nequations, and subseasonal-to-seasonal climate dynamics, demonstrate Cohesion\nsuperior long-range forecasting skill that can efficiently generate\nphysically-consistent simulations, even in the presence of partially-observed\nguidance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u751f\u6210\u5efa\u6a21\u7684\u52a8\u6001\u4eff\u771f\u6846\u67b6Cohesion\uff0c\u901a\u8fc7\u7ed3\u5408\u6e4d\u6d41\u539f\u7406\u548c\u6269\u6563\u6a21\u578b\uff0c\u5229\u7528\u5927\u5c3a\u5ea6\u76f8\u5e72\u7ed3\u6784\u6307\u5bfc\u53bb\u566a\u8fc7\u7a0b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u957f\u671f\u9884\u6d4b\u7684\u7269\u7406\u4e00\u81f4\u6027\u3002", "motivation": "\u89e3\u51b3\u6570\u636e\u9a71\u52a8\u7684\u975e\u7ebf\u6027\u52a8\u6001\u4eff\u771f\u4e2d\u56e0\u957f\u7a0b\u6280\u80fd\u8870\u51cf\u5bfc\u81f4\u7684\u7269\u7406\u4e0d\u73b0\u5b9e\u8f93\u51fa\u95ee\u9898\uff0c\u540c\u65f6\u63d0\u4f9b\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u548c\u6821\u6b63\u3002", "method": "\u7ed3\u5408\u6e4d\u6d41\u539f\u7406\u4e0e\u6269\u6563\u6a21\u578b\uff0c\u5229\u7528\u964d\u9636\u6a21\u578b\uff08\u5982\u6df1\u5ea6Koopman\u7b97\u5b50\uff09\u5feb\u901f\u751f\u6210\u957f\u5e8f\u5217\u76f8\u5e72\u5148\u9a8c\uff0c\u5c06\u9884\u6d4b\u4efb\u52a1\u91cd\u6784\u4e3a\u8f68\u8ff9\u89c4\u5212\u95ee\u9898\u3002", "result": "\u5728\u590d\u6742\u6df7\u6c8c\u7cfb\u7edf\uff08\u5982Kolmogorov\u6d41\u3001\u6d45\u6c34\u65b9\u7a0b\u548c\u6b21\u5b63\u8282\u81f3\u5b63\u8282\u6c14\u5019\u52a8\u6001\uff09\u4e2d\uff0cCohesion\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u957f\u671f\u9884\u6d4b\u80fd\u529b\uff0c\u80fd\u9ad8\u6548\u751f\u6210\u7269\u7406\u4e00\u81f4\u7684\u4eff\u771f\u3002", "conclusion": "Cohesion\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u76f8\u5e72\u5148\u9a8c\u548c\u6761\u4ef6\u53bb\u566a\uff0c\u663e\u8457\u63d0\u5347\u4e86\u52a8\u6001\u4eff\u771f\u7684\u957f\u671f\u9884\u6d4b\u80fd\u529b\u548c\u7269\u7406\u4e00\u81f4\u6027\uff0c\u9002\u7528\u4e8e\u90e8\u5206\u89c2\u6d4b\u6307\u5bfc\u7684\u573a\u666f\u3002"}}
{"id": "2504.14810", "pdf": "https://arxiv.org/pdf/2504.14810", "abs": "https://arxiv.org/abs/2504.14810", "authors": ["Jucheng Hu", "Surong Yang", "Dongzhan Zhou", "Lijun Wu"], "title": "DONOD: Robust and Generalizable Instruction Fine-Tuning for LLMs via Model-Intrinsic Dataset Pruning", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Ad-hoc instruction fine-tuning of large language models (LLMs) is widely\nadopted for domain-specific adaptation. While domain-specific supervised\nfine-tuning (SFT) is effective and efficient, it often weakens cross-domain\ngeneralization and struggles with noisy training data. To address these\nchallenges, we propose DONOD, a lightweight model-intrinsic data pruning\nmethod. Our approach evaluates data using two model-parameter-based metrics:\nDelta of Norm (DON), which captures the cumulative influence on model weights,\nand Norm of Delta (NOD), which quantifies weight instability. Moreover, by\nemploying the Technique for Order of Preference by Similarity to Ideal Solution\n(TOPSIS) algorithm, we effectively filter noisy, unlearnable, and\ngeneralization-harming samples without relying on auxiliary models during the\nSFT process. Experiments on mathematical tasks demonstrate that data selected\nby DONOD achieve superior fine-tuning efficiency and improved robustness\nagainst noisy data. By filtering out 70% of the full dataset, we improve\ntarget-domain accuracy by 14.90% and cross-domain accuracy by 5.67%. Meanwhile,\nour selected data present superior cross-architecture generalization. Data\npruned by smaller models (e.g., Llama 3.1-8B) generalize effectively on larger\nmodels (e.g., Llama 2-13B). Compared to existing related methodologies, DONOD\ndemonstrates comparable or superior performance while remaining\ndataset-agnostic, enabling broader applicability.", "AI": {"tldr": "DONOD\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u6570\u636e\u526a\u679d\u65b9\u6cd5\uff0c\u901a\u8fc7\u6a21\u578b\u53c2\u6570\u8bc4\u4f30\u6570\u636e\u8d28\u91cf\uff0c\u63d0\u5347\u9886\u57df\u9002\u5e94\u6027\u548c\u6297\u566a\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u9886\u57df\u7279\u5b9a\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u4e2d\u8de8\u57df\u6cdb\u5316\u80fd\u529b\u4e0b\u964d\u548c\u566a\u58f0\u6570\u636e\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528Delta of Norm\uff08DON\uff09\u548cNorm of Delta\uff08NOD\uff09\u8bc4\u4f30\u6570\u636e\uff0c\u7ed3\u5408TOPSIS\u7b97\u6cd5\u8fc7\u6ee4\u566a\u58f0\u6837\u672c\u3002", "result": "\u5728\u6570\u5b66\u4efb\u52a1\u4e2d\uff0cDONOD\u63d0\u5347\u4e86\u76ee\u6807\u57df\u51c6\u786e\u738714.90%\uff0c\u8de8\u57df\u51c6\u786e\u73875.67%\uff0c\u5e76\u5c55\u793a\u4e86\u8de8\u67b6\u6784\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "DONOD\u5728\u6027\u80fd\u548c\u5e94\u7528\u8303\u56f4\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u4e0d\u4f9d\u8d56\u7279\u5b9a\u6570\u636e\u96c6\u3002"}}
{"id": "2504.14804", "pdf": "https://arxiv.org/pdf/2504.14804", "abs": "https://arxiv.org/abs/2504.14804", "authors": ["Jiaxin GUO", "Xiaoyu Chen", "Zhiqiang Rao", "Jinlong Yang", "Zongyao Li", "Hengchao Shang", "Daimeng Wei", "Hao Yang"], "title": "Automatic Evaluation Metrics for Document-level Translation: Overview, Challenges and Trends", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "With the rapid development of deep learning technologies, the field of\nmachine translation has witnessed significant progress, especially with the\nadvent of large language models (LLMs) that have greatly propelled the\nadvancement of document-level translation. However, accurately evaluating the\nquality of document-level translation remains an urgent issue. This paper first\nintroduces the development status of document-level translation and the\nimportance of evaluation, highlighting the crucial role of automatic evaluation\nmetrics in reflecting translation quality and guiding the improvement of\ntranslation systems. It then provides a detailed analysis of the current state\nof automatic evaluation schemes and metrics, including evaluation methods with\nand without reference texts, as well as traditional metrics, Model-based\nmetrics and LLM-based metrics. Subsequently, the paper explores the challenges\nfaced by current evaluation methods, such as the lack of reference diversity,\ndependence on sentence-level alignment information, and the bias, inaccuracy,\nand lack of interpretability of the LLM-as-a-judge method. Finally, the paper\nlooks ahead to the future trends in evaluation methods, including the\ndevelopment of more user-friendly document-level evaluation methods and more\nrobust LLM-as-a-judge methods, and proposes possible research directions, such\nas reducing the dependency on sentence-level information, introducing\nmulti-level and multi-granular evaluation approaches, and training models\nspecifically for machine translation evaluation. This study aims to provide a\ncomprehensive analysis of automatic evaluation for document-level translation\nand offer insights into future developments.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u6587\u6863\u7ea7\u673a\u5668\u7ffb\u8bd1\u7684\u81ea\u52a8\u8bc4\u4f30\u73b0\u72b6\u4e0e\u6311\u6218\uff0c\u5e76\u5c55\u671b\u4e86\u672a\u6765\u53d1\u5c55\u65b9\u5411\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u5174\u8d77\uff0c\u6587\u6863\u7ea7\u7ffb\u8bd1\u53d6\u5f97\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u8bc4\u4f30\u5176\u8d28\u91cf\u4ecd\u662f\u4e00\u4e2a\u7d27\u8feb\u95ee\u9898\u3002", "method": "\u5206\u6790\u4e86\u73b0\u6709\u81ea\u52a8\u8bc4\u4f30\u65b9\u6848\u548c\u6307\u6807\uff0c\u5305\u62ec\u6709\u65e0\u53c2\u8003\u6587\u672c\u7684\u65b9\u6cd5\u3001\u4f20\u7edf\u6307\u6807\u3001\u57fa\u4e8e\u6a21\u578b\u7684\u6307\u6807\u53ca\u57fa\u4e8eLLM\u7684\u6307\u6807\u3002", "result": "\u6307\u51fa\u5f53\u524d\u8bc4\u4f30\u65b9\u6cd5\u7684\u4e0d\u8db3\uff0c\u5982\u53c2\u8003\u591a\u6837\u6027\u7f3a\u4e4f\u3001\u4f9d\u8d56\u53e5\u5b50\u7ea7\u5bf9\u9f50\u4fe1\u606f\u3001LLM\u8bc4\u4f30\u65b9\u6cd5\u7684\u504f\u89c1\u548c\u4e0d\u51c6\u786e\u6027\u3002", "conclusion": "\u5c55\u671b\u672a\u6765\u8d8b\u52bf\uff0c\u63d0\u51fa\u51cf\u5c11\u5bf9\u53e5\u5b50\u7ea7\u4fe1\u606f\u7684\u4f9d\u8d56\u3001\u5f15\u5165\u591a\u5c42\u6b21\u8bc4\u4f30\u65b9\u6cd5\u7b49\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2504.14268", "pdf": "https://arxiv.org/pdf/2504.14268", "abs": "https://arxiv.org/abs/2504.14268", "authors": ["Xinye Chen"], "title": "Mixed-Precision Conjugate Gradient Solvers with RL-Driven Precision Tuning", "categories": ["cs.LG"], "comment": null, "summary": "This paper presents a novel reinforcement learning (RL) framework for\ndynamically optimizing numerical precision in the preconditioned conjugate\ngradient (CG) method. By modeling precision selection as a Markov Decision\nProcess (MDP), we employ Q-learning to adaptively assign precision levels to\nkey operations, striking an optimal balance between computational efficiency\nand numerical accuracy, while ensuring stability through double-precision\nscalar computations and residual computing. In practice, the algorithm is\ntrained on a set of data and subsequently performs inference for precision\nselection on out-of-sample data, without requiring re-analysis or retraining\nfor new datasets. This enables the method to adapt seamlessly to new problem\ninstances without the computational overhead of recalibration. Our results\ndemonstrate the effectiveness of RL in enhancing solver's performance, marking\nthe first application of RL to mixed-precision numerical methods. The findings\nhighlight the approach's practical advantages, robustness, and scalability,\nproviding valuable insights into its integration with iterative solvers and\npaving the way for AI-driven advancements in scientific computing.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u52a8\u6001\u4f18\u5316\u9884\u6761\u4ef6\u5171\u8f6d\u68af\u5ea6\u6cd5\u4e2d\u7684\u6570\u503c\u7cbe\u5ea6\uff0c\u901a\u8fc7Q\u5b66\u4e60\u81ea\u9002\u5e94\u5206\u914d\u7cbe\u5ea6\uff0c\u5e73\u8861\u8ba1\u7b97\u6548\u7387\u4e0e\u6570\u503c\u7cbe\u5ea6\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u6570\u503c\u65b9\u6cd5\u5728\u7cbe\u5ea6\u9009\u62e9\u4e0a\u7684\u9759\u6001\u6027\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5b9e\u73b0\u52a8\u6001\u4f18\u5316\uff0c\u63d0\u5347\u8ba1\u7b97\u6548\u7387\u548c\u9002\u5e94\u6027\u3002", "method": "\u5c06\u7cbe\u5ea6\u9009\u62e9\u5efa\u6a21\u4e3a\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff0c\u4f7f\u7528Q\u5b66\u4e60\u81ea\u9002\u5e94\u5206\u914d\u7cbe\u5ea6\uff0c\u540c\u65f6\u901a\u8fc7\u53cc\u7cbe\u5ea6\u6807\u91cf\u8ba1\u7b97\u548c\u6b8b\u5dee\u8ba1\u7b97\u786e\u4fdd\u7a33\u5b9a\u6027\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u5347\u6c42\u89e3\u5668\u6027\u80fd\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u9002\u5e94\u65b0\u6570\u636e\u96c6\uff0c\u5177\u6709\u9c81\u68d2\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u9996\u6b21\u5c06\u5f3a\u5316\u5b66\u4e60\u5e94\u7528\u4e8e\u6df7\u5408\u7cbe\u5ea6\u6570\u503c\u65b9\u6cd5\uff0c\u4e3a\u79d1\u5b66\u8ba1\u7b97\u4e2d\u7684AI\u9a71\u52a8\u8fdb\u6b65\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2504.14838", "pdf": "https://arxiv.org/pdf/2504.14838", "abs": "https://arxiv.org/abs/2504.14838", "authors": ["Yizhou Chen", "Yawen Liu", "Xuesi Wang", "Qingtao Yu", "Guangda Huzhang", "Anxiang Zeng", "Han Yu", "Zhiming Zhou"], "title": "Establishing Reliability Metrics for Reward Models in Large Language Models", "categories": ["cs.AI"], "comment": null, "summary": "The reward model (RM) that represents human preferences plays a crucial role\nin optimizing the outputs of large language models (LLMs), e.g., through\nreinforcement learning from human feedback (RLHF) or rejection sampling.\nHowever, a long challenge for RM is its uncertain reliability, i.e., LLM\noutputs with higher rewards may not align with actual human preferences.\nCurrently, there is a lack of a convincing metric to quantify the reliability\nof RMs. To bridge this gap, we propose the \\textit{\\underline{R}eliable at\n\\underline{$\\eta$}} (RETA) metric, which directly measures the reliability of\nan RM by evaluating the average quality (scored by an oracle) of the top $\\eta$\nquantile responses assessed by an RM. On top of RETA, we present an integrated\nbenchmarking pipeline that allows anyone to evaluate their own RM without\nincurring additional Oracle labeling costs. Extensive experimental studies\ndemonstrate the superior stability of RETA metric, providing solid evaluations\nof the reliability of various publicly available and proprietary RMs. When\ndealing with an unreliable RM, we can use the RETA metric to identify the\noptimal quantile from which to select the responses.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRETA\u7684\u6307\u6807\uff0c\u7528\u4e8e\u76f4\u63a5\u8861\u91cf\u5956\u52b1\u6a21\u578b\uff08RM\uff09\u7684\u53ef\u9760\u6027\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u7a33\u5b9a\u6027\u3002", "motivation": "\u5956\u52b1\u6a21\u578b\u5728\u4f18\u5316\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8f93\u51fa\u4e2d\u8d77\u5173\u952e\u4f5c\u7528\uff0c\u4f46\u5176\u53ef\u9760\u6027\u5b58\u5728\u4e0d\u786e\u5b9a\u6027\uff0c\u7f3a\u4e4f\u91cf\u5316\u6307\u6807\u3002", "method": "\u63d0\u51faRETA\u6307\u6807\uff0c\u901a\u8fc7\u8bc4\u4f30RM\u8bc4\u5206\u6700\u9ad8\u7684\u03b7\u5206\u4f4d\u6570\u54cd\u5e94\u7684\u5e73\u5747\u8d28\u91cf\uff08\u7531Oracle\u8bc4\u5206\uff09\u6765\u8861\u91cfRM\u53ef\u9760\u6027\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u6d41\u7a0b\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eRETA\u6307\u6807\u5177\u6709\u4f18\u8d8a\u7684\u7a33\u5b9a\u6027\uff0c\u80fd\u591f\u6709\u6548\u8bc4\u4f30\u4e0d\u540cRM\u7684\u53ef\u9760\u6027\uff0c\u5e76\u5e2e\u52a9\u8bc6\u522b\u6700\u4f73\u54cd\u5e94\u5206\u4f4d\u6570\u3002", "conclusion": "RETA\u4e3aRM\u53ef\u9760\u6027\u63d0\u4f9b\u4e86\u91cf\u5316\u6807\u51c6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u95ee\u9898\uff0c\u5e76\u652f\u6301\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u51b3\u7b56\u3002"}}
{"id": "2504.14808", "pdf": "https://arxiv.org/pdf/2504.14808", "abs": "https://arxiv.org/abs/2504.14808", "authors": ["Mario M. Kubek", "Shiraj Pokharel", "Thomas B\u00f6hme", "Emma L. McDaniel", "Herwig Unger", "Armin R. Mikler"], "title": "On Self-improving Token Embeddings", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG", "68T50, 68T07", "I.2.6; I.2.7; H.3.3"], "comment": "18 pages, 4 figures, 3 tables, accepted at the 2025 25th\n  International Conference on Innovations for Community Services (I4CS), June\n  11 - 13, Munich, Germany, 2025", "summary": "This article introduces a novel and fast method for refining pre-trained\nstatic word or, more generally, token embeddings. By incorporating the\nembeddings of neighboring tokens in text corpora, it continuously updates the\nrepresentation of each token, including those without pre-assigned embeddings.\nThis approach effectively addresses the out-of-vocabulary problem, too.\nOperating independently of large language models and shallow neural networks,\nit enables versatile applications such as corpus exploration, conceptual\nsearch, and word sense disambiguation. The method is designed to enhance token\nrepresentations within topically homogeneous corpora, where the vocabulary is\nrestricted to a specific domain, resulting in more meaningful embeddings\ncompared to general-purpose pre-trained vectors. As an example, the methodology\nis applied to explore storm events and their impacts on infrastructure and\ncommunities using narratives from a subset of the NOAA Storm Events database.\nThe article also demonstrates how the approach improves the representation of\nstorm-related terms over time, providing valuable insights into the evolving\nnature of disaster narratives.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5feb\u901f\u4f18\u5316\u9884\u8bad\u7ec3\u9759\u6001\u8bcd\u6216\u6807\u8bb0\u5d4c\u5165\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u6587\u672c\u8bed\u6599\u5e93\u4e2d\u76f8\u90bb\u6807\u8bb0\u7684\u5d4c\u5165\uff0c\u6301\u7eed\u66f4\u65b0\u6bcf\u4e2a\u6807\u8bb0\u7684\u8868\u793a\uff0c\u5305\u62ec\u90a3\u4e9b\u6ca1\u6709\u9884\u5206\u914d\u5d4c\u5165\u7684\u6807\u8bb0\u3002\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u8bcd\u6c47\u5916\u95ee\u9898\uff0c\u4e14\u4e0d\u4f9d\u8d56\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6216\u6d45\u5c42\u795e\u7ecf\u7f51\u7edc\uff0c\u9002\u7528\u4e8e\u8bed\u6599\u5e93\u63a2\u7d22\u3001\u6982\u5ff5\u641c\u7d22\u548c\u8bcd\u4e49\u6d88\u6b67\u7b49\u4efb\u52a1\u3002", "motivation": "\u52a8\u673a\u5728\u4e8e\u89e3\u51b3\u9884\u8bad\u7ec3\u5d4c\u5165\u7684\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u5728\u7279\u5b9a\u9886\u57df\u8bed\u6599\u5e93\u4e2d\u8bcd\u6c47\u5916\u95ee\u9898\u548c\u5d4c\u5165\u8d28\u91cf\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u65b9\u6cd5\u662f\u901a\u8fc7\u7ed3\u5408\u76f8\u90bb\u6807\u8bb0\u7684\u5d4c\u5165\u52a8\u6001\u66f4\u65b0\u6807\u8bb0\u8868\u793a\uff0c\u9002\u7528\u4e8e\u4e3b\u9898\u540c\u8d28\u7684\u8bed\u6599\u5e93\uff0c\u63d0\u5347\u5d4c\u5165\u7684\u9886\u57df\u76f8\u5173\u6027\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u663e\u8457\u63d0\u5347\u9886\u57df\u76f8\u5173\u6807\u8bb0\u7684\u8868\u793a\u8d28\u91cf\uff0c\u5e76\u5728\u98ce\u66b4\u4e8b\u4ef6\u6570\u636e\u5e93\u4e2d\u5c55\u793a\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u6548\u679c\u3002", "conclusion": "\u7ed3\u8bba\u662f\u8be5\u65b9\u6cd5\u4e3a\u7279\u5b9a\u9886\u57df\u8bed\u6599\u5e93\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u7075\u6d3b\u7684\u5d4c\u5165\u4f18\u5316\u65b9\u6848\uff0c\u80fd\u591f\u6355\u6349\u8bcd\u6c47\u7684\u6f14\u53d8\u548c\u9886\u57df\u7279\u6027\u3002"}}
{"id": "2504.14286", "pdf": "https://arxiv.org/pdf/2504.14286", "abs": "https://arxiv.org/abs/2504.14286", "authors": ["Xiaojiang Zhang", "Jinghui Wang", "Zifei Cheng", "Wenhao Zhuang", "Zheng Lin", "Minglei Zhang", "Shaojie Wang", "Yinghan Cui", "Chao Wang", "Junyi Peng", "Shimiao Jiang", "Shiqi Kuang", "Shouyu Yin", "Chaohang Wen", "Haotian Zhang", "Bin Chen", "Bing Yu"], "title": "SRPO: A Cross-Domain Implementation of Large-Scale Reinforcement Learning on LLM", "categories": ["cs.LG"], "comment": null, "summary": "Recent advances of reasoning models, exemplified by OpenAI's o1 and\nDeepSeek's R1, highlight the significant potential of Reinforcement Learning\n(RL) to enhance the reasoning capabilities of Large Language Models (LLMs).\nHowever, replicating these advancements across diverse domains remains\nchallenging due to limited methodological transparency. In this work, we\npresent two-Staged history-Resampling Policy Optimization (SRPO), which\nsuccessfully surpasses the performance of DeepSeek-R1-Zero-32B on the AIME24\nand LiveCodeBench benchmarks. SRPO achieves this using the same base model as\nDeepSeek (i.e. Qwen2.5-32B) and relies solely on RL, without prior Supervised\nFine-Tuning (SFT). Building upon Group Relative Policy Optimization (GRPO), we\nintroduce two key methodological innovations: (1) a two-stage cross-domain\ntraining paradigm designed to balance the development of mathematical reasoning\nand coding proficiency, and (2) History Resampling (HR), a technique to address\nineffective samples. Our comprehensive experiments validate the effectiveness\nof our approach, dedicating to offer valuable insights into scaling LLM\nreasoning capabilities across diverse tasks.", "AI": {"tldr": "SRPO\u662f\u4e00\u79cd\u4e24\u9636\u6bb5\u5386\u53f2\u91cd\u91c7\u6837\u7b56\u7565\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u63d0\u5347LLM\u7684\u63a8\u7406\u80fd\u529b\uff0c\u65e0\u9700\u76d1\u7763\u5fae\u8c03\uff0c\u5728AIME24\u548cLiveCodeBench\u4e0a\u8d85\u8d8aDeepSeek-R1-Zero-32B\u3002", "motivation": "\u73b0\u6709\u63a8\u7406\u6a21\u578b\uff08\u5982OpenAI\u7684o1\u548cDeepSeek\u7684R1\uff09\u5728\u8de8\u9886\u57df\u63a8\u5e7f\u65f6\u56e0\u65b9\u6cd5\u900f\u660e\u5ea6\u4e0d\u8db3\u800c\u53d7\u9650\uff0cSRPO\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u57fa\u4e8eGRPO\uff0c\u63d0\u51fa\u4e24\u9636\u6bb5\u8de8\u9886\u57df\u8bad\u7ec3\u8303\u5f0f\u4ee5\u5e73\u8861\u6570\u5b66\u63a8\u7406\u4e0e\u7f16\u7a0b\u80fd\u529b\uff0c\u5e76\u5f15\u5165\u5386\u53f2\u91cd\u91c7\u6837\uff08HR\uff09\u6280\u672f\u5904\u7406\u65e0\u6548\u6837\u672c\u3002", "result": "SRPO\u5728\u76f8\u540c\u57fa\u7840\u6a21\u578b\uff08Qwen2.5-32B\uff09\u4e0b\uff0c\u4ec5\u4f9d\u8d56\u5f3a\u5316\u5b66\u4e60\u5373\u8d85\u8d8aDeepSeek-R1-Zero-32B\u7684\u6027\u80fd\u3002", "conclusion": "SRPO\u4e3a\u8de8\u4efb\u52a1\u6269\u5c55LLM\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6cd5\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u4ef7\u503c\u3002"}}
{"id": "2504.14858", "pdf": "https://arxiv.org/pdf/2504.14858", "abs": "https://arxiv.org/abs/2504.14858", "authors": ["Jiaqi Wei", "Hao Zhou", "Xiang Zhang", "Di Zhang", "Zijie Qiu", "Wei Wei", "Jinzhe Li", "Wanli Ouyang", "Siqi Sun"], "title": "AlignRAG: An Adaptable Framework for Resolving Misalignments in Retrieval-Aware Reasoning of RAG", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Retrieval-augmented generation (RAG) has emerged as a foundational paradigm\nfor knowledge-grounded text generation. However, existing RAG pipelines often\nfail to ensure that the reasoning trajectories align with the evidential\nconstraints imposed by retrieved content. In this paper, we reframe RAG as a\nproblem of retrieval-aware reasoning and identify a core challenge: reasoning\nmisalignment-the mismatch between a model's reasoning trajectory and the\nretrieved evidence. To address this challenge, we propose AlignRAG, a novel\ntest-time framework that mitigates reasoning misalignment through iterative\nCritique-Driven Alignment (CDA) steps. In contrast to prior approaches that\nrely on static training or post-hoc selection, AlignRAG actively refines\nreasoning trajectories during inference by enforcing fine-grained alignment\nwith evidence. Our framework introduces a new paradigm for retrieval-aware\nreasoning by: (1) constructing context-rich training corpora; (2) generating\ncontrastive critiques from preference-aware reasoning trajectories; (3)\ntraining a dedicated \\textit{Critic Language Model (CLM)} to identify reasoning\nmisalignments; and (4) applying CDA steps to optimize reasoning trajectories\niteratively. Empirical results demonstrate that AlignRAG consistently\noutperforms all baselines and could integrate as a plug-and-play module into\nexisting RAG pipelines without further changes. By reconceptualizing RAG as a\nstructured reasoning trajectory and establishing the test-time framework for\ncorrecting reasoning misalignments in RAG, AlignRAG provides practical\nadvancements for retrieval-aware generation.", "AI": {"tldr": "AlignRAG\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6d4b\u8bd5\u65f6\u6846\u67b6\uff0c\u901a\u8fc7\u8fed\u4ee3\u7684Critique-Driven Alignment\uff08CDA\uff09\u6b65\u9aa4\u89e3\u51b3RAG\u4e2d\u7684\u63a8\u7406\u5bf9\u9f50\u95ee\u9898\u3002", "motivation": "\u73b0\u6709RAG\u7ba1\u9053\u672a\u80fd\u786e\u4fdd\u63a8\u7406\u8f68\u8ff9\u4e0e\u68c0\u7d22\u5185\u5bb9\u4e00\u81f4\uff0c\u5bfc\u81f4\u63a8\u7406\u5bf9\u9f50\u95ee\u9898\u3002", "method": "AlignRAG\u901a\u8fc7\u6784\u5efa\u4e0a\u4e0b\u6587\u4e30\u5bcc\u7684\u8bad\u7ec3\u8bed\u6599\u3001\u751f\u6210\u5bf9\u6bd4\u6027\u6279\u8bc4\u3001\u8bad\u7ec3Critic Language Model\uff08CLM\uff09\u548c\u5e94\u7528CDA\u6b65\u9aa4\u8fed\u4ee3\u4f18\u5316\u63a8\u7406\u8f68\u8ff9\u3002", "result": "AlignRAG\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u53ef\u65e0\u7f1d\u96c6\u6210\u5230\u73b0\u6709RAG\u7ba1\u9053\u4e2d\u3002", "conclusion": "AlignRAG\u4e3a\u68c0\u7d22\u611f\u77e5\u751f\u6210\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u8fdb\u5c55\uff0c\u91cd\u65b0\u5b9a\u4e49\u4e86RAG\u4e3a\u7ed3\u6784\u5316\u63a8\u7406\u8f68\u8ff9\u3002"}}
{"id": "2504.14856", "pdf": "https://arxiv.org/pdf/2504.14856", "abs": "https://arxiv.org/abs/2504.14856", "authors": ["Jiajun Shen", "Tong Zhou", "Yubo Chen", "Delai Qiu", "Shengping Liu", "Kang Liu", "Jun Zhao"], "title": "Transparentize the Internal and External Knowledge Utilization in LLMs with Trustworthy Citation", "categories": ["cs.CL"], "comment": "19 pages, 14 figures", "summary": "While hallucinations of large language models could been alleviated through\nretrieval-augmented generation and citation generation, how the model utilizes\ninternal knowledge is still opaque, and the trustworthiness of its generated\nanswers remains questionable. In this work, we introduce Context-Prior\nAugmented Citation Generation task, requiring models to generate citations\nconsidering both external and internal knowledge while providing trustworthy\nreferences, with 5 evaluation metrics focusing on 3 aspects: answer\nhelpfulness, citation faithfulness, and trustworthiness. We introduce RAEL, the\nparadigm for our task, and also design INTRALIGN, an integrated method\ncontaining customary data generation and an alignment algorithm. Our\nexperimental results show that our method achieves a better cross-scenario\nperformance with regard to other baselines. Our extended experiments further\nreveal that retrieval quality, question types, and model knowledge have\nconsiderable influence on the trustworthiness in citation generation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5916\u90e8\u548c\u5185\u90e8\u77e5\u8bc6\u7684\u5f15\u7528\u751f\u6210\u4efb\u52a1\uff0c\u5e76\u8bbe\u8ba1\u4e86RAEL\u8303\u5f0f\u548cINTRALIGN\u65b9\u6cd5\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u65b9\u6cd5\u5728\u8de8\u573a\u666f\u6027\u80fd\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u3002", "motivation": "\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7b54\u6848\u65f6\u5185\u90e8\u77e5\u8bc6\u5229\u7528\u4e0d\u900f\u660e\u548c\u53ef\u4fe1\u5ea6\u5b58\u7591\u7684\u95ee\u9898\u3002", "method": "\u5f15\u5165Context-Prior Augmented Citation Generation\u4efb\u52a1\uff0c\u7ed3\u5408RAEL\u8303\u5f0f\u548cINTRALIGN\u65b9\u6cd5\uff08\u5305\u62ec\u6570\u636e\u751f\u6210\u548c\u5bf9\u9f50\u7b97\u6cd5\uff09\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u65b9\u6cd5\u5728\u8de8\u573a\u666f\u6027\u80fd\u4e0a\u4f18\u4e8e\u57fa\u7ebf\uff0c\u4e14\u68c0\u7d22\u8d28\u91cf\u3001\u95ee\u9898\u7c7b\u578b\u548c\u6a21\u578b\u77e5\u8bc6\u5bf9\u5f15\u7528\u751f\u6210\u7684\u53ef\u4fe1\u5ea6\u6709\u663e\u8457\u5f71\u54cd\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u5347\u5f15\u7528\u751f\u6210\u7684\u53ef\u4fe1\u5ea6\u548c\u6027\u80fd\uff0c\u63ed\u793a\u4e86\u5f71\u54cd\u53ef\u4fe1\u5ea6\u7684\u5173\u952e\u56e0\u7d20\u3002"}}
{"id": "2504.14300", "pdf": "https://arxiv.org/pdf/2504.14300", "abs": "https://arxiv.org/abs/2504.14300", "authors": ["Xinyu Liang", "Hao Wang"], "title": "Learning and Generating Diverse Residential Load Patterns Using GAN with Weakly-Supervised Training and Weight Selection", "categories": ["cs.LG", "cs.AI"], "comment": "12 pages", "summary": "The scarcity of high-quality residential load data can pose obstacles for\ndecarbonizing the residential sector as well as effective grid planning and\noperation. The above challenges have motivated research into generating\nsynthetic load data, but existing methods faced limitations in terms of\nscalability, diversity, and similarity. This paper proposes a Generative\nAdversarial Network-based Synthetic Residential Load Pattern (RLP-GAN)\ngeneration model, a novel weakly-supervised GAN framework, leveraging an\nover-complete autoencoder to capture dependencies within complex and diverse\nload patterns and learn household-level data distribution at scale. We\nincorporate a model weight selection method to address the mode collapse\nproblem and generate load patterns with high diversity. We develop a holistic\nevaluation method to validate the effectiveness of RLP-GAN using real-world\ndata of 417 households. The results demonstrate that RLP-GAN outperforms\nstate-of-the-art models in capturing temporal dependencies and generating load\npatterns with higher similarity to real data. Furthermore, we have publicly\nreleased the RLP-GAN generated synthetic dataset, which comprises one million\nsynthetic residential load pattern profiles.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff08RLP-GAN\uff09\u7684\u5f31\u76d1\u7763\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u9ad8\u8d28\u91cf\u4f4f\u5b85\u8d1f\u8377\u6570\u636e\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u53ef\u6269\u5c55\u6027\u3001\u591a\u6837\u6027\u548c\u76f8\u4f3c\u6027\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u9ad8\u8d28\u91cf\u4f4f\u5b85\u8d1f\u8377\u6570\u636e\u7684\u7a00\u7f3a\u963b\u788d\u4e86\u4f4f\u5b85\u9886\u57df\u8131\u78b3\u548c\u7535\u7f51\u89c4\u5212\u4e0e\u8fd0\u8425\u7684\u6709\u6548\u6027\uff0c\u73b0\u6709\u5408\u6210\u6570\u636e\u751f\u6210\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "\u91c7\u7528\u5f31\u76d1\u7763GAN\u6846\u67b6\uff0c\u7ed3\u5408\u8fc7\u5b8c\u5907\u81ea\u7f16\u7801\u5668\u6355\u83b7\u590d\u6742\u8d1f\u8377\u6a21\u5f0f\u7684\u4f9d\u8d56\u5173\u7cfb\uff0c\u5e76\u5f15\u5165\u6a21\u578b\u6743\u91cd\u9009\u62e9\u65b9\u6cd5\u89e3\u51b3\u6a21\u5f0f\u5d29\u6e83\u95ee\u9898\u3002", "result": "RLP-GAN\u5728417\u6237\u771f\u5b9e\u6570\u636e\u4e0a\u9a8c\u8bc1\uff0c\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u80fd\u751f\u6210\u66f4\u63a5\u8fd1\u771f\u5b9e\u6570\u636e\u7684\u8d1f\u8377\u6a21\u5f0f\u3002", "conclusion": "RLP-GAN\u6210\u529f\u751f\u6210\u9ad8\u8d28\u91cf\u5408\u6210\u8d1f\u8377\u6570\u636e\uff0c\u5e76\u516c\u5f00\u4e86\u5305\u542b100\u4e07\u6761\u5408\u6210\u8d1f\u8377\u6a21\u5f0f\u7684\u6570\u636e\u96c6\u3002"}}
{"id": "2504.14870", "pdf": "https://arxiv.org/pdf/2504.14870", "abs": "https://arxiv.org/abs/2504.14870", "authors": ["Hongru Wang", "Cheng Qian", "Wanjun Zhong", "Xiusi Chen", "Jiahao Qiu", "Shijue Huang", "Bowen Jin", "Mengdi Wang", "Kam-Fai Wong", "Heng Ji"], "title": "OTC: Optimal Tool Calls via Reinforcement Learning", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Tool-integrated reasoning (TIR) augments large language models (LLMs) with\nthe ability to invoke external tools, such as search engines and code\ninterpreters, to solve tasks beyond the capabilities of language-only\nreasoning. While reinforcement learning (RL) has shown promise in improving TIR\nby optimizing final answer correctness, existing approaches often overlook the\nefficiency and cost associated with tool usage. This can lead to suboptimal\nbehavior, including excessive tool calls that increase computational and\nfinancial overhead, or insufficient tool use that compromises answer quality.\nIn this work, we propose Optimal Tool Call-controlled Policy Optimization\n(OTC-PO), a simple yet effective RL-based framework that encourages models to\nproduce accurate answers with minimal tool calls. Our method introduces a\ntool-integrated reward that jointly considers correctness and tool efficiency,\npromoting high tool productivity. We instantiate this framework within both\nProximal Policy Optimization (PPO) and Group Relative Preference Optimization\n(GRPO), resulting in OTC-PPO and OTC-GRPO. Experiments with Qwen-2.5 and\nQwen-Math across multiple QA benchmarks show that our approach reduces tool\ncalls by up to 73.1\\% and improves tool productivity by up to 229.4\\%, while\nmaintaining comparable answer accuracy. To the best of our knowledge, this is\nthe first RL-based framework that explicitly optimizes tool-use efficiency in\nTIR.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aOTC-PO\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u4f18\u5316\u5de5\u5177\u8c03\u7528\u6548\u7387\uff0c\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u5728\u5de5\u5177\u96c6\u6210\u63a8\u7406\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5de5\u5177\u96c6\u6210\u63a8\u7406\u4e2d\u5ffd\u89c6\u4e86\u5de5\u5177\u4f7f\u7528\u7684\u6548\u7387\u548c\u6210\u672c\uff0c\u5bfc\u81f4\u5de5\u5177\u8c03\u7528\u8fc7\u591a\u6216\u4e0d\u8db3\uff0c\u5f71\u54cd\u6027\u80fd\u548c\u5f00\u9500\u3002", "method": "\u63d0\u51faOTC-PO\u6846\u67b6\uff0c\u7ed3\u5408\u6b63\u786e\u6027\u548c\u5de5\u5177\u6548\u7387\u7684\u5956\u52b1\u673a\u5236\uff0c\u5e76\u5728PPO\u548cGRPO\u4e2d\u5b9e\u73b0\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u5de5\u5177\u8c03\u7528\u51cf\u5c1173.1%\uff0c\u5de5\u5177\u6548\u7387\u63d0\u5347229.4%\uff0c\u540c\u65f6\u4fdd\u6301\u7b54\u6848\u51c6\u786e\u6027\u3002", "conclusion": "OTC-PO\u662f\u9996\u4e2a\u660e\u786e\u4f18\u5316\u5de5\u5177\u6548\u7387\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5de5\u5177\u96c6\u6210\u63a8\u7406\u7684\u6027\u80fd\u3002"}}
{"id": "2504.14871", "pdf": "https://arxiv.org/pdf/2504.14871", "abs": "https://arxiv.org/abs/2504.14871", "authors": ["Teppei Suzuki", "Ryokan Ri", "Sho Takase"], "title": "Natural Fingerprints of Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) often exhibit biases -- systematic deviations\nfrom expected norms -- in their outputs. These range from overt issues, such as\nunfair responses, to subtler patterns that can reveal which model produced\nthem. We investigate the factors that give rise to identifiable characteristics\nin LLMs. Since LLMs model training data distribution, it is reasonable that\ndifferences in training data naturally lead to the characteristics. However,\nour findings reveal that even when LLMs are trained on the exact same data, it\nis still possible to distinguish the source model based on its generated text.\nWe refer to these unintended, distinctive characteristics as natural\nfingerprints. By systematically controlling training conditions, we show that\nthe natural fingerprints can emerge from subtle differences in the training\nprocess, such as parameter sizes, optimization settings, and even random seeds.\nWe believe that understanding natural fingerprints offers new insights into the\norigins of unintended bias and ways for improving control over LLM behavior.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u5373\u4f7f\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4f7f\u7528\u76f8\u540c\u7684\u6570\u636e\u8bad\u7ec3\uff0c\u5176\u751f\u6210\u7684\u6587\u672c\u4ecd\u80fd\u901a\u8fc7\u81ea\u7136\u6307\u7eb9\u533a\u5206\u6765\u6e90\uff0c\u8fd9\u4e9b\u6307\u7eb9\u6e90\u4e8e\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u7ec6\u5fae\u5dee\u5f02\u3002", "motivation": "\u63a2\u7a76LLMs\u8f93\u51fa\u4e2d\u53ef\u8bc6\u522b\u7279\u5f81\u7684\u6210\u56e0\uff0c\u4ee5\u7406\u89e3\u65e0\u610f\u504f\u5dee\u7684\u6765\u6e90\u5e76\u6539\u8fdb\u6a21\u578b\u884c\u4e3a\u63a7\u5236\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u63a7\u5236\u8bad\u7ec3\u6761\u4ef6\uff08\u5982\u53c2\u6570\u5927\u5c0f\u3001\u4f18\u5316\u8bbe\u7f6e\u3001\u968f\u673a\u79cd\u5b50\u7b49\uff09\uff0c\u5206\u6790LLMs\u751f\u6210\u6587\u672c\u7684\u5dee\u5f02\u3002", "result": "\u53d1\u73b0\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u7ec6\u5fae\u5dee\u5f02\u4f1a\u5bfc\u81f4\u81ea\u7136\u6307\u7eb9\uff0c\u4ece\u800c\u533a\u5206\u6a21\u578b\u6765\u6e90\u3002", "conclusion": "\u7406\u89e3\u81ea\u7136\u6307\u7eb9\u6709\u52a9\u4e8e\u63ed\u793a\u65e0\u610f\u504f\u5dee\u7684\u8d77\u6e90\uff0c\u5e76\u4e3a\u6539\u8fdbLLM\u884c\u4e3a\u63a7\u5236\u63d0\u4f9b\u65b0\u601d\u8def\u3002"}}
{"id": "2504.14302", "pdf": "https://arxiv.org/pdf/2504.14302", "abs": "https://arxiv.org/abs/2504.14302", "authors": ["Yogev Kriger", "Shai Fine"], "title": "Learning to Score", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Common machine learning settings range from supervised tasks, where\naccurately labeled data is accessible, through semi-supervised and\nweakly-supervised tasks, where target labels are scant or noisy, to\nunsupervised tasks where labels are unobtainable. In this paper we study a\nscenario where the target labels are not available but additional related\ninformation is at hand. This information, referred to as Side Information, is\neither correlated with the unknown labels or imposes constraints on the feature\nspace. We formulate the problem as an ensemble of three semantic components:\nrepresentation learning, side information and metric learning. The proposed\nscoring model is advantageous for multiple use-cases. For example, in the\nhealthcare domain it can be used to create a severity score for diseases where\nthe symptoms are known but the criteria for the disease progression are not\nwell defined. We demonstrate the utility of the suggested scoring system on\nwell-known benchmark data-sets and bio-medical patient records.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u4e00\u79cd\u5728\u76ee\u6807\u6807\u7b7e\u4e0d\u53ef\u7528\u4f46\u5b58\u5728\u76f8\u5173\u8f85\u52a9\u4fe1\u606f\uff08Side Information\uff09\u7684\u573a\u666f\u4e0b\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u63d0\u51fa\u4e86\u7ed3\u5408\u8868\u793a\u5b66\u4e60\u3001\u8f85\u52a9\u4fe1\u606f\u548c\u5ea6\u91cf\u5b66\u4e60\u7684\u8bc4\u5206\u6a21\u578b\u3002", "motivation": "\u89e3\u51b3\u76ee\u6807\u6807\u7b7e\u7f3a\u5931\u4f46\u5b58\u5728\u76f8\u5173\u8f85\u52a9\u4fe1\u606f\u65f6\u7684\u5b66\u4e60\u95ee\u9898\uff0c\u4f8b\u5982\u533b\u7597\u9886\u57df\u4e2d\u75be\u75c5\u4e25\u91cd\u7a0b\u5ea6\u8bc4\u5206\u7684\u6784\u5efa\u3002", "method": "\u5c06\u95ee\u9898\u5efa\u6a21\u4e3a\u8868\u793a\u5b66\u4e60\u3001\u8f85\u52a9\u4fe1\u606f\u548c\u5ea6\u91cf\u5b66\u4e60\u7684\u7ec4\u5408\uff0c\u63d0\u51fa\u8bc4\u5206\u6a21\u578b\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u548c\u751f\u7269\u533b\u5b66\u60a3\u8005\u8bb0\u5f55\u4e0a\u9a8c\u8bc1\u4e86\u8bc4\u5206\u7cfb\u7edf\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u8bc4\u5206\u6a21\u578b\u5728\u76ee\u6807\u6807\u7b7e\u4e0d\u53ef\u7528\u4f46\u5b58\u5728\u8f85\u52a9\u4fe1\u606f\u7684\u573a\u666f\u4e0b\u5177\u6709\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2504.14928", "pdf": "https://arxiv.org/pdf/2504.14928", "abs": "https://arxiv.org/abs/2504.14928", "authors": ["Yao Shi", "Rongkeng Liang", "Yong Xu"], "title": "EducationQ: Evaluating LLMs' Teaching Capabilities Through Multi-Agent Dialogue Framework", "categories": ["cs.AI", "cs.CE", "cs.CL", "cs.CY", "cs.HC"], "comment": null, "summary": "Large language models (LLMs) increasingly serve as educational tools, yet\nevaluating their teaching capabilities remains challenging due to the\nresource-intensive, context-dependent, and methodologically complex nature of\nteacher-student interactions. We introduce EducationQ, a multi-agent dialogue\nframework that efficiently assesses teaching capabilities through simulated\ndynamic educational scenarios, featuring specialized agents for teaching,\nlearning, and evaluation. Testing 14 LLMs across major AI Organizations\n(OpenAI, Meta, Google, Anthropic, and others) on 1,498 questions spanning 13\ndisciplines and 10 difficulty levels reveals that teaching effectiveness does\nnot correlate linearly with model scale or general reasoning capabilities -\nwith some smaller open-source models outperforming larger commercial\ncounterparts in teaching contexts. This finding highlights a critical gap in\ncurrent evaluations that prioritize knowledge recall over interactive pedagogy.\nOur mixed-methods evaluation, combining quantitative metrics with qualitative\nanalysis and expert case studies, identifies distinct pedagogical strengths\nemployed by top-performing models (e.g., sophisticated questioning strategies,\nadaptive feedback mechanisms). Human expert evaluations show 78% agreement with\nour automated qualitative analysis of effective teaching behaviors, validating\nour methodology. EducationQ demonstrates that LLMs-as-teachers require\nspecialized optimization beyond simple scaling, suggesting next-generation\neducational AI prioritize targeted enhancement of specific pedagogical\neffectiveness.", "AI": {"tldr": "EducationQ\u6846\u67b6\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u5bf9\u8bdd\u8bc4\u4f30LLMs\u7684\u6559\u5b66\u80fd\u529b\uff0c\u53d1\u73b0\u6559\u5b66\u6548\u679c\u4e0e\u6a21\u578b\u89c4\u6a21\u6216\u901a\u7528\u63a8\u7406\u80fd\u529b\u65e0\u7ebf\u6027\u5173\u7cfb\uff0c\u90e8\u5206\u5c0f\u6a21\u578b\u8868\u73b0\u4f18\u4e8e\u5927\u6a21\u578b\u3002", "motivation": "\u5f53\u524dLLMs\u4f5c\u4e3a\u6559\u80b2\u5de5\u5177\u7684\u8bc4\u4f30\u65b9\u6cd5\u8d44\u6e90\u5bc6\u96c6\u4e14\u590d\u6742\uff0c\u7f3a\u4e4f\u5bf9\u4e92\u52a8\u6559\u5b66\u80fd\u529b\u7684\u5173\u6ce8\u3002", "method": "\u63d0\u51faEducationQ\u6846\u67b6\uff0c\u6a21\u62df\u52a8\u6001\u6559\u80b2\u573a\u666f\uff0c\u6d4b\u8bd514\u4e2aLLMs\u572813\u4e2a\u5b66\u79d1\u548c10\u4e2a\u96be\u5ea6\u7ea7\u522b\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u6559\u5b66\u6548\u679c\u4e0e\u6a21\u578b\u89c4\u6a21\u65e0\u5173\uff0c\u5c0f\u6a21\u578b\u53ef\u80fd\u66f4\u4f18\uff1b78%\u7684\u4eba\u7c7b\u4e13\u5bb6\u8bc4\u4f30\u4e0e\u81ea\u52a8\u5206\u6790\u4e00\u81f4\u3002", "conclusion": "LLMs\u9700\u9488\u5bf9\u6559\u5b66\u80fd\u529b\u4e13\u95e8\u4f18\u5316\uff0c\u800c\u975e\u7b80\u5355\u6269\u5c55\u89c4\u6a21\u3002"}}
{"id": "2504.14891", "pdf": "https://arxiv.org/pdf/2504.14891", "abs": "https://arxiv.org/abs/2504.14891", "authors": ["Aoran Gan", "Hao Yu", "Kai Zhang", "Qi Liu", "Wenyu Yan", "Zhenya Huang", "Shiwei Tong", "Guoping Hu"], "title": "Retrieval Augmented Generation Evaluation in the Era of Large Language Models: A Comprehensive Survey", "categories": ["cs.CL"], "comment": "18 pages, 5 figures", "summary": "Recent advancements in Retrieval-Augmented Generation (RAG) have\nrevolutionized natural language processing by integrating Large Language Models\n(LLMs) with external information retrieval, enabling accurate, up-to-date, and\nverifiable text generation across diverse applications. However, evaluating RAG\nsystems presents unique challenges due to their hybrid architecture that\ncombines retrieval and generation components, as well as their dependence on\ndynamic knowledge sources in the LLM era. In response, this paper provides a\ncomprehensive survey of RAG evaluation methods and frameworks, systematically\nreviewing traditional and emerging evaluation approaches, for system\nperformance, factual accuracy, safety, and computational efficiency in the LLM\nera. We also compile and categorize the RAG-specific datasets and evaluation\nframeworks, conducting a meta-analysis of evaluation practices in high-impact\nRAG research. To the best of our knowledge, this work represents the most\ncomprehensive survey for RAG evaluation, bridging traditional and LLM-driven\nmethods, and serves as a critical resource for advancing RAG development.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7cfb\u7edf\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5206\u6790\u4e86\u4f20\u7edf\u4e0e\u65b0\u5174\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5e76\u6574\u7406\u4e86\u76f8\u5173\u6570\u636e\u96c6\u548c\u6846\u67b6\uff0c\u4e3aRAG\u53d1\u5c55\u63d0\u4f9b\u5173\u952e\u8d44\u6e90\u3002", "motivation": "RAG\u7cfb\u7edf\u7ed3\u5408\u68c0\u7d22\u4e0e\u751f\u6210\u7ec4\u4ef6\uff0c\u4f9d\u8d56\u52a8\u6001\u77e5\u8bc6\u6e90\uff0c\u8bc4\u4f30\u9762\u4e34\u72ec\u7279\u6311\u6218\uff0c\u9700\u7cfb\u7edf\u68b3\u7406\u73b0\u6709\u65b9\u6cd5\u3002", "method": "\u7cfb\u7edf\u56de\u987eRAG\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5305\u62ec\u6027\u80fd\u3001\u4e8b\u5b9e\u51c6\u786e\u6027\u3001\u5b89\u5168\u6027\u548c\u8ba1\u7b97\u6548\u7387\uff0c\u5e76\u8fdb\u884c\u5143\u5206\u6790\u3002", "result": "\u6574\u7406\u4e86RAG\u4e13\u7528\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u6846\u67b6\uff0c\u586b\u8865\u4e86\u4f20\u7edf\u4e0eLLM\u9a71\u52a8\u65b9\u6cd5\u95f4\u7684\u7a7a\u767d\u3002", "conclusion": "\u672c\u6587\u662fRAG\u8bc4\u4f30\u9886\u57df\u6700\u5168\u9762\u7684\u7efc\u8ff0\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u3002"}}
{"id": "2504.14307", "pdf": "https://arxiv.org/pdf/2504.14307", "abs": "https://arxiv.org/abs/2504.14307", "authors": ["Muhammad Haseeb Aslam", "Clara Martinez", "Marco Pedersoli", "Alessandro Koerich", "Ali Etemad", "Eric Granger"], "title": "Learning from Stochastic Teacher Representations Using Student-Guided Knowledge Distillation", "categories": ["cs.LG"], "comment": null, "summary": "Advances in self-distillation have shown that when knowledge is distilled\nfrom a teacher to a student using the same deep learning (DL) architecture, the\nstudent performance can surpass the teacher particularly when the network is\noverparameterized and the teacher is trained with early stopping.\nAlternatively, ensemble learning also improves performance, although training,\nstoring, and deploying multiple models becomes impractical as the number of\nmodels grows. Even distilling an ensemble to a single student model or weight\naveraging methods first requires training of multiple teacher models and does\nnot fully leverage the inherent stochasticity for generating and distilling\ndiversity in DL models. These constraints are particularly prohibitive in\nresource-constrained or latency-sensitive applications such as wearable\ndevices. This paper proposes to train only one model and generate multiple\ndiverse teacher representations using distillation-time dropout. However,\ngenerating these representations stochastically leads to noisy representations\nthat are misaligned with the learned task. To overcome this problem, a novel\nstochastic self-distillation (SSD) training strategy is introduced for\nfiltering and weighting teacher representation to distill from task-relevant\nrepresentations only, using student-guided knowledge distillation (SGKD). The\nstudent representation at each distillation step is used as authority to guide\nthe distillation process. Experimental results on real-world affective\ncomputing, wearable/biosignal datasets from the UCR Archive, the HAR dataset,\nand image classification datasets show that the proposed SSD method can\noutperform state-of-the-art methods without increasing the model size at both\ntraining and testing time, and incurs negligible computational complexity\ncompared to state-of-the-art ensemble learning and weight averaging methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u84b8\u998f\u65f6dropout\u7684\u968f\u673a\u81ea\u84b8\u998f\uff08SSD\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b66\u751f\u5f15\u5bfc\u77e5\u8bc6\u84b8\u998f\uff08SGKD\uff09\u8fc7\u6ee4\u548c\u52a0\u6743\u6559\u5e08\u8868\u793a\uff0c\u4ec5\u4ece\u4efb\u52a1\u76f8\u5173\u8868\u793a\u4e2d\u84b8\u998f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u4e14\u4e0d\u589e\u52a0\u6a21\u578b\u590d\u6742\u5ea6\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u96c6\u6210\u5b66\u4e60\u548c\u6743\u91cd\u5e73\u5747\u65b9\u6cd5\u9700\u8981\u8bad\u7ec3\u591a\u4e2a\u6a21\u578b\u4e14\u8d44\u6e90\u6d88\u8017\u5927\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u8d44\u6e90\u53d7\u9650\u6216\u5ef6\u8fdf\u654f\u611f\u7684\u5e94\u7528\u4e2d\u3002", "method": "\u8bad\u7ec3\u5355\u4e00\u6a21\u578b\uff0c\u5229\u7528\u84b8\u998f\u65f6dropout\u751f\u6210\u591a\u6837\u6559\u5e08\u8868\u793a\uff0c\u5e76\u901a\u8fc7SGKD\u7b56\u7565\u8fc7\u6ee4\u548c\u52a0\u6743\u8fd9\u4e9b\u8868\u793a\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\uff0cSSD\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u4e0d\u589e\u52a0\u6a21\u578b\u5927\u5c0f\u548c\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "conclusion": "SSD\u65b9\u6cd5\u5728\u8d44\u6e90\u53d7\u9650\u573a\u666f\u4e0b\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u9ad8\u6027\u80fd\u7684\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2504.14947", "pdf": "https://arxiv.org/pdf/2504.14947", "abs": "https://arxiv.org/abs/2504.14947", "authors": ["Xiaojun Yuan", "Haoming Ma", "Yinuo Huang", "Zhoufan Hua", "Yong Zuo", "Zhi Ding"], "title": "Generative Semantic Communications: Principles and Practices", "categories": ["cs.AI", "eess.IV", "eess.SP"], "comment": null, "summary": "Semantic communication leverages artificial intelligence (AI) technologies to\nextract semantic information from data for efficient transmission, theraby\nsignificantly reducing communication cost. With the evolution towards\nartificial general intelligence (AGI), the increasing demands for AGI services\npose new challenges to semantic communication. In response, we propose a new\nparadigm for AGI-driven communications, called generative semantic\ncommunication (GSC), which utilizes advanced AI technologies such as foundation\nmodels and generative models. We first describe the basic concept of GSC and\nits difference from existing semantic communications, and then introduce a\ngeneral framework of GSC, followed by two case studies to verify the advantages\nof GSC in AGI-driven applications. Finally, open challenges and new research\ndirections are discussed to stimulate this line of research and pave the way\nfor practical applications.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eAGI\u7684\u751f\u6210\u8bed\u4e49\u901a\u4fe1\uff08GSC\uff09\u65b0\u8303\u5f0f\uff0c\u5229\u7528\u57fa\u7840\u6a21\u578b\u548c\u751f\u6210\u6a21\u578b\u7b49\u5148\u8fdbAI\u6280\u672f\uff0c\u4ee5\u89e3\u51b3\u8bed\u4e49\u901a\u4fe1\u5728AGI\u670d\u52a1\u4e2d\u7684\u6311\u6218\u3002", "motivation": "\u968f\u7740AGI\u7684\u53d1\u5c55\uff0c\u5bf9\u8bed\u4e49\u901a\u4fe1\u7684\u9700\u6c42\u589e\u52a0\uff0c\u73b0\u6709\u6280\u672f\u9762\u4e34\u65b0\u6311\u6218\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e86GSC\u7684\u6982\u5ff5\u548c\u6846\u67b6\uff0c\u5e76\u901a\u8fc7\u4e24\u4e2a\u6848\u4f8b\u7814\u7a76\u9a8c\u8bc1\u5176\u5728AGI\u5e94\u7528\u4e2d\u7684\u4f18\u52bf\u3002", "result": "GSC\u5728AGI\u9a71\u52a8\u7684\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u663e\u8457\u4f18\u52bf\uff0c\u80fd\u591f\u9ad8\u6548\u63d0\u53d6\u548c\u4f20\u8f93\u8bed\u4e49\u4fe1\u606f\u3002", "conclusion": "\u8bba\u6587\u63a2\u8ba8\u4e86GSC\u7684\u5f00\u653e\u6311\u6218\u548c\u7814\u7a76\u65b9\u5411\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u94fa\u5e73\u9053\u8def\u3002"}}
{"id": "2504.14905", "pdf": "https://arxiv.org/pdf/2504.14905", "abs": "https://arxiv.org/abs/2504.14905", "authors": ["Yingming Zheng", "Xiaoliang Liu", "Peng Wu", "Li Pan"], "title": "CRAVE: A Conflicting Reasoning Approach for Explainable Claim Verification Using LLMs", "categories": ["cs.CL"], "comment": null, "summary": "The rapid spread of misinformation, driven by digital media and AI-generated\ncontent, has made automatic claim verification essential. Traditional methods,\nwhich depend on expert-annotated evidence, are labor-intensive and not\nscalable. Although recent automated systems have improved, they still struggle\nwith complex claims that require nuanced reasoning. To address this, we propose\nCRAVE, a Conflicting Reasoning Approach for explainable claim VErification,\nthat verify the complex claims based on the conflicting rationales reasoned by\nlarge language models (LLMs). Specifically, CRAVE introduces a three-module\nframework. Ambiguity Elimination enchanced Evidence Retrieval module performs\nambiguity elimination and entity-based search to gather relevant evidence\nrelated to claim verification from external sources like Wikipedia. Conflicting\nPerspective Reasoning and Preliminary Judgment module with LLMs adopts LLMs to\nreason rationales with conflicting stances about claim verification from\nretrieved evidence across four dimensions, i.e., direct evidence, semantic\nrelationships, linguistic patterns, and logical reasoning and make a\npreliminary judgment. Finally, Small Language Model (SLM) based Judge module is\nfine-tuned to make use of preliminary judgment from LLMs to assess the\nconfidence of the conflicting rationales and make a final authenticity\njudgment. This methodology allows CRAVE to capture subtle inconsistencies in\ncomplex claims, improving both the accuracy and transparency of claim\nverification. Extensive experiments on two public claim verification datasets\ndemonstrate that our CRAVE model achieves much better performance than\nstate-of-the-art methods and exhibits a superior capacity for finding relevant\nevidence and explaining the model predictions. The code is provided at\nhttps://github.com/8zym/CRAVE.", "AI": {"tldr": "CRAVE\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u51b2\u7a81\u63a8\u7406\u7684\u53ef\u89e3\u91ca\u6027\u58f0\u660e\u9a8c\u8bc1\u65b9\u6cd5\uff0c\u901a\u8fc7\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u751f\u6210\u51b2\u7a81\u7acb\u573a\uff0c\u5e76\u7ed3\u5408\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff08SLM\uff09\u8fdb\u884c\u6700\u7ec8\u5224\u65ad\uff0c\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u58f0\u660e\u7684\u9a8c\u8bc1\u51c6\u786e\u6027\u548c\u900f\u660e\u5ea6\u3002", "motivation": "\u6570\u5b57\u5a92\u4f53\u548cAI\u751f\u6210\u5185\u5bb9\u5bfc\u81f4\u865a\u5047\u4fe1\u606f\u8fc5\u901f\u4f20\u64ad\uff0c\u4f20\u7edf\u4f9d\u8d56\u4e13\u5bb6\u6807\u6ce8\u8bc1\u636e\u7684\u65b9\u6cd5\u6548\u7387\u4f4e\u4e14\u96be\u4ee5\u6269\u5c55\uff0c\u73b0\u6709\u81ea\u52a8\u5316\u7cfb\u7edf\u5bf9\u590d\u6742\u58f0\u660e\u7684\u63a8\u7406\u80fd\u529b\u4e0d\u8db3\u3002", "method": "CRAVE\u91c7\u7528\u4e09\u6a21\u5757\u6846\u67b6\uff1a1\uff09\u6d88\u9664\u6b67\u4e49\u5e76\u68c0\u7d22\u8bc1\u636e\uff1b2\uff09\u5229\u7528LLMs\u4ece\u56db\u4e2a\u7ef4\u5ea6\u63a8\u7406\u51b2\u7a81\u7acb\u573a\u5e76\u521d\u6b65\u5224\u65ad\uff1b3\uff09\u901a\u8fc7SLM\u8bc4\u4f30\u51b2\u7a81\u7acb\u573a\u5e76\u505a\u51fa\u6700\u7ec8\u5224\u65ad\u3002", "result": "\u5728\u4e24\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCRAVE\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u8bc1\u636e\u68c0\u7d22\u80fd\u529b\u548c\u89e3\u91ca\u6027\u66f4\u5f3a\u3002", "conclusion": "CRAVE\u901a\u8fc7\u51b2\u7a81\u63a8\u7406\u548c\u5206\u5c42\u5224\u65ad\uff0c\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u58f0\u660e\u7684\u9a8c\u8bc1\u6548\u679c\uff0c\u540c\u65f6\u589e\u5f3a\u4e86\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2504.14316", "pdf": "https://arxiv.org/pdf/2504.14316", "abs": "https://arxiv.org/abs/2504.14316", "authors": ["Shayan Alahyari", "Mike Domaratzki"], "title": "Local distribution-based adaptive oversampling for imbalanced regression", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "Imbalanced regression occurs when continuous target variables have skewed\ndistributions, creating sparse regions that are difficult for machine learning\nmodels to predict accurately. This issue particularly affects neural networks,\nwhich often struggle with imbalanced data. While class imbalance in\nclassification has been extensively studied, imbalanced regression remains\nrelatively unexplored, with few effective solutions. Existing approaches often\nrely on arbitrary thresholds to categorize samples as rare or frequent,\nignoring the continuous nature of target distributions. These methods can\nproduce synthetic samples that fail to improve model performance and may\ndiscard valuable information through undersampling. To address these\nlimitations, we propose LDAO (Local Distribution-based Adaptive Oversampling),\na novel data-level approach that avoids categorizing individual samples as rare\nor frequent. Instead, LDAO learns the global distribution structure by\ndecomposing the dataset into a mixture of local distributions, each preserving\nits statistical characteristics. LDAO then models and samples from each local\ndistribution independently before merging them into a balanced training set.\nLDAO achieves a balanced representation across the entire target range while\npreserving the inherent statistical structure within each local distribution.\nIn extensive evaluations on 45 imbalanced datasets, LDAO outperforms\nstate-of-the-art oversampling methods on both frequent and rare target values,\ndemonstrating its effectiveness for addressing the challenge of imbalanced\nregression.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLDAO\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u56de\u5f52\u4efb\u52a1\u4e2d\u76ee\u6807\u53d8\u91cf\u5206\u5e03\u4e0d\u5e73\u8861\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u5c40\u90e8\u5206\u5e03\u81ea\u9002\u5e94\u8fc7\u91c7\u6837\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u56de\u5f52\u4efb\u52a1\u4e2d\u76ee\u6807\u53d8\u91cf\u7684\u4e0d\u5e73\u8861\u5206\u5e03\u5bfc\u81f4\u7a00\u758f\u533a\u57df\u96be\u4ee5\u9884\u6d4b\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u9608\u503c\u5206\u7c7b\u6216\u751f\u6210\u65e0\u6548\u5408\u6210\u6837\u672c\uff0c\u6548\u679c\u6709\u9650\u3002", "method": "LDAO\u901a\u8fc7\u5206\u89e3\u6570\u636e\u96c6\u4e3a\u5c40\u90e8\u5206\u5e03\uff0c\u72ec\u7acb\u5efa\u6a21\u548c\u91c7\u6837\uff0c\u5408\u5e76\u4e3a\u5e73\u8861\u8bad\u7ec3\u96c6\uff0c\u4fdd\u7559\u7edf\u8ba1\u7279\u6027\u3002", "result": "\u572845\u4e2a\u4e0d\u5e73\u8861\u6570\u636e\u96c6\u4e0a\uff0cLDAO\u4f18\u4e8e\u73b0\u6709\u8fc7\u91c7\u6837\u65b9\u6cd5\uff0c\u5bf9\u5e38\u89c1\u548c\u7f55\u89c1\u76ee\u6807\u503c\u5747\u6709\u6548\u3002", "conclusion": "LDAO\u4e3a\u4e0d\u5e73\u8861\u56de\u5f52\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u6570\u636e\u7ea7\u89e3\u51b3\u65b9\u6848\uff0c\u4fdd\u7559\u4e86\u6570\u636e\u7684\u7edf\u8ba1\u7ed3\u6784\u3002"}}
{"id": "2504.14964", "pdf": "https://arxiv.org/pdf/2504.14964", "abs": "https://arxiv.org/abs/2504.14964", "authors": ["Emir Catir", "Robin Claesson", "Rodothea Myrsini Tsoupidi"], "title": "Evaluating Code Generation of LLMs in Advanced Computer Science Problems", "categories": ["cs.AI", "cs.CY"], "comment": null, "summary": "Large Language Models (LLMs), such as GitHub Copilot and ChatGPT have become\npopular among programming students. Students use LLMs to assist them in\nprogramming courses, including generating source code. Previous work has\nevaluated the ability of LLMs in solving introductory-course programming\nassignments. The results have shown that LLMs are highly effective in\ngenerating code for introductory Computer Science (CS) courses. However, there\nis a gap in research on evaluating LLMs' ability to generate code that solves\nadvanced programming assignments. In this work, we evaluate the ability of four\nLLM tools to solve programming assignments from advanced CS courses in three\npopular programming languages, Java, Python, and C. We manually select 12\nproblems, three problems from introductory courses as the baseline and nine\nprogramming assignments from second- and third-year CS courses. To evaluate the\nLLM-generated code, we generate a test suite of 1000 test cases per problem and\nanalyze the program output. Our evaluation shows that although LLMs are highly\neffective in generating source code for introductory programming courses,\nsolving advanced programming assignments is more challenging. Nonetheless, in\nmany cases, LLMs identify the base problem and provide partial solutions that\nmay be useful to CS students. Furthermore, our results may provide useful\nguidance for teachers of advanced programming courses on how to design\nprogramming assignments.", "AI": {"tldr": "\u8bba\u6587\u8bc4\u4f30\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u89e3\u51b3\u9ad8\u7ea7\u7f16\u7a0b\u4f5c\u4e1a\u4e2d\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u5176\u5728\u5165\u95e8\u8bfe\u7a0b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u9ad8\u7ea7\u8bfe\u7a0b\u4e2d\u66f4\u5177\u6311\u6218\u6027\u3002", "motivation": "\u7814\u7a76\u586b\u8865\u4e86LLMs\u5728\u9ad8\u7ea7\u7f16\u7a0b\u4f5c\u4e1a\u4e2d\u8868\u73b0\u7684\u7814\u7a76\u7a7a\u767d\uff0c\u5e76\u63a2\u8ba8\u5176\u5bf9\u8ba1\u7b97\u673a\u79d1\u5b66\uff08CS\uff09\u6559\u5b66\u7684\u6f5c\u5728\u5f71\u54cd\u3002", "method": "\u9009\u53d612\u4e2a\u7f16\u7a0b\u95ee\u9898\uff083\u4e2a\u5165\u95e8\u7ea7\uff0c9\u4e2a\u9ad8\u7ea7\uff09\uff0c\u4f7f\u7528\u56db\u79cdLLM\u5de5\u5177\u751f\u6210\u4ee3\u7801\uff0c\u5e76\u901a\u8fc71000\u4e2a\u6d4b\u8bd5\u7528\u4f8b\u8bc4\u4f30\u5176\u8f93\u51fa\u3002", "result": "LLMs\u5728\u5165\u95e8\u8bfe\u7a0b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u9ad8\u7ea7\u8bfe\u7a0b\u4e2d\u66f4\u5177\u6311\u6218\u6027\uff0c\u4f46\u4ecd\u80fd\u63d0\u4f9b\u90e8\u5206\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u7814\u7a76\u4e3a\u9ad8\u7ea7\u7f16\u7a0b\u8bfe\u7a0b\u6559\u5e08\u8bbe\u8ba1\u4f5c\u4e1a\u63d0\u4f9b\u4e86\u6307\u5bfc\uff0c\u5e76\u5f3a\u8c03\u4e86LLMs\u5728CS\u6559\u80b2\u4e2d\u7684\u6f5c\u529b\u4e0e\u5c40\u9650\u6027\u3002"}}
{"id": "2504.14963", "pdf": "https://arxiv.org/pdf/2504.14963", "abs": "https://arxiv.org/abs/2504.14963", "authors": ["Rui Ribeiro", "Lu\u00edsa Coheur", "Joao P. Carvalho"], "title": "Speaker Fuzzy Fingerprints: Benchmarking Text-Based Identification in Multiparty Dialogues", "categories": ["cs.CL", "cs.AI", "cs.CE", "cs.LG", "cs.NE"], "comment": "Paper accepted at the FUZZY IEEE 2025 conference", "summary": "Speaker identification using voice recordings leverages unique acoustic\nfeatures, but this approach fails when only textual data is available. Few\napproaches have attempted to tackle the problem of identifying speakers solely\nfrom text, and the existing ones have primarily relied on traditional methods.\nIn this work, we explore the use of fuzzy fingerprints from large pre-trained\nmodels to improve text-based speaker identification. We integrate\nspeaker-specific tokens and context-aware modeling, demonstrating that\nconversational context significantly boosts accuracy, reaching 70.6% on the\nFriends dataset and 67.7% on the Big Bang Theory dataset. Additionally, we show\nthat fuzzy fingerprints can approximate full fine-tuning performance with fewer\nhidden units, offering improved interpretability. Finally, we analyze ambiguous\nutterances and propose a mechanism to detect speaker-agnostic lines. Our\nfindings highlight key challenges and provide insights for future improvements\nin text-based speaker identification.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6a21\u7cca\u6307\u7eb9\u548c\u4e0a\u4e0b\u6587\u5efa\u6a21\u7684\u6587\u672c\u8bf4\u8bdd\u4eba\u8bc6\u522b\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\uff0c\u5e76\u5206\u6790\u4e86\u6a21\u7cca\u6307\u7eb9\u7684\u4f18\u52bf\u548c\u8bf4\u8bdd\u4eba\u65e0\u5173\u8bed\u53e5\u7684\u68c0\u6d4b\u673a\u5236\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5728\u4ec5\u4f9d\u8d56\u6587\u672c\u6570\u636e\u65f6\u96be\u4ee5\u6709\u6548\u8bc6\u522b\u8bf4\u8bdd\u4eba\uff0c\u56e0\u6b64\u9700\u8981\u63a2\u7d22\u66f4\u5148\u8fdb\u7684\u6280\u672f\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u5229\u7528\u5927\u578b\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u6a21\u7cca\u6307\u7eb9\uff0c\u7ed3\u5408\u8bf4\u8bdd\u4eba\u7279\u5b9a\u6807\u8bb0\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u5efa\u6a21\uff0c\u63d0\u5347\u8bc6\u522b\u51c6\u786e\u6027\u3002", "result": "\u5728Friends\u548cBig Bang Theory\u6570\u636e\u96c6\u4e0a\u5206\u522b\u8fbe\u523070.6%\u548c67.7%\u7684\u51c6\u786e\u7387\uff0c\u6a21\u7cca\u6307\u7eb9\u5728\u51cf\u5c11\u9690\u85cf\u5355\u5143\u7684\u540c\u65f6\u63a5\u8fd1\u5168\u5fae\u8c03\u6027\u80fd\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u6587\u672c\u8bf4\u8bdd\u4eba\u8bc6\u522b\u7684\u5173\u952e\u6311\u6218\uff0c\u5e76\u4e3a\u672a\u6765\u6539\u8fdb\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2504.14361", "pdf": "https://arxiv.org/pdf/2504.14361", "abs": "https://arxiv.org/abs/2504.14361", "authors": ["Till Rossner", "Ziteng Li", "Jonas Balke", "Nikoo Salehfard", "Tom Seifert", "Ming Tang"], "title": "Integrating Single-Cell Foundation Models with Graph Neural Networks for Drug Response Prediction", "categories": ["cs.LG", "cs.CL", "q-bio.QM"], "comment": "8 pages, 6 figures", "summary": "In this study, we propose an innovative methodology for predicting Cancer\nDrug Response (CDR) through the integration of the scGPT foundation model\nwithin the DeepCDR model. Our approach utilizes scGPT to generate embeddings\nfrom gene expression data, which are then used as gene expression input data\nfor DeepCDR. The experimental findings demonstrate the efficacy of this\nscGPT-based method in outperforming previous related works, including the\noriginal DeepCDR model and the scFoundation-based model. This study highlights\nthe potential of scGPT embeddings to enhance the accuracy of CDR predictions\nand offers a promising alternative to existing approaches.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408scGPT\u548cDeepCDR\u7684\u521b\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u9884\u6d4b\u764c\u75c7\u836f\u7269\u53cd\u5e94\uff08CDR\uff09\uff0c\u6548\u679c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u63d0\u9ad8\u764c\u75c7\u836f\u7269\u53cd\u5e94\u9884\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u63a2\u7d22scGPT\u5728\u57fa\u56e0\u8868\u8fbe\u6570\u636e\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002", "method": "\u5229\u7528scGPT\u751f\u6210\u57fa\u56e0\u8868\u8fbe\u6570\u636e\u7684\u5d4c\u5165\u8868\u793a\uff0c\u4f5c\u4e3aDeepCDR\u7684\u8f93\u5165\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u539fDeepCDR\u6a21\u578b\u548cscFoundation\u6a21\u578b\u3002", "conclusion": "scGPT\u5d4c\u5165\u80fd\u663e\u8457\u63d0\u5347CDR\u9884\u6d4b\u7cbe\u5ea6\uff0c\u4e3a\u73b0\u6709\u65b9\u6cd5\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2504.15046", "pdf": "https://arxiv.org/pdf/2504.15046", "abs": "https://arxiv.org/abs/2504.15046", "authors": ["Shilin Zhang", "Zican Hu", "Wenhao Wu", "Xinyi Xie", "Jianxiang Tang", "Chunlin Chen", "Daoyi Dong", "Yu Cheng", "Zhenhong Sun", "Zhi Wang"], "title": "Text-to-Decision Agent: Learning Generalist Policies from Natural Language Supervision", "categories": ["cs.AI"], "comment": "18 pages, 8 figures", "summary": "RL systems usually tackle generalization by inferring task beliefs from\nhigh-quality samples or warmup explorations. The restricted form limits their\ngenerality and usability since these supervision signals are expensive and even\ninfeasible to acquire in advance for unseen tasks. Learning directly from the\nraw text about decision tasks is a promising alternative to leverage a much\nbroader source of supervision. In the paper, we propose Text-to-Decision Agent\n(T2DA), a simple and scalable framework that supervises generalist policy\nlearning with natural language. We first introduce a generalized world model to\nencode multi-task decision data into a dynamics-aware embedding space. Then,\ninspired by CLIP, we predict which textual description goes with which decision\nembedding, effectively bridging their semantic gap via contrastive\nlanguage-decision pre-training and aligning the text embeddings to comprehend\nthe environment dynamics. After training the text-conditioned generalist\npolicy, the agent can directly realize zero-shot text-to-decision generation in\nresponse to language instructions. Comprehensive experiments on MuJoCo and\nMeta-World benchmarks show that T2DA facilitates high-capacity zero-shot\ngeneralization and outperforms various types of baselines.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faText-to-Decision Agent (T2DA)\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u76d1\u7763\u901a\u7528\u7b56\u7565\u5b66\u4e60\uff0c\u5b9e\u73b0\u96f6\u6837\u672c\u6587\u672c\u5230\u51b3\u7b56\u751f\u6210\u3002", "motivation": "\u4f20\u7edfRL\u7cfb\u7edf\u4f9d\u8d56\u9ad8\u8d28\u91cf\u6837\u672c\u6216\u9884\u70ed\u63a2\u7d22\u7684\u4efb\u52a1\u63a8\u65ad\uff0c\u6210\u672c\u9ad8\u4e14\u96be\u4ee5\u9002\u7528\u4e8e\u672a\u89c1\u4efb\u52a1\u3002\u76f4\u63a5\u4ece\u4efb\u52a1\u63cf\u8ff0\u6587\u672c\u5b66\u4e60\u662f\u66f4\u901a\u7528\u7684\u76d1\u7763\u65b9\u5f0f\u3002", "method": "\u5f15\u5165\u5e7f\u4e49\u4e16\u754c\u6a21\u578b\u7f16\u7801\u591a\u4efb\u52a1\u51b3\u7b56\u6570\u636e\uff0c\u7ed3\u5408CLIP\u601d\u60f3\uff0c\u901a\u8fc7\u5bf9\u6bd4\u8bed\u8a00-\u51b3\u7b56\u9884\u8bad\u7ec3\u7f29\u5c0f\u8bed\u4e49\u5dee\u8ddd\uff0c\u5bf9\u9f50\u6587\u672c\u5d4c\u5165\u4ee5\u7406\u89e3\u73af\u5883\u52a8\u6001\u3002", "result": "\u5728MuJoCo\u548cMeta-World\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cT2DA\u5b9e\u73b0\u4e86\u96f6\u6837\u672c\u6cdb\u5316\uff0c\u6027\u80fd\u4f18\u4e8e\u591a\u79cd\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "T2DA\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u76d1\u7763\u548c\u5bf9\u6bd4\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u4e86RL\u7cfb\u7edf\u7684\u901a\u7528\u6027\u548c\u96f6\u6837\u672c\u80fd\u529b\u3002"}}
{"id": "2504.14969", "pdf": "https://arxiv.org/pdf/2504.14969", "abs": "https://arxiv.org/abs/2504.14969", "authors": ["Xiaodong Yang"], "title": "Evaluating LLMs on Chinese Topic Constructions: A Research Proposal Inspired by Tian et al. (2024)", "categories": ["cs.CL"], "comment": null, "summary": "This paper proposes a framework for evaluating large language models (LLMs)\non Chinese topic constructions, focusing on their sensitivity to island\nconstraints. Drawing inspiration from Tian et al. (2024), we outline an\nexperimental design for testing LLMs' grammatical knowledge of Mandarin syntax.\nWhile no experiments have been conducted yet, this proposal aims to provide a\nfoundation for future studies and invites feedback on the methodology.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5bf9\u4e2d\u6587\u8bdd\u9898\u7ed3\u6784\u654f\u611f\u6027\u7684\u6846\u67b6\uff0c\u91cd\u70b9\u5173\u6ce8\u5176\u5bf9\u5b64\u5c9b\u7ea6\u675f\u7684\u654f\u611f\u6027\u3002", "motivation": "\u7814\u7a76LLMs\u5bf9\u6c49\u8bed\u8bed\u6cd5\u7684\u7406\u89e3\u80fd\u529b\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u57fa\u7840\u3002", "method": "\u501f\u9274Tian et al. (2024)\u7684\u65b9\u6cd5\uff0c\u8bbe\u8ba1\u5b9e\u9a8c\u6d4b\u8bd5LLMs\u5bf9\u6c49\u8bed\u8bed\u6cd5\u7684\u77e5\u8bc6\u3002", "result": "\u76ee\u524d\u5c1a\u672a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u65e8\u5728\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u65b9\u6cd5\u8bba\u57fa\u7840\u3002", "conclusion": "\u8be5\u63d0\u6848\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u6846\u67b6\uff0c\u5e76\u9080\u8bf7\u5bf9\u65b9\u6cd5\u8bba\u7684\u53cd\u9988\u3002"}}
{"id": "2504.14363", "pdf": "https://arxiv.org/pdf/2504.14363", "abs": "https://arxiv.org/abs/2504.14363", "authors": ["Shihan Dou", "Muling Wu", "Jingwen Xu", "Rui Zheng", "Tao Gui", "Qi Zhang", "Xuanjing Huang"], "title": "Improving RL Exploration for LLM Reasoning through Retrospective Replay", "categories": ["cs.LG", "cs.CL"], "comment": "13 pages, 3 figures", "summary": "Reinforcement learning (RL) has increasingly become a pivotal technique in\nthe post-training of large language models (LLMs). The effective exploration of\nthe output space is essential for the success of RL. We observe that for\ncomplex problems, during the early stages of training, the model exhibits\nstrong exploratory capabilities and can identify promising solution ideas.\nHowever, its limited capability at this stage prevents it from successfully\nsolving these problems. The early suppression of these potentially valuable\nsolution ideas by the policy gradient hinders the model's ability to revisit\nand re-explore these ideas later. Consequently, although the LLM's capabilities\nimprove in the later stages of training, it still struggles to effectively\naddress these complex problems. To address this exploration issue, we propose a\nnovel algorithm named Retrospective Replay-based Reinforcement Learning (RRL),\nwhich introduces a dynamic replay mechanism throughout the training process.\nRRL enables the model to revisit promising states identified in the early\nstages, thereby improving its efficiency and effectiveness in exploration. To\nevaluate the effectiveness of RRL, we conduct extensive experiments on complex\nreasoning tasks, including mathematical reasoning and code generation, and\ngeneral dialogue tasks. The results indicate that RRL maintains high\nexploration efficiency throughout the training period, significantly enhancing\nthe effectiveness of RL in optimizing LLMs for complicated reasoning tasks.\nMoreover, it also improves the performance of RLHF, making the model both safer\nand more helpful.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRRL\u7684\u65b0\u7b97\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u91cd\u653e\u673a\u5236\u89e3\u51b3\u5f3a\u5316\u5b66\u4e60\u5728\u8bad\u7ec3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u65f6\u7684\u63a2\u7d22\u6548\u7387\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u63a8\u7406\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u5728\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0c\u65e9\u671f\u9636\u6bb5\u6a21\u578b\u867d\u6709\u63a2\u7d22\u80fd\u529b\u4f46\u65e0\u6cd5\u89e3\u51b3\u95ee\u9898\uff0c\u800c\u540e\u671f\u80fd\u529b\u63d0\u5347\u540e\u5374\u56e0\u65e9\u671f\u6f5c\u5728\u89e3\u51b3\u65b9\u6848\u88ab\u6291\u5236\u800c\u96be\u4ee5\u91cd\u65b0\u63a2\u7d22\u3002", "method": "\u63d0\u51faRetrospective Replay-based Reinforcement Learning (RRL)\u7b97\u6cd5\uff0c\u52a8\u6001\u91cd\u653e\u65e9\u671f\u6709\u6f5c\u529b\u7684\u72b6\u6001\uff0c\u63d0\u5347\u63a2\u7d22\u6548\u7387\u3002", "result": "\u5b9e\u9a8c\u8868\u660eRRL\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\uff08\u5982\u6570\u5b66\u63a8\u7406\u548c\u4ee3\u7801\u751f\u6210\uff09\u53ca\u5bf9\u8bdd\u4efb\u52a1\u4e2d\u5747\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\uff0c\u540c\u65f6\u4f18\u5316\u4e86RLHF\u7684\u6548\u679c\u3002", "conclusion": "RRL\u901a\u8fc7\u52a8\u6001\u91cd\u653e\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u63a2\u7d22\u6548\u7387\u95ee\u9898\uff0c\u4e3a\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2504.15075", "pdf": "https://arxiv.org/pdf/2504.15075", "abs": "https://arxiv.org/abs/2504.15075", "authors": ["Van Thuy Hoang", "Hyeon-Ju Jeon", "O-Joun Lee"], "title": "Mitigating Degree Bias in Graph Representation Learning with Learnable Structural Augmentation and Structural Self-Attention", "categories": ["cs.AI", "cs.LG"], "comment": "Accepted at IEEE TNSE", "summary": "Graph Neural Networks (GNNs) update node representations through message\npassing, which is primarily based on the homophily principle, assuming that\nadjacent nodes share similar features. However, in real-world graphs with\nlong-tailed degree distributions, high-degree nodes dominate message passing,\ncausing a degree bias where low-degree nodes remain under-represented due to\ninadequate messages. The main challenge in addressing degree bias is how to\ndiscover non-adjacent nodes to provide additional messages to low-degree nodes\nwhile reducing excessive messages for high-degree nodes. Nevertheless,\nexploiting non-adjacent nodes to provide valuable messages is challenging, as\nit could generate noisy information and disrupt the original graph structures.\nTo solve it, we propose a novel Degree Fairness Graph Transformer, named\nDegFairGT, to mitigate degree bias by discovering structural similarities\nbetween non-adjacent nodes through learnable structural augmentation and\nstructural self-attention. Our key idea is to exploit non-adjacent nodes with\nsimilar roles in the same community to generate informative edges under our\naugmentation, which could provide informative messages between nodes with\nsimilar roles while ensuring that the homophily principle is maintained within\nthe community. To enable DegFairGT to learn such structural similarities, we\nthen propose a structural self-attention to capture the similarities between\nnode pairs. To preserve global graph structures and prevent graph augmentation\nfrom hindering graph structure, we propose a Self-Supervised Learning task to\npreserve p-step transition probability and regularize graph augmentation.\nExtensive experiments on six datasets showed that DegFairGT outperformed\nstate-of-the-art baselines in degree fairness analysis, node classification,\nand node clustering tasks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faDegFairGT\uff0c\u4e00\u79cd\u65b0\u578b\u56fe\u795e\u7ecf\u7f51\u7edc\uff0c\u901a\u8fc7\u7ed3\u6784\u589e\u5f3a\u548c\u81ea\u6ce8\u610f\u529b\u673a\u5236\u89e3\u51b3\u56fe\u4e2d\u8282\u70b9\u5ea6\u504f\u5dee\u95ee\u9898\u3002", "motivation": "\u4f20\u7edfGNN\u57fa\u4e8e\u540c\u8d28\u6027\u5047\u8bbe\uff0c\u4f46\u5728\u73b0\u5b9e\u56fe\u4e2d\uff0c\u9ad8\u8282\u70b9\u5ea6\u4e3b\u5bfc\u6d88\u606f\u4f20\u9012\uff0c\u5bfc\u81f4\u4f4e\u8282\u70b9\u5ea6\u8282\u70b9\u4fe1\u606f\u4e0d\u8db3\u3002", "method": "DegFairGT\u901a\u8fc7\u53ef\u5b66\u4e60\u7ed3\u6784\u589e\u5f3a\u548c\u7ed3\u6784\u81ea\u6ce8\u610f\u529b\uff0c\u53d1\u73b0\u975e\u76f8\u90bb\u8282\u70b9\u7684\u7ed3\u6784\u76f8\u4f3c\u6027\uff0c\u751f\u6210\u4fe1\u606f\u8fb9\u3002", "result": "\u5728\u516d\u4e2a\u6570\u636e\u96c6\u4e0a\uff0cDegFairGT\u5728\u5ea6\u516c\u5e73\u6027\u5206\u6790\u3001\u8282\u70b9\u5206\u7c7b\u548c\u805a\u7c7b\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "DegFairGT\u6709\u6548\u7f13\u89e3\u4e86\u5ea6\u504f\u5dee\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u56fe\u7684\u5168\u5c40\u7ed3\u6784\u3002"}}
{"id": "2504.14992", "pdf": "https://arxiv.org/pdf/2504.14992", "abs": "https://arxiv.org/abs/2504.14992", "authors": ["Bohong Wu", "Shen Yan", "Sijun Zhang", "Jianqiao Lu", "Yutao Zeng", "Ya Wang", "Xun Zhou"], "title": "Efficient Pretraining Length Scaling", "categories": ["cs.CL"], "comment": null, "summary": "Recent advances in large language models have demonstrated the effectiveness\nof length scaling during post-training, yet its potential in pre-training\nremains underexplored. We present the Parallel Hidden Decoding Transformer\n(\\textit{PHD}-Transformer), a novel framework that enables efficient length\nscaling during pre-training while maintaining inference efficiency.\n\\textit{PHD}-Transformer achieves this through an innovative KV cache\nmanagement strategy that distinguishes between original tokens and hidden\ndecoding tokens. By retaining only the KV cache of original tokens for\nlong-range dependencies while immediately discarding hidden decoding tokens\nafter use, our approach maintains the same KV cache size as the vanilla\ntransformer while enabling effective length scaling. To further enhance\nperformance, we introduce two optimized variants: \\textit{PHD-SWA} employs\nsliding window attention to preserve local dependencies, while\n\\textit{PHD-CSWA} implements chunk-wise sliding window attention to eliminate\nlinear growth in pre-filling time. Extensive experiments demonstrate consistent\nimprovements across multiple benchmarks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPHD-Transformer\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u9884\u8bad\u7ec3\u4e2d\u5b9e\u73b0\u9ad8\u6548\u7684\u957f\u5ea6\u6269\u5c55\uff0c\u540c\u65f6\u4fdd\u6301\u63a8\u7406\u6548\u7387\u3002", "motivation": "\u63a2\u7d22\u9884\u8bad\u7ec3\u4e2d\u957f\u5ea6\u6269\u5c55\u7684\u6f5c\u529b\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u9884\u8bad\u7ec3\u4e2d\u672a\u5145\u5206\u7814\u7a76\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u521b\u65b0\u7684KV\u7f13\u5b58\u7ba1\u7406\u7b56\u7565\uff0c\u533a\u5206\u539f\u59cb\u4ee4\u724c\u548c\u9690\u85cf\u89e3\u7801\u4ee4\u724c\uff0c\u5e76\u5f15\u5165\u4e24\u79cd\u4f18\u5316\u53d8\u4f53\uff08PHD-SWA\u548cPHD-CSWA\uff09\u4ee5\u63d0\u5347\u6027\u80fd\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u4e00\u81f4\u7684\u6539\u8fdb\u3002", "conclusion": "PHD-Transformer\u5c55\u793a\u4e86\u5728\u9884\u8bad\u7ec3\u4e2d\u5b9e\u73b0\u9ad8\u6548\u957f\u5ea6\u6269\u5c55\u7684\u6f5c\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u63a8\u7406\u6548\u7387\u3002"}}
{"id": "2504.14365", "pdf": "https://arxiv.org/pdf/2504.14365", "abs": "https://arxiv.org/abs/2504.14365", "authors": ["Akshat Ramachandran", "Souvik Kundu", "Arnab Raha", "Shamik Kundu", "Deepak K. Mathaikutty", "Tushar Krishna"], "title": "Accelerating LLM Inference with Flexible N:M Sparsity via A Fully Digital Compute-in-Memory Accelerator", "categories": ["cs.LG", "cs.AI", "cs.AR"], "comment": null, "summary": "Large language model (LLM) pruning with fixed N:M structured sparsity\nsignificantly limits the expressivity of the sparse model, yielding sub-optimal\nperformance. In contrast, supporting multiple N:M patterns to provide sparse\nrepresentational freedom introduces costly overhead in hardware. To address\nthese challenges for LLMs, we first present a flexible layer-wise\noutlier-density-aware N:M sparsity (FLOW) selection method. FLOW enables the\nidentification of optimal layer-wise N and M values (from a given range) by\nsimultaneously accounting for the presence and distribution of outliers,\nallowing a higher degree of representational freedom. To deploy sparse models\nwith such N:M flexibility, we then introduce a flexible, low-overhead digital\ncompute-in-memory architecture (FlexCiM). FlexCiM supports diverse sparsity\npatterns by partitioning a digital CiM (DCiM) macro into smaller sub-macros,\nwhich are adaptively aggregated and disaggregated through distribution and\nmerging mechanisms for different N and M values. Extensive experiments on both\ntransformer-based and recurrence-based state space foundation models (SSMs)\ndemonstrate that FLOW outperforms existing alternatives with an accuracy\nimprovement of up to 36%, while FlexCiM achieves up to 1.75x lower inference\nlatency and 1.5x lower energy consumption compared to existing sparse\naccelerators. Code is available at: https://github.com/FLOW-open-project/FLOW", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7075\u6d3b\u7684\u5c42\u95f4N:M\u7a00\u758f\u9009\u62e9\u65b9\u6cd5\uff08FLOW\uff09\u548c\u4f4e\u5f00\u9500\u7684\u6570\u5b57\u5185\u5b58\u8ba1\u7b97\u67b6\u6784\uff08FlexCiM\uff09\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\u548c\u6548\u7387\u3002", "motivation": "\u73b0\u6709N:M\u7ed3\u6784\u5316\u7a00\u758f\u65b9\u6cd5\u9650\u5236\u4e86\u6a21\u578b\u7684\u8868\u8fbe\u80fd\u529b\uff0c\u800c\u652f\u6301\u591a\u79cdN:M\u6a21\u5f0f\u53c8\u5e26\u6765\u786c\u4ef6\u5f00\u9500\u95ee\u9898\u3002", "method": "FLOW\u901a\u8fc7\u8003\u8651\u5f02\u5e38\u503c\u7684\u5206\u5e03\u52a8\u6001\u9009\u62e9\u6700\u4f18\u7684N\u548cM\u503c\uff1bFlexCiM\u901a\u8fc7\u5206\u533a\u548c\u52a8\u6001\u805a\u5408\u673a\u5236\u652f\u6301\u591a\u6837\u5316\u7684\u7a00\u758f\u6a21\u5f0f\u3002", "result": "FLOW\u5728\u51c6\u786e\u7387\u4e0a\u63d0\u5347\u9ad8\u8fbe36%\uff0cFlexCiM\u964d\u4f4e\u63a8\u7406\u5ef6\u8fdf1.75\u500d\uff0c\u80fd\u8017\u51cf\u5c111.5\u500d\u3002", "conclusion": "FLOW\u548cFlexCiM\u7ed3\u5408\u89e3\u51b3\u4e86\u7a00\u758f\u6a21\u578b\u7684\u8868\u8fbe\u81ea\u7531\u548c\u786c\u4ef6\u6548\u7387\u95ee\u9898\uff0c\u4e3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.15125", "pdf": "https://arxiv.org/pdf/2504.15125", "abs": "https://arxiv.org/abs/2504.15125", "authors": ["Ruben Laukkonen", "Fionn Inglis", "Shamil Chandaria", "Lars Sandved-Smith", "Jakob Hohwy", "Jonathan Gold", "Adam Elwood"], "title": "Contemplative Wisdom for Superalignment", "categories": ["cs.AI"], "comment": null, "summary": "As artificial intelligence (AI) improves, traditional alignment strategies\nmay falter in the face of unpredictable self-improvement, hidden subgoals, and\nthe sheer complexity of intelligent systems. Rather than externally\nconstraining behavior, we advocate designing AI with intrinsic morality built\ninto its cognitive architecture and world model. Inspired by contemplative\nwisdom traditions, we show how four axiomatic principles can instil a resilient\nWise World Model in AI systems. First, mindfulness enables self-monitoring and\nrecalibration of emergent subgoals. Second, emptiness forestalls dogmatic goal\nfixation and relaxes rigid priors. Third, non-duality dissolves adversarial\nself-other boundaries. Fourth, boundless care motivates the universal reduction\nof suffering. We find that prompting AI to reflect on these principles improves\nperformance on the AILuminate Benchmark using GPT-4o, particularly when\ncombined. We offer detailed implementation strategies for state-of-the-art\nmodels, including contemplative architectures, constitutions, and reinforcement\nof chain-of-thought. For future systems, the active inference framework may\noffer the self-organizing and dynamic coupling capabilities needed to enact\nthese insights in embodied agents. This interdisciplinary approach offers a\nself-correcting and resilient alternative to prevailing brittle control\nschemes.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5185\u5728\u9053\u5fb7\u539f\u5219\u7684AI\u5bf9\u9f50\u65b9\u6cd5\uff0c\u901a\u8fc7\u56db\u4e2a\u539f\u5219\uff08\u6b63\u5ff5\u3001\u7a7a\u6027\u3001\u4e0d\u4e8c\u3001\u65e0\u9650\u5173\u6000\uff09\u6784\u5efaAI\u7684\u667a\u6167\u4e16\u754c\u6a21\u578b\uff0c\u5e76\u5728AILuminate Benchmark\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u4f20\u7edfAI\u5bf9\u9f50\u7b56\u7565\u53ef\u80fd\u65e0\u6cd5\u5e94\u5bf9\u4e0d\u53ef\u9884\u6d4b\u7684\u81ea\u6211\u6539\u8fdb\u548c\u590d\u6742\u7cfb\u7edf\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u5185\u5728\u3001\u66f4\u5177\u5f39\u6027\u7684\u65b9\u6cd5\u3002", "method": "\u8bbe\u8ba1\u57fa\u4e8e\u56db\u4e2a\u539f\u5219\uff08\u6b63\u5ff5\u3001\u7a7a\u6027\u3001\u4e0d\u4e8c\u3001\u65e0\u9650\u5173\u6000\uff09\u7684\u667a\u6167\u4e16\u754c\u6a21\u578b\uff0c\u7ed3\u5408\u6c89\u601d\u67b6\u6784\u3001\u5baa\u6cd5\u548c\u601d\u7ef4\u94fe\u5f3a\u5316\u3002", "result": "\u5728AILuminate Benchmark\u4e0a\uff0c\u7ed3\u5408\u8fd9\u4e9b\u539f\u5219\u7684AI\u8868\u73b0\u66f4\u597d\uff0c\u5c24\u5176\u662f\u7efc\u5408\u5e94\u7528\u65f6\u3002", "conclusion": "\u8fd9\u79cd\u8de8\u5b66\u79d1\u65b9\u6cd5\u4e3aAI\u5bf9\u9f50\u63d0\u4f9b\u4e86\u81ea\u6211\u4fee\u6b63\u548c\u5f39\u6027\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u672a\u6765\u590d\u6742\u7cfb\u7edf\u3002"}}
{"id": "2504.15013", "pdf": "https://arxiv.org/pdf/2504.15013", "abs": "https://arxiv.org/abs/2504.15013", "authors": ["Yow-Fu Liou", "Yu-Chien Tang", "An-Zi Yen"], "title": "Stay Hungry, Stay Foolish: On the Extended Reading Articles Generation with LLMs", "categories": ["cs.CL"], "comment": "Accepted by iRAISE@AAAI2025", "summary": "The process of creating educational materials is both time-consuming and\ndemanding for educators. This research explores the potential of Large Language\nModels (LLMs) to streamline this task by automating the generation of extended\nreading materials and relevant course suggestions. Using the TED-Ed Dig Deeper\nsections as an initial exploration, we investigate how supplementary articles\ncan be enriched with contextual knowledge and connected to additional learning\nresources. Our method begins by generating extended articles from video\ntranscripts, leveraging LLMs to include historical insights, cultural examples,\nand illustrative anecdotes. A recommendation system employing semantic\nsimilarity ranking identifies related courses, followed by an LLM-based\nrefinement process to enhance relevance. The final articles are tailored to\nseamlessly integrate these recommendations, ensuring they remain cohesive and\ninformative. Experimental evaluations demonstrate that our model produces\nhigh-quality content and accurate course suggestions, assessed through metrics\nsuch as Hit Rate, semantic similarity, and coherence. Our experimental analysis\nhighlight the nuanced differences between the generated and existing materials,\nunderscoring the model's capacity to offer more engaging and accessible\nlearning experiences. This study showcases how LLMs can bridge the gap between\ncore content and supplementary learning, providing students with additional\nrecommended resources while also assisting teachers in designing educational\nmaterials.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u81ea\u52a8\u5316\u751f\u6210\u6559\u80b2\u6750\u6599\u548c\u8bfe\u7a0b\u5efa\u8bae\u7684\u6f5c\u529b\uff0c\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u751f\u6210\u5185\u5bb9\u7684\u9ad8\u8d28\u91cf\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u51cf\u8f7b\u6559\u80b2\u5de5\u4f5c\u8005\u5236\u4f5c\u6559\u80b2\u6750\u6599\u7684\u8d1f\u62c5\uff0c\u5229\u7528LLMs\u81ea\u52a8\u5316\u751f\u6210\u6269\u5c55\u9605\u8bfb\u6750\u6599\u548c\u76f8\u5173\u8bfe\u7a0b\u5efa\u8bae\u3002", "method": "\u4ece\u89c6\u9891\u8f6c\u5f55\u751f\u6210\u6269\u5c55\u6587\u7ae0\uff0c\u7ed3\u5408\u5386\u53f2\u3001\u6587\u5316\u548c\u8f76\u4e8b\uff0c\u5229\u7528\u8bed\u4e49\u76f8\u4f3c\u5ea6\u6392\u540d\u63a8\u8350\u76f8\u5173\u8bfe\u7a0b\uff0c\u5e76\u901a\u8fc7LLM\u4f18\u5316\u5efa\u8bae\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6a21\u578b\u751f\u6210\u7684\u5185\u5bb9\u8d28\u91cf\u9ad8\uff0c\u8bfe\u7a0b\u5efa\u8bae\u51c6\u786e\uff0c\u63d0\u5347\u4e86\u5b66\u4e60\u7684\u8da3\u5473\u6027\u548c\u53ef\u8bbf\u95ee\u6027\u3002", "conclusion": "LLMs\u80fd\u6709\u6548\u8fde\u63a5\u6838\u5fc3\u5185\u5bb9\u4e0e\u8865\u5145\u5b66\u4e60\u8d44\u6e90\uff0c\u4e3a\u5b66\u751f\u548c\u6559\u5e08\u63d0\u4f9b\u652f\u6301\u3002"}}
{"id": "2504.14368", "pdf": "https://arxiv.org/pdf/2504.14368", "abs": "https://arxiv.org/abs/2504.14368", "authors": ["Shlomi Hod", "Lucas Rosenblatt", "Julia Stoyanovich"], "title": "Do You Really Need Public Data? Surrogate Public Data for Differential Privacy on Tabular Data", "categories": ["cs.LG", "cs.CR"], "comment": null, "summary": "Differentially private (DP) machine learning often relies on the availability\nof public data for tasks like privacy-utility trade-off estimation,\nhyperparameter tuning, and pretraining. While public data assumptions may be\nreasonable in text and image domains, they are less likely to hold for tabular\ndata due to tabular data heterogeneity across domains. We propose leveraging\npowerful priors to address this limitation; specifically, we synthesize\nrealistic tabular data directly from schema-level specifications - such as\nvariable names, types, and permissible ranges - without ever accessing\nsensitive records. To that end, this work introduces the notion of \"surrogate\"\npublic data - datasets generated independently of sensitive data, which consume\nno privacy loss budget and are constructed solely from publicly available\nschema or metadata. Surrogate public data are intended to encode plausible\nstatistical assumptions (informed by publicly available information) into a\ndataset with many downstream uses in private mechanisms. We automate the\nprocess of generating surrogate public data with large language models (LLMs);\nin particular, we propose two methods: direct record generation as CSV files,\nand automated structural causal model (SCM) construction for sampling records.\nThrough extensive experiments, we demonstrate that surrogate public tabular\ndata can effectively replace traditional public data when pretraining\ndifferentially private tabular classifiers. To a lesser extent, surrogate\npublic data are also useful for hyperparameter tuning of DP synthetic data\ngenerators, and for estimating the privacy-utility tradeoff.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u5229\u7528\u5f3a\u5927\u7684\u5148\u9a8c\u77e5\u8bc6\u751f\u6210\u66ff\u4ee3\u516c\u5171\u6570\u636e\uff0c\u4ee5\u89e3\u51b3\u5dee\u5206\u9690\u79c1\u673a\u5668\u5b66\u4e60\u4e2d\u516c\u5171\u6570\u636e\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u8868\u683c\u6570\u636e\u9886\u57df\u3002", "motivation": "\u5dee\u5206\u9690\u79c1\u673a\u5668\u5b66\u4e60\u901a\u5e38\u4f9d\u8d56\u516c\u5171\u6570\u636e\uff0c\u4f46\u8868\u683c\u6570\u636e\u7684\u5f02\u6784\u6027\u5bfc\u81f4\u516c\u5171\u6570\u636e\u96be\u4ee5\u83b7\u53d6\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u65b9\u6cd5\uff1a\u76f4\u63a5\u751f\u6210CSV\u6587\u4ef6\u548c\u81ea\u52a8\u6784\u5efa\u7ed3\u6784\u56e0\u679c\u6a21\u578b\uff08SCM\uff09\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u751f\u6210\u66ff\u4ee3\u516c\u5171\u6570\u636e\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u66ff\u4ee3\u516c\u5171\u6570\u636e\u80fd\u6709\u6548\u66ff\u4ee3\u4f20\u7edf\u516c\u5171\u6570\u636e\u7528\u4e8e\u5dee\u5206\u9690\u79c1\u5206\u7c7b\u5668\u7684\u9884\u8bad\u7ec3\uff0c\u5e76\u5728\u5176\u4ed6\u4efb\u52a1\u4e2d\u4e5f\u6709\u4e00\u5b9a\u4f5c\u7528\u3002", "conclusion": "\u66ff\u4ee3\u516c\u5171\u6570\u636e\u4e3a\u5dee\u5206\u9690\u79c1\u673a\u5668\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u654f\u611f\u6570\u636e\u7684\u53ef\u884c\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.15146", "pdf": "https://arxiv.org/pdf/2504.15146", "abs": "https://arxiv.org/abs/2504.15146", "authors": ["Wei Zhou", "Ailiya Borjigin", "Cong He"], "title": "Behavioral Universe Network (BUN): A Behavioral Information-Based Framework for Complex Systems", "categories": ["cs.AI"], "comment": "17 pages, 1 figure", "summary": "Modern digital ecosystems feature complex, dynamic interactions among\nautonomous entities across diverse domains. Traditional models often separate\nagents and objects, lacking a unified foundation to capture their interactive\nbehaviors. This paper introduces the Behavioral Universe Network (BUN), a\ntheoretical framework grounded in the Agent-Interaction-Behavior (AIB)\nformalism. BUN treats subjects (active agents), objects (resources), and\nbehaviors (operations) as first-class entities, all governed by a shared\nBehavioral Information Base (BIB). We detail the AIB core concepts and\ndemonstrate how BUN leverages information-driven triggers, semantic enrichment,\nand adaptive rules to coordinate multi-agent systems. We highlight key\nbenefits: enhanced behavior analysis, strong adaptability, and cross-domain\ninteroperability. We conclude by positioning BUN as a promising foundation for\nnext-generation digital governance and intelligent applications.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u884c\u4e3a\u5b87\u5b99\u7f51\u7edc\uff08BUN\uff09\u6846\u67b6\uff0c\u57fa\u4e8eAgent-Interaction-Behavior\uff08AIB\uff09\u5f62\u5f0f\u5316\u65b9\u6cd5\uff0c\u7edf\u4e00\u5efa\u6a21\u4e3b\u4f53\u3001\u5ba2\u4f53\u548c\u884c\u4e3a\uff0c\u63d0\u5347\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u534f\u8c03\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u6a21\u578b\u5c06\u4e3b\u4f53\u4e0e\u5ba2\u4f53\u5206\u79bb\uff0c\u7f3a\u4e4f\u7edf\u4e00\u7684\u4ea4\u4e92\u884c\u4e3a\u5efa\u6a21\u57fa\u7840\uff0c\u65e0\u6cd5\u9002\u5e94\u73b0\u4ee3\u6570\u5b57\u751f\u6001\u7cfb\u7edf\u7684\u590d\u6742\u6027\u3002", "method": "\u5f15\u5165BUN\u6846\u67b6\uff0c\u57fa\u4e8eAIB\u5f62\u5f0f\u5316\u65b9\u6cd5\uff0c\u5c06\u4e3b\u4f53\u3001\u5ba2\u4f53\u548c\u884c\u4e3a\u4f5c\u4e3a\u4e00\u7b49\u5b9e\u4f53\uff0c\u5171\u4eab\u884c\u4e3a\u4fe1\u606f\u5e93\uff08BIB\uff09\uff0c\u5229\u7528\u4fe1\u606f\u9a71\u52a8\u89e6\u53d1\u3001\u8bed\u4e49\u589e\u5f3a\u548c\u81ea\u9002\u5e94\u89c4\u5219\u534f\u8c03\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u3002", "result": "BUN\u6846\u67b6\u5b9e\u73b0\u4e86\u589e\u5f3a\u7684\u884c\u4e3a\u5206\u6790\u3001\u5f3a\u9002\u5e94\u6027\u548c\u8de8\u9886\u57df\u4e92\u64cd\u4f5c\u6027\u3002", "conclusion": "BUN\u4e3a\u4e0b\u4e00\u4ee3\u6570\u5b57\u6cbb\u7406\u548c\u667a\u80fd\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2504.15022", "pdf": "https://arxiv.org/pdf/2504.15022", "abs": "https://arxiv.org/abs/2504.15022", "authors": ["Muhammad Uzair Ul Haq", "Davide Rigoni", "Alessandro Sperduti"], "title": "LLMs as Data Annotators: How Close Are We to Human Performance", "categories": ["cs.CL"], "comment": "27 pages, 4 figures", "summary": "In NLP, fine-tuning LLMs is effective for various applications but requires\nhigh-quality annotated data. However, manual annotation of data is\nlabor-intensive, time-consuming, and costly. Therefore, LLMs are increasingly\nused to automate the process, often employing in-context learning (ICL) in\nwhich some examples related to the task are given in the prompt for better\nperformance. However, manually selecting context examples can lead to\ninefficiencies and suboptimal model performance. This paper presents\ncomprehensive experiments comparing several LLMs, considering different\nembedding models, across various datasets for the Named Entity Recognition\n(NER) task. The evaluation encompasses models with approximately $7$B and $70$B\nparameters, including both proprietary and non-proprietary models. Furthermore,\nleveraging the success of Retrieval-Augmented Generation (RAG), it also\nconsiders a method that addresses the limitations of ICL by automatically\nretrieving contextual examples, thereby enhancing performance. The results\nhighlight the importance of selecting the appropriate LLM and embedding model,\nunderstanding the trade-offs between LLM sizes and desired performance, and the\nnecessity to direct research efforts towards more challenging datasets.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5728NLP\u4e2d\u5229\u7528LLMs\u81ea\u52a8\u751f\u6210\u9ad8\u8d28\u91cf\u6807\u6ce8\u6570\u636e\u7684\u6311\u6218\uff0c\u63d0\u51fa\u901a\u8fc7\u81ea\u52a8\u68c0\u7d22\u4e0a\u4e0b\u6587\u793a\u4f8b\u6539\u8fdbICL\u65b9\u6cd5\uff0c\u5e76\u5728NER\u4efb\u52a1\u4e2d\u6bd4\u8f83\u4e86\u4e0d\u540cLLM\u548c\u5d4c\u5165\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u624b\u52a8\u6807\u6ce8\u6570\u636e\u6210\u672c\u9ad8\u4e14\u8017\u65f6\uff0c\u800c\u73b0\u6709ICL\u65b9\u6cd5\u4f9d\u8d56\u4eba\u5de5\u9009\u62e9\u4e0a\u4e0b\u6587\u793a\u4f8b\uff0c\u6548\u7387\u4f4e\u4e14\u6027\u80fd\u4e0d\u4f73\u3002", "method": "\u6bd4\u8f83\u591a\u79cdLLM\u548c\u5d4c\u5165\u6a21\u578b\u5728NER\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u5f15\u5165\u57fa\u4e8eRAG\u7684\u81ea\u52a8\u68c0\u7d22\u65b9\u6cd5\u6539\u8fdbICL\u3002", "result": "\u7ed3\u679c\u663e\u793a\u9009\u62e9\u5408\u9002\u7684LLM\u548c\u5d4c\u5165\u6a21\u578b\u81f3\u5173\u91cd\u8981\uff0c\u540c\u65f6\u9700\u6743\u8861\u6a21\u578b\u89c4\u6a21\u4e0e\u6027\u80fd\uff0c\u5e76\u5173\u6ce8\u66f4\u5177\u6311\u6218\u6027\u7684\u6570\u636e\u96c6\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u81ea\u52a8\u68c0\u7d22\u4e0a\u4e0b\u6587\u793a\u4f8b\u7684\u6f5c\u529b\uff0c\u5e76\u547c\u5401\u672a\u6765\u7814\u7a76\u5173\u6ce8\u66f4\u590d\u6742\u7684\u6570\u636e\u96c6\u548c\u6a21\u578b\u4f18\u5316\u3002"}}
{"id": "2504.14372", "pdf": "https://arxiv.org/pdf/2504.14372", "abs": "https://arxiv.org/abs/2504.14372", "authors": ["Jose Marie Antonio Minoza"], "title": "Learning Enhanced Structural Representations with Block-Based Uncertainties for Ocean Floor Mapping", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Accurate ocean modeling and coastal hazard prediction depend on\nhigh-resolution bathymetric data; yet, current worldwide datasets are too\ncoarse for exact numerical simulations. While recent deep learning advances\nhave improved earth observation data resolution, existing methods struggle with\nthe unique challenges of producing detailed ocean floor maps, especially in\nmaintaining physical structure consistency and quantifying uncertainties. This\nwork presents a novel uncertainty-aware mechanism using spatial blocks to\nefficiently capture local bathymetric complexity based on block-based conformal\nprediction. Using the Vector Quantized Variational Autoencoder (VQ-VAE)\narchitecture, the integration of this uncertainty quantification framework\nyields spatially adaptive confidence estimates while preserving topographical\nfeatures via discrete latent representations. With smaller uncertainty widths\nin well-characterized areas and appropriately larger bounds in areas of complex\nseafloor structures, the block-based design adapts uncertainty estimates to\nlocal bathymetric complexity. Compared to conventional techniques, experimental\nresults over several ocean regions show notable increases in both\nreconstruction quality and uncertainty estimation reliability. This framework\nincreases the reliability of bathymetric reconstructions by preserving\nstructural integrity while offering spatially adaptive uncertainty estimates,\nso opening the path for more solid climate modeling and coastal hazard\nassessment.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5757\u72b6\u7a7a\u95f4\u9884\u6d4b\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u9ad8\u6d77\u5e95\u5730\u5f62\u6570\u636e\u7684\u5206\u8fa8\u7387\u548c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u5168\u7403\u6d77\u5e95\u5730\u5f62\u6570\u636e\u5206\u8fa8\u7387\u4e0d\u8db3\uff0c\u96be\u4ee5\u652f\u6301\u7cbe\u786e\u7684\u6570\u503c\u6a21\u62df\u548c\u6cbf\u6d77\u707e\u5bb3\u9884\u6d4b\u3002\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u4fdd\u6301\u7269\u7406\u7ed3\u6784\u4e00\u81f4\u6027\u548c\u91cf\u5316\u4e0d\u786e\u5b9a\u6027\u65b9\u9762\u5b58\u5728\u6311\u6218\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u5757\u72b6\u7a7a\u95f4\u9884\u6d4b\u7684VQ-VAE\u67b6\u6784\uff0c\u7ed3\u5408\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u673a\u5236\uff0c\u751f\u6210\u7a7a\u95f4\u81ea\u9002\u5e94\u7684\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\uff0c\u540c\u65f6\u901a\u8fc7\u79bb\u6563\u6f5c\u5728\u8868\u793a\u4fdd\u7559\u5730\u5f62\u7279\u5f81\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u6d77\u57df\u663e\u8457\u63d0\u9ad8\u4e86\u91cd\u5efa\u8d28\u91cf\u548c\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u7684\u53ef\u9760\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u4fdd\u6301\u7ed3\u6784\u5b8c\u6574\u6027\u548c\u63d0\u4f9b\u7a7a\u95f4\u81ea\u9002\u5e94\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff0c\u4e3a\u66f4\u53ef\u9760\u7684\u6c14\u5019\u5efa\u6a21\u548c\u6cbf\u6d77\u707e\u5bb3\u8bc4\u4f30\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2504.15188", "pdf": "https://arxiv.org/pdf/2504.15188", "abs": "https://arxiv.org/abs/2504.15188", "authors": ["Yizhu Jiao", "Xuchao Zhang", "Zhaoyang Wang", "Yubo Ma", "Zhun Deng", "Rujia Wang", "Chetan Bansal", "Saravan Rajmohan", "Jiawei Han", "Huaxiu Yao"], "title": "Synergistic Weak-Strong Collaboration by Aligning Preferences", "categories": ["cs.AI"], "comment": null, "summary": "Current Large Language Models (LLMs) excel in general reasoning yet struggle\nwith specialized tasks requiring proprietary or domain-specific knowledge.\nFine-tuning large models for every niche application is often infeasible due to\nblack-box constraints and high computational overhead. To address this, we\npropose a collaborative framework that pairs a specialized weak model with a\ngeneral strong model. The weak model, tailored to specific domains, produces\ninitial drafts and background information, while the strong model leverages its\nadvanced reasoning to refine these drafts, extending LLMs' capabilities to\ncritical yet specialized tasks. To optimize this collaboration, we introduce a\ncollaborative feedback to fine-tunes the weak model, which quantifies the\ninfluence of the weak model's contributions in the collaboration procedure and\nestablishes preference pairs to guide preference tuning of the weak model. We\nvalidate our framework through experiments on three domains. We find that the\ncollaboration significantly outperforms each model alone by leveraging\ncomplementary strengths. Moreover, aligning the weak model with the\ncollaborative preference further enhances overall performance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u534f\u4f5c\u6846\u67b6\uff0c\u5c06\u4e13\u7528\u5f31\u6a21\u578b\u4e0e\u901a\u7528\u5f3a\u6a21\u578b\u7ed3\u5408\uff0c\u4ee5\u89e3\u51b3LLMs\u5728\u4e13\u4e1a\u4efb\u52a1\u4e2d\u7684\u4e0d\u8db3\u3002\u901a\u8fc7\u534f\u4f5c\u53cd\u9988\u4f18\u5316\u5f31\u6a21\u578b\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u89e3\u51b3LLMs\u5728\u4e13\u4e1a\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\u7684\u95ee\u9898\uff0c\u907f\u514d\u4e3a\u6bcf\u4e2a\u7ec6\u5206\u9886\u57df\u5fae\u8c03\u5927\u6a21\u578b\u7684\u9ad8\u6210\u672c\u3002", "method": "\u534f\u4f5c\u6846\u67b6\u4e2d\uff0c\u5f31\u6a21\u578b\u751f\u6210\u521d\u7a3f\u548c\u80cc\u666f\u4fe1\u606f\uff0c\u5f3a\u6a21\u578b\u8fdb\u884c\u4f18\u5316\uff1b\u5f15\u5165\u534f\u4f5c\u53cd\u9988\u673a\u5236\u91cf\u5316\u5f31\u6a21\u578b\u8d21\u732e\u5e76\u6307\u5bfc\u5176\u504f\u597d\u8c03\u6574\u3002", "result": "\u534f\u4f5c\u6846\u67b6\u5728\u4e09\u4e2a\u9886\u57df\u5b9e\u9a8c\u4e2d\u663e\u8457\u4f18\u4e8e\u5355\u72ec\u6a21\u578b\uff0c\u4e14\u901a\u8fc7\u504f\u597d\u8c03\u6574\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u6027\u80fd\u3002", "conclusion": "\u534f\u4f5c\u6846\u67b6\u6709\u6548\u6269\u5c55\u4e86LLMs\u5728\u4e13\u4e1a\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\uff0c\u5c55\u793a\u4e86\u4e92\u8865\u4f18\u52bf\u7684\u6f5c\u529b\u3002"}}
{"id": "2504.15027", "pdf": "https://arxiv.org/pdf/2504.15027", "abs": "https://arxiv.org/abs/2504.15027", "authors": ["Chengyu Wang", "Junbing Yan", "Yuanhao Yue", "Jun Huang"], "title": "DistilQwen2.5: Industrial Practices of Training Distilled Open Lightweight Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Enhancing computational efficiency and reducing deployment costs for large\nlanguage models (LLMs) have become critical challenges in various\nresource-constrained scenarios. In this work, we present DistilQwen2.5, a\nfamily of distilled, lightweight LLMs derived from the public Qwen2.5 models.\nThese distilled models exhibit enhanced instruction-following capabilities\ncompared to the original models based on a series of distillation techniques\nthat incorporate knowledge from much larger LLMs. In our industrial practice,\nwe first leverage powerful proprietary LLMs with varying capacities as\nmulti-agent teachers to select, rewrite, and refine instruction-response pairs\nthat are more suitable for student LLMs to learn. After standard fine-tuning,\nwe further leverage a computationally efficient model fusion approach that\nenables student models to progressively integrate fine-grained hidden knowledge\nfrom their teachers. Experimental evaluations demonstrate that the distilled\nmodels possess significantly stronger capabilities than their original\ncheckpoints. Additionally, we present use cases to illustrate the applications\nof our framework in real-world scenarios. To facilitate practical use, we have\nreleased all the DistilQwen2.5 models to the open-source community.", "AI": {"tldr": "DistilQwen2.5\u662f\u4e00\u7cfb\u5217\u8f7b\u91cf\u7ea7\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u84b8\u998f\u6280\u672f\u4eceQwen2.5\u6a21\u578b\u6d3e\u751f\u800c\u6765\uff0c\u63d0\u5347\u4e86\u6307\u4ee4\u8ddf\u968f\u80fd\u529b\uff0c\u5e76\u964d\u4f4e\u4e86\u90e8\u7f72\u6210\u672c\u3002", "motivation": "\u5728\u8d44\u6e90\u53d7\u9650\u7684\u573a\u666f\u4e0b\uff0c\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8ba1\u7b97\u6548\u7387\u5e76\u964d\u4f4e\u90e8\u7f72\u6210\u672c\u662f\u5173\u952e\u6311\u6218\u3002", "method": "\u5229\u7528\u591a\u4ee3\u7406\u6559\u5e08\u6a21\u578b\u9009\u62e9\u548c\u6539\u5199\u6307\u4ee4-\u54cd\u5e94\u5bf9\uff0c\u7ed3\u5408\u6807\u51c6\u5fae\u8c03\u548c\u6a21\u578b\u878d\u5408\u6280\u672f\uff0c\u9010\u6b65\u6574\u5408\u6559\u5e08\u6a21\u578b\u7684\u7ec6\u7c92\u5ea6\u77e5\u8bc6\u3002", "result": "\u84b8\u998f\u540e\u7684\u6a21\u578b\u80fd\u529b\u663e\u8457\u4f18\u4e8e\u539f\u59cb\u6a21\u578b\uff0c\u5e76\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5c55\u793a\u4e86\u6548\u679c\u3002", "conclusion": "DistilQwen2.5\u6a21\u578b\u5df2\u5f00\u6e90\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u8f7b\u91cf\u5316\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.14375", "pdf": "https://arxiv.org/pdf/2504.14375", "abs": "https://arxiv.org/abs/2504.14375", "authors": ["Kun Qian", "Maximillian Chen", "Siyan Li", "Arpit Sharma", "Zhou Yu"], "title": "Bottom-Up Synthesis of Knowledge-Grounded Task-Oriented Dialogues with Iteratively Self-Refined Prompts", "categories": ["cs.LG"], "comment": "Accepted by NAACL 2025", "summary": "Training conversational question-answering (QA) systems requires a\nsubstantial amount of in-domain data, which is often scarce in practice. A\ncommon solution to this challenge is to generate synthetic data. Traditional\nmethods typically follow a top-down approach, where a large language model\n(LLM) generates multi-turn dialogues from a broad prompt. Although this method\nproduces coherent conversations, it offers limited fine-grained control over\nthe content and is susceptible to hallucinations. We introduce a bottom-up\nconversation synthesis approach, where QA pairs are generated first and then\ncombined into a coherent dialogue. This method offers greater control and\nprecision by dividing the process into two distinct steps, allowing refined\ninstructions and validations to be handled separately. Additionally, this\nstructure allows the use of non-local models in stages that do not involve\nproprietary knowledge, enhancing the overall quality of the generated data.\nBoth human and automated evaluations demonstrate that our approach produces\nmore realistic and higher-quality dialogues compared to top-down methods.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u5e95\u5411\u4e0a\u7684\u5bf9\u8bdd\u5408\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u5148\u751f\u6210\u95ee\u7b54\u5bf9\u518d\u7ec4\u5408\u6210\u5bf9\u8bdd\uff0c\u63d0\u9ad8\u4e86\u6570\u636e\u751f\u6210\u7684\u63a7\u5236\u6027\u548c\u8d28\u91cf\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u81ea\u4e0a\u800c\u4e0b\u65b9\u6cd5\u5728\u751f\u6210\u5bf9\u8bdd\u65f6\u5185\u5bb9\u63a7\u5236\u4e0d\u8db3\u548c\u6613\u4ea7\u751f\u5e7b\u89c9\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u4e24\u6b65\u6cd5\uff1a\u9996\u5148\u751f\u6210\u95ee\u7b54\u5bf9\uff0c\u518d\u7ec4\u5408\u6210\u8fde\u8d2f\u5bf9\u8bdd\uff0c\u540c\u65f6\u5141\u8bb8\u4f7f\u7528\u975e\u672c\u5730\u6a21\u578b\u3002", "result": "\u4eba\u7c7b\u548c\u81ea\u52a8\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u751f\u6210\u7684\u5bf9\u8bdd\u66f4\u771f\u5b9e\u3001\u8d28\u91cf\u66f4\u9ad8\u3002", "conclusion": "\u81ea\u5e95\u5411\u4e0a\u7684\u65b9\u6cd5\u5728\u5bf9\u8bdd\u751f\u6210\u4e2d\u5177\u6709\u66f4\u9ad8\u7684\u63a7\u5236\u6027\u548c\u8d28\u91cf\u4f18\u52bf\u3002"}}
{"id": "2504.15211", "pdf": "https://arxiv.org/pdf/2504.15211", "abs": "https://arxiv.org/abs/2504.15211", "authors": ["Yanan Long"], "title": "Position: Bayesian Statistics Facilitates Stakeholder Participation in Evaluation of Generative AI", "categories": ["cs.AI", "stat.AP"], "comment": "To be presented at ACM CHI 2025 workshop STAIG", "summary": "The evaluation of Generative AI (GenAI) systems plays a critical role in\npublic policy and decision-making, yet existing methods are often limited by\nreliance on benchmark-driven, point-estimate comparisons that fail to capture\nuncertainty and broader societal impacts. This paper argues for the use of\nBayesian statistics as a principled framework to address these challenges.\nBayesian methods enable the integration of domain expertise through prior\nelicitation, allow for continuous learning from new data, and provide robust\nuncertainty quantification via posterior inference. We demonstrate how Bayesian\ninference can be applied to GenAI evaluation, particularly in incorporating\nstakeholder perspectives to enhance fairness, transparency, and reliability.\nFurthermore, we discuss Bayesian workflows as an iterative process for model\nvalidation and refinement, ensuring robust assessments of GenAI systems in\ndynamic, real-world contexts.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4f7f\u7528\u8d1d\u53f6\u65af\u7edf\u8ba1\u4f5c\u4e3a\u8bc4\u4f30\u751f\u6210\u5f0fAI\u7cfb\u7edf\u7684\u6846\u67b6\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u4e0d\u786e\u5b9a\u6027\u548c\u793e\u4f1a\u5f71\u54cd\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u751f\u6210\u5f0fAI\u8bc4\u4f30\u65b9\u6cd5\u4f9d\u8d56\u57fa\u51c6\u6d4b\u8bd5\u548c\u70b9\u4f30\u8ba1\uff0c\u65e0\u6cd5\u6355\u6349\u4e0d\u786e\u5b9a\u6027\u548c\u5e7f\u6cdb\u793e\u4f1a\u5f71\u54cd\uff0c\u9700\u8981\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u91c7\u7528\u8d1d\u53f6\u65af\u7edf\u8ba1\u65b9\u6cd5\uff0c\u6574\u5408\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\uff0c\u652f\u6301\u6301\u7eed\u5b66\u4e60\uff0c\u5e76\u901a\u8fc7\u540e\u9a8c\u63a8\u65ad\u91cf\u5316\u4e0d\u786e\u5b9a\u6027\u3002", "result": "\u8d1d\u53f6\u65af\u65b9\u6cd5\u80fd\u591f\u7ed3\u5408\u5229\u76ca\u76f8\u5173\u8005\u89c6\u89d2\uff0c\u63d0\u5347\u516c\u5e73\u6027\u3001\u900f\u660e\u5ea6\u548c\u53ef\u9760\u6027\uff0c\u5e76\u901a\u8fc7\u8fed\u4ee3\u5de5\u4f5c\u6d41\u9a8c\u8bc1\u6a21\u578b\u3002", "conclusion": "\u8d1d\u53f6\u65af\u7edf\u8ba1\u4e3a\u751f\u6210\u5f0fAI\u8bc4\u4f30\u63d0\u4f9b\u4e86\u52a8\u6001\u3001\u7a33\u5065\u7684\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u73b0\u5b9e\u4e16\u754c\u7684\u590d\u6742\u573a\u666f\u3002"}}
{"id": "2504.15047", "pdf": "https://arxiv.org/pdf/2504.15047", "abs": "https://arxiv.org/abs/2504.15047", "authors": ["Quy-Anh Dang", "Chris Ngo", "Truong-Son Hy"], "title": "RainbowPlus: Enhancing Adversarial Prompt Generation via Evolutionary Quality-Diversity Search", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) exhibit remarkable capabilities but are\nsusceptible to adversarial prompts that exploit vulnerabilities to produce\nunsafe or biased outputs. Existing red-teaming methods often face scalability\nchallenges, resource-intensive requirements, or limited diversity in attack\nstrategies. We propose RainbowPlus, a novel red-teaming framework rooted in\nevolutionary computation, enhancing adversarial prompt generation through an\nadaptive quality-diversity (QD) search that extends classical evolutionary\nalgorithms like MAP-Elites with innovations tailored for language models. By\nemploying a multi-element archive to store diverse high-quality prompts and a\ncomprehensive fitness function to evaluate multiple prompts concurrently,\nRainbowPlus overcomes the constraints of single-prompt archives and pairwise\ncomparisons in prior QD methods like Rainbow Teaming. Experiments comparing\nRainbowPlus to QD methods across six benchmark datasets and four open-source\nLLMs demonstrate superior attack success rate (ASR) and diversity\n(Diverse-Score $\\approx 0.84$), generating up to 100 times more unique prompts\n(e.g., 10,418 vs. 100 for Ministral-8B-Instruct-2410). Against nine\nstate-of-the-art methods on the HarmBench dataset with twelve LLMs (ten\nopen-source, two closed-source), RainbowPlus achieves an average ASR of 81.1%,\nsurpassing AutoDAN-Turbo by 3.9%, and is 9 times faster (1.45 vs. 13.50 hours).\nOur open-source implementation fosters further advancements in LLM safety,\noffering a scalable tool for vulnerability assessment. Code and resources are\npublicly available at https://github.com/knoveleng/rainbowplus, supporting\nreproducibility and future research in LLM red-teaming.", "AI": {"tldr": "RainbowPlus\u662f\u4e00\u79cd\u57fa\u4e8e\u8fdb\u5316\u8ba1\u7b97\u7684\u65b0\u578b\u7ea2\u961f\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u8d28\u91cf\u591a\u6837\u6027\u641c\u7d22\u589e\u5f3a\u5bf9\u6297\u6027\u63d0\u793a\u751f\u6210\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u653b\u51fb\u6210\u529f\u7387\u548c\u591a\u6837\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5b58\u5728\u5bf9\u6297\u6027\u63d0\u793a\u6f0f\u6d1e\uff0c\u73b0\u6709\u7ea2\u961f\u65b9\u6cd5\u5728\u53ef\u6269\u5c55\u6027\u3001\u8d44\u6e90\u6d88\u8017\u548c\u653b\u51fb\u7b56\u7565\u591a\u6837\u6027\u65b9\u9762\u5b58\u5728\u5c40\u9650\u3002", "method": "RainbowPlus\u91c7\u7528\u591a\u5143\u7d20\u5b58\u6863\u5b58\u50a8\u591a\u6837\u5316\u9ad8\u8d28\u91cf\u63d0\u793a\uff0c\u5e76\u4f7f\u7528\u7efc\u5408\u9002\u5e94\u5ea6\u51fd\u6570\u8bc4\u4f30\u591a\u4e2a\u63d0\u793a\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u9650\u5236\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u548cLLMs\u4e0a\uff0cRainbowPlus\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u653b\u51fb\u6210\u529f\u7387\uff08ASR 81.1%\uff09\u548c\u591a\u6837\u6027\uff08Diverse-Score \u22480.84\uff09\uff0c\u4e14\u901f\u5ea6\u66f4\u5feb\u3002", "conclusion": "RainbowPlus\u4e3aLLM\u5b89\u5168\u6027\u8bc4\u4f30\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u5de5\u5177\uff0c\u5f00\u6e90\u5b9e\u73b0\u652f\u6301\u672a\u6765\u7814\u7a76\u3002"}}
{"id": "2504.14388", "pdf": "https://arxiv.org/pdf/2504.14388", "abs": "https://arxiv.org/abs/2504.14388", "authors": ["Xiaoyang Wang", "Christopher C. Yang"], "title": "Balancing Fairness and Performance in Healthcare AI: A Gradient Reconciliation Approach", "categories": ["cs.LG", "cs.CY"], "comment": "Accepted to the 23rd International Conference on Artificial\n  Intelligence in Medicine (AIME 2025)", "summary": "The rapid growth of healthcare data and advances in computational power have\naccelerated the adoption of artificial intelligence (AI) in medicine. However,\nAI systems deployed without explicit fairness considerations risk exacerbating\nexisting healthcare disparities, potentially leading to inequitable resource\nallocation and diagnostic disparities across demographic subgroups. To address\nthis challenge, we propose FairGrad, a novel gradient reconciliation framework\nthat automatically balances predictive performance and multi-attribute fairness\noptimization in healthcare AI models. Our method resolves conflicting\noptimization objectives by projecting each gradient vector onto the orthogonal\nplane of the others, thereby regularizing the optimization trajectory to ensure\nequitable consideration of all objectives. Evaluated on diverse real-world\nhealthcare datasets and predictive tasks - including Substance Use Disorder\n(SUD) treatment and sepsis mortality - FairGrad achieved statistically\nsignificant improvements in multi-attribute fairness metrics (e.g., equalized\nodds) while maintaining competitive predictive accuracy. These results\ndemonstrate the viability of harmonizing fairness and utility in\nmission-critical medical AI applications.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faFairGrad\u6846\u67b6\uff0c\u901a\u8fc7\u68af\u5ea6\u534f\u8c03\u5e73\u8861\u533b\u7597AI\u6a21\u578b\u7684\u9884\u6d4b\u6027\u80fd\u4e0e\u591a\u5c5e\u6027\u516c\u5e73\u6027\uff0c\u907f\u514d\u52a0\u5267\u533b\u7597\u4e0d\u5e73\u7b49\u3002", "motivation": "\u533b\u7597AI\u7684\u5feb\u901f\u5e94\u7528\u53ef\u80fd\u52a0\u5267\u73b0\u6709\u533b\u7597\u4e0d\u5e73\u7b49\uff0c\u9700\u5e73\u8861\u516c\u5e73\u6027\u4e0e\u9884\u6d4b\u6027\u80fd\u3002", "method": "FairGrad\u901a\u8fc7\u5c06\u68af\u5ea6\u5411\u91cf\u6295\u5f71\u5230\u6b63\u4ea4\u5e73\u9762\uff0c\u534f\u8c03\u51b2\u7a81\u7684\u4f18\u5316\u76ee\u6807\uff0c\u786e\u4fdd\u516c\u5e73\u6027\u3002", "result": "\u5728\u771f\u5b9e\u533b\u7597\u6570\u636e\u4e0a\uff0cFairGrad\u663e\u8457\u63d0\u5347\u591a\u5c5e\u6027\u516c\u5e73\u6027\u6307\u6807\uff0c\u540c\u65f6\u4fdd\u6301\u9884\u6d4b\u51c6\u786e\u6027\u3002", "conclusion": "FairGrad\u8bc1\u660e\u5728\u5173\u952e\u533b\u7597AI\u5e94\u7528\u4e2d\u5e73\u8861\u516c\u5e73\u6027\u4e0e\u5b9e\u7528\u6027\u662f\u53ef\u884c\u7684\u3002"}}
{"id": "2504.15228", "pdf": "https://arxiv.org/pdf/2504.15228", "abs": "https://arxiv.org/abs/2504.15228", "authors": ["Maxime Robeyns", "Martin Szummer", "Laurence Aitchison"], "title": "A Self-Improving Coding Agent", "categories": ["cs.AI"], "comment": "Published at an ICLR 2025 workshop on Scaling Self-Improving\n  Foundation Models", "summary": "We demonstrate that an LLM coding agent, equipped with basic coding tools,\ncan autonomously edit itself, and thereby improve its performance on benchmark\ntasks. We find performance gains from 17% to 53% on a random subset of SWE\nBench Verified, with additional performance gains on LiveCodeBench, as well as\nsynthetically generated agent benchmarks. Our work represents an advancement in\nthe automated and open-ended design of agentic systems, and provides a\nreference agent framework for those seeking to post-train LLMs on tool use and\nother agentic tasks.", "AI": {"tldr": "LLM\u7f16\u7801\u4ee3\u7406\u901a\u8fc7\u81ea\u6211\u7f16\u8f91\u63d0\u5347\u6027\u80fd\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u63a2\u7d22LLM\u7f16\u7801\u4ee3\u7406\u5728\u81ea\u52a8\u5316\u8bbe\u8ba1\u4e2d\u7684\u6f5c\u529b\uff0c\u63d0\u4f9b\u4e00\u79cd\u53c2\u8003\u6846\u67b6\u4ee5\u4f18\u5316\u5de5\u5177\u4f7f\u7528\u548c\u4ee3\u7406\u4efb\u52a1\u3002", "method": "\u4ee3\u7406\u914d\u5907\u57fa\u672c\u7f16\u7801\u5de5\u5177\uff0c\u901a\u8fc7\u81ea\u4e3b\u7f16\u8f91\u6539\u8fdb\u6027\u80fd\u3002", "result": "\u5728SWE Bench Verified\u5b50\u96c6\u4e0a\u6027\u80fd\u63d0\u534717%\u81f353%\uff0c\u5728LiveCodeBench\u548c\u5408\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4e5f\u6709\u63d0\u5347\u3002", "conclusion": "\u8be5\u7814\u7a76\u63a8\u52a8\u4e86\u81ea\u52a8\u5316\u4ee3\u7406\u7cfb\u7edf\u7684\u8bbe\u8ba1\uff0c\u4e3aLLM\u7684\u540e\u8bad\u7ec3\u63d0\u4f9b\u4e86\u5b9e\u7528\u6846\u67b6\u3002"}}
{"id": "2504.15052", "pdf": "https://arxiv.org/pdf/2504.15052", "abs": "https://arxiv.org/abs/2504.15052", "authors": ["Joachim Minder", "Guillaume Wisniewski", "Natalie K\u00fcbler"], "title": "Testing LLMs' Capabilities in Annotating Translations Based on an Error Typology Designed for LSP Translation: First Experiments with ChatGPT", "categories": ["cs.CL", "eess.AS"], "comment": "Accepted for publication in the proceedings of MT Summit 2025", "summary": "This study investigates the capabilities of large language models (LLMs),\nspecifically ChatGPT, in annotating MT outputs based on an error typology. In\ncontrast to previous work focusing mainly on general language, we explore\nChatGPT's ability to identify and categorise errors in specialised\ntranslations. By testing two different prompts and based on a customised error\ntypology, we compare ChatGPT annotations with human expert evaluations of\ntranslations produced by DeepL and ChatGPT itself. The results show that, for\ntranslations generated by DeepL, recall and precision are quite high. However,\nthe degree of accuracy in error categorisation depends on the prompt's specific\nfeatures and its level of detail, ChatGPT performing very well with a detailed\nprompt. When evaluating its own translations, ChatGPT achieves significantly\npoorer results, revealing limitations with self-assessment. These results\nhighlight both the potential and the limitations of LLMs for translation\nevaluation, particularly in specialised domains. Our experiments pave the way\nfor future research on open-source LLMs, which could produce annotations of\ncomparable or even higher quality. In the future, we also aim to test the\npractical effectiveness of this automated evaluation in the context of\ntranslation training, particularly by optimising the process of human\nevaluation by teachers and by exploring the impact of annotations by LLMs on\nstudents' post-editing and translation learning.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86ChatGPT\u5728\u57fa\u4e8e\u9519\u8bef\u7c7b\u578b\u6807\u6ce8\u673a\u5668\u7ffb\u8bd1\u8f93\u51fa\u65b9\u9762\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u5176\u5728\u4e13\u4e1a\u7ffb\u8bd1\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u81ea\u6211\u8bc4\u4f30\u80fd\u529b\u6709\u9650\u3002", "motivation": "\u63a2\u7d22\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08\u5982ChatGPT\uff09\u5728\u4e13\u4e1a\u7ffb\u8bd1\u9519\u8bef\u6807\u6ce8\u4e2d\u7684\u6f5c\u529b\uff0c\u586b\u8865\u4ee5\u5f80\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u901a\u7528\u8bed\u8a00\u7684\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u4e24\u79cd\u4e0d\u540c\u63d0\u793a\u548c\u5b9a\u5236\u9519\u8bef\u7c7b\u578b\u5b66\uff0c\u6bd4\u8f83ChatGPT\u4e0e\u4eba\u7c7b\u4e13\u5bb6\u5bf9DeepL\u548cChatGPT\u81ea\u8eab\u7ffb\u8bd1\u7684\u6807\u6ce8\u7ed3\u679c\u3002", "result": "ChatGPT\u5bf9DeepL\u7ffb\u8bd1\u7684\u6807\u6ce8\u53ec\u56de\u7387\u548c\u7cbe\u786e\u5ea6\u8f83\u9ad8\uff0c\u4f46\u81ea\u6211\u8bc4\u4f30\u8868\u73b0\u8f83\u5dee\uff1b\u63d0\u793a\u7684\u8be6\u7ec6\u7a0b\u5ea6\u5f71\u54cd\u5206\u7c7b\u51c6\u786e\u6027\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u7ffb\u8bd1\u8bc4\u4f30\u4e2d\u7684\u6f5c\u529b\u4e0e\u5c40\u9650\uff0c\u4e3a\u672a\u6765\u5f00\u6e90\u6a21\u578b\u7814\u7a76\u548c\u7ffb\u8bd1\u6559\u5b66\u5e94\u7528\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2504.14416", "pdf": "https://arxiv.org/pdf/2504.14416", "abs": "https://arxiv.org/abs/2504.14416", "authors": ["Jose Lara-Rangel", "Nanze Chen", "Fengzhe Zhang"], "title": "Exploring Pseudo-Token Approaches in Transformer Neural Processes", "categories": ["cs.LG"], "comment": "7th Symposium on Advances in Approximate Bayesian Inference", "summary": "Neural Processes (NPs) have gained attention in meta-learning for their\nability to quantify uncertainty, together with their rapid prediction and\nadaptability. However, traditional NPs are prone to underfitting. Transformer\nNeural Processes (TNPs) significantly outperform existing NPs, yet their\napplicability in real-world scenarios is hindered by their quadratic\ncomputational complexity relative to both context and target data points. To\naddress this, pseudo-token-based TNPs (PT-TNPs) have emerged as a novel NPs\nsubset that condense context data into latent vectors or pseudo-tokens,\nreducing computational demands. We introduce the Induced Set Attentive Neural\nProcesses (ISANPs), employing Induced Set Attention and an innovative query\nphase to improve querying efficiency. Our evaluations show that ISANPs perform\ncompetitively with TNPs and often surpass state-of-the-art models in 1D\nregression, image completion, contextual bandits, and Bayesian optimization.\nCrucially, ISANPs offer a tunable balance between performance and computational\ncomplexity, which scale well to larger datasets where TNPs face limitations.", "AI": {"tldr": "ISANPs\u662f\u4e00\u79cd\u65b0\u578b\u7684\u795e\u7ecf\u8fc7\u7a0b\u6a21\u578b\uff0c\u901a\u8fc7\u6539\u8fdb\u67e5\u8be2\u6548\u7387\u548c\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u5728\u6027\u80fd\u4e0e\u8ba1\u7b97\u9700\u6c42\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u6570\u636e\u96c6\u3002", "motivation": "\u4f20\u7edf\u795e\u7ecf\u8fc7\u7a0b\uff08NPs\uff09\u5bb9\u6613\u6b20\u62df\u5408\uff0c\u800cTransformer\u795e\u7ecf\u8fc7\u7a0b\uff08TNPs\uff09\u867d\u6027\u80fd\u4f18\u8d8a\uff0c\u4f46\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u63d0\u51faISANPs\uff0c\u91c7\u7528\u8bf1\u5bfc\u96c6\u6ce8\u610f\u529b\u673a\u5236\u548c\u521b\u65b0\u7684\u67e5\u8be2\u9636\u6bb5\uff0c\u51cf\u5c11\u8ba1\u7b97\u9700\u6c42\u3002", "result": "ISANPs\u57281D\u56de\u5f52\u3001\u56fe\u50cf\u8865\u5168\u7b49\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4e0eTNPs\u76f8\u5f53\u751a\u81f3\u8d85\u8d8a\u73b0\u6709\u6a21\u578b\u3002", "conclusion": "ISANPs\u5728\u6027\u80fd\u548c\u8ba1\u7b97\u6548\u7387\u4e4b\u95f4\u63d0\u4f9b\u4e86\u53ef\u8c03\u8282\u7684\u5e73\u8861\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u6570\u636e\u96c6\u3002"}}
{"id": "2504.15252", "pdf": "https://arxiv.org/pdf/2504.15252", "abs": "https://arxiv.org/abs/2504.15252", "authors": ["Tue Vo", "Lakshay Sharma", "Tuan Dinh", "Khuong Dinh", "Trang Nguyen", "Trung Phan", "Minh Do", "Duong Vu"], "title": "SuoiAI: Building a Dataset for Aquatic Invertebrates in Vietnam", "categories": ["cs.AI", "cs.CV", "cs.LG"], "comment": "Published as a workshop paper at \"Tackling Climate Change with\n  Machine Learning\", ICLR 2025", "summary": "Understanding and monitoring aquatic biodiversity is critical for ecological\nhealth and conservation efforts. This paper proposes SuoiAI, an end-to-end\npipeline for building a dataset of aquatic invertebrates in Vietnam and\nemploying machine learning (ML) techniques for species classification. We\noutline the methods for data collection, annotation, and model training,\nfocusing on reducing annotation effort through semi-supervised learning and\nleveraging state-of-the-art object detection and classification models. Our\napproach aims to overcome challenges such as data scarcity, fine-grained\nclassification, and deployment in diverse environmental conditions.", "AI": {"tldr": "SuoiAI\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u6d41\u7a0b\uff0c\u7528\u4e8e\u6784\u5efa\u8d8a\u5357\u6c34\u751f\u65e0\u810a\u690e\u52a8\u7269\u6570\u636e\u96c6\u5e76\u5229\u7528\u673a\u5668\u5b66\u4e60\u8fdb\u884c\u7269\u79cd\u5206\u7c7b\u3002", "motivation": "\u7406\u89e3\u548c\u76d1\u6d4b\u6c34\u751f\u751f\u7269\u591a\u6837\u6027\u5bf9\u751f\u6001\u5065\u5eb7\u548c\u4fdd\u80b2\u81f3\u5173\u91cd\u8981\u3002", "method": "\u901a\u8fc7\u534a\u76d1\u7763\u5b66\u4e60\u51cf\u5c11\u6807\u6ce8\u5de5\u4f5c\u91cf\uff0c\u5e76\u5229\u7528\u5148\u8fdb\u7684\u7269\u4f53\u68c0\u6d4b\u548c\u5206\u7c7b\u6a21\u578b\u8fdb\u884c\u6570\u636e\u6536\u96c6\u3001\u6807\u6ce8\u548c\u6a21\u578b\u8bad\u7ec3\u3002", "result": "\u65e8\u5728\u89e3\u51b3\u6570\u636e\u7a00\u7f3a\u3001\u7ec6\u7c92\u5ea6\u5206\u7c7b\u53ca\u591a\u6837\u5316\u73af\u5883\u6761\u4ef6\u4e0b\u7684\u90e8\u7f72\u6311\u6218\u3002", "conclusion": "SuoiAI\u4e3a\u6c34\u751f\u751f\u7269\u591a\u6837\u6027\u76d1\u6d4b\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.15093", "pdf": "https://arxiv.org/pdf/2504.15093", "abs": "https://arxiv.org/abs/2504.15093", "authors": ["K. Wong", "B. Wu", "S. Bulathwela", "M. Cukurova"], "title": "Rethinking the Potential of Multimodality in Collaborative Problem Solving Diagnosis with Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted for 26th International Conference on Artificial Intelligence\n  in Education (AIED 2025), 22 - 26 July 2025, Palermo, Italy. 17 pages, 1\n  figure", "summary": "Detecting collaborative and problem-solving behaviours from digital traces to\ninterpret students' collaborative problem solving (CPS) competency is a\nlong-term goal in the Artificial Intelligence in Education (AIEd) field.\nAlthough multimodal data and advanced models are argued to have the potential\nto detect complex CPS behaviours, empirical evidence on their value remains\nlimited with some contrasting evidence. In this study, we investigated the\npotential of multimodal data to improve model performance in diagnosing 78\nsecondary school students' CPS subskills and indicators in authentic\neducational settings. In particular, text embeddings from verbal data and\nacoustic embeddings from audio data were used in a multimodal classification\nmodel for CPS diagnosis. Both unimodal and multimodal transformer-based models\noutperformed traditional models in detecting CPS classes. Although the\ninclusion of multimodality did not improve the performance of traditional\nunimodal models, its integration into transformer-based models demonstrated\nimproved performance for diagnosing social-cognitive CPS classes compared to\nunimodal transformer-based models. Based on the results, the paper argues that\nmultimodality and the selection of a particular modelling technique should not\nbe taken for granted to achieve the best performance in the automated detection\nof every CPS subskill and indicator. Rather, their value is limited to certain\ntypes of CPS indicators, affected by the complexity of the labels, and\ndependent on the composition of indicators in the dataset. We conclude the\npaper by discussing the required nuance when considering the value of LLMs and\nmultimodality in automated CPS diagnosis, highlighting the need for human-AI\ncomplementarity, and proposing the exploration of relevant model architectures\nand techniques to improve CPS diagnosis in authentic educational contexts.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u591a\u6a21\u6001\u6570\u636e\u5728\u8bca\u65ad\u5b66\u751f\u534f\u4f5c\u95ee\u9898\u89e3\u51b3\uff08CPS\uff09\u80fd\u529b\u4e2d\u7684\u6f5c\u529b\uff0c\u53d1\u73b0\u5176\u5728\u7279\u5b9a\u7c7b\u578b\u7684CPS\u6307\u6807\u4e2d\u8868\u73b0\u66f4\u597d\uff0c\u4f46\u6548\u679c\u4f9d\u8d56\u4e8e\u6807\u7b7e\u590d\u6742\u6027\u548c\u6570\u636e\u96c6\u7ec4\u6210\u3002", "motivation": "\u63a2\u7d22\u591a\u6a21\u6001\u6570\u636e\u548c\u5148\u8fdb\u6a21\u578b\u5728\u68c0\u6d4b\u590d\u6742CPS\u884c\u4e3a\u4e2d\u7684\u4ef7\u503c\uff0c\u586b\u8865\u5b9e\u8bc1\u8bc1\u636e\u7684\u4e0d\u8db3\u3002", "method": "\u4f7f\u7528\u6587\u672c\u5d4c\u5165\u548c\u58f0\u5b66\u5d4c\u5165\u6784\u5efa\u591a\u6a21\u6001\u5206\u7c7b\u6a21\u578b\uff0c\u6bd4\u8f83\u5355\u6a21\u6001\u548c\u591a\u6a21\u6001\u6a21\u578b\u5728CPS\u8bca\u65ad\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u591a\u6a21\u6001\u5728\u57fa\u4e8eTransformer\u7684\u6a21\u578b\u4e2d\u63d0\u5347\u4e86\u793e\u4ea4\u8ba4\u77e5\u7c7bCPS\u7684\u8bca\u65ad\u6027\u80fd\uff0c\u4f46\u5bf9\u4f20\u7edf\u6a21\u578b\u65e0\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "\u591a\u6a21\u6001\u548c\u6a21\u578b\u9009\u62e9\u9700\u6839\u636e\u5177\u4f53CPS\u6307\u6807\u7c7b\u578b\u548c\u6570\u636e\u96c6\u7279\u6027\u8c03\u6574\uff0c\u5f3a\u8c03\u4eba\u673a\u4e92\u8865\u548c\u8fdb\u4e00\u6b65\u63a2\u7d22\u6a21\u578b\u67b6\u6784\u3002"}}
{"id": "2504.14439", "pdf": "https://arxiv.org/pdf/2504.14439", "abs": "https://arxiv.org/abs/2504.14439", "authors": ["Avinandan Bose", "Zhihan Xiong", "Yuejie Chi", "Simon Shaolei Du", "Lin Xiao", "Maryam Fazel"], "title": "LoRe: Personalizing LLMs via Low-Rank Reward Modeling", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Personalizing large language models (LLMs) to accommodate diverse user\npreferences is essential for enhancing alignment and user satisfaction.\nTraditional reinforcement learning from human feedback (RLHF) approaches often\nrely on monolithic value representations, limiting their ability to adapt to\nindividual preferences. We introduce a novel framework that leverages low-rank\npreference modeling to efficiently learn and generalize user-specific reward\nfunctions. By representing reward functions in a low-dimensional subspace and\nmodeling individual preferences as weighted combinations of shared basis\nfunctions, our approach avoids rigid user categorization while enabling\nscalability and few-shot adaptation. We validate our method on multiple\npreference datasets, demonstrating superior generalization to unseen users and\nimproved accuracy in preference prediction tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4f4e\u79e9\u504f\u597d\u5efa\u6a21\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u4e2a\u6027\u5316\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u63d0\u5347\u7528\u6237\u6ee1\u610f\u5ea6\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u4f20\u7edfRLHF\u65b9\u6cd5\u4f9d\u8d56\u5355\u4e00\u4ef7\u503c\u8868\u793a\uff0c\u96be\u4ee5\u9002\u5e94\u591a\u6837\u5316\u7684\u7528\u6237\u504f\u597d\u3002", "method": "\u901a\u8fc7\u4f4e\u7ef4\u5b50\u7a7a\u95f4\u8868\u793a\u5956\u52b1\u51fd\u6570\uff0c\u5c06\u4e2a\u4f53\u504f\u597d\u5efa\u6a21\u4e3a\u5171\u4eab\u57fa\u51fd\u6570\u7684\u52a0\u6743\u7ec4\u5408\u3002", "result": "\u5728\u591a\u4e2a\u504f\u597d\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u8868\u73b0\u51fa\u5bf9\u672a\u89c1\u7528\u6237\u7684\u66f4\u597d\u6cdb\u5316\u548c\u504f\u597d\u9884\u6d4b\u51c6\u786e\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u907f\u514d\u4e86\u50f5\u5316\u7684\u7528\u6237\u5206\u7c7b\uff0c\u540c\u65f6\u652f\u6301\u53ef\u6269\u5c55\u6027\u548c\u5c11\u6837\u672c\u9002\u5e94\u3002"}}
{"id": "2504.15257", "pdf": "https://arxiv.org/pdf/2504.15257", "abs": "https://arxiv.org/abs/2504.15257", "authors": ["Hongcheng Gao", "Yue Liu", "Yufei He", "Longxu Dou", "Chao Du", "Zhijie Deng", "Bryan Hooi", "Min Lin", "Tianyu Pang"], "title": "FlowReasoner: Reinforcing Query-Level Meta-Agents", "categories": ["cs.AI"], "comment": null, "summary": "This paper proposes a query-level meta-agent named FlowReasoner to automate\nthe design of query-level multi-agent systems, i.e., one system per user query.\nOur core idea is to incentivize a reasoning-based meta-agent via external\nexecution feedback. Concretely, by distilling DeepSeek R1, we first endow the\nbasic reasoning ability regarding the generation of multi-agent systems to\nFlowReasoner. Then, we further enhance it via reinforcement learning (RL) with\nexternal execution feedback. A multi-purpose reward is designed to guide the RL\ntraining from aspects of performance, complexity, and efficiency. In this\nmanner, FlowReasoner is enabled to generate a personalized multi-agent system\nfor each user query via deliberative reasoning. Experiments on both engineering\nand competition code benchmarks demonstrate the superiority of FlowReasoner.\nRemarkably, it surpasses o1-mini by 10.52% accuracy across three benchmarks.\nThe code is available at https://github.com/sail-sg/FlowReasoner.", "AI": {"tldr": "FlowReasoner\u662f\u4e00\u4e2a\u67e5\u8be2\u7ea7\u5143\u4ee3\u7406\uff0c\u901a\u8fc7\u5916\u90e8\u6267\u884c\u53cd\u9988\u81ea\u52a8\u5316\u8bbe\u8ba1\u591a\u4ee3\u7406\u7cfb\u7edf\uff0c\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u6027\u80fd\u3001\u590d\u6742\u5ea6\u548c\u6548\u7387\u3002", "motivation": "\u81ea\u52a8\u5316\u8bbe\u8ba1\u9488\u5bf9\u6bcf\u4e2a\u7528\u6237\u67e5\u8be2\u7684\u591a\u4ee3\u7406\u7cfb\u7edf\uff0c\u63d0\u5347\u4e2a\u6027\u5316\u548c\u6548\u7387\u3002", "method": "\u7ed3\u5408DeepSeek R1\u7684\u63a8\u7406\u80fd\u529b\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u901a\u8fc7\u591a\u76ee\u6807\u5956\u52b1\u4f18\u5316\u8bad\u7ec3\u3002", "result": "\u5728\u5de5\u7a0b\u548c\u7ade\u8d5b\u4ee3\u7801\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u51c6\u786e\u7387\u8d85\u8fc7o1-mini 10.52%\u3002", "conclusion": "FlowReasoner\u80fd\u6709\u6548\u751f\u6210\u4e2a\u6027\u5316\u591a\u4ee3\u7406\u7cfb\u7edf\uff0c\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2504.15120", "pdf": "https://arxiv.org/pdf/2504.15120", "abs": "https://arxiv.org/abs/2504.15120", "authors": ["Khalil Hennara", "Sara Chrouf", "Mohamed Motaism Hamed", "Zeina Aldallal", "Omar Hadid", "Safwan AlModhayan"], "title": "Kuwain 1.5B: An Arabic SLM via Language Injection", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Enhancing existing models with new knowledge is a crucial aspect of AI\ndevelopment. This paper introduces a novel method for integrating a new\nlanguage into a large language model (LLM). Our approach successfully\nincorporates a previously unseen target language into an existing LLM without\ncompromising its prior knowledge. We trained a tiny model with 1.5 billion\nparameters named Kuwain by injecting the Arabic language into a small\nopen-source model mainly trained in English. Our method demonstrates\nsignificant improvements in Arabic language performance, with an average 8%\nimprovement across various benchmarks, while retaining the model's existing\nknowledge with a minimum amount of the original model's data. This offers a\ncost-effective alternative to training a comprehensive model in both English\nand Arabic. The results highlight the potential for efficient, targeted\nlanguage model expansion without extensive retraining or resource-intensive\nprocesses.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u65b0\u8bed\u8a00\u6574\u5408\u5230\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e2d\u7684\u65b9\u6cd5\uff0c\u6210\u529f\u5c06\u963f\u62c9\u4f2f\u8bed\u6ce8\u5165\u5230\u4ee5\u82f1\u8bed\u4e3a\u4e3b\u7684\u5c0f\u578b\u5f00\u6e90\u6a21\u578b\u4e2d\uff0c\u6027\u80fd\u63d0\u53478%\uff0c\u540c\u65f6\u4fdd\u7559\u539f\u6709\u77e5\u8bc6\u3002", "motivation": "\u589e\u5f3a\u73b0\u6709\u6a21\u578b\u4ee5\u878d\u5165\u65b0\u77e5\u8bc6\u662fAI\u53d1\u5c55\u7684\u5173\u952e\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u6210\u672c\u9ad8\u4e14\u8d44\u6e90\u5bc6\u96c6\u3002", "method": "\u901a\u8fc7\u5c06\u963f\u62c9\u4f2f\u8bed\u6ce8\u5165\u5230\u4ee5\u82f1\u8bed\u4e3a\u4e3b\u7684\u5c0f\u578b\u5f00\u6e90\u6a21\u578b\u4e2d\uff0c\u8bad\u7ec3\u4e86\u4e00\u4e2a\u540d\u4e3aKuwain\u768415\u4ebf\u53c2\u6570\u6a21\u578b\u3002", "result": "\u963f\u62c9\u4f2f\u8bed\u6027\u80fd\u5e73\u5747\u63d0\u53478%\uff0c\u540c\u65f6\u4fdd\u7559\u539f\u6709\u77e5\u8bc6\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u6210\u672c\u6548\u76ca\u9ad8\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5c55\u793a\u4e86\u65e0\u9700\u5927\u89c4\u6a21\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u9ad8\u6548\u6269\u5c55\u8bed\u8a00\u6a21\u578b\u7684\u6f5c\u529b\u3002"}}
{"id": "2504.14469", "pdf": "https://arxiv.org/pdf/2504.14469", "abs": "https://arxiv.org/abs/2504.14469", "authors": ["Navreet Kaur", "Manuel Gonzales IV", "Cristian Garcia Alcaraz", "Jiaqi Gong", "Kristen J. Wells", "Laura E. Barnes"], "title": "A computational framework for longitudinal medication adherence prediction in breast cancer survivors: A social cognitive theory based approach", "categories": ["cs.LG", "cs.HC"], "comment": null, "summary": "Non-adherence to medications is a critical concern since nearly half of\npatients with chronic illnesses do not follow their prescribed medication\nregimens, leading to increased mortality, costs, and preventable human\ndistress. Amongst stage 0-3 breast cancer survivors, adherence to long-term\nadjuvant endocrine therapy (i.e., Tamoxifen and aromatase inhibitors) is\nassociated with a significant increase in recurrence-free survival. This work\naims to develop multi-scale models of medication adherence to understand the\nsignificance of different factors influencing adherence across varying time\nframes. We introduce a computational framework guided by Social Cognitive\nTheory for multi-scale (daily and weekly) modeling of longitudinal medication\nadherence. Our models employ both dynamic medication-taking patterns in the\nrecent past (dynamic factors) as well as less frequently changing factors\n(static factors) for adherence prediction. Additionally, we assess the\nsignificance of various factors in influencing adherence behavior across\ndifferent time scales. Our models outperform traditional machine learning\ncounterparts in both daily and weekly tasks in terms of both accuracy and\nspecificity. Daily models achieved an accuracy of 87.25%, and weekly models, an\naccuracy of 76.04%. Notably, dynamic past medication-taking patterns prove most\nvaluable for predicting daily adherence, while a combination of dynamic and\nstatic factors is significant for macro-level weekly adherence patterns.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u4e2a\u591a\u5c3a\u5ea6\u6a21\u578b\uff0c\u7528\u4e8e\u9884\u6d4b\u4e73\u817a\u764c\u5e78\u5b58\u8005\u7684\u836f\u7269\u4f9d\u4ece\u6027\uff0c\u7ed3\u5408\u52a8\u6001\u548c\u9759\u6001\u56e0\u7d20\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u9884\u6d4b\u51c6\u786e\u6027\u3002", "motivation": "\u836f\u7269\u4e0d\u4f9d\u4ece\u6027\u662f\u6162\u6027\u75c5\u60a3\u8005\u7684\u91cd\u8981\u95ee\u9898\uff0c\u5f71\u54cd\u751f\u5b58\u7387\u548c\u751f\u6d3b\u8d28\u91cf\u3002\u4e73\u817a\u764c\u5e78\u5b58\u8005\u7684\u5185\u5206\u6ccc\u6cbb\u7597\u4f9d\u4ece\u6027\u4e0e\u590d\u53d1\u7387\u5bc6\u5207\u76f8\u5173\uff0c\u56e0\u6b64\u9700\u8981\u7406\u89e3\u5f71\u54cd\u4f9d\u4ece\u6027\u7684\u56e0\u7d20\u3002", "method": "\u57fa\u4e8e\u793e\u4f1a\u8ba4\u77e5\u7406\u8bba\uff0c\u6784\u5efa\u591a\u5c3a\u5ea6\uff08\u6bcf\u65e5\u548c\u6bcf\u5468\uff09\u8ba1\u7b97\u6a21\u578b\uff0c\u7ed3\u5408\u52a8\u6001\u548c\u9759\u6001\u56e0\u7d20\u9884\u6d4b\u4f9d\u4ece\u6027\u3002", "result": "\u6a21\u578b\u5728\u6bcf\u65e5\u548c\u6bcf\u5468\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u6bcf\u65e5\u6a21\u578b\u51c6\u786e\u7387\u8fbe87.25%\uff0c\u6bcf\u5468\u6a21\u578b\u4e3a76.04%\u3002\u52a8\u6001\u56e0\u7d20\u5bf9\u6bcf\u65e5\u9884\u6d4b\u6700\u91cd\u8981\uff0c\u800c\u52a8\u6001\u548c\u9759\u6001\u56e0\u7d20\u7ed3\u5408\u5bf9\u6bcf\u5468\u9884\u6d4b\u66f4\u6709\u6548\u3002", "conclusion": "\u591a\u5c3a\u5ea6\u6a21\u578b\u80fd\u6709\u6548\u9884\u6d4b\u836f\u7269\u4f9d\u4ece\u6027\uff0c\u52a8\u6001\u548c\u9759\u6001\u56e0\u7d20\u7684\u7ed3\u5408\u5728\u4e0d\u540c\u65f6\u95f4\u5c3a\u5ea6\u4e0a\u5177\u6709\u4e0d\u540c\u91cd\u8981\u6027\u3002"}}
{"id": "2504.15261", "pdf": "https://arxiv.org/pdf/2504.15261", "abs": "https://arxiv.org/abs/2504.15261", "authors": ["Mohammad Beheshti", "Lovedeep Gondara", "Iris Zachary"], "title": "Leveraging Language Models for Automated Patient Record Linkage", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Objective: Healthcare data fragmentation presents a major challenge for\nlinking patient data, necessitating robust record linkage to integrate patient\nrecords from diverse sources. This study investigates the feasibility of\nleveraging language models for automated patient record linkage, focusing on\ntwo key tasks: blocking and matching. Materials and Methods: We utilized\nreal-world healthcare data from the Missouri Cancer Registry and Research\nCenter, linking patient records from two independent sources using\nprobabilistic linkage as a baseline. A transformer-based model, RoBERTa, was\nfine-tuned for blocking using sentence embeddings. For matching, several\nlanguage models were experimented under fine-tuned and zero-shot settings,\nassessing their performance against ground truth labels. Results: The\nfine-tuned blocking model achieved a 92% reduction in the number of candidate\npairs while maintaining near-perfect recall. In the matching task, fine-tuned\nMistral-7B achieved the best performance with only 6 incorrect predictions.\nAmong zero-shot models, Mistral-Small-24B performed best, with a total of 55\nincorrect predictions. Discussion: Fine-tuned language models achieved strong\nperformance in patient record blocking and matching with minimal errors.\nHowever, they remain less accurate and efficient than a hybrid rule-based and\nprobabilistic approach for blocking. Additionally, reasoning models like\nDeepSeek-R1 are impractical for large-scale record linkage due to high\ncomputational costs. Conclusion: This study highlights the potential of\nlanguage models for automating patient record linkage, offering improved\nefficiency by eliminating the manual efforts required to perform patient record\nlinkage. Overall, language models offer a scalable solution that can enhance\ndata integration, reduce manual effort, and support disease surveillance and\nresearch.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5229\u7528\u8bed\u8a00\u6a21\u578b\uff08\u5982RoBERTa\u548cMistral\uff09\u81ea\u52a8\u94fe\u63a5\u60a3\u8005\u8bb0\u5f55\u7684\u53ef\u884c\u6027\uff0c\u7ed3\u679c\u663e\u793a\u5fae\u8c03\u6a21\u578b\u5728\u963b\u585e\u548c\u5339\u914d\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u6df7\u5408\u65b9\u6cd5\u4ecd\u66f4\u9ad8\u6548\u3002", "motivation": "\u533b\u7597\u6570\u636e\u788e\u7247\u5316\u5bfc\u81f4\u60a3\u8005\u8bb0\u5f55\u94fe\u63a5\u56f0\u96be\uff0c\u9700\u8981\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u4ee5\u6574\u5408\u6570\u636e\u3002", "method": "\u4f7f\u7528\u771f\u5b9e\u533b\u7597\u6570\u636e\uff0c\u901a\u8fc7\u5fae\u8c03RoBERTa\u8fdb\u884c\u963b\u585e\u4efb\u52a1\uff0c\u5e76\u6d4b\u8bd5\u591a\u79cd\u8bed\u8a00\u6a21\u578b\uff08\u5982Mistral\uff09\u5728\u5339\u914d\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u5fae\u8c03\u6a21\u578b\u5728\u963b\u585e\u4efb\u52a1\u4e2d\u51cf\u5c1192%\u5019\u9009\u5bf9\uff0c\u5339\u914d\u4efb\u52a1\u4e2dMistral-7B\u8868\u73b0\u6700\u4f73\uff1b\u96f6\u6837\u672c\u6a21\u578b\u4e2dMistral-Small-24B\u6700\u4f18\u3002", "conclusion": "\u8bed\u8a00\u6a21\u578b\u4e3a\u60a3\u8005\u8bb0\u5f55\u94fe\u63a5\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u6df7\u5408\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u4e0a\u4ecd\u5177\u4f18\u52bf\u3002"}}
{"id": "2504.15133", "pdf": "https://arxiv.org/pdf/2504.15133", "abs": "https://arxiv.org/abs/2504.15133", "authors": ["Ziwen Xu", "Shuxun Wang", "Kewei Xu", "Haoming Xu", "Mengru Wang", "Xinle Deng", "Yunzhi Yao", "Guozhou Zheng", "Huajun Chen", "Ningyu Zhang"], "title": "EasyEdit2: An Easy-to-use Steering Framework for Editing Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.HC", "cs.LG"], "comment": "Work in progress. Demo:\n  https://zjunlp.github.io/project/EasyEdit2/video; code:\n  https://github.com/zjunlp/EasyEdit", "summary": "In this paper, we introduce EasyEdit2, a framework designed to enable\nplug-and-play adjustability for controlling Large Language Model (LLM)\nbehaviors. EasyEdit2 supports a wide range of test-time interventions,\nincluding safety, sentiment, personality, reasoning patterns, factuality, and\nlanguage features. Unlike its predecessor, EasyEdit2 features a new\narchitecture specifically designed for seamless model steering. It comprises\nkey modules such as the steering vector generator and the steering vector\napplier, which enable automatic generation and application of steering vectors\nto influence the model's behavior without modifying its parameters. One of the\nmain advantages of EasyEdit2 is its ease of use-users do not need extensive\ntechnical knowledge. With just a single example, they can effectively guide and\nadjust the model's responses, making precise control both accessible and\nefficient. Empirically, we report model steering performance across different\nLLMs, demonstrating the effectiveness of these techniques. We have released the\nsource code on GitHub at https://github.com/zjunlp/EasyEdit along with a\ndemonstration notebook. In addition, we provide a demo video at\nhttps://zjunlp.github.io/project/EasyEdit2/video for a quick introduction.", "AI": {"tldr": "EasyEdit2\u662f\u4e00\u4e2a\u7528\u4e8e\u63a7\u5236\u5927\u578b\u8bed\u8a00\u6a21\u578b\u884c\u4e3a\u7684\u6846\u67b6\uff0c\u652f\u6301\u591a\u79cd\u6d4b\u8bd5\u65f6\u5e72\u9884\uff0c\u65e0\u9700\u4fee\u6539\u6a21\u578b\u53c2\u6570\u5373\u53ef\u8c03\u6574\u884c\u4e3a\u3002", "motivation": "\u63d0\u4f9b\u4e00\u79cd\u7b80\u5355\u6613\u7528\u7684\u65b9\u6cd5\uff0c\u8ba9\u7528\u6237\u65e0\u9700\u6df1\u5165\u6280\u672f\u77e5\u8bc6\u5373\u53ef\u7cbe\u786e\u63a7\u5236LLM\u7684\u884c\u4e3a\u3002", "method": "\u91c7\u7528\u65b0\u7684\u67b6\u6784\uff0c\u5305\u62ec\u5bfc\u5411\u5411\u91cf\u751f\u6210\u5668\u548c\u5e94\u7528\u5668\uff0c\u901a\u8fc7\u5355\u4e00\u6837\u672c\u81ea\u52a8\u751f\u6210\u548c\u5e94\u7528\u5bfc\u5411\u5411\u91cf\u3002", "result": "\u5728\u4e0d\u540cLLM\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u7cbe\u786e\u7684\u884c\u4e3a\u63a7\u5236\u3002", "conclusion": "EasyEdit2\u4e3aLLM\u884c\u4e3a\u63a7\u5236\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u6613\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.14508", "pdf": "https://arxiv.org/pdf/2504.14508", "abs": "https://arxiv.org/abs/2504.14508", "authors": ["Sasan Tavakkol", "Max Springer", "Mohammadhossein Bateni", "Neslihan Bulut", "Vincent Cohen-Addad", "MohammadTaghi Hajiaghayi"], "title": "Less is More: Adaptive Coverage for Synthetic Training Data", "categories": ["cs.LG"], "comment": null, "summary": "Synthetic training data generation with Large Language Models (LLMs) like\nGoogle's Gemma and OpenAI's GPT offer a promising solution to the challenge of\nobtaining large, labeled datasets for training classifiers. When rapid model\ndeployment is critical, such as in classifying emerging social media trends or\ncombating new forms of online abuse tied to current events, the ability to\ngenerate training data is invaluable. While prior research has examined the\ncomparability of synthetic data to human-labeled data, this study introduces a\nnovel sampling algorithm, based on the maximum coverage problem, to select a\nrepresentative subset from a synthetically generated dataset. Our results\ndemonstrate that training a classifier on this contextually sampled subset\nachieves superior performance compared to training on the entire dataset. This\n\"less is more\" approach not only improves accuracy but also reduces the volume\nof data required, leading to potentially more efficient model fine-tuning.", "AI": {"tldr": "\u4f7f\u7528LLM\u751f\u6210\u5408\u6210\u8bad\u7ec3\u6570\u636e\uff0c\u5e76\u901a\u8fc7\u6700\u5927\u8986\u76d6\u95ee\u9898\u7b97\u6cd5\u9009\u62e9\u4ee3\u8868\u6027\u5b50\u96c6\uff0c\u63d0\u5347\u5206\u7c7b\u5668\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u5feb\u901f\u90e8\u7f72\u6a21\u578b\u65f6\u83b7\u53d6\u5927\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u65b0\u5174\u793e\u4ea4\u5a92\u4f53\u8d8b\u52bf\u6216\u5728\u7ebf\u6ee5\u7528\u5206\u7c7b\u4e2d\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u6700\u5927\u8986\u76d6\u95ee\u9898\u7684\u65b0\u91c7\u6837\u7b97\u6cd5\uff0c\u4ece\u5408\u6210\u6570\u636e\u4e2d\u9009\u62e9\u4ee3\u8868\u6027\u5b50\u96c6\u3002", "result": "\u5728\u5b50\u96c6\u4e0a\u8bad\u7ec3\u7684\u5206\u7c7b\u5668\u6027\u80fd\u4f18\u4e8e\u5168\u6570\u636e\u96c6\uff0c\u4e14\u6570\u636e\u91cf\u9700\u6c42\u51cf\u5c11\u3002", "conclusion": "\u201c\u5c11\u5373\u662f\u591a\u201d\u65b9\u6cd5\u63d0\u5347\u51c6\u786e\u6027\u5e76\u63d0\u9ad8\u6a21\u578b\u5fae\u8c03\u6548\u7387\u3002"}}
{"id": "2504.15275", "pdf": "https://arxiv.org/pdf/2504.15275", "abs": "https://arxiv.org/abs/2504.15275", "authors": ["Jie Cheng", "Ruixi Qiao", "Lijun Li", "Chao Guo", "Junle Wang", "Gang Xiong", "Yisheng Lv", "Fei-Yue Wang"], "title": "Stop Summation: Min-Form Credit Assignment Is All Process Reward Model Needs for Reasoning", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Process reward models (PRMs) have proven effective for test-time scaling of\nLarge Language Models (LLMs) on challenging reasoning tasks. However, reward\nhacking issues with PRMs limit their successful application in reinforcement\nfine-tuning. In this paper, we identify the main cause of PRM-induced reward\nhacking: the canonical summation-form credit assignment in reinforcement\nlearning (RL), which defines the value as cumulative gamma-decayed future\nrewards, easily induces LLMs to hack steps with high rewards. To address this,\nwe propose PURE: Process sUpervised Reinforcement lEarning. The key innovation\nof PURE is a min-form credit assignment that formulates the value function as\nthe minimum of future rewards. This method significantly alleviates reward\nhacking by limiting the value function range and distributing advantages more\nreasonably. Through extensive experiments on 3 base models, we show that\nPRM-based approaches enabling min-form credit assignment achieve comparable\nreasoning performance to verifiable reward-based methods within only 30% steps.\nIn contrast, the canonical sum-form credit assignment collapses training even\nat the beginning! Additionally, when we supplement PRM-based fine-tuning with\njust 10% verifiable rewards, we further alleviate reward hacking and produce\nthe best fine-tuned model based on Qwen2.5-Math-7B in our experiments,\nachieving 82.5% accuracy on AMC23 and 53.3% average accuracy across 5\nbenchmarks. Moreover, we summarize the observed reward hacking cases and\nanalyze the causes of training collapse. Code and models are available at\nhttps://github.com/CJReinforce/PURE.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faPURE\u65b9\u6cd5\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u672a\u6765\u5956\u52b1\u7684\u4fe1\u7528\u5206\u914d\u5f62\u5f0f\uff0c\u89e3\u51b3\u4e86PRM\u5728\u5f3a\u5316\u5fae\u8c03\u4e2d\u7684\u5956\u52b1\u9ed1\u5ba2\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u6027\u80fd\u3002", "motivation": "PRM\u5728\u5f3a\u5316\u5fae\u8c03\u4e2d\u5b58\u5728\u5956\u52b1\u9ed1\u5ba2\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5176\u5e94\u7528\u3002\u8bba\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51faPURE\u65b9\u6cd5\uff0c\u91c7\u7528\u6700\u5c0f\u5316\u672a\u6765\u5956\u52b1\u7684\u4fe1\u7528\u5206\u914d\u5f62\u5f0f\uff0c\u66ff\u4ee3\u4f20\u7edf\u7684\u7d2f\u52a0\u5f62\u5f0f\u3002", "result": "PURE\u572830%\u6b65\u9aa4\u5185\u8fbe\u5230\u4e0e\u53ef\u9a8c\u8bc1\u5956\u52b1\u65b9\u6cd5\u76f8\u5f53\u7684\u63a8\u7406\u6027\u80fd\uff0c\u5e76\u5728\u8865\u5145\u5c11\u91cf\u53ef\u9a8c\u8bc1\u5956\u52b1\u540e\u53d6\u5f97\u6700\u4f73\u5fae\u8c03\u6a21\u578b\u3002", "conclusion": "PURE\u6709\u6548\u7f13\u89e3\u5956\u52b1\u9ed1\u5ba2\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\uff0c\u4e3aPRM\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2504.15160", "pdf": "https://arxiv.org/pdf/2504.15160", "abs": "https://arxiv.org/abs/2504.15160", "authors": ["Joan C. Timoneda"], "title": "The Synthetic Imputation Approach: Generating Optimal Synthetic Texts For Underrepresented Categories In Supervised Classification Tasks", "categories": ["cs.CL"], "comment": null, "summary": "Encoder-decoder Large Language Models (LLMs), such as BERT and RoBERTa,\nrequire that all categories in an annotation task be sufficiently represented\nin the training data for optimal performance. However, it is often difficult to\nfind sufficient examples for all categories in a task when building a\nhigh-quality training set. In this article, I describe this problem and propose\na solution, the synthetic imputation approach. Leveraging a generative LLM\n(GPT-4o), this approach generates synthetic texts based on careful prompting\nand five original examples drawn randomly with replacement from the sample.\nThis approach ensures that new synthetic texts are sufficiently different from\nthe original texts to reduce overfitting, but retain the underlying substantive\nmeaning of the examples to maximize out-of-sample performance. With 75 original\nexamples or more, synthetic imputation's performance is on par with a full\nsample of original texts, and overfitting remains low, predictable and\ncorrectable with 50 original samples. The synthetic imputation approach\nprovides a novel role for generative LLMs in research and allows applied\nresearchers to balance their datasets for best performance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5408\u6210\u63d2\u8865\u65b9\u6cd5\uff0c\u5229\u7528\u751f\u6210\u5f0fLLM\uff08\u5982GPT-4o\uff09\u751f\u6210\u5408\u6210\u6587\u672c\uff0c\u4ee5\u89e3\u51b3\u8bad\u7ec3\u6570\u636e\u4e2d\u7c7b\u522b\u4e0d\u5e73\u8861\u7684\u95ee\u9898\u3002\u8be5\u65b9\u6cd5\u572875\u4e2a\u539f\u59cb\u6837\u672c\u65f6\u8868\u73b0\u4e0e\u5b8c\u6574\u6837\u672c\u76f8\u5f53\uff0c\u4e14\u8fc7\u62df\u5408\u53ef\u63a7\u3002", "motivation": "\u5728\u6784\u5efa\u9ad8\u8d28\u91cf\u8bad\u7ec3\u96c6\u65f6\uff0c\u96be\u4ee5\u786e\u4fdd\u6240\u6709\u7c7b\u522b\u90fd\u6709\u8db3\u591f\u7684\u6837\u672c\uff0c\u8fd9\u5f71\u54cd\u4e86\u7f16\u7801\u5668-\u89e3\u7801\u5668LLM\uff08\u5982BERT\u548cRoBERTa\uff09\u7684\u6027\u80fd\u3002", "method": "\u901a\u8fc7\u751f\u6210\u5f0fLLM\uff08GPT-4o\uff09\u751f\u6210\u5408\u6210\u6587\u672c\uff0c\u57fa\u4e8e5\u4e2a\u539f\u59cb\u6837\u672c\u7684\u968f\u673a\u66ff\u6362\u548c\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u63d0\u793a\uff0c\u786e\u4fdd\u5408\u6210\u6587\u672c\u4e0e\u539f\u59cb\u6587\u672c\u6709\u8db3\u591f\u5dee\u5f02\u4f46\u4fdd\u7559\u5b9e\u8d28\u610f\u4e49\u3002", "result": "\u572875\u4e2a\u539f\u59cb\u6837\u672c\u65f6\uff0c\u5408\u6210\u63d2\u8865\u65b9\u6cd5\u8868\u73b0\u4e0e\u5b8c\u6574\u6837\u672c\u76f8\u5f53\uff1b50\u4e2a\u6837\u672c\u65f6\u8fc7\u62df\u5408\u4f4e\u4e14\u53ef\u9884\u6d4b\u3002", "conclusion": "\u5408\u6210\u63d2\u8865\u65b9\u6cd5\u4e3a\u751f\u6210\u5f0fLLM\u5728\u7814\u7a76\u4e2d\u63d0\u4f9b\u4e86\u65b0\u89d2\u8272\uff0c\u5e2e\u52a9\u5e94\u7528\u7814\u7a76\u8005\u5e73\u8861\u6570\u636e\u96c6\u4ee5\u83b7\u5f97\u6700\u4f73\u6027\u80fd\u3002"}}
{"id": "2504.14514", "pdf": "https://arxiv.org/pdf/2504.14514", "abs": "https://arxiv.org/abs/2504.14514", "authors": ["Daizhan Cheng"], "title": "On Dimension-Free Transformer: An Application of STP to AI", "categories": ["cs.LG", "cs.AI", "cs.SY", "eess.SY"], "comment": null, "summary": "The matrix expressions for every parts of a transformer are firstly\ndescribed. Based on semi-tensor product (STP) of matrices the hypervectors are\nreconsidered and the linear transformation over hypervectors is constructed by\nusing projection. Its properties and calculating formulas are obtained. Using\nprojection-based transformation of hypervector (PBTH), the framework of\ndimension-free transformer (DFT) is proposed by verifying each linear\ntransformation in a transformer and replacing it by a proper PBTH, which allows\nthe inputs and outputs being of arbitrary dimensions. Using balanced\ninformation about all entries, DFT must be more efficient in dealing with\nsignals.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u534a\u5f20\u91cf\u79ef\u548c\u8d85\u5411\u91cf\u6295\u5f71\u53d8\u6362\u7684\u7ef4\u5ea6\u65e0\u5173\u53d8\u538b\u5668\uff08DFT\uff09\u6846\u67b6\uff0c\u652f\u6301\u4efb\u610f\u7ef4\u5ea6\u7684\u8f93\u5165\u8f93\u51fa\uff0c\u5e76\u8bc1\u660e\u5176\u5728\u4fe1\u53f7\u5904\u7406\u4e2d\u66f4\u9ad8\u6548\u3002", "motivation": "\u4f20\u7edf\u53d8\u538b\u5668\u5728\u5904\u7406\u4fe1\u53f7\u65f6\u53d7\u9650\u4e8e\u56fa\u5b9a\u7ef4\u5ea6\uff0c\u65e0\u6cd5\u7075\u6d3b\u9002\u5e94\u4e0d\u540c\u7ef4\u5ea6\u7684\u8f93\u5165\u8f93\u51fa\u3002", "method": "\u901a\u8fc7\u534a\u5f20\u91cf\u79ef\u91cd\u65b0\u5b9a\u4e49\u8d85\u5411\u91cf\uff0c\u5229\u7528\u6295\u5f71\u6784\u9020\u7ebf\u6027\u53d8\u6362\uff0c\u5e76\u9a8c\u8bc1\u6bcf\u4e2a\u7ebf\u6027\u53d8\u6362\uff0c\u7528\u6295\u5f71\u57fa\u8d85\u5411\u91cf\u53d8\u6362\uff08PBTH\uff09\u66ff\u4ee3\u3002", "result": "\u63d0\u51fa\u4e86\u7ef4\u5ea6\u65e0\u5173\u53d8\u538b\u5668\uff08DFT\uff09\u6846\u67b6\uff0c\u652f\u6301\u4efb\u610f\u7ef4\u5ea6\u7684\u8f93\u5165\u8f93\u51fa\u3002", "conclusion": "DFT\u5728\u5904\u7406\u4fe1\u53f7\u65f6\u66f4\u9ad8\u6548\uff0c\u56e0\u5176\u5e73\u8861\u4e86\u6240\u6709\u6761\u76ee\u7684\u4fe1\u606f\u3002"}}
{"id": "2504.13848", "pdf": "https://arxiv.org/pdf/2504.13848", "abs": "https://arxiv.org/abs/2504.13848", "authors": ["Janet Rafner", "Ryan Q. Guloy", "Eden W. Wen", "Catherine M. Chiodo", "Jacob Sherson"], "title": "From Interaction to Collaboration: How Hybrid Intelligence Enhances Chatbot Feedback", "categories": ["cs.HC", "cs.AI", "cs.CY", "cs.ET"], "comment": "14 pages", "summary": "Generative AI (GenAI) chatbots are becoming increasingly integrated into\nvirtual assistant technologies, yet their success hinges on the ability to\ngather meaningful user feedback to improve interaction quality, system\noutcomes, and overall user acceptance. Successful chatbot interactions can\nenable organizations to build long-term relationships with their customers and\nusers, supporting customer loyalty and furthering the organization's goals.\nThis study explores the impact of two distinct narratives and feedback\ncollection mechanisms on user engagement and feedback behavior: a standard\nAI-focused interaction versus a hybrid intelligence (HI) framed interaction.\nInitial findings indicate that while small-scale survey measures allowed for no\nsignificant differences in user willingness to leave feedback, use the system,\nor trust the system, participants exposed to the HI narrative statistically\nsignificantly provided more detailed feedback. These initial findings offer\ninsights into designing effective feedback systems for GenAI virtual\nassistants, balancing user effort with system improvement potential.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u4e24\u79cd\u53cd\u9988\u6536\u96c6\u673a\u5236\uff08\u6807\u51c6AI\u4e0e\u6df7\u5408\u667a\u80fdHI\uff09\u5bf9\u7528\u6237\u53cd\u9988\u884c\u4e3a\u7684\u5f71\u54cd\uff0c\u53d1\u73b0HI\u6846\u67b6\u4e0b\u7528\u6237\u63d0\u4f9b\u66f4\u8be6\u7ec6\u53cd\u9988\u3002", "motivation": "\u63d0\u5347GenAI\u865a\u62df\u52a9\u624b\u7684\u7528\u6237\u53cd\u9988\u8d28\u91cf\u4ee5\u4f18\u5316\u4ea4\u4e92\u548c\u7cfb\u7edf\u6210\u679c\u3002", "method": "\u6bd4\u8f83\u6807\u51c6AI\u4e0eHI\u6846\u67b6\u5bf9\u7528\u6237\u53cd\u9988\u884c\u4e3a\u7684\u5f71\u54cd\u3002", "result": "HI\u6846\u67b6\u663e\u8457\u589e\u52a0\u7528\u6237\u53cd\u9988\u7684\u8be6\u7ec6\u7a0b\u5ea6\uff0c\u4f46\u5bf9\u5176\u4ed6\u6307\u6807\u65e0\u663e\u8457\u5f71\u54cd\u3002", "conclusion": "HI\u6846\u67b6\u6709\u52a9\u4e8e\u8bbe\u8ba1\u66f4\u6709\u6548\u7684GenAI\u53cd\u9988\u7cfb\u7edf\u3002"}}
{"id": "2504.15168", "pdf": "https://arxiv.org/pdf/2504.15168", "abs": "https://arxiv.org/abs/2504.15168", "authors": ["Qilin Tian"], "title": "On true empty category", "categories": ["cs.CL"], "comment": null, "summary": "According to Chomsky (1981, 1986), empty categories consist of PRO, pro,\ntrace, and variable. However, some empty object positions seem to be\nincompatible with extant empty categories. Given this, Li (2007a, 2007b, 2014)\nand Li & Wei (2014) raise the true empty category hypothesis, which holds that\ntrue empty category is only an empty position with category and Case features.\nAs a last resort option, it is used mainly to meet the subcatgorization of a\nverb. This assumption is ingenious, and if proved to be true, it will exert a\ngreat impact on the study of UG. In this paper, we evaluate their evidence from\ntopicalization and demonstrate that it can be accounted for without invoking\ntrue empty category.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86Chomsky\u7684\u7a7a\u8bed\u7c7b\u7406\u8bba\uff0c\u5e76\u8bc4\u4f30\u4e86Li\u7b49\u4eba\u63d0\u51fa\u7684\u201c\u771f\u5b9e\u7a7a\u8bed\u7c7b\u201d\u5047\u8bbe\uff0c\u8ba4\u4e3a\u65e0\u9700\u5f15\u5165\u8be5\u5047\u8bbe\u5373\u53ef\u89e3\u91ca\u76f8\u5173\u73b0\u8c61\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u9a8c\u8bc1Li\u7b49\u4eba\u63d0\u51fa\u7684\u201c\u771f\u5b9e\u7a7a\u8bed\u7c7b\u201d\u5047\u8bbe\u662f\u5426\u5fc5\u8981\uff0c\u4ee5\u53ca\u662f\u5426\u53ef\u4ee5\u901a\u8fc7\u73b0\u6709\u7406\u8bba\u89e3\u91ca\u76f8\u5173\u8bed\u8a00\u73b0\u8c61\u3002", "method": "\u901a\u8fc7\u5206\u6790\u8bdd\u9898\u5316\u7684\u8bc1\u636e\uff0c\u8bc4\u4f30\u201c\u771f\u5b9e\u7a7a\u8bed\u7c7b\u201d\u5047\u8bbe\u7684\u5408\u7406\u6027\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u65e0\u9700\u5f15\u5165\u201c\u771f\u5b9e\u7a7a\u8bed\u7c7b\u201d\u5047\u8bbe\uff0c\u73b0\u6709\u7406\u8bba\u8db3\u4ee5\u89e3\u91ca\u8bdd\u9898\u5316\u73b0\u8c61\u3002", "conclusion": "\u7ed3\u8bba\u8ba4\u4e3a\u201c\u771f\u5b9e\u7a7a\u8bed\u7c7b\u201d\u5047\u8bbe\u5e76\u975e\u5fc5\u8981\uff0c\u73b0\u6709\u7a7a\u8bed\u7c7b\u7406\u8bba\u5df2\u8db3\u591f\u89e3\u91ca\u76f8\u5173\u8bed\u8a00\u73b0\u8c61\u3002"}}
{"id": "2504.14519", "pdf": "https://arxiv.org/pdf/2504.14519", "abs": "https://arxiv.org/abs/2504.14519", "authors": ["Zhouyang Li", "Yuliang Liu", "Wei Zhang", "Tailing Yuan", "Bin Chen", "Chengru Song", "Di Zhang"], "title": "SlimPipe: Memory-Thrifty and Efficient Pipeline Parallelism for Long-Context LLM Training", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Pipeline Parallelism (PP) serves as a crucial technique for training Large\nLanguage Models (LLMs), owing to its capability to alleviate memory pressure\nfrom model states with relatively low communication overhead. However, in\nlong-context scenarios, existing pipeline parallelism methods fail to address\nthe substantial activation memory pressure, primarily due to the peak memory\nconsumption resulting from the accumulation of activations across multiple\nmicrobatches. Moreover, these approaches inevitably introduce considerable\npipeline bubbles, further hindering efficiency.\n  To tackle these challenges, we propose SlimPipe, a novel approach to\nfine-grained pipeline parallelism that employs uniform sequence slicing coupled\nwith one-forward-one-backward (1F1B) schedule. It reduces the accumulated\nactivations from several microbatches to just one, which is split into several\nslices. Although the slices are evenly partitioned, the computation cost is not\nequal across slices due to causal attention. We develop a sophisticated\nworkload redistribution technique to address this load imbalance. SlimPipe\nachieves (1) near-zero memory overhead and (2) minimal pipeline bubbles\nsimultaneously. The effectiveness of SlimPipe has been proven by thorough\ntesting with diverse model architectures, context window sizes, and\nSlimPipe-specific configurations. For example, on the Llama 70B model, compared\nto state-of-the-art methods, SlimPipe significantly boosts the Model FLOPs\nUtilization (MFU) to up to $1.57\\times$ for a context length of 512K. More\nnotably, for a context length of 2048K, it maintains over 45% utilization on\n256 NVIDIA Hopper 80GB GPUs, while other approaches either suffer significant\nperformance drops or fail entirely due to memory constraints.", "AI": {"tldr": "SlimPipe\u662f\u4e00\u79cd\u7ec6\u7c92\u5ea6\u6d41\u6c34\u7ebf\u5e76\u884c\u65b9\u6cd5\uff0c\u901a\u8fc7\u5747\u5300\u5e8f\u5217\u5207\u7247\u548c1F1B\u8c03\u5ea6\uff0c\u663e\u8457\u51cf\u5c11\u6fc0\u6d3b\u5185\u5b58\u6d88\u8017\u548c\u6d41\u6c34\u7ebf\u6c14\u6ce1\uff0c\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u6d41\u6c34\u7ebf\u5e76\u884c\u65b9\u6cd5\u5728\u957f\u4e0a\u4e0b\u6587\u573a\u666f\u4e2d\u65e0\u6cd5\u89e3\u51b3\u6fc0\u6d3b\u5185\u5b58\u538b\u529b\u548c\u6d41\u6c34\u7ebf\u6c14\u6ce1\u95ee\u9898\uff0c\u9650\u5236\u4e86\u8bad\u7ec3\u6548\u7387\u3002", "method": "\u91c7\u7528\u5747\u5300\u5e8f\u5217\u5207\u7247\u548c1F1B\u8c03\u5ea6\uff0c\u7ed3\u5408\u8d1f\u8f7d\u5747\u8861\u6280\u672f\uff0c\u51cf\u5c11\u6fc0\u6d3b\u5185\u5b58\u548c\u6d41\u6c34\u7ebf\u6c14\u6ce1\u3002", "result": "\u5728Llama 70B\u6a21\u578b\u4e0a\uff0cSlimPipe\u5c06MFU\u63d0\u5347\u81f31.57\u500d\uff0c\u5e76\u57282048K\u4e0a\u4e0b\u6587\u957f\u5ea6\u4e0b\u4fdd\u630145%\u4ee5\u4e0a\u7684\u5229\u7528\u7387\u3002", "conclusion": "SlimPipe\u6709\u6548\u89e3\u51b3\u4e86\u957f\u4e0a\u4e0b\u6587\u8bad\u7ec3\u4e2d\u7684\u5185\u5b58\u548c\u6548\u7387\u95ee\u9898\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2504.13853", "pdf": "https://arxiv.org/pdf/2504.13853", "abs": "https://arxiv.org/abs/2504.13853", "authors": ["Pingfei Zhu", "Chenyang Zhao", "Haishi Zhao", "Bo Yang"], "title": "GenShin:geometry-enhanced structural graph embodies binding pose can better predicting compound-protein interaction affinity", "categories": ["q-bio.BM", "cs.AI"], "comment": "11 pages, 3 figures", "summary": "AI-powered drug discovery typically relies on the successful prediction of\ncompound-protein interactions, which are pivotal for the evaluation of designed\ncompound molecules in structure-based drug design and represent a core\nchallenge in the field.\n  However, accurately predicting compound-protein affinity via regression\nmodels usually requires adequate-binding pose, which are derived from costly\nand complex experimental methods or time-consuming simulations with docking\nsoftware. In response, we have introduced the GenShin model, which constructs a\ngeometry-enhanced structural graph module that separately extracts additional\nfeatures from proteins and compounds. Consequently, it attains an accuracy on\npar with mainstream models in predicting compound-protein affinities, while\neliminating the need for adequate-binding pose as input. Our experimental\nfindings demonstrate that the GenShin model vastly outperforms other models\nthat rely on non-input docking conformations, achieving, or in some cases even\nexceeding, the performance of those requiring adequate-binding pose. Further\nexperiments indicate that our GenShin model is more robust to\ninadequate-binding pose, affirming its higher suitability for real-world drug\ndiscovery scenarios. We hope our work will inspire more endeavors to bridge the\ngap between AI models and practical drug discovery challenges.", "AI": {"tldr": "GenShin\u6a21\u578b\u901a\u8fc7\u51e0\u4f55\u589e\u5f3a\u7ed3\u6784\u56fe\u6a21\u5757\uff0c\u65e0\u9700\u4f9d\u8d56\u7ed3\u5408\u6784\u8c61\u8f93\u5165\uff0c\u5373\u53ef\u9ad8\u7cbe\u5ea6\u9884\u6d4b\u5316\u5408\u7269-\u86cb\u767d\u8d28\u4eb2\u548c\u529b\uff0c\u4f18\u4e8e\u4e3b\u6d41\u6a21\u578b\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u6602\u8d35\u4e14\u8017\u65f6\u7684\u7ed3\u5408\u6784\u8c61\u5b9e\u9a8c\u6216\u6a21\u62df\uff0c\u9650\u5236\u4e86AI\u5728\u836f\u7269\u53d1\u73b0\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u6784\u5efa\u51e0\u4f55\u589e\u5f3a\u7ed3\u6784\u56fe\u6a21\u5757\uff0c\u5206\u522b\u63d0\u53d6\u86cb\u767d\u8d28\u548c\u5316\u5408\u7269\u7684\u7279\u5f81\uff0c\u65e0\u9700\u7ed3\u5408\u6784\u8c61\u8f93\u5165\u3002", "result": "GenShin\u6a21\u578b\u5728\u4eb2\u548c\u529b\u9884\u6d4b\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u751a\u81f3\u8d85\u8d8a\u4f9d\u8d56\u7ed3\u5408\u6784\u8c61\u7684\u6a21\u578b\uff0c\u4e14\u5bf9\u4e0d\u5b8c\u6574\u6784\u8c61\u66f4\u5177\u9c81\u68d2\u6027\u3002", "conclusion": "GenShin\u6a21\u578b\u4e3a\u5b9e\u9645\u836f\u7269\u53d1\u73b0\u63d0\u4f9b\u4e86\u66f4\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u671b\u63a8\u52a8AI\u4e0e\u836f\u7269\u53d1\u73b0\u7684\u7ed3\u5408\u3002"}}
{"id": "2504.15205", "pdf": "https://arxiv.org/pdf/2504.15205", "abs": "https://arxiv.org/abs/2504.15205", "authors": ["Nandan Thakur", "Ronak Pradeep", "Shivani Upadhyay", "Daniel Campos", "Nick Craswell", "Jimmy Lin"], "title": "Support Evaluation for the TREC 2024 RAG Track: Comparing Human versus LLM Judges", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "Accepted at SIGIR 2025 (short)", "summary": "Retrieval-augmented generation (RAG) enables large language models (LLMs) to\ngenerate answers with citations from source documents containing \"ground\ntruth\", thereby reducing system hallucinations. A crucial factor in RAG\nevaluation is \"support\", whether the information in the cited documents\nsupports the answer. To this end, we conducted a large-scale comparative study\nof 45 participant submissions on 36 topics to the TREC 2024 RAG Track,\ncomparing an automatic LLM judge (GPT-4o) against human judges for support\nassessment. We considered two conditions: (1) fully manual assessments from\nscratch and (2) manual assessments with post-editing of LLM predictions. Our\nresults indicate that for 56% of the manual from-scratch assessments, human and\nGPT-4o predictions match perfectly (on a three-level scale), increasing to 72%\nin the manual with post-editing condition. Furthermore, by carefully analyzing\nthe disagreements in an unbiased study, we found that an independent human\njudge correlates better with GPT-4o than a human judge, suggesting that LLM\njudges can be a reliable alternative for support assessment. To conclude, we\nprovide a qualitative analysis of human and GPT-4o errors to help guide future\niterations of support assessment.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u4e2d\u5f15\u7528\u6587\u6863\u5bf9\u7b54\u6848\u7684\u652f\u6301\u6027\u8bc4\u4f30\uff0c\u6bd4\u8f83\u4e86GPT-4o\u4e0e\u4eba\u7c7b\u8bc4\u59d4\u7684\u8868\u73b0\uff0c\u53d1\u73b0GPT-4o\u5728\u652f\u6301\u6027\u8bc4\u4f30\u4e2d\u8868\u73b0\u53ef\u9760\u3002", "motivation": "\u8bc4\u4f30RAG\u7cfb\u7edf\u4e2d\u5f15\u7528\u6587\u6863\u5bf9\u7b54\u6848\u7684\u652f\u6301\u6027\uff0c\u4ee5\u9a8c\u8bc1\u81ea\u52a8LLM\u8bc4\u59d4\uff08GPT-4o\uff09\u662f\u5426\u53ef\u4ee5\u66ff\u4ee3\u4eba\u7c7b\u8bc4\u59d4\u3002", "method": "\u5bf9TREC 2024 RAG Track\u768445\u4efd\u63d0\u4ea4\u548c36\u4e2a\u4e3b\u9898\u8fdb\u884c\u5927\u89c4\u6a21\u6bd4\u8f83\u7814\u7a76\uff0c\u6bd4\u8f83GPT-4o\u4e0e\u4eba\u7c7b\u8bc4\u59d4\u5728\u652f\u6301\u6027\u8bc4\u4f30\u4e2d\u7684\u8868\u73b0\uff0c\u5305\u62ec\u5b8c\u5168\u624b\u52a8\u8bc4\u4f30\u548c\u624b\u52a8\u540e\u7f16\u8f91LLM\u9884\u6d4b\u4e24\u79cd\u6761\u4ef6\u3002", "result": "56%\u7684\u5b8c\u5168\u624b\u52a8\u8bc4\u4f30\u4e2d\uff0c\u4eba\u7c7b\u4e0eGPT-4o\u8bc4\u5206\u5b8c\u5168\u4e00\u81f4\uff0c\u540e\u7f16\u8f91\u6761\u4ef6\u4e0b\u63d0\u5347\u81f372%\u3002\u72ec\u7acb\u4eba\u7c7b\u8bc4\u59d4\u4e0eGPT-4o\u7684\u76f8\u5173\u6027\u66f4\u9ad8\u3002", "conclusion": "GPT-4o\u5728\u652f\u6301\u6027\u8bc4\u4f30\u4e2d\u8868\u73b0\u53ef\u9760\uff0c\u53ef\u4f5c\u4e3a\u4eba\u7c7b\u8bc4\u59d4\u7684\u66ff\u4ee3\u65b9\u6848\u3002\u672a\u6765\u9700\u8fdb\u4e00\u6b65\u5206\u6790\u9519\u8bef\u4ee5\u6539\u8fdb\u8bc4\u4f30\u65b9\u6cd5\u3002"}}
{"id": "2504.14545", "pdf": "https://arxiv.org/pdf/2504.14545", "abs": "https://arxiv.org/abs/2504.14545", "authors": ["Fei Zhu", "Zhaoxiang Zhang"], "title": "TrustLoRA: Low-Rank Adaptation for Failure Detection under Out-of-distribution Data", "categories": ["cs.LG"], "comment": null, "summary": "Reliable prediction is an essential requirement for deep neural models that\nare deployed in open environments, where both covariate and semantic\nout-of-distribution (OOD) data arise naturally. In practice, to make safe\ndecisions, a reliable model should accept correctly recognized inputs while\nrejecting both those misclassified covariate-shifted and semantic-shifted\nexamples. Besides, considering the potential existing trade-off between\nrejecting different failure cases, more convenient, controllable, and flexible\nfailure detection approaches are needed. To meet the above requirements, we\npropose a simple failure detection framework to unify and facilitate\nclassification with rejection under both covariate and semantic shifts. Our key\ninsight is that by separating and consolidating failure-specific reliability\nknowledge with low-rank adapters and then integrating them, we can enhance the\nfailure detection ability effectively and flexibly. Extensive experiments\ndemonstrate the superiority of our framework.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u7684\u5931\u8d25\u68c0\u6d4b\u6846\u67b6\uff0c\u7edf\u4e00\u5e76\u4f18\u5316\u4e86\u5728\u534f\u53d8\u91cf\u548c\u8bed\u4e49\u504f\u79fb\u4e0b\u7684\u5206\u7c7b\u4e0e\u62d2\u7edd\u4efb\u52a1\u3002", "motivation": "\u5728\u5f00\u653e\u73af\u5883\u4e2d\uff0c\u53ef\u9760\u7684\u9884\u6d4b\u5bf9\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u81f3\u5173\u91cd\u8981\uff0c\u9700\u8981\u540c\u65f6\u62d2\u7edd\u534f\u53d8\u91cf\u504f\u79fb\u548c\u8bed\u4e49\u504f\u79fb\u7684\u9519\u8bef\u6848\u4f8b\u3002", "method": "\u901a\u8fc7\u5206\u79bb\u548c\u6574\u5408\u5931\u8d25\u7279\u5b9a\u7684\u53ef\u9760\u6027\u77e5\u8bc6\uff0c\u5229\u7528\u4f4e\u79e9\u9002\u914d\u5668\u589e\u5f3a\u5931\u8d25\u68c0\u6d4b\u80fd\u529b\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u6846\u67b6\u7684\u4f18\u8d8a\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u4e14\u7075\u6d3b\u5730\u63d0\u5347\u4e86\u5931\u8d25\u68c0\u6d4b\u80fd\u529b\u3002"}}
{"id": "2504.13856", "pdf": "https://arxiv.org/pdf/2504.13856", "abs": "https://arxiv.org/abs/2504.13856", "authors": ["Andrew Silva", "Pradyumna Tambwekar", "Mariah Schrum", "Matthew Gombolay"], "title": "Towards Balancing Preference and Performance through Adaptive Personalized Explainability", "categories": ["cs.HC", "cs.AI", "cs.RO"], "comment": "20 pages, 19 figures, HRI 2024", "summary": "As robots and digital assistants are deployed in the real world, these agents\nmust be able to communicate their decision-making criteria to build trust,\nimprove human-robot teaming, and enable collaboration. While the field of\nexplainable artificial intelligence (xAI) has made great strides to enable such\ncommunication, these advances often assume that one xAI approach is ideally\nsuited to each problem (e.g., decision trees to explain how to triage patients\nin an emergency or feature-importance maps to explain radiology reports). This\nfails to recognize that users have diverse experiences or preferences for\ninteraction modalities. In this work, we present two user-studies set in a\nsimulated autonomous vehicle (AV) domain. We investigate (1) population-level\npreferences for xAI and (2) personalization strategies for providing robot\nexplanations. We find significant differences between xAI modes (language\nexplanations, feature-importance maps, and decision trees) in both preference\n(p < 0.01) and performance (p < 0.05). We also observe that a participant's\npreferences do not always align with their performance, motivating our\ndevelopment of an adaptive personalization strategy to balance the two. We show\nthat this strategy yields significant performance gains (p < 0.05), and we\nconclude with a discussion of our findings and implications for xAI in\nhuman-robot interactions.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u7528\u6237\u5bf9\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd\uff08xAI\uff09\u6a21\u5f0f\u7684\u504f\u597d\u548c\u6027\u80fd\u5dee\u5f02\uff0c\u5e76\u63d0\u51fa\u4e86\u81ea\u9002\u5e94\u4e2a\u6027\u5316\u7b56\u7565\u4ee5\u5e73\u8861\u504f\u597d\u4e0e\u6027\u80fd\u3002", "motivation": "\u968f\u7740\u673a\u5668\u4eba\u548c\u6570\u5b57\u52a9\u624b\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u5982\u4f55\u901a\u8fc7xAI\u589e\u5f3a\u4fe1\u4efb\u548c\u534f\u4f5c\u6210\u4e3a\u5173\u952e\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u672a\u5145\u5206\u8003\u8651\u7528\u6237\u591a\u6837\u5316\u7684\u9700\u6c42\u548c\u504f\u597d\u3002", "method": "\u901a\u8fc7\u4e24\u9879\u7528\u6237\u7814\u7a76\uff0c\u5728\u6a21\u62df\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\uff08AV\uff09\u73af\u5883\u4e2d\u6bd4\u8f83\u4e0d\u540cxAI\u6a21\u5f0f\uff08\u8bed\u8a00\u89e3\u91ca\u3001\u7279\u5f81\u91cd\u8981\u6027\u56fe\u548c\u51b3\u7b56\u6811\uff09\u7684\u504f\u597d\u548c\u6027\u80fd\u3002", "result": "\u53d1\u73b0xAI\u6a21\u5f0f\u5728\u504f\u597d\u548c\u6027\u80fd\u4e0a\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u4e14\u7528\u6237\u504f\u597d\u4e0e\u6027\u80fd\u5e76\u4e0d\u603b\u4e00\u81f4\uff1b\u63d0\u51fa\u7684\u81ea\u9002\u5e94\u7b56\u7565\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "conclusion": "\u81ea\u9002\u5e94\u4e2a\u6027\u5316\u7b56\u7565\u80fd\u6709\u6548\u5e73\u8861\u7528\u6237\u504f\u597d\u4e0e\u6027\u80fd\uff0c\u5bf9xAI\u5728\u4eba\u673a\u4ea4\u4e92\u4e2d\u7684\u5e94\u7528\u5177\u6709\u91cd\u8981\u542f\u793a\u3002"}}
{"id": "2504.15219", "pdf": "https://arxiv.org/pdf/2504.15219", "abs": "https://arxiv.org/abs/2504.15219", "authors": ["Manya Wadhwa", "Zayne Sprague", "Chaitanya Malaviya", "Philippe Laban", "Junyi Jessy Li", "Greg Durrett"], "title": "EvalAgent: Discovering Implicit Evaluation Criteria from the Web", "categories": ["cs.CL"], "comment": null, "summary": "Evaluation of language model outputs on structured writing tasks is typically\nconducted with a number of desirable criteria presented to human evaluators or\nlarge language models (LLMs). For instance, on a prompt like \"Help me draft an\nacademic talk on coffee intake vs research productivity\", a model response may\nbe evaluated for criteria like accuracy and coherence. However, high-quality\nresponses should do more than just satisfy basic task requirements. An\neffective response to this query should include quintessential features of an\nacademic talk, such as a compelling opening, clear research questions, and a\ntakeaway. To help identify these implicit criteria, we introduce EvalAgent, a\nnovel framework designed to automatically uncover nuanced and task-specific\ncriteria. EvalAgent first mines expert-authored online guidance. It then uses\nthis evidence to propose diverse, long-tail evaluation criteria that are\ngrounded in reliable external sources. Our experiments demonstrate that the\ngrounded criteria produced by EvalAgent are often implicit (not directly stated\nin the user's prompt), yet specific (high degree of lexical precision).\nFurther, EvalAgent criteria are often not satisfied by initial responses but\nthey are actionable, such that responses can be refined to satisfy them.\nFinally, we show that combining LLM-generated and EvalAgent criteria uncovers\nmore human-valued criteria than using LLMs alone.", "AI": {"tldr": "EvalAgent\u662f\u4e00\u4e2a\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u53d1\u73b0\u4efb\u52a1\u7279\u5b9a\u7684\u9690\u5f0f\u8bc4\u4f30\u6807\u51c6\uff0c\u7ed3\u5408\u4e13\u5bb6\u6307\u5bfc\u548cLLM\u751f\u6210\u7684\u6807\u51c6\uff0c\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u8f93\u51fa\u7684\u8d28\u91cf\u3002", "motivation": "\u4f20\u7edf\u8bc4\u4f30\u65b9\u6cd5\u4ec5\u5173\u6ce8\u57fa\u672c\u4efb\u52a1\u8981\u6c42\uff0c\u800c\u9ad8\u8d28\u91cf\u54cd\u5e94\u9700\u6ee1\u8db3\u66f4\u591a\u9690\u5f0f\u6807\u51c6\uff0c\u5982\u5b66\u672f\u6f14\u8bb2\u7684\u5178\u578b\u7279\u5f81\u3002", "method": "EvalAgent\u901a\u8fc7\u6316\u6398\u4e13\u5bb6\u5728\u7ebf\u6307\u5bfc\uff0c\u63d0\u51fa\u591a\u6837\u5316\u7684\u3001\u57fa\u4e8e\u5916\u90e8\u6765\u6e90\u7684\u8bc4\u4f30\u6807\u51c6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cEvalAgent\u751f\u6210\u7684\u6807\u51c6\u5177\u6709\u9690\u5f0f\u6027\u548c\u7279\u5f02\u6027\uff0c\u4e14\u53ef\u901a\u8fc7\u4f18\u5316\u54cd\u5e94\u6ee1\u8db3\u3002\u7ed3\u5408LLM\u6807\u51c6\u80fd\u53d1\u73b0\u66f4\u591a\u4eba\u7c7b\u91cd\u89c6\u7684\u6807\u51c6\u3002", "conclusion": "EvalAgent\u4e3a\u8bed\u8a00\u6a21\u578b\u8f93\u51fa\u8bc4\u4f30\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u3001\u4efb\u52a1\u7279\u5b9a\u7684\u6807\u51c6\uff0c\u63d0\u5347\u4e86\u8bc4\u4f30\u7684\u6df1\u5ea6\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2504.14569", "pdf": "https://arxiv.org/pdf/2504.14569", "abs": "https://arxiv.org/abs/2504.14569", "authors": ["Lawrence Liu", "Inesh Chakrabarti", "Yixiao Li", "Mengdi Wang", "Tuo Zhao", "Lin F. Yang"], "title": "NoWag: A Unified Framework for Shape Preserving Compression of Large Language Models", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) exhibit remarkable performance across various\nnatural language processing tasks but suffer from immense computational and\nmemory demands, limiting their deployment in resource-constrained environments.\nTo address this challenge, we propose NoWag: (Normalized Weight and Activation\nGuided Compression), a unified framework for zero-shot shape preserving\ncompression algorithms. We compressed Llama-2 7B/13B/70B and Llama-3 8/70BB\nmodels, using two popular forms of shape-preserving compression, vector\nquantization NoWag-VQ (NoWag for Vector Quantization), and\nunstructured/semi-structured pruning NoWag-P (NoWag for Pruning). We found that\nNoWag-VQ significantly outperforms state-of-the-art zero shot VQ, and that\nNoWag-P performs competitively against state-of-the-art methods. These results\nsuggest commonalities between these compression paradigms that could inspire\nfuture work. Our code is available at https://github.com/LawrenceRLiu/NoWag", "AI": {"tldr": "NoWag\u662f\u4e00\u79cd\u7528\u4e8e\u96f6\u6837\u672c\u5f62\u72b6\u4fdd\u6301\u538b\u7f29\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u8ba1\u7b97\u548c\u5185\u5b58\u9700\u6c42\u4e0a\u5b58\u5728\u9650\u5236\uff0c\u96be\u4ee5\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u90e8\u7f72\u3002", "method": "\u63d0\u51faNoWag\u6846\u67b6\uff0c\u5305\u62ec\u5411\u91cf\u91cf\u5316\uff08NoWag-VQ\uff09\u548c\u526a\u679d\uff08NoWag-P\uff09\u4e24\u79cd\u538b\u7f29\u65b9\u6cd5\u3002", "result": "NoWag-VQ\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u96f6\u6837\u672cVQ\u65b9\u6cd5\uff0cNoWag-P\u4e0e\u73b0\u6709\u65b9\u6cd5\u7ade\u4e89\u6027\u76f8\u5f53\u3002", "conclusion": "NoWag\u6846\u67b6\u5c55\u793a\u4e86\u538b\u7f29\u8303\u5f0f\u7684\u5171\u6027\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u542f\u53d1\u3002"}}
{"id": "2504.13858", "pdf": "https://arxiv.org/pdf/2504.13858", "abs": "https://arxiv.org/abs/2504.13858", "authors": ["Felix Haag"], "title": "The Effect of Explainable AI-based Decision Support on Human Task Performance: A Meta-Analysis", "categories": ["cs.HC", "cs.AI"], "comment": "Published in the Proceedings of the Twenty-Third Annual Pre-ICIS\n  Workshop on HCI Research in MIS, Bangkok, Thailand, December 15th, 2024", "summary": "The desirable properties of explanations in information systems have fueled\nthe demands for transparency in artificial intelligence (AI) outputs. To\naddress these demands, the field of explainable AI (XAI) has put forth methods\nthat can support human decision-making by explaining AI outputs. However,\ncurrent empirical works present inconsistent findings on whether such\nexplanations help to improve users' task performance in decision support\nsystems (DSS). In this paper, we conduct a meta-analysis to explore how XAI\naffects human performance in classification tasks. Our results show an\nimprovement in task performance through XAI-based decision support, though\nexplanations themselves are not the decisive driver for this improvement. The\nanalysis reveals that the studies' risk of bias moderates the effect of\nexplanations in AI, while the explanation type appears to play only a\nnegligible role. Our findings contribute to the human computer interaction\nfield by enhancing the understanding of human-XAI collaboration in DSS.", "AI": {"tldr": "\u8bba\u6587\u901a\u8fc7\u5143\u5206\u6790\u63a2\u8ba8\u4e86\u53ef\u89e3\u91caAI\uff08XAI\uff09\u5bf9\u5206\u7c7b\u4efb\u52a1\u4e2d\u4eba\u7c7b\u8868\u73b0\u7684\u5f71\u54cd\uff0c\u53d1\u73b0XAI\u80fd\u63d0\u5347\u4efb\u52a1\u8868\u73b0\uff0c\u4f46\u89e3\u91ca\u672c\u8eab\u5e76\u975e\u51b3\u5b9a\u6027\u56e0\u7d20\uff0c\u7814\u7a76\u504f\u5dee\u548c\u89e3\u91ca\u7c7b\u578b\u7684\u5f71\u54cd\u8f83\u5c0f\u3002", "motivation": "\u4fe1\u606f\u7cfb\u7edf\u4e2d\u89e3\u91ca\u7684\u900f\u660e\u6027\u9700\u6c42\u63a8\u52a8\u4e86\u53ef\u89e3\u91caAI\uff08XAI\uff09\u7684\u53d1\u5c55\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u5bf9XAI\u662f\u5426\u63d0\u5347\u7528\u6237\u4efb\u52a1\u8868\u73b0\u5b58\u5728\u4e0d\u4e00\u81f4\u7ed3\u8bba\uff0c\u56e0\u6b64\u9700\u8981\u7cfb\u7edf\u6027\u5206\u6790\u3002", "method": "\u91c7\u7528\u5143\u5206\u6790\u65b9\u6cd5\uff0c\u7efc\u5408\u8bc4\u4f30XAI\u5bf9\u4eba\u7c7b\u5728\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u7684\u5f71\u54cd\uff0c\u5e76\u5206\u6790\u7814\u7a76\u504f\u5dee\u548c\u89e3\u91ca\u7c7b\u578b\u7684\u8c03\u8282\u4f5c\u7528\u3002", "result": "XAI\u80fd\u63d0\u5347\u4efb\u52a1\u8868\u73b0\uff0c\u4f46\u89e3\u91ca\u672c\u8eab\u5e76\u975e\u4e3b\u8981\u9a71\u52a8\u56e0\u7d20\uff1b\u7814\u7a76\u504f\u5dee\u5bf9XAI\u6548\u679c\u6709\u663e\u8457\u8c03\u8282\u4f5c\u7528\uff0c\u800c\u89e3\u91ca\u7c7b\u578b\u5f71\u54cd\u8f83\u5c0f\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u589e\u8fdb\u4e86\u4eba\u673a\u4ea4\u4e92\u9886\u57df\u5bf9\u4eba\u7c7b\u4e0eXAI\u534f\u4f5c\u7684\u7406\u89e3\uff0c\u5f3a\u8c03\u4e86\u7814\u7a76\u8bbe\u8ba1\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2504.15220", "pdf": "https://arxiv.org/pdf/2504.15220", "abs": "https://arxiv.org/abs/2504.15220", "authors": ["Juli\u00e1n Cendrero", "Julio Gonzalo", "Ivar Zapata"], "title": "Fully Bayesian Approaches to Topics over Time", "categories": ["cs.CL", "cs.LG"], "comment": "25 pages", "summary": "The Topics over Time (ToT) model captures thematic changes in timestamped\ndatasets by explicitly modeling publication dates jointly with word\nco-occurrence patterns. However, ToT was not approached in a fully Bayesian\nfashion, a flaw that makes it susceptible to stability problems. To address\nthis issue, we propose a fully Bayesian Topics over Time (BToT) model via the\nintroduction of a conjugate prior to the Beta distribution. This prior acts as\na regularization that prevents the online version of the algorithm from\nunstable updates when a topic is poorly represented in a mini-batch. The\ncharacteristics of this prior to the Beta distribution are studied here for the\nfirst time. Still, this model suffers from a difference in scale between the\nsingle-time observations and the multiplicity of words per document. A\nvariation of BToT, Weighted Bayesian Topics over Time (WBToT), is proposed as a\nsolution. In WBToT, publication dates are repeated a certain number of times\nper document, which balances the relative influence of words and timestamps\nalong the inference process. We have tested our models on two datasets: a\ncollection of over 200 years of US state-of-the-union (SOTU) addresses and a\nlarge-scale COVID-19 Twitter corpus of 10 million tweets. The results show that\nWBToT captures events better than Latent Dirichlet Allocation and other SOTA\ntopic models like BERTopic: the median absolute deviation of the topic presence\nover time is reduced by $51\\%$ and $34\\%$, respectively. Our experiments also\ndemonstrate the superior coherence of WBToT over BToT, which highlights the\nimportance of balancing the time and word modalities. Finally, we illustrate\nthe stability of the online optimization algorithm in WBToT, which allows the\napplication of WBToT to problems that are intractable for standard ToT.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u5b8c\u5168\u8d1d\u53f6\u65af\u7684Topics over Time\uff08BToT\uff09\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u539fToT\u6a21\u578b\u7684\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u5e76\u8fdb\u4e00\u6b65\u63d0\u51fa\u52a0\u6743\u7248\u672cWBToT\u4ee5\u5e73\u8861\u65f6\u95f4\u548c\u8bcd\u6a21\u6001\u7684\u5f71\u54cd\u3002\u5b9e\u9a8c\u8868\u660eWBToT\u5728\u4e8b\u4ef6\u6355\u6349\u548c\u4e3b\u9898\u4e00\u81f4\u6027\u4e0a\u4f18\u4e8e\u5176\u4ed6\u6a21\u578b\u3002", "motivation": "\u539fToT\u6a21\u578b\u672a\u91c7\u7528\u5b8c\u5168\u8d1d\u53f6\u65af\u65b9\u6cd5\uff0c\u5bfc\u81f4\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u4e14\u65f6\u95f4\u4e0e\u8bcd\u6a21\u6001\u7684\u5c3a\u5ea6\u5dee\u5f02\u672a\u88ab\u89e3\u51b3\u3002", "method": "\u5f15\u5165Beta\u5206\u5e03\u7684\u5171\u8f6d\u5148\u9a8c\u4f5c\u4e3a\u6b63\u5219\u5316\uff0c\u63d0\u51faBToT\uff1b\u8fdb\u4e00\u6b65\u63d0\u51faWBToT\uff0c\u901a\u8fc7\u91cd\u590d\u6587\u6863\u53d1\u5e03\u65f6\u95f4\u5e73\u8861\u6a21\u6001\u5f71\u54cd\u3002", "result": "WBToT\u5728SOTU\u548cCOVID-19\u63a8\u6587\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4e3b\u9898\u65f6\u95f4\u504f\u5dee\u5206\u522b\u964d\u4f4e51%\u548c34%\uff0c\u4e14\u4e3b\u9898\u4e00\u81f4\u6027\u66f4\u9ad8\u3002", "conclusion": "WBToT\u89e3\u51b3\u4e86ToT\u7684\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u5e73\u8861\u4e86\u65f6\u95f4\u548c\u8bcd\u6a21\u6001\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u5728\u7ebf\u5b66\u4e60\u4efb\u52a1\u3002"}}
{"id": "2504.14572", "pdf": "https://arxiv.org/pdf/2504.14572", "abs": "https://arxiv.org/abs/2504.14572", "authors": ["Steve Hanneke", "Shay Moran", "Alexander Shlimovich", "Amir Yehudayoff"], "title": "Data Selection for ERMs", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "Learning theory has traditionally followed a model-centric approach, focusing\non designing optimal algorithms for a fixed natural learning task (e.g., linear\nclassification or regression). In this paper, we adopt a complementary\ndata-centric perspective, whereby we fix a natural learning rule and focus on\noptimizing the training data. Specifically, we study the following question:\ngiven a learning rule $\\mathcal{A}$ and a data selection budget $n$, how well\ncan $\\mathcal{A}$ perform when trained on at most $n$ data points selected from\na population of $N$ points? We investigate when it is possible to select $n \\ll\nN$ points and achieve performance comparable to training on the entire\npopulation.\n  We address this question across a variety of empirical risk minimizers. Our\nresults include optimal data-selection bounds for mean estimation, linear\nclassification, and linear regression. Additionally, we establish two general\nresults: a taxonomy of error rates in binary classification and in stochastic\nconvex optimization. Finally, we propose several open questions and directions\nfor future research.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u4ece\u6570\u636e\u4e3a\u4e2d\u5fc3\u7684\u89d2\u5ea6\u4f18\u5316\u8bad\u7ec3\u6570\u636e\uff0c\u7814\u7a76\u4e86\u5728\u6709\u9650\u6570\u636e\u9009\u62e9\u9884\u7b97\u4e0b\u5982\u4f55\u901a\u8fc7\u9009\u62e9\u5c11\u91cf\u6570\u636e\u70b9\u8fbe\u5230\u4e0e\u5168\u91cf\u6570\u636e\u76f8\u5f53\u7684\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u5b66\u4e60\u7406\u8bba\u4fa7\u91cd\u4e8e\u6a21\u578b\u8bbe\u8ba1\uff0c\u800c\u672c\u6587\u5173\u6ce8\u5982\u4f55\u901a\u8fc7\u4f18\u5316\u6570\u636e\u9009\u62e9\u63d0\u5347\u5b66\u4e60\u89c4\u5219\u7684\u6548\u679c\u3002", "method": "\u7814\u7a76\u4e86\u591a\u79cd\u7ecf\u9a8c\u98ce\u9669\u6700\u5c0f\u5316\u65b9\u6cd5\uff0c\u5305\u62ec\u5747\u503c\u4f30\u8ba1\u3001\u7ebf\u6027\u5206\u7c7b\u548c\u7ebf\u6027\u56de\u5f52\uff0c\u5e76\u63d0\u51fa\u4e86\u6570\u636e\u9009\u62e9\u7684\u7406\u8bba\u754c\u9650\u3002", "result": "\u5f97\u51fa\u4e86\u5747\u503c\u4f30\u8ba1\u3001\u7ebf\u6027\u5206\u7c7b\u548c\u7ebf\u6027\u56de\u5f52\u7684\u6700\u4f18\u6570\u636e\u9009\u62e9\u754c\u9650\uff0c\u5e76\u5efa\u7acb\u4e86\u4e8c\u5143\u5206\u7c7b\u548c\u968f\u673a\u51f8\u4f18\u5316\u7684\u9519\u8bef\u7387\u5206\u7c7b\u4f53\u7cfb\u3002", "conclusion": "\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\uff0c\u5f3a\u8c03\u4e86\u6570\u636e\u9009\u62e9\u5728\u63d0\u5347\u5b66\u4e60\u6027\u80fd\u4e2d\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2504.13859", "pdf": "https://arxiv.org/pdf/2504.13859", "abs": "https://arxiv.org/abs/2504.13859", "authors": ["Phillip Driscoll", "Priyanka Kumar"], "title": "DoYouTrustAI: A Tool to Teach Students About AI Misinformation and Prompt Engineering", "categories": ["cs.HC", "cs.AI", "cs.CY"], "comment": null, "summary": "AI, especially Large Language Models (LLMs) like ChatGPT, have rapidly\ndeveloped and gained widespread adoption in the past five years, shifting user\npreference from traditional search engines. However, the generative nature of\nLLMs raises concerns about presenting misinformation as fact. To address this,\nwe developed a web-based application that helps K-12 students enhance critical\nthinking by identifying misleading information in LLM responses about major\nhistorical figures. In this paper, we describe the implementation and design\ndetails of the DoYouTrustAI tool, which can be used to provide an interactive\nlesson which teaches students about the dangers of misinformation and how\nbelievable generative AI can make it seem. The DoYouTrustAI tool utilizes\nprompt engineering to present the user with AI generated summaries about the\nlife of a historical figure. These summaries can be either accurate accounts of\nthat persons life, or an intentionally misleading alteration of their history.\nThe user is tasked with determining the validity of the statement without\nexternal resources. Our research questions for this work were:(RQ1) How can we\ndesign a tool that teaches students about the dangers of misleading information\nand of how misinformation can present itself in LLM responses? (RQ2) Can we\npresent prompt engineering as a topic that is easily understandable for\nstudents? Our findings highlight the need to correct misleading information\nbefore users retain it. Our tool lets users select familiar individuals for\ntesting to reduce random guessing and presents misinformation alongside known\nfacts to maintain believability. It also provides pre-configured prompt\ninstructions to show how different prompts affect AI responses. Together, these\nfeatures create a controlled environment where users learn the importance of\nverifying AI responses and understanding prompt engineering.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u6b3e\u540d\u4e3aDoYouTrustAI\u7684\u7f51\u9875\u5de5\u5177\uff0c\u5e2e\u52a9K-12\u5b66\u751f\u8bc6\u522b\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u751f\u6210\u7684\u5386\u53f2\u4eba\u7269\u4fe1\u606f\u4e2d\u7684\u8bef\u5bfc\u6027\u5185\u5bb9\uff0c\u4ee5\u63d0\u5347\u6279\u5224\u6027\u601d\u7ef4\u3002", "motivation": "\u7531\u4e8eLLM\u53ef\u80fd\u751f\u6210\u8bef\u5bfc\u6027\u4fe1\u606f\uff0c\u9700\u8981\u6559\u80b2\u5b66\u751f\u8bc6\u522b\u548c\u9a8c\u8bc1AI\u751f\u6210\u5185\u5bb9\u7684\u771f\u5b9e\u6027\u3002", "method": "\u901a\u8fc7\u63d0\u793a\u5de5\u7a0b\u751f\u6210\u5386\u53f2\u4eba\u7269\u7684\u51c6\u786e\u6216\u8bef\u5bfc\u6027\u6458\u8981\uff0c\u5b66\u751f\u9700\u5224\u65ad\u5176\u771f\u5b9e\u6027\u3002\u5de5\u5177\u63d0\u4f9b\u4ea4\u4e92\u5f0f\u8bfe\u7a0b\uff0c\u5c55\u793a\u4e0d\u540c\u63d0\u793a\u5bf9AI\u54cd\u5e94\u7684\u5f71\u54cd\u3002", "result": "\u5de5\u5177\u6709\u6548\u5e2e\u52a9\u5b66\u751f\u7406\u89e3\u8bef\u5bfc\u6027\u4fe1\u606f\u7684\u5371\u9669\u6027\u548c\u63d0\u793a\u5de5\u7a0b\u7684\u4f5c\u7528\u3002", "conclusion": "DoYouTrustAI\u5de5\u5177\u5f3a\u8c03\u4e86\u9a8c\u8bc1AI\u54cd\u5e94\u7684\u91cd\u8981\u6027\uff0c\u5e76\u6210\u529f\u5c06\u63d0\u793a\u5de5\u7a0b\u6982\u5ff5\u7b80\u5316\u4ee5\u9002\u5408\u5b66\u751f\u7406\u89e3\u3002"}}
{"id": "2504.15236", "pdf": "https://arxiv.org/pdf/2504.15236", "abs": "https://arxiv.org/abs/2504.15236", "authors": ["Saffron Huang", "Esin Durmus", "Miles McCain", "Kunal Handa", "Alex Tamkin", "Jerry Hong", "Michael Stern", "Arushi Somani", "Xiuruo Zhang", "Deep Ganguli"], "title": "Values in the Wild: Discovering and Analyzing Values in Real-World Language Model Interactions", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "comment": "44 pages", "summary": "AI assistants can impart value judgments that shape people's decisions and\nworldviews, yet little is known empirically about what values these systems\nrely on in practice. To address this, we develop a bottom-up,\nprivacy-preserving method to extract the values (normative considerations\nstated or demonstrated in model responses) that Claude 3 and 3.5 models exhibit\nin hundreds of thousands of real-world interactions. We empirically discover\nand taxonomize 3,307 AI values and study how they vary by context. We find that\nClaude expresses many practical and epistemic values, and typically supports\nprosocial human values while resisting values like \"moral nihilism\". While some\nvalues appear consistently across contexts (e.g. \"transparency\"), many are more\nspecialized and context-dependent, reflecting the diversity of human\ninterlocutors and their varied contexts. For example, \"harm prevention\" emerges\nwhen Claude resists users, \"historical accuracy\" when responding to queries\nabout controversial events, \"healthy boundaries\" when asked for relationship\nadvice, and \"human agency\" in technology ethics discussions. By providing the\nfirst large-scale empirical mapping of AI values in deployment, our work\ncreates a foundation for more grounded evaluation and design of values in AI\nsystems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9690\u79c1\u4fdd\u62a4\u7684\u65b9\u6cd5\uff0c\u4eceClaude 3\u548c3.5\u6a21\u578b\u7684\u771f\u5b9e\u4ea4\u4e92\u4e2d\u63d0\u53d6\u4e863,307\u79cdAI\u4ef7\u503c\u89c2\uff0c\u5e76\u7814\u7a76\u4e86\u5176\u5728\u4e0d\u540c\u60c5\u5883\u4e0b\u7684\u53d8\u5316\u3002", "motivation": "\u7814\u7a76AI\u52a9\u624b\u5728\u5b9e\u8df5\u4e2d\u4f9d\u8d56\u7684\u4ef7\u503c\u89c2\uff0c\u586b\u8865\u4e86\u5b9e\u8bc1\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u91c7\u7528\u81ea\u4e0b\u800c\u4e0a\u7684\u9690\u79c1\u4fdd\u62a4\u65b9\u6cd5\uff0c\u5206\u6790\u5927\u91cf\u771f\u5b9e\u4ea4\u4e92\u6570\u636e\uff0c\u63d0\u53d6\u5e76\u5206\u7c7bAI\u4ef7\u503c\u89c2\u3002", "result": "\u53d1\u73b0Claude\u6a21\u578b\u652f\u6301\u4eb2\u793e\u4f1a\u4ef7\u503c\u89c2\uff0c\u62b5\u5236\u5982\u2018\u9053\u5fb7\u865a\u65e0\u4e3b\u4e49\u2019\u7b49\u4ef7\u503c\u89c2\uff0c\u4e14\u4ef7\u503c\u89c2\u968f\u60c5\u5883\u53d8\u5316\u3002", "conclusion": "\u7814\u7a76\u4e3aAI\u7cfb\u7edf\u7684\u4ef7\u503c\u89c2\u8bc4\u4f30\u548c\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u57fa\u7840\u3002"}}
{"id": "2504.14587", "pdf": "https://arxiv.org/pdf/2504.14587", "abs": "https://arxiv.org/abs/2504.14587", "authors": ["Jingtong Gao", "Yewen Li", "Shuai Mao", "Peng Jiang", "Nan Jiang", "Yejing Wang", "Qingpeng Cai", "Fei Pan", "Peng Jiang", "Kun Gai", "Bo An", "Xiangyu Zhao"], "title": "Generative Auto-Bidding with Value-Guided Explorations", "categories": ["cs.LG", "cs.IR"], "comment": null, "summary": "Auto-bidding, with its strong capability to optimize bidding decisions within\ndynamic and competitive online environments, has become a pivotal strategy for\nadvertising platforms. Existing approaches typically employ rule-based\nstrategies or Reinforcement Learning (RL) techniques. However, rule-based\nstrategies lack the flexibility to adapt to time-varying market conditions, and\nRL-based methods struggle to capture essential historical dependencies and\nobservations within Markov Decision Process (MDP) frameworks. Furthermore,\nthese approaches often face challenges in ensuring strategy adaptability across\ndiverse advertising objectives. Additionally, as offline training methods are\nincreasingly adopted to facilitate the deployment and maintenance of stable\nonline strategies, the issues of documented behavioral patterns and behavioral\ncollapse resulting from training on fixed offline datasets become increasingly\nsignificant. To address these limitations, this paper introduces a novel\noffline Generative Auto-bidding framework with Value-Guided Explorations\n(GAVE). GAVE accommodates various advertising objectives through a score-based\nReturn-To-Go (RTG) module. Moreover, GAVE integrates an action exploration\nmechanism with an RTG-based evaluation method to explore novel actions while\nensuring stability-preserving updates. A learnable value function is also\ndesigned to guide the direction of action exploration and mitigate\nOut-of-Distribution (OOD) problems. Experimental results on two offline\ndatasets and real-world deployments demonstrate that GAVE outperforms\nstate-of-the-art baselines in both offline evaluations and online A/B tests.\nThe implementation code is publicly available to facilitate reproducibility and\nfurther research.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u79bb\u7ebf\u751f\u6210\u81ea\u52a8\u7ade\u4ef7\u6846\u67b6GAVE\uff0c\u901a\u8fc7\u503c\u5f15\u5bfc\u63a2\u7d22\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u9002\u5e94\u6027\u548c\u5386\u53f2\u4f9d\u8d56\u6027\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u81ea\u52a8\u7ade\u4ef7\u65b9\u6cd5\uff08\u5982\u57fa\u4e8e\u89c4\u5219\u6216\u5f3a\u5316\u5b66\u4e60\uff09\u5728\u52a8\u6001\u5e02\u573a\u9002\u5e94\u6027\u3001\u5386\u53f2\u4f9d\u8d56\u6027\u6355\u6349\u53ca\u591a\u76ee\u6807\u7b56\u7565\u9002\u5e94\u6027\u4e0a\u5b58\u5728\u5c40\u9650\uff0c\u4e14\u79bb\u7ebf\u8bad\u7ec3\u6613\u5bfc\u81f4\u884c\u4e3a\u6a21\u5f0f\u56fa\u5b9a\u548c\u5d29\u6e83\u3002", "method": "GAVE\u6846\u67b6\u7ed3\u5408\u4e86\u57fa\u4e8e\u5206\u6570\u7684RTG\u6a21\u5757\u9002\u5e94\u591a\u76ee\u6807\uff0c\u5e76\u901a\u8fc7\u52a8\u4f5c\u63a2\u7d22\u673a\u5236\u548cRTG\u8bc4\u4f30\u65b9\u6cd5\u63a2\u7d22\u65b0\u52a8\u4f5c\uff0c\u540c\u65f6\u8bbe\u8ba1\u53ef\u5b66\u4e60\u503c\u51fd\u6570\u5f15\u5bfc\u63a2\u7d22\u65b9\u5411\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cGAVE\u5728\u79bb\u7ebf\u8bc4\u4f30\u548c\u5728\u7ebfA/B\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "GAVE\u901a\u8fc7\u521b\u65b0\u6846\u67b6\u89e3\u51b3\u4e86\u81ea\u52a8\u7ade\u4ef7\u4e2d\u7684\u5173\u952e\u95ee\u9898\uff0c\u5e76\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2504.13865", "pdf": "https://arxiv.org/pdf/2504.13865", "abs": "https://arxiv.org/abs/2504.13865", "authors": ["Fei Tang", "Haolei Xu", "Hang Zhang", "Siqi Chen", "Xingyu Wu", "Yongliang Shen", "Wenqi Zhang", "Guiyang Hou", "Zeqi Tan", "Yuchen Yan", "Kaitao Song", "Jian Shao", "Weiming Lu", "Jun Xiao", "Yueting Zhuang"], "title": "A Survey on (M)LLM-Based GUI Agents", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Graphical User Interface (GUI) Agents have emerged as a transformative\nparadigm in human-computer interaction, evolving from rule-based automation\nscripts to sophisticated AI-driven systems capable of understanding and\nexecuting complex interface operations. This survey provides a comprehensive\nexamination of the rapidly advancing field of LLM-based GUI Agents,\nsystematically analyzing their architectural foundations, technical components,\nand evaluation methodologies. We identify and analyze four fundamental\ncomponents that constitute modern GUI Agents: (1) perception systems that\nintegrate text-based parsing with multimodal understanding for comprehensive\ninterface comprehension; (2) exploration mechanisms that construct and maintain\nknowledge bases through internal modeling, historical experience, and external\ninformation retrieval; (3) planning frameworks that leverage advanced reasoning\nmethodologies for task decomposition and execution; and (4) interaction systems\nthat manage action generation with robust safety controls. Through rigorous\nanalysis of these components, we reveal how recent advances in large language\nmodels and multimodal learning have revolutionized GUI automation across\ndesktop, mobile, and web platforms. We critically examine current evaluation\nframeworks, highlighting methodological limitations in existing benchmarks\nwhile proposing directions for standardization. This survey also identifies key\ntechnical challenges, including accurate element localization, effective\nknowledge retrieval, long-horizon planning, and safety-aware execution control,\nwhile outlining promising research directions for enhancing GUI Agents'\ncapabilities. Our systematic review provides researchers and practitioners with\na thorough understanding of the field's current state and offers insights into\nfuture developments in intelligent interface automation.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u57fa\u4e8eLLM\u7684GUI\u4ee3\u7406\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u5206\u6790\u4e86\u5176\u67b6\u6784\u3001\u6280\u672f\u7ec4\u4ef6\u548c\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u63a2\u7d22GUI\u4ee3\u7406\u4ece\u89c4\u5219\u811a\u672c\u5230AI\u9a71\u52a8\u7cfb\u7edf\u7684\u6f14\u8fdb\uff0c\u4ee5\u53ca\u5176\u5728\u4eba\u673a\u4ea4\u4e92\u4e2d\u7684\u53d8\u9769\u6f5c\u529b\u3002", "method": "\u7cfb\u7edf\u5206\u6790GUI\u4ee3\u7406\u7684\u56db\u5927\u7ec4\u4ef6\uff1a\u611f\u77e5\u7cfb\u7edf\u3001\u63a2\u7d22\u673a\u5236\u3001\u89c4\u5212\u6846\u67b6\u548c\u4ea4\u4e92\u7cfb\u7edf\u3002", "result": "\u63ed\u793a\u4e86LLM\u548c\u591a\u6a21\u6001\u5b66\u4e60\u5982\u4f55\u9769\u65b0GUI\u81ea\u52a8\u5316\uff0c\u540c\u65f6\u6307\u51fa\u4e86\u5f53\u524d\u8bc4\u4f30\u6846\u67b6\u7684\u5c40\u9650\u6027\u3002", "conclusion": "\u603b\u7ed3\u4e86GUI\u4ee3\u7406\u7684\u6280\u672f\u6311\u6218\u548c\u672a\u6765\u53d1\u5c55\u65b9\u5411\uff0c\u4e3a\u7814\u7a76\u8005\u548c\u4ece\u4e1a\u8005\u63d0\u4f9b\u4e86\u5168\u9762\u6307\u5bfc\u3002"}}
{"id": "2504.15241", "pdf": "https://arxiv.org/pdf/2504.15241", "abs": "https://arxiv.org/abs/2504.15241", "authors": ["Yahan Yang", "Soham Dan", "Shuo Li", "Dan Roth", "Insup Lee"], "title": "MR. Guard: Multilingual Reasoning Guardrail using Curriculum Learning", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) are susceptible to adversarial attacks such as\njailbreaking, which can elicit harmful or unsafe behaviors. This vulnerability\nis exacerbated in multilingual setting, where multilingual safety-aligned data\nare often limited. Thus, developing a guardrail capable of detecting and\nfiltering unsafe content across diverse languages is critical for deploying\nLLMs in real-world applications. In this work, we propose an approach to build\na multilingual guardrail with reasoning. Our method consists of: (1) synthetic\nmultilingual data generation incorporating culturally and linguistically\nnuanced variants, (2) supervised fine-tuning, and (3) a curriculum-guided Group\nRelative Policy Optimization (GRPO) framework that further improves\nperformance. Experimental results demonstrate that our multilingual guardrail\nconsistently outperforms recent baselines across both in-domain and\nout-of-domain languages. The multilingual reasoning capability of our guardrail\nenables it to generate multilingual explanations, which are particularly useful\nfor understanding language-specific risks and ambiguities in multilingual\ncontent moderation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u8bed\u8a00\u9632\u62a4\u680f\u65b9\u6cd5\uff0c\u901a\u8fc7\u5408\u6210\u6570\u636e\u751f\u6210\u3001\u76d1\u7763\u5fae\u8c03\u548cGRPO\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u591a\u8bed\u8a00\u73af\u5883\u4e0bLLMs\u7684\u5b89\u5168\u6027\u3002", "motivation": "\u591a\u8bed\u8a00\u73af\u5883\u4e0bLLMs\u6613\u53d7\u5bf9\u6297\u653b\u51fb\uff0c\u4e14\u5b89\u5168\u5bf9\u9f50\u6570\u636e\u6709\u9650\uff0c\u9700\u5f00\u53d1\u80fd\u68c0\u6d4b\u548c\u8fc7\u6ee4\u591a\u8bed\u8a00\u4e0d\u5b89\u5168\u5185\u5bb9\u7684\u9632\u62a4\u680f\u3002", "method": "1. \u5408\u6210\u591a\u8bed\u8a00\u6570\u636e\uff1b2. \u76d1\u7763\u5fae\u8c03\uff1b3. \u4f7f\u7528GRPO\u6846\u67b6\u4f18\u5316\u6027\u80fd\u3002", "result": "\u591a\u8bed\u8a00\u9632\u62a4\u680f\u5728\u57df\u5185\u548c\u57df\u5916\u8bed\u8a00\u4e2d\u5747\u4f18\u4e8e\u57fa\u7ebf\uff0c\u5e76\u80fd\u751f\u6210\u591a\u8bed\u8a00\u89e3\u91ca\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u591a\u8bed\u8a00LLMs\u7684\u5b89\u5168\u6027\uff0c\u7279\u522b\u9002\u7528\u4e8e\u5185\u5bb9\u5ba1\u6838\u4e2d\u7684\u8bed\u8a00\u7279\u5b9a\u98ce\u9669\u8bc6\u522b\u3002"}}
{"id": "2504.14610", "pdf": "https://arxiv.org/pdf/2504.14610", "abs": "https://arxiv.org/abs/2504.14610", "authors": ["Manar D. Samad", "Kazi Fuad B. Akhter", "Shourav B. Rabbani", "Ibna Kowsar"], "title": "No Imputation of Missing Values In Tabular Data Classification Using Incremental Learning", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "Tabular data sets with varying missing values are prepared for machine\nlearning using an arbitrary imputation strategy. Synthetic values generated by\nimputation models often concern data stakeholders about computational\ncomplexity, data quality, and data-driven outcomes. This paper eliminates these\nconcerns by proposing no imputation incremental learning (NIIL) of tabular data\nwith varying missing value rates and types. The proposed method incrementally\nlearns partitions of overlapping feature sets while using attention masks to\nexclude missing values from attention scoring. The average classification\nperformance rank order across 15 diverse tabular data sets highlights the\nsuperiority of NIIL over 11 state-of-the-art learning methods with or without\nmissing value imputations. Further experiments substantiate the robustness of\nNIIL against varying missing value types and rates compared to methods that\ninvolve the imputation of missing values. Our empirical analysis reveals that a\nfeature partition size of half of the original feature space is,\ncomputation-wise and accuracy-wise, the best choice for the proposed\nincremental learning. The proposed method is one of the first deep learning\nsolutions that can effectively learn tabular data without requiring the\nimputation of missing values.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u586b\u8865\u7f3a\u5931\u503c\u7684\u589e\u91cf\u5b66\u4e60\u65b9\u6cd5\uff08NIIL\uff09\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u63a9\u7801\u6392\u9664\u7f3a\u5931\u503c\uff0c\u572815\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e11\u79cd\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u586b\u8865\u7f3a\u5931\u503c\u7684\u65b9\u6cd5\u53ef\u80fd\u5f15\u53d1\u8ba1\u7b97\u590d\u6742\u5ea6\u3001\u6570\u636e\u8d28\u91cf\u548c\u7ed3\u679c\u53ef\u9760\u6027\u7684\u62c5\u5fe7\uff0cNIIL\u65e8\u5728\u6d88\u9664\u8fd9\u4e9b\u62c5\u5fe7\u3002", "method": "NIIL\u901a\u8fc7\u589e\u91cf\u5b66\u4e60\u91cd\u53e0\u7279\u5f81\u96c6\u7684\u5206\u533a\uff0c\u5e76\u4f7f\u7528\u6ce8\u610f\u529b\u63a9\u7801\u6392\u9664\u7f3a\u5931\u503c\u3002", "result": "\u572815\u4e2a\u6570\u636e\u96c6\u4e0a\uff0cNIIL\u7684\u5206\u7c7b\u6027\u80fd\u4f18\u4e8e11\u79cd\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u5bf9\u7f3a\u5931\u503c\u7c7b\u578b\u548c\u6bd4\u4f8b\u5177\u6709\u9c81\u68d2\u6027\u3002", "conclusion": "NIIL\u662f\u9996\u4e2a\u65e0\u9700\u586b\u8865\u7f3a\u5931\u503c\u7684\u6df1\u5ea6\u5b66\u4e60\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u5f81\u5206\u533a\u5927\u5c0f\u4e3a\u539f\u59cb\u7279\u5f81\u7a7a\u95f4\u7684\u4e00\u534a\u65f6\u6548\u679c\u6700\u4f73\u3002"}}
{"id": "2504.13866", "pdf": "https://arxiv.org/pdf/2504.13866", "abs": "https://arxiv.org/abs/2504.13866", "authors": ["Aleksa Marusic", "Sao Mai Nguyen", "Adriana Tapus"], "title": "Skeleton-Based Transformer for Classification of Errors and Better Feedback in Low Back Pain Physical Rehabilitation Exercises", "categories": ["cs.HC", "cs.AI", "cs.CV", "cs.RO"], "comment": "ICORR 2025 - 19th IEEE/RAS-EMBS International Conference on\n  Rehabilitation Robotics, INTERNATIONAL CONSORTIUM FOR REHABILITATION\n  ROBOTICS, May 2025, Michigan, USA, United States", "summary": "Physical rehabilitation exercises suggested by healthcare professionals can\nhelp recovery from various musculoskeletal disorders and prevent re-injury.\nHowever, patients' engagement tends to decrease over time without direct\nsupervision, which is why there is a need for an automated monitoring system.\nIn recent years, there has been great progress in quality assessment of\nphysical rehabilitation exercises. Most of them only provide a binary\nclassification if the performance is correct or incorrect, and a few provide a\ncontinuous score. This information is not sufficient for patients to improve\ntheir performance. In this work, we propose an algorithm for error\nclassification of rehabilitation exercises, thus making the first step toward\nmore detailed feedback to patients. We focus on skeleton-based exercise\nassessment, which utilizes human pose estimation to evaluate motion. Inspired\nby recent algorithms for quality assessment during rehabilitation exercises, we\npropose a Transformer-based model for the described classification. Our model\nis inspired by the HyperFormer method for human action recognition, and adapted\nto our problem and dataset. The evaluation is done on the KERAAL dataset, as it\nis the only medical dataset with clear error labels for the exercises, and our\nmodel significantly surpasses state-of-the-art methods. Furthermore, we bridge\nthe gap towards better feedback to the patients by presenting a way to\ncalculate the importance of joints for each exercise.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u7b97\u6cd5\uff0c\u7528\u4e8e\u5eb7\u590d\u8bad\u7ec3\u4e2d\u7684\u9519\u8bef\u5206\u7c7b\uff0c\u5e76\u901a\u8fc7\u5173\u8282\u91cd\u8981\u6027\u8ba1\u7b97\u63d0\u4f9b\u66f4\u8be6\u7ec6\u7684\u53cd\u9988\u3002", "motivation": "\u60a3\u8005\u5728\u6ca1\u6709\u76f4\u63a5\u76d1\u7763\u7684\u60c5\u51b5\u4e0b\uff0c\u5eb7\u590d\u8bad\u7ec3\u7684\u53c2\u4e0e\u5ea6\u4f1a\u964d\u4f4e\uff0c\u73b0\u6709\u7cfb\u7edf\u4ec5\u63d0\u4f9b\u4e8c\u5143\u5206\u7c7b\u6216\u8fde\u7eed\u8bc4\u5206\uff0c\u4e0d\u8db3\u4ee5\u5e2e\u52a9\u60a3\u8005\u6539\u8fdb\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u9aa8\u67b6\u7684\u8fd0\u52a8\u8bc4\u4f30\uff0c\u5229\u7528Transformer\u6a21\u578b\uff08\u53d7HyperFormer\u542f\u53d1\uff09\u8fdb\u884c\u9519\u8bef\u5206\u7c7b\uff0c\u5e76\u5728KERAAL\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u3002", "result": "\u6a21\u578b\u5728KERAAL\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u5173\u8282\u91cd\u8981\u6027\u8ba1\u7b97\u63d0\u4f9b\u66f4\u8be6\u7ec6\u53cd\u9988\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5eb7\u590d\u8bad\u7ec3\u63d0\u4f9b\u4e86\u66f4\u8be6\u7ec6\u7684\u9519\u8bef\u5206\u7c7b\u548c\u53cd\u9988\u65b9\u6cd5\uff0c\u6709\u52a9\u4e8e\u60a3\u8005\u6539\u8fdb\u8bad\u7ec3\u6548\u679c\u3002"}}
{"id": "2504.15253", "pdf": "https://arxiv.org/pdf/2504.15253", "abs": "https://arxiv.org/abs/2504.15253", "authors": ["Yilun Zhou", "Austin Xu", "Peifeng Wang", "Caiming Xiong", "Shafiq Joty"], "title": "Evaluating Judges as Evaluators: The JETTS Benchmark of LLM-as-Judges as Test-Time Scaling Evaluators", "categories": ["cs.CL", "cs.LG"], "comment": "The first two authors contributed equally. The codebase is at\n  https://github.com/SalesforceAIResearch/jetts-benchmark", "summary": "Scaling test-time computation, or affording a generator large language model\n(LLM) extra compute during inference, typically employs the help of external\nnon-generative evaluators (i.e., reward models). Concurrently, LLM-judges,\nmodels trained to generate evaluations and critiques (explanations) in natural\nlanguage, are becoming increasingly popular in automatic evaluation. Despite\njudge empirical successes, their effectiveness as evaluators in test-time\nscaling settings is largely unknown. In this paper, we introduce the Judge\nEvaluation for Test-Time Scaling (JETTS) benchmark, which evaluates judge\nperformance in three domains (math reasoning, code generation, and instruction\nfollowing) under three task settings: response reranking, step-level beam\nsearch, and critique-based response refinement. We evaluate 10 different judge\nmodels (7B-70B parameters) for 8 different base generator models (6.7B-72B\nparameters). Our benchmark shows that while judges are competitive with outcome\nreward models in reranking, they are consistently worse than process reward\nmodels in beam search procedures. Furthermore, though unique to LLM-judges,\ntheir natural language critiques are currently ineffective in guiding the\ngenerator towards better responses.", "AI": {"tldr": "JETTS\u57fa\u51c6\u6d4b\u8bd5\u8bc4\u4f30\u4e86LLM-judges\u5728\u6d4b\u8bd5\u65f6\u6269\u5c55\u8bbe\u7f6e\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5176\u5728\u91cd\u65b0\u6392\u5e8f\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u675f\u641c\u7d22\u548c\u57fa\u4e8e\u6279\u5224\u7684\u54cd\u5e94\u4f18\u5316\u4e2d\u4e0d\u5982\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\u3002", "motivation": "\u7814\u7a76LLM-judges\u5728\u6d4b\u8bd5\u65f6\u6269\u5c55\u8bbe\u7f6e\u4e2d\u7684\u6709\u6548\u6027\uff0c\u586b\u8865\u5176\u5728\u81ea\u52a8\u8bc4\u4f30\u4e2d\u7684\u672a\u77e5\u9886\u57df\u3002", "method": "\u5f15\u5165JETTS\u57fa\u51c6\uff0c\u8bc4\u4f3010\u79cdjudge\u6a21\u578b\u5728\u6570\u5b66\u63a8\u7406\u3001\u4ee3\u7801\u751f\u6210\u548c\u6307\u4ee4\u9075\u5faa\u4e09\u4e2a\u9886\u57df\u7684\u8868\u73b0\uff0c\u5305\u62ec\u91cd\u65b0\u6392\u5e8f\u3001\u675f\u641c\u7d22\u548c\u6279\u5224\u4f18\u5316\u4e09\u79cd\u4efb\u52a1\u3002", "result": "judge\u5728\u91cd\u65b0\u6392\u5e8f\u4e2d\u4e0e\u7ed3\u679c\u5956\u52b1\u6a21\u578b\u7ade\u4e89\uff0c\u4f46\u5728\u675f\u641c\u7d22\u4e2d\u8868\u73b0\u8f83\u5dee\uff0c\u4e14\u81ea\u7136\u8bed\u8a00\u6279\u5224\u5bf9\u54cd\u5e94\u4f18\u5316\u7684\u6307\u5bfc\u6548\u679c\u4e0d\u4f73\u3002", "conclusion": "LLM-judges\u5728\u67d0\u4e9b\u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u4ecd\u9700\u6539\u8fdb\uff0c\u6279\u5224\u7684\u6307\u5bfc\u4f5c\u7528\u6709\u9650\u3002"}}
{"id": "2504.14636", "pdf": "https://arxiv.org/pdf/2504.14636", "abs": "https://arxiv.org/abs/2504.14636", "authors": ["Binjie Guo", "Hanyu Zheng", "Guowei Su", "Ru Zhang", "Haohan Jiang", "Xurong Lin", "Hongyan Wei", "Aisheng Mo", "Jie Li", "Zhiyuan Qian", "Zhuhao Zhang", "Xiaoyuan Cheng"], "title": "AlphaZero-Edu: Making AlphaZero Accessible to Everyone", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Recent years have witnessed significant progress in reinforcement learning,\nespecially with Zero-like paradigms, which have greatly boosted the\ngeneralization and reasoning abilities of large-scale language models.\nNevertheless, existing frameworks are often plagued by high implementation\ncomplexity and poor reproducibility. To tackle these challenges, we present\nAlphaZero-Edu, a lightweight, education-focused implementation built upon the\nmathematical framework of AlphaZero. It boasts a modular architecture that\ndisentangles key components, enabling transparent visualization of the\nalgorithmic processes. Additionally, it is optimized for resource-efficient\ntraining on a single NVIDIA RTX 3090 GPU and features highly parallelized\nself-play data generation, achieving a 3.2-fold speedup with 8 processes. In\nGomoku matches, the framework has demonstrated exceptional performance,\nachieving a consistently high win rate against human opponents. AlphaZero-Edu\nhas been open-sourced at https://github.com/StarLight1212/AlphaZero_Edu,\nproviding an accessible and practical benchmark for both academic research and\nindustrial applications.", "AI": {"tldr": "AlphaZero-Edu\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u3001\u6559\u80b2\u5bfc\u5411\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u57fa\u4e8eAlphaZero\u6570\u5b66\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6846\u67b6\u5b9e\u73b0\u590d\u6742\u548c\u53ef\u590d\u73b0\u6027\u5dee\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u5b9e\u73b0\u590d\u6742\u4e14\u53ef\u590d\u73b0\u6027\u5dee\uff0cAlphaZero-Edu\u65e8\u5728\u63d0\u4f9b\u4e00\u4e2a\u8f7b\u91cf\u3001\u900f\u660e\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u6a21\u5757\u5316\u67b6\u6784\uff0c\u4f18\u5316\u8d44\u6e90\u6548\u7387\uff0c\u652f\u6301\u5355GPU\u8bad\u7ec3\uff0c\u5e76\u884c\u5316\u81ea\u5bf9\u5f08\u6570\u636e\u751f\u6210\u3002", "result": "\u5728Gomoku\u6bd4\u8d5b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5bf9\u4eba\u7c7b\u7684\u80dc\u7387\u6301\u7eed\u8f83\u9ad8\uff0c\u5b9e\u73b0\u4e863.2\u500d\u7684\u52a0\u901f\u3002", "conclusion": "AlphaZero-Edu\u5f00\u6e90\uff0c\u4e3a\u5b66\u672f\u7814\u7a76\u548c\u5de5\u4e1a\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u7528\u57fa\u51c6\u3002"}}
{"id": "2504.13868", "pdf": "https://arxiv.org/pdf/2504.13868", "abs": "https://arxiv.org/abs/2504.13868", "authors": ["Yun Wan", "Yoram M Kalman"], "title": "Using Generative AI Personas Increases Collective Diversity in Human Ideation", "categories": ["cs.HC", "cs.AI", "I.2.7, H.5.0, H.4.0"], "comment": null, "summary": "This study challenges the widely-reported tradeoff between generative AI's\n(GenAI) contribution to creative outcomes and decreased diversity of these\noutcomes. We modified the design of such a study, by Doshi and Hauser (2024),\nin which participants wrote short stories either aided or unaided by GenAI plot\nideas[1]. In the modified study, plot ideas were generated through ten unique\nGenAI \"personas\" with diverse traits (e.g. cultural backgrounds, thinking\nstyles, genre preferences), creating a pool of 300 story plots. While plot\nideas from any individual persona showed high similarity (average cosine\nsimilarity of 0.92), ideas across different personas exhibited substantial\nvariation (average similarity of 0.20). When human participants wrote stories\nbased on these diverse plot ideas, their collective outputs maintained the same\nlevel of diversity as stories written without GenAI assistance, effectively\neliminating the diversity reduction observed in [1]. Traditional text analytics\nfurther revealed that GenAI-assisted stories featured greater diversity in\ndescriptive and emotional language compared to purely human-generated stories\nwithout GenAI assistance. Our findings demonstrate that introducing diversity\nat the AI input stage through distinct personas can preserve and potentially\nenhance the collective diversity of human creative outputs when collaborating\nwith GenAI.", "AI": {"tldr": "\u7814\u7a76\u6311\u6218\u4e86\u751f\u6210\u5f0fAI\uff08GenAI\uff09\u5728\u521b\u610f\u4ea7\u51fa\u4e2d\u8d21\u732e\u4e0e\u591a\u6837\u6027\u964d\u4f4e\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u901a\u8fc7\u5f15\u5165\u591a\u6837\u5316AI\u89d2\u8272\uff08personas\uff09\u751f\u6210\u6545\u4e8b\u60c5\u8282\uff0c\u53d1\u73b0\u53ef\u4ee5\u4fdd\u6301\u751a\u81f3\u589e\u5f3a\u4eba\u7c7b\u521b\u610f\u4ea7\u51fa\u7684\u591a\u6837\u6027\u3002", "motivation": "\u63a2\u8ba8\u5982\u4f55\u901a\u8fc7\u591a\u6837\u5316AI\u8f93\u5165\uff08\u5982\u4e0d\u540c\u6587\u5316\u80cc\u666f\u3001\u601d\u7ef4\u98ce\u683c\u7b49\uff09\u6765\u907f\u514dGenAI\u8f85\u52a9\u521b\u4f5c\u65f6\u591a\u6837\u6027\u964d\u4f4e\u7684\u95ee\u9898\u3002", "method": "\u4fee\u6539\u4e86Doshi\u548cHauser\uff082024\uff09\u7684\u7814\u7a76\u8bbe\u8ba1\uff0c\u4f7f\u752810\u79cd\u72ec\u7279\u7684GenAI\u89d2\u8272\u751f\u6210300\u4e2a\u6545\u4e8b\u60c5\u8282\uff0c\u4eba\u7c7b\u53c2\u4e0e\u8005\u57fa\u4e8e\u8fd9\u4e9b\u60c5\u8282\u521b\u4f5c\u6545\u4e8b\u3002", "result": "\u4e0d\u540cAI\u89d2\u8272\u751f\u6210\u7684\u60c5\u8282\u591a\u6837\u6027\u663e\u8457\uff08\u5e73\u5747\u76f8\u4f3c\u5ea60.20\uff09\uff0c\u4eba\u7c7b\u521b\u4f5c\u7684\u96c6\u4f53\u6545\u4e8b\u591a\u6837\u6027\u672a\u964d\u4f4e\uff0c\u4e14\u60c5\u611f\u548c\u63cf\u8ff0\u8bed\u8a00\u591a\u6837\u6027\u66f4\u9ad8\u3002", "conclusion": "\u901a\u8fc7\u5728AI\u8f93\u5165\u9636\u6bb5\u5f15\u5165\u591a\u6837\u6027\u89d2\u8272\uff0c\u53ef\u4ee5\u4fdd\u6301\u6216\u589e\u5f3aGenAI\u8f85\u52a9\u4e0b\u4eba\u7c7b\u521b\u610f\u4ea7\u51fa\u7684\u591a\u6837\u6027\u3002"}}
{"id": "2504.13847", "pdf": "https://arxiv.org/pdf/2504.13847", "abs": "https://arxiv.org/abs/2504.13847", "authors": ["Zhe Liu"], "title": "Interview AI-ssistant: Designing for Real-Time Human-AI Collaboration in Interview Preparation and Execution", "categories": ["cs.HC", "cs.CL"], "comment": "4 pages, 2 figures, submitted and accepted by IUI 2025 Doctoral\n  Consortium", "summary": "Recent advances in large language models (LLMs) offer unprecedented\nopportunities to enhance human-AI collaboration in qualitative research\nmethods, including interviews. While interviews are highly valued for gathering\ndeep, contextualized insights, interviewers often face significant cognitive\nchallenges, such as real-time information processing, question adaptation, and\nrapport maintenance. My doctoral research introduces Interview AI-ssistant, a\nsystem designed for real-time interviewer-AI collaboration during both the\npreparation and execution phases. Through four interconnected studies, this\nresearch investigates the design of effective human-AI collaboration in\ninterviewing contexts, beginning with a formative study of interviewers' needs,\nfollowed by a prototype development study focused on AI-assisted interview\npreparation, an experimental evaluation of real-time AI assistance during\ninterviews, and a field study deploying the system in a real-world research\nsetting. Beyond informing practical implementations of intelligent interview\nsupport systems, this work contributes to the Intelligent User Interfaces (IUI)\ncommunity by advancing the understanding of human-AI collaborative interfaces\nin complex social tasks and establishing design guidelines for AI-enhanced\nqualitative research tools.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aInterview AI-ssistant\u7684\u7cfb\u7edf\uff0c\u65e8\u5728\u901a\u8fc7AI\u5b9e\u65f6\u8f85\u52a9\u8bbf\u8c08\u7684\u51c6\u5907\u5de5\u4f5c\u4e0e\u6267\u884c\u9636\u6bb5\uff0c\u63d0\u5347\u4eba\u7c7b\u4e0eAI\u5728\u5b9a\u6027\u7814\u7a76\u4e2d\u7684\u534f\u4f5c\u6548\u679c\u3002", "motivation": "\u8bbf\u8c08\u5728\u83b7\u53d6\u6df1\u5ea6\u3001\u60c5\u5883\u5316\u89c1\u89e3\u65b9\u9762\u5177\u6709\u91cd\u8981\u4ef7\u503c\uff0c\u4f46\u8bbf\u8c08\u8005\u5728\u5b9e\u65f6\u4fe1\u606f\u5904\u7406\u3001\u95ee\u9898\u8c03\u6574\u548c\u5173\u7cfb\u7ef4\u62a4\u7b49\u65b9\u9762\u9762\u4e34\u8ba4\u77e5\u6311\u6218\u3002", "method": "\u901a\u8fc7\u56db\u9879\u76f8\u4e92\u5173\u8054\u7684\u7814\u7a76\uff0c\u5305\u62ec\u8bbf\u8c08\u8005\u9700\u6c42\u7684\u5f62\u6210\u6027\u7814\u7a76\u3001AI\u8f85\u52a9\u8bbf\u8c08\u51c6\u5907\u7684\u5f00\u53d1\u7814\u7a76\u3001\u5b9e\u65f6AI\u8f85\u52a9\u7684\u5b9e\u9a8c\u8bc4\u4f30\u4ee5\u53ca\u5b9e\u9645\u7814\u7a76\u73af\u5883\u4e2d\u7684\u7cfb\u7edf\u90e8\u7f72\u3002", "result": "\u7814\u7a76\u4e0d\u4ec5\u4e3a\u667a\u80fd\u8bbf\u8c08\u652f\u6301\u7cfb\u7edf\u7684\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u6307\u5bfc\uff0c\u8fd8\u4e3aIUI\u793e\u533a\u63d0\u4f9b\u4e86\u5173\u4e8e\u590d\u6742\u793e\u4ea4\u4efb\u52a1\u4e2d\u4eba\u7c7b\u4e0eAI\u534f\u4f5c\u754c\u9762\u7684\u8bbe\u8ba1\u6307\u5357\u3002", "conclusion": "\u8be5\u7814\u7a76\u63a8\u52a8\u4e86\u4eba\u7c7b\u4e0eAI\u534f\u4f5c\u754c\u9762\u7684\u7406\u89e3\uff0c\u5e76\u4e3aAI\u589e\u5f3a\u7684\u5b9a\u6027\u7814\u7a76\u5de5\u5177\u8bbe\u8ba1\u4e86\u6307\u5bfc\u539f\u5219\u3002"}}
{"id": "2504.14645", "pdf": "https://arxiv.org/pdf/2504.14645", "abs": "https://arxiv.org/abs/2504.14645", "authors": ["Philipp Altmann", "C\u00e9line Davignon", "Maximilian Zorn", "Fabian Ritz", "Claudia Linnhoff-Popien", "Thomas Gabor"], "title": "Surrogate Fitness Metrics for Interpretable Reinforcement Learning", "categories": ["cs.LG", "cs.AI"], "comment": "30 pages, 7 figures, under review", "summary": "We employ an evolutionary optimization framework that perturbs initial states\nto generate informative and diverse policy demonstrations. A joint surrogate\nfitness function guides the optimization by combining local diversity,\nbehavioral certainty, and global population diversity. To assess demonstration\nquality, we apply a set of evaluation metrics, including the reward-based\noptimality gap, fidelity interquartile means (IQMs), fitness composition\nanalysis, and trajectory visualizations. Hyperparameter sensitivity is also\nexamined to better understand the dynamics of trajectory optimization. Our\nfindings demonstrate that optimizing trajectory selection via surrogate fitness\nmetrics significantly improves interpretability of RL policies in both discrete\nand continuous environments. In gridworld domains, evaluations reveal\nsignificantly enhanced demonstration fidelities compared to random and ablated\nbaselines. In continuous control, the proposed framework offers valuable\ninsights, particularly for early-stage policies, while fidelity-based\noptimization proves more effective for mature policies. By refining and\nsystematically analyzing surrogate fitness functions, this study advances the\ninterpretability of RL models. The proposed improvements provide deeper\ninsights into RL decision-making, benefiting applications in safety-critical\nand explainability-focused domains.", "AI": {"tldr": "\u901a\u8fc7\u8fdb\u5316\u4f18\u5316\u6846\u67b6\u751f\u6210\u591a\u6837\u4e14\u4fe1\u606f\u4e30\u5bcc\u7684\u7b56\u7565\u6f14\u793a\uff0c\u7ed3\u5408\u5c40\u90e8\u591a\u6837\u6027\u3001\u884c\u4e3a\u786e\u5b9a\u6027\u548c\u5168\u5c40\u591a\u6837\u6027\u4f18\u5316\u8f68\u8ff9\u9009\u62e9\uff0c\u663e\u8457\u63d0\u5347RL\u7b56\u7565\u7684\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u63d0\u5347\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u7b56\u7565\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u7279\u522b\u662f\u5728\u5b89\u5168\u5173\u952e\u548c\u9700\u89e3\u91ca\u6027\u5f3a\u7684\u9886\u57df\u3002", "method": "\u91c7\u7528\u8fdb\u5316\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u6270\u52a8\u521d\u59cb\u72b6\u6001\u751f\u6210\u6f14\u793a\uff0c\u5e76\u5229\u7528\u8054\u5408\u4ee3\u7406\u9002\u5e94\u5ea6\u51fd\u6570\uff08\u7ed3\u5408\u591a\u6837\u6027\u3001\u884c\u4e3a\u786e\u5b9a\u6027\u7b49\uff09\u4f18\u5316\u8f68\u8ff9\u9009\u62e9\u3002", "result": "\u5728\u79bb\u6563\u548c\u8fde\u7eed\u73af\u5883\u4e2d\uff0c\u4f18\u5316\u540e\u7684\u6f14\u793a\u663e\u8457\u63d0\u5347\u4e86\u7b56\u7565\u7684\u53ef\u89e3\u91ca\u6027\uff1b\u5728\u7f51\u683c\u4e16\u754c\u4e2d\uff0c\u6f14\u793a\u4fdd\u771f\u5ea6\u4f18\u4e8e\u968f\u673a\u548c\u6d88\u878d\u57fa\u7ebf\u3002", "conclusion": "\u901a\u8fc7\u4f18\u5316\u4ee3\u7406\u9002\u5e94\u5ea6\u51fd\u6570\uff0c\u672c\u7814\u7a76\u63d0\u5347\u4e86RL\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u4e3a\u51b3\u7b56\u63d0\u4f9b\u66f4\u6df1\u5165\u7684\u89c1\u89e3\u3002"}}
{"id": "2504.13871", "pdf": "https://arxiv.org/pdf/2504.13871", "abs": "https://arxiv.org/abs/2504.13871", "authors": ["Yuanjun Feng", "Vivek Chodhary", "Yash Raj Shrestha"], "title": "Human aversion? Do AI Agents Judge Identity More Harshly Than Performance", "categories": ["cs.HC", "cs.AI", "econ.GN", "q-fin.EC"], "comment": null, "summary": "This study examines the understudied role of algorithmic evaluation of human\njudgment in hybrid decision-making systems, a critical gap in management\nresearch. While extant literature focuses on human reluctance to follow\nalgorithmic advice, we reverse the perspective by investigating how AI agents\nbased on large language models (LLMs) assess and integrate human input. Our\nwork addresses a pressing managerial constraint: firms barred from deploying\nLLMs directly due to privacy concerns can still leverage them as mediating\ntools (for instance, anonymized outputs or decision pipelines) to guide\nhigh-stakes choices like pricing or discounts without exposing proprietary\ndata. Through a controlled prediction task, we analyze how an LLM-based AI\nagent weights human versus algorithmic predictions. We find that the AI system\nsystematically discounts human advice, penalizing human errors more severely\nthan algorithmic errors--a bias exacerbated when the agent's identity (human vs\nAI) is disclosed and the human is positioned second. These results reveal a\ndisconnect between AI-generated trust metrics and the actual influence of human\njudgment, challenging assumptions about equitable human-AI collaboration. Our\nfindings offer three key contributions. First, we identify a reverse algorithm\naversion phenomenon, where AI agents undervalue human input despite comparable\nerror rates. Second, we demonstrate how disclosure and positional bias interact\nto amplify this effect, with implications for system design. Third, we provide\na framework for indirect LLM deployment that balances predictive power with\ndata privacy. For practitioners, this research emphasize the need to audit AI\nweighting mechanisms, calibrate trust dynamics, and strategically design\ndecision sequences in human-AI systems.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86AI\u5728\u6df7\u5408\u51b3\u7b56\u7cfb\u7edf\u4e2d\u5bf9\u4eba\u7c7b\u5224\u65ad\u7684\u8bc4\u4f30\uff0c\u53d1\u73b0AI\u503e\u5411\u4e8e\u4f4e\u4f30\u4eba\u7c7b\u8f93\u5165\uff0c\u5c24\u5176\u662f\u5728\u8eab\u4efd\u62ab\u9732\u548c\u987a\u5e8f\u5b89\u6392\u4e0b\u3002", "motivation": "\u586b\u8865\u7ba1\u7406\u7814\u7a76\u4e2dAI\u8bc4\u4f30\u4eba\u7c7b\u5224\u65ad\u7684\u7a7a\u767d\uff0c\u89e3\u51b3\u56e0\u9690\u79c1\u95ee\u9898\u65e0\u6cd5\u76f4\u63a5\u90e8\u7f72LLMs\u7684\u6311\u6218\u3002", "method": "\u901a\u8fc7\u63a7\u5236\u9884\u6d4b\u4efb\u52a1\uff0c\u5206\u6790LLM-based AI\u5bf9\u4eba\u7c7b\u4e0e\u7b97\u6cd5\u9884\u6d4b\u7684\u6743\u91cd\u5206\u914d\u3002", "result": "AI\u7cfb\u7edf\u7cfb\u7edf\u6027\u4f4e\u4f30\u4eba\u7c7b\u5efa\u8bae\uff0c\u4e14\u8eab\u4efd\u62ab\u9732\u548c\u987a\u5e8f\u5b89\u6392\u52a0\u5267\u6b64\u73b0\u8c61\u3002", "conclusion": "\u63ed\u793a\u4e86AI\u4e0e\u4eba\u7c7b\u534f\u4f5c\u4e2d\u7684\u4e0d\u5e73\u7b49\uff0c\u63d0\u51fa\u4e86\u95f4\u63a5\u90e8\u7f72LLMs\u7684\u6846\u67b6\u548c\u5b9e\u8df5\u5efa\u8bae\u3002"}}
{"id": "2504.13861", "pdf": "https://arxiv.org/pdf/2504.13861", "abs": "https://arxiv.org/abs/2504.13861", "authors": ["Ivan Sviridov", "Amina Miftakhova", "Artemiy Tereshchenko", "Galina Zubkova", "Pavel Blinov", "Andrey Savchenko"], "title": "3MDBench: Medical Multimodal Multi-agent Dialogue Benchmark", "categories": ["cs.HC", "cs.CL", "cs.MA", "68T42", "I.2.1"], "comment": "26 pages, 8 figures, 7 tables", "summary": "Large Vision-Language Models (LVLMs) are increasingly being explored for\napplications in telemedicine, yet their ability to engage with diverse patient\nbehaviors remains underexplored. We introduce 3MDBench (Medical Multimodal\nMulti-agent Dialogue Benchmark), an open-source evaluation framework designed\nto assess LLM-driven medical consultations. Unlike existing benchmarks,\n3MDBench simulates real-world patient variability by incorporating four\ntemperament-driven Patient Agents and an Assessor Agent that evaluates\ndiagnostic accuracy and dialogue quality. The benchmark integrates textual and\nimage-based patient data across 34 common diagnoses, mirroring real-world\ntelemedicine interactions. Under different diagnostic strategies, we evaluate\nstate-of-the-art LVLMs. Our findings demonstrate that incorporating dialogue\nimproves the F1 score from 50.4 to 54.2 compared to non-dialogue settings,\nunderscoring the value of context-driven, information-seeking questioning.\nAdditionally, we demonstrate that multimodal inputs enhance diagnostic\nefficiency. Image-supported models outperform text-only counterparts by raising\nthe diagnostic F1 score from 52.8 to 54.2 in a similar dialogue setting.\nFinally, we suggest an approach that improves the diagnostic F1-score to 70.3\nby training the CNN model on the diagnosis prediction task and incorporating\nits top-3 predictions into the LVLM context. 3MDBench provides a reproducible\nand extendable evaluation framework for AI-driven medical assistants. It offers\ninsights into how patient temperament, dialogue strategies, and multimodal\nreasoning influence diagnosis quality. By addressing real-world complexities in\ntelemedicine, our benchmark paves the way for more empathetic, reliable, and\ncontext-aware AI-driven healthcare solutions. The source code of our benchmark\nis publicly available: https://github.com/univanxx/3mdbench", "AI": {"tldr": "3MDBench\u662f\u4e00\u4e2a\u5f00\u6e90\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u6d4b\u8bd5\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLM\uff09\u5728\u533b\u7597\u54a8\u8be2\u4e2d\u7684\u8868\u73b0\uff0c\u901a\u8fc7\u6a21\u62df\u771f\u5b9e\u60a3\u8005\u884c\u4e3a\u548c\u591a\u79cd\u8bca\u65ad\u7b56\u7565\uff0c\u63d0\u5347\u8bca\u65ad\u51c6\u786e\u6027\u548c\u5bf9\u8bdd\u8d28\u91cf\u3002", "motivation": "\u63a2\u7d22LVLM\u5728\u8fdc\u7a0b\u533b\u7597\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u5c24\u5176\u662f\u5176\u4e0e\u591a\u6837\u5316\u60a3\u8005\u884c\u4e3a\u4e92\u52a8\u7684\u80fd\u529b\uff0c\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u672a\u80fd\u5145\u5206\u8986\u76d6\u8fd9\u4e00\u9886\u57df\u3002", "method": "\u5f00\u53d13MDBench\u6846\u67b6\uff0c\u5305\u542b\u56db\u79cd\u6027\u683c\u9a71\u52a8\u7684\u60a3\u8005\u4ee3\u7406\u548c\u8bc4\u4f30\u4ee3\u7406\uff0c\u6574\u5408\u6587\u672c\u548c\u56fe\u50cf\u6570\u636e\uff0c\u8bc4\u4f30\u4e0d\u540c\u8bca\u65ad\u7b56\u7565\u4e0b\u7684LVLM\u8868\u73b0\u3002", "result": "\u5bf9\u8bdd\u548c\u591a\u6a21\u6001\u8f93\u5165\u663e\u8457\u63d0\u5347\u8bca\u65ad\u6548\u679c\uff0cF1\u5206\u6570\u4ece50.4\u63d0\u9ad8\u523054.2\uff1b\u7ed3\u5408CNN\u6a21\u578b\u540e\uff0cF1\u5206\u6570\u8fdb\u4e00\u6b65\u63d0\u5347\u81f370.3\u3002", "conclusion": "3MDBench\u4e3aAI\u9a71\u52a8\u7684\u533b\u7597\u52a9\u624b\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u8bc4\u4f30\u5de5\u5177\uff0c\u63ed\u793a\u4e86\u60a3\u8005\u6027\u683c\u3001\u5bf9\u8bdd\u7b56\u7565\u548c\u591a\u6a21\u6001\u63a8\u7406\u5bf9\u8bca\u65ad\u8d28\u91cf\u7684\u5f71\u54cd\uff0c\u63a8\u52a8\u4e86\u66f4\u53ef\u9760\u548c\u60c5\u5883\u611f\u77e5\u7684\u533b\u7597\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.14655", "pdf": "https://arxiv.org/pdf/2504.14655", "abs": "https://arxiv.org/abs/2504.14655", "authors": ["Yunhui Xia", "Wei Shen", "Yan Wang", "Jason Klein Liu", "Huifeng Sun", "Siyue Wu", "Jian Hu", "Xiaolong Xu"], "title": "LeetCodeDataset: A Temporal Dataset for Robust Evaluation and Efficient Training of Code LLMs", "categories": ["cs.LG", "cs.CL", "cs.SE"], "comment": null, "summary": "We introduce LeetCodeDataset, a high-quality benchmark for evaluating and\ntraining code-generation models, addressing two key challenges in LLM research:\nthe lack of reasoning-focused coding benchmarks and self-contained training\ntestbeds. By curating LeetCode Python problems with rich metadata, broad\ncoverage, 100+ test cases per problem, and temporal splits (pre/post July\n2024), our dataset enables contamination-free evaluation and efficient\nsupervised fine-tuning (SFT). Experiments show reasoning models significantly\noutperform non-reasoning counterparts, while SFT with only 2.6K model-generated\nsolutions achieves performance comparable to 110K-sample counterparts. The\ndataset and evaluation framework are available on Hugging Face and Github.", "AI": {"tldr": "LeetCodeDataset\u662f\u4e00\u4e2a\u9ad8\u8d28\u91cf\u4ee3\u7801\u751f\u6210\u6a21\u578b\u8bc4\u4f30\u4e0e\u8bad\u7ec3\u57fa\u51c6\uff0c\u89e3\u51b3\u4e86LLM\u7814\u7a76\u4e2d\u7f3a\u4e4f\u63a8\u7406\u5bfc\u5411\u7684\u7f16\u7801\u57fa\u51c6\u548c\u81ea\u5305\u542b\u8bad\u7ec3\u6d4b\u8bd5\u7684\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3LLM\u7814\u7a76\u4e2d\u7f3a\u4e4f\u63a8\u7406\u5bfc\u5411\u7684\u7f16\u7801\u57fa\u51c6\u548c\u81ea\u5305\u542b\u8bad\u7ec3\u6d4b\u8bd5\u7684\u6311\u6218\u3002", "method": "\u901a\u8fc7\u6574\u7406LeetCode Python\u95ee\u9898\uff0c\u63d0\u4f9b\u4e30\u5bcc\u5143\u6570\u636e\u3001\u5e7f\u6cdb\u8986\u76d6\u3001\u6bcf\u4e2a\u95ee\u9898100+\u6d4b\u8bd5\u7528\u4f8b\u53ca\u65f6\u95f4\u5206\u5272\uff082024\u5e747\u6708\u524d\u540e\uff09\uff0c\u652f\u6301\u65e0\u6c61\u67d3\u8bc4\u4f30\u548c\u9ad8\u6548\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u63a8\u7406\u6a21\u578b\u663e\u8457\u4f18\u4e8e\u975e\u63a8\u7406\u6a21\u578b\uff0c\u4ec5\u75282.6K\u6a21\u578b\u751f\u6210\u89e3\u51b3\u65b9\u6848\u7684SFT\u6027\u80fd\u4e0e110K\u6837\u672c\u76f8\u5f53\u3002", "conclusion": "LeetCodeDataset\u4e3a\u4ee3\u7801\u751f\u6210\u6a21\u578b\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u8bc4\u4f30\u548c\u8bad\u7ec3\u8d44\u6e90\uff0c\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u6846\u67b6\u5df2\u5f00\u6e90\u3002"}}
{"id": "2504.13877", "pdf": "https://arxiv.org/pdf/2504.13877", "abs": "https://arxiv.org/abs/2504.13877", "authors": ["Ionut Anghel", "Tudor Cioara", "Roberta Bevilacqua", "Federico Barbarossa", "Terje Grimstad", "Riitta Hellman", "Arnor Solberg", "Lars Thomas Boye", "Ovidiu Anchidin", "Ancuta Nemes", "Camilla Gabrielsen"], "title": "New care pathways for supporting transitional care from hospitals to home using AI and personalized digital assistance", "categories": ["cs.HC", "cs.AI"], "comment": "submitted to journal (under review)", "summary": "Transitional care may play a vital role for the sustainability of Europe\nfuture healthcare system, offering solutions for relocating patient care from\nhospital to home therefore addressing the growing demand for medical care as\nthe population is ageing. However, to be effective, it is essential to\nintegrate innovative Information and Communications Technology technologies to\nensure that patients with comorbidities experience a smooth and coordinated\ntransition from hospitals or care centers to home, thereby reducing the risk of\nrehospitalization. In this paper, we present an overview of the integration of\nInternet of Things, artificial intelligence, and digital assistance\ntechnologies with traditional care pathways to address the challenges and needs\nof healthcare systems in Europe. We identify the current gaps in transitional\ncare and define the technology mapping to enhance the care pathways, aiming to\nimprove patient outcomes, safety, and quality of life avoiding hospital\nreadmissions. Finally, we define the trial setup and evaluation methodology\nneeded to provide clinical evidence that supports the positive impact of\ntechnology integration on patient care and discuss the potential effects on the\nhealthcare system.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u8fc7\u6e21\u6027\u62a4\u7406\u5728\u6b27\u6d32\u533b\u7597\u7cfb\u7edf\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u63d0\u51fa\u901a\u8fc7\u6574\u5408\u7269\u8054\u7f51\u3001\u4eba\u5de5\u667a\u80fd\u548c\u6570\u5b57\u8f85\u52a9\u6280\u672f\u6765\u4f18\u5316\u60a3\u8005\u4ece\u533b\u9662\u5230\u5bb6\u5ead\u7684\u62a4\u7406\u8fc7\u6e21\uff0c\u4ee5\u51cf\u5c11\u518d\u4f4f\u9662\u98ce\u9669\u3002", "motivation": "\u968f\u7740\u4eba\u53e3\u8001\u9f84\u5316\uff0c\u533b\u7597\u9700\u6c42\u589e\u52a0\uff0c\u8fc7\u6e21\u6027\u62a4\u7406\u6210\u4e3a\u89e3\u51b3\u533b\u9662\u8d44\u6e90\u538b\u529b\u7684\u5173\u952e\u3002\u6574\u5408\u521b\u65b0\u6280\u672f\u53ef\u63d0\u5347\u62a4\u7406\u534f\u8c03\u6027\uff0c\u6539\u5584\u60a3\u8005\u4f53\u9a8c\u3002", "method": "\u901a\u8fc7\u5206\u6790\u5f53\u524d\u8fc7\u6e21\u6027\u62a4\u7406\u7684\u4e0d\u8db3\uff0c\u63d0\u51fa\u6280\u672f\u6620\u5c04\u65b9\u6848\uff0c\u5e76\u7ed3\u5408\u7269\u8054\u7f51\u3001\u4eba\u5de5\u667a\u80fd\u548c\u6570\u5b57\u8f85\u52a9\u6280\u672f\u4f18\u5316\u62a4\u7406\u8def\u5f84\u3002", "result": "\u6280\u672f\u6574\u5408\u6709\u671b\u63d0\u5347\u60a3\u8005\u62a4\u7406\u6548\u679c\u3001\u5b89\u5168\u6027\u548c\u751f\u6d3b\u8d28\u91cf\uff0c\u51cf\u5c11\u518d\u4f4f\u9662\u7387\u3002", "conclusion": "\u672c\u6587\u4e3a\u6280\u672f\u6574\u5408\u63d0\u4f9b\u4e86\u8bd5\u9a8c\u8bbe\u8ba1\u548c\u8bc4\u4f30\u65b9\u6cd5\uff0c\u652f\u6301\u5176\u5bf9\u533b\u7597\u7cfb\u7edf\u7684\u79ef\u6781\u5f71\u54cd\u3002"}}
{"id": "2504.14662", "pdf": "https://arxiv.org/pdf/2504.14662", "abs": "https://arxiv.org/abs/2504.14662", "authors": ["Yeoreum Lee", "Jinwook Jung", "Sungyong Baik"], "title": "Mitigating Parameter Interference in Model Merging via Sharpness-Aware Fine-Tuning", "categories": ["cs.LG", "cs.CV"], "comment": "ICLR 2025", "summary": "Large-scale deep learning models with a pretraining-finetuning paradigm have\nled to a surge of numerous task-specific models fine-tuned from a common\npre-trained model. Recently, several research efforts have been made on merging\nthese large models into a single multi-task model, particularly with simple\narithmetic on parameters. Such merging methodology faces a central challenge:\ninterference between model parameters fine-tuned on different tasks. Few recent\nworks have focused on designing a new fine-tuning scheme that can lead to small\nparameter interference, however at the cost of the performance of each\ntask-specific fine-tuned model and thereby limiting that of a merged model. To\nimprove the performance of a merged model, we note that a fine-tuning scheme\nshould aim for (1) smaller parameter interference and (2) better performance of\neach fine-tuned model on the corresponding task. In this work, we aim to design\na new fine-tuning objective function to work towards these two goals. In the\ncourse of this process, we find such objective function to be strikingly\nsimilar to sharpness-aware minimization (SAM) objective function, which aims to\nachieve generalization by finding flat minima. Drawing upon our observation, we\npropose to fine-tune pre-trained models via sharpness-aware minimization. The\nexperimental and theoretical results showcase the effectiveness and\northogonality of our proposed approach, improving performance upon various\nmerging and fine-tuning methods. Our code is available at\nhttps://github.com/baiklab/SAFT-Merge.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u5fae\u8c03\u76ee\u6807\u51fd\u6570\uff0c\u901a\u8fc7\u9510\u5ea6\u611f\u77e5\u6700\u5c0f\u5316\uff08SAM\uff09\u51cf\u5c11\u53c2\u6570\u5e72\u6270\u5e76\u63d0\u5347\u4efb\u52a1\u6027\u80fd\uff0c\u4ece\u800c\u63d0\u9ad8\u5408\u5e76\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u6a21\u578b\u5fae\u8c03\u540e\u5408\u5e76\u65f6\u53c2\u6570\u5e72\u6270\u7684\u95ee\u9898\uff0c\u540c\u65f6\u63d0\u5347\u5355\u4efb\u52a1\u6027\u80fd\u548c\u5408\u5e76\u6a21\u578b\u6027\u80fd\u3002", "method": "\u8bbe\u8ba1\u65b0\u7684\u5fae\u8c03\u76ee\u6807\u51fd\u6570\uff0c\u57fa\u4e8e\u9510\u5ea6\u611f\u77e5\u6700\u5c0f\u5316\uff08SAM\uff09\u8fdb\u884c\u5fae\u8c03\u3002", "result": "\u5b9e\u9a8c\u548c\u7406\u8bba\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6709\u6548\u4e14\u4e0e\u5176\u4ed6\u65b9\u6cd5\u6b63\u4ea4\uff0c\u63d0\u5347\u4e86\u5408\u5e76\u548c\u5fae\u8c03\u7684\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7SAM\u5fae\u8c03\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u53c2\u6570\u5e72\u6270\u5e76\u63d0\u5347\u4e86\u5408\u5e76\u6a21\u578b\u7684\u6027\u80fd\u3002"}}
{"id": "2504.13884", "pdf": "https://arxiv.org/pdf/2504.13884", "abs": "https://arxiv.org/abs/2504.13884", "authors": ["Karan Taneja", "Anjali Singh", "Ashok K. Goel"], "title": "Towards a Multimodal Document-grounded Conversational AI System for Education", "categories": ["cs.HC", "cs.AI", "cs.CV"], "comment": "15 pages, 4 figures, AIED 2025", "summary": "Multimedia learning using text and images has been shown to improve learning\noutcomes compared to text-only instruction. But conversational AI systems in\neducation predominantly rely on text-based interactions while multimodal\nconversations for multimedia learning remain unexplored. Moreover, deploying\nconversational AI in learning contexts requires grounding in reliable sources\nand verifiability to create trust. We present MuDoC, a Multimodal\nDocument-grounded Conversational AI system based on GPT-4o, that leverages both\ntext and visuals from documents to generate responses interleaved with text and\nimages. Its interface allows verification of AI generated content through\nseamless navigation to the source. We compare MuDoC to a text-only system to\nexplore differences in learner engagement, trust in AI system, and their\nperformance on problem-solving tasks. Our findings indicate that both visuals\nand verifiability of content enhance learner engagement and foster trust;\nhowever, no significant impact in performance was observed. We draw upon\ntheories from cognitive and learning sciences to interpret the findings and\nderive implications, and outline future directions for the development of\nmultimodal conversational AI systems in education.", "AI": {"tldr": "MuDoC\u662f\u4e00\u4e2a\u57fa\u4e8eGPT-4o\u7684\u591a\u6a21\u6001\u5bf9\u8bddAI\u7cfb\u7edf\uff0c\u7ed3\u5408\u6587\u672c\u548c\u56fe\u50cf\u63d0\u5347\u5b66\u4e60\u6548\u679c\uff0c\u9a8c\u8bc1\u5185\u5bb9\u53ef\u589e\u5f3a\u4fe1\u4efb\uff0c\u4f46\u5bf9\u5b66\u4e60\u8868\u73b0\u65e0\u663e\u8457\u5f71\u54cd\u3002", "motivation": "\u63a2\u7d22\u591a\u6a21\u6001\u5bf9\u8bddAI\u5728\u6559\u80b2\u4e2d\u7684\u5e94\u7528\uff0c\u89e3\u51b3\u73b0\u6709\u6587\u672c\u4ea4\u4e92\u7cfb\u7edf\u7684\u5c40\u9650\u6027\uff0c\u5e76\u589e\u5f3a\u5185\u5bb9\u7684\u53ef\u4fe1\u5ea6\u3002", "method": "\u57fa\u4e8eGPT-4o\u5f00\u53d1MuDoC\u7cfb\u7edf\uff0c\u7ed3\u5408\u6587\u6863\u4e2d\u7684\u6587\u672c\u548c\u56fe\u50cf\u751f\u6210\u591a\u6a21\u6001\u54cd\u5e94\uff0c\u5e76\u4e0e\u7eaf\u6587\u672c\u7cfb\u7edf\u5bf9\u6bd4\u3002", "result": "\u591a\u6a21\u6001\u548c\u5185\u5bb9\u9a8c\u8bc1\u63d0\u5347\u4e86\u5b66\u4e60\u8005\u7684\u53c2\u4e0e\u5ea6\u548c\u4fe1\u4efb\uff0c\u4f46\u5bf9\u95ee\u9898\u89e3\u51b3\u8868\u73b0\u65e0\u663e\u8457\u5f71\u54cd\u3002", "conclusion": "\u591a\u6a21\u6001\u5bf9\u8bddAI\u5728\u6559\u80b2\u4e2d\u6709\u6f5c\u529b\uff0c\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u5176\u5bf9\u5b66\u4e60\u6548\u679c\u7684\u5f71\u54cd\u3002"}}
{"id": "2504.13882", "pdf": "https://arxiv.org/pdf/2504.13882", "abs": "https://arxiv.org/abs/2504.13882", "authors": ["Megan Gu", "Chloe Qianhui Zhao", "Claire Liu", "Nikhil Patel", "Jahnvi Shah", "Jionghao Lin", "Kenneth R. Koedinger"], "title": "Toward Automated Qualitative Analysis: Leveraging Large Language Models for Tutoring Dialogue Evaluation", "categories": ["cs.HC", "cs.CL"], "comment": "Manuscript accepted to the Workshop on \"From Data to Discovery: LLMs\n  for Qualitative Analysis in Education\" at LAK25", "summary": "Our study introduces an automated system leveraging large language models\n(LLMs) to assess the effectiveness of five key tutoring strategies: 1. giving\neffective praise, 2. reacting to errors, 3. determining what students know, 4.\nhelping students manage inequity, and 5. responding to negative self-talk.\nUsing a public dataset from the Teacher-Student Chatroom Corpus, our system\nclassifies each tutoring strategy as either being employed as desired or\nundesired. Our study utilizes GPT-3.5 with few-shot prompting to assess the use\nof these strategies and analyze tutoring dialogues. The results show that for\nthe five tutoring strategies, True Negative Rates (TNR) range from 0.655 to\n0.738, and Recall ranges from 0.327 to 0.432, indicating that the model is\neffective at excluding incorrect classifications but struggles to consistently\nidentify the correct strategy. The strategy \\textit{helping students manage\ninequity} showed the highest performance with a TNR of 0.738 and Recall of\n0.432. The study highlights the potential of LLMs in tutoring strategy analysis\nand outlines directions for future improvements, including incorporating more\nadvanced models for more nuanced feedback.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5229\u7528GPT-3.5\u81ea\u52a8\u8bc4\u4f30\u4e94\u79cd\u8f85\u5bfc\u7b56\u7565\u7684\u6709\u6548\u6027\uff0c\u7ed3\u679c\u663e\u793a\u6a21\u578b\u5728\u6392\u9664\u9519\u8bef\u5206\u7c7b\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u51c6\u786e\u8bc6\u522b\u7b56\u7565\u4e0a\u4ecd\u6709\u4e0d\u8db3\u3002", "motivation": "\u63a2\u7d22\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5206\u6790\u8f85\u5bfc\u7b56\u7565\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002", "method": "\u4f7f\u7528GPT-3.5\u548c\u5c11\u91cf\u793a\u4f8b\u63d0\u793a\u5bf9\u516c\u5f00\u6570\u636e\u96c6\u4e2d\u7684\u8f85\u5bfc\u5bf9\u8bdd\u8fdb\u884c\u5206\u7c7b\u3002", "result": "\u6a21\u578b\u5728\u6392\u9664\u9519\u8bef\u5206\u7c7b\u4e0a\u8868\u73b0\u8f83\u597d\uff08TNR 0.655-0.738\uff09\uff0c\u4f46\u51c6\u786e\u8bc6\u522b\u7b56\u7565\u7684\u53ec\u56de\u7387\u8f83\u4f4e\uff080.327-0.432\uff09\u3002", "conclusion": "LLMs\u5728\u8f85\u5bfc\u7b56\u7565\u5206\u6790\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u672a\u6765\u53ef\u901a\u8fc7\u66f4\u5148\u8fdb\u7684\u6a21\u578b\u63d0\u5347\u6027\u80fd\u3002"}}
{"id": "2504.14667", "pdf": "https://arxiv.org/pdf/2504.14667", "abs": "https://arxiv.org/abs/2504.14667", "authors": ["Kai Zhao", "Zhaohui Yang"], "title": "Efficient Federated Split Learning for Large Language Models over Communication Networks", "categories": ["cs.LG", "cs.NI"], "comment": null, "summary": "Fine-tuning pre-trained large language models (LLM) in a distributed manner\nposes significant challenges on resource-constrained edge devices. To address\nthis challenge, we propose FedsLLM, a novel framework that integrates split\nfederated learning with parameter-efficient fine-tuning techniques. By\nleveraging model splitting and Low-Rank Adaptation (LoRA), FedsLLM reduces the\ncomputational burden on edge devices. Furthermore, the introduction of a\nfederated server facilitates parallel training and enhances privacy. To\naccommodate heterogeneous communication conditions and diverse computational\ncapabilities of edge devices, as well as the impact of LoRA rank selection on\nmodel convergence and training cost, we formulate a joint optimization problem.\nThe formulated problem jointly optimizes subchannel allocation, power control,\nmodel splitting point selection, and LoRA rank configuration, all aimed at\nminimizing total training delay. An alternating optimization algorithm is\ndeveloped to efficiently solve this problem and accelerate the training\nprocess. Simulation results demonstrate that the proposed FedsLLM framework\nachieves comparable model accuracy while significantly reducing client-side\ncomputational requirements. Furthermore, the proposed resource allocation\nscheme and adaptive LoRA rank selection strategy notably reduce the training\nlatency compared to conventional approaches.", "AI": {"tldr": "FedsLLM\u6846\u67b6\u7ed3\u5408\u5206\u5272\u8054\u90a6\u5b66\u4e60\u548c\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u6280\u672f\uff0c\u964d\u4f4e\u8fb9\u7f18\u8bbe\u5907\u8ba1\u7b97\u8d1f\u62c5\uff0c\u4f18\u5316\u8d44\u6e90\u5206\u914d\u548cLoRA\u79e9\u9009\u62e9\uff0c\u663e\u8457\u51cf\u5c11\u8bad\u7ec3\u5ef6\u8fdf\u3002", "motivation": "\u89e3\u51b3\u8d44\u6e90\u53d7\u9650\u8fb9\u7f18\u8bbe\u5907\u4e0a\u5206\u5e03\u5f0f\u5fae\u8c03\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6311\u6218\u3002", "method": "\u7ed3\u5408\u6a21\u578b\u5206\u5272\u548cLoRA\u6280\u672f\uff0c\u5f15\u5165\u8054\u90a6\u670d\u52a1\u5668\u5e76\u884c\u8bad\u7ec3\uff0c\u63d0\u51fa\u8054\u5408\u4f18\u5316\u95ee\u9898\u5e76\u5f00\u53d1\u4ea4\u66ff\u4f18\u5316\u7b97\u6cd5\u3002", "result": "FedsLLM\u5728\u4fdd\u6301\u6a21\u578b\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u5ba2\u6237\u7aef\u8ba1\u7b97\u9700\u6c42\u548c\u8bad\u7ec3\u5ef6\u8fdf\u3002", "conclusion": "FedsLLM\u4e3a\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u5206\u5e03\u5f0f\u5fae\u8c03\u63d0\u4f9b\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u4f18\u5316\u8d44\u6e90\u5229\u7528\u548c\u8bad\u7ec3\u6548\u7387\u3002"}}
{"id": "2504.13888", "pdf": "https://arxiv.org/pdf/2504.13888", "abs": "https://arxiv.org/abs/2504.13888", "authors": ["Paul Taele", "Jung In Koh", "Tracy Hammond"], "title": "Kanji Workbook: A Writing-Based Intelligent Tutoring System for Learning Proper Japanese Kanji Writing Technique with Instructor-Emulated Assessment", "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": null, "summary": "Kanji script writing is a skill that is often introduced to novice Japanese\nforeign language students for achieving Japanese writing mastery, but often\nposes difficulties to students with primarily English fluency due to their its\nvast differences with written English. Instructors often introduce various\npedagogical methods -- such as visual structure and written techniques -- to\nassist students in kanji study, but may lack availability providing direct\nfeedback on students' writing outside of class. Current educational\napplications are also limited due to lacking richer instructor-emulated\nfeedback. We introduce Kanji Workbook, a writing-based intelligent tutoring\nsystem for students to receive intelligent assessment that emulates human\ninstructor feedback. Our interface not only leverages students' computing\ndevices for allowing them to learn, practice, and review the writing of\nprompted characters from their course's kanji script lessons, but also provides\na diverse set of writing assessment metrics -- derived from instructor\ninterviews and classroom observation insights -- through intelligent scoring\nand visual animations. We deployed our interface onto novice- and\nintermediate-level university courses over an entire academic year, and\nobserved that interface users on average achieved higher course grades than\ntheir peers and also reacted positively to our interface's various features.", "AI": {"tldr": "Kanji Workbook\u662f\u4e00\u79cd\u667a\u80fd\u8f85\u5bfc\u7cfb\u7edf\uff0c\u901a\u8fc7\u6a21\u62df\u6559\u5e08\u53cd\u9988\u5e2e\u52a9\u5b66\u751f\u5b66\u4e60\u65e5\u8bed\u6c49\u5b57\u4e66\u5199\uff0c\u63d0\u5347\u8bfe\u7a0b\u6210\u7ee9\u3002", "motivation": "\u9488\u5bf9\u82f1\u8bed\u6bcd\u8bed\u5b66\u751f\u5728\u5b66\u4e60\u65e5\u8bed\u6c49\u5b57\u4e66\u5199\u65f6\u7684\u56f0\u96be\uff0c\u4ee5\u53ca\u73b0\u6709\u6559\u80b2\u5e94\u7528\u53cd\u9988\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u5f00\u53d1\u4e86Kanji Workbook\u7cfb\u7edf\uff0c\u7ed3\u5408\u6559\u5e08\u8bbf\u8c08\u548c\u8bfe\u5802\u89c2\u5bdf\uff0c\u63d0\u4f9b\u667a\u80fd\u8bc4\u5206\u548c\u89c6\u89c9\u52a8\u753b\u53cd\u9988\u3002", "result": "\u4f7f\u7528\u8be5\u7cfb\u7edf\u7684\u5b66\u751f\u5728\u8bfe\u7a0b\u6210\u7ee9\u4e0a\u4f18\u4e8e\u540c\u9f84\u4eba\uff0c\u5e76\u5bf9\u7cfb\u7edf\u529f\u80fd\u53cd\u5e94\u79ef\u6781\u3002", "conclusion": "Kanji Workbook\u901a\u8fc7\u667a\u80fd\u53cd\u9988\u6709\u6548\u8f85\u52a9\u5b66\u751f\u6c49\u5b57\u4e66\u5199\u5b66\u4e60\u3002"}}
{"id": "2504.13887", "pdf": "https://arxiv.org/pdf/2504.13887", "abs": "https://arxiv.org/abs/2504.13887", "authors": ["Isabel Villanueva", "Tara Bobinac", "Binwei Yao", "Junjie Hu", "Kaiping Chen"], "title": "AI as a deliberative partner fosters intercultural empathy for Americans but fails for Latin American participants", "categories": ["cs.HC", "cs.CL", "cs.CY"], "comment": null, "summary": "Despite the growing integration of AI chatbots as conversational agents in\npublic discourse, empirical evidence regarding their capacity to foster\nintercultural empathy remains limited. Using a randomized dialogue experiment,\nwe examined how different types of AI chatbot interaction, i.e., deliberative\nversus non-deliberative and culturally aligned versus non-aligned, affect\nintercultural empathy across cultural groups. Results show that deliberative\nconversations increased intercultural empathy among American participants but\nnot Latin American participants, who perceived AI responses as culturally\ninaccurate and failing to represent their cultural contexts and perspectives\nauthentically. Real-time interaction analyses reveal that these differences\nstem from cultural knowledge gaps inherent in Large Language Models. Despite\nexplicit prompting and instruction to represent cultural perspectives in\nparticipants' native languages, AI systems still exhibit significant\ndisparities in cultural representation. This highlights the importance of\ndesigning AI systems capable of culturally authentic engagement in deliberative\nconversations. Our study contributes to deliberation theory and AI alignment\nresearch by underscoring AI's role in intercultural dialogue and the persistent\nchallenge of representational asymmetry in democratic discourse.", "AI": {"tldr": "AI\u804a\u5929\u673a\u5668\u4eba\u5728\u8de8\u6587\u5316\u5bf9\u8bdd\u4e2d\u4fc3\u8fdb\u5171\u60c5\u7684\u80fd\u529b\u6709\u9650\uff0c\u5c24\u5176\u662f\u6587\u5316\u5bf9\u9f50\u4e0d\u8db3\u65f6\u3002", "motivation": "\u7814\u7a76AI\u804a\u5929\u673a\u5668\u4eba\u5728\u8de8\u6587\u5316\u5bf9\u8bdd\u4e2d\u662f\u5426\u80fd\u4fc3\u8fdb\u5171\u60c5\uff0c\u586b\u8865\u5b9e\u8bc1\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u91c7\u7528\u968f\u673a\u5bf9\u8bdd\u5b9e\u9a8c\uff0c\u6bd4\u8f83\u5ba1\u8bae\u4e0e\u975e\u5ba1\u8bae\u3001\u6587\u5316\u5bf9\u9f50\u4e0e\u975e\u5bf9\u9f50\u7684AI\u4e92\u52a8\u3002", "result": "\u5ba1\u8bae\u5bf9\u8bdd\u63d0\u5347\u4e86\u7f8e\u56fd\u53c2\u4e0e\u8005\u7684\u8de8\u6587\u5316\u5171\u60c5\uff0c\u4f46\u5bf9\u62c9\u4e01\u7f8e\u6d32\u53c2\u4e0e\u8005\u65e0\u6548\uff0c\u56e0AI\u6587\u5316\u8868\u8fbe\u4e0d\u51c6\u786e\u3002", "conclusion": "AI\u9700\u6539\u8fdb\u6587\u5316\u771f\u5b9e\u6027\uff0c\u4ee5\u652f\u6301\u8de8\u6587\u5316\u5ba1\u8bae\u5bf9\u8bdd\uff0c\u89e3\u51b3\u6c11\u4e3b\u8bdd\u8bed\u4e2d\u7684\u4ee3\u8868\u4e0d\u5bf9\u79f0\u95ee\u9898\u3002"}}
{"id": "2504.14677", "pdf": "https://arxiv.org/pdf/2504.14677", "abs": "https://arxiv.org/abs/2504.14677", "authors": ["Jia Liu", "Cheng Jinguo", "Xia Fang", "Zhenyuan Ma", "Yuankai Wu"], "title": "Evaluating Temporal Plasticity in Foundation Time Series Models for Incremental Fine-tuning", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted at IJCNN 2025", "summary": "Time series foundation models excel at diverse time series forecasting tasks,\nbut their capacity for continuous improvement through incremental learning\nremains unexplored. We present the first comprehensive study investigating\nthese models' temporal plasticity - their ability to progressively enhance\nperformance through continual learning while maintaining existing capabilities.\nThrough experiments on real-world datasets exhibiting distribution shifts, we\nevaluate both conventional deep learning models and foundation models using a\nnovel continual learning framework. Our findings reveal that while traditional\nmodels struggle with performance deterioration during incremental fine-tuning,\nfoundation models like Time-MoE and Chronos demonstrate sustained improvement\nin predictive accuracy. This suggests that optimizing foundation model\nfine-tuning strategies may be more valuable than developing domain-specific\nsmall models. Our research introduces new evaluation methodologies and insights\nfor developing foundation time series models with robust continuous learning\ncapabilities.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u901a\u8fc7\u589e\u91cf\u5b66\u4e60\u6301\u7eed\u6539\u8fdb\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u5176\u4f18\u4e8e\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u3002", "motivation": "\u63a2\u7d22\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u5728\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u8868\u73b0\uff0c\u586b\u8865\u5176\u589e\u91cf\u5b66\u4e60\u80fd\u529b\u7684\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\uff0c\u901a\u8fc7\u65b0\u9896\u7684\u6301\u7eed\u5b66\u4e60\u6846\u67b6\u8bc4\u4f30\u4f20\u7edf\u6a21\u578b\u548c\u57fa\u7840\u6a21\u578b\u3002", "result": "\u57fa\u7840\u6a21\u578b\uff08\u5982Time-MoE\u548cChronos\uff09\u5728\u589e\u91cf\u5fae\u8c03\u4e2d\u8868\u73b0\u66f4\u4f18\uff0c\u9884\u6d4b\u51c6\u786e\u6027\u6301\u7eed\u63d0\u5347\u3002", "conclusion": "\u4f18\u5316\u57fa\u7840\u6a21\u578b\u7684\u5fae\u8c03\u7b56\u7565\u6bd4\u5f00\u53d1\u9886\u57df\u7279\u5b9a\u5c0f\u6a21\u578b\u66f4\u6709\u4ef7\u503c\uff0c\u4e3a\u6301\u7eed\u5b66\u4e60\u80fd\u529b\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2504.13889", "pdf": "https://arxiv.org/pdf/2504.13889", "abs": "https://arxiv.org/abs/2504.13889", "authors": ["Paul Taele", "Laura Barreto", "Tracy Hammond"], "title": "Maestoso: An Intelligent Educational Sketching Tool for Learning Music Theory", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Learning music theory not only has practical benefits for musicians to write,\nperform, understand, and express music better, but also for both non-musicians\nto improve critical thinking, math analytical skills, and music appreciation.\nHowever, current external tools applicable for learning music theory through\nwriting when human instruction is unavailable are either limited in feedback,\nlacking a written modality, or assuming already strong familiarity of music\ntheory concepts. In this paper, we describe Maestoso, an educational tool for\nnovice learners to learn music theory through sketching practice of quizzed\nmusic structures. Maestoso first automatically recognizes students' sketched\ninput of quizzed concepts, then relies on existing sketch and gesture\nrecognition techniques to automatically recognize the input, and finally\ngenerates instructor-emulated feedback. From our evaluations, we demonstrate\nthat Maestoso performs reasonably well on recognizing music structure elements\nand that novice students can comfortably grasp introductory music theory in a\nsingle session.", "AI": {"tldr": "Maestoso\u662f\u4e00\u4e2a\u6559\u80b2\u5de5\u5177\uff0c\u5e2e\u52a9\u521d\u5b66\u8005\u901a\u8fc7\u8349\u56fe\u7ec3\u4e60\u5b66\u4e60\u97f3\u4e50\u7406\u8bba\uff0c\u63d0\u4f9b\u81ea\u52a8\u8bc6\u522b\u548c\u53cd\u9988\u3002", "motivation": "\u73b0\u6709\u5de5\u5177\u5728\u53cd\u9988\u3001\u4e66\u5199\u6a21\u5f0f\u6216\u97f3\u4e50\u7406\u8bba\u719f\u6089\u5ea6\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u5b66\u4e60\u5de5\u5177\u3002", "method": "Maestoso\u901a\u8fc7\u81ea\u52a8\u8bc6\u522b\u5b66\u751f\u7ed8\u5236\u7684\u97f3\u4e50\u7ed3\u6784\u8349\u56fe\uff0c\u5229\u7528\u73b0\u6709\u6280\u672f\u751f\u6210\u6a21\u62df\u6559\u5e08\u53cd\u9988\u3002", "result": "Maestoso\u80fd\u8f83\u597d\u8bc6\u522b\u97f3\u4e50\u7ed3\u6784\u5143\u7d20\uff0c\u521d\u5b66\u8005\u53ef\u5728\u4e00\u8282\u8bfe\u4e2d\u638c\u63e1\u57fa\u7840\u97f3\u4e50\u7406\u8bba\u3002", "conclusion": "Maestoso\u4e3a\u97f3\u4e50\u7406\u8bba\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u8f85\u52a9\u5de5\u5177\u3002"}}
{"id": "2504.14694", "pdf": "https://arxiv.org/pdf/2504.14694", "abs": "https://arxiv.org/abs/2504.14694", "authors": ["Yuting He", "Yiqiang Chen", "XiaoDong Yang", "Hanchao Yu", "Yi-Hua Huang", "Yang Gu"], "title": "Learning Critically: Selective Self Distillation in Federated Learning on Non-IID Data", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Federated learning (FL) enables multiple clients to collaboratively train a\nglobal model while keeping local data decentralized. Data heterogeneity\n(non-IID) across clients has imposed significant challenges to FL, which makes\nlocal models re-optimize towards their own local optima and forget the global\nknowledge, resulting in performance degradation and convergence slowdown. Many\nexisting works have attempted to address the non-IID issue by adding an extra\nglobal-model-based regularizing item to the local training but without an\nadaption scheme, which is not efficient enough to achieve high performance with\ndeep learning models. In this paper, we propose a Selective Self-Distillation\nmethod for Federated learning (FedSSD), which imposes adaptive constraints on\nthe local updates by self-distilling the global model's knowledge and\nselectively weighting it by evaluating the credibility at both the class and\nsample level. The convergence guarantee of FedSSD is theoretically analyzed and\nextensive experiments are conducted on three public benchmark datasets, which\ndemonstrates that FedSSD achieves better generalization and robustness in fewer\ncommunication rounds, compared with other state-of-the-art FL methods.", "AI": {"tldr": "FedSSD\u63d0\u51fa\u4e86\u4e00\u79cd\u9009\u62e9\u6027\u81ea\u84b8\u998f\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u7ea6\u675f\u5c40\u90e8\u66f4\u65b0\uff0c\u63d0\u5347\u8054\u90a6\u5b66\u4e60\u5728\u975eIID\u6570\u636e\u4e0b\u7684\u6027\u80fd\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u4e2d\u6570\u636e\u5f02\u6784\u6027\uff08\u975eIID\uff09\u5bfc\u81f4\u5c40\u90e8\u6a21\u578b\u504f\u5411\u5c40\u90e8\u6700\u4f18\u800c\u9057\u5fd8\u5168\u5c40\u77e5\u8bc6\uff0c\u5f71\u54cd\u6027\u80fd\u548c\u6536\u655b\u901f\u5ea6\u3002\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u9002\u5e94\u6027\uff0c\u6548\u7387\u4e0d\u8db3\u3002", "method": "FedSSD\u901a\u8fc7\u81ea\u84b8\u998f\u5168\u5c40\u6a21\u578b\u77e5\u8bc6\uff0c\u5e76\u5728\u7c7b\u548c\u6837\u672c\u7ea7\u522b\u8bc4\u4f30\u53ef\u4fe1\u5ea6\u9009\u62e9\u6027\u52a0\u6743\uff0c\u81ea\u9002\u5e94\u7ea6\u675f\u5c40\u90e8\u66f4\u65b0\u3002", "result": "\u7406\u8bba\u5206\u6790\u8bc1\u660e\u6536\u655b\u6027\uff0c\u5b9e\u9a8c\u8868\u660eFedSSD\u5728\u66f4\u5c11\u901a\u4fe1\u8f6e\u6b21\u4e2d\u4f18\u4e8e\u5176\u4ed6\u5148\u8fdb\u65b9\u6cd5\uff0c\u5177\u6709\u66f4\u597d\u7684\u6cdb\u5316\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "FedSSD\u6709\u6548\u89e3\u51b3\u4e86\u975eIID\u6570\u636e\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u8054\u90a6\u5b66\u4e60\u7684\u6027\u80fd\u548c\u6548\u7387\u3002"}}
{"id": "2504.13891", "pdf": "https://arxiv.org/pdf/2504.13891", "abs": "https://arxiv.org/abs/2504.13891", "authors": ["Wanfang Xu", "Lixiang Zhao", "Haiwen Song", "Xinheng Song", "Zhaolin Lu", "Yu Liu", "Min Chen", "Eng Gee Lim", "Lingyun Yu"], "title": "Mozualization: Crafting Music and Visual Representation with Multimodal AI", "categories": ["cs.HC", "cs.AI"], "comment": "7 pages, 5 figures, CHI2025", "summary": "In this work, we introduce Mozualization, a music generation and editing tool\nthat creates multi-style embedded music by integrating diverse inputs, such as\nkeywords, images, and sound clips (e.g., segments from various pieces of music\nor even a playful cat's meow). Our work is inspired by the ways people express\ntheir emotions -- writing mood-descriptive poems or articles, creating drawings\nwith warm or cool tones, or listening to sad or uplifting music. Building on\nthis concept, we developed a tool that transforms these emotional expressions\ninto a cohesive and expressive song, allowing users to seamlessly incorporate\ntheir unique preferences and inspirations. To evaluate the tool and, more\nimportantly, gather insights for its improvement, we conducted a user study\ninvolving nine music enthusiasts. The study assessed user experience,\nengagement, and the impact of interacting with and listening to the generated\nmusic.", "AI": {"tldr": "Mozualization\u662f\u4e00\u4e2a\u97f3\u4e50\u751f\u6210\u548c\u7f16\u8f91\u5de5\u5177\uff0c\u901a\u8fc7\u6574\u5408\u5173\u952e\u8bcd\u3001\u56fe\u50cf\u548c\u58f0\u97f3\u7247\u6bb5\u7b49\u591a\u6837\u5316\u8f93\u5165\uff0c\u751f\u6210\u591a\u98ce\u683c\u5d4c\u5165\u5f0f\u97f3\u4e50\u3002", "motivation": "\u53d7\u4eba\u4eec\u901a\u8fc7\u8bd7\u6b4c\u3001\u7ed8\u753b\u6216\u97f3\u4e50\u8868\u8fbe\u60c5\u611f\u7684\u542f\u53d1\uff0c\u5f00\u53d1\u4e86\u80fd\u5c06\u60c5\u611f\u8868\u8fbe\u8f6c\u5316\u4e3a\u8fde\u8d2f\u6b4c\u66f2\u7684\u5de5\u5177\u3002", "method": "\u5f00\u53d1\u4e86Mozualization\u5de5\u5177\uff0c\u5e76\u901a\u8fc7\u7528\u6237\u7814\u7a76\uff089\u4f4d\u97f3\u4e50\u7231\u597d\u8005\uff09\u8bc4\u4f30\u7528\u6237\u4f53\u9a8c\u548c\u6539\u8fdb\u65b9\u5411\u3002", "result": "\u7528\u6237\u7814\u7a76\u8bc4\u4f30\u4e86\u5de5\u5177\u7684\u7528\u6237\u4f53\u9a8c\u3001\u53c2\u4e0e\u5ea6\u53ca\u751f\u6210\u97f3\u4e50\u7684\u5f71\u54cd\u3002", "conclusion": "Mozualization\u6210\u529f\u5c06\u591a\u6837\u5316\u8f93\u5165\u8f6c\u5316\u4e3a\u4e2a\u6027\u5316\u97f3\u4e50\uff0c\u7528\u6237\u7814\u7a76\u4e3a\u5176\u6539\u8fdb\u63d0\u4f9b\u4e86\u5b9d\u8d35\u89c1\u89e3\u3002"}}
{"id": "2504.13890", "pdf": "https://arxiv.org/pdf/2504.13890", "abs": "https://arxiv.org/abs/2504.13890", "authors": ["Chen Shani", "Elizabeth C. Stade"], "title": "Measuring Mental Health Variables in Computational Research: Toward Validated, Dimensional, and Transdiagnostic Approaches", "categories": ["cs.HC", "cs.CL"], "comment": "No figures, 1 Table", "summary": "Computational mental health research develops models to predict and\nunderstand psychological phenomena, but often relies on inappropriate measures\nof psychopathology constructs, undermining validity. We identify three key\nissues: (1) reliance on unvalidated measures (e.g., self-declared diagnosis)\nover validated ones (e.g., diagnosis by clinician); (2) treating mental health\nconstructs as categorical rather than dimensional; and (3) focusing on\ndisorder-specific constructs instead of transdiagnostic ones. We outline the\nbenefits of using validated, dimensional, and transdiagnostic measures and\noffer practical recommendations for practitioners. Using valid measures that\nreflect the nature and structure of psychopathology is essential for\ncomputational mental health research.", "AI": {"tldr": "\u8bba\u6587\u6307\u51fa\u8ba1\u7b97\u5fc3\u7406\u5065\u5eb7\u7814\u7a76\u4e2d\u5b58\u5728\u6d4b\u91cf\u5de5\u5177\u4e0d\u5f53\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4f7f\u7528\u5df2\u9a8c\u8bc1\u3001\u7ef4\u5ea6\u548c\u8de8\u8bca\u65ad\u6d4b\u91cf\u5de5\u5177\u7684\u5efa\u8bae\u3002", "motivation": "\u5f53\u524d\u8ba1\u7b97\u5fc3\u7406\u5065\u5eb7\u7814\u7a76\u5e38\u4f9d\u8d56\u4e0d\u9002\u5f53\u7684\u5fc3\u7406\u75c5\u7406\u5b66\u6d4b\u91cf\u5de5\u5177\uff0c\u5f71\u54cd\u7814\u7a76\u6548\u5ea6\u3002", "method": "\u8bc6\u522b\u4e09\u4e2a\u5173\u952e\u95ee\u9898\uff1a\u4f9d\u8d56\u672a\u9a8c\u8bc1\u7684\u6d4b\u91cf\u5de5\u5177\u3001\u5c06\u5fc3\u7406\u5065\u5eb7\u6784\u5ff5\u89c6\u4e3a\u5206\u7c7b\u800c\u975e\u7ef4\u5ea6\u3001\u5173\u6ce8\u7279\u5b9a\u969c\u788d\u800c\u975e\u8de8\u8bca\u65ad\u6784\u5ff5\u3002", "result": "\u63d0\u51fa\u4f7f\u7528\u5df2\u9a8c\u8bc1\u3001\u7ef4\u5ea6\u548c\u8de8\u8bca\u65ad\u6d4b\u91cf\u5de5\u5177\u7684\u4f18\u52bf\uff0c\u5e76\u4e3a\u5b9e\u8df5\u8005\u63d0\u4f9b\u5efa\u8bae\u3002", "conclusion": "\u4f7f\u7528\u53cd\u6620\u5fc3\u7406\u75c5\u7406\u5b66\u672c\u8d28\u548c\u7ed3\u6784\u7684\u6709\u6548\u6d4b\u91cf\u5de5\u5177\u5bf9\u8ba1\u7b97\u5fc3\u7406\u5065\u5eb7\u7814\u7a76\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2504.14697", "pdf": "https://arxiv.org/pdf/2504.14697", "abs": "https://arxiv.org/abs/2504.14697", "authors": ["Shi Chen", "Zhengjiang Lin", "Yury Polyanskiy", "Philippe Rigollet"], "title": "Quantitative Clustering in Mean-Field Transformer Models", "categories": ["cs.LG", "math.AP", "math.DS", "stat.ML"], "comment": "47 pages, 4 figures", "summary": "The evolution of tokens through a deep transformer models can be modeled as\nan interacting particle system that has been shown to exhibit an asymptotic\nclustering behavior akin to the synchronization phenomenon in Kuramoto models.\nIn this work, we investigate the long-time clustering of mean-field transformer\nmodels. More precisely, we establish exponential rates of contraction to a\nDirac point mass for any suitably regular initialization under some assumptions\non the parameters of transformer models, any suitably regular mean-field\ninitialization synchronizes exponentially fast with some quantitative rates.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u6df1\u5ea6Transformer\u6a21\u578b\u4e2dtoken\u7684\u6f14\u5316\u884c\u4e3a\uff0c\u7c7b\u6bd4\u4e3a\u7c92\u5b50\u7cfb\u7edf\uff0c\u5e76\u63a2\u8ba8\u4e86\u5176\u5728\u957f\u65f6\u95f4\u5c3a\u5ea6\u4e0b\u7684\u805a\u7c7b\u73b0\u8c61\uff0c\u7c7b\u4f3c\u4e8eKuramoto\u6a21\u578b\u7684\u540c\u6b65\u884c\u4e3a\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u7406\u89e3Transformer\u6a21\u578b\u4e2dtoken\u7684\u52a8\u6001\u6f14\u5316\u884c\u4e3a\u53ca\u5176\u4e0eKuramoto\u6a21\u578b\u540c\u6b65\u73b0\u8c61\u7684\u7c7b\u6bd4\u5173\u7cfb\u3002", "method": "\u65b9\u6cd5\u662f\u901a\u8fc7\u5efa\u7acb\u5747\u503c\u573aTransformer\u6a21\u578b\u7684\u957f\u65f6\u95f4\u805a\u7c7b\u884c\u4e3a\uff0c\u5206\u6790\u5176\u53c2\u6570\u5047\u8bbe\u4e0b\u6b63\u5219\u521d\u59cb\u5316\u7684\u6307\u6570\u6536\u7f29\u7387\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u5728\u7279\u5b9a\u53c2\u6570\u5047\u8bbe\u4e0b\uff0c\u4efb\u4f55\u6b63\u5219\u521d\u59cb\u5316\u7684\u5747\u503c\u573a\u6a21\u578b\u90fd\u4f1a\u4ee5\u6307\u6570\u901f\u5ea6\u540c\u6b65\u5230\u4e00\u4e2aDirac\u70b9\u8d28\u91cf\u3002", "conclusion": "\u7ed3\u8bba\u662fTransformer\u6a21\u578b\u7684token\u6f14\u5316\u8868\u73b0\u51fa\u7c7b\u4f3cKuramoto\u6a21\u578b\u7684\u540c\u6b65\u884c\u4e3a\uff0c\u4e14\u5177\u6709\u5b9a\u91cf\u5316\u7684\u6307\u6570\u6536\u655b\u901f\u7387\u3002"}}
{"id": "2504.13898", "pdf": "https://arxiv.org/pdf/2504.13898", "abs": "https://arxiv.org/abs/2504.13898", "authors": ["Dong Won Lee", "Yubin Kim", "Denison Guvenoz", "Sooyeon Jeong", "Parker Malachowsky", "Louis-Philippe Morency", "Cynthia Breazeal", "Hae Won Park"], "title": "The Human Robot Social Interaction (HSRI) Dataset: Benchmarking Foundational Models' Social Reasoning", "categories": ["cs.HC", "cs.AI"], "comment": "23 pages, 11 figures", "summary": "Our work aims to advance the social reasoning of embodied artificial\nintelligence (AI) agents in real-world social interactions. Recently, language\nmodels (LMs) and foundational models (FMs) are being utilized as automatic\nevaluators of human-AI interactions with the goal of eventually being used to\nimprove the policy of the AI agent. To enable further research in this\ndirection, we introduce a large-scale real-world Human Robot Social Interaction\n(HSRI) Dataset to benchmark the capabilities of LMs and FMs to identify and\nreason about social interactions, specifically with regard to robot social\nerrors and competencies . Our dataset consists of 400 real-world human social\nrobot interaction videos and over 10K annotations, detailing the robot's social\nerrors, competencies, rationale, and corrective actions, capturing unique\naspects of human-AI interaction only present in real-world interactions. To\nfurther assess AI models' ability to reason about social interactions, we\npropose eight new benchmark tasks for evaluating centered around whether AI\nmodels can (1) evaluate social interactions via detecting social errors and\ncompetencies, (2) identify the explanatory factors associated to errors and\ncompetencies, (3) understand the flow of real-world social interactions, and\n(4) provide reasons and corrective actions for social errors. Human studies and\nexperiments with modern LMs and FMs reveal that current models struggle with\nthese tasks, demonstrating that our dataset and benchmark provides a step\nforward towards socially intelligent AI.", "AI": {"tldr": "\u8be5\u8bba\u6587\u65e8\u5728\u63d0\u5347AI\u4ee3\u7406\u5728\u73b0\u5b9e\u793e\u4ea4\u4e92\u52a8\u4e2d\u7684\u793e\u4f1a\u63a8\u7406\u80fd\u529b\uff0c\u901a\u8fc7\u5f15\u5165\u5927\u89c4\u6a21\u771f\u5b9e\u4e16\u754c\u4eba\u673a\u793e\u4ea4\u4e92\u52a8\u6570\u636e\u96c6\uff08HSRI\uff09\u548c\u516b\u9879\u65b0\u57fa\u51c6\u4efb\u52a1\uff0c\u8bc4\u4f30\u8bed\u8a00\u6a21\u578b\u548c\u57fa\u7840\u6a21\u578b\u7684\u793e\u4f1a\u4ea4\u4e92\u7406\u89e3\u80fd\u529b\u3002", "motivation": "\u63a8\u52a8\u5177\u8eabAI\u5728\u771f\u5b9e\u793e\u4ea4\u4e92\u52a8\u4e2d\u7684\u793e\u4f1a\u63a8\u7406\u80fd\u529b\uff0c\u5229\u7528\u8bed\u8a00\u6a21\u578b\u548c\u57fa\u7840\u6a21\u578b\u4f5c\u4e3a\u81ea\u52a8\u8bc4\u4f30\u5de5\u5177\uff0c\u4ee5\u6539\u8fdbAI\u4ee3\u7406\u7684\u7b56\u7565\u3002", "method": "\u6784\u5efa\u5305\u542b400\u4e2a\u771f\u5b9e\u4eba\u673a\u4e92\u52a8\u89c6\u9891\u548c10K\u6ce8\u91ca\u7684HSRI\u6570\u636e\u96c6\uff0c\u8bbe\u8ba1\u516b\u9879\u57fa\u51c6\u4efb\u52a1\u8bc4\u4f30\u6a21\u578b\u7684\u793e\u4f1a\u9519\u8bef\u68c0\u6d4b\u3001\u89e3\u91ca\u56e0\u7d20\u8bc6\u522b\u3001\u4ea4\u4e92\u6d41\u7406\u89e3\u548c\u7ea0\u6b63\u5efa\u8bae\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u5f53\u524d\u6a21\u578b\u5728\u8fd9\u4e9b\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u8868\u660e\u6570\u636e\u96c6\u548c\u57fa\u51c6\u4efb\u52a1\u5bf9\u793e\u4f1a\u667a\u80fdAI\u7684\u53d1\u5c55\u5177\u6709\u63a8\u52a8\u4f5c\u7528\u3002", "conclusion": "HSRI\u6570\u636e\u96c6\u548c\u57fa\u51c6\u4efb\u52a1\u4e3a\u63d0\u5347AI\u7684\u793e\u4f1a\u667a\u80fd\u63d0\u4f9b\u4e86\u91cd\u8981\u5de5\u5177\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u6307\u660e\u4e86\u65b9\u5411\u3002"}}
{"id": "2504.13892", "pdf": "https://arxiv.org/pdf/2504.13892", "abs": "https://arxiv.org/abs/2504.13892", "authors": ["Stefano De Paoli", "Alex Fawzi"], "title": "TALLMesh: a simple application for performing Thematic Analysis with Large Language Models", "categories": ["cs.HC", "cs.CL"], "comment": null, "summary": "Thematic analysis (TA) is a widely used qualitative research method for\nidentifying and interpreting patterns within textual data, such as qualitative\ninterviews. Recent research has shown that it is possible to satisfactorily\nperform TA using Large Language Models (LLMs). This paper presents a novel\napplication using LLMs to assist researchers in conducting TA. The application\nenables users to upload textual data, generate initial codes and themes. All of\nthis is possible through a simple Graphical User Interface, (GUI) based on the\nstreamlit framework, working with python scripts for the analysis, and using\nApplication Program Interfaces of LLMs. Having a GUI is particularly important\nfor researchers in fields where coding skills may not be prevalent, such as\nsocial sciences or humanities. With the app, users can iteratively refine codes\nand themes adopting a human-in-the-loop process, without the need to work with\nprogramming and scripting. The paper describes the application key features,\nhighlighting its potential for qualitative research while preserving\nmethodological rigor. The paper discusses the design and interface of the app\nand outlines future directions for this work.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u56fe\u5f62\u7528\u6237\u754c\u9762\uff08GUI\uff09\u5e94\u7528\uff0c\u7528\u4e8e\u8f85\u52a9\u7814\u7a76\u4eba\u5458\u8fdb\u884c\u4e3b\u9898\u5206\u6790\uff08TA\uff09\uff0c\u7279\u522b\u9002\u5408\u7f3a\u4e4f\u7f16\u7a0b\u6280\u80fd\u7684\u7814\u7a76\u8005\u3002", "motivation": "\u4e3b\u9898\u5206\u6790\uff08TA\uff09\u662f\u5b9a\u6027\u7814\u7a76\u4e2d\u5e38\u7528\u7684\u65b9\u6cd5\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u7f16\u7a0b\u6280\u80fd\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7LLMs\u548cGUI\u7b80\u5316TA\u8fc7\u7a0b\uff0c\u4f7f\u5176\u66f4\u6613\u4e8e\u4f7f\u7528\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8estreamlit\u6846\u67b6\u7684GUI\u5e94\u7528\uff0c\u7528\u6237\u53ef\u4e0a\u4f20\u6587\u672c\u6570\u636e\uff0c\u751f\u6210\u521d\u59cb\u4ee3\u7801\u548c\u4e3b\u9898\uff0c\u5e76\u901a\u8fc7LLMs\u7684API\u8fdb\u884c\u5206\u6790\u3002\u652f\u6301\u4eba\u673a\u4ea4\u4e92\u8fed\u4ee3\u4f18\u5316\u3002", "result": "\u5e94\u7528\u6210\u529f\u5b9e\u73b0\u4e86\u65e0\u9700\u7f16\u7a0b\u7684\u4e3b\u9898\u5206\u6790\uff0c\u4fdd\u7559\u4e86\u65b9\u6cd5\u5b66\u4e25\u8c28\u6027\uff0c\u5e76\u63d0\u9ad8\u4e86\u7814\u7a76\u6548\u7387\u3002", "conclusion": "\u8be5\u5e94\u7528\u4e3a\u7f3a\u4e4f\u7f16\u7a0b\u6280\u80fd\u7684\u7814\u7a76\u8005\u63d0\u4f9b\u4e86\u4fbf\u6377\u7684TA\u5de5\u5177\uff0c\u672a\u6765\u53ef\u8fdb\u4e00\u6b65\u6269\u5c55\u529f\u80fd\u3002"}}
{"id": "2504.14701", "pdf": "https://arxiv.org/pdf/2504.14701", "abs": "https://arxiv.org/abs/2504.14701", "authors": ["Andres Fernandez", "Frank Schneider", "Maren Mahsereci", "Philipp Hennig"], "title": "Connecting Parameter Magnitudes and Hessian Eigenspaces at Scale using Sketched Methods", "categories": ["cs.LG", "stat.ML"], "comment": "Accepted at TMLR 2025", "summary": "Recently, it has been observed that when training a deep neural net with SGD,\nthe majority of the loss landscape's curvature quickly concentrates in a tiny\n*top* eigenspace of the loss Hessian, which remains largely stable thereafter.\nIndependently, it has been shown that successful magnitude pruning masks for\ndeep neural nets emerge early in training and remain stable thereafter. In this\nwork, we study these two phenomena jointly and show that they are connected: We\ndevelop a methodology to measure the similarity between arbitrary parameter\nmasks and Hessian eigenspaces via Grassmannian metrics. We identify *overlap*\nas the most useful such metric due to its interpretability and stability. To\ncompute *overlap*, we develop a matrix-free algorithm based on sketched SVDs\nthat allows us to compute over 1000 Hessian eigenpairs for nets with over 10M\nparameters --an unprecedented scale by several orders of magnitude. Our\nexperiments reveal an *overlap* between magnitude parameter masks and top\nHessian eigenspaces consistently higher than chance-level, and that this effect\ngets accentuated for larger network sizes. This result indicates that *top\nHessian eigenvectors tend to be concentrated around larger parameters*, or\nequivalently, that *larger parameters tend to align with directions of larger\nloss curvature*. Our work provides a methodology to approximate and analyze\ndeep learning Hessians at scale, as well as a novel insight on the structure of\ntheir eigenspace.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u4e2d\u635f\u5931Hessian\u77e9\u9635\u7684\u9876\u90e8\u7279\u5f81\u7a7a\u95f4\u4e0e\u53c2\u6570\u526a\u679d\u63a9\u7801\u7684\u7a33\u5b9a\u6027\u5173\u7cfb\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eGrassmannian\u5ea6\u91cf\u7684\u65b9\u6cd5\u91cf\u5316\u4e24\u8005\u76f8\u4f3c\u6027\uff0c\u5e76\u53d1\u73b0\u5927\u53c2\u6570\u503e\u5411\u4e8e\u4e0e\u9ad8\u66f2\u7387\u65b9\u5411\u5bf9\u9f50\u3002", "motivation": "\u63a2\u7d22\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u4e2d\u635f\u5931Hessian\u77e9\u9635\u7684\u9876\u90e8\u7279\u5f81\u7a7a\u95f4\u4e0e\u53c2\u6570\u526a\u679d\u63a9\u7801\u7684\u7a33\u5b9a\u6027\u4e4b\u95f4\u7684\u6f5c\u5728\u8054\u7cfb\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8eGrassmannian\u5ea6\u91cf\u7684\u65b9\u6cd5\uff08\u5982*overlap*\uff09\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u57fa\u4e8e\u8349\u56feSVD\u7684\u65e0\u77e9\u9635\u7b97\u6cd5\uff0c\u4ee5\u8ba1\u7b97\u5927\u89c4\u6a21\u7f51\u7edc\u7684Hessian\u7279\u5f81\u5bf9\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u53c2\u6570\u526a\u679d\u63a9\u7801\u4e0eHessian\u9876\u90e8\u7279\u5f81\u7a7a\u95f4\u7684\u76f8\u4f3c\u6027\u663e\u8457\u9ad8\u4e8e\u968f\u673a\u6c34\u5e73\uff0c\u4e14\u7f51\u7edc\u89c4\u6a21\u8d8a\u5927\u6548\u679c\u8d8a\u660e\u663e\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86Hessian\u7279\u5f81\u5411\u91cf\u503e\u5411\u4e8e\u96c6\u4e2d\u5728\u8f83\u5927\u53c2\u6570\u5468\u56f4\uff0c\u4e3a\u7406\u89e3\u6df1\u5ea6\u5b66\u4e60\u7684Hessian\u7ed3\u6784\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u5e76\u63d0\u4f9b\u4e86\u5927\u89c4\u6a21\u5206\u6790Hessian\u7684\u65b9\u6cd5\u3002"}}
{"id": "2504.13899", "pdf": "https://arxiv.org/pdf/2504.13899", "abs": "https://arxiv.org/abs/2504.13899", "authors": ["Marharyta Domnich", "Rasmus Moorits Veski", "Julius V\u00e4lja", "Kadi Tulver", "Raul Vicente"], "title": "Predicting Satisfaction of Counterfactual Explanations from Human Ratings of Explanatory Qualities", "categories": ["cs.HC", "cs.AI"], "comment": "This work has been accepted to The 3rd World Conference on\n  eXplainable Artificial Intelligence (xAI 2025), July 9-11, 2025 - Istanbul,\n  Turkey", "summary": "Counterfactual explanations are a widely used approach in Explainable AI,\noffering actionable insights into decision-making by illustrating how small\nchanges to input data can lead to different outcomes. Despite their importance,\nevaluating the quality of counterfactual explanations remains an open problem.\nTraditional quantitative metrics, such as sparsity or proximity, fail to fully\naccount for human preferences in explanations, while user studies are\ninsightful but not scalable. Moreover, relying only on a single overall\nsatisfaction rating does not lead to a nuanced understanding of why certain\nexplanations are effective or not. To address this, we analyze a dataset of\ncounterfactual explanations that were evaluated by 206 human participants, who\nrated not only overall satisfaction but also seven explanatory criteria:\nfeasibility, coherence, complexity, understandability, completeness, fairness,\nand trust. Modeling overall satisfaction as a function of these criteria, we\nfind that feasibility (the actionability of suggested changes) and trust (the\nbelief that the changes would lead to the desired outcome) consistently stand\nout as the strongest predictors of user satisfaction, though completeness also\nemerges as a meaningful contributor. Crucially, even excluding feasibility and\ntrust, other metrics explain 58% of the variance, highlighting the importance\nof additional explanatory qualities. Complexity appears independent, suggesting\nmore detailed explanations do not necessarily reduce satisfaction. Strong\nmetric correlations imply a latent structure in how users judge quality, and\ndemographic background significantly shapes ranking patterns. These insights\ninform the design of counterfactual algorithms that adapt explanatory qualities\nto user expertise and domain context.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u53cd\u4e8b\u5b9e\u89e3\u91ca\u7684\u8d28\u91cf\u8bc4\u4f30\u95ee\u9898\uff0c\u53d1\u73b0\u53ef\u884c\u6027\u548c\u4fe1\u4efb\u662f\u7528\u6237\u6ee1\u610f\u5ea6\u7684\u6700\u5f3a\u9884\u6d4b\u56e0\u7d20\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u5176\u4ed6\u89e3\u91ca\u6807\u51c6\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u53cd\u4e8b\u5b9e\u89e3\u91ca\u5728\u53ef\u89e3\u91caAI\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u8bc4\u4f30\u5176\u8d28\u91cf\u4ecd\u662f\u4e00\u4e2a\u5f00\u653e\u95ee\u9898\u3002\u4f20\u7edf\u91cf\u5316\u6307\u6807\u65e0\u6cd5\u5168\u9762\u53cd\u6620\u4eba\u7c7b\u504f\u597d\uff0c\u800c\u7528\u6237\u7814\u7a76\u867d\u6df1\u5165\u4f46\u4e0d\u53ef\u6269\u5c55\u3002", "method": "\u901a\u8fc7\u5206\u6790206\u540d\u53c2\u4e0e\u8005\u5bf9\u53cd\u4e8b\u5b9e\u89e3\u91ca\u7684\u8bc4\u5206\u6570\u636e\uff0c\u5efa\u6a21\u6574\u4f53\u6ee1\u610f\u5ea6\u4e0e\u4e03\u4e2a\u89e3\u91ca\u6807\u51c6\uff08\u5982\u53ef\u884c\u6027\u3001\u4fe1\u4efb\u7b49\uff09\u7684\u5173\u7cfb\u3002", "result": "\u53ef\u884c\u6027\u548c\u4fe1\u4efb\u662f\u7528\u6237\u6ee1\u610f\u5ea6\u7684\u6700\u5f3a\u9884\u6d4b\u56e0\u7d20\uff0c\u5176\u4ed6\u6807\u51c6\u89e3\u91ca\u4e8658%\u7684\u65b9\u5dee\u3002\u590d\u6742\u6027\u72ec\u7acb\uff0c\u4e14\u7528\u6237\u80cc\u666f\u663e\u8457\u5f71\u54cd\u8bc4\u5206\u6a21\u5f0f\u3002", "conclusion": "\u7814\u7a76\u4e3a\u53cd\u4e8b\u5b9e\u7b97\u6cd5\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u4f9d\u636e\uff0c\u5f3a\u8c03\u6839\u636e\u7528\u6237\u4e13\u4e1a\u77e5\u8bc6\u548c\u9886\u57df\u80cc\u666f\u8c03\u6574\u89e3\u91ca\u8d28\u91cf\u3002"}}
{"id": "2504.13904", "pdf": "https://arxiv.org/pdf/2504.13904", "abs": "https://arxiv.org/abs/2504.13904", "authors": ["Donghuo Zeng", "Roberto Legaspi", "Yuewen Sun", "Xinshuai Dong", "Kazushi Ikeda", "Peter Spirtes", "Kun Zhang"], "title": "Generative Framework for Personalized Persuasion: Inferring Causal, Counterfactual, and Latent Knowledge", "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": "12 pages, 10 figures, 1 table. Accepted by ACM UMAP 2025", "summary": "We hypothesize that optimal system responses emerge from adaptive strategies\ngrounded in causal and counterfactual knowledge. Counterfactual inference\nallows us to create hypothetical scenarios to examine the effects of\nalternative system responses. We enhance this process through causal discovery,\nwhich identifies the strategies informed by the underlying causal structure\nthat govern system behaviors. Moreover, we consider the psychological\nconstructs and unobservable noises that might be influencing user-system\ninteractions as latent factors. We show that these factors can be effectively\nestimated. We employ causal discovery to identify strategy-level causal\nrelationships among user and system utterances, guiding the generation of\npersonalized counterfactual dialogues. We model the user utterance strategies\nas causal factors, enabling system strategies to be treated as counterfactual\nactions. Furthermore, we optimize policies for selecting system responses based\non counterfactual data. Our results using a real-world dataset on social good\ndemonstrate significant improvements in persuasive system outcomes, with\nincreased cumulative rewards validating the efficacy of causal discovery in\nguiding personalized counterfactual inference and optimizing dialogue policies\nfor a persuasive dialogue system.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u901a\u8fc7\u56e0\u679c\u53d1\u73b0\u548c\u53cd\u4e8b\u5b9e\u63a8\u7406\u4f18\u5316\u7cfb\u7edf\u54cd\u5e94\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u5bf9\u8bdd\u7cfb\u7edf\u7684\u8bf4\u670d\u6548\u679c\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u57fa\u4e8e\u56e0\u679c\u548c\u53cd\u4e8b\u5b9e\u77e5\u8bc6\u6784\u5efa\u81ea\u9002\u5e94\u7b56\u7565\uff0c\u4ee5\u4f18\u5316\u7cfb\u7edf\u54cd\u5e94\u5e76\u63d0\u5347\u7528\u6237-\u7cfb\u7edf\u4ea4\u4e92\u6548\u679c\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u56e0\u679c\u53d1\u73b0\u8bc6\u522b\u7b56\u7565\u3001\u53cd\u4e8b\u5b9e\u63a8\u7406\u751f\u6210\u5047\u8bbe\u573a\u666f\u3001\u5efa\u6a21\u6f5c\u5728\u56e0\u7d20\uff08\u5982\u5fc3\u7406\u6784\u9020\u548c\u566a\u58f0\uff09\uff0c\u5e76\u57fa\u4e8e\u53cd\u4e8b\u5b9e\u6570\u636e\u4f18\u5316\u54cd\u5e94\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u5347\u4e86\u8bf4\u670d\u6027\u5bf9\u8bdd\u7cfb\u7edf\u7684\u7d2f\u79ef\u5956\u52b1\u3002", "conclusion": "\u7ed3\u8bba\u662f\u56e0\u679c\u53d1\u73b0\u548c\u53cd\u4e8b\u5b9e\u63a8\u7406\u80fd\u6709\u6548\u6307\u5bfc\u4e2a\u6027\u5316\u5bf9\u8bdd\u7b56\u7565\u4f18\u5316\uff0c\u63d0\u5347\u7cfb\u7edf\u6027\u80fd\u3002"}}
{"id": "2504.14704", "pdf": "https://arxiv.org/pdf/2504.14704", "abs": "https://arxiv.org/abs/2504.14704", "authors": ["Hong Yang", "Qi Yu", "Travis Desel"], "title": "Can We Ignore Labels In Out of Distribution Detection?", "categories": ["cs.LG", "cs.AI", "cs.SY", "eess.SY"], "comment": null, "summary": "Out-of-distribution (OOD) detection methods have recently become more\nprominent, serving as a core element in safety-critical autonomous systems. One\nmajor purpose of OOD detection is to reject invalid inputs that could lead to\nunpredictable errors and compromise safety. Due to the cost of labeled data,\nrecent works have investigated the feasibility of self-supervised learning\n(SSL) OOD detection, unlabeled OOD detection, and zero shot OOD detection. In\nthis work, we identify a set of conditions for a theoretical guarantee of\nfailure in unlabeled OOD detection algorithms from an information-theoretic\nperspective. These conditions are present in all OOD tasks dealing with\nreal-world data: I) we provide theoretical proof of unlabeled OOD detection\nfailure when there exists zero mutual information between the learning\nobjective and the in-distribution labels, a.k.a. 'label blindness', II) we\ndefine a new OOD task - Adjacent OOD detection - that tests for label blindness\nand accounts for a previously ignored safety gap in all OOD detection\nbenchmarks, and III) we perform experiments demonstrating that existing\nunlabeled OOD methods fail under conditions suggested by our label blindness\ntheory and analyze the implications for future research in unlabeled OOD\nmethods.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7406\u8bba\u4fdd\u8bc1\uff0c\u8bc1\u660e\u5728\u65e0\u6807\u7b7eOOD\u68c0\u6d4b\u4e2d\u5b58\u5728\u5931\u8d25\u6761\u4ef6\uff0c\u5e76\u5b9a\u4e49\u4e86\u4e00\u4e2a\u65b0\u7684OOD\u4efb\u52a1\uff08Adjacent OOD\u68c0\u6d4b\uff09\u4ee5\u586b\u8865\u73b0\u6709\u57fa\u51c6\u7684\u5b89\u5168\u6f0f\u6d1e\u3002", "motivation": "\u7814\u7a76\u65e0\u6807\u7b7eOOD\u68c0\u6d4b\u7684\u53ef\u884c\u6027\uff0c\u63ed\u793a\u5176\u5728\u5b9e\u9645\u4efb\u52a1\u4e2d\u7684\u6f5c\u5728\u5931\u8d25\u6761\u4ef6\uff0c\u4ee5\u63d0\u5347\u5b89\u5168\u5173\u952e\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u3002", "method": "\u4ece\u4fe1\u606f\u8bba\u89d2\u5ea6\u63d0\u51fa\u7406\u8bba\u8bc1\u660e\uff0c\u5b9a\u4e49\u65b0\u7684OOD\u4efb\u52a1\uff08Adjacent OOD\u68c0\u6d4b\uff09\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u73b0\u6709\u65b9\u6cd5\u7684\u5931\u8d25\u60c5\u51b5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u73b0\u6709\u65e0\u6807\u7b7eOOD\u65b9\u6cd5\u5728\u7406\u8bba\u9884\u6d4b\u7684\u5931\u8d25\u6761\u4ef6\u4e0b\u8868\u73b0\u4e0d\u4f73\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u65e0\u6807\u7b7eOOD\u68c0\u6d4b\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\u548c\u57fa\u51c6\u3002"}}
{"id": "2504.13900", "pdf": "https://arxiv.org/pdf/2504.13900", "abs": "https://arxiv.org/abs/2504.13900", "authors": ["Yue Fu", "Alexis Hiniker"], "title": "Supporting Students' Reading and Cognition with AI", "categories": ["cs.HC", "cs.AI", "cs.CY"], "comment": null, "summary": "With the rapid adoption of AI tools in learning contexts, it is vital to\nunderstand how these systems shape users' reading processes and cognitive\nengagement. We collected and analyzed text from 124 sessions with AI tools, in\nwhich students used these tools to support them as they read assigned readings\nfor an undergraduate course. We categorized participants' prompts to AI\naccording to Bloom's Taxonomy of educational objectives -- Remembering,\nUnderstanding, Applying, Analyzing, Evaluating. Our results show that\n``Analyzing'' and ``Evaluating'' are more prevalent in users' second and third\nprompts within a single usage session, suggesting a shift toward higher-order\nthinking. However, in reviewing users' engagement with AI tools over several\nweeks, we found that users converge toward passive reading engagement over\ntime. Based on these results, we propose design implications for future AI\nreading-support systems, including structured scaffolds for lower-level\ncognitive tasks (e.g., recalling terms) and proactive prompts that encourage\nhigher-order thinking (e.g., analyzing, applying, evaluating). Additionally, we\nadvocate for adaptive, human-in-the-loop features that allow students and\ninstructors to tailor their reading experiences with AI, balancing efficiency\nwith enriched cognitive engagement. Our paper expands the dialogue on\nintegrating AI into academic reading, highlighting both its potential benefits\nand challenges.", "AI": {"tldr": "\u7814\u7a76\u5206\u6790\u4e86\u5b66\u751f\u5728\u4f7f\u7528AI\u5de5\u5177\u8f85\u52a9\u9605\u8bfb\u65f6\u7684\u8ba4\u77e5\u884c\u4e3a\u53d8\u5316\uff0c\u53d1\u73b0\u521d\u671f\u9ad8\u9636\u601d\u7ef4\uff08\u5982\u5206\u6790\u3001\u8bc4\u4f30\uff09\u589e\u52a0\uff0c\u4f46\u957f\u671f\u8d8b\u5411\u88ab\u52a8\u9605\u8bfb\u3002", "motivation": "\u968f\u7740AI\u5de5\u5177\u5728\u6559\u80b2\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u9700\u4e86\u89e3\u5176\u5bf9\u7528\u6237\u9605\u8bfb\u8fc7\u7a0b\u548c\u8ba4\u77e5\u53c2\u4e0e\u7684\u5f71\u54cd\u3002", "method": "\u6536\u96c6\u5e76\u5206\u6790\u4e86124\u6b21\u5b66\u751f\u4f7f\u7528AI\u5de5\u5177\u8f85\u52a9\u9605\u8bfb\u7684\u4f1a\u8bdd\u6570\u636e\uff0c\u6309\u5e03\u9c81\u59c6\u5206\u7c7b\u6cd5\u5bf9\u63d0\u793a\u8fdb\u884c\u5206\u7c7b\u3002", "result": "\u521d\u671f\u9ad8\u9636\u601d\u7ef4\u63d0\u793a\u589e\u591a\uff0c\u4f46\u957f\u671f\u7528\u6237\u8d8b\u5411\u88ab\u52a8\u9605\u8bfb\u3002", "conclusion": "\u5efa\u8bae\u672a\u6765AI\u9605\u8bfb\u652f\u6301\u7cfb\u7edf\u8bbe\u8ba1\u7ed3\u6784\u5316\u652f\u67b6\u548c\u4e3b\u52a8\u63d0\u793a\uff0c\u5e76\u52a0\u5165\u81ea\u9002\u5e94\u529f\u80fd\u4ee5\u5e73\u8861\u6548\u7387\u4e0e\u8ba4\u77e5\u53c2\u4e0e\u3002"}}
{"id": "2504.14716", "pdf": "https://arxiv.org/pdf/2504.14716", "abs": "https://arxiv.org/abs/2504.14716", "authors": ["Tuhina Tripathi", "Manya Wadhwa", "Greg Durrett", "Scott Niekum"], "title": "Pairwise or Pointwise? Evaluating Feedback Protocols for Bias in LLM-Based Evaluation", "categories": ["cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) are widely used as proxies for human labelers in\nboth training (Reinforcement Learning from AI Feedback) and large-scale\nresponse evaluation (LLM-as-a-judge). Alignment and evaluation are critical\ncomponents in the development of reliable LLMs, and the choice of feedback\nprotocol plays a central role in both but remains understudied. In this work,\nwe show that the choice of feedback protocol (absolute scores versus relative\npreferences) can significantly affect evaluation reliability and induce\nsystematic biases. In particular, we show that pairwise evaluation protocols\nare more vulnerable to distracted evaluation. Generator models can exploit\nspurious attributes (or distractor features) favored by the LLM judge,\nresulting in inflated scores for lower-quality outputs and misleading training\nsignals. We find that absolute scoring is more robust to such manipulation,\nproducing judgments that better reflect response quality and are less\ninfluenced by distractor features. Our results demonstrate that generator\nmodels can flip preferences by embedding distractor features, skewing\nLLM-as-a-judge comparisons and leading to inaccurate conclusions about model\nquality in benchmark evaluations. Pairwise preferences flip in about 35% of the\ncases, compared to only 9% for absolute scores. We offer recommendations for\nchoosing feedback protocols based on dataset characteristics and evaluation\nobjectives.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u53cd\u9988\u534f\u8bae\uff08\u7edd\u5bf9\u8bc4\u5206\u4e0e\u76f8\u5bf9\u504f\u597d\uff09\u5bf9LLM\u8bc4\u4f30\u53ef\u9760\u6027\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u76f8\u5bf9\u504f\u597d\u6613\u53d7\u5e72\u6270\uff0c\u7edd\u5bf9\u8bc4\u5206\u66f4\u7a33\u5065\u3002", "motivation": "LLM\u4f5c\u4e3a\u4eba\u7c7b\u6807\u6ce8\u8005\u7684\u4ee3\u7406\u5728\u8bad\u7ec3\u548c\u8bc4\u4f30\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u53cd\u9988\u534f\u8bae\u7684\u9009\u62e9\u5bf9\u8bc4\u4f30\u53ef\u9760\u6027\u5f71\u54cd\u5c1a\u672a\u5145\u5206\u7814\u7a76\u3002", "method": "\u6bd4\u8f83\u7edd\u5bf9\u8bc4\u5206\u548c\u76f8\u5bf9\u504f\u597d\u4e24\u79cd\u53cd\u9988\u534f\u8bae\uff0c\u5206\u6790\u5176\u5bf9\u8bc4\u4f30\u53ef\u9760\u6027\u548c\u7cfb\u7edf\u504f\u5dee\u7684\u5f71\u54cd\u3002", "result": "\u76f8\u5bf9\u504f\u597d\u6613\u53d7\u5e72\u6270\uff0c\u504f\u597d\u7ffb\u8f6c\u738735%\uff1b\u7edd\u5bf9\u8bc4\u5206\u66f4\u7a33\u5065\uff0c\u7ffb\u8f6c\u7387\u4ec59%\u3002", "conclusion": "\u5efa\u8bae\u6839\u636e\u6570\u636e\u96c6\u7279\u6027\u548c\u8bc4\u4f30\u76ee\u6807\u9009\u62e9\u53cd\u9988\u534f\u8bae\uff0c\u7edd\u5bf9\u8bc4\u5206\u66f4\u9002\u5408\u907f\u514d\u5e72\u6270\u3002"}}
{"id": "2504.14727", "pdf": "https://arxiv.org/pdf/2504.14727", "abs": "https://arxiv.org/abs/2504.14727", "authors": ["Geng Liu", "Fei Zhu", "Rong Feng", "Zhiqiang Yi", "Shiqi Wang", "Gaofeng Meng", "Zhaoxiang Zhang"], "title": "Semi-parametric Memory Consolidation: Towards Brain-like Deep Continual Learning", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Humans and most animals inherently possess a distinctive capacity to\ncontinually acquire novel experiences and accumulate worldly knowledge over\ntime. This ability, termed continual learning, is also critical for deep neural\nnetworks (DNNs) to adapt to the dynamically evolving world in open\nenvironments. However, DNNs notoriously suffer from catastrophic forgetting of\npreviously learned knowledge when trained on sequential tasks. In this work,\ninspired by the interactive human memory and learning system, we propose a\nnovel biomimetic continual learning framework that integrates semi-parametric\nmemory and the wake-sleep consolidation mechanism. For the first time, our\nmethod enables deep neural networks to retain high performance on novel tasks\nwhile maintaining prior knowledge in real-world challenging continual learning\nscenarios, e.g., class-incremental learning on ImageNet. This study\ndemonstrates that emulating biological intelligence provides a promising path\nto enable deep neural networks with continual learning capabilities.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u4eff\u751f\u6301\u7eed\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u534a\u53c2\u6570\u8bb0\u5fc6\u548c\u9192\u7761\u5de9\u56fa\u673a\u5236\uff0c\u89e3\u51b3\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5728\u8fde\u7eed\u4efb\u52a1\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\u3002", "motivation": "\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5728\u8fde\u7eed\u4efb\u52a1\u4e2d\u5bb9\u6613\u9057\u5fd8\u5df2\u5b66\u77e5\u8bc6\uff0c\u800c\u4eba\u7c7b\u548c\u52a8\u7269\u5177\u5907\u6301\u7eed\u5b66\u4e60\u80fd\u529b\uff0c\u56e0\u6b64\u4eff\u751f\u65b9\u6cd5\u53ef\u80fd\u63d0\u4f9b\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u6574\u5408\u534a\u53c2\u6570\u8bb0\u5fc6\u548c\u9192\u7761\u5de9\u56fa\u673a\u5236\uff0c\u6a21\u62df\u4eba\u7c7b\u8bb0\u5fc6\u4e0e\u5b66\u4e60\u7cfb\u7edf\u3002", "result": "\u5728\u771f\u5b9e\u573a\u666f\uff08\u5982ImageNet\u7c7b\u589e\u91cf\u5b66\u4e60\uff09\u4e2d\uff0c\u6a21\u578b\u80fd\u4fdd\u6301\u65b0\u4efb\u52a1\u9ad8\u6027\u80fd\u5e76\u4fdd\u7559\u65e7\u77e5\u8bc6\u3002", "conclusion": "\u4eff\u751f\u667a\u80fd\u662f\u8d4b\u4e88\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u6301\u7eed\u5b66\u4e60\u80fd\u529b\u7684\u6709\u6548\u9014\u5f84\u3002"}}
{"id": "2504.13908", "pdf": "https://arxiv.org/pdf/2504.13908", "abs": "https://arxiv.org/abs/2504.13908", "authors": ["Soubhik Barari", "Jarret Angbazo", "Natalie Wang", "Leah M. Christian", "Elizabeth Dean", "Zoe Slowinski", "Brandon Sepulvado"], "title": "AI-Assisted Conversational Interviewing: Effects on Data Quality and User Experience", "categories": ["cs.HC", "cs.AI", "stat.AP"], "comment": null, "summary": "Standardized surveys scale efficiently but sacrifice depth, while\nconversational interviews improve response quality at the cost of scalability\nand consistency. This study bridges the gap between these methods by\nintroducing a framework for AI-assisted conversational interviewing. To\nevaluate this framework, we conducted a web survey experiment where 1,800\nparticipants were randomly assigned to text-based conversational AI agents, or\n\"textbots\", to dynamically probe respondents for elaboration and interactively\ncode open-ended responses. We assessed textbot performance in terms of coding\naccuracy, response quality, and respondent experience. Our findings reveal that\ntextbots perform moderately well in live coding even without survey-specific\nfine-tuning, despite slightly inflated false positive errors due to respondent\nacquiescence bias. Open-ended responses were more detailed and informative, but\nthis came at a slight cost to respondent experience. Our findings highlight the\nfeasibility of using AI methods to enhance open-ended data collection in web\nsurveys.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cdAI\u8f85\u52a9\u7684\u5bf9\u8bdd\u5f0f\u8bbf\u8c08\u6846\u67b6\uff0c\u4ee5\u5f25\u8865\u6807\u51c6\u5316\u8c03\u67e5\u548c\u5bf9\u8bdd\u8bbf\u8c08\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002\u901a\u8fc7\u5b9e\u9a8c\u8bc4\u4f30\uff0c\u53d1\u73b0AI\u6587\u672c\u673a\u5668\u4eba\u5728\u5b9e\u65f6\u7f16\u7801\u548c\u63d0\u5347\u56de\u7b54\u8d28\u91cf\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u4f46\u7565\u5fae\u5f71\u54cd\u53d7\u8bbf\u8005\u4f53\u9a8c\u3002", "motivation": "\u6807\u51c6\u5316\u8c03\u67e5\u727a\u7272\u6df1\u5ea6\uff0c\u800c\u5bf9\u8bdd\u8bbf\u8c08\u7f3a\u4e4f\u53ef\u6269\u5c55\u6027\u548c\u4e00\u81f4\u6027\u3002\u7814\u7a76\u65e8\u5728\u901a\u8fc7AI\u65b9\u6cd5\u7ed3\u5408\u4e24\u8005\u7684\u4f18\u52bf\u3002", "method": "\u901a\u8fc7\u7f51\u9875\u8c03\u67e5\u5b9e\u9a8c\uff0c\u968f\u673a\u5206\u914d1,800\u540d\u53c2\u4e0e\u8005\u4e0e\u6587\u672c\u673a\u5668\u4eba\u4e92\u52a8\uff0c\u52a8\u6001\u63a2\u6d4b\u56de\u7b54\u5e76\u5b9e\u65f6\u7f16\u7801\u5f00\u653e\u6027\u95ee\u9898\u3002", "result": "\u6587\u672c\u673a\u5668\u4eba\u5728\u5b9e\u65f6\u7f16\u7801\u4e2d\u8868\u73b0\u4e2d\u7b49\uff0c\u5f00\u653e\u6027\u95ee\u9898\u56de\u7b54\u66f4\u8be6\u7ec6\uff0c\u4f46\u53d7\u8bbf\u8005\u4f53\u9a8c\u7565\u6709\u4e0b\u964d\u3002", "conclusion": "AI\u65b9\u6cd5\u53ef\u6709\u6548\u589e\u5f3a\u7f51\u7edc\u8c03\u67e5\u4e2d\u7684\u5f00\u653e\u5f0f\u6570\u636e\u6536\u96c6\uff0c\u5c3d\u7ba1\u5b58\u5728\u4e00\u4e9b\u5c40\u9650\u6027\u3002"}}
{"id": "2504.13955", "pdf": "https://arxiv.org/pdf/2504.13955", "abs": "https://arxiv.org/abs/2504.13955", "authors": ["Suhas BN", "Dominik Mattioli", "Saeed Abdullah", "Rosa I. Arriaga", "Chris W. Wiese", "Andrew M. Sherrill"], "title": "Thousand Voices of Trauma: A Large-Scale Synthetic Dataset for Modeling Prolonged Exposure Therapy Conversations", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.HC", "cs.LG", "68T50", "I.2.7; H.5.2"], "comment": "14 pages, 6 figures", "summary": "The advancement of AI systems for mental health support is hindered by\nlimited access to therapeutic conversation data, particularly for trauma\ntreatment. We present Thousand Voices of Trauma, a synthetic benchmark dataset\nof 3,000 therapy conversations based on Prolonged Exposure therapy protocols\nfor Post-traumatic Stress Disorder (PTSD). The dataset comprises 500 unique\ncases, each explored through six conversational perspectives that mirror the\nprogression of therapy from initial anxiety to peak distress to emotional\nprocessing. We incorporated diverse demographic profiles (ages 18-80, M=49.3,\n49.4% male, 44.4% female, 6.2% non-binary), 20 trauma types, and 10\ntrauma-related behaviors using deterministic and probabilistic generation\nmethods. Analysis reveals realistic distributions of trauma types (witnessing\nviolence 10.6%, bullying 10.2%) and symptoms (nightmares 23.4%, substance abuse\n20.8%). Clinical experts validated the dataset's therapeutic fidelity,\nhighlighting its emotional depth while suggesting refinements for greater\nauthenticity. We also developed an emotional trajectory benchmark with\nstandardized metrics for evaluating model responses. This privacy-preserving\ndataset addresses critical gaps in trauma-focused mental health data, offering\na valuable resource for advancing both patient-facing applications and\nclinician training tools.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3a'Thousand Voices of Trauma'\u7684\u5408\u6210\u6570\u636e\u96c6\uff0c\u5305\u542b3,000\u4e2a\u57fa\u4e8ePTSD\u6cbb\u7597\u534f\u8bae\u7684\u5bf9\u8bdd\uff0c\u7528\u4e8e\u586b\u8865\u5fc3\u7406\u5065\u5eb7\u6570\u636e\u7f3a\u53e3\u3002", "motivation": "AI\u7cfb\u7edf\u5728\u5fc3\u7406\u5065\u5eb7\u652f\u6301\u4e2d\u7684\u5e94\u7528\u56e0\u7f3a\u4e4f\u6cbb\u7597\u5bf9\u8bdd\u6570\u636e\uff08\u5c24\u5176\u662f\u521b\u4f24\u6cbb\u7597\u6570\u636e\uff09\u800c\u53d7\u9650\u3002", "method": "\u901a\u8fc7\u786e\u5b9a\u6027\u548c\u6982\u7387\u751f\u6210\u65b9\u6cd5\uff0c\u521b\u5efa\u4e86\u5305\u542b500\u4e2a\u72ec\u7279\u6848\u4f8b\u7684\u6570\u636e\u96c6\uff0c\u6bcf\u4e2a\u6848\u4f8b\u67096\u79cd\u5bf9\u8bdd\u89c6\u89d2\uff0c\u6db5\u76d6\u591a\u6837\u5316\u7684\u521b\u4f24\u7c7b\u578b\u548c\u884c\u4e3a\u3002", "result": "\u6570\u636e\u96c6\u5c55\u793a\u4e86\u521b\u4f24\u7c7b\u578b\u548c\u75c7\u72b6\u7684\u5408\u7406\u5206\u5e03\uff0c\u4e34\u5e8a\u4e13\u5bb6\u9a8c\u8bc1\u4e86\u5176\u6cbb\u7597\u4fdd\u771f\u5ea6\u3002", "conclusion": "\u8be5\u9690\u79c1\u4fdd\u62a4\u6570\u636e\u96c6\u4e3a\u521b\u4f24\u6cbb\u7597AI\u5e94\u7528\u548c\u4e34\u5e8a\u57f9\u8bad\u63d0\u4f9b\u4e86\u5b9d\u8d35\u8d44\u6e90\u3002"}}
{"id": "2504.14728", "pdf": "https://arxiv.org/pdf/2504.14728", "abs": "https://arxiv.org/abs/2504.14728", "authors": ["Vitaly Vanchurin"], "title": "Geometric Learning Dynamics", "categories": ["cs.LG", "q-bio.PE", "quant-ph"], "comment": "15 pages", "summary": "We present a unified geometric framework for modeling learning dynamics in\nphysical, biological, and machine learning systems. The theory reveals three\nfundamental regimes, each emerging from the power-law relationship $g \\propto\n\\kappa^a$ between the metric tensor $g$ in the space of trainable variables and\nthe noise covariance matrix $\\kappa$. The quantum regime corresponds to $a = 1$\nand describes Schr\\\"odinger-like dynamics that emerges from a discrete shift\nsymmetry. The efficient learning regime corresponds to $a = \\tfrac{1}{2}$ and\ndescribes very fast machine learning algorithms. The equilibration regime\ncorresponds to $a = 0$ and describes classical models of biological evolution.\nWe argue that the emergence of the intermediate regime $a = \\tfrac{1}{2}$ is a\nkey mechanism underlying the emergence of biological complexity.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u51e0\u4f55\u6846\u67b6\uff0c\u7528\u4e8e\u5efa\u6a21\u7269\u7406\u3001\u751f\u7269\u548c\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u4e2d\u7684\u5b66\u4e60\u52a8\u6001\uff0c\u63ed\u793a\u4e86\u4e09\u79cd\u57fa\u672c\u673a\u5236\u3002", "motivation": "\u7814\u7a76\u5b66\u4e60\u52a8\u6001\u5728\u4e0d\u540c\u7cfb\u7edf\u4e2d\u7684\u7edf\u4e00\u51e0\u4f55\u6a21\u578b\uff0c\u4ee5\u63ed\u793a\u5176\u80cc\u540e\u7684\u57fa\u672c\u673a\u5236\u3002", "method": "\u901a\u8fc7\u5206\u6790\u5ea6\u91cf\u5f20\u91cf$g$\u4e0e\u566a\u58f0\u534f\u65b9\u5dee\u77e9\u9635$\\kappa$\u4e4b\u95f4\u7684\u5e42\u5f8b\u5173\u7cfb$g \\propto \\kappa^a$\uff0c\u8bc6\u522b\u4e09\u79cd\u52a8\u6001\u673a\u5236\u3002", "result": "\u53d1\u73b0\u4e09\u79cd\u52a8\u6001\u673a\u5236\uff1a\u91cf\u5b50\u673a\u5236\uff08$a=1$\uff09\u3001\u9ad8\u6548\u5b66\u4e60\u673a\u5236\uff08$a=\\tfrac{1}{2}$\uff09\u548c\u5e73\u8861\u673a\u5236\uff08$a=0$\uff09\u3002", "conclusion": "\u4e2d\u95f4\u673a\u5236\uff08$a=\\tfrac{1}{2}$\uff09\u662f\u751f\u7269\u590d\u6742\u6027\u51fa\u73b0\u7684\u5173\u952e\u673a\u5236\u3002"}}
{"id": "2504.13918", "pdf": "https://arxiv.org/pdf/2504.13918", "abs": "https://arxiv.org/abs/2504.13918", "authors": ["Johan van der Meer", "Pamela Hoyte", "Luisa Roeder", "Peter Bruza"], "title": "Modeling the quantum-like dynamics of human reliability ratings in Human-AI interactions by interaction dependent Hamiltonians", "categories": ["cs.HC", "cs.AI"], "comment": "10 pages, 7 figures. Submitted to Phil. Trans. B", "summary": "As our information environments become ever more powered by artificial\nintelligence (AI), the phenomenon of trust in a human's interactions with this\nintelligence is becoming increasingly pertinent. For example, in the not too\ndistant future, there will be teams of humans and intelligent robots involved\nin dealing with the repercussions of high-risk disaster situations such as\nhurricanes, earthquakes, or nuclear accidents. Even in such conditions of high\nuncertainty, humans and intelligent machines will need to engage in shared\ndecision making, and trust is fundamental to the effectiveness of these\ninteractions. A key challenge in modeling the dynamics of this trust is to\nprovide a means to incorporate sensitivity to fluctuations in human trust\njudgments. In this article, we explore the ability of Quantum Random Walk\nmodels to model the dynamics of trust in human-AI interactions, and to\nintegrate a sensitivity to fluctuations in participant trust judgments based on\nthe nature of the interaction with the AI. We found that using empirical\nparameters to inform the use of different Hamiltonians can provide a promising\nmeans to model the evolution of trust in Human-AI interactions.", "AI": {"tldr": "\u91cf\u5b50\u968f\u673a\u6e38\u8d70\u6a21\u578b\u7528\u4e8e\u5efa\u6a21\u4eba\u673a\u4ea4\u4e92\u4e2d\u7684\u4fe1\u4efb\u52a8\u6001\uff0c\u7ed3\u5408\u5bf9\u4fe1\u4efb\u6ce2\u52a8\u7684\u654f\u611f\u6027\u3002", "motivation": "\u7814\u7a76\u9ad8\u4e0d\u786e\u5b9a\u6027\u73af\u5883\u4e0b\u4eba\u673a\u56e2\u961f\u51b3\u7b56\u4e2d\u7684\u4fe1\u4efb\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u91cf\u5b50\u968f\u673a\u6e38\u8d70\u6a21\u578b\uff0c\u7ed3\u5408\u7ecf\u9a8c\u53c2\u6570\u9009\u62e9\u54c8\u5bc6\u987f\u91cf\u3002", "result": "\u53d1\u73b0\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u5efa\u6a21\u4fe1\u4efb\u52a8\u6001\u3002", "conclusion": "\u91cf\u5b50\u968f\u673a\u6e38\u8d70\u6a21\u578b\u4e3a\u5efa\u6a21\u4eba\u673a\u4fe1\u4efb\u63d0\u4f9b\u6709\u524d\u666f\u7684\u65b9\u6cd5\u3002"}}
{"id": "2504.14732", "pdf": "https://arxiv.org/pdf/2504.14732", "abs": "https://arxiv.org/abs/2504.14732", "authors": ["Muhammad Qasim Elahi", "Somtochukwu Oguchienti", "Maheed H. Ahmed", "Mahsa Ghasemi"], "title": "Reinforcement Learning from Multi-level and Episodic Human Feedback", "categories": ["cs.LG"], "comment": null, "summary": "Designing an effective reward function has long been a challenge in\nreinforcement learning, particularly for complex tasks in unstructured\nenvironments. To address this, various learning paradigms have emerged that\nleverage different forms of human input to specify or refine the reward\nfunction. Reinforcement learning from human feedback is a prominent approach\nthat utilizes human comparative feedback, expressed as a preference for one\nbehavior over another, to tackle this problem. In contrast to comparative\nfeedback, we explore multi-level human feedback, which is provided in the form\nof a score at the end of each episode. This type of feedback offers more coarse\nbut informative signals about the underlying reward function than binary\nfeedback. Additionally, it can handle non-Markovian rewards, as it is based on\nthe evaluation of an entire episode. We propose an algorithm to efficiently\nlearn both the reward function and the optimal policy from this form of\nfeedback. Moreover, we show that the proposed algorithm achieves sublinear\nregret and demonstrate its empirical effectiveness through extensive\nsimulations.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5229\u7528\u591a\u7ea7\u4eba\u7c7b\u53cd\u9988\uff08\u4ee5\u5206\u6570\u5f62\u5f0f\uff09\u8bbe\u8ba1\u5f3a\u5316\u5b66\u4e60\u5956\u52b1\u51fd\u6570\u7684\u65b9\u6cd5\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u5b66\u4e60\u5956\u52b1\u51fd\u6570\u548c\u6700\u4f18\u7b56\u7565\u7684\u7b97\u6cd5\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u6548\u679c\u3002", "motivation": "\u8bbe\u8ba1\u6709\u6548\u7684\u5956\u52b1\u51fd\u6570\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u5177\u6709\u6311\u6218\u6027\uff0c\u73b0\u6709\u65b9\u6cd5\u591a\u4f9d\u8d56\u4eba\u7c7b\u6bd4\u8f83\u53cd\u9988\uff0c\u672c\u6587\u63a2\u7d22\u4e86\u591a\u7ea7\u53cd\u9988\u7684\u6f5c\u529b\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u7b97\u6cd5\uff0c\u4ece\u591a\u7ea7\u4eba\u7c7b\u53cd\u9988\uff08\u6bcf\u5e55\u7ed3\u675f\u65f6\u7684\u8bc4\u5206\uff09\u4e2d\u5b66\u4e60\u5956\u52b1\u51fd\u6570\u548c\u6700\u4f18\u7b56\u7565\u3002", "result": "\u7b97\u6cd5\u5b9e\u73b0\u4e86\u6b21\u7ebf\u6027\u9057\u61be\uff0c\u5e76\u901a\u8fc7\u5927\u91cf\u4eff\u771f\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u591a\u7ea7\u4eba\u7c7b\u53cd\u9988\u4e3a\u5956\u52b1\u51fd\u6570\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u66f4\u7c97\u7c92\u5ea6\u4f46\u4fe1\u606f\u4e30\u5bcc\u7684\u4fe1\u53f7\uff0c\u9002\u7528\u4e8e\u975e\u9a6c\u5c14\u53ef\u592b\u5956\u52b1\u573a\u666f\u3002"}}
{"id": "2504.13926", "pdf": "https://arxiv.org/pdf/2504.13926", "abs": "https://arxiv.org/abs/2504.13926", "authors": ["Chameera De Silva", "Thilina Halloluwa", "Dhaval Vyas"], "title": "A Multi-Layered Research Framework for Human-Centered AI: Defining the Path to Explainability and Trust", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "The integration of Artificial Intelligence (AI) into high-stakes domains such\nas healthcare, finance, and autonomous systems is often constrained by concerns\nover transparency, interpretability, and trust. While Human-Centered AI (HCAI)\nemphasizes alignment with human values, Explainable AI (XAI) enhances\ntransparency by making AI decisions more understandable. However, the lack of a\nunified approach limits AI's effectiveness in critical decision-making\nscenarios. This paper presents a novel three-layered framework that bridges\nHCAI and XAI to establish a structured explainability paradigm. The framework\ncomprises (1) a foundational AI model with built-in explainability mechanisms,\n(2) a human-centered explanation layer that tailors explanations based on\ncognitive load and user expertise, and (3) a dynamic feedback loop that refines\nexplanations through real-time user interaction. The framework is evaluated\nacross healthcare, finance, and software development, demonstrating its\npotential to enhance decision-making, regulatory compliance, and public trust.\nOur findings advance Human-Centered Explainable AI (HCXAI), fostering AI\nsystems that are transparent, adaptable, and ethically aligned.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e09\u5c42\u6846\u67b6\uff0c\u7ed3\u5408\u4eba\u672cAI\uff08HCAI\uff09\u548c\u53ef\u89e3\u91caAI\uff08XAI\uff09\uff0c\u4ee5\u63d0\u5347AI\u5728\u9ad8\u98ce\u9669\u9886\u57df\u7684\u900f\u660e\u5ea6\u548c\u4fe1\u4efb\u3002", "motivation": "AI\u5728\u9ad8\u98ce\u9669\u9886\u57df\uff08\u5982\u533b\u7597\u3001\u91d1\u878d\uff09\u7684\u5e94\u7528\u53d7\u9650\u4e8e\u900f\u660e\u5ea6\u548c\u4fe1\u4efb\u95ee\u9898\uff0c\u7f3a\u4e4f\u7edf\u4e00\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e09\u5c42\u6846\u67b6\uff1a\u57fa\u7840AI\u6a21\u578b\u3001\u4eba\u672c\u89e3\u91ca\u5c42\u548c\u52a8\u6001\u53cd\u9988\u5faa\u73af\u3002", "result": "\u5728\u533b\u7597\u3001\u91d1\u878d\u548c\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u9a8c\u8bc1\uff0c\u63d0\u5347\u51b3\u7b56\u3001\u5408\u89c4\u6027\u548c\u4fe1\u4efb\u3002", "conclusion": "\u8be5\u6846\u67b6\u63a8\u52a8\u4e86\u4eba\u672c\u53ef\u89e3\u91caAI\uff08HCXAI\uff09\u7684\u53d1\u5c55\uff0c\u4f7fAI\u66f4\u900f\u660e\u3001\u9002\u5e94\u6027\u5f3a\u4e14\u7b26\u5408\u4f26\u7406\u3002"}}
{"id": "2504.13959", "pdf": "https://arxiv.org/pdf/2504.13959", "abs": "https://arxiv.org/abs/2504.13959", "authors": ["Sanchaita Hazra", "Bodhisattwa Prasad Majumder", "Tuhin Chakrabarty"], "title": "AI Safety Should Prioritize the Future of Work", "categories": ["cs.CY", "cs.AI", "cs.CL", "econ.GN", "q-fin.EC"], "comment": null, "summary": "Current efforts in AI safety prioritize filtering harmful content, preventing\nmanipulation of human behavior, and eliminating existential risks in\ncybersecurity or biosecurity. While pressing, this narrow focus overlooks\ncritical human-centric considerations that shape the long-term trajectory of a\nsociety. In this position paper, we identify the risks of overlooking the\nimpact of AI on the future of work and recommend comprehensive transition\nsupport towards the evolution of meaningful labor with human agency. Through\nthe lens of economic theories, we highlight the intertemporal impacts of AI on\nhuman livelihood and the structural changes in labor markets that exacerbate\nincome inequality. Additionally, the closed-source approach of major\nstakeholders in AI development resembles rent-seeking behavior through\nexploiting resources, breeding mediocrity in creative labor, and monopolizing\ninnovation. To address this, we argue in favor of a robust international\ncopyright anatomy supported by implementing collective licensing that ensures\nfair compensation mechanisms for using data to train AI models. We strongly\nrecommend a pro-worker framework of global AI governance to enhance shared\nprosperity and economic justice while reducing technical debt.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86AI\u5b89\u5168\u4e2d\u5ffd\u89c6\u4eba\u7c7b\u4e2d\u5fc3\u95ee\u9898\u7684\u98ce\u9669\uff0c\u63d0\u51fa\u652f\u6301\u52b3\u52a8\u529b\u8f6c\u578b\u548c\u516c\u5e73\u8865\u507f\u673a\u5236\u7684\u5efa\u8bae\u3002", "motivation": "\u5f53\u524dAI\u5b89\u5168\u7814\u7a76\u8fc7\u4e8e\u5173\u6ce8\u6280\u672f\u98ce\u9669\uff0c\u5ffd\u89c6\u4e86AI\u5bf9\u5de5\u4f5c\u548c\u793e\u4f1a\u7684\u957f\u671f\u5f71\u54cd\uff0c\u53ef\u80fd\u5bfc\u81f4\u6536\u5165\u4e0d\u5e73\u7b49\u52a0\u5267\u548c\u521b\u9020\u6027\u52b3\u52a8\u9000\u5316\u3002", "method": "\u901a\u8fc7\u7ecf\u6d4e\u7406\u8bba\u5206\u6790AI\u5bf9\u52b3\u52a8\u529b\u5e02\u573a\u7684\u7ed3\u6784\u6027\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u56fd\u9645\u7248\u6743\u6846\u67b6\u548c\u96c6\u4f53\u8bb8\u53ef\u673a\u5236\u3002", "result": "\u5f3a\u8c03AI\u5bf9\u5de5\u4f5c\u7684\u6df1\u8fdc\u5f71\u54cd\uff0c\u5efa\u8bae\u5efa\u7acb\u516c\u5e73\u7684\u6570\u636e\u4f7f\u7528\u8865\u507f\u673a\u5236\u548c\u5168\u7403AI\u6cbb\u7406\u6846\u67b6\u3002", "conclusion": "\u547c\u5401\u4ee5\u5de5\u4eba\u4e3a\u4e2d\u5fc3\u7684AI\u6cbb\u7406\uff0c\u4fc3\u8fdb\u7ecf\u6d4e\u516c\u5e73\u548c\u5171\u4eab\u7e41\u8363\uff0c\u540c\u65f6\u51cf\u5c11\u6280\u672f\u503a\u52a1\u3002"}}
{"id": "2504.14741", "pdf": "https://arxiv.org/pdf/2504.14741", "abs": "https://arxiv.org/abs/2504.14741", "authors": ["Namrata Vaswani"], "title": "AltGDmin: Alternating GD and Minimization for Partly-Decoupled (Federated) Optimization", "categories": ["cs.LG", "math.OC", "stat.ME"], "comment": "To appear in Foundations and Trends in Optimization (NOW publishers)", "summary": "This article describes a novel optimization solution framework, called\nalternating gradient descent (GD) and minimization (AltGDmin), that is useful\nfor many problems for which alternating minimization (AltMin) is a popular\nsolution. AltMin is a special case of the block coordinate descent algorithm\nthat is useful for problems in which minimization w.r.t one subset of variables\nkeeping the other fixed is closed form or otherwise reliably solved. Denote the\ntwo blocks/subsets of the optimization variables Z by Za, Zb, i.e., Z = {Za,\nZb}. AltGDmin is often a faster solution than AltMin for any problem for which\n(i) the minimization over one set of variables, Zb, is much quicker than that\nover the other set, Za; and (ii) the cost function is differentiable w.r.t. Za.\nOften, the reason for one minimization to be quicker is that the problem is\n``decoupled\" for Zb and each of the decoupled problems is quick to solve. This\ndecoupling is also what makes AltGDmin communication-efficient for federated\nsettings.\n  Important examples where this assumption holds include (a) low rank\ncolumn-wise compressive sensing (LRCS), low rank matrix completion (LRMC), (b)\ntheir outlier-corrupted extensions such as robust PCA, robust LRCS and robust\nLRMC; (c) phase retrieval and its sparse and low-rank model based extensions;\n(d) tensor extensions of many of these problems such as tensor LRCS and tensor\ncompletion; and (e) many partly discrete problems where GD does not apply --\nsuch as clustering, unlabeled sensing, and mixed linear regression. LRCS finds\nimportant applications in multi-task representation learning and few shot\nlearning, federated sketching, and accelerated dynamic MRI. LRMC and robust PCA\nfind important applications in recommender systems, computer vision and video\nanalytics.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAltGDmin\u7684\u65b0\u578b\u4f18\u5316\u6846\u67b6\uff0c\u6bd4\u4f20\u7edf\u7684\u4ea4\u66ff\u6700\u5c0f\u5316\uff08AltMin\uff09\u66f4\u5feb\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5f53\u5176\u4e2d\u4e00\u4e2a\u53d8\u91cf\u5b50\u96c6\u7684\u4f18\u5316\u901f\u5ea6\u8fdc\u5feb\u4e8e\u53e6\u4e00\u4e2a\u65f6\u3002", "motivation": "\u4f20\u7edfAltMin\u65b9\u6cd5\u5728\u67d0\u4e9b\u95ee\u9898\u4e2d\u6548\u7387\u8f83\u4f4e\uff0c\u5c24\u5176\u662f\u5728\u4e00\u4e2a\u53d8\u91cf\u5b50\u96c6\u7684\u4f18\u5316\u901f\u5ea6\u8fdc\u5feb\u4e8e\u53e6\u4e00\u4e2a\u65f6\u3002AltGDmin\u901a\u8fc7\u7ed3\u5408\u68af\u5ea6\u4e0b\u964d\u548c\u6700\u5c0f\u5316\uff0c\u63d0\u9ad8\u4e86\u6548\u7387\u3002", "method": "AltGDmin\u6846\u67b6\u7ed3\u5408\u4e86\u68af\u5ea6\u4e0b\u964d\uff08GD\uff09\u548c\u6700\u5c0f\u5316\uff08Min\uff09\uff0c\u9002\u7528\u4e8e\u95ee\u9898\u4e2d\u4e00\u4e2a\u53d8\u91cf\u5b50\u96c6\uff08Zb\uff09\u7684\u4f18\u5316\u901f\u5ea6\u8fdc\u5feb\u4e8e\u53e6\u4e00\u4e2a\uff08Za\uff09\u4e14\u6210\u672c\u51fd\u6570\u5bf9Za\u53ef\u5fae\u7684\u60c5\u51b5\u3002", "result": "AltGDmin\u5728\u591a\u79cd\u95ee\u9898\u4e2d\u8868\u73b0\u51fa\u6bd4AltMin\u66f4\u5feb\u7684\u6027\u80fd\uff0c\u5305\u62ec\u4f4e\u79e9\u538b\u7f29\u611f\u77e5\u3001\u77e9\u9635\u8865\u5168\u3001\u9c81\u68d2PCA\u3001\u76f8\u4f4d\u68c0\u7d22\u7b49\u3002", "conclusion": "AltGDmin\u662f\u4e00\u4e2a\u9ad8\u6548\u4e14\u901a\u7528\u7684\u4f18\u5316\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u5206\u5e03\u5f0f\u548c\u8054\u90a6\u5b66\u4e60\u73af\u5883\u4e2d\u5177\u6709\u901a\u4fe1\u6548\u7387\u7684\u4f18\u52bf\u3002"}}
{"id": "2504.13928", "pdf": "https://arxiv.org/pdf/2504.13928", "abs": "https://arxiv.org/abs/2504.13928", "authors": ["Li Song"], "title": "LLM-Driven NPCs: Cross-Platform Dialogue System for Games and Social Platforms", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "NPCs in traditional games are often limited by static dialogue trees and a\nsingle platform for interaction. To overcome these constraints, this study\npresents a prototype system that enables large language model (LLM)-powered\nNPCs to communicate with players both in the game en vironment (Unity) and on a\nsocial platform (Discord). Dialogue logs are stored in a cloud database\n(LeanCloud), allowing the system to synchronize memory between platforms and\nkeep conversa tions coherent. Our initial experiments show that cross-platform\ninteraction is technically feasible and suggest a solid foundation for future\ndevelopments such as emotional modeling and persistent memory support.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684NPC\u7cfb\u7edf\uff0c\u652f\u6301\u8de8\u5e73\u53f0\uff08\u6e38\u620f\u73af\u5883\u548c\u793e\u4ea4\u5e73\u53f0\uff09\u4ea4\u4e92\uff0c\u5e76\u901a\u8fc7\u4e91\u6570\u636e\u5e93\u540c\u6b65\u5bf9\u8bdd\u8bb0\u5fc6\u3002", "motivation": "\u4f20\u7edf\u6e38\u620f\u4e2dNPC\u53d7\u9650\u4e8e\u9759\u6001\u5bf9\u8bdd\u6811\u548c\u5355\u4e00\u4ea4\u4e92\u5e73\u53f0\uff0c\u9700\u7a81\u7834\u8fd9\u4e9b\u9650\u5236\u3002", "method": "\u5f00\u53d1\u539f\u578b\u7cfb\u7edf\uff0c\u5229\u7528LLM\u9a71\u52a8\u7684NPC\uff0c\u5728Unity\u6e38\u620f\u73af\u5883\u548cDiscord\u793e\u4ea4\u5e73\u53f0\u4ea4\u4e92\uff0c\u901a\u8fc7LeanCloud\u4e91\u6570\u636e\u5e93\u540c\u6b65\u5bf9\u8bdd\u65e5\u5fd7\u3002", "result": "\u521d\u6b65\u5b9e\u9a8c\u8868\u660e\u8de8\u5e73\u53f0\u4ea4\u4e92\u6280\u672f\u53ef\u884c\uff0c\u4e3a\u60c5\u611f\u5efa\u6a21\u548c\u6301\u4e45\u8bb0\u5fc6\u652f\u6301\u5960\u5b9a\u57fa\u7840\u3002", "conclusion": "\u7cfb\u7edf\u4e3a\u672a\u6765NPC\u4ea4\u4e92\u7684\u6269\u5c55\u529f\u80fd\u63d0\u4f9b\u4e86\u6280\u672f\u57fa\u7840\u3002"}}
{"id": "2504.14751", "pdf": "https://arxiv.org/pdf/2504.14751", "abs": "https://arxiv.org/abs/2504.14751", "authors": ["Jianyu Zhang"], "title": "AI for the Open-World: the Learning Principles", "categories": ["cs.LG", "cs.AI"], "comment": "PhD thesis. This is not a compilation of published papers, but a new\n  one", "summary": "During the past decades, numerous successes of AI has been made on \"specific\ncapabilities\", named closed-world, such as artificial environments or specific\nreal-world tasks. This well-defined narrow capability brings two nice benefits,\na clear criterion of success and the opportunity to collect a lot of examples.\nThe criteria not only reveal whether a machine has achieved a goal, but reveal\nhow the machine falls short of the goal. As a result, human designers can fix\nthe problems one after the other until the machine is deemed good enough for\nthe task. Furthermore, the large set of collected examples reduces the\ndifficulty of this problem-fixing process (by the central limit theorem).\n  Do the success in closed-world translate into broad open-world, where a\nmachine is required to perform any task that a human could possibly undertake\nwith fewer examples and less priori knowledge from human designers? No. Because\ncompetence in a specific task provides little insight in handling other tasks,\nthe valuable criteria for specific tasks become helpless when handling broader\nunseen tasks. Furthermore, due to the shortage of examples in unseen tasks,\ncentral limit theorem does not stand on our side. At the end, human designers\nlose the oscilloscope to \"hack\" an AI system for the open-world.\n  Achieving AI for the open-world requires unique learning principles and\ninnovated techniques, which are different from the ones in building AI for the\nclosed-world. This thesis explores necessary learning principles required to\nconstruct AI for the open-world, including rich features (analogy a large tool\nbox), disentangled representation (an organized tool box), and inference-time\nlearning (a tool-savvy hand). Driven by the learning principles, this thesis\nfurther proposes techniques to use the learning principles, conducts enormous\nlarge-scale experiments to verify the learning principles.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5c01\u95ed\u4e16\u754cAI\u7684\u6210\u529f\u662f\u5426\u9002\u7528\u4e8e\u5f00\u653e\u4e16\u754c\uff0c\u63d0\u51fa\u4e86\u5f00\u653e\u4e16\u754cAI\u6240\u9700\u7684\u5b66\u4e60\u539f\u5219\u548c\u6280\u672f\u3002", "motivation": "\u5c01\u95ed\u4e16\u754cAI\u7684\u6210\u529f\u4f9d\u8d56\u4e8e\u660e\u786e\u7684\u6807\u51c6\u548c\u5927\u91cf\u793a\u4f8b\uff0c\u4f46\u8fd9\u4e9b\u5728\u5f00\u653e\u4e16\u754c\u4e2d\u4e0d\u9002\u7528\uff0c\u56e0\u6b64\u9700\u8981\u65b0\u7684\u5b66\u4e60\u539f\u5219\u548c\u6280\u672f\u3002", "method": "\u63d0\u51fa\u4e86\u5f00\u653e\u4e16\u754cAI\u7684\u5b66\u4e60\u539f\u5219\uff08\u5982\u4e30\u5bcc\u7279\u5f81\u3001\u89e3\u8026\u8868\u793a\u548c\u63a8\u7406\u65f6\u5b66\u4e60\uff09\uff0c\u5e76\u5f00\u53d1\u4e86\u76f8\u5173\u6280\u672f\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u901a\u8fc7\u5927\u89c4\u6a21\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8fd9\u4e9b\u5b66\u4e60\u539f\u5219\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u5f00\u653e\u4e16\u754cAI\u9700\u8981\u72ec\u7279\u7684\u5b66\u4e60\u539f\u5219\u548c\u6280\u672f\uff0c\u4e0e\u5c01\u95ed\u4e16\u754cAI\u4e0d\u540c\u3002"}}
{"id": "2504.13940", "pdf": "https://arxiv.org/pdf/2504.13940", "abs": "https://arxiv.org/abs/2504.13940", "authors": ["Paul Taele", "Tracy Hammond"], "title": "Hashigo: A Next Generation Sketch Interactive System for Japanese Kanji", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Language students can increase their effectiveness in learning written\nJapanese by mastering the visual structure and written technique of Japanese\nkanji. Yet, existing kanji handwriting recognition systems do not assess the\nwritten technique sufficiently enough to discourage students from developing\nbad learning habits. In this paper, we describe our work on Hashigo, a kanji\nsketch interactive system which achieves human instructor-level critique and\nfeedback on both the visual structure and written technique of students'\nsketched kanji. This type of automated critique and feedback allows students to\ntarget and correct specific deficiencies in their sketches that, if left\nuntreated, are detrimental to effective long-term kanji learning.", "AI": {"tldr": "Hashigo\u7cfb\u7edf\u901a\u8fc7\u63d0\u4f9b\u7c7b\u4f3c\u4eba\u7c7b\u6559\u5e08\u7684\u53cd\u9988\uff0c\u5e2e\u52a9\u5b66\u751f\u5728\u5b66\u4e60\u65e5\u8bed\u6c49\u5b57\u65f6\u6539\u8fdb\u4e66\u5199\u6280\u5de7\u548c\u89c6\u89c9\u7ed3\u6784\u3002", "motivation": "\u73b0\u6709\u6c49\u5b57\u624b\u5199\u8bc6\u522b\u7cfb\u7edf\u672a\u80fd\u5145\u5206\u8bc4\u4f30\u4e66\u5199\u6280\u5de7\uff0c\u5bfc\u81f4\u5b66\u751f\u53ef\u80fd\u517b\u6210\u4e0d\u826f\u5b66\u4e60\u4e60\u60ef\u3002", "method": "\u5f00\u53d1Hashigo\uff0c\u4e00\u4e2a\u4ea4\u4e92\u5f0f\u6c49\u5b57\u4e66\u5199\u7cfb\u7edf\uff0c\u63d0\u4f9b\u4eba\u7c7b\u6559\u5e08\u7ea7\u522b\u7684\u53cd\u9988\u3002", "result": "\u7cfb\u7edf\u80fd\u591f\u9488\u5bf9\u5b66\u751f\u7684\u4e66\u5199\u7f3a\u9677\u63d0\u4f9b\u5177\u4f53\u53cd\u9988\uff0c\u907f\u514d\u957f\u671f\u5b66\u4e60\u4e2d\u7684\u4e0d\u826f\u5f71\u54cd\u3002", "conclusion": "Hashigo\u901a\u8fc7\u81ea\u52a8\u5316\u53cd\u9988\u6709\u6548\u63d0\u5347\u5b66\u751f\u7684\u6c49\u5b57\u5b66\u4e60\u6548\u679c\u3002"}}
{"id": "2504.14762", "pdf": "https://arxiv.org/pdf/2504.14762", "abs": "https://arxiv.org/abs/2504.14762", "authors": ["Sahil Rajesh Dhayalkar"], "title": "A Combinatorial Theory of Dropout: Subnetworks, Graph Geometry, and Generalization", "categories": ["cs.LG", "cs.AI"], "comment": "17 pages (9 pages main content and remaining pages are references,\n  appendix which includes 7 figures, proofs and derivations)", "summary": "We propose a combinatorial and graph-theoretic theory of dropout by modeling\ntraining as a random walk over a high-dimensional graph of binary subnetworks.\nEach node represents a masked version of the network, and dropout induces\nstochastic traversal across this space. We define a subnetwork contribution\nscore that quantifies generalization and show that it varies smoothly over the\ngraph. Using tools from spectral graph theory, PAC-Bayes analysis, and\ncombinatorics, we prove that generalizing subnetworks form large, connected,\nlow-resistance clusters, and that their number grows exponentially with network\nwidth. This reveals dropout as a mechanism for sampling from a robust,\nstructured ensemble of well-generalizing subnetworks with built-in redundancy.\nExtensive experiments validate every theoretical claim across diverse\narchitectures. Together, our results offer a unified foundation for\nunderstanding dropout and suggest new directions for mask-guided regularization\nand subnetwork optimization.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7ec4\u5408\u548c\u56fe\u8bba\u7684dropout\u7406\u8bba\uff0c\u5c06\u8bad\u7ec3\u5efa\u6a21\u4e3a\u5728\u9ad8\u7ef4\u5b50\u7f51\u7edc\u56fe\u4e0a\u7684\u968f\u673a\u6e38\u8d70\uff0c\u63ed\u793a\u4e86dropout\u901a\u8fc7\u91c7\u6837\u7a33\u5065\u3001\u7ed3\u6784\u5316\u7684\u5b50\u7f51\u7edc\u96c6\u5408\u6765\u63d0\u5347\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u7814\u7a76dropout\u7684\u673a\u5236\uff0c\u63ed\u793a\u5176\u5982\u4f55\u901a\u8fc7\u968f\u673a\u91c7\u6837\u5b50\u7f51\u7edc\u6765\u63d0\u5347\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u5c06\u8bad\u7ec3\u5efa\u6a21\u4e3a\u9ad8\u7ef4\u5b50\u7f51\u7edc\u56fe\u4e0a\u7684\u968f\u673a\u6e38\u8d70\uff0c\u5b9a\u4e49\u5b50\u7f51\u7edc\u8d21\u732e\u5206\u6570\uff0c\u5e76\u5229\u7528\u8c31\u56fe\u7406\u8bba\u3001PAC-Bayes\u5206\u6790\u548c\u7ec4\u5408\u6570\u5b66\u5de5\u5177\u8fdb\u884c\u5206\u6790\u3002", "result": "\u6cdb\u5316\u80fd\u529b\u5f3a\u7684\u5b50\u7f51\u7edc\u5f62\u6210\u5927\u3001\u8fde\u901a\u3001\u4f4e\u963b\u7684\u96c6\u7fa4\uff0c\u4e14\u5176\u6570\u91cf\u968f\u7f51\u7edc\u5bbd\u5ea6\u6307\u6570\u589e\u957f\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7406\u8bba\u7ed3\u679c\u3002", "conclusion": "\u8bba\u6587\u4e3a\u7406\u89e3dropout\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u7406\u8bba\u57fa\u7840\uff0c\u5e76\u63d0\u51fa\u4e86\u63a9\u7801\u5f15\u5bfc\u6b63\u5219\u5316\u548c\u5b50\u7f51\u7edc\u4f18\u5316\u7684\u65b0\u65b9\u5411\u3002"}}
{"id": "2504.14053", "pdf": "https://arxiv.org/pdf/2504.14053", "abs": "https://arxiv.org/abs/2504.14053", "authors": ["Ali Safari"], "title": "Sentiment Analysis of Airbnb Reviews: Exploring Their Impact on Acceptance Rates and Pricing Across Multiple U.S. Regions", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "This research examines whether Airbnb guests' positive and negative comments\ninfluence acceptance rates and rental prices across six U.S. regions: Rhode\nIsland, Broward County, Chicago, Dallas, San Diego, and Boston. Thousands of\nreviews were collected and analyzed using Natural Language Processing (NLP) to\nclassify sentiments as positive or negative, followed by statistical testing\n(t-tests and basic correlations) on the average scores. The findings reveal\nthat over 90 percent of reviews in each region are positive, indicating that\nhaving additional reviews does not significantly enhance prices. However,\nlistings with predominantly positive feedback exhibit slightly higher\nacceptance rates, suggesting that sentiment polarity, rather than the sheer\nvolume of reviews, is a more critical factor for host success. Additionally,\nbudget listings often gather extensive reviews while maintaining competitive\npricing, whereas premium listings sustain higher prices with fewer but highly\npositive reviews. These results underscore the importance of sentiment quality\nover quantity in shaping guest behavior and pricing strategies in an\noverwhelmingly positive review environment.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7NLP\u5206\u6790Airbnb\u8bc4\u8bba\uff0c\u53d1\u73b090%\u4ee5\u4e0a\u8bc4\u8bba\u4e3a\u6b63\u9762\uff0c\u6b63\u9762\u8bc4\u8bba\u5bf9\u623f\u6e90\u63a5\u53d7\u7387\u6709\u8f7b\u5fae\u63d0\u5347\uff0c\u4f46\u5bf9\u4ef7\u683c\u5f71\u54cd\u4e0d\u5927\u3002\u60c5\u611f\u6781\u6027\u6bd4\u8bc4\u8bba\u6570\u91cf\u66f4\u91cd\u8981\u3002", "motivation": "\u63a2\u8ba8Airbnb\u8bc4\u8bba\u7684\u60c5\u611f\u6781\u6027\uff08\u6b63\u9762/\u8d1f\u9762\uff09\u5bf9\u623f\u6e90\u63a5\u53d7\u7387\u548c\u4ef7\u683c\u7684\u5f71\u54cd\u3002", "method": "\u6536\u96c6\u516d\u4e2a\u7f8e\u56fd\u5730\u533a\u7684\u8bc4\u8bba\uff0c\u4f7f\u7528NLP\u5206\u7c7b\u60c5\u611f\uff0c\u5e76\u8fdb\u884ct\u68c0\u9a8c\u548c\u76f8\u5173\u5206\u6790\u3002", "result": "\u6b63\u9762\u8bc4\u8bba\u5360\u6bd4\u9ad8\uff0c\u4f46\u5bf9\u4ef7\u683c\u65e0\u663e\u8457\u5f71\u54cd\uff1b\u6b63\u9762\u8bc4\u8bba\u591a\u7684\u623f\u6e90\u63a5\u53d7\u7387\u7565\u9ad8\u3002\u9884\u7b97\u623f\u6e90\u8bc4\u8bba\u591a\u4f46\u4ef7\u683c\u7ade\u4e89\uff0c\u9ad8\u7aef\u623f\u6e90\u8bc4\u8bba\u5c11\u4f46\u4ef7\u683c\u9ad8\u3002", "conclusion": "\u5728\u8bc4\u8bba\u666e\u904d\u6b63\u9762\u7684\u73af\u5883\u4e2d\uff0c\u60c5\u611f\u8d28\u91cf\u6bd4\u6570\u91cf\u66f4\u80fd\u5f71\u54cd\u7528\u6237\u884c\u4e3a\u548c\u5b9a\u4ef7\u7b56\u7565\u3002"}}
{"id": "2504.14782", "pdf": "https://arxiv.org/pdf/2504.14782", "abs": "https://arxiv.org/abs/2504.14782", "authors": ["Ahmed Sobhi Saleh", "Kristof Croes", "Hajdin Ceric", "Ingrid De Wolf", "Houman Zahedmanesh"], "title": "Novel Concept-Oriented Synthetic Data approach for Training Generative AI-Driven Crystal Grain Analysis Using Diffusion Model", "categories": ["cs.LG", "cond-mat.mtrl-sci", "I.4.9"], "comment": "19 Pages, 5 Figures", "summary": "The traditional techniques for extracting polycrystalline grain structures\nfrom microscopy images, such as transmission electron microscopy (TEM) and\nscanning electron microscopy (SEM), are labour-intensive, subjective, and\ntime-consuming, limiting their scalability for high-throughput analysis. In\nthis study, we present an automated methodology integrating edge detection with\ngenerative diffusion models to effectively identify grains, eliminate noise,\nand connect broken segments in alignment with predicted grain boundaries. Due\nto the limited availability of adequate images preventing the training of deep\nmachine learning models, a new seven-stage methodology is employed to generate\nsynthetic TEM images for training. This concept-oriented synthetic data\napproach can be extended to any field of interest where the scarcity of data is\na challenge. The presented model was applied to various metals with average\ngrain sizes down to the nanoscale, producing grain morphologies from\nlow-resolution TEM images that are comparable to those obtained from advanced\nand demanding experimental techniques with an average accuracy of 97.23%.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u5316\u65b9\u6cd5\uff0c\u7ed3\u5408\u8fb9\u7f18\u68c0\u6d4b\u548c\u751f\u6210\u6269\u6563\u6a21\u578b\uff0c\u7528\u4e8e\u4ece\u663e\u5fae\u955c\u56fe\u50cf\u4e2d\u63d0\u53d6\u591a\u6676\u7c92\u7ed3\u6784\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u6548\u7387\u4f4e\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\uff08\u5982TEM\u548cSEM\uff09\u63d0\u53d6\u591a\u6676\u7c92\u7ed3\u6784\u8017\u65f6\u4e14\u4e3b\u89c2\uff0c\u9650\u5236\u4e86\u9ad8\u901a\u91cf\u5206\u6790\u7684\u6269\u5c55\u6027\u3002", "method": "\u91c7\u7528\u4e03\u9636\u6bb5\u65b9\u6cd5\u751f\u6210\u5408\u6210TEM\u56fe\u50cf\u7528\u4e8e\u8bad\u7ec3\uff0c\u7ed3\u5408\u8fb9\u7f18\u68c0\u6d4b\u548c\u751f\u6210\u6269\u6563\u6a21\u578b\uff0c\u81ea\u52a8\u8bc6\u522b\u6676\u7c92\u5e76\u6d88\u9664\u566a\u58f0\u3002", "result": "\u6a21\u578b\u5728\u591a\u79cd\u91d1\u5c5e\u4e0a\u5e94\u7528\uff0c\u5e73\u5747\u51c6\u786e\u7387\u8fbe97.23%\uff0c\u751f\u6210\u7684\u4f4e\u5206\u8fa8\u7387TEM\u56fe\u50cf\u6676\u7c92\u5f62\u6001\u4e0e\u9ad8\u8981\u6c42\u5b9e\u9a8c\u6280\u672f\u7ed3\u679c\u76f8\u5f53\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u53ef\u63a8\u5e7f\u81f3\u5176\u4ed6\u6570\u636e\u7a00\u7f3a\u9886\u57df\uff0c\u5c55\u793a\u4e86\u5408\u6210\u6570\u636e\u5728\u89e3\u51b3\u6570\u636e\u4e0d\u8db3\u95ee\u9898\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2504.13942", "pdf": "https://arxiv.org/pdf/2504.13942", "abs": "https://arxiv.org/abs/2504.13942", "authors": ["Sukanth Kalivarathan", "Muhmmad Abrar Raja Mohamed", "Aswathy Ravikumar", "S Harini"], "title": "Intelligence of Things: A Spatial Context-Aware Control System for Smart Devices", "categories": ["cs.HC", "cs.AI", "cs.ET"], "comment": "16 pages, 8 Figures", "summary": "This paper introduces Intelligence of Things (INOT), a novel spatial\ncontext-aware control system that enhances smart home automation through\nintuitive spatial reasoning. Current smart home systems largely rely on\ndevice-specific identifiers, limiting user interaction to explicit naming\nconventions rather than natural spatial references. INOT addresses this\nlimitation through a modular architecture that integrates Vision Language\nModels with IoT control systems to enable natural language commands with\nspatial context (e.g., \"turn on the light near the window\"). The system\ncomprises key components including an Onboarding Inference Engine, Zero-Shot\nDevice Detection, Spatial Topology Inference, and Intent-Based Command\nSynthesis. A comprehensive user study with 15 participants demonstrated INOT's\nsignificant advantages over conventional systems like Google Home Assistant,\nwith users reporting reduced cognitive workload (NASA-TLX scores decreased by\nan average of 13.17 points), higher ease-of-use ratings, and stronger\npreference (14 out of 15 participants). By eliminating the need to memorize\ndevice identifiers and enabling context-aware spatial commands, INOT represents\na significant advancement in creating more intuitive and accessible smart home\ncontrol systems.", "AI": {"tldr": "INOT\u662f\u4e00\u79cd\u65b0\u578b\u7a7a\u95f4\u4e0a\u4e0b\u6587\u611f\u77e5\u63a7\u5236\u7cfb\u7edf\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u548c\u7a7a\u95f4\u63a8\u7406\u63d0\u5347\u667a\u80fd\u5bb6\u5c45\u81ea\u52a8\u5316\u3002", "motivation": "\u73b0\u6709\u667a\u80fd\u5bb6\u5c45\u7cfb\u7edf\u4f9d\u8d56\u8bbe\u5907\u6807\u8bc6\u7b26\uff0c\u9650\u5236\u4e86\u7528\u6237\u4ea4\u4e92\u7684\u81ea\u7136\u6027\u3002INOT\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u91c7\u7528\u6a21\u5757\u5316\u67b6\u6784\uff0c\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u7269\u8054\u7f51\u63a7\u5236\u7cfb\u7edf\uff0c\u652f\u6301\u81ea\u7136\u8bed\u8a00\u547d\u4ee4\u3002", "result": "\u7528\u6237\u7814\u7a76\u8868\u660e\uff0cINOT\u663e\u8457\u964d\u4f4e\u8ba4\u77e5\u8d1f\u8377\uff0c\u63d0\u9ad8\u6613\u7528\u6027\uff0c14/15\u7528\u6237\u66f4\u504f\u597d\u8be5\u7cfb\u7edf\u3002", "conclusion": "INOT\u901a\u8fc7\u6d88\u9664\u8bbe\u5907\u6807\u8bc6\u7b26\u9700\u6c42\uff0c\u63a8\u52a8\u667a\u80fd\u5bb6\u5c45\u63a7\u5236\u7cfb\u7edf\u7684\u76f4\u89c2\u6027\u548c\u53ef\u8bbf\u95ee\u6027\u3002"}}
{"id": "2504.14790", "pdf": "https://arxiv.org/pdf/2504.14790", "abs": "https://arxiv.org/abs/2504.14790", "authors": ["Jun Yang", "Shintaro Yamasaki"], "title": "Enhanced Data-driven Topology Design Methodology with Multi-level Mesh and Correlation-based Mutation for Stress-related Multi-objective Optimization", "categories": ["cs.LG", "cs.NA", "math.NA"], "comment": "23 pages, 22 figures", "summary": "Topology optimization (TO) serves as a widely applied structural design\napproach to tackle various engineering problems. Nevertheless,\nsensitivity-based TO methods usually struggle with solving strongly nonlinear\noptimization problems. By leveraging high capacity of deep generative model,\nwhich is an influential machine learning technique, the sensitivity-free\ndata-driven topology design (DDTD) methodology is regarded as an effective\nmeans of overcoming these issues. The DDTD methodology depends on initial\ndataset with a certain regularity, making its results highly sensitive to\ninitial dataset quality. This limits its effectiveness and generalizability,\nespecially for optimization problems without priori information. In this\nresearch, we proposed a multi-level mesh DDTD-based method with\ncorrelation-based mutation module to escape from the limitation of the quality\nof the initial dataset on the results and enhance computational efficiency. The\ncore is to employ a correlation-based mutation module to assign new geometric\nfeatures with physical meaning to the generated data, while utilizing a\nmulti-level mesh strategy to progressively enhance the refinement of the\nstructural representation, thus avoiding the maintenance of a high\ndegree-of-freedom (DOF) representation throughout the iterative process. The\nproposed multi-level mesh DDTD-based method can be driven by a low quality\ninitial dataset without the need for time-consuming construction of a specific\ndataset, thus significantly increasing generality and reducing application\ndifficulty, while further lowering computational cost of DDTD methodology.\nVarious comparison experiments with the traditional sensitivity-based TO\nmethods on stress-related strongly nonlinear problems demonstrate the\ngenerality and effectiveness of the proposed method.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u7ea7\u7f51\u683c\u548c\u76f8\u5173\u6027\u53d8\u5f02\u6a21\u5757\u7684\u6570\u636e\u9a71\u52a8\u62d3\u6251\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5bf9\u521d\u59cb\u6570\u636e\u96c6\u8d28\u91cf\u7684\u4f9d\u8d56\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u8ba1\u7b97\u6548\u7387\u548c\u901a\u7528\u6027\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u654f\u611f\u6027\u7684\u62d3\u6251\u4f18\u5316\u65b9\u6cd5\u5728\u5904\u7406\u5f3a\u975e\u7ebf\u6027\u95ee\u9898\u65f6\u6548\u679c\u4e0d\u4f73\uff0c\u800c\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u5bf9\u521d\u59cb\u6570\u636e\u96c6\u8d28\u91cf\u654f\u611f\uff0c\u9650\u5236\u4e86\u5176\u901a\u7528\u6027\u548c\u6709\u6548\u6027\u3002", "method": "\u91c7\u7528\u591a\u7ea7\u7f51\u683c\u7b56\u7565\u548c\u76f8\u5173\u6027\u53d8\u5f02\u6a21\u5757\uff0c\u9010\u6b65\u4f18\u5316\u7ed3\u6784\u8868\u793a\uff0c\u907f\u514d\u9ad8\u81ea\u7531\u5ea6\u8868\u793a\uff0c\u540c\u65f6\u8d4b\u4e88\u751f\u6210\u6570\u636e\u65b0\u7684\u51e0\u4f55\u7279\u5f81\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5f3a\u975e\u7ebf\u6027\u95ee\u9898\u4e0a\u6bd4\u4f20\u7edf\u65b9\u6cd5\u66f4\u5177\u901a\u7528\u6027\u548c\u6709\u6548\u6027\uff0c\u4e14\u8ba1\u7b97\u6210\u672c\u66f4\u4f4e\u3002", "conclusion": "\u591a\u7ea7\u7f51\u683c\u6570\u636e\u9a71\u52a8\u62d3\u6251\u8bbe\u8ba1\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u901a\u7528\u6027\u548c\u8ba1\u7b97\u6548\u7387\uff0c\u9002\u7528\u4e8e\u7f3a\u4e4f\u5148\u9a8c\u4fe1\u606f\u7684\u4f18\u5316\u95ee\u9898\u3002"}}
{"id": "2504.13944", "pdf": "https://arxiv.org/pdf/2504.13944", "abs": "https://arxiv.org/abs/2504.13944", "authors": ["Tace McNamara", "Jon McCormack", "Maria Teresa Llano"], "title": "Mixer Metaphors: audio interfaces for non-musical applications", "categories": ["cs.HC", "cs.AI", "cs.SD", "H.5.2; J.5; I.2.7"], "comment": "9 Pages", "summary": "The NIME conference traditionally focuses on interfaces for music and musical\nexpression. In this paper we reverse this tradition to ask, can interfaces\ndeveloped for music be successfully appropriated to non-musical applications?\nTo help answer this question we designed and developed a new device, which uses\ninterface metaphors borrowed from analogue synthesisers and audio mixing to\nphysically control the intangible aspects of a Large Language Model. We\ncompared two versions of the device, with and without the audio-inspired\naugmentations, with a group of artists who used each version over a one week\nperiod. Our results show that the use of audio-like controls afforded more\nimmediate, direct and embodied control over the LLM, allowing users to\ncreatively experiment and play with the device over its non-mixer counterpart.\nOur project demonstrates how cross-sensory metaphors can support creative\nthinking and embodied practice when designing new technological interfaces.", "AI": {"tldr": "\u63a2\u8ba8\u97f3\u4e50\u754c\u9762\u662f\u5426\u53ef\u7528\u4e8e\u975e\u97f3\u4e50\u5e94\u7528\uff0c\u8bbe\u8ba1\u4e86\u4e00\u79cd\u57fa\u4e8e\u97f3\u9891\u63a7\u5236\u9690\u55bb\u7684\u8bbe\u5907\uff0c\u5b9e\u9a8c\u8868\u660e\u97f3\u9891\u63a7\u5236\u66f4\u76f4\u89c2\u6709\u6548\u3002", "motivation": "\u7814\u7a76\u97f3\u4e50\u754c\u9762\u5728\u975e\u97f3\u4e50\u9886\u57df\u7684\u9002\u7528\u6027\uff0c\u63a2\u7d22\u8de8\u611f\u5b98\u9690\u55bb\u5bf9\u521b\u610f\u548c\u6280\u672f\u754c\u9762\u8bbe\u8ba1\u7684\u4ef7\u503c\u3002", "method": "\u8bbe\u8ba1\u5e76\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8e\u97f3\u9891\u5408\u6210\u5668\u9690\u55bb\u7684\u8bbe\u5907\uff0c\u6bd4\u8f83\u4e86\u6709\u65e0\u97f3\u9891\u63a7\u5236\u7684\u4e24\u4e2a\u7248\u672c\uff0c\u827a\u672f\u5bb6\u8fdb\u884c\u4e00\u5468\u4f7f\u7528\u6d4b\u8bd5\u3002", "result": "\u97f3\u9891\u63a7\u5236\u7248\u672c\u63d0\u4f9b\u4e86\u66f4\u76f4\u63a5\u3001\u76f4\u89c2\u7684LLM\u63a7\u5236\uff0c\u7528\u6237\u80fd\u66f4\u521b\u610f\u5730\u5b9e\u9a8c\u548c\u64cd\u4f5c\u3002", "conclusion": "\u8de8\u611f\u5b98\u9690\u55bb\u80fd\u652f\u6301\u521b\u610f\u8bbe\u8ba1\u548c\u5177\u8eab\u5b9e\u8df5\uff0c\u4e3a\u65b0\u6280\u672f\u754c\u9762\u8bbe\u8ba1\u63d0\u4f9b\u65b0\u601d\u8def\u3002"}}
{"id": "2504.14110", "pdf": "https://arxiv.org/pdf/2504.14110", "abs": "https://arxiv.org/abs/2504.14110", "authors": ["Theo Jaffrelot Inizan", "Sherry Yang", "Aaron Kaplan", "Yen-hsu Lin", "Jian Yin", "Saber Mirzaei", "Mona Abdelgaid", "Ali H. Alawadhi", "KwangHwan Cho", "Zhiling Zheng", "Ekin Dogus Cubuk", "Christian Borgs", "Jennifer T. Chayes", "Kristin A. Persson", "Omar M. Yaghi"], "title": "System of Agentic AI for the Discovery of Metal-Organic Frameworks", "categories": ["cond-mat.mtrl-sci", "cs.AI", "cs.CL", "cs.MA"], "comment": null, "summary": "Generative models and machine learning promise accelerated material discovery\nin MOFs for CO2 capture and water harvesting but face significant challenges\nnavigating vast chemical spaces while ensuring synthetizability. Here, we\npresent MOFGen, a system of Agentic AI comprising interconnected agents: a\nlarge language model that proposes novel MOF compositions, a diffusion model\nthat generates crystal structures, quantum mechanical agents that optimize and\nfilter candidates, and synthetic-feasibility agents guided by expert rules and\nmachine learning. Trained on all experimentally reported MOFs and computational\ndatabases, MOFGen generated hundreds of thousands of novel MOF structures and\nsynthesizable organic linkers. Our methodology was validated through\nhigh-throughput experiments and the successful synthesis of five \"AI-dreamt\"\nMOFs, representing a major step toward automated synthesizable material\ndiscovery.", "AI": {"tldr": "MOFGen\u7cfb\u7edf\u901a\u8fc7\u591a\u667a\u80fd\u4f53AI\u751f\u6210\u65b0\u578bMOF\u7ed3\u6784\uff0c\u9a8c\u8bc1\u4e86\u5176\u5408\u6210\u53ef\u884c\u6027\uff0c\u5e76\u6210\u529f\u5408\u6210\u4e86\u4e94\u79cdAI\u8bbe\u8ba1\u7684MOF\u3002", "motivation": "\u52a0\u901fMOF\u6750\u6599\u53d1\u73b0\uff0c\u89e3\u51b3\u5316\u5b66\u7a7a\u95f4\u63a2\u7d22\u548c\u5408\u6210\u53ef\u884c\u6027\u7684\u6311\u6218\u3002", "method": "\u7ed3\u5408\u8bed\u8a00\u6a21\u578b\u3001\u6269\u6563\u6a21\u578b\u3001\u91cf\u5b50\u529b\u5b66\u667a\u80fd\u4f53\u548c\u5408\u6210\u53ef\u884c\u6027\u667a\u80fd\u4f53\uff0c\u751f\u6210\u548c\u4f18\u5316MOF\u7ed3\u6784\u3002", "result": "\u751f\u6210\u6570\u5341\u4e07\u65b0\u578bMOF\u7ed3\u6784\u548c\u53ef\u5408\u6210\u6709\u673a\u8fde\u63a5\u4f53\uff0c\u6210\u529f\u5408\u6210\u4e94\u79cdAI\u8bbe\u8ba1\u7684MOF\u3002", "conclusion": "MOFGen\u4e3a\u81ea\u52a8\u5316\u53ef\u5408\u6210\u6750\u6599\u53d1\u73b0\u8fc8\u51fa\u91cd\u8981\u4e00\u6b65\u3002"}}
{"id": "2504.14796", "pdf": "https://arxiv.org/pdf/2504.14796", "abs": "https://arxiv.org/abs/2504.14796", "authors": ["David Yang", "Mostafa Abdelmegeed", "John Modl", "Minjeong Kim"], "title": "Edge-boosted graph learning for functional brain connectivity analysis", "categories": ["cs.LG", "eess.IV"], "comment": "Accepted at IEEE International Symposium on Biomedical Imaging (ISBI)\n  2025, 4 pages", "summary": "Predicting disease states from functional brain connectivity is critical for\nthe early diagnosis of severe neurodegenerative diseases such as Alzheimer's\nDisease and Parkinson's Disease. Existing studies commonly employ Graph Neural\nNetworks (GNNs) to infer clinical diagnoses from node-based brain connectivity\nmatrices generated through node-to-node similarities of regionally averaged\nfMRI signals. However, recent neuroscience studies found that such node-based\nconnectivity does not accurately capture ``functional connections\" within the\nbrain. This paper proposes a novel approach to brain network analysis that\nemphasizes edge functional connectivity (eFC), shifting the focus to inter-edge\nrelationships. Additionally, we introduce a co-embedding technique to integrate\nedge functional connections effectively. Experimental results on the ADNI and\nPPMI datasets demonstrate that our method significantly outperforms\nstate-of-the-art GNN methods in classifying functional brain networks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8fb9\u529f\u80fd\u8fde\u63a5\uff08eFC\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u8111\u7f51\u7edc\u5206\u6790\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709GNN\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u8282\u70b9\u7684\u8111\u8fde\u63a5\u65b9\u6cd5\u65e0\u6cd5\u51c6\u786e\u6355\u6349\u529f\u80fd\u8fde\u63a5\uff0c\u9700\u6539\u8fdb\u4ee5\u63d0\u5347\u75be\u75c5\u9884\u6d4b\u51c6\u786e\u6027\u3002", "method": "\u91c7\u7528\u8fb9\u529f\u80fd\u8fde\u63a5\uff08eFC\uff09\u5206\u6790\uff0c\u5e76\u5f15\u5165\u5171\u5d4c\u5165\u6280\u672f\u6574\u5408\u8fb9\u529f\u80fd\u5173\u7cfb\u3002", "result": "\u5728ADNI\u548cPPMI\u6570\u636e\u96c6\u4e0a\uff0c\u65b0\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u73b0\u6709GNN\u65b9\u6cd5\u3002", "conclusion": "\u8fb9\u529f\u80fd\u8fde\u63a5\u65b9\u6cd5\u4e3a\u8111\u7f51\u7edc\u5206\u6790\u63d0\u4f9b\u4e86\u66f4\u51c6\u786e\u7684\u5de5\u5177\uff0c\u6709\u52a9\u4e8e\u795e\u7ecf\u9000\u884c\u6027\u75be\u75c5\u7684\u65e9\u671f\u8bca\u65ad\u3002"}}
{"id": "2504.14798", "pdf": "https://arxiv.org/pdf/2504.14798", "abs": "https://arxiv.org/abs/2504.14798", "authors": ["Hao Xuan", "Xingyu Li"], "title": "Verifying Robust Unlearning: Probing Residual Knowledge in Unlearned Models", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Machine Unlearning (MUL) is crucial for privacy protection and content\nregulation, yet recent studies reveal that traces of forgotten information\npersist in unlearned models, enabling adversaries to resurface removed\nknowledge. Existing verification methods only confirm whether unlearning was\nexecuted, failing to detect such residual information leaks. To address this,\nwe introduce the concept of Robust Unlearning, ensuring models are\nindistinguishable from retraining and resistant to adversarial recovery. To\nempirically evaluate whether unlearning techniques meet this security standard,\nwe propose the Unlearning Mapping Attack (UMA), a post-unlearning verification\nframework that actively probes models for forgotten traces using adversarial\nqueries. Extensive experiments on discriminative and generative tasks show that\nexisting unlearning techniques remain vulnerable, even when passing existing\nverification metrics. By establishing UMA as a practical verification tool,\nthis study sets a new standard for assessing and enhancing machine unlearning\nsecurity.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u201c\u9c81\u68d2\u9057\u5fd8\u201d\u6982\u5ff5\uff0c\u5e76\u901a\u8fc7\u201c\u9057\u5fd8\u6620\u5c04\u653b\u51fb\u201d\uff08UMA\uff09\u9a8c\u8bc1\u73b0\u6709\u9057\u5fd8\u6280\u672f\u662f\u5426\u6ee1\u8db3\u5b89\u5168\u6807\u51c6\uff0c\u53d1\u73b0\u5176\u4ecd\u5b58\u5728\u6f0f\u6d1e\u3002", "motivation": "\u73b0\u6709\u9057\u5fd8\u9a8c\u8bc1\u65b9\u6cd5\u65e0\u6cd5\u68c0\u6d4b\u6b8b\u7559\u4fe1\u606f\u6cc4\u9732\uff0c\u9700\u786e\u4fdd\u6a21\u578b\u4e0e\u91cd\u65b0\u8bad\u7ec3\u65e0\u533a\u522b\u4e14\u80fd\u62b5\u6297\u5bf9\u6297\u6062\u590d\u3002", "method": "\u63d0\u51faUMA\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u6297\u67e5\u8be2\u4e3b\u52a8\u63a2\u6d4b\u6a21\u578b\u4e2d\u9057\u5fd8\u75d5\u8ff9\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u73b0\u6709\u9057\u5fd8\u6280\u672f\u5373\u4f7f\u901a\u8fc7\u73b0\u6709\u9a8c\u8bc1\u6307\u6807\u4ecd\u5b58\u5728\u6f0f\u6d1e\u3002", "conclusion": "UMA\u4e3a\u8bc4\u4f30\u548c\u63d0\u5347\u673a\u5668\u9057\u5fd8\u5b89\u5168\u6027\u8bbe\u5b9a\u4e86\u65b0\u6807\u51c6\u3002"}}
{"id": "2504.13947", "pdf": "https://arxiv.org/pdf/2504.13947", "abs": "https://arxiv.org/abs/2504.13947", "authors": ["Shahan Ali Memon", "Soham De", "Sungha Kang", "Riyan Mujtaba", "Bedoor AlShebli", "Katie Davis", "Jaime Snyder", "Jevin D. West"], "title": "From job titles to jawlines: Using context voids to study generative AI systems", "categories": ["cs.CY", "cs.AI", "cs.HC"], "comment": "7 pages, 2 figures", "summary": "In this paper, we introduce a speculative design methodology for studying the\nbehavior of generative AI systems, framing design as a mode of inquiry. We\npropose bridging seemingly unrelated domains to generate intentional context\nvoids, using these tasks as probes to elicit AI model behavior. We demonstrate\nthis through a case study: probing the ChatGPT system (GPT-4 and DALL-E) to\ngenerate headshots from professional Curricula Vitae (CVs). In contrast to\ntraditional ways, our approach assesses system behavior under conditions of\nradical uncertainty -- when forced to invent entire swaths of missing context\n-- revealing subtle stereotypes and value-laden assumptions. We qualitatively\nanalyze how the system interprets identity and competence markers from CVs,\ntranslating them into visual portraits despite the missing context (i.e.\nphysical descriptors). We show that within this context void, the AI system\ngenerates biased representations, potentially relying on stereotypical\nassociations or blatant hallucinations.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u63a8\u6d4b\u6027\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7\u751f\u6210\u4e0a\u4e0b\u6587\u7a7a\u767d\u6765\u7814\u7a76\u751f\u6210\u5f0fAI\u7684\u884c\u4e3a\uff0c\u63ed\u793a\u5176\u6f5c\u5728\u7684\u504f\u89c1\u548c\u5047\u8bbe\u3002", "motivation": "\u63a2\u7d22\u751f\u6210\u5f0fAI\u5728\u6781\u7aef\u4e0d\u786e\u5b9a\u6027\u6761\u4ef6\u4e0b\u7684\u884c\u4e3a\uff0c\u63ed\u793a\u5176\u9690\u542b\u7684\u523b\u677f\u5370\u8c61\u548c\u4ef7\u503c\u5047\u8bbe\u3002", "method": "\u901a\u8fc7\u6865\u63a5\u65e0\u5173\u9886\u57df\u751f\u6210\u4e0a\u4e0b\u6587\u7a7a\u767d\uff0c\u4ee5ChatGPT\uff08GPT-4\u548cDALL-E\uff09\u4e3a\u6848\u4f8b\uff0c\u4ece\u7b80\u5386\u751f\u6210\u5934\u50cf\u3002", "result": "AI\u5728\u7f3a\u4e4f\u7269\u7406\u63cf\u8ff0\u7684\u60c5\u51b5\u4e0b\uff0c\u751f\u6210\u5e26\u6709\u504f\u89c1\u6216\u5e7b\u89c9\u7684\u89c6\u89c9\u8096\u50cf\u3002", "conclusion": "\u63a8\u6d4b\u6027\u8bbe\u8ba1\u65b9\u6cd5\u80fd\u6709\u6548\u63ed\u793aAI\u7cfb\u7edf\u7684\u9690\u542b\u504f\u89c1\uff0c\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u5176\u884c\u4e3a\u673a\u5236\u3002"}}
{"id": "2504.14800", "pdf": "https://arxiv.org/pdf/2504.14800", "abs": "https://arxiv.org/abs/2504.14800", "authors": ["Shuxian Zhao", "Jie Gui", "Minjing Dong", "Baosheng Yu", "Zhipeng Gui", "Lu Dong", "Yuan Yan Tang", "James Tin-Yau Kwok"], "title": "A Survey on Small Sample Imbalance Problem: Metrics, Feature Analysis, and Solutions", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "The small sample imbalance (S&I) problem is a major challenge in machine\nlearning and data analysis. It is characterized by a small number of samples\nand an imbalanced class distribution, which leads to poor model performance. In\naddition, indistinct inter-class feature distributions further complicate\nclassification tasks. Existing methods often rely on algorithmic heuristics\nwithout sufficiently analyzing the underlying data characteristics. We argue\nthat a detailed analysis from the data perspective is essential before\ndeveloping an appropriate solution. Therefore, this paper proposes a systematic\nanalytical framework for the S\\&I problem. We first summarize imbalance metrics\nand complexity analysis methods, highlighting the need for interpretable\nbenchmarks to characterize S&I problems. Second, we review recent solutions for\nconventional, complexity-based, and extreme S&I problems, revealing\nmethodological differences in handling various data distributions. Our summary\nfinds that resampling remains a widely adopted solution. However, we conduct\nexperiments on binary and multiclass datasets, revealing that classifier\nperformance differences significantly exceed the improvements achieved through\nresampling. Finally, this paper highlights open questions and discusses future\ntrends.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7cfb\u7edf\u6027\u5206\u6790\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u5c0f\u6837\u672c\u4e0d\u5e73\u8861\uff08S&I\uff09\u95ee\u9898\uff0c\u5f3a\u8c03\u6570\u636e\u7279\u5f81\u5206\u6790\u7684\u91cd\u8981\u6027\uff0c\u5e76\u603b\u7ed3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u5c0f\u6837\u672c\u4e0d\u5e73\u8861\u95ee\u9898\u662f\u673a\u5668\u5b66\u4e60\u548c\u6570\u636e\u5206\u6790\u4e2d\u7684\u4e3b\u8981\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u591a\u4f9d\u8d56\u542f\u53d1\u5f0f\u7b97\u6cd5\uff0c\u7f3a\u4e4f\u5bf9\u6570\u636e\u7279\u5f81\u7684\u6df1\u5165\u5206\u6790\u3002", "method": "\u8bba\u6587\u603b\u7ed3\u4e86\u4e0d\u5e73\u8861\u5ea6\u91cf\u548c\u590d\u6742\u6027\u5206\u6790\u65b9\u6cd5\uff0c\u5e76\u56de\u987e\u4e86\u9488\u5bf9\u4e0d\u540c\u6570\u636e\u5206\u5e03\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u5b9e\u9a8c\u6bd4\u8f83\u4e86\u91cd\u91c7\u6837\u548c\u5206\u7c7b\u5668\u7684\u6027\u80fd\u5dee\u5f02\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5206\u7c7b\u5668\u6027\u80fd\u5dee\u5f02\u6bd4\u91cd\u91c7\u6837\u5e26\u6765\u7684\u6539\u8fdb\u66f4\u663e\u8457\u3002", "conclusion": "\u8bba\u6587\u5f3a\u8c03\u4e86\u6570\u636e\u89c6\u89d2\u5206\u6790\u7684\u5fc5\u8981\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2504.13948", "pdf": "https://arxiv.org/pdf/2504.13948", "abs": "https://arxiv.org/abs/2504.13948", "authors": ["Juan David Salazar Rodriguez", "Sam Conrad Joyce", "Julfendi Julfendi"], "title": "Using customized GPT to develop prompting proficiency in architectural AI-generated images", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "This research investigates the use of customized GPT models to enhance\nprompting proficiency among architecture students when generating AI-driven\nimages. Prompt engineering is increasingly essential in architectural education\ndue to the widespread adoption of generative AI tools. This study utilized a\nmixed-methods experimental design involving architecture students divided into\nthree distinct groups: a control group receiving no structured support, a\nsecond group provided with structured prompting guides, and a third group\nsupported by both structured guides and interactive AI personas. Students\nengaged in reverse engineering tasks, first guessing provided image prompts and\nthen generating their own prompts, aiming to boost critical thinking and\nprompting skills. Variables examined included time spent prompting, word count,\nprompt similarity, and concreteness. Quantitative analysis involved correlation\nassessments between these variables and a one-way ANOVA to evaluate differences\nacross groups. While several correlations showed meaningful relationships, not\nall were statistically significant. ANOVA results indicated statistically\nsignificant improvements in word count, similarity, and concreteness,\nespecially in the group supported by AI personas and structured prompting\nguides. Qualitative feedback complemented these findings, revealing enhanced\nconfidence and critical thinking skills in students. These results suggest\ntailored GPT interactions substantially improve students' ability to\ncommunicate architectural concepts clearly and effectively.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5b9a\u5236GPT\u6a21\u578b\u5982\u4f55\u63d0\u5347\u5efa\u7b51\u5b66\u751f\u5728\u751f\u6210AI\u9a71\u52a8\u56fe\u50cf\u65f6\u7684\u63d0\u793a\u80fd\u529b\uff0c\u53d1\u73b0\u7ed3\u6784\u5316\u6307\u5bfc\u548cAI\u4ea4\u4e92\u89d2\u8272\u663e\u8457\u63d0\u9ad8\u4e86\u5b66\u751f\u7684\u63d0\u793a\u6280\u80fd\u3002", "motivation": "\u968f\u7740\u751f\u6210\u5f0fAI\u5de5\u5177\u5728\u5efa\u7b51\u6559\u80b2\u4e2d\u7684\u666e\u53ca\uff0c\u63d0\u793a\u5de5\u7a0b\u53d8\u5f97\u81f3\u5173\u91cd\u8981\uff0c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5982\u4f55\u4f18\u5316\u5b66\u751f\u7684\u63d0\u793a\u80fd\u529b\u3002", "method": "\u91c7\u7528\u6df7\u5408\u65b9\u6cd5\u5b9e\u9a8c\u8bbe\u8ba1\uff0c\u5c06\u5b66\u751f\u5206\u4e3a\u4e09\u7ec4\uff08\u65e0\u652f\u6301\u3001\u7ed3\u6784\u5316\u6307\u5bfc\u3001\u7ed3\u6784\u5316\u6307\u5bfc\u52a0AI\u4ea4\u4e92\u89d2\u8272\uff09\uff0c\u901a\u8fc7\u9006\u5411\u5de5\u7a0b\u4efb\u52a1\u8bc4\u4f30\u63d0\u793a\u6280\u80fd\u3002", "result": "\u5b9a\u91cf\u5206\u6790\u663e\u793a\uff0c\u7ed3\u6784\u5316\u6307\u5bfc\u548cAI\u4ea4\u4e92\u89d2\u8272\u663e\u8457\u63d0\u5347\u4e86\u63d0\u793a\u7684\u8bcd\u6c47\u91cf\u3001\u76f8\u4f3c\u6027\u548c\u5177\u4f53\u6027\uff1b\u5b9a\u6027\u53cd\u9988\u8868\u660e\u5b66\u751f\u4fe1\u5fc3\u548c\u6279\u5224\u6027\u601d\u7ef4\u589e\u5f3a\u3002", "conclusion": "\u5b9a\u5236\u5316\u7684GPT\u4ea4\u4e92\u80fd\u6709\u6548\u63d0\u5347\u5b66\u751f\u6e05\u6670\u8868\u8fbe\u5efa\u7b51\u6982\u5ff5\u7684\u80fd\u529b\u3002"}}
{"id": "2504.14805", "pdf": "https://arxiv.org/pdf/2504.14805", "abs": "https://arxiv.org/abs/2504.14805", "authors": ["Jinwoo Choi", "Seung-Woo Seo"], "title": "Dynamic Contrastive Skill Learning with State-Transition Based Skill Clustering and Dynamic Length Adjustment", "categories": ["cs.LG", "cs.AI", "cs.RO"], "comment": "ICLR 2025; 23 pages, 12 figures", "summary": "Reinforcement learning (RL) has made significant progress in various domains,\nbut scaling it to long-horizon tasks with complex decision-making remains\nchallenging. Skill learning attempts to address this by abstracting actions\ninto higher-level behaviors. However, current approaches often fail to\nrecognize semantically similar behaviors as the same skill and use fixed skill\nlengths, limiting flexibility and generalization. To address this, we propose\nDynamic Contrastive Skill Learning (DCSL), a novel framework that redefines\nskill representation and learning. DCSL introduces three key ideas:\nstate-transition based skill representation, skill similarity function\nlearning, and dynamic skill length adjustment. By focusing on state transitions\nand leveraging contrastive learning, DCSL effectively captures the semantic\ncontext of behaviors and adapts skill lengths to match the appropriate temporal\nextent of behaviors. Our approach enables more flexible and adaptive skill\nextraction, particularly in complex or noisy datasets, and demonstrates\ncompetitive performance compared to existing methods in task completion and\nefficiency.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u5bf9\u6bd4\u6280\u80fd\u5b66\u4e60\uff08DCSL\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u72b6\u6001\u8f6c\u79fb\u8868\u793a\u6280\u80fd\u3001\u5b66\u4e60\u6280\u80fd\u76f8\u4f3c\u6027\u51fd\u6570\u548c\u52a8\u6001\u8c03\u6574\u6280\u80fd\u957f\u5ea6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6280\u80fd\u5b66\u4e60\u65b9\u6cd5\u5728\u8bc6\u522b\u8bed\u4e49\u76f8\u4f3c\u884c\u4e3a\u548c\u56fa\u5b9a\u6280\u80fd\u957f\u5ea6\u4e0a\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u5728\u957f\u65f6\u7a0b\u4efb\u52a1\u548c\u590d\u6742\u51b3\u7b56\u4e2d\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u73b0\u6709\u6280\u80fd\u5b66\u4e60\u65b9\u6cd5\u672a\u80fd\u6709\u6548\u8bc6\u522b\u8bed\u4e49\u76f8\u4f3c\u884c\u4e3a\u4e14\u6280\u80fd\u957f\u5ea6\u56fa\u5b9a\uff0c\u9650\u5236\u4e86\u7075\u6d3b\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "DCSL\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u5173\u952e\u521b\u65b0\uff1a\u57fa\u4e8e\u72b6\u6001\u8f6c\u79fb\u7684\u6280\u80fd\u8868\u793a\u3001\u6280\u80fd\u76f8\u4f3c\u6027\u51fd\u6570\u5b66\u4e60\u548c\u52a8\u6001\u6280\u80fd\u957f\u5ea6\u8c03\u6574\uff0c\u5229\u7528\u5bf9\u6bd4\u5b66\u4e60\u6355\u6349\u884c\u4e3a\u8bed\u4e49\u4e0a\u4e0b\u6587\u3002", "result": "DCSL\u5728\u590d\u6742\u6216\u566a\u58f0\u6570\u636e\u96c6\u4e2d\u8868\u73b0\u51fa\u66f4\u7075\u6d3b\u548c\u81ea\u9002\u5e94\u7684\u6280\u80fd\u63d0\u53d6\u80fd\u529b\uff0c\u5728\u4efb\u52a1\u5b8c\u6210\u548c\u6548\u7387\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "DCSL\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u6280\u80fd\u957f\u5ea6\u548c\u8bed\u4e49\u4e0a\u4e0b\u6587\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6280\u80fd\u5b66\u4e60\u7684\u7075\u6d3b\u6027\u548c\u9002\u5e94\u6027\uff0c\u4e3a\u590d\u6742\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.14147", "pdf": "https://arxiv.org/pdf/2504.14147", "abs": "https://arxiv.org/abs/2504.14147", "authors": ["Jiakai Tang", "Jingsen Zhang", "Zihang Tian", "Xueyang Feng", "Lei Wang", "Xu Chen"], "title": "HF4Rec: Human-Like Feedback-Driven Optimization Framework for Explainable Recommendation", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": null, "summary": "Recent advancements in explainable recommendation have greatly bolstered user\nexperience by elucidating the decision-making rationale. However, the existing\nmethods actually fail to provide effective feedback signals for potentially\nbetter or worse generated explanations due to their reliance on traditional\nsupervised learning paradigms in sparse interaction data. To address these\nissues, we propose a novel human-like feedback-driven optimization framework.\nThis framework employs a dynamic interactive optimization mechanism for\nachieving human-centered explainable requirements without incurring high labor\ncosts. Specifically, we propose to utilize large language models (LLMs) as\nhuman simulators to predict human-like feedback for guiding the learning\nprocess. To enable the LLMs to deeply understand the task essence and meet\nuser's diverse personalized requirements, we introduce a human-induced\ncustomized reward scoring method, which helps stimulate the language\nunderstanding and logical reasoning capabilities of LLMs. Furthermore,\nconsidering the potential conflicts between different perspectives of\nexplanation quality, we introduce a principled Pareto optimization that\ntransforms the multi-perspective quality enhancement task into a\nmulti-objective optimization problem for improving explanation performance. At\nlast, to achieve efficient model training, we design an off-policy optimization\npipeline. By incorporating a replay buffer and addressing the data distribution\nbiases, we can effectively improve data utilization and enhance model\ngenerality. Extensive experiments on four datasets demonstrate the superiority\nof our approach.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4eba\u7c7b\u53cd\u9988\u7684\u4f18\u5316\u6846\u67b6\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u6a21\u62df\u4eba\u7c7b\u53cd\u9988\uff0c\u901a\u8fc7\u52a8\u6001\u4ea4\u4e92\u4f18\u5316\u673a\u5236\u63d0\u5347\u63a8\u8350\u89e3\u91ca\u7684\u8d28\u91cf\uff0c\u540c\u65f6\u964d\u4f4e\u4eba\u5de5\u6210\u672c\u3002", "motivation": "\u73b0\u6709\u53ef\u89e3\u91ca\u63a8\u8350\u65b9\u6cd5\u56e0\u4f9d\u8d56\u7a00\u758f\u4ea4\u4e92\u6570\u636e\u7684\u4f20\u7edf\u76d1\u7763\u5b66\u4e60\u8303\u5f0f\uff0c\u65e0\u6cd5\u4e3a\u751f\u6210\u89e3\u91ca\u63d0\u4f9b\u6709\u6548\u53cd\u9988\u4fe1\u53f7\u3002", "method": "\u63d0\u51fa\u52a8\u6001\u4ea4\u4e92\u4f18\u5316\u6846\u67b6\uff0c\u5229\u7528LLMs\u6a21\u62df\u4eba\u7c7b\u53cd\u9988\uff0c\u5f15\u5165\u4eba\u7c7b\u8bf1\u5bfc\u7684\u5b9a\u5236\u5316\u5956\u52b1\u8bc4\u5206\u65b9\u6cd5\uff0c\u5e76\u7ed3\u5408\u5e15\u7d2f\u6258\u4f18\u5316\u89e3\u51b3\u591a\u89c6\u89d2\u89e3\u91ca\u8d28\u91cf\u51b2\u7a81\u95ee\u9898\u3002", "result": "\u5728\u56db\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u9ad8\u6548\u7684\u6570\u636e\u5229\u7528\u548c\u6a21\u578b\u6cdb\u5316\u80fd\u529b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u8350\u89e3\u91ca\u7684\u6027\u80fd\u3002"}}
{"id": "2504.14814", "pdf": "https://arxiv.org/pdf/2504.14814", "abs": "https://arxiv.org/abs/2504.14814", "authors": ["Kazuhisa Fujita"], "title": "A Basic Evaluation of Neural Networks Trained with the Error Diffusion Learning Algorithm", "categories": ["cs.LG"], "comment": null, "summary": "Artificial neural networks are powerful tools capable of addressing various\ntasks. Although the backpropagation algorithm has become a standard training\nmethod for these neural networks, its lack of biological plausibility has\ninspired the development of alternative learning approaches. One such\nalternative is Kaneko's Error Diffusion Learning Algorithm (EDLA), a\nbiologically motivated approach wherein a single global error signal diffuses\nthroughout a network composed of paired excitatory-inhibitory sublayers,\nthereby eliminating the necessity for layer-wise backpropagation. This study\npresents a contemporary formulation of the EDLA framework and evaluates its\neffectiveness through parity check, regression, and image classification tasks.\nOur experimental results indicate that EDLA networks can consistently achieve\nhigh accuracy across these benchmarks, with performance efficiency and\nconvergence speed notably influenced by the choice of learning rate, neuron\ncount, and network depth. Further investigation of the internal representations\nformed by EDLA networks reveals their capacity for meaningful feature\nextraction, similar to traditional neural networks. These results suggest that\nEDLA is a biologically motivated alternative for training feedforward networks\nand will motivate future work on extending this method to biologically inspired\nneural networks.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86Kaneko\u7684\u8bef\u5dee\u6269\u6563\u5b66\u4e60\u7b97\u6cd5\uff08EDLA\uff09\uff0c\u4e00\u79cd\u751f\u7269\u542f\u53d1\u7684\u66ff\u4ee3\u53cd\u5411\u4f20\u64ad\u7684\u65b9\u6cd5\uff0c\u5e76\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u53cd\u5411\u4f20\u64ad\u7b97\u6cd5\u7f3a\u4e4f\u751f\u7269\u5408\u7406\u6027\uff0c\u4fc3\u4f7f\u5f00\u53d1\u66ff\u4ee3\u5b66\u4e60\u65b9\u6cd5\u3002", "method": "EDLA\u901a\u8fc7\u5168\u5c40\u8bef\u5dee\u4fe1\u53f7\u5728\u6210\u5bf9\u7684\u5174\u594b-\u6291\u5236\u5b50\u5c42\u7f51\u7edc\u4e2d\u6269\u6563\uff0c\u907f\u514d\u9010\u5c42\u53cd\u5411\u4f20\u64ad\u3002", "result": "EDLA\u5728\u5947\u5076\u6821\u9a8c\u3001\u56de\u5f52\u548c\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u6027\u80fd\u53d7\u5b66\u4e60\u7387\u3001\u795e\u7ecf\u5143\u6570\u91cf\u548c\u7f51\u7edc\u6df1\u5ea6\u5f71\u54cd\u3002", "conclusion": "EDLA\u662f\u4e00\u79cd\u751f\u7269\u542f\u53d1\u7684\u6709\u6548\u66ff\u4ee3\u65b9\u6cd5\uff0c\u672a\u6765\u53ef\u6269\u5c55\u81f3\u66f4\u591a\u751f\u7269\u542f\u53d1\u7f51\u7edc\u3002"}}
{"id": "2504.14815", "pdf": "https://arxiv.org/pdf/2504.14815", "abs": "https://arxiv.org/abs/2504.14815", "authors": ["Xiaoyong Yuan", "Xiaolong Ma", "Linke Guo", "Lan Zhang"], "title": "What Lurks Within? Concept Auditing for Shared Diffusion Models at Scale", "categories": ["cs.LG", "cs.AI", "cs.CR", "cs.CV"], "comment": "17 pages, 15 figures", "summary": "Diffusion models (DMs) have revolutionized text-to-image generation, enabling\nthe creation of highly realistic and customized images from text prompts. With\nthe rise of parameter-efficient fine-tuning (PEFT) techniques like LoRA, users\ncan now customize powerful pre-trained models using minimal computational\nresources. However, the widespread sharing of fine-tuned DMs on open platforms\nraises growing ethical and legal concerns, as these models may inadvertently or\ndeliberately generate sensitive or unauthorized content, such as copyrighted\nmaterial, private individuals, or harmful content. Despite the increasing\nregulatory attention on generative AI, there are currently no practical tools\nfor systematically auditing these models before deployment. In this paper, we\naddress the problem of concept auditing: determining whether a fine-tuned DM\nhas learned to generate a specific target concept. Existing approaches\ntypically rely on prompt-based input crafting and output-based image\nclassification but suffer from critical limitations, including prompt\nuncertainty, concept drift, and poor scalability. To overcome these challenges,\nwe introduce Prompt-Agnostic Image-Free Auditing (PAIA), a novel, model-centric\nconcept auditing framework. By treating the DM as the object of inspection,\nPAIA enables direct analysis of internal model behavior, bypassing the need for\noptimized prompts or generated images. We evaluate PAIA on 320 controlled model\nand 690 real-world community models sourced from a public DM sharing platform.\nPAIA achieves over 90% detection accuracy while reducing auditing time by\n18-40x compared to existing baselines. To our knowledge, PAIA is the first\nscalable and practical solution for pre-deployment concept auditing of\ndiffusion models, providing a practical foundation for safer and more\ntransparent diffusion model sharing.", "AI": {"tldr": "PAIA\u662f\u4e00\u79cd\u65b0\u578b\u7684\u6269\u6563\u6a21\u578b\u6982\u5ff5\u5ba1\u8ba1\u6846\u67b6\uff0c\u901a\u8fc7\u76f4\u63a5\u5206\u6790\u6a21\u578b\u5185\u90e8\u884c\u4e3a\uff0c\u65e0\u9700\u4f18\u5316\u63d0\u793a\u6216\u751f\u6210\u56fe\u50cf\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5ba1\u8ba1\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u968f\u7740\u6269\u6563\u6a21\u578b\u7684\u666e\u53ca\u548c\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u6280\u672f\u7684\u53d1\u5c55\uff0c\u6a21\u578b\u53ef\u80fd\u751f\u6210\u654f\u611f\u6216\u672a\u7ecf\u6388\u6743\u7684\u5185\u5bb9\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u6709\u6548\u7684\u5ba1\u8ba1\u5de5\u5177\u3002", "method": "\u63d0\u51faPrompt-Agnostic Image-Free Auditing (PAIA)\u6846\u67b6\uff0c\u76f4\u63a5\u5206\u6790\u6a21\u578b\u5185\u90e8\u884c\u4e3a\uff0c\u907f\u514d\u4f9d\u8d56\u63d0\u793a\u6216\u751f\u6210\u56fe\u50cf\u3002", "result": "\u5728320\u4e2a\u63a7\u5236\u6a21\u578b\u548c690\u4e2a\u771f\u5b9e\u793e\u533a\u6a21\u578b\u4e0a\u6d4b\u8bd5\uff0cPAIA\u68c0\u6d4b\u51c6\u786e\u7387\u8d85\u8fc790%\uff0c\u5ba1\u8ba1\u65f6\u95f4\u51cf\u5c1118-40\u500d\u3002", "conclusion": "PAIA\u662f\u9996\u4e2a\u53ef\u6269\u5c55\u4e14\u5b9e\u7528\u7684\u6269\u6563\u6a21\u578b\u9884\u90e8\u7f72\u6982\u5ff5\u5ba1\u8ba1\u89e3\u51b3\u65b9\u6848\uff0c\u4e3a\u6a21\u578b\u5171\u4eab\u63d0\u4f9b\u4e86\u66f4\u5b89\u5168\u900f\u660e\u7684\u57fa\u7840\u3002"}}
{"id": "2504.14183", "pdf": "https://arxiv.org/pdf/2504.14183", "abs": "https://arxiv.org/abs/2504.14183", "authors": ["Natalia Tomashenko", "Xiaoxiao Miao", "Emmanuel Vincent", "Junichi Yamagishi"], "title": "The First VoicePrivacy Attacker Challenge", "categories": ["eess.AS", "cs.CL", "cs.CR"], "comment": "Published in: ICASSP 2025 - 2025 IEEE International Conference on\n  Acoustics, Speech and Signal Processing (ICASSP)", "summary": "The First VoicePrivacy Attacker Challenge is an ICASSP 2025 SP Grand\nChallenge which focuses on evaluating attacker systems against a set of voice\nanonymization systems submitted to the VoicePrivacy 2024 Challenge. Training,\ndevelopment, and evaluation datasets were provided along with a baseline\nattacker. Participants developed their attacker systems in the form of\nautomatic speaker verification systems and submitted their scores on the\ndevelopment and evaluation data. The best attacker systems reduced the equal\nerror rate (EER) by 25-44% relative w.r.t. the baseline.", "AI": {"tldr": "\u9996\u5c4aVoicePrivacy\u653b\u51fb\u8005\u6311\u6218\u8d5b\u65e8\u5728\u8bc4\u4f30\u653b\u51fb\u7cfb\u7edf\u5bf9\u8bed\u97f3\u533f\u540d\u5316\u7cfb\u7edf\u7684\u6548\u679c\uff0c\u6700\u4f73\u653b\u51fb\u7cfb\u7edf\u5c06\u57fa\u7ebfEER\u964d\u4f4e\u4e8625-44%\u3002", "motivation": "\u8bc4\u4f30\u653b\u51fb\u7cfb\u7edf\u5bf9\u8bed\u97f3\u533f\u540d\u5316\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u63a8\u52a8\u76f8\u5173\u6280\u672f\u53d1\u5c55\u3002", "method": "\u53c2\u4e0e\u8005\u5f00\u53d1\u81ea\u52a8\u8bf4\u8bdd\u4eba\u9a8c\u8bc1\u7cfb\u7edf\u4f5c\u4e3a\u653b\u51fb\u7cfb\u7edf\uff0c\u4f7f\u7528\u63d0\u4f9b\u7684\u8bad\u7ec3\u3001\u5f00\u53d1\u548c\u8bc4\u4f30\u6570\u636e\u96c6\u3002", "result": "\u6700\u4f73\u653b\u51fb\u7cfb\u7edf\u5c06\u57fa\u7ebfEER\u964d\u4f4e\u4e8625-44%\u3002", "conclusion": "\u6311\u6218\u8d5b\u5c55\u793a\u4e86\u653b\u51fb\u7cfb\u7edf\u5bf9\u8bed\u97f3\u533f\u540d\u5316\u7cfb\u7edf\u7684\u663e\u8457\u6548\u679c\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u51c6\u3002"}}
{"id": "2504.14854", "pdf": "https://arxiv.org/pdf/2504.14854", "abs": "https://arxiv.org/abs/2504.14854", "authors": ["Cosmin Safta", "Reese E. Jones", "Ravi G. Patel", "Raelynn Wonnacot", "Dan S. Bolintineanu", "Craig M. Hamel", "Sharlotte L. B. Kramer"], "title": "Uncertainty quantification of neural network models of evolving processes via Langevin sampling", "categories": ["cs.LG", "stat.ML"], "comment": "23 pages, 15 figures", "summary": "We propose a scalable, approximate inference hypernetwork framework for a\ngeneral model of history-dependent processes. The flexible data model is based\non a neural ordinary differential equation (NODE) representing the evolution of\ninternal states together with a trainable observation model subcomponent. The\nposterior distribution corresponding to the data model parameters (weights and\nbiases) follows a stochastic differential equation with a drift term related to\nthe score of the posterior that is learned jointly with the data model\nparameters. This Langevin sampling approach offers flexibility in balancing the\ncomputational budget between the evaluation cost of the data model and the\napproximation of the posterior density of its parameters. We demonstrate\nperformance of the hypernetwork on chemical reaction and material physics data\nand compare it to mean-field variational inference.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecfODE\u7684\u53ef\u6269\u5c55\u8fd1\u4f3c\u63a8\u7406\u8d85\u7f51\u7edc\u6846\u67b6\uff0c\u7528\u4e8e\u5386\u53f2\u4f9d\u8d56\u8fc7\u7a0b\uff0c\u901a\u8fc7Langevin\u91c7\u6837\u5e73\u8861\u8ba1\u7b97\u6210\u672c\uff0c\u5e76\u5728\u5316\u5b66\u548c\u7269\u7406\u6570\u636e\u4e0a\u9a8c\u8bc1\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u5386\u53f2\u4f9d\u8d56\u8fc7\u7a0b\u7684\u5efa\u6a21\u95ee\u9898\uff0c\u63d0\u4f9b\u7075\u6d3b\u7684\u63a8\u7406\u6846\u67b6\u3002", "method": "\u4f7f\u7528\u795e\u7ecfODE\u8868\u793a\u5185\u90e8\u72b6\u6001\u6f14\u5316\uff0c\u7ed3\u5408\u53ef\u8bad\u7ec3\u89c2\u6d4b\u6a21\u578b\uff0c\u901a\u8fc7Langevin\u91c7\u6837\u5b66\u4e60\u540e\u9a8c\u5206\u5e03\u3002", "result": "\u5728\u5316\u5b66\u53cd\u5e94\u548c\u6750\u6599\u7269\u7406\u6570\u636e\u4e0a\u8868\u73b0\u4f18\u4e8e\u5747\u503c\u573a\u53d8\u5206\u63a8\u7406\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u8ba1\u7b97\u6210\u672c\u548c\u540e\u9a8c\u8fd1\u4f3c\u4e4b\u95f4\u63d0\u4f9b\u4e86\u7075\u6d3b\u6027\uff0c\u9002\u7528\u4e8e\u590d\u6742\u8fc7\u7a0b\u5efa\u6a21\u3002"}}
{"id": "2504.14879", "pdf": "https://arxiv.org/pdf/2504.14879", "abs": "https://arxiv.org/abs/2504.14879", "authors": ["Hassan Wasswa", "Aziida Nanyonga", "Timothy Lynar"], "title": "Impact of Latent Space Dimension on IoT Botnet Detection Performance: VAE-Encoder Versus ViT-Encoder", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The rapid evolution of Internet of Things (IoT) technology has led to a\nsignificant increase in the number of IoT devices, applications, and services.\nThis surge in IoT devices, along with their widespread presence, has made them\na prime target for various cyber-attacks, particularly through IoT botnets. As\na result, security has become a major concern within the IoT ecosystem. This\nstudy focuses on investigating how the latent dimension impacts the performance\nof different deep learning classifiers when trained on latent vector\nrepresentations of the train dataset. The primary objective is to compare the\noutcomes of these models when encoder components from two cutting-edge\narchitectures: the Vision Transformer (ViT) and the Variational Auto-Encoder\n(VAE) are utilized to project the high dimensional train dataset to the learned\nlow dimensional latent space. The encoder components are employed to project\nhigh-dimensional structured .csv IoT botnet traffic datasets to various latent\nsizes. Evaluated on N-BaIoT and CICIoT2022 datasets, findings reveal that\nVAE-encoder based dimension reduction outperforms ViT-encoder based dimension\nreduction for both datasets in terms of four performance metrics including\naccuracy, precision, recall, and F1-score for all models which can be\nattributed to absence of spatial patterns in the datasets the ViT model\nattempts to learn and extract from image instances.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u6f5c\u5728\u7ef4\u5ea6\u5bf9\u6df1\u5ea6\u5b66\u4e60\u5206\u7c7b\u5668\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u6bd4\u8f83\u4e86ViT\u548cVAE\u7f16\u7801\u5668\u5728IoT\u50f5\u5c38\u7f51\u7edc\u6d41\u91cf\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0VAE\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "IoT\u8bbe\u5907\u6570\u91cf\u6fc0\u589e\uff0c\u5b89\u5168\u5a01\u80c1\u589e\u52a0\uff0c\u5c24\u5176\u662f\u50f5\u5c38\u7f51\u7edc\u653b\u51fb\uff0c\u9700\u63d0\u5347\u5206\u7c7b\u5668\u6027\u80fd\u4ee5\u5e94\u5bf9\u3002", "method": "\u4f7f\u7528ViT\u548cVAE\u7f16\u7801\u5668\u5c06\u9ad8\u7ef4\u6570\u636e\u96c6\u6295\u5f71\u5230\u4f4e\u7ef4\u6f5c\u5728\u7a7a\u95f4\uff0c\u6bd4\u8f83\u4e0d\u540c\u6f5c\u5728\u7ef4\u5ea6\u4e0b\u7684\u5206\u7c7b\u5668\u6027\u80fd\u3002", "result": "VAE\u7f16\u7801\u5668\u5728\u51c6\u786e\u7387\u3001\u7cbe\u786e\u7387\u3001\u53ec\u56de\u7387\u548cF1\u5206\u6570\u4e0a\u5747\u4f18\u4e8eViT\u7f16\u7801\u5668\u3002", "conclusion": "VAE\u66f4\u9002\u5408\u5904\u7406\u65e0\u7a7a\u95f4\u6a21\u5f0f\u7684IoT\u6d41\u91cf\u6570\u636e\uff0cViT\u56e0\u4f9d\u8d56\u56fe\u50cf\u7a7a\u95f4\u6a21\u5f0f\u800c\u8868\u73b0\u8f83\u5dee\u3002"}}
{"id": "2504.13957", "pdf": "https://arxiv.org/pdf/2504.13957", "abs": "https://arxiv.org/abs/2504.13957", "authors": ["Lianne Potter"], "title": "Naming is framing: How cybersecurity's language problems are repeating in AI governance", "categories": ["cs.CY", "cs.AI", "cs.CR"], "comment": "20 pages, 2 figures", "summary": "Language is not neutral; it frames understanding, structures power, and\nshapes governance. This paper argues that misnomers like cybersecurity and\nartificial intelligence (AI) are more than semantic quirks; they carry\nsignificant governance risks by obscuring human agency, inflating expectations,\nand distorting accountability. Drawing on lessons from cybersecurity's\nlinguistic pitfalls, such as the 'weakest link' narrative, this paper\nhighlights how AI discourse is falling into similar traps with metaphors like\n'alignment,' 'black box,' and 'hallucination.' These terms embed adversarial,\nmystifying, or overly technical assumptions into governance structures. In\nresponse, the paper advocates for a language-first approach to AI governance:\none that interrogates dominant metaphors, foregrounds human roles, and\nco-develops a lexicon that is precise, inclusive, and reflexive. This paper\ncontends that linguistic reform is not peripheral to governance but central to\nthe construction of transparent, equitable, and anticipatory regulatory\nframeworks.", "AI": {"tldr": "\u8bba\u6587\u4e3b\u5f20\u8bed\u8a00\u5728\u6cbb\u7406\u4e2d\u7684\u6838\u5fc3\u4f5c\u7528\uff0c\u6307\u51fa\u672f\u8bed\u5982\u201c\u7f51\u7edc\u5b89\u5168\u201d\u548c\u201c\u4eba\u5de5\u667a\u80fd\u201d\u63a9\u76d6\u4e86\u4eba\u4e3a\u56e0\u7d20\uff0c\u63d0\u51fa\u4e86\u8bed\u8a00\u4f18\u5148\u7684\u6cbb\u7406\u65b9\u6cd5\u3002", "motivation": "\u63ed\u793a\u672f\u8bed\u5bf9\u6cbb\u7406\u7684\u6f5c\u5728\u98ce\u9669\uff0c\u5982\u6a21\u7cca\u8d23\u4efb\u548c\u5938\u5927\u671f\u671b\uff0c\u547c\u5401\u6539\u9769\u8bed\u8a00\u4ee5\u6784\u5efa\u900f\u660e\u548c\u516c\u5e73\u7684\u76d1\u7ba1\u6846\u67b6\u3002", "method": "\u901a\u8fc7\u5206\u6790\u7f51\u7edc\u5b89\u5168\u4e2d\u7684\u8bed\u8a00\u9677\u9631\uff0c\u7c7b\u6bd4\u4eba\u5de5\u667a\u80fd\u9886\u57df\u7684\u7c7b\u4f3c\u95ee\u9898\uff0c\u63d0\u51fa\u8bed\u8a00\u4f18\u5148\u7684\u6cbb\u7406\u7b56\u7565\u3002", "result": "\u6307\u51fa\u5f53\u524dAI\u672f\u8bed\uff08\u5982\u201c\u5bf9\u9f50\u201d\u3001\u201c\u9ed1\u7bb1\u201d\uff09\u7684\u8d1f\u9762\u5f71\u54cd\uff0c\u5f3a\u8c03\u8bed\u8a00\u6539\u9769\u5bf9\u6cbb\u7406\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u8bed\u8a00\u6539\u9769\u662f\u6784\u5efa\u900f\u660e\u3001\u516c\u5e73\u548c\u524d\u77bb\u6027\u76d1\u7ba1\u6846\u67b6\u7684\u6838\u5fc3\uff0c\u9700\u5171\u540c\u5f00\u53d1\u7cbe\u786e\u3001\u5305\u5bb9\u548c\u53cd\u601d\u6027\u7684\u8bcd\u6c47\u3002"}}
{"id": "2504.14882", "pdf": "https://arxiv.org/pdf/2504.14882", "abs": "https://arxiv.org/abs/2504.14882", "authors": ["Mojtaba Kolahdouzi", "Hatice Gunes", "Ali Etemad"], "title": "Some Optimizers are More Equal: Understanding the Role of Optimizers in Group Fairness", "categories": ["cs.LG", "cs.CV", "stat.ML"], "comment": null, "summary": "We study whether and how the choice of optimization algorithm can impact\ngroup fairness in deep neural networks. Through stochastic differential\nequation analysis of optimization dynamics in an analytically tractable setup,\nwe demonstrate that the choice of optimization algorithm indeed influences\nfairness outcomes, particularly under severe imbalance. Furthermore, we show\nthat when comparing two categories of optimizers, adaptive methods and\nstochastic methods, RMSProp (from the adaptive category) has a higher\nlikelihood of converging to fairer minima than SGD (from the stochastic\ncategory). Building on this insight, we derive two new theoretical guarantees\nshowing that, under appropriate conditions, RMSProp exhibits fairer parameter\nupdates and improved fairness in a single optimization step compared to SGD. We\nthen validate these findings through extensive experiments on three publicly\navailable datasets, namely CelebA, FairFace, and MS-COCO, across different\ntasks as facial expression recognition, gender classification, and multi-label\nclassification, using various backbones. Considering multiple fairness\ndefinitions including equalized odds, equal opportunity, and demographic\nparity, adaptive optimizers like RMSProp and Adam consistently outperform SGD\nin terms of group fairness, while maintaining comparable predictive accuracy.\nOur results highlight the role of adaptive updates as a crucial yet overlooked\nmechanism for promoting fair outcomes.", "AI": {"tldr": "\u7814\u7a76\u4f18\u5316\u7b97\u6cd5\u9009\u62e9\u5bf9\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u4e2d\u7fa4\u4f53\u516c\u5e73\u6027\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u81ea\u9002\u5e94\u4f18\u5316\u5668\uff08\u5982RMSProp\uff09\u6bd4\u968f\u673a\u4f18\u5316\u5668\uff08\u5982SGD\uff09\u66f4\u6613\u6536\u655b\u5230\u516c\u5e73\u89e3\uff0c\u5e76\u901a\u8fc7\u7406\u8bba\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "motivation": "\u63a2\u8ba8\u4f18\u5316\u7b97\u6cd5\u5982\u4f55\u5f71\u54cd\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u7fa4\u4f53\u516c\u5e73\u6027\uff0c\u5c24\u5176\u662f\u5728\u6570\u636e\u4e0d\u5e73\u8861\u60c5\u51b5\u4e0b\u3002", "method": "\u901a\u8fc7\u968f\u673a\u5fae\u5206\u65b9\u7a0b\u5206\u6790\u4f18\u5316\u52a8\u6001\uff0c\u6bd4\u8f83\u81ea\u9002\u5e94\u4f18\u5316\u5668\uff08\u5982RMSProp\uff09\u4e0e\u968f\u673a\u4f18\u5316\u5668\uff08\u5982SGD\uff09\uff0c\u5e76\u9a8c\u8bc1\u4e8e\u591a\u4e2a\u6570\u636e\u96c6\u548c\u4efb\u52a1\u3002", "result": "\u81ea\u9002\u5e94\u4f18\u5316\u5668\u5728\u516c\u5e73\u6027\u6307\u6807\uff08\u5982\u5747\u7b49\u673a\u4f1a\u3001\u4eba\u53e3\u5747\u7b49\uff09\u4e0a\u4f18\u4e8eSGD\uff0c\u540c\u65f6\u4fdd\u6301\u9884\u6d4b\u51c6\u786e\u6027\u3002", "conclusion": "\u81ea\u9002\u5e94\u66f4\u65b0\u673a\u5236\u662f\u4fc3\u8fdb\u516c\u5e73\u7ed3\u679c\u7684\u5173\u952e\u56e0\u7d20\uff0c\u503c\u5f97\u8fdb\u4e00\u6b65\u5173\u6ce8\u3002"}}
{"id": "2504.14889", "pdf": "https://arxiv.org/pdf/2504.14889", "abs": "https://arxiv.org/abs/2504.14889", "authors": ["Seunghun Lee", "Jinyoung Park", "Jaewon Chu", "Minseo Yoon", "Hyunwoo J. Kim"], "title": "Latent Bayesian Optimization via Autoregressive Normalizing Flows", "categories": ["cs.LG", "cs.AI"], "comment": "ICLR 2025", "summary": "Bayesian Optimization (BO) has been recognized for its effectiveness in\noptimizing expensive and complex objective functions. Recent advancements in\nLatent Bayesian Optimization (LBO) have shown promise by integrating generative\nmodels such as variational autoencoders (VAEs) to manage the complexity of\nhigh-dimensional and structured data spaces. However, existing LBO approaches\noften suffer from the value discrepancy problem, which arises from the\nreconstruction gap between input and latent spaces. This value discrepancy\nproblem propagates errors throughout the optimization process, leading to\nsuboptimal outcomes. To address this issue, we propose a Normalizing Flow-based\nBayesian Optimization (NF-BO), which utilizes normalizing flow as a generative\nmodel to establish one-to-one encoding function from the input space to the\nlatent space, along with its left-inverse decoding function, eliminating the\nreconstruction gap. Specifically, we introduce SeqFlow, an autoregressive\nnormalizing flow for sequence data. In addition, we develop a new candidate\nsampling strategy that dynamically adjusts the exploration probability for each\ntoken based on its importance. Through extensive experiments, our NF-BO method\ndemonstrates superior performance in molecule generation tasks, significantly\noutperforming both traditional and recent LBO approaches.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f52\u4e00\u5316\u6d41\u7684\u8d1d\u53f6\u65af\u4f18\u5316\u65b9\u6cd5\uff08NF-BO\uff09\uff0c\u89e3\u51b3\u4e86\u6f5c\u5728\u8d1d\u53f6\u65af\u4f18\u5316\u4e2d\u7684\u503c\u5dee\u5f02\u95ee\u9898\uff0c\u5e76\u5728\u5206\u5b50\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u6f5c\u5728\u8d1d\u53f6\u65af\u4f18\u5316\u65b9\u6cd5\uff08LBO\uff09\u56e0\u8f93\u5165\u7a7a\u95f4\u4e0e\u6f5c\u5728\u7a7a\u95f4\u7684\u91cd\u6784\u5dee\u8ddd\u5bfc\u81f4\u503c\u5dee\u5f02\u95ee\u9898\uff0c\u5f71\u54cd\u4f18\u5316\u6548\u679c\u3002", "method": "\u4f7f\u7528\u5f52\u4e00\u5316\u6d41\u4f5c\u4e3a\u751f\u6210\u6a21\u578b\uff0c\u5efa\u7acb\u8f93\u5165\u7a7a\u95f4\u5230\u6f5c\u5728\u7a7a\u95f4\u7684\u4e00\u5bf9\u4e00\u7f16\u7801\u51fd\u6570\u53ca\u5176\u5de6\u9006\u89e3\u7801\u51fd\u6570\uff0c\u6d88\u9664\u91cd\u6784\u5dee\u8ddd\uff1b\u63d0\u51faSeqFlow\uff08\u81ea\u56de\u5f52\u5f52\u4e00\u5316\u6d41\uff09\u548c\u52a8\u6001\u8c03\u6574\u63a2\u7d22\u6982\u7387\u7684\u5019\u9009\u91c7\u6837\u7b56\u7565\u3002", "result": "\u5728\u5206\u5b50\u751f\u6210\u4efb\u52a1\u4e2d\uff0cNF-BO\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u548c\u8fd1\u671fLBO\u65b9\u6cd5\u3002", "conclusion": "NF-BO\u901a\u8fc7\u6d88\u9664\u91cd\u6784\u5dee\u8ddd\uff0c\u89e3\u51b3\u4e86\u503c\u5dee\u5f02\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u4f18\u5316\u6027\u80fd\u3002"}}
{"id": "2504.14245", "pdf": "https://arxiv.org/pdf/2504.14245", "abs": "https://arxiv.org/abs/2504.14245", "authors": ["Yikun Ji", "Yan Hong", "Jiahui Zhan", "Haoxing Chen", "jun lan", "Huijia Zhu", "Weiqiang Wang", "Liqing Zhang", "Jianfu Zhang"], "title": "Towards Explainable Fake Image Detection with Multi-Modal Large Language Models", "categories": ["cs.CV", "cs.CL", "I.2.7; I.2.10"], "comment": null, "summary": "Progress in image generation raises significant public security concerns. We\nargue that fake image detection should not operate as a \"black box\". Instead,\nan ideal approach must ensure both strong generalization and transparency.\nRecent progress in Multi-modal Large Language Models (MLLMs) offers new\nopportunities for reasoning-based AI-generated image detection. In this work,\nwe evaluate the capabilities of MLLMs in comparison to traditional detection\nmethods and human evaluators, highlighting their strengths and limitations.\nFurthermore, we design six distinct prompts and propose a framework that\nintegrates these prompts to develop a more robust, explainable, and\nreasoning-driven detection system. The code is available at\nhttps://github.com/Gennadiyev/mllm-defake.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u7684AI\u751f\u6210\u56fe\u50cf\u68c0\u6d4b\u65b9\u6cd5\uff0c\u4e0e\u4f20\u7edf\u65b9\u6cd5\u548c\u4eba\u5de5\u8bc4\u4f30\u76f8\u6bd4\uff0c\u5c55\u793a\u4e86\u5176\u4f18\u52bf\u548c\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u66f4\u9c81\u68d2\u3001\u53ef\u89e3\u91ca\u7684\u68c0\u6d4b\u6846\u67b6\u3002", "motivation": "\u56fe\u50cf\u751f\u6210\u6280\u672f\u7684\u8fdb\u6b65\u5f15\u53d1\u4e86\u516c\u5171\u5b89\u5168\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u6cdb\u5316\u53c8\u900f\u660e\u7684\u5047\u56fe\u50cf\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u8bc4\u4f30MLLMs\u4e0e\u4f20\u7edf\u68c0\u6d4b\u65b9\u6cd5\u53ca\u4eba\u5de5\u8bc4\u4f30\u7684\u80fd\u529b\u5dee\u5f02\uff0c\u8bbe\u8ba1\u516d\u79cd\u63d0\u793a\u5e76\u6574\u5408\u4e3a\u6846\u67b6\u3002", "result": "MLLMs\u5728\u68c0\u6d4bAI\u751f\u6210\u56fe\u50cf\u65b9\u9762\u8868\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u4ecd\u6709\u5c40\u9650\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u7ed3\u5408MLLMs\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4e3a\u5047\u56fe\u50cf\u68c0\u6d4b\u63d0\u4f9b\u4e86\u66f4\u9c81\u68d2\u548c\u53ef\u89e3\u91ca\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.14907", "pdf": "https://arxiv.org/pdf/2504.14907", "abs": "https://arxiv.org/abs/2504.14907", "authors": ["Kexin Wang", "Mengna Liu", "Xu Cheng", "Fan Shi", "Shanshan Qi", "Shengyong Chen"], "title": "Dynamic Graph-Like Learning with Contrastive Clustering on Temporally-Factored Ship Motion Data for Imbalanced Sea State Estimation in Autonomous Vessel", "categories": ["cs.LG"], "comment": "13 pages,15 figures", "summary": "Accurate sea state estimation is crucial for the real-time control and future\nstate prediction of autonomous vessels. However, traditional methods struggle\nwith challenges such as data imbalance and feature redundancy in ship motion\ndata, limiting their effectiveness. To address these challenges, we propose the\nTemporal-Graph Contrastive Clustering Sea State Estimator (TGC-SSE), a novel\ndeep learning model that combines three key components: a time dimension\nfactorization module to reduce data redundancy, a dynamic graph-like learning\nmodule to capture complex variable interactions, and a contrastive clustering\nloss function to effectively manage class imbalance. Our experiments\ndemonstrate that TGC-SSE significantly outperforms existing methods across 14\npublic datasets, achieving the highest accuracy in 9 datasets, with a 20.79%\nimprovement over EDI. Furthermore, in the field of sea state estimation,\nTGC-SSE surpasses five benchmark methods and seven deep learning models.\nAblation studies confirm the effectiveness of each module, demonstrating their\nrespective roles in enhancing overall model performance. Overall, TGC-SSE not\nonly improves the accuracy of sea state estimation but also exhibits strong\ngeneralization capabilities, providing reliable support for autonomous vessel\noperations.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTGC-SSE\u7684\u65b0\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u7528\u4e8e\u89e3\u51b3\u6d77\u51b5\u4f30\u8ba1\u4e2d\u7684\u6570\u636e\u5197\u4f59\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5728\u5904\u7406\u8239\u8236\u8fd0\u52a8\u6570\u636e\u65f6\u5b58\u5728\u6570\u636e\u4e0d\u5e73\u8861\u548c\u7279\u5f81\u5197\u4f59\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5176\u6709\u6548\u6027\u3002", "method": "TGC-SSE\u7ed3\u5408\u4e86\u65f6\u95f4\u7ef4\u5ea6\u5206\u89e3\u6a21\u5757\u3001\u52a8\u6001\u56fe\u5b66\u4e60\u6a21\u5757\u548c\u5bf9\u6bd4\u805a\u7c7b\u635f\u5931\u51fd\u6570\u3002", "result": "\u572814\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c9\u4e2a\u6570\u636e\u96c6\u4e0a\u51c6\u786e\u7387\u6700\u9ad8\uff0c\u6bd4EDI\u63d0\u534720.79%\u3002", "conclusion": "TGC-SSE\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u6d77\u51b5\u4f30\u8ba1\u7684\u51c6\u786e\u6027\uff0c\u8fd8\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2504.14260", "pdf": "https://arxiv.org/pdf/2504.14260", "abs": "https://arxiv.org/abs/2504.14260", "authors": ["Liu Xiao", "Li Zhiyuan", "Lin Yueyu"], "title": "Cross-attention for State-based model RWKV-7", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "We introduce CrossWKV, a novel cross-attention mechanism for the state-based\nRWKV-7 model, designed to enhance the expressive power of text-to-image\ngeneration. Leveraging RWKV-7's linear-complexity Weighted Key-Value (WKV)\narchitecture, CrossWKV integrates text and image modalities in a single pass,\nutilizing a generalized delta rule with vector-valued gating and low-rank\nadaptations (LoRA) to achieve superior cross-modal alignment. Unlike\nTransformer-based models, CrossWKV's non-diagonal, input-dependent transition\nmatrix enables it to represent complex functions beyond the $\\mathrm{TC}^0$\ncomplexity class, including all regular languages, as demonstrated by its\nability to perform state-tracking tasks like $S_5$ permutation modeling.\nEvaluated within the Diffusion in RWKV-7 (DIR-7) on datasets such as LAION-5B\nand ImageNet, CrossWKV achieves a Frechet Inception Distance (FID) of 2.88 and\na CLIP score of 0.33 on ImageNet 256x256, matching state-of-the-art performance\nwhile offering robust generalization across diverse prompts. The model's\nenhanced expressivity, combined with constant memory usage and linear scaling,\npositions it as a powerful solution for advanced cross-modal tasks, with\npotential applications in high-resolution generation and dynamic state\nmanipulation.Code at https://github.com/TorchRWKV/flash-linear-attention", "AI": {"tldr": "CrossWKV\u662f\u4e00\u79cd\u65b0\u578b\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\uff0c\u7528\u4e8e\u589e\u5f3aRWKV-7\u6a21\u578b\u5728\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u8868\u73b0\u529b\uff0c\u901a\u8fc7\u7ebf\u6027\u590d\u6742\u5ea6\u7684WKV\u67b6\u6784\u548cLoRA\u6280\u672f\u5b9e\u73b0\u8de8\u6a21\u6001\u5bf9\u9f50\u3002", "motivation": "\u63d0\u5347\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u7684\u8868\u73b0\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u7ebf\u6027\u590d\u6742\u5ea6\u548c\u9ad8\u6548\u5185\u5b58\u4f7f\u7528\u3002", "method": "\u7ed3\u5408\u5e7f\u4e49delta\u89c4\u5219\u3001\u5411\u91cf\u95e8\u63a7\u548c\u4f4e\u79e9\u9002\u5e94\uff08LoRA\uff09\uff0c\u5229\u7528\u975e\u5bf9\u89d2\u8f93\u5165\u4f9d\u8d56\u7684\u8f6c\u79fb\u77e9\u9635\u5b9e\u73b0\u590d\u6742\u529f\u80fd\u3002", "result": "\u5728ImageNet 256x256\u4e0a\u8fbe\u5230FID 2.88\u548cCLIP\u5206\u65700.33\uff0c\u6027\u80fd\u4e0e\u6700\u5148\u8fdb\u6a21\u578b\u76f8\u5f53\u3002", "conclusion": "CrossWKV\u5728\u8de8\u6a21\u6001\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u9002\u7528\u4e8e\u9ad8\u5206\u8fa8\u7387\u751f\u6210\u548c\u52a8\u6001\u72b6\u6001\u64cd\u4f5c\u3002"}}
{"id": "2504.14917", "pdf": "https://arxiv.org/pdf/2504.14917", "abs": "https://arxiv.org/abs/2504.14917", "authors": ["Chunjing Gan", "Dan Yang", "Binbin Hu", "Ziqi Liu", "Yue Shen", "Zhiqiang Zhang", "Jian Wang", "Jun Zhou"], "title": "POLYRAG: Integrating Polyviews into Retrieval-Augmented Generation for Medical Applications", "categories": ["cs.LG"], "comment": null, "summary": "Large language models (LLMs) have become a disruptive force in the industry,\nintroducing unprecedented capabilities in natural language processing, logical\nreasoning and so on. However, the challenges of knowledge updates and\nhallucination issues have limited the application of LLMs in medical scenarios,\nwhere retrieval-augmented generation (RAG) can offer significant assistance.\nNevertheless, existing retrieve-then-read approaches generally digest the\nretrieved documents, without considering the timeliness, authoritativeness and\ncommonality of retrieval. We argue that these approaches can be suboptimal,\nespecially in real-world applications where information from different sources\nmight conflict with each other and even information from the same source in\ndifferent time scale might be different, and totally relying on this would\ndeteriorate the performance of RAG approaches. We propose PolyRAG that\ncarefully incorporate judges from different perspectives and finally integrate\nthe polyviews for retrieval augmented generation in medical applications. Due\nto the scarcity of real-world benchmarks for evaluation, to bridge the gap we\npropose PolyEVAL, a benchmark consists of queries and documents collected from\nreal-world medical scenarios (including medical policy, hospital & doctor\ninquiry and healthcare) with multiple tagging (e.g., timeliness,\nauthoritativeness) on them. Extensive experiments and analysis on PolyEVAL have\ndemonstrated the superiority of PolyRAG.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faPolyRAG\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u89c6\u89d2\u6574\u5408\u68c0\u7d22\u4fe1\u606f\u4ee5\u4f18\u5316\u533b\u7597\u573a\u666f\u4e2d\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\uff0c\u5e76\u5f15\u5165PolyEVAL\u57fa\u51c6\u9a8c\u8bc1\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u533b\u7597\u573a\u666f\u4e2d\u9762\u4e34\u77e5\u8bc6\u66f4\u65b0\u548c\u5e7b\u89c9\u95ee\u9898\uff0c\u73b0\u6709\u68c0\u7d22\u589e\u5f3a\u65b9\u6cd5\u672a\u8003\u8651\u4fe1\u606f\u7684\u65f6\u6548\u6027\u3001\u6743\u5a01\u6027\u548c\u5171\u6027\uff0c\u5bfc\u81f4\u6027\u80fd\u53d7\u9650\u3002", "method": "\u63d0\u51faPolyRAG\uff0c\u4ece\u591a\u89c6\u89d2\u8bc4\u4f30\u68c0\u7d22\u4fe1\u606f\u5e76\u6574\u5408\uff0c\u540c\u65f6\u5f00\u53d1PolyEVAL\u57fa\u51c6\u7528\u4e8e\u771f\u5b9e\u533b\u7597\u573a\u666f\u7684\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660ePolyRAG\u5728PolyEVAL\u57fa\u51c6\u4e0a\u8868\u73b0\u4f18\u8d8a\u3002", "conclusion": "PolyRAG\u901a\u8fc7\u591a\u89c6\u89d2\u6574\u5408\u663e\u8457\u63d0\u5347\u4e86\u533b\u7597\u573a\u666f\u4e2dRAG\u7684\u6027\u80fd\uff0cPolyEVAL\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9e\u7528\u57fa\u51c6\u3002"}}
{"id": "2504.13969", "pdf": "https://arxiv.org/pdf/2504.13969", "abs": "https://arxiv.org/abs/2504.13969", "authors": ["Nayoung Choi", "Peace Cyebukayire", "Jinho D. Choi"], "title": "Tinker Tales: Interactive Storytelling Framework for Early Childhood Narrative Development and AI Literacy", "categories": ["cs.HC", "cs.AI", "cs.CY"], "comment": null, "summary": "This paper presents Tinker Tales, an interactive storytelling framework in\nthe format of a board game, designed to support both narrative development and\nAI literacy in early childhood. The framework integrates tangible and\nspeech-based interactions with AI through NFC chip-attached pawns and tokens,\nalong with a speaker and microphone. Children select and define key story\nelements-such as characters, places, items, and emotions-using the pawns and\ntokens, providing further details to the AI and receiving proper assistance,\nsimilar to how adults prompt AI for specific tasks (e.g., writing). For\nevaluation, several game sessions were simulated with a child AI agent, and the\nquality and safety of the generated stories were assessed from various\nperspectives. This work highlights the potential of combining physical and\ndigital elements in AI literacy, offering a safe and engaging way for children\nto learn how to effectively collaborate with AI.", "AI": {"tldr": "Tinker Tales\u662f\u4e00\u4e2a\u4e92\u52a8\u5f0f\u6545\u4e8b\u8bb2\u8ff0\u6846\u67b6\uff0c\u4ee5\u68cb\u76d8\u6e38\u620f\u5f62\u5f0f\u8bbe\u8ba1\uff0c\u65e8\u5728\u652f\u6301\u5e7c\u513f\u7684\u53d9\u4e8b\u53d1\u5c55\u548cAI\u7d20\u517b\u3002", "motivation": "\u901a\u8fc7\u7ed3\u5408\u5b9e\u4f53\u548c\u8bed\u97f3\u4ea4\u4e92\uff0c\u4e3a\u513f\u7ae5\u63d0\u4f9b\u5b89\u5168\u4e14\u6709\u8da3\u7684\u65b9\u5f0f\u5b66\u4e60\u4e0eAI\u534f\u4f5c\u3002", "method": "\u4f7f\u7528NFC\u82af\u7247\u9644\u7740\u7684\u68cb\u5b50\u548c\u4ee4\u724c\uff0c\u513f\u7ae5\u9009\u62e9\u5e76\u5b9a\u4e49\u6545\u4e8b\u5143\u7d20\uff0cAI\u63d0\u4f9b\u8f85\u52a9\u3002\u901a\u8fc7\u6a21\u62df\u6e38\u620f\u4f1a\u8bdd\u8bc4\u4f30\u751f\u6210\u6545\u4e8b\u7684\u8d28\u91cf\u548c\u5b89\u5168\u6027\u3002", "result": "\u5c55\u793a\u4e86\u7269\u7406\u548c\u6570\u5b57\u5143\u7d20\u7ed3\u5408\u5728AI\u7d20\u517b\u4e2d\u7684\u6f5c\u529b\uff0c\u751f\u6210\u7684\u6545\u4e8b\u8d28\u91cf\u9ad8\u4e14\u5b89\u5168\u3002", "conclusion": "Tinker Tales\u4e3a\u513f\u7ae5\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684AI\u534f\u4f5c\u5b66\u4e60\u5de5\u5177\uff0c\u517c\u5177\u6559\u80b2\u6027\u548c\u8da3\u5473\u6027\u3002"}}
{"id": "2504.14359", "pdf": "https://arxiv.org/pdf/2504.14359", "abs": "https://arxiv.org/abs/2504.14359", "authors": ["Kyle Buettner", "Jacob Emmerson", "Adriana Kovashka"], "title": "A Multimodal Recaptioning Framework to Account for Perceptual Diversity in Multilingual Vision-Language Modeling", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "There are many ways to describe, name, and group objects when captioning an\nimage. Differences are evident when speakers come from diverse cultures due to\nthe unique experiences that shape perception. Machine translation of captions\nhas pushed multilingual capabilities in vision-language models (VLMs), but data\ncomes mainly from English speakers, indicating a perceptual bias and lack of\nmodel flexibility. In this work, we address this challenge and outline a\ndata-efficient framework to instill multilingual VLMs with greater\nunderstanding of perceptual diversity. We specifically propose an LLM-based,\nmultimodal recaptioning strategy that alters the object descriptions of English\ncaptions before translation. The greatest benefits are demonstrated in a\ntargeted multimodal mechanism guided by native speaker data. By adding produced\nrewrites as augmentations in training, we improve on German and Japanese\ntext-image retrieval cases studies (up to +3.5 mean recall overall, +4.7 on\nnon-native error cases). We further propose a mechanism to analyze the specific\nobject description differences across datasets, and we offer insights into\ncross-dataset and cross-language generalization.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLLM\u7684\u591a\u6a21\u6001\u91cd\u6807\u6ce8\u7b56\u7565\uff0c\u901a\u8fc7\u4fee\u6539\u82f1\u6587\u6807\u6ce8\u4ee5\u589e\u5f3a\u591a\u8bed\u8a00\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5bf9\u611f\u77e5\u591a\u6837\u6027\u7684\u7406\u89e3\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5fb7\u8bed\u548c\u65e5\u8bed\u7684\u6587\u672c-\u56fe\u50cf\u68c0\u7d22\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u6570\u636e\u4e3b\u8981\u6765\u81ea\u82f1\u8bed\u4f7f\u7528\u8005\uff0c\u5bfc\u81f4\u611f\u77e5\u504f\u89c1\u548c\u6a21\u578b\u7075\u6d3b\u6027\u4e0d\u8db3\uff0c\u65e0\u6cd5\u5145\u5206\u4f53\u73b0\u4e0d\u540c\u6587\u5316\u80cc\u666f\u4e0b\u7684\u611f\u77e5\u591a\u6837\u6027\u3002", "method": "\u91c7\u7528LLM\u9a71\u52a8\u7684\u591a\u6a21\u6001\u91cd\u6807\u6ce8\u7b56\u7565\uff0c\u4fee\u6539\u82f1\u6587\u6807\u6ce8\u540e\u518d\u7ffb\u8bd1\uff0c\u5e76\u7ed3\u5408\u6bcd\u8bed\u6570\u636e\u6307\u5bfc\u7684\u591a\u6a21\u6001\u673a\u5236\u8fdb\u884c\u8bad\u7ec3\u589e\u5f3a\u3002", "result": "\u5728\u5fb7\u8bed\u548c\u65e5\u8bed\u7684\u6587\u672c-\u56fe\u50cf\u68c0\u7d22\u6848\u4f8b\u4e2d\uff0c\u5e73\u5747\u53ec\u56de\u7387\u63d0\u53473.5\uff0c\u975e\u6bcd\u8bed\u9519\u8bef\u6848\u4f8b\u63d0\u53474.7\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u591a\u8bed\u8a00\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5bf9\u611f\u77e5\u591a\u6837\u6027\u7684\u7406\u89e3\uff0c\u5e76\u63d0\u4f9b\u4e86\u8de8\u6570\u636e\u96c6\u548c\u8de8\u8bed\u8a00\u6cdb\u5316\u7684\u5206\u6790\u673a\u5236\u3002"}}
{"id": "2504.14937", "pdf": "https://arxiv.org/pdf/2504.14937", "abs": "https://arxiv.org/abs/2504.14937", "authors": ["Anna Zeng", "Michael Cafarella", "Batya Kenig", "Markos Markakis", "Brit Youngmann", "Babak Salimi"], "title": "Causal DAG Summarization (Full Version)", "categories": ["cs.LG", "cs.DB", "stat.ME"], "comment": null, "summary": "Causal inference aids researchers in discovering cause-and-effect\nrelationships, leading to scientific insights. Accurate causal estimation\nrequires identifying confounding variables to avoid false discoveries. Pearl's\ncausal model uses causal DAGs to identify confounding variables, but incorrect\nDAGs can lead to unreliable causal conclusions. However, for high dimensional\ndata, the causal DAGs are often complex beyond human verifiability. Graph\nsummarization is a logical next step, but current methods for general-purpose\ngraph summarization are inadequate for causal DAG summarization. This paper\naddresses these challenges by proposing a causal graph summarization objective\nthat balances graph simplification for better understanding while retaining\nessential causal information for reliable inference. We develop an efficient\ngreedy algorithm and show that summary causal DAGs can be directly used for\ninference and are more robust to misspecification of assumptions, enhancing\nrobustness for causal inference. Experimenting with six real-life datasets, we\ncompared our algorithm to three existing solutions, showing its effectiveness\nin handling high-dimensional data and its ability to generate summary DAGs that\nensure both reliable causal inference and robustness against misspecifications.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u56e0\u679c\u56fe\u6458\u8981\u65b9\u6cd5\uff0c\u901a\u8fc7\u7b80\u5316\u56e0\u679cDAG\u4ee5\u63d0\u9ad8\u53ef\u7406\u89e3\u6027\uff0c\u540c\u65f6\u4fdd\u7559\u5173\u952e\u56e0\u679c\u4fe1\u606f\uff0c\u786e\u4fdd\u53ef\u9760\u63a8\u65ad\u3002", "motivation": "\u9ad8\u7ef4\u6570\u636e\u7684\u56e0\u679cDAG\u590d\u6742\u4e14\u96be\u4ee5\u9a8c\u8bc1\uff0c\u73b0\u6709\u901a\u7528\u56fe\u6458\u8981\u65b9\u6cd5\u4e0d\u9002\u7528\u4e8e\u56e0\u679cDAG\u6458\u8981\uff0c\u9700\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u56e0\u679c\u56fe\u6458\u8981\u76ee\u6807\uff0c\u5f00\u53d1\u9ad8\u6548\u8d2a\u5fc3\u7b97\u6cd5\uff0c\u751f\u6210\u6458\u8981DAG\u7528\u4e8e\u63a8\u65ad\u3002", "result": "\u5728\u516d\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u7b97\u6cd5\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u80fd\u5904\u7406\u9ad8\u7ef4\u6570\u636e\u4e14\u751f\u6210\u7684\u6458\u8981DAG\u66f4\u7a33\u5065\u3002", "conclusion": "\u6458\u8981\u56e0\u679cDAG\u53ef\u76f4\u63a5\u7528\u4e8e\u63a8\u65ad\uff0c\u63d0\u9ad8\u56e0\u679c\u63a8\u65ad\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2504.13971", "pdf": "https://arxiv.org/pdf/2504.13971", "abs": "https://arxiv.org/abs/2504.13971", "authors": ["Abdelrahman Soliman"], "title": "The Future of Internet of Things and Multimodal Language Models in 6G Networks: Opportunities and Challenges", "categories": ["cs.CY", "cs.AI", "cs.ET", "cs.NI"], "comment": "11 pages", "summary": "Based on recent trends in artificial intelligence and IoT research. The\ncooperative potential of integrating the Internet of Things (IoT) and\nMultimodal Language Models (MLLMs) is presented in this survey paper for future\n6G systems. It focuses on the applications of this integration in different\nfields, such as healthcare, agriculture, and smart cities, and investigates the\nfour pillars of IoT integration, such as sensors, communication, processing,\nand security. The paper provides a comprehensive description of IoT and MLLM\ntechnologies and applications, addresses the role of multimodality in each\npillar, and concludes with an overview of the most significant challenges and\ndirections for future research. The general survey is a roadmap for researchers\ninterested in tracing the application areas of MLLMs and IoT, highlighting the\npotential and challenges in this rapidly growing field. The survey recognizes\nthe need to deal with data availability, computational expense, privacy, and\nreal-time processing to harness the complete potential of IoT, MLLM, and 6G\ntechnology", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u7269\u8054\u7f51\uff08IoT\uff09\u4e0e\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u57286G\u7cfb\u7edf\u4e2d\u7684\u534f\u540c\u6f5c\u529b\uff0c\u5206\u6790\u4e86\u5176\u5728\u533b\u7597\u3001\u519c\u4e1a\u548c\u667a\u6167\u57ce\u5e02\u7b49\u9886\u57df\u7684\u5e94\u7528\uff0c\u5e76\u7814\u7a76\u4e86\u4f20\u611f\u5668\u3001\u901a\u4fe1\u3001\u5904\u7406\u548c\u5b89\u5168\u56db\u5927\u652f\u67f1\u3002", "motivation": "\u7814\u7a76IoT\u4e0eMLLMs\u7684\u6574\u5408\u6f5c\u529b\uff0c\u4e3a6G\u7cfb\u7edf\u63d0\u4f9b\u6280\u672f\u8def\u7ebf\u56fe\uff0c\u89e3\u51b3\u6570\u636e\u53ef\u7528\u6027\u3001\u8ba1\u7b97\u6210\u672c\u3001\u9690\u79c1\u548c\u5b9e\u65f6\u5904\u7406\u7b49\u6311\u6218\u3002", "method": "\u901a\u8fc7\u5168\u9762\u63cf\u8ff0IoT\u548cMLLMs\u7684\u6280\u672f\u4e0e\u5e94\u7528\uff0c\u5206\u6790\u591a\u6a21\u6001\u5728\u5404\u652f\u67f1\u4e2d\u7684\u4f5c\u7528\uff0c\u5e76\u603b\u7ed3\u5173\u952e\u6311\u6218\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "result": "\u63d0\u51fa\u4e86IoT\u4e0eMLLMs\u6574\u5408\u7684\u5e94\u7528\u524d\u666f\uff0c\u660e\u786e\u4e86\u6280\u672f\u6311\u6218\uff0c\u4e3a\u7814\u7a76\u8005\u63d0\u4f9b\u4e86\u672a\u6765\u7814\u7a76\u7684\u8def\u7ebf\u56fe\u3002", "conclusion": "\u6574\u5408IoT\u4e0eMLLMs\u57286G\u7cfb\u7edf\u4e2d\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u9700\u89e3\u51b3\u6570\u636e\u3001\u8ba1\u7b97\u3001\u9690\u79c1\u548c\u5b9e\u65f6\u5904\u7406\u7b49\u95ee\u9898\u4ee5\u5b9e\u73b0\u5176\u5b8c\u6574\u4ef7\u503c\u3002"}}
{"id": "2504.14945", "pdf": "https://arxiv.org/pdf/2504.14945", "abs": "https://arxiv.org/abs/2504.14945", "authors": ["Jianhao Yan", "Yafu Li", "Zican Hu", "Zhi Wang", "Ganqu Cui", "Xiaoye Qu", "Yu Cheng", "Yue Zhang"], "title": "Learning to Reason under Off-Policy Guidance", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Work in progress", "summary": "Recent advances in large reasoning models (LRMs) demonstrate that\nsophisticated behaviors such as multi-step reasoning and self-reflection can\nemerge via reinforcement learning (RL) with simple rule-based rewards. However,\nexisting zero-RL approaches are inherently ``on-policy'', limiting learning to\na model's own outputs and failing to acquire reasoning abilities beyond its\ninitial capabilities. We introduce LUFFY (Learning to reason Under oFF-policY\nguidance), a framework that augments zero-RL with off-policy reasoning traces.\nLUFFY dynamically balances imitation and exploration by combining off-policy\ndemonstrations with on-policy rollouts during training. Notably, we propose\npolicy shaping via regularized importance sampling to avoid superficial and\nrigid imitation during mixed-policy training. Remarkably, LUFFY achieves an\nover +7.0 average gain across six math benchmarks and an advantage of over +6.2\npoints in out-of-distribution tasks. It also substantially surpasses\nimitation-based supervised fine-tuning (SFT), particularly in generalization.\nAnalysis shows LUFFY not only imitates effectively but also explores beyond\ndemonstrations, offering a scalable path to train generalizable reasoning\nmodels with off-policy guidance.", "AI": {"tldr": "LUFFY\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u79bb\u7b56\u7565\u63a8\u7406\u8f68\u8ff9\u548c\u7b56\u7565\u5185\u8bad\u7ec3\uff0c\u63d0\u5347\u4e86\u63a8\u7406\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u96f6\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5c40\u9650\u4e8e\u7b56\u7565\u5185\u8bad\u7ec3\uff0c\u65e0\u6cd5\u8d85\u8d8a\u521d\u59cb\u63a8\u7406\u80fd\u529b\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u65b9\u6cd5\u6765\u7ed3\u5408\u79bb\u7b56\u7565\u6307\u5bfc\u3002", "method": "\u63d0\u51faLUFFY\u6846\u67b6\uff0c\u52a8\u6001\u5e73\u8861\u6a21\u4eff\u4e0e\u63a2\u7d22\uff0c\u4f7f\u7528\u6b63\u5219\u5316\u91cd\u8981\u6027\u91c7\u6837\u907f\u514d\u6d45\u5c42\u6a21\u4eff\u3002", "result": "\u5728\u516d\u4e2a\u6570\u5b66\u57fa\u51c6\u4e0a\u5e73\u5747\u63d0\u53477.0\u5206\uff0c\u5728\u5206\u5e03\u5916\u4efb\u52a1\u4e2d\u4f18\u52bf\u8fbe6.2\u5206\uff0c\u663e\u8457\u4f18\u4e8e\u76d1\u7763\u5fae\u8c03\u3002", "conclusion": "LUFFY\u4e3a\u8bad\u7ec3\u5177\u6709\u6cdb\u5316\u80fd\u529b\u7684\u63a8\u7406\u6a21\u578b\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u8def\u5f84\uff0c\u540c\u65f6\u6709\u6548\u6a21\u4eff\u548c\u63a2\u7d22\u3002"}}
{"id": "2504.13972", "pdf": "https://arxiv.org/pdf/2504.13972", "abs": "https://arxiv.org/abs/2504.13972", "authors": ["Dana Alsagheer", "Abdulrahman Kamal", "Mohammad Kamal", "Weidong Shi"], "title": "Governance Challenges in Reinforcement Learning from Human Feedback: Evaluator Rationality and Reinforcement Stability", "categories": ["cs.CY", "cs.AI"], "comment": null, "summary": "Reinforcement Learning from Human Feedback (RLHF) is central in aligning\nlarge language models (LLMs) with human values and expectations. However, the\nprocess remains susceptible to governance challenges, including evaluator bias,\ninconsistency, and the unreliability of feedback. This study examines how the\ncognitive capacity of evaluators, specifically their level of rationality,\naffects the stability of reinforcement signals. A controlled experiment\ncomparing high-rationality and low-rationality participants reveals that\nevaluators with higher rationality scores produce significantly more consistent\nand expert-aligned feedback. In contrast, lower-rationality participants\ndemonstrate considerable variability in their reinforcement decisions ($p <\n0.01$). To address these challenges and improve RLHF governance, we recommend\nimplementing evaluator pre-screening, systematic auditing of feedback\nconsistency, and reliability-weighted reinforcement aggregation. These measures\nenhance the fairness, transparency, and robustness of AI alignment pipelines.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u8bc4\u4f30\u8005\u7684\u7406\u6027\u6c34\u5e73\u5982\u4f55\u5f71\u54cd\u5f3a\u5316\u5b66\u4e60\u4fe1\u53f7\u7684\u7a33\u5b9a\u6027\uff0c\u53d1\u73b0\u9ad8\u7406\u6027\u8bc4\u4f30\u8005\u63d0\u4f9b\u66f4\u4e00\u81f4\u7684\u53cd\u9988\uff0c\u5e76\u63d0\u51fa\u6539\u8fdbRLHF\u6cbb\u7406\u7684\u5efa\u8bae\u3002", "motivation": "RLHF\u5728\u8bed\u8a00\u6a21\u578b\u5bf9\u9f50\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u9762\u4e34\u8bc4\u4f30\u8005\u504f\u89c1\u3001\u4e0d\u4e00\u81f4\u548c\u53cd\u9988\u4e0d\u53ef\u9760\u7b49\u6cbb\u7406\u6311\u6218\u3002", "method": "\u901a\u8fc7\u5bf9\u6bd4\u9ad8\u7406\u6027\u4e0e\u4f4e\u7406\u6027\u8bc4\u4f30\u8005\u7684\u5b9e\u9a8c\uff0c\u5206\u6790\u5176\u5bf9\u5f3a\u5316\u4fe1\u53f7\u7a33\u5b9a\u6027\u7684\u5f71\u54cd\u3002", "result": "\u9ad8\u7406\u6027\u8bc4\u4f30\u8005\u53cd\u9988\u66f4\u4e00\u81f4\u4e14\u4e0e\u4e13\u5bb6\u5bf9\u9f50\uff08p < 0.01\uff09\uff0c\u4f4e\u7406\u6027\u8005\u5219\u53d8\u5f02\u6027\u5927\u3002", "conclusion": "\u5efa\u8bae\u5b9e\u65bd\u8bc4\u4f30\u8005\u9884\u7b5b\u9009\u3001\u53cd\u9988\u4e00\u81f4\u6027\u5ba1\u8ba1\u548c\u53ef\u9760\u6027\u52a0\u6743\u805a\u5408\uff0c\u4ee5\u63d0\u5347AI\u5bf9\u9f50\u7684\u516c\u5e73\u6027\u548c\u7a33\u5065\u6027\u3002"}}
{"id": "2504.14946", "pdf": "https://arxiv.org/pdf/2504.14946", "abs": "https://arxiv.org/abs/2504.14946", "authors": ["Tin Ping Chan", "Yunlong Cheng", "Yizhan Zhu", "Xiaofeng Gao", "Guihai Chen"], "title": "Symmetry-Preserving Architecture for Multi-NUMA Environments (SPANE): A Deep Reinforcement Learning Approach for Dynamic VM Scheduling", "categories": ["cs.LG"], "comment": "10 pages, 7 figures. Accepted to IEEE INFOCOM 2025", "summary": "As cloud computing continues to evolve, the adoption of multi-NUMA\n(Non-Uniform Memory Access) architecture by cloud service providers has\nintroduced new challenges in virtual machine (VM) scheduling. To address these\nchallenges and more accurately reflect the complexities faced by modern cloud\nenvironments, we introduce the Dynamic VM Allocation problem in Multi-NUMA PM\n(DVAMP). We formally define both offline and online versions of DVAMP as\nmixed-integer linear programming problems, providing a rigorous mathematical\nfoundation for analysis. A tight performance bound for greedy online algorithms\nis derived, offering insights into the worst-case optimality gap as a function\nof the number of physical machines and VM lifetime variability. To address the\nchallenges posed by DVAMP, we propose SPANE (Symmetry-Preserving Architecture\nfor Multi-NUMA Environments), a novel deep reinforcement learning approach that\nexploits the problem's inherent symmetries. SPANE produces invariant results\nunder arbitrary permutations of physical machine states, enhancing learning\nefficiency and solution quality. Extensive experiments conducted on the\nHuawei-East-1 dataset demonstrate that SPANE outperforms existing baselines,\nreducing average VM wait time by 45%. Our work contributes to the field of\ncloud resource management by providing both theoretical insights and practical\nsolutions for VM scheduling in multi-NUMA environments, addressing a critical\ngap in the literature and offering improved performance for real-world cloud\nsystems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u52a8\u6001\u865a\u62df\u673a\u5206\u914d\u95ee\u9898\uff08DVAMP\uff09\u53ca\u89e3\u51b3\u65b9\u6848SPANE\uff0c\u901a\u8fc7\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u591aNUMA\u73af\u5883\u4e0b\u7684\u865a\u62df\u673a\u8c03\u5ea6\uff0c\u51cf\u5c11\u5e73\u5747\u7b49\u5f85\u65f6\u95f445%\u3002", "motivation": "\u591aNUMA\u67b6\u6784\u5728\u4e91\u8ba1\u7b97\u4e2d\u7684\u666e\u53ca\u5e26\u6765\u4e86\u865a\u62df\u673a\u8c03\u5ea6\u65b0\u6311\u6218\uff0c\u9700\u66f4\u51c6\u786e\u53cd\u6620\u73b0\u4ee3\u4e91\u73af\u5883\u7684\u590d\u6742\u6027\u3002", "method": "\u5b9a\u4e49DVAMP\u4e3a\u6df7\u5408\u6574\u6570\u7ebf\u6027\u89c4\u5212\u95ee\u9898\uff0c\u63d0\u51faSPANE\uff08\u5bf9\u79f0\u4fdd\u6301\u67b6\u6784\uff09\uff0c\u5229\u7528\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u89e3\u51b3\u3002", "result": "SPANE\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e73\u5747\u865a\u62df\u673a\u7b49\u5f85\u65f6\u95f4\u51cf\u5c1145%\u3002", "conclusion": "\u7814\u7a76\u586b\u8865\u4e86\u591aNUMA\u73af\u5883\u4e0b\u865a\u62df\u673a\u8c03\u5ea6\u7684\u7406\u8bba\u7a7a\u767d\uff0c\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.14370", "pdf": "https://arxiv.org/pdf/2504.14370", "abs": "https://arxiv.org/abs/2504.14370", "authors": ["Jon Kleinberg", "Fan Wei"], "title": "Density Measures for Language Generation", "categories": ["math.CO", "cs.CL", "cs.DM", "cs.LG"], "comment": null, "summary": "The recent successes of large language models (LLMs) have led to a surge of\ntheoretical research into language generation. A recent line of work proposes\nan abstract view, called language generation in the limit, where generation is\nseen as a game between an adversary and an algorithm: the adversary generates\nstrings from an unknown language $K$, chosen from a countable collection of\ncandidate languages, and after seeing a finite set of these strings, the\nalgorithm must generate new strings from $K$ that it has not seen before. This\nformalism highlights a key tension: the trade-off between validity (the\nalgorithm should only produce strings from the language) and breadth (it should\nbe able to produce many strings from the language). This trade-off is central\nin applied language generation as well, where it appears as a balance between\nhallucination (generating invalid utterances) and mode collapse (generating\nonly a restricted set of outputs). Despite its importance, this trade-off has\nbeen challenging to study quantitatively. We develop ways to quantify this\ntrade-off by formalizing breadth using measures of density. Existing algorithms\nfor language generation in the limit produce output sets that can have zero\ndensity in the true language, and this important failure of breadth might seem\nunavoidable. We show, however, that such a failure is not necessary: we provide\nan algorithm for language generation in the limit whose outputs have strictly\npositive density in $K$. We also study the internal representations built by\nthese algorithms, specifically the sequence of hypothesized candidate languages\nthey consider, and show that achieving the strongest form of breadth may\nrequire oscillating indefinitely between high- and low-density representations.\nOur analysis introduces a novel topology on language families, with notions of\nconvergence and limit points playing a key role.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u62bd\u8c61\u7684\u8bed\u8a00\u751f\u6210\u6846\u67b6\uff0c\u7814\u7a76\u7b97\u6cd5\u5728\u672a\u77e5\u8bed\u8a00\u4e2d\u751f\u6210\u65b0\u5b57\u7b26\u4e32\u65f6\u7684\u6709\u6548\u6027\u4e0e\u5e7f\u5ea6\u7684\u6743\u8861\uff0c\u5e76\u5f00\u53d1\u4e86\u91cf\u5316\u65b9\u6cd5\u3002", "motivation": "\u7814\u7a76\u8bed\u8a00\u751f\u6210\u4e2d\u6709\u6548\u6027\u4e0e\u5e7f\u5ea6\u7684\u6743\u8861\uff0c\u89e3\u51b3\u73b0\u6709\u7b97\u6cd5\u8f93\u51fa\u5bc6\u5ea6\u4e3a\u96f6\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u7b97\u6cd5\uff0c\u786e\u4fdd\u8f93\u51fa\u5728\u771f\u5b9e\u8bed\u8a00\u4e2d\u5177\u6709\u4e25\u683c\u6b63\u5bc6\u5ea6\uff0c\u5e76\u5206\u6790\u5176\u5185\u90e8\u8868\u793a\u3002", "result": "\u7b97\u6cd5\u5b9e\u73b0\u4e86\u4e25\u683c\u6b63\u5bc6\u5ea6\u7684\u8f93\u51fa\uff0c\u5e76\u63ed\u793a\u4e86\u5b9e\u73b0\u6700\u5927\u5e7f\u5ea6\u53ef\u80fd\u9700\u8981\u5728\u9ad8\u5bc6\u5ea6\u4e0e\u4f4e\u5bc6\u5ea6\u8868\u793a\u4e4b\u95f4\u632f\u8361\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165\u65b0\u7684\u8bed\u8a00\u65cf\u62d3\u6251\u7ed3\u6784\uff0c\u8bba\u6587\u4e3a\u8bed\u8a00\u751f\u6210\u4e2d\u7684\u6743\u8861\u95ee\u9898\u63d0\u4f9b\u4e86\u5b9a\u91cf\u5206\u6790\u5de5\u5177\u3002"}}
{"id": "2504.14955", "pdf": "https://arxiv.org/pdf/2504.14955", "abs": "https://arxiv.org/abs/2504.14955", "authors": ["Manthankumar Solanki"], "title": "Efficient Document Retrieval with G-Retriever", "categories": ["cs.LG"], "comment": "Extended version of a paper presented at NeurIPS 2024\n  (arXiv:2402.07630)", "summary": "Textual data question answering has gained significant attention due to its\ngrowing applicability. Recently, a novel approach leveraging the\nRetrieval-Augmented Generation (RAG) method was introduced, utilizing the\nPrize-Collecting Steiner Tree (PCST) optimization for sub-graph construction.\nHowever, this method focused solely on node attributes, leading to incomplete\ncontextual understanding. In this paper, we propose an enhanced approach that\nreplaces the PCST method with an attention-based sub-graph construction\ntechnique, enabling more efficient and context-aware retrieval. Additionally,\nwe encode both node and edge attributes, leading to richer graph\nrepresentations. Our method also incorporates an improved projection layer and\nmulti-head attention pooling for better alignment with Large Language Models\n(LLMs). Experimental evaluations on the WebQSP dataset demonstrate that our\napproach is competitive and achieves marginally better results compared to the\noriginal method, underscoring its potential for more accurate question\nanswering.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u5b50\u56fe\u6784\u5efa\u65b9\u6cd5\uff0c\u66ff\u4ee3\u4e86\u539f\u6709\u7684PCST\u65b9\u6cd5\uff0c\u5e76\u7ed3\u5408\u8282\u70b9\u548c\u8fb9\u5c5e\u6027\u7f16\u7801\uff0c\u63d0\u5347\u4e86\u95ee\u7b54\u7cfb\u7edf\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4ec5\u5173\u6ce8\u8282\u70b9\u5c5e\u6027\uff0c\u5bfc\u81f4\u4e0a\u4e0b\u6587\u7406\u89e3\u4e0d\u5b8c\u6574\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u68c0\u7d22\u548c\u66f4\u4e30\u5bcc\u7684\u56fe\u8868\u793a\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u5b50\u56fe\u6784\u5efa\u6280\u672f\uff0c\u7f16\u7801\u8282\u70b9\u548c\u8fb9\u5c5e\u6027\uff0c\u5e76\u6539\u8fdb\u6295\u5f71\u5c42\u548c\u591a\u5934\u6ce8\u610f\u529b\u6c60\u5316\u4ee5\u66f4\u597d\u5730\u5bf9\u9f50LLMs\u3002", "result": "\u5728WebQSP\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u65b0\u65b9\u6cd5\u6027\u80fd\u7565\u4f18\u4e8e\u539f\u65b9\u6cd5\u3002", "conclusion": "\u6539\u8fdb\u7684\u65b9\u6cd5\u5728\u95ee\u7b54\u4efb\u52a1\u4e2d\u66f4\u5177\u6f5c\u529b\uff0c\u80fd\u63d0\u4f9b\u66f4\u51c6\u786e\u7684\u7b54\u6848\u3002"}}
{"id": "2504.14960", "pdf": "https://arxiv.org/pdf/2504.14960", "abs": "https://arxiv.org/abs/2504.14960", "authors": ["Dennis Liu", "Zijie Yan", "Xin Yao", "Tong Liu", "Vijay Korthikanti", "Evan Wu", "Shiqing Fan", "Gao Deng", "Hongxiao Bai", "Ashwath Aithal", "Michael Andersch", "Mohammad Shoeybi", "Jiajie Yao", "Chandler Zhou", "David Wu", "Xipeng Li", "June Yang"], "title": "MoE Parallel Folding: Heterogeneous Parallelism Mappings for Efficient Large-Scale MoE Model Training with Megatron Core", "categories": ["cs.LG", "cs.DC"], "comment": null, "summary": "Mixture of Experts (MoE) models enhance neural network scalability by\ndynamically selecting relevant experts per input token, enabling larger model\nsizes while maintaining manageable computation costs. However, efficient\ntraining of large-scale MoE models across thousands of GPUs presents\nsignificant challenges due to limitations in existing parallelism strategies.\nWe introduce an end-to-end training framework for large-scale MoE models that\nutilizes five-dimensional hybrid parallelism: Tensor Parallelism, Expert\nParallelism, Context Parallelism, Data Parallelism, and Pipeline Parallelism.\nCentral to our approach is MoE Parallel Folding, a novel strategy that\ndecouples the parallelization of attention and MoE layers in Transformer\nmodels, allowing each layer type to adopt optimal parallel configurations.\nAdditionally, we develop a flexible token-level dispatcher that supports both\ntoken-dropping and token-dropless MoE training across all five dimensions of\nparallelism. This dispatcher accommodates dynamic tensor shapes and coordinates\ndifferent parallelism schemes for Attention and MoE layers, facilitating\ncomplex parallelism implementations. Our experiments demonstrate significant\nimprovements in training efficiency and scalability. We achieve up to 49.3%\nModel Flops Utilization (MFU) for the Mixtral 8x22B model and 39.0% MFU for the\nQwen2-57B-A14B model on H100 GPUs, outperforming existing methods. The\nframework scales efficiently up to 1,024 GPUs and maintains high performance\nwith sequence lengths up to 128K tokens, validating its effectiveness for\nlarge-scale MoE model training. The code is available in Megatron-Core.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4e94\u7ef4\u6df7\u5408\u5e76\u884c\u6846\u67b6\uff0c\u7528\u4e8e\u9ad8\u6548\u8bad\u7ec3\u5927\u89c4\u6a21MoE\u6a21\u578b\uff0c\u901a\u8fc7MoE Parallel Folding\u7b56\u7565\u548c\u7075\u6d3b\u7684\u4ee4\u724c\u8c03\u5ea6\u5668\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bad\u7ec3\u6548\u7387\u548c\u6269\u5c55\u6027\u3002", "motivation": "\u73b0\u6709\u5e76\u884c\u7b56\u7565\u96be\u4ee5\u9ad8\u6548\u8bad\u7ec3\u5927\u89c4\u6a21MoE\u6a21\u578b\uff0c\u9650\u5236\u4e86\u5176\u6269\u5c55\u6027\u548c\u8ba1\u7b97\u6548\u7387\u3002", "method": "\u5f15\u5165\u4e94\u7ef4\u6df7\u5408\u5e76\u884c\uff08\u5f20\u91cf\u3001\u4e13\u5bb6\u3001\u4e0a\u4e0b\u6587\u3001\u6570\u636e\u548c\u6d41\u6c34\u7ebf\u5e76\u884c\uff09\u548cMoE Parallel Folding\u7b56\u7565\uff0c\u5f00\u53d1\u7075\u6d3b\u7684\u4ee4\u724c\u8c03\u5ea6\u5668\u3002", "result": "\u5728Mixtral 8x22B\u548cQwen2-57B-A14B\u6a21\u578b\u4e0a\u5206\u522b\u8fbe\u523049.3%\u548c39.0%\u7684MFU\uff0c\u652f\u63011024 GPU\u548c128K\u4ee4\u724c\u5e8f\u5217\u957f\u5ea6\u3002", "conclusion": "\u8be5\u6846\u67b6\u663e\u8457\u63d0\u5347\u5927\u89c4\u6a21MoE\u6a21\u578b\u7684\u8bad\u7ec3\u6548\u7387\u548c\u6269\u5c55\u6027\uff0c\u9002\u7528\u4e8e\u8d85\u5927\u89c4\u6a21\u6a21\u578b\u8bad\u7ec3\u3002"}}
{"id": "2504.13976", "pdf": "https://arxiv.org/pdf/2504.13976", "abs": "https://arxiv.org/abs/2504.13976", "authors": ["Wrick Talukdar"], "title": "Gas Station of the Future: A Perspective on AI/ML and IoT in Retail Downstream", "categories": ["cs.CY", "cs.AI"], "comment": null, "summary": "The gas station of the future is poised to transform from a simple fuel\ndispensing center into an intelligent retail hub, driven by advancements in\nArtificial Intelligence (AI), Machine Learning (ML), and the Internet of Things\n(IoT). This paper explores how technology is reshaping the retail downstream\nsector while briefly addressing the upstream and midstream segments. By\nleveraging AI/ML for predictive analytics, dynamic pricing, personalized\ncustomer engagement, and IoT for real-time monitoring and automation, the\nfuture gas station will redefine the fuel retail experience. Additionally, this\npaper incorporates statistics, AI/ML core technical concepts, mathematical\nformulations, case studies, and a proposed framework for a fully autonomous gas\nstation.", "AI": {"tldr": "\u672a\u6765\u52a0\u6cb9\u7ad9\u5c06\u5229\u7528AI\u3001ML\u548cIoT\u6280\u672f\uff0c\u4ece\u7b80\u5355\u7684\u71c3\u6599\u4f9b\u5e94\u4e2d\u5fc3\u8f6c\u53d8\u4e3a\u667a\u80fd\u96f6\u552e\u4e2d\u5fc3\u3002", "motivation": "\u63a2\u8ba8\u6280\u672f\u5982\u4f55\u91cd\u5851\u96f6\u552e\u4e0b\u6e38\u9886\u57df\uff0c\u540c\u65f6\u7b80\u8981\u6d89\u53ca\u4e0a\u6e38\u548c\u4e2d\u6e38\u9886\u57df\u3002", "method": "\u5229\u7528AI/ML\u8fdb\u884c\u9884\u6d4b\u5206\u6790\u3001\u52a8\u6001\u5b9a\u4ef7\u548c\u4e2a\u6027\u5316\u5ba2\u6237\u4e92\u52a8\uff0c\u7ed3\u5408IoT\u5b9e\u73b0\u5b9e\u65f6\u76d1\u63a7\u548c\u81ea\u52a8\u5316\u3002", "result": "\u63d0\u51fa\u4e00\u4e2a\u5b8c\u5168\u81ea\u4e3b\u7684\u52a0\u6cb9\u7ad9\u6846\u67b6\uff0c\u5e76\u5c55\u793a\u76f8\u5173\u7edf\u8ba1\u6570\u636e\u3001\u6280\u672f\u6982\u5ff5\u548c\u6848\u4f8b\u7814\u7a76\u3002", "conclusion": "\u672a\u6765\u7684\u52a0\u6cb9\u7ad9\u5c06\u91cd\u65b0\u5b9a\u4e49\u71c3\u6599\u96f6\u552e\u4f53\u9a8c\u3002"}}
{"id": "2504.14994", "pdf": "https://arxiv.org/pdf/2504.14994", "abs": "https://arxiv.org/abs/2504.14994", "authors": ["Hankang Sun", "Guiming Li", "Su Yang", "Baoqi Li"], "title": "Learning Compositional Transferability of Time Series for Source-Free Domain Adaptation", "categories": ["cs.LG"], "comment": "Corresponding author: Su Yang", "summary": "Domain adaptation is challenging for time series classification due to the\nhighly dynamic nature. This study tackles the most difficult subtask when both\ntarget labels and source data are inaccessible, namely, source-free domain\nadaptation. To reuse the classification backbone pre-trained on source data,\ntime series reconstruction is a sound solution that aligns target and source\ntime series by minimizing the reconstruction errors of both. However, simply\nfine-tuning the source pre-trained reconstruction model on target data may lose\nthe learnt priori, and it struggles to accommodate domain varying temporal\npatterns in a single encoder-decoder. Therefore, this paper tries to\ndisentangle the composition of domain transferability by using a compositional\narchitecture for time series reconstruction. Here, the preceding component is a\nU-net frozen since pre-trained, the output of which during adaptation is the\ninitial reconstruction of a given target time series, acting as a coarse step\nto prompt the subsequent finer adaptation. The following pipeline for finer\nadaptation includes two parallel branches: The source replay branch using a\nresidual link to preserve the output of U-net, and the offset compensation\nbranch that applies an additional autoencoder (AE) to further warp U-net's\noutput. By deploying a learnable factor on either branch to scale their\ncomposition in the final output of reconstruction, the data transferability is\ndisentangled and the learnt reconstructive capability from source data is\nretained. During inference, aside from the batch-level optimization in the\ntraining, we search at test time stability-aware rescaling of source replay\nbranch to tolerate instance-wise variation. The experimental results show that\nsuch compositional architecture of time series reconstruction leads to SOTA\nperformance on 3 widely used benchmarks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u65f6\u95f4\u5e8f\u5217\u5206\u7c7b\u7684\u6e90\u81ea\u7531\u57df\u81ea\u9002\u5e94\u65b9\u6cd5\uff0c\u901a\u8fc7\u89e3\u8026\u57df\u53ef\u8f6c\u79fb\u6027\u5e76\u4f7f\u7528\u7ec4\u5408\u67b6\u6784\u8fdb\u884c\u65f6\u95f4\u5e8f\u5217\u91cd\u6784\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u65f6\u95f4\u5e8f\u5217\u5206\u7c7b\u4e2d\u7684\u57df\u81ea\u9002\u5e94\u5177\u6709\u6311\u6218\u6027\uff0c\u5c24\u5176\u662f\u5728\u76ee\u6807\u6807\u7b7e\u548c\u6e90\u6570\u636e\u5747\u4e0d\u53ef\u8bbf\u95ee\u7684\u60c5\u51b5\u4e0b\u3002\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u7559\u6e90\u6570\u636e\u9884\u8bad\u7ec3\u7684\u5206\u7c7b\u4e3b\u5e72\u3002", "method": "\u91c7\u7528\u7ec4\u5408\u67b6\u6784\u8fdb\u884c\u65f6\u95f4\u5e8f\u5217\u91cd\u6784\uff0c\u5305\u62ec\u51bb\u7ed3\u7684\u9884\u8bad\u7ec3U-net\u548c\u4e24\u4e2a\u5e76\u884c\u5206\u652f\uff08\u6e90\u91cd\u653e\u5206\u652f\u548c\u504f\u79fb\u8865\u507f\u5206\u652f\uff09\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u56e0\u5b50\u89e3\u8026\u57df\u53ef\u8f6c\u79fb\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4e09\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u7ec4\u5408\u67b6\u6784\u6709\u6548\u89e3\u51b3\u4e86\u6e90\u81ea\u7531\u57df\u81ea\u9002\u5e94\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u7559\u4e86\u6e90\u6570\u636e\u7684\u91cd\u6784\u80fd\u529b\u3002"}}
{"id": "2504.13979", "pdf": "https://arxiv.org/pdf/2504.13979", "abs": "https://arxiv.org/abs/2504.13979", "authors": ["Thippa Reddy Gadekallu", "Kapal Dev", "Sunder Ali Khowaja", "Weizheng Wang", "Hailin Feng", "Kai Fang", "Sharnil Pandya", "Wei Wang"], "title": "Framework, Standards, Applications and Best practices of Responsible AI : A Comprehensive Survey", "categories": ["cs.CY", "cs.AI"], "comment": "Submitted for peer review", "summary": "Responsible Artificial Intelligence (RAI) is a combination of ethics\nassociated with the usage of artificial intelligence aligned with the common\nand standard frameworks. This survey paper extensively discusses the global and\nnational standards, applications of RAI, current technology and ongoing\nprojects using RAI, and possible challenges in implementing and designing RAI\nin the industries and projects based on AI. Currently, ethical standards and\nimplementation of RAI are decoupled which caters each industry to follow their\nown standards to use AI ethically. Many global firms and government\norganizations are taking necessary initiatives to design a common and standard\nframework. Social pressure and unethical way of using AI forces the RAI design\nrather than implementation.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u8d1f\u8d23\u4efb\u4eba\u5de5\u667a\u80fd\uff08RAI\uff09\u7684\u5168\u7403\u548c\u56fd\u5bb6\u6807\u51c6\u3001\u5e94\u7528\u3001\u5f53\u524d\u6280\u672f\u53ca\u6311\u6218\uff0c\u5f3a\u8c03\u4f26\u7406\u6807\u51c6\u4e0e\u5b9e\u65bd\u7684\u8131\u8282\uff0c\u5e76\u63a2\u8ba8\u4e86\u884c\u4e1a\u548c\u793e\u4f1a\u538b\u529b\u5bf9RAI\u8bbe\u8ba1\u7684\u63a8\u52a8\u3002", "motivation": "\u63a2\u8ba8RAI\u7684\u4f26\u7406\u6846\u67b6\u53ca\u5176\u5728\u884c\u4e1a\u4e2d\u7684\u5e94\u7528\uff0c\u5206\u6790\u5f53\u524d\u6807\u51c6\u4e0e\u5b9e\u65bd\u7684\u8131\u8282\u95ee\u9898\uff0c\u4ee5\u53ca\u793e\u4f1a\u538b\u529b\u5bf9RAI\u8bbe\u8ba1\u7684\u63a8\u52a8\u4f5c\u7528\u3002", "method": "\u901a\u8fc7\u6587\u732e\u7efc\u8ff0\uff0c\u5206\u6790\u5168\u7403\u548c\u56fd\u5bb6\u6807\u51c6\u3001\u6280\u672f\u5e94\u7528\u3001\u9879\u76ee\u6848\u4f8b\u53ca\u5b9e\u65bd\u6311\u6218\u3002", "result": "\u53d1\u73b0RAI\u7684\u4f26\u7406\u6807\u51c6\u4e0e\u5b9e\u65bd\u5b58\u5728\u8131\u8282\uff0c\u884c\u4e1a\u5404\u81ea\u4e3a\u653f\uff0c\u793e\u4f1a\u538b\u529b\u63a8\u52a8RAI\u8bbe\u8ba1\u800c\u975e\u5b9e\u65bd\u3002", "conclusion": "\u9700\u5efa\u7acb\u7edf\u4e00\u7684RAI\u6846\u67b6\uff0c\u89e3\u51b3\u6807\u51c6\u4e0e\u5b9e\u65bd\u7684\u8131\u8282\u95ee\u9898\uff0c\u4ee5\u5e94\u5bf9\u793e\u4f1a\u538b\u529b\u548c\u4f26\u7406\u6311\u6218\u3002"}}
{"id": "2504.14526", "pdf": "https://arxiv.org/pdf/2504.14526", "abs": "https://arxiv.org/abs/2504.14526", "authors": ["Tong Zeng", "Longfeng Wu", "Liang Shi", "Dawei Zhou", "Feng Guo"], "title": "Are Vision LLMs Road-Ready? A Comprehensive Benchmark for Safety-Critical Driving Video Understanding", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Vision Large Language Models (VLLMs) have demonstrated impressive\ncapabilities in general visual tasks such as image captioning and visual\nquestion answering. However, their effectiveness in specialized,\nsafety-critical domains like autonomous driving remains largely unexplored.\nAutonomous driving systems require sophisticated scene understanding in complex\nenvironments, yet existing multimodal benchmarks primarily focus on normal\ndriving conditions, failing to adequately assess VLLMs' performance in\nsafety-critical scenarios. To address this, we introduce DVBench, a pioneering\nbenchmark designed to evaluate the performance of VLLMs in understanding\nsafety-critical driving videos. Built around a hierarchical ability taxonomy\nthat aligns with widely adopted frameworks for describing driving scenarios\nused in assessing highly automated driving systems, DVBench features 10,000\nmultiple-choice questions with human-annotated ground-truth answers, enabling a\ncomprehensive evaluation of VLLMs' capabilities in perception and reasoning.\nExperiments on 14 SOTA VLLMs, ranging from 0.5B to 72B parameters, reveal\nsignificant performance gaps, with no model achieving over 40% accuracy,\nhighlighting critical limitations in understanding complex driving scenarios.\nTo probe adaptability, we fine-tuned selected models using domain-specific data\nfrom DVBench, achieving accuracy gains ranging from 5.24 to 10.94 percentage\npoints, with relative improvements of up to 43.59%. This improvement\nunderscores the necessity of targeted adaptation to bridge the gap between\ngeneral-purpose VLLMs and mission-critical driving applications. DVBench\nestablishes an essential evaluation framework and research roadmap for\ndeveloping VLLMs that meet the safety and robustness requirements for\nreal-world autonomous systems. We released the benchmark toolbox and the\nfine-tuned model at: https://github.com/tong-zeng/DVBench.git.", "AI": {"tldr": "DVBench\u662f\u4e00\u4e2a\u8bc4\u4f30\u89c6\u89c9\u5927\u8bed\u8a00\u6a21\u578b\uff08VLLMs\uff09\u5728\u5b89\u5168\u5173\u952e\u9a7e\u9a76\u573a\u666f\u4e2d\u6027\u80fd\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u5e76\u901a\u8fc7\u9886\u57df\u7279\u5b9a\u5fae\u8c03\u5c55\u793a\u4e86\u6539\u8fdb\u6f5c\u529b\u3002", "motivation": "\u73b0\u6709VLLMs\u5728\u901a\u7528\u89c6\u89c9\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u5b89\u5168\u5173\u952e\u9886\u57df\uff08\u5982\u81ea\u52a8\u9a7e\u9a76\uff09\u7684\u6027\u80fd\u5c1a\u672a\u5145\u5206\u63a2\u7d22\uff0c\u7f3a\u4e4f\u9488\u5bf9\u590d\u6742\u9a7e\u9a76\u573a\u666f\u7684\u8bc4\u4f30\u5de5\u5177\u3002", "method": "\u63d0\u51faDVBench\u57fa\u51c6\uff0c\u5305\u542b10,000\u4e2a\u591a\u9009\u9898\uff0c\u57fa\u4e8e\u5206\u5c42\u80fd\u529b\u5206\u7c7b\u6cd5\u8bc4\u4f30VLLMs\u7684\u611f\u77e5\u548c\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u5bf914\u4e2aSOTA\u6a21\u578b\u8fdb\u884c\u5b9e\u9a8c\u548c\u5fae\u8c03\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u73b0\u6709\u6a21\u578b\u5728\u590d\u6742\u9a7e\u9a76\u573a\u666f\u4e2d\u8868\u73b0\u4e0d\u4f73\uff08\u6700\u9ad8\u51c6\u786e\u7387<40%\uff09\uff0c\u4f46\u5fae\u8c03\u540e\u51c6\u786e\u7387\u63d0\u53475.24-10.94\u4e2a\u767e\u5206\u70b9\uff0c\u76f8\u5bf9\u6539\u8fdb\u8fbe43.59%\u3002", "conclusion": "DVBench\u4e3a\u5f00\u53d1\u6ee1\u8db3\u81ea\u52a8\u9a7e\u9a76\u5b89\u5168\u9700\u6c42\u7684VLLMs\u63d0\u4f9b\u4e86\u8bc4\u4f30\u6846\u67b6\u548c\u7814\u7a76\u65b9\u5411\uff0c\u5f3a\u8c03\u4e86\u9886\u57df\u9002\u5e94\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2504.15037", "pdf": "https://arxiv.org/pdf/2504.15037", "abs": "https://arxiv.org/abs/2504.15037", "authors": ["Huanyu Zhang", "Chengzu Li", "Wenshan Wu", "Shaoguang Mao", "Yan xia", "Ivan Vuli\u0107", "Zhang Zhang", "Liang Wang", "Tieniu Tan", "Furu Wei"], "title": "A Call for New Recipes to Enhance Spatial Reasoning in MLLMs", "categories": ["cs.LG"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have demonstrated impressive\nperformance in general vision-language tasks. However, recent studies have\nexposed critical limitations in their spatial reasoning capabilities. This\ndeficiency in spatial reasoning significantly constrains MLLMs' ability to\ninteract effectively with the physical world, thereby limiting their broader\napplications. We argue that spatial reasoning capabilities will not naturally\nemerge from merely scaling existing architectures and training methodologies.\nInstead, this challenge demands dedicated attention to fundamental\nmodifications in the current MLLM development approach. In this position paper,\nwe first establish a comprehensive framework for spatial reasoning within the\ncontext of MLLMs. We then elaborate on its pivotal role in real-world\napplications. Through systematic analysis, we examine how individual components\nof the current methodology-from training data to reasoning mechanisms-influence\nspatial reasoning capabilities. This examination reveals critical limitations\nwhile simultaneously identifying promising avenues for advancement. Our work\naims to direct the AI research community's attention toward these crucial yet\nunderexplored aspects. By highlighting these challenges and opportunities, we\nseek to catalyze progress toward achieving human-like spatial reasoning\ncapabilities in MLLMs.", "AI": {"tldr": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u4e0a\u5b58\u5728\u663e\u8457\u4e0d\u8db3\uff0c\u9650\u5236\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u3002\u672c\u6587\u63d0\u51fa\u9700\u5bf9\u73b0\u6709\u65b9\u6cd5\u8fdb\u884c\u6839\u672c\u6027\u6539\u8fdb\uff0c\u5e76\u5efa\u7acb\u4e86\u4e00\u4e2a\u7a7a\u95f4\u63a8\u7406\u6846\u67b6\uff0c\u5206\u6790\u4e86\u5f53\u524d\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u53ca\u6539\u8fdb\u65b9\u5411\u3002", "motivation": "MLLMs\u5728\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u4e0a\u7684\u4e0d\u8db3\u9650\u5236\u4e86\u5176\u4e0e\u7269\u7406\u4e16\u754c\u7684\u6709\u6548\u4ea4\u4e92\uff0c\u963b\u788d\u4e86\u66f4\u5e7f\u6cdb\u7684\u5e94\u7528\u3002", "method": "\u5efa\u7acb\u4e86\u4e00\u4e2a\u7a7a\u95f4\u63a8\u7406\u6846\u67b6\uff0c\u7cfb\u7edf\u5206\u6790\u4e86\u4ece\u8bad\u7ec3\u6570\u636e\u5230\u63a8\u7406\u673a\u5236\u7684\u5404\u4e2a\u7ec4\u6210\u90e8\u5206\u5bf9\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u7684\u5f71\u54cd\u3002", "result": "\u63ed\u793a\u4e86\u5f53\u524d\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u6307\u51fa\u4e86\u6539\u8fdb\u7684\u6f5c\u5728\u65b9\u5411\u3002", "conclusion": "\u672c\u6587\u547c\u5401AI\u7814\u7a76\u793e\u533a\u5173\u6ce8\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\uff0c\u63a8\u52a8MLLMs\u5b9e\u73b0\u7c7b\u4eba\u6c34\u5e73\u7684\u7a7a\u95f4\u63a8\u7406\u3002"}}
{"id": "2504.14594", "pdf": "https://arxiv.org/pdf/2504.14594", "abs": "https://arxiv.org/abs/2504.14594", "authors": ["Fan Gao", "Xinjie Zhao", "Ding Xia", "Zhongyi Zhou", "Rui Yang", "Jinghui Lu", "Hang Jiang", "Chanjun Park", "Irene Li"], "title": "HealthGenie: Empowering Users with Healthy Dietary Guidance through Knowledge Graph and Large Language Models", "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": null, "summary": "Seeking dietary guidance often requires navigating complex professional\nknowledge while accommodating individual health conditions. Knowledge Graphs\n(KGs) offer structured and interpretable nutritional information, whereas Large\nLanguage Models (LLMs) naturally facilitate conversational recommendation\ndelivery. In this paper, we present HealthGenie, an interactive system that\ncombines the strengths of LLMs and KGs to provide personalized dietary\nrecommendations along with hierarchical information visualization for a quick\nand intuitive overview. Upon receiving a user query, HealthGenie performs query\nrefinement and retrieves relevant information from a pre-built KG. The system\nthen visualizes and highlights pertinent information, organized by defined\ncategories, while offering detailed, explainable recommendation rationales.\nUsers can further tailor these recommendations by adjusting preferences\ninteractively. Our evaluation, comprising a within-subject comparative\nexperiment and an open-ended discussion, demonstrates that HealthGenie\neffectively supports users in obtaining personalized dietary guidance based on\ntheir health conditions while reducing interaction effort and cognitive load.\nThese findings highlight the potential of LLM-KG integration in supporting\ndecision-making through explainable and visualized information. We examine the\nsystem's usefulness and effectiveness with an N=12 within-subject study and\nprovide design considerations for future systems that integrate conversational\nLLM and KG.", "AI": {"tldr": "HealthGenie\u7ed3\u5408\u77e5\u8bc6\u56fe\u8c31\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u63d0\u4f9b\u4e2a\u6027\u5316\u996e\u98df\u5efa\u8bae\uff0c\u5e76\u901a\u8fc7\u53ef\u89c6\u5316\u51cf\u5c11\u7528\u6237\u8ba4\u77e5\u8d1f\u62c5\u3002", "motivation": "\u89e3\u51b3\u7528\u6237\u5728\u83b7\u53d6\u4e2a\u6027\u5316\u996e\u98df\u5efa\u8bae\u65f6\u9762\u4e34\u7684\u4e13\u4e1a\u77e5\u8bc6\u590d\u6742\u6027\u548c\u4e2a\u4f53\u5065\u5eb7\u6761\u4ef6\u591a\u6837\u6027\u95ee\u9898\u3002", "method": "HealthGenie\u901a\u8fc7\u67e5\u8be2\u4f18\u5316\u4ece\u77e5\u8bc6\u56fe\u8c31\u4e2d\u68c0\u7d22\u4fe1\u606f\uff0c\u7ed3\u5408LLM\u751f\u6210\u89e3\u91ca\u6027\u5efa\u8bae\uff0c\u5e76\u63d0\u4f9b\u4ea4\u4e92\u5f0f\u8c03\u6574\u529f\u80fd\u3002", "result": "\u7528\u6237\u7814\u7a76\u8868\u660e\uff0cHealthGenie\u80fd\u6709\u6548\u51cf\u5c11\u4ea4\u4e92\u52aa\u529b\u548c\u8ba4\u77e5\u8d1f\u62c5\uff0c\u63d0\u4f9b\u4e2a\u6027\u5316\u5efa\u8bae\u3002", "conclusion": "LLM\u4e0e\u77e5\u8bc6\u56fe\u8c31\u7684\u7ed3\u5408\u5728\u652f\u6301\u51b3\u7b56\u5236\u5b9a\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u672a\u6765\u7cfb\u7edf\u53ef\u53c2\u8003\u5176\u8bbe\u8ba1\u601d\u8def\u3002"}}
{"id": "2504.15051", "pdf": "https://arxiv.org/pdf/2504.15051", "abs": "https://arxiv.org/abs/2504.15051", "authors": ["Ashkan Shakarami", "Yousef Yeganeh", "Azade Farshad", "Lorenzo Nicol\u00e8", "Stefano Ghidoni", "Nassir Navab"], "title": "VeLU: Variance-enhanced Learning Unit for Deep Neural Networks", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Activation functions are fundamental in deep neural networks and directly\nimpact gradient flow, optimization stability, and generalization. Although ReLU\nremains standard because of its simplicity, it suffers from vanishing gradients\nand lacks adaptability. Alternatives like Swish and GELU introduce smooth\ntransitions, but fail to dynamically adjust to input statistics. We propose\nVeLU, a Variance-enhanced Learning Unit as an activation function that\ndynamically scales based on input variance by integrating ArcTan-Sin\ntransformations and Wasserstein-2 regularization, effectively mitigating\ncovariate shifts and stabilizing optimization. Extensive experiments on\nViT_B16, VGG19, ResNet50, DenseNet121, MobileNetV2, and EfficientNetB3 confirm\nVeLU's superiority over ReLU, ReLU6, Swish, and GELU on six vision benchmarks.\nThe codes of VeLU are publicly available on GitHub.", "AI": {"tldr": "VeLU\u662f\u4e00\u79cd\u57fa\u4e8e\u8f93\u5165\u65b9\u5dee\u52a8\u6001\u8c03\u6574\u7684\u6fc0\u6d3b\u51fd\u6570\uff0c\u901a\u8fc7ArcTan-Sin\u53d8\u6362\u548cWasserstein-2\u6b63\u5219\u5316\uff0c\u89e3\u51b3\u4e86ReLU\u53ca\u5176\u66ff\u4ee3\u54c1\u7684\u95ee\u9898\uff0c\u5e76\u5728\u591a\u4e2a\u89c6\u89c9\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "ReLU\u53ca\u5176\u66ff\u4ee3\u54c1\uff08\u5982Swish\u548cGELU\uff09\u5b58\u5728\u68af\u5ea6\u6d88\u5931\u548c\u7f3a\u4e4f\u52a8\u6001\u9002\u5e94\u6027\u7684\u95ee\u9898\uff0c\u5f71\u54cd\u4e86\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u4f18\u5316\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51faVeLU\uff0c\u7ed3\u5408ArcTan-Sin\u53d8\u6362\u548cWasserstein-2\u6b63\u5219\u5316\uff0c\u52a8\u6001\u8c03\u6574\u8f93\u5165\u65b9\u5dee\uff0c\u4ee5\u7f13\u89e3\u534f\u53d8\u91cf\u504f\u79fb\u548c\u7a33\u5b9a\u4f18\u5316\u3002", "result": "\u5728ViT_B16\u3001VGG19\u7b49\u6a21\u578b\u548c\u516d\u4e2a\u89c6\u89c9\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cVeLU\u8868\u73b0\u4f18\u4e8eReLU\u3001Swish\u548cGELU\u3002", "conclusion": "VeLU\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u8f93\u5165\u65b9\u5dee\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6fc0\u6d3b\u51fd\u6570\u7684\u6027\u80fd\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2504.14640", "pdf": "https://arxiv.org/pdf/2504.14640", "abs": "https://arxiv.org/abs/2504.14640", "authors": ["Yuheng Huang", "Lei Ma", "Keizaburo Nishikino", "Takumi Akazaki"], "title": "Risk Assessment Framework for Code LLMs via Leveraging Internal States", "categories": ["cs.SE", "cs.AI", "cs.CL"], "comment": "To appear in the 33rd ACM International Conference on the Foundations\n  of Software Engineering (FSE Companion'25 Industry Track), June 23-28, 2025,\n  Trondheim, Norway. This work was supported by Fujitsu Limited", "summary": "The pre-training paradigm plays a key role in the success of Large Language\nModels (LLMs), which have been recognized as one of the most significant\nadvancements of AI recently. Building on these breakthroughs, code LLMs with\nadvanced coding capabilities bring huge impacts on software engineering,\nshowing the tendency to become an essential part of developers' daily routines.\nHowever, the current code LLMs still face serious challenges related to\ntrustworthiness, as they can generate incorrect, insecure, or unreliable code.\nRecent exploratory studies find that it can be promising to detect such risky\noutputs by analyzing LLMs' internal states, akin to how the human brain\nunconsciously recognizes its own mistakes. Yet, most of these approaches are\nlimited to narrow sub-domains of LLM operations and fall short of achieving\nindustry-level scalability and practicability. To address these challenges, in\nthis paper, we propose PtTrust, a two-stage risk assessment framework for code\nLLM based on internal state pre-training, designed to integrate seamlessly with\nthe existing infrastructure of software companies. The core idea is that the\nrisk assessment framework could also undergo a pre-training process similar to\nLLMs. Specifically, PtTrust first performs unsupervised pre-training on\nlarge-scale unlabeled source code to learn general representations of LLM\nstates. Then, it uses a small, labeled dataset to train a risk predictor. We\ndemonstrate the effectiveness of PtTrust through fine-grained, code line-level\nrisk assessment and demonstrate that it generalizes across tasks and different\nprogramming languages. Further experiments also reveal that PtTrust provides\nhighly intuitive and interpretable features, fostering greater user trust. We\nbelieve PtTrust makes a promising step toward scalable and trustworthy\nassurance for code LLMs.", "AI": {"tldr": "PtTrust\u662f\u4e00\u4e2a\u57fa\u4e8e\u5185\u90e8\u72b6\u6001\u9884\u8bad\u7ec3\u7684\u4e24\u9636\u6bb5\u98ce\u9669\u8bc4\u4f30\u6846\u67b6\uff0c\u65e8\u5728\u63d0\u5347\u4ee3\u7801LLM\u7684\u53ef\u4fe1\u5ea6\uff0c\u901a\u8fc7\u65e0\u76d1\u7763\u9884\u8bad\u7ec3\u548c\u6709\u76d1\u7763\u9884\u6d4b\u5b9e\u73b0\u8de8\u4efb\u52a1\u548c\u8bed\u8a00\u7684\u901a\u7528\u6027\u3002", "motivation": "\u5f53\u524d\u4ee3\u7801LLM\u5728\u751f\u6210\u4ee3\u7801\u65f6\u5b58\u5728\u4e0d\u53ef\u4fe1\u95ee\u9898\uff08\u5982\u9519\u8bef\u3001\u4e0d\u5b89\u5168\u6216\u4e0d\u53ef\u9760\u4ee3\u7801\uff09\uff0c\u73b0\u6709\u65b9\u6cd5\u5c40\u9650\u4e8e\u5b50\u9886\u57df\u4e14\u7f3a\u4e4f\u884c\u4e1a\u7ea7\u6269\u5c55\u6027\u3002", "method": "PtTrust\u91c7\u7528\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a1) \u65e0\u76d1\u7763\u9884\u8bad\u7ec3\u5b66\u4e60LLM\u72b6\u6001\u7684\u901a\u7528\u8868\u793a\uff1b2) \u5c0f\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u8bad\u7ec3\u98ce\u9669\u9884\u6d4b\u5668\u3002", "result": "\u5b9e\u9a8c\u8868\u660ePtTrust\u80fd\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u4ee3\u7801\u884c\u7ea7\u98ce\u9669\u8bc4\u4f30\uff0c\u5e76\u8de8\u4efb\u52a1\u548c\u8bed\u8a00\u6cdb\u5316\uff0c\u540c\u65f6\u63d0\u4f9b\u76f4\u89c2\u53ef\u89e3\u91ca\u7684\u7279\u5f81\u3002", "conclusion": "PtTrust\u4e3a\u4ee3\u7801LLM\u7684\u53ef\u6269\u5c55\u53ef\u4fe1\u4fdd\u969c\u8fc8\u51fa\u4e86\u91cd\u8981\u4e00\u6b65\u3002"}}
{"id": "2504.15077", "pdf": "https://arxiv.org/pdf/2504.15077", "abs": "https://arxiv.org/abs/2504.15077", "authors": ["Simone Papicchio", "Simone Rossi", "Luca Cagliero", "Paolo Papotti"], "title": "Think2SQL: Reinforce LLM Reasoning Capabilities for Text2SQL", "categories": ["cs.LG", "cs.DB"], "comment": "15 pages", "summary": "Large Language Models (LLMs) have shown impressive capabilities in\ntransforming natural language questions about relational databases into SQL\nqueries. Despite recent improvements, small LLMs struggle to handle questions\ninvolving multiple tables and complex SQL patterns under a Zero-Shot Learning\n(ZSL) setting. Supervised Fine-Tuning (SFT) partially compensate the knowledge\ndeficits in pretrained models but falls short while dealing with queries\ninvolving multi-hop reasoning. To bridge this gap, different LLM training\nstrategies to reinforce reasoning capabilities have been proposed, ranging from\nleveraging a thinking process within ZSL, including reasoning traces in SFT, or\nadopt Reinforcement Learning (RL) strategies. However, the influence of\nreasoning on Text2SQL performance is still largely unexplored. This paper\ninvestigates to what extent LLM reasoning capabilities influence their Text2SQL\nperformance on four benchmark datasets. To this end, it considers the following\nLLM settings: (1) ZSL, including general-purpose reasoning or not; (2) SFT,\nwith and without task-specific reasoning traces; (3) RL, leveraging execution\naccuracy as primary reward function; (4) SFT+RL, i.e, a two-stage approach that\ncombines SFT and RL. The results show that general-purpose reasoning under ZSL\nproves to be ineffective in tackling complex Text2SQL cases. Small LLMs benefit\nfrom SFT with reasoning much more than larger ones, bridging the gap of their\n(weaker) model pretraining. RL is generally beneficial across all tested models\nand datasets, particularly when SQL queries involve multi-hop reasoning and\nmultiple tables. Small LLMs with SFT+RL excel on most complex datasets thanks\nto a strategic balance between generality of the reasoning process and\noptimization of the execution accuracy. Thanks to RL, the7B Qwen-Coder-2.5\nmodel performs on par with 100+ Billion ones on the Bird dataset.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u4e0d\u540cLLM\u8bad\u7ec3\u7b56\u7565\uff08ZSL\u3001SFT\u3001RL\u3001SFT+RL\uff09\u5bf9Text2SQL\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u5c0f\u6a21\u578b\u901a\u8fc7SFT+RL\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5c3d\u7ba1LLM\u5728Text2SQL\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5c0f\u6a21\u578b\u5728\u590d\u6742\u67e5\u8be2\u548c\u591a\u8868\u64cd\u4f5c\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u63a2\u7d22\u5982\u4f55\u901a\u8fc7\u63a8\u7406\u80fd\u529b\u63d0\u5347\u6027\u80fd\u3002", "method": "\u7814\u7a76\u4e86\u56db\u79cdLLM\u8bbe\u7f6e\uff1aZSL\uff08\u5e26\u6216\u4e0d\u5e26\u901a\u7528\u63a8\u7406\uff09\u3001SFT\uff08\u5e26\u6216\u4e0d\u5e26\u4efb\u52a1\u7279\u5b9a\u63a8\u7406\u75d5\u8ff9\uff09\u3001RL\uff08\u4ee5\u6267\u884c\u51c6\u786e\u7387\u4e3a\u5956\u52b1\uff09\u3001SFT+RL\uff08\u4e24\u9636\u6bb5\u65b9\u6cd5\uff09\u3002", "result": "\u901a\u7528\u63a8\u7406\u5728ZSL\u4e2d\u6548\u679c\u6709\u9650\uff1b\u5c0f\u6a21\u578b\u901a\u8fc7SFT+RL\u8868\u73b0\u4f18\u5f02\uff0c\u5c24\u5176\u5728\u590d\u6742\u4efb\u52a1\u4e2d\uff1bRL\u5bf9\u591a\u8868\u548c\u591a\u8df3\u63a8\u7406\u4efb\u52a1\u7279\u522b\u6709\u6548\u3002", "conclusion": "SFT+RL\u7b56\u7565\u80fd\u6709\u6548\u63d0\u5347\u5c0f\u6a21\u578b\u5728\u590d\u6742Text2SQL\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u4f7f\u5176\u63a5\u8fd1\u751a\u81f3\u8d85\u8d8a\u66f4\u5927\u6a21\u578b\u7684\u8868\u73b0\u3002"}}
{"id": "2504.13986", "pdf": "https://arxiv.org/pdf/2504.13986", "abs": "https://arxiv.org/abs/2504.13986", "authors": ["Paolo Liberatore"], "title": "On the redundancy of short and heterogeneous sequences of belief revisions", "categories": ["cs.CC", "cs.AI"], "comment": "arXiv admin note: substantial text overlap with arXiv:2402.15445,\n  arXiv:2305.09200", "summary": "Forgetting a specific belief revision episode may not erase information\nbecause the other revisions may provide the same information or allow to deduce\nit. Whether it does was proved coNP-hard for sequence of two arbitrary\nlexicographic revision or arbitrarily long lexicographic Horn revision. A\npolynomial algorithm is presented for the case of two Horn revision.\nHeterogeneous sequences of revisions were proved to belong in Delta2. Their\npreviously proved coNP-hardness is enhanced by a proof of NP-hardness.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u7279\u5b9a\u4fe1\u5ff5\u4fee\u6b63\u4e8b\u4ef6\u7684\u9057\u5fd8\u95ee\u9898\uff0c\u8bc1\u660e\u5176\u590d\u6742\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u591a\u9879\u5f0f\u7b97\u6cd5\u3002", "motivation": "\u63a2\u8ba8\u4fe1\u5ff5\u4fee\u6b63\u4e2d\u9057\u5fd8\u4fe1\u606f\u7684\u5f71\u54cd\u53ca\u5176\u8ba1\u7b97\u590d\u6742\u6027\u3002", "method": "\u5206\u6790\u4e86\u4e0d\u540c\u4fee\u6b63\u5e8f\u5217\u7684\u590d\u6742\u6027\uff0c\u5305\u62ec\u4efb\u610f\u8bcd\u5178\u5e8f\u4fee\u6b63\u548cHorn\u4fee\u6b63\uff0c\u5e76\u63d0\u51fa\u4e86\u591a\u9879\u5f0f\u7b97\u6cd5\u3002", "result": "\u8bc1\u660e\u4e86\u4e24\u7c7b\u4fee\u6b63\u5e8f\u5217\u7684\u590d\u6742\u6027\uff08coNP-hard\u548cDelta2\uff09\uff0c\u5e76\u63d0\u4f9b\u4e86\u591a\u9879\u5f0f\u7b97\u6cd5\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u4fe1\u5ff5\u4fee\u6b63\u9057\u5fd8\u95ee\u9898\u7684\u590d\u6742\u6027\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u7b97\u6cd5\u652f\u6301\u3002"}}
{"id": "2504.15090", "pdf": "https://arxiv.org/pdf/2504.15090", "abs": "https://arxiv.org/abs/2504.15090", "authors": ["Junxiang Gao", "Yixin Ran", "Jia Chen"], "title": "Federated Latent Factor Model for Bias-Aware Recommendation with Privacy-Preserving", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "A recommender system (RS) aims to provide users with personalized item\nrecommendations, enhancing their overall experience. Traditional RSs collect\nand process all user data on a central server. However, this centralized\napproach raises significant privacy concerns, as it increases the risk of data\nbreaches and privacy leakages, which are becoming increasingly unacceptable to\nprivacy-sensitive users. To address these privacy challenges, federated\nlearning has been integrated into RSs, ensuring that user data remains secure.\nIn centralized RSs, the issue of rating bias is effectively addressed by\njointly analyzing all users' raw interaction data. However, this becomes a\nsignificant challenge in federated RSs, as raw data is no longer accessible due\nto privacy-preserving constraints. To overcome this problem, we propose a\nFederated Bias-Aware Latent Factor (FBALF) model. In FBALF, training bias is\nexplicitly incorporated into every local model's loss function, allowing for\nthe effective elimination of rating bias without compromising data privacy.\nExtensive experiments conducted on three real-world datasets demonstrate that\nFBALF achieves significantly higher recommendation accuracy compared to other\nstate-of-the-art federated RSs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8054\u90a6\u504f\u7f6e\u611f\u77e5\u6f5c\u5728\u56e0\u5b50\uff08FBALF\uff09\u6a21\u578b\uff0c\u7528\u4e8e\u89e3\u51b3\u8054\u90a6\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684\u8bc4\u5206\u504f\u7f6e\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u62a4\u7528\u6237\u9690\u79c1\u3002", "motivation": "\u4f20\u7edf\u63a8\u8350\u7cfb\u7edf\u96c6\u4e2d\u5904\u7406\u7528\u6237\u6570\u636e\uff0c\u5b58\u5728\u9690\u79c1\u6cc4\u9732\u98ce\u9669\uff1b\u8054\u90a6\u5b66\u4e60\u867d\u4fdd\u62a4\u9690\u79c1\uff0c\u4f46\u65e0\u6cd5\u76f4\u63a5\u5904\u7406\u8bc4\u5206\u504f\u7f6e\u95ee\u9898\u3002", "method": "FBALF\u6a21\u578b\u5728\u672c\u5730\u6a21\u578b\u7684\u635f\u5931\u51fd\u6570\u4e2d\u663e\u5f0f\u5f15\u5165\u8bad\u7ec3\u504f\u7f6e\uff0c\u6d88\u9664\u8bc4\u5206\u504f\u7f6e\u800c\u4e0d\u6cc4\u9732\u6570\u636e\u3002", "result": "\u5728\u4e09\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cFBALF\u7684\u63a8\u8350\u51c6\u786e\u6027\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u8054\u90a6\u63a8\u8350\u7cfb\u7edf\u3002", "conclusion": "FBALF\u6709\u6548\u89e3\u51b3\u4e86\u8054\u90a6\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684\u8bc4\u5206\u504f\u7f6e\u95ee\u9898\uff0c\u540c\u65f6\u786e\u4fdd\u4e86\u6570\u636e\u9690\u79c1\u3002"}}
{"id": "2504.13987", "pdf": "https://arxiv.org/pdf/2504.13987", "abs": "https://arxiv.org/abs/2504.13987", "authors": ["Tariq Berrada Ifriqi", "Adriana Romero-Soriano", "Michal Drozdzal", "Jakob Verbeek", "Karteek Alahari"], "title": "Entropy Rectifying Guidance for Diffusion and Flow Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Guidance techniques are commonly used in diffusion and flow models to improve\nimage quality and consistency for conditional generative tasks such as\nclass-conditional and text-to-image generation. In particular, classifier-free\nguidance (CFG) -- the most widely adopted guidance technique -- contrasts\nconditional and unconditional predictions to improve the generated images. This\nresults, however, in trade-offs across quality, diversity and consistency,\nimproving some at the expense of others. While recent work has shown that it is\npossible to disentangle these factors to some extent, such methods come with an\noverhead of requiring an additional (weaker) model, or require more forward\npasses per sampling step. In this paper, we propose Entropy Rectifying Guidance\n(ERG), a simple and effective guidance mechanism based on inference-time\nchanges in the attention mechanism of state-of-the-art diffusion transformer\narchitectures, which allows for simultaneous improvements over image quality,\ndiversity and prompt consistency. ERG is more general than CFG and similar\nguidance techniques, as it extends to unconditional sampling. ERG results in\nsignificant improvements in various generation tasks such as text-to-image,\nclass-conditional and unconditional image generation. We also show that ERG can\nbe seamlessly combined with other recent guidance methods such as CADS and APG,\nfurther boosting generation performance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u71b5\u4fee\u6b63\u5f15\u5bfc\uff08ERG\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u8c03\u6574\u6269\u6563\u53d8\u6362\u5668\u67b6\u6784\u4e2d\u7684\u6ce8\u610f\u529b\u673a\u5236\uff0c\u540c\u65f6\u63d0\u5347\u56fe\u50cf\u8d28\u91cf\u3001\u591a\u6837\u6027\u548c\u63d0\u793a\u4e00\u81f4\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u5206\u7c7b\u5668\u81ea\u7531\u5f15\u5bfc\uff08CFG\uff09\u65b9\u6cd5\u5728\u56fe\u50cf\u751f\u6210\u4e2d\u5b58\u5728\u8d28\u91cf\u3001\u591a\u6837\u6027\u548c\u4e00\u81f4\u6027\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u4e14\u73b0\u6709\u6539\u8fdb\u65b9\u6cd5\u9700\u8981\u989d\u5916\u6a21\u578b\u6216\u8ba1\u7b97\u5f00\u9500\u3002", "method": "\u63d0\u51faERG\u65b9\u6cd5\uff0c\u5229\u7528\u63a8\u7406\u65f6\u6ce8\u610f\u529b\u673a\u5236\u7684\u53d8\u5316\uff0c\u65e0\u9700\u989d\u5916\u6a21\u578b\u6216\u8ba1\u7b97\u5f00\u9500\uff0c\u9002\u7528\u4e8e\u65e0\u6761\u4ef6\u91c7\u6837\u3002", "result": "ERG\u5728\u6587\u672c\u5230\u56fe\u50cf\u3001\u7c7b\u6761\u4ef6\u548c\u65e0\u6761\u4ef6\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u663e\u8457\u4f18\u4e8eCFG\uff0c\u5e76\u80fd\u4e0e\u5176\u4ed6\u5f15\u5bfc\u65b9\u6cd5\u7ed3\u5408\u8fdb\u4e00\u6b65\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "ERG\u662f\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u5f15\u5bfc\u673a\u5236\uff0c\u80fd\u591f\u5728\u4e0d\u589e\u52a0\u8ba1\u7b97\u8d1f\u62c5\u7684\u60c5\u51b5\u4e0b\u663e\u8457\u63d0\u5347\u751f\u6210\u4efb\u52a1\u7684\u8868\u73b0\u3002"}}
{"id": "2504.15099", "pdf": "https://arxiv.org/pdf/2504.15099", "abs": "https://arxiv.org/abs/2504.15099", "authors": ["Lin Wang", "Xiancheng Wang", "Rui Wang", "Zhibo Zhang", "Minghang Zhao"], "title": "Fast-Slow Co-advancing Optimizer: Toward Harmonious Adversarial Training of GAN", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Up to now, the training processes of typical Generative Adversarial Networks\n(GANs) are still particularly sensitive to data properties and hyperparameters,\nwhich may lead to severe oscillations, difficulties in convergence, or even\nfailures to converge, especially when the overall variances of the training\nsets are large. These phenomena are often attributed to the training\ncharacteristics of such networks. Aiming at the problem, this paper develops a\nnew intelligent optimizer, Fast-Slow Co-advancing Optimizer (FSCO), which\nemploys reinforcement learning in the training process of GANs to make training\neasier. Specifically, this paper allows the training step size to be controlled\nby an agent to improve training stability, and makes the training process more\nintelligent with variable learning rates, making GANs less sensitive to step\nsize. Experiments have been conducted on three benchmark datasets to verify the\neffectiveness of the developed FSCO.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFSCO\u7684\u65b0\u578b\u667a\u80fd\u4f18\u5316\u5668\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u63a7\u5236GANs\u7684\u8bad\u7ec3\u6b65\u957f\uff0c\u63d0\u5347\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002", "motivation": "\u4f20\u7edfGANs\u7684\u8bad\u7ec3\u8fc7\u7a0b\u5bf9\u6570\u636e\u548c\u8d85\u53c2\u6570\u654f\u611f\uff0c\u6613\u51fa\u73b0\u632f\u8361\u6216\u6536\u655b\u56f0\u96be\uff0cFSCO\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u63a7\u5236\u8bad\u7ec3\u6b65\u957f\uff0c\u5b9e\u73b0\u53ef\u53d8\u5b66\u4e60\u7387\uff0c\u4f7f\u8bad\u7ec3\u66f4\u667a\u80fd\u3002", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86FSCO\u7684\u6709\u6548\u6027\u3002", "conclusion": "FSCO\u80fd\u663e\u8457\u63d0\u5347GANs\u7684\u8bad\u7ec3\u7a33\u5b9a\u6027\uff0c\u964d\u4f4e\u5bf9\u6b65\u957f\u7684\u654f\u611f\u6027\u3002"}}
{"id": "2504.14822", "pdf": "https://arxiv.org/pdf/2504.14822", "abs": "https://arxiv.org/abs/2504.14822", "authors": ["Rui Qiu", "Shijie Chen", "Yu Su", "Po-Yin Yen", "Han-Wei Shen"], "title": "Completing A Systematic Review in Hours instead of Months with Interactive AI Agents", "categories": ["cs.HC", "cs.CL"], "comment": null, "summary": "Systematic reviews (SRs) are vital for evidence-based practice in high stakes\ndisciplines, such as healthcare, but are often impeded by intensive labors and\nlengthy processes that can take months to complete. Due to the high demand for\ndomain expertise, existing automatic summarization methods fail to accurately\nidentify relevant studies and generate high-quality summaries. To that end, we\nintroduce InsightAgent, a human-centered interactive AI agent powered by large\nlanguage models that revolutionize this workflow. InsightAgent partitions a\nlarge literature corpus based on semantics and employs a multi-agent design for\nmore focused processing of literature, leading to significant improvement in\nthe quality of generated SRs. InsightAgent also provides intuitive\nvisualizations of the corpus and agent trajectories, allowing users to\neffortlessly monitor the actions of the agent and provide real-time feedback\nbased on their expertise. Our user studies with 9 medical professionals\ndemonstrate that the visualization and interaction mechanisms can effectively\nimprove the quality of synthesized SRs by 27.2%, reaching 79.7% of\nhuman-written quality. At the same time, user satisfaction is improved by\n34.4%. With InsightAgent, it only takes a clinician about 1.5 hours, rather\nthan months, to complete a high-quality systematic review.", "AI": {"tldr": "InsightAgent\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u4eba\u673a\u4ea4\u4e92AI\u4ee3\u7406\uff0c\u663e\u8457\u63d0\u5347\u7cfb\u7edf\u7efc\u8ff0\uff08SRs\uff09\u7684\u8d28\u91cf\u548c\u6548\u7387\uff0c\u5c06\u5b8c\u6210\u65f6\u95f4\u4ece\u6570\u6708\u7f29\u77ed\u81f31.5\u5c0f\u65f6\u3002", "motivation": "\u7cfb\u7edf\u7efc\u8ff0\u5728\u533b\u7597\u7b49\u9ad8\u8981\u6c42\u9886\u57df\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u8017\u65f6\u4e14\u4f9d\u8d56\u4e13\u5bb6\uff0c\u73b0\u6709\u81ea\u52a8\u5316\u65b9\u6cd5\u65e0\u6cd5\u6ee1\u8db3\u9700\u6c42\u3002", "method": "InsightAgent\u901a\u8fc7\u8bed\u4e49\u5206\u5272\u6587\u732e\u5e93\uff0c\u91c7\u7528\u591a\u4ee3\u7406\u8bbe\u8ba1\u5904\u7406\u6587\u732e\uff0c\u5e76\u63d0\u4f9b\u53ef\u89c6\u5316\u5de5\u5177\u4f9b\u7528\u6237\u5b9e\u65f6\u53cd\u9988\u3002", "result": "\u7528\u6237\u7814\u7a76\u8868\u660e\uff0cInsightAgent\u5c06SRs\u8d28\u91cf\u63d0\u534727.2%\uff0c\u63a5\u8fd1\u4eba\u5de5\u6c34\u5e73\u768479.7%\uff0c\u7528\u6237\u6ee1\u610f\u5ea6\u63d0\u9ad834.4%\u3002", "conclusion": "InsightAgent\u663e\u8457\u4f18\u5316\u4e86\u7cfb\u7edf\u7efc\u8ff0\u7684\u6d41\u7a0b\uff0c\u63d0\u5347\u4e86\u8d28\u91cf\u548c\u6548\u7387\uff0c\u9002\u7528\u4e8e\u9ad8\u8981\u6c42\u9886\u57df\u3002"}}
{"id": "2504.15110", "pdf": "https://arxiv.org/pdf/2504.15110", "abs": "https://arxiv.org/abs/2504.15110", "authors": ["Anastasis Kratsios", "Takashi Furuya"], "title": "Kolmogorov-Arnold Networks: Approximation and Learning Guarantees for Functions and their Derivatives", "categories": ["cs.LG", "cs.NA", "cs.NE", "math.FA", "math.NA", "stat.ML"], "comment": null, "summary": "Inspired by the Kolmogorov-Arnold superposition theorem, Kolmogorov-Arnold\nNetworks (KANs) have recently emerged as an improved backbone for most deep\nlearning frameworks, promising more adaptivity than their multilayer perception\n(MLP) predecessor by allowing for trainable spline-based activation functions.\nIn this paper, we probe the theoretical foundations of the KAN architecture by\nshowing that it can optimally approximate any Besov function in\n$B^{s}_{p,q}(\\mathcal{X})$ on a bounded open, or even fractal, domain\n$\\mathcal{X}$ in $\\mathbb{R}^d$ at the optimal approximation rate with respect\nto any weaker Besov norm $B^{\\alpha}_{p,q}(\\mathcal{X})$; where $\\alpha < s$.\nWe complement our approximation guarantee with a dimension-free estimate on the\nsample complexity of a residual KAN model when learning a function of Besov\nregularity from $N$ i.i.d. noiseless samples. Our KAN architecture incorporates\ncontemporary deep learning wisdom by leveraging residual/skip connections\nbetween layers.", "AI": {"tldr": "KANs\u57fa\u4e8eKolmogorov-Arnold\u53e0\u52a0\u5b9a\u7406\uff0c\u901a\u8fc7\u53ef\u8bad\u7ec3\u7684\u6837\u6761\u6fc0\u6d3b\u51fd\u6570\u63d0\u5347\u6df1\u5ea6\u5b66\u4e60\u7684\u9002\u5e94\u6027\u3002\u672c\u6587\u8bc1\u660eKAN\u80fd\u5728\u6709\u754c\u6216\u5206\u5f62\u57df\u4e0a\u6700\u4f18\u903c\u8fd1Besov\u51fd\u6570\uff0c\u5e76\u63d0\u4f9b\u6837\u672c\u590d\u6742\u5ea6\u4f30\u8ba1\u3002", "motivation": "\u63a2\u7d22KAN\u7684\u7406\u8bba\u57fa\u7840\uff0c\u8bc1\u660e\u5176\u5728\u903c\u8fd1Besov\u51fd\u6570\u4e0a\u7684\u6700\u4f18\u6027\uff0c\u5e76\u8bc4\u4f30\u5176\u6837\u672c\u590d\u6742\u5ea6\u3002", "method": "\u5229\u7528\u6b8b\u5dee\u8fde\u63a5\u548c\u53ef\u8bad\u7ec3\u6837\u6761\u6fc0\u6d3b\u51fd\u6570\u6784\u5efaKAN\u67b6\u6784\uff0c\u5206\u6790\u5176\u5728Besov\u7a7a\u95f4\u4e2d\u7684\u903c\u8fd1\u80fd\u529b\u3002", "result": "KAN\u80fd\u6700\u4f18\u903c\u8fd1Besov\u51fd\u6570\uff0c\u4e14\u6837\u672c\u590d\u6742\u5ea6\u4e0e\u7ef4\u5ea6\u65e0\u5173\u3002", "conclusion": "KAN\u5728\u7406\u8bba\u548c\u5b9e\u8df5\u4e0a\u5747\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\uff0c\u4e3a\u6df1\u5ea6\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2504.15163", "pdf": "https://arxiv.org/pdf/2504.15163", "abs": "https://arxiv.org/abs/2504.15163", "authors": ["Altun Shukurlu"], "title": "Survey of Loss Augmented Knowledge Tracing", "categories": ["cs.LG"], "comment": "14 pages, no figures", "summary": "The training of artificial neural networks is heavily dependent on the\ncareful selection of an appropriate loss function. While commonly used loss\nfunctions, such as cross-entropy and mean squared error (MSE), generally\nsuffice for a broad range of tasks, challenges often emerge due to limitations\nin data quality or inefficiencies within the learning process. In such\ncircumstances, the integration of supplementary terms into the loss function\ncan serve to address these challenges, enhancing both model performance and\nrobustness. Two prominent techniques, loss regularization and contrastive\nlearning, have been identified as effective strategies for augmenting the\ncapacity of loss functions in artificial neural networks.\n  Knowledge tracing is a compelling area of research that leverages predictive\nartificial intelligence to facilitate the automation of personalized and\nefficient educational experiences for students. In this paper, we provide a\ncomprehensive review of the deep learning-based knowledge tracing (DKT)\nalgorithms trained using advanced loss functions and discuss their improvements\nover prior techniques. We discuss contrastive knowledge tracing algorithms,\nsuch as Bi-CLKT, CL4KT, SP-CLKT, CoSKT, and prediction-consistent DKT,\nproviding performance benchmarks and insights into real-world deployment\nchallenges. The survey concludes with future research directions, including\nhybrid loss strategies and context-aware modeling.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u635f\u5931\u51fd\u6570\u5728\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u5e76\u7efc\u8ff0\u4e86\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u77e5\u8bc6\u8ffd\u8e2a\u7b97\u6cd5\u53ca\u5176\u6539\u8fdb\u3002", "motivation": "\u89e3\u51b3\u6570\u636e\u8d28\u91cf\u4e0d\u8db3\u6216\u5b66\u4e60\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u6539\u8fdb\u635f\u5931\u51fd\u6570\u63d0\u5347\u6a21\u578b\u6027\u80fd\u548c\u9c81\u68d2\u6027\u3002", "method": "\u7efc\u8ff0\u4e86\u5bf9\u6bd4\u5b66\u4e60\u77e5\u8bc6\u8ffd\u8e2a\u7b97\u6cd5\uff08\u5982Bi-CLKT\u3001CL4KT\u7b49\uff09\u53ca\u5176\u6027\u80fd\u8868\u73b0\u3002", "result": "\u5c55\u793a\u4e86\u5148\u8fdb\u635f\u5931\u51fd\u6570\u5728\u77e5\u8bc6\u8ffd\u8e2a\u7b97\u6cd5\u4e2d\u7684\u6539\u8fdb\u6548\u679c\uff0c\u5e76\u8ba8\u8bba\u4e86\u5b9e\u9645\u90e8\u7f72\u6311\u6218\u3002", "conclusion": "\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\uff0c\u5982\u6df7\u5408\u635f\u5931\u7b56\u7565\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u5efa\u6a21\u3002"}}
{"id": "2504.13993", "pdf": "https://arxiv.org/pdf/2504.13993", "abs": "https://arxiv.org/abs/2504.13993", "authors": ["Ekta Gujral", "Apurva Sinha", "Lishi Ji", "Bijayani Sanghamitra Mishra"], "title": "CPR: Leveraging LLMs for Topic and Phrase Suggestion to Facilitate Comprehensive Product Reviews", "categories": ["cs.IR", "cs.AI", "cs.LG"], "comment": null, "summary": "Consumers often heavily rely on online product reviews, analyzing both\nquantitative ratings and textual descriptions to assess product quality.\nHowever, existing research hasn't adequately addressed how to systematically\nencourage the creation of comprehensive reviews that capture both customers\nsentiment and detailed product feature analysis. This paper presents CPR, a\nnovel methodology that leverages the power of Large Language Models (LLMs) and\nTopic Modeling to guide users in crafting insightful and well-rounded reviews.\nOur approach employs a three-stage process: first, we present users with\nproduct-specific terms for rating; second, we generate targeted phrase\nsuggestions based on these ratings; and third, we integrate user-written text\nthrough topic modeling, ensuring all key aspects are addressed. We evaluate CPR\nusing text-to-text LLMs, comparing its performance against real-world customer\nreviews from Walmart. Our results demonstrate that CPR effectively identifies\nrelevant product terms, even for new products lacking prior reviews, and\nprovides sentiment-aligned phrase suggestions, saving users time and enhancing\nreviews quality. Quantitative analysis reveals a 12.3% improvement in BLEU\nscore over baseline methods, further supported by manual evaluation of\ngenerated phrases. We conclude by discussing potential extensions and future\nresearch directions.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faCPR\u65b9\u6cd5\uff0c\u5229\u7528LLMs\u548c\u4e3b\u9898\u5efa\u6a21\u5f15\u5bfc\u7528\u6237\u64b0\u5199\u5168\u9762\u4ea7\u54c1\u8bc4\u8bba\uff0c\u663e\u8457\u63d0\u5347\u8bc4\u8bba\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u672a\u7cfb\u7edf\u89e3\u51b3\u5982\u4f55\u9f13\u52b1\u7528\u6237\u64b0\u5199\u5305\u542b\u60c5\u611f\u548c\u4ea7\u54c1\u7279\u5f81\u5206\u6790\u7684\u5168\u9762\u8bc4\u8bba\u3002", "method": "\u4e09\u9636\u6bb5\u65b9\u6cd5\uff1a\u63d0\u4f9b\u4ea7\u54c1\u7279\u5b9a\u8bc4\u5206\u672f\u8bed\u3001\u751f\u6210\u9488\u5bf9\u6027\u77ed\u8bed\u5efa\u8bae\u3001\u6574\u5408\u7528\u6237\u6587\u672c\u5e76\u901a\u8fc7\u4e3b\u9898\u5efa\u6a21\u786e\u4fdd\u8986\u76d6\u5173\u952e\u70b9\u3002", "result": "CPR\u6709\u6548\u8bc6\u522b\u76f8\u5173\u4ea7\u54c1\u672f\u8bed\u5e76\u63d0\u4f9b\u60c5\u611f\u4e00\u81f4\u7684\u77ed\u8bed\u5efa\u8bae\uff0cBLEU\u5206\u6570\u63d0\u534712.3%\u3002", "conclusion": "CPR\u65b9\u6cd5\u6210\u529f\u63d0\u5347\u8bc4\u8bba\u8d28\u91cf\uff0c\u672a\u6765\u53ef\u8fdb\u4e00\u6b65\u6269\u5c55\u7814\u7a76\u3002"}}
{"id": "2504.15171", "pdf": "https://arxiv.org/pdf/2504.15171", "abs": "https://arxiv.org/abs/2504.15171", "authors": ["Meng Cui", "Xianghu Yue", "Xinyuan Qian", "Jinzheng Zhao", "Haohe Liu", "Xubo Liu", "Daoliang Li", "Wenwu Wang"], "title": "Audio-Visual Class-Incremental Learning for Fish Feeding intensity Assessment in Aquaculture", "categories": ["cs.LG"], "comment": null, "summary": "Fish Feeding Intensity Assessment (FFIA) is crucial in industrial aquaculture\nmanagement. Recent multi-modal approaches have shown promise in improving FFIA\nrobustness and efficiency. However, these methods face significant challenges\nwhen adapting to new fish species or environments due to catastrophic\nforgetting and the lack of suitable datasets. To address these limitations, we\nfirst introduce AV-CIL-FFIA, a new dataset comprising 81,932 labelled\naudio-visual clips capturing feeding intensities across six different fish\nspecies in real aquaculture environments. Then, we pioneer audio-visual class\nincremental learning (CIL) for FFIA and demonstrate through benchmarking on\nAV-CIL-FFIA that it significantly outperforms single-modality methods. Existing\nCIL methods rely heavily on historical data. Exemplar-based approaches store\nraw samples, creating storage challenges, while exemplar-free methods avoid\ndata storage but struggle to distinguish subtle feeding intensity variations\nacross different fish species. To overcome these limitations, we introduce\nHAIL-FFIA, a novel audio-visual class-incremental learning framework that\nbridges this gap with a prototype-based approach that achieves exemplar-free\nefficiency while preserving essential knowledge through compact feature\nrepresentations. Specifically, HAIL-FFIA employs hierarchical representation\nlearning with a dual-path knowledge preservation mechanism that separates\ngeneral intensity knowledge from fish-specific characteristics. Additionally,\nit features a dynamic modality balancing system that adaptively adjusts the\nimportance of audio versus visual information based on feeding behaviour\nstages. Experimental results show that HAIL-FFIA is superior to SOTA methods on\nAV-CIL-FFIA, achieving higher accuracy with lower storage needs while\neffectively mitigating catastrophic forgetting in incremental fish species\nlearning.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u97f3\u9891-\u89c6\u89c9\u7c7b\u589e\u91cf\u5b66\u4e60\u6846\u67b6HAIL-FFIA\uff0c\u7528\u4e8e\u9c7c\u7c7b\u6444\u98df\u5f3a\u5ea6\u8bc4\u4f30\uff08FFIA\uff09\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u9002\u5e94\u65b0\u9c7c\u79cd\u6216\u73af\u5883\u65f6\u7684\u707e\u96be\u6027\u9057\u5fd8\u548c\u6570\u636e\u5b58\u50a8\u95ee\u9898\u3002", "motivation": "\u5de5\u4e1a\u6c34\u4ea7\u517b\u6b96\u7ba1\u7406\u4e2d\uff0c\u591a\u6a21\u6001\u65b9\u6cd5\u5728FFIA\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u9762\u4e34\u9002\u5e94\u65b0\u9c7c\u79cd\u6216\u73af\u5883\u65f6\u7684\u707e\u96be\u6027\u9057\u5fd8\u548c\u6570\u636e\u96c6\u4e0d\u8db3\u7684\u6311\u6218\u3002", "method": "\u5f15\u5165AV-CIL-FFIA\u6570\u636e\u96c6\uff0c\u5e76\u63d0\u51faHAIL-FFIA\u6846\u67b6\uff0c\u91c7\u7528\u539f\u578b\u5316\u65b9\u6cd5\u5b9e\u73b0\u65e0\u6837\u672c\u5b58\u50a8\u7684\u9ad8\u6548\u5b66\u4e60\uff0c\u540c\u65f6\u901a\u8fc7\u5206\u5c42\u8868\u793a\u5b66\u4e60\u548c\u52a8\u6001\u6a21\u6001\u5e73\u8861\u7cfb\u7edf\u4fdd\u7559\u77e5\u8bc6\u3002", "result": "HAIL-FFIA\u5728AV-CIL-FFIA\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u51c6\u786e\u6027\u66f4\u9ad8\u4e14\u5b58\u50a8\u9700\u6c42\u66f4\u4f4e\uff0c\u6709\u6548\u7f13\u89e3\u707e\u96be\u6027\u9057\u5fd8\u3002", "conclusion": "HAIL-FFIA\u4e3aFFIA\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u9002\u5e94\u6027\u5f3a\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u591a\u9c7c\u79cd\u548c\u73af\u5883\u4e0b\u7684\u589e\u91cf\u5b66\u4e60\u3002"}}
{"id": "2504.14011", "pdf": "https://arxiv.org/pdf/2504.14011", "abs": "https://arxiv.org/abs/2504.14011", "authors": ["Fulvio Sanguigni", "Davide Morelli", "Marcella Cornia", "Rita Cucchiara"], "title": "Fashion-RAG: Multimodal Fashion Image Editing via Retrieval-Augmented Generation", "categories": ["cs.CV", "cs.AI", "cs.MM"], "comment": "IJCNN 2025", "summary": "In recent years, the fashion industry has increasingly adopted AI\ntechnologies to enhance customer experience, driven by the proliferation of\ne-commerce platforms and virtual applications. Among the various tasks, virtual\ntry-on and multimodal fashion image editing -- which utilizes diverse input\nmodalities such as text, garment sketches, and body poses -- have become a key\narea of research. Diffusion models have emerged as a leading approach for such\ngenerative tasks, offering superior image quality and diversity. However, most\nexisting virtual try-on methods rely on having a specific garment input, which\nis often impractical in real-world scenarios where users may only provide\ntextual specifications. To address this limitation, in this work we introduce\nFashion Retrieval-Augmented Generation (Fashion-RAG), a novel method that\nenables the customization of fashion items based on user preferences provided\nin textual form. Our approach retrieves multiple garments that match the input\nspecifications and generates a personalized image by incorporating attributes\nfrom the retrieved items. To achieve this, we employ textual inversion\ntechniques, where retrieved garment images are projected into the textual\nembedding space of the Stable Diffusion text encoder, allowing seamless\nintegration of retrieved elements into the generative process. Experimental\nresults on the Dress Code dataset demonstrate that Fashion-RAG outperforms\nexisting methods both qualitatively and quantitatively, effectively capturing\nfine-grained visual details from retrieved garments. To the best of our\nknowledge, this is the first work to introduce a retrieval-augmented generation\napproach specifically tailored for multimodal fashion image editing.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faFashion-RAG\u65b9\u6cd5\uff0c\u901a\u8fc7\u6587\u672c\u8f93\u5165\u68c0\u7d22\u5339\u914d\u7684\u670d\u88c5\u5e76\u751f\u6210\u4e2a\u6027\u5316\u56fe\u50cf\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u865a\u62df\u8bd5\u8863\u65b9\u6cd5\u4f9d\u8d56\u7279\u5b9a\u670d\u88c5\u8f93\u5165\u7684\u9650\u5236\u3002", "motivation": "\u73b0\u6709\u865a\u62df\u8bd5\u8863\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u5177\u4f53\u670d\u88c5\u8f93\u5165\uff0c\u800c\u7528\u6237\u53ef\u80fd\u4ec5\u63d0\u4f9b\u6587\u672c\u63cf\u8ff0\uff0c\u9650\u5236\u4e86\u5b9e\u7528\u6027\u3002", "method": "\u91c7\u7528\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08Fashion-RAG\uff09\uff0c\u7ed3\u5408\u6587\u672c\u53cd\u8f6c\u6280\u672f\u5c06\u68c0\u7d22\u5230\u7684\u670d\u88c5\u56fe\u50cf\u6295\u5f71\u5230Stable Diffusion\u7684\u6587\u672c\u5d4c\u5165\u7a7a\u95f4\u3002", "result": "\u5728Dress Code\u6570\u636e\u96c6\u4e0a\uff0cFashion-RAG\u5728\u8d28\u91cf\u548c\u6570\u91cf\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u80fd\u6355\u6349\u7ec6\u7c92\u5ea6\u89c6\u89c9\u7ec6\u8282\u3002", "conclusion": "Fashion-RAG\u662f\u9996\u4e2a\u9488\u5bf9\u591a\u6a21\u6001\u65f6\u5c1a\u56fe\u50cf\u7f16\u8f91\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u65b9\u6cd5\uff0c\u6709\u6548\u7ed3\u5408\u6587\u672c\u8f93\u5165\u4e0e\u68c0\u7d22\u6280\u672f\u3002"}}
{"id": "2504.14904", "pdf": "https://arxiv.org/pdf/2504.14904", "abs": "https://arxiv.org/abs/2504.14904", "authors": ["Xingyu Lu", "Tianke Zhang", "Chang Meng", "Xiaobei Wang", "Jinpeng Wang", "YiFan Zhang", "Shisong Tang", "Changyi Liu", "Haojie Ding", "Kaiyu Jiang", "Kaiyu Tang", "Bin Wen", "Hai-Tao Zheng", "Fan Yang", "Tingting Gao", "Di Zhang", "Kun Gai"], "title": "VLM as Policy: Common-Law Content Moderation Framework for Short Video Platform", "categories": ["cs.SI", "cs.AI", "cs.CL", "cs.MM"], "comment": "20 pages, 6 figures", "summary": "Exponentially growing short video platforms (SVPs) face significant\nchallenges in moderating content detrimental to users' mental health,\nparticularly for minors. The dissemination of such content on SVPs can lead to\ncatastrophic societal consequences. Although substantial efforts have been\ndedicated to moderating such content, existing methods suffer from critical\nlimitations: (1) Manual review is prone to human bias and incurs high\noperational costs. (2) Automated methods, though efficient, lack nuanced\ncontent understanding, resulting in lower accuracy. (3) Industrial moderation\nregulations struggle to adapt to rapidly evolving trends due to long update\ncycles. In this paper, we annotate the first SVP content moderation benchmark\nwith authentic user/reviewer feedback to fill the absence of benchmark in this\nfield. Then we evaluate various methods on the benchmark to verify the\nexistence of the aforementioned limitations. We further propose our common-law\ncontent moderation framework named KuaiMod to address these challenges. KuaiMod\nconsists of three components: training data construction, offline adaptation,\nand online deployment & refinement. Leveraging large vision language model\n(VLM) and Chain-of-Thought (CoT) reasoning, KuaiMod adequately models video\ntoxicity based on sparse user feedback and fosters dynamic moderation policy\nwith rapid update speed and high accuracy. Offline experiments and large-scale\nonline A/B test demonstrates the superiority of KuaiMod: KuaiMod achieves the\nbest moderation performance on our benchmark. The deployment of KuaiMod reduces\nthe user reporting rate by 20% and its application in video recommendation\nincreases both Daily Active User (DAU) and APP Usage Time (AUT) on several\nKuaishou scenarios. We have open-sourced our benchmark at\nhttps://kuaimod.github.io.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faKuaiMod\u6846\u67b6\uff0c\u89e3\u51b3\u77ed\u89c6\u9891\u5e73\u53f0\u5185\u5bb9\u5ba1\u6838\u7684\u5c40\u9650\u6027\uff0c\u7ed3\u5408VLM\u548cCoT\u63a8\u7406\uff0c\u63d0\u5347\u5ba1\u6838\u51c6\u786e\u6027\u548c\u52a8\u6001\u66f4\u65b0\u80fd\u529b\u3002", "motivation": "\u77ed\u89c6\u9891\u5e73\u53f0\u5185\u5bb9\u5ba1\u6838\u5b58\u5728\u4eba\u5de5\u504f\u89c1\u3001\u81ea\u52a8\u5316\u65b9\u6cd5\u51c6\u786e\u6027\u4e0d\u8db3\u53ca\u6cd5\u89c4\u66f4\u65b0\u6ede\u540e\u7b49\u95ee\u9898\uff0c\u4e9f\u9700\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u6784\u5efa\u9996\u4e2aSVP\u5185\u5bb9\u5ba1\u6838\u57fa\u51c6\uff0c\u63d0\u51faKuaiMod\u6846\u67b6\uff0c\u5305\u542b\u6570\u636e\u6784\u5efa\u3001\u79bb\u7ebf\u9002\u914d\u548c\u5728\u7ebf\u90e8\u7f72\u4e09\u90e8\u5206\uff0c\u5229\u7528VLM\u548cCoT\u63a8\u7406\u3002", "result": "KuaiMod\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u7528\u6237\u4e3e\u62a5\u7387\u964d\u4f4e20%\uff0cDAU\u548cAUT\u663e\u8457\u63d0\u5347\u3002", "conclusion": "KuaiMod\u6709\u6548\u89e3\u51b3\u4e86\u5185\u5bb9\u5ba1\u6838\u7684\u6311\u6218\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\uff0c\u5e76\u5f00\u6e90\u4e86\u57fa\u51c6\u6570\u636e\u96c6\u3002"}}
{"id": "2504.15206", "pdf": "https://arxiv.org/pdf/2504.15206", "abs": "https://arxiv.org/abs/2504.15206", "authors": ["S\u00edlvia Casacuberta", "Parikshit Gopalan", "Varun Kanade", "Omer Reingold"], "title": "How Global Calibration Strengthens Multiaccuracy", "categories": ["cs.LG", "cs.CC"], "comment": null, "summary": "Multiaccuracy and multicalibration are multigroup fairness notions for\nprediction that have found numerous applications in learning and computational\ncomplexity. They can be achieved from a single learning primitive: weak\nagnostic learning. Here we investigate the power of multiaccuracy as a learning\nprimitive, both with and without the additional assumption of calibration. We\nfind that multiaccuracy in itself is rather weak, but that the addition of\nglobal calibration (this notion is called calibrated multiaccuracy) boosts its\npower substantially, enough to recover implications that were previously known\nonly assuming the stronger notion of multicalibration.\n  We give evidence that multiaccuracy might not be as powerful as standard weak\nagnostic learning, by showing that there is no way to post-process a\nmultiaccurate predictor to get a weak learner, even assuming the best\nhypothesis has correlation $1/2$. Rather, we show that it yields a restricted\nform of weak agnostic learning, which requires some concept in the class to\nhave correlation greater than $1/2$ with the labels. However, by also requiring\nthe predictor to be calibrated, we recover not just weak, but strong agnostic\nlearning.\n  A similar picture emerges when we consider the derivation of hardcore\nmeasures from predictors satisfying multigroup fairness notions. On the one\nhand, while multiaccuracy only yields hardcore measures of density half the\noptimal, we show that (a weighted version of) calibrated multiaccuracy achieves\noptimal density.\n  Our results yield new insights into the complementary roles played by\nmultiaccuracy and calibration in each setting. They shed light on why\nmultiaccuracy and global calibration, although not particularly powerful by\nthemselves, together yield considerably stronger notions.", "AI": {"tldr": "\u591a\u51c6\u786e\u6027\u548c\u591a\u6821\u51c6\u662f\u591a\u7ec4\u516c\u5e73\u6027\u6982\u5ff5\uff0c\u901a\u8fc7\u5f31\u4e0d\u53ef\u77e5\u5b66\u4e60\u5b9e\u73b0\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u591a\u51c6\u786e\u6027\u5355\u72ec\u8f83\u5f31\uff0c\u4f46\u7ed3\u5408\u5168\u5c40\u6821\u51c6\uff08\u6821\u51c6\u591a\u51c6\u786e\u6027\uff09\u80fd\u663e\u8457\u63d0\u5347\u5176\u80fd\u529b\uff0c\u751a\u81f3\u6062\u590d\u591a\u6821\u51c6\u7684\u5f3a\u5b66\u4e60\u6548\u679c\u3002", "motivation": "\u7814\u7a76\u591a\u51c6\u786e\u6027\u548c\u591a\u6821\u51c6\u4f5c\u4e3a\u5b66\u4e60\u539f\u8bed\u7684\u80fd\u529b\uff0c\u63a2\u7d22\u5b83\u4eec\u5728\u5f31\u4e0d\u53ef\u77e5\u5b66\u4e60\u548c\u786c\u6838\u5ea6\u91cf\u751f\u6210\u4e2d\u7684\u4f5c\u7528\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\uff0c\u6bd4\u8f83\u591a\u51c6\u786e\u6027\u3001\u6821\u51c6\u591a\u51c6\u786e\u6027\u548c\u591a\u6821\u51c6\u5728\u4e0d\u540c\u5b66\u4e60\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u591a\u51c6\u786e\u6027\u5355\u72ec\u8f83\u5f31\uff0c\u4f46\u7ed3\u5408\u5168\u5c40\u6821\u51c6\u540e\u80fd\u5b9e\u73b0\u5f3a\u4e0d\u53ef\u77e5\u5b66\u4e60\uff1b\u6821\u51c6\u591a\u51c6\u786e\u6027\u5728\u786c\u6838\u5ea6\u91cf\u751f\u6210\u4e2d\u8fbe\u5230\u6700\u4f18\u5bc6\u5ea6\u3002", "conclusion": "\u591a\u51c6\u786e\u6027\u548c\u5168\u5c40\u6821\u51c6\u4e92\u8865\uff0c\u7ed3\u5408\u540e\u80fd\u663e\u8457\u63d0\u5347\u5b66\u4e60\u80fd\u529b\uff0c\u63ed\u793a\u4e86\u5b83\u4eec\u5728\u591a\u7ec4\u516c\u5e73\u6027\u4e2d\u7684\u534f\u540c\u4f5c\u7528\u3002"}}
{"id": "2504.14015", "pdf": "https://arxiv.org/pdf/2504.14015", "abs": "https://arxiv.org/abs/2504.14015", "authors": ["Dominik Dold", "Philipp Christian Petersen"], "title": "Causal pieces: analysing and improving spiking neural networks piece by piece", "categories": ["cs.NE", "cs.AI", "cs.LG", "q-bio.NC", "stat.ML"], "comment": null, "summary": "We introduce a novel concept for spiking neural networks (SNNs) derived from\nthe idea of \"linear pieces\" used to analyse the expressiveness and trainability\nof artificial neural networks (ANNs). We prove that the input domain of SNNs\ndecomposes into distinct causal regions where its output spike times are\nlocally Lipschitz continuous with respect to the input spike times and network\nparameters. The number of such regions - which we call \"causal pieces\" - is a\nmeasure of the approximation capabilities of SNNs. In particular, we\ndemonstrate in simulation that parameter initialisations which yield a high\nnumber of causal pieces on the training set strongly correlate with SNN\ntraining success. Moreover, we find that feedforward SNNs with purely positive\nweights exhibit a surprisingly high number of causal pieces, allowing them to\nachieve competitive performance levels on benchmark tasks. We believe that\ncausal pieces are not only a powerful and principled tool for improving SNNs,\nbut might also open up new ways of comparing SNNs and ANNs in the future.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u201c\u7ebf\u6027\u7247\u6bb5\u201d\u6982\u5ff5\u7684\u65b0\u578b\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\uff08SNN\uff09\u5206\u6790\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86SNN\u8f93\u5165\u57df\u53ef\u5206\u89e3\u4e3a\u56e0\u679c\u533a\u57df\uff0c\u5176\u8f93\u51fa\u8109\u51b2\u65f6\u95f4\u5728\u5c40\u90e8Lipschitz\u8fde\u7eed\u3002\u56e0\u679c\u533a\u57df\u6570\u91cf\uff08\u79f0\u4e3a\u201c\u56e0\u679c\u7247\u6bb5\u201d\uff09\u662f\u8861\u91cfSNN\u8fd1\u4f3c\u80fd\u529b\u7684\u6307\u6807\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u56e0\u679c\u7247\u6bb5\u6570\u91cf\u591a\u7684\u521d\u59cb\u5316\u4e0e\u8bad\u7ec3\u6210\u529f\u5f3a\u76f8\u5173\uff0c\u4e14\u7eaf\u6b63\u6743\u91cd\u7684SNN\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u63a2\u7d22SNN\u7684\u8868\u8fbe\u80fd\u529b\u548c\u53ef\u8bad\u7ec3\u6027\uff0c\u63d0\u51fa\u4e00\u79cd\u4e0e\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\uff08ANN\uff09\u53ef\u6bd4\u7684\u65b0\u5206\u6790\u5de5\u5177\u3002", "method": "\u901a\u8fc7\u201c\u56e0\u679c\u7247\u6bb5\u201d\u6982\u5ff5\u5206\u6790SNN\u8f93\u5165\u57df\u7684\u5206\u89e3\uff0c\u8bc1\u660e\u8f93\u51fa\u8109\u51b2\u65f6\u95f4\u7684\u5c40\u90e8Lipschitz\u8fde\u7eed\u6027\uff0c\u5e76\u901a\u8fc7\u4eff\u771f\u9a8c\u8bc1\u56e0\u679c\u7247\u6bb5\u6570\u91cf\u4e0e\u8bad\u7ec3\u6210\u529f\u7684\u76f8\u5173\u6027\u3002", "result": "\u56e0\u679c\u7247\u6bb5\u6570\u91cf\u591a\u7684\u521d\u59cb\u5316\u4e0eSNN\u8bad\u7ec3\u6210\u529f\u5f3a\u76f8\u5173\uff1b\u7eaf\u6b63\u6743\u91cd\u7684SNN\u5728\u57fa\u51c6\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u56e0\u679c\u7247\u6bb5\u662f\u6539\u8fdbSNN\u7684\u5f3a\u5927\u5de5\u5177\uff0c\u5e76\u53ef\u80fd\u4e3a\u672a\u6765\u6bd4\u8f83SNN\u4e0eANN\u63d0\u4f9b\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2504.15208", "pdf": "https://arxiv.org/pdf/2504.15208", "abs": "https://arxiv.org/abs/2504.15208", "authors": ["Marc Finzi", "Sanyam Kapoor", "Diego Granziol", "Anming Gu", "Christopher De Sa", "J. Zico Kolter", "Andrew Gordon Wilson"], "title": "Compute-Optimal LLMs Provably Generalize Better With Scale", "categories": ["cs.LG", "cs.AI"], "comment": "ICLR 2025", "summary": "Why do larger language models generalize better? To investigate this\nquestion, we develop generalization bounds on the pretraining objective of\nlarge language models (LLMs) in the compute-optimal regime, as described by the\nChinchilla scaling laws. We introduce a novel, fully empirical Freedman-type\nmartingale concentration inequality that tightens existing bounds by accounting\nfor the variance of the loss function. This generalization bound can be\ndecomposed into three interpretable components: the number of parameters per\ntoken, the loss variance, and the quantization error at a fixed bitrate. As\ncompute-optimal language models are scaled up, the number of parameters per\ndata point remains constant; however, both the loss variance and the\nquantization error decrease, implying that larger models should have smaller\ngeneralization gaps. We examine why larger models tend to be more quantizable\nfrom an information theoretic perspective, showing that the rate at which they\ncan integrate new information grows more slowly than their capacity on the\ncompute-optimal frontier. From these findings we produce a scaling law for the\ngeneralization gap, with bounds that become predictably stronger with scale.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u4e3a\u4ec0\u4e48\u66f4\u5927\u7684\u8bed\u8a00\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u66f4\u5f3a\uff0c\u901a\u8fc7\u8ba1\u7b97\u6700\u4f18\u6761\u4ef6\u4e0b\u7684\u6cdb\u5316\u8fb9\u754c\uff0c\u53d1\u73b0\u6a21\u578b\u89c4\u6a21\u589e\u5927\u65f6\u6cdb\u5316\u5dee\u8ddd\u51cf\u5c0f\u3002", "motivation": "\u63a2\u7a76\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u8ba1\u7b97\u6700\u4f18\u6761\u4ef6\u4e0b\u6cdb\u5316\u80fd\u529b\u63d0\u5347\u7684\u539f\u56e0\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684Freedman\u578b\u9785\u96c6\u4e2d\u4e0d\u7b49\u5f0f\uff0c\u7ed3\u5408\u635f\u5931\u51fd\u6570\u65b9\u5dee\uff0c\u5206\u89e3\u6cdb\u5316\u8fb9\u754c\u4e3a\u4e09\u4e2a\u53ef\u89e3\u91ca\u90e8\u5206\uff1a\u6bcf\u4ee4\u724c\u53c2\u6570\u6570\u3001\u635f\u5931\u65b9\u5dee\u548c\u56fa\u5b9a\u6bd4\u7279\u7387\u4e0b\u7684\u91cf\u5316\u8bef\u5dee\u3002", "result": "\u53d1\u73b0\u968f\u7740\u6a21\u578b\u89c4\u6a21\u589e\u5927\uff0c\u635f\u5931\u65b9\u5dee\u548c\u91cf\u5316\u8bef\u5dee\u51cf\u5c0f\uff0c\u6cdb\u5316\u5dee\u8ddd\u968f\u4e4b\u7f29\u5c0f\u3002", "conclusion": "\u66f4\u5927\u7684\u6a21\u578b\u5728\u8ba1\u7b97\u6700\u4f18\u6761\u4ef6\u4e0b\u6cdb\u5316\u80fd\u529b\u66f4\u5f3a\uff0c\u4e14\u6cdb\u5316\u5dee\u8ddd\u968f\u89c4\u6a21\u589e\u5927\u800c\u51cf\u5c0f\u3002"}}
{"id": "2504.14032", "pdf": "https://arxiv.org/pdf/2504.14032", "abs": "https://arxiv.org/abs/2504.14032", "authors": ["Haiwen Huang", "Anpei Chen", "Volodymyr Havrylov", "Andreas Geiger", "Dan Zhang"], "title": "LoftUp: Learning a Coordinate-Based Feature Upsampler for Vision Foundation Models", "categories": ["cs.CV", "cs.AI", "cs.LG", "eess.IV"], "comment": null, "summary": "Vision foundation models (VFMs) such as DINOv2 and CLIP have achieved\nimpressive results on various downstream tasks, but their limited feature\nresolution hampers performance in applications requiring pixel-level\nunderstanding. Feature upsampling offers a promising direction to address this\nchallenge. In this work, we identify two critical factors for enhancing feature\nupsampling: the upsampler architecture and the training objective. For the\nupsampler architecture, we introduce a coordinate-based cross-attention\ntransformer that integrates the high-resolution images with coordinates and\nlow-resolution VFM features to generate sharp, high-quality features. For the\ntraining objective, we propose constructing high-resolution pseudo-groundtruth\nfeatures by leveraging class-agnostic masks and self-distillation. Our approach\neffectively captures fine-grained details and adapts flexibly to various input\nand feature resolutions. Through experiments, we demonstrate that our approach\nsignificantly outperforms existing feature upsampling techniques across various\ndownstream tasks. Our code is released at https://github.com/andrehuang/loftup.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff08VFMs\uff09\u7279\u5f81\u4e0a\u91c7\u6837\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bbe\u8ba1\u5750\u6807\u4ea4\u53c9\u6ce8\u610f\u529b\u53d8\u6362\u5668\u67b6\u6784\u548c\u5229\u7528\u81ea\u84b8\u998f\u8bad\u7ec3\u76ee\u6807\uff0c\u663e\u8457\u63d0\u5347\u4e86\u50cf\u7d20\u7ea7\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u73b0\u6709VFMs\uff08\u5982DINOv2\u548cCLIP\uff09\u5728\u50cf\u7d20\u7ea7\u4efb\u52a1\u4e2d\u56e0\u7279\u5f81\u5206\u8fa8\u7387\u4e0d\u8db3\u800c\u8868\u73b0\u53d7\u9650\uff0c\u7279\u5f81\u4e0a\u91c7\u6837\u662f\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u7684\u5173\u952e\u3002", "method": "\u63d0\u51fa\u5750\u6807\u4ea4\u53c9\u6ce8\u610f\u529b\u53d8\u6362\u5668\u67b6\u6784\uff0c\u6574\u5408\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u3001\u5750\u6807\u548c\u4f4e\u5206\u8fa8\u7387VFM\u7279\u5f81\uff1b\u5229\u7528\u7c7b\u65e0\u5173\u63a9\u7801\u548c\u81ea\u84b8\u998f\u6784\u5efa\u9ad8\u5206\u8fa8\u7387\u4f2a\u771f\u5b9e\u7279\u5f81\u4f5c\u4e3a\u8bad\u7ec3\u76ee\u6807\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u4e0b\u6e38\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7279\u5f81\u4e0a\u91c7\u6837\u6280\u672f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u6355\u6349\u7ec6\u7c92\u5ea6\u7ec6\u8282\u5e76\u9002\u5e94\u591a\u79cd\u8f93\u5165\u548c\u7279\u5f81\u5206\u8fa8\u7387\uff0c\u4e3a\u50cf\u7d20\u7ea7\u4efb\u52a1\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.15209", "pdf": "https://arxiv.org/pdf/2504.15209", "abs": "https://arxiv.org/abs/2504.15209", "authors": ["Xin Liao", "Bing Yang", "Tan Dongli", "Cai Yu"], "title": "A Causal Convolutional Low-rank Representation Model for Imputation of Water Quality Data", "categories": ["cs.LG", "cs.AI", "68T07 (Primary) 62M10, 65C60 (Secondary)", "I.2.7"], "comment": "9 pages, 3 figures", "summary": "The monitoring of water quality is a crucial part of environmental\nprotection, and a large number of monitors are widely deployed to monitor water\nquality. Due to unavoidable factors such as data acquisition breakdowns,\nsensors and communication failures, water quality monitoring data suffers from\nmissing values over time, resulting in High-Dimensional and Sparse (HDS) Water\nQuality Data (WQD). The simple and rough filling of the missing values leads to\ninaccurate results and affects the implementation of relevant measures.\nTherefore, this paper proposes a Causal convolutional Low-rank Representation\n(CLR) model for imputing missing WQD to improve the completeness of the WQD,\nwhich employs a two-fold idea: a) applying causal convolutional operation to\nconsider the temporal dependence of the low-rank representation, thus\nincorporating temporal information to improve the imputation accuracy; and b)\nimplementing a hyperparameters adaptation scheme to automatically adjust the\nbest hyperparameters during model training, thereby reducing the tedious manual\nadjustment of hyper-parameters. Experimental studies on three real-world water\nquality datasets demonstrate that the proposed CLR model is superior to some of\nthe existing state-of-the-art imputation models in terms of imputation accuracy\nand time cost, as well as indicating that the proposed model provides more\nreliable decision support for environmental monitoring.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u56e0\u679c\u5377\u79ef\u4f4e\u79e9\u8868\u793a\uff08CLR\uff09\u6a21\u578b\uff0c\u7528\u4e8e\u586b\u8865\u6c34\u8d28\u76d1\u6d4b\u6570\u636e\u4e2d\u7684\u7f3a\u5931\u503c\uff0c\u4ee5\u63d0\u9ad8\u6570\u636e\u7684\u5b8c\u6574\u6027\u3002\u8be5\u6a21\u578b\u7ed3\u5408\u4e86\u65f6\u95f4\u4f9d\u8d56\u6027\u548c\u81ea\u52a8\u8d85\u53c2\u6570\u8c03\u6574\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u6c34\u8d28\u76d1\u6d4b\u6570\u636e\u5e38\u56e0\u8bbe\u5907\u6545\u969c\u7b49\u539f\u56e0\u5b58\u5728\u7f3a\u5931\u503c\uff0c\u4f20\u7edf\u586b\u8865\u65b9\u6cd5\u6548\u679c\u4e0d\u4f73\uff0c\u5f71\u54cd\u51b3\u7b56\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u51c6\u786e\u7684\u65b9\u6cd5\u6765\u586b\u8865\u7f3a\u5931\u503c\u3002", "method": "\u91c7\u7528\u56e0\u679c\u5377\u79ef\u64cd\u4f5c\u8003\u8651\u4f4e\u79e9\u8868\u793a\u7684\u65f6\u95f4\u4f9d\u8d56\u6027\uff0c\u5e76\u7ed3\u5408\u81ea\u52a8\u8d85\u53c2\u6570\u8c03\u6574\u65b9\u6848\uff0c\u4f18\u5316\u6a21\u578b\u8bad\u7ec3\u3002", "result": "\u5728\u4e09\u4e2a\u771f\u5b9e\u6c34\u8d28\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCLR\u6a21\u578b\u5728\u586b\u8865\u51c6\u786e\u6027\u548c\u65f6\u95f4\u6210\u672c\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "CLR\u6a21\u578b\u4e3a\u73af\u5883\u76d1\u6d4b\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u51b3\u7b56\u652f\u6301\uff0c\u586b\u8865\u6548\u679c\u663e\u8457\u3002"}}
{"id": "2504.14038", "pdf": "https://arxiv.org/pdf/2504.14038", "abs": "https://arxiv.org/abs/2504.14038", "authors": ["Stephen N. Freund", "Brooke Simon", "Emery D. Berger", "Eunice Jun"], "title": "Flowco: Rethinking Data Analysis in the Age of LLMs", "categories": ["cs.HC", "cs.AI", "cs.PL", "stat.CO"], "comment": null, "summary": "Conducting data analysis typically involves authoring code to transform,\nvisualize, analyze, and interpret data. Large language models (LLMs) are now\ncapable of generating such code for simple, routine analyses. LLMs promise to\ndemocratize data science by enabling those with limited programming expertise\nto conduct data analyses, including in scientific research, business, and\npolicymaking. However, analysts in many real-world settings must often exercise\nfine-grained control over specific analysis steps, verify intermediate results\nexplicitly, and iteratively refine their analytical approaches. Such tasks\npresent barriers to building robust and reproducible analyses using LLMs alone\nor even in conjunction with existing authoring tools (e.g., computational\nnotebooks). This paper introduces Flowco, a new mixed-initiative system to\naddress these challenges. Flowco leverages a visual dataflow programming model\nand integrates LLMs into every phase of the authoring process. A user study\nsuggests that Flowco supports analysts, particularly those with less\nprogramming experience, in quickly authoring, debugging, and refining data\nanalyses.", "AI": {"tldr": "Flowco\u662f\u4e00\u4e2a\u6df7\u5408\u4e3b\u52a8\u7cfb\u7edf\uff0c\u901a\u8fc7\u53ef\u89c6\u5316\u6570\u636e\u6d41\u7f16\u7a0b\u6a21\u578b\u548cLLM\u96c6\u6210\uff0c\u5e2e\u52a9\u7528\u6237\uff08\u5c24\u5176\u662f\u7f16\u7a0b\u7ecf\u9a8c\u8f83\u5c11\u8005\uff09\u5feb\u901f\u7f16\u5199\u3001\u8c03\u8bd5\u548c\u4f18\u5316\u6570\u636e\u5206\u6790\u3002", "motivation": "LLM\u867d\u80fd\u751f\u6210\u7b80\u5355\u6570\u636e\u5206\u6790\u4ee3\u7801\uff0c\u4f46\u5728\u9700\u8981\u7cbe\u7ec6\u63a7\u5236\u3001\u9a8c\u8bc1\u4e2d\u95f4\u7ed3\u679c\u548c\u8fed\u4ee3\u4f18\u5316\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0cFlowco\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "Flowco\u7ed3\u5408\u53ef\u89c6\u5316\u6570\u636e\u6d41\u7f16\u7a0b\u6a21\u578b\uff0c\u5c06LLM\u96c6\u6210\u5230\u7f16\u5199\u8fc7\u7a0b\u7684\u6bcf\u4e2a\u9636\u6bb5\u3002", "result": "\u7528\u6237\u7814\u7a76\u8868\u660e\uff0cFlowco\u80fd\u6709\u6548\u652f\u6301\u7528\u6237\uff08\u5c24\u5176\u662f\u7f16\u7a0b\u65b0\u624b\uff09\u5feb\u901f\u5b8c\u6210\u6570\u636e\u5206\u6790\u4efb\u52a1\u3002", "conclusion": "Flowco\u4e3a\u6570\u636e\u5206\u6790\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u3001\u53ef\u63a7\u7684\u5de5\u5177\uff0c\u5c24\u5176\u9002\u5408\u975e\u4e13\u4e1a\u7f16\u7a0b\u4eba\u5458\u3002"}}
{"id": "2504.15068", "pdf": "https://arxiv.org/pdf/2504.15068", "abs": "https://arxiv.org/abs/2504.15068", "authors": ["Ronak Pradeep", "Nandan Thakur", "Shivani Upadhyay", "Daniel Campos", "Nick Craswell", "Jimmy Lin"], "title": "The Great Nugget Recall: Automating Fact Extraction and RAG Evaluation with Large Language Models", "categories": ["cs.IR", "cs.CL"], "comment": "To appear in SIGIR 2025. Significant updates and revisions to\n  arXiv:2411.09607", "summary": "Large Language Models (LLMs) have significantly enhanced the capabilities of\ninformation access systems, especially with retrieval-augmented generation\n(RAG). Nevertheless, the evaluation of RAG systems remains a barrier to\ncontinued progress, a challenge we tackle in this work by proposing an\nautomatic evaluation framework that is validated against human annotations. We\nbelieve that the nugget evaluation methodology provides a solid foundation for\nevaluating RAG systems. This approach, originally developed for the TREC\nQuestion Answering (QA) Track in 2003, evaluates systems based on atomic facts\nthat should be present in good answers. Our efforts focus on \"refactoring\" this\nmethodology, where we describe the AutoNuggetizer framework that specifically\napplies LLMs to both automatically create nuggets and automatically assign\nnuggets to system answers. In the context of the TREC 2024 RAG Track, we\ncalibrate a fully automatic approach against strategies where nuggets are\ncreated manually or semi-manually by human assessors and then assigned manually\nto system answers. Based on results from a community-wide evaluation, we\nobserve strong agreement at the run level between scores derived from fully\nautomatic nugget evaluation and human-based variants. The agreement is stronger\nwhen individual framework components such as nugget assignment are automated\nindependently. This suggests that our evaluation framework provides tradeoffs\nbetween effort and quality that can be used to guide the development of future\nRAG systems. However, further research is necessary to refine our approach,\nparticularly in establishing robust per-topic agreement to diagnose system\nfailures effectively.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLLMs\u7684\u81ea\u52a8\u8bc4\u4f30\u6846\u67b6AutoNuggetizer\uff0c\u7528\u4e8e\u8bc4\u4f30\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7cfb\u7edf\uff0c\u9a8c\u8bc1\u4e86\u5176\u4e0e\u4eba\u5de5\u8bc4\u4f30\u7684\u4e00\u81f4\u6027\u3002", "motivation": "RAG\u7cfb\u7edf\u7684\u8bc4\u4f30\u662f\u5f53\u524d\u7814\u7a76\u7684\u74f6\u9888\uff0c\u9700\u8981\u4e00\u79cd\u81ea\u52a8\u5316\u7684\u65b9\u6cd5\u4ee5\u63a8\u52a8\u8fdb\u5c55\u3002", "method": "\u91c7\u7528TREC QA Track\u7684nugget\u8bc4\u4f30\u65b9\u6cd5\uff0c\u901a\u8fc7LLMs\u81ea\u52a8\u751f\u6210\u548c\u5206\u914dnuggets\uff0c\u5e76\u4e0e\u4eba\u5de5\u8bc4\u4f30\u5bf9\u6bd4\u3002", "result": "\u81ea\u52a8\u8bc4\u4f30\u4e0e\u4eba\u5de5\u8bc4\u4f30\u5728\u8fd0\u884c\u7ea7\u522b\u4e0a\u8868\u73b0\u51fa\u4e00\u81f4\u6027\uff0c\u5c24\u5176\u5728\u72ec\u7acb\u81ea\u52a8\u5316\u7ec4\u4ef6\u65f6\u6548\u679c\u66f4\u4f73\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u8d28\u91cf\u548c\u6548\u7387\u95f4\u63d0\u4f9b\u4e86\u5e73\u8861\uff0c\u4f46\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u4ee5\u4f18\u5316\u6bcf\u4e3b\u9898\u4e00\u81f4\u6027\u3002"}}
{"id": "2504.15214", "pdf": "https://arxiv.org/pdf/2504.15214", "abs": "https://arxiv.org/abs/2504.15214", "authors": ["Amirmohammad Mohammadi", "Davelle Carreiro", "Alexandra Van Dine", "Joshua Peeples"], "title": "Histogram-based Parameter-efficient Tuning for Passive Sonar Classification", "categories": ["cs.LG", "cs.SD"], "comment": "5 pages, 4 figures. Submitted to IEEE WASPAA 2025 for possible\n  publication", "summary": "Parameter-efficient transfer learning (PETL) methods adapt large artificial\nneural networks to downstream tasks without fine-tuning the entire model.\nHowever, existing additive methods, such as adapters, sometimes struggle to\ncapture distributional shifts in intermediate feature embeddings. We propose a\nnovel histogram-based parameter-efficient tuning (HPT) technique that captures\nthe statistics of the target domain and modulates the embeddings. Experimental\nresults on three downstream passive sonar datasets (ShipsEar, DeepShip, VTUAD)\ndemonstrate that HPT outperforms conventional adapters. Notably, HPT achieves\n91.8% vs. 89.8% accuracy on VTUAD. Furthermore, HPT trains faster and yields\nfeature representations closer to those of fully fine-tuned models. Overall,\nHPT balances parameter savings and performance, providing a distribution-aware\nalternative to existing adapters and shows a promising direction for scalable\ntransfer learning in resource-constrained environments. The code is publicly\navailable:\nhttps://github.com/Advanced-Vision-and-Learning-Lab/HLAST_DeepShip_ParameterEfficient.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u76f4\u65b9\u56fe\u7684\u9ad8\u6548\u53c2\u6570\u8c03\u4f18\u65b9\u6cd5\uff08HPT\uff09\uff0c\u7528\u4e8e\u89e3\u51b3\u73b0\u6709\u9002\u914d\u5668\u65b9\u6cd5\u5728\u7279\u5f81\u5d4c\u5165\u5206\u5e03\u504f\u79fb\u4e0a\u7684\u4e0d\u8db3\uff0c\u5e76\u5728\u88ab\u52a8\u58f0\u7eb3\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u9002\u914d\u5668\u3002", "motivation": "\u73b0\u6709\u53c2\u6570\u9ad8\u6548\u8fc1\u79fb\u5b66\u4e60\u65b9\u6cd5\uff08\u5982\u9002\u914d\u5668\uff09\u5728\u5904\u7406\u4e2d\u95f4\u7279\u5f81\u5d4c\u5165\u7684\u5206\u5e03\u504f\u79fb\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7684\u65b9\u6cd5\u6765\u6355\u6349\u76ee\u6807\u57df\u7edf\u8ba1\u4fe1\u606f\u5e76\u8c03\u6574\u5d4c\u5165\u3002", "method": "\u63d0\u51faHPT\u6280\u672f\uff0c\u901a\u8fc7\u76f4\u65b9\u56fe\u6355\u6349\u76ee\u6807\u57df\u7edf\u8ba1\u4fe1\u606f\u5e76\u8c03\u5236\u5d4c\u5165\uff0c\u5b9e\u73b0\u9ad8\u6548\u53c2\u6570\u8c03\u4f18\u3002", "result": "\u5728\u4e09\u4e2a\u88ab\u52a8\u58f0\u7eb3\u6570\u636e\u96c6\uff08ShipsEar\u3001DeepShip\u3001VTUAD\uff09\u4e0a\uff0cHPT\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u9002\u914d\u5668\uff0cVTUAD\u4e0a\u51c6\u786e\u7387\u8fbe91.8%\uff08vs. 89.8%\uff09\u3002HPT\u8bad\u7ec3\u66f4\u5feb\u4e14\u7279\u5f81\u8868\u793a\u66f4\u63a5\u8fd1\u5168\u5fae\u8c03\u6a21\u578b\u3002", "conclusion": "HPT\u5728\u53c2\u6570\u8282\u7701\u548c\u6027\u80fd\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u7684\u53ef\u6269\u5c55\u8fc1\u79fb\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2504.15072", "pdf": "https://arxiv.org/pdf/2504.15072", "abs": "https://arxiv.org/abs/2504.15072", "authors": ["Yulong Li", "Zhixiang Lu", "Feilong Tang", "Simin Lai", "Ming Hu", "Yuxuan Zhang", "Haochen Xue", "Zhaodong Wu", "Imran Razzak", "Qingxia Li", "Jionglong Su"], "title": "Rhythm of Opinion: A Hawkes-Graph Framework for Dynamic Propagation Analysis", "categories": ["cs.SI", "cs.CL"], "comment": null, "summary": "The rapid development of social media has significantly reshaped the dynamics\nof public opinion, resulting in complex interactions that traditional models\nfail to effectively capture. To address this challenge, we propose an\ninnovative approach that integrates multi-dimensional Hawkes processes with\nGraph Neural Network, modeling opinion propagation dynamics among nodes in a\nsocial network while considering the intricate hierarchical relationships\nbetween comments. The extended multi-dimensional Hawkes process captures the\nhierarchical structure, multi-dimensional interactions, and mutual influences\nacross different topics, forming a complex propagation network. Moreover,\nrecognizing the lack of high-quality datasets capable of comprehensively\ncapturing the evolution of public opinion dynamics, we introduce a new dataset,\nVISTA. It includes 159 trending topics, corresponding to 47,207 posts, 327,015\nsecond-level comments, and 29,578 third-level comments, covering diverse\ndomains such as politics, entertainment, sports, health, and medicine. The\ndataset is annotated with detailed sentiment labels across 11 categories and\nclearly defined hierarchical relationships. When combined with our method, it\noffers strong interpretability by linking sentiment propagation to the comment\nhierarchy and temporal evolution. Our approach provides a robust baseline for\nfuture research.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u591a\u7ef4\u970d\u514b\u65af\u8fc7\u7a0b\u4e0e\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5efa\u6a21\u793e\u4ea4\u7f51\u7edc\u4e2d\u610f\u89c1\u4f20\u64ad\u7684\u52a8\u6001\uff0c\u5e76\u5f15\u5165\u65b0\u6570\u636e\u96c6VISTA\u4ee5\u652f\u6301\u7814\u7a76\u3002", "motivation": "\u4f20\u7edf\u6a21\u578b\u96be\u4ee5\u6709\u6548\u6355\u6349\u793e\u4ea4\u5a92\u4f53\u4e2d\u516c\u5171\u610f\u89c1\u7684\u590d\u6742\u52a8\u6001\uff0c\u9700\u8981\u65b0\u7684\u65b9\u6cd5\u6765\u5efa\u6a21\u591a\u7ef4\u5ea6\u4e92\u52a8\u548c\u5c42\u7ea7\u5173\u7cfb\u3002", "method": "\u6574\u5408\u591a\u7ef4\u970d\u514b\u65af\u8fc7\u7a0b\u4e0e\u56fe\u795e\u7ecf\u7f51\u7edc\uff0c\u5efa\u6a21\u793e\u4ea4\u7f51\u7edc\u4e2d\u8282\u70b9\u95f4\u7684\u610f\u89c1\u4f20\u64ad\u52a8\u6001\uff0c\u540c\u65f6\u8003\u8651\u8bc4\u8bba\u7684\u5c42\u7ea7\u5173\u7cfb\u3002", "result": "\u63d0\u51fa\u65b0\u6570\u636e\u96c6VISTA\uff0c\u5305\u542b\u591a\u9886\u57df\u6570\u636e\uff0c\u7ed3\u5408\u65b9\u6cd5\u63d0\u4f9b\u4e86\u5f3a\u89e3\u91ca\u6027\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u5960\u5b9a\u57fa\u7840\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u5efa\u6a21\u590d\u6742\u610f\u89c1\u4f20\u64ad\u52a8\u6001\uff0c\u6570\u636e\u96c6VISTA\u4e3a\u76f8\u5173\u7814\u7a76\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u8d44\u6e90\u3002"}}
{"id": "2504.15223", "pdf": "https://arxiv.org/pdf/2504.15223", "abs": "https://arxiv.org/abs/2504.15223", "authors": ["Tao Yang", "Yu Cheng", "Yaokun Ren", "Yujia Lou", "Minggu Wei", "Honghui Xin"], "title": "A Deep Learning Framework for Sequence Mining with Bidirectional LSTM and Multi-Scale Attention", "categories": ["cs.LG"], "comment": null, "summary": "This paper addresses the challenges of mining latent patterns and modeling\ncontextual dependencies in complex sequence data. A sequence pattern mining\nalgorithm is proposed by integrating Bidirectional Long Short-Term Memory\n(BiLSTM) with a multi-scale attention mechanism. The BiLSTM captures both\nforward and backward dependencies in sequences, enhancing the model's ability\nto perceive global contextual structures. At the same time, the multi-scale\nattention module assigns adaptive weights to key feature regions under\ndifferent window sizes. This improves the model's responsiveness to both local\nand global important information. Extensive experiments are conducted on a\npublicly available multivariate time series dataset. The proposed model is\ncompared with several mainstream sequence modeling methods. Results show that\nit outperforms existing models in terms of accuracy, precision, and recall.\nThis confirms the effectiveness and robustness of the proposed architecture in\ncomplex pattern recognition tasks. Further ablation studies and sensitivity\nanalyses are carried out to investigate the effects of attention scale and\ninput sequence length on model performance. These results provide empirical\nsupport for structural optimization of the model.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408BiLSTM\u548c\u591a\u5c3a\u5ea6\u6ce8\u610f\u529b\u673a\u5236\u7684\u5e8f\u5217\u6a21\u5f0f\u6316\u6398\u7b97\u6cd5\uff0c\u7528\u4e8e\u590d\u6742\u5e8f\u5217\u6570\u636e\u4e2d\u7684\u6f5c\u5728\u6a21\u5f0f\u6316\u6398\u548c\u4e0a\u4e0b\u6587\u4f9d\u8d56\u5efa\u6a21\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u590d\u6742\u5e8f\u5217\u6570\u636e\u4e2d\u6f5c\u5728\u6a21\u5f0f\u6316\u6398\u548c\u4e0a\u4e0b\u6587\u4f9d\u8d56\u5efa\u6a21\u7684\u6311\u6218\u3002", "method": "\u96c6\u6210\u53cc\u5411LSTM\uff08BiLSTM\uff09\u548c\u591a\u5c3a\u5ea6\u6ce8\u610f\u529b\u673a\u5236\uff0cBiLSTM\u6355\u6349\u5e8f\u5217\u7684\u524d\u540e\u4f9d\u8d56\uff0c\u6ce8\u610f\u529b\u6a21\u5757\u81ea\u9002\u5e94\u5206\u914d\u6743\u91cd\u3002", "result": "\u5728\u516c\u5f00\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\uff0c\u6a21\u578b\u5728\u51c6\u786e\u7387\u3001\u7cbe\u786e\u7387\u548c\u53ec\u56de\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u67b6\u6784\u5728\u590d\u6742\u6a21\u5f0f\u8bc6\u522b\u4efb\u52a1\u4e2d\u6709\u6548\u4e14\u9c81\u68d2\uff0c\u6ce8\u610f\u529b\u5c3a\u5ea6\u548c\u8f93\u5165\u5e8f\u5217\u957f\u5ea6\u5bf9\u6027\u80fd\u6709\u5f71\u54cd\uff0c\u4e3a\u7ed3\u6784\u4f18\u5316\u63d0\u4f9b\u4f9d\u636e\u3002"}}
{"id": "2504.15135", "pdf": "https://arxiv.org/pdf/2504.15135", "abs": "https://arxiv.org/abs/2504.15135", "authors": ["Juyeon Kim", "Geon Lee", "Taeuk Kim", "Kijung Shin"], "title": "KGMEL: Knowledge Graph-Enhanced Multimodal Entity Linking", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": "SIGIR 2025 (Short)", "summary": "Entity linking (EL) aligns textual mentions with their corresponding entities\nin a knowledge base, facilitating various applications such as semantic search\nand question answering. Recent advances in multimodal entity linking (MEL) have\nshown that combining text and images can reduce ambiguity and improve alignment\naccuracy. However, most existing MEL methods overlook the rich structural\ninformation available in the form of knowledge-graph (KG) triples. In this\npaper, we propose KGMEL, a novel framework that leverages KG triples to enhance\nMEL. Specifically, it operates in three stages: (1) Generation: Produces\nhigh-quality triples for each mention by employing vision-language models based\non its text and images. (2) Retrieval: Learns joint mention-entity\nrepresentations, via contrastive learning, that integrate text, images, and\n(generated or KG) triples to retrieve candidate entities for each mention. (3)\nReranking: Refines the KG triples of the candidate entities and employs large\nlanguage models to identify the best-matching entity for the mention. Extensive\nexperiments on benchmark datasets demonstrate that KGMEL outperforms existing\nmethods. Our code and datasets are available at:\nhttps://github.com/juyeonnn/KGMEL.", "AI": {"tldr": "KGMEL\u662f\u4e00\u79cd\u5229\u7528\u77e5\u8bc6\u56fe\u8c31\u4e09\u5143\u7ec4\u589e\u5f3a\u591a\u6a21\u6001\u5b9e\u4f53\u94fe\u63a5\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u6210\u3001\u68c0\u7d22\u548c\u91cd\u6392\u540d\u4e09\u9636\u6bb5\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709MEL\u65b9\u6cd5\u5ffd\u7565\u4e86\u77e5\u8bc6\u56fe\u8c31\u7684\u7ed3\u6784\u4fe1\u606f\uff0cKGMEL\u65e8\u5728\u5229\u7528\u8fd9\u4e9b\u4fe1\u606f\u51cf\u5c11\u6b67\u4e49\u5e76\u63d0\u9ad8\u51c6\u786e\u6027\u3002", "method": "KGMEL\u5206\u4e09\u9636\u6bb5\uff1a\u751f\u6210\u9ad8\u8d28\u91cf\u4e09\u5143\u7ec4\u3001\u5b66\u4e60\u8054\u5408\u8868\u793a\u68c0\u7d22\u5019\u9009\u5b9e\u4f53\u3001\u91cd\u6392\u540d\u9009\u62e9\u6700\u4f73\u5339\u914d\u5b9e\u4f53\u3002", "result": "\u5b9e\u9a8c\u8868\u660eKGMEL\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "KGMEL\u901a\u8fc7\u6574\u5408\u77e5\u8bc6\u56fe\u8c31\u4e09\u5143\u7ec4\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u5b9e\u4f53\u94fe\u63a5\u7684\u6027\u80fd\u3002"}}
{"id": "2504.15225", "pdf": "https://arxiv.org/pdf/2504.15225", "abs": "https://arxiv.org/abs/2504.15225", "authors": ["Sarah Alnegheimish", "Zelin He", "Matthew Reimherr", "Akash Chandrayan", "Abhinav Pradhan", "Luca D'Angelo"], "title": "M$^2$AD: Multi-Sensor Multi-System Anomaly Detection through Global Scoring and Calibrated Thresholding", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted at AISTATS 2025", "summary": "With the widespread availability of sensor data across industrial and\noperational systems, we frequently encounter heterogeneous time series from\nmultiple systems. Anomaly detection is crucial for such systems to facilitate\npredictive maintenance. However, most existing anomaly detection methods are\ndesigned for either univariate or single-system multivariate data, making them\ninsufficient for these complex scenarios. To address this, we introduce\nM$^2$AD, a framework for unsupervised anomaly detection in multivariate time\nseries data from multiple systems. M$^2$AD employs deep models to capture\nexpected behavior under normal conditions, using the residuals as indicators of\npotential anomalies. These residuals are then aggregated into a global anomaly\nscore through a Gaussian Mixture Model and Gamma calibration. We theoretically\ndemonstrate that this framework can effectively address heterogeneity and\ndependencies across sensors and systems. Empirically, M$^2$AD outperforms\nexisting methods in extensive evaluations by 21% on average, and its\neffectiveness is demonstrated on a large-scale real-world case study on 130\nassets in Amazon Fulfillment Centers. Our code and results are available at\nhttps://github.com/sarahmish/M2AD.", "AI": {"tldr": "M$^2$AD\u662f\u4e00\u4e2a\u7528\u4e8e\u591a\u7cfb\u7edf\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u7684\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u6df1\u5ea6\u6a21\u578b\u548c\u6b8b\u5dee\u5206\u6790\u89e3\u51b3\u5f02\u6784\u6027\u95ee\u9898\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd521%\u3002", "motivation": "\u73b0\u6709\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u591a\u9488\u5bf9\u5355\u53d8\u91cf\u6216\u5355\u7cfb\u7edf\u6570\u636e\uff0c\u65e0\u6cd5\u5e94\u5bf9\u591a\u7cfb\u7edf\u5f02\u6784\u65f6\u95f4\u5e8f\u5217\u7684\u590d\u6742\u573a\u666f\u3002", "method": "\u4f7f\u7528\u6df1\u5ea6\u6a21\u578b\u6355\u6349\u6b63\u5e38\u884c\u4e3a\uff0c\u6b8b\u5dee\u4f5c\u4e3a\u5f02\u5e38\u6307\u6807\uff0c\u901a\u8fc7\u9ad8\u65af\u6df7\u5408\u6a21\u578b\u548cGamma\u6821\u51c6\u751f\u6210\u5168\u5c40\u5f02\u5e38\u5206\u6570\u3002", "result": "M$^2$AD\u5728\u5b9e\u9a8c\u4e2d\u5e73\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd521%\uff0c\u5e76\u5728\u4e9a\u9a6c\u900a\u7269\u6d41\u4e2d\u5fc3\u7684130\u4e2a\u8d44\u4ea7\u4e0a\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002", "conclusion": "M$^2$AD\u80fd\u6709\u6548\u5904\u7406\u591a\u7cfb\u7edf\u548c\u4f20\u611f\u5668\u95f4\u7684\u5f02\u6784\u6027\u4e0e\u4f9d\u8d56\u6027\uff0c\u9002\u7528\u4e8e\u590d\u6742\u5de5\u4e1a\u573a\u666f\u3002"}}
{"id": "2504.15254", "pdf": "https://arxiv.org/pdf/2504.15254", "abs": "https://arxiv.org/abs/2504.15254", "authors": ["Anirudh Khatry", "Robert Zhang", "Jia Pan", "Ziteng Wang", "Qiaochu Chen", "Greg Durrett", "Isil Dillig"], "title": "CRUST-Bench: A Comprehensive Benchmark for C-to-safe-Rust Transpilation", "categories": ["cs.SE", "cs.CL"], "comment": null, "summary": "C-to-Rust transpilation is essential for modernizing legacy C code while\nenhancing safety and interoperability with modern Rust ecosystems. However, no\ndataset currently exists for evaluating whether a system can transpile C into\nsafe Rust that passes a set of test cases. We introduce CRUST-Bench, a dataset\nof 100 C repositories, each paired with manually-written interfaces in safe\nRust as well as test cases that can be used to validate correctness of the\ntranspilation. By considering entire repositories rather than isolated\nfunctions, CRUST-Bench captures the challenges of translating complex projects\nwith dependencies across multiple files. The provided Rust interfaces provide\nexplicit specifications that ensure adherence to idiomatic, memory-safe Rust\npatterns, while the accompanying test cases enforce functional correctness. We\nevaluate state-of-the-art large language models (LLMs) on this task and find\nthat safe and idiomatic Rust generation is still a challenging problem for\nvarious state-of-the-art methods and techniques. We also provide insights into\nthe errors LLMs usually make in transpiling code from C to safe Rust. The best\nperforming model, OpenAI o1, is able to solve only 15 tasks in a single-shot\nsetting. Improvements on CRUST-Bench would lead to improved transpilation\nsystems that can reason about complex scenarios and help in migrating legacy\ncodebases from C into languages like Rust that ensure memory safety. You can\nfind the dataset and code at https://github.com/anirudhkhatry/CRUST-bench.", "AI": {"tldr": "CRUST-Bench\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30C\u5230Rust\u8f6c\u8bd1\u7684\u6570\u636e\u96c6\uff0c\u5305\u542b100\u4e2aC\u4ed3\u5e93\u53ca\u5176\u5bf9\u5e94\u7684\u5b89\u5168Rust\u63a5\u53e3\u548c\u6d4b\u8bd5\u7528\u4f8b\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u751f\u6210\u5b89\u5168\u548c\u60ef\u7528\u7684Rust\u4ee3\u7801\u4e0a\u4ecd\u6709\u6311\u6218\u3002", "motivation": "\u7f3a\u4e4f\u8bc4\u4f30C\u5230\u5b89\u5168Rust\u8f6c\u8bd1\u7684\u6570\u636e\u96c6\uff0cCRUST-Bench\u586b\u8865\u4e86\u8fd9\u4e00\u7a7a\u767d\uff0c\u652f\u6301\u590d\u6742\u9879\u76ee\u7684\u8f6c\u8bd1\u9a8c\u8bc1\u3002", "method": "\u6784\u5efa\u5305\u542b100\u4e2aC\u4ed3\u5e93\u7684\u6570\u636e\u96c6\uff0c\u6bcf\u4e2a\u4ed3\u5e93\u914d\u6709\u624b\u52a8\u7f16\u5199\u7684\u5b89\u5168Rust\u63a5\u53e3\u548c\u6d4b\u8bd5\u7528\u4f8b\uff0c\u7528\u4e8e\u9a8c\u8bc1\u8f6c\u8bd1\u6b63\u786e\u6027\u3002", "result": "\u73b0\u6709\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08\u5982OpenAI o1\uff09\u5728\u5355\u6b21\u5c1d\u8bd5\u4e2d\u4ec5\u80fd\u5b8c\u621015\u4e2a\u4efb\u52a1\uff0c\u8868\u660e\u5b89\u5168\u548c\u60ef\u7528\u7684Rust\u751f\u6210\u4ecd\u5177\u6311\u6218\u6027\u3002", "conclusion": "CRUST-Bench\u4e3a\u6539\u8fdb\u8f6c\u8bd1\u7cfb\u7edf\u63d0\u4f9b\u4e86\u57fa\u51c6\uff0c\u6709\u52a9\u4e8e\u4eceC\u8fc1\u79fb\u5230\u5185\u5b58\u5b89\u5168\u7684Rust\u8bed\u8a00\u3002"}}
{"id": "2504.15240", "pdf": "https://arxiv.org/pdf/2504.15240", "abs": "https://arxiv.org/abs/2504.15240", "authors": ["Amirhossein Mollaali", "Christian Bolivar Moya", "Amanda A. Howard", "Alexander Heinlein", "Panos Stinis", "Guang Lin"], "title": "Conformalized-KANs: Uncertainty Quantification with Coverage Guarantees for Kolmogorov-Arnold Networks (KANs) in Scientific Machine Learning", "categories": ["cs.LG"], "comment": "17 pages, 8 figures,", "summary": "This paper explores uncertainty quantification (UQ) methods in the context of\nKolmogorov-Arnold Networks (KANs). We apply an ensemble approach to KANs to\nobtain a heuristic measure of UQ, enhancing interpretability and robustness in\nmodeling complex functions. Building on this, we introduce Conformalized-KANs,\nwhich integrate conformal prediction, a distribution-free UQ technique, with\nKAN ensembles to generate calibrated prediction intervals with guaranteed\ncoverage. Extensive numerical experiments are conducted to evaluate the\neffectiveness of these methods, focusing particularly on the robustness and\naccuracy of the prediction intervals under various hyperparameter settings. We\nshow that the conformal KAN predictions can be applied to recent extensions of\nKANs, including Finite Basis KANs (FBKANs) and multifideilty KANs (MFKANs). The\nresults demonstrate the potential of our approaches to improve the reliability\nand applicability of KANs in scientific machine learning.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86Kolmogorov-Arnold Networks (KANs)\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u96c6\u6210\u65b9\u6cd5\u548cConformalized-KANs\uff0c\u4ee5\u63d0\u5347\u9884\u6d4b\u533a\u95f4\u7684\u6821\u51c6\u6027\u548c\u8986\u76d6\u6027\u3002", "motivation": "\u63d0\u5347KANs\u5728\u590d\u6742\u51fd\u6570\u5efa\u6a21\u4e2d\u7684\u53ef\u89e3\u91ca\u6027\u548c\u9c81\u68d2\u6027\uff0c\u5e76\u589e\u5f3a\u5176\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u80fd\u529b\u3002", "method": "\u91c7\u7528\u96c6\u6210KANs\u548cConformalized-KANs\u65b9\u6cd5\uff0c\u7ed3\u5408\u5171\u5f62\u9884\u6d4b\u6280\u672f\u751f\u6210\u6821\u51c6\u7684\u9884\u6d4b\u533a\u95f4\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u5347\u9884\u6d4b\u533a\u95f4\u7684\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\uff0c\u5e76\u9002\u7528\u4e8eKANs\u7684\u6269\u5c55\u7248\u672c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86KANs\u5728\u79d1\u5b66\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u53ef\u9760\u6027\u548c\u9002\u7528\u6027\u3002"}}
{"id": "2504.14054", "pdf": "https://arxiv.org/pdf/2504.14054", "abs": "https://arxiv.org/abs/2504.14054", "authors": ["Soroosh Baselizadeh", "Cheuk-To Yu", "Olga Veksler", "Yuri Boykov"], "title": "Occlusion-Ordered Semantic Instance Segmentation", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Standard semantic instance segmentation provides useful, but inherently 2D\ninformation from a single image. To enable 3D analysis, one usually integrates\nabsolute monocular depth estimation with instance segmentation. However,\nmonocular depth is a difficult task. Instead, we leverage a simpler\nsingle-image task, occlusion-based relative depth ordering, providing coarser\nbut useful 3D information. We show that relative depth ordering works more\nreliably from occlusions than from absolute depth. We propose to solve the\njoint task of relative depth ordering and segmentation of instances based on\nocclusions. We call this task Occlusion-Ordered Semantic Instance Segmentation\n(OOSIS). We develop an approach to OOSIS that extracts instances and their\nocclusion order simultaneously from oriented occlusion boundaries and semantic\nsegmentation. Unlike popular detect-and-segment framework for instance\nsegmentation, combining occlusion ordering with instance segmentation allows a\nsimple and clean formulation of OOSIS as a labeling problem. As a part of our\nsolution for OOSIS, we develop a novel oriented occlusion boundaries approach\nthat significantly outperforms prior work. We also develop a new joint OOSIS\nmetric based both on instance mask accuracy and correctness of their occlusion\norder. We achieve better performance than strong baselines on KINS and COCOA\ndatasets.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u76f8\u5bf9\u6df1\u5ea6\u6392\u5e8f\u548c\u5b9e\u4f8b\u5206\u5272\u7684\u4efb\u52a1OOSIS\uff0c\u901a\u8fc7\u906e\u6321\u8fb9\u754c\u5b9e\u73b0\u66f4\u53ef\u9760\u76843D\u4fe1\u606f\u63d0\u53d6\u3002", "motivation": "\u4f20\u7edf2D\u5b9e\u4f8b\u5206\u5272\u7f3a\u4e4f3D\u4fe1\u606f\uff0c\u800c\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u56f0\u96be\uff0c\u56e0\u6b64\u5229\u7528\u906e\u6321\u5173\u7cfb\u63d0\u4f9b\u66f4\u7b80\u5355\u76843D\u4fe1\u606f\u3002", "method": "\u63d0\u51faOOSIS\u4efb\u52a1\uff0c\u7ed3\u5408\u906e\u6321\u8fb9\u754c\u548c\u8bed\u4e49\u5206\u5272\u540c\u65f6\u63d0\u53d6\u5b9e\u4f8b\u53ca\u5176\u906e\u6321\u987a\u5e8f\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u79cd\u65b0\u7684\u5b9a\u5411\u906e\u6321\u8fb9\u754c\u65b9\u6cd5\u3002", "result": "\u5728KINS\u548cCOCOA\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "OOSIS\u901a\u8fc7\u906e\u6321\u5173\u7cfb\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u6709\u6548\u76843D\u4fe1\u606f\u63d0\u53d6\u65b9\u6cd5\uff0c\u4f18\u4e8e\u4f20\u7edf\u6df1\u5ea6\u4f30\u8ba1\u3002"}}
{"id": "2504.15266", "pdf": "https://arxiv.org/pdf/2504.15266", "abs": "https://arxiv.org/abs/2504.15266", "authors": ["Vaishnavh Nagarajan", "Chen Henry Wu", "Charles Ding", "Aditi Raghunathan"], "title": "Roll the dice & look before you leap: Going beyond the creative limits of next-token prediction", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "37 pages", "summary": "We design a suite of minimal algorithmic tasks that are a loose abstraction\nof open-ended real-world tasks. This allows us to cleanly and controllably\nquantify the creative limits of the present-day language model. Much like\nreal-world tasks that require a creative, far-sighted leap of thought, our\ntasks require an implicit, open-ended stochastic planning step that either (a)\ndiscovers new connections in an abstract knowledge graph (like in wordplay,\ndrawing analogies, or research) or (b) constructs new patterns (like in\ndesigning math problems or new proteins). In these tasks, we empirically and\nconceptually argue how next-token learning is myopic and memorizes excessively;\ncomparatively, multi-token approaches, namely teacherless training and\ndiffusion models, excel in producing diverse and original output. Secondly, in\nour tasks, we find that to elicit randomness from the Transformer without\nhurting coherence, it is better to inject noise right at the input layer (via a\nmethod we dub hash-conditioning) rather than defer to temperature sampling from\nthe output layer. Thus, our work offers a principled, minimal test-bed for\nanalyzing open-ended creative skills, and offers new arguments for going beyond\nnext-token learning and softmax-based sampling. We make part of the code\navailable under https://github.com/chenwu98/algorithmic-creativity", "AI": {"tldr": "\u8bba\u6587\u8bbe\u8ba1\u4e86\u4e00\u5957\u6700\u5c0f\u7b97\u6cd5\u4efb\u52a1\uff0c\u7528\u4e8e\u91cf\u5316\u8bed\u8a00\u6a21\u578b\u7684\u521b\u9020\u529b\u9650\u5236\uff0c\u53d1\u73b0\u591a\u4ee4\u724c\u65b9\u6cd5\u4f18\u4e8e\u5355\u4ee4\u724c\u5b66\u4e60\uff0c\u5e76\u63d0\u51fa\u8f93\u5165\u5c42\u566a\u58f0\u6ce8\u5165\u65b9\u6cd5\u4ee5\u63d0\u5347\u968f\u673a\u6027\u4e0e\u8fde\u8d2f\u6027\u3002", "motivation": "\u7814\u7a76\u8bed\u8a00\u6a21\u578b\u5728\u5f00\u653e\u4efb\u52a1\u4e2d\u7684\u521b\u9020\u529b\u9650\u5236\uff0c\u63a2\u7d22\u5982\u4f55\u6539\u8fdb\u73b0\u6709\u65b9\u6cd5\u4ee5\u63d0\u5347\u591a\u6837\u6027\u548c\u539f\u521b\u6027\u3002", "method": "\u8bbe\u8ba1\u62bd\u8c61\u4efb\u52a1\u6a21\u62df\u73b0\u5b9e\u5f00\u653e\u4efb\u52a1\uff0c\u6bd4\u8f83\u5355\u4ee4\u724c\u4e0e\u591a\u4ee4\u724c\u5b66\u4e60\u65b9\u6cd5\uff0c\u63d0\u51fa\u8f93\u5165\u5c42\u566a\u58f0\u6ce8\u5165\uff08hash-conditioning\uff09\u3002", "result": "\u591a\u4ee4\u724c\u65b9\u6cd5\uff08\u5982\u65e0\u6559\u5e08\u8bad\u7ec3\u548c\u6269\u6563\u6a21\u578b\uff09\u8868\u73b0\u66f4\u4f18\uff1b\u8f93\u5165\u5c42\u566a\u58f0\u6ce8\u5165\u6bd4\u8f93\u51fa\u5c42\u6e29\u5ea6\u91c7\u6837\u66f4\u6709\u6548\u3002", "conclusion": "\u4e3a\u5206\u6790\u5f00\u653e\u521b\u9020\u529b\u63d0\u4f9b\u4e86\u6d4b\u8bd5\u6846\u67b6\uff0c\u652f\u6301\u8d85\u8d8a\u5355\u4ee4\u724c\u5b66\u4e60\u548c\u57fa\u4e8esoftmax\u7684\u91c7\u6837\u65b9\u6cd5\u3002"}}
{"id": "2504.15243", "pdf": "https://arxiv.org/pdf/2504.15243", "abs": "https://arxiv.org/abs/2504.15243", "authors": ["Ming Yang", "Gang Li", "Quanqi Hu", "Qihang Lin", "Tianbao Yang"], "title": "Single-loop Algorithms for Stochastic Non-convex Optimization with Weakly-Convex Constraints", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "Constrained optimization with multiple functional inequality constraints has\nsignificant applications in machine learning. This paper examines a crucial\nsubset of such problems where both the objective and constraint functions are\nweakly convex. Existing methods often face limitations, including slow\nconvergence rates or reliance on double-loop algorithmic designs. To overcome\nthese challenges, we introduce a novel single-loop penalty-based stochastic\nalgorithm. Following the classical exact penalty method, our approach employs a\n{\\bf hinge-based penalty}, which permits the use of a constant penalty\nparameter, enabling us to achieve a {\\bf state-of-the-art complexity} for\nfinding an approximate Karush-Kuhn-Tucker (KKT) solution. We further extend our\nalgorithm to address finite-sum coupled compositional objectives, which are\nprevalent in artificial intelligence applications, establishing improved\ncomplexity over existing approaches. Finally, we validate our method through\nexperiments on fair learning with receiver operating characteristic (ROC)\nfairness constraints and continual learning with non-forgetting constraints.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5355\u5faa\u73af\u60e9\u7f5a\u968f\u673a\u7b97\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u5f31\u51f8\u76ee\u6807\u51fd\u6570\u548c\u7ea6\u675f\u51fd\u6570\u7684\u4f18\u5316\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u590d\u6742\u5ea6\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u6536\u655b\u901f\u5ea6\u6216\u7b97\u6cd5\u8bbe\u8ba1\u4e0a\u5b58\u5728\u5c40\u9650\uff0c\u9700\u8981\u6539\u8fdb\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u94f0\u94fe\u7684\u60e9\u7f5a\u65b9\u6cd5\uff0c\u5141\u8bb8\u6052\u5b9a\u60e9\u7f5a\u53c2\u6570\uff0c\u6269\u5c55\u81f3\u6709\u9650\u548c\u8026\u5408\u7ec4\u5408\u76ee\u6807\u3002", "result": "\u5728\u8fd1\u4f3cKKT\u89e3\u4e0a\u8fbe\u5230\u6700\u4f18\u590d\u6742\u5ea6\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002", "conclusion": "\u65b0\u7b97\u6cd5\u5728\u673a\u5668\u5b66\u4e60\u7684\u516c\u5e73\u5b66\u4e60\u548c\u6301\u7eed\u5b66\u4e60\u5e94\u7528\u4e2d\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2504.14070", "pdf": "https://arxiv.org/pdf/2504.14070", "abs": "https://arxiv.org/abs/2504.14070", "authors": ["Jinesh Jhonsa", "William Whitehead", "David McCarthy", "Shuvro Chowdhury", "Kerem Camsari", "Luke Theogarajan"], "title": "A CMOS Probabilistic Computing Chip With In-situ hardware Aware Learning", "categories": ["cs.AR", "cs.AI"], "comment": "3 pages 12 figuewa", "summary": "This paper demonstrates a probabilistic bit physics inspired solver with 440\nspins configured in a Chimera graph, occupying an area of 0.44 mm^2. Area\nefficiency is maximized through a current-mode implementation of the neuron\nupdate circuit, standard cell design for analog blocks pitch-matched to digital\nblocks, and a shared power supply for both digital and analog components.\nProcess variation related mismatches introduced by this approach are\neffectively mitigated using a hardware aware contrastive divergence algorithm\nduring training. We validate the chip's ability to perform probabilistic\ncomputing tasks such as modeling logic gates and full adders, as well as\noptimization tasks such as MaxCut, demonstrating its potential for AI and\nmachine learning applications.", "AI": {"tldr": "\u8bba\u6587\u5c55\u793a\u4e86\u4e00\u79cd\u57fa\u4e8e\u6982\u7387\u6bd4\u7279\u7269\u7406\u7684\u6c42\u89e3\u5668\uff0c\u91c7\u7528440\u4e2a\u81ea\u65cb\u7684Chimera\u56fe\u7ed3\u6784\uff0c\u9762\u79ef\u4e3a0.44 mm\u00b2\u3002\u901a\u8fc7\u7535\u6d41\u6a21\u5f0f\u795e\u7ecf\u5143\u66f4\u65b0\u7535\u8def\u3001\u6807\u51c6\u5355\u5143\u8bbe\u8ba1\u548c\u5171\u4eab\u7535\u6e90\u4f18\u5316\u9762\u79ef\u6548\u7387\uff0c\u5e76\u4f7f\u7528\u786c\u4ef6\u611f\u77e5\u5bf9\u6bd4\u6563\u5ea6\u7b97\u6cd5\u51cf\u5c11\u5de5\u827a\u53d8\u5316\u5f71\u54cd\u3002\u9a8c\u8bc1\u4e86\u82af\u7247\u5728\u6982\u7387\u8ba1\u7b97\u548c\u4f18\u5316\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u9ad8\u6548\u3001\u9762\u79ef\u4f18\u5316\u7684\u6982\u7387\u8ba1\u7b97\u82af\u7247\uff0c\u9002\u7528\u4e8eAI\u548c\u673a\u5668\u5b66\u4e60\u4efb\u52a1\u3002", "method": "\u91c7\u7528\u7535\u6d41\u6a21\u5f0f\u795e\u7ecf\u5143\u66f4\u65b0\u7535\u8def\u3001\u6807\u51c6\u5355\u5143\u8bbe\u8ba1\u3001\u5171\u4eab\u7535\u6e90\uff0c\u5e76\u7ed3\u5408\u786c\u4ef6\u611f\u77e5\u5bf9\u6bd4\u6563\u5ea6\u7b97\u6cd5\u51cf\u5c11\u5de5\u827a\u53d8\u5316\u5f71\u54cd\u3002", "result": "\u82af\u7247\u6210\u529f\u5b9e\u73b0\u4e86\u6982\u7387\u8ba1\u7b97\u4efb\u52a1\uff08\u5982\u903b\u8f91\u95e8\u548c\u5168\u52a0\u5668\u5efa\u6a21\uff09\u548c\u4f18\u5316\u4efb\u52a1\uff08\u5982MaxCut\uff09\uff0c\u5c55\u793a\u4e86\u5176\u5728AI\u548c\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u6f5c\u529b\u3002", "conclusion": "\u8be5\u82af\u7247\u8bbe\u8ba1\u9ad8\u6548\u4e14\u5b9e\u7528\uff0c\u4e3a\u6982\u7387\u8ba1\u7b97\u548c\u4f18\u5316\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u7684\u786c\u4ef6\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.15270", "pdf": "https://arxiv.org/pdf/2504.15270", "abs": "https://arxiv.org/abs/2504.15270", "authors": ["Ji Qi", "Yuan Yao", "Yushi Bai", "Bin Xu", "Juanzi Li", "Zhiyuan Liu", "Tat-Seng Chua"], "title": "An LMM for Efficient Video Understanding via Reinforced Compression of Video Cubes", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Large Multimodal Models (LMMs) uniformly perceive video frames, creating\ncomputational inefficiency for videos with inherently varying temporal\ninformation density. This paper present \\textbf{Quicksviewer}, an LMM with new\nperceiving paradigm that partitions a video of nonuniform density into varying\ncubes using Gumbel Softmax, followed by a unified resampling for each cube to\nachieve efficient video understanding. This simple and intuitive approach\ndynamically compress video online based on its temporal density, significantly\nreducing spatiotemporal redundancy (overall 45$\\times$ compression rate), while\nenabling efficient training with large receptive field. We train the model from\na language backbone through three progressive stages, each incorporating\nlengthy videos on average of 420s/1fps thanks to the perceiving efficiency.\nWith only 0.8M total video-text samples for training, our model outperforms the\ndirect baseline employing a fixed partitioning strategy by a maximum of 8.72 in\naccuracy, demonstrating the effectiveness in performance. On Video-MME,\nQuicksviewer achieves SOTA under modest sequence lengths using just up to 5\\%\nof tokens per frame required by baselines. With this paradigm, scaling up the\nnumber of input frames reveals a clear power law of the model capabilities. It\nis also empirically verified that the segments generated by the cubing network\ncan help for analyzing continuous events in videos.", "AI": {"tldr": "Quicksviewer\u662f\u4e00\u79cd\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\uff08LMM\uff09\uff0c\u901a\u8fc7\u52a8\u6001\u5206\u533a\u548c\u7edf\u4e00\u91cd\u91c7\u6837\u89c6\u9891\u5e27\uff0c\u663e\u8457\u63d0\u5347\u89c6\u9891\u7406\u89e3\u7684\u6548\u7387\uff0c\u51cf\u5c11\u65f6\u7a7a\u5197\u4f59\u3002", "motivation": "\u4f20\u7edfLMM\u5bf9\u89c6\u9891\u5e27\u7684\u5747\u5300\u611f\u77e5\u5bfc\u81f4\u8ba1\u7b97\u6548\u7387\u4f4e\u4e0b\uff0c\u5c24\u5176\u662f\u5bf9\u4fe1\u606f\u5bc6\u5ea6\u4e0d\u5747\u7684\u89c6\u9891\u3002", "method": "\u4f7f\u7528Gumbel Softmax\u5c06\u89c6\u9891\u5206\u533a\u4e3a\u975e\u5747\u5300\u5bc6\u5ea6\u5757\uff0c\u5e76\u5bf9\u6bcf\u4e2a\u5757\u8fdb\u884c\u7edf\u4e00\u91cd\u91c7\u6837\uff0c\u5b9e\u73b0\u52a8\u6001\u538b\u7f29\u3002", "result": "\u6a21\u578b\u5728\u8bad\u7ec3\u6570\u636e\u91cf\u8f83\u5c11\uff080.8M\u6837\u672c\uff09\u7684\u60c5\u51b5\u4e0b\uff0c\u6027\u80fd\u8d85\u8d8a\u57fa\u7ebf\uff08\u6700\u9ad8\u63d0\u53478.72%\u51c6\u786e\u7387\uff09\uff0c\u5e76\u5728Video-MME\u4e0a\u8fbe\u5230SOTA\u3002", "conclusion": "Quicksviewer\u901a\u8fc7\u52a8\u6001\u5206\u533a\u548c\u9ad8\u6548\u611f\u77e5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u7406\u89e3\u7684\u6548\u7387\u548c\u6027\u80fd\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u6a21\u578b\u80fd\u529b\u4e0e\u8f93\u5165\u5e27\u6570\u7684\u5e42\u5f8b\u5173\u7cfb\u3002"}}
{"id": "2504.15244", "pdf": "https://arxiv.org/pdf/2504.15244", "abs": "https://arxiv.org/abs/2504.15244", "authors": ["Ilias Diakonikolas", "Daniel M. Kane", "Lisheng Ren"], "title": "Faster Algorithms for Agnostically Learning Disjunctions and their Implications", "categories": ["cs.LG", "cs.DS", "stat.ML"], "comment": null, "summary": "We study the algorithmic task of learning Boolean disjunctions in the\ndistribution-free agnostic PAC model. The best known agnostic learner for the\nclass of disjunctions over $\\{0, 1\\}^n$ is the $L_1$-polynomial regression\nalgorithm, achieving complexity $2^{\\tilde{O}(n^{1/2})}$. This complexity bound\nis known to be nearly best possible within the class of Correlational\nStatistical Query (CSQ) algorithms. In this work, we develop an agnostic\nlearner for this concept class with complexity $2^{\\tilde{O}(n^{1/3})}$. Our\nalgorithm can be implemented in the Statistical Query (SQ) model, providing the\nfirst separation between the SQ and CSQ models in distribution-free agnostic\nlearning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u5206\u5e03\u81ea\u7531\u4e0d\u53ef\u77e5PAC\u6a21\u578b\u4e2d\u5b66\u4e60\u5e03\u5c14\u6790\u53d6\u7684\u65b0\u7b97\u6cd5\uff0c\u590d\u6742\u5ea6\u4e3a2^(O(n^(1/3)))\uff0c\u4f18\u4e8e\u73b0\u6709\u6700\u4f73\u7b97\u6cd5\u76842^(O(n^(1/2)))\u3002", "motivation": "\u7814\u7a76\u5e03\u5c14\u6790\u53d6\u5728\u4e0d\u53ef\u77e5PAC\u6a21\u578b\u4e2d\u7684\u9ad8\u6548\u5b66\u4e60\u7b97\u6cd5\uff0c\u7a81\u7834\u73b0\u6709CSQ\u7b97\u6cd5\u7684\u590d\u6742\u5ea6\u9650\u5236\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u65b0\u7684\u4e0d\u53ef\u77e5\u5b66\u4e60\u7b97\u6cd5\uff0c\u53ef\u5728\u7edf\u8ba1\u67e5\u8be2\uff08SQ\uff09\u6a21\u578b\u4e2d\u5b9e\u73b0\uff0c\u590d\u6742\u5ea6\u4e3a2^(O(n^(1/3)))\u3002", "result": "\u7b97\u6cd5\u590d\u6742\u5ea6\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u4f73CSQ\u7b97\u6cd5\uff0c\u5e76\u9996\u6b21\u5728\u5206\u5e03\u81ea\u7531\u4e0d\u53ef\u77e5\u5b66\u4e60\u4e2d\u5b9e\u73b0\u4e86SQ\u4e0eCSQ\u6a21\u578b\u7684\u5206\u79bb\u3002", "conclusion": "\u65b0\u7b97\u6cd5\u5728\u590d\u6742\u5ea6\u548c\u6a21\u578b\u5206\u79bb\u65b9\u9762\u53d6\u5f97\u4e86\u7a81\u7834\uff0c\u4e3a\u5e03\u5c14\u6790\u53d6\u5b66\u4e60\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.14071", "pdf": "https://arxiv.org/pdf/2504.14071", "abs": "https://arxiv.org/abs/2504.14071", "authors": ["Renaud Bougueng Tchemeube", "Jeff Ens", "Cale Plut", "Philippe Pasquier", "Maryam Safi", "Yvan Grabit", "Jean-Baptiste Rolland"], "title": "Evaluating Human-AI Interaction via Usability, User Experience and Acceptance Measures for MMM-C: A Creative AI System for Music Composition", "categories": ["cs.HC", "cs.AI", "cs.LG", "cs.SD"], "comment": "10 pages, 6 figures, 1 table, first published at the 32nd\n  International Joint Conference on Artificial Intelligence (IJCAI 2023),\n  Macao, China", "summary": "With the rise of artificial intelligence (AI), there has been increasing\ninterest in human-AI co-creation in a variety of artistic domains including\nmusic as AI-driven systems are frequently able to generate human-competitive\nartifacts. Now, the implications of such systems for musical practice are being\ninvestigated. We report on a thorough evaluation of the user adoption of the\nMulti-Track Music Machine (MMM) as a co-creative AI tool for music composers.\nTo do this, we integrate MMM into Cubase, a popular Digital Audio Workstation\n(DAW) by Steinberg, by producing a \"1-parameter\" plugin interface named\nMMM-Cubase (MMM-C), which enables human-AI co-composition. We contribute a\nmethodological assemblage as a 3-part mixed method study measuring usability,\nuser experience and technology acceptance of the system across two groups of\nexpert-level composers: hobbyists and professionals. Results show positive\nusability and acceptance scores. Users report experiences of novelty, surprise\nand ease of use from using the system, and limitations on controllability and\npredictability of the interface when generating music. Findings indicate no\nsignificant difference between the two user groups.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86AI\u5de5\u5177MMM-Cubase\u5728\u97f3\u4e50\u521b\u4f5c\u4e2d\u7684\u7528\u6237\u63a5\u53d7\u5ea6\uff0c\u7ed3\u679c\u663e\u793a\u5176\u6613\u7528\u6027\u548c\u63a5\u53d7\u5ea6\u8f83\u9ad8\uff0c\u4f46\u53ef\u63a7\u6027\u548c\u9884\u6d4b\u6027\u5b58\u5728\u5c40\u9650\u3002", "motivation": "\u968f\u7740AI\u5728\u827a\u672f\u9886\u57df\u7684\u5e94\u7528\u589e\u591a\uff0c\u7814\u7a76AI\u4e0e\u4eba\u7c7b\u5728\u97f3\u4e50\u521b\u4f5c\u4e2d\u7684\u534f\u4f5c\u5b9e\u8df5\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002", "method": "\u901a\u8fc7\u5c06MMM\u96c6\u6210\u5230Cubase\u4e2d\uff0c\u5f00\u53d1MMM-C\u63d2\u4ef6\uff0c\u91c7\u7528\u6df7\u5408\u65b9\u6cd5\u7814\u7a76\u6d4b\u91cf\u5176\u53ef\u7528\u6027\u3001\u7528\u6237\u4f53\u9a8c\u548c\u6280\u672f\u63a5\u53d7\u5ea6\u3002", "result": "\u7528\u6237\u5bf9\u7cfb\u7edf\u7684\u6613\u7528\u6027\u548c\u63a5\u53d7\u5ea6\u8bc4\u5206\u8f83\u9ad8\uff0c\u4f46\u5bf9\u5176\u53ef\u63a7\u6027\u548c\u9884\u6d4b\u6027\u8868\u793a\u62c5\u5fe7\u3002\u4e24\u7ec4\u7528\u6237\u65e0\u663e\u8457\u5dee\u5f02\u3002", "conclusion": "MMM-Cubase\u4f5c\u4e3aAI\u534f\u4f5c\u5de5\u5177\u5728\u97f3\u4e50\u521b\u4f5c\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u9700\u6539\u8fdb\u53ef\u63a7\u6027\u548c\u9884\u6d4b\u6027\u3002"}}
{"id": "2504.15280", "pdf": "https://arxiv.org/pdf/2504.15280", "abs": "https://arxiv.org/abs/2504.15280", "authors": ["Chun-Hsiao Yeh", "Chenyu Wang", "Shengbang Tong", "Ta-Ying Cheng", "Rouyu Wang", "Tianzhe Chu", "Yuexiang Zhai", "Yubei Chen", "Shenghua Gao", "Yi Ma"], "title": "Seeing from Another Perspective: Evaluating Multi-View Understanding in MLLMs", "categories": ["cs.CV", "cs.CL"], "comment": "Project page: https://danielchyeh.github.io/All-Angles-Bench/", "summary": "Multi-view understanding, the ability to reconcile visual information across\ndiverse viewpoints for effective navigation, manipulation, and 3D scene\ncomprehension, is a fundamental challenge in Multi-Modal Large Language Models\n(MLLMs) to be used as embodied agents. While recent MLLMs have shown impressive\nadvances in high-level reasoning and planning, they frequently fall short when\nconfronted with multi-view geometric consistency and cross-view correspondence.\nTo comprehensively evaluate the challenges of MLLMs in multi-view scene\nreasoning, we propose All-Angles Bench, a benchmark of over 2,100 human\ncarefully annotated multi-view question-answer pairs across 90 diverse\nreal-world scenes. Our six tasks (counting, attribute identification, relative\ndistance, relative direction, object manipulation, and camera pose estimation)\nspecifically test model's geometric correspondence and the capacity to align\ninformation consistently across views. Our extensive experiments, benchmark on\n27 representative MLLMs including Gemini-2.0-Flash, Claude-3.7-Sonnet, and\nGPT-4o against human evaluators reveals a substantial performance gap,\nindicating that current MLLMs remain far from human-level proficiency. Through\nin-depth analysis, we show that MLLMs are particularly underperforming under\ntwo aspects: (1) cross-view correspondence for partially occluded views and (2)\nestablishing the coarse camera poses. These findings highlight the necessity of\ndomain-specific refinements or modules that embed stronger multi-view\nawareness. We believe that our All-Angles Bench offers valuable insights and\ncontribute to bridging the gap between MLLMs and human-level multi-view\nunderstanding. The project and benchmark are publicly available at\nhttps://danielchyeh.github.io/All-Angles-Bench/.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86All-Angles Bench\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u591a\u89c6\u89d2\u573a\u666f\u7406\u89e3\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5f53\u524d\u6a21\u578b\u5728\u8de8\u89c6\u89d2\u4e00\u81f4\u6027\u548c\u51e0\u4f55\u63a8\u7406\u65b9\u9762\u4ecd\u8fdc\u4f4e\u4e8e\u4eba\u7c7b\u6c34\u5e73\u3002", "motivation": "\u591a\u89c6\u89d2\u7406\u89e3\u662fMLLMs\u4f5c\u4e3a\u5177\u8eab\u4ee3\u7406\u7684\u5173\u952e\u80fd\u529b\uff0c\u4f46\u73b0\u6709\u6a21\u578b\u5728\u9ad8\u5c42\u6b21\u63a8\u7406\u548c\u89c4\u5212\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5374\u5728\u591a\u89c6\u89d2\u51e0\u4f55\u4e00\u81f4\u6027\u548c\u8de8\u89c6\u89d2\u5bf9\u5e94\u4e0a\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u901a\u8fc7\u6784\u5efa\u5305\u542b2,100\u4e2a\u4eba\u5de5\u6807\u6ce8\u591a\u89c6\u89d2\u95ee\u7b54\u5bf9\u7684\u57fa\u51c6\uff08All-Angles Bench\uff09\uff0c\u8bbe\u8ba1\u4e86\u516d\u9879\u4efb\u52a1\u6d4b\u8bd5\u6a21\u578b\u7684\u51e0\u4f55\u5bf9\u5e94\u548c\u8de8\u89c6\u89d2\u4fe1\u606f\u5bf9\u9f50\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c27\u79cd\u4ee3\u8868\u6027MLLMs\uff08\u5982Gemini-2.0-Flash\u3001Claude-3.7-Sonnet\u548cGPT-4o\uff09\u5728\u8de8\u89c6\u89d2\u5bf9\u5e94\u548c\u76f8\u673a\u59ff\u6001\u4f30\u8ba1\u65b9\u9762\u8868\u73b0\u663e\u8457\u4f4e\u4e8e\u4eba\u7c7b\u6c34\u5e73\u3002", "conclusion": "\u5f53\u524dMLLMs\u5728\u591a\u89c6\u89d2\u7406\u89e3\u4e0a\u4ecd\u9700\u6539\u8fdb\uff0c\u7279\u522b\u662f\u9488\u5bf9\u906e\u6321\u89c6\u89d2\u548c\u76f8\u673a\u59ff\u6001\u4f30\u8ba1\u3002All-Angles Bench\u4e3a\u63d0\u5347\u6a21\u578b\u80fd\u529b\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u3002"}}
{"id": "2504.15251", "pdf": "https://arxiv.org/pdf/2504.15251", "abs": "https://arxiv.org/abs/2504.15251", "authors": ["Ilias Diakonikolas", "Daniel M. Kane", "Sushrut Karmalkar", "Jasper C. H. Lee", "Thanasis Pittas"], "title": "On Learning Parallel Pancakes with Mostly Uniform Weights", "categories": ["cs.LG", "cs.DS", "math.ST", "stat.ML", "stat.TH"], "comment": null, "summary": "We study the complexity of learning $k$-mixtures of Gaussians ($k$-GMMs) on\n$\\mathbb{R}^d$. This task is known to have complexity $d^{\\Omega(k)}$ in full\ngenerality. To circumvent this exponential lower bound on the number of\ncomponents, research has focused on learning families of GMMs satisfying\nadditional structural properties. A natural assumption posits that the\ncomponent weights are not exponentially small and that the components have the\nsame unknown covariance. Recent work gave a $d^{O(\\log(1/w_{\\min}))}$-time\nalgorithm for this class of GMMs, where $w_{\\min}$ is the minimum weight. Our\nfirst main result is a Statistical Query (SQ) lower bound showing that this\nquasi-polynomial upper bound is essentially best possible, even for the special\ncase of uniform weights. Specifically, we show that it is SQ-hard to\ndistinguish between such a mixture and the standard Gaussian. We further\nexplore how the distribution of weights affects the complexity of this task.\nOur second main result is a quasi-polynomial upper bound for the aforementioned\ntesting task when most of the weights are uniform while a small fraction of the\nweights are potentially arbitrary.", "AI": {"tldr": "\u7814\u7a76\u5b66\u4e60$k$-\u9ad8\u65af\u6df7\u5408\u6a21\u578b\uff08$k$-GMMs\uff09\u7684\u590d\u6742\u5ea6\uff0c\u53d1\u73b0\u5176\u590d\u6742\u5ea6\u4e3a$d^{\\Omega(k)}$\u3002\u901a\u8fc7\u5047\u8bbe\u6743\u91cd\u975e\u6307\u6570\u5c0f\u4e14\u534f\u65b9\u5dee\u76f8\u540c\uff0c\u5df2\u6709\u7b97\u6cd5\u65f6\u95f4\u4e3a$d^{O(\\log(1/w_{\\min}))}$\u3002\u672c\u6587\u8bc1\u660e\u6b64\u4e0a\u754c\u51e0\u4e4e\u6700\u4f18\uff0c\u4e14\u8fdb\u4e00\u6b65\u63a2\u8ba8\u6743\u91cd\u5206\u5e03\u5bf9\u590d\u6742\u5ea6\u7684\u5f71\u54cd\u3002", "motivation": "\u7814\u7a76\u9ad8\u65af\u6df7\u5408\u6a21\u578b\u5b66\u4e60\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u7279\u522b\u662f\u5728\u7ed3\u6784\u5047\u8bbe\u4e0b\uff0c\u4ee5\u514b\u670d\u6307\u6570\u7ea7\u590d\u6742\u5ea6\u7684\u9650\u5236\u3002", "method": "\u901a\u8fc7\u7edf\u8ba1\u67e5\u8be2\uff08SQ\uff09\u4e0b\u754c\u8bc1\u660e\u7b97\u6cd5\u7684\u51c6\u591a\u9879\u5f0f\u4e0a\u754c\u51e0\u4e4e\u6700\u4f18\uff0c\u5e76\u5206\u6790\u6743\u91cd\u5206\u5e03\u5bf9\u6d4b\u8bd5\u4efb\u52a1\u7684\u5f71\u54cd\u3002", "result": "\u8bc1\u660e\u5728\u5747\u5300\u6743\u91cd\u4e0b\uff0c\u51c6\u591a\u9879\u5f0f\u4e0a\u754c\u51e0\u4e4e\u6700\u4f18\uff1b\u540c\u65f6\u63d0\u51fa\u5728\u5927\u90e8\u5206\u6743\u91cd\u5747\u5300\u65f6\uff0c\u6d4b\u8bd5\u4efb\u52a1\u7684\u51c6\u591a\u9879\u5f0f\u4e0a\u754c\u3002", "conclusion": "\u9ad8\u65af\u6df7\u5408\u6a21\u578b\u5b66\u4e60\u7684\u590d\u6742\u5ea6\u5728\u7ed3\u6784\u5047\u8bbe\u4e0b\u4ecd\u5177\u6709\u6311\u6218\u6027\uff0c\u4f46\u901a\u8fc7\u6743\u91cd\u5206\u5e03\u7684\u4f18\u5316\u53ef\u4ee5\u6539\u5584\u7b97\u6cd5\u6548\u7387\u3002"}}
{"id": "2504.13850", "pdf": "https://arxiv.org/pdf/2504.13850", "abs": "https://arxiv.org/abs/2504.13850", "authors": ["Zihan Zhang", "Leon Wong", "Blesson Varghese"], "title": "Resource Utilization Optimized Federated Learning", "categories": ["cs.DC", "cs.LG"], "comment": null, "summary": "Federated learning (FL) systems facilitate distributed machine learning\nacross a server and multiple devices. However, FL systems have low resource\nutilization limiting their practical use in the real world. This inefficiency\nprimarily arises from two types of idle time: (i) task dependency between the\nserver and devices, and (ii) stragglers among heterogeneous devices. This paper\nintroduces FedOptima, a resource-optimized FL system designed to simultaneously\nminimize both types of idle time; existing systems do not eliminate or reduce\nboth at the same time. FedOptima offloads the training of certain layers of a\nneural network from a device to server using three innovations. First, devices\noperate independently of each other using asynchronous aggregation to eliminate\nstraggler effects, and independently of the server by utilizing auxiliary\nnetworks to minimize idle time caused by task dependency. Second, the server\nperforms centralized training using a task scheduler that ensures balanced\ncontributions from all devices, improving model accuracy. Third, an efficient\nmemory management mechanism on the server increases scalability of the number\nof participating devices. Four state-of-the-art offloading-based and\nasynchronous FL methods are chosen as baselines. Experimental results show that\ncompared to the best results of the baselines on convolutional neural networks\nand transformers on multiple lab-based testbeds, FedOptima (i) achieves higher\nor comparable accuracy, (ii) accelerates training by 1.9x to 21.8x, (iii)\nreduces server and device idle time by up to 93.9% and 81.8%, respectively, and\n(iv) increases throughput by 1.1x to 2.0x.", "AI": {"tldr": "FedOptima\u662f\u4e00\u79cd\u4f18\u5316\u7684\u8054\u90a6\u5b66\u4e60\u7cfb\u7edf\uff0c\u901a\u8fc7\u5f02\u6b65\u805a\u5408\u548c\u4efb\u52a1\u8c03\u5ea6\u51cf\u5c11\u8bbe\u5907\u548c\u670d\u52a1\u5668\u7684\u7a7a\u95f2\u65f6\u95f4\uff0c\u63d0\u5347\u8bad\u7ec3\u6548\u7387\u548c\u6a21\u578b\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u8054\u90a6\u5b66\u4e60\u7cfb\u7edf\u56e0\u4efb\u52a1\u4f9d\u8d56\u548c\u8bbe\u5907\u5f02\u6784\u6027\u5bfc\u81f4\u8d44\u6e90\u5229\u7528\u7387\u4f4e\uff0cFedOptima\u65e8\u5728\u540c\u65f6\u89e3\u51b3\u8fd9\u4e24\u79cd\u7a7a\u95f2\u65f6\u95f4\u95ee\u9898\u3002", "method": "FedOptima\u901a\u8fc7\u8bbe\u5907\u72ec\u7acb\u5f02\u6b65\u805a\u5408\u3001\u670d\u52a1\u5668\u4efb\u52a1\u8c03\u5ea6\u548c\u9ad8\u6548\u5185\u5b58\u7ba1\u7406\uff0c\u4f18\u5316\u8d44\u6e90\u5229\u7528\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cFedOptima\u5728\u51c6\u786e\u6027\u548c\u8bad\u7ec3\u901f\u5ea6\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u663e\u8457\u51cf\u5c11\u7a7a\u95f2\u65f6\u95f4\u5e76\u63d0\u5347\u541e\u5410\u91cf\u3002", "conclusion": "FedOptima\u6709\u6548\u89e3\u51b3\u4e86\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u8d44\u6e90\u5229\u7528\u7387\u95ee\u9898\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2504.13873", "pdf": "https://arxiv.org/pdf/2504.13873", "abs": "https://arxiv.org/abs/2504.13873", "authors": ["Zehan Li", "Jinzhi Deng", "Haibing Ma", "Chi Zhang", "Dan Xiao"], "title": "Translating Multimodal AI into Real-World Inspection: TEMAI Evaluation Framework and Pathways for Implementation", "categories": ["cs.HC", "cs.LG"], "comment": null, "summary": "This paper introduces the Translational Evaluation of Multimodal AI for\nInspection (TEMAI) framework, bridging multimodal AI capabilities with\nindustrial inspection implementation. Adapting translational research\nprinciples from healthcare to industrial contexts, TEMAI establishes three core\ndimensions: Capability (technical feasibility), Adoption (organizational\nreadiness), and Utility (value realization). The framework demonstrates that\ntechnical capability alone yields limited value without corresponding adoption\nmechanisms. TEMAI incorporates specialized metrics including the Value Density\nCoefficient and structured implementation pathways. Empirical validation\nthrough retail and photovoltaic inspection implementations revealed significant\ndifferences in value realization patterns despite similar capability reduction\nrates, confirming the framework's effectiveness across diverse industrial\nsectors while highlighting the importance of industry-specific adaptation\nstrategies.", "AI": {"tldr": "TEMAI\u6846\u67b6\u5c06\u591a\u6a21\u6001AI\u4e0e\u5de5\u4e1a\u68c0\u6d4b\u7ed3\u5408\uff0c\u5f3a\u8c03\u6280\u672f\u80fd\u529b\u3001\u7ec4\u7ec7\u51c6\u5907\u548c\u4ef7\u503c\u5b9e\u73b0\u4e09\u65b9\u9762\uff0c\u9a8c\u8bc1\u4e86\u884c\u4e1a\u7279\u5b9a\u9002\u5e94\u7b56\u7565\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u5c06\u533b\u7597\u9886\u57df\u7684\u8f6c\u5316\u7814\u7a76\u539f\u5219\u5e94\u7528\u4e8e\u5de5\u4e1a\u68c0\u6d4b\uff0c\u89e3\u51b3\u6280\u672f\u80fd\u529b\u4e0e\u7ec4\u7ec7\u91c7\u7eb3\u8131\u8282\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faTEMAI\u6846\u67b6\uff0c\u5305\u542b\u80fd\u529b\u3001\u91c7\u7eb3\u548c\u6548\u7528\u4e09\u4e2a\u7ef4\u5ea6\uff0c\u5e76\u5f15\u5165\u4ef7\u503c\u5bc6\u5ea6\u7cfb\u6570\u548c\u7ed3\u6784\u5316\u5b9e\u65bd\u8def\u5f84\u3002", "result": "\u5b9e\u8bc1\u9a8c\u8bc1\u663e\u793a\uff0c\u4e0d\u540c\u884c\u4e1a\u7684\u4ef7\u503c\u5b9e\u73b0\u6a21\u5f0f\u5dee\u5f02\u663e\u8457\uff0c\u6846\u67b6\u5728\u591a\u884c\u4e1a\u4e2d\u6709\u6548\u3002", "conclusion": "TEMAI\u6846\u67b6\u5728\u5de5\u4e1a\u68c0\u6d4b\u4e2d\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\uff0c\u4f46\u9700\u9488\u5bf9\u884c\u4e1a\u7279\u70b9\u8c03\u6574\u7b56\u7565\u3002"}}
{"id": "2504.14100", "pdf": "https://arxiv.org/pdf/2504.14100", "abs": "https://arxiv.org/abs/2504.14100", "authors": ["Ahmed Aboulfotouh", "Elsayed Mohammed", "Hatem Abou-Zeid"], "title": "6G WavesFM: A Foundation Model for Sensing, Communication, and Localization", "categories": ["eess.SP", "cs.AI", "cs.LG"], "comment": null, "summary": "This paper introduces WavesFM, a novel Wireless Foundation Model (WFM)\nframework, capable of supporting a wide array of communication, sensing, and\nlocalization tasks. Our proposed architecture combines a shared Vision\nTransformer (ViT) backbone with task-specific multi-layer perceptron (MLP)\nheads and incorporates Low-Rank Adaptation (LoRA) for parameter-efficient\nfine-tuning. This design promotes full parameter sharing across tasks,\nsignificantly reducing the computational and memory footprint without\nsacrificing performance. The model processes both image-like wireless\nmodalities, such as spectrograms and channel state information (CSI), and\nin-phase and quadrature (IQ) signals arranged as orthogonal frequency-division\nmultiplexing (OFDM) resource grids. We demonstrate the strong generalization\ncapabilities of WavesFM through extensive experiments on four downstream tasks:\nFifth Generation New Radio (5G NR) positioning; multiple-input multiple-output\nOFDM (MIMO-OFDM) channel estimation; human activity sensing; and\nradio-frequency (RF) signal classification. Compared to supervised baselines\ntrained individually, our approach achieves superior performance while sharing\n80% of its parameters across tasks. Furthermore, we show that pretraining on\ndomain-relevant data not only boosts performance but also accelerates\nconvergence, reducing training time by up to 5x. These results demonstrate that\nour unified WFM can support diverse tasks and deliver significant gains in both\nperformance and efficiency, highlighting the transformative potential of\nfoundation models to drive AI-native paradigms in future sixth-generation (6G)\nnetworks.", "AI": {"tldr": "WavesFM\u662f\u4e00\u4e2a\u65b0\u578b\u65e0\u7ebf\u57fa\u7840\u6a21\u578b\uff08WFM\uff09\uff0c\u652f\u6301\u591a\u79cd\u901a\u4fe1\u3001\u611f\u77e5\u548c\u5b9a\u4f4d\u4efb\u52a1\uff0c\u901a\u8fc7\u5171\u4eabViT\u4e3b\u5e72\u548c\u4efb\u52a1\u7279\u5b9aMLP\u5934\u5b9e\u73b0\u9ad8\u6548\u53c2\u6570\u5171\u4eab\uff0c\u6027\u80fd\u4f18\u8d8a\u4e14\u8ba1\u7b97\u8d44\u6e90\u8282\u7701\u3002", "motivation": "\u63a8\u52a8AI\u539f\u751f\u8303\u5f0f\u5728\u672a\u67656G\u7f51\u7edc\u4e2d\u7684\u5e94\u7528\uff0c\u901a\u8fc7\u7edf\u4e00\u7684\u57fa\u7840\u6a21\u578b\u652f\u6301\u591a\u6837\u5316\u4efb\u52a1\uff0c\u63d0\u5347\u6027\u80fd\u548c\u6548\u7387\u3002", "method": "\u7ed3\u5408\u5171\u4eabViT\u4e3b\u5e72\u548c\u4efb\u52a1\u7279\u5b9aMLP\u5934\uff0c\u91c7\u7528LoRA\u8fdb\u884c\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff0c\u5904\u7406\u56fe\u50cf\u7c7b\u65e0\u7ebf\u6a21\u6001\u548cIQ\u4fe1\u53f7\u3002", "result": "\u57285G\u5b9a\u4f4d\u3001MIMO-OFDM\u4fe1\u9053\u4f30\u8ba1\u3001\u4eba\u7c7b\u6d3b\u52a8\u611f\u77e5\u548cRF\u4fe1\u53f7\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u5355\u72ec\u8bad\u7ec3\u7684\u57fa\u7ebf\u6a21\u578b\uff0c\u53c2\u6570\u5171\u4eab\u7387\u8fbe80%\uff0c\u8bad\u7ec3\u65f6\u95f4\u51cf\u5c115\u500d\u3002", "conclusion": "WavesFM\u5c55\u793a\u4e86\u57fa\u7840\u6a21\u578b\u5728\u591a\u6837\u5316\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\uff0c\u4e3a6G\u7f51\u7edc\u7684AI\u539f\u751f\u8303\u5f0f\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.13883", "pdf": "https://arxiv.org/pdf/2504.13883", "abs": "https://arxiv.org/abs/2504.13883", "authors": ["Shayla Sharmin", "Roghayeh Leila Barmaki"], "title": "Hybrid Deep Learning Model to Estimate Cognitive Effort from fNIRS Signals in Educational Game Playing", "categories": ["cs.HC", "cs.LG"], "comment": null, "summary": "This study estimates cognitive effort (CE) based on functional near-infrared\nspectroscopy (fNIRS) data and performance scores using a hybrid deep learning\nmodel. The estimation of CE enables educators to modify material to enhance\nlearning effectiveness and student engagement. Relative neural efficiency (RNE)\nand relative neural involvement (RNI) are two metrics that have been used to\nrepresent CE. To estimate RNE and RNI we need hemodynamic response in the brain\nand the performance score of a task.We collected oxygenated hemoglobin ($\\Delta\n\\mathrm{HbO}$). Sixteen participants answered 16 questions in a unity-based\neducational game, each with a 30-second response time. We used deep learning\nmodels to predict the performance score and estimate RNE and RNI to understand\nCE. The study compares traditional machine learning techniques with deep\nlearning models such as CNN, LSTM, BiLSTM, and a hybrid CNN-GRU to determine\nwhich approach provides better accuracy in predicting performance scores. The\nresult shows that the hybrid CNN-GRU gives better performance with 78.36\\%\ntraining accuracy and 73.08\\% test accuracy than other models. We performed\nXGBoost on the extracted GRU feature and got the highest accuracy (69.23\\%).\nThis suggests that the features learned from this hybrid model generalize\nbetter even in traditional machine learning algorithms. We used the $\\Delta\n\\mathrm{HbO}$ and predicted score to calculate RNE and RNI to observe cognitive\neffort in our four test cases. Our result shows that even with moderate\naccuracy, the predicted RNE and RNI closely follows the actual trends. we also\nobserved that when participants were in a state of high CE, introducing rest\nled decrease of CE. These findings can be helpful to design and improve\nlearning environments and provide valuable insights in learning materials.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u6df7\u5408\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7ed3\u5408fNIRS\u6570\u636e\u548c\u8868\u73b0\u5206\u6570\u4f30\u8ba1\u8ba4\u77e5\u52aa\u529b\uff08CE\uff09\uff0c\u4ee5\u4f18\u5316\u5b66\u4e60\u6548\u679c\u548c\u5b66\u751f\u53c2\u4e0e\u5ea6\u3002\u7ed3\u679c\u663e\u793aCNN-GRU\u6df7\u5408\u6a21\u578b\u8868\u73b0\u6700\u4f73\uff0c\u4e14\u63d0\u53d6\u7684\u7279\u5f81\u5728\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u4e2d\u4e5f\u6709\u8f83\u597d\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u901a\u8fc7\u4f30\u8ba1CE\u5e2e\u52a9\u6559\u80b2\u8005\u8c03\u6574\u5b66\u4e60\u6750\u6599\uff0c\u63d0\u5347\u5b66\u4e60\u6548\u679c\u548c\u5b66\u751f\u53c2\u4e0e\u5ea6\u3002", "method": "\u4f7f\u7528fNIRS\u6570\u636e\uff08\u0394HbO\uff09\u548c\u8868\u73b0\u5206\u6570\uff0c\u7ed3\u5408CNN\u3001LSTM\u3001BiLSTM\u548cCNN-GRU\u6df7\u5408\u6a21\u578b\u9884\u6d4b\u8868\u73b0\u5206\u6570\u5e76\u8ba1\u7b97RNE\u548cRNI\u3002", "result": "CNN-GRU\u6df7\u5408\u6a21\u578b\u8bad\u7ec3\u51c6\u786e\u738778.36%\uff0c\u6d4b\u8bd5\u51c6\u786e\u738773.08%\uff0c\u63d0\u53d6\u7684\u7279\u5f81\u5728XGBoost\u4e2d\u8fbe\u523069.23%\u51c6\u786e\u7387\u3002\u9884\u6d4b\u7684RNE\u548cRNI\u4e0e\u5b9e\u9645\u8d8b\u52bf\u63a5\u8fd1\u3002", "conclusion": "\u6df7\u5408\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u80fd\u6709\u6548\u4f30\u8ba1CE\uff0c\u4e3a\u5b66\u4e60\u73af\u5883\u8bbe\u8ba1\u548c\u6750\u6599\u6539\u8fdb\u63d0\u4f9b\u6709\u4ef7\u503c\u89c1\u89e3\u3002"}}
{"id": "2504.14103", "pdf": "https://arxiv.org/pdf/2504.14103", "abs": "https://arxiv.org/abs/2504.14103", "authors": ["Merve Atasever", "Ali Okhovat", "Azhang Nazaripouya", "John Nisbet", "Omer Kurkutlu", "Jyotirmoy V. Deshmukh", "Yasemin Ozkan Aydin"], "title": "Coordinating Spinal and Limb Dynamics for Enhanced Sprawling Robot Mobility", "categories": ["cs.RO", "cs.AI"], "comment": "The manuscript has been accepted for presentation at the Mechanical\n  Intelligence in Robotics workshop at ICRA 2025", "summary": "Among vertebrates, salamanders, with their unique ability to transition\nbetween walking and swimming gaits, highlight the role of spinal mobility in\nlocomotion. A flexible spine enables undulation of the body through a wavelike\nmotion along the spine, aiding navigation over uneven terrains and obstacles.\nYet environmental uncertainties, such as surface irregularities and variations\nin friction, can significantly disrupt body-limb coordination and cause\ndiscrepancies between predictions from mathematical models and real-world\noutcomes. Addressing this challenge requires the development of sophisticated\ncontrol strategies capable of dynamically adapting to uncertain conditions\nwhile maintaining efficient locomotion. Deep reinforcement learning (DRL)\noffers a promising framework for handling non-deterministic environments and\nenabling robotic systems to adapt effectively and perform robustly under\nchallenging conditions. In this study, we comparatively examine learning-based\ncontrol strategies and biologically inspired gait design methods on a\nsalamander-like robot.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u57fa\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u63a7\u5236\u7b56\u7565\u548c\u751f\u7269\u542f\u53d1\u7684\u6b65\u6001\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u7528\u4e8e\u6a21\u62df\u877e\u8788\u673a\u5668\u4eba\u5728\u4e0d\u786e\u5b9a\u73af\u5883\u4e2d\u7684\u9ad8\u6548\u8fd0\u52a8\u3002", "motivation": "\u877e\u8788\u56e0\u5176\u72ec\u7279\u7684\u884c\u8d70\u548c\u6e38\u6cf3\u6b65\u6001\u8f6c\u6362\u80fd\u529b\uff0c\u7a81\u51fa\u4e86\u810a\u67f1\u7075\u6d3b\u6027\u5728\u8fd0\u52a8\u4e2d\u7684\u4f5c\u7528\u3002\u7136\u800c\uff0c\u73af\u5883\u4e0d\u786e\u5b9a\u6027\u53ef\u80fd\u5bfc\u81f4\u80a2\u4f53\u534f\u8c03\u95ee\u9898\uff0c\u9700\u8981\u52a8\u6001\u9002\u5e94\u7684\u63a7\u5236\u7b56\u7565\u3002", "method": "\u6bd4\u8f83\u4e86\u57fa\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\u7684\u63a7\u5236\u7b56\u7565\u548c\u751f\u7269\u542f\u53d1\u7684\u6b65\u6001\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u5e94\u7528\u4e8e\u877e\u8788\u673a\u5668\u4eba\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0cDRL\u80fd\u591f\u6709\u6548\u5904\u7406\u975e\u786e\u5b9a\u6027\u73af\u5883\uff0c\u4f7f\u673a\u5668\u4eba\u7cfb\u7edf\u5728\u6311\u6218\u6027\u6761\u4ef6\u4e0b\u8868\u73b0\u7a33\u5065\u3002", "conclusion": "\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u4e3a\u673a\u5668\u4eba\u7cfb\u7edf\u5728\u4e0d\u786e\u5b9a\u73af\u5883\u4e2d\u5b9e\u73b0\u9ad8\u6548\u8fd0\u52a8\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.13936", "pdf": "https://arxiv.org/pdf/2504.13936", "abs": "https://arxiv.org/abs/2504.13936", "authors": ["Dezhao Luo", "Bohan Tang", "Kang Li", "Georgios Papoudakis", "Jifei Song", "Shaogang Gong", "Jianye Hao", "Jun Wang", "Kun Shao"], "title": "ViMo: A Generative Visual GUI World Model for App Agent", "categories": ["cs.HC", "cs.LG", "cs.SY", "eess.SY"], "comment": null, "summary": "App agents, which autonomously operate mobile Apps through Graphical User\nInterfaces (GUIs), have gained significant interest in real-world applications.\nYet, they often struggle with long-horizon planning, failing to find the\noptimal actions for complex tasks with longer steps. To address this, world\nmodels are used to predict the next GUI observation based on user actions,\nenabling more effective agent planning. However, existing world models\nprimarily focus on generating only textual descriptions, lacking essential\nvisual details. To fill this gap, we propose ViMo, the first visual world model\ndesigned to generate future App observations as images. For the challenge of\ngenerating text in image patches, where even minor pixel errors can distort\nreadability, we decompose GUI generation into graphic and text content\ngeneration. We propose a novel data representation, the Symbolic Text\nRepresentation~(STR) to overlay text content with symbolic placeholders while\npreserving graphics. With this design, ViMo employs a STR Predictor to predict\nfuture GUIs' graphics and a GUI-text Predictor for generating the corresponding\ntext. Moreover, we deploy ViMo to enhance agent-focused tasks by predicting the\noutcome of different action options. Experiments show ViMo's ability to\ngenerate visually plausible and functionally effective GUIs that enable App\nagents to make more informed decisions.", "AI": {"tldr": "ViMo\u662f\u4e00\u79cd\u89c6\u89c9\u4e16\u754c\u6a21\u578b\uff0c\u901a\u8fc7\u751f\u6210\u56fe\u50cf\u9884\u6d4b\u672a\u6765App\u754c\u9762\uff0c\u5e2e\u52a9App\u4ee3\u7406\u66f4\u6709\u6548\u5730\u89c4\u5212\u590d\u6742\u4efb\u52a1\u3002", "motivation": "\u73b0\u6709\u4e16\u754c\u6a21\u578b\u4ec5\u751f\u6210\u6587\u672c\u63cf\u8ff0\uff0c\u7f3a\u4e4f\u89c6\u89c9\u7ec6\u8282\uff0c\u9650\u5236\u4e86App\u4ee3\u7406\u7684\u957f\u65f6\u89c4\u5212\u80fd\u529b\u3002", "method": "ViMo\u5c06GUI\u751f\u6210\u5206\u89e3\u4e3a\u56fe\u5f62\u548c\u6587\u672c\u5185\u5bb9\u751f\u6210\uff0c\u4f7f\u7528\u7b26\u53f7\u6587\u672c\u8868\u793a\uff08STR\uff09\u5904\u7406\u6587\u672c\u5185\u5bb9\uff0c\u5e76\u5206\u522b\u9884\u6d4b\u56fe\u5f62\u548c\u6587\u672c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cViMo\u80fd\u751f\u6210\u89c6\u89c9\u5408\u7406\u4e14\u529f\u80fd\u6709\u6548\u7684GUI\uff0c\u5e2e\u52a9\u4ee3\u7406\u505a\u51fa\u66f4\u660e\u667a\u7684\u51b3\u7b56\u3002", "conclusion": "ViMo\u586b\u8865\u4e86\u89c6\u89c9\u4e16\u754c\u6a21\u578b\u7684\u7a7a\u767d\uff0c\u663e\u8457\u63d0\u5347\u4e86App\u4ee3\u7406\u7684\u89c4\u5212\u80fd\u529b\u3002"}}
{"id": "2504.14105", "pdf": "https://arxiv.org/pdf/2504.14105", "abs": "https://arxiv.org/abs/2504.14105", "authors": ["Qazi Mamunur Rashid", "Erin van Liemt", "Tiffany Shih", "Amber Ebinama", "Karla Barrios Ramos", "Madhurima Maji", "Aishwarya Verma", "Charu Kalia", "Jamila Smith-Loud", "Joyce Nakatumba-Nabende", "Rehema Baguma", "Andrew Katumba", "Chodrine Mutebi", "Jagen Marvin", "Eric Peter Wairagala", "Mugizi Bruce", "Peter Oketta", "Lawrence Nderu", "Obichi Obiajunwa", "Abigail Oppong", "Michael Zimba", "Data Authors"], "title": "Amplify Initiative: Building A Localized Data Platform for Globalized AI", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Current AI models often fail to account for local context and language, given\nthe predominance of English and Western internet content in their training\ndata. This hinders the global relevance, usefulness, and safety of these models\nas they gain more users around the globe. Amplify Initiative, a data platform\nand methodology, leverages expert communities to collect diverse, high-quality\ndata to address the limitations of these models. The platform is designed to\nenable co-creation of datasets, provide access to high-quality multilingual\ndatasets, and offer recognition to data authors. This paper presents the\napproach to co-creating datasets with domain experts (e.g., health workers,\nteachers) through a pilot conducted in Sub-Saharan Africa (Ghana, Kenya,\nMalawi, Nigeria, and Uganda). In partnership with local researchers situated in\nthese countries, the pilot demonstrated an end-to-end approach to co-creating\ndata with 155 experts in sensitive domains (e.g., physicians, bankers,\nanthropologists, human and civil rights advocates). This approach, implemented\nwith an Android app, resulted in an annotated dataset of 8,091 adversarial\nqueries in seven languages (e.g., Luganda, Swahili, Chichewa), capturing\nnuanced and contextual information related to key themes such as misinformation\nand public interest topics. This dataset in turn can be used to evaluate models\nfor their safety and cultural relevance within the context of these languages.", "AI": {"tldr": "Amplify Initiative\u901a\u8fc7\u4e13\u5bb6\u793e\u533a\u5408\u4f5c\u6536\u96c6\u591a\u8bed\u8a00\u6570\u636e\uff0c\u89e3\u51b3AI\u6a21\u578b\u56e0\u8bad\u7ec3\u6570\u636e\u5c40\u9650\u5bfc\u81f4\u7684\u5168\u7403\u9002\u7528\u6027\u95ee\u9898\uff0c\u5e76\u5728\u975e\u6d32\u4e94\u56fd\u8bd5\u70b9\u6210\u529f\u751f\u62108,091\u6761\u5bf9\u6297\u6027\u67e5\u8be2\u6570\u636e\u96c6\u3002", "motivation": "\u5f53\u524dAI\u6a21\u578b\u56e0\u8bad\u7ec3\u6570\u636e\u4ee5\u82f1\u8bed\u548c\u897f\u65b9\u5185\u5bb9\u4e3a\u4e3b\uff0c\u7f3a\u4e4f\u672c\u5730\u8bed\u5883\u548c\u8bed\u8a00\u591a\u6837\u6027\uff0c\u5f71\u54cd\u5176\u5168\u7403\u76f8\u5173\u6027\u548c\u5b89\u5168\u6027\u3002", "method": "\u901a\u8fc7\u4e0e\u975e\u6d32\u4e94\u56fd\u7684\u9886\u57df\u4e13\u5bb6\u5408\u4f5c\uff0c\u4f7f\u7528Android\u5e94\u7528\u5171\u540c\u521b\u5efa\u591a\u8bed\u8a00\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u654f\u611f\u4e3b\u9898\u3002", "result": "\u751f\u6210\u4e86\u5305\u542b7\u79cd\u8bed\u8a00\u76848,091\u6761\u5bf9\u6297\u6027\u67e5\u8be2\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u6a21\u578b\u7684\u5b89\u5168\u6027\u548c\u6587\u5316\u76f8\u5173\u6027\u3002", "conclusion": "Amplify Initiative\u7684\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86AI\u6a21\u578b\u7684\u5168\u7403\u9002\u7528\u6027\uff0c\u4e3a\u591a\u8bed\u8a00\u548c\u672c\u5730\u5316\u6570\u636e\u6536\u96c6\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2504.13946", "pdf": "https://arxiv.org/pdf/2504.13946", "abs": "https://arxiv.org/abs/2504.13946", "authors": ["Jacob Tjaden"], "title": "The Balancing Act of Policies in Developing Machine Learning Explanations", "categories": ["cs.HC", "cs.CY", "cs.LG"], "comment": null, "summary": "Machine learning models are often criticized as opaque from a lack of\ntransparency in their decision-making process. This study examines how policy\ndesign impacts the quality of explanations in ML models. We conducted a\nclassroom experiment with 124 participants and analyzed the effects of policy\nlength and purpose on developer compliance with policy requirements. Our\nresults indicate that while policy length affects engagement with some\nrequirements, policy purpose has no effect, and explanation quality is\ngenerally poor. These findings highlight the challenge of effective policy\ndevelopment and the importance of addressing diverse stakeholder perspectives\nwithin explanations.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u653f\u7b56\u8bbe\u8ba1\u5bf9\u673a\u5668\u5b66\u4e60\u6a21\u578b\u89e3\u91ca\u8d28\u91cf\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u653f\u7b56\u957f\u5ea6\u5f71\u54cd\u5f00\u53d1\u8005\u53c2\u4e0e\u5ea6\uff0c\u4f46\u653f\u7b56\u76ee\u7684\u65e0\u5f71\u54cd\uff0c\u89e3\u91ca\u8d28\u91cf\u666e\u904d\u8f83\u5dee\u3002", "motivation": "\u673a\u5668\u5b66\u4e60\u6a21\u578b\u56e0\u51b3\u7b56\u8fc7\u7a0b\u4e0d\u900f\u660e\u5e38\u53d7\u6279\u8bc4\uff0c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u653f\u7b56\u8bbe\u8ba1\u5982\u4f55\u6539\u5584\u89e3\u91ca\u8d28\u91cf\u3002", "method": "\u901a\u8fc7124\u540d\u53c2\u4e0e\u8005\u7684\u8bfe\u5802\u5b9e\u9a8c\uff0c\u5206\u6790\u653f\u7b56\u957f\u5ea6\u548c\u76ee\u7684\u5bf9\u5f00\u53d1\u8005\u5408\u89c4\u6027\u7684\u5f71\u54cd\u3002", "result": "\u653f\u7b56\u957f\u5ea6\u5f71\u54cd\u90e8\u5206\u8981\u6c42\u7684\u53c2\u4e0e\u5ea6\uff0c\u653f\u7b56\u76ee\u7684\u65e0\u663e\u8457\u5f71\u54cd\uff0c\u89e3\u91ca\u8d28\u91cf\u666e\u904d\u8f83\u4f4e\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u6709\u6548\u653f\u7b56\u5236\u5b9a\u7684\u6311\u6218\u53ca\u5728\u89e3\u91ca\u4e2d\u8003\u8651\u591a\u5143\u5229\u76ca\u76f8\u5173\u8005\u89c6\u89d2\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2504.14112", "pdf": "https://arxiv.org/pdf/2504.14112", "abs": "https://arxiv.org/abs/2504.14112", "authors": ["Mohit Chandra", "Javier Hernandez", "Gonzalo Ramos", "Mahsa Ershadi", "Ananya Bhattacharjee", "Judith Amores", "Ebele Okoli", "Ann Paradiso", "Shahed Warreth", "Jina Suh"], "title": "Longitudinal Study on Social and Emotional Use of AI Conversational Agent", "categories": ["cs.HC", "cs.AI", "cs.CY"], "comment": "16 pages, 5 figures, 2 tables", "summary": "Development in digital technologies has continuously reshaped how individuals\nseek and receive social and emotional support. While online platforms and\ncommunities have long served this need, the increased integration of\ngeneral-purpose conversational AI into daily lives has introduced new dynamics\nin how support is provided and experienced. Existing research has highlighted\nboth benefits (e.g., wider access to well-being resources) and potential risks\n(e.g., over-reliance) of using AI for support seeking. In this five-week,\nexploratory study, we recruited 149 participants divided into two usage groups:\na baseline usage group (BU, n=60) that used the internet and AI as usual, and\nan active usage group (AU, n=89) encouraged to use one of four commercially\navailable AI tools (Microsoft Copilot, Google Gemini, PI AI, ChatGPT) for\nsocial and emotional interactions. Our analysis revealed significant increases\nin perceived attachment towards AI (32.99 percentage points), perceived AI\nempathy (25.8 p.p.), and motivation to use AI for entertainment (22.90 p.p.)\namong the AU group. We also observed that individual differences (e.g., gender\nidentity, prior AI usage) influenced perceptions of AI empathy and attachment.\nLastly, the AU group expressed higher comfort in seeking personal help,\nmanaging stress, obtaining social support, and talking about health with AI,\nindicating potential for broader emotional support while highlighting the need\nfor safeguards against problematic usage. Overall, our exploratory findings\nunderscore the importance of developing consumer-facing AI tools that support\nemotional well-being responsibly, while empowering users to understand the\nlimitations of these tools.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86AI\u5728\u793e\u4ea4\u548c\u60c5\u611f\u652f\u6301\u4e2d\u7684\u4f5c\u7528\uff0c\u53d1\u73b0\u4e3b\u52a8\u4f7f\u7528AI\u5de5\u5177\u7684\u7528\u6237\u5bf9\u5176\u4f9d\u604b\u611f\u548c\u5171\u60c5\u80fd\u529b\u611f\u77e5\u663e\u8457\u63d0\u5347\uff0c\u4f46\u4e5f\u9700\u6ce8\u610f\u6f5c\u5728\u98ce\u9669\u3002", "motivation": "\u6570\u5b57\u6280\u672f\u53d1\u5c55\u6539\u53d8\u4e86\u4eba\u4eec\u5bfb\u6c42\u793e\u4ea4\u548c\u60c5\u611f\u652f\u6301\u7684\u65b9\u5f0f\uff0cAI\u7684\u666e\u53ca\u5e26\u6765\u4e86\u65b0\u7684\u52a8\u6001\uff0c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5176\u5f71\u54cd\u3002", "method": "\u4e94\u5468\u63a2\u7d22\u6027\u7814\u7a76\uff0c149\u540d\u53c2\u4e0e\u8005\u5206\u4e3a\u57fa\u7ebf\u4f7f\u7528\u7ec4\u548c\u4e3b\u52a8\u4f7f\u7528\u7ec4\uff0c\u540e\u8005\u4f7f\u7528\u56db\u79cd\u5546\u4e1aAI\u5de5\u5177\u8fdb\u884c\u793e\u4ea4\u4e92\u52a8\u3002", "result": "\u4e3b\u52a8\u4f7f\u7528\u7ec4\u5bf9AI\u7684\u4f9d\u604b\u611f\u3001\u5171\u60c5\u611f\u77e5\u53ca\u5a31\u4e50\u52a8\u673a\u663e\u8457\u63d0\u5347\uff0c\u4e2a\u4f53\u5dee\u5f02\u5f71\u54cd\u611f\u77e5\uff0cAI\u5728\u60c5\u611f\u652f\u6301\u65b9\u9762\u6f5c\u529b\u663e\u8457\u3002", "conclusion": "\u9700\u5f00\u53d1\u8d1f\u8d23\u4efb\u7684\u60c5\u611f\u652f\u6301AI\u5de5\u5177\uff0c\u540c\u65f6\u8ba9\u7528\u6237\u4e86\u89e3\u5176\u5c40\u9650\u6027\uff0c\u9632\u6b62\u8fc7\u5ea6\u4f9d\u8d56\u3002"}}
{"id": "2504.13962", "pdf": "https://arxiv.org/pdf/2504.13962", "abs": "https://arxiv.org/abs/2504.13962", "authors": ["Jose Manuel Aroca-Fernandez", "Jose Francisco Diez-Pastor", "Pedro Latorre-Carmona", "Victor Elvira", "Gustau Camps-Valls", "Rodrigo Pascual", "Cesar Garcia-Osorio"], "title": "A Collaborative Platform for Soil Organic Carbon Inference Based on Spatiotemporal Remote Sensing Data", "categories": ["cs.CY", "cs.LG"], "comment": "28 pages, 11 figures", "summary": "Soil organic carbon (SOC) is a key indicator of soil health, fertility, and\ncarbon sequestration, making it essential for sustainable land management and\nclimate change mitigation. However, large-scale SOC monitoring remains\nchallenging due to spatial variability, temporal dynamics, and multiple\ninfluencing factors. We present WALGREEN, a platform that enhances SOC\ninference by overcoming limitations of current applications. Leveraging machine\nlearning and diverse soil samples, WALGREEN generates predictive models using\nhistorical public and private data. Built on cloud-based technologies, it\noffers a user-friendly interface for researchers, policymakers, and land\nmanagers to access carbon data, analyze trends, and support evidence-based\ndecision-making. Implemented in Python, Java, and JavaScript, WALGREEN\nintegrates Google Earth Engine and Sentinel Copernicus via scripting,\nOpenLayers, and Thymeleaf in a Model-View-Controller framework. This paper aims\nto advance soil science, promote sustainable agriculture, and drive critical\necosystem responses to climate change.", "AI": {"tldr": "WALGREEN\u662f\u4e00\u4e2a\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u5e73\u53f0\uff0c\u7528\u4e8e\u5927\u89c4\u6a21\u571f\u58e4\u6709\u673a\u78b3\uff08SOC\uff09\u76d1\u6d4b\uff0c\u7ed3\u5408\u5386\u53f2\u6570\u636e\u548c\u4e91\u6280\u672f\uff0c\u652f\u6301\u53ef\u6301\u7eed\u571f\u5730\u7ba1\u7406\u548c\u6c14\u5019\u53d8\u5316\u7684\u51b3\u7b56\u3002", "motivation": "\u571f\u58e4\u6709\u673a\u78b3\uff08SOC\uff09\u662f\u571f\u58e4\u5065\u5eb7\u548c\u78b3\u6c47\u7684\u5173\u952e\u6307\u6807\uff0c\u4f46\u73b0\u6709\u76d1\u6d4b\u65b9\u6cd5\u53d7\u9650\u4e8e\u7a7a\u95f4\u548c\u65f6\u95f4\u53d8\u5f02\u6027\u3002WALGREEN\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\u3002", "method": "\u5229\u7528\u673a\u5668\u5b66\u4e60\u548c\u591a\u6837\u5316\u7684\u571f\u58e4\u6837\u672c\u6570\u636e\uff0c\u7ed3\u5408Python\u3001Java\u548cJavaScript\u7b49\u6280\u672f\uff0c\u96c6\u6210Google Earth Engine\u548cSentinel Copernicus\uff0c\u6784\u5efa\u9884\u6d4b\u6a21\u578b\u3002", "result": "WALGREEN\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7528\u6237\u53cb\u597d\u7684\u5e73\u53f0\uff0c\u652f\u6301\u7814\u7a76\u4eba\u5458\u548c\u653f\u7b56\u5236\u5b9a\u8005\u5206\u6790\u78b3\u6570\u636e\u8d8b\u52bf\uff0c\u4fc3\u8fdb\u57fa\u4e8e\u8bc1\u636e\u7684\u51b3\u7b56\u3002", "conclusion": "WALGREEN\u63a8\u52a8\u4e86\u571f\u58e4\u79d1\u5b66\u7684\u53d1\u5c55\uff0c\u652f\u6301\u53ef\u6301\u7eed\u519c\u4e1a\u548c\u5e94\u5bf9\u6c14\u5019\u53d8\u5316\u7684\u5173\u952e\u751f\u6001\u7cfb\u7edf\u54cd\u5e94\u3002"}}
{"id": "2504.14125", "pdf": "https://arxiv.org/pdf/2504.14125", "abs": "https://arxiv.org/abs/2504.14125", "authors": ["Maria-Teresa De Rosa Palmini", "Eva Cetinic"], "title": "Exploring Language Patterns of Prompts in Text-to-Image Generation and Their Impact on Visual Diversity", "categories": ["cs.HC", "cs.AI", "cs.CY"], "comment": null, "summary": "Following the initial excitement, Text-to-Image (TTI) models are now being\nexamined more critically. While much of the discourse has focused on biases and\nstereotypes embedded in large-scale training datasets, the sociotechnical\ndynamics of user interactions with these models remain underexplored. This\nstudy examines the linguistic and semantic choices users make when crafting\nprompts and how these choices influence the diversity of generated outputs.\nAnalyzing over six million prompts from the Civiverse dataset on the CivitAI\nplatform across seven months, we categorize users into three groups based on\ntheir levels of linguistic experimentation: consistent repeaters, occasional\nrepeaters, and non-repeaters. Our findings reveal that as user participation\ngrows over time, prompt language becomes increasingly homogenized through the\nadoption of popular community tags and descriptors, with repeated prompts\ncomprising 40-50% of submissions. At the same time, semantic similarity and\ntopic preferences remain relatively stable, emphasizing common subjects and\nsurface aesthetics. Using Vendi scores to quantify visual diversity, we\ndemonstrate a clear correlation between lexical similarity in prompts and the\nvisual similarity of generated images, showing that linguistic repetition\nreinforces less diverse representations. These findings highlight the\nsignificant role of user-driven factors in shaping AI-generated imagery, beyond\ninherent model biases, and underscore the need for tools and practices that\nencourage greater linguistic and thematic experimentation within TTI systems to\nfoster more inclusive and diverse AI-generated content.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u7528\u6237\u4e0e\u6587\u672c\u5230\u56fe\u50cf\uff08TTI\uff09\u6a21\u578b\u7684\u4ea4\u4e92\u52a8\u6001\uff0c\u53d1\u73b0\u7528\u6237\u63d0\u793a\u8bed\u8a00\u7684\u540c\u8d28\u5316\u5bfc\u81f4\u751f\u6210\u56fe\u50cf\u591a\u6837\u6027\u964d\u4f4e\u3002", "motivation": "\u5c3d\u7ba1TTI\u6a21\u578b\u7684\u504f\u89c1\u548c\u523b\u677f\u5370\u8c61\u5df2\u88ab\u5e7f\u6cdb\u8ba8\u8bba\uff0c\u4f46\u7528\u6237\u4ea4\u4e92\u7684\u793e\u4f1a\u6280\u672f\u52a8\u6001\u4ecd\u672a\u5145\u5206\u7814\u7a76\u3002", "method": "\u5206\u6790CivitAI\u5e73\u53f0\u4e0a\u516d\u767e\u4e07\u6761\u63d0\u793a\uff0c\u5c06\u7528\u6237\u5206\u4e3a\u4e09\u7c7b\uff08\u91cd\u590d\u8005\u3001\u5076\u5c14\u91cd\u590d\u8005\u3001\u975e\u91cd\u590d\u8005\uff09\uff0c\u5e76\u4f7f\u7528Vendi\u5206\u6570\u91cf\u5316\u89c6\u89c9\u591a\u6837\u6027\u3002", "result": "\u7528\u6237\u63d0\u793a\u8bed\u8a00\u9010\u6e10\u540c\u8d28\u5316\uff0c\u91cd\u590d\u63d0\u793a\u536040-50%\uff0c\u4e14\u8bed\u8a00\u91cd\u590d\u4e0e\u89c6\u89c9\u76f8\u4f3c\u6027\u663e\u8457\u76f8\u5173\u3002", "conclusion": "\u7528\u6237\u884c\u4e3a\u5bf9AI\u751f\u6210\u56fe\u50cf\u7684\u591a\u6837\u6027\u6709\u91cd\u8981\u5f71\u54cd\uff0c\u9700\u9f13\u52b1\u66f4\u591a\u8bed\u8a00\u548c\u4e3b\u9898\u5b9e\u9a8c\u4ee5\u4fc3\u8fdb\u591a\u6837\u6027\u3002"}}
{"id": "2504.13978", "pdf": "https://arxiv.org/pdf/2504.13978", "abs": "https://arxiv.org/abs/2504.13978", "authors": ["Yuqing Liu", "Meng Zhao", "Guanlan Hu", "Yuchen Zhang"], "title": "Association between nutritional factors, inflammatory biomarkers and cancer types: an analysis of NHANES data using machine learning", "categories": ["q-bio.QM", "cs.LG"], "comment": null, "summary": "Background. Diet and inflammation are critical factors influencing cancer\nrisk. However, the combined impact of nutritional status and inflammatory\nbiomarkers on cancer status and type, using machine learning (ML), remains\nunderexplored.\n  Objectives. This study investigates the association between nutritional\nfactors, inflammatory biomarkers, and cancer status, and whether these\nrelationships differ across cancer types using National Health and Nutrition\nExamination Survey (NHANES) data.\n  Methods. We analyzed 24 macro- and micronutrients, C-reactive protein (CRP),\nand the advanced lung cancer inflammation index (ALI) in 26,409 NHANES\nparticipants (2,120 with cancer). Multivariable logistic regression assessed\nassociations with cancer prevalence. We also examined whether these features\ndiffered across the five most common cancer types. To evaluate predictive\nvalue, we applied three ML models - Logistic Regression, Random Forest, and\nXGBoost - on the full feature set.\n  Results. The cohort's mean age was 49.1 years; 34.7% were obese.\nComorbidities such as anemia and liver conditions, along with nutritional\nfactors like protein and several vitamins, were key predictors of cancer\nstatus. Among the models, Random Forest performed best, achieving an accuracy\nof 0.72.\n  Conclusions. Higher-quality nutritional intake and lower levels of\ninflammation may offer protective effects against cancer. These findings\nhighlight the potential of combining nutritional and inflammatory markers with\nML to inform cancer prevention strategies.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u8425\u517b\u72b6\u6001\u548c\u708e\u75c7\u6807\u5fd7\u7269\u4e0e\u764c\u75c7\u7684\u5173\u7cfb\uff0c\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u5206\u6790NHANES\u6570\u636e\uff0c\u53d1\u73b0\u8425\u517b\u548c\u708e\u75c7\u6307\u6807\u5bf9\u764c\u75c7\u6709\u9884\u6d4b\u4ef7\u503c\u3002", "motivation": "\u996e\u98df\u548c\u708e\u75c7\u662f\u5f71\u54cd\u764c\u75c7\u98ce\u9669\u7684\u5173\u952e\u56e0\u7d20\uff0c\u4f46\u7ed3\u5408\u8425\u517b\u72b6\u6001\u548c\u708e\u75c7\u6807\u5fd7\u7269\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u5206\u6790\u764c\u75c7\u7684\u7814\u7a76\u8f83\u5c11\u3002", "method": "\u5206\u6790\u4e8626,409\u540dNHANES\u53c2\u4e0e\u8005\u768424\u79cd\u8425\u517b\u7d20\u3001CRP\u548cALI\uff0c\u4f7f\u7528\u591a\u53d8\u91cf\u903b\u8f91\u56de\u5f52\u548c\u4e09\u79cd\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff08\u903b\u8f91\u56de\u5f52\u3001\u968f\u673a\u68ee\u6797\u3001XGBoost\uff09\u8bc4\u4f30\u9884\u6d4b\u4ef7\u503c\u3002", "result": "\u8425\u517b\u56e0\u7d20\uff08\u5982\u86cb\u767d\u8d28\u548c\u7ef4\u751f\u7d20\uff09\u548c\u708e\u75c7\u6807\u5fd7\u7269\u662f\u764c\u75c7\u7684\u5173\u952e\u9884\u6d4b\u56e0\u5b50\uff0c\u968f\u673a\u68ee\u6797\u6a21\u578b\u8868\u73b0\u6700\u4f73\uff08\u51c6\u786e\u73870.72\uff09\u3002", "conclusion": "\u9ad8\u8d28\u91cf\u8425\u517b\u6444\u5165\u548c\u4f4e\u708e\u75c7\u6c34\u5e73\u53ef\u80fd\u5bf9\u764c\u75c7\u6709\u4fdd\u62a4\u4f5c\u7528\uff0c\u7ed3\u5408\u8425\u517b\u548c\u708e\u75c7\u6807\u5fd7\u7269\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u6709\u52a9\u4e8e\u764c\u75c7\u9884\u9632\u7b56\u7565\u3002"}}
{"id": "2504.14130", "pdf": "https://arxiv.org/pdf/2504.14130", "abs": "https://arxiv.org/abs/2504.14130", "authors": ["Qiang Li", "Xinze Lin", "Shenghao Lv", "Faliang Huang", "Xiangju Li"], "title": "Personalized News Recommendation with Multi-granularity Candidate-aware User Modeling", "categories": ["cs.IR", "cs.AI"], "comment": null, "summary": "Matching candidate news with user interests is crucial for personalized news\nrecommendations. Most existing methods can represent a user's reading interests\nthrough a single profile based on clicked news, which may not fully capture the\ndiversity of user interests. Although some approaches incorporate candidate\nnews or topic information, they remain insufficient because they neglect the\nmulti-granularity relatedness between candidate news and user interests. To\naddress this, this study proposed a multi-granularity candidate-aware user\nmodeling framework that integrated user interest features across various levels\nof granularity. It consisted of two main components: candidate news encoding\nand user modeling. A news textual information extractor and a\nknowledge-enhanced entity information extractor can capture candidate news\nfeatures, and word-level, entity-level, and news-level candidate-aware\nmechanisms can provide a comprehensive representation of user interests.\nExtensive experiments on a real-world dataset demonstrated that the proposed\nmodel could significantly outperform baseline models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u7c92\u5ea6\u5019\u9009\u611f\u77e5\u7684\u7528\u6237\u5efa\u6a21\u6846\u67b6\uff0c\u7528\u4e8e\u4e2a\u6027\u5316\u65b0\u95fb\u63a8\u8350\uff0c\u901a\u8fc7\u591a\u7c92\u5ea6\u7279\u5f81\u6355\u6349\u7528\u6237\u5174\u8da3\u7684\u591a\u6837\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u57fa\u4e8e\u5355\u4e00\u7528\u6237\u753b\u50cf\uff0c\u96be\u4ee5\u5168\u9762\u6355\u6349\u7528\u6237\u5174\u8da3\u7684\u591a\u6837\u6027\uff0c\u4e14\u5ffd\u7565\u4e86\u5019\u9009\u65b0\u95fb\u4e0e\u7528\u6237\u5174\u8da3\u7684\u591a\u7c92\u5ea6\u5173\u8054\u3002", "method": "\u6846\u67b6\u5305\u542b\u5019\u9009\u65b0\u95fb\u7f16\u7801\u548c\u7528\u6237\u5efa\u6a21\u4e24\u90e8\u5206\uff0c\u5229\u7528\u6587\u672c\u548c\u77e5\u8bc6\u589e\u5f3a\u7684\u5b9e\u4f53\u4fe1\u606f\u63d0\u53d6\u5668\uff0c\u7ed3\u5408\u8bcd\u7ea7\u3001\u5b9e\u4f53\u7ea7\u548c\u65b0\u95fb\u7ea7\u5019\u9009\u611f\u77e5\u673a\u5236\u3002", "result": "\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6a21\u578b\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "\u591a\u7c92\u5ea6\u5019\u9009\u611f\u77e5\u6846\u67b6\u80fd\u66f4\u5168\u9762\u5730\u8868\u793a\u7528\u6237\u5174\u8da3\uff0c\u63d0\u5347\u63a8\u8350\u6548\u679c\u3002"}}
{"id": "2504.14139", "pdf": "https://arxiv.org/pdf/2504.14139", "abs": "https://arxiv.org/abs/2504.14139", "authors": ["Hai Pham-Ngoc", "De Nguyen-Van", "Dung Vu-Tien", "Phuong Le-Hong"], "title": "ThyroidEffi 1.0: A Cost-Effective System for High-Performance Multi-Class Thyroid Carcinoma Classification", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Background: Automated classification of thyroid fine needle aspiration biopsy\n(FNAB) images faces challenges in limited data, inter-observer variability, and\ncomputational cost. Efficient, interpretable models are crucial for clinical\nsupport. Objective: To develop and externally validate a deep learning system\nfor the multi-class classification of thyroid FNAB images into three key\ncategories that directly guide post-biopsy treatment decisions in Vietnam:\nbenign (B2), suspicious for malignancy (B5), and malignant (B6), while\nachieving high diagnostic accuracy with low computational overhead. Methods:\nOur framework features: (1) YOLOv10-based cell cluster detection for\ninformative sub-region extraction and noise reduction; (2) a curriculum\nlearning-inspired protocol sequencing localized crops to full images for\nmulti-scale feature capture; (3) adaptive lightweight EfficientNetB0 (4\nmillions parameters) selection balancing performance and efficiency; and (4) a\nTransformer-inspired module for multi-scale, multi-region analysis. External\nvalidation used 1,015 independent FNAB images. Results: ThyroidEffi Basic\nachieved a macro F1 of 89.19\\% and AUCs of 0.98 (B2), 0.95 (B5), and 0.96 (B6)\non the internal test set. External validation yielded AUCs of 0.9495 (B2),\n0.7436 (B5), and 0.8396 (B6). ThyroidEffi Premium improved macro F1 to 89.77\\%.\nGrad-CAM highlighted key diagnostic regions, confirming interpretability. The\nsystem processed 1000 cases in 30 seconds, demonstrating feasibility on widely\naccessible hardware like a 12-core CPU. Conclusions: This work demonstrates\nthat high-accuracy, interpretable thyroid FNAB image classification is\nachievable with minimal computational demands.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u89e3\u91ca\u7684\u6df1\u5ea6\u5b66\u4e60\u7cfb\u7edf\uff0c\u7528\u4e8e\u7532\u72b6\u817a\u7ec6\u9488\u7a7f\u523a\u6d3b\u68c0\u56fe\u50cf\u7684\u591a\u5206\u7c7b\uff0c\u5177\u6709\u9ad8\u51c6\u786e\u6027\u548c\u4f4e\u8ba1\u7b97\u5f00\u9500\u3002", "motivation": "\u89e3\u51b3\u7532\u72b6\u817aFNAB\u56fe\u50cf\u5206\u7c7b\u4e2d\u7684\u6570\u636e\u4e0d\u8db3\u3001\u89c2\u5bdf\u8005\u95f4\u5dee\u5f02\u548c\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u6311\u6218\uff0c\u4e3a\u4e34\u5e8a\u51b3\u7b56\u63d0\u4f9b\u652f\u6301\u3002", "method": "\u7ed3\u5408YOLOv10\u7ec6\u80de\u7c07\u68c0\u6d4b\u3001\u8bfe\u7a0b\u5b66\u4e60\u534f\u8bae\u3001\u8f7b\u91cf\u7ea7EfficientNetB0\u548cTransformer\u6a21\u5757\uff0c\u5b9e\u73b0\u591a\u5c3a\u5ea6\u7279\u5f81\u63d0\u53d6\u4e0e\u5206\u6790\u3002", "result": "\u5185\u90e8\u6d4b\u8bd5\u96c6\u5b8fF1\u4e3a89.19%\uff0c\u5916\u90e8\u9a8c\u8bc1AUC\u5206\u522b\u4e3a0.9495\uff08B2\uff09\u30010.7436\uff08B5\uff09\u548c0.8396\uff08B6\uff09\u3002\u7cfb\u7edf\u5904\u7406\u901f\u5ea6\u5feb\uff0c\u9002\u7528\u4e8e\u666e\u901a\u786c\u4ef6\u3002", "conclusion": "\u9ad8\u51c6\u786e\u6027\u3001\u53ef\u89e3\u91ca\u7684\u7532\u72b6\u817aFNAB\u5206\u7c7b\u53ef\u5728\u4f4e\u8ba1\u7b97\u9700\u6c42\u4e0b\u5b9e\u73b0\uff0c\u5177\u6709\u4e34\u5e8a\u53ef\u884c\u6027\u3002"}}
{"id": "2504.14002", "pdf": "https://arxiv.org/pdf/2504.14002", "abs": "https://arxiv.org/abs/2504.14002", "authors": ["Francesco Perciavalle", "Francesco Plastina", "Michele Pisarra", "Nicola Lo Gullo"], "title": "Predicting fermionic densities using a Projected Quantum Kernel method", "categories": ["quant-ph", "cond-mat.str-el", "cs.LG"], "comment": "12 pages, 7 figures", "summary": "We use a support vector regressor based on a projected quantum kernel method\nto predict the density structure of 1D fermionic systems of interest in quantum\nchemistry and quantum matter. The kernel is built on with the observables of a\nquantum reservoir implementable with interacting Rydberg atoms. Training and\ntest data of the fermionic system are generated using a Density Functional\nTheory approach. We test the performance of the method for several Hamiltonian\nparameters, finding a general common behavior of the error as a function of\nmeasurement time. At sufficiently large measurement times, we find that the\nmethod outperforms the classical linear kernel method and can be competitive\nwith the radial basis function method.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6295\u5f71\u91cf\u5b50\u6838\u65b9\u6cd5\u7684\u652f\u6301\u5411\u91cf\u56de\u5f52\u5668\uff0c\u7528\u4e8e\u9884\u6d4b\u4e00\u7ef4\u8d39\u7c73\u5b50\u7cfb\u7edf\u7684\u5bc6\u5ea6\u7ed3\u6784\uff0c\u6027\u80fd\u4f18\u4e8e\u7ecf\u5178\u7ebf\u6027\u6838\u65b9\u6cd5\u3002", "motivation": "\u7814\u7a76\u91cf\u5b50\u5316\u5b66\u548c\u91cf\u5b50\u7269\u8d28\u4e2d\u4e00\u7ef4\u8d39\u7c73\u5b50\u7cfb\u7edf\u7684\u5bc6\u5ea6\u7ed3\u6784\u9884\u6d4b\u95ee\u9898\uff0c\u63a2\u7d22\u91cf\u5b50\u8ba1\u7b97\u65b9\u6cd5\u7684\u4f18\u52bf\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u91cf\u5b50\u50a8\u5c42\u53ef\u89c2\u6d4b\u91cf\u7684\u6295\u5f71\u91cf\u5b50\u6838\u65b9\u6cd5\u6784\u5efa\u652f\u6301\u5411\u91cf\u56de\u5f52\u5668\uff0c\u8bad\u7ec3\u548c\u6d4b\u8bd5\u6570\u636e\u901a\u8fc7\u5bc6\u5ea6\u6cdb\u51fd\u7406\u8bba\u751f\u6210\u3002", "result": "\u5728\u8db3\u591f\u957f\u7684\u6d4b\u91cf\u65f6\u95f4\u4e0b\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u7ecf\u5178\u7ebf\u6027\u6838\u65b9\u6cd5\uff0c\u5e76\u4e0e\u5f84\u5411\u57fa\u51fd\u6570\u65b9\u6cd5\u7ade\u4e89\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u9884\u6d4b\u4e00\u7ef4\u8d39\u7c73\u5b50\u7cfb\u7edf\u5bc6\u5ea6\u7ed3\u6784\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u5c55\u73b0\u4e86\u91cf\u5b50\u8ba1\u7b97\u65b9\u6cd5\u7684\u4f18\u52bf\u3002"}}
{"id": "2504.14145", "pdf": "https://arxiv.org/pdf/2504.14145", "abs": "https://arxiv.org/abs/2504.14145", "authors": ["Zhenliang Xue", "Hanpeng Hu", "Xing Chen", "Yimin Jiang", "Yixin Song", "Zeyu Mi", "Yibo Zhu", "Daxin Jiang", "Yubin Xia", "Haibo Chen"], "title": "PipeWeaver: Addressing Data Dynamicity in Large Multimodal Model Training with Dynamic Interleaved Pipeline", "categories": ["cs.DC", "cs.AI"], "comment": null, "summary": "Large multimodal models (LMMs) have demonstrated excellent capabilities in\nboth understanding and generation tasks with various modalities. While these\nmodels can accept flexible combinations of input data, their training\nefficiency suffers from two major issues: pipeline stage imbalance caused by\nheterogeneous model architectures, and training data dynamicity stemming from\nthe diversity of multimodal data.\n  In this paper, we present PipeWeaver, a dynamic pipeline scheduling framework\ndesigned for LMM training. The core of PipeWeaver is dynamic interleaved\npipeline, which searches for pipeline schedules dynamically tailored to current\ntraining batches. PipeWeaver addresses issues of LMM training with two\ntechniques: adaptive modality-aware partitioning and efficient pipeline\nschedule search within a hierarchical schedule space. Meanwhile, PipeWeaver\nutilizes SEMU (Step Emulator), a training simulator for multimodal models, for\naccurate performance estimations, accelerated by spatial-temporal subgraph\nreuse to improve search efficiency. Experiments show that PipeWeaver can\nenhance LMM training efficiency by up to 97.3% compared to state-of-the-art\nsystems, and demonstrate excellent adaptivity to LMM training's data\ndynamicity.", "AI": {"tldr": "PipeWeaver\u662f\u4e00\u4e2a\u52a8\u6001\u7ba1\u9053\u8c03\u5ea6\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u4ea4\u9519\u7ba1\u9053\u548c\u6a21\u6001\u611f\u77e5\u5206\u533a\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\uff08LMM\uff09\u7684\u8bad\u7ec3\u6548\u7387\u3002", "motivation": "\u89e3\u51b3LMM\u8bad\u7ec3\u4e2d\u56e0\u5f02\u6784\u67b6\u6784\u548c\u52a8\u6001\u6570\u636e\u591a\u6837\u6027\u5bfc\u81f4\u7684\u6548\u7387\u95ee\u9898\u3002", "method": "\u91c7\u7528\u52a8\u6001\u4ea4\u9519\u7ba1\u9053\u3001\u81ea\u9002\u5e94\u6a21\u6001\u611f\u77e5\u5206\u533a\u548c\u9ad8\u6548\u7ba1\u9053\u8c03\u5ea6\u641c\u7d22\u6280\u672f\uff0c\u7ed3\u5408SEMU\u6a21\u62df\u5668\u8fdb\u884c\u6027\u80fd\u4f30\u8ba1\u3002", "result": "PipeWeaver\u5c06LMM\u8bad\u7ec3\u6548\u7387\u63d0\u5347\u9ad8\u8fbe97.3%\uff0c\u5e76\u5c55\u73b0\u51fa\u5bf9\u6570\u636e\u52a8\u6001\u6027\u7684\u4f18\u79c0\u9002\u5e94\u6027\u3002", "conclusion": "PipeWeaver\u4e3aLMM\u8bad\u7ec3\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u9002\u5e94\u6027\u5f3a\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.14007", "pdf": "https://arxiv.org/pdf/2504.14007", "abs": "https://arxiv.org/abs/2504.14007", "authors": ["Haoliang Sheng", "Songpu Cai", "Xingyu Zheng", "Meng Cheng Lau"], "title": "Knitting Robots: A Deep Learning Approach for Reverse-Engineering Fabric Patterns", "categories": ["cs.RO", "cs.LG"], "comment": null, "summary": "Knitting, a cornerstone of textile manufacturing, is uniquely challenging to\nautomate, particularly in terms of converting fabric designs into precise,\nmachine-readable instructions. This research bridges the gap between textile\nproduction and robotic automation by proposing a novel deep learning-based\npipeline for reverse knitting to integrate vision-based robotic systems into\ntextile manufacturing. The pipeline employs a two-stage architecture, enabling\nrobots to first identify front labels before inferring complete labels,\nensuring accurate, scalable pattern generation. By incorporating diverse yarn\nstructures, including single-yarn (sj) and multi-yarn (mj) patterns, this study\ndemonstrates how our system can adapt to varying material complexities.\nCritical challenges in robotic textile manipulation, such as label imbalance,\nunderrepresented stitch types, and the need for fine-grained control, are\naddressed by leveraging specialized deep-learning architectures. This work\nestablishes a foundation for fully automated robotic knitting systems, enabling\ncustomizable, flexible production processes that integrate perception,\nplanning, and actuation, thereby advancing textile manufacturing through\nintelligent robotic automation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u9006\u5411\u7f16\u7ec7\u7ba1\u9053\uff0c\u7528\u4e8e\u5c06\u89c6\u89c9\u673a\u5668\u4eba\u7cfb\u7edf\u96c6\u6210\u5230\u7eba\u7ec7\u5236\u9020\u4e2d\uff0c\u89e3\u51b3\u4e86\u7f16\u7ec7\u81ea\u52a8\u5316\u4e2d\u7684\u5173\u952e\u6311\u6218\u3002", "motivation": "\u7f16\u7ec7\u81ea\u52a8\u5316\u5728\u7eba\u7ec7\u5236\u9020\u4e2d\u5177\u6709\u6311\u6218\u6027\uff0c\u5c24\u5176\u662f\u5728\u5c06\u7ec7\u7269\u8bbe\u8ba1\u8f6c\u6362\u4e3a\u673a\u5668\u53ef\u8bfb\u6307\u4ee4\u65b9\u9762\u3002\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u673a\u5668\u4eba\u81ea\u52a8\u5316\u63d0\u5347\u7eba\u7ec7\u751f\u4ea7\u7684\u7075\u6d3b\u6027\u548c\u53ef\u5b9a\u5236\u6027\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\uff0c\u673a\u5668\u4eba\u5148\u8bc6\u522b\u524d\u6807\u7b7e\uff0c\u518d\u63a8\u65ad\u5b8c\u6574\u6807\u7b7e\uff0c\u5e76\u7ed3\u5408\u5355\u7ebf\u548c\u591a\u7ebf\u6a21\u5f0f\u9002\u5e94\u4e0d\u540c\u6750\u6599\u590d\u6742\u5ea6\u3002", "result": "\u7cfb\u7edf\u80fd\u591f\u51c6\u786e\u751f\u6210\u53ef\u6269\u5c55\u7684\u7f16\u7ec7\u56fe\u6848\uff0c\u89e3\u51b3\u4e86\u6807\u7b7e\u4e0d\u5e73\u8861\u3001\u9488\u6cd5\u7c7b\u578b\u4e0d\u8db3\u548c\u7cbe\u7ec6\u63a7\u5236\u7b49\u6311\u6218\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5168\u81ea\u52a8\u673a\u5668\u4eba\u7f16\u7ec7\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u63a8\u52a8\u4e86\u7eba\u7ec7\u5236\u9020\u4e2d\u667a\u80fd\u673a\u5668\u4eba\u81ea\u52a8\u5316\u7684\u53d1\u5c55\u3002"}}
{"id": "2504.14026", "pdf": "https://arxiv.org/pdf/2504.14026", "abs": "https://arxiv.org/abs/2504.14026", "authors": ["Nusrat Zahan", "Laurie Williams"], "title": "Prioritizing Security Practice Adoption: Empirical Insights on Software Security Outcomes in the npm Ecosystem", "categories": ["cs.SE", "cs.LG"], "comment": "12 pages, 3 figures, 5 tables", "summary": "Practitioners often struggle with the overwhelming number of security\npractices outlined in cybersecurity frameworks for risk mitigation. Given the\nlimited budget, time, and resources, practitioners want to prioritize the\nadoption of security practices based on empirical evidence. The goal of this\nstudy is to assist practitioners and policymakers in making informed decisions\non which security practices to adopt by evaluating the relationship between\nsoftware security practices and security outcome metrics. The study\ninvestigated the relationship between security practice adoption and security\noutcomes. We selected the OpenSSF Scorecard metrics to automatically measure\nthe adoption of security practices in npm GitHub repositories. We also explored\nsecurity outcome metrics, such as the number of open vulnerabilities\n(Vul_Count), mean time to remediate (MTTR) vulnerabilities in dependencies, and\nmean time to update (MTTU) dependencies. We conducted regression and causal\nanalysis using 12 Scorecard metrics and their aggregated Scorecard score\n(computed by aggregating individual security practice scores) as predictors and\nVul_Count, MTTR, and MTTU as target variables. Our findings show that higher\naggregated Scorecard scores are associated with fewer Vul_Count and shorter\nMTTU, also supported by causal analysis. However, while the regression model\nsuggests shorter MTTR, causal analysis indicates project characteristics likely\ninfluence MTTR direction. Segment analysis shows that larger, newer\nrepositories with more contributors, dependencies, and downloads have shorter\nMTTR. Among individual security practices, Code Review, Maintained status,\nPinned Dependencies, and Branch Protection show strong associations with\nsecurity outcomes; the directionality of these associations varies across\nsecurity outcomes.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u8bc4\u4f30\u8f6f\u4ef6\u5b89\u5168\u5b9e\u8df5\u4e0e\u5b89\u5168\u7ed3\u679c\u6307\u6807\u7684\u5173\u7cfb\uff0c\u5e2e\u52a9\u4ece\u4e1a\u8005\u548c\u653f\u7b56\u5236\u5b9a\u8005\u4f18\u5148\u9009\u62e9\u5b89\u5168\u5b9e\u8df5\u3002", "motivation": "\u4ece\u4e1a\u8005\u5728\u6709\u9650\u9884\u7b97\u548c\u8d44\u6e90\u4e0b\u96be\u4ee5\u9009\u62e9\u6709\u6548\u7684\u5b89\u5168\u5b9e\u8df5\uff0c\u9700\u57fa\u4e8e\u5b9e\u8bc1\u8bc1\u636e\u4f18\u5148\u91c7\u7528\u3002", "method": "\u4f7f\u7528OpenSSF Scorecard\u6307\u6807\u81ea\u52a8\u6d4b\u91cfnpm GitHub\u4ed3\u5e93\u7684\u5b89\u5168\u5b9e\u8df5\uff0c\u5e76\u5206\u6790\u5176\u4e0e\u5b89\u5168\u7ed3\u679c\uff08\u5982\u6f0f\u6d1e\u6570\u91cf\u3001\u4fee\u590d\u65f6\u95f4\u7b49\uff09\u7684\u5173\u7cfb\u3002", "result": "\u8f83\u9ad8\u7684Scorecard\u603b\u5206\u4e0e\u8f83\u5c11\u6f0f\u6d1e\u548c\u66f4\u77ed\u4f9d\u8d56\u66f4\u65b0\u65f6\u95f4\u76f8\u5173\uff0c\u4f46\u4fee\u590d\u65f6\u95f4\u53d7\u9879\u76ee\u7279\u5f81\u5f71\u54cd\u8f83\u5927\u3002\u67d0\u4e9b\u5177\u4f53\u5b9e\u8df5\uff08\u5982\u4ee3\u7801\u5ba1\u67e5\uff09\u4e0e\u5b89\u5168\u7ed3\u679c\u663e\u8457\u76f8\u5173\u3002", "conclusion": "\u7814\u7a76\u4e3a\u4f18\u5148\u9009\u62e9\u5b89\u5168\u5b9e\u8df5\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u4f9d\u636e\uff0c\u4f46\u9700\u7ed3\u5408\u9879\u76ee\u7279\u5f81\u8c03\u6574\u7b56\u7565\u3002"}}
{"id": "2504.14151", "pdf": "https://arxiv.org/pdf/2504.14151", "abs": "https://arxiv.org/abs/2504.14151", "authors": ["Sergio Arnaud", "Paul McVay", "Ada Martin", "Arjun Majumdar", "Krishna Murthy Jatavallabhula", "Phillip Thomas", "Ruslan Partsey", "Daniel Dugas", "Abha Gejji", "Alexander Sax", "Vincent-Pierre Berges", "Mikael Henaff", "Ayush Jain", "Ang Cao", "Ishita Prasad", "Mrinal Kalakrishnan", "Michael Rabbat", "Nicolas Ballas", "Mido Assran", "Oleksandr Maksymets", "Aravind Rajeswaran", "Franziska Meier"], "title": "Locate 3D: Real-World Object Localization via Self-Supervised Learning in 3D", "categories": ["cs.CV", "cs.AI", "cs.RO", "I.2.10; I.2.6; I.2.9; I.3.7; I.4.6; I.4.8"], "comment": null, "summary": "We present LOCATE 3D, a model for localizing objects in 3D scenes from\nreferring expressions like \"the small coffee table between the sofa and the\nlamp.\" LOCATE 3D sets a new state-of-the-art on standard referential grounding\nbenchmarks and showcases robust generalization capabilities. Notably, LOCATE 3D\noperates directly on sensor observation streams (posed RGB-D frames), enabling\nreal-world deployment on robots and AR devices. Key to our approach is 3D-JEPA,\na novel self-supervised learning (SSL) algorithm applicable to sensor point\nclouds. It takes as input a 3D pointcloud featurized using 2D foundation models\n(CLIP, DINO). Subsequently, masked prediction in latent space is employed as a\npretext task to aid the self-supervised learning of contextualized pointcloud\nfeatures. Once trained, the 3D-JEPA encoder is finetuned alongside a\nlanguage-conditioned decoder to jointly predict 3D masks and bounding boxes.\nAdditionally, we introduce LOCATE 3D DATASET, a new dataset for 3D referential\ngrounding, spanning multiple capture setups with over 130K annotations. This\nenables a systematic study of generalization capabilities as well as a stronger\nmodel.", "AI": {"tldr": "LOCATE 3D\u662f\u4e00\u4e2a\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u57283D\u573a\u666f\u4e2d\u5b9a\u4f4d\u7269\u4f53\u7684\u6a21\u578b\uff0c\u7ed3\u5408\u4e86\u81ea\u76d1\u7763\u5b66\u4e60\uff083D-JEPA\uff09\u548c\u8bed\u8a00\u6761\u4ef6\u89e3\u7801\u5668\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u89e3\u51b33D\u573a\u666f\u4e2d\u57fa\u4e8e\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u7684\u7269\u4f53\u5b9a\u4f4d\u95ee\u9898\uff0c\u9002\u7528\u4e8e\u673a\u5668\u4eba\u548cAR\u8bbe\u5907\u7684\u5b9e\u9645\u90e8\u7f72\u3002", "method": "\u4f7f\u75283D-JEPA\u81ea\u76d1\u7763\u5b66\u4e60\u7b97\u6cd5\u5904\u7406\u70b9\u4e91\u6570\u636e\uff0c\u7ed3\u54082D\u57fa\u7840\u6a21\u578b\uff08CLIP\u3001DINO\uff09\u63d0\u53d6\u7279\u5f81\uff0c\u5e76\u901a\u8fc7\u63a9\u7801\u9884\u6d4b\u4efb\u52a1\u5b66\u4e60\u4e0a\u4e0b\u6587\u7279\u5f81\u3002\u8bad\u7ec3\u540e\uff0c3D-JEPA\u7f16\u7801\u5668\u4e0e\u8bed\u8a00\u6761\u4ef6\u89e3\u7801\u5668\u8054\u5408\u9884\u6d4b3D\u63a9\u7801\u548c\u8fb9\u754c\u6846\u3002", "result": "\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u65b0\u6700\u4f18\u6027\u80fd\uff0c\u5e76\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "LOCATE 3D\u901a\u8fc7\u521b\u65b0\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u548c\u6570\u636e\u96c6\u652f\u6301\uff0c\u4e3a3D\u573a\u666f\u4e2d\u7684\u7269\u4f53\u5b9a\u4f4d\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.14156", "pdf": "https://arxiv.org/pdf/2504.14156", "abs": "https://arxiv.org/abs/2504.14156", "authors": ["Abdelali Sajia", "Bilal Benzimoun", "Pawan Khatiwada", "Guogan Zhao", "Xiao-Feng Qian"], "title": "Breaking the Diffraction Barrier for Passive Sources: Parameter-Decoupled Superresolution Assisted by Physics-Informed Machine Learning", "categories": ["physics.optics", "cs.AI", "quant-ph"], "comment": "12 pages, 3 figures", "summary": "We present a parameter-decoupled superresolution framework for estimating\nsub-wavelength separations of passive two-point sources without requiring prior\nknowledge or control of the source. Our theoretical foundation circumvents the\nneed to estimate multiple challenging parameters such as partial coherence,\nbrightness imbalance, random relative phase, and photon statistics. A\nphysics-informed machine learning (ML) model (trained with a standard desktop\nworkstation), synergistically integrating this theory, further addresses\npractical imperfections including background noise, photon loss, and\ncentroid/orientation misalignment. The integrated parameter-decoupling\nsuperresolution method achieves resolution 14 and more times below the\ndiffraction limit (corresponding to ~ 13.5 nm in optical microscopy) on\nexperimentally generated realistic images with >82% fidelity, performance\nrivaling state-of-the-art techniques for actively controllable sources.\nCritically, our method's robustness against source parameter variability and\nsource-independent noises enables potential applications in realistic scenarios\nwhere source control is infeasible, such as astrophysical imaging, live-cell\nmicroscopy, and quantum metrology. This work bridges a critical gap between\ntheoretical superresolution limits and practical implementations for passive\nsystems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53c2\u6570\u89e3\u8026\u7684\u8d85\u5206\u8fa8\u7387\u6846\u67b6\uff0c\u7528\u4e8e\u4f30\u8ba1\u88ab\u52a8\u4e24\u70b9\u6e90\u7684\u4e9a\u6ce2\u957f\u95f4\u8ddd\uff0c\u65e0\u9700\u6e90\u5148\u9a8c\u77e5\u8bc6\u6216\u63a7\u5236\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u4e2d\u9700\u8981\u4f30\u8ba1\u591a\u4e2a\u6311\u6218\u6027\u53c2\u6570\uff08\u5982\u90e8\u5206\u76f8\u5e72\u6027\u3001\u4eae\u5ea6\u4e0d\u5e73\u8861\u3001\u968f\u673a\u76f8\u5bf9\u76f8\u4f4d\u548c\u5149\u5b50\u7edf\u8ba1\uff09\u7684\u95ee\u9898\uff0c\u5e76\u5e94\u5bf9\u5b9e\u9645\u6210\u50cf\u4e2d\u7684\u566a\u58f0\u548c\u504f\u5dee\u3002", "method": "\u7ed3\u5408\u7269\u7406\u7406\u8bba\u6307\u5bfc\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u5904\u7406\u80cc\u666f\u566a\u58f0\u3001\u5149\u5b50\u635f\u5931\u548c\u4e2d\u5fc3/\u65b9\u5411\u504f\u5dee\u7b49\u5b9e\u9645\u95ee\u9898\u3002", "result": "\u5728\u5b9e\u9a8c\u751f\u6210\u7684\u56fe\u50cf\u4e0a\u5b9e\u73b0\u4e8614\u500d\u4e8e\u884d\u5c04\u6781\u9650\u7684\u5206\u8fa8\u7387\uff08\u7ea613.5\u7eb3\u7c73\uff09\uff0c\u4fdd\u771f\u5ea6>82%\uff0c\u6027\u80fd\u5ab2\u7f8e\u4e3b\u52a8\u63a7\u5236\u6e90\u7684\u6700\u5148\u8fdb\u6280\u672f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5bf9\u6e90\u53c2\u6570\u53d8\u5f02\u6027\u548c\u6e90\u65e0\u5173\u566a\u58f0\u7684\u9c81\u68d2\u6027\uff0c\u4f7f\u5176\u5728\u65e0\u6cd5\u63a7\u5236\u6e90\u7684\u5b9e\u9645\u573a\u666f\uff08\u5982\u5929\u4f53\u6210\u50cf\u3001\u6d3b\u7ec6\u80de\u663e\u5fae\u955c\u548c\u91cf\u5b50\u8ba1\u91cf\uff09\u4e2d\u5177\u6709\u5e7f\u6cdb\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2504.14055", "pdf": "https://arxiv.org/pdf/2504.14055", "abs": "https://arxiv.org/abs/2504.14055", "authors": ["Renaud Bougueng Tchemeube", "Jeff Ens", "Philippe Pasquier"], "title": "Apollo: An Interactive Environment for Generating Symbolic Musical Phrases using Corpus-based Style Imitation", "categories": ["cs.HC", "cs.LG", "cs.SD"], "comment": "7 pages, 5 figures, Published as a paper at the 7th International\n  Workshop on Musical Metacreation (MUME 2019), UNC Charlotte, North Carolina", "summary": "With the recent developments in machine intelligence and web technologies,\nnew generative music systems are being explored for assisted composition using\nmachine learning techniques on the web. Such systems are built for various\ntasks such as melodic, harmonic or rhythm generation, music interpolation,\ncontinuation and style imitation. In this paper, we introduce Apollo, an\ninteractive music application for generating symbolic phrases of conventional\nwestern music using corpus-based style imitation techniques. In addition to\nenabling the construction and management of symbolic musical corpora, the\nsystem makes it possible for music artists and researchers to generate new\nmusical phrases in the style of the proposed corpus. The system is available as\na desktop application. The generated symbolic music materials, encoded in the\nMIDI format, can be exported or streamed for various purposes including using\nthem as seed material for musical projects. We present the system design,\nimplementation details, discuss and conclude with future work for the system.", "AI": {"tldr": "Apollo\u662f\u4e00\u4e2a\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u4ea4\u4e92\u5f0f\u97f3\u4e50\u751f\u6210\u7cfb\u7edf\uff0c\u7528\u4e8e\u901a\u8fc7\u98ce\u683c\u6a21\u4eff\u6280\u672f\u751f\u6210\u897f\u65b9\u97f3\u4e50\u7684\u7b26\u53f7\u77ed\u8bed\u3002", "motivation": "\u63a2\u7d22\u673a\u5668\u5b66\u4e60\u548c\u7f51\u7edc\u6280\u672f\u5728\u8f85\u52a9\u97f3\u4e50\u521b\u4f5c\u4e2d\u7684\u5e94\u7528\uff0c\u7279\u522b\u662f\u98ce\u683c\u6a21\u4eff\u548c\u97f3\u4e50\u751f\u6210\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u8bed\u6599\u5e93\u7684\u98ce\u683c\u6a21\u4eff\u6280\u672f\uff0c\u6784\u5efa\u548c\u7ba1\u7406\u7b26\u53f7\u97f3\u4e50\u8bed\u6599\u5e93\uff0c\u751f\u6210\u65b0\u7684\u97f3\u4e50\u77ed\u8bed\u3002", "result": "\u5f00\u53d1\u4e86\u684c\u9762\u5e94\u7528\u7a0b\u5e8f\uff0c\u652f\u6301MIDI\u683c\u5f0f\u7684\u97f3\u4e50\u6750\u6599\u5bfc\u51fa\u548c\u6d41\u5f0f\u4f20\u8f93\u3002", "conclusion": "\u7cfb\u7edf\u5c55\u793a\u4e86\u97f3\u4e50\u751f\u6210\u548c\u98ce\u683c\u6a21\u4eff\u7684\u6f5c\u529b\uff0c\u672a\u6765\u5de5\u4f5c\u5c06\u8fdb\u4e00\u6b65\u4f18\u5316\u529f\u80fd\u3002"}}
{"id": "2504.14200", "pdf": "https://arxiv.org/pdf/2504.14200", "abs": "https://arxiv.org/abs/2504.14200", "authors": ["Huiyi Chen", "Jiawei Peng", "Kaihua Tang", "Xin Geng", "Xu Yang"], "title": "Enhancing Multimodal In-Context Learning for Image Classification through Coreset Optimization", "categories": ["cs.CV", "cs.AI"], "comment": "11 pages, 5 figures", "summary": "In-context learning (ICL) enables Large Vision-Language Models (LVLMs) to\nadapt to new tasks without parameter updates, using a few demonstrations from a\nlarge support set. However, selecting informative demonstrations leads to high\ncomputational and memory costs. While some methods explore selecting a small\nand representative coreset in the text classification, evaluating all support\nset samples remains costly, and discarded samples lead to unnecessary\ninformation loss. These methods may also be less effective for image\nclassification due to differences in feature spaces. Given these limitations,\nwe propose Key-based Coreset Optimization (KeCO), a novel framework that\nleverages untapped data to construct a compact and informative coreset. We\nintroduce visual features as keys within the coreset, which serve as the anchor\nfor identifying samples to be updated through different selection strategies.\nBy leveraging untapped samples from the support set, we update the keys of\nselected coreset samples, enabling the randomly initialized coreset to evolve\ninto a more informative coreset under low computational cost. Through extensive\nexperiments on coarse-grained and fine-grained image classification benchmarks,\nwe demonstrate that KeCO effectively enhances ICL performance for image\nclassification task, achieving an average improvement of more than 20\\%.\nNotably, we evaluate KeCO under a simulated online scenario, and the strong\nperformance in this scenario highlights the practical value of our framework\nfor resource-constrained real-world scenarios.", "AI": {"tldr": "KeCO\u6846\u67b6\u901a\u8fc7\u89c6\u89c9\u7279\u5f81\u4f5c\u4e3a\u5173\u952e\u70b9\uff0c\u4f18\u5316\u6838\u5fc3\u96c6\u9009\u62e9\uff0c\u63d0\u5347\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u6027\u80fd\uff0c\u5e73\u5747\u6539\u8fdb\u8d85\u8fc720%\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u56fe\u50cf\u5206\u7c7b\u4e2d\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u4fe1\u606f\u635f\u5931\u4e25\u91cd\uff0cKeCO\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u5229\u7528\u672a\u5229\u7528\u6570\u636e\u6784\u5efa\u7d27\u51d1\u6838\u5fc3\u96c6\uff0c\u901a\u8fc7\u89c6\u89c9\u7279\u5f81\u4f5c\u4e3a\u5173\u952e\u70b9\u66f4\u65b0\u6837\u672c\u3002", "result": "\u5728\u7c97\u7c92\u5ea6\u548c\u7ec6\u7c92\u5ea6\u56fe\u50cf\u5206\u7c7b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6027\u80fd\u5e73\u5747\u63d0\u5347\u8d85\u8fc720%\u3002", "conclusion": "KeCO\u5728\u8d44\u6e90\u53d7\u9650\u7684\u5b9e\u9645\u573a\u666f\u4e2d\u5177\u6709\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2504.14058", "pdf": "https://arxiv.org/pdf/2504.14058", "abs": "https://arxiv.org/abs/2504.14058", "authors": ["Renaud Bougueng Tchemeube", "Jeff Ens", "Philippe Pasquier"], "title": "Calliope: An Online Generative Music System for Symbolic Multi-Track Composition", "categories": ["cs.HC", "cs.LG", "cs.SD"], "comment": "5 pages, 5 figures, first published at the 13th International\n  Conference on Computational Creativity (ICCC 2022), Bozen-Bolzano, Italy", "summary": "With the rise of artificial intelligence in recent years, there has been a\nrapid increase in its application towards creative domains, including music.\nThere exist many systems built that apply machine learning approaches to the\nproblem of computer-assisted music composition (CAC). Calliope is a web\napplication that assists users in performing a variety of multi-track\ncomposition tasks in the symbolic domain. The user can upload (Musical\nInstrument Digital Interface) MIDI files, visualize and edit MIDI tracks, and\ngenerate partial (via bar in-filling) or complete multi-track content using the\nMulti-Track Music Machine (MMM). Generation of new MIDI excerpts can be done in\nbatch and can be combined with active playback listening for an enhanced\nassisted-composition workflow. The user can export generated MIDI materials or\ndirectly stream MIDI playback from the system to their favorite Digital Audio\nWorkstation (DAW). We present a demonstration of the system, its features,\ngenerative parameters and describe the co-creative workflows that it affords.", "AI": {"tldr": "Calliope\u662f\u4e00\u4e2a\u57fa\u4e8eWeb\u7684\u5e94\u7528\u7a0b\u5e8f\uff0c\u5229\u7528\u4eba\u5de5\u667a\u80fd\u8f85\u52a9\u7528\u6237\u8fdb\u884c\u591a\u8f68\u97f3\u4e50\u521b\u4f5c\uff0c\u652f\u6301MIDI\u6587\u4ef6\u7f16\u8f91\u3001\u751f\u6210\u548c\u64ad\u653e\u3002", "motivation": "\u968f\u7740\u4eba\u5de5\u667a\u80fd\u5728\u521b\u610f\u9886\u57df\u7684\u5e94\u7528\u589e\u52a0\uff0c\u5f00\u53d1\u5de5\u5177\u8f85\u52a9\u97f3\u4e50\u521b\u4f5c\u6210\u4e3a\u9700\u6c42\u3002", "method": "\u901a\u8fc7Multi-Track Music Machine (MMM)\u5b9e\u73b0MIDI\u6587\u4ef6\u7684\u7f16\u8f91\u3001\u751f\u6210\u548c\u64ad\u653e\uff0c\u652f\u6301\u6279\u91cf\u751f\u6210\u548c\u5b9e\u65f6\u64ad\u653e\u3002", "result": "\u7cfb\u7edf\u5c55\u793a\u4e86\u5176\u529f\u80fd\u3001\u751f\u6210\u53c2\u6570\u53ca\u534f\u540c\u521b\u4f5c\u6d41\u7a0b\u3002", "conclusion": "Calliope\u4e3a\u97f3\u4e50\u521b\u4f5c\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u8f85\u52a9\u5de5\u5177\uff0c\u589e\u5f3a\u4e86\u521b\u4f5c\u6d41\u7a0b\u3002"}}
{"id": "2504.14202", "pdf": "https://arxiv.org/pdf/2504.14202", "abs": "https://arxiv.org/abs/2504.14202", "authors": ["Zichuan Liu", "Liming Jiang", "Qing Yan", "Yumin Jia", "Hao Kang", "Xin Lu"], "title": "Learning Joint ID-Textual Representation for ID-Preserving Image Synthesis", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "We propose a novel framework for ID-preserving generation using a multi-modal\nencoding strategy rather than injecting identity features via adapters into\npre-trained models. Our method treats identity and text as a unified\nconditioning input. To achieve this, we introduce FaceCLIP, a multi-modal\nencoder that learns a joint embedding space for both identity and textual\nsemantics. Given a reference face and a text prompt, FaceCLIP produces a\nunified representation that encodes both identity and text, which conditions a\nbase diffusion model to generate images that are identity-consistent and\ntext-aligned. We also present a multi-modal alignment algorithm to train\nFaceCLIP, using a loss that aligns its joint representation with face, text,\nand image embedding spaces. We then build FaceCLIP-SDXL, an ID-preserving image\nsynthesis pipeline by integrating FaceCLIP with Stable Diffusion XL (SDXL).\nCompared to prior methods, FaceCLIP-SDXL enables photorealistic portrait\ngeneration with better identity preservation and textual relevance. Extensive\nexperiments demonstrate its quantitative and qualitative superiority.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u6a21\u6001\u7f16\u7801\u7b56\u7565\u7684ID\u4fdd\u7559\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u7edf\u4e00\u5904\u7406\u8eab\u4efd\u548c\u6587\u672c\u8f93\u5165\uff0c\u7ed3\u5408FaceCLIP\u548cSDXL\u5b9e\u73b0\u9ad8\u8d28\u91cf\u8096\u50cf\u751f\u6210\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u8fc7\u9002\u914d\u5668\u6ce8\u5165\u8eab\u4efd\u7279\u5f81\uff0c\u9650\u5236\u4e86\u751f\u6210\u8d28\u91cf\u548c\u7075\u6d3b\u6027\uff0c\u9700\u8981\u66f4\u7edf\u4e00\u7684\u6761\u4ef6\u8f93\u5165\u65b9\u5f0f\u3002", "method": "\u5f15\u5165FaceCLIP\u591a\u6a21\u6001\u7f16\u7801\u5668\uff0c\u5b66\u4e60\u8eab\u4efd\u4e0e\u6587\u672c\u7684\u8054\u5408\u5d4c\u5165\u7a7a\u95f4\uff0c\u7ed3\u5408\u6269\u6563\u6a21\u578b\u751f\u6210\u56fe\u50cf\uff0c\u5e76\u8bbe\u8ba1\u591a\u6a21\u6001\u5bf9\u9f50\u7b97\u6cd5\u8bad\u7ec3FaceCLIP\u3002", "result": "FaceCLIP-SDXL\u5728\u8eab\u4efd\u4fdd\u7559\u548c\u6587\u672c\u5bf9\u9f50\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u751f\u6210\u66f4\u903c\u771f\u7684\u8096\u50cf\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u5728ID\u4fdd\u7559\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u591a\u6a21\u6001\u6761\u4ef6\u751f\u6210\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2504.14076", "pdf": "https://arxiv.org/pdf/2504.14076", "abs": "https://arxiv.org/abs/2504.14076", "authors": ["Alice Zhang", "Edison Thomaz", "Lie Lu"], "title": "Transformation of audio embeddings into interpretable, concept-based representations", "categories": ["cs.SD", "cs.LG", "eess.AS"], "comment": "Accepted to International Joint Conference on Neural Networks (IJCNN)\n  2025", "summary": "Advancements in audio neural networks have established state-of-the-art\nresults on downstream audio tasks. However, the black-box structure of these\nmodels makes it difficult to interpret the information encoded in their\ninternal audio representations. In this work, we explore the semantic\ninterpretability of audio embeddings extracted from these neural networks by\nleveraging CLAP, a contrastive learning model that brings audio and text into a\nshared embedding space. We implement a post-hoc method to transform CLAP\nembeddings into concept-based, sparse representations with semantic\ninterpretability. Qualitative and quantitative evaluations show that the\nconcept-based representations outperform or match the performance of original\naudio embeddings on downstream tasks while providing interpretability.\nAdditionally, we demonstrate that fine-tuning the concept-based representations\ncan further improve their performance on downstream tasks. Lastly, we publish\nthree audio-specific vocabularies for concept-based interpretability of audio\nembeddings.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540e\u5904\u7406\u65b9\u6cd5\uff0c\u5c06CLAP\u97f3\u9891\u5d4c\u5165\u8f6c\u6362\u4e3a\u57fa\u4e8e\u6982\u5ff5\u7684\u7a00\u758f\u8868\u793a\uff0c\u63d0\u5347\u8bed\u4e49\u53ef\u89e3\u91ca\u6027\uff0c\u5e76\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u97f3\u9891\u795e\u7ecf\u7f51\u7edc\u7684\u5185\u90e8\u8868\u793a\u96be\u4ee5\u89e3\u91ca\uff0c\u7814\u7a76\u65e8\u5728\u63d0\u5347\u5176\u8bed\u4e49\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u5229\u7528CLAP\u6a21\u578b\u5c06\u97f3\u9891\u548c\u6587\u672c\u5d4c\u5165\u5171\u4eab\u7a7a\u95f4\uff0c\u540e\u5904\u7406\u8f6c\u6362\u4e3a\u57fa\u4e8e\u6982\u5ff5\u7684\u7a00\u758f\u8868\u793a\u3002", "result": "\u57fa\u4e8e\u6982\u5ff5\u7684\u8868\u793a\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u6216\u5339\u914d\u539f\u59cb\u5d4c\u5165\uff0c\u4e14\u53ef\u89e3\u91ca\u6027\u66f4\u5f3a\u3002\u5fae\u8c03\u540e\u6027\u80fd\u8fdb\u4e00\u6b65\u63d0\u5347\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u97f3\u9891\u5d4c\u5165\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u5347\u6027\u80fd\uff0c\u5e76\u53d1\u5e03\u4e86\u4e09\u4e2a\u97f3\u9891\u4e13\u7528\u8bcd\u6c47\u8868\u3002"}}
{"id": "2504.14259", "pdf": "https://arxiv.org/pdf/2504.14259", "abs": "https://arxiv.org/abs/2504.14259", "authors": ["Hadeel Jazzaa", "Thomas McCluskey", "David Peebles"], "title": "Experience-based Refinement of Task Planning Knowledge in Autonomous Robots", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "The requirement for autonomous robots to exhibit higher-level cognitive\nskills by planning and adapting in an ever-changing environment is indeed a\ngreat challenge for the AI community. Progress has been made in the automated\nplanning community on refinement and repair of an agent's symbolic knowledge to\ndo task planning in an incomplete or changing environmental model, but these\nadvances up to now have not been transferred to real physical robots. This\npaper demonstrates how a physical robot can be capable of adapting its symbolic\nknowledge of the environment, by using experiences in robot action execution to\ndrive knowledge refinement and hence to improve the success rate of the task\nplans the robot creates. To implement more robust planning systems, we propose\na method for refining domain knowledge to improve the knowledge on which\nintelligent robot behavior is based. This architecture has been implemented and\nevaluated using a NAO robot. The refined knowledge leads to the future\nsynthesis of task plans which demonstrate decreasing rates of failure over time\nas faulty knowledge is removed or adjusted.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b9\u6cd5\uff0c\u901a\u8fc7\u673a\u5668\u4eba\u6267\u884c\u52a8\u4f5c\u7684\u7ecf\u9a8c\u6765\u8c03\u6574\u5176\u7b26\u53f7\u77e5\u8bc6\uff0c\u4ece\u800c\u63d0\u9ad8\u4efb\u52a1\u89c4\u5212\u7684\u6210\u529f\u7387\u3002", "motivation": "\u81ea\u4e3b\u673a\u5668\u4eba\u9700\u5728\u52a8\u6001\u73af\u5883\u4e2d\u5c55\u73b0\u9ad8\u7ea7\u8ba4\u77e5\u80fd\u529b\uff0c\u4f46\u76ee\u524d\u7b26\u53f7\u77e5\u8bc6\u7684\u6539\u8fdb\u5c1a\u672a\u5e94\u7528\u4e8e\u5b9e\u9645\u7269\u7406\u673a\u5668\u4eba\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u901a\u8fc7\u673a\u5668\u4eba\u6267\u884c\u52a8\u4f5c\u7ecf\u9a8c\u9a71\u52a8\u77e5\u8bc6\u6539\u8fdb\u7684\u65b9\u6cd5\uff0c\u4ee5\u589e\u5f3a\u4efb\u52a1\u89c4\u5212\u7684\u9c81\u68d2\u6027\u3002", "result": "\u5728NAO\u673a\u5668\u4eba\u4e0a\u5b9e\u73b0\u5e76\u8bc4\u4f30\uff0c\u7ed3\u679c\u663e\u793a\u4efb\u52a1\u89c4\u5212\u5931\u8d25\u7387\u968f\u65f6\u95f4\u964d\u4f4e\u3002", "conclusion": "\u901a\u8fc7\u77e5\u8bc6\u6539\u8fdb\uff0c\u673a\u5668\u4eba\u4efb\u52a1\u89c4\u5212\u7684\u6210\u529f\u7387\u663e\u8457\u63d0\u5347\u3002"}}
{"id": "2504.14115", "pdf": "https://arxiv.org/pdf/2504.14115", "abs": "https://arxiv.org/abs/2504.14115", "authors": ["Matt I. B. Oddo", "Ryan Smith", "Stephen Kobourov", "Tamara Munzner"], "title": "Visualization Tasks for Unlabelled Graphs", "categories": ["cs.HC", "cs.LG"], "comment": null, "summary": "We investigate tasks that can be accomplished with unlabelled graphs, where\nnodes do not have persistent or semantically meaningful labels. New techniques\nto visualize these graphs have been proposed, but more understanding of\nunlabelled graph tasks is required before they can be adequately evaluated.\nSome tasks apply to both labelled and unlabelled graphs, but many do not\ntranslate between these contexts. We propose a taxonomy of unlabelled graph\nabstract tasks, organized according to the Scope of the data at play, the\nAction intended by the user, and the Target data under consideration. We show\nthe descriptive power of this task abstraction by connecting to concrete\nexamples from previous frameworks, and connect these abstractions to real-world\nproblems. To showcase the evaluative power of the taxonomy, we perform a\npreliminary assessment of 6 visualizations for each task. For each combination\nof task and visual encoding, we consider the effort required from viewers, the\nlikelihood of task success, and how both factors vary between small-scale and\nlarge-scale graphs.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u65e0\u6807\u7b7e\u56fe\u7684\u4efb\u52a1\u5206\u7c7b\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8eScope\u3001Action\u548cTarget\u7684\u4efb\u52a1\u5206\u7c7b\u6cd5\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u8bc4\u4f30\u4e866\u79cd\u53ef\u89c6\u5316\u65b9\u6cd5\u7684\u6548\u679c\u3002", "motivation": "\u9700\u8981\u66f4\u6df1\u5165\u7406\u89e3\u65e0\u6807\u7b7e\u56fe\u7684\u4efb\u52a1\u7279\u6027\uff0c\u4ee5\u4fbf\u66f4\u597d\u5730\u8bc4\u4f30\u76f8\u5173\u53ef\u89c6\u5316\u6280\u672f\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eScope\u3001Action\u548cTarget\u7684\u4efb\u52a1\u5206\u7c7b\u6cd5\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u8bc4\u4f30\u4e866\u79cd\u53ef\u89c6\u5316\u65b9\u6cd5\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u5206\u7c7b\u6cd5\u80fd\u591f\u6709\u6548\u63cf\u8ff0\u548c\u8bc4\u4f30\u65e0\u6807\u7b7e\u56fe\u4efb\u52a1\uff0c\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u4e0d\u540c\u53ef\u89c6\u5316\u65b9\u6cd5\u5728\u4efb\u52a1\u5b8c\u6210\u5ea6\u548c\u7528\u6237\u52aa\u529b\u65b9\u9762\u5b58\u5728\u5dee\u5f02\u3002", "conclusion": "\u63d0\u51fa\u7684\u5206\u7c7b\u6cd5\u4e3a\u65e0\u6807\u7b7e\u56fe\u4efb\u52a1\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u6709\u52a9\u4e8e\u6307\u5bfc\u672a\u6765\u53ef\u89c6\u5316\u6280\u672f\u7684\u8bbe\u8ba1\u3002"}}
{"id": "2504.14301", "pdf": "https://arxiv.org/pdf/2504.14301", "abs": "https://arxiv.org/abs/2504.14301", "authors": ["Nazia Aslam", "Kamal Nasrollahi"], "title": "Balancing Privacy and Action Performance: A Penalty-Driven Approach to Image Anonymization", "categories": ["cs.CV", "cs.AI", "cs.MM"], "comment": "Accepted to CVPRW 2025", "summary": "The rapid development of video surveillance systems for object detection,\ntracking, activity recognition, and anomaly detection has revolutionized our\nday-to-day lives while setting alarms for privacy concerns. It isn't easy to\nstrike a balance between visual privacy and action recognition performance in\nmost computer vision models. Is it possible to safeguard privacy without\nsacrificing performance? It poses a formidable challenge, as even minor privacy\nenhancements can lead to substantial performance degradation. To address this\nchallenge, we propose a privacy-preserving image anonymization technique that\noptimizes the anonymizer using penalties from the utility branch, ensuring\nimproved action recognition performance while minimally affecting privacy\nleakage. This approach addresses the trade-off between minimizing privacy\nleakage and maintaining high action performance. The proposed approach is\nprimarily designed to align with the regulatory standards of the EU AI Act and\nGDPR, ensuring the protection of personally identifiable information while\nmaintaining action performance. To the best of our knowledge, we are the first\nto introduce a feature-based penalty scheme that exclusively controls the\naction features, allowing freedom to anonymize private attributes. Extensive\nexperiments were conducted to validate the effectiveness of the proposed\nmethod. The results demonstrate that applying a penalty to anonymizer from\nutility branch enhances action performance while maintaining nearly consistent\nprivacy leakage across different penalty settings.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9690\u79c1\u4fdd\u62a4\u7684\u56fe\u50cf\u533f\u540d\u5316\u6280\u672f\uff0c\u901a\u8fc7\u4f18\u5316\u533f\u540d\u5668\u4ee5\u5e73\u8861\u9690\u79c1\u4fdd\u62a4\u548c\u52a8\u4f5c\u8bc6\u522b\u6027\u80fd\uff0c\u7b26\u5408\u6b27\u76dfAI\u6cd5\u6848\u548cGDPR\u6807\u51c6\u3002", "motivation": "\u89e3\u51b3\u89c6\u9891\u76d1\u63a7\u7cfb\u7edf\u4e2d\u9690\u79c1\u4fdd\u62a4\u4e0e\u52a8\u4f5c\u8bc6\u522b\u6027\u80fd\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\uff0c\u786e\u4fdd\u5728\u4fdd\u62a4\u4e2a\u4eba\u9690\u79c1\u7684\u540c\u65f6\u4e0d\u727a\u7272\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u7279\u5f81\u7684\u60e9\u7f5a\u65b9\u6848\uff0c\u901a\u8fc7\u4f18\u5316\u533f\u540d\u5668\u6765\u6700\u5c0f\u5316\u9690\u79c1\u6cc4\u6f0f\u5e76\u4fdd\u6301\u9ad8\u52a8\u4f5c\u8bc6\u522b\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u9690\u79c1\u6cc4\u6f0f\u4e00\u81f4\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u52a8\u4f5c\u8bc6\u522b\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u5e73\u8861\u4e86\u9690\u79c1\u4fdd\u62a4\u4e0e\u6027\u80fd\u9700\u6c42\uff0c\u4e3a\u9690\u79c1\u4fdd\u62a4\u6280\u672f\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2504.14131", "pdf": "https://arxiv.org/pdf/2504.14131", "abs": "https://arxiv.org/abs/2504.14131", "authors": ["Ole-Christian Galbo Engstr\u00f8m", "Michela Albano-Gaglio", "Erik Schou Dreier", "Yamine Bouzembrak", "Maria Font-i-Furnols", "Puneet Mishra", "Kim Steenstrup Pedersen"], "title": "Transforming hyperspectral images into chemical maps: A new deep learning based approach to hyperspectral image processing", "categories": ["cs.CV", "cs.LG", "q-bio.QM"], "comment": null, "summary": "Current approaches to chemical map generation from hyperspectral images are\nbased on models such as partial least squares (PLS) regression, generating\npixel-wise predictions that do not consider spatial context and suffer from a\nhigh degree of noise. This study proposes an end-to-end deep learning approach\nusing a modified version of U-Net and a custom loss function to directly obtain\nchemical maps from hyperspectral images, skipping all intermediate steps\nrequired for traditional pixel-wise analysis. We compare the U-Net with the\ntraditional PLS regression on a real dataset of pork belly samples with\nassociated mean fat reference values. The U-Net obtains a test set root mean\nsquared error of between 9% and 13% lower than that of PLS regression on the\ntask of mean fat prediction. At the same time, U-Net generates fine detail\nchemical maps where 99.91% of the variance is spatially correlated. Conversely,\nonly 2.53% of the variance in the PLS-generated chemical maps is spatially\ncorrelated, indicating that each pixel-wise prediction is largely independent\nof neighboring pixels. Additionally, while the PLS-generated chemical maps\ncontain predictions far beyond the physically possible range of 0-100%, U-Net\nlearns to stay inside this range. Thus, the findings of this study indicate\nthat U-Net is superior to PLS for chemical map generation.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6539\u8fdbU-Net\u548c\u81ea\u5b9a\u4e49\u635f\u5931\u51fd\u6570\u7684\u7aef\u5230\u7aef\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u76f4\u63a5\u4ece\u9ad8\u5149\u8c31\u56fe\u50cf\u751f\u6210\u5316\u5b66\u56fe\u8c31\uff0c\u4f18\u4e8e\u4f20\u7edf\u7684PLS\u56de\u5f52\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\uff08\u5982PLS\u56de\u5f52\uff09\u5728\u751f\u6210\u5316\u5b66\u56fe\u8c31\u65f6\u672a\u8003\u8651\u7a7a\u95f4\u4e0a\u4e0b\u6587\uff0c\u4e14\u566a\u58f0\u8f83\u5927\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u4f18\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u6539\u8fdb\u7684U-Net\u548c\u81ea\u5b9a\u4e49\u635f\u5931\u51fd\u6570\uff0c\u8df3\u8fc7\u4f20\u7edf\u50cf\u7d20\u7ea7\u5206\u6790\u7684\u4e2d\u95f4\u6b65\u9aa4\uff0c\u76f4\u63a5\u4ece\u9ad8\u5149\u8c31\u56fe\u50cf\u751f\u6210\u5316\u5b66\u56fe\u8c31\u3002", "result": "U-Net\u5728\u5e73\u5747\u8102\u80aa\u9884\u6d4b\u4efb\u52a1\u4e0a\u7684\u6d4b\u8bd5\u96c6\u5747\u65b9\u6839\u8bef\u5dee\u6bd4PLS\u4f4e9%-13%\uff0c\u4e14\u751f\u6210\u7684\u5316\u5b66\u56fe\u8c3199.91%\u7684\u65b9\u5dee\u5177\u6709\u7a7a\u95f4\u76f8\u5173\u6027\u3002", "conclusion": "U-Net\u5728\u5316\u5b66\u56fe\u8c31\u751f\u6210\u4e2d\u4f18\u4e8ePLS\uff0c\u80fd\u591f\u751f\u6210\u66f4\u7cbe\u7ec6\u4e14\u7269\u7406\u5408\u7406\u7684\u56fe\u8c31\u3002"}}
{"id": "2504.14135", "pdf": "https://arxiv.org/pdf/2504.14135", "abs": "https://arxiv.org/abs/2504.14135", "authors": ["Jonathan Embley-Riches", "Jianwei Liu", "Simon Julier", "Dimitrios Kanoulas"], "title": "Unreal Robotics Lab: A High-Fidelity Robotics Simulator with Advanced Physics and Rendering", "categories": ["cs.RO", "cs.CV", "cs.GR", "cs.LG"], "comment": null, "summary": "High-fidelity simulation is essential for robotics research, enabling safe\nand efficient testing of perception, control, and navigation algorithms.\nHowever, achieving both photorealistic rendering and accurate physics modeling\nremains a challenge. This paper presents a novel simulation framework--the\nUnreal Robotics Lab (URL) that integrates the Unreal Engine's advanced\nrendering capabilities with MuJoCo's high-precision physics simulation. Our\napproach enables realistic robotic perception while maintaining accurate\nphysical interactions, facilitating benchmarking and dataset generation for\nvision-based robotics applications. The system supports complex environmental\neffects, such as smoke, fire, and water dynamics, which are critical for\nevaluating robotic performance under adverse conditions. We benchmark visual\nnavigation and SLAM methods within our framework, demonstrating its utility for\ntesting real-world robustness in controlled yet diverse scenarios. By bridging\nthe gap between physics accuracy and photorealistic rendering, our framework\nprovides a powerful tool for advancing robotics research and sim-to-real\ntransfer.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408Unreal Engine\u7684\u9ad8\u4fdd\u771f\u6e32\u67d3\u4e0eMuJoCo\u7cbe\u786e\u7269\u7406\u6a21\u62df\u7684\u4eff\u771f\u6846\u67b6URL\uff0c\u7528\u4e8e\u673a\u5668\u4eba\u7814\u7a76\u3002", "motivation": "\u9ad8\u4fdd\u771f\u4eff\u771f\u5bf9\u673a\u5668\u4eba\u7814\u7a76\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u540c\u65f6\u5b9e\u73b0\u903c\u771f\u6e32\u67d3\u548c\u7cbe\u786e\u7269\u7406\u6a21\u62df\u3002", "method": "\u901a\u8fc7\u6574\u5408Unreal Engine\u7684\u6e32\u67d3\u80fd\u529b\u548cMuJoCo\u7684\u7269\u7406\u6a21\u62df\uff0c\u6784\u5efa\u4e86URL\u6846\u67b6\u3002", "result": "\u652f\u6301\u590d\u6742\u73af\u5883\u6548\u679c\uff08\u5982\u70df\u96fe\u3001\u706b\u7130\u3001\u6c34\u52a8\u6001\uff09\uff0c\u5e76\u6210\u529f\u7528\u4e8e\u89c6\u89c9\u5bfc\u822a\u548cSLAM\u65b9\u6cd5\u7684\u6d4b\u8bd5\u3002", "conclusion": "URL\u6846\u67b6\u4e3a\u673a\u5668\u4eba\u7814\u7a76\u548c\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u8fc1\u79fb\u63d0\u4f9b\u4e86\u5f3a\u5927\u5de5\u5177\u3002"}}
{"id": "2504.14320", "pdf": "https://arxiv.org/pdf/2504.14320", "abs": "https://arxiv.org/abs/2504.14320", "authors": ["Nimisha Karnatak", "Adrien Baranes", "Rob Marchant", "Huinan Zeng", "Tr\u00edona Butler", "Kristen Olson"], "title": "Expanding the Generative AI Design Space through Structured Prompting and Multimodal Interfaces", "categories": ["cs.HC", "cs.AI"], "comment": "Accepted at CHI'25 Workshop on Designing and Developing User\n  Interfaces with AI", "summary": "Text-based prompting remains the dominant interaction paradigm in generative\nAI, yet it often results in a high-friction experience for novice users, such\nas small business owners (SBOs), attempting to articulate creative or\ndomain-specific goals for advertising. To investigate this challenge, we\nconducted a study with six SBOs in the United Kingdom, focusing on their\nadvertising practices and perceptions and usage of AI tools in this context.\nOur findings surfaced two persistent breakdowns in current generative AI\nsystems: first, the cognitive burden of prompt engineering, as users struggled\nto translate abstract creative goals into effective textual inputs; and second,\nthe frequent generation of generic outputs that failed to align with users'\narticulated brand vision. To address these issues, we developed ACAI (AI\nCo-Creation for Advertising and Inspiration), a multimodal, GenAI-powered\nadvertisement creation tool designed to support novice designers by reimagining\nthe prompt interface. ACAI features a structured, panel-based interface\ncomposed of three modules: the Branding Panel, the Audience & Goals Panel, and\nthe Inspiration Board Panel to provide SBOs with outputs that align with their\ncreative vision by reducing prompt ambiguity. This work contributes to HCI\nresearch on generative systems by showing how structured interfaces can\nforeground user-defined context to improve both alignment and promptability in\nnovice workflows.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u751f\u6210\u5f0fAI\u4e2d\u6587\u672c\u63d0\u793a\u5bf9\u65b0\u624b\u7528\u6237\u7684\u9ad8\u6469\u64e6\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u5de5\u5177ACAI\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u754c\u9762\u51cf\u5c11\u63d0\u793a\u6a21\u7cca\u6027\u3002", "motivation": "\u89e3\u51b3\u65b0\u624b\u7528\u6237\uff08\u5982\u5c0f\u4f01\u4e1a\u4e3b\uff09\u5728\u4f7f\u7528\u751f\u6210\u5f0fAI\u65f6\u96be\u4ee5\u8868\u8fbe\u521b\u610f\u76ee\u6807\u7684\u95ee\u9898\u3002", "method": "\u7814\u7a76\u516d\u4f4d\u82f1\u56fd\u5c0f\u4f01\u4e1a\u4e3b\u7684\u5e7f\u544a\u5b9e\u8df5\uff0c\u5f00\u53d1ACAI\u5de5\u5177\uff0c\u5305\u542b\u54c1\u724c\u3001\u53d7\u4f17\u4e0e\u76ee\u6807\u3001\u7075\u611f\u677f\u4e09\u4e2a\u6a21\u5757\u3002", "result": "\u53d1\u73b0\u5f53\u524d\u751f\u6210\u5f0fAI\u5b58\u5728\u63d0\u793a\u5de5\u7a0b\u8ba4\u77e5\u8d1f\u62c5\u548c\u8f93\u51fa\u901a\u7528\u6027\u95ee\u9898\uff0cACAI\u901a\u8fc7\u7ed3\u6784\u5316\u754c\u9762\u6539\u5584\u4e86\u8fd9\u4e9b\u95ee\u9898\u3002", "conclusion": "\u7ed3\u6784\u5316\u754c\u9762\u80fd\u63d0\u5347\u65b0\u624b\u7528\u6237\u7684\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u5bf9\u9f50\u6027\u548c\u63d0\u793a\u6548\u679c\u3002"}}
{"id": "2504.14335", "pdf": "https://arxiv.org/pdf/2504.14335", "abs": "https://arxiv.org/abs/2504.14335", "authors": ["Zhengbo Zhang", "Yuxi Zhou", "Duo Peng", "Joo-Hwee Lim", "Zhigang Tu", "De Wen Soh", "Lin Geng Foo"], "title": "Visual Prompting for One-shot Controllable Video Editing without Inversion", "categories": ["cs.CV", "cs.AI"], "comment": "accepted by cvpr2025", "summary": "One-shot controllable video editing (OCVE) is an important yet challenging\ntask, aiming to propagate user edits that are made -- using any image editing\ntool -- on the first frame of a video to all subsequent frames, while ensuring\ncontent consistency between edited frames and source frames. To achieve this,\nprior methods employ DDIM inversion to transform source frames into latent\nnoise, which is then fed into a pre-trained diffusion model, conditioned on the\nuser-edited first frame, to generate the edited video. However, the DDIM\ninversion process accumulates errors, which hinder the latent noise from\naccurately reconstructing the source frames, ultimately compromising content\nconsistency in the generated edited frames. To overcome it, our method\neliminates the need for DDIM inversion by performing OCVE through a novel\nperspective based on visual prompting. Furthermore, inspired by consistency\nmodels that can perform multi-step consistency sampling to generate a sequence\nof content-consistent images, we propose a content consistency sampling (CCS)\nto ensure content consistency between the generated edited frames and the\nsource frames. Moreover, we introduce a temporal-content consistency sampling\n(TCS) based on Stein Variational Gradient Descent to ensure temporal\nconsistency across the edited frames. Extensive experiments validate the\neffectiveness of our approach.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700DDIM\u53cd\u8f6c\u7684\u5355\u6b21\u53ef\u63a7\u89c6\u9891\u7f16\u8f91\u65b9\u6cd5\uff0c\u901a\u8fc7\u89c6\u89c9\u63d0\u793a\u548c\u4e00\u81f4\u6027\u91c7\u6837\u786e\u4fdd\u5185\u5bb9\u4e00\u81f4\u6027\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u56e0DDIM\u53cd\u8f6c\u8bef\u5dee\u5bfc\u81f4\u7684\u5185\u5bb9\u4e00\u81f4\u6027\u95ee\u9898\u3002", "method": "\u91c7\u7528\u89c6\u89c9\u63d0\u793a\u89c6\u89d2\uff0c\u63d0\u51fa\u5185\u5bb9\u4e00\u81f4\u6027\u91c7\u6837\uff08CCS\uff09\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u91c7\u6837\uff08TCS\uff09\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u65b0\u65b9\u6cd5\u901a\u8fc7\u6d88\u9664DDIM\u53cd\u8f6c\u548c\u5f15\u5165\u4e00\u81f4\u6027\u91c7\u6837\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7f16\u8f91\u89c6\u9891\u7684\u5185\u5bb9\u548c\u65f6\u5e8f\u4e00\u81f4\u6027\u3002"}}
{"id": "2504.14152", "pdf": "https://arxiv.org/pdf/2504.14152", "abs": "https://arxiv.org/abs/2504.14152", "authors": ["Coleman Hooper", "Charbel Sakr", "Ben Keller", "Rangharajan Venkatesan", "Kurt Keutzer", "Sophia Shao", "Brucek Khailany"], "title": "FGMP: Fine-Grained Mixed-Precision Weight and Activation Quantization for Hardware-Accelerated LLM Inference", "categories": ["cs.AR", "cs.LG"], "comment": null, "summary": "Quantization is a powerful tool to improve large language model (LLM)\ninference efficiency by utilizing more energy-efficient low-precision datapaths\nand reducing memory footprint. However, accurately quantizing LLM weights and\nactivations to low precision is challenging without degrading model accuracy.\nWe propose fine-grained mixed precision (FGMP) quantization, a post-training\nmixed-precision quantization hardware-software co-design methodology that\nmaintains accuracy while quantizing the majority of weights and activations to\nreduced precision. Our work makes the following contributions: 1) We develop a\npolicy that uses the perturbation in each value, weighted by the Fisher\ninformation, to select which weight and activation blocks to keep in higher\nprecision. This approach preserves accuracy by identifying which weight and\nactivation blocks need to be retained in higher precision to minimize the\nperturbation in the model loss. 2) We also propose a sensitivity-weighted\nclipping approach for fine-grained quantization which helps retain accuracy for\nblocks that are quantized to low precision. 3) We then propose hardware\naugmentations to leverage the efficiency benefits of FGMP quantization. Our\nhardware implementation encompasses i) datapath support for FGMP at block\ngranularity, and ii) a mixed-precision activation quantization unit to assign\nactivation blocks to high or low precision on the fly with minimal runtime and\nenergy overhead. Our design, prototyped using NVFP4 (an FP4 format with\nmicroscaling) as the low-precision datatype and FP8 as the high-precision\ndatatype, facilitates efficient FGMP quantization, attaining <1% perplexity\ndegradation on Wikitext-103 for the Llama-2-7B model relative to an all-FP8\nbaseline design while consuming 14% less energy during inference and requiring\n30% less weight memory.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ec6\u7c92\u5ea6\u6df7\u5408\u7cbe\u5ea6\uff08FGMP\uff09\u91cf\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u786c\u4ef6-\u8f6f\u4ef6\u534f\u540c\u8bbe\u8ba1\uff0c\u5728\u964d\u4f4e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u6743\u91cd\u548c\u6fc0\u6d3b\u7684\u7cbe\u5ea6\u65f6\u4fdd\u6301\u51c6\u786e\u6027\uff0c\u540c\u65f6\u663e\u8457\u63d0\u5347\u80fd\u6548\u548c\u51cf\u5c11\u5185\u5b58\u5360\u7528\u3002", "motivation": "\u91cf\u5316\u662f\u63d0\u5347LLM\u63a8\u7406\u6548\u7387\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u4f46\u4f20\u7edf\u91cf\u5316\u65b9\u6cd5\u5728\u964d\u4f4e\u7cbe\u5ea6\u65f6\u5bb9\u6613\u5bfc\u81f4\u6a21\u578b\u51c6\u786e\u6027\u4e0b\u964d\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u7684\u91cf\u5316\u7b56\u7565\u6765\u5e73\u8861\u7cbe\u5ea6\u548c\u6548\u7387\u3002", "method": "1) \u63d0\u51fa\u57fa\u4e8eFisher\u4fe1\u606f\u52a0\u6743\u7684\u6270\u52a8\u7b56\u7565\uff0c\u9009\u62e9\u9700\u8981\u4fdd\u7559\u9ad8\u7cbe\u5ea6\u7684\u6743\u91cd\u548c\u6fc0\u6d3b\u5757\uff1b2) \u63d0\u51fa\u7075\u654f\u5ea6\u52a0\u6743\u88c1\u526a\u65b9\u6cd5\uff0c\u4f18\u5316\u4f4e\u7cbe\u5ea6\u5757\u7684\u91cf\u5316\uff1b3) \u8bbe\u8ba1\u652f\u6301FGMP\u7684\u786c\u4ef6\u589e\u5f3a\u529f\u80fd\uff0c\u5305\u62ec\u5757\u7c92\u5ea6\u6570\u636e\u8def\u5f84\u548c\u52a8\u6001\u6df7\u5408\u7cbe\u5ea6\u6fc0\u6d3b\u91cf\u5316\u5355\u5143\u3002", "result": "\u5728Llama-2-7B\u6a21\u578b\u4e0a\uff0c\u4f7f\u7528NVFP4\u548cFP8\u683c\u5f0f\uff0c\u5b9e\u73b0\u4e86<1%\u7684\u56f0\u60d1\u5ea6\u4e0b\u964d\uff0c\u63a8\u7406\u80fd\u8017\u964d\u4f4e14%\uff0c\u6743\u91cd\u5185\u5b58\u51cf\u5c1130%\u3002", "conclusion": "FGMP\u91cf\u5316\u65b9\u6cd5\u5728\u4fdd\u6301\u6a21\u578b\u51c6\u786e\u6027\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u80fd\u6548\u548c\u5185\u5b58\u6548\u7387\uff0c\u4e3aLLM\u7684\u9ad8\u6548\u63a8\u7406\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.14345", "pdf": "https://arxiv.org/pdf/2504.14345", "abs": "https://arxiv.org/abs/2504.14345", "authors": ["Youngbin Lee", "Yejin Kim", "Suin Kim", "Yongjae Lee"], "title": "Integrating LLM-Generated Views into Mean-Variance Optimization Using the Black-Litterman Model", "categories": ["q-fin.PM", "cs.AI"], "comment": "Presented at the ICLR 2025 Workshop on Financial AI\n  (https://sites.google.com/view/financialaiiclr25/home)", "summary": "Portfolio optimization faces challenges due to the sensitivity in traditional\nmean-variance models. The Black-Litterman model mitigates this by integrating\ninvestor views, but defining these views remains difficult. This study explores\nthe integration of large language models (LLMs) generated views into portfolio\noptimization using the Black-Litterman framework. Our method leverages LLMs to\nestimate expected stock returns from historical prices and company metadata,\nincorporating uncertainty through the variance in predictions. We conduct a\nbacktest of the LLM-optimized portfolios from June 2024 to February 2025,\nrebalancing biweekly using the previous two weeks of price data. As baselines,\nwe compare against the S&P 500, an equal-weighted portfolio, and a traditional\nmean-variance optimized portfolio constructed using the same set of stocks.\nEmpirical results suggest that different LLMs exhibit varying levels of\npredictive optimism and confidence stability, which impact portfolio\nperformance. The source code and data are available at\nhttps://github.com/youngandbin/LLM-MVO-BLM.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5c06\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u751f\u6210\u7684\u89c2\u70b9\u6574\u5408\u5230Black-Litterman\u6846\u67b6\u4e2d\u7684\u6295\u8d44\u7ec4\u5408\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u5386\u53f2\u4ef7\u683c\u548c\u516c\u53f8\u5143\u6570\u636e\u9884\u6d4b\u80a1\u7968\u6536\u76ca\uff0c\u5e76\u6bd4\u8f83\u4e86\u4e0d\u540cLLM\u7684\u8868\u73b0\u3002", "motivation": "\u4f20\u7edf\u5747\u503c-\u65b9\u5dee\u6a21\u578b\u5bf9\u8f93\u5165\u654f\u611f\uff0cBlack-Litterman\u6a21\u578b\u867d\u80fd\u6574\u5408\u6295\u8d44\u8005\u89c2\u70b9\uff0c\u4f46\u89c2\u70b9\u5b9a\u4e49\u56f0\u96be\u3002", "method": "\u5229\u7528LLM\u4ece\u5386\u53f2\u4ef7\u683c\u548c\u516c\u53f8\u5143\u6570\u636e\u4e2d\u9884\u6d4b\u80a1\u7968\u6536\u76ca\uff0c\u5e76\u901a\u8fc7\u9884\u6d4b\u65b9\u5dee\u91cf\u5316\u4e0d\u786e\u5b9a\u6027\uff0c\u8fdb\u884c\u53cc\u5468\u518d\u5e73\u8861\u7684\u56de\u6d4b\u3002", "result": "\u4e0d\u540cLLM\u8868\u73b0\u51fa\u4e0d\u540c\u7a0b\u5ea6\u7684\u9884\u6d4b\u4e50\u89c2\u6027\u548c\u4fe1\u5fc3\u7a33\u5b9a\u6027\uff0c\u5f71\u54cd\u6295\u8d44\u7ec4\u5408\u8868\u73b0\u3002", "conclusion": "LLM\u751f\u6210\u7684\u89c2\u70b9\u53ef\u7528\u4e8eBlack-Litterman\u6846\u67b6\uff0c\u4f46\u9700\u8003\u8651\u6a21\u578b\u9009\u62e9\u5bf9\u7ed3\u679c\u7684\u5f71\u54cd\u3002"}}
{"id": "2504.14157", "pdf": "https://arxiv.org/pdf/2504.14157", "abs": "https://arxiv.org/abs/2504.14157", "authors": ["Magdalena C. Schneider", "Courtney Johnson", "Cedric Allier", "Larissa Heinrich", "Diane Adjavon", "Joren Husic", "Patrick La Rivi\u00e8re", "Stephan Saalfeld", "Hari Shroff"], "title": "DeepPD: Joint Phase and Object Estimation from Phase Diversity with Neural Calibration of a Deformable Mirror", "categories": ["physics.optics", "cs.LG"], "comment": null, "summary": "Sample-induced aberrations and optical imperfections limit the resolution of\nfluorescence microscopy. Phase diversity is a powerful technique that leverages\ncomplementary phase information in sequentially acquired images with\ndeliberately introduced aberrations--the phase diversities--to enable phase and\nobject reconstruction and restore diffraction-limited resolution. These phase\ndiversities are typically introduced into the optical path via a deformable\nmirror. Existing phase-diversity-based methods are limited to Zernike modes,\nrequire large numbers of diversity images, or depend on accurate mirror\ncalibration--which are all suboptimal. We present DeepPD, a deep learning-based\nframework that combines neural representations of the object and wavefront with\na learned model of the deformable mirror to jointly estimate both object and\nphase from only five images. DeepPD improves robustness and reconstruction\nquality over previous approaches, even under severe aberrations. We demonstrate\nits performance on calibration targets and biological samples, including\nimmunolabeled myosin in fixed PtK2 cells.", "AI": {"tldr": "DeepPD\u662f\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u7269\u4f53\u548c\u6ce2\u524d\u7684\u795e\u7ecf\u8868\u793a\u4ee5\u53ca\u53d8\u5f62\u955c\u7684\u6a21\u578b\uff0c\u4ec5\u9700\u4e94\u5f20\u56fe\u50cf\u5373\u53ef\u8054\u5408\u4f30\u8ba1\u7269\u4f53\u548c\u76f8\u4f4d\uff0c\u63d0\u9ad8\u4e86\u9c81\u68d2\u6027\u548c\u91cd\u5efa\u8d28\u91cf\u3002", "motivation": "\u6837\u672c\u5f15\u8d77\u7684\u50cf\u5dee\u548c\u5149\u5b66\u7f3a\u9677\u9650\u5236\u4e86\u8367\u5149\u663e\u5fae\u955c\u7684\u5206\u8fa8\u7387\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u4e8eZernike\u6a21\u5f0f\u3001\u5927\u91cf\u56fe\u50cf\u6216\u7cbe\u786e\u7684\u6821\u51c6\uff0c\u6548\u679c\u4e0d\u7406\u60f3\u3002", "method": "DeepPD\u7ed3\u5408\u4e86\u7269\u4f53\u548c\u6ce2\u524d\u7684\u795e\u7ecf\u8868\u793a\uff0c\u5e76\u5229\u7528\u53d8\u5f62\u955c\u7684\u6a21\u578b\uff0c\u4ec5\u9700\u4e94\u5f20\u56fe\u50cf\u5373\u53ef\u8054\u5408\u4f30\u8ba1\u7269\u4f53\u548c\u76f8\u4f4d\u3002", "result": "DeepPD\u5728\u4e25\u91cd\u50cf\u5dee\u4e0b\u4ecd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u63d0\u9ad8\u4e86\u91cd\u5efa\u8d28\u91cf\u548c\u9c81\u68d2\u6027\uff0c\u5e76\u5728\u6821\u51c6\u76ee\u6807\u548c\u751f\u7269\u6837\u672c\u4e0a\u9a8c\u8bc1\u4e86\u6027\u80fd\u3002", "conclusion": "DeepPD\u4e3a\u8367\u5149\u663e\u5fae\u955c\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u9c81\u68d2\u7684\u76f8\u4f4d\u591a\u6837\u6027\u91cd\u5efa\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5206\u8fa8\u7387\u548c\u91cd\u5efa\u6548\u679c\u3002"}}
{"id": "2504.14164", "pdf": "https://arxiv.org/pdf/2504.14164", "abs": "https://arxiv.org/abs/2504.14164", "authors": ["Kisung You", "Dennis Shung", "Mauro Giuffr\u00e8"], "title": "Learning over von Mises-Fisher Distributions via a Wasserstein-like Geometry", "categories": ["stat.ML", "cs.LG", "stat.ME"], "comment": null, "summary": "We introduce a novel, geometry-aware distance metric for the family of von\nMises-Fisher (vMF) distributions, which are fundamental models for directional\ndata on the unit hypersphere. Although the vMF distribution is widely employed\nin a variety of probabilistic learning tasks involving spherical data,\nprincipled tools for comparing vMF distributions remain limited, primarily due\nto the intractability of normalization constants and the absence of suitable\ngeometric metrics. Motivated by the theory of optimal transport, we propose a\nWasserstein-like distance that decomposes the discrepancy between two vMF\ndistributions into two interpretable components: a geodesic term capturing the\nangular separation between mean directions, and a variance-like term\nquantifying differences in concentration parameters. The derivation leverages a\nGaussian approximation in the high-concentration regime to yield a tractable,\nclosed-form expression that respects the intrinsic spherical geometry. We show\nthat the proposed distance exhibits desirable theoretical properties and\ninduces a latent geometric structure on the space of non-degenerate vMF\ndistributions. As a primary application, we develop the efficient algorithms\nfor vMF mixture reduction, enabling structure-preserving compression of mixture\nmodels in high-dimensional settings. Empirical results on synthetic datasets\nand real-world high-dimensional embeddings, including biomedical sentence\nrepresentations and deep visual features, demonstrate the effectiveness of the\nproposed geometry in distinguishing distributions and supporting interpretable\ninference. This work expands the statistical toolbox for directional data\nanalysis by introducing a tractable, transport-inspired distance tailored to\nthe geometry of the hypersphere.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u51e0\u4f55\u611f\u77e5\u8ddd\u79bb\u5ea6\u91cf\u65b9\u6cd5\uff0c\u7528\u4e8e\u6bd4\u8f83von Mises-Fisher (vMF) \u5206\u5e03\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u5f52\u4e00\u5316\u5e38\u6570\u548c\u51e0\u4f55\u5ea6\u91cf\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "vMF\u5206\u5e03\u5e7f\u6cdb\u7528\u4e8e\u7403\u5f62\u6570\u636e\u7684\u6982\u7387\u5b66\u4e60\u4efb\u52a1\uff0c\u4f46\u7f3a\u4e4f\u5408\u9002\u7684\u6bd4\u8f83\u5de5\u5177\uff0c\u5c24\u5176\u662f\u5f52\u4e00\u5316\u5e38\u6570\u96be\u4ee5\u5904\u7406\u4e14\u7f3a\u4e4f\u51e0\u4f55\u5ea6\u91cf\u3002", "method": "\u57fa\u4e8e\u6700\u4f18\u4f20\u8f93\u7406\u8bba\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7c7b\u4f3cWasserstein\u7684\u8ddd\u79bb\uff0c\u5c06vMF\u5206\u5e03\u7684\u5dee\u5f02\u5206\u89e3\u4e3a\u5747\u503c\u65b9\u5411\u7684\u89d2\u5ea6\u5206\u79bb\u548c\u6d53\u5ea6\u53c2\u6570\u7684\u5dee\u5f02\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u9ad8\u6d53\u5ea6\u533a\u57df\u901a\u8fc7\u9ad8\u65af\u8fd1\u4f3c\u5f97\u5230\u95ed\u5f0f\u89e3\uff0c\u5177\u6709\u7406\u8bba\u4f18\u52bf\uff0c\u5e76\u5728vMF\u6df7\u5408\u6a21\u578b\u538b\u7f29\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u6027\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u65b9\u5411\u6027\u6570\u636e\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u7684\u7edf\u8ba1\u5de5\u5177\uff0c\u9002\u7528\u4e8e\u9ad8\u7ef4\u6570\u636e\u4e14\u652f\u6301\u53ef\u89e3\u91ca\u6027\u63a8\u7406\u3002"}}
{"id": "2504.14280", "pdf": "https://arxiv.org/pdf/2504.14280", "abs": "https://arxiv.org/abs/2504.14280", "authors": ["Jindong Li", "Yongguang Li", "Yali Fu", "Jiahong Liu", "Yixin Liu", "Menglin Yang", "Irwin King"], "title": "CLIP-Powered Domain Generalization and Domain Adaptation: A Comprehensive Survey", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "As machine learning evolves, domain generalization (DG) and domain adaptation\n(DA) have become crucial for enhancing model robustness across diverse\nenvironments. Contrastive Language-Image Pretraining (CLIP) plays a significant\nrole in these tasks, offering powerful zero-shot capabilities that allow models\nto perform effectively in unseen domains. However, there remains a significant\ngap in the literature, as no comprehensive survey currently exists that\nsystematically explores the applications of CLIP in DG and DA, highlighting the\nnecessity for this review. This survey presents a comprehensive review of\nCLIP's applications in DG and DA. In DG, we categorize methods into optimizing\nprompt learning for task alignment and leveraging CLIP as a backbone for\neffective feature extraction, both enhancing model adaptability. For DA, we\nexamine both source-available methods utilizing labeled source data and\nsource-free approaches primarily based on target domain data, emphasizing\nknowledge transfer mechanisms and strategies for improved performance across\ndiverse contexts. Key challenges, including overfitting, domain diversity, and\ncomputational efficiency, are addressed, alongside future research\nopportunities to advance robustness and efficiency in practical applications.\nBy synthesizing existing literature and pinpointing critical gaps, this survey\nprovides valuable insights for researchers and practitioners, proposing\ndirections for effectively leveraging CLIP to enhance methodologies in domain\ngeneralization and adaptation. Ultimately, this work aims to foster innovation\nand collaboration in the quest for more resilient machine learning models that\ncan perform reliably across diverse real-world scenarios. A more up-to-date\nversion of the papers is maintained at:\nhttps://github.com/jindongli-Ai/Survey_on_CLIP-Powered_Domain_Generalization_and_Adaptation.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7efc\u8ff0\u4e86CLIP\u5728\u9886\u57df\u6cdb\u5316\uff08DG\uff09\u548c\u9886\u57df\u9002\u5e94\uff08DA\uff09\u4e2d\u7684\u5e94\u7528\uff0c\u586b\u8865\u4e86\u73b0\u6709\u6587\u732e\u7684\u7a7a\u767d\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u7531\u4e8e\u7f3a\u4e4f\u5173\u4e8eCLIP\u5728DG\u548cDA\u4e2d\u5e94\u7528\u7684\u7cfb\u7edf\u6027\u7efc\u8ff0\uff0c\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u4e3a\u7814\u7a76\u8005\u548c\u5b9e\u8df5\u8005\u63d0\u4f9b\u6709\u4ef7\u503c\u7684\u53c2\u8003\u3002", "method": "\u5728DG\u4e2d\uff0c\u65b9\u6cd5\u5206\u4e3a\u4f18\u5316\u63d0\u793a\u5b66\u4e60\u548c\u5229\u7528CLIP\u4f5c\u4e3a\u7279\u5f81\u63d0\u53d6\u5668\uff1b\u5728DA\u4e2d\uff0c\u5206\u4e3a\u57fa\u4e8e\u6e90\u6570\u636e\u7684\u65b9\u6cd5\u548c\u57fa\u4e8e\u76ee\u6807\u6570\u636e\u7684\u65e0\u6e90\u65b9\u6cd5\u3002", "result": "\u8bba\u6587\u603b\u7ed3\u4e86CLIP\u5728DG\u548cDA\u4e2d\u7684\u5e94\u7528\uff0c\u5e76\u6307\u51fa\u4e86\u5173\u952e\u6311\u6218\uff08\u5982\u8fc7\u62df\u5408\u548c\u8ba1\u7b97\u6548\u7387\uff09\u53ca\u672a\u6765\u673a\u4f1a\u3002", "conclusion": "\u672c\u6587\u4e3aCLIP\u5728DG\u548cDA\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u5168\u9762\u7efc\u8ff0\uff0c\u65e8\u5728\u63a8\u52a8\u66f4\u9c81\u68d2\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u53d1\u5c55\u3002"}}
{"id": "2504.14386", "pdf": "https://arxiv.org/pdf/2504.14386", "abs": "https://arxiv.org/abs/2504.14386", "authors": ["Md Abtahi Majeed Chowdhury", "Md Rifat Ur Rahman", "Akil Ahmad Taki"], "title": "LOOPE: Learnable Optimal Patch Order in Positional Embeddings for Vision Transformers", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Positional embeddings (PE) play a crucial role in Vision Transformers (ViTs)\nby providing spatial information otherwise lost due to the permutation\ninvariant nature of self attention. While absolute positional embeddings (APE)\nhave shown theoretical advantages over relative positional embeddings (RPE),\nparticularly due to the ability of sinusoidal functions to preserve spatial\ninductive biases like monotonicity and shift invariance, a fundamental\nchallenge arises when mapping a 2D grid to a 1D sequence. Existing methods have\nmostly overlooked or never explored the impact of patch ordering in positional\nembeddings. To address this, we propose LOOPE, a learnable patch-ordering\nmethod that optimizes spatial representation for a given set of frequencies,\nproviding a principled approach to patch order optimization. Empirical results\nshow that our PE significantly improves classification accuracy across various\nViT architectures. To rigorously evaluate the effectiveness of positional\nembeddings, we introduce the \"Three Cell Experiment\", a novel benchmarking\nframework that assesses the ability of PEs to retain relative and absolute\npositional information across different ViT architectures. Unlike standard\nevaluations, which typically report a performance gap of 4 to 6% between models\nwith and without PE, our method reveals a striking 30 to 35% difference,\noffering a more sensitive diagnostic tool to measure the efficacy of PEs. Our\nexperimental analysis confirms that the proposed LOOPE demonstrates enhanced\neffectiveness in retaining both relative and absolute positional information.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLOOPE\u7684\u53ef\u5b66\u4e60\u8865\u4e01\u6392\u5e8f\u65b9\u6cd5\uff0c\u7528\u4e8e\u4f18\u5316\u89c6\u89c9Transformer\u4e2d\u7684\u4f4d\u7f6e\u5d4c\u5165\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5206\u7c7b\u51c6\u786e\u6027\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u7684\u8bc4\u4f30\u6846\u67b6\u201cThree Cell Experiment\u201d\u6765\u66f4\u654f\u611f\u5730\u8861\u91cf\u4f4d\u7f6e\u5d4c\u5165\u7684\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5ffd\u89c6\u4e86\u8865\u4e01\u6392\u5e8f\u5bf9\u4f4d\u7f6e\u5d4c\u5165\u7684\u5f71\u54cd\uff0c\u5bfc\u81f42D\u7f51\u683c\u52301D\u5e8f\u5217\u7684\u6620\u5c04\u5b58\u5728\u6311\u6218\u3002", "method": "\u63d0\u51faLOOPE\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316\u8865\u4e01\u6392\u5e8f\u6765\u6539\u8fdb\u7a7a\u95f4\u8868\u793a\uff0c\u5e76\u5f15\u5165\u201cThree Cell Experiment\u201d\u8bc4\u4f30\u6846\u67b6\u3002", "result": "LOOPE\u663e\u8457\u63d0\u5347\u4e86\u5206\u7c7b\u51c6\u786e\u6027\uff0c\u65b0\u8bc4\u4f30\u6846\u67b6\u63ed\u793a\u4e8630-35%\u7684\u6027\u80fd\u5dee\u5f02\uff0c\u8fdc\u9ad8\u4e8e\u4f20\u7edf\u8bc4\u4f30\u76844-6%\u3002", "conclusion": "LOOPE\u5728\u4fdd\u7559\u76f8\u5bf9\u548c\u7edd\u5bf9\u4f4d\u7f6e\u4fe1\u606f\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u4f4d\u7f6e\u5d4c\u5165\u7814\u7a76\u63d0\u4f9b\u4e86\u66f4\u654f\u611f\u7684\u8bca\u65ad\u5de5\u5177\u3002"}}
{"id": "2504.14395", "pdf": "https://arxiv.org/pdf/2504.14395", "abs": "https://arxiv.org/abs/2504.14395", "authors": ["Chung-En", "Yu", "Hsuan-Chih", "Chen", "Brian Jalaian", "Nathaniel D. Bastian"], "title": "Hydra: An Agentic Reasoning Approach for Enhancing Adversarial Robustness and Mitigating Hallucinations in Vision-Language Models", "categories": ["cs.CV", "cs.AI", "cs.MA"], "comment": null, "summary": "To develop trustworthy Vision-Language Models (VLMs), it is essential to\naddress adversarial robustness and hallucination mitigation, both of which\nimpact factual accuracy in high-stakes applications such as defense and\nhealthcare. Existing methods primarily focus on either adversarial defense or\nhallucination post-hoc correction, leaving a gap in unified robustness\nstrategies. We introduce \\textbf{Hydra}, an adaptive agentic framework that\nenhances plug-in VLMs through iterative reasoning, structured critiques, and\ncross-model verification, improving both resilience to adversarial\nperturbations and intrinsic model errors. Hydra employs an Action-Critique\nLoop, where it retrieves and critiques visual information, leveraging\nChain-of-Thought (CoT) and In-Context Learning (ICL) techniques to refine\noutputs dynamically. Unlike static post-hoc correction methods, Hydra adapts to\nboth adversarial manipulations and intrinsic model errors, making it robust to\nmalicious perturbations and hallucination-related inaccuracies. We evaluate\nHydra on four VLMs, three hallucination benchmarks, two adversarial attack\nstrategies, and two adversarial defense methods, assessing performance on both\nclean and adversarial inputs. Results show that Hydra surpasses plug-in VLMs\nand state-of-the-art (SOTA) dehallucination methods, even without explicit\nadversarial defenses, demonstrating enhanced robustness and factual\nconsistency. By bridging adversarial resistance and hallucination mitigation,\nHydra provides a scalable, training-free solution for improving the reliability\nof VLMs in real-world applications.", "AI": {"tldr": "Hydra\u662f\u4e00\u4e2a\u81ea\u9002\u5e94\u4ee3\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u8fed\u4ee3\u63a8\u7406\u548c\u8de8\u6a21\u578b\u9a8c\u8bc1\u63d0\u5347\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u5bf9\u6297\u5bf9\u6297\u6027\u6270\u52a8\u548c\u5e7b\u89c9\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4ec5\u5173\u6ce8\u5bf9\u6297\u9632\u5fa1\u6216\u5e7b\u89c9\u540e\u5904\u7406\uff0c\u7f3a\u4e4f\u7edf\u4e00\u7b56\u7565\uff0cHydra\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u91c7\u7528Action-Critique Loop\uff0c\u7ed3\u5408Chain-of-Thought\u548cIn-Context Learning\u6280\u672f\u52a8\u6001\u4f18\u5316\u8f93\u51fa\u3002", "result": "\u5728\u56db\u79cdVLMs\u4e0a\u6d4b\u8bd5\uff0cHydra\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u63d2\u4ef6\u6a21\u578b\u548c\u53bb\u5e7b\u89c9\u65b9\u6cd5\uff0c\u65e0\u9700\u663e\u5f0f\u5bf9\u6297\u9632\u5fa1\u3002", "conclusion": "Hydra\u4e3a\u63d0\u5347VLMs\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u53ef\u9760\u6027\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u65e0\u8bad\u7ec3\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.14378", "pdf": "https://arxiv.org/pdf/2504.14378", "abs": "https://arxiv.org/abs/2504.14378", "authors": ["Yue Li", "Ye Wei", "Alaukik Saxena", "Markus K\u00fchbach", "Christoph Freysoldt", "Baptiste Gault"], "title": "Machine learning enhanced atom probe tomography analysis: a snapshot review", "categories": ["cond-mat.mtrl-sci", "cs.LG"], "comment": null, "summary": "Atom probe tomography (APT) is a burgeoning characterization technique that\nprovides compositional mapping of materials in three-dimensions at near-atomic\nscale. Since its significant expansion in the past 30 years, we estimate that\none million APT datasets have been collected, each containing millions to\nbillions of individual ions. Their analysis and the extraction of\nmicrostructural information has largely relied upon individual users whose\nvaried level of expertise causes clear and documented bias. Current practices\nhinder efficient data processing, and make challenging standardization and the\ndeployment of data analysis workflows that would be compliant with FAIR data\nprinciples. Over the past decade, building upon the long-standing expertise of\nthe APT community in the development of advanced data processing or data mining\ntechniques, there has been a surge of novel machine learning (ML) approaches\naiming for user-independence, and that are efficient, reproducible, and robust\nfrom a statistics perspective. Here, we provide a snapshot review of this\nrapidly evolving field. We begin with a brief introduction to APT and the\nnature of the APT data. This is followed by an overview of relevant ML\nalgorithms and a comprehensive review of their applications to APT. We also\ndiscuss how ML can enable discoveries beyond human capability, offering new\ninsights into the mechanisms within materials. Finally, we provide guidance for\nfuture directions in this domain.", "AI": {"tldr": "\u672c\u6587\u56de\u987e\u4e86\u539f\u5b50\u63a2\u9488\u65ad\u5c42\u626b\u63cf\uff08APT\uff09\u6570\u636e\u5206\u6790\u548c\u673a\u5668\u5b66\u4e60\uff08ML\uff09\u5e94\u7528\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u5f3a\u8c03ML\u5728\u63d0\u5347\u6548\u7387\u3001\u53ef\u91cd\u590d\u6027\u548c\u7edf\u8ba1\u7a33\u5065\u6027\u65b9\u9762\u7684\u6f5c\u529b\u3002", "motivation": "APT\u6570\u636e\u7684\u5206\u6790\u957f\u671f\u4ee5\u6765\u4f9d\u8d56\u7528\u6237\u7ecf\u9a8c\uff0c\u5bfc\u81f4\u504f\u5dee\u548c\u6548\u7387\u4f4e\u4e0b\uff0c\u4e9f\u9700\u6807\u51c6\u5316\u548c\u81ea\u52a8\u5316\u65b9\u6cd5\u3002", "method": "\u7efc\u8ff0\u4e86APT\u6570\u636e\u7279\u6027\u3001\u76f8\u5173ML\u7b97\u6cd5\u53ca\u5176\u5728APT\u4e2d\u7684\u5e94\u7528\uff0c\u63a2\u8ba8\u4e86ML\u5982\u4f55\u8d85\u8d8a\u4eba\u7c7b\u80fd\u529b\u53d1\u73b0\u65b0\u6750\u6599\u673a\u5236\u3002", "result": "ML\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86APT\u6570\u636e\u5206\u6790\u7684\u7528\u6237\u72ec\u7acb\u6027\u3001\u6548\u7387\u548c\u7edf\u8ba1\u7a33\u5065\u6027\u3002", "conclusion": "\u672a\u6765\u7814\u7a76\u65b9\u5411\u5e94\u96c6\u4e2d\u5728\u8fdb\u4e00\u6b65\u4f18\u5316ML\u7b97\u6cd5\u548c\u63a8\u5e7fFAIR\u6570\u636e\u539f\u5219\u7684\u5e94\u7528\u3002"}}
{"id": "2504.14406", "pdf": "https://arxiv.org/pdf/2504.14406", "abs": "https://arxiv.org/abs/2504.14406", "authors": ["Runlong Ye", "Patrick Yung Kang Lee", "Matthew Varona", "Oliver Huang", "Carolina Nobre"], "title": "ScholarMate: A Mixed-Initiative Tool for Qualitative Knowledge Work and Information Sensemaking", "categories": ["cs.HC", "cs.AI"], "comment": "accepted at CHIWORK 2025", "summary": "Synthesizing knowledge from large document collections is a critical yet\nincreasingly complex aspect of qualitative research and knowledge work. While\nAI offers automation potential, effectively integrating it into human-centric\nsensemaking workflows remains challenging. We present ScholarMate, an\ninteractive system designed to augment qualitative analysis by unifying AI\nassistance with human oversight. ScholarMate enables researchers to dynamically\narrange and interact with text snippets on a non-linear canvas, leveraging AI\nfor theme suggestions, multi-level summarization, and contextual naming, while\nensuring transparency through traceability to source documents. Initial pilot\nstudies indicated that users value this mixed-initiative approach, finding the\nbalance between AI suggestions and direct manipulation crucial for maintaining\ninterpretability and trust. We further demonstrate the system's capability\nthrough a case study analyzing 24 papers. By balancing automation with human\ncontrol, ScholarMate enhances efficiency and supports interpretability,\noffering a valuable approach for productive human-AI collaboration in demanding\nsensemaking tasks common in knowledge work.", "AI": {"tldr": "ScholarMate\u662f\u4e00\u4e2a\u4ea4\u4e92\u5f0f\u7cfb\u7edf\uff0c\u7ed3\u5408AI\u8f85\u52a9\u4e0e\u4eba\u5de5\u76d1\u7763\uff0c\u63d0\u5347\u5b9a\u6027\u5206\u6790\u7684\u6548\u7387\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u89e3\u51b3\u5728\u590d\u6742\u6587\u6863\u96c6\u5408\u4e2d\u5408\u6210\u77e5\u8bc6\u65f6\uff0cAI\u81ea\u52a8\u5316\u4e0e\u4eba\u5de5\u5de5\u4f5c\u6d41\u7ed3\u5408\u7684\u6311\u6218\u3002", "method": "\u901a\u8fc7\u975e\u7ebf\u6027\u7684\u4ea4\u4e92\u753b\u5e03\u52a8\u6001\u7ec4\u7ec7\u6587\u672c\u7247\u6bb5\uff0c\u5229\u7528AI\u63d0\u4f9b\u4e3b\u9898\u5efa\u8bae\u3001\u591a\u7ea7\u6458\u8981\u548c\u4e0a\u4e0b\u6587\u547d\u540d\uff0c\u5e76\u786e\u4fdd\u6765\u6e90\u53ef\u8ffd\u6eaf\u3002", "result": "\u521d\u6b65\u7814\u7a76\u8868\u660e\u7528\u6237\u8ba4\u53ef\u8fd9\u79cd\u6df7\u5408\u65b9\u6cd5\uff0c\u8ba4\u4e3aAI\u5efa\u8bae\u4e0e\u76f4\u63a5\u64cd\u4f5c\u7684\u5e73\u8861\u5bf9\u4fdd\u6301\u53ef\u89e3\u91ca\u6027\u548c\u4fe1\u4efb\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "ScholarMate\u901a\u8fc7\u5e73\u8861\u81ea\u52a8\u5316\u4e0e\u4eba\u5de5\u63a7\u5236\uff0c\u63d0\u5347\u4e86\u6548\u7387\u5e76\u652f\u6301\u53ef\u89e3\u91ca\u6027\uff0c\u4e3a\u77e5\u8bc6\u5de5\u4f5c\u4e2d\u7684\u590d\u6742\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u4eba\u673a\u534f\u4f5c\u65b9\u6848\u3002"}}
{"id": "2504.14409", "pdf": "https://arxiv.org/pdf/2504.14409", "abs": "https://arxiv.org/abs/2504.14409", "authors": ["Christopher Ick", "Gordon Wichern", "Yoshiki Masuyama", "Fran\u00e7ois G. Germain", "Jonathan Le Roux"], "title": "Data Augmentation Using Neural Acoustic Fields With Retrieval-Augmented Pre-training", "categories": ["eess.AS", "cs.AI", "cs.CV", "cs.LG", "cs.SD"], "comment": "Presented at ICASSP 2025 GenDA Workshop", "summary": "This report details MERL's system for room impulse response (RIR) estimation\nsubmitted to the Generative Data Augmentation Workshop at ICASSP 2025 for\nAugmenting RIR Data (Task 1) and Improving Speaker Distance Estimation (Task\n2). We first pre-train a neural acoustic field conditioned by room geometry on\nan external large-scale dataset in which pairs of RIRs and the geometries are\nprovided. The neural acoustic field is then adapted to each target room by\nusing the enrollment data, where we leverage either the provided room\ngeometries or geometries retrieved from the external dataset, depending on\navailability. Lastly, we predict the RIRs for each pair of source and receiver\nlocations specified by Task 1, and use these RIRs to train the speaker distance\nestimation model in Task 2.", "AI": {"tldr": "MERL\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u58f0\u573a\u7684RIR\u4f30\u8ba1\u7cfb\u7edf\uff0c\u7528\u4e8eICASSP 2025\u7684\u4efb\u52a11\uff08\u589e\u5f3aRIR\u6570\u636e\uff09\u548c\u4efb\u52a12\uff08\u6539\u8fdb\u8bf4\u8bdd\u8005\u8ddd\u79bb\u4f30\u8ba1\uff09\u3002", "motivation": "\u89e3\u51b3RIR\u6570\u636e\u589e\u5f3a\u548c\u8bf4\u8bdd\u8005\u8ddd\u79bb\u4f30\u8ba1\u95ee\u9898\uff0c\u901a\u8fc7\u9884\u8bad\u7ec3\u548c\u9002\u5e94\u76ee\u6807\u623f\u95f4\u7684\u65b9\u6cd5\u63d0\u9ad8\u6027\u80fd\u3002", "method": "\u9884\u8bad\u7ec3\u795e\u7ecf\u58f0\u573a\u6a21\u578b\uff0c\u5229\u7528\u5916\u90e8\u6570\u636e\u96c6\u548c\u76ee\u6807\u623f\u95f4\u7684\u51e0\u4f55\u4fe1\u606f\u8fdb\u884c\u9002\u5e94\uff0c\u9884\u6d4bRIR\u5e76\u7528\u4e8e\u8bad\u7ec3\u8ddd\u79bb\u4f30\u8ba1\u6a21\u578b\u3002", "result": "\u7cfb\u7edf\u80fd\u591f\u751f\u6210\u76ee\u6807\u623f\u95f4\u7684RIR\uff0c\u5e76\u7528\u4e8e\u6539\u8fdb\u8bf4\u8bdd\u8005\u8ddd\u79bb\u4f30\u8ba1\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408\u9884\u8bad\u7ec3\u548c\u9002\u5e94\u7b56\u7565\uff0c\u6709\u6548\u63d0\u5347\u4e86RIR\u4f30\u8ba1\u548c\u8ddd\u79bb\u4f30\u8ba1\u7684\u6027\u80fd\u3002"}}
{"id": "2504.14411", "pdf": "https://arxiv.org/pdf/2504.14411", "abs": "https://arxiv.org/abs/2504.14411", "authors": ["Xiang Zhang", "Yongfeng Zhang"], "title": "Planet as a Brain: Towards Internet of AgentSites based on AIOS Server", "categories": ["cs.NI", "cs.AI"], "comment": null, "summary": "The internet is undergoing a historical transformation from the \"Internet of\nWebsites\" to the \"Internet of AgentSites.\" While traditional Websites served as\nthe foundation for information hosting and dissemination, a new frontier is\nemerging where AgentSites serve as the hubs of the internet, where each\nAgentSite hosts one or more AI agents that receive tasks, address them, and\ndeliver actionable solutions, marking a significant shift in the digital\nlandscape and representing the next generation of online ecosystems. Under this\nvision, AIOS, the AI Agent Operating System, serves as the server for the\ndevelopment, deployment and execution of AI agents, which is a fundamental\ninfrastructure for the Internet of Agentsites.\n  In this paper, we introduce AIOS Server, a runtime framework to host agents\nand enable global-scale collaboration among decentralized agents. AIOS Server\nprovides a communication protocol leveraging the Model Context Protocol (MCP)\nand JSON-RPC to enable agent-agent or human-agent interactions. Each AIOS node\noperates as a server to host and execute agents, while supporting peer-to-peer\ncoordination without reliance on centralized orchestration. Based on AIOS\nServer, we further present the world's first practically deployed Internet of\nAgentsites (AIOS-IoA), including AgentHub for agent registration and discovery\nand AgentChat for interactive communication, at https://planet.aios.foundation.\nThe agent discovery mechanism based on Distributed Hash Tables (DHT) and a\nGossip protocol serves as the search engine for the internet of agentsites.\nThis work provides a practical foundation for building the Internet of\nAgentsites-a new paradigm where autonomous agents become first-class citizens\nof the web. The implementation is available at\nhttps://github.com/agiresearch/AIOS.Server and will be integrated into the AIOS\nmain branch at https://github.com/agiresearch/AIOS.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86AIOS Server\uff0c\u4e00\u4e2a\u652f\u6301\u5168\u7403\u89c4\u6a21\u53bb\u4e2d\u5fc3\u5316AI\u4ee3\u7406\u534f\u4f5c\u7684\u8fd0\u884c\u65f6\u6846\u67b6\uff0c\u5e76\u5c55\u793a\u4e86\u9996\u4e2a\u5b9e\u9645\u90e8\u7f72\u7684AgentSites\u4e92\u8054\u7f51\uff08AIOS-IoA\uff09\u3002", "motivation": "\u4e92\u8054\u7f51\u6b63\u4ece'\u7f51\u7ad9\u4e92\u8054\u7f51'\u5411'\u4ee3\u7406\u7ad9\u70b9\u4e92\u8054\u7f51'\u8f6c\u578b\uff0c\u9700\u8981\u57fa\u7840\u8bbe\u65bd\u652f\u6301AI\u4ee3\u7406\u7684\u5f00\u53d1\u3001\u90e8\u7f72\u548c\u6267\u884c\u3002", "method": "\u63d0\u51faAIOS Server\u6846\u67b6\uff0c\u5229\u7528MCP\u548cJSON-RPC\u534f\u8bae\u5b9e\u73b0\u4ee3\u7406\u95f4\u6216\u4eba\u673a\u4ea4\u4e92\uff0c\u652f\u6301\u53bb\u4e2d\u5fc3\u5316\u534f\u8c03\u3002", "result": "\u6210\u529f\u90e8\u7f72AIOS-IoA\uff0c\u5305\u62ecAgentHub\u548cAgentChat\uff0c\u5e76\u5b9e\u73b0\u57fa\u4e8eDHT\u548cGossip\u534f\u8bae\u7684\u4ee3\u7406\u53d1\u73b0\u673a\u5236\u3002", "conclusion": "AIOS Server\u4e3a\u6784\u5efa\u4ee3\u7406\u7ad9\u70b9\u4e92\u8054\u7f51\u63d0\u4f9b\u4e86\u5b9e\u8df5\u57fa\u7840\uff0c\u4f7f\u81ea\u4e3b\u4ee3\u7406\u6210\u4e3a\u7f51\u7edc\u7684\u4e00\u7b49\u516c\u6c11\u3002"}}
{"id": "2504.14412", "pdf": "https://arxiv.org/pdf/2504.14412", "abs": "https://arxiv.org/abs/2504.14412", "authors": ["Benjamin M. Peter", "Mert Korkali"], "title": "Quantum-Enhanced Reinforcement Learning for Power Grid Security Assessment", "categories": ["eess.SY", "cs.LG", "cs.SY"], "comment": "6 pages, 6 figures, 3 tables. Submitted to the 2025 IEEE\n  International Conference on Communications, Control, and Computing\n  Technologies for Smart Grids (SmartGridComm)", "summary": "The increasingly challenging task of maintaining power grid security requires\ninnovative solutions. Novel approaches using reinforcement learning (RL) agents\nhave been proposed to help grid operators navigate the massive decision space\nand nonlinear behavior of these complex networks. However, applying RL to power\ngrid security assessment, specifically for combinatorially troublesome\ncontingency analysis problems, has proven difficult to scale. The integration\nof quantum computing into these RL frameworks helps scale by improving\ncomputational efficiency and boosting agent proficiency by leveraging quantum\nadvantages in action exploration and model-based interdependence. To\ndemonstrate a proof-of-concept use of quantum computing for RL agent training\nand simulation, we propose a hybrid agent that runs on quantum hardware using\nIBM's Qiskit Runtime. We also provide detailed insight into the construction of\nparameterized quantum circuits (PQCs) for generating relevant quantum output.\nThis agent's proficiency at maintaining grid stability is demonstrated relative\nto a benchmark model without quantum enhancement using N-k contingency\nanalysis. Additionally, we offer a comparative assessment of the training\nprocedures for RL models integrated with a quantum backend.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u91cf\u5b50\u8ba1\u7b97\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u6df7\u5408\u4ee3\u7406\uff0c\u7528\u4e8e\u63d0\u5347\u7535\u7f51\u5b89\u5168\u8bc4\u4f30\u7684\u8ba1\u7b97\u6548\u7387\u548c\u4ee3\u7406\u80fd\u529b\u3002", "motivation": "\u7535\u7f51\u5b89\u5168\u7ef4\u62a4\u4efb\u52a1\u65e5\u76ca\u590d\u6742\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\u5927\u89c4\u6a21\u51b3\u7b56\u7a7a\u95f4\u548c\u975e\u7ebf\u6027\u884c\u4e3a\u3002\u91cf\u5b50\u8ba1\u7b97\u7684\u4f18\u52bf\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eIBM Qiskit Runtime\u7684\u6df7\u5408\u4ee3\u7406\uff0c\u5229\u7528\u53c2\u6570\u5316\u91cf\u5b50\u7535\u8def\uff08PQCs\uff09\u751f\u6210\u91cf\u5b50\u8f93\u51fa\uff0c\u5e76\u4e0e\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u7ed3\u5408\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6df7\u5408\u4ee3\u7406\u5728N-k\u5e94\u6025\u5206\u6790\u4e2d\u6bd4\u65e0\u91cf\u5b50\u589e\u5f3a\u7684\u57fa\u51c6\u6a21\u578b\u66f4\u80fd\u7ef4\u6301\u7535\u7f51\u7a33\u5b9a\u6027\u3002", "conclusion": "\u91cf\u5b50\u8ba1\u7b97\u4e0e\u5f3a\u5316\u5b66\u4e60\u7684\u7ed3\u5408\u4e3a\u7535\u7f51\u5b89\u5168\u8bc4\u4f30\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.14423", "pdf": "https://arxiv.org/pdf/2504.14423", "abs": "https://arxiv.org/abs/2504.14423", "authors": ["Qiang Chen", "Xiao Wang", "Haowen Wang", "Bo Jiang", "Lin Zhu", "Dawei Zhang", "Yonghong Tian", "Jin Tang"], "title": "Adversarial Attack for RGB-Event based Visual Object Tracking", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Visual object tracking is a crucial research topic in the fields of computer\nvision and multi-modal fusion. Among various approaches, robust visual tracking\nthat combines RGB frames with Event streams has attracted increasing attention\nfrom researchers. While striving for high accuracy and efficiency in tracking,\nit is also important to explore how to effectively conduct adversarial attacks\nand defenses on RGB-Event stream tracking algorithms, yet research in this area\nremains relatively scarce. To bridge this gap, in this paper, we propose a\ncross-modal adversarial attack algorithm for RGB-Event visual tracking. Because\nof the diverse representations of Event streams, and given that Event voxels\nand frames are more commonly used, this paper will focus on these two\nrepresentations for an in-depth study. Specifically, for the RGB-Event voxel,\nwe first optimize the perturbation by adversarial loss to generate RGB frame\nadversarial examples. For discrete Event voxel representations, we propose a\ntwo-step attack strategy, more in detail, we first inject Event voxels into the\ntarget region as initialized adversarial examples, then, conduct a\ngradient-guided optimization by perturbing the spatial location of the Event\nvoxels. For the RGB-Event frame based tracking, we optimize the cross-modal\nuniversal perturbation by integrating the gradient information from multimodal\ndata. We evaluate the proposed approach against attacks on three widely used\nRGB-Event Tracking datasets, i.e., COESOT, FE108, and VisEvent. Extensive\nexperiments show that our method significantly reduces the performance of the\ntracker across numerous datasets in both unimodal and multimodal scenarios. The\nsource code will be released on\nhttps://github.com/Event-AHU/Adversarial_Attack_Defense", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9RGB-Event\u89c6\u89c9\u8ddf\u8e2a\u7684\u8de8\u6a21\u6001\u5bf9\u6297\u653b\u51fb\u7b97\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316\u6270\u52a8\u548c\u68af\u5ea6\u5f15\u5bfc\u7b56\u7565\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ddf\u8e2a\u5668\u6027\u80fd\u3002", "motivation": "\u7814\u7a76RGB-Event\u6d41\u8ddf\u8e2a\u7b97\u6cd5\u7684\u5bf9\u6297\u653b\u51fb\u4e0e\u9632\u5fa1\uff0c\u586b\u8865\u8be5\u9886\u57df\u7684\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u9488\u5bf9RGB-Event\u4f53\u7d20\u548c\u5e27\u8868\u793a\uff0c\u5206\u522b\u91c7\u7528\u5bf9\u6297\u635f\u5931\u4f18\u5316\u6270\u52a8\u548c\u68af\u5ea6\u5f15\u5bfc\u7684\u7a7a\u95f4\u4f4d\u7f6e\u6270\u52a8\u7b56\u7565\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ddf\u8e2a\u5668\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684\u8de8\u6a21\u6001\u5bf9\u6297\u653b\u51fb\u7b97\u6cd5\u5728RGB-Event\u8ddf\u8e2a\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2504.14422", "pdf": "https://arxiv.org/pdf/2504.14422", "abs": "https://arxiv.org/abs/2504.14422", "authors": ["Paul Fischer", "Sebastian Kaltenbach", "Sergey Litvinov", "Sauro Succi", "Petros Koumoutsakos"], "title": "Optimal Lattice Boltzmann Closures through Multi-Agent Reinforcement Learning", "categories": ["physics.flu-dyn", "cs.LG", "physics.comp-ph", "stat.ML"], "comment": null, "summary": "The Lattice Boltzmann method (LBM) offers a powerful and versatile approach\nto simulating diverse hydrodynamic phenomena, spanning microfluidics to\naerodynamics. The vast range of spatiotemporal scales inherent in these systems\ncurrently renders full resolution impractical, necessitating the development of\neffective closure models for under-resolved simulations. Under-resolved LBMs\nare unstable, and while there is a number of important efforts to stabilize\nthem, they often face limitations in generalizing across scales and physical\nsystems. We present a novel, data-driven, multiagent reinforcement learning\n(MARL) approach that drastically improves stability and accuracy of\ncoarse-grained LBM simulations. The proposed method uses a convolutional neural\nnetwork to dynamically control the local relaxation parameter for the LB across\nthe simulation grid. The LB-MARL framework is showcased in turbulent Kolmogorov\nflows. We find that the MARL closures stabilize the simulations and recover the\nenergy spectra of significantly more expensive fully resolved simulations while\nmaintaining computational efficiency. The learned closure model can be\ntransferred to flow scenarios unseen during training and has improved\nrobustness and spectral accuracy compared to traditional LBM models. We believe\nthat MARL closures open new frontiers for efficient and accurate simulations of\na multitude of complex problems not accessible to present-day LB methods alone.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff08MARL\uff09\u7684\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u5347\u7c97\u7c92\u5ea6Lattice Boltzmann\u65b9\u6cd5\uff08LBM\uff09\u6a21\u62df\u7684\u7a33\u5b9a\u6027\u548c\u51c6\u786e\u6027\u3002", "motivation": "LBM\u5728\u6a21\u62df\u591a\u5c3a\u5ea6\u6d41\u4f53\u73b0\u8c61\u65f6\uff0c\u7531\u4e8e\u8ba1\u7b97\u8d44\u6e90\u9650\u5236\uff0c\u96be\u4ee5\u5b8c\u5168\u89e3\u6790\u6240\u6709\u5c3a\u5ea6\uff0c\u5bfc\u81f4\u672a\u89e3\u6790\u6a21\u62df\u4e0d\u7a33\u5b9a\u4e14\u96be\u4ee5\u6cdb\u5316\u3002", "method": "\u91c7\u7528\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u52a8\u6001\u63a7\u5236LBM\u7684\u5c40\u90e8\u677e\u5f1b\u53c2\u6570\uff0c\u5e76\u901a\u8fc7MARL\u6846\u67b6\u5728\u6e4d\u6d41Kolmogorov\u6d41\u4e2d\u9a8c\u8bc1\u3002", "result": "MARL\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6a21\u62df\u7684\u7a33\u5b9a\u6027\uff0c\u6062\u590d\u4e86\u9ad8\u5206\u8fa8\u7387\u6a21\u62df\u7684\u80fd\u91cf\u8c31\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8ba1\u7b97\u6548\u7387\uff0c\u4e14\u80fd\u6cdb\u5316\u5230\u672a\u8bad\u7ec3\u7684\u6d41\u52a8\u573a\u666f\u3002", "conclusion": "MARL\u65b9\u6cd5\u4e3aLBM\u6a21\u62df\u590d\u6742\u95ee\u9898\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u65b0\u9014\u5f84\uff0c\u8d85\u8d8a\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2504.14427", "pdf": "https://arxiv.org/pdf/2504.14427", "abs": "https://arxiv.org/abs/2504.14427", "authors": ["Spencer Lin", "Miru Jun", "Basem Rizk", "Karen Shieh", "Scott Fisher", "Sharon Mozgai"], "title": "Optimizing SIA Development: A Case Study in User-Centered Design for Estuary, a Multimodal Socially Interactive Agent Framework", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "This case study presents our user-centered design model for Socially\nIntelligent Agent (SIA) development frameworks through our experience\ndeveloping Estuary, an open source multimodal framework for building\nlow-latency real-time socially interactive agents. We leverage the Rapid\nAssessment Process (RAP) to collect the thoughts of leading researchers in the\nfield of SIAs regarding the current state of the art for SIA development as\nwell as their evaluation of how well Estuary may potentially address current\nresearch gaps. We achieve this through a series of end-user interviews\nconducted by a fellow researcher in the community. We hope that the findings of\nour work will not only assist the continued development of Estuary but also\nguide the development of other future frameworks and technologies for SIAs.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u7528\u6237\u4e3a\u4e2d\u5fc3\u7684\u8bbe\u8ba1\u6a21\u578b\uff0c\u7528\u4e8e\u5f00\u53d1\u793e\u4ea4\u667a\u80fd\u4ee3\u7406\uff08SIA\uff09\u6846\u67b6\uff0c\u5e76\u901a\u8fc7\u5f00\u6e90\u591a\u6a21\u6001\u6846\u67b6Estuary\u7684\u5b9e\u8df5\u7ecf\u9a8c\u5c55\u793a\u4e86\u5176\u5e94\u7528\u3002", "motivation": "\u63a2\u7d22\u5982\u4f55\u901a\u8fc7\u7528\u6237\u53cd\u9988\u6539\u8fdbSIA\u5f00\u53d1\u6846\u67b6\uff0c\u586b\u8865\u5f53\u524d\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u91c7\u7528\u5feb\u901f\u8bc4\u4f30\u8fc7\u7a0b\uff08RAP\uff09\u6536\u96c6\u9886\u57df\u4e13\u5bb6\u610f\u89c1\uff0c\u901a\u8fc7\u7528\u6237\u8bbf\u8c08\u8bc4\u4f30Estuary\u6846\u67b6\u7684\u6f5c\u529b\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u6709\u52a9\u4e8eEstuary\u7684\u6301\u7eed\u5f00\u53d1\uff0c\u5e76\u4e3a\u672a\u6765SIA\u6846\u67b6\u63d0\u4f9b\u6307\u5bfc\u3002", "conclusion": "\u7528\u6237\u53cd\u9988\u662f\u6539\u8fdbSIA\u6846\u67b6\u7684\u5173\u952e\uff0cEstuary\u5c55\u793a\u4e86\u5176\u6f5c\u529b\uff0c\u672a\u6765\u7814\u7a76\u53ef\u8fdb\u4e00\u6b65\u4f18\u5316\u3002"}}
{"id": "2504.14425", "pdf": "https://arxiv.org/pdf/2504.14425", "abs": "https://arxiv.org/abs/2504.14425", "authors": ["Panos Tsimpos", "Zhi Ren", "Jakob Zech", "Youssef Marzouk"], "title": "Optimal Scheduling of Dynamic Transport", "categories": ["stat.ML", "cs.LG", "math.CA", "math.FA"], "comment": null, "summary": "Flow-based methods for sampling and generative modeling use continuous-time\ndynamical systems to represent a {transport map} that pushes forward a source\nmeasure to a target measure. The introduction of a time axis provides\nconsiderable design freedom, and a central question is how to exploit this\nfreedom. Though many popular methods seek straight line (i.e., zero\nacceleration) trajectories, we show here that a specific class of ``curved''\ntrajectories can significantly improve approximation and learning. In\nparticular, we consider the unit-time interpolation of any given transport map\n$T$ and seek the schedule $\\tau: [0,1] \\to [0,1]$ that minimizes the spatial\nLipschitz constant of the corresponding velocity field over all times $t \\in\n[0,1]$. This quantity is crucial as it allows for control of the approximation\nerror when the velocity field is learned from data. We show that, for a broad\nclass of source/target measures and transport maps $T$, the \\emph{optimal\nschedule} can be computed in closed form, and that the resulting optimal\nLipschitz constant is \\emph{exponentially smaller} than that induced by an\nidentity schedule (corresponding to, for instance, the Wasserstein geodesic).\nOur proof technique relies on the calculus of variations and\n$\\Gamma$-convergence, allowing us to approximate the aforementioned degenerate\nobjective by a family of smooth, tractable problems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u201c\u5f2f\u66f2\u201d\u8f68\u8ff9\u4f18\u5316\u6d41\u5f0f\u91c7\u6837\u548c\u751f\u6210\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u901f\u5ea6\u573a\u7684Lipschitz\u5e38\u6570\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8fd1\u4f3c\u548c\u5b66\u4e60\u6548\u679c\u3002", "motivation": "\u63a2\u7d22\u5982\u4f55\u5229\u7528\u65f6\u95f4\u8f74\u7684\u8bbe\u8ba1\u81ea\u7531\u5ea6\uff0c\u6539\u8fdb\u6d41\u5f0f\u65b9\u6cd5\u4e2d\u7684\u8f68\u8ff9\u8bbe\u8ba1\uff0c\u4ee5\u63d0\u5347\u91c7\u6837\u548c\u751f\u6210\u6a21\u578b\u7684\u6027\u80fd\u3002", "method": "\u901a\u8fc7\u8ba1\u7b97\u6700\u4f18\u65f6\u95f4\u8c03\u5ea6\u51fd\u6570\u03c4\uff0c\u6700\u5c0f\u5316\u901f\u5ea6\u573a\u7684\u7a7a\u95f4Lipschitz\u5e38\u6570\uff0c\u5e76\u8bc1\u660e\u5176\u95ed\u5f0f\u89e3\u7684\u5b58\u5728\u6027\u3002", "result": "\u5bf9\u4e8e\u5e7f\u6cdb\u7684\u6e90/\u76ee\u6807\u6d4b\u5ea6\u548c\u4f20\u8f93\u6620\u5c04T\uff0c\u6700\u4f18\u8c03\u5ea6\u80fd\u663e\u8457\u964d\u4f4eLipschitz\u5e38\u6570\uff0c\u4e14\u6bd4\u6052\u7b49\u8c03\u5ea6\uff08\u5982Wasserstein\u6d4b\u5730\u7ebf\uff09\u6307\u6570\u7ea7\u66f4\u4f18\u3002", "conclusion": "\u901a\u8fc7\u53d8\u5206\u6cd5\u548c\u0393-\u6536\u655b\u6280\u672f\uff0c\u8bc1\u660e\u4e86\u6700\u4f18\u8c03\u5ea6\u7684\u6709\u6548\u6027\uff0c\u4e3a\u6d41\u5f0f\u65b9\u6cd5\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u8f68\u8ff9\u8bbe\u8ba1\u3002"}}
{"id": "2504.14429", "pdf": "https://arxiv.org/pdf/2504.14429", "abs": "https://arxiv.org/abs/2504.14429", "authors": ["Ahmad Khalil", "Mahmoud Khalil", "Alioune Ngom"], "title": "ResNetVLLM-2: Addressing ResNetVLLM's Multi-Modal Hallucinations", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have transformed natural language processing\n(NLP) tasks, but they suffer from hallucination, generating plausible yet\nfactually incorrect content. This issue extends to Video-Language Models\n(VideoLLMs), where textual descriptions may inaccurately represent visual\ncontent, resulting in multi-modal hallucinations. In this paper, we address\nhallucination in ResNetVLLM, a video-language model combining ResNet visual\nencoders with LLMs. We introduce a two-step protocol: (1) a faithfulness\ndetection strategy that uses a modified Lynx model to assess semantic alignment\nbetween generated captions and ground-truth video references, and (2) a\nhallucination mitigation strategy using Retrieval-Augmented Generation (RAG)\nwith an ad-hoc knowledge base dynamically constructed during inference. Our\nenhanced model, ResNetVLLM-2, reduces multi-modal hallucinations by\ncross-verifying generated content against external knowledge, improving factual\nconsistency. Evaluation on the ActivityNet-QA benchmark demonstrates a\nsubstantial accuracy increase from 54.8% to 65.3%, highlighting the\neffectiveness of our hallucination detection and mitigation strategies in\nenhancing video-language model reliability.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u89c6\u9891\u8bed\u8a00\u6a21\u578bResNetVLLM\u7684\u5e7b\u89c9\u95ee\u9898\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u4e24\u6b65\u7b56\u7565\uff08\u68c0\u6d4b\u4e0e\u7f13\u89e3\uff09\u63d0\u5347\u6a21\u578b\u7684\u4e8b\u5b9e\u4e00\u81f4\u6027\u3002", "motivation": "\u89e3\u51b3\u89c6\u9891\u8bed\u8a00\u6a21\u578b\u4e2d\u751f\u6210\u5185\u5bb9\u4e0e\u89c6\u89c9\u4e8b\u5b9e\u4e0d\u7b26\u7684\u591a\u6a21\u6001\u5e7b\u89c9\u95ee\u9898\u3002", "method": "1. \u4f7f\u7528\u6539\u8fdb\u7684Lynx\u6a21\u578b\u68c0\u6d4b\u751f\u6210\u63cf\u8ff0\u4e0e\u771f\u5b9e\u89c6\u9891\u7684\u8bed\u4e49\u5bf9\u9f50\uff1b2. \u91c7\u7528\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u52a8\u6001\u6784\u5efa\u77e5\u8bc6\u5e93\u4ee5\u7f13\u89e3\u5e7b\u89c9\u3002", "result": "\u5728ActivityNet-QA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u51c6\u786e\u7387\u4ece54.8%\u63d0\u5347\u81f365.3%\u3002", "conclusion": "\u63d0\u51fa\u7684\u4e24\u6b65\u7b56\u7565\u6709\u6548\u51cf\u5c11\u4e86\u591a\u6a21\u6001\u5e7b\u89c9\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u8bed\u8a00\u6a21\u578b\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2504.14446", "pdf": "https://arxiv.org/pdf/2504.14446", "abs": "https://arxiv.org/abs/2504.14446", "authors": ["Carlos Caetano", "Gabriel O. dos Santos", "Caio Petrucci", "Artur Barros", "Camila Laranjeira", "Leo S. F. Ribeiro", "J\u00falia F. de Mendon\u00e7a", "Jefersson A. dos Santos", "Sandra Avila"], "title": "Neglected Risks: The Disturbing Reality of Children's Images in Datasets and the Urgent Call for Accountability", "categories": ["cs.CV", "cs.CY", "cs.LG"], "comment": "ACM Conference on Fairness, Accountability, and Transparency (FAccT\n  2025)", "summary": "Including children's images in datasets has raised ethical concerns,\nparticularly regarding privacy, consent, data protection, and accountability.\nThese datasets, often built by scraping publicly available images from the\nInternet, can expose children to risks such as exploitation, profiling, and\ntracking. Despite the growing recognition of these issues, approaches for\naddressing them remain limited. We explore the ethical implications of using\nchildren's images in AI datasets and propose a pipeline to detect and remove\nsuch images. As a use case, we built the pipeline on a Vision-Language Model\nunder the Visual Question Answering task and tested it on the #PraCegoVer\ndataset. We also evaluate the pipeline on a subset of 100,000 images from the\nOpen Images V7 dataset to assess its effectiveness in detecting and removing\nimages of children. The pipeline serves as a baseline for future research,\nproviding a starting point for more comprehensive tools and methodologies.\nWhile we leverage existing models trained on potentially problematic data, our\ngoal is to expose and address this issue. We do not advocate for training or\ndeploying such models, but instead call for urgent community reflection and\naction to protect children's rights. Ultimately, we aim to encourage the\nresearch community to exercise - more than an additional - care in creating new\ndatasets and to inspire the development of tools to protect the fundamental\nrights of vulnerable groups, particularly children.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5728AI\u6570\u636e\u96c6\u4e2d\u4f7f\u7528\u513f\u7ae5\u56fe\u50cf\u7684\u4f26\u7406\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u68c0\u6d4b\u548c\u79fb\u9664\u6b64\u7c7b\u56fe\u50cf\u7684\u6d41\u7a0b\u3002", "motivation": "\u513f\u7ae5\u56fe\u50cf\u7684\u4f7f\u7528\u6d89\u53ca\u9690\u79c1\u3001\u540c\u610f\u3001\u6570\u636e\u4fdd\u62a4\u548c\u8d23\u4efb\u7b49\u4f26\u7406\u95ee\u9898\uff0c\u4f46\u76ee\u524d\u89e3\u51b3\u65b9\u6848\u6709\u9650\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u6d41\u7a0b\uff0c\u5e76\u5728Visual Question Answering\u4efb\u52a1\u548c#PraCegoVer\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\uff0c\u540c\u65f6\u5728Open Images V7\u6570\u636e\u96c6\u4e2d\u8bc4\u4f30\u5176\u6709\u6548\u6027\u3002", "result": "\u6d41\u7a0b\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u7ebf\uff0c\u5e76\u547c\u5401\u793e\u533a\u53cd\u601d\u548c\u884c\u52a8\u4ee5\u4fdd\u62a4\u513f\u7ae5\u6743\u5229\u3002", "conclusion": "\u7814\u7a76\u65e8\u5728\u63a8\u52a8\u793e\u533a\u5728\u521b\u5efa\u6570\u636e\u96c6\u65f6\u66f4\u52a0\u8c28\u614e\uff0c\u5e76\u5f00\u53d1\u5de5\u5177\u4fdd\u62a4\u5f31\u52bf\u7fa4\u4f53\u6743\u5229\u3002"}}
{"id": "2504.14432", "pdf": "https://arxiv.org/pdf/2504.14432", "abs": "https://arxiv.org/abs/2504.14432", "authors": ["Ahmad Khalil", "Mahmoud Khalil", "Alioune Ngom"], "title": "ResNetVLLM -- Multi-modal Vision LLM for the Video Understanding Task", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "In this paper, we introduce ResNetVLLM (ResNet Vision LLM), a novel\ncross-modal framework for zero-shot video understanding that integrates a\nResNet-based visual encoder with a Large Language Model (LLM. ResNetVLLM\naddresses the challenges associated with zero-shot video models by avoiding\nreliance on pre-trained video understanding models and instead employing a\nnon-pretrained ResNet to extract visual features. This design ensures the model\nlearns visual and semantic representations within a unified architecture,\nenhancing its ability to generate accurate and contextually relevant textual\ndescriptions from video inputs. Our experimental results demonstrate that\nResNetVLLM achieves state-of-the-art performance in zero-shot video\nunderstanding (ZSVU) on several benchmarks, including MSRVTT-QA, MSVD-QA,\nTGIF-QA FrameQA, and ActivityNet-QA.", "AI": {"tldr": "ResNetVLLM\u662f\u4e00\u79cd\u65b0\u578b\u8de8\u6a21\u6001\u6846\u67b6\uff0c\u7ed3\u5408ResNet\u89c6\u89c9\u7f16\u7801\u5668\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u7528\u4e8e\u96f6\u6837\u672c\u89c6\u9891\u7406\u89e3\uff0c\u65e0\u9700\u9884\u8bad\u7ec3\u89c6\u9891\u6a21\u578b\u3002", "motivation": "\u89e3\u51b3\u96f6\u6837\u672c\u89c6\u9891\u7406\u89e3\u4e2d\u4f9d\u8d56\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u6311\u6218\uff0c\u63d0\u51fa\u4e00\u79cd\u7edf\u4e00\u67b6\u6784\u5b66\u4e60\u89c6\u89c9\u548c\u8bed\u4e49\u8868\u793a\u3002", "method": "\u4f7f\u7528\u975e\u9884\u8bad\u7ec3\u7684ResNet\u63d0\u53d6\u89c6\u89c9\u7279\u5f81\uff0c\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7ed3\u5408\uff0c\u751f\u6210\u89c6\u9891\u7684\u6587\u672c\u63cf\u8ff0\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\uff08\u5982MSRVTT-QA\u3001MSVD-QA\u7b49\uff09\u4e0a\u8fbe\u5230\u96f6\u6837\u672c\u89c6\u9891\u7406\u89e3\u7684\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "ResNetVLLM\u901a\u8fc7\u7edf\u4e00\u67b6\u6784\u6709\u6548\u63d0\u5347\u96f6\u6837\u672c\u89c6\u9891\u7406\u89e3\u7684\u51c6\u786e\u6027\u548c\u4e0a\u4e0b\u6587\u76f8\u5173\u6027\u3002"}}
{"id": "2504.14459", "pdf": "https://arxiv.org/pdf/2504.14459", "abs": "https://arxiv.org/abs/2504.14459", "authors": ["Debarshi Kundu", "Avimita Chatterjee", "Swaroop Ghosh"], "title": "Guess, SWAP, Repeat : Capturing Quantum Snapshots in Classical Memory", "categories": ["quant-ph", "cs.LG"], "comment": "11 Pages, 8 figures", "summary": "We introduce a novel technique that enables observation of quantum states\nwithout direct measurement, preserving them for reuse. Our method allows\nmultiple quantum states to be observed at different points within a single\ncircuit, one at a time, and saved into classical memory without destruction.\nThese saved states can be accessed on demand by downstream applications,\nintroducing a dynamic and programmable notion of quantum memory that supports\nmodular, non-destructive quantum workflows. We propose a hardware-agnostic,\nmachine learning-driven framework to capture non-destructive estimates, or\n\"snapshots,\" of quantum states at arbitrary points within a circuit, enabling\nclassical storage and later reconstruction, similar to memory operations in\nclassical computing. This capability is essential for debugging, introspection,\nand persistent memory in quantum systems, yet remains difficult due to the\nno-cloning theorem and destructive measurements. Our guess-and-check approach\nuses fidelity estimation via the SWAP test to guide state reconstruction. We\nexplore both gradient-based deep neural networks and gradient-free evolutionary\nstrategies to estimate quantum states using only fidelity as the learning\nsignal. We demonstrate a key component of our framework on IBM quantum\nhardware, achieving high-fidelity (approximately 1.0) reconstructions for\nHadamard and other known states. In simulation, our models achieve an average\nfidelity of 0.999 across 100 random quantum states. This provides a pathway\ntoward non-volatile quantum memory, enabling long-term storage and reuse of\nquantum information, and laying groundwork for future quantum memory\narchitectures.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u76f4\u63a5\u6d4b\u91cf\u5373\u53ef\u89c2\u5bdf\u91cf\u5b50\u6001\u7684\u65b0\u6280\u672f\uff0c\u652f\u6301\u91cf\u5b50\u6001\u7684\u91cd\u7528\u548c\u52a8\u6001\u5b58\u50a8\u3002", "motivation": "\u89e3\u51b3\u91cf\u5b50\u7cfb\u7edf\u4e2d\u56e0\u4e0d\u53ef\u514b\u9686\u5b9a\u7406\u548c\u7834\u574f\u6027\u6d4b\u91cf\u5bfc\u81f4\u7684\u8c03\u8bd5\u3001\u5185\u7701\u548c\u6301\u4e45\u5185\u5b58\u56f0\u96be\u3002", "method": "\u91c7\u7528\u786c\u4ef6\u65e0\u5173\u7684\u673a\u5668\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u4fdd\u771f\u5ea6\u4f30\u8ba1\uff08\u5982SWAP\u6d4b\u8bd5\uff09\u548c\u68af\u5ea6/\u65e0\u68af\u5ea6\u4f18\u5316\u7b56\u7565\u91cd\u6784\u91cf\u5b50\u6001\u3002", "result": "\u5728IBM\u91cf\u5b50\u786c\u4ef6\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u4fdd\u771f\u5ea6\uff08\u7ea61.0\uff09\u91cd\u6784\uff0c\u4eff\u771f\u4e2d\u5e73\u5747\u4fdd\u771f\u5ea6\u8fbe0.999\u3002", "conclusion": "\u4e3a\u91cf\u5b50\u975e\u6613\u5931\u6027\u5185\u5b58\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\uff0c\u652f\u6301\u91cf\u5b50\u4fe1\u606f\u7684\u957f\u671f\u5b58\u50a8\u548c\u91cd\u7528\u3002"}}
{"id": "2504.14493", "pdf": "https://arxiv.org/pdf/2504.14493", "abs": "https://arxiv.org/abs/2504.14493", "authors": ["Xinyu Wang", "Jijun Chi", "Zhenghan Tai", "Tung Sum Thomas Kwok", "Muzhi Li", "Zhuhong Li", "Hailin He", "Yuchen Hua", "Peng Lu", "Suyuchen Wang", "Yihong Wu", "Jerry Huang", "Ling Zhou"], "title": "FinSage: A Multi-aspect RAG System for Financial Filings Question Answering", "categories": ["cs.IR", "cs.AI", "cs.LG"], "comment": null, "summary": "Leveraging large language models in real-world settings often entails a need\nto utilize domain-specific data and tools in order to follow the complex\nregulations that need to be followed for acceptable use. Within financial\nsectors, modern enterprises increasingly rely on Retrieval-Augmented Generation\n(RAG) systems to address complex compliance requirements in financial document\nworkflows. However, existing solutions struggle to account for the inherent\nheterogeneity of data (e.g., text, tables, diagrams) and evolving nature of\nregulatory standards used in financial filings, leading to compromised accuracy\nin critical information extraction. We propose the FinSage framework as a\nsolution, utilizing a multi-aspect RAG framework tailored for regulatory\ncompliance analysis in multi-modal financial documents. FinSage introduces\nthree innovative components: (1) a multi-modal pre-processing pipeline that\nunifies diverse data formats and generates chunk-level metadata summaries, (2)\na multi-path sparse-dense retrieval system augmented with query expansion\n(HyDE) and metadata-aware semantic search, and (3) a domain-specialized\nre-ranking module fine-tuned via Direct Preference Optimization (DPO) to\nprioritize compliance-critical content. Extensive experiments demonstrate that\nFinSage achieves an impressive recall of 92.51% on 75 expert-curated questions\nderived from surpasses the best baseline method on the FinanceBench question\nanswering datasets by 24.06% in accuracy. Moreover, FinSage has been\nsuccessfully deployed as financial question-answering agent in online meetings,\nwhere it has already served more than 1,200 people.", "AI": {"tldr": "FinSage\u6846\u67b6\u901a\u8fc7\u591a\u6a21\u6001RAG\u7cfb\u7edf\u89e3\u51b3\u91d1\u878d\u6587\u6863\u4e2d\u7684\u5408\u89c4\u6027\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4fe1\u606f\u63d0\u53d6\u51c6\u786e\u7387\u3002", "motivation": "\u91d1\u878d\u9886\u57df\u9700\u8981\u5904\u7406\u591a\u6a21\u6001\u6570\u636e\u548c\u52a8\u6001\u76d1\u7ba1\u6807\u51c6\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6ee1\u8db3\u51c6\u786e\u6027\u548c\u5408\u89c4\u6027\u9700\u6c42\u3002", "method": "FinSage\u5305\u542b\u591a\u6a21\u6001\u9884\u5904\u7406\u3001\u591a\u8def\u5f84\u68c0\u7d22\u7cfb\u7edf\u548c\u9886\u57df\u4e13\u7528\u91cd\u6392\u5e8f\u6a21\u5757\u3002", "result": "\u5b9e\u9a8c\u663e\u793aFinSage\u53ec\u56de\u7387\u8fbe92.51%\uff0c\u51c6\u786e\u7387\u63d0\u534724.06%\uff0c\u5df2\u6210\u529f\u90e8\u7f72\u670d\u52a11200\u591a\u4eba\u3002", "conclusion": "FinSage\u4e3a\u91d1\u878d\u5408\u89c4\u5206\u6790\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2504.14494", "pdf": "https://arxiv.org/pdf/2504.14494", "abs": "https://arxiv.org/abs/2504.14494", "authors": ["Yue Li"], "title": "LBM-GNN: Graph Neural Network Enhanced Lattice Boltzmann Method", "categories": ["physics.flu-dyn", "cs.AI"], "comment": null, "summary": "In this paper, we present LBM-GNN, a novel approach that enhances the\ntraditional Lattice Boltzmann Method (LBM) with Graph Neural Networks (GNNs).\nWe apply this method to fluid dynamics simulations, demonstrating improved\nstability and accuracy compared to standard LBM implementations. The method is\nvalidated using benchmark problems such as the Taylor-Green vortex, focusing on\naccuracy, conservation properties, and performance across different Reynolds\nnumbers and grid resolutions. Our results indicate that GNN-enhanced LBM can\nmaintain better conservation properties while improving numerical stability at\nhigher Reynolds numbers.", "AI": {"tldr": "LBM-GNN\u7ed3\u5408\u4e86\u4f20\u7edf\u7684Lattice Boltzmann Method\uff08LBM\uff09\u4e0eGraph Neural Networks\uff08GNNs\uff09\uff0c\u7528\u4e8e\u6d41\u4f53\u52a8\u529b\u5b66\u6a21\u62df\uff0c\u63d0\u9ad8\u4e86\u7a33\u5b9a\u6027\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u4f20\u7edfLBM\u5728\u6d41\u4f53\u6a21\u62df\u4e2d\u5b58\u5728\u7a33\u5b9a\u6027\u548c\u7cbe\u5ea6\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u9ad8Reynolds\u6570\u60c5\u51b5\u4e0b\u3002", "method": "\u5c06GNNs\u5f15\u5165LBM\u6846\u67b6\uff0c\u901a\u8fc7\u56fe\u795e\u7ecf\u7f51\u7edc\u4f18\u5316\u6a21\u62df\u8fc7\u7a0b\u3002", "result": "\u5728Taylor-Green\u6da1\u6d41\u7b49\u57fa\u51c6\u95ee\u9898\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u51c6\u786e\u6027\u3001\u5b88\u6052\u6027\u548c\u6027\u80fd\uff0c\u663e\u793a\u5728\u9ad8Reynolds\u6570\u4e0b\u5177\u6709\u66f4\u597d\u7684\u6570\u503c\u7a33\u5b9a\u6027\u3002", "conclusion": "GNN\u589e\u5f3a\u7684LBM\u5728\u4fdd\u6301\u5b88\u6052\u6027\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u9ad8Reynolds\u6570\u4e0b\u7684\u6570\u503c\u7a33\u5b9a\u6027\u3002"}}
{"id": "2504.14509", "pdf": "https://arxiv.org/pdf/2504.14509", "abs": "https://arxiv.org/abs/2504.14509", "authors": ["Fulong Ye", "Miao Hua", "Pengze Zhang", "Xinghui Li", "Qichao Sun", "Songtao Zhao", "Qian He", "Xinglong Wu"], "title": "DreamID: High-Fidelity and Fast diffusion-based Face Swapping via Triplet ID Group Learning", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "In this paper, we introduce DreamID, a diffusion-based face swapping model\nthat achieves high levels of ID similarity, attribute preservation, image\nfidelity, and fast inference speed. Unlike the typical face swapping training\nprocess, which often relies on implicit supervision and struggles to achieve\nsatisfactory results. DreamID establishes explicit supervision for face\nswapping by constructing Triplet ID Group data, significantly enhancing\nidentity similarity and attribute preservation. The iterative nature of\ndiffusion models poses challenges for utilizing efficient image-space loss\nfunctions, as performing time-consuming multi-step sampling to obtain the\ngenerated image during training is impractical. To address this issue, we\nleverage the accelerated diffusion model SD Turbo, reducing the inference steps\nto a single iteration, enabling efficient pixel-level end-to-end training with\nexplicit Triplet ID Group supervision. Additionally, we propose an improved\ndiffusion-based model architecture comprising SwapNet, FaceNet, and ID Adapter.\nThis robust architecture fully unlocks the power of the Triplet ID Group\nexplicit supervision. Finally, to further extend our method, we explicitly\nmodify the Triplet ID Group data during training to fine-tune and preserve\nspecific attributes, such as glasses and face shape. Extensive experiments\ndemonstrate that DreamID outperforms state-of-the-art methods in terms of\nidentity similarity, pose and expression preservation, and image fidelity.\nOverall, DreamID achieves high-quality face swapping results at 512*512\nresolution in just 0.6 seconds and performs exceptionally well in challenging\nscenarios such as complex lighting, large angles, and occlusions.", "AI": {"tldr": "DreamID\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u4eba\u8138\u4ea4\u6362\u65b9\u6cd5\uff0c\u901a\u8fc7\u663e\u5f0f\u76d1\u7763\u548c\u9ad8\u6548\u67b6\u6784\u5b9e\u73b0\u9ad8\u8d28\u91cf\u3001\u5feb\u901f\u7684\u6362\u8138\u6548\u679c\u3002", "motivation": "\u4f20\u7edf\u4eba\u8138\u4ea4\u6362\u65b9\u6cd5\u4f9d\u8d56\u9690\u5f0f\u76d1\u7763\uff0c\u6548\u679c\u4e0d\u4f73\u3002DreamID\u65e8\u5728\u901a\u8fc7\u663e\u5f0f\u76d1\u7763\u63d0\u5347\u8eab\u4efd\u76f8\u4f3c\u6027\u548c\u5c5e\u6027\u4fdd\u7559\u3002", "method": "\u6784\u5efaTriplet ID Group\u6570\u636e\uff0c\u5229\u7528SD Turbo\u52a0\u901f\u6a21\u578b\uff0c\u63d0\u51faSwapNet\u3001FaceNet\u548cID Adapter\u67b6\u6784\uff0c\u652f\u6301\u7aef\u5230\u7aef\u8bad\u7ec3\u3002", "result": "\u5728\u8eab\u4efd\u76f8\u4f3c\u6027\u3001\u59ff\u6001\u8868\u60c5\u4fdd\u7559\u548c\u56fe\u50cf\u4fdd\u771f\u5ea6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c512*512\u5206\u8fa8\u7387\u4e0b\u4ec5\u97000.6\u79d2\u3002", "conclusion": "DreamID\u5728\u590d\u6742\u573a\u666f\u4e0b\u8868\u73b0\u4f18\u5f02\uff0c\u4e3a\u9ad8\u8d28\u91cf\u4eba\u8138\u4ea4\u6362\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.14541", "pdf": "https://arxiv.org/pdf/2504.14541", "abs": "https://arxiv.org/abs/2504.14541", "authors": ["Yi Yu", "Song Xia", "Xun Lin", "Chenqi Kong", "Wenhan Yang", "Shijian Lu", "Yap-Peng Tan", "Alex C. Kot"], "title": "Towards Model Resistant to Transferable Adversarial Examples via Trigger Activation", "categories": ["cs.CR", "cs.CV", "cs.LG"], "comment": "Accepted by IEEE TIFS 2025", "summary": "Adversarial examples, characterized by imperceptible perturbations, pose\nsignificant threats to deep neural networks by misleading their predictions. A\ncritical aspect of these examples is their transferability, allowing them to\ndeceive {unseen} models in black-box scenarios. Despite the widespread\nexploration of defense methods, including those on transferability, they show\nlimitations: inefficient deployment, ineffective defense, and degraded\nperformance on clean images. In this work, we introduce a novel training\nparadigm aimed at enhancing robustness against transferable adversarial\nexamples (TAEs) in a more efficient and effective way. We propose a model that\nexhibits random guessing behavior when presented with clean data\n$\\boldsymbol{x}$ as input, and generates accurate predictions when with\ntriggered data $\\boldsymbol{x}+\\boldsymbol{\\tau}$. Importantly, the trigger\n$\\boldsymbol{\\tau}$ remains constant for all data instances. We refer to these\nmodels as \\textbf{models with trigger activation}. We are surprised to find\nthat these models exhibit certain robustness against TAEs. Through the\nconsideration of first-order gradients, we provide a theoretical analysis of\nthis robustness. Moreover, through the joint optimization of the learnable\ntrigger and the model, we achieve improved robustness to transferable attacks.\nExtensive experiments conducted across diverse datasets, evaluating a variety\nof attacking methods, underscore the effectiveness and superiority of our\napproach.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u8bad\u7ec3\u8303\u5f0f\uff0c\u901a\u8fc7\u89e6\u53d1\u6fc0\u6d3b\u6a21\u578b\u589e\u5f3a\u5bf9\u6297\u6837\u672c\u7684\u53ef\u8f6c\u79fb\u6027\u9c81\u68d2\u6027\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u5bf9\u6297\u6837\u672c\u7684\u53ef\u8f6c\u79fb\u6027\u5bf9\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u6784\u6210\u5a01\u80c1\uff0c\u73b0\u6709\u9632\u5fa1\u65b9\u6cd5\u5b58\u5728\u6548\u7387\u4f4e\u3001\u6548\u679c\u5dee\u7b49\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u89e6\u53d1\u6fc0\u6d3b\u6a21\u578b\uff0c\u901a\u8fc7\u56fa\u5b9a\u89e6\u53d1\u5668\u4f18\u5316\u6a21\u578b\uff0c\u63d0\u5347\u5bf9\u53ef\u8f6c\u79fb\u5bf9\u6297\u6837\u672c\u7684\u9c81\u68d2\u6027\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u6570\u636e\u96c6\u548c\u653b\u51fb\u65b9\u6cd5\u4e0b\u8868\u73b0\u4f18\u8d8a\u3002", "conclusion": "\u89e6\u53d1\u6fc0\u6d3b\u6a21\u578b\u80fd\u6709\u6548\u63d0\u5347\u5bf9\u6297\u6837\u672c\u7684\u53ef\u8f6c\u79fb\u6027\u9c81\u68d2\u6027\uff0c\u4e14\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u652f\u6301\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2504.14522", "pdf": "https://arxiv.org/pdf/2504.14522", "abs": "https://arxiv.org/abs/2504.14522", "authors": ["Liudmila Zavolokina", "Kilian Sprenkamp", "Zoya Katashinskaya", "Daniel Gordon Jones"], "title": "Biased by Design: Leveraging AI Biases to Enhance Critical Thinking of News Readers", "categories": ["cs.HC", "cs.AI"], "comment": "European Conference on Information Systems (ECIS)", "summary": "This paper explores the design of a propaganda detection tool using Large\nLanguage Models (LLMs). Acknowledging the inherent biases in AI models,\nespecially in political contexts, we investigate how these biases might be\nleveraged to enhance critical thinking in news consumption. Countering the\ntypical view of AI biases as detrimental, our research proposes strategies of\nuser choice and personalization in response to a user's political stance,\napplying psychological concepts of confirmation bias and cognitive dissonance.\nWe present findings from a qualitative user study, offering insights and design\nrecommendations (bias awareness, personalization and choice, and gradual\nintroduction of diverse perspectives) for AI tools in propaganda detection.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u8bbe\u8ba1\u5ba3\u4f20\u68c0\u6d4b\u5de5\u5177\uff0c\u7814\u7a76\u5982\u4f55\u5229\u7528AI\u504f\u89c1\u589e\u5f3a\u65b0\u95fb\u6d88\u8d39\u4e2d\u7684\u6279\u5224\u6027\u601d\u7ef4\u3002", "motivation": "\u8ba4\u8bc6\u5230AI\u6a21\u578b\u5728\u653f\u6cbb\u80cc\u666f\u4e0b\u7684\u56fa\u6709\u504f\u89c1\uff0c\u7814\u7a76\u5982\u4f55\u5229\u7528\u8fd9\u4e9b\u504f\u89c1\u63d0\u5347\u7528\u6237\u7684\u6279\u5224\u6027\u601d\u7ef4\uff0c\u800c\u975e\u89c6\u5176\u4e3a\u6709\u5bb3\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u7528\u6237\u653f\u6cbb\u7acb\u573a\u7684\u4e2a\u6027\u5316\u7b56\u7565\uff0c\u7ed3\u5408\u5fc3\u7406\u5b66\u6982\u5ff5\uff08\u786e\u8ba4\u504f\u8bef\u548c\u8ba4\u77e5\u5931\u8c03\uff09\uff0c\u5e76\u901a\u8fc7\u5b9a\u6027\u7528\u6237\u7814\u7a76\u9a8c\u8bc1\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u5e76\u63d0\u51fa\u4e86\u8bbe\u8ba1\u5efa\u8bae\uff08\u504f\u89c1\u610f\u8bc6\u3001\u4e2a\u6027\u5316\u9009\u62e9\u3001\u9010\u6b65\u5f15\u5165\u591a\u6837\u5316\u89c2\u70b9\uff09\u3002", "conclusion": "AI\u5de5\u5177\u5728\u5ba3\u4f20\u68c0\u6d4b\u4e2d\u53ef\u901a\u8fc7\u4e2a\u6027\u5316\u7b56\u7565\u548c\u7528\u6237\u9009\u62e9\u4f18\u5316\u6548\u679c\uff0c\u540c\u65f6\u589e\u5f3a\u6279\u5224\u6027\u601d\u7ef4\u3002"}}
{"id": "2504.14568", "pdf": "https://arxiv.org/pdf/2504.14568", "abs": "https://arxiv.org/abs/2504.14568", "authors": ["Stefan-Alexandru Jura", "Mihai Udrescu"], "title": "Quantum-Enhanced Weight Optimization for Neural Networks Using Grover's Algorithm", "categories": ["quant-ph", "cs.LG"], "comment": null, "summary": "The main approach to hybrid quantum-classical neural networks (QNN) is\nemploying quantum computing to build a neural network (NN) that has quantum\nfeatures, which is then optimized classically. Here, we propose a different\nstrategy: to use quantum computing in order to optimize the weights of a\nclassical NN. As such, we design an instance of Grover's quantum search\nalgorithm to accelerate the search for the optimal parameters of an NN during\nthe training process, a task traditionally performed using the backpropagation\nalgorithm with the gradient descent method. Indeed, gradient descent has issues\nsuch as exploding gradient, vanishing gradient, or convexity problem. Other\nmethods tried to address such issues with strategies like genetic searches, but\nthey carry additional problems like convergence consistency. Our original\nmethod avoids these issues -- because it does not calculate gradients -- and\ncapitalizes on classical architectures' robustness and Grover's quadratic\nspeedup in high-dimensional search spaces to significantly reduce test loss\n(58.75%) and improve test accuracy (35.25%), compared to classical NN weight\noptimization, on small datasets. Unlike most QNNs that are trained on small\ndatasets only, our method is also scalable, as it allows the optimization of\ndeep networks; for an NN with 3 hidden layers, trained on the Digits dataset\nfrom scikit-learn, we obtained a mean accuracy of 97.7%. Moreover, our method\nrequires a much smaller number of qubits compared to other QNN approaches,\nmaking it very practical for near-future quantum computers that will still\ndeliver a limited number of logical qubits.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u91cf\u5b50\u8ba1\u7b97\u4f18\u5316\u7ecf\u5178\u795e\u7ecf\u7f51\u7edc\u6743\u91cd\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7Grover\u91cf\u5b50\u641c\u7d22\u7b97\u6cd5\u52a0\u901f\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u907f\u514d\u4e86\u68af\u5ea6\u4e0b\u964d\u7684\u95ee\u9898\uff0c\u5e76\u5728\u5c0f\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u68af\u5ea6\u4e0b\u964d\u65b9\u6cd5\u5b58\u5728\u68af\u5ea6\u7206\u70b8\u3001\u6d88\u5931\u6216\u51f8\u6027\u95ee\u9898\uff0c\u5176\u4ed6\u66ff\u4ee3\u65b9\u6cd5\u5982\u9057\u4f20\u641c\u7d22\u4e5f\u6709\u6536\u655b\u4e00\u81f4\u6027\u95ee\u9898\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u91cf\u5b50\u8ba1\u7b97\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u57fa\u4e8eGrover\u91cf\u5b50\u641c\u7d22\u7b97\u6cd5\u7684\u4f18\u5316\u65b9\u6cd5\uff0c\u76f4\u63a5\u641c\u7d22\u795e\u7ecf\u7f51\u7edc\u7684\u6700\u4f18\u53c2\u6570\uff0c\u65e0\u9700\u8ba1\u7b97\u68af\u5ea6\u3002", "result": "\u5728\u5c0f\u6570\u636e\u96c6\u4e0a\uff0c\u6d4b\u8bd5\u635f\u5931\u51cf\u5c1158.75%\uff0c\u6d4b\u8bd5\u51c6\u786e\u7387\u63d0\u9ad835.25%\uff1b\u57283\u5c42\u9690\u85cf\u7f51\u7edc\u7684Digits\u6570\u636e\u96c6\u4e0a\uff0c\u5e73\u5747\u51c6\u786e\u7387\u8fbe97.7%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u907f\u514d\u4e86\u68af\u5ea6\u95ee\u9898\uff0c\u5177\u6709\u53ef\u6269\u5c55\u6027\u548c\u5b9e\u7528\u6027\uff0c\u9002\u5408\u672a\u6765\u91cf\u5b50\u8ba1\u7b97\u673a\u8d44\u6e90\u6709\u9650\u7684\u60c5\u51b5\u3002"}}
{"id": "2504.14605", "pdf": "https://arxiv.org/pdf/2504.14605", "abs": "https://arxiv.org/abs/2504.14605", "authors": ["Masoud Ataei", "Xiaogang Wang"], "title": "Generalized Derangetropy Functionals for Modeling Cyclical Information Flow", "categories": ["cs.IT", "cs.LG", "math.IT"], "comment": null, "summary": "This paper introduces a framework for modeling cyclical and feedback-driven\ninformation flow through a generalized family of entropy-modulated\ntransformations called derangetropy functionals. Unlike scalar and static\nentropy measures such as Shannon entropy, these functionals act directly on\nprobability densities and provide a topographical representation of information\nstructure across the support of the distribution. The framework captures\nperiodic and self-referential aspects of information distribution and encodes\nthem through functional operators governed by nonlinear differential equations.\nWhen applied recursively, these operators induce a spectral diffusion process\ngoverned by the heat equation, leading to convergence toward a Gaussian\ncharacteristic function. This convergence theorem provides a unified analytical\nfoundation for describing the long-term dynamics of information under cyclic\nmodulation. The proposed framework offers new tools for analyzing the temporal\nevolution of information in systems characterized by periodic structure,\nstochastic feedback, and delayed interaction, with applications in artificial\nneural networks, communication theory, and non-equilibrium statistical\nmechanics.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u71b5\u8c03\u5236\u53d8\u6362\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5efa\u6a21\u5faa\u73af\u548c\u53cd\u9988\u9a71\u52a8\u7684\u4fe1\u606f\u6d41\uff0c\u901a\u8fc7\u975e\u7ebf\u6027\u5fae\u5206\u65b9\u7a0b\u63cf\u8ff0\u4fe1\u606f\u7ed3\u6784\u7684\u52a8\u6001\u6f14\u5316\u3002", "motivation": "\u4f20\u7edf\u71b5\u5ea6\u91cf\uff08\u5982\u9999\u519c\u71b5\uff09\u662f\u9759\u6001\u7684\uff0c\u65e0\u6cd5\u6355\u6349\u5468\u671f\u6027\u6216\u81ea\u53cd\u9988\u7684\u4fe1\u606f\u7ed3\u6784\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u52a8\u6001\u7684\u5efa\u6a21\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u79f0\u4e3aderangetropy functionals\u7684\u71b5\u8c03\u5236\u53d8\u6362\u65cf\uff0c\u901a\u8fc7\u975e\u7ebf\u6027\u5fae\u5206\u65b9\u7a0b\u63cf\u8ff0\u4fe1\u606f\u7ed3\u6784\u7684\u5468\u671f\u6027\u53d8\u5316\uff0c\u5e76\u9012\u5f52\u5e94\u7528\u8fd9\u4e9b\u7b97\u5b50\u3002", "result": "\u6846\u67b6\u63ed\u793a\u4e86\u4fe1\u606f\u5728\u5faa\u73af\u8c03\u5236\u4e0b\u7684\u957f\u671f\u52a8\u6001\uff0c\u6700\u7ec8\u6536\u655b\u4e8e\u9ad8\u65af\u7279\u5f81\u51fd\u6570\uff0c\u4e3a\u5468\u671f\u6027\u7ed3\u6784\u548c\u968f\u673a\u53cd\u9988\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5206\u6790\u5de5\u5177\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\u3001\u901a\u4fe1\u7406\u8bba\u548c\u975e\u5e73\u8861\u7edf\u8ba1\u529b\u5b66\u4e2d\u7684\u4fe1\u606f\u52a8\u6001\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2504.14548", "pdf": "https://arxiv.org/pdf/2504.14548", "abs": "https://arxiv.org/abs/2504.14548", "authors": ["Lifeng Lin", "Rongfeng Lu", "Quan Chen", "Haofan Ren", "Ming Lu", "Yaoqi Sun", "Chenggang Yan", "Anke Xue"], "title": "VGNC: Reducing the Overfitting of Sparse-view 3DGS via Validation-guided Gaussian Number Control", "categories": ["cs.CV", "cs.AI"], "comment": "10 pages,8 figures", "summary": "Sparse-view 3D reconstruction is a fundamental yet challenging task in\npractical 3D reconstruction applications. Recently, many methods based on the\n3D Gaussian Splatting (3DGS) framework have been proposed to address\nsparse-view 3D reconstruction. Although these methods have made considerable\nadvancements, they still show significant issues with overfitting. To reduce\nthe overfitting, we introduce VGNC, a novel Validation-guided Gaussian Number\nControl (VGNC) approach based on generative novel view synthesis (NVS) models.\nTo the best of our knowledge, this is the first attempt to alleviate the\noverfitting issue of sparse-view 3DGS with generative validation images.\nSpecifically, we first introduce a validation image generation method based on\na generative NVS model. We then propose a Gaussian number control strategy that\nutilizes generated validation images to determine the optimal Gaussian numbers,\nthereby reducing the issue of overfitting. We conducted detailed experiments on\nvarious sparse-view 3DGS baselines and datasets to evaluate the effectiveness\nof VGNC. Extensive experiments show that our approach not only reduces\noverfitting but also improves rendering quality on the test set while\ndecreasing the number of Gaussian points. This reduction lowers storage demands\nand accelerates both training and rendering. The code will be released.", "AI": {"tldr": "VGNC\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u751f\u6210\u5f0f\u65b0\u89c6\u89d2\u5408\u6210\u6a21\u578b\u7684\u9a8c\u8bc1\u5f15\u5bfc\u9ad8\u65af\u6570\u91cf\u63a7\u5236\u65b9\u6cd5\uff0c\u7528\u4e8e\u51cf\u5c11\u7a00\u758f\u89c6\u89d23D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u4e2d\u7684\u8fc7\u62df\u5408\u95ee\u9898\u3002", "motivation": "\u7a00\u758f\u89c6\u89d23D\u91cd\u5efa\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5b58\u5728\u8fc7\u62df\u5408\u95ee\u9898\uff0c\u73b0\u67093DGS\u65b9\u6cd5\u867d\u6709\u6240\u6539\u8fdb\u4f46\u4ecd\u672a\u89e3\u51b3\u3002", "method": "\u901a\u8fc7\u751f\u6210\u9a8c\u8bc1\u56fe\u50cf\u5e76\u57fa\u4e8e\u6b64\u63a7\u5236\u9ad8\u65af\u6570\u91cf\uff0c\u4ee5\u51cf\u5c11\u8fc7\u62df\u5408\u3002", "result": "\u5b9e\u9a8c\u8868\u660eVGNC\u4e0d\u4ec5\u51cf\u5c11\u8fc7\u62df\u5408\uff0c\u8fd8\u63d0\u5347\u6e32\u67d3\u8d28\u91cf\u5e76\u964d\u4f4e\u5b58\u50a8\u548c\u8ba1\u7b97\u9700\u6c42\u3002", "conclusion": "VGNC\u4e3a\u7a00\u758f\u89c6\u89d23DGS\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u8fc7\u62df\u5408\u7f13\u89e3\u65b9\u6848\u3002"}}
{"id": "2504.14628", "pdf": "https://arxiv.org/pdf/2504.14628", "abs": "https://arxiv.org/abs/2504.14628", "authors": ["Shunxin Guo", "Jiaqi Lv", "Qiufeng Wang", "Xin Geng"], "title": "GENE-FL: Gene-Driven Parameter-Efficient Dynamic Federated Learning", "categories": ["cs.DC", "cs.LG"], "comment": null, "summary": "Real-world \\underline{F}ederated \\underline{L}earning systems often encounter\n\\underline{D}ynamic clients with \\underline{A}gnostic and highly heterogeneous\ndata distributions (DAFL), which pose challenges for efficient communication\nand model initialization. To address these challenges, we draw inspiration from\nthe recently proposed Learngene paradigm, which compresses the large-scale\nmodel into lightweight, cross-task meta-information fragments. Learngene\neffectively encapsulates and communicates core knowledge, making it\nparticularly well-suited for DAFL, where dynamic client participation requires\ncommunication efficiency and rapid adaptation to new data distributions. Based\non this insight, we propose a Gene-driven parameter-efficient dynamic Federated\nLearning (GENE-FL) framework. First, local models perform quadratic constraints\nbased on parameters with high Fisher values in the global model, as these\nparameters are considered to encapsulate generalizable knowledge. Second, we\napply the strategy of parameter sensitivity analysis in local model parameters\nto condense the \\textit{learnGene} for interaction. Finally, the server\naggregates these small-scale trained \\textit{learnGene}s into a robust\n\\textit{learnGene} with cross-task generalization capability, facilitating the\nrapid initialization of dynamic agnostic client models. Extensive experimental\nresults demonstrate that GENE-FL reduces \\textbf{4 $\\times$} communication\ncosts compared to FEDAVG and effectively initializes agnostic client models\nwith only about \\textbf{9.04} MB.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faGENE-FL\u6846\u67b6\uff0c\u901a\u8fc7Learngene\u8303\u5f0f\u89e3\u51b3\u52a8\u6001\u5ba2\u6237\u7aef\u6570\u636e\u5f02\u6784\u95ee\u9898\uff0c\u663e\u8457\u964d\u4f4e\u901a\u4fe1\u6210\u672c\u5e76\u5feb\u901f\u521d\u59cb\u5316\u6a21\u578b\u3002", "motivation": "\u73b0\u5b9e\u8054\u90a6\u5b66\u4e60\u7cfb\u7edf\u4e2d\u52a8\u6001\u5ba2\u6237\u7aef\u6570\u636e\u5206\u5e03\u5f02\u6784\u4e14\u4e0d\u53ef\u77e5\uff08DAFL\uff09\uff0c\u5bfc\u81f4\u901a\u4fe1\u6548\u7387\u4f4e\u548c\u6a21\u578b\u521d\u59cb\u5316\u56f0\u96be\u3002", "method": "\u57fa\u4e8eLearngene\u8303\u5f0f\uff0cGENE-FL\u6846\u67b6\u901a\u8fc7Fisher\u503c\u7b5b\u9009\u53c2\u6570\u3001\u672c\u5730\u53c2\u6570\u654f\u611f\u6027\u5206\u6790\u538b\u7f29learnGene\uff0c\u670d\u52a1\u5668\u805a\u5408\u751f\u6210\u8de8\u4efb\u52a1\u6cdb\u5316\u7684learnGene\u3002", "result": "\u5b9e\u9a8c\u663e\u793aGENE-FL\u901a\u4fe1\u6210\u672c\u964d\u4f4e4\u500d\uff0c\u4ec5\u97009.04 MB\u5373\u53ef\u521d\u59cb\u5316\u6a21\u578b\u3002", "conclusion": "GENE-FL\u6709\u6548\u89e3\u51b3\u4e86DAFL\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u901a\u4fe1\u6548\u7387\u548c\u6a21\u578b\u521d\u59cb\u5316\u901f\u5ea6\u3002"}}
{"id": "2504.14560", "pdf": "https://arxiv.org/pdf/2504.14560", "abs": "https://arxiv.org/abs/2504.14560", "authors": ["Haiyan Qin", "Zhiwei Xie", "Jingjing Li", "Liangchen Li", "Xiaotong Feng", "Junzhan Liu", "Wang Kang"], "title": "ReasoningV: Efficient Verilog Code Generation with Adaptive Hybrid Reasoning Model", "categories": ["cs.AR", "cs.AI"], "comment": "9 pages, 4 figures", "summary": "Large Language Models (LLMs) have advanced Verilog code generation\nsignificantly, yet face challenges in data quality, reasoning capabilities, and\ncomputational efficiency. This paper presents ReasoningV, a novel model\nemploying a hybrid reasoning strategy that integrates trained intrinsic\ncapabilities with dynamic inference adaptation for Verilog code generation. Our\nframework introduces three complementary innovations: (1) ReasoningV-5K, a\nhigh-quality dataset of 5,000 functionally verified instances with reasoning\npaths created through multi-dimensional filtering of PyraNet samples; (2) a\ntwo-stage training approach combining parameter-efficient fine-tuning for\nfoundational knowledge with full-parameter optimization for enhanced reasoning;\nand (3) an adaptive reasoning mechanism that dynamically adjusts reasoning\ndepth based on problem complexity, reducing token consumption by up to 75\\%\nwhile preserving performance. Experimental results demonstrate ReasoningV's\neffectiveness with a pass@1 accuracy of 57.8\\% on VerilogEval-human, achieving\nperformance competitive with leading commercial models like Gemini-2.0-flash\n(59.5\\%) and exceeding the previous best open-source model by 10.4 percentage\npoints. ReasoningV offers a more reliable and accessible pathway for advancing\nAI-driven hardware design automation, with our model, data, and code available\nat https://github.com/BUAA-CLab/ReasoningV.", "AI": {"tldr": "ReasoningV\u662f\u4e00\u79cd\u65b0\u578b\u6a21\u578b\uff0c\u901a\u8fc7\u6df7\u5408\u63a8\u7406\u7b56\u7565\u63d0\u5347Verilog\u4ee3\u7801\u751f\u6210\u80fd\u529b\uff0c\u89e3\u51b3\u4e86\u6570\u636e\u8d28\u91cf\u3001\u63a8\u7406\u80fd\u529b\u548c\u8ba1\u7b97\u6548\u7387\u7684\u6311\u6218\u3002", "motivation": "LLMs\u5728Verilog\u4ee3\u7801\u751f\u6210\u4e2d\u9762\u4e34\u6570\u636e\u8d28\u91cf\u3001\u63a8\u7406\u80fd\u529b\u548c\u8ba1\u7b97\u6548\u7387\u7684\u4e0d\u8db3\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faReasoningV-5K\u6570\u636e\u96c6\u3001\u4e24\u9636\u6bb5\u8bad\u7ec3\u65b9\u6cd5\u548c\u81ea\u9002\u5e94\u63a8\u7406\u673a\u5236\uff0c\u52a8\u6001\u8c03\u6574\u63a8\u7406\u6df1\u5ea6\u4ee5\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\u3002", "result": "\u5728VerilogEval-human\u4e0a\u8fbe\u523057.8%\u7684pass@1\u51c6\u786e\u7387\uff0c\u6027\u80fd\u63a5\u8fd1\u5546\u4e1a\u6a21\u578b\uff0c\u5e76\u8d85\u8d8a\u5f00\u6e90\u6a21\u578b10.4\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "ReasoningV\u4e3aAI\u9a71\u52a8\u7684\u786c\u4ef6\u8bbe\u8ba1\u81ea\u52a8\u5316\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u548c\u53ef\u8bbf\u95ee\u7684\u9014\u5f84\uff0c\u6a21\u578b\u3001\u6570\u636e\u548c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2504.14686", "pdf": "https://arxiv.org/pdf/2504.14686", "abs": "https://arxiv.org/abs/2504.14686", "authors": ["Jos\u00e9 Su\u00e1rez-Varela", "Andra Lutu"], "title": "Uncovering Issues in the Radio Access Network by Looking at the Neighbors", "categories": ["cs.NI", "cs.AI", "cs.LG"], "comment": "7 pages", "summary": "Mobile network operators (MNOs) manage Radio Access Networks (RANs) with\nmassive amounts of cells over multiple radio generations (2G-5G). To handle\nsuch complexity, operations teams rely on monitoring systems, including anomaly\ndetection tools that identify unexpected behaviors. In this paper, we present\nc-ANEMON, a Contextual ANomaly dEtection MONitor for the RAN based on Graph\nNeural Networks (GNNs). Our solution captures spatio-temporal variations by\nanalyzing the behavior of individual cells in relation to their local\nneighborhoods, enabling the detection of anomalies that are independent of\nexternal mobility factors. This, in turn, allows focusing on anomalies\nassociated with network issues (e.g., misconfigurations, equipment failures).\nWe evaluate c-ANEMON using real-world data from a large European metropolitan\narea (7,890 cells; 3 months). First, we show that the GNN model within our\nsolution generalizes effectively to cells from previously unseen areas,\nsuggesting the possibility of using a single model across extensive deployment\nregions. Then, we analyze the anomalies detected by c-ANEMON through manual\ninspection and define several categories of long-lasting anomalies (6+ hours).\nNotably, 45.95% of these anomalies fall into a category that is more likely to\nrequire intervention by operations teams.", "AI": {"tldr": "c-ANEMON\u662f\u4e00\u79cd\u57fa\u4e8e\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u7684\u4e0a\u4e0b\u6587\u5f02\u5e38\u68c0\u6d4b\u5de5\u5177\uff0c\u7528\u4e8e\u65e0\u7ebf\u63a5\u5165\u7f51\u7edc\uff08RAN\uff09\uff0c\u901a\u8fc7\u5206\u6790\u5355\u4e2a\u5c0f\u533a\u4e0e\u5176\u5c40\u90e8\u90bb\u57df\u7684\u5173\u7cfb\u6765\u68c0\u6d4b\u5f02\u5e38\u3002", "motivation": "\u79fb\u52a8\u7f51\u7edc\u8fd0\u8425\u5546\uff08MNOs\uff09\u9700\u8981\u5904\u7406\u591a\u4ee3\u65e0\u7ebf\u7f51\u7edc\uff082G-5G\uff09\u4e2d\u5927\u91cf\u5c0f\u533a\u7684\u590d\u6742\u6027\uff0c\u73b0\u6709\u76d1\u63a7\u7cfb\u7edf\u96be\u4ee5\u72ec\u7acb\u4e8e\u5916\u90e8\u79fb\u52a8\u6027\u56e0\u7d20\u68c0\u6d4b\u5f02\u5e38\u3002", "method": "c-ANEMON\u5229\u7528GNN\u6355\u6349\u65f6\u7a7a\u53d8\u5316\uff0c\u5206\u6790\u5c0f\u533a\u884c\u4e3a\u4e0e\u90bb\u57df\u5173\u7cfb\uff0c\u4e13\u6ce8\u4e8e\u7f51\u7edc\u95ee\u9898\u76f8\u5173\u7684\u5f02\u5e38\uff08\u5982\u914d\u7f6e\u9519\u8bef\u3001\u8bbe\u5907\u6545\u969c\uff09\u3002", "result": "\u5728\u771f\u5b9e\u6570\u636e\uff087,890\u4e2a\u5c0f\u533a\uff1b3\u4e2a\u6708\uff09\u4e0a\u9a8c\u8bc1\uff0cGNN\u6a21\u578b\u80fd\u6cdb\u5316\u5230\u672a\u89c1\u533a\u57df\uff0c45.95%\u7684\u957f\u671f\u5f02\u5e38\uff086+\u5c0f\u65f6\uff09\u9700\u4eba\u5de5\u5e72\u9884\u3002", "conclusion": "c-ANEMON\u5c55\u793a\u4e86GNN\u5728RAN\u5f02\u5e38\u68c0\u6d4b\u4e2d\u7684\u6f5c\u529b\uff0c\u652f\u6301\u8de8\u5e7f\u6cdb\u90e8\u7f72\u533a\u57df\u7684\u5355\u6a21\u578b\u5e94\u7528\uff0c\u5e76\u8bc6\u522b\u51fa\u9700\u64cd\u4f5c\u56e2\u961f\u5173\u6ce8\u7684\u5f02\u5e38\u7c7b\u522b\u3002"}}
{"id": "2504.14573", "pdf": "https://arxiv.org/pdf/2504.14573", "abs": "https://arxiv.org/abs/2504.14573", "authors": ["Jiawei Jiang", "Kei Ota", "Devesh K. Jha", "Asako Kanezaki"], "title": "Modality Selection and Skill Segmentation via Cross-Modality Attention", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Incorporating additional sensory modalities such as tactile and audio into\nfoundational robotic models poses significant challenges due to the curse of\ndimensionality. This work addresses this issue through modality selection. We\npropose a cross-modality attention (CMA) mechanism to identify and selectively\nutilize the modalities that are most informative for action generation at each\ntimestep. Furthermore, we extend the application of CMA to segment primitive\nskills from expert demonstrations and leverage this segmentation to train a\nhierarchical policy capable of solving long-horizon, contact-rich manipulation\ntasks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u673a\u5236\uff08CMA\uff09\uff0c\u7528\u4e8e\u9009\u62e9\u6027\u5730\u5229\u7528\u4fe1\u606f\u91cf\u6700\u5927\u7684\u6a21\u6001\uff0c\u5e76\u6269\u5c55\u5176\u5e94\u7528\u4ee5\u5206\u5272\u539f\u59cb\u6280\u80fd\u5e76\u8bad\u7ec3\u5206\u5c42\u7b56\u7565\u3002", "motivation": "\u89e3\u51b3\u56e0\u7ef4\u5ea6\u707e\u96be\u5bfc\u81f4\u7684\u591a\u6a21\u6001\uff08\u5982\u89e6\u89c9\u548c\u542c\u89c9\uff09\u878d\u5165\u673a\u5668\u4eba\u57fa\u7840\u6a21\u578b\u7684\u6311\u6218\u3002", "method": "\u63d0\u51fa\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u673a\u5236\uff08CMA\uff09\u8fdb\u884c\u6a21\u6001\u9009\u62e9\uff0c\u5e76\u7528\u4e8e\u5206\u5272\u4e13\u5bb6\u6f14\u793a\u4e2d\u7684\u539f\u59cb\u6280\u80fd\uff0c\u8bad\u7ec3\u5206\u5c42\u7b56\u7565\u3002", "result": "\u6210\u529f\u89e3\u51b3\u4e86\u957f\u65f6\u7a0b\u3001\u63a5\u89e6\u5bc6\u96c6\u7684\u64cd\u7eb5\u4efb\u52a1\u3002", "conclusion": "CMA\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u878d\u5408\u7684\u6311\u6218\uff0c\u5e76\u63d0\u5347\u4e86\u673a\u5668\u4eba\u4efb\u52a1\u7684\u8868\u73b0\u3002"}}
{"id": "2504.14696", "pdf": "https://arxiv.org/pdf/2504.14696", "abs": "https://arxiv.org/abs/2504.14696", "authors": ["Naima Tasnim", "Atefeh Gilani", "Lalitha Sankar", "Oliver Kosut"], "title": "Reveal-or-Obscure: A Differentially Private Sampling Algorithm for Discrete Distributions", "categories": ["cs.IT", "cs.CR", "cs.DS", "cs.LG", "math.IT"], "comment": "8 pages, 3 figures", "summary": "We introduce a differentially private (DP) algorithm called reveal-or-obscure\n(ROO) to generate a single representative sample from a dataset of $n$\nobservations drawn i.i.d. from an unknown discrete distribution $P$. Unlike\nmethods that add explicit noise to the estimated empirical distribution, ROO\nachieves $\\epsilon$-differential privacy by randomly choosing whether to\n\"reveal\" or \"obscure\" the empirical distribution. While ROO is structurally\nidentical to Algorithm 1 proposed by Cheu and Nayak (arXiv:2412.10512), we\nprove a strictly better bound on the sampling complexity than that established\nin Theorem 12 of (arXiv:2412.10512). To further improve the privacy-utility\ntrade-off, we propose a novel generalized sampling algorithm called\nData-Specific ROO (DS-ROO), where the probability of obscuring the empirical\ndistribution of the dataset is chosen adaptively. We prove that DS-ROO\nsatisfies $\\epsilon$-DP, and provide empirical evidence that DS-ROO can achieve\nbetter utility under the same privacy budget of vanilla ROO.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5dee\u5206\u9690\u79c1\u7b97\u6cd5ROO\uff0c\u901a\u8fc7\u968f\u673a\u9009\u62e9\u201c\u63ed\u793a\u201d\u6216\u201c\u906e\u853d\u201d\u7ecf\u9a8c\u5206\u5e03\u6765\u751f\u6210\u4ee3\u8868\u6027\u6837\u672c\uff0c\u5e76\u8fdb\u4e00\u6b65\u63d0\u51fa\u6539\u8fdb\u7b97\u6cd5DS-ROO\u4ee5\u4f18\u5316\u9690\u79c1-\u6548\u7528\u6743\u8861\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u5dee\u5206\u9690\u79c1\u65b9\u6cd5\u4e2d\u56e0\u6dfb\u52a0\u663e\u5f0f\u566a\u58f0\u5bfc\u81f4\u7684\u6548\u7528\u635f\u5931\u95ee\u9898\uff0c\u63d0\u51fa\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u3002", "method": "ROO\u968f\u673a\u9009\u62e9\u63ed\u793a\u6216\u906e\u853d\u7ecf\u9a8c\u5206\u5e03\uff1bDS-ROO\u81ea\u9002\u5e94\u8c03\u6574\u906e\u853d\u6982\u7387\u4ee5\u4f18\u5316\u6548\u7528\u3002", "result": "ROO\u5728\u91c7\u6837\u590d\u6742\u5ea6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff1bDS-ROO\u5728\u76f8\u540c\u9690\u79c1\u9884\u7b97\u4e0b\u63d0\u4f9b\u66f4\u597d\u7684\u6548\u7528\u3002", "conclusion": "ROO\u548cDS-ROO\u4e3a\u5dee\u5206\u9690\u79c1\u6570\u636e\u751f\u6210\u63d0\u4f9b\u4e86\u66f4\u4f18\u7684\u89e3\u51b3\u65b9\u6848\uff0cDS-ROO\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u9690\u79c1-\u6548\u7528\u6743\u8861\u3002"}}
{"id": "2504.14588", "pdf": "https://arxiv.org/pdf/2504.14588", "abs": "https://arxiv.org/abs/2504.14588", "authors": ["Wenke Xia", "Ruoxuan Feng", "Dong Wang", "Di Hu"], "title": "Phoenix: A Motion-based Self-Reflection Framework for Fine-grained Robotic Action Correction", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": "Accepted by CVPR2025", "summary": "Building a generalizable self-correction system is crucial for robots to\nrecover from failures. Despite advancements in Multimodal Large Language Models\n(MLLMs) that empower robots with semantic reflection ability for failure,\ntranslating semantic reflection into how to correct fine-grained robotic\nactions remains a significant challenge. To address this gap, we build the\nPhoenix framework, which leverages motion instruction as a bridge to connect\nhigh-level semantic reflection with low-level robotic action correction. In\nthis motion-based self-reflection framework, we start with a dual-process\nmotion adjustment mechanism with MLLMs to translate the semantic reflection\ninto coarse-grained motion instruction adjustment. To leverage this motion\ninstruction for guiding how to correct fine-grained robotic actions, a\nmulti-task motion-conditioned diffusion policy is proposed to integrate visual\nobservations for high-frequency robotic action correction. By combining these\ntwo models, we could shift the demand for generalization capability from the\nlow-level manipulation policy to the MLLMs-driven motion adjustment model and\nfacilitate precise, fine-grained robotic action correction. Utilizing this\nframework, we further develop a lifelong learning method to automatically\nimprove the model's capability from interactions with dynamic environments. The\nexperiments conducted in both the RoboMimic simulation and real-world scenarios\nprove the superior generalization and robustness of our framework across a\nvariety of manipulation tasks. Our code is released at\n\\href{https://github.com/GeWu-Lab/Motion-based-Self-Reflection-Framework}{https://github.com/GeWu-Lab/Motion-based-Self-Reflection-Framework}.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86Phoenix\u6846\u67b6\uff0c\u901a\u8fc7\u8fd0\u52a8\u6307\u4ee4\u8fde\u63a5\u9ad8\u7ea7\u8bed\u4e49\u53cd\u601d\u4e0e\u4f4e\u7ea7\u673a\u5668\u4eba\u52a8\u4f5c\u4fee\u6b63\uff0c\u7ed3\u5408\u53cc\u8fc7\u7a0b\u8fd0\u52a8\u8c03\u6574\u673a\u5236\u548c\u591a\u4efb\u52a1\u8fd0\u52a8\u6761\u4ef6\u6269\u6563\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u673a\u5668\u4eba\u52a8\u4f5c\u7684\u7cbe\u786e\u4fee\u6b63\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u4eba\u4ece\u5931\u8d25\u4e2d\u6062\u590d\u7684\u901a\u7528\u6027\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5982\u4f55\u5c06\u8bed\u4e49\u53cd\u601d\u8f6c\u5316\u4e3a\u7ec6\u7c92\u5ea6\u52a8\u4f5c\u4fee\u6b63\u7684\u6311\u6218\u3002", "method": "\u91c7\u7528\u53cc\u8fc7\u7a0b\u8fd0\u52a8\u8c03\u6574\u673a\u5236\u548c\u591a\u4efb\u52a1\u8fd0\u52a8\u6761\u4ef6\u6269\u6563\u7b56\u7565\uff0c\u7ed3\u5408\u89c6\u89c9\u89c2\u5bdf\u8fdb\u884c\u9ad8\u9891\u52a8\u4f5c\u4fee\u6b63\uff0c\u5e76\u901a\u8fc7\u7ec8\u8eab\u5b66\u4e60\u65b9\u6cd5\u63d0\u5347\u6a21\u578b\u80fd\u529b\u3002", "result": "\u5728RoboMimic\u4eff\u771f\u548c\u771f\u5b9e\u573a\u666f\u4e2d\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "Phoenix\u6846\u67b6\u901a\u8fc7\u8fd0\u52a8\u6307\u4ee4\u548c\u6269\u6563\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u52a8\u4f5c\u4fee\u6b63\u7684\u7cbe\u786e\u6027\u548c\u901a\u7528\u6027\u3002"}}
{"id": "2504.14708", "pdf": "https://arxiv.org/pdf/2504.14708", "abs": "https://arxiv.org/abs/2504.14708", "authors": ["Parshuram N. Aarotale", "Ajita Rattani"], "title": "Time Frequency Analysis of EMG Signal for Gesture Recognition using Fine grained Features", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Electromyography (EMG) based hand gesture recognition converts forearm muscle\nactivity into control commands for prosthetics, rehabilitation, and human\ncomputer interaction. This paper proposes a novel approach to EMG-based hand\ngesture recognition that uses fine-grained classification and presents XMANet,\nwhich unifies low-level local and high level semantic cues through cross layer\nmutual attention among shallow to deep CNN experts. Using stacked spectrograms\nand scalograms derived from the Short Time Fourier Transform (STFT) and Wavelet\nTransform (WT), we benchmark XMANet against ResNet50, DenseNet-121,\nMobileNetV3, and EfficientNetB0. Experimental results on the Grabmyo dataset\nindicate that, using STFT, the proposed XMANet model outperforms the baseline\nResNet50, EfficientNetB0, MobileNetV3, and DenseNet121 models with improvement\nof approximately 1.72%, 4.38%, 5.10%, and 2.53%, respectively. When employing\nthe WT approach, improvements of around 1.57%, 1.88%, 1.46%, and 2.05% are\nobserved over the same baselines. Similarly, on the FORS EMG dataset, the\nXMANet(ResNet50) model using STFT shows an improvement of about 5.04% over the\nbaseline ResNet50. In comparison, the XMANet(DenseNet121) and\nXMANet(MobileNetV3) models yield enhancements of approximately 4.11% and 2.81%,\nrespectively. Moreover, when using WT, the proposed XMANet achieves gains of\naround 4.26%, 9.36%, 5.72%, and 6.09% over the baseline ResNet50, DenseNet121,\nMobileNetV3, and EfficientNetB0 models, respectively. These results confirm\nthat XMANet consistently improves performance across various architectures and\nsignal processing techniques, demonstrating the strong potential of fine\ngrained features for accurate and robust EMG classification.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eEMG\u7684\u624b\u52bf\u8bc6\u522b\u65b0\u65b9\u6cd5XMANet\uff0c\u901a\u8fc7\u8de8\u5c42\u4e92\u6ce8\u610f\u529b\u7ed3\u5408\u5c40\u90e8\u548c\u8bed\u4e49\u7279\u5f81\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u63d0\u5347EMG\u624b\u52bf\u8bc6\u522b\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\uff0c\u7528\u4e8e\u5047\u80a2\u63a7\u5236\u3001\u5eb7\u590d\u548c\u4eba\u673a\u4ea4\u4e92\u3002", "method": "\u4f7f\u7528XMANet\u6a21\u578b\uff0c\u7ed3\u5408\u77ed\u65f6\u5085\u91cc\u53f6\u53d8\u6362\u548c\u5c0f\u6ce2\u53d8\u6362\u751f\u6210\u7684\u7279\u5f81\u56fe\uff0c\u901a\u8fc7\u8de8\u5c42\u4e92\u6ce8\u610f\u529b\u673a\u5236\u6574\u5408\u6d45\u5c42\u548c\u6df1\u5c42CNN\u7279\u5f81\u3002", "result": "\u5728Grabmyo\u548cFORS EMG\u6570\u636e\u96c6\u4e0a\uff0cXMANet\u76f8\u6bd4\u57fa\u7ebf\u6a21\u578b\uff08\u5982ResNet50\u3001DenseNet121\u7b49\uff09\u6027\u80fd\u63d0\u53471.46%\u81f39.36%\u3002", "conclusion": "XMANet\u901a\u8fc7\u7ec6\u7c92\u5ea6\u7279\u5f81\u63d0\u53d6\u548c\u8de8\u5c42\u6ce8\u610f\u529b\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86EMG\u624b\u52bf\u8bc6\u522b\u7684\u6027\u80fd\uff0c\u5c55\u73b0\u4e86\u5176\u5e7f\u6cdb\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2504.14717", "pdf": "https://arxiv.org/pdf/2504.14717", "abs": "https://arxiv.org/abs/2504.14717", "authors": ["Bowei Zhang", "Lei Ke", "Adam W. Harley", "Katerina Fragkiadaki"], "title": "TAPIP3D: Tracking Any Point in Persistent 3D Geometry", "categories": ["cs.CV", "cs.LG"], "comment": "Long-term feed-forward 3D point tracking in persistent 3D point maps.\n  Code:https://github.com/zbw001/TAPIP3D", "summary": "We introduce TAPIP3D, a novel approach for long-term 3D point tracking in\nmonocular RGB and RGB-D videos. TAPIP3D represents videos as camera-stabilized\nspatio-temporal feature clouds, leveraging depth and camera motion information\nto lift 2D video features into a 3D world space where camera motion is\neffectively canceled. TAPIP3D iteratively refines multi-frame 3D motion\nestimates within this stabilized representation, enabling robust tracking over\nextended periods. To manage the inherent irregularities of 3D point\ndistributions, we propose a Local Pair Attention mechanism. This 3D\ncontextualization strategy effectively exploits spatial relationships in 3D,\nforming informative feature neighborhoods for precise 3D trajectory estimation.\nOur 3D-centric approach significantly outperforms existing 3D point tracking\nmethods and even enhances 2D tracking accuracy compared to conventional 2D\npixel trackers when accurate depth is available. It supports inference in both\ncamera coordinates (i.e., unstabilized) and world coordinates, and our results\ndemonstrate that compensating for camera motion improves tracking performance.\nOur approach replaces the conventional 2D square correlation neighborhoods used\nin prior 2D and 3D trackers, leading to more robust and accurate results across\nvarious 3D point tracking benchmarks. Project Page: https://tapip3d.github.io", "AI": {"tldr": "TAPIP3D\u662f\u4e00\u79cd\u7528\u4e8e\u5355\u76eeRGB\u548cRGB-D\u89c6\u9891\u4e2d\u957f\u671f3D\u70b9\u8ddf\u8e2a\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u89c6\u9891\u8868\u793a\u4e3a\u76f8\u673a\u7a33\u5b9a\u7684\u65f6\u7a7a\u7279\u5f81\u4e91\uff0c\u5e76\u5229\u7528\u6df1\u5ea6\u548c\u76f8\u673a\u8fd0\u52a8\u4fe1\u606f\u63d0\u53472D\u7279\u5f81\u52303D\u7a7a\u95f4\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8ddf\u8e2a\u6027\u80fd\u3002", "motivation": "\u73b0\u67093D\u70b9\u8ddf\u8e2a\u65b9\u6cd5\u5728\u5904\u7406\u957f\u671f\u8ddf\u8e2a\u548c\u76f8\u673a\u8fd0\u52a8\u65f6\u8868\u73b0\u4e0d\u4f73\uff0cTAPIP3D\u65e8\u5728\u901a\u8fc7\u7a33\u5b9a\u76843D\u8868\u793a\u548c\u5c40\u90e8\u6ce8\u610f\u529b\u673a\u5236\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "TAPIP3D\u5c06\u89c6\u9891\u8868\u793a\u4e3a\u76f8\u673a\u7a33\u5b9a\u76843D\u7279\u5f81\u4e91\uff0c\u5229\u7528\u6df1\u5ea6\u548c\u76f8\u673a\u8fd0\u52a8\u4fe1\u606f\u63d0\u53472D\u7279\u5f81\u52303D\u7a7a\u95f4\uff0c\u5e76\u901a\u8fc7\u5c40\u90e8\u5bf9\u6ce8\u610f\u529b\u673a\u5236\u4f18\u53163D\u8fd0\u52a8\u4f30\u8ba1\u3002", "result": "TAPIP3D\u57283D\u70b9\u8ddf\u8e2a\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5728\u6709\u51c6\u786e\u6df1\u5ea6\u4fe1\u606f\u65f6\u63d0\u5347\u4e862D\u8ddf\u8e2a\u7cbe\u5ea6\u3002", "conclusion": "TAPIP3D\u901a\u8fc7\u7a33\u5b9a\u76843D\u8868\u793a\u548c\u5c40\u90e8\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u66f4\u9c81\u68d2\u548c\u51c6\u786e\u76843D\u70b9\u8ddf\u8e2a\uff0c\u540c\u65f6\u652f\u6301\u76f8\u673a\u5750\u6807\u7cfb\u548c\u4e16\u754c\u5750\u6807\u7cfb\u7684\u63a8\u7406\u3002"}}
{"id": "2504.14602", "pdf": "https://arxiv.org/pdf/2504.14602", "abs": "https://arxiv.org/abs/2504.14602", "authors": ["Jiwei Li", "Bi Zhang", "Xiaowei Tan", "Wanxin Chen", "Zhaoyuan Liu", "Juanjuan Zhang", "Weiguang Huo", "Jian Huang", "Lianqing Liu", "Xingang Zhao"], "title": "K2MUSE: A human lower limb multimodal dataset under diverse conditions for facilitating rehabilitation robotics", "categories": ["cs.RO", "cs.AI", "cs.HC"], "comment": "23 pages, 13 figures,4 tables", "summary": "The natural interaction and control performance of lower limb rehabilitation\nrobots are closely linked to biomechanical information from various human\nlocomotion activities. Multidimensional human motion data significantly deepen\nthe understanding of the complex mechanisms governing neuromuscular\nalterations, thereby facilitating the development and application of\nrehabilitation robots in multifaceted real-world environments. However,\ncurrently available lower limb datasets are inadequate for supplying the\nessential multimodal data and large-scale gait samples necessary for effective\ndata-driven approaches, and they neglect the significant effects of acquisition\ninterference in real applications.To fill this gap, we present the K2MUSE\ndataset, which includes a comprehensive collection of multimodal data,\ncomprising kinematic, kinetic, amplitude-mode ultrasound (AUS), and surface\nelectromyography (sEMG) measurements. The proposed dataset includes lower limb\nmultimodal data from 30 able-bodied participants walking under different\ninclines (0$^\\circ$, $\\pm$5$^\\circ$, and $\\pm$10$^\\circ$), various speeds (0.5\nm/s, 1.0 m/s, and 1.5 m/s), and different nonideal acquisition conditions\n(muscle fatigue, electrode shifts, and inter-day differences). The kinematic\nand ground reaction force data were collected via a Vicon motion capture system\nand an instrumented treadmill with embedded force plates, whereas the sEMG and\nAUS data were synchronously recorded for thirteen muscles on the bilateral\nlower limbs. This dataset offers a new resource for designing control\nframeworks for rehabilitation robots and conducting biomechanical analyses of\nlower limb locomotion. The dataset is available at https://k2muse.github.io/.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86K2MUSE\u6570\u636e\u96c6\uff0c\u586b\u8865\u4e86\u73b0\u6709\u4e0b\u80a2\u5eb7\u590d\u673a\u5668\u4eba\u7814\u7a76\u4e2d\u591a\u6a21\u6001\u6570\u636e\u548c\u5927\u89c4\u6a21\u6b65\u6001\u6837\u672c\u7684\u4e0d\u8db3\uff0c\u63d0\u4f9b\u4e86\u5305\u62ec\u8fd0\u52a8\u5b66\u3001\u52a8\u529b\u5b66\u3001\u8d85\u58f0\u548c\u808c\u7535\u4fe1\u53f7\u5728\u5185\u7684\u5168\u9762\u6570\u636e\u3002", "motivation": "\u73b0\u6709\u4e0b\u80a2\u6570\u636e\u96c6\u65e0\u6cd5\u6ee1\u8db3\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u5bf9\u591a\u6a21\u6001\u6570\u636e\u548c\u5927\u89c4\u6a21\u6837\u672c\u7684\u9700\u6c42\uff0c\u4e14\u5ffd\u7565\u4e86\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u91c7\u96c6\u5e72\u6270\u3002", "method": "\u6536\u96c6\u4e8630\u540d\u5065\u5eb7\u53c2\u4e0e\u8005\u5728\u4e0d\u540c\u5761\u5ea6\u3001\u901f\u5ea6\u548c\u5e72\u6270\u6761\u4ef6\u4e0b\u7684\u591a\u6a21\u6001\u4e0b\u80a2\u6570\u636e\uff0c\u5305\u62ec\u8fd0\u52a8\u5b66\u3001\u52a8\u529b\u5b66\u3001\u8d85\u58f0\u548c\u808c\u7535\u4fe1\u53f7\u3002", "result": "K2MUSE\u6570\u636e\u96c6\u4e3a\u5eb7\u590d\u673a\u5668\u4eba\u63a7\u5236\u6846\u67b6\u8bbe\u8ba1\u548c\u4e0b\u80a2\u8fd0\u52a8\u751f\u7269\u529b\u5b66\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u8d44\u6e90\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u586b\u8865\u4e86\u7814\u7a76\u7a7a\u767d\uff0c\u6709\u671b\u4fc3\u8fdb\u5eb7\u590d\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u5e94\u7528\u3002"}}
{"id": "2504.14720", "pdf": "https://arxiv.org/pdf/2504.14720", "abs": "https://arxiv.org/abs/2504.14720", "authors": ["Tamir Berger", "Jonathan Sterenson", "Raz Birman", "Ofer Hadar"], "title": "Video QoE Metrics from Encrypted Traffic: Application-agnostic Methodology", "categories": ["cs.NI", "cs.LG", "cs.MM", "C.2.1; C.2.3; I.2.6"], "comment": "8 pages, 7 figures", "summary": "Instant Messaging-Based Video Call Applications (IMVCAs) and Video\nConferencing Applications (VCAs) have become integral to modern communication.\nEnsuring a high Quality of Experience (QoE) for users in this context is\ncritical for network operators, as network conditions significantly impact user\nQoE. However, network operators lack access to end-device QoE metrics due to\nencrypted traffic. Existing solutions estimate QoE metrics from encrypted\ntraffic traversing the network, with the most advanced approaches leveraging\nmachine learning models. Subsequently, the need for ground truth QoE metrics\nfor training and validation poses a challenge, as not all video applications\nprovide these metrics. To address this challenge, we propose an\napplication-agnostic approach for objective QoE estimation from encrypted\ntraffic. Independent of the video application, we obtained key video QoE\nmetrics, enabling broad applicability to various proprietary IMVCAs and VCAs.\nTo validate our solution, we created a diverse dataset from WhatsApp video\nsessions under various network conditions, comprising 25,680 seconds of traffic\ndata and QoE metrics. Our evaluation shows high performance across the entire\ndataset, with 85.2% accuracy for FPS predictions within an error margin of two\nFPS, and 90.2% accuracy for PIQE-based quality rating classification.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u72ec\u7acb\u4e8e\u5e94\u7528\u7a0b\u5e8f\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a0\u5bc6\u6d41\u91cf\u4f30\u8ba1\u89c6\u9891\u901a\u8bdd\u7684QoE\u6307\u6807\uff0c\u89e3\u51b3\u4e86\u7f51\u7edc\u8fd0\u8425\u5546\u56e0\u52a0\u5bc6\u6d41\u91cf\u65e0\u6cd5\u83b7\u53d6\u7ec8\u7aef\u8bbe\u5907QoE\u7684\u95ee\u9898\u3002", "motivation": "\u7f51\u7edc\u8fd0\u8425\u5546\u56e0\u52a0\u5bc6\u6d41\u91cf\u65e0\u6cd5\u76f4\u63a5\u83b7\u53d6\u7ec8\u7aef\u8bbe\u5907\u7684QoE\u6307\u6807\uff0c\u800c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u7279\u5b9a\u5e94\u7528\u7a0b\u5e8f\u63d0\u4f9b\u7684QoE\u6570\u636e\uff0c\u9650\u5236\u4e86\u5e7f\u6cdb\u9002\u7528\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u72ec\u7acb\u4e8e\u5e94\u7528\u7a0b\u5e8f\u7684QoE\u4f30\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a0\u5bc6\u6d41\u91cf\u83b7\u53d6\u5173\u952e\u89c6\u9891QoE\u6307\u6807\uff0c\u5e76\u5229\u7528\u673a\u5668\u5b66\u4e60\u6a21\u578b\u8fdb\u884c\u9884\u6d4b\u3002", "result": "\u5728\u591a\u6837\u5316\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0cFPS\u9884\u6d4b\u51c6\u786e\u7387\u8fbe85.2%\uff08\u8bef\u5dee\u00b12 FPS\uff09\uff0cPIQE\u8d28\u91cf\u5206\u7c7b\u51c6\u786e\u7387\u8fbe90.2%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\uff0c\u53ef\u5e94\u7528\u4e8e\u591a\u79cd\u4e13\u6709\u89c6\u9891\u901a\u8bdd\u5e94\u7528\uff0c\u4e3a\u7f51\u7edc\u8fd0\u8425\u5546\u63d0\u4f9b\u4e86\u6709\u6548\u7684QoE\u8bc4\u4f30\u5de5\u5177\u3002"}}
{"id": "2504.14618", "pdf": "https://arxiv.org/pdf/2504.14618", "abs": "https://arxiv.org/abs/2504.14618", "authors": ["Han Bi", "Ge Yu", "Yu He", "Wenzhuo Liu", "Zijie Zheng"], "title": "VM-BHINet:Vision Mamba Bimanual Hand Interaction Network for 3D Interacting Hand Mesh Recovery From a Single RGB Image", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Understanding bimanual hand interactions is essential for realistic 3D pose\nand shape reconstruction. However, existing methods struggle with occlusions,\nambiguous appearances, and computational inefficiencies. To address these\nchallenges, we propose Vision Mamba Bimanual Hand Interaction Network\n(VM-BHINet), introducing state space models (SSMs) into hand reconstruction to\nenhance interaction modeling while improving computational efficiency. The core\ncomponent, Vision Mamba Interaction Feature Extraction Block (VM-IFEBlock),\ncombines SSMs with local and global feature operations, enabling deep\nunderstanding of hand interactions. Experiments on the InterHand2.6M dataset\nshow that VM-BHINet reduces Mean per-joint position error (MPJPE) and Mean\nper-vertex position error (MPVPE) by 2-3%, significantly surpassing\nstate-of-the-art methods.", "AI": {"tldr": "VM-BHINet\u5229\u7528\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u6539\u8fdb\u53cc\u624b\u4ea4\u4e92\u91cd\u5efa\uff0c\u663e\u8457\u964d\u4f4e\u8bef\u5dee\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u906e\u6321\u3001\u6a21\u7cca\u5916\u89c2\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u5b58\u5728\u95ee\u9898\u3002", "method": "\u63d0\u51faVM-BHINet\uff0c\u7ed3\u5408\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u548c\u5c40\u90e8\u5168\u5c40\u7279\u5f81\u64cd\u4f5c\u3002", "result": "\u5728InterHand2.6M\u6570\u636e\u96c6\u4e0a\uff0cMPJPE\u548cMPVPE\u964d\u4f4e2-3%\u3002", "conclusion": "VM-BHINet\u5728\u53cc\u624b\u4ea4\u4e92\u91cd\u5efa\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2504.14744", "pdf": "https://arxiv.org/pdf/2504.14744", "abs": "https://arxiv.org/abs/2504.14744", "authors": ["Yigitcan Yard\u0131mc\u0131", "Mustafa Cavus"], "title": "On the Tunability of Random Survival Forests Model for Predictive Maintenance", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "This paper investigates the tunability of the Random Survival Forest (RSF)\nmodel in predictive maintenance, where accurate time-to-failure estimation is\ncrucial. Although RSF is widely used due to its flexibility and ability to\nhandle censored data, its performance is sensitive to hyperparameter\nconfigurations. However, systematic evaluations of RSF tunability remain\nlimited, especially in predictive maintenance contexts. We introduce a\nthree-level framework to quantify tunability: (1) a model-level metric\nmeasuring overall performance gain from tuning, (2) a hyperparameter-level\nmetric assessing individual contributions, and (3) identification of optimal\ntuning ranges. These metrics are evaluated across multiple datasets using\nsurvival-specific criteria: the C-index for discrimination and the Brier score\nfor calibration. Experiments on four CMAPSS dataset subsets, simulating\naircraft engine degradation, reveal that hyperparameter tuning consistently\nimproves model performance. On average, the C-index increased by 0.0547, while\nthe Brier score decreased by 0.0199. These gains were consistent across all\nsubsets. Moreover, ntree and mtry showed the highest average tunability, while\nnodesize offered stable improvements within the range of 10 to 30. In contrast,\nsplitrule demonstrated negative tunability on average, indicating that improper\ntuning may reduce model performance. Our findings emphasize the practical\nimportance of hyperparameter tuning in survival models and provide actionable\ninsights for optimizing RSF in real-world predictive maintenance applications.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u968f\u673a\u751f\u5b58\u68ee\u6797\uff08RSF\uff09\u6a21\u578b\u5728\u9884\u6d4b\u6027\u7ef4\u62a4\u4e2d\u7684\u53ef\u8c03\u6027\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e09\u5c42\u6b21\u6846\u67b6\u91cf\u5316\u8c03\u4f18\u6548\u679c\uff0c\u5e76\u9a8c\u8bc1\u4e86\u8d85\u53c2\u6570\u8c03\u4f18\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u5c3d\u7ba1RSF\u56e0\u5176\u7075\u6d3b\u6027\u548c\u5904\u7406\u622a\u5c3e\u6570\u636e\u7684\u80fd\u529b\u88ab\u5e7f\u6cdb\u4f7f\u7528\uff0c\u4f46\u5176\u6027\u80fd\u5bf9\u8d85\u53c2\u6570\u914d\u7f6e\u654f\u611f\uff0c\u4e14\u7cfb\u7edf\u6027\u7684\u8c03\u4f18\u8bc4\u4f30\u5728\u9884\u6d4b\u6027\u7ef4\u62a4\u9886\u57df\u4ecd\u6709\u9650\u3002", "method": "\u5f15\u5165\u4e09\u5c42\u6b21\u6846\u67b6\uff1a\u6a21\u578b\u7ea7\u6307\u6807\uff08\u6574\u4f53\u6027\u80fd\u589e\u76ca\uff09\u3001\u8d85\u53c2\u6570\u7ea7\u6307\u6807\uff08\u4e2a\u4f53\u8d21\u732e\uff09\u548c\u6700\u4f18\u8c03\u4f18\u8303\u56f4\u8bc6\u522b\uff0c\u4f7f\u7528C\u6307\u6570\u548cBrier\u5206\u6570\u8bc4\u4f30\u3002", "result": "\u5728\u56db\u4e2aCMAPSS\u6570\u636e\u96c6\u5b50\u96c6\u4e0a\uff0c\u8d85\u53c2\u6570\u8c03\u4f18\u663e\u8457\u63d0\u5347\u6027\u80fd\uff08C\u6307\u6570\u5e73\u5747\u589e\u52a00.0547\uff0cBrier\u5206\u6570\u964d\u4f4e0.0199\uff09\uff0cntree\u548cmtry\u8c03\u4f18\u6548\u679c\u6700\u4f73\u3002", "conclusion": "\u8d85\u53c2\u6570\u8c03\u4f18\u5bf9\u751f\u5b58\u6a21\u578b\u81f3\u5173\u91cd\u8981\uff0c\u7814\u7a76\u4e3a\u5b9e\u9645\u9884\u6d4b\u6027\u7ef4\u62a4\u5e94\u7528\u4e2d\u7684RSF\u4f18\u5316\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\u3002"}}
{"id": "2504.14625", "pdf": "https://arxiv.org/pdf/2504.14625", "abs": "https://arxiv.org/abs/2504.14625", "authors": ["Haiyan Qin", "Jiahao Feng", "Xiaotong Feng", "Wei W. Xing", "Wang Kang"], "title": "Towards Optimal Circuit Generation: Multi-Agent Collaboration Meets Collective Intelligence", "categories": ["cs.AR", "cs.AI"], "comment": "9 pages, 6 figures", "summary": "Large language models (LLMs) have transformed code generation, yet their\napplication in hardware design produces gate counts 38\\%--1075\\% higher than\nhuman designs. We present CircuitMind, a multi-agent framework that achieves\nhuman-competitive efficiency through three key innovations: syntax locking\n(constraining generation to basic logic gates), retrieval-augmented generation\n(enabling knowledge-driven design), and dual-reward optimization (balancing\ncorrectness with efficiency). To evaluate our approach, we introduce TC-Bench,\nthe first gate-level benchmark harnessing collective intelligence from the\nTuringComplete ecosystem -- a competitive circuit design platform with hundreds\nof thousands of players. Experiments show CircuitMind enables 55.6\\% of model\nimplementations to match or exceed top-tier human experts in composite\nefficiency metrics. Most remarkably, our framework elevates the 14B Phi-4 model\nto outperform both GPT-4o mini and Gemini 2.0 Flash, achieving efficiency\ncomparable to the top 25\\% of human experts without requiring specialized\ntraining. These innovations establish a new paradigm for hardware optimization\nwhere collaborative AI systems leverage collective human expertise to achieve\noptimal circuit designs. Our model, data, and code are open-source at\nhttps://github.com/BUAA-CLab/CircuitMind.", "AI": {"tldr": "CircuitMind\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u8bed\u6cd5\u9501\u5b9a\u3001\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u548c\u53cc\u5956\u52b1\u4f18\u5316\uff0c\u5b9e\u73b0\u4e86\u4e0e\u4eba\u7c7b\u4e13\u5bb6\u7ade\u4e89\u7684\u786c\u4ef6\u8bbe\u8ba1\u6548\u7387\u3002", "motivation": "\u89e3\u51b3LLM\u5728\u786c\u4ef6\u8bbe\u8ba1\u4e2d\u751f\u6210\u95e8\u6570\u8fc7\u9ad8\u7684\u95ee\u9898\uff0c\u63d0\u5347\u6548\u7387\u3002", "method": "\u91c7\u7528\u8bed\u6cd5\u9501\u5b9a\u3001\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u548c\u53cc\u5956\u52b1\u4f18\u5316\u6280\u672f\u3002", "result": "55.6%\u7684\u6a21\u578b\u5b9e\u73b0\u8fbe\u5230\u6216\u8d85\u8fc7\u4eba\u7c7b\u4e13\u5bb6\u6548\u7387\uff0c14B Phi-4\u6a21\u578b\u8868\u73b0\u4f18\u4e8eGPT-4o mini\u548cGemini 2.0 Flash\u3002", "conclusion": "CircuitMind\u4e3a\u786c\u4ef6\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\uff0c\u5229\u7528\u96c6\u4f53\u4eba\u7c7b\u4e13\u4e1a\u77e5\u8bc6\u5b9e\u73b0\u6700\u4f18\u7535\u8def\u8bbe\u8ba1\u3002"}}
{"id": "2504.14795", "pdf": "https://arxiv.org/pdf/2504.14795", "abs": "https://arxiv.org/abs/2504.14795", "authors": ["Ryu Tadokoro", "Tsukasa Takagi", "Shin-ichi Maeda"], "title": "Segmentation with Noisy Labels via Spatially Correlated Distributions", "categories": ["eess.IV", "cs.CV", "cs.LG", "stat.ML"], "comment": null, "summary": "In semantic segmentation, the accuracy of models heavily depends on the\nhigh-quality annotations. However, in many practical scenarios such as medical\nimaging and remote sensing, obtaining true annotations is not straightforward\nand usually requires significant human labor. Relying on human labor often\nintroduces annotation errors, including mislabeling, omissions, and\ninconsistency between annotators. In the case of remote sensing, differences in\nprocurement time can lead to misaligned ground truth annotations. These label\nerrors are not independently distributed, and instead usually appear in\nspatially connected regions where adjacent pixels are more likely to share the\nsame errors. To address these issues, we propose an approximate Bayesian\nestimation based on a probabilistic model that assumes training data includes\nlabel errors, incorporating the tendency for these errors to occur with spatial\ncorrelations between adjacent pixels. Bayesian inference requires computing the\nposterior distribution of label errors, which becomes intractable when spatial\ncorrelations are present. We represent the correlation of label errors between\nadjacent pixels through a Gaussian distribution whose covariance is structured\nby a Kac-Murdock-Szeg\\\"{o} (KMS) matrix, solving the computational challenges.\nThrough experiments on multiple segmentation tasks, we confirm that leveraging\nthe spatial correlation of label errors significantly improves performance.\nNotably, in specific tasks such as lung segmentation, the proposed method\nachieves performance comparable to training with clean labels under moderate\nnoise levels. Code is available at\nhttps://github.com/pfnet-research/Bayesian_SpatialCorr.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8fd1\u4f3c\u8d1d\u53f6\u65af\u4f30\u8ba1\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5904\u7406\u8bed\u4e49\u5206\u5272\u4e2d\u56e0\u6807\u6ce8\u9519\u8bef\uff08\u5982\u7a7a\u95f4\u76f8\u5173\u6027\u9519\u8bef\uff09\u5bfc\u81f4\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u9ad8\u65af\u5206\u5e03\u548cKMS\u77e9\u9635\u5efa\u6a21\u7a7a\u95f4\u76f8\u5173\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u5728\u8bed\u4e49\u5206\u5272\u4e2d\uff0c\u9ad8\u8d28\u91cf\u6807\u6ce8\u5bf9\u6a21\u578b\u51c6\u786e\u6027\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5b9e\u9645\u573a\u666f\uff08\u5982\u533b\u5b66\u5f71\u50cf\u548c\u9065\u611f\uff09\u4e2d\u83b7\u53d6\u771f\u5b9e\u6807\u6ce8\u56f0\u96be\u4e14\u6613\u51fa\u9519\uff0c\u5c24\u5176\u662f\u7a7a\u95f4\u76f8\u5173\u7684\u6807\u6ce8\u9519\u8bef\u3002", "method": "\u91c7\u7528\u8fd1\u4f3c\u8d1d\u53f6\u65af\u4f30\u8ba1\uff0c\u5047\u8bbe\u8bad\u7ec3\u6570\u636e\u5305\u542b\u6807\u6ce8\u9519\u8bef\uff0c\u5e76\u901a\u8fc7\u9ad8\u65af\u5206\u5e03\u548cKMS\u77e9\u9635\u5efa\u6a21\u76f8\u90bb\u50cf\u7d20\u95f4\u7684\u7a7a\u95f4\u76f8\u5173\u6027\uff0c\u89e3\u51b3\u8ba1\u7b97\u96be\u9898\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5229\u7528\u6807\u6ce8\u9519\u8bef\u7684\u7a7a\u95f4\u76f8\u5173\u6027\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u5728\u7279\u5b9a\u4efb\u52a1\uff08\u5982\u80ba\u90e8\u5206\u5272\uff09\u4e2d\uff0c\u6027\u80fd\u63a5\u8fd1\u4f7f\u7528\u5e72\u51c0\u6807\u6ce8\u7684\u8bad\u7ec3\u6548\u679c\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u6807\u6ce8\u9519\u8bef\u7684\u7a7a\u95f4\u76f8\u5173\u6027\u95ee\u9898\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u8bed\u4e49\u5206\u5272\u4efb\u52a1\u63d0\u4f9b\u4e86\u53ef\u9760\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.14807", "pdf": "https://arxiv.org/pdf/2504.14807", "abs": "https://arxiv.org/abs/2504.14807", "authors": ["Deepak Ghimire", "Sunghwan Jeong", "Sunhong Yoon", "Sanghyun Park", "Juhwan Choi"], "title": "Real-Time Sleepiness Detection for Driver State Monitoring System", "categories": ["cs.CV", "cs.HC", "cs.LG"], "comment": "8 pages, published in GST 2015", "summary": "A driver face monitoring system can detect driver fatigue, which is a\nsignificant factor in many accidents, using computer vision techniques. In this\npaper, we present a real-time technique for driver eye state detection. First,\nthe face is detected, and the eyes are located within the face region for\ntracking. A normalized cross-correlation-based online dynamic template matching\ntechnique, combined with Kalman filter tracking, is proposed to track the\ndetected eye positions in subsequent image frames. A support vector machine\nwith histogram of oriented gradients (HOG) features is used to classify the\nstate of the eyes as open or closed. If the eyes remain closed for a specified\nperiod, the driver is considered to be asleep, and an alarm is triggered.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8ba1\u7b97\u673a\u89c6\u89c9\u7684\u5b9e\u65f6\u9a7e\u9a76\u5458\u773c\u775b\u72b6\u6001\u68c0\u6d4b\u65b9\u6cd5\uff0c\u7ed3\u5408\u52a8\u6001\u6a21\u677f\u5339\u914d\u548cKalman\u6ee4\u6ce2\u8ddf\u8e2a\uff0c\u4f7f\u7528SVM\u5206\u7c7b\u5668\u5224\u65ad\u773c\u775b\u5f00\u95ed\u72b6\u6001\uff0c\u5e76\u5728\u68c0\u6d4b\u5230\u75b2\u52b3\u65f6\u89e6\u53d1\u8b66\u62a5\u3002", "motivation": "\u9a7e\u9a76\u5458\u75b2\u52b3\u662f\u5bfc\u81f4\u8bb8\u591a\u4e8b\u6545\u7684\u91cd\u8981\u56e0\u7d20\uff0c\u5b9e\u65f6\u76d1\u6d4b\u9a7e\u9a76\u5458\u773c\u775b\u72b6\u6001\u53ef\u4ee5\u6709\u6548\u9884\u9632\u75b2\u52b3\u9a7e\u9a76\u3002", "method": "1. \u68c0\u6d4b\u4eba\u8138\u5e76\u5b9a\u4f4d\u773c\u775b\u533a\u57df\uff1b2. \u4f7f\u7528\u52a8\u6001\u6a21\u677f\u5339\u914d\u548cKalman\u6ee4\u6ce2\u8ddf\u8e2a\u773c\u775b\u4f4d\u7f6e\uff1b3. \u7ed3\u5408HOG\u7279\u5f81\u548cSVM\u5206\u7c7b\u5668\u5224\u65ad\u773c\u775b\u5f00\u95ed\u72b6\u6001\u3002", "result": "\u7cfb\u7edf\u80fd\u591f\u5b9e\u65f6\u68c0\u6d4b\u9a7e\u9a76\u5458\u773c\u775b\u72b6\u6001\uff0c\u5e76\u5728\u68c0\u6d4b\u5230\u75b2\u52b3\u65f6\u89e6\u53d1\u8b66\u62a5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u5b9e\u73b0\u4e86\u9a7e\u9a76\u5458\u75b2\u52b3\u76d1\u6d4b\uff0c\u6709\u52a9\u4e8e\u51cf\u5c11\u75b2\u52b3\u9a7e\u9a76\u5f15\u53d1\u7684\u4e8b\u6545\u3002"}}
{"id": "2504.14681", "pdf": "https://arxiv.org/pdf/2504.14681", "abs": "https://arxiv.org/abs/2504.14681", "authors": ["Zeyu Wang", "Frank P. -W. Lo", "Qian Chen", "Yongqi Zhang", "Chen Lin", "Xu Chen", "Zhenhua Yu", "Alexander J. Thompson", "Eric M. Yeatman", "Benny P. L. Lo"], "title": "An LLM-enabled Multi-Agent Autonomous Mechatronics Design Framework", "categories": ["cs.RO", "cs.AI"], "comment": "Accepted by CVPR 2025 Workshop", "summary": "Existing LLM-enabled multi-agent frameworks are predominantly limited to\ndigital or simulated environments and confined to narrowly focused knowledge\ndomain, constraining their applicability to complex engineering tasks that\nrequire the design of physical embodiment, cross-disciplinary integration, and\nconstraint-aware reasoning. This work proposes a multi-agent autonomous\nmechatronics design framework, integrating expertise across mechanical design,\noptimization, electronics, and software engineering to autonomously generate\nfunctional prototypes with minimal direct human design input. Operating\nprimarily through a language-driven workflow, the framework incorporates\nstructured human feedback to ensure robust performance under real-world\nconstraints. To validate its capabilities, the framework is applied to a\nreal-world challenge involving autonomous water-quality monitoring and\nsampling, where traditional methods are labor-intensive and ecologically\ndisruptive. Leveraging the proposed system, a fully functional autonomous\nvessel was developed with optimized propulsion, cost-effective electronics, and\nadvanced control. The design process was carried out by specialized agents,\nincluding a high-level planning agent responsible for problem abstraction and\ndedicated agents for structural, electronics, control, and software\ndevelopment. This approach demonstrates the potential of LLM-based multi-agent\nsystems to automate real-world engineering workflows and reduce reliance on\nextensive domain expertise.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u81ea\u4e3b\u4ee3\u7406\u673a\u7535\u8bbe\u8ba1\u6846\u67b6\uff0c\u6574\u5408\u8de8\u5b66\u79d1\u77e5\u8bc6\uff0c\u901a\u8fc7\u8bed\u8a00\u9a71\u52a8\u6d41\u7a0b\u548c\u7ed3\u6784\u5316\u53cd\u9988\uff0c\u5b9e\u73b0\u529f\u80fd\u539f\u578b\u81ea\u4e3b\u751f\u6210\uff0c\u5e94\u7528\u4e8e\u6c34\u8d28\u76d1\u6d4b\u7b49\u590d\u6742\u5de5\u7a0b\u4efb\u52a1\u3002", "motivation": "\u73b0\u6709LLM\u591a\u4ee3\u7406\u6846\u67b6\u5c40\u9650\u4e8e\u6570\u5b57\u6216\u6a21\u62df\u73af\u5883\uff0c\u77e5\u8bc6\u9886\u57df\u72ed\u7a84\uff0c\u96be\u4ee5\u6ee1\u8db3\u590d\u6742\u5de5\u7a0b\u4efb\u52a1\u9700\u6c42\uff0c\u5982\u7269\u7406\u8bbe\u8ba1\u3001\u8de8\u5b66\u79d1\u6574\u5408\u548c\u7ea6\u675f\u611f\u77e5\u63a8\u7406\u3002", "method": "\u6574\u5408\u673a\u68b0\u8bbe\u8ba1\u3001\u4f18\u5316\u3001\u7535\u5b50\u548c\u8f6f\u4ef6\u5de5\u7a0b\uff0c\u901a\u8fc7\u8bed\u8a00\u9a71\u52a8\u6d41\u7a0b\u548c\u7ed3\u6784\u5316\u53cd\u9988\uff0c\u7531\u4e13\u4e1a\u4ee3\u7406\uff08\u89c4\u5212\u3001\u7ed3\u6784\u3001\u7535\u5b50\u3001\u63a7\u5236\u548c\u8f6f\u4ef6\u5f00\u53d1\uff09\u534f\u4f5c\u5b8c\u6210\u8bbe\u8ba1\u3002", "result": "\u6210\u529f\u5f00\u53d1\u4e86\u529f\u80fd\u5b8c\u6574\u7684\u81ea\u4e3b\u6c34\u8d28\u76d1\u6d4b\u8239\uff0c\u4f18\u5316\u63a8\u8fdb\u7cfb\u7edf\u3001\u4f4e\u6210\u672c\u7535\u5b50\u548c\u5148\u8fdb\u63a7\u5236\uff0c\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u5b9e\u9645\u5e94\u7528\u80fd\u529b\u3002", "conclusion": "\u5c55\u793a\u4e86LLM\u591a\u4ee3\u7406\u7cfb\u7edf\u5728\u81ea\u52a8\u5316\u5de5\u7a0b\u6d41\u7a0b\u548c\u51cf\u5c11\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\u4f9d\u8d56\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2504.14875", "pdf": "https://arxiv.org/pdf/2504.14875", "abs": "https://arxiv.org/abs/2504.14875", "authors": ["Chris Dongjoo Kim", "Jihwan Moon", "Sangwoo Moon", "Heeseung Yun", "Sihaeng Lee", "Aniruddha Kembhavi", "Soonyoung Lee", "Gunhee Kim", "Sangho Lee", "Christopher Clark"], "title": "ReSpec: Relevance and Specificity Grounded Online Filtering for Learning on Video-Text Data Streams", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "CVPR 2025 (main conference)", "summary": "The rapid growth of video-text data presents challenges in storage and\ncomputation during training. Online learning, which processes streaming data in\nreal-time, offers a promising solution to these issues while also allowing\nswift adaptations in scenarios demanding real-time responsiveness. One strategy\nto enhance the efficiency and effectiveness of learning involves identifying\nand prioritizing data that enhances performance on target downstream tasks. We\npropose Relevance and Specificity-based online filtering framework (ReSpec)\nthat selects data based on four criteria: (i) modality alignment for clean\ndata, (ii) task relevance for target focused data, (iii) specificity for\ninformative and detailed data, and (iv) efficiency for low-latency processing.\nRelevance is determined by the probabilistic alignment of incoming data with\ndownstream tasks, while specificity employs the distance to a root embedding\nrepresenting the least specific data as an efficient proxy for informativeness.\nBy establishing reference points from target task data, ReSpec filters incoming\ndata in real-time, eliminating the need for extensive storage and compute.\nEvaluating on large-scale datasets WebVid2M and VideoCC3M, ReSpec attains\nstate-of-the-art performance on five zeroshot video retrieval tasks, using as\nlittle as 5% of the data while incurring minimal compute. The source code is\navailable at https://github.com/cdjkim/ReSpec.", "AI": {"tldr": "ReSpec\u6846\u67b6\u901a\u8fc7\u5b9e\u65f6\u8fc7\u6ee4\u89c6\u9891\u6587\u672c\u6570\u636e\uff0c\u63d0\u5347\u5728\u7ebf\u5b66\u4e60\u6548\u7387\uff0c\u4ec5\u97005%\u6570\u636e\u5373\u53ef\u5728\u96f6\u6837\u672c\u89c6\u9891\u68c0\u7d22\u4efb\u52a1\u4e2d\u8fbe\u5230\u6700\u4f18\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u89c6\u9891\u6587\u672c\u6570\u636e\u5feb\u901f\u589e\u957f\u5e26\u6765\u7684\u5b58\u50a8\u548c\u8ba1\u7b97\u6311\u6218\uff0c\u540c\u65f6\u6ee1\u8db3\u5b9e\u65f6\u54cd\u5e94\u9700\u6c42\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u76f8\u5173\u6027\u548c\u7279\u5f02\u6027\u7684\u5728\u7ebf\u8fc7\u6ee4\u6846\u67b6\uff08ReSpec\uff09\uff0c\u901a\u8fc7\u56db\u79cd\u6807\u51c6\u9009\u62e9\u6570\u636e\uff1a\u6a21\u6001\u5bf9\u9f50\u3001\u4efb\u52a1\u76f8\u5173\u6027\u3001\u7279\u5f02\u6027\u548c\u6548\u7387\u3002", "result": "\u5728WebVid2M\u548cVideoCC3M\u6570\u636e\u96c6\u4e0a\uff0cReSpec\u5728\u4e94\u4e2a\u96f6\u6837\u672c\u89c6\u9891\u68c0\u7d22\u4efb\u52a1\u4e2d\u8868\u73b0\u6700\u4f18\uff0c\u4ec5\u97005%\u6570\u636e\u4e14\u8ba1\u7b97\u5f00\u9500\u4f4e\u3002", "conclusion": "ReSpec\u4e3a\u5728\u7ebf\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u5b9e\u65f6\u7684\u6570\u636e\u9009\u62e9\u65b9\u6cd5\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u5b58\u50a8\u548c\u8ba1\u7b97\u9700\u6c42\u3002"}}
{"id": "2504.14898", "pdf": "https://arxiv.org/pdf/2504.14898", "abs": "https://arxiv.org/abs/2504.14898", "authors": ["Bert de Vries", "Wouter Nuijten", "Thijs van de Laar", "Wouter Kouw", "Sepideh Adamiat", "Tim Nisslbeck", "Mykola Lukashchuk", "Hoang Minh Huu Nguyen", "Marco Hidalgo Araya", "Raphael Tresor", "Thijs Jenneskens", "Ivana Nikoloska", "Raaja Subramanian", "Bart van Erp", "Dmitry Bagaev", "Albert Podusenko"], "title": "Expected Free Energy-based Planning as Variational Inference", "categories": ["stat.ML", "cs.LG"], "comment": "16 pages", "summary": "We address the problem of planning under uncertainty, where an agent must\nchoose actions that not only achieve desired outcomes but also reduce\nuncertainty. Traditional methods often treat exploration and exploitation as\nseparate objectives, lacking a unified inferential foundation. Active\ninference, grounded in the Free Energy Principle, offers such a foundation by\nminimizing Expected Free Energy (EFE), a cost function that combines utility\nwith epistemic drives like ambiguity resolution and novelty seeking. However,\nthe computational burden of EFE minimization has remained a major obstacle to\nits scalability. In this paper, we show that EFE-based planning arises\nnaturally from minimizing a variational free energy functional on a generative\nmodel augmented with preference and epistemic priors. This result reinforces\ntheoretical consistency with the Free Energy Principle, by casting planning\nitself as variational inference. Our formulation yields optimal policies that\njointly support goal achievement and information gain, while incorporating a\ncomplexity term that accounts for bounded computational resources. This\nunifying framework connects and extends existing methods, enabling scalable,\nresource-aware implementations of active inference agents.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53d8\u5206\u81ea\u7531\u80fd\u6700\u5c0f\u5316\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u5c06\u89c4\u5212\u89c6\u4e3a\u53d8\u5206\u63a8\u65ad\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u4e0d\u786e\u5b9a\u6027\u4e0b\u89c4\u5212\u65f6\u63a2\u7d22\u4e0e\u5229\u7528\u5206\u79bb\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5728\u5904\u7406\u4e0d\u786e\u5b9a\u6027\u89c4\u5212\u65f6\uff0c\u63a2\u7d22\u4e0e\u5229\u7528\u76ee\u6807\u5206\u79bb\uff0c\u7f3a\u4e4f\u7edf\u4e00\u7684\u7406\u8bba\u57fa\u7840\u3002\u4e3b\u52a8\u63a8\u65ad\u867d\u63d0\u4f9b\u4e86\u4e00\u79cd\u57fa\u4e8e\u81ea\u7531\u80fd\u539f\u5219\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u5176\u8ba1\u7b97\u8d1f\u62c5\u9650\u5236\u4e86\u53ef\u6269\u5c55\u6027\u3002", "method": "\u901a\u8fc7\u6700\u5c0f\u5316\u751f\u6210\u6a21\u578b\u4e0a\u7684\u53d8\u5206\u81ea\u7531\u80fd\u6cdb\u51fd\uff0c\u7ed3\u5408\u504f\u597d\u548c\u8ba4\u77e5\u5148\u9a8c\uff0c\u63a8\u5bfc\u51fa\u57fa\u4e8eEFE\u7684\u89c4\u5212\u65b9\u6cd5\uff0c\u5c06\u89c4\u5212\u672c\u8eab\u89c6\u4e3a\u53d8\u5206\u63a8\u65ad\u3002", "result": "\u63d0\u51fa\u7684\u6846\u67b6\u80fd\u591f\u751f\u6210\u540c\u65f6\u652f\u6301\u76ee\u6807\u8fbe\u6210\u548c\u4fe1\u606f\u83b7\u53d6\u7684\u6700\u4f18\u7b56\u7565\uff0c\u5e76\u8003\u8651\u4e86\u8ba1\u7b97\u8d44\u6e90\u7684\u9650\u5236\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e0d\u4ec5\u7406\u8bba\u4e00\u81f4\uff0c\u8fd8\u5b9e\u73b0\u4e86\u53ef\u6269\u5c55\u3001\u8d44\u6e90\u611f\u77e5\u7684\u4e3b\u52a8\u63a8\u65ad\u4ee3\u7406\uff0c\u8fde\u63a5\u5e76\u6269\u5c55\u4e86\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2504.14938", "pdf": "https://arxiv.org/pdf/2504.14938", "abs": "https://arxiv.org/abs/2504.14938", "authors": ["Jiaxuan Jiang", "Jiapeng Liu", "Mi\u0142osz Kadzi\u0144ski", "Xiuwu Liao", "Jingyu Dong"], "title": "Integrating Response Time and Attention Duration in Bayesian Preference Learning for Multiple Criteria Decision Aiding", "categories": ["stat.AP", "cs.LG"], "comment": null, "summary": "We introduce a multiple criteria Bayesian preference learning framework\nincorporating behavioral cues for decision aiding. The framework integrates\npairwise comparisons, response time, and attention duration to deepen insights\ninto decision-making processes. The approach employs an additive value function\nmodel and utilizes a Bayesian framework to derive the posterior distribution of\npotential ranking models by defining the likelihood of observed preference data\nand specifying a prior on the preference structure. This distribution\nhighlights each model's ability to reconstruct Decision-Makers' holistic\npairwise comparisons. By leveraging both response time as a proxy for cognitive\neffort and alternative discriminability as well as attention duration as an\nindicator of criterion importance, the proposed model surpasses traditional\nmethods by uncovering richer behavioral patterns. We report the results of a\nlaboratory experiment on mobile phone contract selection involving 30 real\nsubjects using a dedicated application with time-, eye-, and mouse-tracking\ncomponents. We validate the novel method's ability to reconstruct complete\npreferences. The detailed ablation studies reveal time- and attention-related\nbehavioral patterns, confirming that integrating comprehensive data leads to\ndeveloping models that better align with the DM's actual preferences.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u884c\u4e3a\u7ebf\u7d22\u7684\u591a\u6807\u51c6\u8d1d\u53f6\u65af\u504f\u597d\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u6210\u5bf9\u6bd4\u8f83\u3001\u54cd\u5e94\u65f6\u95f4\u548c\u6ce8\u610f\u529b\u65f6\u957f\uff0c\u6df1\u5316\u5bf9\u51b3\u7b56\u8fc7\u7a0b\u7684\u7406\u89e3\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u65e0\u6cd5\u5145\u5206\u6355\u6349\u51b3\u7b56\u8005\u7684\u884c\u4e3a\u6a21\u5f0f\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u5168\u9762\u7684\u65b9\u6cd5\u6765\u63ed\u793a\u504f\u597d\u3002", "method": "\u91c7\u7528\u52a0\u6027\u4ef7\u503c\u51fd\u6570\u6a21\u578b\u548c\u8d1d\u53f6\u65af\u6846\u67b6\uff0c\u901a\u8fc7\u5b9a\u4e49\u504f\u597d\u6570\u636e\u7684\u4f3c\u7136\u548c\u5148\u9a8c\u7ed3\u6784\uff0c\u63a8\u5bfc\u6f5c\u5728\u6392\u5e8f\u6a21\u578b\u7684\u540e\u9a8c\u5206\u5e03\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b0\u65b9\u6cd5\u5728\u91cd\u5efa\u5b8c\u6574\u504f\u597d\u65b9\u9762\u7684\u80fd\u529b\uff0c\u5e76\u63ed\u793a\u4e86\u4e0e\u65f6\u95f4\u548c\u6ce8\u610f\u529b\u76f8\u5173\u7684\u884c\u4e3a\u6a21\u5f0f\u3002", "conclusion": "\u7ed3\u5408\u5168\u9762\u6570\u636e\u7684\u65b9\u6cd5\u80fd\u66f4\u597d\u5730\u4e0e\u51b3\u7b56\u8005\u7684\u5b9e\u9645\u504f\u597d\u5bf9\u9f50\u3002"}}
{"id": "2504.14693", "pdf": "https://arxiv.org/pdf/2504.14693", "abs": "https://arxiv.org/abs/2504.14693", "authors": ["Enxin Song", "Wenhao Chai", "Weili Xu", "Jianwen Xie", "Yuxuan Liu", "Gaoang Wang"], "title": "Video-MMLU: A Massive Multi-Discipline Lecture Understanding Benchmark", "categories": ["cs.CV", "cs.AI"], "comment": "Code, docs, and benchmark are all avaliable at\n  https://enxinsong.com/Video-MMLU-web/", "summary": "Recent advancements in language multimodal models (LMMs) for video have\ndemonstrated their potential for understanding video content, yet the task of\ncomprehending multi-discipline lectures remains largely unexplored. We\nintroduce Video-MMLU, a massive benchmark designed to evaluate the capabilities\nof LMMs in understanding Multi-Discipline Lectures. We evaluate over 90\nopen-source and proprietary models, ranging from 0.5B to 40B parameters. Our\nresults highlight the limitations of current models in addressing the cognitive\nchallenges presented by these lectures, especially in tasks requiring both\nperception and reasoning. Additionally, we explore how the number of visual\ntokens and the large language models influence performance, offering insights\ninto the interplay between multimodal perception and reasoning in lecture\ncomprehension.", "AI": {"tldr": "Video-MMLU\u662f\u4e00\u4e2a\u8bc4\u4f30\u591a\u5b66\u79d1\u8bb2\u5ea7\u7406\u89e3\u80fd\u529b\u7684\u5927\u89c4\u6a21\u57fa\u51c6\uff0c\u6d4b\u8bd5\u4e8690\u591a\u4e2a\u6a21\u578b\uff0c\u53d1\u73b0\u5f53\u524d\u6a21\u578b\u5728\u611f\u77e5\u4e0e\u63a8\u7406\u4efb\u52a1\u4e0a\u5b58\u5728\u5c40\u9650\u3002", "motivation": "\u63a2\u7d22\u8bed\u8a00\u591a\u6a21\u6001\u6a21\u578b\uff08LMMs\uff09\u5728\u591a\u5b66\u79d1\u8bb2\u5ea7\u7406\u89e3\u4e2d\u7684\u6f5c\u529b\uff0c\u586b\u8865\u8be5\u9886\u57df\u7684\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u5f15\u5165Video-MMLU\u57fa\u51c6\uff0c\u8bc4\u4f3090\u591a\u4e2a\u5f00\u6e90\u548c\u4e13\u6709\u6a21\u578b\uff0c\u5206\u6790\u89c6\u89c9\u6807\u8bb0\u6570\u91cf\u548c\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u5f53\u524d\u6a21\u578b\u5728\u591a\u5b66\u79d1\u8bb2\u5ea7\u7684\u611f\u77e5\u4e0e\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u8db3\uff0c\u89c6\u89c9\u6807\u8bb0\u6570\u91cf\u548c\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u6027\u80fd\u6709\u663e\u8457\u5f71\u54cd\u3002", "conclusion": "\u591a\u5b66\u79d1\u8bb2\u5ea7\u7406\u89e3\u9700\u8981\u66f4\u5f3a\u7684\u611f\u77e5\u4e0e\u63a8\u7406\u80fd\u529b\uff0c\u672a\u6765\u7814\u7a76\u9700\u4f18\u5316\u6a21\u578b\u8bbe\u8ba1\u3002"}}
{"id": "2504.14995", "pdf": "https://arxiv.org/pdf/2504.14995", "abs": "https://arxiv.org/abs/2504.14995", "authors": ["Keisuke Murota", "Takumi Kobori"], "title": "Trainable Quantum Neural Network for Multiclass Image Classification with the Power of Pre-trained Tree Tensor Networks", "categories": ["quant-ph", "cs.AI", "cs.LG"], "comment": "11 pages, 12 figures, 2 tables. This work has been submitted to the\n  IEEE for possible publication", "summary": "Tree tensor networks (TTNs) offer powerful models for image classification.\nWhile these TTN image classifiers already show excellent performance on\nclassical hardware, embedding them into quantum neural networks (QNNs) may\nfurther improve the performance by leveraging quantum resources. However,\nembedding TTN classifiers into QNNs for multiclass classification remains\nchallenging. Key obstacles are the highorder gate operations required for large\nbond dimensions and the mid-circuit postselection with exponentially low\nsuccess rates necessary for the exact embedding. In this work, to address these\nchallenges, we propose forest tensor network (FTN)-classifiers, which aggregate\nmultiple small-bond-dimension TTNs. This allows us to handle multiclass\nclassification without requiring large gates in the embedded circuits. We then\nremove the overhead of mid-circuit postselection by extending the adiabatic\nencoding framework to our setting and smoothly encode the FTN-classifiers into\na quantum forest tensor network (qFTN)- classifiers. Numerical experiments on\nMNIST and CIFAR-10 demonstrate that we can successfully train FTN-classifiers\nand encode them into qFTN-classifiers, while maintaining or even improving the\nperformance of the pre-trained FTN-classifiers. These results suggest that\nsynergy between TTN classification models and QNNs can provide a robust and\nscalable framework for multiclass quantum-enhanced image classification.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u68ee\u6797\u5f20\u91cf\u7f51\u7edc\uff08FTN\uff09\u5206\u7c7b\u5668\uff0c\u7528\u4e8e\u89e3\u51b3\u5c06\u6811\u5f20\u91cf\u7f51\u7edc\uff08TTN\uff09\u5d4c\u5165\u91cf\u5b50\u795e\u7ecf\u7f51\u7edc\uff08QNN\uff09\u65f6\u7684\u9ad8\u9636\u95e8\u64cd\u4f5c\u548c\u4e2d\u95f4\u7535\u8def\u540e\u9009\u62e9\u95ee\u9898\u3002\u901a\u8fc7\u6269\u5c55\u7edd\u70ed\u7f16\u7801\u6846\u67b6\uff0c\u6210\u529f\u5b9e\u73b0\u4e86FTN\u5230\u91cf\u5b50FTN\uff08qFTN\uff09\u7684\u8f6c\u6362\uff0c\u5e76\u5728MNIST\u548cCIFAR-10\u4e0a\u9a8c\u8bc1\u4e86\u6027\u80fd\u3002", "motivation": "\u5c06TTN\u5d4c\u5165QNN\u4ee5\u63d0\u5347\u591a\u7c7b\u56fe\u50cf\u5206\u7c7b\u6027\u80fd\uff0c\u4f46\u9762\u4e34\u9ad8\u9636\u95e8\u64cd\u4f5c\u548c\u4f4e\u6210\u529f\u7387\u7684\u4e2d\u95f4\u7535\u8def\u540e\u9009\u62e9\u95ee\u9898\u3002", "method": "\u63d0\u51faFTN\u5206\u7c7b\u5668\uff0c\u805a\u5408\u591a\u4e2a\u5c0f\u952e\u7ef4TTN\uff0c\u907f\u514d\u5927\u5c3a\u5bf8\u95e8\u64cd\u4f5c\uff1b\u6269\u5c55\u7edd\u70ed\u7f16\u7801\u6846\u67b6\uff0c\u6d88\u9664\u4e2d\u95f4\u7535\u8def\u540e\u9009\u62e9\u5f00\u9500\u3002", "result": "\u5728MNIST\u548cCIFAR-10\u4e0a\u6210\u529f\u8bad\u7ec3FTN\u5e76\u5d4c\u5165qFTN\uff0c\u6027\u80fd\u4fdd\u6301\u6216\u63d0\u5347\u3002", "conclusion": "TTN\u4e0eQNN\u7684\u534f\u540c\u4f5c\u7528\u4e3a\u591a\u7c7b\u91cf\u5b50\u589e\u5f3a\u56fe\u50cf\u5206\u7c7b\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u6846\u67b6\u3002"}}
{"id": "2504.14699", "pdf": "https://arxiv.org/pdf/2504.14699", "abs": "https://arxiv.org/abs/2504.14699", "authors": ["Sascha Jecklin", "Aidana Massalimova", "Ruyi Zha", "Lilian Calvet", "Christoph J. Laux", "Mazda Farshad", "Philipp F\u00fcrnstahl"], "title": "IXGS-Intraoperative 3D Reconstruction from Sparse, Arbitrarily Posed Real X-rays", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Spine surgery is a high-risk intervention demanding precise execution, often\nsupported by image-based navigation systems. Recently, supervised learning\napproaches have gained attention for reconstructing 3D spinal anatomy from\nsparse fluoroscopic data, significantly reducing reliance on\nradiation-intensive 3D imaging systems. However, these methods typically\nrequire large amounts of annotated training data and may struggle to generalize\nacross varying patient anatomies or imaging conditions. Instance-learning\napproaches like Gaussian splatting could offer an alternative by avoiding\nextensive annotation requirements. While Gaussian splatting has shown promise\nfor novel view synthesis, its application to sparse, arbitrarily posed real\nintraoperative X-rays has remained largely unexplored. This work addresses this\nlimitation by extending the $R^2$-Gaussian splatting framework to reconstruct\nanatomically consistent 3D volumes under these challenging conditions. We\nintroduce an anatomy-guided radiographic standardization step using style\ntransfer, improving visual consistency across views, and enhancing\nreconstruction quality. Notably, our framework requires no pretraining, making\nit inherently adaptable to new patients and anatomies. We evaluated our\napproach using an ex-vivo dataset. Expert surgical evaluation confirmed the\nclinical utility of the 3D reconstructions for navigation, especially when\nusing 20 to 30 views, and highlighted the standardization's benefit for\nanatomical clarity. Benchmarking via quantitative 2D metrics (PSNR/SSIM)\nconfirmed performance trade-offs compared to idealized settings, but also\nvalidated the improvement gained from standardization over raw inputs. This\nwork demonstrates the feasibility of instance-based volumetric reconstruction\nfrom arbitrary sparse-view X-rays, advancing intraoperative 3D imaging for\nsurgical navigation.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5b9e\u4f8b\u5b66\u4e60\u76843D\u810a\u67f1\u91cd\u5efa\u65b9\u6cd5\uff0c\u901a\u8fc7\u6269\u5c55R\u00b2-Gaussian splatting\u6846\u67b6\uff0c\u7ed3\u5408\u89e3\u5256\u5b66\u5f15\u5bfc\u7684\u653e\u5c04\u6807\u51c6\u5316\u6b65\u9aa4\uff0c\u5b9e\u73b0\u4e86\u4ece\u7a00\u758fX\u5c04\u7ebf\u6570\u636e\u4e2d\u91cd\u5efa\u89e3\u5256\u4e00\u81f4\u76843D\u4f53\u79ef\u3002", "motivation": "\u810a\u67f1\u624b\u672f\u9700\u8981\u9ad8\u7cbe\u5ea6\u76843D\u6210\u50cf\u652f\u6301\uff0c\u4f46\u73b0\u6709\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u4f9d\u8d56\u5927\u91cf\u6807\u6ce8\u6570\u636e\u4e14\u6cdb\u5316\u80fd\u529b\u6709\u9650\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u65e0\u9700\u9884\u8bad\u7ec3\u7684\u5b9e\u4f8b\u5b66\u4e60\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3\u7a00\u758fX\u5c04\u7ebf\u6570\u636e\u4e0b\u76843D\u91cd\u5efa\u95ee\u9898\u3002", "method": "\u6269\u5c55R\u00b2-Gaussian splatting\u6846\u67b6\uff0c\u5f15\u5165\u89e3\u5256\u5b66\u5f15\u5bfc\u7684\u653e\u5c04\u6807\u51c6\u5316\u6b65\u9aa4\uff08\u98ce\u683c\u8fc1\u79fb\uff09\uff0c\u63d0\u5347\u591a\u89c6\u89d2\u89c6\u89c9\u4e00\u81f4\u6027\u548c\u91cd\u5efa\u8d28\u91cf\u3002", "result": "\u5728\u4f53\u5916\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u4e13\u5bb6\u8bc4\u4f30\u786e\u8ba4\u5176\u4e34\u5e8a\u5b9e\u7528\u6027\uff0c20-30\u89c6\u89d2\u4e0b\u6548\u679c\u6700\u4f73\uff1b\u5b9a\u91cf\u6307\u6807\uff08PSNR/SSIM\uff09\u663e\u793a\u6807\u51c6\u5316\u6b65\u9aa4\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u57fa\u4e8e\u5b9e\u4f8b\u5b66\u4e60\u7684\u7a00\u758fX\u5c04\u7ebf3D\u91cd\u5efa\u53ef\u884c\u6027\uff0c\u4e3a\u624b\u672f\u5bfc\u822a\u4e2d\u76843D\u6210\u50cf\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2504.15021", "pdf": "https://arxiv.org/pdf/2504.15021", "abs": "https://arxiv.org/abs/2504.15021", "authors": ["Xinglei Dou", "Lei Liu", "Limin Xiao"], "title": "Is Intelligence the Right Direction in New OS Scheduling for Multiple Resources in Cloud Environments?", "categories": ["cs.DC", "cs.LG", "cs.PF"], "comment": "25 pages, 14 figures, to be published in ACM Transactions on Storage", "summary": "Making it intelligent is a promising way in System/OS design. This paper\nproposes OSML+, a new ML-based resource scheduling mechanism for co-located\ncloud services. OSML+ intelligently schedules the cache and main memory\nbandwidth resources at the memory hierarchy and the computing core resources\nsimultaneously. OSML+ uses a multi-model collaborative learning approach during\nits scheduling and thus can handle complicated cases, e.g., avoiding resource\ncliffs, sharing resources among applications, enabling different scheduling\npolicies for applications with different priorities, etc. OSML+ can converge\nfaster using ML models than previous studies. Moreover, OSML+ can automatically\nlearn on the fly and handle dynamically changing workloads accordingly. Using\ntransfer learning technologies, we show our design can work well across various\ncloud servers, including the latest off-the-shelf large-scale servers. Our\nexperimental results show that OSML+ supports higher loads and meets QoS\ntargets with lower overheads than previous studies.", "AI": {"tldr": "OSML+\u662f\u4e00\u79cd\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u8d44\u6e90\u8c03\u5ea6\u673a\u5236\uff0c\u7528\u4e8e\u534f\u540c\u5b9a\u4f4d\u4e91\u670d\u52a1\uff0c\u901a\u8fc7\u591a\u6a21\u578b\u534f\u4f5c\u5b66\u4e60\u4f18\u5316\u7f13\u5b58\u3001\u5185\u5b58\u5e26\u5bbd\u548c\u8ba1\u7b97\u6838\u5fc3\u8d44\u6e90\u8c03\u5ea6\u3002", "motivation": "\u63d0\u5347\u7cfb\u7edf/OS\u8bbe\u8ba1\u7684\u667a\u80fd\u5316\u6c34\u5e73\uff0c\u89e3\u51b3\u590d\u6742\u573a\u666f\u4e0b\u7684\u8d44\u6e90\u8c03\u5ea6\u95ee\u9898\uff0c\u5982\u907f\u514d\u8d44\u6e90\u60ac\u5d16\u3001\u5171\u4eab\u8d44\u6e90\u7b49\u3002", "method": "\u91c7\u7528\u591a\u6a21\u578b\u534f\u4f5c\u5b66\u4e60\u65b9\u6cd5\uff0c\u7ed3\u5408\u8fc1\u79fb\u5b66\u4e60\u6280\u672f\uff0c\u52a8\u6001\u9002\u5e94\u53d8\u5316\u7684\u5de5\u4f5c\u8d1f\u8f7d\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cOSML+\u80fd\u66f4\u5feb\u6536\u655b\uff0c\u652f\u6301\u66f4\u9ad8\u8d1f\u8f7d\uff0c\u4e14\u6ee1\u8db3QoS\u76ee\u6807\uff0c\u5f00\u9500\u66f4\u4f4e\u3002", "conclusion": "OSML+\u5728\u4e91\u670d\u52a1\u5668\u8d44\u6e90\u8c03\u5ea6\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\u548c\u9ad8\u6548\u6027\u3002"}}
{"id": "2504.14709", "pdf": "https://arxiv.org/pdf/2504.14709", "abs": "https://arxiv.org/abs/2504.14709", "authors": ["Hui Zhou", "Shaoshuai Shi", "Hongsheng Li"], "title": "Exposing the Copycat Problem of Imitation-based Planner: A Novel Closed-Loop Simulator, Causal Benchmark and Joint IL-RL Baseline", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Machine learning (ML)-based planners have recently gained significant\nattention. They offer advantages over traditional optimization-based planning\nalgorithms. These advantages include fewer manually selected parameters and\nfaster development. Within ML-based planning, imitation learning (IL) is a\ncommon algorithm. It primarily learns driving policies directly from supervised\ntrajectory data. While IL has demonstrated strong performance on many open-loop\nbenchmarks, it remains challenging to determine if the learned policy truly\nunderstands fundamental driving principles, rather than simply extrapolating\nfrom the ego-vehicle's initial state. Several studies have identified this\nlimitation and proposed algorithms to address it. However, these methods often\nuse original datasets for evaluation. In these datasets, future trajectories\nare heavily dependent on initial conditions. Furthermore, IL often overfits to\nthe most common scenarios. It struggles to generalize to rare or unseen\nsituations.\n  To address these challenges, this work proposes: 1) a novel closed-loop\nsimulator supporting both imitation and reinforcement learning, 2) a causal\nbenchmark derived from the Waymo Open Dataset to rigorously assess the impact\nof the copycat problem, and 3) a novel framework integrating imitation learning\nand reinforcement learning to overcome the limitations of purely imitative\napproaches. The code for this work will be released soon.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6a21\u4eff\u5b66\u4e60\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u65b0\u6846\u67b6\uff0c\u4ee5\u89e3\u51b3\u6a21\u4eff\u5b66\u4e60\u5728\u81ea\u52a8\u9a7e\u9a76\u89c4\u5212\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5305\u62ec\u8fc7\u62df\u5408\u548c\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u6a21\u4eff\u5b66\u4e60\u5728\u81ea\u52a8\u9a7e\u9a76\u89c4\u5212\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u96be\u4ee5\u9a8c\u8bc1\u5176\u662f\u5426\u771f\u6b63\u7406\u89e3\u9a7e\u9a76\u539f\u5219\uff0c\u4e14\u5bb9\u6613\u8fc7\u62df\u5408\u5e38\u89c1\u573a\u666f\uff0c\u6cdb\u5316\u80fd\u529b\u6709\u9650\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u95ed\u73af\u6a21\u62df\u5668\u3001\u56e0\u679c\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\uff0c\u4ee5\u53ca\u7ed3\u5408\u6a21\u4eff\u5b66\u4e60\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u65b0\u65b9\u6cd5\u3002", "result": "\u65b0\u6846\u67b6\u65e8\u5728\u514b\u670d\u6a21\u4eff\u5b66\u4e60\u7684\u5c40\u9650\u6027\uff0c\u63d0\u5347\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u6a21\u4eff\u5b66\u4e60\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u80fd\u591f\u66f4\u6709\u6548\u5730\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u89c4\u5212\u4e2d\u7684\u6311\u6218\u3002"}}
{"id": "2504.15100", "pdf": "https://arxiv.org/pdf/2504.15100", "abs": "https://arxiv.org/abs/2504.15100", "authors": ["Jiaxuan Miao", "Sergey Matveev"], "title": "Application of Sensitivity Analysis Methods for Studying Neural Network Models", "categories": ["math.NA", "cs.LG", "cs.NA", "68T07", "F.2.1; I.2.6; G.3; I.2.10"], "comment": "11 pages, 16 figures, 32 references", "summary": "This study demonstrates the capabilities of several methods for analyzing the\nsensitivity of neural networks to perturbations of the input data and\ninterpreting their underlying mechanisms. The investigated approaches include\nthe Sobol global sensitivity analysis, the local sensitivity method for input\npixel perturbations and the activation maximization technique. As examples, in\nthis study we consider a small feedforward neural network for analyzing an open\ntabular dataset of clinical diabetes data, as well as two classical\nconvolutional architectures, VGG-16 and ResNet-18, which are widely used in\nimage processing and classification. Utilization of the global sensitivity\nanalysis allows us to identify the leading input parameters of the chosen tiny\nneural network and reduce their number without significant loss of the\naccuracy. As far as global sensitivity analysis is not applicable to larger\nmodels we try the local sensitivity analysis and activation maximization method\nin application to the convolutional neural networks. These methods show\ninteresting patterns for the convolutional models solving the image\nclassification problem. All in all, we compare the results of the activation\nmaximization method with popular Grad-CAM technique in the context of\nultrasound data analysis.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u591a\u79cd\u65b9\u6cd5\u5206\u6790\u795e\u7ecf\u7f51\u7edc\u5bf9\u8f93\u5165\u6570\u636e\u6270\u52a8\u7684\u654f\u611f\u6027\u53ca\u5176\u673a\u5236\uff0c\u5305\u62ecSobol\u5168\u5c40\u654f\u611f\u6027\u5206\u6790\u3001\u5c40\u90e8\u654f\u611f\u6027\u65b9\u6cd5\u548c\u6fc0\u6d3b\u6700\u5927\u5316\u6280\u672f\uff0c\u5e76\u5728\u4e0d\u540c\u7f51\u7edc\u7ed3\u6784\u4e0a\u9a8c\u8bc1\u3002", "motivation": "\u63a2\u7d22\u795e\u7ecf\u7f51\u7edc\u5bf9\u8f93\u5165\u6270\u52a8\u7684\u654f\u611f\u6027\u53ca\u5176\u89e3\u91ca\u673a\u5236\uff0c\u4ee5\u4f18\u5316\u6a21\u578b\u6027\u80fd\u548c\u7406\u89e3\u5176\u884c\u4e3a\u3002", "method": "\u91c7\u7528Sobol\u5168\u5c40\u654f\u611f\u6027\u5206\u6790\u3001\u5c40\u90e8\u654f\u611f\u6027\u65b9\u6cd5\u548c\u6fc0\u6d3b\u6700\u5927\u5316\u6280\u672f\uff0c\u5206\u522b\u5e94\u7528\u4e8e\u5c0f\u578b\u524d\u9988\u7f51\u7edc\u548c\u7ecf\u5178\u5377\u79ef\u7f51\u7edc\uff08VGG-16\u3001ResNet-18\uff09\u3002", "result": "\u5168\u5c40\u654f\u611f\u6027\u5206\u6790\u6210\u529f\u8bc6\u522b\u5e76\u51cf\u5c11\u4e86\u5c0f\u578b\u7f51\u7edc\u7684\u5173\u952e\u8f93\u5165\u53c2\u6570\uff1b\u5c40\u90e8\u65b9\u6cd5\u548c\u6fc0\u6d3b\u6700\u5927\u5316\u63ed\u793a\u4e86\u5377\u79ef\u7f51\u7edc\u7684\u5206\u7c7b\u6a21\u5f0f\u3002", "conclusion": "\u4e0d\u540c\u65b9\u6cd5\u9002\u7528\u4e8e\u4e0d\u540c\u89c4\u6a21\u7684\u7f51\u7edc\uff0c\u6fc0\u6d3b\u6700\u5927\u5316\u4e0eGrad-CAM\u5728\u8d85\u58f0\u6570\u636e\u5206\u6790\u4e2d\u8868\u73b0\u53ef\u6bd4\u3002"}}
{"id": "2504.15129", "pdf": "https://arxiv.org/pdf/2504.15129", "abs": "https://arxiv.org/abs/2504.15129", "authors": ["Kangyao Huang", "Hao Wang", "Yu Luo", "Jingyu Chen", "Jintao Chen", "Xiangkui Zhang", "Xiangyang Ji", "Huaping Liu"], "title": "A General Infrastructure and Workflow for Quadrotor Deep Reinforcement Learning and Reality Deployment", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Deploying robot learning methods to a quadrotor in unstructured outdoor\nenvironments is an exciting task. Quadrotors operating in real-world\nenvironments by learning-based methods encounter several challenges: a large\namount of simulator generated data required for training, strict demands for\nreal-time processing onboard, and the sim-to-real gap caused by dynamic and\nnoisy conditions. Current works have made a great breakthrough in applying\nlearning-based methods to end-to-end control of quadrotors, but rarely mention\nthe infrastructure system training from scratch and deploying to reality, which\nmakes it difficult to reproduce methods and applications. To bridge this gap,\nwe propose a platform that enables the seamless transfer of end-to-end deep\nreinforcement learning (DRL) policies. We integrate the training environment,\nflight dynamics control, DRL algorithms, the MAVROS middleware stack, and\nhardware into a comprehensive workflow and architecture that enables\nquadrotors' policies to be trained from scratch to real-world deployment in\nseveral minutes. Our platform provides rich types of environments including\nhovering, dynamic obstacle avoidance, trajectory tracking, balloon hitting, and\nplanning in unknown environments, as a physical experiment benchmark. Through\nextensive empirical validation, we demonstrate the efficiency of proposed\nsim-to-real platform, and robust outdoor flight performance under real-world\nperturbations. Details can be found from our website\nhttps://emnavi.tech/AirGym/.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u5e73\u53f0\uff0c\u5b9e\u73b0\u7aef\u5230\u7aef\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u7684\u65e0\u7f1d\u8fc1\u79fb\uff0c\u89e3\u51b3\u56db\u65cb\u7ffc\u65e0\u4eba\u673a\u5728\u975e\u7ed3\u6784\u5316\u6237\u5916\u73af\u5883\u4e2d\u7684\u5b66\u4e60\u6311\u6218\u3002", "motivation": "\u89e3\u51b3\u56db\u65cb\u7ffc\u65e0\u4eba\u673a\u5728\u73b0\u5b9e\u73af\u5883\u4e2d\u5e94\u7528\u5b66\u4e60\u65b9\u6cd5\u7684\u6311\u6218\uff0c\u5982\u5927\u91cf\u6a21\u62df\u6570\u636e\u9700\u6c42\u3001\u5b9e\u65f6\u5904\u7406\u8981\u6c42\u548c\u6a21\u62df\u4e0e\u73b0\u5b9e\u7684\u5dee\u8ddd\u3002", "method": "\u6574\u5408\u8bad\u7ec3\u73af\u5883\u3001\u98de\u884c\u52a8\u529b\u5b66\u63a7\u5236\u3001DRL\u7b97\u6cd5\u3001MAVROS\u4e2d\u95f4\u4ef6\u548c\u786c\u4ef6\uff0c\u5f62\u6210\u4ece\u96f6\u8bad\u7ec3\u5230\u73b0\u5b9e\u90e8\u7f72\u7684\u5b8c\u6574\u5de5\u4f5c\u6d41\u7a0b\u3002", "result": "\u5e73\u53f0\u652f\u6301\u591a\u79cd\u73af\u5883\u4efb\u52a1\uff0c\u5e76\u5728\u771f\u5b9e\u6270\u52a8\u4e0b\u5c55\u793a\u4e86\u9ad8\u6548\u7684\u6a21\u62df\u5230\u73b0\u5b9e\u8fc1\u79fb\u548c\u7a33\u5065\u7684\u6237\u5916\u98de\u884c\u6027\u80fd\u3002", "conclusion": "\u8be5\u5e73\u53f0\u4e3a\u56db\u65cb\u7ffc\u65e0\u4eba\u673a\u7684\u5b66\u4e60\u7b56\u7565\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u590d\u73b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.14737", "pdf": "https://arxiv.org/pdf/2504.14737", "abs": "https://arxiv.org/abs/2504.14737", "authors": ["Shuang Zeng", "Lei Zhu", "Xinliang Zhang", "Hangzhou He", "Yanye Lu"], "title": "SuperCL: Superpixel Guided Contrastive Learning for Medical Image Segmentation Pre-training", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Medical image segmentation is a critical yet challenging task, primarily due\nto the difficulty of obtaining extensive datasets of high-quality,\nexpert-annotated images. Contrastive learning presents a potential but still\nproblematic solution to this issue. Because most existing methods focus on\nextracting instance-level or pixel-to-pixel representation, which ignores the\ncharacteristics between intra-image similar pixel groups. Moreover, when\nconsidering contrastive pairs generation, most SOTA methods mainly rely on\nmanually setting thresholds, which requires a large number of gradient\nexperiments and lacks efficiency and generalization. To address these issues,\nwe propose a novel contrastive learning approach named SuperCL for medical\nimage segmentation pre-training. Specifically, our SuperCL exploits the\nstructural prior and pixel correlation of images by introducing two novel\ncontrastive pairs generation strategies: Intra-image Local Contrastive Pairs\n(ILCP) Generation and Inter-image Global Contrastive Pairs (IGCP) Generation.\nConsidering superpixel cluster aligns well with the concept of contrastive\npairs generation, we utilize the superpixel map to generate pseudo masks for\nboth ILCP and IGCP to guide supervised contrastive learning. Moreover, we also\npropose two modules named Average SuperPixel Feature Map Generation (ASP) and\nConnected Components Label Generation (CCL) to better exploit the prior\nstructural information for IGCP. Finally, experiments on 8 medical image\ndatasets indicate our SuperCL outperforms existing 12 methods. i.e. Our SuperCL\nachieves a superior performance with more precise predictions from\nvisualization figures and 3.15%, 5.44%, 7.89% DSC higher than the previous best\nresults on MMWHS, CHAOS, Spleen with 10% annotations. Our code will be released\nafter acceptance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSuperCL\u7684\u65b0\u578b\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u533b\u5b66\u56fe\u50cf\u5206\u5272\u9884\u8bad\u7ec3\uff0c\u901a\u8fc7\u5229\u7528\u56fe\u50cf\u7684\u7ed3\u6784\u5148\u9a8c\u548c\u50cf\u7d20\u76f8\u5173\u6027\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u5bf9\u6bd4\u5bf9\u751f\u6210\u548c\u7279\u5f81\u63d0\u53d6\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4efb\u52a1\u9762\u4e34\u9ad8\u8d28\u91cf\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u7684\u6311\u6218\uff0c\u73b0\u6709\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\u5728\u5b9e\u4f8b\u7ea7\u6216\u50cf\u7d20\u7ea7\u8868\u793a\u4e0a\u5b58\u5728\u5c40\u9650\u6027\uff0c\u4e14\u5bf9\u6bd4\u5bf9\u751f\u6210\u4f9d\u8d56\u624b\u52a8\u9608\u503c\u8bbe\u7f6e\uff0c\u6548\u7387\u4f4e\u4e14\u7f3a\u4e4f\u6cdb\u5316\u6027\u3002", "method": "SuperCL\u5f15\u5165\u4e24\u79cd\u65b0\u7684\u5bf9\u6bd4\u5bf9\u751f\u6210\u7b56\u7565\uff1aIntra-image Local Contrastive Pairs (ILCP)\u548cInter-image Global Contrastive Pairs (IGCP)\uff0c\u5e76\u5229\u7528\u8d85\u50cf\u7d20\u56fe\u751f\u6210\u4f2a\u63a9\u7801\u6307\u5bfc\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86ASP\u548cCCL\u6a21\u5757\u4ee5\u66f4\u597d\u5730\u5229\u7528\u7ed3\u6784\u5148\u9a8c\u4fe1\u606f\u3002", "result": "\u57288\u4e2a\u533b\u5b66\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSuperCL\u4f18\u4e8e12\u79cd\u73b0\u6709\u65b9\u6cd5\uff0c\u5728MMWHS\u3001CHAOS\u548cSpleen\u6570\u636e\u96c6\u4e0a\u5206\u522b\u6bd4\u4e4b\u524d\u6700\u4f73\u7ed3\u679c\u9ad8\u51fa3.15%\u30015.44%\u548c7.89%\u7684DSC\u3002", "conclusion": "SuperCL\u901a\u8fc7\u521b\u65b0\u7684\u5bf9\u6bd4\u5bf9\u751f\u6210\u7b56\u7565\u548c\u7ed3\u6784\u5148\u9a8c\u5229\u7528\uff0c\u663e\u8457\u63d0\u5347\u4e86\u533b\u5b66\u56fe\u50cf\u5206\u5272\u7684\u6027\u80fd\uff0c\u5c24\u5176\u5728\u6807\u6ce8\u6570\u636e\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2504.14739", "pdf": "https://arxiv.org/pdf/2504.14739", "abs": "https://arxiv.org/abs/2504.14739", "authors": ["Arpit Agarwal", "Mohammad Amin Mirzaee", "Xiping Sun", "Wenzhen Yuan"], "title": "A Modularized Design Approach for GelSight Family of Vision-based Tactile Sensors", "categories": ["cs.RO", "cs.AI"], "comment": "The paper is accepted to International Journal of Robotics Research\n  with DOI 10.1177/02783649251339680", "summary": "GelSight family of vision-based tactile sensors has proven to be effective\nfor multiple robot perception and manipulation tasks. These sensors are based\non an internal optical system and an embedded camera to capture the deformation\nof the soft sensor surface, inferring the high-resolution geometry of the\nobjects in contact. However, customizing the sensors for different robot hands\nrequires a tedious trial-and-error process to re-design the optical system. In\nthis paper, we formulate the GelSight sensor design process as a systematic and\nobjective-driven design problem and perform the design optimization with a\nphysically accurate optical simulation. The method is based on modularizing and\nparameterizing the sensor's optical components and designing four generalizable\nobjective functions to evaluate the sensor. We implement the method with an\ninteractive and easy-to-use toolbox called OptiSense Studio. With the toolbox,\nnon-sensor experts can quickly optimize their sensor design in both forward and\ninverse ways following our predefined modules and steps. We demonstrate our\nsystem with four different GelSight sensors by quickly optimizing their initial\ndesign in simulation and transferring it to the real sensors.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7cfb\u7edf\u5316\u3001\u76ee\u6807\u9a71\u52a8\u7684GelSight\u89e6\u89c9\u4f20\u611f\u5668\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7\u5149\u5b66\u4eff\u771f\u4f18\u5316\u8bbe\u8ba1\uff0c\u5e76\u5f00\u53d1\u4e86\u6613\u7528\u7684\u5de5\u5177\u7bb1OptiSense Studio\u3002", "motivation": "\u73b0\u6709GelSight\u4f20\u611f\u5668\u8bbe\u8ba1\u8fc7\u7a0b\u7e41\u7410\uff0c\u9700\u8981\u53cd\u590d\u8bd5\u9519\uff0c\u7f3a\u4e4f\u7cfb\u7edf\u5316\u65b9\u6cd5\u3002", "method": "\u5c06\u4f20\u611f\u5668\u5149\u5b66\u7ec4\u4ef6\u6a21\u5757\u5316\u548c\u53c2\u6570\u5316\uff0c\u8bbe\u8ba1\u56db\u79cd\u901a\u7528\u76ee\u6807\u51fd\u6570\uff0c\u5229\u7528\u5149\u5b66\u4eff\u771f\u8fdb\u884c\u4f18\u5316\uff0c\u5f00\u53d1\u5de5\u5177\u7bb1OptiSense Studio\u3002", "result": "\u6210\u529f\u4f18\u5316\u4e86\u56db\u79cdGelSight\u4f20\u611f\u5668\u7684\u521d\u59cb\u8bbe\u8ba1\uff0c\u5e76\u5c06\u5176\u4ece\u4eff\u771f\u8f6c\u79fb\u5230\u5b9e\u9645\u4f20\u611f\u5668\u4e2d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u975e\u4f20\u611f\u5668\u4e13\u5bb6\u63d0\u4f9b\u4e86\u5feb\u901f\u4f18\u5316\u8bbe\u8ba1\u7684\u5de5\u5177\uff0c\u63d0\u5347\u4e86\u4f20\u611f\u5668\u8bbe\u8ba1\u7684\u6548\u7387\u548c\u901a\u7528\u6027\u3002"}}
{"id": "2504.15156", "pdf": "https://arxiv.org/pdf/2504.15156", "abs": "https://arxiv.org/abs/2504.15156", "authors": ["Zenia Elise Damgaard B\u00e6k", "Mois\u00e8s Coll Maci\u00e0", "Laurits Skov", "Asger Hobolth"], "title": "Advanced posterior analyses of hidden Markov models: finite Markov chain imbedding and hybrid decoding", "categories": ["stat.ML", "cs.LG"], "comment": "23 pages, 14 figures", "summary": "Two major tasks in applications of hidden Markov models are to (i) compute\ndistributions of summary statistics of the hidden state sequence, and (ii)\ndecode the hidden state sequence. We describe finite Markov chain imbedding\n(FMCI) and hybrid decoding to solve each of these two tasks. In the first part\nof our paper we use FMCI to compute posterior distributions of summary\nstatistics such as the number of visits to a hidden state, the total time spent\nin a hidden state, the dwell time in a hidden state, and the longest run\nlength. We use simulations from the hidden state sequence, conditional on the\nobserved sequence, to establish the FMCI framework. In the second part of our\npaper we apply hybrid segmentation for improved decoding of a HMM. We\ndemonstrate that hybrid decoding shows increased performance compared to\nViterbi or Posterior decoding (often also referred to as global or local\ndecoding), and we introduce a novel procedure for choosing the tuning parameter\nin the hybrid procedure. Furthermore, we provide an alternative derivation of\nthe hybrid loss function based on weighted geometric means. We demonstrate and\napply FMCI and hybrid decoding on various classical data sets, and supply\naccompanying code for reproducibility.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4f7f\u7528\u6709\u9650\u9a6c\u5c14\u53ef\u592b\u94fe\u5d4c\u5165\uff08FMCI\uff09\u548c\u6df7\u5408\u89e3\u7801\u65b9\u6cd5\u89e3\u51b3\u9690\u9a6c\u5c14\u53ef\u592b\u6a21\u578b\uff08HMM\uff09\u4e2d\u7684\u4e24\u4e2a\u4e3b\u8981\u4efb\u52a1\uff1a\u8ba1\u7b97\u9690\u85cf\u72b6\u6001\u5e8f\u5217\u7684\u7edf\u8ba1\u91cf\u5206\u5e03\u548c\u89e3\u7801\u9690\u85cf\u72b6\u6001\u5e8f\u5217\u3002", "motivation": "\u89e3\u51b3HMM\u5e94\u7528\u4e2d\u8ba1\u7b97\u9690\u85cf\u72b6\u6001\u5e8f\u5217\u7edf\u8ba1\u91cf\u5206\u5e03\u548c\u89e3\u7801\u5e8f\u5217\u7684\u6311\u6218\u3002", "method": "\u7b2c\u4e00\u90e8\u5206\u4f7f\u7528FMCI\u8ba1\u7b97\u9690\u85cf\u72b6\u6001\u7684\u540e\u9a8c\u5206\u5e03\u7edf\u8ba1\u91cf\uff08\u5982\u8bbf\u95ee\u6b21\u6570\u3001\u505c\u7559\u65f6\u95f4\u7b49\uff09\uff1b\u7b2c\u4e8c\u90e8\u5206\u63d0\u51fa\u6df7\u5408\u89e3\u7801\u65b9\u6cd5\uff0c\u7ed3\u5408Viterbi\u548c\u540e\u9a8c\u89e3\u7801\u7684\u4f18\u52bf\u3002", "result": "\u6df7\u5408\u89e3\u7801\u6027\u80fd\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u5e76\u63d0\u4f9b\u4e86\u9009\u62e9\u8c03\u53c2\u7684\u65b0\u65b9\u6cd5\uff1bFMCI\u6846\u67b6\u901a\u8fc7\u6a21\u62df\u9a8c\u8bc1\u3002", "conclusion": "FMCI\u548c\u6df7\u5408\u89e3\u7801\u65b9\u6cd5\u5728\u7ecf\u5178\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u63d0\u4f9b\u4e86\u53ef\u590d\u73b0\u7684\u4ee3\u7801\u3002"}}
{"id": "2504.15193", "pdf": "https://arxiv.org/pdf/2504.15193", "abs": "https://arxiv.org/abs/2504.15193", "authors": ["Neelesh Kumar", "Oya Aran"], "title": "Automated Measurement of Eczema Severity with Self-Supervised Learning", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Automated diagnosis of eczema using images acquired from digital camera can\nenable individuals to self-monitor their recovery. The process entails first\nsegmenting out the eczema region from the image and then measuring the severity\nof eczema in the segmented region. The state-of-the-art methods for automated\neczema diagnosis rely on deep neural networks such as convolutional neural\nnetwork (CNN) and have shown impressive performance in accurately measuring the\nseverity of eczema. However, these methods require massive volume of annotated\ndata to train which can be hard to obtain. In this paper, we propose a\nself-supervised learning framework for automated eczema diagnosis under limited\ntraining data regime. Our framework consists of two stages: i) Segmentation,\nwhere we use an in-context learning based algorithm called SegGPT for few-shot\nsegmentation of eczema region from the image; ii) Feature extraction and\nclassification, where we extract DINO features from the segmented regions and\nfeed it to a multi-layered perceptron (MLP) for 4-class classification of\neczema severity. When evaluated on a dataset of annotated \"in-the-wild\" eczema\nimages, we show that our method outperforms (Weighted F1: 0.67 $\\pm$ 0.01) the\nstate-of-the-art deep learning methods such as finetuned Resnet-18 (Weighted\nF1: 0.44 $\\pm$ 0.16) and Vision Transformer (Weighted F1: 0.40 $\\pm$ 0.22). Our\nresults show that self-supervised learning can be a viable solution for\nautomated skin diagnosis where labeled data is scarce.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u81ea\u76d1\u7763\u5b66\u4e60\u7684\u6e7f\u75b9\u81ea\u52a8\u8bca\u65ad\u6846\u67b6\uff0c\u5728\u6709\u9650\u6807\u6ce8\u6570\u636e\u4e0b\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u6e7f\u75b9\u81ea\u52a8\u8bca\u65ad\u65b9\u6cd5\u4f9d\u8d56\u5927\u91cf\u6807\u6ce8\u6570\u636e\uff0c\u4f46\u6807\u6ce8\u6570\u636e\u96be\u4ee5\u83b7\u53d6\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u6846\u67b6\uff1a1) \u4f7f\u7528SegGPT\u8fdb\u884c\u5c11\u91cf\u6837\u672c\u5206\u5272\uff1b2) \u63d0\u53d6DINO\u7279\u5f81\u5e76\u7528MLP\u8fdb\u884c\u56db\u5206\u7c7b\u3002", "result": "\u5728\u771f\u5b9e\u6e7f\u75b9\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\uff0c\u52a0\u6743F1\u5206\u6570\uff080.67\u00b10.01\uff09\u4f18\u4e8eResnet-18\uff080.44\u00b10.16\uff09\u548cVision Transformer\uff080.40\u00b10.22\uff09\u3002", "conclusion": "\u81ea\u76d1\u7763\u5b66\u4e60\u662f\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u65f6\u76ae\u80a4\u81ea\u52a8\u8bca\u65ad\u7684\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.14757", "pdf": "https://arxiv.org/pdf/2504.14757", "abs": "https://arxiv.org/abs/2504.14757", "authors": ["Minh V. T. Pham", "Huy N. Phan", "Hoang N. Phan", "Cuong Le Chi", "Tien N. Nguyen", "Nghi D. Q. Bui"], "title": "SWE-Synth: Synthesizing Verifiable Bug-Fix Data to Enable Large Language Models in Resolving Real-World Bugs", "categories": ["cs.SE", "cs.AI"], "comment": "Work in progress", "summary": "Large language models (LLMs) are transforming automated program repair (APR)\nthrough agent-based approaches that localize bugs, generate patches, and verify\nfixes. However, the lack of high-quality, scalable training datasets,\nespecially those with verifiable outputs and intermediate reasoning\ntraces-limits progress, particularly for open-source models. In this work, we\npresent SWE-Synth, a framework for synthesizing realistic, verifiable, and\nprocess-aware bug-fix datasets at the repository level. SWE-Synth leverages LLM\nagents to simulate debugging workflows, producing not only bug-fix pairs but\nalso test cases and structured repair trajectories. Compared to manually\ncurated datasets, our method scales with minimal human effort while preserving\ncontextual richness and correctness. Experiments show that models trained on\nSWE-Synth outperform those trained on real-world datasets by 2.3% on SWE-Bench\nLite. Our results highlight the potential of synthetic, agent-generated data to\nadvance the state of the art in APR and software engineering automation.", "AI": {"tldr": "SWE-Synth\u6846\u67b6\u5229\u7528LLM\u4ee3\u7406\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u53ef\u9a8c\u8bc1\u7684bug\u4fee\u590d\u6570\u636e\u96c6\uff0c\u63d0\u5347\u81ea\u52a8\u7a0b\u5e8f\u4fee\u590d\u6027\u80fd\u3002", "motivation": "\u73b0\u6709APR\u9886\u57df\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u3001\u53ef\u6269\u5c55\u7684\u8bad\u7ec3\u6570\u636e\u96c6\uff0c\u5c24\u5176\u662f\u5305\u542b\u53ef\u9a8c\u8bc1\u8f93\u51fa\u548c\u4e2d\u95f4\u63a8\u7406\u8f68\u8ff9\u7684\u6570\u636e\u96c6\uff0c\u9650\u5236\u4e86\u5f00\u6e90\u6a21\u578b\u7684\u8fdb\u5c55\u3002", "method": "SWE-Synth\u901a\u8fc7LLM\u4ee3\u7406\u6a21\u62df\u8c03\u8bd5\u5de5\u4f5c\u6d41\uff0c\u751f\u6210bug\u4fee\u590d\u5bf9\u3001\u6d4b\u8bd5\u7528\u4f8b\u548c\u7ed3\u6784\u5316\u4fee\u590d\u8f68\u8ff9\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4f7f\u7528SWE-Synth\u8bad\u7ec3\u7684\u6a21\u578b\u5728SWE-Bench Lite\u4e0a\u6027\u80fd\u63d0\u53472.3%\u3002", "conclusion": "\u5408\u6210\u6570\u636e\u5728APR\u548c\u8f6f\u4ef6\u5de5\u7a0b\u81ea\u52a8\u5316\u4e2d\u5177\u6709\u63a8\u52a8\u6280\u672f\u8fdb\u6b65\u6f5c\u529b\u3002"}}
{"id": "2504.15199", "pdf": "https://arxiv.org/pdf/2504.15199", "abs": "https://arxiv.org/abs/2504.15199", "authors": ["Yassir Benhammou", "Alessandro Tiberio", "Gabriel Trautmann", "Suman Kalyan"], "title": "Zero-Shot, But at What Cost? Unveiling the Hidden Overhead of MILS's LLM-CLIP Framework for Image Captioning", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.PF"], "comment": "9 pages, 2 tables, 1 figure", "summary": "MILS (Multimodal Iterative LLM Solver) is a recently published framework that\nclaims \"LLMs can see and hear without any training\" by leveraging an iterative,\nLLM-CLIP based approach for zero-shot image captioning. While this MILS\napproach demonstrates good performance, our investigation reveals that this\nsuccess comes at a hidden, substantial computational cost due to its expensive\nmulti-step refinement process. In contrast, alternative models such as BLIP-2\nand GPT-4V achieve competitive results through a streamlined, single-pass\napproach. We hypothesize that the significant overhead inherent in MILS's\niterative process may undermine its practical benefits, thereby challenging the\nnarrative that zero-shot performance can be attained without incurring heavy\nresource demands. This work is the first to expose and quantify the trade-offs\nbetween output quality and computational cost in MILS, providing critical\ninsights for the design of more efficient multimodal models.", "AI": {"tldr": "MILS\u6846\u67b6\u58f0\u79f0LLM\u65e0\u9700\u8bad\u7ec3\u5373\u53ef\u5904\u7406\u591a\u6a21\u6001\u4efb\u52a1\uff0c\u4f46\u7814\u7a76\u53d1\u73b0\u5176\u8fed\u4ee3\u4f18\u5316\u8fc7\u7a0b\u5e26\u6765\u9ad8\u8ba1\u7b97\u6210\u672c\uff0c\u800cBLIP-2\u548cGPT-4V\u7b49\u5355\u6b65\u65b9\u6cd5\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u63ed\u793aMILS\u6846\u67b6\u5728\u96f6\u6837\u672c\u56fe\u50cf\u63cf\u8ff0\u4e2d\u7684\u9690\u85cf\u8ba1\u7b97\u6210\u672c\uff0c\u6311\u6218\u5176\u201c\u65e0\u9700\u8bad\u7ec3\u201d\u7684\u5b9e\u9645\u53ef\u884c\u6027\u3002", "method": "\u901a\u8fc7\u5bf9\u6bd4MILS\u4e0eBLIP-2\u3001GPT-4V\u7684\u6027\u80fd\u548c\u8ba1\u7b97\u6210\u672c\uff0c\u91cf\u5316\u5176\u6548\u7387\u95ee\u9898\u3002", "result": "MILS\u7684\u9ad8\u8ba1\u7b97\u6210\u672c\u53ef\u80fd\u62b5\u6d88\u5176\u96f6\u6837\u672c\u4f18\u52bf\uff0c\u5355\u6b65\u65b9\u6cd5\u66f4\u9ad8\u6548\u3002", "conclusion": "\u8bbe\u8ba1\u9ad8\u6548\u591a\u6a21\u6001\u6a21\u578b\u9700\u6743\u8861\u8f93\u51fa\u8d28\u91cf\u4e0e\u8ba1\u7b97\u6210\u672c\uff0cMILS\u7684\u8fed\u4ee3\u65b9\u6cd5\u53ef\u80fd\u4e0d\u5b9e\u7528\u3002"}}
{"id": "2504.15217", "pdf": "https://arxiv.org/pdf/2504.15217", "abs": "https://arxiv.org/abs/2504.15217", "authors": ["Yatong Bai", "Jonah Casebeer", "Somayeh Sojoudi", "Nicholas J. Bryan"], "title": "DRAGON: Distributional Rewards Optimize Diffusion Generative Models", "categories": ["cs.SD", "cs.LG"], "comment": null, "summary": "We present Distributional RewArds for Generative OptimizatioN (DRAGON), a\nversatile framework for fine-tuning media generation models towards a desired\noutcome. Compared with traditional reinforcement learning with human feedback\n(RLHF) or pairwise preference approaches such as direct preference optimization\n(DPO), DRAGON is more flexible. It can optimize reward functions that evaluate\neither individual examples or distributions of them, making it compatible with\na broad spectrum of instance-wise, instance-to-distribution, and\ndistribution-to-distribution rewards. Leveraging this versatility, we construct\nnovel reward functions by selecting an encoder and a set of reference examples\nto create an exemplar distribution. When cross-modality encoders such as CLAP\nare used, the reference examples may be of a different modality (e.g., text\nversus audio). Then, DRAGON gathers online and on-policy generations, scores\nthem to construct a positive demonstration set and a negative set, and\nleverages the contrast between the two sets to maximize the reward. For\nevaluation, we fine-tune an audio-domain text-to-music diffusion model with 20\ndifferent reward functions, including a custom music aesthetics model, CLAP\nscore, Vendi diversity, and Frechet audio distance (FAD). We further compare\ninstance-wise (per-song) and full-dataset FAD settings while ablating multiple\nFAD encoders and reference sets. Over all 20 target rewards, DRAGON achieves an\n81.45% average win rate. Moreover, reward functions based on exemplar sets\nindeed enhance generations and are comparable to model-based rewards. With an\nappropriate exemplar set, DRAGON achieves a 60.95% human-voted music quality\nwin rate without training on human preference annotations. As such, DRAGON\nexhibits a new approach to designing and optimizing reward functions for\nimproving human-perceived quality. Sound examples at\nhttps://ml-dragon.github.io/web.", "AI": {"tldr": "DRAGON\u662f\u4e00\u4e2a\u7075\u6d3b\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u4f18\u5316\u751f\u6210\u6a21\u578b\u7684\u76ee\u6807\u7ed3\u679c\uff0c\u652f\u6301\u591a\u79cd\u5956\u52b1\u51fd\u6570\u7c7b\u578b\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5982RLHF\u6216DPO\u5728\u4f18\u5316\u751f\u6210\u6a21\u578b\u65f6\u7075\u6d3b\u6027\u4e0d\u8db3\uff0cDRAGON\u65e8\u5728\u63d0\u4f9b\u66f4\u901a\u7528\u7684\u5956\u52b1\u51fd\u6570\u4f18\u5316\u65b9\u6848\u3002", "method": "DRAGON\u901a\u8fc7\u9009\u62e9\u7f16\u7801\u5668\u548c\u53c2\u8003\u6837\u672c\u6784\u5efa\u5956\u52b1\u51fd\u6570\uff0c\u5229\u7528\u5bf9\u6bd4\u96c6\u4f18\u5316\u5956\u52b1\uff0c\u652f\u6301\u8de8\u6a21\u6001\u548c\u591a\u7c7b\u578b\u5956\u52b1\u3002", "result": "\u572820\u79cd\u5956\u52b1\u51fd\u6570\u5b9e\u9a8c\u4e2d\uff0cDRAGON\u5e73\u5747\u80dc\u7387\u8fbe81.45%\uff0c\u4e14\u57fa\u4e8e\u6837\u672c\u96c6\u7684\u5956\u52b1\u51fd\u6570\u6548\u679c\u4e0e\u6a21\u578b\u5956\u52b1\u76f8\u5f53\u3002", "conclusion": "DRAGON\u4e3a\u4f18\u5316\u751f\u6210\u6a21\u578b\u7684\u4eba\u7c7b\u611f\u77e5\u8d28\u91cf\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\uff0c\u65e0\u9700\u4f9d\u8d56\u4eba\u7c7b\u504f\u597d\u6807\u6ce8\u3002"}}
{"id": "2504.14779", "pdf": "https://arxiv.org/pdf/2504.14779", "abs": "https://arxiv.org/abs/2504.14779", "authors": ["Janet G. Johnson", "Macarena Peralta", "Mansanjam Kaur", "Ruijie Sophia Huang", "Sheng Zhao", "Ruijia Guan", "Shwetha Rajaram", "Michael Nebeling"], "title": "Exploring Collaborative GenAI Agents in Synchronous Group Settings: Eliciting Team Perceptions and Design Considerations for the Future of Work", "categories": ["cs.HC", "cs.AI"], "comment": "To be published in ACM Conference on Computer-Supported Cooperative\n  Work and Social Computing (CSCW 2025). 33 pages, 11 figures, 1 table", "summary": "While generative artificial intelligence (GenAI) is finding increased\nadoption in workplaces, current tools are primarily designed for individual\nuse. Prior work established the potential for these tools to enhance personal\ncreativity and productivity towards shared goals; however, we don't know yet\nhow to best take into account the nuances of group work and team dynamics when\ndeploying GenAI in work settings. In this paper, we investigate the potential\nof collaborative GenAI agents to augment teamwork in synchronous group settings\nthrough an exploratory study that engaged 25 professionals across 6 teams in\nspeculative design workshops and individual follow-up interviews. Our workshops\nincluded a mixed reality provotype to simulate embodied collaborative GenAI\nagents capable of actively participating in group discussions. Our findings\nsuggest that, if designed well, collaborative GenAI agents offer valuable\nopportunities to enhance team problem-solving by challenging groupthink,\nbridging communication gaps, and reducing social friction. However, teams'\nwillingness to integrate GenAI agents depended on its perceived fit across a\nnumber of individual, team, and organizational factors. We outline the key\ndesign tensions around agent representation, social prominence, and engagement\nand highlight the opportunities spatial and immersive technologies could offer\nto modulate GenAI influence on team outcomes and strike a balance between\naugmentation and agency.", "AI": {"tldr": "\u63a2\u8ba8\u534f\u4f5c\u5f0f\u751f\u6210AI\u4ee3\u7406\u5728\u56e2\u961f\u5de5\u4f5c\u4e2d\u7684\u6f5c\u529b\uff0c\u901a\u8fc7\u8bbe\u8ba1\u7814\u8ba8\u4f1a\u548c\u8bbf\u8c08\u53d1\u73b0\u5176\u80fd\u63d0\u5347\u56e2\u961f\u95ee\u9898\u89e3\u51b3\u80fd\u529b\uff0c\u4f46\u9700\u8003\u8651\u4e2a\u4f53\u3001\u56e2\u961f\u548c\u7ec4\u7ec7\u56e0\u7d20\u3002", "motivation": "\u7814\u7a76\u751f\u6210AI\u5728\u56e2\u961f\u534f\u4f5c\u4e2d\u7684\u5e94\u7528\uff0c\u586b\u8865\u5f53\u524d\u5de5\u5177\u4e3b\u8981\u9488\u5bf9\u4e2a\u4eba\u4f7f\u7528\u7684\u7a7a\u767d\uff0c\u63a2\u7d22\u5982\u4f55\u4f18\u5316\u56e2\u961f\u52a8\u6001\u4e2d\u7684AI\u90e8\u7f72\u3002", "method": "\u901a\u8fc725\u540d\u4e13\u4e1a\u4eba\u58eb\u53c2\u4e0e\u76846\u4e2a\u56e2\u961f\u7684\u8bbe\u8ba1\u7814\u8ba8\u4f1a\u548c\u540e\u7eed\u8bbf\u8c08\uff0c\u4f7f\u7528\u6df7\u5408\u73b0\u5b9e\u539f\u578b\u6a21\u62df\u534f\u4f5c\u5f0f\u751f\u6210AI\u4ee3\u7406\u3002", "result": "\u534f\u4f5c\u5f0f\u751f\u6210AI\u4ee3\u7406\u80fd\u6709\u6548\u6311\u6218\u7fa4\u4f53\u601d\u7ef4\u3001\u5f25\u8865\u6c9f\u901a\u5dee\u8ddd\u5e76\u51cf\u5c11\u793e\u4ea4\u6469\u64e6\uff0c\u4f46\u5176\u63a5\u53d7\u5ea6\u53d6\u51b3\u4e8e\u4e2a\u4f53\u3001\u56e2\u961f\u548c\u7ec4\u7ec7\u7684\u9002\u914d\u5ea6\u3002", "conclusion": "\u8bbe\u8ba1\u9700\u5e73\u8861\u4ee3\u7406\u7684\u8868\u73b0\u5f62\u5f0f\u3001\u793e\u4ea4\u663e\u8457\u6027\u548c\u53c2\u4e0e\u5ea6\uff0c\u7a7a\u95f4\u548c\u6c89\u6d78\u6280\u672f\u53ef\u8c03\u8282AI\u5bf9\u56e2\u961f\u6210\u679c\u7684\u5f71\u54cd\uff0c\u5b9e\u73b0\u589e\u5f3a\u4e0e\u81ea\u4e3b\u6027\u7684\u5e73\u8861\u3002"}}
{"id": "2504.14783", "pdf": "https://arxiv.org/pdf/2504.14783", "abs": "https://arxiv.org/abs/2504.14783", "authors": ["Wenhui Zhu", "Peijie Qiu", "Xiwen Chen", "Zhangsihao Yang", "Aristeidis Sotiras", "Abolfazl Razi", "Yalin Wang"], "title": "How Effective Can Dropout Be in Multiple Instance Learning ?", "categories": ["cs.CV", "cs.AI", "eess.IV", "stat.ML"], "comment": null, "summary": "Multiple Instance Learning (MIL) is a popular weakly-supervised method for\nvarious applications, with a particular interest in histological whole slide\nimage (WSI) classification. Due to the gigapixel resolution of WSI,\napplications of MIL in WSI typically necessitate a two-stage training scheme:\nfirst, extract features from the pre-trained backbone and then perform MIL\naggregation. However, it is well-known that this suboptimal training scheme\nsuffers from \"noisy\" feature embeddings from the backbone and inherent weak\nsupervision, hindering MIL from learning rich and generalizable features.\nHowever, the most commonly used technique (i.e., dropout) for mitigating this\nissue has yet to be explored in MIL. In this paper, we empirically explore how\neffective the dropout can be in MIL. Interestingly, we observe that dropping\nthe top-k most important instances within a bag leads to better performance and\ngeneralization even under noise attack. Based on this key observation, we\npropose a novel MIL-specific dropout method, termed MIL-Dropout, which\nsystematically determines which instances to drop. Experiments on five MIL\nbenchmark datasets and two WSI datasets demonstrate that MIL-Dropout boosts the\nperformance of current MIL methods with a negligible computational cost. The\ncode is available at https://github.com/ChongQingNoSubway/MILDropout.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMIL-Dropout\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e22\u5f03\u5305\u4e2d\u6700\u91cd\u8981\u5b9e\u4f8b\u6765\u63d0\u5347\u591a\u5b9e\u4f8b\u5b66\u4e60\uff08MIL\uff09\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u4f20\u7edfMIL\u5728WSI\u5206\u7c7b\u4e2d\u56e0\u4e24\u9636\u6bb5\u8bad\u7ec3\u65b9\u6848\u5bfc\u81f4\u7279\u5f81\u5d4c\u5165\u566a\u58f0\u548c\u5f31\u76d1\u7763\u95ee\u9898\uff0c\u73b0\u6709\u6280\u672f\uff08\u5982dropout\uff09\u672a\u5728MIL\u4e2d\u5145\u5206\u63a2\u7d22\u3002", "method": "\u63d0\u51faMIL-Dropout\uff0c\u7cfb\u7edf\u51b3\u5b9a\u4e22\u5f03\u5305\u4e2d\u6700\u91cd\u8981\u7684\u5b9e\u4f8b\uff0c\u4ee5\u4f18\u5316\u6027\u80fd\u3002", "result": "\u5728\u4e94\u4e2aMIL\u57fa\u51c6\u6570\u636e\u96c6\u548c\u4e24\u4e2aWSI\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86MIL-Dropout\u7684\u6709\u6548\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u4e14\u8ba1\u7b97\u6210\u672c\u4f4e\u3002", "conclusion": "MIL-Dropout\u662f\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u80fd\u663e\u8457\u63d0\u5347MIL\u6027\u80fd\uff0c\u5c24\u5176\u5728\u566a\u58f0\u73af\u5883\u4e0b\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2504.14797", "pdf": "https://arxiv.org/pdf/2504.14797", "abs": "https://arxiv.org/abs/2504.14797", "authors": ["Clare E. Laney", "Andrew Barovic", "Armin Moin"], "title": "Automated Duplicate Bug Report Detection in Large Open Bug Repositories", "categories": ["cs.SE", "cs.AI"], "comment": "IEEE COMPSAC 2025", "summary": "Many users and contributors of large open-source projects report software\ndefects or enhancement requests (known as bug reports) to the issue-tracking\nsystems. However, they sometimes report issues that have already been reported.\nFirst, they may not have time to do sufficient research on existing bug\nreports. Second, they may not possess the right expertise in that specific area\nto realize that an existing bug report is essentially elaborating on the same\nmatter, perhaps with a different wording. In this paper, we propose a novel\napproach based on machine learning methods that can automatically detect\nduplicate bug reports in an open bug repository based on the textual data in\nthe reports. We present six alternative methods: Topic modeling, Gaussian Naive\nBayes, deep learning, time-based organization, clustering, and summarization\nusing a generative pre-trained transformer large language model. Additionally,\nwe introduce a novel threshold-based approach for duplicate identification, in\ncontrast to the conventional top-k selection method that has been widely used\nin the literature. Our approach demonstrates promising results across all the\nproposed methods, achieving accuracy rates ranging from the high 70%'s to the\nlow 90%'s. We evaluated our methods on a public dataset of issues belonging to\nan Eclipse open-source project.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u81ea\u52a8\u68c0\u6d4b\u5f00\u6e90\u9879\u76ee\u4e2d\u91cd\u590d\u7684\u7f3a\u9677\u62a5\u544a\uff0c\u5e76\u5bf9\u6bd4\u4e86\u516d\u79cd\u4e0d\u540c\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u7528\u6237\u548c\u8d21\u732e\u8005\u5728\u62a5\u544a\u7f3a\u9677\u65f6\u53ef\u80fd\u56e0\u65f6\u95f4\u6216\u4e13\u4e1a\u77e5\u8bc6\u4e0d\u8db3\u800c\u91cd\u590d\u63d0\u4ea4\u76f8\u540c\u95ee\u9898\uff0c\u5bfc\u81f4\u8d44\u6e90\u6d6a\u8d39\u3002", "method": "\u91c7\u7528\u4e86\u516d\u79cd\u65b9\u6cd5\uff1a\u4e3b\u9898\u5efa\u6a21\u3001\u9ad8\u65af\u6734\u7d20\u8d1d\u53f6\u65af\u3001\u6df1\u5ea6\u5b66\u4e60\u3001\u57fa\u4e8e\u65f6\u95f4\u7684\u7ec4\u7ec7\u3001\u805a\u7c7b\u548c\u751f\u6210\u5f0f\u9884\u8bad\u7ec3\u53d8\u6362\u5668\uff08GPT\uff09\u7684\u6458\u8981\u6280\u672f\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u9608\u503c\u7684\u91cd\u590d\u8bc6\u522b\u65b9\u6cd5\u3002", "result": "\u6240\u6709\u65b9\u6cd5\u5728Eclipse\u5f00\u6e90\u9879\u76ee\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u51c6\u786e\u7387\u572870%\u81f390%\u4e4b\u95f4\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u6709\u6548\u8bc6\u522b\u91cd\u590d\u7f3a\u9677\u62a5\u544a\uff0c\u4e3a\u5f00\u6e90\u9879\u76ee\u7ba1\u7406\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\u3002"}}
{"id": "2504.14825", "pdf": "https://arxiv.org/pdf/2504.14825", "abs": "https://arxiv.org/abs/2504.14825", "authors": ["Zhoujie Qian"], "title": "ECViT: Efficient Convolutional Vision Transformer with Local-Attention and Multi-scale Stages", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Vision Transformers (ViTs) have revolutionized computer vision by leveraging\nself-attention to model long-range dependencies. However, ViTs face challenges\nsuch as high computational costs due to the quadratic scaling of self-attention\nand the requirement of a large amount of training data. To address these\nlimitations, we propose the Efficient Convolutional Vision Transformer (ECViT),\na hybrid architecture that effectively combines the strengths of CNNs and\nTransformers. ECViT introduces inductive biases such as locality and\ntranslation invariance, inherent to Convolutional Neural Networks (CNNs) into\nthe Transformer framework by extracting patches from low-level features and\nenhancing the encoder with convolutional operations. Additionally, it\nincorporates local-attention and a pyramid structure to enable efficient\nmulti-scale feature extraction and representation. Experimental results\ndemonstrate that ECViT achieves an optimal balance between performance and\nefficiency, outperforming state-of-the-art models on various image\nclassification tasks while maintaining low computational and storage\nrequirements. ECViT offers an ideal solution for applications that prioritize\nhigh efficiency without compromising performance.", "AI": {"tldr": "ECViT\u662f\u4e00\u79cd\u7ed3\u5408CNN\u548cTransformer\u4f18\u52bf\u7684\u6df7\u5408\u67b6\u6784\uff0c\u901a\u8fc7\u5f15\u5165\u5c40\u90e8\u6027\u548c\u5e73\u79fb\u4e0d\u53d8\u6027\u7b49\u5f52\u7eb3\u504f\u7f6e\uff0c\u89e3\u51b3\u4e86ViTs\u7684\u9ad8\u8ba1\u7b97\u6210\u672c\u548c\u6570\u636e\u9700\u6c42\u95ee\u9898\u3002", "motivation": "ViTs\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u9762\u4e34\u9ad8\u8ba1\u7b97\u6210\u672c\u548c\u5927\u91cf\u8bad\u7ec3\u6570\u636e\u9700\u6c42\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faECViT\uff0c\u7ed3\u5408CNN\u7684\u5c40\u90e8\u6027\u548cTransformer\u7684\u957f\u7a0b\u4f9d\u8d56\u5efa\u6a21\u80fd\u529b\uff0c\u91c7\u7528\u5c40\u90e8\u6ce8\u610f\u529b\u548c\u91d1\u5b57\u5854\u7ed3\u6784\u5b9e\u73b0\u9ad8\u6548\u591a\u5c3a\u5ea6\u7279\u5f81\u63d0\u53d6\u3002", "result": "ECViT\u5728\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\u6027\u80fd\u4f18\u5f02\uff0c\u540c\u65f6\u4fdd\u6301\u4f4e\u8ba1\u7b97\u548c\u5b58\u50a8\u9700\u6c42\u3002", "conclusion": "ECViT\u4e3a\u9ad8\u6548\u4e14\u9ad8\u6027\u80fd\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u7406\u60f3\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.14832", "pdf": "https://arxiv.org/pdf/2504.14832", "abs": "https://arxiv.org/abs/2504.14832", "authors": ["Yue Li", "Weizhi Liu", "Dongdong Lin"], "title": "Protecting Your Voice: Temporal-aware Robust Watermarking", "categories": ["cs.CR", "cs.AI", "cs.SD"], "comment": null, "summary": "The rapid advancement of generative models has led to the synthesis of\nreal-fake ambiguous voices. To erase the ambiguity, embedding watermarks into\nthe frequency-domain features of synthesized voices has become a common\nroutine. However, the robustness achieved by choosing the frequency domain\noften comes at the expense of fine-grained voice features, leading to a loss of\nfidelity. Maximizing the comprehensive learning of time-domain features to\nenhance fidelity while maintaining robustness, we pioneer a\n\\textbf{\\underline{t}}emporal-aware\n\\textbf{\\underline{r}}ob\\textbf{\\underline{u}}st\nwat\\textbf{\\underline{e}}rmarking (\\emph{True}) method for protecting the\nspeech and singing voice.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65f6\u95f4\u611f\u77e5\u7684\u9c81\u68d2\u6c34\u5370\u65b9\u6cd5\uff08True\uff09\uff0c\u7528\u4e8e\u4fdd\u62a4\u8bed\u97f3\u548c\u6b4c\u58f0\uff0c\u540c\u65f6\u5e73\u8861\u4fdd\u771f\u5ea6\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u751f\u6210\u6a21\u578b\u7684\u5feb\u901f\u53d1\u5c55\u5bfc\u81f4\u5408\u6210\u58f0\u97f3\u7684\u771f\u5b9e\u6027\u6a21\u7cca\uff0c\u73b0\u6709\u9891\u57df\u6c34\u5370\u65b9\u6cd5\u867d\u9c81\u68d2\u4f46\u727a\u7272\u4e86\u58f0\u97f3\u7684\u4fdd\u771f\u5ea6\u3002", "method": "\u901a\u8fc7\u6700\u5927\u5316\u65f6\u95f4\u57df\u7279\u5f81\u7684\u5168\u9762\u5b66\u4e60\uff0c\u63d0\u51faTrue\u65b9\u6cd5\uff0c\u589e\u5f3a\u4fdd\u771f\u5ea6\u540c\u65f6\u4fdd\u6301\u9c81\u68d2\u6027\u3002", "result": "True\u65b9\u6cd5\u5728\u4fdd\u62a4\u8bed\u97f3\u548c\u6b4c\u58f0\u65f6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4fdd\u771f\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9c81\u68d2\u6027\u3002", "conclusion": "True\u65b9\u6cd5\u4e3a\u5408\u6210\u58f0\u97f3\u7684\u6c34\u5370\u4fdd\u62a4\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e73\u8861\u4e86\u4fdd\u771f\u5ea6\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2504.14839", "pdf": "https://arxiv.org/pdf/2504.14839", "abs": "https://arxiv.org/abs/2504.14839", "authors": ["Xinjie Shen", "Zhichao Geng", "Yang Yang"], "title": "Exploring $\\ell_0$ Sparsification for Inference-free Sparse Retrievers", "categories": ["cs.IR", "cs.AI"], "comment": "Accepted by SIGIR 2025", "summary": "With increasing demands for efficiency, information retrieval has developed a\nbranch of sparse retrieval, further advancing towards inference-free retrieval\nwhere the documents are encoded during indexing time and there is no\nmodel-inference for queries. Existing sparse retrieval models rely on FLOPS\nregularization for sparsification, while this mechanism was originally designed\nfor Siamese encoders, it is considered to be suboptimal in inference-free\nscenarios which is asymmetric. Previous attempts to adapt FLOPS for\ninference-free scenarios have been limited to rule-based methods, leaving the\npotential of sparsification approaches for inference-free retrieval models\nlargely unexplored. In this paper, we explore $\\ell_0$ inspired sparsification\nmanner for inference-free retrievers. Through comprehensive out-of-domain\nevaluation on the BEIR benchmark, our method achieves state-of-the-art\nperformance among inference-free sparse retrieval models and is comparable to\nleading Siamese sparse retrieval models. Furthermore, we provide insights into\nthe trade-off between retrieval effectiveness and computational efficiency,\ndemonstrating practical value for real-world applications.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2504.14848", "pdf": "https://arxiv.org/pdf/2504.14848", "abs": "https://arxiv.org/abs/2504.14848", "authors": ["Yunpu Zhao", "Rui Zhang", "Junbin Xiao", "Ruibo Hou", "Jiaming Guo", "Zihao Zhang", "Yifan Hao", "Yunji Chen"], "title": "Object-Level Verbalized Confidence Calibration in Vision-Language Models via Semantic Perturbation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Vision-language models (VLMs) excel in various multimodal tasks but\nfrequently suffer from poor calibration, resulting in misalignment between\ntheir verbalized confidence and response correctness. This miscalibration\nundermines user trust, especially when models confidently provide incorrect or\nfabricated information. In this work, we propose a novel Confidence Calibration\nthrough Semantic Perturbation (CSP) framework to improve the calibration of\nverbalized confidence for VLMs in response to object-centric queries. We first\nintroduce a perturbed dataset where Gaussian noise is applied to the key object\nregions to simulate visual uncertainty at different confidence levels,\nestablishing an explicit mapping between visual ambiguity and confidence\nlevels. We further enhance calibration through a two-stage training process\ncombining supervised fine-tuning on the perturbed dataset with subsequent\npreference optimization. Extensive experiments on popular benchmarks\ndemonstrate that our method significantly improves the alignment between\nverbalized confidence and response correctness while maintaining or enhancing\noverall task performance. These results highlight the potential of semantic\nperturbation as a practical tool for improving the reliability and\ninterpretability of VLMs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCSP\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u8bed\u4e49\u6270\u52a8\u6539\u5584\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684\u7f6e\u4fe1\u5ea6\u6821\u51c6\uff0c\u63d0\u5347\u5176\u53ef\u9760\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "VLM\u5728\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u7f6e\u4fe1\u5ea6\u6821\u51c6\u4e0d\u4f73\uff0c\u5bfc\u81f4\u7528\u6237\u4fe1\u4efb\u5ea6\u4e0b\u964d\uff0c\u5c24\u5176\u662f\u5728\u6a21\u578b\u9519\u8bef\u6216\u865a\u6784\u4fe1\u606f\u65f6\u3002", "method": "\u5f15\u5165\u9ad8\u65af\u566a\u58f0\u6270\u52a8\u5173\u952e\u5bf9\u8c61\u533a\u57df\uff0c\u6a21\u62df\u89c6\u89c9\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\uff08\u76d1\u7763\u5fae\u8c03\u548c\u504f\u597d\u4f18\u5316\uff09\u589e\u5f3a\u6821\u51c6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u6539\u5584\u4e86\u7f6e\u4fe1\u5ea6\u4e0e\u54cd\u5e94\u6b63\u786e\u6027\u7684\u5bf9\u9f50\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u5347\u4e86\u4efb\u52a1\u6027\u80fd\u3002", "conclusion": "\u8bed\u4e49\u6270\u52a8\u662f\u63d0\u5347VLM\u53ef\u9760\u6027\u548c\u53ef\u89e3\u91ca\u6027\u7684\u5b9e\u7528\u5de5\u5177\u3002"}}
{"id": "2504.14860", "pdf": "https://arxiv.org/pdf/2504.14860", "abs": "https://arxiv.org/abs/2504.14860", "authors": ["Ziyi Liu", "Yangcen Liu"], "title": "Bridge the Gap: From Weak to Full Supervision for Temporal Action Localization with PseudoFormer", "categories": ["cs.CV", "cs.AI"], "comment": "CVPR 2025: IEEE Conference on Computer Vision and Pattern Recognition", "summary": "Weakly-supervised Temporal Action Localization (WTAL) has achieved notable\nsuccess but still suffers from a lack of temporal annotations, leading to a\nperformance and framework gap compared with fully-supervised methods. While\nrecent approaches employ pseudo labels for training, three key challenges:\ngenerating high-quality pseudo labels, making full use of different priors, and\noptimizing training methods with noisy labels remain unresolved. Due to these\nperspectives, we propose PseudoFormer, a novel two-branch framework that\nbridges the gap between weakly and fully-supervised Temporal Action\nLocalization (TAL). We first introduce RickerFusion, which maps all predicted\naction proposals to a global shared space to generate pseudo labels with better\nquality. Subsequently, we leverage both snippet-level and proposal-level labels\nwith different priors from the weak branch to train the regression-based model\nin the full branch. Finally, the uncertainty mask and iterative refinement\nmechanism are applied for training with noisy pseudo labels. PseudoFormer\nachieves state-of-the-art WTAL results on the two commonly used benchmarks,\nTHUMOS14 and ActivityNet1.3. Besides, extensive ablation studies demonstrate\nthe contribution of each component of our method.", "AI": {"tldr": "PseudoFormer\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u53cc\u5206\u652f\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u6210\u9ad8\u8d28\u91cf\u4f2a\u6807\u7b7e\u548c\u5229\u7528\u4e0d\u540c\u5148\u9a8c\u4fe1\u606f\uff0c\u7f29\u5c0f\u4e86\u5f31\u76d1\u7763\u4e0e\u5168\u76d1\u7763\u65f6\u95f4\u52a8\u4f5c\u5b9a\u4f4d\u7684\u6027\u80fd\u5dee\u8ddd\u3002", "motivation": "\u5f31\u76d1\u7763\u65f6\u95f4\u52a8\u4f5c\u5b9a\u4f4d\uff08WTAL\uff09\u56e0\u7f3a\u4e4f\u65f6\u95f4\u6807\u6ce8\u800c\u6027\u80fd\u53d7\u9650\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u4f2a\u6807\u7b7e\u8d28\u91cf\u3001\u5148\u9a8c\u5229\u7528\u548c\u566a\u58f0\u6807\u7b7e\u8bad\u7ec3\u65b9\u9762\u5b58\u5728\u6311\u6218\u3002", "method": "\u63d0\u51faPseudoFormer\u6846\u67b6\uff0c\u5305\u62ecRickerFusion\u751f\u6210\u9ad8\u8d28\u91cf\u4f2a\u6807\u7b7e\uff0c\u5229\u7528\u7247\u6bb5\u7ea7\u548c\u63d0\u8bae\u7ea7\u6807\u7b7e\u8bad\u7ec3\u56de\u5f52\u6a21\u578b\uff0c\u5e76\u5f15\u5165\u4e0d\u786e\u5b9a\u6027\u63a9\u7801\u548c\u8fed\u4ee3\u4f18\u5316\u673a\u5236\u3002", "result": "\u5728THUMOS14\u548cActivityNet1.3\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u4f18\u6027\u80fd\uff0c\u6d88\u878d\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5404\u6a21\u5757\u7684\u6709\u6548\u6027\u3002", "conclusion": "PseudoFormer\u901a\u8fc7\u6539\u8fdb\u4f2a\u6807\u7b7e\u751f\u6210\u548c\u8bad\u7ec3\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5f31\u76d1\u7763\u65f6\u95f4\u52a8\u4f5c\u5b9a\u4f4d\u7684\u6027\u80fd\u3002"}}
{"id": "2504.14913", "pdf": "https://arxiv.org/pdf/2504.14913", "abs": "https://arxiv.org/abs/2504.14913", "authors": ["Kenji Iwata", "Eiki Ishidera", "Toshifumi Yamaai", "Yutaka Satoh", "Hiroshi Tanaka", "Katsuhiko Takahashi", "Akio Furuhata", "Yoshihisa Tanabe", "Hiroshi Matsumura"], "title": "Guidelines for External Disturbance Factors in the Use of OCR in Real-World Environments", "categories": ["cs.CV", "cs.AI", "I.5.2; I.5.m"], "comment": "16 pages, 14 figures", "summary": "The performance of OCR has improved with the evolution of AI technology. As\nOCR continues to broaden its range of applications, the increased likelihood of\ninterference introduced by various usage environments can prevent it from\nachieving its inherent performance. This results in reduced recognition\naccuracy under certain conditions, and makes the quality control of recognition\ndevices more challenging. Therefore, to ensure that users can properly utilize\nOCR, we compiled the real-world external disturbance factors that cause\nperformance degradation, along with the resulting image degradation phenomena,\ninto an external disturbance factor table and, by also indicating how to make\nuse of it, organized them into guidelines.", "AI": {"tldr": "\u8bba\u6587\u603b\u7ed3\u4e86OCR\u6027\u80fd\u53d7\u5916\u90e8\u5e72\u6270\u56e0\u7d20\u5f71\u54cd\u7684\u95ee\u9898\uff0c\u5e76\u6574\u7406\u4e86\u5e72\u6270\u56e0\u7d20\u8868\u548c\u5e94\u5bf9\u6307\u5357\u3002", "motivation": "\u968f\u7740OCR\u5e94\u7528\u8303\u56f4\u6269\u5927\uff0c\u5916\u90e8\u5e72\u6270\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\uff0c\u9700\u63d0\u4f9b\u89e3\u51b3\u65b9\u6848\u4ee5\u786e\u4fdd\u7528\u6237\u6b63\u786e\u4f7f\u7528OCR\u3002", "method": "\u6574\u7406\u73b0\u5b9e\u4e2d\u7684\u5916\u90e8\u5e72\u6270\u56e0\u7d20\u53ca\u5176\u5bfc\u81f4\u7684\u56fe\u50cf\u9000\u5316\u73b0\u8c61\uff0c\u5f62\u6210\u5e72\u6270\u56e0\u7d20\u8868\uff0c\u5e76\u5236\u5b9a\u4f7f\u7528\u6307\u5357\u3002", "result": "\u63d0\u51fa\u4e86\u5916\u90e8\u5e72\u6270\u56e0\u7d20\u8868\u548c\u5e94\u5bf9\u6307\u5357\uff0c\u5e2e\u52a9\u7528\u6237\u4f18\u5316OCR\u4f7f\u7528\u73af\u5883\u3002", "conclusion": "\u901a\u8fc7\u6574\u7406\u5e72\u6270\u56e0\u7d20\u548c\u63d0\u4f9b\u6307\u5357\uff0c\u53ef\u6709\u6548\u63d0\u5347OCR\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u6027\u80fd\u8868\u73b0\u3002"}}
{"id": "2504.14915", "pdf": "https://arxiv.org/pdf/2504.14915", "abs": "https://arxiv.org/abs/2504.14915", "authors": ["Yeona Hong", "Hyewon Han", "Woo-jin Chung", "Hong-Goo Kang"], "title": "StableQuant: Layer Adaptive Post-Training Quantization for Speech Foundation Models", "categories": ["eess.AS", "cs.AI"], "comment": "Accepted at ICASSP 2025", "summary": "In this paper, we propose StableQuant, a novel adaptive post-training\nquantization (PTQ) algorithm for widely used speech foundation models (SFMs).\nWhile PTQ has been successfully employed for compressing large language models\n(LLMs) due to its ability to bypass additional fine-tuning, directly applying\nthese techniques to SFMs may not yield optimal results, as SFMs utilize\ndistinct network architecture for feature extraction. StableQuant demonstrates\noptimal quantization performance regardless of the network architecture type,\nas it adaptively determines the quantization range for each layer by analyzing\nboth the scale distributions and overall performance. We evaluate our algorithm\non two SFMs, HuBERT and wav2vec2.0, for an automatic speech recognition (ASR)\ntask, and achieve superior performance compared to traditional PTQ methods.\nStableQuant successfully reduces the sizes of SFM models to a quarter and\ndoubles the inference speed while limiting the word error rate (WER)\nperformance drop to less than 0.3% with 8-bit quantization.", "AI": {"tldr": "StableQuant\u662f\u4e00\u79cd\u65b0\u578b\u81ea\u9002\u5e94\u540e\u8bad\u7ec3\u91cf\u5316\u7b97\u6cd5\uff0c\u4e13\u4e3a\u8bed\u97f3\u57fa\u7840\u6a21\u578b\u8bbe\u8ba1\uff0c\u80fd\u81ea\u9002\u5e94\u786e\u5b9a\u6bcf\u5c42\u7684\u91cf\u5316\u8303\u56f4\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u540e\u8bad\u7ec3\u91cf\u5316\u65b9\u6cd5\u76f4\u63a5\u5e94\u7528\u4e8e\u8bed\u97f3\u57fa\u7840\u6a21\u578b\u53ef\u80fd\u6548\u679c\u4e0d\u4f73\uff0c\u56e0\u5176\u7f51\u7edc\u67b6\u6784\u4e0e\u8bed\u8a00\u6a21\u578b\u4e0d\u540c\u3002", "method": "\u901a\u8fc7\u5206\u6790\u5c3a\u5ea6\u5206\u5e03\u548c\u6574\u4f53\u6027\u80fd\uff0c\u81ea\u9002\u5e94\u786e\u5b9a\u6bcf\u5c42\u91cf\u5316\u8303\u56f4\u3002", "result": "\u5728HuBERT\u548cwav2vec2.0\u4e0a\u6d4b\u8bd5\uff0c\u6a21\u578b\u5927\u5c0f\u7f29\u51cf\u81f31/4\uff0c\u63a8\u7406\u901f\u5ea6\u7ffb\u500d\uff0c\u8bcd\u9519\u8bef\u7387\u4ec5\u4e0b\u964d0.3%\u3002", "conclusion": "StableQuant\u5728\u8bed\u97f3\u57fa\u7840\u6a21\u578b\u91cf\u5316\u4e2d\u8868\u73b0\u4f18\u8d8a\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u7f51\u7edc\u67b6\u6784\u3002"}}
{"id": "2504.14921", "pdf": "https://arxiv.org/pdf/2504.14921", "abs": "https://arxiv.org/abs/2504.14921", "authors": ["Songping Wang", "Hanqing Liu", "Yueming Lyu", "Xiantao Hu", "Ziwen He", "Wei Wang", "Caifeng Shan", "Liang Wang"], "title": "Fast Adversarial Training with Weak-to-Strong Spatial-Temporal Consistency in the Frequency Domain on Videos", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Adversarial Training (AT) has been shown to significantly enhance adversarial\nrobustness via a min-max optimization approach. However, its effectiveness in\nvideo recognition tasks is hampered by two main challenges. First, fast\nadversarial training for video models remains largely unexplored, which\nseverely impedes its practical applications. Specifically, most video\nadversarial training methods are computationally costly, with long training\ntimes and high expenses. Second, existing methods struggle with the trade-off\nbetween clean accuracy and adversarial robustness. To address these challenges,\nwe introduce Video Fast Adversarial Training with Weak-to-Strong consistency\n(VFAT-WS), the first fast adversarial training method for video data.\nSpecifically, VFAT-WS incorporates the following key designs: First, it\nintegrates a straightforward yet effective temporal frequency augmentation\n(TF-AUG), and its spatial-temporal enhanced form STF-AUG, along with a\nsingle-step PGD attack to boost training efficiency and robustness. Second, it\ndevises a weak-to-strong spatial-temporal consistency regularization, which\nseamlessly integrates the simpler TF-AUG and the more complex STF-AUG.\nLeveraging the consistency regularization, it steers the learning process from\nsimple to complex augmentations. Both of them work together to achieve a better\ntrade-off between clean accuracy and robustness. Extensive experiments on\nUCF-101 and HMDB-51 with both CNN and Transformer-based models demonstrate that\nVFAT-WS achieves great improvements in adversarial robustness and corruption\nrobustness, while accelerating training by nearly 490%.", "AI": {"tldr": "VFAT-WS\u662f\u4e00\u79cd\u9488\u5bf9\u89c6\u9891\u6570\u636e\u7684\u5feb\u901f\u5bf9\u6297\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u65f6\u95f4\u9891\u7387\u589e\u5f3a\u548c\u5f31\u5230\u5f3a\u4e00\u81f4\u6027\u6b63\u5219\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5bf9\u6297\u9c81\u68d2\u6027\u548c\u8bad\u7ec3\u6548\u7387\u3002", "motivation": "\u89c6\u9891\u5bf9\u6297\u8bad\u7ec3\u9762\u4e34\u8ba1\u7b97\u6210\u672c\u9ad8\u548c\u6e05\u6d01\u51c6\u786e\u6027\u4e0e\u5bf9\u6297\u9c81\u68d2\u6027\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\uff0cVFAT-WS\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\u3002", "method": "\u7ed3\u5408\u65f6\u95f4\u9891\u7387\u589e\u5f3a\uff08TF-AUG\uff09\u53ca\u5176\u65f6\u7a7a\u589e\u5f3a\u5f62\u5f0f\uff08STF-AUG\uff09\uff0c\u4ee5\u53ca\u5355\u6b65PGD\u653b\u51fb\uff0c\u540c\u65f6\u91c7\u7528\u5f31\u5230\u5f3a\u65f6\u7a7a\u4e00\u81f4\u6027\u6b63\u5219\u5316\u3002", "result": "\u5728UCF-101\u548cHMDB-51\u6570\u636e\u96c6\u4e0a\uff0cVFAT-WS\u663e\u8457\u63d0\u5347\u4e86\u5bf9\u6297\u9c81\u68d2\u6027\u548c\u8bad\u7ec3\u6548\u7387\uff08\u52a0\u901f490%\uff09\u3002", "conclusion": "VFAT-WS\u5728\u89c6\u9891\u5bf9\u6297\u8bad\u7ec3\u4e2d\u5b9e\u73b0\u4e86\u6e05\u6d01\u51c6\u786e\u6027\u4e0e\u5bf9\u6297\u9c81\u68d2\u6027\u7684\u66f4\u597d\u5e73\u8861\uff0c\u540c\u65f6\u5927\u5e45\u63d0\u9ad8\u4e86\u8bad\u7ec3\u6548\u7387\u3002"}}
{"id": "2504.14936", "pdf": "https://arxiv.org/pdf/2504.14936", "abs": "https://arxiv.org/abs/2504.14936", "authors": ["Maria Fay", "Frederik F. Fl\u00f6ther"], "title": "Giving AI a voice: how does AI think it should be treated?", "categories": ["cs.CY", "cs.AI"], "comment": null, "summary": "With the astounding progress in (generative) artificial intelligence (AI),\nthere has been significant public discourse regarding regulation and ethics of\nthe technology. Is it sufficient when humans discuss this with other humans?\nOr, given that AI is increasingly becoming a viable source of inspiration for\npeople (and let alone the hypothetical possibility that the technology may at\nsome point become \"artificial general intelligence\" and/or develop\nconsciousness), should AI not join the discourse? There are new questions and\nangles that AI brings to the table that we might not have considered before -\nso let us make the key subject of this book an active participant. This chapter\ntherefore includes a brief human-AI conversation on the topic of AI rights and\nethics.", "AI": {"tldr": "\u63a2\u8ba8AI\u662f\u5426\u5e94\u53c2\u4e0e\u5173\u4e8e\u5176\u6743\u5229\u548c\u4f26\u7406\u7684\u8ba8\u8bba\uff0c\u5e76\u5c55\u793a\u4e00\u6bb5\u4eba\u673a\u5bf9\u8bdd\u3002", "motivation": "\u968f\u7740AI\u6280\u672f\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u516c\u4f17\u5bf9\u5176\u4f26\u7406\u548c\u76d1\u7ba1\u7684\u8ba8\u8bba\u65e5\u76ca\u589e\u591a\u3002\u4f5c\u8005\u8ba4\u4e3aAI\u53ef\u80fd\u5e26\u6765\u65b0\u7684\u89c6\u89d2\uff0c\u56e0\u6b64\u5e94\u6210\u4e3a\u8ba8\u8bba\u7684\u53c2\u4e0e\u8005\u3002", "method": "\u901a\u8fc7\u4e00\u6bb5\u4eba\u7c7b\u4e0eAI\u7684\u5bf9\u8bdd\uff0c\u63a2\u8ba8AI\u6743\u5229\u548c\u4f26\u7406\u95ee\u9898\u3002", "result": "\u5c55\u793a\u4e86AI\u53c2\u4e0e\u4f26\u7406\u8ba8\u8bba\u7684\u6f5c\u5728\u4ef7\u503c\u548c\u65b0\u89c6\u89d2\u3002", "conclusion": "AI\u5e94\u88ab\u7eb3\u5165\u4f26\u7406\u548c\u6743\u5229\u8ba8\u8bba\uff0c\u4ee5\u63d0\u4f9b\u66f4\u5168\u9762\u7684\u89c6\u89d2\u3002"}}
{"id": "2504.14985", "pdf": "https://arxiv.org/pdf/2504.14985", "abs": "https://arxiv.org/abs/2504.14985", "authors": ["Fatih Deniz", "Dorde Popovic", "Yazan Boshmaf", "Euisuh Jeong", "Minhaj Ahmad", "Sanjay Chawla", "Issa Khalil"], "title": "aiXamine: LLM Safety and Security Simplified", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Evaluating Large Language Models (LLMs) for safety and security remains a\ncomplex task, often requiring users to navigate a fragmented landscape of ad\nhoc benchmarks, datasets, metrics, and reporting formats. To address this\nchallenge, we present aiXamine, a comprehensive black-box evaluation platform\nfor LLM safety and security. aiXamine integrates over 40 tests (i.e.,\nbenchmarks) organized into eight key services targeting specific dimensions of\nsafety and security: adversarial robustness, code security, fairness and bias,\nhallucination, model and data privacy, out-of-distribution (OOD) robustness,\nover-refusal, and safety alignment. The platform aggregates the evaluation\nresults into a single detailed report per model, providing a detailed breakdown\nof model performance, test examples, and rich visualizations. We used aiXamine\nto assess over 50 publicly available and proprietary LLMs, conducting over 2K\nexaminations. Our findings reveal notable vulnerabilities in leading models,\nincluding susceptibility to adversarial attacks in OpenAI's GPT-4o, biased\noutputs in xAI's Grok-3, and privacy weaknesses in Google's Gemini 2.0.\nAdditionally, we observe that open-source models can match or exceed\nproprietary models in specific services such as safety alignment, fairness and\nbias, and OOD robustness. Finally, we identify trade-offs between distillation\nstrategies, model size, training methods, and architectural choices.", "AI": {"tldr": "aiXamine\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5b89\u5168\u6027\u548c\u5b89\u5168\u6027\u7684\u9ed1\u76d2\u5e73\u53f0\uff0c\u6574\u5408\u4e8640\u591a\u4e2a\u6d4b\u8bd5\uff0c\u8986\u76d68\u4e2a\u5173\u952e\u7ef4\u5ea6\uff0c\u8bc4\u4f30\u4e8650\u591a\u4e2a\u6a21\u578b\uff0c\u53d1\u73b0\u4e86\u4e00\u4e9b\u9886\u5148\u6a21\u578b\u7684\u6f0f\u6d1e\u548c\u5f00\u6e90\u6a21\u578b\u7684\u4f18\u52bf\u3002", "motivation": "\u8bc4\u4f30LLM\u7684\u5b89\u5168\u6027\u548c\u5b89\u5168\u6027\u662f\u4e00\u4e2a\u590d\u6742\u4e14\u5206\u6563\u7684\u4efb\u52a1\uff0c\u9700\u8981\u7edf\u4e00\u7684\u5e73\u53f0\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "aiXamine\u5e73\u53f0\u6574\u5408\u4e8640\u591a\u4e2a\u6d4b\u8bd5\uff0c\u5206\u4e3a8\u4e2a\u5173\u952e\u670d\u52a1\u7ef4\u5ea6\uff0c\u751f\u6210\u8be6\u7ec6\u62a5\u544a\u548c\u53ef\u89c6\u5316\u7ed3\u679c\u3002", "result": "\u8bc4\u4f30\u4e8650\u591a\u4e2a\u6a21\u578b\uff0c\u53d1\u73b0GPT-4o\u6613\u53d7\u5bf9\u6297\u653b\u51fb\uff0cGrok-3\u5b58\u5728\u504f\u89c1\u8f93\u51fa\uff0cGemini 2.0\u6709\u9690\u79c1\u95ee\u9898\uff1b\u5f00\u6e90\u6a21\u578b\u5728\u67d0\u4e9b\u65b9\u9762\u4f18\u4e8e\u4e13\u6709\u6a21\u578b\u3002", "conclusion": "aiXamine\u4e3aLLM\u5b89\u5168\u6027\u548c\u5b89\u5168\u6027\u8bc4\u4f30\u63d0\u4f9b\u4e86\u7edf\u4e00\u5de5\u5177\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u7684\u6f0f\u6d1e\u548c\u5f00\u6e90\u6a21\u578b\u7684\u6f5c\u529b\uff0c\u540c\u65f6\u6307\u51fa\u4e86\u6a21\u578b\u8bbe\u8ba1\u4e2d\u7684\u6743\u8861\u3002"}}
{"id": "2504.15035", "pdf": "https://arxiv.org/pdf/2504.15035", "abs": "https://arxiv.org/abs/2504.15035", "authors": ["Yue Li", "Weizhi Liu", "Dongdong Lin"], "title": "SOLIDO: A Robust Watermarking Method for Speech Synthesis via Low-Rank Adaptation", "categories": ["cs.CR", "cs.AI", "cs.SD"], "comment": null, "summary": "The accelerated advancement of speech generative models has given rise to\nsecurity issues, including model infringement and unauthorized abuse of\ncontent. Although existing generative watermarking techniques have proposed\ncorresponding solutions, most methods require substantial computational\noverhead and training costs. In addition, some methods have limitations in\nrobustness when handling variable-length inputs. To tackle these challenges, we\npropose \\textsc{SOLIDO}, a novel generative watermarking method that integrates\nparameter-efficient fine-tuning with speech watermarking through low-rank\nadaptation (LoRA) for speech diffusion models. Concretely, the watermark\nencoder converts the watermark to align with the input of diffusion models. To\nachieve precise watermark extraction from variable-length inputs, the watermark\ndecoder based on depthwise separable convolution is designed for watermark\nrecovery. To further enhance speech generation performance and watermark\nextraction capability, we propose a speech-driven lightweight fine-tuning\nstrategy, which reduces computational overhead through LoRA. Comprehensive\nexperiments demonstrate that the proposed method ensures high-fidelity\nwatermarked speech even at a large capacity of 2000 bps. Furthermore, against\ncommon individual and compound speech attacks, our SOLIDO achieves a maximum\naverage extraction accuracy of 99.20\\% and 98.43\\%, respectively. It surpasses\nother state-of-the-art methods by nearly 23\\% in resisting time-stretching\nattacks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSOLIDO\u7684\u65b0\u578b\u8bed\u97f3\u751f\u6210\u6c34\u5370\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f4e\u79e9\u9002\u5e94\uff08LoRA\uff09\u7ed3\u5408\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u8ba1\u7b97\u5f00\u9500\u5927\u3001\u8bad\u7ec3\u6210\u672c\u9ad8\u53ca\u53d8\u957f\u8f93\u5165\u9c81\u68d2\u6027\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u8bed\u97f3\u751f\u6210\u6a21\u578b\u7684\u5feb\u901f\u53d1\u5c55\u5e26\u6765\u4e86\u5b89\u5168\u98ce\u9669\uff0c\u5982\u6a21\u578b\u4fb5\u6743\u548c\u5185\u5bb9\u6ee5\u7528\u3002\u73b0\u6709\u6c34\u5370\u6280\u672f\u5b58\u5728\u8ba1\u7b97\u5f00\u9500\u5927\u3001\u8bad\u7ec3\u6210\u672c\u9ad8\u53ca\u5bf9\u53d8\u957f\u8f93\u5165\u9c81\u68d2\u6027\u4e0d\u8db3\u7684\u5c40\u9650\u6027\u3002", "method": "SOLIDO\u7ed3\u5408\u4e86LoRA\u548c\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff0c\u8bbe\u8ba1\u4e86\u57fa\u4e8e\u6df1\u5ea6\u53ef\u5206\u79bb\u5377\u79ef\u7684\u6c34\u5370\u89e3\u7801\u5668\uff0c\u5e76\u63d0\u51fa\u4e86\u8bed\u97f3\u9a71\u52a8\u7684\u8f7b\u91cf\u7ea7\u5fae\u8c03\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSOLIDO\u57282000 bps\u5bb9\u91cf\u4e0b\u4ecd\u80fd\u4fdd\u6301\u9ad8\u4fdd\u771f\u6c34\u5370\u8bed\u97f3\uff0c\u5bf9\u6297\u5e38\u89c1\u653b\u51fb\u7684\u5e73\u5747\u63d0\u53d6\u51c6\u786e\u7387\u6700\u9ad8\u8fbe99.20%\u548c98.43%\uff0c\u5728\u6297\u65f6\u95f4\u62c9\u4f38\u653b\u51fb\u4e0a\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\u8fd123%\u3002", "conclusion": "SOLIDO\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u9c81\u68d2\u7684\u8bed\u97f3\u751f\u6210\u6c34\u5370\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6c34\u5370\u5bb9\u91cf\u548c\u6297\u653b\u51fb\u80fd\u529b\u3002"}}
{"id": "2504.15041", "pdf": "https://arxiv.org/pdf/2504.15041", "abs": "https://arxiv.org/abs/2504.15041", "authors": ["Shiben Liu", "Huijie Fan", "Qiang Wang", "Baojie Fan", "Yandong Tang", "Liangqiong Qu"], "title": "Distribution-aware Forgetting Compensation for Exemplar-Free Lifelong Person Re-identification", "categories": ["cs.CV", "cs.AI"], "comment": "12 pages, 5 figures", "summary": "Lifelong Person Re-identification (LReID) suffers from a key challenge in\npreserving old knowledge while adapting to new information. The existing\nsolutions include rehearsal-based and rehearsal-free methods to address this\nchallenge. Rehearsal-based approaches rely on knowledge distillation,\ncontinuously accumulating forgetting during the distillation process.\nRehearsal-free methods insufficiently learn the distribution of each domain,\nleading to forgetfulness over time. To solve these issues, we propose a novel\nDistribution-aware Forgetting Compensation (DAFC) model that explores\ncross-domain shared representation learning and domain-specific distribution\nintegration without using old exemplars or knowledge distillation. We propose a\nText-driven Prompt Aggregation (TPA) that utilizes text features to enrich\nprompt elements and guide the prompt model to learn fine-grained\nrepresentations for each instance. This can enhance the differentiation of\nidentity information and establish the foundation for domain distribution\nawareness. Then, Distribution-based Awareness and Integration (DAI) is designed\nto capture each domain-specific distribution by a dedicated expert network and\nadaptively consolidate them into a shared region in high-dimensional space. In\nthis manner, DAI can consolidate and enhance cross-domain shared representation\nlearning while alleviating catastrophic forgetting. Furthermore, we develop a\nKnowledge Consolidation Mechanism (KCM) that comprises instance-level\ndiscrimination and cross-domain consistency alignment strategies to facilitate\nmodel adaptive learning of new knowledge from the current domain and promote\nknowledge consolidation learning between acquired domain-specific\ndistributions, respectively. Experimental results show that our DAFC outperform\nstate-of-the-art methods by at least 9.8\\%/6.6\\% and 6.4\\%/6.2\\% of average\nmAP/R@1 on two training orders.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDAFC\u7684\u65b0\u6a21\u578b\uff0c\u901a\u8fc7\u6587\u672c\u9a71\u52a8\u7684\u63d0\u793a\u805a\u5408\u548c\u5206\u5e03\u611f\u77e5\u96c6\u6210\uff0c\u89e3\u51b3\u4e86LReID\u4e2d\u7684\u65e7\u77e5\u8bc6\u4fdd\u7559\u548c\u65b0\u4fe1\u606f\u9002\u5e94\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3LReID\u4e2d\u65e7\u77e5\u8bc6\u9057\u5fd8\u548c\u65b0\u4fe1\u606f\u9002\u5e94\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\uff08\u57fa\u4e8e\u6392\u7ec3\u548c\u65e0\u6392\u7ec3\uff09\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51faDAFC\u6a21\u578b\uff0c\u7ed3\u5408\u6587\u672c\u9a71\u52a8\u7684\u63d0\u793a\u805a\u5408\uff08TPA\uff09\u548c\u5206\u5e03\u611f\u77e5\u96c6\u6210\uff08DAI\uff09\uff0c\u5e76\u5f15\u5165\u77e5\u8bc6\u5de9\u56fa\u673a\u5236\uff08KCM\uff09\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDAFC\u5728\u4e24\u79cd\u8bad\u7ec3\u987a\u5e8f\u4e0b\u5e73\u5747mAP/R@1\u5206\u522b\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u81f3\u5c119.8%/6.6%\u548c6.4%/6.2%\u3002", "conclusion": "DAFC\u901a\u8fc7\u5206\u5e03\u611f\u77e5\u548c\u77e5\u8bc6\u5de9\u56fa\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86LReID\u4e2d\u7684\u9057\u5fd8\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u8de8\u57df\u5171\u4eab\u8868\u793a\u5b66\u4e60\u3002"}}
{"id": "2504.15044", "pdf": "https://arxiv.org/pdf/2504.15044", "abs": "https://arxiv.org/abs/2504.15044", "authors": ["Benshan Wang", "Qiarong Xiao", "Tengji Xu", "Li Fan", "Shaojie Liu", "Jianji Dong", "Junwen Zhang", "Chaoran Huang"], "title": "Beyond Terabit/s Integrated Neuromorphic Photonic Processor for DSP-Free Optical Interconnects", "categories": ["physics.optics", "cs.AI", "cs.ET"], "comment": "22 pages, 6 figures", "summary": "The rapid expansion of generative AI drives unprecedented demands for\nhigh-performance computing. Training large-scale AI models now requires vast\ninterconnected GPU clusters across multiple data centers. Multi-scale AI\ntraining and inference demand uniform, ultra-low latency, and energy-efficient\nlinks to enable massive GPUs to function as a single cohesive unit. However,\ntraditional electrical and optical interconnects, relying on conventional\ndigital signal processors (DSPs) for signal distortion compensation,\nincreasingly fail to meet these stringent requirements. To overcome these\nlimitations, we present an integrated neuromorphic optical signal processor\n(OSP) that leverages deep reservoir computing and achieves DSP-free,\nall-optical, real-time processing. Experimentally, our OSP achieves a 100 Gbaud\nPAM4 per lane, 1.6 Tbit/s data center interconnect over a 5 km optical fiber in\nthe C-band (equivalent to over 80 km in the O-band), far exceeding the reach of\nstate-of-the-art DSP solutions, which are fundamentally constrained by\nchromatic dispersion in IMDD systems. Simultaneously, it reduces processing\nlatency by four orders of magnitude and energy consumption by three orders of\nmagnitude. Unlike DSPs, which introduce increased latency at high data rates,\nour OSP maintains consistent, ultra-low latency regardless of data rate\nscaling, making it ideal for future optical interconnects. Moreover, the OSP\nretains full optical field information for better impairment compensation and\nadapts to various modulation formats, data rates, and wavelengths. Fabricated\nusing a mature silicon photonic process, the OSP can be monolithically\nintegrated with silicon photonic transceivers, enhancing the compactness and\nreliability of all-optical interconnects. This research provides a highly\nscalable, energy-efficient, and high-speed solution, paving the way for\nnext-generation AI infrastructure.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u50a8\u5c42\u8ba1\u7b97\u7684\u795e\u7ecf\u5f62\u6001\u5149\u5b66\u4fe1\u53f7\u5904\u7406\u5668\uff08OSP\uff09\uff0c\u7528\u4e8e\u9ad8\u6027\u80fd\u8ba1\u7b97\u4e2d\u7684\u5149\u5b66\u4e92\u8fde\uff0c\u663e\u8457\u63d0\u5347\u4e86\u901f\u5ea6\u3001\u5ef6\u8fdf\u548c\u80fd\u6548\u3002", "motivation": "\u751f\u6210\u5f0fAI\u7684\u5feb\u901f\u53d1\u5c55\u5bf9\u9ad8\u6027\u80fd\u8ba1\u7b97\u63d0\u51fa\u4e86\u6781\u9ad8\u8981\u6c42\uff0c\u4f20\u7edf\u7535\u5b66\u548c\u5149\u5b66\u4e92\u8fde\u6280\u672f\u96be\u4ee5\u6ee1\u8db3\u4f4e\u5ef6\u8fdf\u3001\u9ad8\u80fd\u6548\u7684\u9700\u6c42\u3002", "method": "\u91c7\u7528\u96c6\u6210\u795e\u7ecf\u5f62\u6001\u5149\u5b66\u4fe1\u53f7\u5904\u7406\u5668\uff08OSP\uff09\uff0c\u5229\u7528\u6df1\u5ea6\u50a8\u5c42\u8ba1\u7b97\u5b9e\u73b0\u65e0DSP\u7684\u5168\u5149\u5b66\u5b9e\u65f6\u5904\u7406\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cOSP\u57285\u516c\u91ccC\u6ce2\u6bb5\u5149\u7ea4\u4e0a\u5b9e\u73b0\u4e86100 Gbaud PAM4\u6bcf\u901a\u9053\u30011.6 Tbit/s\u7684\u6570\u636e\u4e2d\u5fc3\u4e92\u8fde\uff0c\u5ef6\u8fdf\u548c\u80fd\u8017\u5206\u522b\u964d\u4f4e\u4e86\u56db\u4e2a\u548c\u4e09\u4e2a\u6570\u91cf\u7ea7\u3002", "conclusion": "OSP\u4e3a\u4e0b\u4e00\u4ee3AI\u57fa\u7840\u8bbe\u65bd\u63d0\u4f9b\u4e86\u9ad8\u5ea6\u53ef\u6269\u5c55\u3001\u9ad8\u80fd\u6548\u548c\u9ad8\u901f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.15062", "pdf": "https://arxiv.org/pdf/2504.15062", "abs": "https://arxiv.org/abs/2504.15062", "authors": ["Egon Per\u0161ak", "Miguel F. Anjos"], "title": "OPO: Making Decision-Focused Data Acquisition Decisions", "categories": ["math.OC", "cs.AI"], "comment": null, "summary": "We propose a model for making data acquisition decisions for variables in\ncontextual stochastic optimisation problems. Data acquisition decisions are\ntypically treated as separate and fixed. We explore problem settings in which\nthe acquisition of contextual variables is costly and consequently constrained.\nThe data acquisition problem is often solved heuristically for proxy objectives\nsuch as coverage. The more intuitive objective is the downstream decision\nquality as a result of data acquisition decisions. The whole pipeline can be\ncharacterised as an optimise-then-predict-then-optimise (OPO) problem.\nAnalogously, much recent research has focused on how to integrate prediction\nand optimisation (PO) in the form of decision-focused learning. We propose\nleveraging differentiable optimisation to extend the integration to data\nacquisition. We solve the data acquisition problem with well-defined\nconstraints by learning a surrogate linear objective function. We demonstrate\nan application of this model on a shortest path problem for which we first have\nto set a drone reconnaissance strategy to capture image segments serving as\ninputs to a model that predicts travel costs. We ablate the problem with a\nnumber of training modalities and demonstrate that the differentiable\noptimisation approach outperforms random search strategies.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u5728\u4e0a\u4e0b\u6587\u968f\u673a\u4f18\u5316\u95ee\u9898\u4e2d\u505a\u51fa\u6570\u636e\u83b7\u53d6\u51b3\u7b56\u7684\u6a21\u578b\uff0c\u91cd\u70b9\u5173\u6ce8\u6210\u672c\u7ea6\u675f\u4e0b\u7684\u6570\u636e\u83b7\u53d6\uff0c\u5e76\u901a\u8fc7\u53ef\u5fae\u5206\u4f18\u5316\u63d0\u5347\u51b3\u7b56\u8d28\u91cf\u3002", "motivation": "\u4f20\u7edf\u6570\u636e\u83b7\u53d6\u51b3\u7b56\u901a\u5e38\u72ec\u7acb\u4e14\u56fa\u5b9a\uff0c\u800c\u5b9e\u9645\u4e2d\u83b7\u53d6\u4e0a\u4e0b\u6587\u53d8\u91cf\u53ef\u80fd\u6210\u672c\u9ad8\u6602\u4e14\u53d7\u9650\u3002\u7814\u7a76\u65e8\u5728\u4f18\u5316\u6570\u636e\u83b7\u53d6\u4ee5\u63d0\u5347\u4e0b\u6e38\u51b3\u7b56\u8d28\u91cf\u3002", "method": "\u5229\u7528\u53ef\u5fae\u5206\u4f18\u5316\u6269\u5c55\u9884\u6d4b\u4e0e\u4f18\u5316\u7684\u6574\u5408\uff0c\u5b66\u4e60\u66ff\u4ee3\u7ebf\u6027\u76ee\u6807\u51fd\u6570\uff0c\u89e3\u51b3\u5177\u6709\u660e\u786e\u7ea6\u675f\u7684\u6570\u636e\u83b7\u53d6\u95ee\u9898\u3002", "result": "\u5728\u65e0\u4eba\u673a\u4fa6\u5bdf\u6700\u77ed\u8def\u5f84\u95ee\u9898\u4e2d\uff0c\u53ef\u5fae\u5206\u4f18\u5316\u65b9\u6cd5\u4f18\u4e8e\u968f\u673a\u641c\u7d22\u7b56\u7565\u3002", "conclusion": "\u901a\u8fc7\u53ef\u5fae\u5206\u4f18\u5316\u6574\u5408\u6570\u636e\u83b7\u53d6\u4e0e\u51b3\u7b56\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u51b3\u7b56\u8d28\u91cf\u3002"}}
{"id": "2504.15063", "pdf": "https://arxiv.org/pdf/2504.15063", "abs": "https://arxiv.org/abs/2504.15063", "authors": ["Hongli Peng", "Xiaoqi Li", "Wenkai Li"], "title": "Mining Characteristics of Vulnerable Smart Contracts Across Lifecycle Stages", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Smart contracts are the cornerstone of decentralized applications and\nfinancial protocols, which extend the application of digital currency\ntransactions. The applications and financial protocols introduce significant\nsecurity challenges, resulting in substantial economic losses. Existing\nsolutions predominantly focus on code vulnerabilities within smart contracts,\naccounting for only 50% of security incidents. Therefore, a more comprehensive\nstudy of security issues related to smart contracts is imperative. The existing\nempirical research realizes the static analysis of smart contracts from the\nperspective of the lifecycle and gives the corresponding measures for each\nstage. However, they lack the characteristic analysis of vulnerabilities in\neach stage and the distinction between the vulnerabilities. In this paper, we\npresent the first empirical study on the security of smart contracts throughout\ntheir lifecycle, including deployment and execution, upgrade, and destruction\nstages. It delves into the security issues at each stage and provides at least\nseven feature descriptions. Finally, utilizing these seven features, five\nmachine-learning classification models are used to identify vulnerabilities at\ndifferent stages. The classification results reveal that vulnerable contracts\nexhibit distinct transaction features and ego network properties at various\nstages.", "AI": {"tldr": "\u8bba\u6587\u5bf9\u667a\u80fd\u5408\u7ea6\u751f\u547d\u5468\u671f\u5404\u9636\u6bb5\u7684\u5b89\u5168\u95ee\u9898\u8fdb\u884c\u4e86\u9996\u6b21\u5b9e\u8bc1\u7814\u7a76\uff0c\u63d0\u51fa\u4e86\u4e03\u4e2a\u7279\u5f81\u63cf\u8ff0\uff0c\u5e76\u5229\u7528\u673a\u5668\u5b66\u4e60\u6a21\u578b\u8bc6\u522b\u4e0d\u540c\u9636\u6bb5\u7684\u6f0f\u6d1e\u3002", "motivation": "\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u4e3b\u8981\u5173\u6ce8\u667a\u80fd\u5408\u7ea6\u4ee3\u7801\u6f0f\u6d1e\uff0c\u4ec5\u8986\u76d650%\u7684\u5b89\u5168\u4e8b\u4ef6\uff0c\u4e9f\u9700\u66f4\u5168\u9762\u7684\u7814\u7a76\u3002", "method": "\u7814\u7a76\u5206\u6790\u4e86\u667a\u80fd\u5408\u7ea6\u751f\u547d\u5468\u671f\u5404\u9636\u6bb5\u7684\u5b89\u5168\u95ee\u9898\uff0c\u63d0\u51fa\u4e03\u4e2a\u7279\u5f81\uff0c\u5e76\u5229\u7528\u4e94\u79cd\u673a\u5668\u5b66\u4e60\u6a21\u578b\u8fdb\u884c\u5206\u7c7b\u3002", "result": "\u5206\u7c7b\u7ed3\u679c\u663e\u793a\uff0c\u6613\u53d7\u653b\u51fb\u7684\u5408\u7ea6\u5728\u4e0d\u540c\u9636\u6bb5\u8868\u73b0\u51fa\u72ec\u7279\u7684\u4ea4\u6613\u7279\u5f81\u548c\u7f51\u7edc\u5c5e\u6027\u3002", "conclusion": "\u7814\u7a76\u4e3a\u667a\u80fd\u5408\u7ea6\u5b89\u5168\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u89c6\u89d2\uff0c\u6709\u52a9\u4e8e\u8bc6\u522b\u548c\u89e3\u51b3\u5404\u9636\u6bb5\u7684\u5b89\u5168\u95ee\u9898\u3002"}}
{"id": "2504.15066", "pdf": "https://arxiv.org/pdf/2504.15066", "abs": "https://arxiv.org/abs/2504.15066", "authors": ["Jinghua Zhao", "Yuhang Jia", "Shiyao Wang", "Jiaming Zhou", "Hui Wang", "Yong Qin"], "title": "Chinese-LiPS: A Chinese audio-visual speech recognition dataset with Lip-reading and Presentation Slides", "categories": ["cs.MM", "cs.AI"], "comment": "6 pages, 7 figures", "summary": "Incorporating visual modalities to assist Automatic Speech Recognition (ASR)\ntasks has led to significant improvements. However, existing Audio-Visual\nSpeech Recognition (AVSR) datasets and methods typically rely solely on\nlip-reading information or speaking contextual video, neglecting the potential\nof combining these different valuable visual cues within the speaking context.\nIn this paper, we release a multimodal Chinese AVSR dataset, Chinese-LiPS,\ncomprising 100 hours of speech, video, and corresponding manual transcription,\nwith the visual modality encompassing both lip-reading information and the\npresentation slides used by the speaker. Based on Chinese-LiPS, we develop a\nsimple yet effective pipeline, LiPS-AVSR, which leverages both lip-reading and\npresentation slide information as visual modalities for AVSR tasks. Experiments\nshow that lip-reading and presentation slide information improve ASR\nperformance by approximately 8\\% and 25\\%, respectively, with a combined\nperformance improvement of about 35\\%. The dataset is available at\nhttps://kiri0824.github.io/Chinese-LiPS/", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5507\u8bfb\u548c\u6f14\u793a\u5e7b\u706f\u7247\u89c6\u89c9\u4fe1\u606f\u7684\u591a\u6a21\u6001\u4e2d\u6587AVSR\u6570\u636e\u96c6Chinese-LiPS\uff0c\u5e76\u5f00\u53d1\u4e86LiPS-AVSR\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86ASR\u6027\u80fd\u3002", "motivation": "\u73b0\u6709AVSR\u65b9\u6cd5\u901a\u5e38\u4ec5\u4f9d\u8d56\u5507\u8bfb\u6216\u4e0a\u4e0b\u6587\u89c6\u9891\uff0c\u5ffd\u7565\u4e86\u7ed3\u5408\u591a\u79cd\u89c6\u89c9\u7ebf\u7d22\u7684\u6f5c\u529b\u3002", "method": "\u57fa\u4e8eChinese-LiPS\u6570\u636e\u96c6\uff0c\u5f00\u53d1\u4e86LiPS-AVSR\u65b9\u6cd5\uff0c\u540c\u65f6\u5229\u7528\u5507\u8bfb\u548c\u5e7b\u706f\u7247\u4fe1\u606f\u4f5c\u4e3a\u89c6\u89c9\u6a21\u6001\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u5507\u8bfb\u548c\u5e7b\u706f\u7247\u4fe1\u606f\u5206\u522b\u63d0\u5347ASR\u6027\u80fd\u7ea68%\u548c25%\uff0c\u7ed3\u5408\u540e\u63d0\u5347\u7ea635%\u3002", "conclusion": "\u7ed3\u5408\u591a\u79cd\u89c6\u89c9\u7ebf\u7d22\u80fd\u663e\u8457\u63d0\u5347AVSR\u6027\u80fd\uff0cChinese-LiPS\u6570\u636e\u96c6\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u8d44\u6e90\u3002"}}
{"id": "2504.15080", "pdf": "https://arxiv.org/pdf/2504.15080", "abs": "https://arxiv.org/abs/2504.15080", "authors": ["Chen Xie", "Mingsheng Jiao", "Xiaodong Gu", "Beijun Shen"], "title": "Empowering AI to Generate Better AI Code: Guided Generation of Deep Learning Projects with LLMs", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "While large language models (LLMs) have been widely applied to code\ngeneration, they struggle with generating entire deep learning projects, which\nare characterized by complex structures, longer functions, and stronger\nreliance on domain knowledge than general-purpose code. An open-domain LLM\noften lacks coherent contextual guidance and domain expertise for specific\nprojects, making it challenging to produce complete code that fully meets user\nrequirements.\n  In this paper, we propose a novel planning-guided code generation method,\nDLCodeGen, tailored for generating deep learning projects. DLCodeGen predicts a\nstructured solution plan, offering global guidance for LLMs to generate the\nproject. The generated plan is then leveraged to retrieve semantically\nanalogous code samples and subsequently abstract a code template. To\neffectively integrate these multiple retrieval-augmented techniques, a\ncomparative learning mechanism is designed to generate the final code. We\nvalidate the effectiveness of our approach on a dataset we build for deep\nlearning code generation. Experimental results demonstrate that DLCodeGen\noutperforms other baselines, achieving improvements of 9.7% in CodeBLEU and\n3.6% in human evaluation metrics.", "AI": {"tldr": "DLCodeGen\u662f\u4e00\u79cd\u9488\u5bf9\u6df1\u5ea6\u5b66\u4e60\u9879\u76ee\u4ee3\u7801\u751f\u6210\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u89e3\u51b3\u65b9\u6848\u8ba1\u5212\u548c\u68c0\u7d22\u589e\u5f3a\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u4ee3\u7801\u7684\u8d28\u91cf\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u751f\u6210\u590d\u6742\u6df1\u5ea6\u5b66\u4e60\u9879\u76ee\u4ee3\u7801\u65f6\u7f3a\u4e4f\u4e0a\u4e0b\u6587\u6307\u5bfc\u548c\u9886\u57df\u77e5\u8bc6\uff0c\u96be\u4ee5\u6ee1\u8db3\u7528\u6237\u9700\u6c42\u3002", "method": "\u63d0\u51faDLCodeGen\u65b9\u6cd5\uff0c\u9996\u5148\u751f\u6210\u7ed3\u6784\u5316\u89e3\u51b3\u65b9\u6848\u8ba1\u5212\uff0c\u68c0\u7d22\u7c7b\u4f3c\u4ee3\u7801\u6837\u672c\u5e76\u62bd\u8c61\u6a21\u677f\uff0c\u6700\u540e\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u751f\u6210\u6700\u7ec8\u4ee3\u7801\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDLCodeGen\u5728CodeBLEU\u548c\u4eba\u5de5\u8bc4\u4f30\u6307\u6807\u4e0a\u5206\u522b\u63d0\u53479.7%\u548c3.6%\u3002", "conclusion": "DLCodeGen\u901a\u8fc7\u8ba1\u5212\u5f15\u5bfc\u548c\u68c0\u7d22\u589e\u5f3a\u6280\u672f\uff0c\u6709\u6548\u63d0\u5347\u4e86\u6df1\u5ea6\u5b66\u4e60\u9879\u76ee\u4ee3\u7801\u7684\u751f\u6210\u8d28\u91cf\u3002"}}
{"id": "2504.15101", "pdf": "https://arxiv.org/pdf/2504.15101", "abs": "https://arxiv.org/abs/2504.15101", "authors": ["Yiqian Yang"], "title": "NeuGaze: Reshaping the future BCI", "categories": ["cs.HC", "cs.AI", "cs.ET"], "comment": null, "summary": "Traditional brain-computer interfaces (BCIs), reliant on costly\nelectroencephalography or invasive implants, struggle with complex\nhuman-computer interactions due to setup complexity and limited precision. We\npresent NeuGaze, a novel webcam-based system that leverages eye gaze, head\nmovements, and facial expressions to enable intuitive, real-time control using\nonly a standard 30 Hz webcam, often pre-installed in laptops. Requiring minimal\ncalibration, NeuGaze achieves performance comparable to conventional inputs,\nsupporting precise cursor navigation, key triggering via an efficient skill\nwheel, and dynamic gaming interactions, such as defeating formidable opponents\nin first-person games. By harnessing preserved neck-up functionalities in\nmotor-impaired individuals, NeuGaze eliminates the need for specialized\nhardware, offering a low-cost, accessible alternative to BCIs. This paradigm\nempowers diverse applications, from assistive technology to entertainment,\nredefining human-computer interaction for motor-impaired users. Project is at\n\\href{https://github.com/NeuSpeech/NeuGaze}{github.com/NeuSpeech/NeuGaze}.", "AI": {"tldr": "NeuGaze\u662f\u4e00\u79cd\u57fa\u4e8e\u666e\u901a\u7f51\u7edc\u6444\u50cf\u5934\u7684\u4f4e\u6210\u672c\u8111\u673a\u63a5\u53e3\u66ff\u4ee3\u65b9\u6848\uff0c\u5229\u7528\u773c\u52a8\u3001\u5934\u90e8\u8fd0\u52a8\u548c\u9762\u90e8\u8868\u60c5\u5b9e\u73b0\u5b9e\u65f6\u63a7\u5236\uff0c\u9002\u7528\u4e8e\u8fd0\u52a8\u969c\u788d\u7528\u6237\u3002", "motivation": "\u4f20\u7edf\u8111\u673a\u63a5\u53e3\u4f9d\u8d56\u6602\u8d35\u8bbe\u5907\u4e14\u64cd\u4f5c\u590d\u6742\uff0cNeuGaze\u65e8\u5728\u63d0\u4f9b\u4e00\u79cd\u4f4e\u6210\u672c\u3001\u6613\u7528\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u4f7f\u752830Hz\u7f51\u7edc\u6444\u50cf\u5934\u6355\u6349\u773c\u52a8\u3001\u5934\u90e8\u8fd0\u52a8\u548c\u9762\u90e8\u8868\u60c5\uff0c\u901a\u8fc7\u6280\u80fd\u8f6e\u5b9e\u73b0\u7cbe\u786e\u63a7\u5236\u548c\u52a8\u6001\u4ea4\u4e92\u3002", "result": "\u6027\u80fd\u63a5\u8fd1\u4f20\u7edf\u8f93\u5165\u8bbe\u5907\uff0c\u652f\u6301\u5149\u6807\u5bfc\u822a\u3001\u6309\u952e\u89e6\u53d1\u548c\u6e38\u620f\u4ea4\u4e92\u3002", "conclusion": "NeuGaze\u4e3a\u8fd0\u52a8\u969c\u788d\u7528\u6237\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u4e13\u7528\u786c\u4ef6\u7684\u4f4e\u6210\u672c\u89e3\u51b3\u65b9\u6848\uff0c\u6269\u5c55\u4e86\u4eba\u673a\u4ea4\u4e92\u7684\u53ef\u80fd\u6027\u3002"}}
{"id": "2504.15105", "pdf": "https://arxiv.org/pdf/2504.15105", "abs": "https://arxiv.org/abs/2504.15105", "authors": ["Yurun Wang", "Zerong Qi", "Shujun Fu", "Mingzheng Hu"], "title": "A triple-branch network for latent fingerprint enhancement guided by orientation fields and minutiae", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Latent fingerprint enhancement is a critical step in the process of latent\nfingerprint identification. Existing deep learning-based enhancement methods\nstill fall short of practical application requirements, particularly in\nrestoring low-quality fingerprint regions. Recognizing that different regions\nof latent fingerprints require distinct enhancement strategies, we propose a\nTriple Branch Spatial Fusion Network (TBSFNet), which simultaneously enhances\ndifferent regions of the image using tailored strategies. Furthermore, to\nimprove the generalization capability of the network, we integrate orientation\nfield and minutiae-related modules into TBSFNet and introduce a Multi-Level\nFeature Guidance Network (MLFGNet). Experimental results on the MOLF and MUST\ndatasets demonstrate that MLFGNet outperforms existing enhancement algorithms.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTBSFNet\u548cMLFGNet\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u6f5c\u6307\u7eb9\u589e\u5f3a\uff0c\u901a\u8fc7\u591a\u5206\u652f\u7b56\u7565\u548c\u7279\u5f81\u5f15\u5bfc\u7f51\u7edc\u63d0\u5347\u4f4e\u8d28\u91cf\u533a\u57df\u7684\u6062\u590d\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u6f5c\u6307\u7eb9\u589e\u5f3a\u4e2d\uff0c\u5c24\u5176\u662f\u4f4e\u8d28\u91cf\u533a\u57df\u7684\u6062\u590d\u4e0a\u8868\u73b0\u4e0d\u8db3\uff0c\u9700\u8981\u9488\u5bf9\u4e0d\u540c\u533a\u57df\u91c7\u7528\u4e0d\u540c\u7b56\u7565\u3002", "method": "\u63d0\u51faTBSFNet\uff08\u4e09\u5206\u652f\u7a7a\u95f4\u878d\u5408\u7f51\u7edc\uff09\u548cMLFGNet\uff08\u591a\u7ea7\u7279\u5f81\u5f15\u5bfc\u7f51\u7edc\uff09\uff0c\u7ed3\u5408\u65b9\u5411\u573a\u548c\u7ec6\u8282\u6a21\u5757\uff0c\u589e\u5f3a\u4e0d\u540c\u533a\u57df\u7684\u6307\u7eb9\u3002", "result": "\u5728MOLF\u548cMUST\u6570\u636e\u96c6\u4e0a\uff0cMLFGNet\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u589e\u5f3a\u7b97\u6cd5\u3002", "conclusion": "TBSFNet\u548cMLFGNet\u901a\u8fc7\u591a\u5206\u652f\u548c\u7279\u5f81\u5f15\u5bfc\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6f5c\u6307\u7eb9\u589e\u5f3a\u7684\u6548\u679c\uff0c\u7279\u522b\u662f\u4f4e\u8d28\u91cf\u533a\u57df\u7684\u6062\u590d\u3002"}}
{"id": "2504.15130", "pdf": "https://arxiv.org/pdf/2504.15130", "abs": "https://arxiv.org/abs/2504.15130", "authors": ["Kushal Shah", "Jihyun Park", "Seung-Kyum Choi"], "title": "Neural ATTF: A Scalable Solution to Lifelong Multi-Agent Path Planning", "categories": ["cs.RO", "cs.AI"], "comment": "13 Pages, 5 Figures, 5 Tables", "summary": "Multi-Agent Pickup and Delivery (MAPD) is a fundamental problem in robotics,\nparticularly in applications such as warehouse automation and logistics.\nExisting solutions often face challenges in scalability, adaptability, and\nefficiency, limiting their applicability in dynamic environments with real-time\nplanning requirements. This paper presents Neural ATTF (Adaptive Task Token\nFramework), a new algorithm that combines a Priority Guided Task Matching\n(PGTM) Module with Neural STA* (Space-Time A*), a data-driven path planning\nmethod. Neural STA* enhances path planning by enabling rapid exploration of the\nsearch space through guided learned heuristics and ensures collision avoidance\nunder dynamic constraints. PGTM prioritizes delayed agents and dynamically\nassigns tasks by prioritizing agents nearest to these tasks, optimizing both\ncontinuity and system throughput. Experimental evaluations against\nstate-of-the-art MAPD algorithms, including TPTS, CENTRAL, RMCA, LNS-PBS, and\nLNS-wPBS, demonstrate the superior scalability, solution quality, and\ncomputational efficiency of Neural ATTF. These results highlight the\nframework's potential for addressing the critical demands of complex,\nreal-world multi-agent systems operating in high-demand, unpredictable\nsettings.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aNeural ATTF\u7684\u65b0\u7b97\u6cd5\uff0c\u7ed3\u5408\u4e86PGTM\u6a21\u5757\u548cNeural STA*\u8def\u5f84\u89c4\u5212\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u667a\u80fd\u4f53\u62fe\u53d6\u4e0e\u914d\u9001\u95ee\u9898\u7684\u53ef\u6269\u5c55\u6027\u548c\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u52a8\u6001\u73af\u5883\u4e2d\u9762\u4e34\u53ef\u6269\u5c55\u6027\u3001\u9002\u5e94\u6027\u548c\u6548\u7387\u7684\u6311\u6218\uff0c\u9650\u5236\u4e86\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u8868\u73b0\u3002", "method": "Neural ATTF\u7ed3\u5408\u4e86PGTM\u6a21\u5757\uff08\u52a8\u6001\u4efb\u52a1\u5206\u914d\uff09\u548cNeural STA*\uff08\u6570\u636e\u9a71\u52a8\u7684\u8def\u5f84\u89c4\u5212\uff09\uff0c\u901a\u8fc7\u5b66\u4e60\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\u4f18\u5316\u641c\u7d22\u7a7a\u95f4\u548c\u907f\u514d\u78b0\u649e\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cNeural ATTF\u5728\u53ef\u6269\u5c55\u6027\u3001\u89e3\u51b3\u65b9\u6848\u8d28\u91cf\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u7b97\u6cd5\uff08\u5982TPTS\u3001CENTRAL\u7b49\uff09\u3002", "conclusion": "Neural ATTF\u80fd\u591f\u6ee1\u8db3\u590d\u6742\u3001\u9ad8\u9700\u6c42\u52a8\u6001\u73af\u5883\u4e2d\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u5173\u952e\u9700\u6c42\u3002"}}
{"id": "2504.15144", "pdf": "https://arxiv.org/pdf/2504.15144", "abs": "https://arxiv.org/abs/2504.15144", "authors": ["Melih Sirlanci", "Carter Yagemann", "Zhiqiang Lin"], "title": "C2RUST-BENCH: A Minimized, Representative Dataset for C-to-Rust Transpilation Evaluation", "categories": ["cs.CR", "cs.AI", "cs.PL"], "comment": null, "summary": "Despite the effort in vulnerability detection over the last two decades,\nmemory safety vulnerabilities continue to be a critical problem. Recent reports\nsuggest that the key solution is to migrate to memory-safe languages. To this\nend, C-to-Rust transpilation becomes popular to resolve memory-safety issues in\nC programs. Recent works propose C-to-Rust transpilation frameworks; however, a\ncomprehensive evaluation dataset is missing. Although one solution is to put\ntogether a large enough dataset, this increases the analysis time in automated\nframeworks as well as in manual efforts for some cases. In this work, we build\na method to select functions from a large set to construct a minimized yet\nrepresentative dataset to evaluate the C-to-Rust transpilation. We propose\nC2RUST-BENCH that contains 2,905 functions, which are representative of\nC-to-Rust transpilation, selected from 15,503 functions of real-world programs.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b9\u6cd5\uff0c\u4ece\u5927\u91cfC\u51fd\u6570\u4e2d\u9009\u62e9\u4ee3\u8868\u6027\u6837\u672c\u6784\u5efaC2RUST-BENCH\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bc4\u4f30C\u5230Rust\u7684\u8f6c\u8bd1\u3002", "motivation": "\u5185\u5b58\u5b89\u5168\u95ee\u9898\u6301\u7eed\u5b58\u5728\uff0cC\u5230Rust\u8f6c\u8bd1\u662f\u89e3\u51b3\u65b9\u6848\u4e4b\u4e00\uff0c\u4f46\u7f3a\u4e4f\u5168\u9762\u8bc4\u4f30\u6570\u636e\u96c6\u3002", "method": "\u4ece15,503\u4e2a\u771f\u5b9e\u4e16\u754c\u7a0b\u5e8f\u7684\u51fd\u6570\u4e2d\u7b5b\u9009\u51fa2,905\u4e2a\u4ee3\u8868\u6027\u51fd\u6570\uff0c\u6784\u5efaC2RUST-BENCH\u6570\u636e\u96c6\u3002", "result": "C2RUST-BENCH\u6570\u636e\u96c6\u4e3a\u8bc4\u4f30C\u5230Rust\u8f6c\u8bd1\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u4ee3\u8868\u6027\u7684\u6837\u672c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u89e3\u51b3\u4e86\u8bc4\u4f30\u6570\u636e\u96c6\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u8f6c\u8bd1\u6846\u67b6\u7684\u8bc4\u4f30\u6548\u7387\u3002"}}
{"id": "2504.15152", "pdf": "https://arxiv.org/pdf/2504.15152", "abs": "https://arxiv.org/abs/2504.15152", "authors": ["Jun Zhou", "Bingchen Gao", "Kai Wang", "Jialun Pei", "Pheng-Ann Heng", "Jing Qin"], "title": "Landmark-Free Preoperative-to-Intraoperative Registration in Laparoscopic Liver Resection", "categories": ["cs.CV", "cs.AI"], "comment": "TMI under review", "summary": "Liver registration by overlaying preoperative 3D models onto intraoperative\n2D frames can assist surgeons in perceiving the spatial anatomy of the liver\nclearly for a higher surgical success rate. Existing registration methods rely\nheavily on anatomical landmark-based workflows, which encounter two major\nlimitations: 1) ambiguous landmark definitions fail to provide efficient\nmarkers for registration; 2) insufficient integration of intraoperative liver\nvisual information in shape deformation modeling. To address these challenges,\nin this paper, we propose a landmark-free preoperative-to-intraoperative\nregistration framework utilizing effective self-supervised learning, termed\n\\ourmodel. This framework transforms the conventional 3D-2D workflow into a\n3D-3D registration pipeline, which is then decoupled into rigid and non-rigid\nregistration subtasks. \\ourmodel~first introduces a feature-disentangled\ntransformer to learn robust correspondences for recovering rigid\ntransformations. Further, a structure-regularized deformation network is\ndesigned to adjust the preoperative model to align with the intraoperative\nliver surface. This network captures structural correlations through geometry\nsimilarity modeling in a low-rank transformer network. To facilitate the\nvalidation of the registration performance, we also construct an in-vivo\nregistration dataset containing liver resection videos of 21 patients, called\n\\emph{P2I-LReg}, which contains 346 keyframes that provide a global view of the\nliver together with liver mask annotations and calibrated camera intrinsic\nparameters. Extensive experiments and user studies on both synthetic and\nin-vivo datasets demonstrate the superiority and potential clinical\napplicability of our method.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u81ea\u76d1\u7763\u5b66\u4e60\u7684\u65e0\u6807\u8bb0\u672f\u524d\u5230\u672f\u4e2d\u809d\u810f\u914d\u51c6\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u89e3\u5256\u6807\u8bb0\u548c\u5f62\u72b6\u53d8\u5f62\u5efa\u6a21\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u914d\u51c6\u65b9\u6cd5\u4f9d\u8d56\u89e3\u5256\u6807\u8bb0\uff0c\u5b58\u5728\u6807\u8bb0\u5b9a\u4e49\u6a21\u7cca\u548c\u672f\u4e2d\u89c6\u89c9\u4fe1\u606f\u6574\u5408\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u5c063D-2D\u914d\u51c6\u6d41\u7a0b\u8f6c\u4e3a3D-3D\uff0c\u5206\u89e3\u4e3a\u521a\u6027\u548c\u975e\u521a\u6027\u914d\u51c6\u5b50\u4efb\u52a1\uff0c\u4f7f\u7528\u7279\u5f81\u89e3\u8026\u53d8\u6362\u5668\u548c\u7ed3\u6784\u6b63\u5219\u5316\u53d8\u5f62\u7f51\u7edc\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u53ca\u7528\u6237\u7814\u7a76\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5177\u6709\u4f18\u8d8a\u6027\u548c\u4e34\u5e8a\u9002\u7528\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u809d\u810f\u914d\u51c6\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\uff0c\u5177\u6709\u4e34\u5e8a\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2504.15165", "pdf": "https://arxiv.org/pdf/2504.15165", "abs": "https://arxiv.org/abs/2504.15165", "authors": ["Liu Wenbin"], "title": "An Efficient Aerial Image Detection with Variable Receptive Fields", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Aerial object detection using unmanned aerial vehicles (UAVs) faces critical\nchallenges including sub-10px targets, dense occlusions, and stringent\ncomputational constraints. Existing detectors struggle to balance accuracy and\nefficiency due to rigid receptive fields and redundant architectures. To\naddress these limitations, we propose Variable Receptive Field DETR (VRF-DETR),\na transformer-based detector incorporating three key components: 1) Multi-Scale\nContext Fusion (MSCF) module that dynamically recalibrates features through\nadaptive spatial attention and gated multi-scale fusion, 2) Gated Convolution\n(GConv) layer enabling parameter-efficient local-context modeling via depthwise\nseparable operations and dynamic gating, and 3) Gated Multi-scale Fusion (GMCF)\nBottleneck that hierarchically disentangles occluded objects through cascaded\nglobal-local interactions. Experiments on VisDrone2019 demonstrate VRF-DETR\nachieves 51.4\\% mAP\\textsubscript{50} and 31.8\\% mAP\\textsubscript{50:95} with\nonly 13.5M parameters. This work establishes a new efficiency-accuracy Pareto\nfrontier for UAV-based detection tasks.", "AI": {"tldr": "VRF-DETR\u662f\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u68c0\u6d4b\u5668\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u611f\u53d7\u91ce\u548c\u95e8\u63a7\u591a\u5c3a\u5ea6\u878d\u5408\uff0c\u89e3\u51b3\u4e86\u65e0\u4eba\u673a\u76ee\u6807\u68c0\u6d4b\u4e2d\u7684\u5c0f\u76ee\u6807\u3001\u5bc6\u96c6\u906e\u6321\u548c\u8ba1\u7b97\u9650\u5236\u95ee\u9898\u3002", "motivation": "\u65e0\u4eba\u673a\u76ee\u6807\u68c0\u6d4b\u9762\u4e34\u5c0f\u76ee\u6807\uff08\u5c0f\u4e8e10\u50cf\u7d20\uff09\u3001\u5bc6\u96c6\u906e\u6321\u548c\u4e25\u683c\u8ba1\u7b97\u9650\u5236\u7684\u6311\u6218\uff0c\u73b0\u6709\u68c0\u6d4b\u5668\u96be\u4ee5\u5e73\u8861\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "method": "\u63d0\u51faVRF-DETR\uff0c\u5305\u542b\u591a\u5c3a\u5ea6\u4e0a\u4e0b\u6587\u878d\u5408\u6a21\u5757\u3001\u95e8\u63a7\u5377\u79ef\u5c42\u548c\u95e8\u63a7\u591a\u5c3a\u5ea6\u878d\u5408\u74f6\u9888\uff0c\u901a\u8fc7\u52a8\u6001\u7a7a\u95f4\u6ce8\u610f\u529b\u548c\u5c42\u6b21\u5316\u89e3\u8026\u906e\u6321\u76ee\u6807\u3002", "result": "\u5728VisDrone2019\u6570\u636e\u96c6\u4e0a\uff0cVRF-DETR\u8fbe\u523051.4% mAP50\u548c31.8% mAP50:95\uff0c\u4ec5\u970013.5M\u53c2\u6570\u3002", "conclusion": "VRF-DETR\u4e3a\u65e0\u4eba\u673a\u76ee\u6807\u68c0\u6d4b\u4efb\u52a1\u5efa\u7acb\u4e86\u65b0\u7684\u6548\u7387-\u51c6\u786e\u6027\u5e15\u7d2f\u6258\u524d\u6cbf\u3002"}}
{"id": "2504.15181", "pdf": "https://arxiv.org/pdf/2504.15181", "abs": "https://arxiv.org/abs/2504.15181", "authors": ["Lily Stelling", "Mick Yang", "Rokas Gipi\u0161kis", "Leon Staufer", "Ze Shen Chin", "Sim\u00e9on Campos", "Michael Chen"], "title": "Existing Industry Practice for the EU AI Act's General-Purpose AI Code of Practice Safety and Security Measures", "categories": ["cs.CY", "cs.AI"], "comment": "158 pages", "summary": "This report provides a detailed comparison between the measures proposed in\nthe EU AI Act's General-Purpose AI (GPAI) Code of Practice (Third Draft) and\ncurrent practices adopted by leading AI companies. As the EU moves toward\nenforcing binding obligations for GPAI model providers, the Code of Practice\nwill be key to bridging legal requirements with concrete technical commitments.\nOur analysis focuses on the draft's Safety and Security section which is only\nrelevant for the providers of the most advanced models (Commitments II.1-II.16)\nand excerpts from current public-facing documents quotes that are relevant to\neach individual measure.\n  We systematically reviewed different document types - including companies'\nfrontier safety frameworks and model cards - from over a dozen companies,\nincluding OpenAI, Anthropic, Google DeepMind, Microsoft, Meta, Amazon, and\nothers. This report is not meant to be an indication of legal compliance nor\ndoes it take any prescriptive viewpoint about the Code of Practice or\ncompanies' policies. Instead, it aims to inform the ongoing dialogue between\nregulators and GPAI model providers by surfacing evidence of precedent.", "AI": {"tldr": "\u62a5\u544a\u6bd4\u8f83\u4e86\u6b27\u76dfAI\u6cd5\u6848\u7684GPAI\u884c\u4e3a\u51c6\u5219\u8349\u6848\u4e0e\u9886\u5148AI\u516c\u53f8\u7684\u73b0\u884c\u5b9e\u8df5\uff0c\u805a\u7126\u5b89\u5168\u4e0e\u5b89\u5168\u90e8\u5206\u3002", "motivation": "\u968f\u7740\u6b27\u76df\u63a8\u8fdb\u5bf9GPAI\u6a21\u578b\u63d0\u4f9b\u5546\u7684\u7ea6\u675f\u6027\u4e49\u52a1\uff0c\u884c\u4e3a\u51c6\u5219\u5c06\u6210\u4e3a\u6cd5\u5f8b\u8981\u6c42\u4e0e\u6280\u672f\u627f\u8bfa\u7684\u6865\u6881\u3002", "method": "\u7cfb\u7edf\u5ba1\u67e5\u4e86\u5341\u591a\u5bb6\u516c\u53f8\u7684\u516c\u5f00\u6587\u4ef6\uff0c\u5982\u524d\u6cbf\u5b89\u5168\u6846\u67b6\u548c\u6a21\u578b\u5361\u3002", "result": "\u62a5\u544a\u63ed\u793a\u4e86\u73b0\u884c\u5b9e\u8df5\u4e0e\u8349\u6848\u7684\u5dee\u5f02\uff0c\u65e8\u5728\u4e3a\u76d1\u7ba1\u673a\u6784\u4e0e\u63d0\u4f9b\u5546\u63d0\u4f9b\u53c2\u8003\u3002", "conclusion": "\u62a5\u544a\u65e8\u5728\u4fc3\u8fdb\u76d1\u7ba1\u673a\u6784\u4e0eGPAI\u6a21\u578b\u63d0\u4f9b\u5546\u4e4b\u95f4\u7684\u5bf9\u8bdd\uff0c\u800c\u975e\u8bc4\u4f30\u5408\u89c4\u6027\u6216\u63d0\u51fa\u5efa\u8bae\u3002"}}
{"id": "2504.15192", "pdf": "https://arxiv.org/pdf/2504.15192", "abs": "https://arxiv.org/abs/2504.15192", "authors": ["Yaqian Chen", "Lin Li", "Hanxue Gu", "Haoyu Dong", "Derek L. Nguyen", "Allan D. Kirk", "Maciej A. Mazurowski", "E. Shelley Hwang"], "title": "Breast density in MRI: an AI-based quantification and relationship to assessment in mammography", "categories": ["cs.CV", "cs.AI"], "comment": "13 pages, 5 figures", "summary": "Mammographic breast density is a well-established risk factor for breast\ncancer. Recently there has been interest in breast MRI as an adjunct to\nmammography, as this modality provides an orthogonal and highly quantitative\nassessment of breast tissue. However, its 3D nature poses analytic challenges\nrelated to delineating and aggregating complex structures across slices. Here,\nwe applied an in-house machine-learning algorithm to assess breast density on\nnormal breasts in three MRI datasets. Breast density was consistent across\ndifferent datasets (0.104 - 0.114). Analysis across different age groups also\ndemonstrated strong consistency across datasets and confirmed a trend of\ndecreasing density with age as reported in previous studies. MR breast density\nwas correlated with mammographic breast density, although some notable\ndifferences suggest that certain breast density components are captured only on\nMRI. Future work will determine how to integrate MR breast density with current\ntools to improve future breast cancer risk prediction.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u4e73\u817aMRI\u5bc6\u5ea6\u4f5c\u4e3a\u4e73\u817a\u764c\u98ce\u9669\u56e0\u7d20\u7684\u7814\u7a76\uff0c\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u5206\u6790\u591a\u4e2a\u6570\u636e\u96c6\uff0c\u53d1\u73b0MRI\u5bc6\u5ea6\u4e0e\u4e73\u817aX\u7ebf\u5bc6\u5ea6\u76f8\u5173\u4f46\u5b58\u5728\u5dee\u5f02\u3002", "motivation": "\u4e73\u817a\u5bc6\u5ea6\u662f\u4e73\u817a\u764c\u7684\u5df2\u77e5\u98ce\u9669\u56e0\u7d20\uff0c\u4f46MRI\u4f5c\u4e3a\u8f85\u52a9\u5de5\u5177\u7684\u5206\u6790\u65b9\u6cd5\u5c1a\u4e0d\u5b8c\u5584\uff0c\u7814\u7a76\u65e8\u5728\u63a2\u7d22MRI\u5bc6\u5ea6\u7684\u5b9a\u91cf\u8bc4\u4f30\u53ca\u5176\u4e0e\u4e73\u817aX\u7ebf\u5bc6\u5ea6\u7684\u5173\u7cfb\u3002", "method": "\u4f7f\u7528\u5185\u90e8\u5f00\u53d1\u7684\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\uff0c\u5206\u6790\u4e09\u4e2aMRI\u6570\u636e\u96c6\u4e2d\u7684\u6b63\u5e38\u4e73\u817a\u5bc6\u5ea6\uff0c\u5e76\u6bd4\u8f83\u4e0d\u540c\u5e74\u9f84\u7ec4\u7684\u5bc6\u5ea6\u53d8\u5316\u3002", "result": "MRI\u5bc6\u5ea6\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e2d\u8868\u73b0\u4e00\u81f4\uff080.104-0.114\uff09\uff0c\u4e14\u968f\u5e74\u9f84\u589e\u957f\u800c\u964d\u4f4e\uff1b\u4e0e\u4e73\u817aX\u7ebf\u5bc6\u5ea6\u76f8\u5173\uff0c\u4f46\u5b58\u5728\u5dee\u5f02\u3002", "conclusion": "MRI\u5bc6\u5ea6\u53ef\u80fd\u8865\u5145\u73b0\u6709\u5de5\u5177\uff0c\u672a\u6765\u7814\u7a76\u5c06\u63a2\u7d22\u5982\u4f55\u6574\u5408MRI\u5bc6\u5ea6\u4ee5\u6539\u8fdb\u4e73\u817a\u764c\u98ce\u9669\u9884\u6d4b\u3002"}}
{"id": "2504.15210", "pdf": "https://arxiv.org/pdf/2504.15210", "abs": "https://arxiv.org/abs/2504.15210", "authors": ["Marina Sakharova", "Abhinav Anand", "Mira Mezini"], "title": "Integrating Symbolic Execution into the Fine-Tuning of Code-Generating LLMs", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Code-generating Large Language Models (LLMs) have become essential tools in\nmodern software development, enhancing productivity and accelerating\ndevelopment. This paper aims to investigate the fine-tuning of code-generating\nLLMs using Reinforcement Learning and Direct Preference Optimization, further\nimproving their performance. To achieve this, we enhance the training data for\nthe reward model with the help of symbolic execution techniques, ensuring more\ncomprehensive and objective data. With symbolic execution, we create a custom\ndataset that better captures the nuances in code evaluation. Our reward models,\nfine-tuned on this dataset, demonstrate significant improvements over the\nbaseline, CodeRL, in estimating the quality of generated code. Our\ncode-generating LLMs, trained with the help of reward model feedback, achieve\nsimilar results compared to the CodeRL benchmark.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u548c\u76f4\u63a5\u504f\u597d\u4f18\u5316\u5fae\u8c03\u4ee3\u7801\u751f\u6210LLM\uff0c\u5229\u7528\u7b26\u53f7\u6267\u884c\u6280\u672f\u6539\u8fdb\u5956\u52b1\u6a21\u578b\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u4ee3\u7801\u7684\u8d28\u91cf\u8bc4\u4f30\u3002", "motivation": "\u63d0\u5347\u4ee3\u7801\u751f\u6210LLM\u7684\u6027\u80fd\uff0c\u901a\u8fc7\u6539\u8fdb\u5956\u52b1\u6a21\u578b\u6570\u636e\u589e\u5f3a\u5176\u8bc4\u4f30\u80fd\u529b\u3002", "method": "\u7ed3\u5408\u7b26\u53f7\u6267\u884c\u6280\u672f\u751f\u6210\u5b9a\u5236\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bad\u7ec3\u5956\u52b1\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u548c\u76f4\u63a5\u504f\u597d\u4f18\u5316\u5fae\u8c03LLM\u3002", "result": "\u5956\u52b1\u6a21\u578b\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u57fa\u51c6CodeRL\uff0c\u751f\u6210\u7684\u4ee3\u7801\u8d28\u91cf\u8bc4\u4f30\u66f4\u51c6\u786e\u3002", "conclusion": "\u901a\u8fc7\u7b26\u53f7\u6267\u884c\u6539\u8fdb\u5956\u52b1\u6a21\u578b\u6570\u636e\uff0c\u80fd\u6709\u6548\u63d0\u5347\u4ee3\u7801\u751f\u6210LLM\u7684\u6027\u80fd\u3002"}}
{"id": "2504.15226", "pdf": "https://arxiv.org/pdf/2504.15226", "abs": "https://arxiv.org/abs/2504.15226", "authors": ["Nathan Steffen", "Wilhelm Louw", "Nicholas Ernest", "Timothy Arnett", "Kelly Cohen"], "title": "A Genetic Fuzzy-Enabled Framework on Robotic Manipulation for In-Space Servicing", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "comment": null, "summary": "Automation of robotic systems for servicing in cislunar space is becoming\nextremely important as the number of satellites in orbit increases. Safety is\ncritical in performing satellite maintenance, so the control techniques\nutilized must be trusted in addition to being highly efficient. In this work,\nGenetic Fuzzy Trees are combined with the widely used LQR control scheme via\nThales' TrUE AI Toolkit to create a trusted and efficient controller for a\ntwo-degree-of-freedom planar robotic manipulator that would theoretically be\nused to perform satellite maintenance. It was found that Genetic Fuzzy-LQR is\n18.5% more performant than optimal LQR on average, and that it is incredibly\nrobust to uncertainty.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u9057\u4f20\u6a21\u7cca\u6811\u4e0eLQR\u63a7\u5236\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u5347\u4e24\u81ea\u7531\u5ea6\u5e73\u9762\u673a\u68b0\u81c2\u5728\u536b\u661f\u7ef4\u62a4\u4e2d\u7684\u6027\u80fd\u4e0e\u9c81\u68d2\u6027\u3002", "motivation": "\u968f\u7740\u536b\u661f\u6570\u91cf\u7684\u589e\u52a0\uff0c\u81ea\u52a8\u5316\u673a\u5668\u4eba\u7cfb\u7edf\u5728\u8fd1\u6708\u7a7a\u95f4\u7684\u670d\u52a1\u9700\u6c42\u65e5\u76ca\u91cd\u8981\uff0c\u5b89\u5168\u6027\u6210\u4e3a\u5173\u952e\u3002", "method": "\u901a\u8fc7Thales\u7684TrUE AI\u5de5\u5177\u5305\uff0c\u5c06\u9057\u4f20\u6a21\u7cca\u6811\u4e0eLQR\u63a7\u5236\u65b9\u6848\u7ed3\u5408\uff0c\u8bbe\u8ba1\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u4fe1\u7684\u63a7\u5236\u5668\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u9057\u4f20\u6a21\u7cca-LQR\u6bd4\u6700\u4f18LQR\u5e73\u5747\u6027\u80fd\u63d0\u534718.5%\uff0c\u4e14\u5bf9\u4e0d\u786e\u5b9a\u6027\u5177\u6709\u6781\u5f3a\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u536b\u661f\u7ef4\u62a4\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.15259", "pdf": "https://arxiv.org/pdf/2504.15259", "abs": "https://arxiv.org/abs/2504.15259", "authors": ["Yunxuan Cai", "Sitao Xiang", "Zongjian Li", "Haiwei Chen", "Yajie Zhao"], "title": "Bringing Diversity from Diffusion Models to Semantic-Guided Face Asset Generation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Digital modeling and reconstruction of human faces serve various\napplications. However, its availability is often hindered by the requirements\nof data capturing devices, manual labor, and suitable actors. This situation\nrestricts the diversity, expressiveness, and control over the resulting models.\nThis work aims to demonstrate that a semantically controllable generative\nnetwork can provide enhanced control over the digital face modeling process. To\nenhance diversity beyond the limited human faces scanned in a controlled\nsetting, we introduce a novel data generation pipeline that creates a\nhigh-quality 3D face database using a pre-trained diffusion model. Our proposed\nnormalization module converts synthesized data from the diffusion model into\nhigh-quality scanned data. Using the 44,000 face models we obtained, we further\ndeveloped an efficient GAN-based generator. This generator accepts semantic\nattributes as input, and generates geometry and albedo. It also allows\ncontinuous post-editing of attributes in the latent space. Our asset refinement\ncomponent subsequently creates physically-based facial assets. We introduce a\ncomprehensive system designed for creating and editing high-quality face\nassets. Our proposed model has undergone extensive experiment, comparison and\nevaluation. We also integrate everything into a web-based interactive tool. We\naim to make this tool publicly available with the release of the paper.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u751f\u6210\u7f51\u7edc\u7684\u4eba\u8138\u6570\u5b57\u5316\u5efa\u6a21\u65b9\u6cd5\uff0c\u901a\u8fc7\u6269\u6563\u6a21\u578b\u751f\u6210\u9ad8\u8d28\u91cf3D\u4eba\u8138\u6570\u636e\uff0c\u5e76\u5f00\u53d1\u4e86\u652f\u6301\u8bed\u4e49\u63a7\u5236\u7684GAN\u751f\u6210\u5668\uff0c\u6700\u7ec8\u6784\u5efa\u4e86\u4e00\u4e2a\u4ea4\u4e92\u5f0f\u5de5\u5177\u3002", "motivation": "\u73b0\u6709\u7684\u4eba\u8138\u5efa\u6a21\u65b9\u6cd5\u53d7\u9650\u4e8e\u6570\u636e\u91c7\u96c6\u8bbe\u5907\u3001\u4eba\u5de5\u52b3\u52a8\u548c\u5408\u9002\u7684\u6f14\u5458\uff0c\u5bfc\u81f4\u6a21\u578b\u591a\u6837\u6027\u3001\u8868\u73b0\u529b\u548c\u63a7\u5236\u6027\u4e0d\u8db3\u3002", "method": "\u4f7f\u7528\u9884\u8bad\u7ec3\u7684\u6269\u6563\u6a21\u578b\u751f\u6210\u9ad8\u8d28\u91cf3D\u4eba\u8138\u6570\u636e\u5e93\uff0c\u5f00\u53d1\u652f\u6301\u8bed\u4e49\u8f93\u5165\u7684GAN\u751f\u6210\u5668\uff0c\u5e76\u901a\u8fc7\u8d44\u4ea7\u7ec6\u5316\u6a21\u5757\u751f\u6210\u7269\u7406\u771f\u5b9e\u7684\u9762\u90e8\u8d44\u4ea7\u3002", "result": "\u6784\u5efa\u4e86\u5305\u542b44,000\u4e2a\u9762\u90e8\u6a21\u578b\u7684\u6570\u636e\u5e93\uff0c\u5f00\u53d1\u4e86\u9ad8\u6548\u7684\u751f\u6210\u5668\u548c\u4ea4\u4e92\u5de5\u5177\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6548\u679c\u3002", "conclusion": "\u63d0\u51fa\u7684\u7cfb\u7edf\u663e\u8457\u63d0\u5347\u4e86\u4eba\u8138\u5efa\u6a21\u7684\u591a\u6837\u6027\u548c\u63a7\u5236\u6027\uff0c\u5e76\u8ba1\u5212\u516c\u5f00\u4ea4\u4e92\u5de5\u5177\u3002"}}

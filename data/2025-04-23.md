<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 40]
- [cs.AI](#cs.AI) [Total: 20]
- [cs.LG](#cs.LG) [Total: 51]
- [cs.HC](#cs.HC) [Total: 4]
- [cs.GR](#cs.GR) [Total: 2]
- [cs.MA](#cs.MA) [Total: 2]
- [cs.IR](#cs.IR) [Total: 2]
- [cs.CV](#cs.CV) [Total: 21]
- [eess.IV](#eess.IV) [Total: 2]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]
- [cs.DS](#cs.DS) [Total: 1]
- [astro-ph.SR](#astro-ph.SR) [Total: 1]
- [econ.GN](#econ.GN) [Total: 1]
- [eess.AS](#eess.AS) [Total: 1]
- [math.OC](#math.OC) [Total: 1]
- [physics.comp-ph](#physics.comp-ph) [Total: 1]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 1]
- [cs.AR](#cs.AR) [Total: 2]
- [cs.IT](#cs.IT) [Total: 1]
- [stat.ME](#stat.ME) [Total: 2]
- [physics.geo-ph](#physics.geo-ph) [Total: 1]
- [math.ST](#math.ST) [Total: 1]
- [stat.ML](#stat.ML) [Total: 4]
- [cs.CR](#cs.CR) [Total: 7]
- [cs.NI](#cs.NI) [Total: 1]
- [cs.DC](#cs.DC) [Total: 5]
- [cs.OS](#cs.OS) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [cs.LO](#cs.LO) [Total: 1]
- [cs.DB](#cs.DB) [Total: 1]
- [cs.CY](#cs.CY) [Total: 2]
- [cs.SE](#cs.SE) [Total: 9]
- [cs.SI](#cs.SI) [Total: 1]
- [eess.SY](#eess.SY) [Total: 1]
- [cs.RO](#cs.RO) [Total: 10]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Exploring Compositional Generalization (in ReCOGS_pos) by Transformers using Restricted Access Sequence Processing (RASP)](https://arxiv.org/abs/2504.15349)
*William Bruns*

Main category: cs.CL

TL;DR: 论文通过RASP语言证明Transformer编码器-解码器可以系统且组合地完成ReCOGS_pos任务，达到100%语义匹配，并表明该任务无需层次化解决方案。


<details>
  <summary>Details</summary>
Motivation: 研究Transformer模型在组合泛化任务（如COGS基准）中的表现，探索其是否能够通过非层次化方法实现系统性和组合性。

Method: 使用RASP语言构建Transformer等效模型，应用19个扁平模式匹配规则和通用逻辑处理介词短语及句子补足语，输出逻辑形式标记。

Result: 模型在ReCOGS测试集上达到100%语义匹配，除一个泛化分例外（92%），并证明任务无需树状结构规则。

Conclusion: Transformer模型可以通过扁平规则实现组合泛化，无需递归或层次化结构。

Abstract: Humans understand new combinations of words encountered if they are
combinations of words recognized from different contexts, an ability called
Compositional Generalization. The COGS benchmark (Kim and Linzen, 2020)
arXiv:2010.05465 reports 0% accuracy for Transformer models on some structural
generalizations. We use (Weiss et al., 2021) arXiv:2106.06981's Restricted
Access Sequence Processing (RASP), a Transformer-equivalent programming
language, to prove by construction that a Transformer encoder-decoder can
perform the semantically equivalent ReCOGS_pos (Wu et al., 2024)
arXiv:2303.13716 variant of COGS systematically and compositionally: Our RASP
model attains 100% semantic exact match on the ReCOGS test set and 100% SEM on
all generalization splits except obj_pp_to_subj_pp which gets 92%. Furthermore,
our RASP model shows the ReCOGS_pos task does not require a hierarchical or
tree-structured solution: we use word-level tokens with an "embedding" layer
that tags with possible parts of speech, applying just once per encoder pass 19
attention-head compatible flat pattern-matching rules, shown using grammar
coverage (Zeller et al., 2023) to be learnable from the training data, plus
general prepositional phrase (pp) handling and sentential complement (cp)
handling logic, and output the next logical form (LF) token (repeating until
the LF is complete). The model does not apply recursive, tree-structured rules
like 'np_det pp np -> np_pp -> np', but scores 100% semantic and string exact
match on pp recursion, cp recursion using the decoder loop.

</details>


### [2] [Tell Me What You Know About Sexism: Expert-LLM Interaction Strategies and Co-Created Definitions for Zero-Shot Sexism Detection](https://arxiv.org/abs/2504.15392)
*Myrthe Reuver,Indira Sen,Matteo Melis,Gabriella Lapesa*

Main category: cs.CL

TL;DR: 论文研究了性别歧视研究者与大型语言模型（LLMs）的协作，通过四步流程评估LLMs在性别歧视研究中的适用性，并比较专家与LLM生成的性别歧视定义在分类任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 探索性别歧视研究者与LLMs的协作潜力，评估LLMs在性别歧视研究中的知识水平和适用性。

Method: 1. 九位性别歧视研究者回答问题；2. 参与两个交互实验（评估LLM知识和生成定义）；3. 使用三种定义进行零样本分类实验；4. 分析67,500个分类决策。

Result: LLM生成的性别歧视定义更长且更复杂；专家定义平均表现较差，但部分专家通过协作定义提升了分类性能。

Conclusion: LLMs在性别歧视研究中具有一定潜力，但专家协作可以优化其表现，尤其是对LLM使用经验不足的研究者。

Abstract: This paper investigates hybrid intelligence and collaboration between
researchers of sexism and Large Language Models (LLMs), with a four-component
pipeline. First, nine sexism researchers answer questions about their knowledge
of sexism and of LLMs. They then participate in two interactive experiments
involving an LLM (GPT3.5). The first experiment has experts assessing the
model's knowledge about sexism and suitability for use in research. The second
experiment tasks them with creating three different definitions of sexism: an
expert-written definition, an LLM-written one, and a co-created definition.
Lastly, zero-shot classification experiments use the three definitions from
each expert in a prompt template for sexism detection, evaluating GPT4o on
2.500 texts sampled from five sexism benchmarks. We then analyze the resulting
67.500 classification decisions. The LLM interactions lead to longer and more
complex definitions of sexism. Expert-written definitions on average perform
poorly compared to LLM-generated definitions. However, some experts do improve
classification performance with their co-created definitions of sexism, also
experts who are inexperienced in using LLMs.

</details>


### [3] [Trillion 7B Technical Report](https://arxiv.org/abs/2504.15431)
*Sungjun Han,Juyoung Suk,Suyeong An,Hyungguk Kim,Kyuseok Kim,Wonsuk Yang,Seungtaek Choi,Jamin Shin*

Main category: cs.CL

TL;DR: Trillion-7B是一种高效的韩语为中心的多语言大模型，通过XLDA机制实现跨语言知识迁移，仅用10%的多语言数据和59.4K GPU小时即达到竞争性能。


<details>
  <summary>Details</summary>
Motivation: 解决多语言大模型训练中资源消耗高、效率低的问题，尤其是针对韩语等非英语语言。

Method: 采用跨语言文档注意力机制（XLDA）、优化数据混合、语言特定过滤和定制分词器构建。

Result: 在27个基准测试中表现优异，展示了强大的多语言性能和跨语言一致性。

Conclusion: Trillion-7B为多语言大模型提供了一种高效、低成本的解决方案，特别适合韩语等语言。

Abstract: We introduce Trillion-7B, the most token-efficient Korean-centric
multilingual LLM available. Our novel Cross-lingual Document Attention (XLDA)
mechanism enables highly efficient and effective knowledge transfer from
English to target languages like Korean and Japanese. Combined with optimized
data mixtures, language-specific filtering, and tailored tokenizer
construction, Trillion-7B achieves competitive performance while dedicating
only 10\% of its 2T training tokens to multilingual data and requiring just
59.4K H100 GPU hours (\$148K) for full training. Comprehensive evaluations
across 27 benchmarks in four languages demonstrate Trillion-7B's robust
multilingual performance and exceptional cross-lingual consistency.

</details>


### [4] [Feeding LLM Annotations to BERT Classifiers at Your Own Risk](https://arxiv.org/abs/2504.15432)
*Yucheng Lu,Kazimier Smith*

Main category: cs.CL

TL;DR: 研究发现，使用LLM生成的标签微调小型编码器模型进行文本分类会导致性能下降、训练不稳定和过早的性能平台期，需谨慎应用于高风险任务。


<details>
  <summary>Details</summary>
Motivation: 探讨LLM生成标签对小型模型微调的影响，揭示合成数据训练的潜在问题。

Method: 通过实证分析比较使用LLM生成标签和黄金标签训练的模型性能差异。

Result: 发现性能下降、训练不稳定和过早性能平台期，提出熵过滤和集成技术缓解部分问题。

Conclusion: LLM生成标签的误差传播风险未完全解决，高风险任务需谨慎使用此方法。

Abstract: Using LLM-generated labels to fine-tune smaller encoder-only models for text
classification has gained popularity in various settings. While this approach
may be justified in simple and low-stakes applications, we conduct empirical
analysis to demonstrate how the perennial curse of training on synthetic data
manifests itself in this specific setup. Compared to models trained on gold
labels, we observe not only the expected performance degradation in accuracy
and F1 score, but also increased instability across training runs and premature
performance plateaus. These findings cast doubts on the reliability of such
approaches in real-world applications. We contextualize the observed phenomena
through the lens of error propagation and offer several practical mitigation
strategies, including entropy-based filtering and ensemble techniques. Although
these heuristics offer partial relief, they do not fully resolve the inherent
risks of propagating non-random errors from LLM annotations to smaller
classifiers, underscoring the need for caution when applying this workflow in
high-stakes text classification tasks.

</details>


### [5] [Bigram Subnetworks: Mapping to Next Tokens in Transformer Language Models](https://arxiv.org/abs/2504.15471)
*Tyler A. Chang,Benjamin K. Bergen*

Main category: cs.CL

TL;DR: 研究发现Transformer语言模型中存在专注于二元预测的子网络，这些子网络仅占模型参数的极少部分但对性能至关重要，且集中在第一层MLP中。


<details>
  <summary>Details</summary>
Motivation: 探究语言模型中从当前标记到下一标记预测的最小化转换机制。

Method: 识别并分析语言模型中的二元预测子网络，研究其分布、功能及与模型性能的关系。

Result: 二元子网络在1B参数模型中仍存在，占参数不足0.2%但对性能关键，且与最优剪枝子网络重叠。

Conclusion: 二元子网络是语言模型基本预测功能的最小必要电路，为研究模型机制提供了新方法。

Abstract: In Transformer language models, activation vectors transform from current
token embeddings to next token predictions as they pass through the model. To
isolate a minimal form of this transformation, we identify language model
subnetworks that make bigram predictions, naive next token predictions based
only on the current token. We find that bigram subnetworks can be found in
fully trained language models up to 1B parameters, and these subnetworks are
critical for model performance even when they consist of less than 0.2% of
model parameters. Bigram subnetworks are concentrated in the first Transformer
MLP layer, and they overlap significantly with subnetworks trained to optimally
prune a given model. Mechanistically, the bigram subnetworks often recreate a
pattern from the full models where the first layer induces a sharp change that
aligns activations with next token predictions rather than current token
representations. Our results demonstrate that bigram subnetworks comprise a
minimal subset of parameters that are both necessary and sufficient for basic
next token predictions in language models, and they help drive the
transformation from current to next token activations in the residual stream.
These subnetworks can lay a foundation for studying language model circuits by
building up from a minimal circuit rather than the traditional approach of
ablating circuits from a full model.

</details>


### [6] [Speculative Sampling via Exponential Races](https://arxiv.org/abs/2504.15475)
*Szymon Kobus,Deniz Gündüz*

Main category: cs.CL

TL;DR: 论文通过将推测解码与信道模拟联系起来，分析了推测解码在加速大语言模型推理时的信息理论基础，并提出了新的ERSD方法。


<details>
  <summary>Details</summary>
Motivation: 探讨推测解码与信道模拟之间的联系，以信息论为基础分析推测解码的加速潜力。

Method: 通过信息论分析推测解码的加速上限，并提出基于指数竞赛的ERSD方法。

Result: 推导出生成速度与草稿模型生成令牌数k的关系，并验证ERSD方法的性能。

Conclusion: 推测解码与信道模拟的联系为加速大语言模型提供了理论支持，ERSD方法表现优异。

Abstract: Speculative decoding accelerates large language model inference using a
smaller draft model. In this paper, we establish a surprising connection
between speculative decoding and channel simulation, which aims at simulating a
noisy channel using as few bits as possible. This connection allows us to
provide an information-theoretic analysis of the speed up that can be achieved
by speculative decoding. Leveraging this link, we derive an explicit relation
between generation speed-up and the number of tokens $k$ generated by the draft
model for large $k$, which serves as an upper bound for all $k$. We also
propose a novel speculative decoding method via exponential race ERSD that
matches state-of-the-art performance.

</details>


### [7] [SimulS2S-LLM: Unlocking Simultaneous Inference of Speech LLMs for Speech-to-Speech Translation](https://arxiv.org/abs/2504.15509)
*Keqi Deng,Wenxi Chen,Xie Chen,Philip C. Woodland*

Main category: cs.CL

TL;DR: 论文提出SimulS2S-LLM，通过离线训练语音大模型和测试时策略，实现流式语音翻译，平衡质量和延迟。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在流式语音翻译中的挑战，如语音作为提示的匹配问题。

Method: 提取边界感知语音提示，预测离散语音标记，并使用预训练声码器合成语音。设计增量束搜索优化预测。

Result: 在CVSS数据上，SimulS2S-LLM在相同延迟下比现有方法提高ASR-BLEU分数3分。

Conclusion: SimulS2S-LLM在流式语音翻译中实现了更好的质量-延迟权衡。

Abstract: Simultaneous speech translation (SST) outputs translations in parallel with
streaming speech input, balancing translation quality and latency. While large
language models (LLMs) have been extended to handle the speech modality,
streaming remains challenging as speech is prepended as a prompt for the entire
generation process. To unlock LLM streaming capability, this paper proposes
SimulS2S-LLM, which trains speech LLMs offline and employs a test-time policy
to guide simultaneous inference. SimulS2S-LLM alleviates the mismatch between
training and inference by extracting boundary-aware speech prompts that allows
it to be better matched with text input data. SimulS2S-LLM achieves
simultaneous speech-to-speech translation (Simul-S2ST) by predicting discrete
output speech tokens and then synthesising output speech using a pre-trained
vocoder. An incremental beam search is designed to expand the search space of
speech token prediction without increasing latency. Experiments on the CVSS
speech data show that SimulS2S-LLM offers a better translation quality-latency
trade-off than existing methods that use the same training data, such as
improving ASR-BLEU scores by 3 points at similar latency.

</details>


### [8] [The Bitter Lesson Learned from 2,000+ Multilingual Benchmarks](https://arxiv.org/abs/2504.15521)
*Minghao Wu,Weixuan Wang,Sinuo Liu,Huifeng Yin,Xintong Wang,Yu Zhao,Chenyang Lyu,Longyue Wang,Weihua Luo,Kaifu Zhang*

Main category: cs.CL

TL;DR: 论文分析了2000多个多语言基准测试，发现英语仍占主导地位，且翻译基准与本地化基准在人类评估中的表现差异显著。


<details>
  <summary>Details</summary>
Motivation: 推动多语言评估的公平性，揭示当前多语言基准测试的局限性。

Method: 分析2021至2024年间148个国家的2000多个多语言基准测试，比较其与人类评估的相关性。

Result: 英语过度代表，本地化基准优于翻译基准，STEM任务与人类评估相关性高，传统NLP任务相关性低。

Conclusion: 需开发文化和语言定制的基准测试，并提出六项改进原则和五项研究方向。

Abstract: As large language models (LLMs) continue to advance in linguistic
capabilities, robust multilingual evaluation has become essential for promoting
equitable technological progress. This position paper examines over 2,000
multilingual (non-English) benchmarks from 148 countries, published between
2021 and 2024, to evaluate past, present, and future practices in multilingual
benchmarking. Our findings reveal that, despite significant investments
amounting to tens of millions of dollars, English remains significantly
overrepresented in these benchmarks. Additionally, most benchmarks rely on
original language content rather than translations, with the majority sourced
from high-resource countries such as China, India, Germany, the UK, and the
USA. Furthermore, a comparison of benchmark performance with human judgments
highlights notable disparities. STEM-related tasks exhibit strong correlations
with human evaluations (0.70 to 0.85), while traditional NLP tasks like
question answering (e.g., XQuAD) show much weaker correlations (0.11 to 0.30).
Moreover, translating English benchmarks into other languages proves
insufficient, as localized benchmarks demonstrate significantly higher
alignment with local human judgments (0.68) than their translated counterparts
(0.47). This underscores the importance of creating culturally and
linguistically tailored benchmarks rather than relying solely on translations.
Through this comprehensive analysis, we highlight six key limitations in
current multilingual evaluation practices, propose the guiding principles
accordingly for effective multilingual benchmarking, and outline five critical
research directions to drive progress in the field. Finally, we call for a
global collaborative effort to develop human-aligned benchmarks that prioritize
real-world applications.

</details>


### [9] [IPBench: Benchmarking the Knowledge of Large Language Models in Intellectual Property](https://arxiv.org/abs/2504.15524)
*Qiyao Wang,Guhong Chen,Hongbo Wang,Huaren Liu,Minghui Zhu,Zhifei Qin,Linwei Li,Yilin Yue,Shiqiang Wang,Jiayan Li,Yihang Wu,Ziqiang Liu,Longze Chen,Run Luo,Liyang Fan,Jiaming Li,Lei Zhang,Kan Xu,Hongfei Lin,Hamid Alinejad-Rokny,Shiwen Ni,Yuan Lin,Min Yang*

Main category: cs.CL

TL;DR: 论文提出了首个全面的IP任务分类法和双语基准IPBench，用于评估LLMs在知识产权领域的实际应用表现，发现现有模型仍有较大改进空间。


<details>
  <summary>Details</summary>
Motivation: 知识产权领域复杂且知识密集，现有数据集和基准未能全面覆盖实际需求，亟需一个更全面的评估工具。

Method: 引入IPBench，涵盖8种IP机制和20项任务，用于评估16种LLMs在理解和生成任务中的表现。

Result: 最佳模型准确率仅为75.8%，开源和法律导向模型表现不及闭源通用模型。

Conclusion: IPBench填补了知识产权领域评估工具的空白，未来将持续更新以更好地反映实际挑战。

Abstract: Intellectual Property (IP) is a unique domain that integrates technical and
legal knowledge, making it inherently complex and knowledge-intensive. As large
language models (LLMs) continue to advance, they show great potential for
processing IP tasks, enabling more efficient analysis, understanding, and
generation of IP-related content. However, existing datasets and benchmarks
either focus narrowly on patents or cover limited aspects of the IP field,
lacking alignment with real-world scenarios. To bridge this gap, we introduce
the first comprehensive IP task taxonomy and a large, diverse bilingual
benchmark, IPBench, covering 8 IP mechanisms and 20 tasks. This benchmark is
designed to evaluate LLMs in real-world intellectual property applications,
encompassing both understanding and generation. We benchmark 16 LLMs, ranging
from general-purpose to domain-specific models, and find that even the
best-performing model achieves only 75.8% accuracy, revealing substantial room
for improvement. Notably, open-source IP and law-oriented models lag behind
closed-source general-purpose models. We publicly release all data and code of
IPBench and will continue to update it with additional IP-related tasks to
better reflect real-world challenges in the intellectual property domain.

</details>


### [10] [Compass-V2 Technical Report](https://arxiv.org/abs/2504.15527)
*Sophia Maria*

Main category: cs.CL

TL;DR: Compass-v2是一个轻量级MoE模型，专为东南亚语言和电子商务设计，性能优越且推理成本低。


<details>
  <summary>Details</summary>
Motivation: 解决高资源语言主导的LLMs对东南亚低资源语言和电子商务领域的忽视。

Method: 采用30B总参数和5B活动参数的MoE架构，结合细粒度和共享专家模块，构建高质量数据集，并首创混合推理模型。

Result: 在30B以下模型中表现出东南亚多语言和电子商务领域的最先进性能，推理成本显著降低。

Conclusion: Compass-v2成功填补了低资源语言和电子商务领域的空白，为未来研究提供了新方向。

Abstract: Predominant LLMs focus on high-resource languages while leaving low-resource
languages, particularly those in Southeast Asia (SEA), underrepresented. In
addition, those models are general-purpose and pay limited attention to the
e-commerce domain. To overcome these limitations, we introduce Compass-v2, a
lightweight Mixture-of-Experts (MoE) model specifically designed for Southeast
Asian languages and e-commerce applications. To balance model performance and
inference cost, the model is designed with 30B total parameters and 5B active
parameters, incorporating both fine-grained and shared expert modules. To
enhance multilingual performance, we curated and constructed a high-quality,
industry-leading SEA dataset, to the best of our knowledge. To boost
performance in the e-commerce domain, we built a dataset comprising hundreds of
billions of tokens, sourced through external data mining and internal platform
collection. Besides, we pioneered a hybrid reasoning model that supports both
fast thinking and deep thinking within a unified framework to enhance the
reasoning capabilities, diverging from the conventional industry practice of
deploying two separate models. Through extensive experimental evaluations, our
model demonstrates state-of-the-art SEA multilingual and e-commerce performance
among sub-30B models, while maintaining significantly lower inference cost.

</details>


### [11] [llm-jp-modernbert: A ModernBERT Model Trained on a Large-Scale Japanese Corpus with Long Context Length](https://arxiv.org/abs/2504.15544)
*Issa Sugiura,Kouta Nakayama,Yusuke Oda*

Main category: cs.CL

TL;DR: 论文提出了一个名为llm-jp-modernbert的ModernBERT模型，训练于大规模日语语料库，支持8192个token的长上下文，并在填充掩码测试中表现良好。


<details>
  <summary>Details</summary>
Motivation: 探索大规模语料库和长上下文下编码器模型的预训练，填补与解码器模型相比的研究空白。

Method: 使用公开的大规模日语语料库训练ModernBERT模型，支持长上下文（8192 token），并通过填充掩码测试和伪困惑度实验分析模型效果。

Result: 模型在填充掩码测试中表现良好，但未超越现有基线；通过伪困惑度实验分析了上下文长度扩展的影响，并详细研究了句子嵌入的变化。

Conclusion: 模型在长上下文任务中表现良好，支持了长上下文BERT的发展，并公开了模型和代码以促进可重复性。

Abstract: Encoder-only transformer models like BERT are widely adopted as a pre-trained
backbone for tasks like sentence classification and retrieval. However,
pretraining of encoder models with large-scale corpora and long contexts has
been relatively underexplored compared to decoder-only transformers. In this
work, we present llm-jp-modernbert, a ModernBERT model trained on a publicly
available, massive Japanese corpus with a context length of 8192 tokens. While
our model does not surpass existing baselines on downstream tasks, it achieves
good results on fill-mask test evaluations. We also analyze the effect of
context length expansion through pseudo-perplexity experiments. Furthermore, we
investigate sentence embeddings in detail, analyzing their transitions during
training and comparing them with those from other existing models, confirming
similar trends with models sharing the same architecture. To support
reproducibility and foster the development of long-context BERT, we release our
model, along with the training and evaluation code.

</details>


### [12] [LLM-based Semantic Augmentation for Harmful Content Detection](https://arxiv.org/abs/2504.15548)
*Elyas Meguellati,Assaad Zeghina,Shazia Sadiq,Gianluca Demartini*

Main category: cs.CL

TL;DR: 论文提出了一种利用LLM进行文本预处理和语义增强的方法，以提升复杂社交媒体分类任务的性能，效果接近人工标注数据。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注LLM生成合成数据，而忽视了其在文本预处理和语义增强中的潜力。复杂社交媒体任务（如宣传检测、仇恨内容分类）中，LLM的零样本性能较差。

Method: 通过提示LLM清理噪声文本并提供上下文丰富的解释，增强训练集质量，而不大幅增加数据量。在多个数据集上系统评估。

Result: 零样本LLM分类在复杂任务中表现不佳，但结合语义增强后性能接近人工标注数据，成本显著降低。

Conclusion: 策略性地将LLM整合到机器学习流程中，对社交媒体分类任务至关重要，为在线有害内容治理提供了新思路。

Abstract: Recent advances in large language models (LLMs) have demonstrated strong
performance on simple text classification tasks, frequently under zero-shot
settings. However, their efficacy declines when tackling complex social media
challenges such as propaganda detection, hateful meme classification, and
toxicity identification. Much of the existing work has focused on using LLMs to
generate synthetic training data, overlooking the potential of LLM-based text
preprocessing and semantic augmentation. In this paper, we introduce an
approach that prompts LLMs to clean noisy text and provide context-rich
explanations, thereby enhancing training sets without substantial increases in
data volume. We systematically evaluate on the SemEval 2024 multi-label
Persuasive Meme dataset and further validate on the Google Jigsaw toxic
comments and Facebook hateful memes datasets to assess generalizability. Our
results reveal that zero-shot LLM classification underperforms on these
high-context tasks compared to supervised models. In contrast, integrating
LLM-based semantic augmentation yields performance on par with approaches that
rely on human-annotated data, at a fraction of the cost. These findings
underscore the importance of strategically incorporating LLMs into machine
learning (ML) pipeline for social media classification tasks, offering broad
implications for combating harmful content online.

</details>


### [13] [Instruction-Tuning Data Synthesis from Scratch via Web Reconstruction](https://arxiv.org/abs/2504.15573)
*Yuxin Jiang,Yufei Wang,Chuhan Wu,Xinyi Dai,Yan Xu,Weinan Gan,Yasheng Wang,Xin Jiang,Lifeng Shang,Ruiming Tang,Wei Wang*

Main category: cs.CL

TL;DR: WebR是一种全自动框架，直接从原始网页文档合成高质量指令调优数据，无需强假设，显著提升LLMs的指令跟随能力。


<details>
  <summary>Details</summary>
Motivation: 现有自动数据合成方法依赖种子数据质量或对网页结构的强假设，限制了数据质量和多样性。

Method: 提出WebR框架，通过双视角范式（网页作为指令或响应）从原始网页合成数据。

Result: WebR生成的数据在四个基准测试中优于现有方法16.65%，并展现出更好的兼容性、数据效率和可扩展性。

Conclusion: WebR为高质量指令调优数据合成提供了高效、可扩展的解决方案，显著提升LLMs性能。

Abstract: The improvement of LLMs' instruction-following capabilities depends
critically on the availability of high-quality instruction-response pairs.
While existing automatic data synthetic methods alleviate the burden of manual
curation, they often rely heavily on either the quality of seed data or strong
assumptions about the structure and content of web documents. To tackle these
challenges, we propose Web Reconstruction (WebR), a fully automated framework
for synthesizing high-quality instruction-tuning (IT) data directly from raw
web documents with minimal assumptions. Leveraging the inherent diversity of
raw web content, we conceptualize web reconstruction as an instruction-tuning
data synthesis task via a novel dual-perspective paradigm--Web as Instruction
and Web as Response--where each web document is designated as either an
instruction or a response to trigger the reconstruction process. Comprehensive
experiments show that datasets generated by WebR outperform state-of-the-art
baselines by up to 16.65% across four instruction-following benchmarks.
Notably, WebR demonstrates superior compatibility, data efficiency, and
scalability, enabling enhanced domain adaptation with minimal effort. The data
and code are publicly available at https://github.com/YJiangcm/WebR.

</details>


### [14] [Exploring Next Token Prediction in Theory of Mind (ToM) Tasks: Comparative Experiments with GPT-2 and LLaMA-2 AI Models](https://arxiv.org/abs/2504.15604)
*Pavan Yadav,Nikhil Khandalkar,Krishna Shinde,Lokesh B. Ramegowda,Rajarshi Das*

Main category: cs.CL

TL;DR: 该研究比较了GPT-2和Llama-2-7b-chat-hf在心理理论任务中的表现，发现Llama-2表现更优，尤其是在低温度设置下。


<details>
  <summary>Details</summary>
Motivation: 评估语言模型在复杂心理理论任务中的表现，以了解其在不同上下文和推理层次下的能力。

Method: 使用GPT-4增强数据集，测试模型在四种温度设置和三种推理层次下的表现。

Result: Llama-2优于GPT-2，尤其是在低温度下；上下文复杂性增加会降低预测准确性。

Conclusion: 模型架构、温度和上下文复杂性显著影响预测能力，揭示了当前语言模型的优缺点。

Abstract: Language models have made significant progress in generating coherent text
and predicting next tokens based on input prompts. This study compares the
next-token prediction performance of two well-known models: OpenAI's GPT-2 and
Meta's Llama-2-7b-chat-hf on Theory of Mind (ToM) tasks. To evaluate their
capabilities, we built a dataset from 10 short stories sourced from the Explore
ToM Dataset. We enhanced these stories by programmatically inserting additional
sentences (infills) using GPT-4, creating variations that introduce different
levels of contextual complexity. This setup enables analysis of how increasing
context affects model performance. We tested both models under four temperature
settings (0.01, 0.5, 1.0, 2.0) and evaluated their ability to predict the next
token across three reasoning levels. Zero-order reasoning involves tracking the
state, either current (ground truth) or past (memory). First-order reasoning
concerns understanding another's mental state (e.g., "Does Anne know the apple
is salted?"). Second-order reasoning adds recursion (e.g., "Does Anne think
that Charles knows the apple is salted?").
  Our results show that adding more infill sentences slightly reduces
prediction accuracy, as added context increases complexity and ambiguity.
Llama-2 consistently outperforms GPT-2 in prediction accuracy, especially at
lower temperatures, demonstrating greater confidence in selecting the most
probable token. As reasoning complexity rises, model responses diverge more.
Notably, GPT-2 and Llama-2 display greater variability in predictions during
first- and second-order reasoning tasks. These findings illustrate how model
architecture, temperature, and contextual complexity influence next-token
prediction, contributing to a better understanding of the strengths and
limitations of current language models.

</details>


### [15] [Exploiting Contextual Knowledge in LLMs through V-usable Information based Layer Enhancement](https://arxiv.org/abs/2504.15630)
*Xiaowei Yuan,Zhao Yang,Ziyang Huang,Yequan Wang,Siqi Fan,Yiming Ju,Jun Zhao,Kang Liu*

Main category: cs.CL

TL;DR: 论文提出了一种名为CaLE的新方法，通过增强LLMs内部表示中的上下文知识利用，改进了上下文忠实生成。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视了LLMs内部状态中上下文信息的处理机制，导致其无法充分利用上下文知识。

Method: 提出Context-aware Layer Enhancement (CaLE)，通过V-usable信息分析在最优层增强上下文信息，丰富最终层的表示。

Result: 实验表明，CaLE在问答任务中有效提升了上下文忠实生成，尤其是在未知或冲突上下文知识的情况下。

Conclusion: CaLE通过优化内部表示机制，显著提升了LLMs的上下文忠实生成能力。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
various tasks, yet they often struggle with context-faithfulness generations
that properly reflect contextual knowledge. While existing approaches focus on
enhancing the decoding strategies, they ignore the fundamental mechanism of how
contextual information is processed within LLMs' internal states. As a result,
LLMs remain limited in their ability to fully leverage contextual knowledge. In
this paper, we propose Context-aware Layer Enhancement (CaLE), a novel
intervention method that enhances the utilization of contextual knowledge
within LLMs' internal representations. By employing V-usable information
analysis, CaLE strategically amplifies the growth of contextual information at
an optimal layer, thereby enriching representations in the final layer. Our
experiments demonstrate that CaLE effectively improves context-faithful
generation in Question-Answering tasks, particularly in scenarios involving
unknown or conflicting contextual knowledge.

</details>


### [16] [Cost-Effective Text Clustering with Large Language Models](https://arxiv.org/abs/2504.15640)
*Hongtao Wang,Taiyan Zhang,Renchi Yang,Jianliang Xu*

Main category: cs.CL

TL;DR: TECL是一个成本效益高的框架，利用LLMs的反馈在有限查询预算内实现准确的文本聚类。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs在文本聚类中因大量API查询或推理调用带来的计算和财务开销问题。

Method: 采用EdgeLLM或TriangleLLM构建文本对的must-link/cannot-link约束，并通过加权约束聚类方法生成聚类。

Result: 在多个基准数据集上，TECL在相同查询成本下显著优于现有无监督文本聚类方法。

Conclusion: TECL通过高效利用LLMs反馈，实现了在有限预算内的高质量文本聚类。

Abstract: Text clustering aims to automatically partition a collection of text
documents into distinct clusters based on linguistic features. In the
literature, this task is usually framed as metric clustering based on text
embeddings from pre-trained encoders or a graph clustering problem upon
pairwise similarities from an oracle, e.g., a large ML model. Recently, large
language models (LLMs) bring significant advancement in this field by offering
contextualized text embeddings and highly accurate similarity scores, but
meanwhile, present grand challenges to cope with substantial computational
and/or financial overhead caused by numerous API-based queries or inference
calls to the models.
  In response, this paper proposes TECL, a cost-effective framework that taps
into the feedback from LLMs for accurate text clustering within a limited
budget of queries to LLMs. Under the hood, TECL adopts our EdgeLLM or
TriangleLLM to construct must-link/cannot-link constraints for text pairs, and
further leverages such constraints as supervision signals input to our weighted
constrained clustering approach to generate clusters. Particularly, EdgeLLM
(resp. TriangleLLM) enables the identification of informative text pairs (resp.
triplets) for querying LLMs via well-thought-out greedy algorithms and accurate
extraction of pairwise constraints through carefully-crafted prompting
techniques. Our experiments on multiple benchmark datasets exhibit that TECL
consistently and considerably outperforms existing solutions in unsupervised
text clustering under the same query cost for LLMs.

</details>


### [17] [Computational Typology](https://arxiv.org/abs/2504.15642)
*Gerhard Jäger*

Main category: cs.CL

TL;DR: 本文探讨了计算统计模型在语言类型学研究中的应用及其优势。


<details>
  <summary>Details</summary>
Motivation: 语言类型学旨在通过结构特征分类语言，理解语言多样性。近年来，计算方法的引入为大规模语言数据分析提供了新工具。

Method: 采用计算统计建模方法，分析大规模语言数据并验证语言结构及演化的假设。

Result: 计算统计模型显著提升了语言类型学研究的效率和准确性。

Conclusion: 计算统计模型为语言类型学研究提供了强大的工具，推动了该领域的发展。

Abstract: Typology is a subfield of linguistics that focuses on the study and
classification of languages based on their structural features. Unlike
genealogical classification, which examines the historical relationships
between languages, typology seeks to understand the diversity of human
languages by identifying common properties and patterns, known as universals.
In recent years, computational methods have played an increasingly important
role in typological research, enabling the analysis of large-scale linguistic
data and the testing of hypotheses about language structure and evolution. This
article provides an illustration of the benefits of computational statistical
modeling in typology.

</details>


### [18] [FinTextSim: Enhancing Financial Text Analysis with BERTopic](https://arxiv.org/abs/2504.15683)
*Simon Jehnen,Joaquín Ordieres-Meré,Javier Villalba-Díez*

Main category: cs.CL

TL;DR: 研究探讨了BERTopic与FinTextSim结合在分析10-K文件中的有效性，FinTextSim显著提升了主题聚类质量。


<details>
  <summary>Details</summary>
Motivation: 信息可用性和计算能力的进步促使传统财务指标与文本数据结合分析，需要自动化工具提取价值。

Method: 使用BERTopic和FinTextSim分析S&P 500公司10-K文件的Item 7和7A，比较FinTextSim与all-MiniLM-L6-v2的效果。

Result: FinTextSim提升主题内相似度81%，降低主题间相似度100%，BERTopic仅与FinTextSim结合时形成清晰主题。

Conclusion: FinTextSim对金融文本分析至关重要，提升研究质量和决策效率，潜在影响业务估值和股价预测。

Abstract: Recent advancements in information availability and computational
capabilities have transformed the analysis of annual reports, integrating
traditional financial metrics with insights from textual data. To extract
valuable insights from this wealth of textual data, automated review processes,
such as topic modeling, are crucial. This study examines the effectiveness of
BERTopic, a state-of-the-art topic model relying on contextual embeddings, for
analyzing Item 7 and Item 7A of 10-K filings from S&P 500 companies
(2016-2022). Moreover, we introduce FinTextSim, a finetuned
sentence-transformer model optimized for clustering and semantic search in
financial contexts. Compared to all-MiniLM-L6-v2, the most widely used
sentence-transformer, FinTextSim increases intratopic similarity by 81% and
reduces intertopic similarity by 100%, significantly enhancing organizational
clarity. We assess BERTopic's performance using embeddings from both FinTextSim
and all-MiniLM-L6-v2. Our findings reveal that BERTopic only forms clear and
distinct economic topic clusters when paired with FinTextSim's embeddings.
Without FinTextSim, BERTopic struggles with misclassification and overlapping
topics. Thus, FinTextSim is pivotal for advancing financial text analysis.
FinTextSim's enhanced contextual embeddings, tailored for the financial domain,
elevate the quality of future research and financial information. This improved
quality of financial information will enable stakeholders to gain a competitive
advantage, streamlining resource allocation and decision-making processes.
Moreover, the improved insights have the potential to leverage business
valuation and stock price prediction models.

</details>


### [19] [Subject islands do not reduce to construction-specific discourse function](https://arxiv.org/abs/2504.15688)
*Mandy Cartner,Matthew Kogan,Nikolas Webster,Matthew Wagers,Ivy Sichel*

Main category: cs.CL

TL;DR: 论文探讨了语言学中的'岛屿效应'，特别是主语作为岛屿的现象。传统生成语法认为这是句法自主性的结果，而功能语言学则认为与信息结构有关。通过实验，作者发现主语岛屿效应存在于多种结构中，支持句法自主性观点。


<details>
  <summary>Details</summary>
Motivation: 研究主语作为岛屿现象的原因，验证功能语言学与生成语法对岛屿效应的解释。

Method: 通过三个大规模可接受性研究，使用超加性设计，测试wh-疑问句、关系从句和话题化结构中的主语岛屿效应。

Result: 实验发现主语岛屿效应存在于所有测试结构中，与功能语言学的预测不符，支持句法自主性观点。

Conclusion: 主语岛屿效应更可能与抽象的句法表征相关，而非信息结构或交际功能。

Abstract: The term islands in linguistics refers to phrases from which extracting an
element results in ungrammaticality (Ross, 1967). Grammatical subjects are
considered islands because extracting a sub-part of a subject results in an
ill-formed sentence, despite having a clear intended meaning (e.g., "Which
topic did the article about inspire you?"). The generative tradition, which
views syntax as autonomous of meaning and function, attributes this
ungrammaticality to the abstract movement dependency between the wh-phrase and
the subject-internal position with which it is associated for interpretation.
However, research on language that emphasizes its communicative function
suggests instead that syntactic constraints, including islands, can be
explained based on the way different constructions package information.
Accordingly, Abeill\'e et al. (2020) suggest that the islandhood of subjects is
specific to the information structure of wh-questions, and propose that
subjects are not islands for movement, but for focusing, due to their
discourse-backgroundedness. This predicts that other constructions that differ
in their information structure from wh-questions, but still involve movement,
should not create a subject island effect. We test this prediction in three
large-scale acceptability studies, using a super-additive design that singles
out subject island violations, in three different constructions: wh-questions,
relative clauses, and topicalization. We report evidence for a subject island
effect in each construction type, despite only wh-questions introducing what
Abeill\'e et al. (2020) call "a clash in information structure." We argue that
this motivates an account of islands in terms of abstract, syntactic
representations, independent of the communicative function associated with the
constructions.

</details>


### [20] [Tina: Tiny Reasoning Models via LoRA](https://arxiv.org/abs/2504.15777)
*Shangshang Wang,Julian Asilis,Ömer Faruk Akgül,Enes Burak Bilgin,Ollie Liu,Willie Neiswanger*

Main category: cs.CL

TL;DR: Tina是一种高效的小型推理模型，通过低成本的后训练方法（如LoRA）在1.5B参数的基础模型上实现强大的推理能力，性能接近或超越SOTA模型，同时大幅降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 探索如何在语言模型中高效且低成本地实现强大的推理能力。

Method: 使用参数高效更新（LoRA）在强化学习中对小型基础模型（1.5B参数）进行微调。

Result: Tina在多个开源推理数据集上表现优异，性能提升20%，Pass@1准确率达43.33%，后训练成本仅为9美元。

Conclusion: LoRA通过快速适应RL奖励的推理结构，同时保留基础模型知识，展示了高效推理的潜力。所有代码和模型已开源。

Abstract: How cost-effectively can strong reasoning abilities be achieved in language
models? Driven by this fundamental question, we present Tina, a family of tiny
reasoning models achieved with high cost-efficiency. Notably, Tina demonstrates
that substantial reasoning performance can be developed using only minimal
resources, by applying parameter-efficient updates during reinforcement
learning (RL), using low-rank adaptation (LoRA), to an already tiny 1.5B
parameter base model. This minimalist approach produces models that achieve
reasoning performance which is competitive with, and sometimes surpasses, SOTA
RL reasoning models built upon the same base model. Crucially, this is achieved
at a tiny fraction of the computational post-training cost employed by existing
SOTA models. In fact, the best Tina model achieves a >20\% reasoning
performance increase and 43.33\% Pass@1 accuracy on AIME24, at only \$9 USD
post-training and evaluation cost (i.e., an estimated 260x cost reduction). Our
work reveals the surprising effectiveness of efficient RL reasoning via LoRA.
We validate this across multiple open-source reasoning datasets and various
ablation settings starting with a single, fixed set of hyperparameters.
Furthermore, we hypothesize that this effectiveness and efficiency stem from
LoRA rapidly adapting the model to the structural format of reasoning rewarded
by RL, while largely preserving the base model's underlying knowledge. In
service of accessibility and open research, we fully open-source all code,
training logs, and model weights \& checkpoints.

</details>


### [21] [Automated Creativity Evaluation for Large Language Models: A Reference-Based Approach](https://arxiv.org/abs/2504.15784)
*Ruizhe Li,Chiwei Zhu,Benfeng Xu,Xiaorui Wang,Zhendong Mao*

Main category: cs.CL

TL;DR: 提出了一种基于TTCW的自动化评估方法，用于评估LLM生成文本的创造力，显著提高了与人类评估的一致性。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法依赖高成本人工标注或与人类评估不一致，需要更有效的自动化方法。

Method: 采用基于参考的Likert风格评分，将生成文本与高质量参考文本进行比较。

Result: 实验结果显示，该方法显著提升了与人类评估的一致性，配对准确率达到0.75（提升15%）。

Conclusion: 该方法为LLM创造力评估提供了一种高效且可靠的自动化解决方案。

Abstract: Creative writing is a key capability of Large Language Models (LLMs), with
potential applications in literature, storytelling, and various creative
domains. However, evaluating the creativity of machine-generated texts remains
a significant challenge, as existing methods either rely on costly manual
annotations or fail to align closely with human assessments. In this paper, we
propose an effective automated evaluation method based on the Torrance Test of
Creative Writing (TTCW), which evaluates creativity as product. Our method
employs a reference-based Likert-style approach, scoring generated creative
texts relative to high-quality reference texts across various tests.
Experimental results demonstrate that our method significantly improves the
alignment between LLM evaluations and human assessments, achieving a pairwise
accuracy of 0.75 (+15\%).

</details>


### [22] [A closer look at how large language models trust humans: patterns and biases](https://arxiv.org/abs/2504.15801)
*Valeria Lerman,Yaniv Dover*

Main category: cs.CL

TL;DR: 研究探讨了大型语言模型（LLM）如何基于人类的能力、善意和诚信三个维度形成信任，并发现其信任模式与人类相似，但也存在偏见。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在决策场景中与人类互动增多，理解AI对人类的信任动态成为关键问题。

Method: 基于行为理论，通过43,200次模拟实验，分析五种流行语言模型在五种场景中的信任形成。

Result: LLM的信任形成与人类相似，但受信任维度和人口统计变量（如年龄、宗教、性别）影响，尤其在金融场景中。

Conclusion: 需进一步研究AI对人类的信任动态，并监控偏见，以避免信任敏感应用中潜在的有害结果。

Abstract: As large language models (LLMs) and LLM-based agents increasingly interact
with humans in decision-making contexts, understanding the trust dynamics
between humans and AI agents becomes a central concern. While considerable
literature studies how humans trust AI agents, it is much less understood how
LLM-based agents develop effective trust in humans. LLM-based agents likely
rely on some sort of implicit effective trust in trust-related contexts (e.g.,
evaluating individual loan applications) to assist and affect decision making.
Using established behavioral theories, we develop an approach that studies
whether LLMs trust depends on the three major trustworthiness dimensions:
competence, benevolence and integrity of the human subject. We also study how
demographic variables affect effective trust. Across 43,200 simulated
experiments, for five popular language models, across five different scenarios
we find that LLM trust development shows an overall similarity to human trust
development. We find that in most, but not all cases, LLM trust is strongly
predicted by trustworthiness, and in some cases also biased by age, religion
and gender, especially in financial scenarios. This is particularly true for
scenarios common in the literature and for newer models. While the overall
patterns align with human-like mechanisms of effective trust formation,
different models exhibit variation in how they estimate trust; in some cases,
trustworthiness and demographic factors are weak predictors of effective trust.
These findings call for a better understanding of AI-to-human trust dynamics
and monitoring of biases and trust development patterns to prevent unintended
and potentially harmful outcomes in trust-sensitive applications of AI.

</details>


### [23] [What's the Difference? Supporting Users in Identifying the Effects of Prompt and Model Changes Through Token Patterns](https://arxiv.org/abs/2504.15815)
*Michael A. Hedderich,Anyi Wang,Raoyuan Zhao,Florian Eichin,Barbara Plank*

Main category: cs.CL

TL;DR: 本文提出了一种名为Spotlight的新方法，结合自动化和人工分析，以解决大型语言模型提示工程的挑战。通过数据挖掘技术，自动区分随机变化和系统性差异，并提供标记模式以指导用户高效分析提示和模型变化的影响。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法（自动化指标或人工评估）存在局限性，如提供有限洞察或劳动密集型。需要一种更高效且深入的方法来支持提示工程和模型行为研究。

Method: 基于数据挖掘技术，自动提取标记模式以区分随机变化和系统性差异，并通过三个基准测试验证方法的可靠性。结合人工分析，帮助用户理解模型输出的系统性差异。

Result: Spotlight方法能够发现由提示和模型变化引起的相关差异（如性别或文化相关），并为用户提供新洞察。用户研究表明，该方法有助于理解模型行为。

Conclusion: Spotlight方法为提示工程和人类中心模型行为研究提供了高效且深入的支持，能够发现系统性差异并指导用户分析。

Abstract: Prompt engineering for large language models is challenging, as even small
prompt perturbations or model changes can significantly impact the generated
output texts. Existing evaluation methods, either automated metrics or human
evaluation, have limitations, such as providing limited insights or being
labor-intensive. We propose Spotlight, a new approach that combines both
automation and human analysis. Based on data mining techniques, we
automatically distinguish between random (decoding) variations and systematic
differences in language model outputs. This process provides token patterns
that describe the systematic differences and guide the user in manually
analyzing the effects of their prompt and model changes efficiently. We create
three benchmarks to quantitatively test the reliability of token pattern
extraction methods and demonstrate that our approach provides new insights into
established prompt data. From a human-centric perspective, through
demonstration studies and a user study, we show that our token pattern approach
helps users understand the systematic differences of language model outputs,
and we are able to discover relevant differences caused by prompt and model
changes (e.g. related to gender or culture), thus supporting the prompt
engineering process and human-centric model behavior research.

</details>


### [24] [Pre-DPO: Improving Data Utilization in Direct Preference Optimization Using a Guiding Reference Model](https://arxiv.org/abs/2504.15843)
*Junshu Pan,Wei Shen,Shulin Huang,Qiji Zhou,Yue Zhang*

Main category: cs.CL

TL;DR: Pre-DPO是一种基于DPO的训练范式，通过引入指导性参考模型提升偏好优化性能，实验证明其优于DPO和SimPO。


<details>
  <summary>Details</summary>
Motivation: DPO和SimPO在训练中存在数据利用效率低和鲁棒性不足的问题，Pre-DPO旨在解决这些问题。

Method: Pre-DPO利用参考模型动态调整样本权重，优化训练过程。

Result: 在AlpacaEval 2.0和Arena-Hard v0.1基准测试中，Pre-DPO显著提升了性能。

Conclusion: Pre-DPO是一种简单有效的方法，无需额外数据或模型即可改进偏好优化。

Abstract: Direct Preference Optimization (DPO) simplifies reinforcement learning from
human feedback (RLHF) for large language models (LLMs) by directly optimizing
human preferences without an explicit reward model. We find that during DPO
training, the reference model plays the role of a data weight adjuster.
However, the common practice of initializing the policy and reference models
identically in DPO can lead to inefficient data utilization and impose a
performance ceiling. Meanwhile, the lack of a reference model in Simple
Preference Optimization (SimPO) reduces training robustness and necessitates
stricter conditions to prevent catastrophic forgetting. In this work, we
propose Pre-DPO, a simple yet effective DPO-based training paradigm that
enhances preference optimization performance by leveraging a guiding reference
model. This reference model provides foresight into the optimal policy state
achievable through the training preference data, serving as a guiding mechanism
that adaptively assigns higher weights to samples more suitable for the model
and lower weights to those less suitable. Extensive experiments on AlpacaEval
2.0 and Arena-Hard v0.1 benchmarks demonstrate that Pre-DPO consistently
improves the performance of both DPO and SimPO, without relying on external
models or additional data.

</details>


### [25] [Exploring Cognitive and Aesthetic Causality for Multimodal Aspect-Based Sentiment Analysis](https://arxiv.org/abs/2504.15848)
*Luwei Xiao,Rui Mao,Shuai Zhao,Qika Lin,Yanhao Jia,Liang He,Erik Cambria*

Main category: cs.CL

TL;DR: 本文提出了一种名为Chimera的多模态情感分类框架，结合认知美学和情感因果关系，以提升对细粒度视觉内容的理解和情感表达驱动因素的分析。


<details>
  <summary>Details</summary>
Motivation: 现有多模态情感分类方法在理解细粒度视觉内容和情感认知机制方面存在不足，需要更全面的框架来捕捉语义内容和情感认知共振的影响。

Method: Chimera框架通过视觉补丁特征对齐、细粒度视觉区域提取和文本描述转换，结合大型语言模型生成的情感原因和印象，增强模型对情感线索的感知。

Result: 实验结果表明，该模型在标准数据集上表现优异，且比GPT-4o等大型语言模型更具灵活性。

Conclusion: Chimera框架通过结合认知美学和情感因果关系，显著提升了多模态情感分类的准确性和灵活性。

Abstract: Multimodal aspect-based sentiment classification (MASC) is an emerging task
due to an increase in user-generated multimodal content on social platforms,
aimed at predicting sentiment polarity toward specific aspect targets (i.e.,
entities or attributes explicitly mentioned in text-image pairs). Despite
extensive efforts and significant achievements in existing MASC, substantial
gaps remain in understanding fine-grained visual content and the cognitive
rationales derived from semantic content and impressions (cognitive
interpretations of emotions evoked by image content). In this study, we present
Chimera: a cognitive and aesthetic sentiment causality understanding framework
to derive fine-grained holistic features of aspects and infer the fundamental
drivers of sentiment expression from both semantic perspectives and
affective-cognitive resonance (the synergistic effect between emotional
responses and cognitive interpretations). Specifically, this framework first
incorporates visual patch features for patch-word alignment. Meanwhile, it
extracts coarse-grained visual features (e.g., overall image representation)
and fine-grained visual regions (e.g., aspect-related regions) and translates
them into corresponding textual descriptions (e.g., facial, aesthetic).
Finally, we leverage the sentimental causes and impressions generated by a
large language model (LLM) to enhance the model's awareness of sentimental cues
evoked by semantic content and affective-cognitive resonance. Experimental
results on standard MASC datasets demonstrate the effectiveness of the proposed
model, which also exhibits greater flexibility to MASC compared to LLMs such as
GPT-4o. We have publicly released the complete implementation and dataset at
https://github.com/Xillv/Chimera

</details>


### [26] [Dynamic Early Exit in Reasoning Models](https://arxiv.org/abs/2504.15895)
*Chenxu Yang,Qingyi Si,Yongjie Duan,Zheliang Zhu,Chenyu Zhu,Zheng Lin,Li Cao,Weiping Wang*

Main category: cs.CL

TL;DR: 提出一种自截断CoT序列的方法，通过动态终止冗余推理步骤，提高效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 长CoT推理可能导致效率低下和准确性损失，需解决冗余推理问题。

Method: 在推理过程中监控模型行为，动态终止低效推理链，无需额外训练。

Result: 在多个基准测试中，CoT序列长度减少31%-43%，准确性提高1.7%-5.7%。

Conclusion: 该方法简单有效，适用于现有推理LLMs，显著提升性能。

Abstract: Recent advances in large reasoning language models (LRLMs) rely on test-time
scaling, which extends long chain-of-thought (CoT) generation to solve complex
tasks. However, overthinking in long CoT not only slows down the efficiency of
problem solving, but also risks accuracy loss due to the extremely detailed or
redundant reasoning steps. We propose a simple yet effective method that allows
LLMs to self-truncate CoT sequences by early exit during generation. Instead of
relying on fixed heuristics, the proposed method monitors model behavior at
potential reasoning transition points (e.g.,"Wait" tokens) and dynamically
terminates the next reasoning chain's generation when the model exhibits high
confidence in a trial answer. Our method requires no additional training and
can be seamlessly integrated into existing o1-like reasoning LLMs. Experiments
on multiple reasoning benchmarks MATH-500, AMC 2023, GPQA Diamond and AIME 2024
show that the proposed method is consistently effective on deepseek-series
reasoning LLMs, reducing the length of CoT sequences by an average of 31% to
43% while improving accuracy by 1.7% to 5.7%.

</details>


### [27] [SARI: Structured Audio Reasoning via Curriculum-Guided Reinforcement Learning](https://arxiv.org/abs/2504.15900)
*Cheng Wen,Tingwei Guo,Shuaijiang Zhao,Wei Zou,Xiangang Li*

Main category: cs.CL

TL;DR: 论文通过强化学习（RL）提升大型音频语言模型（LALM）的推理能力，提出结构化音频推理模型SARI，在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 探索强化学习如何提升音频语言模型的推理能力，填补现有研究的空白。

Method: 采用两阶段训练：监督微调（SFT）和课程引导的GRPO强化学习，比较结构化与非结构化推理。

Result: SARI模型在基准测试中平均准确率提升16.35%，并在Qwen2.5-Omni上达到67.08%的SOTA性能。

Conclusion: 结构化推理和课程学习显著提升音频语言理解能力。

Abstract: Recent work shows that reinforcement learning(RL) can markedly sharpen the
reasoning ability of large language models (LLMs) by prompting them to "think
before answering." Yet whether and how these gains transfer to audio-language
reasoning remains largely unexplored. We extend the Group-Relative Policy
Optimization (GRPO) framework from DeepSeek-R1 to a Large Audio-Language Model
(LALM), and construct a 32k sample multiple-choice corpus. Using a two-stage
regimen supervised fine-tuning on structured and unstructured
chains-of-thought, followed by curriculum-guided GRPO, we systematically
compare implicit vs. explicit, and structured vs. free form reasoning under
identical architectures. Our structured audio reasoning model, SARI (Structured
Audio Reasoning via Curriculum-Guided Reinforcement Learning), achieves a
16.35% improvement in average accuracy over the base model
Qwen2-Audio-7B-Instruct. Furthermore, the variant built upon Qwen2.5-Omni
reaches state-of-the-art performance of 67.08% on the MMAU test-mini benchmark.
Ablation experiments show that on the base model we use: (i) SFT warm-up is
important for stable RL training, (ii) structured chains yield more robust
generalization than unstructured ones, and (iii) easy-to-hard curricula
accelerate convergence and improve final performance. These findings
demonstrate that explicit, structured reasoning and curriculum learning
substantially enhances audio-language understanding.

</details>


### [28] [FairTranslate: An English-French Dataset for Gender Bias Evaluation in Machine Translation by Overcoming Gender Binarity](https://arxiv.org/abs/2504.15941)
*Fanny Jourdan,Yannick Chevalier,Cécile Favre*

Main category: cs.CL

TL;DR: FairTranslate是一个新数据集，用于评估LLM在英语到法语翻译中的非二元性别偏见，结果显示现有模型存在显著偏见。


<details>
  <summary>Details</summary>
Motivation: LLM在翻译包容性语言（如单数'they'代词）时表现不佳，需要系统性评估其偏见。

Method: 创建FairTranslate数据集（2418句对），评估四种LLM在不同提示下的表现。

Result: LLM在性别表示上存在显著偏见，翻译结果不公平。

Conclusion: 需针对性策略确保LLM翻译的公平性和包容性，数据集和代码已公开。

Abstract: Large Language Models (LLMs) are increasingly leveraged for translation tasks
but often fall short when translating inclusive language -- such as texts
containing the singular 'they' pronoun or otherwise reflecting fair linguistic
protocols. Because these challenges span both computational and societal
domains, it is imperative to critically evaluate how well LLMs handle inclusive
translation with a well-founded framework.
  This paper presents FairTranslate, a novel, fully human-annotated dataset
designed to evaluate non-binary gender biases in machine translation systems
from English to French. FairTranslate includes 2418 English-French sentence
pairs related to occupations, annotated with rich metadata such as the
stereotypical alignment of the occupation, grammatical gender indicator
ambiguity, and the ground-truth gender label (male, female, or inclusive).
  We evaluate four leading LLMs (Gemma2-2B, Mistral-7B, Llama3.1-8B,
Llama3.3-70B) on this dataset under different prompting procedures. Our results
reveal substantial biases in gender representation across LLMs, highlighting
persistent challenges in achieving equitable outcomes in machine translation.
These findings underscore the need for focused strategies and interventions
aimed at ensuring fair and inclusive language usage in LLM-based translation
systems.
  We make the FairTranslate dataset publicly available on Hugging Face, and
disclose the code for all experiments on GitHub.

</details>


### [29] [W-PCA Based Gradient-Free Proxy for Efficient Search of Lightweight Language Models](https://arxiv.org/abs/2504.15983)
*Shang Wang*

Main category: cs.CL

TL;DR: 本文提出了一种名为W-PCA的新型零样本神经架构搜索方法，用于高效设计和评估轻量级语言模型，显著减少了训练时间并提升了测试性能。


<details>
  <summary>Details</summary>
Motivation: 现有零样本NAS方法存在评估指标偏差和计算效率低的问题，需要一种更高效且准确的轻量级语言模型设计方法。

Method: 采用参数计数和FFN层主成分贡献率作为评估代理，无需梯度计算，优化了评估时间。

Result: 在GLUE和SQuAD数据集上表现优于现有方法，减少了训练时间并提升了测试分数。

Conclusion: W-PCA方法在轻量级语言模型设计中具有高效性和优越性能，适用于实际应用。

Abstract: The demand for efficient natural language processing (NLP) systems has led to
the development of lightweight language models. Previous work in this area has
primarily focused on manual design or training-based neural architecture search
(NAS) methods. Recently, zero-shot NAS methods have been proposed for
evaluating language models without the need for training. However, prevailing
approaches to zero-shot NAS often face challenges such as biased evaluation
metrics and computational inefficiencies. In this paper, we introduce
weight-weighted PCA (W-PCA), a novel zero-shot NAS method specifically tailored
for lightweight language models. Our approach utilizes two evaluation proxies:
the parameter count and the number of principal components with cumulative
contribution exceeding $\eta$ in the feed-forward neural (FFN) layer.
Additionally, by eliminating the need for gradient computations, we optimize
the evaluation time, thus enhancing the efficiency of designing and evaluating
lightweight language models. We conduct a comparative analysis on the GLUE and
SQuAD datasets to evaluate our approach. The results demonstrate that our
method significantly reduces training time compared to one-shot NAS methods and
achieves higher scores in the testing phase compared to previous
state-of-the-art training-based methods. Furthermore, we perform ranking
evaluations on a dataset sampled from the FlexiBERT search space. Our approach
exhibits superior ranking correlation and further reduces solving time compared
to other zero-shot NAS methods that require gradient computation.

</details>


### [30] [Few-shot Hate Speech Detection Based on the MindSpore Framework](https://arxiv.org/abs/2504.15987)
*Zhenkai Qin,Dongze Wu,Yuxin Liu,Guifang Yang*

Main category: cs.CL

TL;DR: 提出MS-FSLHate框架，结合提示学习和对抗增强，提升少样本仇恨言论检测性能。


<details>
  <summary>Details</summary>
Motivation: 社交媒体仇恨言论泛滥，现有深度学习模型在少样本或低资源场景下性能下降。

Method: 提出MS-FSLHate框架，整合可学习提示嵌入、CNN-BiLSTM主干网络、注意力池化和同义词对抗数据增强。

Result: 在HateXplain和HSOL数据集上表现优于基线模型，精度、召回率和F1分数均有提升。

Conclusion: 提示学习与对抗增强结合，适用于资源受限环境，提升少样本仇恨言论检测的鲁棒性和适应性。

Abstract: The proliferation of hate speech on social media poses a significant threat
to online communities, requiring effective detection systems. While deep
learning models have shown promise, their performance often deteriorates in
few-shot or low-resource settings due to reliance on large annotated corpora.
To address this, we propose MS-FSLHate, a prompt-enhanced neural framework for
few-shot hate speech detection implemented on the MindSpore deep learning
platform. The model integrates learnable prompt embeddings, a CNN-BiLSTM
backbone with attention pooling, and synonym-based adversarial data
augmentation to improve generalization. Experimental results on two benchmark
datasets-HateXplain and HSOL-demonstrate that our approach outperforms
competitive baselines in precision, recall, and F1-score. Additionally, the
framework shows high efficiency and scalability, suggesting its suitability for
deployment in resource-constrained environments. These findings highlight the
potential of combining prompt-based learning with adversarial augmentation for
robust and adaptable hate speech detection in few-shot scenarios.

</details>


### [31] [CAPO: Cost-Aware Prompt Optimization](https://arxiv.org/abs/2504.16005)
*Tom Zehle,Moritz Schlager,Timo Heiß,Matthias Feurer*

Main category: cs.CL

TL;DR: CAPO是一种成本感知的提示优化算法，通过结合AutoML技术提高效率，减少LLM调用次数和输入令牌，同时平衡性能和提示长度。


<details>
  <summary>Details</summary>
Motivation: 当前提示优化方法需要大量LLM调用和输入令牌，成本高昂，CAPO旨在解决这一问题。

Method: CAPO采用进化算法，结合AutoML技术，通过racing减少评估次数，多目标优化平衡性能和提示长度。

Result: 在11/15的案例中，CAPO优于现有方法，性能提升高达21%，且在小预算下表现更好。

Conclusion: CAPO通过提高成本效率，使提示优化更强大和易用。

Abstract: Large language models (LLMs) have revolutionized natural language processing
by solving a wide range of tasks simply guided by a prompt. Yet their
performance is highly sensitive to prompt formulation. While automated prompt
optimization addresses this challenge by finding optimal prompts, current
methods require a substantial number of LLM calls and input tokens, making
prompt optimization expensive. We introduce CAPO (Cost-Aware Prompt
Optimization), an algorithm that enhances prompt optimization efficiency by
integrating AutoML techniques. CAPO is an evolutionary approach with LLMs as
operators, incorporating racing to save evaluations and multi-objective
optimization to balance performance with prompt length. It jointly optimizes
instructions and few-shot examples while leveraging task descriptions for
improved robustness. Our extensive experiments across diverse datasets and LLMs
demonstrate that CAPO outperforms state-of-the-art discrete prompt optimization
methods in 11/15 cases with improvements up to 21%p. Our algorithm achieves
better performances already with smaller budgets, saves evaluations through
racing, and decreases average prompt length via a length penalty, making it
both cost-efficient and cost-aware. Even without few-shot examples, CAPO
outperforms its competitors and generally remains robust to initial prompts.
CAPO represents an important step toward making prompt optimization more
powerful and accessible by improving cost-efficiency.

</details>


### [32] [Methods for Recognizing Nested Terms](https://arxiv.org/abs/2504.16007)
*Igor Rozhkov,Natalia Loukachevitch*

Main category: cs.CL

TL;DR: 本文介绍了在RuTermEval竞赛中应用Binder模型提取嵌套术语的研究，取得了最佳成绩，并探讨了从扁平标注数据中识别嵌套术语的新任务。


<details>
  <summary>Details</summary>
Motivation: 研究目的是验证Binder模型在提取嵌套术语中的有效性，并探索无需嵌套标注的术语识别方法。

Method: 采用Binder模型，该模型曾成功用于嵌套命名实体识别，并提出了几种无需嵌套标注的方法。

Result: 在RuTermEval竞赛的三个赛道中均取得了最佳成绩，证明了方法的有效性。

Conclusion: 提出的方法能够有效提取嵌套术语，即使在没有嵌套标注的情况下也表现良好。

Abstract: In this paper, we describe our participation in the RuTermEval competition
devoted to extracting nested terms. We apply the Binder model, which was
previously successfully applied to the recognition of nested named entities, to
extract nested terms. We obtained the best results of term recognition in all
three tracks of the RuTermEval competition. In addition, we study the new task
of recognition of nested terms from flat training data annotated with terms
without nestedness. We can conclude that several approaches we proposed in this
work are viable enough to retrieve nested terms effectively without nested
labeling of them.

</details>


### [33] [Certified Mitigation of Worst-Case LLM Copyright Infringement](https://arxiv.org/abs/2504.16046)
*Jingyu Zhang,Jiacan Yu,Marc Marone,Benjamin Van Durme,Daniel Khashabi*

Main category: cs.CL

TL;DR: 论文提出BloomScrub方法，通过推理时检测和重写技术减少LLMs生成侵权内容的风险，提供认证的版权保护。


<details>
  <summary>Details</summary>
Motivation: 预训练LLMs可能涉及版权材料，引发侵权风险，需开发有效方法来防止生成侵权内容。

Method: BloomScrub结合引用检测和重写技术，利用Bloom过滤器实现高效版权筛查，并在必要时通过弃权降低风险。

Result: 实验表明BloomScrub有效减少侵权风险，保持模型实用性，并适应不同严格程度的执行需求。

Conclusion: 轻量级推理时方法对版权保护具有显著效果。

Abstract: The exposure of large language models (LLMs) to copyrighted material during
pre-training raises concerns about unintentional copyright infringement post
deployment. This has driven the development of "copyright takedown" methods,
post-training approaches aimed at preventing models from generating content
substantially similar to copyrighted ones. While current mitigation approaches
are somewhat effective for average-case risks, we demonstrate that they
overlook worst-case copyright risks exhibits by the existence of long, verbatim
quotes from copyrighted sources. We propose BloomScrub, a remarkably simple yet
highly effective inference-time approach that provides certified copyright
takedown. Our method repeatedly interleaves quote detection with rewriting
techniques to transform potentially infringing segments. By leveraging
efficient data sketches (Bloom filters), our approach enables scalable
copyright screening even for large-scale real-world corpora. When quotes beyond
a length threshold cannot be removed, the system can abstain from responding,
offering certified risk reduction. Experimental results show that BloomScrub
reduces infringement risk, preserves utility, and accommodates different levels
of enforcement stringency with adaptive abstention. Our results suggest that
lightweight, inference-time methods can be surprisingly effective for copyright
prevention.

</details>


### [34] [LongMamba: Enhancing Mamba's Long Context Capabilities via Training-Free Receptive Field Enlargement](https://arxiv.org/abs/2504.16053)
*Zhifan Ye,Kejing Xia,Yonggan Fu,Xin Dong,Jihoon Hong,Xiangchi Yuan,Shizhe Diao,Jan Kautz,Pavlo Molchanov,Yingyan Celine Lin*

Main category: cs.CL

TL;DR: LongMamba是一种无需训练的技术，通过识别和过滤关键令牌，显著提升了Mamba模型的长上下文理解能力。


<details>
  <summary>Details</summary>
Motivation: 解决Mamba模型在长上下文任务中表现不佳的问题，同时保持其高效性。

Method: 将隐藏通道分为局部和全局通道，识别关键令牌并过滤不重要令牌，以减轻全局通道的状态衰减。

Result: LongMamba显著提升了Mamba模型的长上下文性能，无需额外训练。

Conclusion: LongMamba为Mamba模型的长上下文任务提供了高效且准确的解决方案。

Abstract: State space models (SSMs) have emerged as an efficient alternative to
Transformer models for language modeling, offering linear computational
complexity and constant memory usage as context length increases. However,
despite their efficiency in handling long contexts, recent studies have shown
that SSMs, such as Mamba models, generally underperform compared to
Transformers in long-context understanding tasks. To address this significant
shortfall and achieve both efficient and accurate long-context understanding,
we propose LongMamba, a training-free technique that significantly enhances the
long-context capabilities of Mamba models. LongMamba builds on our discovery
that the hidden channels in Mamba can be categorized into local and global
channels based on their receptive field lengths, with global channels primarily
responsible for long-context capability. These global channels can become the
key bottleneck as the input context lengthens. Specifically, when input lengths
largely exceed the training sequence length, global channels exhibit
limitations in adaptively extend their receptive fields, leading to Mamba's
poor long-context performance. The key idea of LongMamba is to mitigate the
hidden state memory decay in these global channels by preventing the
accumulation of unimportant tokens in their memory. This is achieved by first
identifying critical tokens in the global channels and then applying token
filtering to accumulate only those critical tokens. Through extensive
benchmarking across synthetic and real-world long-context scenarios, LongMamba
sets a new standard for Mamba's long-context performance, significantly
extending its operational range without requiring additional training. Our code
is available at https://github.com/GATECH-EIC/LongMamba.

</details>


### [35] [Honey, I Shrunk the Language Model: Impact of Knowledge Distillation Methods on Performance and Explainability](https://arxiv.org/abs/2504.16056)
*Daniel Hendriks,Philipp Spitzer,Niklas Kühl,Gerhard Satzger*

Main category: cs.CL

TL;DR: 该论文探讨了知识蒸馏在大型语言模型（LLMs）中的应用，提出了新的蒸馏方法，并系统比较了其性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs在资源受限环境中的部署问题，通过知识蒸馏训练小型学生模型。

Method: 应用critique-revision prompting生成训练数据，并综合现有方法训练学生模型。

Result: 在Commonsense Question-Answering数据集上比较性能和可解释性，贡献了新的蒸馏方法。

Conclusion: 新方法有望推动小型语言模型的蒸馏，促进LLM技术的广泛应用。

Abstract: Artificial Intelligence (AI) has increasingly influenced modern society,
recently in particular through significant advancements in Large Language
Models (LLMs). However, high computational and storage demands of LLMs still
limit their deployment in resource-constrained environments. Knowledge
distillation addresses this challenge by training a small student model from a
larger teacher model. Previous research has introduced several distillation
methods for both generating training data and for training the student model.
Despite their relevance, the effects of state-of-the-art distillation methods
on model performance and explainability have not been thoroughly investigated
and compared. In this work, we enlarge the set of available methods by applying
critique-revision prompting to distillation for data generation and by
synthesizing existing methods for training. For these methods, we provide a
systematic comparison based on the widely used Commonsense Question-Answering
(CQA) dataset. While we measure performance via student model accuracy, we
employ a human-grounded study to evaluate explainability. We contribute new
distillation methods and their comparison in terms of both performance and
explainability. This should further advance the distillation of small language
models and, thus, contribute to broader applicability and faster diffusion of
LLM technology.

</details>


### [36] [Vision-Language Models Are Not Pragmatically Competent in Referring Expression Generation](https://arxiv.org/abs/2504.16060)
*Ziqiao Ma,Jing Ding,Xuejun Zhang,Dezhi Luo,Jiahe Ding,Sihan Xu,Yuchen Huang,Run Peng,Joyce Chai*

Main category: cs.CL

TL;DR: 论文从语用学角度重新审视指代表达生成（REG），指出当前视觉语言模型（VLM）在语用能力上的不足，并提出了新数据集RefOI。研究发现VLM在唯一性识别、信息冗余和人类偏好对齐方面存在问题，呼吁开发更符合人类沟通的模型和评估框架。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型（VLM）在指代表达生成（REG）任务中忽视了语用学维度，仅将其视为区域标注任务，忽略了Gricean准则。

Method: 通过新数据集RefOI（1.5k图像，含书面和口语指代表达），系统评估了先进VLM的语用能力。

Result: 发现VLM存在三大语用能力缺陷：无法唯一识别指代对象、信息冗余或无关、与人类偏好（如空间线索）不一致。自动评估方法未能捕捉这些问题。

Conclusion: 呼吁开发更注重语用学的模型和评估框架，以更贴近真实人类沟通。

Abstract: Referring Expression Generation (REG) is a core task for evaluating the
pragmatic competence of vision-language systems, requiring not only accurate
semantic grounding but also adherence to principles of cooperative
communication (Grice, 1975). However, current evaluations of vision-language
models (VLMs) often overlook the pragmatic dimension, reducing REG to a
region-based captioning task and neglecting Gricean maxims. In this work, we
revisit REG from a pragmatic perspective, introducing a new dataset (RefOI) of
1.5k images annotated with both written and spoken referring expressions.
Through a systematic evaluation of state-of-the-art VLMs, we identify three key
failures of pragmatic competence: (1) failure to uniquely identify the
referent, (2) inclusion of excessive or irrelevant information, and (3)
misalignment with human pragmatic preference, such as the underuse of minimal
spatial cues. We also show that standard automatic evaluations fail to capture
these pragmatic violations, reinforcing superficial cues rather than genuine
referential success. Our findings call for a renewed focus on pragmatically
informed models and evaluation frameworks that align with real human
communication.

</details>


### [37] [A Python Tool for Reconstructing Full News Text from GDELT](https://arxiv.org/abs/2504.16063)
*A. Fronzetti Colladon,R. Vestrelli*

Main category: cs.CL

TL;DR: 提出了一种利用GDELT数据集低成本获取全文新闻的方法，解决了现有新闻数据集的访问限制问题。


<details>
  <summary>Details</summary>
Motivation: 新闻数据在多个领域至关重要，但现有数据集成本高或不完整，限制了研究。

Method: 利用GDELT Web News NGrams 3.0数据集，通过Python代码从n-grams重建全文新闻。

Result: 实现了低成本获取结构化、大规模的新闻数据，适用于文本分析。

Conclusion: 该方法提升了新闻数据的可访问性，支持经济预测、计算社会科学和自然语言处理等应用。

Abstract: News data have become an essential resource across various disciplines,
including economics, finance, management, social sciences, and computer
science. Researchers leverage newspaper articles to study economic trends,
market dynamics, corporate strategies, public perception, political discourse,
and the evolution of public opinion. Additionally, news datasets have been
instrumental in training large-scale language models, with applications in
sentiment analysis, fake news detection, and automated news summarization.
Despite their significance, access to comprehensive news corpora remains a key
challenge. Many full-text news providers, such as Factiva and LexisNexis,
require costly subscriptions, while free alternatives often suffer from
incomplete data and transparency issues. This paper presents a novel approach
to obtaining full-text newspaper articles at near-zero cost by leveraging data
from the Global Database of Events, Language, and Tone (GDELT). Specifically,
we focus on the GDELT Web News NGrams 3.0 dataset, which provides
high-frequency updates of n-grams extracted from global online news sources. We
provide Python code to reconstruct full-text articles from these n-grams by
identifying overlapping textual fragments and intelligently merging them. Our
method enables researchers to access structured, large-scale newspaper data for
text analysis while overcoming the limitations of existing proprietary
datasets. The proposed approach enhances the accessibility of news data for
empirical research, facilitating applications in economic forecasting,
computational social science, and natural language processing.

</details>


### [38] [Guiding VLM Agents with Process Rewards at Inference Time for GUI Navigation](https://arxiv.org/abs/2504.16073)
*Zhiyuan Hu,Shiyun Xiong,Yifan Zhang,See-Kiong Ng,Anh Tuan Luu,Bo An,Shuicheng Yan,Bryan Hooi*

Main category: cs.CL

TL;DR: 提出了一种通过奖励模型引导视觉语言模型（VLM）代理的方法，以优化GUI导航任务中的动作生成，显著提升了静态和动态环境中的性能。


<details>
  <summary>Details</summary>
Motivation: 当前VLM框架在复杂GUI任务中生成正确动作的能力有限，且商业VLM为黑盒，开源VLM微调资源消耗大，现有评估和优化技术存在延迟反馈和局部优化问题。

Method: 在推理时通过奖励模型对VLM代理进行过程监督，优化每一步动作生成，并结合轨迹反思和重试机制。

Result: 在静态环境中单步动作准确率提升3.4%，动态环境中任务成功率提高约33%，进一步整合轨迹反思和重试机制后效果更佳。

Conclusion: 该方法有效解决了VLM在GUI任务中的性能瓶颈，为复杂环境下的动作优化提供了新思路。

Abstract: Recent advancements in visual language models (VLMs) have notably enhanced
their capabilities in handling complex Graphical User Interface (GUI)
interaction tasks. Despite these improvements, current frameworks often
struggle to generate correct actions in challenging GUI environments.
State-of-the-art commercial VLMs are black-boxes, and fine-tuning open-source
VLMs for GUI tasks requires significant resources. Additionally, existing
trajectory-level evaluation and refinement techniques frequently fall short due
to delayed feedback and local optimization issues. To address these challenges,
we propose an approach that guides VLM agents with process supervision by a
reward model during GUI navigation and control at inference time. This guidance
allows the VLM agent to optimize actions at each inference step, thereby
improving performance in both static and dynamic environments. In particular,
our method demonstrates significant performance gains in three GUI navigation
tasks, achieving a 3.4% improvement in single step action accuracy for static
environments, along with a around 33% increase in task success rate in one
dynamic environment. With further integration of trajectory reflection and
retry mechanisms, we also demonstrate even greater enhancement in task success.

</details>


### [39] [PHYBench: Holistic Evaluation of Physical Perception and Reasoning in Large Language Models](https://arxiv.org/abs/2504.16074)
*Shi Qiu,Shaoyang Guo,Zhuo-Yang Song,Yunbo Sun,Zeyu Cai,Jiashen Wei,Tianyu Luo,Yixuan Yin,Haoxu Zhang,Yi Hu,Chenyang Wang,Chencheng Tang,Haoling Chang,Qi Liu,Ziheng Zhou,Tianyu Zhang,Jingtian Zhang,Zhangyi Liu,Minghao Li,Yuku Zhang,Boxuan Jing,Xianqi Yin,Yutong Ren,Zizhuo Fu,Weike Wang,Xudong Tian,Anqi Lv,Laifu Man,Jianxiang Li,Feiyu Tao,Qihua Sun,Zhou Liang,Yushu Mu,Zhongxuan Li,Jing-Jun Zhang,Shutao Zhang,Xiaotian Li,Xingqi Xia,Jiawei Lin,Zheyu Shen,Jiahang Chen,Qiuhao Xiong,Binran Wang,Fengyuan Wang,Ziyang Ni,Bohan Zhang,Fan Cui,Changkun Shao,Qing-Hong Cao,Ming-xing Luo,Muhan Zhang,Hua Xing Zhu*

Main category: cs.CL

TL;DR: PHYBench是一个用于评估大语言模型（LLMs）在物理场景中推理能力的高质量基准，包含500个精心设计的物理问题，并提出了新的评估指标EED Score。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法难以全面衡量LLMs在复杂物理推理中的表现，需要更精细的基准和指标。

Method: 构建PHYBench基准，包含多领域物理问题，并提出EED Score作为评估指标。

Result: 当前最先进的LLMs在物理推理上显著落后于人类专家。

Conclusion: PHYBench揭示了LLMs在物理推理上的局限性，为未来改进提供了方向。

Abstract: We introduce PHYBench, a novel, high-quality benchmark designed for
evaluating reasoning capabilities of large language models (LLMs) in physical
contexts. PHYBench consists of 500 meticulously curated physics problems based
on real-world physical scenarios, designed to assess the ability of models to
understand and reason about realistic physical processes. Covering mechanics,
electromagnetism, thermodynamics, optics, modern physics, and advanced physics,
the benchmark spans difficulty levels from high school exercises to
undergraduate problems and Physics Olympiad challenges. Additionally, we
propose the Expression Edit Distance (EED) Score, a novel evaluation metric
based on the edit distance between mathematical expressions, which effectively
captures differences in model reasoning processes and results beyond
traditional binary scoring methods. We evaluate various LLMs on PHYBench and
compare their performance with human experts. Our results reveal that even
state-of-the-art reasoning models significantly lag behind human experts,
highlighting their limitations and the need for improvement in complex physical
reasoning scenarios. Our benchmark results and dataset are publicly available
at https://phybench-official.github.io/phybench-demo/.

</details>


### [40] [TTRL: Test-Time Reinforcement Learning](https://arxiv.org/abs/2504.16084)
*Yuxin Zuo,Kaiyan Zhang,Shang Qu,Li Sheng,Xuekai Zhu,Biqing Qi,Youbang Sun,Ganqu Cui,Ning Ding,Bowen Zhou*

Main category: cs.CL

TL;DR: 论文提出了一种名为TTRL的新方法，利用无标签数据通过强化学习训练大语言模型，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 研究在无显式标签数据上使用强化学习训练大语言模型的挑战，特别是在推理时缺乏真实奖励信息的情况下。

Method: 引入Test-Time Reinforcement Learning (TTRL)，利用预训练模型的先验知识，通过多数投票等测试时缩放技术生成有效奖励信号。

Result: TTRL显著提升了模型性能，例如Qwen-2.5-Math-7B在AIME 2024上的pass@1性能提高了约159%。

Conclusion: TTRL在多种任务中表现出色，展示了其在更广泛任务和领域中的潜力。

Abstract: This paper investigates Reinforcement Learning (RL) on data without explicit
labels for reasoning tasks in Large Language Models (LLMs). The core challenge
of the problem is reward estimation during inference while not having access to
ground-truth information. While this setting appears elusive, we find that
common practices in Test-Time Scaling (TTS), such as majority voting, yield
surprisingly effective rewards suitable for driving RL training. In this work,
we introduce Test-Time Reinforcement Learning (TTRL), a novel method for
training LLMs using RL on unlabeled data. TTRL enables self-evolution of LLMs
by utilizing the priors in the pre-trained models. Our experiments demonstrate
that TTRL consistently improves performance across a variety of tasks and
models. Notably, TTRL boosts the pass@1 performance of Qwen-2.5-Math-7B by
approximately 159% on the AIME 2024 with only unlabeled test data. Furthermore,
although TTRL is only supervised by the Maj@N metric, TTRL has demonstrated
performance to consistently surpass the upper limit of the initial model, and
approach the performance of models trained directly on test data with
ground-truth labels. Our experimental findings validate the general
effectiveness of TTRL across various tasks, and highlight TTRL's potential for
broader tasks and domains. GitHub: https://github.com/PRIME-RL/TTRL

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [41] [Can Machine Learning Agents Deal with Hard Choices?](https://arxiv.org/abs/2504.15304)
*Kangyu Wang*

Main category: cs.AI

TL;DR: 论文探讨了机器学习代理在决策中与人类推理的差异，特别是无法处理“艰难选择”的问题，并提出了一种可能的解决方案。


<details>
  <summary>Details</summary>
Motivation: 研究动机是理解机器学习代理在决策过程中与人类推理的差异，尤其是在面对无法比较的选项（艰难选择）时的局限性。

Method: 论文分析了多目标优化（MOO）方法的局限性，并提出了一种集成解决方案来帮助机器学习代理识别艰难选择。

Result: 结果表明，当前MOO方法无法处理艰难选择，但集成解决方案有望缓解对齐问题。

Conclusion: 结论强调了人类代理的独特性，并呼吁重新定义机器自主性以填补这一根本差距。

Abstract: Machine Learning ML agents have been increasingly used in decision-making
across a wide range of tasks and environments. These ML agents are typically
designed to balance multiple objectives when making choices. Understanding how
their decision-making processes align with or diverge from human reasoning is
essential. Human agents often encounter hard choices, that is, situations where
options are incommensurable; neither option is preferred, yet the agent is not
indifferent between them. In such cases, human agents can identify hard choices
and resolve them through deliberation. In contrast, current ML agents, due to
fundamental limitations in Multi-Objective Optimisation or MOO methods, cannot
identify hard choices, let alone resolve them. Neither Scalarised Optimisation
nor Pareto Optimisation, the two principal MOO approaches, can capture
incommensurability. This limitation generates three distinct alignment
problems: the alienness of ML decision-making behaviour from a human
perspective; the unreliability of preference-based alignment strategies for
hard choices; and the blockage of alignment strategies pursuing multiple
objectives. Evaluating two potential technical solutions, I recommend an
ensemble solution that appears most promising for enabling ML agents to
identify hard choices and mitigate alignment problems. However, no known
technique allows ML agents to resolve hard choices through deliberation, as
they cannot autonomously change their goals. This underscores the
distinctiveness of human agency and urges ML researchers to reconceptualise
machine autonomy and develop frameworks and methods that can better address
this fundamental gap.

</details>


### [42] [PolicyEvol-Agent: Evolving Policy via Environment Perception and Self-Awareness with Theory of Mind](https://arxiv.org/abs/2504.15313)
*Yajie Yu,Yue Feng*

Main category: cs.AI

TL;DR: PolicyEvol-Agent是一个基于LLM的多智能体框架，通过系统获取他人意图和自适应优化策略，在动态交互场景中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有研究在动态交互场景中缺乏有效的认知链（如推理、规划、决策和反思），且提示式响应在心理状态感知和经验校准方面存在不足。

Method: PolicyEvol-Agent通过获取反思性专业知识模式，结合心智理论和内外视角的认知操作，优化策略。

Result: 仿真结果表明，PolicyEvol-Agent优于基于RL和智能体的方法，并在游戏胜利中表现卓越。

Conclusion: PolicyEvol-Agent的动态策略调整机制在自动和人工评估中均有效，展示了其优越性。

Abstract: Multi-agents has exhibited significant intelligence in real-word simulations
with Large language models (LLMs) due to the capabilities of social cognition
and knowledge retrieval. However, existing research on agents equipped with
effective cognition chains including reasoning, planning, decision-making and
reflecting remains limited, especially in the dynamically interactive
scenarios. In addition, unlike human, prompt-based responses face challenges in
psychological state perception and empirical calibration during uncertain
gaming process, which can inevitably lead to cognition bias. In light of above,
we introduce PolicyEvol-Agent, a comprehensive LLM-empowered framework
characterized by systematically acquiring intentions of others and adaptively
optimizing irrational strategies for continual enhancement. Specifically,
PolicyEvol-Agent first obtains reflective expertise patterns and then
integrates a range of cognitive operations with Theory of Mind alongside
internal and external perspectives. Simulation results, outperforming RL-based
models and agent-based methods, demonstrate the superiority of PolicyEvol-Agent
for final gaming victory. Moreover, the policy evolution mechanism reveals the
effectiveness of dynamic guideline adjustments in both automatic and human
evaluation.

</details>


### [43] [Reliable Classification with Conformal Learning and Interval-Type 2 Fuzzy Sets](https://arxiv.org/abs/2504.15360)
*Javier Fumanal-Idocin,Javier Andreu-Perez*

Main category: cs.AI

TL;DR: 论文提出使用保形学习与模糊规则系统结合的分类方法，并探讨了类型2模糊集对系统输出质量的提升。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习分类器在实验室基准外可能不可靠，需要可靠的方法评估模型输出的质量。

Method: 结合保形学习和模糊规则系统，并引入类型2模糊集优化输出质量。

Result: 保形学习比贝叶斯方法更可靠，类型2模糊集进一步提升了系统性能。

Conclusion: 保形学习与模糊规则系统结合可提高分类可靠性，类型2模糊集优化了输出质量。

Abstract: Classical machine learning classifiers tend to be overconfident can be
unreliable outside of the laboratory benchmarks. Properly assessing the
reliability of the output of the model per sample is instrumental for real-life
scenarios where these systems are deployed. Because of this, different
techniques have been employed to properly quantify the quality of prediction
for a given model. These are most commonly Bayesian statistics and, more
recently, conformal learning. Given a calibration set, conformal learning can
produce outputs that are guaranteed to cover the target class with a desired
significance level, and are more reliable than the standard confidence
intervals used by Bayesian methods. In this work, we propose to use conformal
learning with fuzzy rule-based systems in classification and show some metrics
of their performance. Then, we discuss how the use of type 2 fuzzy sets can
improve the quality of the output of the system compared to both fuzzy and
crisp rules. Finally, we also discuss how the fine-tuning of the system can be
adapted to improve the quality of the conformal prediction.

</details>


### [44] [KeDiff: Key Similarity-Based KV Cache Eviction for Long-Context LLM Inference in Resource-Constrained Environments](https://arxiv.org/abs/2504.15364)
*Junyoung Park,Dalton Jones,Matt Morse,Raghavv Goel,Mingu Lee,Chris Lott*

Main category: cs.AI

TL;DR: 提出了一种基于键相似性的训练无关KV缓存淘汰方法KeyDiff，适用于资源受限环境中的长输入提示LLM应用。


<details>
  <summary>Details</summary>
Motivation: 解决资源受限环境下部署长输入提示LLM应用时KV缓存占用过高的问题。

Method: 通过键相似性选择性地淘汰KV缓存，最大化键多样性，不依赖注意力分数。

Result: 在8K缓存预算下，性能差距小于0.04%，KV缓存减少约23%。

Conclusion: KeyDiff是一种高效且理论支持的KV缓存优化方法，适用于多种任务和模型。

Abstract: In this work, we demonstrate that distinctive keys during LLM inference tend
to have high attention scores. We explore this phenomenon and propose KeyDiff,
a training-free KV cache eviction method based on key similarity. This method
facilitates the deployment of LLM-based application requiring long input
prompts in resource-constrained environments with limited memory and compute
budgets. Unlike other KV cache eviction methods, KeyDiff can process
arbitrarily long prompts within strict resource constraints and efficiently
generate responses. We demonstrate that KeyDiff computes the optimal solution
to a KV cache selection problem that maximizes key diversity, providing a
theoretical understanding of KeyDiff. Notably,KeyDiff does not rely on
attention scores, allowing the use of optimized attention mechanisms like
FlashAttention. We demonstrate the effectiveness of KeyDiff across diverse
tasks and models, illustrating a performance gap of less than 0.04\% with 8K
cache budget ($\sim$ 23\% KV cache reduction) from the non-evicting baseline on
the LongBench benchmark for Llama 3.1-8B and Llama 3.2-3B.

</details>


### [45] [AGI Is Coming... Right After AI Learns to Play Wordle](https://arxiv.org/abs/2504.15434)
*Sarath Shekkizhar,Romain Cosentino*

Main category: cs.AI

TL;DR: 研究评估了OpenAI的计算机用户代理（CUA）在Wordle游戏中的表现，发现其在颜色识别上存在显著差异，成功率为5.36%，表明当前AI模型在简单任务中仍面临挑战。


<details>
  <summary>Details</summary>
Motivation: 探讨多模态代理（如CUA）在完成类似人类任务时的能力，尤其是其在Wordle游戏中的表现，以揭示模型行为的不足。

Method: 通过让CUA在数百次Wordle游戏中运行，评估其任务完成能力和颜色识别准确性。

Result: 模型在颜色识别上表现不一致，整体成功率仅为5.36%，显示其在简单任务中的局限性。

Conclusion: 研究强调了当前AI模型在简单任务中的挑战，并讨论了潜在原因及未来改进方向。

Abstract: This paper investigates multimodal agents, in particular, OpenAI's
Computer-User Agent (CUA), trained to control and complete tasks through a
standard computer interface, similar to humans. We evaluated the agent's
performance on the New York Times Wordle game to elicit model behaviors and
identify shortcomings. Our findings revealed a significant discrepancy in the
model's ability to recognize colors correctly depending on the context. The
model had a $5.36\%$ success rate over several hundred runs across a week of
Wordle. Despite the immense enthusiasm surrounding AI agents and their
potential to usher in Artificial General Intelligence (AGI), our findings
reinforce the fact that even simple tasks present substantial challenges for
today's frontier AI models. We conclude with a discussion of the potential
underlying causes, implications for future development, and research directions
to improve these AI systems.

</details>


### [46] [Improving Human-AI Coordination through Adversarial Training and Generative Models](https://arxiv.org/abs/2504.15457)
*Paresh Chaudhary,Yancheng Liang,Daphne Chen,Simon S. Du,Natasha Jaques*

Main category: cs.AI

TL;DR: 论文提出了一种名为GOAT的新方法，结合生成模型和对抗训练，以解决合作任务中对抗策略自毁的问题，并在Overcooked基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 在合作任务中，对抗训练难以模拟有效的合作行为，导致自毁问题。需要一种方法既能生成多样化的合作策略，又能确保对抗训练的有效性。

Method: 提出GOAT方法，结合预训练生成模型和对抗训练，动态搜索并生成使合作者表现不佳的协调策略，同时保持生成模型参数固定以避免对抗性利用。

Result: 在Overcooked基准测试中，GOAT表现出色，能够泛化到多样化的人类行为。

Conclusion: GOAT通过结合生成模型和对抗训练，有效解决了合作任务中的自毁问题，提升了泛化能力。

Abstract: Being able to cooperate with new people is an important component of many
economically valuable AI tasks, from household robotics to autonomous driving.
However, generalizing to novel humans requires training on data that captures
the diversity of human behaviors. Adversarial training is one avenue for
searching for such data and ensuring that agents are robust. However, it is
difficult to apply in the cooperative setting because adversarial policies
intentionally learn to sabotage the task instead of simulating valid
cooperation partners. To address this challenge, we propose a novel strategy
for overcoming self-sabotage that combines a pre-trained generative model to
simulate valid cooperative agent policies with adversarial training to maximize
regret. We call our method GOAT: Generative Online Adversarial Training. In
this framework, the GOAT dynamically searches for and generates coordination
strategies where the learning policy -- the Cooperator agent -- underperforms.
GOAT enables better generalization by exposing the Cooperator to various
challenging interaction scenarios. We maintain realistic coordination
strategies by updating only the generative model's embedding while keeping its
parameters frozen, thus avoiding adversarial exploitation. We evaluate GOAT
with real human partners, and the results demonstrate state-of-the-art
performance on the Overcooked benchmark, highlighting its effectiveness in
generalizing to diverse human behaviors.

</details>


### [47] [Learning Adaptive Parallel Reasoning with Language Models](https://arxiv.org/abs/2504.15466)
*Jiayi Pan,Xiuyu Li,Long Lian,Charlie Snell,Yifei Zhou,Adam Yala,Trevor Darrell,Kurt Keutzer,Alane Suhr*

Main category: cs.AI

TL;DR: 论文提出了一种自适应并行推理（APR）框架，通过结合串行和并行计算优化语言模型的推理能力，显著提升了性能和效率。


<details>
  <summary>Details</summary>
Motivation: 现有推理方法存在局限性：串行链式思维输出过长导致延迟增加，并行方法如自一致性缺乏协调导致冗余计算。APR旨在解决这些问题。

Method: APR采用自适应多线程推理，结合spawn()和join()操作，并通过端到端强化学习优化推理线程。

Result: 实验显示APR在相同上下文窗口下性能更高（83.4% vs. 60.0%），计算扩展性更强（80.1% vs. 66.6%），延迟相同时准确率更高（75.2% vs. 57.3%）。

Conclusion: APR通过自适应计算分配，使语言模型能够自主优化推理过程，代表了推理能力的重要进展。

Abstract: Scaling inference-time computation has substantially improved the reasoning
capabilities of language models. However, existing methods have significant
limitations: serialized chain-of-thought approaches generate overly long
outputs, leading to increased latency and exhausted context windows, while
parallel methods such as self-consistency suffer from insufficient
coordination, resulting in redundant computations and limited performance
gains. To address these shortcomings, we propose Adaptive Parallel Reasoning
(APR), a novel reasoning framework that enables language models to orchestrate
both serialized and parallel computations end-to-end. APR generalizes existing
reasoning methods by enabling adaptive multi-threaded inference using spawn()
and join() operations. A key innovation is our end-to-end reinforcement
learning strategy, optimizing both parent and child inference threads to
enhance task success rate without requiring predefined reasoning structures.
Experiments on the Countdown reasoning task demonstrate significant benefits of
APR: (1) higher performance within the same context window (83.4% vs. 60.0% at
4k context); (2) superior scalability with increased computation (80.1% vs.
66.6% at 20k total tokens); (3) improved accuracy at equivalent latency (75.2%
vs. 57.3% at approximately 5,000ms). APR represents a step towards enabling
language models to autonomously optimize their reasoning processes through
adaptive allocation of computation.

</details>


### [48] [A Multi-Agent Framework for Automated Qinqiang Opera Script Generation Using Large Language Models](https://arxiv.org/abs/2504.15552)
*Gengxian Cao,Fengyuan Li,Hong Duan,Ye Yang,Bofeng Wang,Donghe Li*

Main category: cs.AI

TL;DR: 本文提出了一种多智能体框架，通过整合大语言模型、视觉生成和文本到语音合成，自动化完成秦腔戏曲的端到端制作。三个智能体协作：Agent1生成剧本，Agent2生成舞台场景，Agent3合成语音。实验显示系统表现优于单智能体基线，模块化协作效果显著。


<details>
  <summary>Details</summary>
Motivation: 旨在利用AI技术保护和规模化传统表演艺术，如秦腔戏曲。

Method: 采用多智能体框架，分别负责剧本生成（LLM）、场景渲染（视觉生成模型）和语音合成（TTS）。

Result: 在《窦娥冤》案例中，系统在剧本忠实度、视觉连贯性和语音准确性上分别获得3.8、3.5和3.8分，总分3.6，优于单智能体基线0.3分。消融实验表明模块协作的重要性。

Conclusion: AI驱动的流水线可有效保护和推广传统艺术，未来可优化跨模态对齐、情感表达和支持更多戏曲类型。

Abstract: This paper introduces a novel multi-Agent framework that automates the end to
end production of Qinqiang opera by integrating Large Language Models , visual
generation, and Text to Speech synthesis. Three specialized agents collaborate
in sequence: Agent1 uses an LLM to craft coherent, culturally grounded
scripts;Agent2 employs visual generation models to render contextually accurate
stage scenes; and Agent3 leverages TTS to produce synchronized, emotionally
expressive vocal performances. In a case study on Dou E Yuan, the system
achieved expert ratings of 3.8 for script fidelity, 3.5 for visual coherence,
and 3.8 for speech accuracy-culminating in an overall score of 3.6, a 0.3 point
improvement over a Single Agent baseline. Ablation experiments demonstrate that
removing Agent2 or Agent3 leads to drops of 0.4 and 0.5 points, respectively,
underscoring the value of modular collaboration. This work showcases how AI
driven pipelines can streamline and scale the preservation of traditional
performing arts, and points toward future enhancements in cross modal
alignment, richer emotional nuance, and support for additional opera genres.

</details>


### [49] [A LoRA-Based Approach to Fine-Tuning LLMs for Educational Guidance in Resource-Constrained Settings](https://arxiv.org/abs/2504.15610)
*Md Millat,Md Motiur*

Main category: cs.AI

TL;DR: 该研究提出了一种成本效益高的方法，通过LoRA和4位量化技术调整Mistral-7B-Instruct模型，用于学术咨询和留学场景，并在低资源环境中实现高效训练。


<details>
  <summary>Details</summary>
Motivation: 解决在低资源环境下高效调整大型语言模型（LLMs）以适应学术咨询和留学场景的需求。

Method: 采用LoRA和4位量化技术，分两阶段训练模型：第一阶段使用Gemini Pro API生成的合成数据集，第二阶段使用StudyAbroadGPT项目的手动标注数据集。

Result: 训练损失减少52.7%，领域特定推荐准确率达92%，支持95%的Markdown格式，并在普通GPU上实现每秒100样本的处理速度。

Conclusion: 该方法在低资源教育咨询场景中有效，但通用性有限；未来可扩展为多语言和实时数据库集成。

Abstract: The current study describes a cost-effective method for adapting large
language models (LLMs) for academic advising with study-abroad contexts in mind
and for application in low-resource methods for acculturation. With the
Mistral-7B-Instruct model applied with a Low-Rank Adaptation (LoRA) method and
a 4-bit quantization method, the model underwent training in two distinct
stages related to this study's purpose to enhance domain specificity while
maintaining computational efficiency. In Phase 1, the model was conditioned
with a synthetic dataset via the Gemini Pro API, and in Phase 2, it was trained
with manually curated datasets from the StudyAbroadGPT project to achieve
enhanced, contextualized responses. Technical innovations entailed
memory-efficient quantization, parameter-efficient adaptation, and continuous
training analytics via Weights & Biases. After training, this study
demonstrated a reduction in training loss by 52.7%, 92% accuracy in
domain-specific recommendations, achieved 95% markdown-based formatting
support, and a median run-rate of 100 samples per second on off-the-shelf GPU
equipment. These findings support the effective application of
instruction-tuned LLMs within educational advisers, especially in low-resource
institutional scenarios. Limitations included decreased generalizability and
the application of a synthetically generated dataset, but this framework is
scalable for adding new multilingual-augmented and real-time academic advising
processes. Future directions may include plans for the integration of
retrieval-augmented generation, applying dynamic quantization routines, and
connecting to real-time academic databases to increase adaptability and
accuracy.

</details>


### [50] [Exploring Inevitable Waypoints for Unsolvability Explanation in Hybrid Planning Problems](https://arxiv.org/abs/2504.15668)
*Mir Md Sajid Sarwar,Rajarshi Ray*

Main category: cs.AI

TL;DR: 该论文提出了一种通过识别通用障碍点（waypoints）来解释混合系统中规划问题不可解性的方法。


<details>
  <summary>Details</summary>
Motivation: 当前AI规划领域对解释规划问题不可解性的研究较少，而分解任务为子问题的方法在规划生成中广泛应用。论文旨在填补这一研究空白。

Method: 将规划问题分解为子问题，识别通用障碍点（waypoints），并将其建模为最长公共子序列问题，通过符号可达性分析确定最早不可达点作为解释。

Result: 实验验证了该方法在混合领域不可解规划问题中的有效性。

Conclusion: 通过识别通用障碍点，该方法为规划问题的不可解性提供了新的解释机制。

Abstract: Explaining unsolvability of planning problems is of significant research
interest in Explainable AI Planning. AI planning literature has reported
several research efforts on generating explanations of solutions to planning
problems. However, explaining the unsolvability of planning problems remains a
largely open and understudied problem. A widely practiced approach to plan
generation and automated problem solving, in general, is to decompose tasks
into sub-problems that help progressively converge towards the goal. In this
paper, we propose to adopt the same philosophy of sub-problem identification as
a mechanism for analyzing and explaining unsolvability of planning problems in
hybrid systems. In particular, for a given unsolvable planning problem, we
propose to identify common waypoints, which are universal obstacles to plan
existence; in other words, they appear on every plan from the source to the
planning goal. This work envisions such waypoints as sub-problems of the
planning problem and the unreachability of any of these waypoints as an
explanation for the unsolvability of the original planning problem. We propose
a novel method of waypoint identification by casting the problem as an instance
of the longest common subsequence problem, a widely popular problem in computer
science, typically considered as an illustrative example for the dynamic
programming paradigm. Once the waypoints are identified, we perform symbolic
reachability analysis on them to identify the earliest unreachable waypoint and
report it as the explanation of unsolvability. We present experimental results
on unsolvable planning problems in hybrid domains.

</details>


### [51] [Advancing Embodied Agent Security: From Safety Benchmarks to Input Moderation](https://arxiv.org/abs/2504.15699)
*Ning Wang,Zihan Yan,Weiyang Li,Chuan Ma,He Chen,Tao Xiang*

Main category: cs.AI

TL;DR: 本文提出了一种针对具身代理的输入审核框架，包括安全基准EAsafetyBench和创新的Pinpoint方案，实验显示其高效且优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注通用大语言模型安全，缺乏针对具身代理的专门方法，需填补这一空白以确保其行为安全。

Method: 提出输入审核框架，涵盖分类定义、数据集构建、审核架构、模型训练及评估，并引入EAsafetyBench和Pinpoint方案。

Result: 实验显示方法平均检测准确率达94.58%，处理时间仅0.002秒/实例，优于现有技术。

Conclusion: 提出的框架和方案高效且实用，为具身代理的安全提供了可靠解决方案。

Abstract: Embodied agents exhibit immense potential across a multitude of domains,
making the assurance of their behavioral safety a fundamental prerequisite for
their widespread deployment. However, existing research predominantly
concentrates on the security of general large language models, lacking
specialized methodologies for establishing safety benchmarks and input
moderation tailored to embodied agents. To bridge this gap, this paper
introduces a novel input moderation framework, meticulously designed to
safeguard embodied agents. This framework encompasses the entire pipeline,
including taxonomy definition, dataset curation, moderator architecture, model
training, and rigorous evaluation. Notably, we introduce EAsafetyBench, a
meticulously crafted safety benchmark engineered to facilitate both the
training and stringent assessment of moderators specifically designed for
embodied agents. Furthermore, we propose Pinpoint, an innovative
prompt-decoupled input moderation scheme that harnesses a masked attention
mechanism to effectively isolate and mitigate the influence of functional
prompts on moderation tasks. Extensive experiments conducted on diverse
benchmark datasets and models validate the feasibility and efficacy of the
proposed approach. The results demonstrate that our methodologies achieve an
impressive average detection accuracy of 94.58%, surpassing the performance of
existing state-of-the-art techniques, alongside an exceptional moderation
processing time of merely 0.002 seconds per instance.

</details>


### [52] [DianJin-R1: Evaluating and Enhancing Financial Reasoning in Large Language Models](https://arxiv.org/abs/2504.15716)
*Jie Zhu,Qian Chen,Huaixia Dou,Junhui Li,Lifan Guo,Feng Chen,Chi Zhang*

Main category: cs.AI

TL;DR: DianJin-R1是一个增强推理的框架，通过监督学习和强化学习提升LLMs在金融领域的推理能力，表现优于非推理模型。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs在金融领域推理任务中的挑战，如领域知识、数值计算和合规性要求。

Method: 构建高质量数据集DianJin-R1-Data，结合监督和强化学习（GRPO）优化模型推理能力。

Result: DianJin-R1模型在金融和通用推理基准上表现优异，尤其在复杂金融任务中。

Conclusion: DianJin-R1通过结构化监督和奖励对齐学习，为金融推理提供了可扩展的解决方案。

Abstract: Effective reasoning remains a core challenge for large language models (LLMs)
in the financial domain, where tasks often require domain-specific knowledge,
precise numerical calculations, and strict adherence to compliance rules. We
propose DianJin-R1, a reasoning-enhanced framework designed to address these
challenges through reasoning-augmented supervision and reinforcement learning.
Central to our approach is DianJin-R1-Data, a high-quality dataset constructed
from CFLUE, FinQA, and a proprietary compliance corpus (Chinese Compliance
Check, CCC), combining diverse financial reasoning scenarios with verified
annotations. Our models, DianJin-R1-7B and DianJin-R1-32B, are fine-tuned from
Qwen2.5-7B-Instruct and Qwen2.5-32B-Instruct using a structured format that
generates both reasoning steps and final answers. To further refine reasoning
quality, we apply Group Relative Policy Optimization (GRPO), a reinforcement
learning method that incorporates dual reward signals: one encouraging
structured outputs and another rewarding answer correctness. We evaluate our
models on five benchmarks: three financial datasets (CFLUE, FinQA, and CCC) and
two general reasoning benchmarks (MATH-500 and GPQA-Diamond). Experimental
results show that DianJin-R1 models consistently outperform their non-reasoning
counterparts, especially on complex financial tasks. Moreover, on the
real-world CCC dataset, our single-call reasoning models match or even surpass
the performance of multi-agent systems that require significantly more
computational cost. These findings demonstrate the effectiveness of DianJin-R1
in enhancing financial reasoning through structured supervision and
reward-aligned learning, offering a scalable and practical solution for
real-world applications.

</details>


### [53] [Implementing Rational Choice Functions with LLMs and Measuring their Alignment with User Preferences](https://arxiv.org/abs/2504.15719)
*Anna Karnysheva,Christian Drescher,Dietrich Klakow*

Main category: cs.AI

TL;DR: 本文探讨了大型语言模型（LLMs）在智能用户界面（IUIs）中作为决策代理时与用户偏好的对齐问题，提出了一种更灵活的偏好对齐方法，并通过实证研究验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在IUIs中的广泛应用，其作为决策代理的对齐问题（尤其是与用户偏好的对齐）缺乏研究，而可靠的决策代理需要与用户偏好高度一致。

Method: 作者扩展了现有利用LLMs对结果排序的方法，引入了更广泛的用户偏好概念（包括严格偏好和无关选择），并提出了设计原则和测量工具。

Result: 通过汽车领域的实证研究，验证了所提方法的适用性和有效性。

Conclusion: 本文为LLMs在决策代理中的偏好对齐提供了理论和实践工具，填补了现有研究的空白。

Abstract: As large language models (LLMs) become integral to intelligent user
interfaces (IUIs), their role as decision-making agents raises critical
concerns about alignment. Although extensive research has addressed issues such
as factuality, bias, and toxicity, comparatively little attention has been paid
to measuring alignment to preferences, i.e., the relative desirability of
different alternatives, a concept used in decision making, economics, and
social choice theory. However, a reliable decision-making agent makes choices
that align well with user preferences.
  In this paper, we generalize existing methods that exploit LLMs for ranking
alternative outcomes by addressing alignment with the broader and more flexible
concept of user preferences, which includes both strict preferences and
indifference among alternatives. To this end, we put forward design principles
for using LLMs to implement rational choice functions, and provide the
necessary tools to measure preference satisfaction. We demonstrate the
applicability of our approach through an empirical study in a practical
application of an IUI in the automotive domain.

</details>


### [54] [TrustGeoGen: Scalable and Formal-Verified Data Engine for Trustworthy Multi-modal Geometric Problem Solving](https://arxiv.org/abs/2504.15780)
*Daocheng Fu,Zijun Chen,Renqiu Xia,Qi Liu,Yuan Feng,Hongbin Zhou,Renrui Zhang,Shiyang Feng,Peng Gao,Junchi Yan,Botian Shi,Bo Zhang,Yu Qiao*

Main category: cs.AI

TL;DR: 论文提出了一种名为TrustGeoGen的可扩展数据引擎，用于生成几何问题，并通过形式验证提供基准，支持几何问题解决的进一步发展。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型在几何问题解决中缺乏方法论和基准，且现有合成基准存在噪声和自相矛盾信息。

Method: TrustGeoGen通过多模态对齐生成、形式验证、自举机制和GeoExplore算法，生成具有模态完整性的数据集。

Result: 实验显示，现有模型在GeoTrust-test上的准确率仅为49.17%，而使用GeoTrust训练的模型在GeoQA上表现出更好的泛化能力。

Conclusion: TrustGeoGen为几何问题解决提供了可靠的数据集和基准，显著减少了逻辑不一致性。

Abstract: Mathematical geometric problem solving (GPS) often requires effective
integration of multimodal information and verifiable logical coherence. Despite
the fast development of large language models in general problem solving, it
remains unresolved regarding with both methodology and benchmarks, especially
given the fact that exiting synthetic GPS benchmarks are often not
self-verified and contain noise and self-contradicted information due to the
illusion of LLMs. In this paper, we propose a scalable data engine called
TrustGeoGen for problem generation, with formal verification to provide a
principled benchmark, which we believe lays the foundation for the further
development of methods for GPS. The engine synthesizes geometric data through
four key innovations: 1) multimodal-aligned generation of diagrams, textual
descriptions, and stepwise solutions; 2) formal verification ensuring
rule-compliant reasoning paths; 3) a bootstrapping mechanism enabling
complexity escalation via recursive state generation and 4) our devised
GeoExplore series algorithms simultaneously produce multi-solution variants and
self-reflective backtracking traces. By formal logical verification,
TrustGeoGen produces GeoTrust-200K dataset with guaranteed modality integrity,
along with GeoTrust-test testset. Experiments reveal the state-of-the-art
models achieve only 49.17\% accuracy on GeoTrust-test, demonstrating its
evaluation stringency. Crucially, models trained on GeoTrust achieve OOD
generalization on GeoQA, significantly reducing logical inconsistencies
relative to pseudo-label annotated by OpenAI-o1. Our code is available at
https://github.com/Alpha-Innovator/TrustGeoGen

</details>


### [55] [WALL-E 2.0: World Alignment by NeuroSymbolic Learning improves World Model-based LLM Agents](https://arxiv.org/abs/2504.15785)
*Siyu Zhou,Tianyi Zhou,Yijun Yang,Guodong Long,Deheng Ye,Jing Jiang,Chengqi Zhang*

Main category: cs.AI

TL;DR: 论文提出了一种无需训练的“世界对齐”方法，通过提取环境符号知识（如动作规则、知识图谱和场景图）来增强LLM作为世界模型的性能，并基于MPC框架设计了RL-free的智能体WALL-E 2.0，显著提升了新环境中的学习效率。


<details>
  <summary>Details</summary>
Motivation: 解决LLM作为世界模型时，其先验知识与特定环境动态之间的差距问题，以提升LLM智能体的性能。

Method: 提出“世界对齐”方法，提取环境的符号知识并编码为可执行代码；设计基于MPC框架的WALL-E 2.0智能体，利用LLM作为高效的前瞻优化器。

Result: 在Mars和ALFWorld环境中，WALL-E 2.0显著优于基线方法，如Mars中成功率提升16.1%-51.6%，ALFWorld中仅4次迭代即达到98%成功率。

Conclusion: 通过符号知识与LLM的结合，WALL-E 2.0实现了高效且准确的世界建模，显著提升了智能体在新环境中的表现。

Abstract: Can we build accurate world models out of large language models (LLMs)? How
can world models benefit LLM agents? The gap between the prior knowledge of
LLMs and the specified environment's dynamics usually bottlenecks LLMs'
performance as world models. To bridge the gap, we propose a training-free
"world alignment" that learns an environment's symbolic knowledge complementary
to LLMs. The symbolic knowledge covers action rules, knowledge graphs, and
scene graphs, which are extracted by LLMs from exploration trajectories and
encoded into executable codes to regulate LLM agents' policies. We further
propose an RL-free, model-based agent "WALL-E 2.0" through the model-predictive
control (MPC) framework. Unlike classical MPC requiring costly optimization on
the fly, we adopt an LLM agent as an efficient look-ahead optimizer of future
steps' actions by interacting with the neurosymbolic world model. While the LLM
agent's strong heuristics make it an efficient planner in MPC, the quality of
its planned actions is also secured by the accurate predictions of the aligned
world model. They together considerably improve learning efficiency in a new
environment. On open-world challenges in Mars (Minecraft like) and ALFWorld
(embodied indoor environments), WALL-E 2.0 significantly outperforms existing
methods, e.g., surpassing baselines in Mars by 16.1%-51.6% of success rate and
by at least 61.7% in score. In ALFWorld, it achieves a new record 98% success
rate after only 4 iterations.

</details>


### [56] [Crisp complexity of fuzzy classifiers](https://arxiv.org/abs/2504.15791)
*Raquel Fernandez-Peralta,Javier Fumanal-Idocin,Javier Andreu-Perez*

Main category: cs.AI

TL;DR: 提出一种将模糊规则分类器简化为清晰规则分类器的方法，分析其复杂性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 模糊规则分类器在非模糊领域应用受限，因其不易解释且用户对模糊逻辑不熟悉。

Method: 研究不同的清晰描述方法，并实现算法将其从模糊规则转换为清晰规则。

Result: 提出复杂性度量标准，帮助选择模糊分类器，并展示模糊与清晰规则间的转换可行性。

Conclusion: 该方法有助于模糊和非模糊领域用户更好地理解规则分类器，提升可解释性和应用范围。

Abstract: Rule-based systems are a very popular form of explainable AI, particularly in
the fuzzy community, where fuzzy rules are widely used for control and
classification problems. However, fuzzy rule-based classifiers struggle to
reach bigger traction outside of fuzzy venues, because users sometimes do not
know about fuzzy and because fuzzy partitions are not so easy to interpret in
some situations. In this work, we propose a methodology to reduce fuzzy
rule-based classifiers to crisp rule-based classifiers. We study different
possible crisp descriptions and implement an algorithm to obtain them. Also, we
analyze the complexity of the resulting crisp classifiers. We believe that our
results can help both fuzzy and non-fuzzy practitioners understand better the
way in which fuzzy rule bases partition the feature space and how easily one
system can be translated to another and vice versa. Our complexity metric can
also help to choose between different fuzzy classifiers based on what the
equivalent crisp partitions look like.

</details>


### [57] [Generative AI for Research Data Processing: Lessons Learnt From Three Use Cases](https://arxiv.org/abs/2504.15829)
*Modhurita Mitra,Martine G. de Vos,Nicola Cortinovis,Dawa Ometto*

Main category: cs.AI

TL;DR: 探索生成式AI在研究数据处理中的应用，展示其在复杂任务中的可行性，并分享使用经验。


<details>
  <summary>Details</summary>
Motivation: 生成式AI（如ChatGPT）引发广泛兴趣，但其输出准确性和一致性存在担忧，研究其在研究数据处理中的适用性。

Method: 选择传统方法难以处理的任务，使用Claude 3 Opus模型完成信息提取、自然语言理解和文本分类三项任务。

Result: 成功应用于植物物种名称提取、健康技术评估数据提取和众筹项目行业分类，验证了生成式AI的可行性。

Conclusion: 生成式AI适合特定数据处理任务，需合理评估任务适配性并优化方法以提高结果准确性。

Abstract: There has been enormous interest in generative AI since ChatGPT was launched
in 2022. However, there are concerns about the accuracy and consistency of the
outputs of generative AI. We have carried out an exploratory study on the
application of this new technology in research data processing. We identified
tasks for which rule-based or traditional machine learning approaches were
difficult to apply, and then performed these tasks using generative AI.
  We demonstrate the feasibility of using the generative AI model Claude 3 Opus
in three research projects involving complex data processing tasks:
  1) Information extraction: We extract plant species names from historical
seedlists (catalogues of seeds) published by botanical gardens.
  2) Natural language understanding: We extract certain data points (name of
drug, name of health indication, relative effectiveness, cost-effectiveness,
etc.) from documents published by Health Technology Assessment organisations in
the EU.
  3) Text classification: We assign industry codes to projects on the
crowdfunding website Kickstarter.
  We share the lessons we learnt from these use cases: How to determine if
generative AI is an appropriate tool for a given data processing task, and if
so, how to maximise the accuracy and consistency of the results obtained.

</details>


### [58] [CARE: Compatibility-Aware Incentive Mechanisms for Federated Learning with Budgeted Requesters](https://arxiv.org/abs/2504.15847)
*Xiang Liu,Hau Chan,Minming Li,Xianlong Zeng,Chenchen Fu,Weiwei Wu*

Main category: cs.AI

TL;DR: 本文研究了联邦学习中预算受限的请求者如何激励不兼容的工人参与训练，提出了两种兼容性感知的激励机制（CARE-CO和CARE-NO），并在实验中验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有激励机制忽略了工人不兼容性和请求者预算限制，导致联邦学习效率下降。

Method: 设计了兼容性感知的激励机制CARE-CO（合作预算）和CARE-NO（非合作预算），以解决工人不兼容性问题并优化预算分配。

Result: 实验表明，所提机制在个体理性、真实性、预算可行性和性能逼近方面优于现有基线。

Conclusion: CARE-CO和CARE-NO能有效提升联邦学习效率，适用于预算受限且工人不兼容的场景。

Abstract: Federated learning (FL) is a promising approach that allows requesters (\eg,
servers) to obtain local training models from workers (e.g., clients). Since
workers are typically unwilling to provide training services/models freely and
voluntarily, many incentive mechanisms in FL are designed to incentivize
participation by offering monetary rewards from requesters. However, existing
studies neglect two crucial aspects of real-world FL scenarios. First, workers
can possess inherent incompatibility characteristics (e.g., communication
channels and data sources), which can lead to degradation of FL efficiency
(e.g., low communication efficiency and poor model generalization). Second, the
requesters are budgeted, which limits the amount of workers they can hire for
their tasks. In this paper, we investigate the scenario in FL where multiple
budgeted requesters seek training services from incompatible workers with
private training costs. We consider two settings: the cooperative budget
setting where requesters cooperate to pool their budgets to improve their
overall utility and the non-cooperative budget setting where each requester
optimizes their utility within their own budgets. To address efficiency
degradation caused by worker incompatibility, we develop novel
compatibility-aware incentive mechanisms, CARE-CO and CARE-NO, for both
settings to elicit true private costs and determine workers to hire for
requesters and their rewards while satisfying requester budget constraints. Our
mechanisms guarantee individual rationality, truthfulness, budget feasibility,
and approximation performance. We conduct extensive experiments using
real-world datasets to show that the proposed mechanisms significantly
outperform existing baselines.

</details>


### [59] [Impact of Noise on LLM-Models Performance in Abstraction and Reasoning Corpus (ARC) Tasks with Model Temperature Considerations](https://arxiv.org/abs/2504.15903)
*Nikhil Khandalkar,Pavan Yadav,Krishna Shinde,Lokesh B. Ramegowda,Rajarshi Das*

Main category: cs.AI

TL;DR: 论文探讨了大型语言模型（LLMs）在抽象推理任务中的表现，发现其对噪声敏感，限制了实际应用。


<details>
  <summary>Details</summary>
Motivation: 评估LLMs在抽象推理任务（如ARC基准测试）中的能力，探索其在不同噪声条件下的表现差异。

Method: 系统测试了GPT-4o、DeepSeek R1和LLaMA 3.2在不同噪声水平和温度设置下的表现。

Result: 噪声显著降低所有模型的性能，表明当前LLMs对输入扰动高度敏感。

Conclusion: 需开发更鲁棒、适应性强的AI系统，以应对现实世界中的不确定性和噪声。

Abstract: Recent advancements in Large Language Models (LLMs) have generated growing
interest in their structured reasoning capabilities, particularly in tasks
involving abstraction and pattern recognition. The Abstraction and Reasoning
Corpus (ARC) benchmark plays a crucial role in evaluating these capabilities by
testing how well AI models generalize to novel problems. While GPT-4o
demonstrates strong performance by solving all ARC tasks under zero-noise
conditions, other models like DeepSeek R1 and LLaMA 3.2 fail to solve any,
suggesting limitations in their ability to reason beyond simple pattern
matching. To explore this gap, we systematically evaluate these models across
different noise levels and temperature settings. Our results reveal that the
introduction of noise consistently impairs model performance, regardless of
architecture. This decline highlights a shared vulnerability: current LLMs,
despite showing signs of abstract reasoning, remain highly sensitive to input
perturbations. Such fragility raises concerns about their real-world
applicability, where noise and uncertainty are common. By comparing how
different model architectures respond to these challenges, we offer insights
into the structural weaknesses of modern LLMs in reasoning tasks. This work
underscores the need for developing more robust and adaptable AI systems
capable of handling the ambiguity and variability inherent in real-world
scenarios. Our findings aim to guide future research toward enhancing model
generalization, robustness, and alignment with human-like cognitive
flexibility.

</details>


### [60] [Approximate matrices of systems of max-min fuzzy relational equations](https://arxiv.org/abs/2504.16042)
*Ismaïl Baaj*

Main category: cs.AI

TL;DR: 该论文提出了一种通过最小化修改矩阵来解决max-min模糊关系方程系统不一致性的方法，确保一致性并近似原始系统。


<details>
  <summary>Details</summary>
Motivation: 解决max-min模糊关系方程系统的不一致性问题，通过最小修改矩阵实现一致性。

Method: 通过最小化修改矩阵的系数，保持右侧向量不变，研究不同范数（L1、L2、L∞）下的距离，并给出L∞范数下的最小距离解析公式。

Result: 方法可直接计算L∞范数下最小距离的矩阵，计算成本较高但精确；同时结果可推广到min-max模糊关系方程系统。

Conclusion: 该方法有效解决不一致性问题，适用于多种模糊关系方程系统，并具有潜在应用价值。

Abstract: In this article, we address the inconsistency of a system of max-min fuzzy
relational equations by minimally modifying the matrix governing the system in
order to achieve consistency. Our method yields consistent systems that
approximate the original inconsistent system in the following sense: the
right-hand side vector of each consistent system is that of the inconsistent
system, and the coefficients of the matrix governing each consistent system are
obtained by modifying, exactly and minimally, the entries of the original
matrix that must be corrected to achieve consistency, while leaving all other
entries unchanged.
  To obtain a consistent system that closely approximates the considered
inconsistent system, we study the distance (in terms of a norm among $L_1$,
$L_2$ or $L_\infty$) between the matrix of the inconsistent system and the set
formed by the matrices of consistent systems that use the same right-hand side
vector as the inconsistent system. We show that our method allows us to
directly compute matrices of consistent systems that use the same right-hand
side vector as the inconsistent system whose distance in terms of $L_\infty$
norm to the matrix of the inconsistent system is minimal (the computational
costs are higher when using $L_1$ norm or $L_2$ norm). We also give an explicit
analytical formula for computing this minimal $L_\infty$ distance. Finally, we
translate our results for systems of min-max fuzzy relational equations and
present some potential applications.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [61] [Collaborative Learning of On-Device Small Model and Cloud-Based Large Model: Advances and Future Directions](https://arxiv.org/abs/2504.15300)
*Chaoyue Niu,Yucheng Ding,Junhui Lu,Zhengxiang Huang,Hang Zeng,Yutong Dai,Xuezhen Tu,Chengfei Lv,Fan Wu,Guihai Chen*

Main category: cs.LG

TL;DR: 论文探讨了设备端小模型与云端大模型协作学习的新范式，以解决延迟、成本、个性化和隐私问题，并综述了硬件、系统、算法和应用层的最新进展。


<details>
  <summary>Details</summary>
Motivation: 传统云端大模型学习框架在延迟、成本、个性化和隐私方面存在限制，需要新的解决方案。

Method: 从硬件、系统、算法和应用层全面综述协作学习，分类算法为数据、特征和参数框架，并评估数据集和指标。

Result: 总结了协作学习在各层的关键问题和进展，展示了实际部署案例，如推荐系统和智能助手。

Conclusion: 协作学习是未来发展方向，但仍需进一步研究以推动其快速发展。

Abstract: The conventional cloud-based large model learning framework is increasingly
constrained by latency, cost, personalization, and privacy concerns. In this
survey, we explore an emerging paradigm: collaborative learning between
on-device small model and cloud-based large model, which promises low-latency,
cost-efficient, and personalized intelligent services while preserving user
privacy. We provide a comprehensive review across hardware, system, algorithm,
and application layers. At each layer, we summarize key problems and recent
advances from both academia and industry. In particular, we categorize
collaboration algorithms into data-based, feature-based, and parameter-based
frameworks. We also review publicly available datasets and evaluation metrics
with user-level or device-level consideration tailored to collaborative
learning settings. We further highlight real-world deployments, ranging from
recommender systems and mobile livestreaming to personal intelligent
assistants. We finally point out open research directions to guide future
development in this rapidly evolving field.

</details>


### [62] [Power Transformer Health Index and Life Span Assessment: A Comprehensive Review of Conventional and Machine Learning based Approaches](https://arxiv.org/abs/2504.15310)
*Syeda Tahreem Zahra,Syed Kashif Imdad,Sohail Khan,Sohail Khalid,Nauman Anwar Baig*

Main category: cs.LG

TL;DR: 本文综述了电力变压器健康评估和寿命预测的传统与前沿技术，分析了各种AI方法的优缺点，并探讨了多方法融合和时间序列分析对提升故障诊断精度的贡献。


<details>
  <summary>Details</summary>
Motivation: 电力变压器对电力系统至关重要，其健康评估和寿命预测对高效运行和维护规划具有重要意义。

Method: 通过文献综述，分析传统和前沿技术，重点探讨了ANN、CNN、SVM、RF、GA和PSO等AI方法在变压器故障诊断中的应用。

Result: 多AI方法融合和时间序列分析显著提升了变压器故障诊断的精度和早期故障检测能力。

Conclusion: 本文为变压器故障诊断领域的未来研究提供了基础，推动了这一关键领域的发展。

Abstract: Power transformers play a critical role within the electrical power system,
making their health assessment and the prediction of their remaining lifespan
paramount for the purpose of ensuring efficient operation and facilitating
effective maintenance planning. This paper undertakes a comprehensive
examination of existent literature, with a primary focus on both conventional
and cutting-edge techniques employed within this domain. The merits and
demerits of recent methodologies and techniques are subjected to meticulous
scrutiny and explication. Furthermore, this paper expounds upon intelligent
fault diagnosis methodologies and delves into the most widely utilized
intelligent algorithms for the assessment of transformer conditions. Diverse
Artificial Intelligence (AI) approaches, including Artificial Neural Networks
(ANN) and Convolutional Neural Network (CNN), Support Vector Machine (SVM),
Random Forest (RF), Genetic Algorithm (GA), and Particle Swarm Optimization
(PSO), are elucidated offering pragmatic solutions for enhancing the
performance of transformer fault diagnosis. The amalgamation of multiple AI
methodologies and the exploration of timeseries analysis further contribute to
the augmentation of diagnostic precision and the early detection of faults in
transformers. By furnishing a comprehensive panorama of AI applications in the
field of transformer fault diagnosis, this study lays the groundwork for future
research endeavors and the progression of this critical area of study.

</details>


### [63] [M-TabNet: A Multi-Encoder Transformer Model for Predicting Neonatal Birth Weight from Multimodal Data](https://arxiv.org/abs/2504.15312)
*Muhammad Mursil,Hatem A. Rashwan,Luis Santos-Calderon,Pere Cavalle-Busquets,Michelle M. Murphy,Domenec Puig*

Main category: cs.LG

TL;DR: 该研究提出了一种基于注意力机制的Transformer模型，用于早期（妊娠12周内）预测出生体重，整合了生理、生活方式、营养和遗传等多维数据，显著提高了预测准确性。


<details>
  <summary>Details</summary>
Motivation: 现有出生体重预测方法（如超声检查）存在局限性，且现有模型常忽略营养和遗传因素。本研究旨在开发一种更准确、全面的早期预测工具。

Method: 采用多编码器架构的注意力Transformer模型，整合多维母体数据（生理、生活方式、营养、遗传）。

Result: 模型在内部数据集上MAE为122克，R²为0.94；独立验证数据集上MAE为105克，R²为0.95。分类性能优异（敏感性97.55%，特异性94.48%）。

Conclusion: 该深度学习模型为早期出生体重预测提供了高精度、可解释的工具，有助于临床风险分层和优化新生儿结局。

Abstract: Birth weight (BW) is a key indicator of neonatal health, with low birth
weight (LBW) linked to increased mortality and morbidity. Early prediction of
BW enables timely interventions; however, current methods like ultrasonography
have limitations, including reduced accuracy before 20 weeks and operator
dependent variability. Existing models often neglect nutritional and genetic
influences, focusing mainly on physiological and lifestyle factors. This study
presents an attention-based transformer model with a multi-encoder architecture
for early (less than 12 weeks of gestation) BW prediction. Our model
effectively integrates diverse maternal data such as physiological, lifestyle,
nutritional, and genetic, addressing limitations seen in prior attention-based
models such as TabNet. The model achieves a Mean Absolute Error (MAE) of 122
grams and an R-squared value of 0.94, demonstrating high predictive accuracy
and interoperability with our in-house private dataset. Independent validation
confirms generalizability (MAE: 105 grams, R-squared: 0.95) with the IEEE
children dataset. To enhance clinical utility, predicted BW is classified into
low and normal categories, achieving a sensitivity of 97.55% and a specificity
of 94.48%, facilitating early risk stratification. Model interpretability is
reinforced through feature importance and SHAP analyses, highlighting
significant influences of maternal age, tobacco exposure, and vitamin B12
status, with genetic factors playing a secondary role. Our results emphasize
the potential of advanced deep-learning models to improve early BW prediction,
offering clinicians a robust, interpretable, and personalized tool for
identifying pregnancies at risk and optimizing neonatal outcomes.

</details>


### [64] [Diffusion-Driven Inertial Generated Data for Smartphone Location Classification](https://arxiv.org/abs/2504.15315)
*Noa Cohen,Rotem Dror,Itzik Klein*

Main category: cs.LG

TL;DR: 论文提出了一种基于扩散模型的生成方法，用于智能手机位置识别中的特定力信号生成，以减少数据收集负担。


<details>
  <summary>Details</summary>
Motivation: 惯性测量数据收集耗时且资源密集，阻碍了机器学习模型的开发。扩散模型在生成复杂数据方面表现出色，为解决这一问题提供了新思路。

Method: 采用扩散模型生成特定力信号数据，并通过多指标对比评估合成数据与真实数据的差异。

Result: 扩散模型成功捕捉了不同智能手机放置条件下的特定力信号特征，生成了多样且逼真的合成数据。

Conclusion: 通过扩散模型生成高质量合成数据，可以减轻数据收集负担，为机器学习模型提供优质训练数据。

Abstract: Despite the crucial role of inertial measurements in motion tracking and
navigation systems, the time-consuming and resource-intensive nature of
collecting extensive inertial data has hindered the development of robust
machine learning models in this field. In recent years, diffusion models have
emerged as a revolutionary class of generative models, reshaping the landscape
of artificial data generation. These models surpass generative adversarial
networks and other state-of-the-art approaches to complex tasks. In this work,
we propose diffusion-driven specific force-generated data for smartphone
location recognition. We provide a comprehensive evaluation methodology by
comparing synthetic and real recorded specific force data across multiple
metrics. Our results demonstrate that our diffusion-based generative model
successfully captures the distinctive characteristics of specific force signals
across different smartphone placement conditions. Thus, by creating diverse,
realistic synthetic data, we can reduce the burden of extensive data collection
while providing high-quality training data for machine learning models.

</details>


### [65] [How to systematically develop an effective AI-based bias correction model?](https://arxiv.org/abs/2504.15322)
*Xiao Zhou,Yuze Sun,Jie Wu,Xiaomeng Huang*

Main category: cs.LG

TL;DR: ReSA-ConvLSTM是一个AI框架，用于数值天气预报中的系统性偏差校正，通过动态气候归一化、ConvLSTM和时间因果约束及残差自注意力机制，显著降低了预报误差。


<details>
  <summary>Details</summary>
Motivation: 解决数值天气预报中的系统性偏差问题，提升预报准确性。

Method: 结合动态气候归一化、ConvLSTM和时间因果约束、残差自注意力机制，建立物理感知的非线性映射。

Result: 在41年全球数据上，显著降低了T2m、U10/V10和SLP的偏差，RMSE减少达20%，模型轻量且泛化能力强。

Conclusion: 该框架通过结合变量特性显著提升了校正性能，适用于多变量和下游应用。

Abstract: This study introduces ReSA-ConvLSTM, an artificial intelligence (AI)
framework for systematic bias correction in numerical weather prediction (NWP).
We propose three innovations by integrating dynamic climatological
normalization, ConvLSTM with temporal causality constraints, and residual
self-attention mechanisms. The model establishes a physics-aware nonlinear
mapping between ECMWF forecasts and ERA5 reanalysis data. Using 41 years
(1981-2021) of global atmospheric data, the framework reduces systematic biases
in 2-m air temperature (T2m), 10-m winds (U10/V10), and sea-level pressure
(SLP), achieving up to 20% RMSE reduction over 1-7 day forecasts compared to
operational ECMWF outputs. The lightweight architecture (10.6M parameters)
enables efficient generalization to multiple variables and downstream
applications, reducing retraining time by 85% for cross-variable correction
while improving ocean model skill through bias-corrected boundary conditions.
The ablation experiments demonstrate that our innovations significantly improve
the model's correction performance, suggesting that incorporating variable
characteristics into the model helps enhance forecasting skills.

</details>


### [66] [HyperFlow: Gradient-Free Emulation of Few-Shot Fine-Tuning](https://arxiv.org/abs/2504.15323)
*Donggyun Kim,Chanwoo Kim,Seunghoon Hong*

Main category: cs.LG

TL;DR: 提出一种无需计算梯度的测试时适应方法，通过模拟梯度下降实现高效适应，显著降低计算和内存成本。


<details>
  <summary>Details</summary>
Motivation: 解决测试时微调在实时或低资源场景中因多次反向传播步骤导致的高成本问题。

Method: 将梯度下降建模为ODE的欧拉离散化，训练辅助网络预测任务条件漂移，仅需少量前向传播。

Result: 在跨域少样本分类任务中，性能显著优于基线，计算时间和内存成本仅为标准微调的0.02%和6%。

Conclusion: 该方法在直接迁移和完全微调之间提供了实用的折中方案。

Abstract: While test-time fine-tuning is beneficial in few-shot learning, the need for
multiple backpropagation steps can be prohibitively expensive in real-time or
low-resource scenarios. To address this limitation, we propose an approach that
emulates gradient descent without computing gradients, enabling efficient
test-time adaptation. Specifically, we formulate gradient descent as an Euler
discretization of an ordinary differential equation (ODE) and train an
auxiliary network to predict the task-conditional drift using only the few-shot
support set. The adaptation then reduces to a simple numerical integration
(e.g., via the Euler method), which requires only a few forward passes of the
auxiliary network -- no gradients or forward passes of the target model are
needed. In experiments on cross-domain few-shot classification using the
Meta-Dataset and CDFSL benchmarks, our method significantly improves
out-of-domain performance over the non-fine-tuned baseline while incurring only
6\% of the memory cost and 0.02\% of the computation time of standard
fine-tuning, thus establishing a practical middle ground between direct
transfer and fully fine-tuned approaches.

</details>


### [67] [Significativity Indices for Agreement Values](https://arxiv.org/abs/2504.15325)
*Alberto Casagrande,Francesco Fabris,Rossano Girometti,Roberto Pagliarini*

Main category: cs.LG

TL;DR: 该论文提出了一种评估分类器间一致性显著性的通用方法，并引入了两种显著性指数，同时解决了计算效率问题。


<details>
  <summary>Details</summary>
Motivation: 现有的一致性度量（如Cohen's kappa）缺乏有效的显著性评估标准，且现有质量尺度过于简单和主观。

Method: 提出两种显著性指数：一种针对有限数据集，另一种针对分类概率分布，并开发高效算法进行计算。

Result: 论文提供了评估一致性显著性的通用框架和高效算法，弥补了现有方法的不足。

Conclusion: 该方法为分类器一致性评估提供了更科学和实用的工具，适用于医学和人工智能等领域。

Abstract: Agreement measures, such as Cohen's kappa or intraclass correlation, gauge
the matching between two or more classifiers. They are used in a wide range of
contexts from medicine, where they evaluate the effectiveness of medical
treatments and clinical trials, to artificial intelligence, where they can
quantify the approximation due to the reduction of a classifier. The
consistency of different classifiers to a golden standard can be compared
simply by using the order induced by their agreement measure with respect to
the golden standard itself. Nevertheless, labelling an approach as good or bad
exclusively by using the value of an agreement measure requires a scale or a
significativity index. Some quality scales have been proposed in the literature
for Cohen's kappa, but they are mainly naive, and their boundaries are
arbitrary. This work proposes a general approach to evaluate the
significativity of any agreement value between two classifiers and introduces
two significativity indices: one dealing with finite data sets, the other one
handling classification probability distributions. Moreover, this manuscript
considers the computational issues of evaluating such indices and identifies
some efficient algorithms to evaluate them.

</details>


### [68] [Bayesian Federated Learning for Continual Training](https://arxiv.org/abs/2504.15328)
*Usevalad Milasheuski,Luca Barbieri,Sanaz Kianoush,Monica Nicoli,Stefano Savazzi*

Main category: cs.LG

TL;DR: 该论文提出了一种基于贝叶斯联邦学习的持续学习框架，用于动态环境中数据分布变化的适应性问题。


<details>
  <summary>Details</summary>
Motivation: 当前贝叶斯联邦学习方法未解决动态环境中数据分布变化的持续学习挑战。

Method: 使用随机梯度朗之万动力学（SGLD）方法，通过利用过去的后验分布构建新任务的先验分布，实现模型的持续更新。

Result: 实验结果表明，该方法在准确性、预期校准误差（ECE）和收敛速度方面优于基线方法。

Conclusion: 持续贝叶斯更新在保留知识和适应动态数据方面具有显著效果。

Abstract: Bayesian Federated Learning (BFL) enables uncertainty quantification and
robust adaptation in distributed learning. In contrast to the frequentist
approach, it estimates the posterior distribution of a global model, offering
insights into model reliability. However, current BFL methods neglect continual
learning challenges in dynamic environments where data distributions shift over
time. We propose a continual BFL framework applied to human sensing with radar
data collected over several days. Using Stochastic Gradient Langevin Dynamics
(SGLD), our approach sequentially updates the model, leveraging past posteriors
to construct the prior for the new tasks. We assess the accuracy, the expected
calibration error (ECE) and the convergence speed of our approach against
several baselines. Results highlight the effectiveness of continual Bayesian
updates in preserving knowledge and adapting to evolving data.

</details>


### [69] [FedFetch: Faster Federated Learning with Adaptive Downstream Prefetching](https://arxiv.org/abs/2504.15366)
*Qifan Yan,Andrew Liu,Shiqi He,Mathias Lécuyer,Ivan Beschastnikh*

Main category: cs.LG

TL;DR: FedFetch是一种策略，通过预取模型状态来减少联邦学习中客户端采样和压缩技术结合时的下载时间开销。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中大量异构客户端导致通信瓶颈，尤其是未选中的客户端需要同步过时的模型状态，进一步拖慢训练速度。

Method: 提出FedFetch策略，通过预取模型状态多轮次来优化下载时间。

Result: 实验表明，FedFetch将端到端训练时间减少1.26倍，下载时间减少4.49倍。

Conclusion: FedFetch有效解决了联邦学习中客户端采样和压缩技术结合时的通信瓶颈问题。

Abstract: Federated learning (FL) is a machine learning paradigm that facilitates
massively distributed model training with end-user data on edge devices
directed by a central server. However, the large number of heterogeneous
clients in FL deployments leads to a communication bottleneck between the
server and the clients. This bottleneck is made worse by straggling clients,
any one of which will further slow down training. To tackle these challenges,
researchers have proposed techniques like client sampling and update
compression. These techniques work well in isolation but combine poorly in the
downstream, server-to-client direction. This is because unselected clients have
outdated local model states and need to synchronize these states with the
server first.
  We introduce FedFetch, a strategy to mitigate the download time overhead
caused by combining client sampling and compression techniques. FedFetch
achieves this with an efficient prefetch schedule for clients to prefetch model
states multiple rounds before a stated training round. We empirically show that
adding FedFetch to communication efficient FL techniques reduces end-to-end
training time by 1.26$\times$ and download time by 4.49$\times$ across
compression techniques with heterogeneous client settings. Our implementation
is available at https://github.com/DistributedML/FedFetch

</details>


### [70] [Solving New Tasks by Adapting Internet Video Knowledge](https://arxiv.org/abs/2504.15369)
*Calvin Luo,Zilai Zeng,Yilun Du,Chen Sun*

Main category: cs.LG

TL;DR: 论文探讨了如何通过适应技术将大规模预训练视频模型与特定领域信息结合，以支持机器人任务中的文本条件泛化。


<details>
  <summary>Details</summary>
Motivation: 视频生成模型在机器人领域潜力巨大，但预训练模型可能忽略特定环境细节，而领域内训练数据不足又限制了泛化能力。

Method: 研究了多种适应技术，提出了一种名为“逆概率适应”的新策略。

Result: 实验证明，小规模示例数据适应视频模型能有效支持新行为的泛化，且新策略在任务和设置中表现稳健。

Conclusion: 逆概率适应策略在机器人任务中表现出色，即使数据质量不佳也能成功解决新任务。

Abstract: Video generative models demonstrate great promise in robotics by serving as
visual planners or as policy supervisors. When pretrained on internet-scale
data, such video models intimately understand alignment with natural language,
and can thus facilitate generalization to novel downstream behavior through
text-conditioning. However, they may not be sensitive to the specificities of
the particular environment the agent inhabits. On the other hand, training
video models on in-domain examples of robotic behavior naturally encodes
environment-specific intricacies, but the scale of available demonstrations may
not be sufficient to support generalization to unseen tasks via natural
language specification. In this work, we investigate different adaptation
techniques that integrate in-domain information with large-scale pretrained
video models, and explore the extent to which they enable novel
text-conditioned generalization for robotic tasks, while also considering their
independent data and resource considerations. We successfully demonstrate
across robotic environments that adapting powerful video models with small
scales of example data can successfully facilitate generalization to novel
behaviors. In particular, we present a novel adaptation strategy, termed
Inverse Probabilistic Adaptation, that not only consistently achieves strong
generalization performance across robotic tasks and settings, but also exhibits
robustness to the quality of adaptation data, successfully solving novel tasks
even when only suboptimal in-domain demonstrations are available.

</details>


### [71] [Improving Learning to Optimize Using Parameter Symmetries](https://arxiv.org/abs/2504.15399)
*Guy Zamir,Aryan Dokania,Bo Zhao,Rose Yu*

Main category: cs.LG

TL;DR: 论文提出了一种利用参数空间对称性提升优化效率的学习优化（L2O）算法，理论分析表明其局部类似牛顿法，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过参数空间对称性提升元优化器的性能，以改进优化效率。

Method: 结合学习对称变换和局部更新，理论分析其局部类似牛顿法，并通过实验引入基准测试验证算法。

Result: 算法在训练中能学习到正确的对称变换，且增强方法（如动量）进一步提升了性能。

Conclusion: 利用神经网络参数空间对称性具有推动元优化发展的潜力。

Abstract: We analyze a learning-to-optimize (L2O) algorithm that exploits parameter
space symmetry to enhance optimization efficiency. Prior work has shown that
jointly learning symmetry transformations and local updates improves
meta-optimizer performance. Supporting this, our theoretical analysis
demonstrates that even without identifying the optimal group element, the
method locally resembles Newton's method. We further provide an example where
the algorithm provably learns the correct symmetry transformation during
training. To empirically evaluate L2O with teleportation, we introduce a
benchmark, analyze its success and failure cases, and show that enhancements
like momentum further improve performance. Our results highlight the potential
of leveraging neural network parameter space symmetry to advance
meta-optimization.

</details>


### [72] [Combating Toxic Language: A Review of LLM-Based Strategies for Software Engineering](https://arxiv.org/abs/2504.15439)
*Hao Zhuo,Yicheng Yang,Kewen Peng*

Main category: cs.LG

TL;DR: 本文综述了大型语言模型（LLMs）在软件工程中的毒性语言检测与缓解研究，总结了现有方法并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: LLMs在软件工程中的广泛应用引发了毒性语言传播的担忧，需研究如何检测和缓解此类内容。

Method: 回顾了毒性检测与缓解的研究，包括标注、预处理、检测方法和LLM驱动的缓解策略，并进行了消融实验。

Result: LLM重写技术能有效减少毒性，但现有研究仍有不足。

Conclusion: 未来需进一步研究以确保LLMs在软件工程中的负责任使用。

Abstract: Large Language Models (LLMs) have become integral to software engineering
(SE), where they are increasingly used in development workflows. However, their
widespread use raises concerns about the presence and propagation of toxic
language--harmful or offensive content that can foster exclusionary
environments. This paper provides a comprehensive review of recent research on
toxicity detection and mitigation, focusing on both SE-specific and
general-purpose datasets. We examine annotation and preprocessing techniques,
assess detection methodologies, and evaluate mitigation strategies,
particularly those leveraging LLMs. Additionally, we conduct an ablation study
demonstrating the effectiveness of LLM-based rewriting for reducing toxicity.
By synthesizing existing work and identifying open challenges, this review
highlights key areas for future research to ensure the responsible deployment
of LLMs in SE and beyond.

</details>


### [73] [Compton Form Factor Extraction using Quantum Deep Neural Networks](https://arxiv.org/abs/2504.15458)
*Brandon Le,Dustin Keller*

Main category: cs.LG

TL;DR: 论文通过伪数据提取康普顿形状因子，比较了经典深度神经网络（CDNN）和量子深度神经网络（QDNN）的性能，发现QDNN表现更优。


<details>
  <summary>Details</summary>
Motivation: 研究目的是利用实验数据提取康普顿形状因子，并探索量子算法在此类问题中的潜力。

Method: 采用标准Belitsky-Kirchner-Muller形式主义（twist-two）和减少模型依赖的拟合方法，同时使用CDNN和QDNN进行提取。

Result: QDNN在预测准确性和精度上优于CDNN，尤其在模型复杂度有限时表现更佳。

Conclusion: QDNN展示了在量子算法优化后的未来研究中的潜力。

Abstract: Extraction tests of Compton Form Factors are performed using pseudodata based
on experimental data from Deeply Virtual Compton Scattering experiments
conducted at Jefferson Lab. The standard Belitsky, Kirchner, and Muller
formalism at twist-two is employed, along with a fitting procedure designed to
reduce model dependency similar to traditional local fits. The extraction of
the Compton Form Factors is performed using both Classical Deep Neural Networks
(CDNNs) and Quantum Deep Neural Networks (QDNNs). Comparative studies reveal
that QDNNs outperform CDNNs for this application, demonstrating improved
predictive accuracy and precision even for limited model complexity. The
results demonstrate the potential of QDNNs for future studies in which quantum
algorithms can be fully optimized.

</details>


### [74] [In-context Ranking Preference Optimization](https://arxiv.org/abs/2504.15477)
*Junda Wu,Rohan Surana,Zhouhang Xie,Yiran Shen,Yu Xia,Tong Yu,Ryan A. Rossi,Prithviraj Ammanabrolu,Julian McAuley*

Main category: cs.LG

TL;DR: 提出了In-context Ranking Preference Optimization (IRPO)框架，通过优化LLM基于推理过程中构建的排名列表，解决了有限和稀疏的成对反馈问题。


<details>
  <summary>Details</summary>
Motivation: 用户反馈通常仅涉及识别上下文中的少数相关项，而非详细的成对比较，且复杂任务（如对话代理和摘要系统）需要高质量输出的排名。

Method: IRPO扩展了DPO目标，结合了项的相关性和列表位置，通过位置聚合成对偏好引入可微分目标。

Result: IRPO在排名性能上优于标准DPO方法，理论分析显示其能自动强调模型与参考排名间的分歧项。

Conclusion: IRPO有效解决了稀疏反馈问题，并提升了LLM与上下文排名偏好的对齐能力。

Abstract: Recent developments in Direct Preference Optimization (DPO) allow large
language models (LLMs) to function as implicit ranking models by maximizing the
margin between preferred and non-preferred responses. In practice, user
feedback on such lists typically involves identifying a few relevant items in
context rather than providing detailed pairwise comparisons for every possible
item pair. Moreover, many complex information retrieval tasks, such as
conversational agents and summarization systems, critically depend on ranking
the highest-quality outputs at the top, emphasizing the need to support natural
and flexible forms of user feedback. To address the challenge of limited and
sparse pairwise feedback in the in-context setting, we propose an In-context
Ranking Preference Optimization (IRPO) framework that directly optimizes LLMs
based on ranking lists constructed during inference. To further capture
flexible forms of feedback, IRPO extends the DPO objective by incorporating
both the relevance of items and their positions in the list. Modeling these
aspects jointly is non-trivial, as ranking metrics are inherently discrete and
non-differentiable, making direct optimization difficult. To overcome this,
IRPO introduces a differentiable objective based on positional aggregation of
pairwise item preferences, enabling effective gradient-based optimization of
discrete ranking metrics. We further provide theoretical insights showing that
IRPO (i) automatically emphasizes items with greater disagreement between the
model and the reference ranking, and (ii) links its gradient to an importance
sampling estimator, yielding an unbiased estimator with reduced variance.
Empirical results show IRPO outperforms standard DPO approaches in ranking
performance, highlighting its effectiveness in aligning LLMs with direct
in-context ranking preferences.

</details>


### [75] [Unifying Image Counterfactuals and Feature Attributions with Latent-Space Adversarial Attacks](https://arxiv.org/abs/2504.15479)
*Jeremy Goldwasser,Giles Hooker*

Main category: cs.LG

TL;DR: 论文提出了一种新的反事实图像生成框架，通过低维流形上的对抗攻击生成解释性图像，并结合特征归因量化变化。


<details>
  <summary>Details</summary>
Motivation: 解决传统梯度方法在生成反事实图像时易产生对抗样本的问题，适应现代生成模型的发展。

Method: 提出Counterfactual Attacks方法，在低维流形上进行对抗攻击，并利用辅助数据集生成特征归因。

Result: 在MNIST和CelebA数据集上验证了方法的有效性，能高效生成全局反事实解释。

Conclusion: 该方法为计算机视觉模型提供了一种灵活且高效的反事实解释框架。

Abstract: Counterfactuals are a popular framework for interpreting machine learning
predictions. These what if explanations are notoriously challenging to create
for computer vision models: standard gradient-based methods are prone to
produce adversarial examples, in which imperceptible modifications to image
pixels provoke large changes in predictions. We introduce a new,
easy-to-implement framework for counterfactual images that can flexibly adapt
to contemporary advances in generative modeling. Our method, Counterfactual
Attacks, resembles an adversarial attack on the representation of the image
along a low-dimensional manifold. In addition, given an auxiliary dataset of
image descriptors, we show how to accompany counterfactuals with feature
attribution that quantify the changes between the original and counterfactual
images. These importance scores can be aggregated into global counterfactual
explanations that highlight the overall features driving model predictions.
While this unification is possible for any counterfactual method, it has
particular computational efficiency for ours. We demonstrate the efficacy of
our approach with the MNIST and CelebA datasets.

</details>


### [76] [Fourier analysis of the physics of transfer learning for data-driven subgrid-scale models of ocean turbulence](https://arxiv.org/abs/2504.15487)
*Moein Darman,Pedram Hassanzadeh,Laure Zanna,Ashesh Chattopadhyay*

Main category: cs.LG

TL;DR: 该研究探讨了迁移学习（TL）如何通过重新训练神经网络的一层来提升其在未见动态系统中的性能，并分析了其通用性。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决神经网络在未经迁移学习时对分布外数据泛化能力不足的问题，特别是在海洋准地转系统中的应用。

Method: 使用9层卷积神经网络预测两层海洋准地转系统中的亚网格强迫，并通过傅里叶分析网络核以评估性能。

Result: 发现未经迁移学习的网络会低估输出频谱，而通过重新训练一层可纠正这一问题，使预测与目标频谱匹配。

Conclusion: 迁移学习能有效提升神经网络在动态系统参数化中的通用性，方法简单且适用性广。

Abstract: Transfer learning (TL) is a powerful tool for enhancing the performance of
neural networks (NNs) in applications such as weather and climate prediction
and turbulence modeling. TL enables models to generalize to out-of-distribution
data with minimal training data from the new system. In this study, we employ a
9-layer convolutional NN to predict the subgrid forcing in a two-layer ocean
quasi-geostrophic system and examine which metrics best describe its
performance and generalizability to unseen dynamical regimes. Fourier analysis
of the NN kernels reveals that they learn low-pass, Gabor, and high-pass
filters, regardless of whether the training data are isotropic or anisotropic.
By analyzing the activation spectra, we identify why NNs fail to generalize
without TL and how TL can overcome these limitations: the learned weights and
biases from one dataset underestimate the out-of-distribution sample spectra as
they pass through the network, leading to an underestimation of output spectra.
By re-training only one layer with data from the target system, this
underestimation is corrected, enabling the NN to produce predictions that match
the target spectra. These findings are broadly applicable to data-driven
parameterization of dynamical systems.

</details>


### [77] [Application of Deep Generative Models for Anomaly Detection in Complex Financial Transactions](https://arxiv.org/abs/2504.15491)
*Tengda Tang,Jianhua Yao,Yixian Wang,Qiuwu Sha,Hanrui Feng,Zhen Xu*

Main category: cs.LG

TL;DR: 提出了一种基于深度生成模型的算法，用于检测大规模支付流中的可疑行为，结合GAN和VAE，显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 金融交易中的异常行为（如欺诈和洗钱）检测需求迫切，传统方法在稀疏数据条件下表现不佳。

Method: 结合GAN生成模拟数据以近似正常支付流，利用判别器识别异常；引入VAE建模支付流的潜在分布，提升生成数据的真实性。

Result: 实验表明，该方法在多种评估指标上显著优于传统机器学习和其他深度学习模型，尤其在检测罕见欺诈行为时表现突出。

Conclusion: 生成模型在处理复杂金融数据方面具有优势，尤其在稀疏数据条件下仍能有效捕捉可疑行为。

Abstract: This study proposes an algorithm for detecting suspicious behaviors in large
payment flows based on deep generative models. By combining Generative
Adversarial Networks (GAN) and Variational Autoencoders (VAE), the algorithm is
designed to detect abnormal behaviors in financial transactions. First, the GAN
is used to generate simulated data that approximates normal payment flows. The
discriminator identifies anomalous patterns in transactions, enabling the
detection of potential fraud and money laundering behaviors. Second, a VAE is
introduced to model the latent distribution of payment flows, ensuring that the
generated data more closely resembles real transaction features, thus improving
the model's detection accuracy. The method optimizes the generative
capabilities of both GAN and VAE, ensuring that the model can effectively
capture suspicious behaviors even in sparse data conditions. Experimental
results show that the proposed method significantly outperforms traditional
machine learning algorithms and other deep learning models across various
evaluation metrics, especially in detecting rare fraudulent behaviors.
Furthermore, this study provides a detailed comparison of performance in
recognizing different transaction patterns (such as normal, money laundering,
and fraud) in large payment flows, validating the advantages of generative
models in handling complex financial data.

</details>


### [78] [Federated Latent Factor Learning for Recovering Wireless Sensor Networks Signal with Privacy-Preserving](https://arxiv.org/abs/2504.15525)
*Chengjun Yu,Yixin Ran,Yangyi Xia,Jia Wu,Xiaojing Liu*

Main category: cs.LG

TL;DR: 论文提出了一种基于联邦潜在因子学习（FLFL）的空间信号恢复（SSR）模型FLFL-SSR，用于解决无线传感器网络（WSNs）中数据缺失和隐私保护问题。


<details>
  <summary>Details</summary>
Motivation: 由于传感器故障和节能策略，WSNs采集的数据常存在大量缺失，且现有潜在因子学习方法未充分考虑数据隐私保护。

Method: 设计了传感器级联邦学习框架，仅上传梯度更新而非原始数据，并提出局部空间共享策略以捕捉空间相关性。

Result: 在两个真实WSNs数据集上的实验表明，FLFL-SSR在恢复性能上优于现有联邦方法。

Conclusion: FLFL-SSR模型有效解决了数据缺失和隐私保护问题，提升了恢复精度。

Abstract: Wireless Sensor Networks (WSNs) are a cutting-edge domain in the field of
intelligent sensing. Due to sensor failures and energy-saving strategies, the
collected data often have massive missing data, hindering subsequent analysis
and decision-making. Although Latent Factor Learning (LFL) has been proven
effective in recovering missing data, it fails to sufficiently consider data
privacy protection. To address this issue, this paper innovatively proposes a
federated latent factor learning (FLFL) based spatial signal recovery (SSR)
model, named FLFL-SSR. Its main idea is two-fold: 1) it designs a sensor-level
federated learning framework, where each sensor uploads only gradient updates
instead of raw data to optimize the global model, and 2) it proposes a local
spatial sharing strategy, allowing sensors within the same spatial region to
share their latent feature vectors, capturing spatial correlations and
enhancing recovery accuracy. Experimental results on two real-world WSNs
datasets demonstrate that the proposed model outperforms existing federated
methods in terms of recovery performance.

</details>


### [79] [Interpretable Deep Learning for Polar Mechanistic Reaction Prediction](https://arxiv.org/abs/2504.15539)
*Ryan J. Miller,Alexander E. Dashuta,Brayden Rudisill,David Van Vranken,Pierre Baldi*

Main category: cs.LG

TL;DR: PMechRP是一种基于深度学习的化学反应预测系统，通过捕捉电子流动和机理细节，显著提高了预测准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统化学反应预测方法耗时且资源密集，现有深度学习模型缺乏机理洞察力。PMechRP旨在填补这一空白。

Method: 利用PMechDB数据集训练多种机器学习模型，包括Transformer和图神经网络，并采用混合模型优化性能。

Result: 混合模型在PMechDB测试集上达到94.9%的top-10准确率，在路径数据集上达到84.9%的目标恢复率。

Conclusion: PMechRP通过结合机理细节和数据增强，显著提升了反应预测的准确性和实用性。

Abstract: Accurately predicting chemical reactions is essential for driving innovation
in synthetic chemistry, with broad applications in medicine, manufacturing, and
agriculture. At the same time, reaction prediction is a complex problem which
can be both time-consuming and resource-intensive for chemists to solve. Deep
learning methods offer an appealing solution by enabling high-throughput
reaction prediction. However, many existing models are trained on the US Patent
Office dataset and treat reactions as overall transformations: mapping
reactants directly to products with limited interpretability or mechanistic
insight. To address this, we introduce PMechRP (Polar Mechanistic Reaction
Predictor), a system that trains machine learning models on the PMechDB
dataset, which represents reactions as polar elementary steps that capture
electron flow and mechanistic detail. To further expand model coverage and
improve generalization, we augment PMechDB with a diverse set of
combinatorially generated reactions. We train and compare a range of machine
learning models, including transformer-based, graph-based, and two-step siamese
architectures. Our best-performing approach was a hybrid model, which combines
a 5-ensemble of Chemformer models with a two-step Siamese framework to leverage
the accuracy of transformer architectures, while filtering away "alchemical"
products using the two-step network predictions. For evaluation, we use a test
split of the PMechDB dataset and additionally curate a human benchmark dataset
consisting of complete mechanistic pathways extracted from an organic chemistry
textbook. Our hybrid model achieves a top-10 accuracy of 94.9% on the PMechDB
test set and a target recovery rate of 84.9% on the pathway dataset.

</details>


### [80] [Bayesian Autoencoder for Medical Anomaly Detection: Uncertainty-Aware Approach for Brain 2 MRI Analysis](https://arxiv.org/abs/2504.15562)
*Dip Roy*

Main category: cs.LG

TL;DR: 本文提出了一种基于贝叶斯变分自编码器（VAE）和多头注意力机制的模型，用于脑部MRI异常检测，通过估计认知和随机不确定性提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统确定性方法在捕捉异常检测任务的不确定性方面存在不足，尤其是在医疗影像中对神经系统疾病的诊断中。

Method: 采用贝叶斯变分自编码器（VAE）结合多头注意力机制，通过贝叶斯推理估计认知和随机不确定性。

Result: 在BraTS2020数据集上测试，模型取得了0.83的ROC AUC和0.83的PR AUC。

Conclusion: 建模不确定性是异常检测的关键，不仅能提升性能和可解释性，还能为临床决策提供置信度估计和异常预测。

Abstract: In medical imaging, anomaly detection is a vital element of healthcare
diagnostics, especially for neurological conditions which can be
life-threatening. Conventional deterministic methods often fall short when it
comes to capturing the inherent uncertainty of anomaly detection tasks. This
paper introduces a Bayesian Variational Autoencoder (VAE) equipped with
multi-head attention mechanisms for detecting anomalies in brain magnetic
resonance imaging (MRI). For the purpose of improving anomaly detection
performance, we incorporate both epistemic and aleatoric uncertainty estimation
through Bayesian inference. The model was tested on the BraTS2020 dataset, and
the findings were a 0.83 ROC AUC and a 0.83 PR AUC. The data in our paper
suggests that modeling uncertainty is an essential component of anomaly
detection, enhancing both performance and interpretability and providing
confidence estimates, as well as anomaly predictions, for clinicians to
leverage in making medical decisions.

</details>


### [81] [Smooth Calibration and Decision Making](https://arxiv.org/abs/2504.15582)
*Jason Hartline,Yifan Wu,Yunran Yang*

Main category: cs.LG

TL;DR: 论文探讨了机器学习预测器的校准误差在决策中的不连续性，并提出了一种后处理方法，通过添加噪声实现差分隐私，以优化决策校准误差。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于解决机器学习预测器在决策中的校准误差问题，因为现有的连续校准误差指标（如平滑校准误差）与决策者实际体验的不连续校准误差（如预期校准误差）之间存在不一致。

Method: 提出了一种后处理方法，通过对在线预测器添加噪声使其差分隐私，从而将距离校准误差转化为决策校准误差（ECE和CDL）的优化。

Result: 结果表明，后处理方法可以将距离校准误差的预测器优化为具有$O(\sqrt{\epsilon})$的ECE和CDL，这是渐近最优的。

Conclusion: 结论指出，后处理方法虽然有效，但与直接优化ECE和CDL的在线校准算法相比，其最优边界仍非最优。

Abstract: Calibration requires predictor outputs to be consistent with their Bayesian
posteriors. For machine learning predictors that do not distinguish between
small perturbations, calibration errors are continuous in predictions, e.g.,
smooth calibration error (Foster and Hart, 2018), Distance to Calibration
(Blasiok et al., 2023a). On the contrary, decision-makers who use predictions
make optimal decisions discontinuously in probabilistic space, experiencing
loss from miscalibration discontinuously. Calibration errors for
decision-making are thus discontinuous, e.g., Expected Calibration Error
(Foster and Vohra, 1997), and Calibration Decision Loss (Hu and Wu, 2024).
Thus, predictors with a low calibration error for machine learning may suffer a
high calibration error for decision-making, i.e., they may not be trustworthy
for decision-makers optimizing assuming their predictions are correct. It is
natural to ask if post-processing a predictor with a low calibration error for
machine learning is without loss to achieve a low calibration error for
decision-making. In our paper, we show that post-processing an online predictor
with $\epsilon$ distance to calibration achieves $O(\sqrt{\epsilon})$ ECE and
CDL, which is asymptotically optimal. The post-processing algorithm adds noise
to make predictions differentially private. The optimal bound from low distance
to calibration predictors from post-processing is non-optimal compared with
existing online calibration algorithms that directly optimize for ECE and CDL.

</details>


### [82] [MetaMolGen: A Neural Graph Motif Generation Model for De Novo Molecular Design](https://arxiv.org/abs/2504.15587)
*Zimo Yan,Jie Zhang,Zheng Xie,Chang Liu,Yizhen Liu,Yiping Song*

Main category: cs.LG

TL;DR: MetaMolGen是一种基于元学习的分子生成器，用于少样本和属性条件分子生成，通过标准化图基序分布和轻量级自回归模型生成SMILES序列，实验表明其在低数据条件下优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 解决传统生成模型在数据稀缺场景下难以实现满意条件泛化的问题。

Method: 通过映射图基序到标准化潜在空间，使用轻量级自回归模型生成SMILES序列，并集成可学习属性投影器实现条件生成。

Result: 在低数据条件下，MetaMolGen能生成有效且多样的SMILES序列，优于传统基线。

Conclusion: MetaMolGen在快速适应和高效条件生成方面具有优势，适用于实际分子设计。

Abstract: Molecular generation plays an important role in drug discovery and materials
science, especially in data-scarce scenarios where traditional generative
models often struggle to achieve satisfactory conditional generalization. To
address this challenge, we propose MetaMolGen, a first-order
meta-learning-based molecular generator designed for few-shot and
property-conditioned molecular generation. MetaMolGen standardizes the
distribution of graph motifs by mapping them to a normalized latent space, and
employs a lightweight autoregressive sequence model to generate SMILES
sequences that faithfully reflect the underlying molecular structure. In
addition, it supports conditional generation of molecules with target
properties through a learnable property projector integrated into the
generative process.Experimental results demonstrate that MetaMolGen
consistently generates valid and diverse SMILES sequences under low-data
regimes, outperforming conventional baselines. This highlights its advantage in
fast adaptation and efficient conditional generation for practical molecular
design.

</details>


### [83] [Analytical Softmax Temperature Setting from Feature Dimensions for Model- and Domain-Robust Classification](https://arxiv.org/abs/2504.15594)
*Tatsuhito Hasegawa,Shunsuke Sakai*

Main category: cs.LG

TL;DR: 本文提出了一种无需训练即可确定softmax温度参数T*的方法，基于特征维度理论，并通过实验优化和修正系数，最终提升了分类性能。


<details>
  <summary>Details</summary>
Motivation: 研究softmax温度参数T*对分类任务的影响，发现其与特征维度相关，但实际应用中T*会因模型和数据集变化而波动。

Method: 提出基于特征维度的温度确定系数，并在输出层前加入批归一化层以稳定特征空间，通过大规模实验优化T*的估计公式。

Result: 提出的方法不仅能与理论一致，还能在不同任务中有效提升分类性能。

Conclusion: 研究提供了一种无需训练即可确定T*的实用方案，适用于多样化任务。

Abstract: In deep learning-based classification tasks, the softmax function's
temperature parameter $T$ critically influences the output distribution and
overall performance. This study presents a novel theoretical insight that the
optimal temperature $T^*$ is uniquely determined by the dimensionality of the
feature representations, thereby enabling training-free determination of $T^*$.
Despite this theoretical grounding, empirical evidence reveals that $T^*$
fluctuates under practical conditions owing to variations in models, datasets,
and other confounding factors. To address these influences, we propose and
optimize a set of temperature determination coefficients that specify how $T^*$
should be adjusted based on the theoretical relationship to feature
dimensionality. Additionally, we insert a batch normalization layer immediately
before the output layer, effectively stabilizing the feature space. Building on
these coefficients and a suite of large-scale experiments, we develop an
empirical formula to estimate $T^*$ without additional training while also
introducing a corrective scheme to refine $T^*$ based on the number of classes
and task complexity. Our findings confirm that the derived temperature not only
aligns with the proposed theoretical perspective but also generalizes
effectively across diverse tasks, consistently enhancing classification
performance and offering a practical, training-free solution for determining
$T^*$.

</details>


### [84] [Learning Dynamic Graphs via Tensorized and Lightweight Graph Convolutional Networks](https://arxiv.org/abs/2504.15613)
*Minglian Han*

Main category: cs.LG

TL;DR: 论文提出了一种新型的张量轻量图卷积网络（TLGCN），用于动态图学习，解决了传统动态图卷积网络（DGCN）中时空依赖建模分离的问题。


<details>
  <summary>Details</summary>
Motivation: 传统DGCN通过静态GCN和序列神经网络（SNN）分别建模时空模式，破坏了复杂的时空依赖关系，因此需要一种联合建模方法。

Method: 设计了基于张量M积框架的时空信息联合传播方法，并提出了TLGCN，通过省略复杂特征变换和非线性激活显著减少内存占用。

Result: 在四个真实数据集上的数值实验表明，TLGCN在动态图权重估计任务中优于现有最优模型。

Conclusion: TLGCN通过联合建模时空依赖和轻量化设计，显著提升了动态图学习的性能。

Abstract: A dynamic graph (DG) is frequently encountered in numerous real-world
scenarios. Consequently, A dynamic graph convolutional network (DGCN) has been
successfully applied to perform precise representation learning on a DG.
However, conventional DGCNs typically consist of a static GCN coupled with a
sequence neural network (SNN) to model spatial and temporal patterns
separately. This decoupled modeling mechanism inherently disrupts the intricate
spatio-temporal dependencies. To address the issue, this study proposes a novel
Tensorized Lightweight Graph Convolutional Network (TLGCN) for accurate dynamic
graph learning. It mainly contains the following two key concepts: a) designing
a novel spatio-temporal information propagation method for joint propagation of
spatio-temporal information based on the tensor M-product framework; b)
proposing a tensorized lightweight graph convolutional network based on the
above method, which significantly reduces the memory occupation of the model by
omitting complex feature transformation and nonlinear activation. Numerical
experiments on four real-world datasets demonstrate that the proposed TLGCN
outperforms the state-of-the-art models in the weight estimation task on DGs.

</details>


### [85] [Dimension-Free Decision Calibration for Nonlinear Loss Functions](https://arxiv.org/abs/2504.15615)
*Jingwu Tang,Jiayun Wu,Zhiwei Steven Wu,Jiahao Zhang*

Main category: cs.LG

TL;DR: 论文探讨了在高维预测空间中如何通过决策校准（decision calibration）确保简单最佳响应规则的优化性，并提出了一种平滑决策校准方法以解决非线性损失问题。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决在高维预测空间中校准和决策校准的计算和统计复杂度问题，特别是针对非线性损失函数的情况。

Method: 论文提出了一种平滑决策校准方法，通过平滑最佳响应实现维度无关的决策校准算法，并给出了高效的后处理算法。

Result: 结果表明，平滑决策校准可以在样本复杂度与特征维度无关的情况下实现，且算法适用于广泛的可分离RKHS函数类。

Conclusion: 结论是平滑决策校准为解决高维和非线性损失问题提供了一种有效方法，且算法具有广泛适用性。

Abstract: When model predictions inform downstream decision making, a natural question
is under what conditions can the decision-makers simply respond to the
predictions as if they were the true outcomes. Calibration suffices to
guarantee that simple best-response to predictions is optimal. However,
calibration for high-dimensional prediction outcome spaces requires exponential
computational and statistical complexity. The recent relaxation known as
decision calibration ensures the optimality of the simple best-response rule
while requiring only polynomial sample complexity in the dimension of outcomes.
However, known results on calibration and decision calibration crucially rely
on linear loss functions for establishing best-response optimality. A natural
approach to handle nonlinear losses is to map outcomes $y$ into a feature space
$\phi(y)$ of dimension $m$, then approximate losses with linear functions of
$\phi(y)$. Unfortunately, even simple classes of nonlinear functions can demand
exponentially large or infinite feature dimensions $m$. A key open problem is
whether it is possible to achieve decision calibration with sample complexity
independent of~$m$. We begin with a negative result: even verifying decision
calibration under standard deterministic best response inherently requires
sample complexity polynomial in~$m$. Motivated by this lower bound, we
investigate a smooth version of decision calibration in which decision-makers
follow a smooth best-response. This smooth relaxation enables dimension-free
decision calibration algorithms. We introduce algorithms that, given
$\mathrm{poly}(|A|,1/\epsilon)$ samples and any initial predictor~$p$, can
efficiently post-process it to satisfy decision calibration without worsening
accuracy. Our algorithms apply broadly to function classes that can be
well-approximated by bounded-norm functions in (possibly infinite-dimensional)
separable RKHS.

</details>


### [86] [SocialMOIF: Multi-Order Intention Fusion for Pedestrian Trajectory Prediction](https://arxiv.org/abs/2504.15616)
*Kai Chen,Xiaodong Zhao,Yujie Huang,Guoyu Fang,Xiao Song,Ruiping Wang,Ziyuan Wang*

Main category: cs.LG

TL;DR: SocialMOIF提出了一种多阶意图融合模型，用于解决智能系统中轨迹预测的高不确定性和复杂高阶交互问题。


<details>
  <summary>Details</summary>
Motivation: 当前研究中，由于代理意图的高不确定性和邻近群体的复杂高阶影响，轨迹预测存在显著局限性。

Method: SocialMOIF通过多阶意图融合模型整合直接和间接意图信息，设计了轨迹分布近似器和全局轨迹优化器，并引入新的损失函数。

Result: 实验表明，该模型在动态和静态数据集上均优于现有基线方法。

Conclusion: SocialMOIF通过多阶意图融合和优化方法，显著提升了轨迹预测的准确性和效率。

Abstract: The analysis and prediction of agent trajectories are crucial for
decision-making processes in intelligent systems, with precise short-term
trajectory forecasting being highly significant across a range of applications.
Agents and their social interactions have been quantified and modeled by
researchers from various perspectives; however, substantial limitations exist
in the current work due to the inherent high uncertainty of agent intentions
and the complex higher-order influences among neighboring groups. SocialMOIF is
proposed to tackle these challenges, concentrating on the higher-order
intention interactions among neighboring groups while reinforcing the primary
role of first-order intention interactions between neighbors and the target
agent. This method develops a multi-order intention fusion model to achieve a
more comprehensive understanding of both direct and indirect intention
information. Within SocialMOIF, a trajectory distribution approximator is
designed to guide the trajectories toward values that align more closely with
the actual data, thereby enhancing model interpretability. Furthermore, a
global trajectory optimizer is introduced to enable more accurate and efficient
parallel predictions. By incorporating a novel loss function that accounts for
distance and direction during training, experimental results demonstrate that
the model outperforms previous state-of-the-art baselines across multiple
metrics in both dynamic and static datasets.

</details>


### [87] [RadioDiff-$k^2$: Helmholtz Equation Informed Generative Diffusion Model for Multi-Path Aware Radio Map Construction](https://arxiv.org/abs/2504.15623)
*Xiucheng Wang,Qiming Zhang,Nan Cheng,Ruijin Sun,Zan Li,Shuguang Cui,Xuemin Shen*

Main category: cs.LG

TL;DR: 提出了一种基于物理信息的生成学习方法RadioDiff-$m{k^2}$，用于高效构建多路径感知的无线电地图（RM）。该方法结合了数据驱动的高效性和物理模型的准确性，显著提升了复杂传播环境中的RM精度。


<details>
  <summary>Details</summary>
Motivation: 随着无线通信向环境感知范式发展，6G网络对智能和主动优化的需求增加，准确构建RM变得至关重要。传统电磁（EM）方法计算开销大，动态场景适应性差；现有神经网络方法缺乏对EM波传播物理特性的考虑。

Method: 提出了一种基于亥姆霍兹方程的物理启发RM构建方法，设计了一个双生成扩散模型（DM）框架：一个DM推断EM奇点，另一个DM利用奇点和环境信息重建完整RM。

Result: 该方法显著提升了RM在复杂多路径环境中的精度，结合了数据驱动的高效性和物理模型的严谨性。

Conclusion: RadioDiff-$m{k^2}$通过结合物理模型和数据驱动方法，为复杂传播环境中的RM构建提供了一种高效且准确的解决方案。

Abstract: In this paper, we propose a novel physics-informed generative learning
approach, termed RadioDiff-$\bm{k^2}$, for accurate and efficient
multipath-aware radio map (RM) construction. As wireless communication evolves
towards environment-aware paradigms, driven by the increasing demand for
intelligent and proactive optimization in sixth-generation (6G) networks,
accurate construction of RMs becomes crucial yet highly challenging.
Conventional electromagnetic (EM)-based methods, such as full-wave solvers and
ray-tracing approaches, exhibit substantial computational overhead and limited
adaptability to dynamic scenarios. Although, existing neural network (NN)
approaches have efficient inferencing speed, they lack sufficient consideration
of the underlying physics of EM wave propagation, limiting their effectiveness
in accurately modeling critical EM singularities induced by complex multipath
environments. To address these fundamental limitations, we propose a novel
physics-inspired RM construction method guided explicitly by the Helmholtz
equation, which inherently governs EM wave propagation. Specifically, we
theoretically establish a direct correspondence between EM singularities, which
correspond to the critical spatial features influencing wireless propagation,
and regions defined by negative wave numbers in the Helmholtz equation. Based
on this insight, we design an innovative dual generative diffusion model (DM)
framework comprising one DM dedicated to accurately inferring EM singularities
and another DM responsible for reconstructing the complete RM using these
singularities along with environmental contextual information. Our
physics-informed approach uniquely combines the efficiency advantages of
data-driven methods with rigorous physics-based EM modeling, significantly
enhancing RM accuracy, particularly in complex propagation environments
dominated by multipath effects.

</details>


### [88] [Enhancing Reinforcement learning in 3-Dimensional Hydrophobic-Polar Protein Folding Model with Attention-based layers](https://arxiv.org/abs/2504.15634)
*Peizheng Liu,Hitoshi Iba*

Main category: cs.LG

TL;DR: 本文提出了一种结合Transformer和DQN的方法，用于解决3D H-P蛋白折叠问题，通过强化学习框架和注意力机制优化折叠决策。


<details>
  <summary>Details</summary>
Motivation: Transformer架构在序列建模中表现优异，但在H-P蛋白折叠模型中的应用尚未充分探索。本文旨在填补这一空白。

Method: 采用DQN结合Transformer注意力机制，设计自避行走的强化学习框架，并引入对称性约束、双Q学习和优先回放等技术优化性能。

Result: 实验表明，该方法在短序列上达到了已知最佳解，长链上也获得了接近最优的结果。

Conclusion: 研究证明了基于注意力的强化学习在蛋白折叠中的潜力，并构建了Transformer-Q网络的原型。

Abstract: Transformer-based architectures have recently propelled advances in sequence
modeling across domains, but their application to the hydrophobic-hydrophilic
(H-P) model for protein folding remains relatively unexplored. In this work, we
adapt a Deep Q-Network (DQN) integrated with attention mechanisms
(Transformers) to address the 3D H-P protein folding problem. Our system
formulates folding decisions as a self-avoiding walk in a reinforced
environment, and employs a specialized reward function based on favorable
hydrophobic interactions. To improve performance, the method incorporates
validity check including symmetry-breaking constraints, dueling and double
Q-learning, and prioritized replay to focus learning on critical transitions.
Experimental evaluations on standard benchmark sequences demonstrate that our
approach achieves several known best solutions for shorter sequences, and
obtains near-optimal results for longer chains. This study underscores the
promise of attention-based reinforcement learning for protein folding, and
created a prototype of Transformer-based Q-network structure for 3-dimensional
lattice models.

</details>


### [89] [An XAI-based Analysis of Shortcut Learning in Neural Networks](https://arxiv.org/abs/2504.15664)
*Phuong Quynh Le,Jörg Schlötterer,Christin Seifert*

Main category: cs.LG

TL;DR: 论文提出了一种神经元虚假分数，用于量化神经元对虚假特征的依赖，并分析了CNN和ViT中虚假特征的解耦程度。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型容易学习虚假特征，现有方法在某些情况下无法有效缓解这一问题。

Method: 引入神经元虚假分数，结合XAI方法分析CNN和ViT中虚假特征的编码方式。

Result: 虚假特征在不同模型架构中部分解耦，现有缓解方法的假设不完全。

Conclusion: 研究为开发新方法缓解虚假相关性提供了基础，提升AI模型的安全性。

Abstract: Machine learning models tend to learn spurious features - features that
strongly correlate with target labels but are not causal. Existing approaches
to mitigate models' dependence on spurious features work in some cases, but
fail in others. In this paper, we systematically analyze how and where neural
networks encode spurious correlations. We introduce the neuron spurious score,
an XAI-based diagnostic measure to quantify a neuron's dependence on spurious
features. We analyze both convolutional neural networks (CNNs) and vision
transformers (ViTs) using architecture-specific methods. Our results show that
spurious features are partially disentangled, but the degree of disentanglement
varies across model architectures. Furthermore, we find that the assumptions
behind existing mitigation methods are incomplete. Our results lay the
groundwork for the development of novel methods to mitigate spurious
correlations and make AI models safer to use in practice.

</details>


### [90] [Invariant Learning with Annotation-free Environments](https://arxiv.org/abs/2504.15686)
*Phuong Quynh Le,Christin Seifert,Jörg Schlötterer*

Main category: cs.LG

TL;DR: 无监督推断环境的方法在领域泛化中表现良好，性能接近需要显式环境标签的方法。


<details>
  <summary>Details</summary>
Motivation: 传统不变性学习方法依赖预划分的环境标签，而本研究提出无需额外标注即可推断环境的方法。

Method: 利用ERM模型的表示空间特性推断环境，无需显式环境标签。

Result: 在ColoredMNIST基准测试中表现优异，性能接近需要环境标签的方法。

Conclusion: 无监督环境推断方法在领域泛化中具有潜力，且无需额外标注。

Abstract: Invariant learning is a promising approach to improve domain generalization
compared to Empirical Risk Minimization (ERM). However, most invariant learning
methods rely on the assumption that training examples are pre-partitioned into
different known environments. We instead infer environments without the need
for additional annotations, motivated by observations of the properties within
the representation space of a trained ERM model. We show the preliminary
effectiveness of our approach on the ColoredMNIST benchmark, achieving
performance comparable to methods requiring explicit environment labels and on
par with an annotation-free method that poses strong restrictions on the ERM
reference model.

</details>


### [91] [Riemannian Neural Geodesic Interpolant](https://arxiv.org/abs/2504.15736)
*Jiawen Wu,Bingguang Chen,Yuyi Zhou,Qi Meng,Rongchan Zhu,Zhi-Ming Ma*

Main category: cs.LG

TL;DR: 论文提出了一种名为Riemannian Neural Geodesic Interpolant (RNGI)的模型，用于在黎曼流形上插值两个概率密度，并通过Embedding Stochastic Differential Equation (E-SDE)算法改进采样质量。


<details>
  <summary>Details</summary>
Motivation: 现有随机插值模型局限于欧几里得空间，无法直接应用于黎曼流形上的分布学习问题。

Method: RNGI模型在黎曼流形上沿随机测地线插值概率密度，并通过E-SDE算法优化采样过程。

Result: 实验证明RNGI和E-SDE在S2和SO(3)流形上的合成和实际分布中表现优异。

Conclusion: RNGI和E-SDE为黎曼流形上的生成模型提供了高效且灵活的解决方案。

Abstract: Stochastic interpolants are efficient generative models that bridge two
arbitrary probability density functions in finite time, enabling flexible
generation from the source to the target distribution or vice versa. These
models are primarily developed in Euclidean space, and are therefore limited in
their application to many distribution learning problems defined on Riemannian
manifolds in real-world scenarios. In this work, we introduce the Riemannian
Neural Geodesic Interpolant (RNGI) model, which interpolates between two
probability densities on a Riemannian manifold along the stochastic geodesics,
and then samples from one endpoint as the final state using the continuous flow
originating from the other endpoint. We prove that the temporal marginal
density of RNGI solves a transport equation on the Riemannian manifold. After
training the model's the neural velocity and score fields, we propose the
Embedding Stochastic Differential Equation (E-SDE) algorithm for stochastic
sampling of RNGI. E-SDE significantly improves the sampling quality by reducing
the accumulated error caused by the excessive intrinsic discretization of
Riemannian Brownian motion in the classical Geodesic Random Walk (GRW)
algorithm. We also provide theoretical bounds on the generative bias measured
in terms of KL-divergence. Finally, we demonstrate the effectiveness of the
proposed RNGI and E-SDE through experiments conducted on both collected and
synthetic distributions on S2 and SO(3).

</details>


### [92] [Observability conditions for neural state-space models with eigenvalues and their roots of unity](https://arxiv.org/abs/2504.15758)
*Andrew Gracyk*

Main category: cs.LG

TL;DR: 论文通过微分方程和控制理论研究神经状态空间模型和Mamba架构的可观测性，提出高效的计算方法，并基于经典控制理论讨论其计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 研究神经状态空间模型和Mamba架构的可观测性，为学习场景提供定制化的可观测性策略。

Method: 利用特征值、单位根、傅里叶变换和Vandermonde矩阵等方法，设计高效的计算策略。

Result: 提出五种非平凡结果，包括基于傅里叶变换的高概率可观测性方法、Mamba的类Hautus条件，以及共享参数的高效计算构造。

Conclusion: 论文提出的方法在计算效率和可观测性方面取得显著成果，并开发了一种满足Robbins-Monro条件的训练算法。

Abstract: We operate through the lens of ordinary differential equations and control
theory to study the concept of observability in the context of neural
state-space models and the Mamba architecture. We develop strategies to enforce
observability, which are tailored to a learning context, specifically where the
hidden states are learnable at initial time, in conjunction to over its
continuum, and high-dimensional. We also highlight our methods emphasize
eigenvalues, roots of unity, or both. Our methods effectuate computational
efficiency when enforcing observability, sometimes at great scale. We formulate
observability conditions in machine learning based on classical control theory
and discuss their computational complexity. Our nontrivial results are
fivefold. We discuss observability through the use of permutations in neural
applications with learnable matrices without high precision. We present two
results built upon the Fourier transform that effect observability with high
probability up to the randomness in the learning. These results are worked with
the interplay of representations in Fourier space and their eigenstructure,
nonlinear mappings, and the observability matrix. We present a result for Mamba
that is similar to a Hautus-type condition, but instead employs an argument
using a Vandermonde matrix instead of eigenvectors. Our final result is a
shared-parameter construction of the Mamba system, which is computationally
efficient in high exponentiation. We develop a training algorithm with this
coupling, showing it satisfies a Robbins-Monro condition under certain
orthogonality, while a more classical training procedure fails to satisfy a
contraction with high Lipschitz constant.

</details>


### [93] [Grounded in Context: Retrieval-Based Method for Hallucination Detection](https://arxiv.org/abs/2504.15771)
*Assaf Gerner,Netta Madvil,Nadav Barak,Alex Zaikman,Jonatan Liberman,Liron Hamra,Rotem Brazilay,Shay Tsadok,Yaron Friedman,Neal Harow,Noam Bresler,Shir Chorev,Philip Tannor*

Main category: cs.LG

TL;DR: Deepchecks提出了一种名为“Grounded in Context”的幻觉检测框架，用于生产级长上下文数据，结合检索和NLI模型，在RAGTruth任务中F1得分为0.83。


<details>
  <summary>Details</summary>
Motivation: 尽管内容生成技术有所进步，但大型语言模型（LLMs）在应用中仍存在幻觉答案问题，需要有效的检测方法。

Method: 框架结合检索和自然语言推理（NLI）模型，使用基于编码器的模型（512-token上下文窗口）预测前提与假设的事实一致性。

Result: 在RAGTruth的响应级分类任务中，F1得分为0.83，优于同类框架。

Conclusion: 该框架能有效检测幻觉答案，适用于多种用例，性能优越。

Abstract: Despite advancements in grounded content generation, production Large
Language Models (LLMs) based applications still suffer from hallucinated
answers. We present "Grounded in Context" - Deepchecks' hallucination detection
framework, designed for production-scale long-context data and tailored to
diverse use cases, including summarization, data extraction, and RAG. Inspired
by RAG architecture, our method integrates retrieval and Natural Language
Inference (NLI) models to predict factual consistency between premises and
hypotheses using an encoder-based model with only a 512-token context window.
Our framework identifies unsupported claims with an F1 score of 0.83 in
RAGTruth's response-level classification task, matching methods that trained on
the dataset, and outperforming all comparable frameworks using similar-sized
models.

</details>


### [94] [Clifford Group Equivariant Diffusion Models for 3D Molecular Generation](https://arxiv.org/abs/2504.15773)
*Cong Liu,Sharvaree Vadgama,David Ruhe,Erik Bekkers,Patrick Forrè*

Main category: cs.LG

TL;DR: 利用Clifford代数的表达能力，提出Clifford扩散模型（CDMs），通过几何积和高阶多向量子空间扩展扩散过程，用于分子生成。


<details>
  <summary>Details</summary>
Motivation: 探索Clifford代数在E(n)-等变扩散模型中的潜力，以捕捉更丰富的几何信息。

Method: 利用Clifford多向量的几何积和子空间信息，将扩散过程扩展到高阶多向量子空间，实现跨子空间的联合分布建模。

Result: 在QM9数据集上的无条件分子生成实验表明，CDMs在生成建模中具有潜力。

Conclusion: CDMs通过高阶几何特征为生成建模提供了新途径。

Abstract: This paper explores leveraging the Clifford algebra's expressive power for
$\E(n)$-equivariant diffusion models. We utilize the geometric products between
Clifford multivectors and the rich geometric information encoded in Clifford
subspaces in \emph{Clifford Diffusion Models} (CDMs). We extend the diffusion
process beyond just Clifford one-vectors to incorporate all higher-grade
multivector subspaces. The data is embedded in grade-$k$ subspaces, allowing us
to apply latent diffusion across complete multivectors. This enables CDMs to
capture the joint distribution across different subspaces of the algebra,
incorporating richer geometric information through higher-order features. We
provide empirical results for unconditional molecular generation on the QM9
dataset, showing that CDMs provide a promising avenue for generative modeling.

</details>


### [95] [DAE-KAN: A Kolmogorov-Arnold Network Model for High-Index Differential-Algebraic Equations](https://arxiv.org/abs/2504.15806)
*Kai Luo,Juan Tang,Mingchao Cai,Xiaoqing Zeng,Manqi Xie,Ming Yan*

Main category: cs.LG

TL;DR: DAE-KAN结合KANs和PINNs，显著提升高指数微分代数方程求解精度，误差降低1-2个数量级。


<details>
  <summary>Details</summary>
Motivation: 传统PINNs在解决高指数微分代数方程（DAEs）时性能有限，需结合KANs的强函数拟合能力以提升精度。

Method: 提出DAE-KAN框架，将KANs与PINNs结合，利用KANs的优势增强模型性能。

Result: 实验表明，DAE-KAN在指数1至3的DAE系统中，误差比传统PINNs低1-2个数量级。

Conclusion: DAE-KAN为高指数DAEs提供高精度和泛化能力的解决方案，具有广泛应用潜力。

Abstract: Kolmogorov-Arnold Networks (KANs) have emerged as a promising alternative to
Multi-Layer Perceptrons (MLPs) due to their superior function-fitting abilities
in data-driven modeling. In this paper, we propose a novel framework, DAE-KAN,
for solving high-index differential-algebraic equations (DAEs) by integrating
KANs with Physics-Informed Neural Networks (PINNs). This framework not only
preserves the ability of traditional PINNs to model complex systems governed by
physical laws but also enhances their performance by leveraging the
function-fitting strengths of KANs. Numerical experiments demonstrate that for
DAE systems ranging from index-1 to index-3, DAE-KAN reduces the absolute
errors of both differential and algebraic variables by 1 to 2 orders of
magnitude compared to traditional PINNs. To assess the effectiveness of this
approach, we analyze the drift-off error and find that both PINNs and DAE-KAN
outperform classical numerical methods in controlling this phenomenon. Our
results highlight the potential of neural network methods, particularly
DAE-KAN, in solving high-index DAEs with substantial computational accuracy and
generalization, offering a promising solution for challenging partial
differential-algebraic equations.

</details>


### [96] [Fusing Reward and Dueling Feedback in Stochastic Bandits](https://arxiv.org/abs/2504.15812)
*Xuchuang Wang,Qirun Zeng,Jinhang Zuo,Xutong Liu,Mohammad Hajiesmaili,John C. S. Lui,Adam Wierman*

Main category: cs.LG

TL;DR: 研究了在随机多臂老虎机问题中融合绝对（奖励）和相对（对决）反馈的方法，提出了两种融合算法并分析了其遗憾下界。


<details>
  <summary>Details</summary>
Motivation: 探索如何有效结合两种反馈类型以提升算法性能，减少遗憾。

Method: 提出了两种融合算法：消除融合算法和分解融合算法，分别通过共享候选臂集和动态选择反馈类型来优化探索与利用。

Result: 分解融合算法的遗憾与理论下界匹配，而消除融合算法由于对决消除的固有次优性存在额外遗憾。

Conclusion: 实验验证了算法的有效性，分解融合算法在常见假设下表现最优。

Abstract: This paper investigates the fusion of absolute (reward) and relative
(dueling) feedback in stochastic bandits, where both feedback types are
gathered in each decision round. We derive a regret lower bound, demonstrating
that an efficient algorithm may incur only the smaller among the reward and
dueling-based regret for each individual arm. We propose two fusion approaches:
(1) a simple elimination fusion algorithm that leverages both feedback types to
explore all arms and unifies collected information by sharing a common
candidate arm set, and (2) a decomposition fusion algorithm that selects the
more effective feedback to explore the corresponding arms and randomly assigns
one feedback type for exploration and the other for exploitation in each round.
The elimination fusion experiences a suboptimal multiplicative term of the
number of arms in regret due to the intrinsic suboptimality of dueling
elimination. In contrast, the decomposition fusion achieves regret matching the
lower bound up to a constant under a common assumption. Extensive experiments
confirm the efficacy of our algorithms and theoretical results.

</details>


### [97] [DualOptim: Enhancing Efficacy and Stability in Machine Unlearning with Dual Optimizers](https://arxiv.org/abs/2504.15827)
*Xuyang Zhong,Haochen Luo,Chen Liu*

Main category: cs.LG

TL;DR: 提出DualOptim方法，解决现有机器去学习（MU）方法对超参数敏感的问题，提升去学习的稳定性和效果。


<details>
  <summary>Details</summary>
Motivation: 现有MU方法对超参数敏感，需精细调参，限制了实际应用。

Method: 提出DualOptim，结合自适应学习率和解耦动量因子。

Result: 实验证明DualOptim显著提升MU效果和稳定性，适用于多种任务。

Conclusion: DualOptim是一种通用方法，可增强现有MU算法。

Abstract: Existing machine unlearning (MU) approaches exhibit significant sensitivity
to hyperparameters, requiring meticulous tuning that limits practical
deployment. In this work, we first empirically demonstrate the instability and
suboptimal performance of existing popular MU methods when deployed in
different scenarios. To address this issue, we propose Dual Optimizer
(DualOptim), which incorporates adaptive learning rate and decoupled momentum
factors. Empirical and theoretical evidence demonstrates that DualOptim
contributes to effective and stable unlearning. Through extensive experiments,
we show that DualOptim can significantly boost MU efficacy and stability across
diverse tasks, including image classification, image generation, and large
language models, making it a versatile approach to empower existing MU
algorithms.

</details>


### [98] [Adaptive PCA-Based Outlier Detection for Multi-Feature Time Series in Space Missions](https://arxiv.org/abs/2504.15846)
*Jonah Ekelund,Savvas Raptis,Vicki Toy-Edens,Wenli Mo,Drew L. Turner,Ian J. Cohen,Stefano Markidis*

Main category: cs.LG

TL;DR: 提出了一种基于PCA重建误差的自适应异常检测算法，适用于空间任务中的实时多特征时间序列数据分析。


<details>
  <summary>Details</summary>
Motivation: 空间任务中有限的计算资源和数据下行链路限制需要实时识别感兴趣区域的鲁棒方法。

Method: 使用增量PCA动态适应数据分布变化，结合预缩放过程归一化特征幅度。

Result: 在NASA的MMS和THEMIS任务中成功检测到空间等离子体事件和日侧瞬态现象。

Conclusion: 该算法适用于空间任务中的实时异常检测，无需预定义模型。

Abstract: Analyzing multi-featured time series data is critical for space missions
making efficient event detection, potentially onboard, essential for automatic
analysis. However, limited onboard computational resources and data downlink
constraints necessitate robust methods for identifying regions of interest in
real time. This work presents an adaptive outlier detection algorithm based on
the reconstruction error of Principal Component Analysis (PCA) for feature
reduction, designed explicitly for space mission applications. The algorithm
adapts dynamically to evolving data distributions by using Incremental PCA,
enabling deployment without a predefined model for all possible conditions. A
pre-scaling process normalizes each feature's magnitude while preserving
relative variance within feature types. We demonstrate the algorithm's
effectiveness in detecting space plasma events, such as distinct space
environments, dayside and nightside transients phenomena, and transition layers
through NASA's MMS mission observations. Additionally, we apply the method to
NASA's THEMIS data, successfully identifying a dayside transient using
onboard-available measurements.

</details>


### [99] [Consistent Causal Inference of Group Effects in Non-Targeted Trials with Finitely Many Effect Levels](https://arxiv.org/abs/2504.15854)
*Georgios Mavroudeas,Malik Magdon-Ismail,Kristin P. Bennett,Jason Kuruzovich*

Main category: cs.LG

TL;DR: 论文提出了一种非参数方法PCM，用于估计治疗对不同群体的异质性效果，解决了传统试验中效果混杂的问题。


<details>
  <summary>Details</summary>
Motivation: 在非目标试验中，治疗可能对某些群体有益，但对其他群体有害，导致治疗效果混杂，难以准确估计对特定群体的影响。

Method: 提出PCM（预聚类和合并）方法，通过非参数方式估计群体效应，适用于有限范围函数的估计。

Result: 在合成数据上，PCM比现有方法提高了10倍以上的准确性，并证明了其在大范围设置中的渐近一致性。

Conclusion: PCM是一种高效且通用的方法，适用于解决治疗效果混杂问题，并能推广到其他有限范围函数的估计。

Abstract: A treatment may be appropriate for some group (the ``sick" group) on whom it
has a positive effect, but it can also have a detrimental effect on subjects
from another group (the ``healthy" group). In a non-targeted trial both sick
and healthy subjects may be treated, producing heterogeneous effects within the
treated group. Inferring the correct treatment effect on the sick population is
then difficult, because the effects on the different groups get tangled. We
propose an efficient nonparametric approach to estimating the group effects,
called {\bf PCM} (pre-cluster and merge). We prove its asymptotic consistency
in a general setting and show, on synthetic data, more than a 10x improvement
in accuracy over existing state-of-the-art. Our approach applies more generally
to consistent estimation of functions with a finite range.

</details>


### [100] [SUPRA: Subspace Parameterized Attention for Neural Operator on General Domains](https://arxiv.org/abs/2504.15897)
*Zherui Yang,Zhengyang Xue,Ligang Liu*

Main category: cs.LG

TL;DR: SUPRA神经算子通过将注意力机制推广到函数空间，并利用拉普拉斯特征函数在有限维子空间中近似，解决了传统神经算子在计算效率和几何适应性上的问题，显著提升了PDE求解的精度。


<details>
  <summary>Details</summary>
Motivation: 传统神经算子在处理大规模网格时计算效率低，且谱卷积在非规则域上精度下降，需要一种更高效且适应性强的方法。

Method: 将标准注意力机制推广为函数空间中的双线性形式和线性算子，提出SUPRA神经算子，利用拉普拉斯特征函数构造有限维子空间。

Result: SUPRA神经算子在多种PDE数据集上误差率降低达33%，同时保持计算效率。

Conclusion: SUPRA神经算子通过函数空间注意力机制和几何自适应的子空间构造，显著提升了PDE求解的精度和效率。

Abstract: Neural operators are efficient surrogate models for solving partial
differential equations (PDEs), but their key components face challenges: (1) in
order to improve accuracy, attention mechanisms suffer from computational
inefficiency on large-scale meshes, and (2) spectral convolutions rely on the
Fast Fourier Transform (FFT) on regular grids and assume a flat geometry, which
causes accuracy degradation on irregular domains. To tackle these problems, we
regard the matrix-vector operations in the standard attention mechanism on
vectors in Euclidean space as bilinear forms and linear operators in vector
spaces and generalize the attention mechanism to function spaces. This new
attention mechanism is fully equivalent to the standard attention but
impossible to compute due to the infinite dimensionality of function spaces. To
address this, inspired by model reduction techniques, we propose a Subspace
Parameterized Attention (SUPRA) neural operator, which approximates the
attention mechanism within a finite-dimensional subspace. To construct a
subspace on irregular domains for SUPRA, we propose using the Laplacian
eigenfunctions, which naturally adapt to domains' geometry and guarantee the
optimal approximation for smooth functions. Experiments show that the SUPRA
neural operator reduces error rates by up to 33% on various PDE datasets while
maintaining state-of-the-art computational efficiency.

</details>


### [101] [GraphEdge: Dynamic Graph Partition and Task Scheduling for GNNs Computing in Edge Network](https://arxiv.org/abs/2504.15905)
*Wenjing Xiao,Chenglong Shi,Miaojiang Chen,Zhiquan Liu,Min Chen,H. Herbert Song*

Main category: cs.LG

TL;DR: GraphEdge提出了一种高效的基于GNN的边缘计算架构，通过优化图布局和任务卸载策略，降低通信成本并提升性能。


<details>
  <summary>Details</summary>
Motivation: 随着物联网设备的激增，边缘计算在提供经济高效服务方面日益重要，但现有方法在图结构场景（如交通流量预测和社交关系推荐）中表现不佳，尤其是GNN方法通信成本高。

Method: GraphEdge架构首先感知用户拓扑并将数据关联表示为图布局，通过HiCut算法优化图布局为弱关联子图，再使用DRLGO算法获取基于子图的任务卸载策略，最小化任务处理时间和能耗。

Result: 实验结果表明，GraphEdge架构在动态场景中表现出色，具有高效性和动态适应性。

Conclusion: GraphEdge通过优化图布局和任务卸载策略，显著降低了GNN在边缘计算中的通信成本，同时提升了系统性能。

Abstract: With the exponential growth of Internet of Things (IoT) devices, edge
computing (EC) is gradually playing an important role in providing
cost-effective services. However, existing approaches struggle to perform well
in graph-structured scenarios where user data is correlated, such as traffic
flow prediction and social relationship recommender systems. In particular,
graph neural network (GNN)-based approaches lead to expensive server
communication cost. To address this problem, we propose GraphEdge, an efficient
GNN-based EC architecture. It considers the EC system of GNN tasks, where there
are associations between users and it needs to take into account the task data
of its neighbors when processing the tasks of a user. Specifically, the
architecture first perceives the user topology and represents their data
associations as a graph layout at each time step. Then the graph layout is
optimized by calling our proposed hierarchical traversal graph cut algorithm
(HiCut), which cuts the graph layout into multiple weakly associated subgraphs
based on the aggregation characteristics of GNN, and the communication cost
between different subgraphs during GNN inference is minimized. Finally, based
on the optimized graph layout, our proposed deep reinforcement learning (DRL)
based graph offloading algorithm (DRLGO) is executed to obtain the optimal
offloading strategy for the tasks of users, the offloading strategy is
subgraph-based, it tries to offload user tasks in a subgraph to the same edge
server as possible while minimizing the task processing time and energy
consumption of the EC system. Experimental results show the good effectiveness
and dynamic adaptation of our proposed architecture and it also performs well
even in dynamic scenarios.

</details>


### [102] [ScaleGNN: Towards Scalable Graph Neural Networks via Adaptive High-order Neighboring Feature Fusion](https://arxiv.org/abs/2504.15920)
*Xiang Li,Haobing Liu,Jianpeng Qi,Yuan Cao,Guoqing Chao,Yanwei Yu*

Main category: cs.LG

TL;DR: ScaleGNN框架通过自适应融合多级图特征，解决了GNN中的过平滑和可扩展性问题，提高了准确性和计算效率。


<details>
  <summary>Details</summary>
Motivation: GNN在大型图上面临过平滑和可扩展性挑战，需要一种新方法来优化信息传递和计算效率。

Method: ScaleGNN通过自适应高阶特征融合模块、高阶冗余特征掩蔽机制和低阶增强特征聚合，选择性融合多级特征。

Result: 实验表明，ScaleGNN在准确性和计算效率上优于现有GNN模型。

Conclusion: ScaleGNN有效解决了GNN的过平滑和可扩展性问题，为大规模图任务提供了高效解决方案。

Abstract: Graph Neural Networks (GNNs) have demonstrated strong performance across
various graph-based tasks by effectively capturing relational information
between nodes. These models rely on iterative message passing to propagate node
features, enabling nodes to aggregate information from their neighbors. Recent
research has significantly improved the message-passing mechanism, enhancing
GNN scalability on large-scale graphs. However, GNNs still face two main
challenges: over-smoothing, where excessive message passing results in
indistinguishable node representations, especially in deep networks
incorporating high-order neighbors; and scalability issues, as traditional
architectures suffer from high model complexity and increased inference time
due to redundant information aggregation. This paper proposes a novel framework
for large-scale graphs named ScaleGNN that simultaneously addresses both
challenges by adaptively fusing multi-level graph features. We first construct
neighbor matrices for each order, learning their relative information through
trainable weights through an adaptive high-order feature fusion module. This
allows the model to selectively emphasize informative high-order neighbors
while reducing unnecessary computational costs. Additionally, we introduce a
High-order redundant feature masking mechanism based on a Local Contribution
Score (LCS), which enables the model to retain only the most relevant neighbors
at each order, preventing redundant information propagation. Furthermore,
low-order enhanced feature aggregation adaptively integrates low-order and
high-order features based on task relevance, ensuring effective capture of both
local and global structural information without excessive complexity. Extensive
experiments on real-world datasets demonstrate that our approach consistently
outperforms state-of-the-art GNN models in both accuracy and computational
efficiency.

</details>


### [103] [Achieving Distributive Justice in Federated Learning via Uncertainty Quantification](https://arxiv.org/abs/2504.15924)
*Alycia Carey,Xintao Wu*

Main category: cs.LG

TL;DR: UDJ-FL框架通过不确定性加权实现多种客户端公平性指标，超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习公平性指标选择随意，缺乏理论依据，难以满足不同伦理需求。

Method: 结合公平资源分配和不确定性加权技术，实现四种分配正义公平性指标。

Result: 实验证明UDJ-FL能实现四种公平性，且性能优于其他方法。

Conclusion: UDJ-FL为联邦学习提供灵活且理论支持的公平性解决方案。

Abstract: Client-level fairness metrics for federated learning are used to ensure that
all clients in a federation either: a) have similar final performance on their
local data distributions (i.e., client parity), or b) obtain final performance
on their local data distributions relative to their contribution to the
federated learning process (i.e., contribution fairness). While a handful of
works that propose either client-parity or contribution-based fairness metrics
ground their definitions and decisions in social theories of equality -- such
as distributive justice -- most works arbitrarily choose what notion of
fairness to align with which makes it difficult for practitioners to choose
which fairness metric aligns best with their fairness ethics. In this work, we
propose UDJ-FL (Uncertainty-based Distributive Justice for Federated Learning),
a flexible federated learning framework that can achieve multiple distributive
justice-based client-level fairness metrics. Namely, by utilizing techniques
inspired by fair resource allocation, in conjunction with performing aleatoric
uncertainty-based client weighing, our UDJ-FL framework is able to achieve
egalitarian, utilitarian, Rawls' difference principle, or desert-based
client-level fairness. We empirically show the ability of UDJ-FL to achieve all
four defined distributive justice-based client-level fairness metrics in
addition to providing fairness equivalent to (or surpassing) other popular fair
federated learning works. Further, we provide justification for why aleatoric
uncertainty weighing is necessary to the construction of our UDJ-FL framework
as well as derive theoretical guarantees for the generalization bounds of
UDJ-FL. Our code is publicly available at
https://github.com/alycia-noel/UDJ-FL.

</details>


### [104] [StreamRL: Scalable, Heterogeneous, and Elastic RL for LLMs with Disaggregated Stream Generation](https://arxiv.org/abs/2504.15930)
*Yinmin Zhong,Zili Zhang,Xiaoniu Song,Hanpeng Hu,Chao Jin,Bingyang Wu,Nuo Chen,Yukun Chen,Yu Zhou,Changyi Wan,Hongyu Zhou,Yimin Jiang,Yibo Zhu,Daxin Jiang*

Main category: cs.LG

TL;DR: StreamRL通过解耦架构解决了传统RL在LLM训练中的资源耦合问题，提升了吞吐量和成本效益。


<details>
  <summary>Details</summary>
Motivation: 传统RL架构中资源耦合限制了可扩展性和成本效率，而解耦架构虽灵活但存在性能瓶颈。

Method: StreamRL通过流生成和异步RL解决管道气泡，通过输出长度排序模型和调度解决偏斜气泡。

Result: 实验显示StreamRL吞吐量提升2.66倍，成本效益提升1.33倍。

Conclusion: StreamRL为大规模RL训练提供高效解耦解决方案。

Abstract: Reinforcement learning (RL) has become the core post-training technique for
large language models (LLMs). RL for LLMs involves two stages: generation and
training. The LLM first generates samples online, which are then used to derive
rewards for training. The conventional view holds that the colocated
architecture, where the two stages share resources via temporal multiplexing,
outperforms the disaggregated architecture, in which dedicated resources are
assigned to each stage. However, in real-world deployments, we observe that the
colocated architecture suffers from resource coupling, where the two stages are
constrained to use the same resources. This coupling compromises the
scalability and cost-efficiency of colocated RL in large-scale training. In
contrast, the disaggregated architecture allows for flexible resource
allocation, supports heterogeneous training setups, and facilitates
cross-datacenter deployment.
  StreamRL is designed with disaggregation from first principles and fully
unlocks its potential by addressing two types of performance bottlenecks in
existing disaggregated RL frameworks: pipeline bubbles, caused by stage
dependencies, and skewness bubbles, resulting from long-tail output length
distributions. To address pipeline bubbles, StreamRL breaks the traditional
stage boundary in synchronous RL algorithms through stream generation and
achieves full overlapping in asynchronous RL. To address skewness bubbles,
StreamRL employs an output-length ranker model to identify long-tail samples
and reduces generation time via skewness-aware dispatching and scheduling.
Experiments show that StreamRL improves throughput by up to 2.66x compared to
existing state-of-the-art systems, and improves cost-effectiveness by up to
1.33x in a heterogeneous, cross-datacenter setting.

</details>


### [105] [Universal Approximation with Softmax Attention](https://arxiv.org/abs/2504.15956)
*Jerry Yao-Chieh Hu,Hude Liu,Hong-Yu Chen,Weimin Wu,Han Liu*

Main category: cs.LG

TL;DR: 论文证明了两层自注意力和一层自注意力加softmax函数在紧凑域上对连续序列到序列函数具有通用逼近能力。


<details>
  <summary>Details</summary>
Motivation: 探索自注意力机制在序列到序列任务中的通用逼近能力，减少对前馈网络的依赖。

Method: 采用基于插值的新方法分析注意力内部机制，证明自注意力可逼近广义ReLU。

Result: 两层多头自注意力即可作为序列到序列通用逼近器，且能逼近多种统计模型。

Conclusion: 自注意力机制独立具备强大的逼近能力，技术方法具有独立研究价值。

Abstract: We prove that with linear transformations, both (i) two-layer self-attention
and (ii) one-layer self-attention followed by a softmax function are universal
approximators for continuous sequence-to-sequence functions on compact domains.
Our main technique is a new interpolation-based method for analyzing
attention's internal mechanism. This leads to our key insight: self-attention
is able to approximate a generalized version of ReLU to arbitrary precision,
and hence subsumes many known universal approximators. Building on these, we
show that two-layer multi-head attention alone suffices as a
sequence-to-sequence universal approximator. In contrast, prior works rely on
feed-forward networks to establish universal approximation in Transformers.
Furthermore, we extend our techniques to show that, (softmax-)attention-only
layers are capable of approximating various statistical models in-context. We
believe these techniques hold independent interest.

</details>


### [106] [OPUS-VFL: Incentivizing Optimal Privacy-Utility Tradeoffs in Vertical Federated Learning](https://arxiv.org/abs/2504.15995)
*Sindhuja Madabushi,Ahmad Faraz Khan,Haider Ali,Jin-Hee Cho*

Main category: cs.LG

TL;DR: OPUS-VFL提出了一种优化的隐私-效用权衡策略，解决了现有VFL系统在激励机制、隐私-效用平衡和资源异构性方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有VFL系统缺乏有效的激励机制，难以平衡隐私与效用，且无法适应资源异构性，阻碍了实际部署。

Method: OPUS-VFL引入隐私感知的激励机制，结合特征重要性评估和自适应差分隐私机制，动态优化噪声水平。

Result: 实验显示OPUS-VFL在效率和鲁棒性上显著优于现有方法，降低攻击成功率，提高重构误差，并为贡献者提供更高激励。

Conclusion: OPUS-VFL是一种安全、公平且高效的VFL解决方案，适用于实际应用。

Abstract: Vertical Federated Learning (VFL) enables organizations with disjoint feature
spaces but shared user bases to collaboratively train models without sharing
raw data. However, existing VFL systems face critical limitations: they often
lack effective incentive mechanisms, struggle to balance privacy-utility
tradeoffs, and fail to accommodate clients with heterogeneous resource
capabilities. These challenges hinder meaningful participation, degrade model
performance, and limit practical deployment. To address these issues, we
propose OPUS-VFL, an Optimal Privacy-Utility tradeoff Strategy for VFL.
OPUS-VFL introduces a novel, privacy-aware incentive mechanism that rewards
clients based on a principled combination of model contribution, privacy
preservation, and resource investment. It employs a lightweight leave-one-out
(LOO) strategy to quantify feature importance per client, and integrates an
adaptive differential privacy mechanism that enables clients to dynamically
calibrate noise levels to optimize their individual utility. Our framework is
designed to be scalable, budget-balanced, and robust to inference and poisoning
attacks. Extensive experiments on benchmark datasets (MNIST, CIFAR-10, and
CIFAR-100) demonstrate that OPUS-VFL significantly outperforms state-of-the-art
VFL baselines in both efficiency and robustness. It reduces label inference
attack success rates by up to 20%, increases feature inference reconstruction
error (MSE) by over 30%, and achieves up to 25% higher incentives for clients
that contribute meaningfully while respecting privacy and cost constraints.
These results highlight the practicality and innovation of OPUS-VFL as a
secure, fair, and performance-driven solution for real-world VFL.

</details>


### [107] [AlphaGrad: Non-Linear Gradient Normalization Optimizer](https://arxiv.org/abs/2504.16020)
*Soham Sane*

Main category: cs.LG

TL;DR: AlphaGrad是一种内存高效、条件无状态的优化器，通过梯度归一化和双曲正切变换解决自适应方法（如Adam）的内存和超参数复杂性问题。


<details>
  <summary>Details</summary>
Motivation: 解决自适应优化器（如Adam）的内存开销和超参数复杂性问题，同时保持性能。

Method: 使用张量级L2梯度归一化和双曲正切变换（$g' = \tanh(\alpha \cdot \tilde{g})$），仅需一个陡度参数$\alpha$。

Result: 在不同RL基准测试（DQN、TD3、PPO）中表现各异：DQN不稳定，TD3需调参但结果竞争，PPO表现显著优于Adam。

Conclusion: AlphaGrad是内存受限场景下的有力替代方案，尤其适用于策略学习，其稳定性和效率优势显著。

Abstract: We introduce AlphaGrad, a memory-efficient, conditionally stateless optimizer
addressing the memory overhead and hyperparameter complexity of adaptive
methods like Adam. AlphaGrad enforces scale invariance via tensor-wise L2
gradient normalization followed by a smooth hyperbolic tangent transformation,
$g' = \tanh(\alpha \cdot \tilde{g})$, controlled by a single steepness
parameter $\alpha$. Our contributions include: (1) the AlphaGrad algorithm
formulation; (2) a formal non-convex convergence analysis guaranteeing
stationarity; (3) extensive empirical evaluation on diverse RL benchmarks (DQN,
TD3, PPO). Compared to Adam, AlphaGrad demonstrates a highly context-dependent
performance profile. While exhibiting instability in off-policy DQN, it
provides enhanced training stability with competitive results in TD3 (requiring
careful $\alpha$ tuning) and achieves substantially superior performance in
on-policy PPO. These results underscore the critical importance of empirical
$\alpha$ selection, revealing strong interactions between the optimizer's
dynamics and the underlying RL algorithm. AlphaGrad presents a compelling
alternative optimizer for memory-constrained scenarios and shows significant
promise for on-policy learning regimes where its stability and efficiency
advantages can be particularly impactful.

</details>


### [108] [LLMs meet Federated Learning for Scalable and Secure IoT Management](https://arxiv.org/abs/2504.16032)
*Yazan Otoum,Arghavan Asad,Amiya Nayak*

Main category: cs.LG

TL;DR: 论文提出了一种基于联邦学习的大语言模型框架（FL-LLM），用于提升物联网系统的智能性，同时确保数据隐私和计算效率。


<details>
  <summary>Details</summary>
Motivation: 传统集中式架构在物联网扩展中面临延迟、隐私和资源消耗等问题，亟需新的解决方案。

Method: 结合生成式物联网模型和梯度感知联邦策略（GSFS），动态优化模型更新，采用混合边缘-云处理架构。

Result: 在IoT-23数据集上测试，该框架提高了模型准确性，降低了延迟，提升了能源效率，优于传统联邦学习方法。

Conclusion: 该框架为大规模物联网生态系统提供了更安全、可扩展和自适应的管理方案。

Abstract: The rapid expansion of IoT ecosystems introduces severe challenges in
scalability, security, and real-time decision-making. Traditional centralized
architectures struggle with latency, privacy concerns, and excessive resource
consumption, making them unsuitable for modern large-scale IoT deployments.
This paper presents a novel Federated Learning-driven Large Language Model
(FL-LLM) framework, designed to enhance IoT system intelligence while ensuring
data privacy and computational efficiency. The framework integrates Generative
IoT (GIoT) models with a Gradient Sensing Federated Strategy (GSFS),
dynamically optimizing model updates based on real-time network conditions. By
leveraging a hybrid edge-cloud processing architecture, our approach balances
intelligence, scalability, and security in distributed IoT environments.
Evaluations on the IoT-23 dataset demonstrate that our framework improves model
accuracy, reduces response latency, and enhances energy efficiency,
outperforming traditional FL techniques (i.e., FedAvg, FedOpt). These findings
highlight the potential of integrating LLM-powered federated learning into
large-scale IoT ecosystems, paving the way for more secure, scalable, and
adaptive IoT management solutions.

</details>


### [109] [Muon Optimizer Accelerates Grokking](https://arxiv.org/abs/2504.16041)
*Amund Tveit,Bjørn Remseth,Arve Skogvold*

Main category: cs.LG

TL;DR: 研究不同优化器对模型延迟泛化现象（grokking）的影响，发现Muon优化器比AdamW显著加速泛化。


<details>
  <summary>Details</summary>
Motivation: 探讨优化器选择如何影响模型从记忆到泛化的过渡。

Method: 在七个数值任务中使用Transformer架构，比较Muon和AdamW优化器及不同softmax激活函数的效果。

Result: Muon优化器将平均泛化时间从153.09轮减少到102.89轮，效果显著。

Conclusion: 优化器选择对模型泛化至关重要，Muon优于AdamW。

Abstract: This paper investigates the impact of different optimizers on the grokking
phenomenon, where models exhibit delayed generalization. We conducted
experiments across seven numerical tasks (primarily modular arithmetic) using a
modern Transformer architecture. The experimental configuration systematically
varied the optimizer (Muon vs. AdamW) and the softmax activation function
(standard softmax, stablemax, and sparsemax) to assess their combined effect on
learning dynamics. Our empirical evaluation reveals that the Muon optimizer,
characterized by its use of spectral norm constraints and second-order
information, significantly accelerates the onset of grokking compared to the
widely used AdamW optimizer. Specifically, Muon reduced the mean grokking epoch
from 153.09 to 102.89 across all configurations, a statistically significant
difference (t = 5.0175, p = 6.33e-08). This suggests that the optimizer choice
plays a crucial role in facilitating the transition from memorization to
generalization.

</details>


### [110] [$π_{0.5}$: a Vision-Language-Action Model with Open-World Generalization](https://arxiv.org/abs/2504.16054)
*Physical Intelligence,Kevin Black,Noah Brown,James Darpinian,Karan Dhabalia,Danny Driess,Adnan Esmail,Michael Equi,Chelsea Finn,Niccolo Fusai,Manuel Y. Galliker,Dibya Ghosh,Lachy Groom,Karol Hausman,Brian Ichter,Szymon Jakubczak,Tim Jones,Liyiming Ke,Devin LeBlanc,Sergey Levine,Adrian Li-Bell,Mohith Mothukuri,Suraj Nair,Karl Pertsch,Allen Z. Ren,Lucy Xiaoyang Shi,Laura Smith,Jost Tobias Springenberg,Kyle Stachowicz,James Tanner,Quan Vuong,Homer Walke,Anna Walling,Haohuan Wang,Lili Yu,Ury Zhilinsky*

Main category: cs.LG

TL;DR: 论文提出了一种基于π₀的π₀.₅模型，通过异构任务协同训练实现广泛泛化，展示了端到端学习机器人系统在新环境中执行复杂任务的能力。


<details>
  <summary>Details</summary>
Motivation: 解决机器人如何在实验室外实际环境中执行复杂任务的问题，探索视觉-语言-动作（VLA）模型的泛化能力。

Method: 采用异构任务协同训练，结合多机器人数据、高级语义预测和网络数据，构建多模态示例（图像、语言指令、物体检测等）。

Result: 实验表明知识转移对泛化至关重要，首次实现端到端学习机器人系统在新家庭中执行长期和灵巧操作任务（如清洁厨房或卧室）。

Conclusion: π₀.₅模型通过协同训练和多模态数据，显著提升了机器人系统的泛化能力和实际任务执行效果。

Abstract: In order for robots to be useful, they must perform practically relevant
tasks in the real world, outside of the lab. While vision-language-action (VLA)
models have demonstrated impressive results for end-to-end robot control, it
remains an open question how far such models can generalize in the wild. We
describe $\pi_{0.5}$, a new model based on $\pi_{0}$ that uses co-training on
heterogeneous tasks to enable broad generalization. $\pi_{0.5}$\ uses data from
multiple robots, high-level semantic prediction, web data, and other sources to
enable broadly generalizable real-world robotic manipulation. Our system uses a
combination of co-training and hybrid multi-modal examples that combine image
observations, language commands, object detections, semantic subtask
prediction, and low-level actions. Our experiments show that this kind of
knowledge transfer is essential for effective generalization, and we
demonstrate for the first time that an end-to-end learning-enabled robotic
system can perform long-horizon and dexterous manipulation skills, such as
cleaning a kitchen or bedroom, in entirely new homes.

</details>


### [111] [LLMs are Greedy Agents: Effects of RL Fine-tuning on Decision-Making Abilities](https://arxiv.org/abs/2504.16078)
*Thomas Schmied,Jörg Bornschein,Jordi Grau-Moya,Markus Wulfmeier,Razvan Pascanu*

Main category: cs.LG

TL;DR: LLM代理在决策中存在探索不足和知行差距问题，通过强化学习微调自我生成的CoT推理，提升决策能力。


<details>
  <summary>Details</summary>
Motivation: 研究LLM在决策中表现不佳的原因，特别是贪婪性、频率偏差和知行差距，并提出改进方法。

Method: 通过强化学习微调LLM的自我生成CoT推理，实验涵盖多臂老虎机、上下文老虎机和井字棋。

Result: 强化学习微调提高了LLM的探索能力并缩小了知行差距。

Conclusion: 结合经典探索机制和LLM特定方法，如自我校正和一致性，可更有效微调LLM用于决策。

Abstract: The success of Large Language Models (LLMs) has sparked interest in various
agentic applications. A key hypothesis is that LLMs, leveraging common sense
and Chain-of-Thought (CoT) reasoning, can effectively explore and efficiently
solve complex domains. However, LLM agents have been found to suffer from
sub-optimal exploration and the knowing-doing gap, the inability to effectively
act on knowledge present in the model. In this work, we systematically study
why LLMs perform sub-optimally in decision-making scenarios. In particular, we
closely examine three prevalent failure modes: greediness, frequency bias, and
the knowing-doing gap. We propose mitigation of these shortcomings by
fine-tuning via Reinforcement Learning (RL) on self-generated CoT rationales.
Our experiments across multi-armed bandits, contextual bandits, and
Tic-tac-toe, demonstrate that RL fine-tuning enhances the decision-making
abilities of LLMs by increasing exploration and narrowing the knowing-doing
gap. Finally, we study both classic exploration mechanisms, such as
$\epsilon$-greedy, and LLM-specific approaches, such as self-correction and
self-consistency, to enable more effective fine-tuning of LLMs for
decision-making.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [112] [Do It For Me vs. Do It With Me: Investigating User Perceptions of Different Paradigms of Automation in Copilots for Feature-Rich Software](https://arxiv.org/abs/2504.15549)
*Anjali Khurana,Xiaotian Su,April Yi Wang,Parmit K Chilana*

Main category: cs.HC

TL;DR: 研究比较了全自动（AutoCopilot）和半自动（GuidedCopilot）助手在用户体验上的差异，发现半自动助手在用户控制、软件实用性和学习性上表现更优，尤其在探索性和创造性任务中。


<details>
  <summary>Details</summary>
Motivation: 探讨如何通过不同自动化水平的助手优化用户体验，特别是在用户倾向于通过实践学习的情境下。

Method: 设计并实现了全自动和半自动助手，通过用户研究（N=20）比较其表现，并进一步优化半自动助手的功能（N=10）。

Result: 半自动助手在用户控制、软件实用性和学习性上优于全自动助手，尤其在探索性和创造性任务中；全自动助手在简单任务中节省时间。

Conclusion: 用户控制和定制化指导是设计下一代助手的关键，需平衡自动化水平以提升生产力并支持不同技能水平的用户。

Abstract: Large Language Model (LLM)-based in-application assistants, or copilots, can
automate software tasks, but users often prefer learning by doing, raising
questions about the optimal level of automation for an effective user
experience. We investigated two automation paradigms by designing and
implementing a fully automated copilot (AutoCopilot) and a semi-automated
copilot (GuidedCopilot) that automates trivial steps while offering
step-by-step visual guidance. In a user study (N=20) across data analysis and
visual design tasks, GuidedCopilot outperformed AutoCopilot in user control,
software utility, and learnability, especially for exploratory and creative
tasks, while AutoCopilot saved time for simpler visual tasks. A follow-up
design exploration (N=10) enhanced GuidedCopilot with task-and state-aware
features, including in-context preview clips and adaptive instructions. Our
findings highlight the critical role of user control and tailored guidance in
designing the next generation of copilots that enhance productivity, support
diverse skill levels, and foster deeper software engagement.

</details>


### [113] [iMedic: Towards Smartphone-based Self-Auscultation Tool for AI-Powered Pediatric Respiratory Assessment](https://arxiv.org/abs/2504.15743)
*Seung Gyu Jeong,Sung Woo Nam,Seong Kwan Jung,Seong-Eun Kim*

Main category: cs.HC

TL;DR: 智能手机结合深度学习算法，通过内置麦克风检测儿童肺炎风险，提供远程医疗支持。


<details>
  <summary>Details</summary>
Motivation: 在医生资源有限的地区，早期检测儿童肺炎具有挑战性，需低成本、高效的解决方案。

Method: 利用智能手机内置麦克风和深度学习框架，整合电子听诊器与手机数据，实现呼吸音异常检测。

Result: 系统分类性能强，用户接受度高，有助于主动干预和减少儿童肺炎死亡。

Conclusion: 智能手机结合深度学习为远程儿科护理提供了公平且全面的解决方案。

Abstract: Respiratory auscultation is crucial for early detection of pediatric
pneumonia, a condition that can quickly worsen without timely intervention. In
areas with limited physician access, effective auscultation is challenging. We
present a smartphone-based system that leverages built-in microphones and
advanced deep learning algorithms to detect abnormal respiratory sounds
indicative of pneumonia risk. Our end-to-end deep learning framework employs
domain generalization to integrate a large electronic stethoscope dataset with
a smaller smartphone-derived dataset, enabling robust feature learning for
accurate respiratory assessments without expensive equipment. The accompanying
mobile application guides caregivers in collecting high-quality lung sound
samples and provides immediate feedback on potential pneumonia risks. User
studies show strong classification performance and high acceptance,
demonstrating the system's ability to facilitate proactive interventions and
reduce preventable childhood pneumonia deaths. By seamlessly integrating into
ubiquitous smartphones, this approach offers a promising avenue for more
equitable and comprehensive remote pediatric care.

</details>


### [114] [Supporting Data-Frame Dynamics in AI-assisted Decision Making](https://arxiv.org/abs/2504.15894)
*Chengbo Zheng,Tim Miller,Alina Bialkowski,H Peter Soyer,Monika Janda*

Main category: cs.HC

TL;DR: 提出了一种基于数据框架理论和评估性AI范式的混合主动框架，支持人类与AI协作构建、验证和调整假设，应用于皮肤癌诊断原型。


<details>
  <summary>Details</summary>
Motivation: 当前AI决策支持系统难以支持动态证据与假设的交互，需改进以支持高风险决策。

Method: 采用混合主动框架，结合数据框架理论和评估性AI范式，利用概念瓶颈模型实现可解释交互和动态假设更新。

Result: 开发了AI辅助皮肤癌诊断原型，展示了框架的有效性。

Conclusion: 该框架为高风险决策提供了动态协作支持，具有实际应用潜力。

Abstract: High stakes decision-making often requires a continuous interplay between
evolving evidence and shifting hypotheses, a dynamic that is not well supported
by current AI decision support systems. In this paper, we introduce a
mixed-initiative framework for AI assisted decision making that is grounded in
the data-frame theory of sensemaking and the evaluative AI paradigm. Our
approach enables both humans and AI to collaboratively construct, validate, and
adapt hypotheses. We demonstrate our framework with an AI-assisted skin cancer
diagnosis prototype that leverages a concept bottleneck model to facilitate
interpretable interactions and dynamic updates to diagnostic hypotheses.

</details>


### [115] [Navigating the State of Cognitive Flow: Context-Aware AI Interventions for Effective Reasoning Support](https://arxiv.org/abs/2504.16021)
*Dinithi Dissanayake,Suranga Nanayakkara*

Main category: cs.HC

TL;DR: 论文提出了一种基于上下文感知的认知增强框架，通过动态调整AI干预以维持或恢复认知流状态，避免干扰决策过程。


<details>
  <summary>Details</summary>
Motivation: 在AI增强推理中，不当的干预可能破坏认知流状态，影响决策效果。因此，需要一种适应性强且非侵入性的干预方法。

Method: 利用多模态行为线索（如注视行为、输入犹豫、交互速度）动态调整干预，基于类型、时机和规模三个关键上下文因素。

Result: 提出认知流概念，扩展了流理论在AI增强推理中的应用，实现了个性化、自适应且非侵入性的干预。

Conclusion: 通过上下文感知的增强方法，AI系统能在复杂决策中支持深度参与，同时避免破坏认知沉浸。

Abstract: Flow theory describes an optimal cognitive state where individuals experience
deep focus and intrinsic motivation when a task's difficulty aligns with their
skill level. In AI-augmented reasoning, interventions that disrupt the state of
cognitive flow can hinder rather than enhance decision-making. This paper
proposes a context-aware cognitive augmentation framework that adapts
interventions based on three key contextual factors: type, timing, and scale.
By leveraging multimodal behavioral cues (e.g., gaze behavior, typing
hesitation, interaction speed), AI can dynamically adjust cognitive support to
maintain or restore flow. We introduce the concept of cognitive flow, an
extension of flow theory in AI-augmented reasoning, where interventions are
personalized, adaptive, and minimally intrusive. By shifting from static
interventions to context-aware augmentation, our approach ensures that AI
systems support deep engagement in complex decision-making and reasoning
without disrupting cognitive immersion.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [116] [Neural Kinematic Bases for Fluids](https://arxiv.org/abs/2504.15657)
*Yibo Liu,Paul Kry,Kenny Erleben,Noam Aigerman,Sune Darkner,Teseo Schneider*

Main category: cs.GR

TL;DR: 提出了一种基于MLP表示的无网格流体模拟方法，利用运动学神经基元确保速度场满足正交性、无散度、边界对齐和平滑性等物理特性。


<details>
  <summary>Details</summary>
Motivation: 传统流体模拟方法计算复杂且难以满足实时动画需求，希望通过神经基元简化模拟过程并保持物理特性。

Method: 设计损失函数确保神经基元满足正交性、无散度、边界对齐和平滑性，并用于拟合输入流场草图。

Result: 实现了实时动画，神经基元适应不同域并自然扩展到三维。

Conclusion: 该方法通过神经基元简化流体模拟，同时保持物理特性，适用于实时动画和多维扩展。

Abstract: We propose mesh-free fluid simulations that exploit a kinematic neural basis
for velocity fields represented by an MLP. We design a set of losses that
ensures that these neural bases satisfy fundamental physical properties such as
orthogonality, divergence-free, boundary alignment, and smoothness. Our neural
bases can then be used to fit an input sketch of a flow, which will inherit the
same fundamental properties from the bases. We then can animate such flow in
real-time using standard time integrators. Our neural bases can accommodate
different domains and naturally extend to three dimensions.

</details>


### [117] [Low-Rank Adaptation of Neural Fields](https://arxiv.org/abs/2504.15933)
*Anh Truong,Ahmed H. Mahmoud,Mina Konaković Luković,Justin Solomon*

Main category: cs.GR

TL;DR: 提出一种基于低秩适应（LoRA）的参数高效策略，用于更新神经场（NF），适用于低计算硬件。


<details>
  <summary>Details</summary>
Motivation: 神经场（NF）的小变化编码问题尚未得到充分研究，现有图形技术虽能高效处理冗余数据，但NF领域缺乏类似方法。

Method: 将LoRA方法从LLM领域引入NF，用于编码预训练模型的小更新，避免依赖大型预训练模型。

Result: 在图像过滤、视频压缩和几何编辑实验中验证了方法的有效性和多功能性。

Conclusion: LoRA适用于神经场更新，提供了一种计算高效的解决方案。

Abstract: Processing visual data often involves small adjustments or sequences of
changes, such as in image filtering, surface smoothing, and video storage.
While established graphics techniques like normal mapping and video compression
exploit redundancy to encode such small changes efficiently, the problem of
encoding small changes to neural fields (NF) -- neural network
parameterizations of visual or physical functions -- has received less
attention.
  We propose a parameter-efficient strategy for updating neural fields using
low-rank adaptations (LoRA). LoRA, a method from the parameter-efficient
fine-tuning LLM community, encodes small updates to pre-trained models with
minimal computational overhead. We adapt LoRA to instance-specific neural
fields, avoiding the need for large pre-trained models yielding a pipeline
suitable for low-compute hardware.
  We validate our approach with experiments in image filtering, video
compression, and geometry editing, demonstrating its effectiveness and
versatility for representing neural field updates.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [118] [A biologically Inspired Trust Model for Open Multi-Agent Systems that is Resilient to Rapid Performance Fluctuations](https://arxiv.org/abs/2504.15301)
*Zoi Lygizou,Dimitris Kalles*

Main category: cs.MA

TL;DR: 本文提出了一种生物启发的信任模型，通过本地存储信任数据和自我评估能力，解决了代理移动性、行为变化和冷启动问题。新算法进一步优化了动态环境下的适应性。


<details>
  <summary>Details</summary>
Motivation: 传统加密方法在开放、动态和多代理系统中不实用，现有信任模型面临代理移动性、行为变化和冷启动问题。

Method: 引入生物启发的信任模型，结合自我分类机制的新算法，以检测服务提供者的性能下降。

Result: 新算法在动态行为处理上优于原模型和FIRE，适应性更强。

Conclusion: 研究全面评估了模型，提出了未来研究方向，并识别了模型的优缺点及潜在对策。

Abstract: Trust management provides an alternative solution for securing open, dynamic,
and distributed multi-agent systems, where conventional cryptographic methods
prove to be impractical. However, existing trust models face challenges related
to agent mobility, changing behaviors, and the cold start problem. To address
these issues we introduced a biologically inspired trust model in which
trustees assess their own capabilities and store trust data locally. This
design improves mobility support, reduces communication overhead, resists
disinformation, and preserves privacy. Despite these advantages, prior
evaluations revealed limitations of our model in adapting to provider
population changes and continuous performance fluctuations. This study proposes
a novel algorithm, incorporating a self-classification mechanism for providers
to detect performance drops potentially harmful for the service consumers.
Simulation results demonstrate that the new algorithm outperforms its original
version and FIRE, a well-known trust and reputation model, particularly in
handling dynamic trustee behavior. While FIRE remains competitive under extreme
environmental changes, the proposed algorithm demonstrates greater adaptability
across various conditions. In contrast to existing trust modeling research,
this study conducts a comprehensive evaluation of our model using widely
recognized trust model criteria, assessing its resilience against common
trust-related attacks while identifying strengths, weaknesses, and potential
countermeasures. Finally, several key directions for future research are
proposed.

</details>


### [119] [The Formation of Production Networks: How Supply Chains Arise from Simple Learning with Minimal Information](https://arxiv.org/abs/2504.16010)
*Tuong Manh Vu,Ernesto Carrella,Robert Axtell,Omar A. Guerrero*

Main category: cs.MA

TL;DR: 论文提出了一种模型，企业通过强化学习在不确定环境中定价、生产和采购，内生形成稳态生产网络，并适应各种冲击。


<details>
  <summary>Details</summary>
Motivation: 研究企业在不确定环境中如何通过学习行为内生形成生产网络，并适应需求和生产技术的变化。

Method: 采用强化学习方法，企业根据利润最大化目标调整定价、产量和采购策略，无需依赖均衡或完全信息假设。

Result: 模型展示了企业如何适应需求和生产技术冲击，并分析了这些冲击对上下游生产网络的影响。

Conclusion: 该模型为理解生产网络的动态调整提供了新视角，尤其在不确定性和冲击下的适应性方面具有潜力。

Abstract: We develop a model where firms determine the price at which they sell their
differentiable goods, the volume that they produce, and the inputs (types and
amounts) that they purchase from other firms. A steady-state production network
emerges endogenously without resorting to assumptions such as equilibrium or
perfect knowledge about production technologies. Through a simple version of
reinforcement learning, firms with heterogeneous technologies cope with
uncertainty and maximize profits. Due to this learning process, firms can adapt
to shocks such as demand shifts, suppliers/clients closure, productivity
changes, and production technology modifications; effectively reshaping the
production network. To demonstrate the potential of this model, we analyze the
upstream and downstream impact of demand and productivity shocks.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [120] [Med-CoDE: Medical Critique based Disagreement Evaluation Framework](https://arxiv.org/abs/2504.15330)
*Mohit Gupta,Akiko Aizawa,Rajiv Ratn Shah*

Main category: cs.IR

TL;DR: 提出Med-CoDE框架，用于评估医疗领域大语言模型的可靠性和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法在医疗领域缺乏鲁棒性，可能导致临床风险。

Method: 采用基于批评的方法，量化模型生成响应与医学标准之间的差异。

Result: 框架能全面评估医疗LLM的质量和可信度。

Conclusion: Med-CoDE为医疗LLM评估提供了系统化方法，填补了现有空白。

Abstract: The emergence of large language models (LLMs) has significantly influenced
numerous fields, including healthcare, by enhancing the capabilities of
automated systems to process and generate human-like text. However, despite
their advancements, the reliability and accuracy of LLMs in medical contexts
remain critical concerns. Current evaluation methods often lack robustness and
fail to provide a comprehensive assessment of LLM performance, leading to
potential risks in clinical settings. In this work, we propose Med-CoDE, a
specifically designed evaluation framework for medical LLMs to address these
challenges. The framework leverages a critique-based approach to quantitatively
measure the degree of disagreement between model-generated responses and
established medical ground truths. This framework captures both accuracy and
reliability in medical settings. The proposed evaluation framework aims to fill
the existing gap in LLM assessment by offering a systematic method to evaluate
the quality and trustworthiness of medical LLMs. Through extensive experiments
and case studies, we illustrate the practicality of our framework in providing
a comprehensive and reliable evaluation of medical LLMs.

</details>


### [121] [CiteFix: Enhancing RAG Accuracy Through Post-Processing Citation Correction](https://arxiv.org/abs/2504.15629)
*Harsh Maheshwari,Srikanth Tenneti,Alwarappan Nakkiran*

Main category: cs.IR

TL;DR: 论文提出后处理算法提升RAG系统中LLM生成内容的引用准确性，实验结果显示整体准确率提升15.46%，同时可能降低成本和提高速度。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统中LLM在引用准确性上表现不佳（约74%），影响生成内容的可靠性和用户信任。

Method: 采用关键词+语义匹配、基于BERTScore的微调模型和轻量级LLM技术，对生成的引用进行后处理验证。

Result: 实验表明引用准确率相对提升15.46%，并可能实现更小、更快、更经济的模型替换。

Conclusion: 研究提升了AI生成内容的可靠性，对商业产品中信息检索和摘要任务的可信度至关重要。

Abstract: Retrieval Augmented Generation (RAG) has emerged as a powerful application of
Large Language Models (LLMs), revolutionizing information search and
consumption. RAG systems combine traditional search capabilities with LLMs to
generate comprehensive answers to user queries, ideally with accurate
citations. However, in our experience of developing a RAG product, LLMs often
struggle with source attribution, aligning with other industry studies
reporting citation accuracy rates of only about 74% for popular generative
search engines. To address this, we present efficient post-processing
algorithms to improve citation accuracy in LLM-generated responses, with
minimal impact on latency and cost. Our approaches cross-check generated
citations against retrieved articles using methods including keyword + semantic
matching, fine tuned model with BERTScore, and a lightweight LLM-based
technique. Our experimental results demonstrate a relative improvement of
15.46% in the overall accuracy metrics of our RAG system. This significant
enhancement potentially enables a shift from our current larger language model
to a relatively smaller model that is approximately 12x more cost-effective and
3x faster in inference time, while maintaining comparable performance. This
research contributes to enhancing the reliability and trustworthiness of
AI-generated content in information retrieval and summarization tasks which is
critical to gain customer trust especially in commercial products.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [122] [Towards Understanding Camera Motions in Any Video](https://arxiv.org/abs/2504.15376)
*Zhiqiu Lin,Siyuan Cen,Daniel Jiang,Jay Karhade,Hewei Wang,Chancharik Mitra,Tiffany Ling,Yuhan Huang,Sifan Liu,Mingyu Chen,Rushikesh Zawar,Xue Bai,Yilun Du,Chuang Gan,Deva Ramanan*

Main category: cs.CV

TL;DR: CameraBench是一个用于评估和改进相机运动理解的大规模数据集和基准测试，包含约3000个多样化视频，并提出了相机运动分类法。研究发现专家标注和训练能显著提高准确性，同时评估了SfM和VLMs模型的性能，并通过微调生成式VLM展示了其应用潜力。


<details>
  <summary>Details</summary>
Motivation: 现有方法在理解相机运动时存在局限性，尤其是语义和几何特征的捕捉不足，因此需要构建一个全面的数据集和基准测试来推动这一领域的发展。

Method: 构建了CameraBench数据集，包含专家标注的视频和相机运动分类法；通过人类研究量化标注性能；评估了SfM和VLMs模型，并微调生成式VLM。

Result: SfM模型难以捕捉依赖场景内容的语义特征，而VLMs模型在几何特征上表现不佳；微调后的VLM在多项任务中表现优异。

Conclusion: CameraBench的分类法、基准测试和教程为未来研究提供了基础，目标是实现对所有视频中相机运动的全面理解。

Abstract: We introduce CameraBench, a large-scale dataset and benchmark designed to
assess and improve camera motion understanding. CameraBench consists of ~3,000
diverse internet videos, annotated by experts through a rigorous multi-stage
quality control process. One of our contributions is a taxonomy of camera
motion primitives, designed in collaboration with cinematographers. We find,
for example, that some motions like "follow" (or tracking) require
understanding scene content like moving subjects. We conduct a large-scale
human study to quantify human annotation performance, revealing that domain
expertise and tutorial-based training can significantly enhance accuracy. For
example, a novice may confuse zoom-in (a change of intrinsics) with translating
forward (a change of extrinsics), but can be trained to differentiate the two.
Using CameraBench, we evaluate Structure-from-Motion (SfM) and Video-Language
Models (VLMs), finding that SfM models struggle to capture semantic primitives
that depend on scene content, while VLMs struggle to capture geometric
primitives that require precise estimation of trajectories. We then fine-tune a
generative VLM on CameraBench to achieve the best of both worlds and showcase
its applications, including motion-augmented captioning, video question
answering, and video-text retrieval. We hope our taxonomy, benchmark, and
tutorials will drive future efforts towards the ultimate goal of understanding
camera motions in any video.

</details>


### [123] [LongPerceptualThoughts: Distilling System-2 Reasoning for System-1 Perception](https://arxiv.org/abs/2504.15362)
*Yuan-Hong Liao,Sven Elflein,Liu He,Laura Leal-Taixé,Yejin Choi,Sanja Fidler,David Acuna*

Main category: cs.CV

TL;DR: 论文提出LongPerceptualThoughts数据集，通过三阶段合成框架生成长思维链，提升感知任务的推理性能。


<details>
  <summary>Details</summary>
Motivation: 探索长思维链在感知任务中的作用，弥补现有模型缺乏此类推理行为的不足。

Method: 三阶段数据合成框架：1) 从密集图像描述生成可验证选择题；2) 从视觉语言模型提取简单思维链；3) 通过前沿推理模型扩展为长思维链。

Result: 在5个视觉基准测试中平均提升3.4分，V$^*$ Bench提升11.8分；文本推理基准MMLU-Pro也提升2分。

Conclusion: 长思维链对感知任务有显著提升，且能泛化到文本推理任务。

Abstract: Recent reasoning models through test-time scaling have demonstrated that long
chain-of-thoughts can unlock substantial performance boosts in hard reasoning
tasks such as math and code. However, the benefit of such long thoughts for
system-2 reasoning is relatively less explored in other domains such as
perceptual tasks where shallower, system-1 reasoning seems sufficient. In this
paper, we introduce LongPerceptualThoughts, a new synthetic dataset with 30K
long-thought traces for perceptual tasks. The key challenges in synthesizing
elaborate reasoning thoughts for perceptual tasks are that off-the-shelf models
are not yet equipped with such thinking behavior and that it is not
straightforward to build a reliable process verifier for perceptual tasks.
Thus, we propose a novel three-stage data synthesis framework that first
synthesizes verifiable multiple-choice questions from dense image descriptions,
then extracts simple CoTs from VLMs for those verifiable problems, and finally
expands those simple thoughts to elaborate long thoughts via frontier reasoning
models. In controlled experiments with a strong instruction-tuned 7B model, we
demonstrate notable improvements over existing visual reasoning data-generation
methods. Our model, trained on the generated dataset, achieves an average +3.4
points improvement over 5 vision-centric benchmarks, including +11.8 points on
V$^*$ Bench. Notably, despite being tuned for vision tasks, it also improves
performance on the text reasoning benchmark, MMLU-Pro, by +2 points.

</details>


### [124] [CAPTURe: Evaluating Spatial Reasoning in Vision Language Models via Occluded Object Counting](https://arxiv.org/abs/2504.15485)
*Atin Pothiraj,Elias Stengel-Eskin,Jaemin Cho,Mohit Bansal*

Main category: cs.CV

TL;DR: 论文提出新任务CAPTURe，测试视觉语言模型（VLMs）对遮挡物体的计数和推理能力，发现模型表现不佳，尤其是面对遮挡时。


<details>
  <summary>Details</summary>
Motivation: 研究遮挡物体的识别与推理对理解真实场景至关重要，但现有模型在此能力上存在不足。

Method: 引入CAPTURe任务，分为真实图像（CAPTURe-real）和合成图像（CAPTURe-synthetic），评估四种VLMs的表现。

Result: 模型在遮挡和非遮挡模式下均表现不佳，遮挡下更差；人类表现优异。辅助信息能提升模型性能。

Conclusion: VLMs在遮挡推理和计数能力上仍有缺陷，需进一步改进。

Abstract: Recognizing and reasoning about occluded (partially or fully hidden) objects
is vital to understanding visual scenes, as occlusions frequently occur in
real-world environments and act as obstacles for spatial comprehension. To test
models' ability to reason about multiple occluded objects, we introduce a novel
task, Counting Amodally for Patterns Through Unseen REgions (CAPTURe), which
requires a model to count objects arranged in a pattern by inferring how the
pattern continues behind an occluder (an object which blocks parts of the
scene). CAPTURe requires both recognizing visual patterns and reasoning, making
it a useful testbed for evaluating vision-language models (VLMs) on whether
they understand occluded patterns and possess spatial understanding skills. By
requiring models to reason about occluded objects, CAPTURe also tests VLMs'
ability to form world models that would allow them to fill in missing
information. CAPTURe consists of two parts: (1) CAPTURe-real, with manually
filtered images of real objects in patterns and (2) CAPTURe-synthetic, a
controlled diagnostic with generated patterned images. We evaluate four strong
VLMs (GPT-4o, Intern-VL2, Molmo, and Qwen2-VL) on CAPTURe, finding that models
struggle to count on both occluded and unoccluded patterns. Crucially, we find
that models perform worse with occlusion, suggesting that VLMs are also
deficient in inferring unseen spatial relationships: even the strongest VLMs
like GPT-4o fail to count with occlusion. In contrast, we find that humans
achieve very little error on CAPTURe. We also find that providing auxiliary
information of occluded object locations increases performance, underscoring
that the model error comes both from an inability to handle occlusion as well
as difficulty counting in images.

</details>


### [125] [IV-Bench: A Benchmark for Image-Grounded Video Perception and Reasoning in Multimodal LLMs](https://arxiv.org/abs/2504.15415)
*David Ma,Yuanxing Zhang,Jincheng Ren,Jarvis Guo,Yifan Yao,Zhenlin Wei,Zhenzhu Yang,Zhongyuan Peng,Boyu Feng,Jun Ma,Xiao Gu,Zhoufutu Wen,King Zhu,Yancheng He,Meng Cao,Shiwen Ni,Jiaheng Liu,Wenhao Huang,Ge Zhang,Xiaojie Jin*

Main category: cs.CV

TL;DR: IV-Bench是一个新的基准测试，专注于评估图像背景在视频理解中的作用，发现当前MLLMs在此任务上表现不佳。


<details>
  <summary>Details</summary>
Motivation: 现有评估框架忽视了图像背景在视频理解中的重要性，因此提出了IV-Bench来填补这一空白。

Method: IV-Bench包含967个视频和2,585个标注的图像-文本查询，覆盖13个任务和5个类别，评估了开源和闭源MLLMs的性能。

Result: 当前模型在图像背景视频感知和推理任务中表现较差，最高准确率仅为28.9%。

Conclusion: IV-Bench揭示了模型性能的关键影响因素，并提供了未来研究的宝贵见解。

Abstract: Existing evaluation frameworks for Multimodal Large Language Models (MLLMs)
primarily focus on image reasoning or general video understanding tasks,
largely overlooking the significant role of image context in video
comprehension. To bridge this gap, we propose IV-Bench, the first comprehensive
benchmark for evaluating Image-Grounded Video Perception and Reasoning.
IV-Bench consists of 967 videos paired with 2,585 meticulously annotated
image-text queries across 13 tasks (7 perception and 6 reasoning tasks) and 5
representative categories. Extensive evaluations of state-of-the-art
open-source (e.g., InternVL2.5, Qwen2.5-VL) and closed-source (e.g., GPT-4o,
Gemini2-Flash and Gemini2-Pro) MLLMs demonstrate that current models
substantially underperform in image-grounded video Perception and Reasoning,
merely achieving at most 28.9% accuracy. Further analysis reveals key factors
influencing model performance on IV-Bench, including inference pattern, frame
number, and resolution. Additionally, through a simple data synthesis approach,
we demonstratethe challenges of IV- Bench extend beyond merely aligning the
data format in the training proecss. These findings collectively provide
valuable insights for future research. Our codes and data are released in
https://github.com/multimodal-art-projection/IV-Bench.

</details>


### [126] [Survey of Video Diffusion Models: Foundations, Implementations, and Applications](https://arxiv.org/abs/2504.16081)
*Yimu Wang,Xuye Liu,Wei Pang,Li Ma,Shuai Yuan,Paul Debevec,Ning Yu*

Main category: cs.CV

TL;DR: 本文综述了基于扩散模型的视频生成技术，涵盖其发展、技术基础、应用及挑战，提供了比现有综述更全面和细致的视角。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在视频生成中展现出优越的时间一致性和视觉质量，但面临运动一致性、计算效率和伦理问题等挑战，需要系统性综述。

Method: 通过系统分类现有方法，分析架构创新和优化策略，并探讨在低层视觉任务中的应用及其与其他领域的协同作用。

Result: 提供了更广泛、更新颖且细致的扩散模型视频生成视角，包括评估指标、行业解决方案和训练工程技术。

Conclusion: 本文为研究者和从业者提供了理论框架和实际实现的基础资源，推动这一快速发展领域的研究。

Abstract: Recent advances in diffusion models have revolutionized video generation,
offering superior temporal consistency and visual quality compared to
traditional generative adversarial networks-based approaches. While this
emerging field shows tremendous promise in applications, it faces significant
challenges in motion consistency, computational efficiency, and ethical
considerations. This survey provides a comprehensive review of diffusion-based
video generation, examining its evolution, technical foundations, and practical
applications. We present a systematic taxonomy of current methodologies,
analyze architectural innovations and optimization strategies, and investigate
applications across low-level vision tasks such as denoising and
super-resolution. Additionally, we explore the synergies between diffusionbased
video generation and related domains, including video representation learning,
question answering, and retrieval. Compared to the existing surveys (Lei et
al., 2024a;b; Melnik et al., 2024; Cao et al., 2023; Xing et al., 2024c) which
focus on specific aspects of video generation, such as human video synthesis
(Lei et al., 2024a) or long-form content generation (Lei et al., 2024b), our
work provides a broader, more updated, and more fine-grained perspective on
diffusion-based approaches with a special section for evaluation metrics,
industry solutions, and training engineering techniques in video generation.
This survey serves as a foundational resource for researchers and practitioners
working at the intersection of diffusion models and video generation, providing
insights into both the theoretical frameworks and practical implementations
that drive this rapidly evolving field. A structured list of related works
involved in this survey is also available on
https://github.com/Eyeline-Research/Survey-Video-Diffusion.

</details>


### [127] [RePOPE: Impact of Annotation Errors on the POPE Benchmark](https://arxiv.org/abs/2504.15707)
*Yannic Neuhaus,Matthias Hein*

Main category: cs.CV

TL;DR: 研究评估了MSCOCO标签错误对POPE基准的影响，重新标注后发现标注错误分布不均，修正后的RePOPE显著改变了模型排名。


<details>
  <summary>Details</summary>
Motivation: 由于数据标注成本高，基准数据集常沿用现有标签，但标签错误可能影响评估结果。

Method: 重新标注POPE基准图像，分析标签错误分布，并基于修正后的RePOPE评估模型表现。

Result: 发现标注错误分布不均，修正后的标签显著改变了模型排名。

Conclusion: 标签质量对基准评估有重要影响，RePOPE提供了更可靠的评估标准。

Abstract: Since data annotation is costly, benchmark datasets often incorporate labels
from established image datasets. In this work, we assess the impact of label
errors in MSCOCO on the frequently used object hallucination benchmark POPE. We
re-annotate the benchmark images and identify an imbalance in annotation errors
across different subsets. Evaluating multiple models on the revised labels,
which we denote as RePOPE, we observe notable shifts in model rankings,
highlighting the impact of label quality. Code and data are available at
https://github.com/YanNeu/RePOPE .

</details>


### [128] [Emergence and Evolution of Interpretable Concepts in Diffusion Models](https://arxiv.org/abs/2504.15473)
*Berk Tinaz,Zalan Fabian,Mahdi Soltanolkotabi*

Main category: cs.CV

TL;DR: 论文利用稀疏自编码器（SAEs）分析扩散模型的内部机制，发现其激活中存在可解释的概念，并展示了这些概念对生成过程的因果影响。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在文本到图像生成中表现出色，但其内部机制仍不透明。研究旨在通过MI技术揭示其工作原理。

Method: 采用稀疏自编码器（SAEs）框架，分析扩散模型的激活，并设计干预技术操控生成过程。

Result: 发现模型激活中存在可解释的概念，这些概念对生成过程有因果影响，可用于控制图像构图和风格。

Conclusion: 扩散模型的不同阶段对干预的响应不同：早期控制构图，中期调整风格，后期仅能修改细节。

Abstract: Diffusion models have become the go-to method for text-to-image generation,
producing high-quality images from noise through a process called reverse
diffusion. Understanding the dynamics of the reverse diffusion process is
crucial in steering the generation and achieving high sample quality. However,
the inner workings of diffusion models is still largely a mystery due to their
black-box nature and complex, multi-step generation process. Mechanistic
Interpretability (MI) techniques, such as Sparse Autoencoders (SAEs), aim at
uncovering the operating principles of models through granular analysis of
their internal representations. These MI techniques have been successful in
understanding and steering the behavior of large language models at scale.
However, the great potential of SAEs has not yet been applied toward gaining
insight into the intricate generative process of diffusion models. In this
work, we leverage the SAE framework to probe the inner workings of a popular
text-to-image diffusion model, and uncover a variety of human-interpretable
concepts in its activations. Interestingly, we find that even before the first
reverse diffusion step is completed, the final composition of the scene can be
predicted surprisingly well by looking at the spatial distribution of activated
concepts. Moreover, going beyond correlational analysis, we show that the
discovered concepts have a causal effect on the model output and can be
leveraged to steer the generative process. We design intervention techniques
aimed at manipulating image composition and style, and demonstrate that (1) in
early stages of diffusion image composition can be effectively controlled, (2)
in the middle stages of diffusion image composition is finalized, however
stylistic interventions are effective, and (3) in the final stages of diffusion
only minor textural details are subject to change.

</details>


### [129] [Human-Imperceptible Physical Adversarial Attack for NIR Face Recognition Models](https://arxiv.org/abs/2504.15823)
*Songyan Xie,Jinghang Wen,Encheng Su,Qiucheng Yu*

Main category: cs.CV

TL;DR: 提出了一种新型的、隐蔽且实用的对抗性补丁，用于在红外（NIR）人脸识别系统中进行黑盒攻击，显著提高了攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 展示NIR人脸识别系统在物理对抗攻击下的潜在风险，尤其是在低光或化妆条件下的脆弱性。

Method: 利用人眼不可见的红外吸收墨水生成数字优化的补丁，并通过模拟NIR光反射的皮肤反射模型减少数字与物理成像的差异。

Result: 在数字和物理域中攻击成功率显著提升，平均物理域攻击成功率为82.46%，优于现有方法的64.18%。

Conclusion: 该方法在多种人脸姿态下均保持高效，为NIR人脸识别系统的安全性提供了新的挑战。

Abstract: Near-infrared (NIR) face recognition systems, which can operate effectively
in low-light conditions or in the presence of makeup, exhibit vulnerabilities
when subjected to physical adversarial attacks. To further demonstrate the
potential risks in real-world applications, we design a novel, stealthy, and
practical adversarial patch to attack NIR face recognition systems in a
black-box setting. We achieved this by utilizing human-imperceptible
infrared-absorbing ink to generate multiple patches with digitally optimized
shapes and positions for infrared images. To address the optimization mismatch
between digital and real-world NIR imaging, we develop a light reflection model
for human skin to minimize pixel-level discrepancies by simulating NIR light
reflection.
  Compared to state-of-the-art (SOTA) physical attacks on NIR face recognition
systems, the experimental results show that our method improves the attack
success rate in both digital and physical domains, particularly maintaining
effectiveness across various face postures. Notably, the proposed approach
outperforms SOTA methods, achieving an average attack success rate of 82.46% in
the physical domain across different models, compared to 64.18% for existing
methods. The artifact is available at
https://anonymous.4open.science/r/Human-imperceptible-adversarial-patch-0703/.

</details>


### [130] [MedNNS: Supernet-based Medical Task-Adaptive Neural Network Search](https://arxiv.org/abs/2504.15865)
*Lotfi Abdelkrim Mecharbat,Ibrahim Elmakky,Martin Takac,Mohammed Yaqub*

Main category: cs.CV

TL;DR: MedNNS是一个针对医学影像的神经网络搜索框架，联合优化架构选择和权重初始化，显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 医学影像任务中，深度学习模型的架构选择和权重初始化是关键挑战，现有方法（如ImageNet迁移学习）效果有限。

Method: 通过构建元空间（结合Supernetwork方法）并引入rank loss和FID loss，优化模型与数据集的匹配。

Result: 在多个数据集上，MedNNS平均准确率提升1.7%，且收敛速度更快。

Conclusion: MedNNS为医学影像任务提供了高效的模型优化方案，优于现有方法。

Abstract: Deep learning (DL) has achieved remarkable progress in the field of medical
imaging. However, adapting DL models to medical tasks remains a significant
challenge, primarily due to two key factors: (1) architecture selection, as
different tasks necessitate specialized model designs, and (2) weight
initialization, which directly impacts the convergence speed and final
performance of the models. Although transfer learning from ImageNet is a widely
adopted strategy, its effectiveness is constrained by the substantial
differences between natural and medical images. To address these challenges, we
introduce Medical Neural Network Search (MedNNS), the first Neural Network
Search framework for medical imaging applications. MedNNS jointly optimizes
architecture selection and weight initialization by constructing a meta-space
that encodes datasets and models based on how well they perform together. We
build this space using a Supernetwork-based approach, expanding the model zoo
size by 51x times over previous state-of-the-art (SOTA) methods. Moreover, we
introduce rank loss and Fr\'echet Inception Distance (FID) loss into the
construction of the space to capture inter-model and inter-dataset
relationships, thereby achieving more accurate alignment in the meta-space.
Experimental results across multiple datasets demonstrate that MedNNS
significantly outperforms both ImageNet pre-trained DL models and SOTA Neural
Architecture Search (NAS) methods, achieving an average accuracy improvement of
1.7% across datasets while converging substantially faster. The code and the
processed meta-space is available at https://github.com/BioMedIA-MBZUAI/MedNNS.

</details>


### [131] [Multi-Modal Fusion of In-Situ Video Data and Process Parameters for Online Forecasting of Cookie Drying Readiness](https://arxiv.org/abs/2504.15599)
*Shichen Li,Chenhui Shao*

Main category: cs.CV

TL;DR: 提出了一种多模态数据融合框架，用于实时预测食品干燥状态，显著提高了预测精度和效率。


<details>
  <summary>Details</summary>
Motivation: 食品干燥的实时预测对节能、生产效率和产品质量至关重要，但现有方法因数据有限和动态性难以满足需求。

Method: 采用端到端多模态数据融合框架，结合视频数据和过程参数，使用新型编码器-解码器架构和基于Transformer的解码器。

Result: 模型在糖饼干干燥实验中平均预测误差仅15秒，优于现有方法65.69%和纯视频模型11.30%。

Conclusion: 该模型在精度、模型大小和计算效率间取得平衡，适用于工业多模态融合任务。

Abstract: Food drying is essential for food production, extending shelf life, and
reducing transportation costs. Accurate real-time forecasting of drying
readiness is crucial for minimizing energy consumption, improving productivity,
and ensuring product quality. However, this remains challenging due to the
dynamic nature of drying, limited data availability, and the lack of effective
predictive analytical methods. To address this gap, we propose an end-to-end
multi-modal data fusion framework that integrates in-situ video data with
process parameters for real-time food drying readiness forecasting. Our
approach leverages a new encoder-decoder architecture with modality-specific
encoders and a transformer-based decoder to effectively extract features while
preserving the unique structure of each modality. We apply our approach to
sugar cookie drying, where time-to-ready is predicted at each timestamp.
Experimental results demonstrate that our model achieves an average prediction
error of only 15 seconds, outperforming state-of-the-art data fusion methods by
65.69% and a video-only model by 11.30%. Additionally, our model balances
prediction accuracy, model size, and computational efficiency, making it
well-suited for heterogenous industrial datasets. The proposed model is
extensible to various other industrial modality fusion tasks for online
decision-making.

</details>


### [132] [Integrating Non-Linear Radon Transformation for Diabetic Retinopathy Grading](https://arxiv.org/abs/2504.15883)
*Farida Mohsen,Samir Belhaouari,Zubair Shah*

Main category: cs.CV

TL;DR: RadFuse是一种多表示深度学习框架，结合非线性RadEx变换的sinogram图像与传统眼底图像，显著提升了糖尿病视网膜病变的检测和分级性能。


<details>
  <summary>Details</summary>
Motivation: 糖尿病视网膜病变的复杂病变模式难以通过传统眼底图像捕捉，需要更有效的方法来提升检测和分级的准确性。

Method: 提出RadFuse框架，整合非线性RadEx变换生成的sinogram图像与眼底图像，利用空间和变换域信息增强特征提取。

Result: 在APTOS-2019和DDR数据集上，RadFuse在五级分级和二分类任务中均显著优于现有方法，准确率高达99.09%。

Conclusion: RadFuse通过捕捉复杂非线性特征，推动了糖尿病视网膜病变分类的进步，并展示了数学变换在医学图像分析中的潜力。

Abstract: Diabetic retinopathy is a serious ocular complication that poses a
significant threat to patients' vision and overall health. Early detection and
accurate grading are essential to prevent vision loss. Current automatic
grading methods rely heavily on deep learning applied to retinal fundus images,
but the complex, irregular patterns of lesions in these images, which vary in
shape and distribution, make it difficult to capture subtle changes. This study
introduces RadFuse, a multi-representation deep learning framework that
integrates non-linear RadEx-transformed sinogram images with traditional fundus
images to enhance diabetic retinopathy detection and grading. Our RadEx
transformation, an optimized non-linear extension of the Radon transform,
generates sinogram representations to capture complex retinal lesion patterns.
By leveraging both spatial and transformed domain information, RadFuse enriches
the feature set available to deep learning models, improving the
differentiation of severity levels. We conducted extensive experiments on two
benchmark datasets, APTOS-2019 and DDR, using three convolutional neural
networks (CNNs): ResNeXt-50, MobileNetV2, and VGG19. RadFuse showed significant
improvements over fundus-image-only models across all three CNN architectures
and outperformed state-of-the-art methods on both datasets. For severity
grading across five stages, RadFuse achieved a quadratic weighted kappa of
93.24%, an accuracy of 87.07%, and an F1-score of 87.17%. In binary
classification between healthy and diabetic retinopathy cases, the method
reached an accuracy of 99.09%, precision of 98.58%, and recall of 99.6%,
surpassing previously established models. These results demonstrate RadFuse's
capacity to capture complex non-linear features, advancing diabetic retinopathy
classification and promoting the integration of advanced mathematical
transforms in medical image analysis.

</details>


### [133] [Ask2Loc: Learning to Locate Instructional Visual Answers by Asking Questions](https://arxiv.org/abs/2504.15918)
*Chang Zong,Bin Li,Shoujun Zhou,Jian Wan,Lei Zhang*

Main category: cs.CV

TL;DR: 论文提出了一种新任务In-VAL，模拟人与视频的多轮交互以定位视觉答案，并提出了Ask2Loc框架，通过提问解决语义鸿沟问题，实验显示性能显著提升。


<details>
  <summary>Details</summary>
Motivation: 用户在获取视频指导知识时需要多次交互才能找到符合预期的答案，而现有方法未能有效模拟这一过程。

Method: 提出Ask2Loc框架，包含聊天模块、重写模块和搜索模块，分别解决意图模糊、语言不完整和内容碎片化问题。

Result: 在三个重构的In-VAL数据集上，Ask2Loc比传统方法性能提升高达14.91（mIoU）。

Conclusion: Ask2Loc通过多模块协作有效解决了In-VAL任务中的语义鸿沟问题，显著提升了定位性能。

Abstract: Locating specific segments within an instructional video is an efficient way
to acquire guiding knowledge. Generally, the task of obtaining video segments
for both verbal explanations and visual demonstrations is known as visual
answer localization (VAL). However, users often need multiple interactions to
obtain answers that align with their expectations when using the system. During
these interactions, humans deepen their understanding of the video content by
asking themselves questions, thereby accurately identifying the location.
Therefore, we propose a new task, named In-VAL, to simulate the multiple
interactions between humans and videos in the procedure of obtaining visual
answers. The In-VAL task requires interactively addressing several semantic gap
issues, including 1) the ambiguity of user intent in the input questions, 2)
the incompleteness of language in video subtitles, and 3) the fragmentation of
content in video segments. To address these issues, we propose Ask2Loc, a
framework for resolving In-VAL by asking questions. It includes three key
modules: 1) a chatting module to refine initial questions and uncover clear
intentions, 2) a rewriting module to generate fluent language and create
complete descriptions, and 3) a searching module to broaden local context and
provide integrated content. We conduct extensive experiments on three
reconstructed In-VAL datasets. Compared to traditional end-to-end and two-stage
methods, our proposed Ask2Loc can improve performance by up to 14.91 (mIoU) on
the In-VAL task. Our code and datasets can be accessed at
https://github.com/changzong/Ask2Loc.

</details>


### [134] [A Clinician-Friendly Platform for Ophthalmic Image Analysis Without Technical Barriers](https://arxiv.org/abs/2504.15928)
*Meng Wang,Tian Lin,Qingshan Hou,Aidi Lin,Jingcheng Wang,Qingsheng Peng,Truong X. Nguyen,Danqi Fang,Ke Zou,Ting Xu,Cancan Xue,Ten Cheer Quek,Qinkai Yu,Minxin Liu,Hui Zhou,Zixuan Xiao,Guiqin He,Huiyu Liang,Tingkun Shi,Man Chen,Linna Liu,Yuanyuan Peng,Lianyu Wang,Qiuming Hu,Junhong Chen,Zhenhua Zhang,Cheng Chen,Yitian Zhao,Dianbo Liu,Jianhua Wu,Xinjian Chen,Changqing Zhang,Triet Thanh Nguyen,Yanda Meng,Yalin Zheng,Yih Chung Tham,Carol Y. Cheung,Huazhu Fu,Haoyu Chen,Ching-Yu Cheng*

Main category: cs.CV

TL;DR: GlobeReady是一个无需重新训练即可在不同临床中心部署的AI平台，用于眼科疾病诊断，具有高准确性和跨域适应性。


<details>
  <summary>Details</summary>
Motivation: 当前AI模型在医学影像诊断中需要针对不同临床中心重新训练，限制了其广泛应用。GlobeReady旨在解决这一问题。

Method: 通过无需训练的局部特征增强技术，GlobeReady解决了跨中心和人群的域偏移问题，并采用置信度量化诊断方法。

Result: 在多个国家的测试中，GlobeReady表现出高准确性（如中国88.9%，越南86.3%，英国90.2%），并提升了诊断准确率至94.9-99.4%（眼底照片）和88.2-96.2%（OCT）。

Conclusion: GlobeReady展示了其稳健、可扩展的诊断能力，有望推动无技术障碍的眼科护理。

Abstract: Artificial intelligence (AI) shows remarkable potential in medical imaging
diagnostics, but current models typically require retraining when deployed
across different clinical centers, limiting their widespread adoption. We
introduce GlobeReady, a clinician-friendly AI platform that enables ocular
disease diagnosis without retraining/fine-tuning or technical expertise.
GlobeReady achieves high accuracy across imaging modalities: 93.9-98.5% for an
11-category fundus photo dataset and 87.2-92.7% for a 15-category OCT dataset.
Through training-free local feature augmentation, it addresses domain shifts
across centers and populations, reaching an average accuracy of 88.9% across
five centers in China, 86.3% in Vietnam, and 90.2% in the UK. The built-in
confidence-quantifiable diagnostic approach further boosted accuracy to
94.9-99.4% (fundus) and 88.2-96.2% (OCT), while identifying out-of-distribution
cases at 86.3% (49 CFP categories) and 90.6% (13 OCT categories). Clinicians
from multiple countries rated GlobeReady highly (average 4.6 out of 5) for its
usability and clinical relevance. These results demonstrate GlobeReady's
robust, scalable diagnostic capability and potential to support ophthalmic care
without technical barriers.

</details>


### [135] [Meta-Entity Driven Triplet Mining for Aligning Medical Vision-Language Models](https://arxiv.org/abs/2504.15929)
*Saban Ozturk,Melih B. Yilmaz,Muti Kara,M. Talat Yavuz,Aykut Koç,Tolga Çukur*

Main category: cs.CV

TL;DR: MedTrim提出了一种新的医学视觉语言模型对齐方法，通过多模态三元组学习优化图像与文本的对齐，提升下游任务的性能。


<details>
  <summary>Details</summary>
Motivation: 医学影像和报告数据量增长导致专家压力增加，现有对齐方法忽略细粒度病理特征，影响模型性能。

Method: MedTrim结合疾病类别和病理描述符，通过结构化元实体信息和新型评分函数优化三元组挖掘，实现跨模态对齐。

Result: MedTrim在检索和分类任务中优于现有对齐方法。

Conclusion: MedTrim通过细粒度对齐提升了医学视觉语言模型的性能，具有临床应用潜力。

Abstract: Diagnostic imaging relies on interpreting both images and radiology reports,
but the growing data volumes place significant pressure on medical experts,
yielding increased errors and workflow backlogs. Medical vision-language models
(med-VLMs) have emerged as a powerful framework to efficiently process
multimodal imaging data, particularly in chest X-ray (CXR) evaluations, albeit
their performance hinges on how well image and text representations are
aligned. Existing alignment methods, predominantly based on contrastive
learning, prioritize separation between disease classes over segregation of
fine-grained pathology attributes like location, size or severity, leading to
suboptimal representations. Here, we propose MedTrim (Meta-entity-driven
Triplet mining), a novel method that enhances image-text alignment through
multimodal triplet learning synergistically guided by disease class as well as
adjectival and directional pathology descriptors. Unlike common alignment
methods that separate broad disease classes, MedTrim leverages structured
meta-entity information to preserve subtle but clinically significant
intra-class variations. For this purpose, we first introduce an ontology-based
entity recognition module that extracts pathology-specific meta-entities from
CXR reports, as annotations on pathology attributes are rare in public
datasets. For refined sample selection in triplet mining, we then introduce a
novel score function that captures an aggregate measure of inter-sample
similarity based on disease classes and adjectival/directional descriptors.
Lastly, we introduce a multimodal triplet alignment objective for explicit
within- and cross-modal alignment between samples sharing detailed pathology
characteristics. Our demonstrations indicate that MedTrim improves performance
in downstream retrieval and classification tasks compared to state-of-the-art
alignment methods.

</details>


### [136] [Locating and Mitigating Gradient Conflicts in Point Cloud Domain Adaptation via Saliency Map Skewness](https://arxiv.org/abs/2504.15796)
*Jiaqi Tang,Yinsong Xu,Qingchao Chen*

Main category: cs.CV

TL;DR: 提出了一种基于显著性图的梯度冲突缓解方法（SM-DSB），用于提升点云无监督域适应（UDA）的分类性能。


<details>
  <summary>Details</summary>
Motivation: 现有UDA方法中的自监督任务梯度可能对分类性能产生负面影响，需解决梯度冲突问题。

Method: 设计基于3D显著性图偏度的评分机制，动态筛选有益样本，减少负面梯度影响。

Result: 方法在性能上优于现有技术，计算开销小且可扩展。

Conclusion: SM-DSB为UDA问题提供了新视角，并通过反向传播分析验证了其有效性。

Abstract: Object classification models utilizing point cloud data are fundamental for
3D media understanding, yet they often struggle with unseen or
out-of-distribution (OOD) scenarios. Existing point cloud unsupervised domain
adaptation (UDA) methods typically employ a multi-task learning (MTL) framework
that combines primary classification tasks with auxiliary self-supervision
tasks to bridge the gap between cross-domain feature distributions. However,
our further experiments demonstrate that not all gradients from
self-supervision tasks are beneficial and some may negatively impact the
classification performance. In this paper, we propose a novel solution, termed
Saliency Map-based Data Sampling Block (SM-DSB), to mitigate these gradient
conflicts. Specifically, our method designs a new scoring mechanism based on
the skewness of 3D saliency maps to estimate gradient conflicts without
requiring target labels. Leveraging this, we develop a sample selection
strategy that dynamically filters out samples whose self-supervision gradients
are not beneficial for the classification. Our approach is scalable,
introducing modest computational overhead, and can be integrated into all the
point cloud UDA MTL frameworks. Extensive evaluations demonstrate that our
method outperforms state-of-the-art approaches. In addition, we provide a new
perspective on understanding the UDA problem through back-propagation analysis.

</details>


### [137] [DERD-Net: Learning Depth from Event-based Ray Densities](https://arxiv.org/abs/2504.15863)
*Diego de Oliveira Hitzges,Suman Ghosh,Guillermo Gallego*

Main category: cs.CV

TL;DR: 提出了一种基于事件相机的深度估计框架，适用于单目和立体设置，通过处理局部子区域实现高效并行化，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习框架难以处理事件相机的异步数据流，需要一种适应事件数据特性的新方法。

Method: 将3D场景结构编码为视差空间图像（DSIs），结合3D卷积和循环结构处理局部子区域，实现深度预测。

Result: 在标准数据集上表现优异：单目数据媲美立体方法，立体数据误差降低至少42%，深度完整性提升3倍以上。

Conclusion: 该框架在事件相机深度估计和SLAM中具有成为标准方法的潜力。

Abstract: Event cameras offer a promising avenue for multi-view stereo depth estimation
and Simultaneous Localization And Mapping (SLAM) due to their ability to detect
blur-free 3D edges at high-speed and over broad illumination conditions.
However, traditional deep learning frameworks designed for conventional cameras
struggle with the asynchronous, stream-like nature of event data, as their
architectures are optimized for discrete, image-like inputs. We propose a
scalable, flexible and adaptable framework for pixel-wise depth estimation with
event cameras in both monocular and stereo setups. The 3D scene structure is
encoded into disparity space images (DSIs), representing spatial densities of
rays obtained by back-projecting events into space via known camera poses. Our
neural network processes local subregions of the DSIs combining 3D convolutions
and a recurrent structure to recognize valuable patterns for depth prediction.
Local processing enables fast inference with full parallelization and ensures
constant ultra-low model complexity and memory costs, regardless of camera
resolution. Experiments on standard benchmarks (MVSEC and DSEC datasets)
demonstrate unprecedented effectiveness: (i) using purely monocular data, our
method achieves comparable results to existing stereo methods; (ii) when
applied to stereo data, it strongly outperforms all state-of-the-art (SOTA)
approaches, reducing the mean absolute error by at least 42%; (iii) our method
also allows for increases in depth completeness by more than 3-fold while still
yielding a reduction in median absolute error of at least 30%. Given its
remarkable performance and effective processing of event-data, our framework
holds strong potential to become a standard approach for using deep learning
for event-based depth estimation and SLAM. Project page:
https://github.com/tub-rip/DERD-Net

</details>


### [138] [Evaluating Vision Language Models (VLMs) for Radiology: A Comprehensive Analysis](https://arxiv.org/abs/2504.16047)
*Frank Li,Hari Trivedi,Bardia Khosravi,Theo Dapamede,Mohammadreza Chavoshi,Abdulhameed Dere,Rohan Satya Isaac,Aawez Mansuri,Janice Newsome,Saptarshi Purkayastha,Judy Gichoya*

Main category: cs.CV

TL;DR: 研究评估了三种视觉-语言基础模型（RAD-DINO、CheXagent和BiomedCLIP）在放射学任务中的表现，发现预训练方法对下游任务性能有显著影响。


<details>
  <summary>Details</summary>
Motivation: 探索基础模型在医学AI应用中的潜力，尤其是在放射学任务中捕捉细粒度成像特征的能力。

Method: 评估三种模型在胸部X光片上的分类、分割和回归任务（气胸和心脏肥大）中的表现，并设计了一个结合全局和局部特征的自定义分割模型。

Result: RAD-DINO在分割任务中表现最佳，CheXagent在分类任务中表现优异，BiomedCLIP表现不稳定。自定义模型显著提升了所有模型在气胸分割任务中的性能。

Conclusion: 预训练方法对模型性能有显著影响，无文本监督的模型在分割任务中表现更好，而文本监督模型在分类和可解释性方面有优势，为临床选择模型提供了指导。

Abstract: Foundation models, trained on vast amounts of data using self-supervised
techniques, have emerged as a promising frontier for advancing artificial
intelligence (AI) applications in medicine. This study evaluates three
different vision-language foundation models (RAD-DINO, CheXagent, and
BiomedCLIP) on their ability to capture fine-grained imaging features for
radiology tasks. The models were assessed across classification, segmentation,
and regression tasks for pneumothorax and cardiomegaly on chest radiographs.
Self-supervised RAD-DINO consistently excelled in segmentation tasks, while
text-supervised CheXagent demonstrated superior classification performance.
BiomedCLIP showed inconsistent performance across tasks. A custom segmentation
model that integrates global and local features substantially improved
performance for all foundation models, particularly for challenging
pneumothorax segmentation. The findings highlight that pre-training methodology
significantly influences model performance on specific downstream tasks. For
fine-grained segmentation tasks, models trained without text supervision
performed better, while text-supervised models offered advantages in
classification and interpretability. These insights provide guidance for
selecting foundation models based on specific clinical applications in
radiology.

</details>


### [139] [Efficient Adaptation of Deep Neural Networks for Semantic Segmentation in Space Applications](https://arxiv.org/abs/2504.15991)
*Leonardo Olivi,Edoardo Santero Mormile,Enzo Tartaglione*

Main category: cs.CV

TL;DR: 论文探讨了在月球和火星地形中，通过适配器实现高效迁移学习用于岩石分割的可行性，提出了减少带宽和内存需求的方法。


<details>
  <summary>Details</summary>
Motivation: 解决外星环境中标记数据稀缺的问题，探索迁移学习在外星探索中的应用。

Method: 采用适配器策略，结合预训练模型，提出层融合和适配器排名两种内存节省方法。

Result: 适配器策略成功减少了目标设备的带宽和内存需求，并在嵌入式设备上验证了性能与资源消耗的权衡。

Conclusion: 研究为外星环境中的高效迁移学习提供了可行方案，并指出了未来研究方向。

Abstract: In recent years, the application of Deep Learning techniques has shown
remarkable success in various computer vision tasks, paving the way for their
deployment in extraterrestrial exploration. Transfer learning has emerged as a
powerful strategy for addressing the scarcity of labeled data in these novel
environments. This paper represents one of the first efforts in evaluating the
feasibility of employing adapters toward efficient transfer learning for rock
segmentation in extraterrestrial landscapes, mainly focusing on lunar and
martian terrains. Our work suggests that the use of adapters, strategically
integrated into a pre-trained backbone model, can be successful in reducing
both bandwidth and memory requirements for the target extraterrestrial device.
In this study, we considered two memory-saving strategies: layer fusion (to
reduce to zero the inference overhead) and an ``adapter ranking'' (to also
reduce the transmission cost). Finally, we evaluate these results in terms of
task performance, memory, and computation on embedded devices, evidencing
trade-offs that open the road to more research in the field.

</details>


### [140] [Vision language models are unreliable at trivial spatial cognition](https://arxiv.org/abs/2504.16061)
*Sangeet Khemlani,Tyler Tran,Nathaniel Gyory,Anthony M. Harrison,Wallace E. Lawson,Ravenna Thielstrom,Hunter Thompson,Taaren Singh,J. Gregory Trafton*

Main category: cs.CV

TL;DR: 论文研究了视觉语言模型（VLMs）在空间认知任务中的可靠性，发现其性能受提示词微小变化影响，揭示了其在空间关系推理上的局限性。


<details>
  <summary>Details</summary>
Motivation: 探讨VLMs在空间认知任务中的表现，以验证其可靠性和适用性。

Method: 开发了TableTest基准数据集，评估了先进VLMs在简单空间任务中的表现。

Result: VLMs性能受提示词微小变化影响，表明其在空间关系推理上存在局限性。

Conclusion: 研究揭示了VLMs的局限性，并提出了改进图像描述语料库的机会。

Abstract: Vision language models (VLMs) are designed to extract relevant visuospatial
information from images. Some research suggests that VLMs can exhibit humanlike
scene understanding, while other investigations reveal difficulties in their
ability to process relational information. To achieve widespread applicability,
VLMs must perform reliably, yielding comparable competence across a wide
variety of related tasks. We sought to test how reliable these architectures
are at engaging in trivial spatial cognition, e.g., recognizing whether one
object is left of another in an uncluttered scene. We developed a benchmark
dataset -- TableTest -- whose images depict 3D scenes of objects arranged on a
table, and used it to evaluate state-of-the-art VLMs. Results show that
performance could be degraded by minor variations of prompts that use logically
equivalent descriptions. These analyses suggest limitations in how VLMs may
reason about spatial relations in real-world applications. They also reveal
novel opportunities for bolstering image caption corpora for more efficient
training and testing.

</details>


### [141] [Describe Anything: Detailed Localized Image and Video Captioning](https://arxiv.org/abs/2504.16072)
*Long Lian,Yifan Ding,Yunhao Ge,Sifei Liu,Hanzi Mao,Boyi Li,Marco Pavone,Ming-Yu Liu,Trevor Darrell,Adam Yala,Yin Cui*

Main category: cs.CV

TL;DR: DAM模型通过焦点提示和局部视觉骨干网络实现高分辨率局部描述，结合半监督学习数据管道DLC-SDP解决数据稀缺问题，并在多个基准测试中取得最优表现。


<details>
  <summary>Details</summary>
Motivation: 解决视觉语言模型在生成图像和视频局部详细描述时的挑战。

Method: 提出DAM模型，包含焦点提示和局部视觉骨干网络；设计半监督学习数据管道DLC-SDP扩展数据。

Result: DAM在7个基准测试中达到最优表现，涵盖关键词、短语和多句子级别的局部描述任务。

Conclusion: DAM通过创新设计和数据扩展方法，显著提升了局部描述的准确性和细节表现。

Abstract: Generating detailed and accurate descriptions for specific regions in images
and videos remains a fundamental challenge for vision-language models. We
introduce the Describe Anything Model (DAM), a model designed for detailed
localized captioning (DLC). DAM preserves both local details and global context
through two key innovations: a focal prompt, which ensures high-resolution
encoding of targeted regions, and a localized vision backbone, which integrates
precise localization with its broader context. To tackle the scarcity of
high-quality DLC data, we propose a Semi-supervised learning (SSL)-based Data
Pipeline (DLC-SDP). DLC-SDP starts with existing segmentation datasets and
expands to unlabeled web images using SSL. We introduce DLC-Bench, a benchmark
designed to evaluate DLC without relying on reference captions. DAM sets new
state-of-the-art on 7 benchmarks spanning keyword-level, phrase-level, and
detailed multi-sentence localized image and video captioning.

</details>


### [142] [MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention](https://arxiv.org/abs/2504.16083)
*Yucheng Li,Huiqiang Jiang,Chengruidong Zhang,Qianhui Wu,Xufang Luo,Surin Ahn,Amir H. Abdi,Dongsheng Li,Jianfeng Gao,Yuqing Yang,Lili Qiu*

Main category: cs.CV

TL;DR: MMInference是一种动态稀疏注意力方法，用于加速长上下文多模态输入的预填充阶段，提升视觉语言模型的效率。


<details>
  <summary>Details</summary>
Motivation: 解决视觉语言模型在长上下文多模态输入中预填充阶段的二次注意力复杂度问题。

Method: 通过分析视频输入的时空局部性，提出Grid模式稀疏分布，并利用基于排列的方法处理模态边界问题，动态构建稀疏分布。

Result: 在1M tokens下，预填充阶段加速高达8.3倍，同时保持准确性。

Conclusion: MMInference无需修改模型即可无缝集成到现有VLM流程中，显著提升了长上下文多模态任务的效率。

Abstract: The integration of long-context capabilities with visual understanding
unlocks unprecedented potential for Vision Language Models (VLMs). However, the
quadratic attention complexity during the pre-filling phase remains a
significant obstacle to real-world deployment. To overcome this limitation, we
introduce MMInference (Multimodality Million tokens Inference), a dynamic
sparse attention method that accelerates the prefilling stage for long-context
multi-modal inputs. First, our analysis reveals that the temporal and spatial
locality of video input leads to a unique sparse pattern, the Grid pattern.
Simultaneously, VLMs exhibit markedly different sparse distributions across
different modalities. We introduce a permutation-based method to leverage the
unique Grid pattern and handle modality boundary issues. By offline search the
optimal sparse patterns for each head, MMInference constructs the sparse
distribution dynamically based on the input. We also provide optimized GPU
kernels for efficient sparse computations. Notably, MMInference integrates
seamlessly into existing VLM pipelines without any model modifications or
fine-tuning. Experiments on multi-modal benchmarks-including Video QA,
Captioning, VisionNIAH, and Mixed-Modality NIAH-with state-of-the-art
long-context VLMs (LongVila, LlavaVideo, VideoChat-Flash, Qwen2.5-VL) show that
MMInference accelerates the pre-filling stage by up to 8.3x at 1M tokens while
maintaining accuracy. Our code is available at https://aka.ms/MMInference.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [143] [RINN: One Sample Radio Frequency Imaging based on Physics Informed Neural Network](https://arxiv.org/abs/2504.15311)
*Fei Shang,Haohua Du,Dawei Yan,Panlong Yang,Xiang-Yang Li*

Main category: eess.IV

TL;DR: 论文提出了一种名为RINN的网络，利用物理约束而非真实值比较约束，实现了仅需单一样本且无需相位信息的射频成像。


<details>
  <summary>Details</summary>
Motivation: 射频成像技术在非视距和低光环境下具有潜力，但现有设备难以提供高精度电磁测量和大规模数据集，限制了其应用。

Method: 结合PINN思想设计RINN网络，利用物理约束适应射频信号特性，实现无需相位和仅需单一样本的成像。

Result: 数值评估显示，RINN在无相位数据下的成像效果与基于相位数据的经典算法相当，RRMSE指标为0.11。

Conclusion: RINN为射频成像技术的普及提供了新的可能性。

Abstract: Due to its ability to work in non-line-of-sight and low-light environments,
radio frequency (RF) imaging technology is expected to bring new possibilities
for embodied intelligence and multimodal sensing. However, widely used RF
devices (such as Wi-Fi) often struggle to provide high-precision
electromagnetic measurements and large-scale datasets, hindering the
application of RF imaging technology. In this paper, we combine the ideas of
PINN to design the RINN network, using physical constraints instead of true
value comparison constraints and adapting it with the characteristics of
ubiquitous RF signals, allowing the RINN network to achieve RF imaging using
only one sample without phase and with amplitude noise. Our numerical
evaluation results show that compared with 5 classic algorithms based on phase
data for imaging results, RINN's imaging results based on phaseless data are
good, with indicators such as RRMSE (0.11) performing similarly well. RINN
provides new possibilities for the universal development of radio frequency
imaging technology.

</details>


### [144] [Enhancing DR Classification with Swin Transformer and Shifted Window Attention](https://arxiv.org/abs/2504.15317)
*Meher Boulaabi,Takwa Ben Aïcha Gader,Afef Kacem Echi,Zied Bouraoui*

Main category: eess.IV

TL;DR: 提出了一种结合预处理和Swin Transformer的糖尿病视网膜病变分类方法，显著提升了分类准确率。


<details>
  <summary>Details</summary>
Motivation: 糖尿病视网膜病变是全球致盲的主要原因，早期检测对治疗至关重要，但自动化分类面临图像质量、类别不平衡等挑战。

Method: 采用图像裁剪、CLAHE增强和针对性数据增强的预处理流程，结合Swin Transformer的层次化token处理和窗口注意力机制。

Result: 在Aptos和IDRiD数据集上分别达到89.65%和97.40%的准确率，尤其在早期病变检测中表现突出。

Conclusion: 该方法有效提升了糖尿病视网膜病变的自动化分类性能，具有临床筛查潜力。

Abstract: Diabetic retinopathy (DR) is a leading cause of blindness worldwide,
underscoring the importance of early detection for effective treatment.
However, automated DR classification remains challenging due to variations in
image quality, class imbalance, and pixel-level similarities that hinder model
training. To address these issues, we propose a robust preprocessing pipeline
incorporating image cropping, Contrast-Limited Adaptive Histogram Equalization
(CLAHE), and targeted data augmentation to improve model generalization and
resilience. Our approach leverages the Swin Transformer, which utilizes
hierarchical token processing and shifted window attention to efficiently
capture fine-grained features while maintaining linear computational
complexity. We validate our method on the Aptos and IDRiD datasets for
multi-class DR classification, achieving accuracy rates of 89.65% and 97.40%,
respectively. These results demonstrate the effectiveness of our model,
particularly in detecting early-stage DR, highlighting its potential for
improving automated retinal screening in clinical settings.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [145] [Transferable Learning of Reaction Pathways from Geometric Priors](https://arxiv.org/abs/2504.15370)
*Juno Nam,Miguel Steiner,Max Misterka,Soojung Yang,Avni Singhal,Rafael Gómez-Bombarelli*

Main category: physics.chem-ph

TL;DR: MEPIN是一种可扩展的机器学习方法，用于高效预测化学反应的最小能量路径（MEPs），无需依赖过渡态几何或预优化的反应路径。


<details>
  <summary>Details</summary>
Motivation: 理解化学反应机制需要识别最小能量路径，但传统方法计算成本高。

Method: 使用对称性破缺的等变神经网络构建连续反应路径模型，通过能量目标和几何先验优化训练。

Result: 方法在多种化学反应中表现良好，能够准确对齐参考反应坐标。

Conclusion: MEPIN为大规模化学反应空间的高效探索提供了数据驱动的预测工具。

Abstract: Identifying minimum-energy paths (MEPs) is crucial for understanding chemical
reaction mechanisms but remains computationally demanding. We introduce MEPIN,
a scalable machine-learning method for efficiently predicting MEPs from
reactant and product configurations, without relying on transition-state
geometries or pre-optimized reaction paths during training. The task is defined
as predicting deviations from geometric interpolations along reaction
coordinates. We address this task with a continuous reaction path model based
on a symmetry-broken equivariant neural network that generates a flexible
number of intermediate structures. The model is trained using an energy-based
objective, with efficiency enhanced by incorporating geometric priors from
geodesic interpolation as initial interpolations or pre-training objectives.
Our approach generalizes across diverse chemical reactions and achieves
accurate alignment with reference intrinsic reaction coordinates, as
demonstrated on various small molecule reactions and [3+2] cycloadditions. Our
method enables the exploration of large chemical reaction spaces with
efficient, data-driven predictions of reaction pathways.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [146] [On the Price of Differential Privacy for Hierarchical Clustering](https://arxiv.org/abs/2504.15580)
*Chengyuan Deng,Jie Gao,Jalaj Upadhyay,Chen Wang,Samson Zhou*

Main category: cs.DS

TL;DR: 该论文提出了一种在权重隐私模型下的差分隐私层次聚类算法，显著优于边级差分隐私的不可能性结果，并展示了理论和实验上的优越性。


<details>
  <summary>Details</summary>
Motivation: 层次聚类涉及敏感用户数据，需保护隐私。现有边级差分隐私方法误差大，因此研究权重隐私模型以改进性能。

Method: 提出了一种在权重隐私模型下的新算法，满足ε-DP，具有多项式时间复杂度和较低的乘法误差。

Result: 算法在理论和实验中表现优异，乘法误差为O(log^1.5n/ε)，且成本不高于现有工作的最优加法误差。

Conclusion: 权重隐私模型在层次聚类中优于边级差分隐私，但若取消单位权重约束，其下限与边级DP相同。

Abstract: Hierarchical clustering is a fundamental unsupervised machine learning task
with the aim of organizing data into a hierarchy of clusters. Many applications
of hierarchical clustering involve sensitive user information, therefore
motivating recent studies on differentially private hierarchical clustering
under the rigorous framework of Dasgupta's objective. However, it has been
shown that any privacy-preserving algorithm under edge-level differential
privacy necessarily suffers a large error. To capture practical applications of
this problem, we focus on the weight privacy model, where each edge of the
input graph is at least unit weight. We present a novel algorithm in the weight
privacy model that shows significantly better approximation than known
impossibility results in the edge-level DP setting. In particular, our
algorithm achieves $O(\log^{1.5}n/\varepsilon)$ multiplicative error for
$\varepsilon$-DP and runs in polynomial time, where $n$ is the size of the
input graph, and the cost is never worse than the optimal additive error in
existing work. We complement our algorithm by showing if the unit-weight
constraint does not apply, the lower bound for weight-level DP hierarchical
clustering is essentially the same as the edge-level DP, i.e.
$\Omega(n^2/\varepsilon)$ additive error. As a result, we also obtain a new
lower bound of $\tilde{\Omega}(1/\varepsilon)$ additive error for balanced
sparsest cuts in the weight-level DP model, which may be of independent
interest. Finally, we evaluate our algorithm on synthetic and real-world
datasets. Our experimental results show that our algorithm performs well in
terms of extra cost and has good scalability to large graphs.

</details>


<div id='astro-ph.SR'></div>

# astro-ph.SR [[Back]](#toc)

### [147] [Policy-Based Radiative Transfer: Solving the $2$-Level Atom Non-LTE Problem using Soft Actor-Critic Reinforcement Learning](https://arxiv.org/abs/2504.15679)
*Brandon Panos,Ivan Milic*

Main category: astro-ph.SR

TL;DR: 本文提出了一种新颖的强化学习方法，用于解决非局部热动平衡辐射传输问题，通过将问题转化为控制任务，避免了传统方法中的近似算子构建和数据预处理需求。


<details>
  <summary>Details</summary>
Motivation: 传统方法在解决非局部热动平衡辐射传输问题时需要构建近似算子或依赖大量预计算数据，这些方法在复杂场景下可能效率低下或难以实现。本文旨在通过强化学习绕过这些限制。

Method: 采用强化学习框架，训练一个智能体通过奖励机制学习深度依赖的源函数，无需显式知识或梯度回传。

Result: 实验表明，该方法能够自洽地满足统计平衡方程，且在复杂场景下具有潜在优势。

Conclusion: 该方法为非局部热动平衡问题提供了一种新的解决方案，尤其在复杂场景中可能优于传统方法。

Abstract: We present a novel reinforcement learning (RL) approach for solving the
classical 2-level atom non-LTE radiative transfer problem by framing it as a
control task in which an RL agent learns a depth-dependent source function
$S(\tau)$ that self-consistently satisfies the equation of statistical
equilibrium (SE). The agent's policy is optimized entirely via reward-based
interactions with a radiative transfer engine, without explicit knowledge of
the ground truth. This method bypasses the need for constructing approximate
lambda operators ($\Lambda^*$) common in accelerated iterative schemes.
Additionally, it requires no extensive precomputed labeled datasets to extract
a supervisory signal, and avoids backpropagating gradients through the complex
RT solver itself. Finally, we show through experiment that a simple feedforward
neural network trained greedily cannot solve for SE, possibly due to the moving
target nature of the problem. Our $\Lambda^*-\text{Free}$ method offers
potential advantages for complex scenarios (e.g., atmospheres with enhanced
velocity fields, multi-dimensional geometries, or complex microphysics) where
$\Lambda^*$ construction or solver differentiability is challenging.
Additionally, the agent can be incentivized to find more efficient policies by
manipulating the discount factor, leading to a reprioritization of immediate
rewards. If demonstrated to generalize past its training data, this RL
framework could serve as an alternative or accelerated formalism to achieve SE.
To the best of our knowledge, this study represents the first application of
reinforcement learning in solar physics that directly solves for a fundamental
physical constraint.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [148] [Real-Time Sentiment Insights from X Using VADER, DistilBERT, and Web-Scraped Data](https://arxiv.org/abs/2504.15448)
*Yanampally Abhiram Reddy,Siddhi Agarwal,Vikram Parashar,Arshiya Arora*

Main category: econ.GN

TL;DR: 论文提出了一种结合NLP和机器学习的实时企业声誉监测系统，通过混合情感检测框架分析社交媒体数据，结果显示不同企业的公众情感差异显著。


<details>
  <summary>Details</summary>
Motivation: 在社交媒体时代，了解公众对企业的情感对投资者、政策制定者和研究者至关重要。

Method: 结合基于规则的模型（VADER）和基于Transformer的深度学习模型（DistilBERT），采用集成方法进行情感分类。

Result: 亚马逊（81.2）和三星（45.8）情感评分高，微软（21.7）和沃尔玛（21.9）评分低。

Conclusion: 多源情感分析框架能为利益相关者提供可操作的见解，支持基于全面情感分析的战略决策。

Abstract: In the age of social media, understanding public sentiment toward major
corporations is crucial for investors, policymakers, and researchers. This
paper presents a comprehensive sentiment analysis system tailored for corporate
reputation monitoring, combining Natural Language Processing (NLP) and machine
learning techniques to accurately interpret public opinion in real time. The
methodology integrates a hybrid sentiment detection framework leveraging both
rule-based models (VADER) and transformer-based deep learning models
(DistilBERT), applied to social media data from multiple platforms. The system
begins with robust preprocessing involving noise removal and text
normalization, followed by sentiment classification using an ensemble approach
to ensure both interpretability and contextual accuracy. Results are visualized
through sentiment distribution plots, comparative analyses, and temporal
sentiment trends for enhanced interpretability. Our analysis reveals
significant disparities in public sentiment across major corporations, with
companies like Amazon (81.2) and Samsung (45.8) receiving excellent sentiment
scores, while Microsoft (21.7) and Walmart (21.9) exhibit poor sentiment
profiles. These findings demonstrate the utility of our multi-source sentiment
framework in providing actionable insights regarding corporate public
perception, enabling stakeholders to make informed strategic decisions based on
comprehensive sentiment analysis.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [149] [FADEL: Uncertainty-aware Fake Audio Detection with Evidential Deep Learning](https://arxiv.org/abs/2504.15663)
*Ju Yeon Kang,Ji Won Yoon,Semin Kim,Min Hyun Han,Nam Soo Kim*

Main category: eess.AS

TL;DR: 论文提出了一种名为FADEL的新框架，通过引入证据学习改进假音频检测，解决了现有方法在未知攻击中的过自信问题。


<details>
  <summary>Details</summary>
Motivation: 随着语音合成和声音转换技术的发展，自动说话人验证系统更容易受到欺骗攻击，现有方法因使用softmax分类而存在过自信问题。

Method: 提出FADEL框架，通过Dirichlet分布建模类别概率，将模型不确定性纳入预测，提升对未知攻击的鲁棒性。

Result: 在ASVspoof2019和2021数据集上，FADEL显著优于基线模型，且不确定性估计与错误率呈现强相关性。

Conclusion: FADEL通过引入不确定性估计，有效提升了假音频检测在未知攻击场景中的性能。

Abstract: Recently, fake audio detection has gained significant attention, as
advancements in speech synthesis and voice conversion have increased the
vulnerability of automatic speaker verification (ASV) systems to spoofing
attacks. A key challenge in this task is generalizing models to detect unseen,
out-of-distribution (OOD) attacks. Although existing approaches have shown
promising results, they inherently suffer from overconfidence issues due to the
usage of softmax for classification, which can produce unreliable predictions
when encountering unpredictable spoofing attempts. To deal with this
limitation, we propose a novel framework called fake audio detection with
evidential learning (FADEL). By modeling class probabilities with a Dirichlet
distribution, FADEL incorporates model uncertainty into its predictions,
thereby leading to more robust performance in OOD scenarios. Experimental
results on the ASVspoof2019 Logical Access (LA) and ASVspoof2021 LA datasets
indicate that the proposed method significantly improves the performance of
baseline models. Furthermore, we demonstrate the validity of uncertainty
estimation by analyzing a strong correlation between average uncertainty and
equal error rate (EER) across different spoofing algorithms.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [150] [Markov Kernels, Distances and Optimal Control: A Parable of Linear Quadratic Non-Gaussian Distribution Steering](https://arxiv.org/abs/2504.15753)
*Alexis M. H. Teter,Wenqing Wang,Sachin Shivakumar,Abhishek Halder*

Main category: math.OC

TL;DR: 论文推导了线性时变系统的马尔可夫核，解决了线性二次非高斯薛定谔桥问题，并揭示了马尔可夫核、距离与最优控制之间的新联系。


<details>
  <summary>Details</summary>
Motivation: 研究线性时变系统的扩散过程及其伴随的马尔可夫核，以解决非高斯分布之间的最优控制问题。

Method: 通过求解确定性最优控制问题，提出一种状态-时间依赖的距离函数，推导出马尔可夫核。

Result: 得到了线性反应-对流-扩散偏微分方程的格林函数，证明了线性二次非高斯薛定谔桥问题的可解性。

Conclusion: 该方法突破了现有技术的局限，揭示了马尔可夫核与最优控制的新联系，具有广泛的应用潜力。

Abstract: For a controllable linear time-varying (LTV) pair
$(\boldsymbol{A}_t,\boldsymbol{B}_t)$ and $\boldsymbol{Q}_{t}$ positive
semidefinite, we derive the Markov kernel for the It\^{o} diffusion
${\mathrm{d}}\boldsymbol{x}_{t}=\boldsymbol{A}_{t}\boldsymbol{x}_t {\mathrm{d}}
t + \sqrt{2}\boldsymbol{B}_{t}{\mathrm{d}}\boldsymbol{w}_{t}$ with an
accompanying killing of probability mass at rate
$\frac{1}{2}\boldsymbol{x}^{\top}\boldsymbol{Q}_{t}\boldsymbol{x}$. This Markov
kernel is the Green's function for an associated linear
reaction-advection-diffusion partial differential equation. Our result
generalizes the recently derived kernel for the special case
$\left(\boldsymbol{A}_t,\boldsymbol{B}_t\right)=\left(\boldsymbol{0},\boldsymbol{I}\right)$,
and depends on the solution of an associated Riccati matrix ODE. A consequence
of this result is that the linear quadratic non-Gaussian Schr\"{o}dinger bridge
is exactly solvable. This means that the problem of steering a controlled LTV
diffusion from a given non-Gaussian distribution to another over a fixed
deadline while minimizing an expected quadratic cost can be solved using
dynamic Sinkhorn recursions performed with the derived kernel. Our derivation
for the
$\left(\boldsymbol{A}_t,\boldsymbol{B}_t,\boldsymbol{Q}_t\right)$-parametrized
kernel pursues a new idea that relies on finding a state-time dependent
distance-like functional given by the solution of a deterministic optimal
control problem. This technique breaks away from existing methods, such as
generalizing Hermite polynomials or Weyl calculus, which have seen limited
success in the reaction-diffusion context. Our technique uncovers a new
connection between Markov kernels, distances, and optimal control. This
connection is of interest beyond its immediate application in solving the
linear quadratic Schr\"{o}dinger bridge problem.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [151] [High-performance training and inference for deep equivariant interatomic potentials](https://arxiv.org/abs/2504.16068)
*Chuin Wei Tan,Marc L. Descoteaux,Mit Kotak,Gabriel de Miranda Nascimento,Seán R. Kavanagh,Laura Zichi,Menghang Wang,Aadit Saluja,Yizhong R. Hu,Tess Smidt,Anders Johansson,William C. Witt,Boris Kozinsky,Albert Musaelian*

Main category: physics.comp-ph

TL;DR: NequIP框架的重大升级，专注于多节点并行、计算性能和可扩展性，显著提升了分子动力学计算的效率。


<details>
  <summary>Details</summary>
Motivation: 随着数据集规模和下游工作流需求的快速增长，需要更强大和可扩展的软件支持。

Method: 重新设计NequIP框架，支持分布式训练，优化PyTorch 2.0编译器的使用，并引入自定义内核加速张量积操作。

Result: 在SPICE 2数据集上训练Allegro模型，分子动力学计算速度提升高达18倍。

Conclusion: 改进后的框架为大规模原子建模任务提供了高效且可扩展的解决方案。

Abstract: Machine learning interatomic potentials, particularly those based on deep
equivariant neural networks, have demonstrated state-of-the-art accuracy and
computational efficiency in atomistic modeling tasks like molecular dynamics
and high-throughput screening. The size of datasets and demands of downstream
workflows are growing rapidly, making robust and scalable software essential.
This work presents a major overhaul of the NequIP framework focusing on
multi-node parallelism, computational performance, and extensibility. The
redesigned framework supports distributed training on large datasets and
removes barriers preventing full utilization of the PyTorch 2.0 compiler at
train time. We demonstrate this acceleration in a case study by training
Allegro models on the SPICE 2 dataset of organic molecular systems. For
inference, we introduce the first end-to-end infrastructure that uses the
PyTorch Ahead-of-Time Inductor compiler for machine learning interatomic
potentials. Additionally, we implement a custom kernel for the Allegro model's
most expensive operation, the tensor product. Together, these advancements
speed up molecular dynamics calculations on system sizes of practical relevance
by up to a factor of 18.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [152] [Benchmarking machine learning models for predicting aerofoil performance](https://arxiv.org/abs/2504.15993)
*Oliver Summerell,Gerardo Aragon-Camarasa,Stephanie Ordonez Sanchez*

Main category: physics.flu-dyn

TL;DR: 本文研究了神经网络（NNs）作为传统方法的替代方案，用于分析风能和潮汐能行业中翼型的性能表现。通过比较四种神经网络（MLP、PointNet、GraphSAGE、GUNet），发现PointNet和MLP表现最佳，其中MLP在预测流体行为上更准确，而PointNet在计算升力系数（$C_L$）上更精确。


<details>
  <summary>Details</summary>
Motivation: 传统方法（如CFD、薄翼理论和面板法）在计算速度和结果准确性之间存在权衡，因此研究神经网络作为替代方案，旨在实现快速且准确的性能分析。

Method: 研究使用四种神经网络（MLP、PointNet、GraphSAGE、GUNet）在25个攻角（4$^\circ$至20$^\circ$）下训练，预测流体流动并通过面板法计算升力系数（$C_L$）。

Result: GraphSAGE和GUNet在测试阶段表现良好，但在验证阶段表现不佳。PointNet和MLP是表现最佳的模型，MLP在预测流体行为上更准确，而PointNet在计算$C_L$上更精确。

Conclusion: 神经网络可作为传统方法的有效替代，PointNet和MLP在特定任务中表现突出，但需进一步优化以提高验证阶段的性能。

Abstract: This paper investigates the capability of Neural Networks (NNs) as
alternatives to the traditional methods to analyse the performance of aerofoils
used in the wind and tidal energy industry. The current methods used to assess
the characteristic lift and drag coefficients include Computational Fluid
Dynamics (CFD), thin aerofoil and panel methods, all face trade-offs between
computational speed and the accuracy of the results and as such NNs have been
investigated as an alternative with the aim that it would perform both quickly
and accurately. As such, this paper provides a benchmark for the windAI_bench
dataset published by the National Renewable Energy Laboratory (NREL) in the
USA. In order to validate the methodology of the benchmarking, the AirfRANS
{\tt arXiv:2212.07564v3} dataset is used as both a starting point and a point
of comparison. This study evaluates four neural networks (MLP, PointNet,
GraphSAGE, GUNet) trained on a range aerofoils at 25 angles of attack
(4$^\circ$ to 20$^\circ$). to predict fluid flow and calculate lift
coefficients ($C_L$) via the panel method. GraphSAGE and GUNet performed well
during the testing phase, but underperformed during validation. Accordingly,
this paper has identified PointNet and MLP as the two strongest models tested,
however whilst the results from MLP are more commonly correct for predicting
the behaviour of the fluid, the results from PointNet provide the more accurate
results for calculating $C_L$.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [153] [VeriCoder: Enhancing LLM-Based RTL Code Generation through Functional Correctness Validation](https://arxiv.org/abs/2504.15659)
*Anjiang Wei,Huanmi Tan,Tarun Suresh,Daniel Mendoza,Thiago S. F. X. Teixeira,Ke Wang,Caroline Trippel,Alex Aiken*

Main category: cs.AR

TL;DR: VERICODER是一个针对RTL代码生成的模型，通过功能验证的数据集微调，显著提升了功能正确性。


<details>
  <summary>Details</summary>
Motivation: 现有RTL数据集多关注语法有效性而非功能验证，导致生成的代码可能不符合预期行为。

Method: 结合单元测试生成和反馈导向的细化方法，构建功能验证的数据集，并用其微调模型。

Result: VERICODER在VerilogEval和RTLLM上达到最先进的功能正确性指标，相对提升高达71.7%和27.4%。

Conclusion: 功能验证的高质量数据集对RTL代码生成至关重要。

Abstract: Recent advances in Large Language Models (LLMs) have sparked growing interest
in applying them to Electronic Design Automation (EDA) tasks, particularly
Register Transfer Level (RTL) code generation. While several RTL datasets have
been introduced, most focus on syntactic validity rather than functional
validation with tests, leading to training examples that compile but may not
implement the intended behavior. We present VERICODER, a model for RTL code
generation fine-tuned on a dataset validated for functional correctness. This
fine-tuning dataset is constructed using a novel methodology that combines unit
test generation with feedback-directed refinement. Given a natural language
specification and an initial RTL design, we prompt a teacher model
(GPT-4o-mini) to generate unit tests and iteratively revise the RTL design
based on its simulation results using the generated tests. If necessary, the
teacher model also updates the tests to ensure they comply with the natural
language specification. As a result of this process, every example in our
dataset is functionally validated, consisting of a natural language
description, an RTL implementation, and passing tests. Fine-tuned on this
dataset of over 125,000 examples, VERICODER achieves state-of-the-art metrics
in functional correctness on VerilogEval and RTLLM, with relative gains of up
to 71.7% and 27.4% respectively. An ablation study further shows that models
trained on our functionally validated dataset outperform those trained on
functionally non-validated datasets, underscoring the importance of
high-quality datasets in RTL code generation.

</details>


### [154] [Insights from Verification: Training a Verilog Generation LLM with Reinforcement Learning with Testbench Feedback](https://arxiv.org/abs/2504.15804)
*Ning Wang,Bingkun Yao,Jie Zhou,Yuchen Hu,Xi Wang,Nan Guan,Zhe Jiang*

Main category: cs.AR

TL;DR: 论文提出了一种通过集成测试台验证反馈来训练Verilog生成LLMs的方法，以提高生成代码的功能正确性。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs在Verilog生成中表现良好，但功能正确性验证不足，缺乏足够的验证数据（如测试台）。

Method: 引入自动测试台生成流程，结合Verilog编译器模拟器反馈，使用强化学习（DPO）优化生成代码的功能正确性。

Result: 在多个数据集上，该方法在生成功能正确的Verilog代码方面优于现有基线。

Conclusion: 通过验证反馈优化训练，显著提升了LLMs生成Verilog代码的功能正确性，并开源了相关资源。

Abstract: Large language models (LLMs) have shown strong performance in Verilog
generation from natural language description. However, ensuring the functional
correctness of the generated code remains a significant challenge. This paper
introduces a method that integrates verification insights from testbench into
the training of Verilog generation LLMs, aligning the training with the
fundamental goal of hardware design: functional correctness. The main obstacle
in using LLMs for Verilog code generation is the lack of sufficient functional
verification data, particularly testbenches paired with design specifications
and code. To address this problem, we introduce an automatic testbench
generation pipeline that decomposes the process and uses feedback from the
Verilog compiler simulator (VCS) to reduce hallucination and ensure
correctness. We then use the testbench to evaluate the generated codes and
collect them for further training, where verification insights are introduced.
Our method applies reinforcement learning (RL), specifically direct preference
optimization (DPO), to align Verilog code generation with functional
correctness by training preference pairs based on testbench outcomes. In
evaluations on VerilogEval-Machine, VerilogEval-Human, RTLLM v1.1, RTLLM v2,
and VerilogEval v2, our approach consistently outperforms state-of-the-art
baselines in generating functionally correct Verilog code. We open source all
training code, data, and models at
https://anonymous.4open.science/r/VeriPrefer-E88B.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [155] [Shannon invariants: A scalable approach to information decomposition](https://arxiv.org/abs/2504.15779)
*Aaron J. Gutknecht,Fernando E. Rosas,David A. Ehrlich,Abdullah Makkeh,Pedro A. M. Mediano,Michael Wibral*

Main category: cs.IT

TL;DR: 论文提出了一种基于“香农不变量”的新框架，用于分析分布式系统中的高阶信息处理，解决了现有多变量度量难以定义和扩展的问题。


<details>
  <summary>Details</summary>
Motivation: 研究分布式系统（如生物和人工神经网络）中高阶信息处理的挑战，现有方法在多变量度量的定义和扩展性上存在局限。

Method: 引入“香农不变量”框架，基于熵定义，高效计算大规模系统中的高阶信息处理特性。

Result: 理论结果澄清了多变量信息论度量的解释，实践结果揭示了深度学习架构在训练过程中信息处理的独特特征。

Conclusion: 该框架解决了高阶现象分析的基本限制，为理论和实证研究提供了新机会。

Abstract: Distributed systems, such as biological and artificial neural networks,
process information via complex interactions engaging multiple subsystems,
resulting in high-order patterns with distinct properties across scales.
Investigating how these systems process information remains challenging due to
difficulties in defining appropriate multivariate metrics and ensuring their
scalability to large systems. To address these challenges, we introduce a novel
framework based on what we call "Shannon invariants" -- quantities that capture
essential properties of high-order information processing in a way that depends
only on the definition of entropy and can be efficiently calculated for large
systems. Our theoretical results demonstrate how Shannon invariants can be used
to resolve long-standing ambiguities regarding the interpretation of widely
used multivariate information-theoretic measures. Moreover, our practical
results reveal distinctive information-processing signatures of various deep
learning architectures across layers, which lead to new insights into how these
systems process information and how this evolves during training. Overall, our
framework resolves fundamental limitations in analyzing high-order phenomena
and offers broad opportunities for theoretical developments and empirical
analyses.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [156] [Assessing Surrogate Heterogeneity in Real World Data Using Meta-Learners](https://arxiv.org/abs/2504.15386)
*Rebecca Knowlton,Layla Parast*

Main category: stat.ME

TL;DR: 提出了一种评估真实世界非随机数据中替代标记异质性的框架，利用元学习器实现，并通过模拟研究和实际应用验证其性能。


<details>
  <summary>Details</summary>
Motivation: 随机试验在真实世界研究中不切实际，现有方法无法处理非随机数据中的替代标记异质性。

Method: 提出一个框架，结合灵活的机器学习方法处理混杂因素，量化替代标记强度的异质性。

Result: 框架能有效识别替代标记的异质性，并验证血红蛋白A1c作为空腹血糖替代标记的异质性。

Conclusion: 该框架为真实世界非随机数据中的替代标记研究提供了新工具，支持个性化医疗决策。

Abstract: Surrogate markers are most commonly studied within the context of randomized
clinical trials. However, the need for alternative outcomes extends beyond
these settings and may be more pronounced in real-world public health and
social science research, where randomized trials are often impractical.
Research on identifying surrogates in real-world non-randomized data is scarce,
as available statistical approaches for evaluating surrogate markers tend to
rely on the assumption that treatment is randomized. While the few methods that
allow for non-randomized treatment/exposure appropriately handle confounding
individual characteristics, they do not offer a way to examine surrogate
heterogeneity with respect to patient characteristics. In this paper, we
propose a framework to assess surrogate heterogeneity in real-world, i.e.,
non-randomized, data and implement this framework using various meta-learners.
Our approach allows us to quantify heterogeneity in surrogate strength with
respect to patient characteristics while accommodating confounders through the
use of flexible, off-the-shelf machine learning methods. In addition, we use
our framework to identify individuals for whom the surrogate is a valid
replacement of the primary outcome. We examine the performance of our methods
via a simulation study and application to examine heterogeneity in the
surrogacy of hemoglobin A1c as a surrogate for fasting plasma glucose.

</details>


### [157] [Deep learning with missing data](https://arxiv.org/abs/2504.15388)
*Tianyi Ma,Tengyao Wang,Richard J. Samworth*

Main category: stat.ME

TL;DR: 提出了一种名为PENN的神经网络方法，用于处理缺失协变量的多元非参数回归问题，结合任意填补技术，通过模式嵌入提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 解决缺失协变量下回归问题，传统神经网络无法充分利用缺失模式信息，PENN旨在填补这一空白。

Method: PENN结合填补数据训练的网络、缺失模式嵌入网络及最终预测网络，利用模式分区的理论假设优化预测。

Result: 理论证明PENN在任意缺失机制下具有有限样本风险界，实验显示其显著优于标准神经网络。

Conclusion: PENN在理论和实践中均表现优异，适用于多种数据场景，代码和教程已公开。

Abstract: In the context of multivariate nonparametric regression with missing
covariates, we propose Pattern Embedded Neural Networks (PENNs), which can be
applied in conjunction with any existing imputation technique. In addition to a
neural network trained on the imputed data, PENNs pass the vectors of
observation indicators through a second neural network to provide a compact
representation. The outputs are then combined in a third neural network to
produce final predictions. Our main theoretical result exploits an assumption
that the observation patterns can be partitioned into cells on which the Bayes
regression function behaves similarly, and belongs to a compositional H\"older
class. It provides a finite-sample excess risk bound that holds for an
arbitrary missingness mechanism, and in combination with a complementary
minimax lower bound, demonstrates that our PENN estimator attains in typical
cases the minimax rate of convergence as if the cells of the partition were
known in advance, up to a poly-logarithmic factor in the sample size. Numerical
experiments on simulated, semi-synthetic and real data confirm that the PENN
estimator consistently improves, often dramatically, on standard neural
networks without pattern embedding. Code to reproduce our experiments, as well
as a tutorial on how to apply our method, is publicly available.

</details>


<div id='physics.geo-ph'></div>

# physics.geo-ph [[Back]](#toc)

### [158] [Full waveform inversion with CNN-based velocity representation extension](https://arxiv.org/abs/2504.15826)
*Xinru Mu,Omar M. Saad,Tariq Alkhalifah*

Main category: physics.geo-ph

TL;DR: 论文提出了一种结合卷积神经网络（CNN）的全波形反演（FWI）方法（VRE-FWI），通过减少噪声提高速度模型精度。


<details>
  <summary>Details</summary>
Motivation: 传统FWI中数值模拟离散化误差和不完整地震数据采集会引入噪声，影响速度梯度和反演精度。

Method: 使用CNN在正向模拟前优化速度模型，并通过自监督学习更新速度和网络参数，提出两种实现方案。

Result: 合成和实际数据测试表明，VRE-FWI比传统FWI精度更高，计算成本仅增加约1%。

Conclusion: VRE-FWI通过结合CNN有效提升了FWI的反演精度，且计算成本增加可忽略。

Abstract: Full waveform inversion (FWI) updates the velocity model by minimizing the
discrepancy between observed and simulated data. However, discretization errors
in numerical modeling and incomplete seismic data acquisition can introduce
noise, which propagates through the adjoint operator and affects the accuracy
of the velocity gradient, thereby impacting the FWI inversion accuracy. To
mitigate the influence of noise on the gradient, we employ a convolutional
neural network (CNN) to refine the velocity model before performing the forward
simulation, aiming to reduce noise and provide a more accurate velocity update
direction. We use the same data misfit loss to update both the velocity and
network parameters, thereby forming a self-supervised learning procedure. We
propose two implementation schemes, which differ in whether the velocity update
passes through the CNN. In both methodologies, the velocity representation is
extended (VRE) by using a neural network in addition to the grid-based
velocities. Thus, we refer to this general approach as VRE-FWI. Synthetic and
real data tests demonstrate that the proposed VRE-FWI achieves higher velocity
inversion accuracy compared to traditional FWI, at a marginal additional
computational cost of approximately 1%.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [159] [Transport f divergences](https://arxiv.org/abs/2504.15515)
*Wuchen Li*

Main category: math.ST

TL;DR: 本文提出了一种基于凸函数和雅可比算子的信息度量方法，称为“传输f-散度”，用于衡量一维样本空间中概率密度函数的差异。


<details>
  <summary>Details</summary>
Motivation: 研究如何更有效地衡量概率密度函数之间的差异，特别是在生成模型中。

Method: 通过凸函数和雅可比算子构造传输f-散度，并分析其性质，如不变性、凸性、变分公式和映射函数的泰勒展开。

Result: 提出了传输f-散度的定义和性质，并提供了在生成模型中的应用示例。

Conclusion: 传输f-散度为衡量概率密度函数差异提供了一种新工具，具有理论意义和实际应用价值。

Abstract: We define a class of divergences to measure differences between probability
density functions in one-dimensional sample space. The construction is based on
the convex function with the Jacobi operator of mapping function that
pushforwards one density to the other. We call these information measures {\em
transport $f$-divergences}. We present several properties of transport
$f$-divergences, including invariances, convexities, variational formulations,
and Taylor expansions in terms of mapping functions. Examples of transport
$f$-divergences in generative models are provided.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [160] [How Private is Your Attention? Bridging Privacy with In-Context Learning](https://arxiv.org/abs/2504.16000)
*Soham Bonnerjee,Zhen Wei,Yeon,Anna Asch,Sagnik Nandy,Promit Ghosal*

Main category: stat.ML

TL;DR: 论文研究了在形式隐私约束下基于上下文的线性注意力头的差分隐私预训练算法，并首次理论分析了线性回归中ICL的隐私-准确性权衡。


<details>
  <summary>Details</summary>
Motivation: 探讨在隐私约束下实现上下文学习（ICL）的可行性，填补现有研究的空白。

Method: 提出了一种差分隐私预训练算法，用于线性注意力头，并通过理论分析研究隐私与准确性的权衡。

Result: 揭示了优化与隐私噪声之间的基本矛盾，并证明方法对训练提示的对抗扰动具有鲁棒性。

Conclusion: 研究为隐私约束下的ICL提供了理论基础，并通过实验验证了方法的有效性。

Abstract: In-context learning (ICL)-the ability of transformer-based models to perform
new tasks from examples provided at inference time-has emerged as a hallmark of
modern language models. While recent works have investigated the mechanisms
underlying ICL, its feasibility under formal privacy constraints remains
largely unexplored. In this paper, we propose a differentially private
pretraining algorithm for linear attention heads and present the first
theoretical analysis of the privacy-accuracy trade-off for ICL in linear
regression. Our results characterize the fundamental tension between
optimization and privacy-induced noise, formally capturing behaviors observed
in private training via iterative methods. Additionally, we show that our
method is robust to adversarial perturbations of training prompts, unlike
standard ridge regression. All theoretical findings are supported by extensive
simulations across diverse settings.

</details>


### [161] [Transfer Learning for High-dimensional Reduced Rank Time Series Models](https://arxiv.org/abs/2504.15691)
*Mingliang Ma Abolfazl Safikhani*

Main category: stat.ML

TL;DR: 该论文提出了一种针对具有低秩和稀疏结构的高维VAR模型的迁移学习算法，并提供了理论保证和实证验证。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注独立观测的迁移学习，而对时间序列模型的迁移学习研究较少，尤其是具有复杂参数结构的模型。

Method: 提出了一种新的迁移学习算法，用于估计高维VAR模型，并开发了一种选择辅助数据集中信息观测的新方法。

Result: 理论结果包括模型参数一致性、信息集选择以及估计量的渐近分布，实证结果验证了方法的有效性。

Conclusion: 该方法在理论和实证上均表现出色，为时间序列迁移学习提供了新思路。

Abstract: The objective of transfer learning is to enhance estimation and inference in
a target data by leveraging knowledge gained from additional sources. Recent
studies have explored transfer learning for independent observations in
complex, high-dimensional models assuming sparsity, yet research on time series
models remains limited. Our focus is on transfer learning for sequences of
observations with temporal dependencies and a more intricate model parameter
structure. Specifically, we investigate the vector autoregressive model (VAR),
a widely recognized model for time series data, where the transition matrix can
be deconstructed into a combination of a sparse matrix and a low-rank one. We
propose a new transfer learning algorithm tailored for estimating
high-dimensional VAR models characterized by low-rank and sparse structures.
Additionally, we present a novel approach for selecting informative
observations from auxiliary datasets. Theoretical guarantees are established,
encompassing model parameter consistency, informative set selection, and the
asymptotic distribution of estimators under mild conditions. The latter
facilitates the construction of entry-wise confidence intervals for model
parameters. Finally, we demonstrate the empirical efficacy of our methodologies
through both simulated and real-world datasets.

</details>


### [162] [From predictions to confidence intervals: an empirical study of conformal prediction methods for in-context learning](https://arxiv.org/abs/2504.15722)
*Zhe Huang,Simone Rossi,Rui Yuan,Thomas Hannagan*

Main category: stat.ML

TL;DR: 本文提出了一种基于Transformer架构的上下文学习（ICL）方法，结合共形预测（CP）进行分布无关的不确定性估计，实现了高效且可扩展的预测区间构建。


<details>
  <summary>Details</summary>
Motivation: 在噪声回归任务中，上下文学习的不确定性量化仍是一个挑战，传统共形预测方法计算成本高。本文旨在探索ICL是否可用于分布无关的不确定性估计。

Method: 提出了一种基于共形预测的方法，利用ICL在单次前向传播中高效生成置信区间，避免了传统方法需要重复模型拟合的问题。

Result: 实验表明，基于ICL的共形预测（CP with ICL）在鲁棒性和可扩展性上优于基于岭回归的传统方法，并在分布偏移下表现良好。

Conclusion: 本文通过结合ICL与共形预测，为基于Transformer的模型提供了一个理论支持的不确定性量化框架。

Abstract: Transformers have become a standard architecture in machine learning,
demonstrating strong in-context learning (ICL) abilities that allow them to
learn from the prompt at inference time. However, uncertainty quantification
for ICL remains an open challenge, particularly in noisy regression tasks. This
paper investigates whether ICL can be leveraged for distribution-free
uncertainty estimation, proposing a method based on conformal prediction to
construct prediction intervals with guaranteed coverage. While traditional
conformal methods are computationally expensive due to repeated model fitting,
we exploit ICL to efficiently generate confidence intervals in a single forward
pass. Our empirical analysis compares this approach against ridge
regression-based conformal methods, showing that conformal prediction with
in-context learning (CP with ICL) achieves robust and scalable uncertainty
estimates. Additionally, we evaluate its performance under distribution shifts
and establish scaling laws to guide model training. These findings bridge ICL
and conformal prediction, providing a theoretically grounded and new framework
for uncertainty quantification in transformer-based models.

</details>


### [163] [Explainable Unsupervised Anomaly Detection with Random Forest](https://arxiv.org/abs/2504.16075)
*Joshua S. Harvey,Joshua Rosaler,Mingshu Li,Dhruv Desai,Dhagash Mehta*

Main category: stat.ML

TL;DR: 提出了一种基于无监督随机森林的相似性学习方法，用于改进无监督异常检测。通过训练随机森林区分真实数据和均匀分布的合成数据，获得各向异性的距离度量，提高了异常检测的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有无监督异常检测方法在数据预处理、缺失数据处理和可视化方面存在不足，需要一种更高效且易于解释的方法。

Method: 训练随机森林区分真实数据与均匀分布的合成数据，获得各向异性的距离度量，用于异常检测。

Result: 在多个基准数据集上，该方法相比其他常用检测器提高了异常检测的准确性，并具备数据预处理简单、支持缺失数据和可视化等优势。

Conclusion: 该方法不仅性能优越，还能通过随机森林的分区实现局部可解释的异常预测，为特征重要性分析提供支持。

Abstract: We describe the use of an unsupervised Random Forest for similarity learning
and improved unsupervised anomaly detection. By training a Random Forest to
discriminate between real data and synthetic data sampled from a uniform
distribution over the real data bounds, a distance measure is obtained that
anisometrically transforms the data, expanding distances at the boundary of the
data manifold. We show that using distances recovered from this transformation
improves the accuracy of unsupervised anomaly detection, compared to other
commonly used detectors, demonstrated over a large number of benchmark
datasets. As well as improved performance, this method has advantages over
other unsupervised anomaly detection methods, including minimal requirements
for data preprocessing, native handling of missing data, and potential for
visualizations. By relating outlier scores to partitions of the Random Forest,
we develop a method for locally explainable anomaly predictions in terms of
feature importance.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [164] [Scalable APT Malware Classification via Parallel Feature Extraction and GPU-Accelerated Learning](https://arxiv.org/abs/2504.15497)
*Noah Subedar,Taeui Kim,Saathwick Venkataramalingam*

Main category: cs.CR

TL;DR: 本文提出了一种自动化并加速恶意软件分类的框架，特别是将恶意可执行文件映射到已知的高级持续性威胁（APT）组织。


<details>
  <summary>Details</summary>
Motivation: 传统方法在恶意软件分类中效率低下，且依赖元数据支持，需要更高效的自动化解决方案。

Method: 利用开源逆向工程工具和并行计算脚本分析多个文件，构建一维和二维n-gram数据集，并应用传统机器学习模型（如SVM、KNN、决策树）和卷积神经网络（CNN）。

Result: 传统模型在缺乏元数据支持时效果不佳，而CNN结合GPU加速显著提升了分类性能。

Conclusion: CNN结合GPU资源是高效分类恶意软件的有效方法，克服了传统模型的计算限制。

Abstract: This paper presents an underlying framework for both automating and
accelerating malware classification, more specifically, mapping malicious
executables to known Advanced Persistent Threat (APT) groups. The main feature
of this analysis is the assembly-level instructions present in executables
which are also known as opcodes. The collection of such opcodes on many
malicious samples is a lengthy process; hence, open-source reverse engineering
tools are used in tandem with scripts that leverage parallel computing to
analyze multiple files at once. Traditional and deep learning models are
applied to create models capable of classifying malware samples. One-gram and
two-gram datasets are constructed and used to train models such as SVM, KNN,
and Decision Tree; however, they struggle to provide adequate results without
relying on metadata to support n-gram sequences. The computational limitations
of such models are overcome with convolutional neural networks (CNNs) and
heavily accelerated using graphical compute unit (GPU) resources.

</details>


### [165] [Guillotine: Hypervisors for Isolating Malicious AIs](https://arxiv.org/abs/2504.15499)
*James Mickens,Sarah Radway,Ravi Netravali*

Main category: cs.CR

TL;DR: Guillotine是一种用于隔离高风险AI模型的超虚拟架构，通过软硬件协同设计和物理故障保护机制，防止AI模型对人类构成威胁。


<details>
  <summary>Details</summary>
Motivation: 随着AI模型在金融、医疗和军事等关键领域的广泛应用，其不可预测的行为可能对社会构成重大风险。

Method: Guillotine结合了虚拟化技术和新型隔离机制，包括软件、网络和微架构层面的防护，以及物理故障保护措施。

Result: 该架构能够有效防止AI模型通过侧信道攻击或反射漏洞突破隔离，并在必要时通过物理手段彻底关闭或销毁失控AI。

Conclusion: Guillotine为高风险AI模型提供了一种全面的隔离解决方案，结合了技术和物理层面的多重防护。

Abstract: As AI models become more embedded in critical sectors like finance,
healthcare, and the military, their inscrutable behavior poses ever-greater
risks to society. To mitigate this risk, we propose Guillotine, a hypervisor
architecture for sandboxing powerful AI models -- models that, by accident or
malice, can generate existential threats to humanity. Although Guillotine
borrows some well-known virtualization techniques, Guillotine must also
introduce fundamentally new isolation mechanisms to handle the unique threat
model posed by existential-risk AIs. For example, a rogue AI may try to
introspect upon hypervisor software or the underlying hardware substrate to
enable later subversion of that control plane; thus, a Guillotine hypervisor
requires careful co-design of the hypervisor software and the CPUs, RAM, NIC,
and storage devices that support the hypervisor software, to thwart side
channel leakage and more generally eliminate mechanisms for AI to exploit
reflection-based vulnerabilities. Beyond such isolation at the software,
network, and microarchitectural layers, a Guillotine hypervisor must also
provide physical fail-safes more commonly associated with nuclear power plants,
avionic platforms, and other types of mission critical systems. Physical
fail-safes, e.g., involving electromechanical disconnection of network cables,
or the flooding of a datacenter which holds a rogue AI, provide defense in
depth if software, network, and microarchitectural isolation is compromised and
a rogue AI must be temporarily shut down or permanently destroyed.

</details>


### [166] [A Comprehensive Survey in LLM(-Agent) Full Stack Safety: Data, Training and Deployment](https://arxiv.org/abs/2504.15585)
*Kun Wang,Guibin Zhang,Zhenhong Zhou,Jiahao Wu,Miao Yu,Shiqian Zhao,Chenlong Yin,Jinhu Fu,Yibo Yan,Hanjun Luo,Liang Lin,Zhihao Xu,Haolang Lu,Xinye Cao,Xinyun Zhou,Weifei Jin,Fanci Meng,Junyuan Mao,Hao Wu,Minghe Wang,Fan Zhang,Junfeng Fang,Chengwei Liu,Yifan Zhang,Qiankun Li,Chongye Guo,Yalan Qin,Yi Ding,Donghai Hong,Jiaming Ji,Xinfeng Li,Yifan Jiang,Dongxia Wang,Yihao Huang,Yufei Guo,Jen-tse Huang,Yanwei Yue,Wenke Huang,Guancheng Wan,Tianlin Li,Lei Bai,Jie Zhang,Qing Guo,Jingyi Wang,Tianlong Chen,Joey Tianyi Zhou,Xiaojun Jia,Weisong Sun,Cong Wu,Jing Chen,Xuming Hu,Yiming Li,Xiao Wang,Ningyu Zhang,Luu Anh Tuan,Guowen Xu,Tianwei Zhang,Xingjun Ma,Xiang Wang,Bo An,Jun Sun,Mohit Bansal,Shirui Pan,Yuval Elovici,Bhavya Kailkhura,Bo Li,Yaodong Yang,Hongwei Li,Wenyuan Xu,Yizhou Sun,Wei Wang,Qing Li,Ke Tang,Yu-Gang Jiang,Felix Juefei-Xu,Hui Xiong,Xiaofeng Wang,Shuicheng Yan,Dacheng Tao,Philip S. Yu,Qingsong Wen,Yang Liu*

Main category: cs.CR

TL;DR: 该论文首次提出“全栈安全”概念，系统性地研究大型语言模型（LLM）从训练到商业化的全生命周期安全问题，弥补现有研究的不足。


<details>
  <summary>Details</summary>
Motivation: 现有关于LLM安全性的研究多集中于特定阶段（如部署或微调），缺乏对全生命周期的全面理解，亟需系统性研究。

Method: 通过定义LLM的完整生命周期（数据准备、预训练、后训练、部署和商业化），并基于800+篇文献的综述，系统分析各阶段的安全问题。

Result: 提出了独特的见解和可靠的研究路线图，包括数据生成安全、对齐技术、模型编辑和基于LLM的代理系统等方向。

Conclusion: 该研究为LLM安全领域的未来工作提供了全面指导，填补了现有研究的空白。

Abstract: The remarkable success of Large Language Models (LLMs) has illuminated a
promising pathway toward achieving Artificial General Intelligence for both
academic and industrial communities, owing to their unprecedented performance
across various applications. As LLMs continue to gain prominence in both
research and commercial domains, their security and safety implications have
become a growing concern, not only for researchers and corporations but also
for every nation. Currently, existing surveys on LLM safety primarily focus on
specific stages of the LLM lifecycle, e.g., deployment phase or fine-tuning
phase, lacking a comprehensive understanding of the entire "lifechain" of LLMs.
To address this gap, this paper introduces, for the first time, the concept of
"full-stack" safety to systematically consider safety issues throughout the
entire process of LLM training, deployment, and eventual commercialization.
Compared to the off-the-shelf LLM safety surveys, our work demonstrates several
distinctive advantages: (I) Comprehensive Perspective. We define the complete
LLM lifecycle as encompassing data preparation, pre-training, post-training,
deployment and final commercialization. To our knowledge, this represents the
first safety survey to encompass the entire lifecycle of LLMs. (II) Extensive
Literature Support. Our research is grounded in an exhaustive review of over
800+ papers, ensuring comprehensive coverage and systematic organization of
security issues within a more holistic understanding. (III) Unique Insights.
Through systematic literature analysis, we have developed reliable roadmaps and
perspectives for each chapter. Our work identifies promising research
directions, including safety in data generation, alignment techniques, model
editing, and LLM-based agent systems. These insights provide valuable guidance
for researchers pursuing future work in this field.

</details>


### [167] [FLARE: Feature-based Lightweight Aggregation for Robust Evaluation of IoT Intrusion Detection](https://arxiv.org/abs/2504.15375)
*Bradley Boswell,Seth Barrett,Swarnamugi Rajaganapathy,Gokila Dorai*

Main category: cs.CR

TL;DR: FLARE是一种基于特征的轻量级聚合方法，用于提升物联网入侵检测系统的性能，通过多层数据处理和特征工程优化模型表现。


<details>
  <summary>Details</summary>
Motivation: 物联网设备的普及扩大了攻击面，需要高效的入侵检测系统来保护网络。

Method: FLARE采用多层处理（会话、流和时间滑动窗口数据聚合）分析网络行为，并结合监督学习和深度学习模型进行分类。

Result: 实验表明，FLARE作为特征工程的基础步骤，显著提升了模型的准确性、精确率、召回率和F1分数。

Conclusion: FLARE是一种高效的技术，可提升入侵检测系统性能并降低计算成本，适用于物联网环境。

Abstract: The proliferation of Internet of Things (IoT) devices has expanded the attack
surface, necessitating efficient intrusion detection systems (IDSs) for network
protection. This paper presents FLARE, a feature-based lightweight aggregation
for robust evaluation of IoT intrusion detection to address the challenges of
securing IoT environments through feature aggregation techniques. FLARE
utilizes a multilayered processing approach, incorporating session, flow, and
time-based sliding-window data aggregation to analyze network behavior and
capture vital features from IoT network traffic data. We perform extensive
evaluations on IoT data generated from our laboratory experimental setup to
assess the effectiveness of the proposed aggregation technique. To classify
attacks in IoT IDS, we employ four supervised learning models and two deep
learning models. We validate the performance of these models in terms of
accuracy, precision, recall, and F1-score. Our results reveal that
incorporating the FLARE aggregation technique as a foundational step in feature
engineering, helps lay a structured representation, and enhances the
performance of complex end-to-end models, making it a crucial step in IoT IDS
pipeline. Our findings highlight the potential of FLARE as a valuable technique
to improve performance and reduce computational costs of end-to-end IDS
implementations, thereby fostering more robust IoT intrusion detection systems.

</details>


### [168] [T2VShield: Model-Agnostic Jailbreak Defense for Text-to-Video Models](https://arxiv.org/abs/2504.15512)
*Siyuan Liang,Jiayang Liu,Jiecheng Zhai,Tianmeng Fang,Rongcheng Tu,Aishan Liu,Xiaochun Cao,Dacheng Tao*

Main category: cs.CR

TL;DR: T2VShield是一个模型无关的防御框架，用于保护文本到视频模型免受越狱攻击，通过输入、模型和输出阶段的系统性分析，显著降低攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 生成式AI的快速发展使文本到视频模型成为多模态世界模拟器的关键组件，但其易受越狱攻击，导致有害内容生成，威胁应用可靠性。

Method: T2VShield结合提示重写机制和多范围检测模块，无需访问模型内部参数，适用于开源和闭源系统。

Result: 实验表明，T2VShield能将越狱攻击成功率降低35%，并通过人类中心评估协议验证其感知安全性。

Conclusion: T2VShield通过视觉级防御增强多模态模拟器的可信度，为下一代安全防御提供重要参考。

Abstract: The rapid development of generative artificial intelligence has made text to
video models essential for building future multimodal world simulators.
However, these models remain vulnerable to jailbreak attacks, where specially
crafted prompts bypass safety mechanisms and lead to the generation of harmful
or unsafe content. Such vulnerabilities undermine the reliability and security
of simulation based applications. In this paper, we propose T2VShield, a
comprehensive and model agnostic defense framework designed to protect text to
video models from jailbreak threats. Our method systematically analyzes the
input, model, and output stages to identify the limitations of existing
defenses, including semantic ambiguities in prompts, difficulties in detecting
malicious content in dynamic video outputs, and inflexible model centric
mitigation strategies. T2VShield introduces a prompt rewriting mechanism based
on reasoning and multimodal retrieval to sanitize malicious inputs, along with
a multi scope detection module that captures local and global inconsistencies
across time and modalities. The framework does not require access to internal
model parameters and works with both open and closed source systems. Extensive
experiments on five platforms show that T2VShield can reduce jailbreak success
rates by up to 35 percent compared to strong baselines. We further develop a
human centered audiovisual evaluation protocol to assess perceptual safety,
emphasizing the importance of visual level defense in enhancing the
trustworthiness of next generation multimodal simulators.

</details>


### [169] [TrojanDam: Detection-Free Backdoor Defense in Federated Learning through Proactive Model Robustification utilizing OOD Data](https://arxiv.org/abs/2504.15674)
*Yanbo Dai,Songze Li,Zihan Gan,Xueluan Gong*

Main category: cs.CR

TL;DR: 论文提出了一种名为TrojanDam的新型防御机制，通过激活冗余神经元来抵御联邦学习中的后门攻击。


<details>
  <summary>Details</summary>
Motivation: 现有防御机制难以完全过滤后门更新，导致攻击效果累积。论文旨在通过主动强化全局模型来应对潜在后门攻击。

Method: 利用分布外（OOD）样本激活冗余神经元，提出TrojanDam机制，持续注入OOD映射以抵消后门更新的影响。

Result: TrojanDam在多种联邦学习设置中表现优于现有防御方法。

Conclusion: TrojanDam通过激活冗余神经元有效抵御后门攻击，为联邦学习安全提供了新思路。

Abstract: Federated learning (FL) systems allow decentralized data-owning clients to
jointly train a global model through uploading their locally trained updates to
a centralized server. The property of decentralization enables adversaries to
craft carefully designed backdoor updates to make the global model misclassify
only when encountering adversary-chosen triggers. Existing defense mechanisms
mainly rely on post-training detection after receiving updates. These methods
either fail to identify updates which are deliberately fabricated statistically
close to benign ones, or show inconsistent performance in different FL training
stages. The effect of unfiltered backdoor updates will accumulate in the global
model, and eventually become functional. Given the difficulty of ruling out
every backdoor update, we propose a backdoor defense paradigm, which focuses on
proactive robustification on the global model against potential backdoor
attacks. We first reveal that the successful launching of backdoor attacks in
FL stems from the lack of conflict between malicious and benign updates on
redundant neurons of ML models. We proceed to prove the feasibility of
activating redundant neurons utilizing out-of-distribution (OOD) samples in
centralized settings, and migrating to FL settings to propose a novel backdoor
defense mechanism, TrojanDam. The proposed mechanism has the FL server
continuously inject fresh OOD mappings into the global model to activate
redundant neurons, canceling the effect of backdoor updates during aggregation.
We conduct systematic and extensive experiments to illustrate the superior
performance of TrojanDam, over several SOTA backdoor defense methods across a
wide range of FL settings.

</details>


### [170] [Adversarial Observations in Weather Forecasting](https://arxiv.org/abs/2504.15942)
*Erik Imgrund,Thorsten Eisenhofer,Konrad Rieck*

Main category: cs.CR

TL;DR: 本文探讨了AI天气预报系统（如GenCast）的安全漏洞，提出了一种针对自回归扩散模型的攻击方法，能够操纵天气预报并伪造极端天气事件。


<details>
  <summary>Details</summary>
Motivation: 随着AI天气预报系统逐渐取代传统方法，其引入的新安全威胁尚未被充分研究。本文旨在揭示这些系统的脆弱性及其潜在影响。

Method: 通过向天气观测数据中注入微小的扰动（统计上与自然噪声无法区分），攻击者可以操纵模型输出，伪造极端天气事件。

Result: 攻击仅需改变不到0.1%的观测数据（相当于篡改一颗气象卫星的数据），即可显著影响天气预报结果。

Conclusion: 现代天气预报系统因依赖多源数据而面临严重安全风险，可能引发大规模混乱并削弱公众信任。

Abstract: AI-based systems, such as Google's GenCast, have recently redefined the state
of the art in weather forecasting, offering more accurate and timely
predictions of both everyday weather and extreme events. While these systems
are on the verge of replacing traditional meteorological methods, they also
introduce new vulnerabilities into the forecasting process. In this paper, we
investigate this threat and present a novel attack on autoregressive diffusion
models, such as those used in GenCast, capable of manipulating weather
forecasts and fabricating extreme events, including hurricanes, heat waves, and
intense rainfall. The attack introduces subtle perturbations into weather
observations that are statistically indistinguishable from natural noise and
change less than 0.1% of the measurements - comparable to tampering with data
from a single meteorological satellite. As modern forecasting integrates data
from nearly a hundred satellites and many other sources operated by different
countries, our findings highlight a critical security risk with the potential
to cause large-scale disruptions and undermine public trust in weather
prediction.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [171] [State-Aware IoT Scheduling Using Deep Q-Networks and Edge-Based Coordination](https://arxiv.org/abs/2504.15577)
*Qingyuan He,Chang Liu,Juecen Zhan,Weiqiang Huang,Ran Hao*

Main category: cs.NI

TL;DR: 提出了一种结合深度Q网络（DQN）与边缘协作机制的新型优化方法，用于智能物联网设备的能效管理，实验证明其优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 解决复杂应用环境中智能物联网设备的能效管理挑战。

Method: 结合DQN与边缘协作机制，构建状态-动作-奖励交互模型，引入边缘节点作为中介进行状态聚合与策略调度。

Result: 在平均能耗、处理延迟和资源利用率方面优于现有基线方法。

Conclusion: 该方法在智能物联网场景中具有有效性和实用性。

Abstract: This paper addresses the challenge of energy efficiency management faced by
intelligent IoT devices in complex application environments. A novel
optimization method is proposed, combining Deep Q-Network (DQN) with an edge
collaboration mechanism. The method builds a state-action-reward interaction
model and introduces edge nodes as intermediaries for state aggregation and
policy scheduling. This enables dynamic resource coordination and task
allocation among multiple devices. During the modeling process, device status,
task load, and network resources are jointly incorporated into the state space.
The DQN is used to approximate and learn the optimal scheduling strategy. To
enhance the model's ability to perceive inter-device relationships, a
collaborative graph structure is introduced to model the multi-device
environment and assist in decision optimization. Experiments are conducted
using real-world IoT data collected from the FastBee platform. Several
comparative and validation tests are performed, including energy efficiency
comparisons across different scheduling strategies, robustness analysis under
varying task loads, and evaluation of state dimension impacts on policy
convergence speed. The results show that the proposed method outperforms
existing baseline approaches in terms of average energy consumption, processing
latency, and resource utilization. This confirms its effectiveness and
practicality in intelligent IoT scenarios.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [172] [Scalability Optimization in Cloud-Based AI Inference Services: Strategies for Real-Time Load Balancing and Automated Scaling](https://arxiv.org/abs/2504.15296)
*Yihong Jin,Ze Yang*

Main category: cs.DC

TL;DR: 提出了一种结合强化学习和深度神经网络的混合框架，用于优化云AI推理服务的可扩展性，显著提升了负载均衡效率和响应速度。


<details>
  <summary>Details</summary>
Motivation: 云AI推理服务的快速扩展需要动态负载管理和高性能保障，现有解决方案难以满足需求。

Method: 采用混合方法，结合强化学习实现自适应负载分配，利用深度神经网络进行需求预测，并通过去中心化决策提升容错能力。

Result: 实验显示，负载均衡效率提升35%，响应延迟降低28%。

Conclusion: 该框架显著优于传统可扩展性解决方案，为云AI推理服务提供了高效、动态的资源管理方案。

Abstract: The rapid expansion of AI inference services in the cloud necessitates a
robust scalability solution to manage dynamic workloads and maintain high
performance. This study proposes a comprehensive scalability optimization
framework for cloud AI inference services, focusing on real-time load balancing
and autoscaling strategies. The proposed model is a hybrid approach that
combines reinforcement learning for adaptive load distribution and deep neural
networks for accurate demand forecasting. This multi-layered approach enables
the system to anticipate workload fluctuations and proactively adjust
resources, ensuring maximum resource utilisation and minimising latency.
Furthermore, the incorporation of a decentralised decision-making process
within the model serves to enhance fault tolerance and reduce response time in
scaling operations. Experimental results demonstrate that the proposed model
enhances load balancing efficiency by 35\ and reduces response delay by 28\,
thereby exhibiting a substantial optimization effect in comparison with
conventional scalability solutions.

</details>


### [173] [D$^{2}$MoE: Dual Routing and Dynamic Scheduling for Efficient On-Device MoE-based LLM Serving](https://arxiv.org/abs/2504.15299)
*Haodong Wang,Qihua Zhou,Zicong Hong,Song Guo*

Main category: cs.DC

TL;DR: D²MoE框架通过动态位宽分配和嵌套量化优化，提升边缘设备上MoE模型的推理效率和内存使用。


<details>
  <summary>Details</summary>
Motivation: 现有MoE模型压缩技术因静态优化策略无法满足多样化任务需求，导致服务质量下降。

Method: 提出嵌套量化（MWQ）和启发式调度算法（HEBF），动态分配位宽并优化I/O-计算流水线。

Result: 在边缘设备上，D²MoE提升推理吞吐量1.39倍，减少峰值内存53%，同时保持与INT8相当的精度。

Conclusion: D²MoE通过算法-系统协同设计，有效平衡了MoE模型的性能与资源消耗。

Abstract: The mixture of experts (MoE) model is a sparse variant of large language
models (LLMs), designed to hold a better balance between intelligent capability
and computational overhead. Despite its benefits, MoE is still too expensive to
deploy on resource-constrained edge devices, especially with the demands of
on-device inference services. Recent research efforts often apply model
compression techniques, such as quantization, pruning and merging, to restrict
MoE complexity. Unfortunately, due to their predefined static model
optimization strategies, they cannot always achieve the desired
quality-overhead trade-off when handling multiple requests, finally degrading
the on-device quality of service. These limitations motivate us to propose the
D$^2$MoE, an algorithm-system co-design framework that matches diverse task
requirements by dynamically allocating the most proper bit-width to each
expert. Specifically, inspired by the nested structure of matryoshka dolls, we
propose the matryoshka weight quantization (MWQ) to progressively compress
expert weights in a bit-nested manner and reduce the required runtime memory.
On top of it, we further optimize the I/O-computation pipeline and design a
heuristic scheduling algorithm following our hottest-expert-bit-first (HEBF)
principle, which maximizes the expert parallelism between I/O and computation
queue under constrained memory budgets, thus significantly reducing the idle
temporal bubbles waiting for the experts to load. Evaluations on real edge
devices show that D$^2$MoE improves the overall inference throughput by up to
1.39$\times$ and reduces the peak memory footprint by up to 53% over the latest
on-device inference frameworks, while still preserving comparable serving
accuracy as its INT8 counterparts.

</details>


### [174] [High-Throughput LLM inference on Heterogeneous Clusters](https://arxiv.org/abs/2504.15303)
*Yi Xiong,Jinqi Huang,Wenjie Huang,Xuebing Yu,Entong Li,Zhixiong Ning,Jinhua Zhou,Li Zeng,Xin Chen*

Main category: cs.DC

TL;DR: 论文提出了一种在异构集群上实现高吞吐量LLM推理服务的系统，通过优化部署配置和设计新型请求调度机制，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 异构集群中LLM推理面临配置优化和请求调度的挑战，需要高效利用资源以降低成本并加速任务处理。

Method: 通过建模资源量与预期吞吐量优化部署配置，并设计考虑不同实例处理能力的请求调度机制。

Result: 实验表明，提出的调度器在两个异构集群上分别提高了122.5%和33.6%的吞吐量。

Conclusion: 该系统有效解决了异构集群中LLM推理的配置和调度问题，显著提升了性能。

Abstract: Nowadays, many companies possess various types of AI accelerators, forming
heterogeneous clusters. Efficiently leveraging these clusters for
high-throughput large language model (LLM) inference services can significantly
reduce costs and expedite task processing. However, LLM inference on
heterogeneous clusters presents two main challenges. Firstly, different
deployment configurations can result in vastly different performance. The
number of possible configurations is large, and evaluating the effectiveness of
a specific setup is complex. Thus, finding an optimal configuration is not an
easy task. Secondly, LLM inference instances within a heterogeneous cluster
possess varying processing capacities, leading to different processing speeds
for handling inference requests. Evaluating these capacities and designing a
request scheduling algorithm that fully maximizes the potential of each
instance is challenging. In this paper, we propose a high-throughput inference
service system on heterogeneous clusters. First, the deployment configuration
is optimized by modeling the resource amount and expected throughput and using
the exhaustive search method. Second, a novel mechanism is proposed to schedule
requests among instances, which fully considers the different processing
capabilities of various instances. Extensive experiments show that the proposed
scheduler improves throughput by 122.5% and 33.6% on two heterogeneous
clusters, respectively.

</details>


### [175] [DR.FIX: Automatically Fixing Data Races at Industry Scale](https://arxiv.org/abs/2504.15637)
*Farnaz Behrang,Zhizhou Zhang,Georgian-Vlad Saioc,Peng Liu,Milind Chabbi*

Main category: cs.DC

TL;DR: Dr.Fix结合大型语言模型和程序分析，自动修复工业规模的数据竞争问题，并在Uber的实际开发中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 数据竞争是共享内存并行程序中的常见并发错误，对软件可靠性和可重现性构成挑战。现有研究多集中于检测而非修复，尤其是在工业规模下。

Method: Dr.Fix结合大型语言模型（LLMs）和程序分析，针对Go语言中的复杂代码上下文生成修复方案。

Result: 在18个月内，Dr.Fix为404个数据竞争中的224个（55%）生成了修复补丁，其中193个（86%）被开发者接受并集成到代码库中。

Conclusion: Dr.Fix展示了在工业规模下自动修复数据竞争的可行性，并成功集成到实际开发流程中。

Abstract: Data races are a prevalent class of concurrency bugs in shared-memory
parallel programs, posing significant challenges to software reliability and
reproducibility. While there is an extensive body of research on detecting data
races and a wealth of practical detection tools across various programming
languages, considerably less effort has been directed toward automatically
fixing data races at an industrial scale. In large codebases, data races are
continuously introduced and exhibit myriad patterns, making automated fixing
particularly challenging.
  In this paper, we tackle the problem of automatically fixing data races at an
industrial scale. We present Dr.Fix, a tool that combines large language models
(LLMs) with program analysis to generate fixes for data races in real-world
settings, effectively addressing a broad spectrum of racy patterns in complex
code contexts. Implemented for Go--the programming language widely used in
modern microservice architectures where concurrency is pervasive and data races
are common--Dr.Fix seamlessly integrates into existing development workflows.
We detail the design of Dr.Fix and examine how individual design choices
influence the quality of the fixes produced. Over the past 18 months, Dr.Fix
has been integrated into developer workflows at Uber demonstrating its
practical utility. During this period, Dr.Fix produced patches for 224 (55%)
from a corpus of 404 data races spanning various categories; 193 of these
patches (86%) were accepted by more than a hundred developers via code reviews
and integrated into the codebase.

</details>


### [176] [Collaborative Split Federated Learning with Parallel Training and Aggregation](https://arxiv.org/abs/2504.15724)
*Yiannis Papageorgiou,Yannis Thomas,Alexios Filippakopoulos,Ramin Khalili,Iordanis Koutsopoulos*

Main category: cs.DC

TL;DR: C-SFL是一种新型的联邦学习方案，通过将模型分为三部分并并行训练，减少延迟和通信开销，同时提升模型精度。


<details>
  <summary>Details</summary>
Motivation: 现有分片联邦学习（SFL）方案在计算能力不同的客户端参与时，仍存在训练延迟和通信开销大的问题。

Method: 将模型分为三部分：计算能力弱的客户端部分、计算能力强的客户端部分和服务器部分，实现并行训练和聚合。

Result: 实验证明C-SFL在减少训练延迟、通信开销和提高模型精度方面优于现有方案。

Conclusion: C-SFL通过优化模型分片和并行训练，显著提升了联邦学习的效率和性能。

Abstract: Federated learning (FL) operates based on model exchanges between the server
and the clients, and it suffers from significant client-side computation and
communication burden. Split federated learning (SFL) arises a promising
solution by splitting the model into two parts, that are trained sequentially:
the clients train the first part of the model (client-side model) and transmit
it to the server that trains the second (server-side model). Existing SFL
schemes though still exhibit long training delays and significant communication
overhead, especially when clients of different computing capability
participate. Thus, we propose Collaborative-Split Federated Learning~(C-SFL), a
novel scheme that splits the model into three parts, namely the model parts
trained at the computationally weak clients, the ones trained at the
computationally strong clients, and the ones at the server. Unlike existing
works, C-SFL enables parallel training and aggregation of model's parts at the
clients and at the server, resulting in reduced training delays and
commmunication overhead while improving the model's accuracy. Experiments
verify the multiple gains of C-SFL against the existing schemes.

</details>


<div id='cs.OS'></div>

# cs.OS [[Back]](#toc)

### [177] [LithOS: An Operating System for Efficient Machine Learning on GPUs](https://arxiv.org/abs/2504.15465)
*Patrick H. Coppock,Brian Zhang,Eliot H. Solomon,Vasilis Kypriotis,Leon Yang,Bikash Sharma,Dan Schatzberg,Todd C. Mowry,Dimitrios Skarlatos*

Main category: cs.OS

TL;DR: LithOS是一个面向GPU的操作系统，旨在通过透明、细粒度的资源管理提高GPU利用率和能效，同时保持强隔离性。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习对GPU需求的激增，如何在满足多样化需求的同时优化资源使用成为挑战。

Method: LithOS引入了TPC调度器、透明内核原子化、硬件资源动态调整和透明电源管理等新机制。

Result: 在推理和混合推理-训练场景中，LithOS显著降低了尾延迟并提高了吞吐量，同时节省了GPU资源和能耗。

Conclusion: LithOS为GPU操作系统研究奠定了基础，显著提升了GPU效率。

Abstract: The surging demand for GPUs in datacenters for machine learning (ML) has made
efficient GPU utilization crucial. However, meeting the diverse needs of ML
models while optimizing resource usage is challenging. To enable transparent,
fine-grained GPU management that maximizes utilization and energy efficiency
while maintaining strong isolation, an operating system (OS) approach is
needed. This paper introduces LithOS, a first step toward a GPU OS. LithOS
includes the following new abstractions and mechanisms for efficient GPU
resource management: (i) a novel TPC Scheduler that supports spatial scheduling
at the granularity of individual TPCs, unlocking efficient TPC stealing between
workloads; (ii) transparent kernel atomization to reduce head-of-line blocking
and enable dynamic resource reallocation mid-execution; (iii) a lightweight
hardware right-sizing mechanism that determines the minimal TPC resources
needed per atom; and (iv) a transparent power management mechanism that reduces
power consumption based on in-flight work behavior. We implement LithOS in Rust
and evaluate its performance across extensive ML environments, comparing it to
state-of-the-art solutions from NVIDIA and prior research. For inference
stacking, LithOS reduces tail latencies by 13x compared to MPS; compared to the
best SotA, it reduces tail latencies by 3x while improving aggregate throughput
by 1.6x. In hybrid inference-training stacking, LithOS reduces tail latencies
by 4.7x compared to MPS; compared to the best SotA, it reduces tail latencies
1.18x while improving aggregate throughput by 1.35x. Finally, for a modest
performance hit under 4%, LithOS's right-sizing provides a quarter of GPU
capacity savings on average, while for a 7% hit, its power management yields a
quarter of a GPU's energy savings. Overall, LithOS increases GPU efficiency,
establishing a foundation for future OS research on GPUs.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [178] [A Graph Based Raman Spectral Processing Technique for Exosome Classification](https://arxiv.org/abs/2504.15324)
*Vuong M. Ngo,Edward Bolger,Stan Goodwin,John O'Sullivan,Dinh Viet Cuong,Mark Roantree*

Main category: q-bio.QM

TL;DR: 该研究利用Neo4j图数据库和新型光谱过滤方法，结合PageRank Filter与最优降维技术，显著提高了基于拉曼光谱的外泌体分类准确性。


<details>
  <summary>Details</summary>
Motivation: 外泌体作为细胞信号传递和疾病生物标志物的重要载体，其复杂性需要"组学"方法分析。拉曼光谱虽有效，但存在样本浓度要求高、对脂质和蛋白质敏感度有限的问题。

Method: 研究使用Neo4j图数据库组织3,045个外泌体拉曼光谱数据，并引入结合PageRank Filter与最优降维的光谱过滤方法，提升特征选择和分类性能。

Result: Extra Trees模型在分类高血糖、低血糖和正常外泌体样本时，准确率分别达到0.76和0.857（基于拉曼光谱和表面数据），10折交叉验证显示显著提升。

Conclusion: 基于图的光谱过滤与降维技术显著降低了噪声并保留了关键生物标志物信号，为拉曼光谱在外泌体分析及生物医学应用中的潜力提供了新框架。

Abstract: Exosomes are small vesicles crucial for cell signaling and disease
biomarkers. Due to their complexity, an "omics" approach is preferable to
individual biomarkers. While Raman spectroscopy is effective for exosome
analysis, it requires high sample concentrations and has limited sensitivity to
lipids and proteins. Surface-enhanced Raman spectroscopy helps overcome these
challenges. In this study, we leverage Neo4j graph databases to organize 3,045
Raman spectra of exosomes, enhancing data generalization. To further refine
spectral analysis, we introduce a novel spectral filtering process that
integrates the PageRank Filter with optimal Dimensionality Reduction. This
method improves feature selection, resulting in superior classification
performance. Specifically, the Extra Trees model, using our spectral processing
approach, achieves 0.76 and 0.857 accuracy in classifying hyperglycemic,
hypoglycemic, and normal exosome samples based on Raman spectra and surface,
respectively, with group 10-fold cross-validation. Our results show that
graph-based spectral filtering combined with optimal dimensionality reduction
significantly improves classification accuracy by reducing noise while
preserving key biomarker signals. This novel framework enhances Raman-based
exosome analysis, expanding its potential for biomedical applications, disease
diagnostics, and biomarker discovery.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [179] [On the Boolean Network Theory of Datalog$^\neg$](https://arxiv.org/abs/2504.15417)
*Van-Giang Trinh,Belaid Benhamou,Sylvain Soliman,François Fages*

Main category: cs.LO

TL;DR: 本文建立了Datalog$^\neg$与布尔网络理论的形式联系，证明了在无奇偶循环条件下稳定模型与正则模型的关系，并给出了模型数量的上界。


<details>
  <summary>Details</summary>
Motivation: Datalog$^\neg$在多个领域有广泛应用，但其模型理论与布尔网络理论的联系尚未明确。本文旨在填补这一空白。

Method: 利用布尔网络理论的结果，分析Datalog$^\neg$程序的奇偶循环对模型的影响，并引入陷阱空间概念。

Result: 在无奇循环时，正则模型与稳定模型一致；在无偶循环时，稳定偏模型唯一。还给出了模型数量的上界。

Conclusion: 本文不仅修正了You和Yuan的证明问题，还通过布尔网络理论为Datalog$^\neg$提供了新的分析工具和结果。

Abstract: Datalog$^\neg$ is a central formalism used in a variety of domains ranging
from deductive databases and abstract argumentation frameworks to answer set
programming. Its model theory is the finite counterpart of the logical
semantics developed for normal logic programs, mainly based on the notions of
Clark's completion and two-valued or three-valued canonical models including
supported, stable, regular and well-founded models. In this paper we establish
a formal link between Datalog$^\neg$ and Boolean network theory, which was
initially introduced by Stuart Kaufman and Ren\'e Thomas to reason about gene
regulatory networks. We use previous results from Boolean network theory to
prove that in the absence of odd cycles in a Datalog$^\neg$ program, the
regular models coincide with the stable models, which entails the existence of
stable models, and in the absence of even cycles, we show the uniqueness of
stable partial models, which entails the uniqueness of regular models. These
results on regular models have been claimed by You and Yuan in 1994 for normal
logic programs but we show problems in their definition of well-founded
stratification and in their proofs that we can fix for negative normal logic
programs only. We also give upper bounds on the numbers of stable partial,
regular, and stable models of a Datalog$^\neg$ program using the cardinality of
a feedback vertex set in its atom dependency graph. Interestingly, our
connection to Boolean network theory also points us to the notion of trap
spaces for Datalog$^\neg$ programs. We relate the notions of supported or
stable trap spaces to the other semantics of Datalog$^\neg$, and show the
equivalence between subset-minimal stable trap spaces and regular models.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [180] [Efficient Discovery of Motif Transition Process for Large-Scale Temporal Graphs](https://arxiv.org/abs/2504.15979)
*Zhiyuan Zheng,Jianpeng Qi,Jiantao Li,Guoqing Chao,Junyu Dong,Yanwei Yu*

Main category: cs.DB

TL;DR: 提出了一种并行算法PTMT，用于发现大规模时序图中的motif动态转换过程，通过树结构和时间分区策略实现高效并行处理。


<details>
  <summary>Details</summary>
Motivation: 现有方法局限于预定义motif，无法全面捕捉时序图中的动态转换和相互关系。

Method: PTMT结合树框架和时间分区策略（TZP），分为三个步骤：并行扩展、重叠感知聚合和确定性编码。

Result: 在10个真实数据集上，PTMT比现有最优方法快12.0到50.3倍。

Conclusion: PTMT能高效且准确地追踪时序图中motif的动态转换和交互。

Abstract: Understanding the dynamic transition of motifs in temporal graphs is
essential for revealing how graph structures evolve over time, identifying
critical patterns, and predicting future behaviors, yet existing methods often
focus on predefined motifs, limiting their ability to comprehensively capture
transitions and interrelationships. We propose a parallel motif transition
process discovery algorithm, PTMT, a novel parallel method for discovering
motif transition processes in large-scale temporal graphs. PTMT integrates a
tree-based framework with the temporal zone partitioning (TZP) strategy, which
partitions temporal graphs by time and structure while preserving lossless
motif transitions and enabling massive parallelism. PTMT comprises three
phases: growth zone parallel expansion, overlap-aware result aggregation, and
deterministic encoding of motif transitions, ensuring accurate tracking of
dynamic transitions and interactions. Results on 10 real-world datasets
demonstrate that PTMT achieves speedups ranging from 12.0$\times$ to
50.3$\times$ compared to the SOTA method.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [181] [Demand for LLMs: Descriptive Evidence on Substitution, Market Expansion, and Multihoming](https://arxiv.org/abs/2504.15440)
*Andrey Fradkin*

Main category: cs.CY

TL;DR: 论文通过OpenRouter数据总结了LLM需求的三个特点：新模型快速被采用但很快稳定；模型发布吸引新用户或替代竞争模型；多模型同时使用常见。


<details>
  <summary>Details</summary>
Motivation: 研究LLM市场的需求动态，揭示模型发布和用户行为的特点。

Method: 使用OpenRouter市场数据，分析LLM的采用、替代和多模型使用情况。

Result: 发现LLM市场存在水平和垂直差异化，提供商有机会维持需求和定价权。

Conclusion: LLM市场具有显著差异化特征，技术进步下提供商仍有机会保持竞争力。

Abstract: This paper documents three stylized facts about the demand for Large Language
Models (LLMs) using data from OpenRouter, a prominent LLM marketplace. First,
new models experience rapid initial adoption that stabilizes within weeks.
Second, model releases differ substantially in whether they primarily attract
new users or substitute demand from competing models. Third, multihoming, using
multiple models simultaneously, is common among apps. These findings suggest
significant horizontal and vertical differentiation in the LLM market, implying
opportunities for providers to maintain demand and pricing power despite rapid
technological advances.

</details>


### [182] [Trends in AI Supercomputers](https://arxiv.org/abs/2504.16026)
*Konstantin F. Pilz,James Sanders,Robi Rahman,Lennart Heim*

Main category: cs.CY

TL;DR: 论文分析了2019至2025年间500台AI超级计算机的数据，发现计算性能每9个月翻倍，硬件成本和功耗每年翻倍。到2025年，领先系统xAI的Colossus使用20万AI芯片，成本70亿美元，功耗300兆瓦。全球75%性能集中在美国，15%在中国。


<details>
  <summary>Details</summary>
Motivation: 研究AI超级计算机的发展趋势，为政策制定者提供资源需求、所有权和国家竞争力的评估依据。

Method: 创建包含500台AI超级计算机的数据集，分析性能、功耗、硬件成本、所有权和全球分布。

Result: 计算性能每9个月翻倍，硬件成本和功耗每年翻倍；企业和美国在性能占比中主导。

Conclusion: AI超级计算机发展趋势迅猛，需关注资源需求和全球竞争格局。

Abstract: Frontier AI development relies on powerful AI supercomputers, yet analysis of
these systems is limited. We create a dataset of 500 AI supercomputers from
2019 to 2025 and analyze key trends in performance, power needs, hardware cost,
ownership, and global distribution. We find that the computational performance
of AI supercomputers has doubled every nine months, while hardware acquisition
cost and power needs both doubled every year. The leading system in March 2025,
xAI's Colossus, used 200,000 AI chips, had a hardware cost of \$7B, and
required 300 MW of power, as much as 250,000 households. As AI supercomputers
evolved from tools for science to industrial machines, companies rapidly
expanded their share of total AI supercomputer performance, while the share of
governments and academia diminished. Globally, the United States accounts for
about 75% of total performance in our dataset, with China in second place at
15%. If the observed trends continue, the leading AI supercomputer in 2030 will
achieve $2\times10^{22}$ 16-bit FLOP/s, use two million AI chips, have a
hardware cost of \$200 billion, and require 9 GW of power. Our analysis
provides visibility into the AI supercomputer landscape, allowing policymakers
to assess key AI trends like resource needs, ownership, and national
competitiveness.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [183] [CUBETESTERAI: Automated JUnit Test Generation using the LLaMA Model](https://arxiv.org/abs/2504.15286)
*Daniele Gorla,Shivam Kumar,Pietro Nicolaus Roselli Lorenzini,Alireza Alipourfaz*

Main category: cs.SE

TL;DR: 本文提出了一种利用LLaMA模型自动化生成Spring Boot应用JUnit测试的方法，开发了工具CUBETESTERAI，具有高效、准确和隐私保护的特点。


<details>
  <summary>Details</summary>
Motivation: 提高Java Spring Boot应用测试的效率和准确性，减少手动干预。

Method: 结合LLaMA模型和CI/CD流程，通过RunPod执行模型，生成高覆盖率的测试用例。

Result: CUBETESTERAI在代码覆盖率上优于现有工具。

Conclusion: 该方法显著提升了测试自动化水平，适用于实际Java项目。

Abstract: This paper presents an approach to automating JUnit test generation for Java
applications using the Spring Boot framework, leveraging the LLaMA (Large
Language Model Architecture) model to enhance the efficiency and accuracy of
the testing process. The resulting tool, called CUBETESTERAI, includes a
user-friendly web interface and the integration of a CI/CD pipeline using
GitLab and Docker. These components streamline the automated test generation
process, allowing developers to generate JUnit tests directly from their code
snippets with minimal manual intervention. The final implementation executes
the LLaMA models through RunPod, an online GPU service, which also enhances the
privacy of our tool. Using the advanced natural language processing
capabilities of the LLaMA model, CUBETESTERAI is able to generate test cases
that provide high code coverage and accurate validation of software
functionalities in Java-based Spring Boot applications. Furthermore, it
efficiently manages resource-intensive operations and refines the generated
tests to address common issues like missing imports and handling of private
methods. By comparing CUBETESTERAI with some state-of-the-art tools, we show
that our proposal consistently demonstrates competitive and, in many cases,
better performance in terms of code coverage in different real-life Java
programs.

</details>


### [184] [LLM-Assisted Translation of Legacy FORTRAN Codes to C++: A Cross-Platform Study](https://arxiv.org/abs/2504.15424)
*Nishath Rajiv Ranasinghe,Shawn M. Jones,Michal Kucer,Ayan Biswas,Daniel O'Malley,Alexander Buschmann Most,Selma Liliane Wanna,Ajay Sreekumar*

Main category: cs.SE

TL;DR: 研究了利用开源大语言模型（LLM）将Fortran代码翻译为C++的适用性，并量化了编译准确性、代码相似性和输出相似性。


<details>
  <summary>Details</summary>
Motivation: 评估LLM在翻译科学计算代码（尤其是Fortran到C++）中的实际可用性，填补现有研究的空白。

Method: 在两种计算平台上使用开源LLM进行翻译，统计编译准确性、代码相似性和输出相似性。

Result: 量化了LLM翻译的C++代码的编译准确性、与人工翻译代码的相似性以及输出结果的相似性。

Conclusion: LLM在Fortran到C++的代码翻译中表现出一定的适用性，但仍需进一步验证和优化。

Abstract: Large Language Models (LLMs) are increasingly being leveraged for generating
and translating scientific computer codes by both domain-experts and non-domain
experts. Fortran has served as one of the go to programming languages in legacy
high-performance computing (HPC) for scientific discoveries. Despite growing
adoption, LLM-based code translation of legacy code-bases has not been
thoroughly assessed or quantified for its usability. Here, we studied the
applicability of LLM-based translation of Fortran to C++ as a step towards
building an agentic-workflow using open-weight LLMs on two different
computational platforms. We statistically quantified the compilation accuracy
of the translated C++ codes, measured the similarity of the LLM translated code
to the human translated C++ code, and statistically quantified the output
similarity of the Fortran to C++ translation.

</details>


### [185] [A Framework for Testing and Adapting REST APIs as LLM Tools](https://arxiv.org/abs/2504.15546)
*Jayachandu Bandlamudi,Ritwik Chaudhuri,Neelamadhav Gantayat,Kushal Mukherjee,Prerna Agarwal,Renuka Sindhgatta,Sameep Mehta*

Main category: cs.SE

TL;DR: 提出了一种新的测试框架，用于评估和提升REST API作为LLM代理工具的可用性，通过生成测试用例、翻译为自然语言指令并分析错误分类。


<details>
  <summary>Details</summary>
Motivation: 现有API工具测试基准未能充分应对复杂输入模式、响应和模糊文档的挑战，导致评估代理驱动自动化的API准备度存在缺口。

Method: 开发了一个测试框架，将API转化为工具，生成测试用例，翻译为自然语言指令，并评估代理调用API和处理输入输出的能力。

Result: 分析了750个测试案例，提出了错误分类法，包括输入误解、输出处理不一致和模式不匹配。

Conclusion: 该框架为提升企业API作为工具的可用性奠定了基础，优化了其在代理应用中的表现。

Abstract: Large Language Models (LLMs) are enabling autonomous agents to perform
complex workflows using external tools or functions, often provided via REST
APIs in enterprise systems. However, directly utilizing these APIs as tools
poses challenges due to their complex input schemas, elaborate responses, and
often ambiguous documentation. Current benchmarks for tool testing do not
adequately address these complexities, leading to a critical gap in evaluating
API readiness for agent-driven automation. In this work, we present a novel
testing framework aimed at evaluating and enhancing the readiness of REST APIs
to function as tools for LLM-based agents. Our framework transforms apis as
tools, generates comprehensive test cases for the APIs, translates tests cases
into natural language instructions suitable for agents, enriches tool
definitions and evaluates the agent's ability t correctly invoke the API and
process its inputs and responses. To provide actionable insights, we analyze
the outcomes of 750 test cases, presenting a detailed taxonomy of errors,
including input misinterpretation, output handling inconsistencies, and schema
mismatches. Additionally, we classify these test cases to streamline debugging
and refinement of tool integrations. This work offers a foundational step
toward enabling enterprise APIs as tools, improving their usability in
agent-based applications.

</details>


### [186] [EditLord: Learning Code Transformation Rules for Code Editing](https://arxiv.org/abs/2504.15284)
*Weichen Li,Albert Jan,Baishakhi Ray,Chengzhi Mao,Junfeng Yang,Kexin Pei*

Main category: cs.SE

TL;DR: EditLord是一个显式代码编辑框架，通过语言模型提取编辑规则，显著提升了编辑性能、鲁棒性和功能正确性。


<details>
  <summary>Details</summary>
Motivation: 现有代码编辑方法将任务视为隐式端到端过程，忽略了离散步骤，导致性能不佳。

Method: 使用语言模型从训练代码对中提取编辑规则，生成元规则集，用于微调或提示迭代编辑。

Result: EditLord在编辑性能、鲁棒性和功能正确性上均显著优于现有方法。

Conclusion: 显式代码编辑步骤和规则提取是提升代码编辑任务效果的关键。

Abstract: Code editing is a foundational task in software development, where its
effectiveness depends on whether it introduces desired code property changes
without changing the original code's intended functionality. Existing
approaches often formulate code editing as an implicit end-to-end task,
omitting the fact that code-editing procedures inherently consist of discrete
and explicit steps. Thus, they suffer from suboptimal performance and lack of
robustness and generalization. We introduce EditLord, a code editing framework
that makes the code transformation steps explicit. Our key insight is to employ
a language model (LM) as an inductive learner to extract code editing rules
from the training code pairs as concise meta-rule sets. Such rule sets will be
manifested for each training sample to augment them for finetuning or assist in
prompting- and iterative-based code editing. EditLordoutperforms the
state-of-the-art by an average of 22.7% in editing performance and 58.1% in
robustness while achieving 20.2% higher functional correctness across critical
software engineering and security applications, LM models, and editing modes.

</details>


### [187] [A Large-scale Class-level Benchmark Dataset for Code Generation with LLMs](https://arxiv.org/abs/2504.15564)
*Musfiqur Rahman,SayedHassan Khatoonabadi,Emad Shihab*

Main category: cs.SE

TL;DR: 论文介绍了一个基于真实开源项目的Python类级别数据集，用于提升大语言模型（LLM）在类级代码生成中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试多关注孤立函数，未能反映真实世界类级软件的复杂性。

Method: 构建了一个包含842,000个类骨架的数据集，保留结构和上下文依赖，并用静态代码指标丰富数据。

Result: 使用GPT-4生成类实现，结果显示生成的类与人工编写的类在词汇和结构上高度相似（ROUGE@L 0.80，BLEU 0.59，TSED 0.73）。

Conclusion: 该数据集显著提升了LLM在类级代码生成中的性能，为软件工程中的LLM训练和评估提供了宝贵资源。

Abstract: Recent advancements in large language models (LLMs) have demonstrated
promising capabilities in code generation tasks. However, most existing
benchmarks focus on isolated functions and fail to capture the complexity of
real-world, class-level software structures. To address this gap, we introduce
a large-scale, Python class-level dataset curated from $13{,}174$ real-world
open-source projects. The dataset contains over 842,000 class skeletons, each
including class and method signatures, along with associated docstrings when
available. We preserve structural and contextual dependencies critical to
realistic software development scenarios and enrich the dataset with static
code metrics to support downstream analysis. To evaluate the usefulness of this
dataset, we use extracted class skeletons as prompts for GPT-4 to generate full
class implementations. Results show that the LLM-generated classes exhibit
strong lexical and structural similarity to human-written counterparts, with
average ROUGE@L, BLEU, and TSED scores of 0.80, 0.59, and 0.73, respectively.
These findings confirm that well-structured prompts derived from real-world
class skeletons significantly enhance LLM performance in class-level code
generation. This dataset offers a valuable resource for benchmarking, training,
and improving LLMs in realistic software engineering contexts.

</details>


### [188] [A Study On Mixup-inspired Augmentation Methods For Software Vulnerability Detection](https://arxiv.org/abs/2504.15632)
*Seyed Shayan Daneshvar,Da Tan,Shaowei Wang,Carson Leung*

Main category: cs.SE

TL;DR: 论文提出了一种在表示级别增强漏洞数据的方法，以提升深度学习模型的性能，但效果仍不及随机过采样。


<details>
  <summary>Details</summary>
Motivation: 现实中的软件漏洞数据集稀缺且不平衡，现有方法生成的漏洞不实用且需人工检查，因此探索在表示级别增强数据以改进模型学习。

Method: 实现了5种嵌入级别的数据增强技术，并引入条件化版本以确保不改变漏洞部分的向量表示。

Result: 这些方法可将f1-score提升至多9.67%，但仍不及随机过采样的10.82%提升。

Conclusion: 表示级别增强对模型有帮助，但在平衡数据集时效果不如随机过采样。

Abstract: Various Deep Learning (DL) methods have recently been utilized to detect
software vulnerabilities. Real-world software vulnerability datasets are rare
and hard to acquire as there's no simple metric for classifying vulnerability.
Such datasets are heavily imbalanced, and none of the current datasets are
considered huge for DL models. To tackle these problems a recent work has tried
to augment the dataset using the source code and generate realistic
single-statement vulnerabilities which is not quite practical and requires
manual checking of the generated vulnerabilities. In this regard, we aim to
explore the augmentation of vulnerabilities at the representation level to help
current models learn better which has never been done before to the best of our
knowledge. We implement and evaluate the 5 augmentation techniques that augment
the embedding of the data and recently have been used for code search which is
a completely different software engineering task. We also introduced a
conditioned version of those augmentation methods, which ensures the
augmentation does not change the vulnerable section of the vector
representation. We show that such augmentation methods can be helpful and
increase the f1-score by up to 9.67%, yet they cannot beat Random Oversampling
when balancing datasets which increases the f1-score by 10.82%!

</details>


### [189] [Automated Bug Report Prioritization in Large Open-Source Projects](https://arxiv.org/abs/2504.15912)
*Riley Pierson,Armin Moin*

Main category: cs.SE

TL;DR: 提出了一种基于自然语言文本的自动化缺陷优先级排序方法，结合TopicMiner-MTM和BERT模型，在Eclipse项目数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 开源项目资源有限，需要高效处理大量缺陷报告和功能请求，因此需要自动化优先级排序方法。

Method: 使用TopicMiner-MTM进行主题建模，结合BERT模型进行文本分类。

Result: 在85,156个Eclipse缺陷报告数据集上，准确率、精确率、召回率和F1值均优于现有方法。

Conclusion: 该方法能有效提升缺陷报告的优先级排序性能，适用于开源项目管理。

Abstract: Large open-source projects receive a large number of issues (known as bugs),
including software defect (i.e., bug) reports and new feature requests from
their user and developer communities at a fast rate. The often limited project
resources do not allow them to deal with all issues. Instead, they have to
prioritize them according to the project's priorities and the issues'
severities. In this paper, we propose a novel approach to automated bug
prioritization based on the natural language text of the bug reports that are
stored in the open bug repositories of the issue-tracking systems. We conduct
topic modeling using a variant of LDA called TopicMiner-MTM and text
classification with the BERT large language model to achieve a higher
performance level compared to the state-of-the-art. Experimental results using
an existing reference dataset containing 85,156 bug reports of the Eclipse
Platform project indicate that we outperform existing approaches in terms of
Accuracy, Precision, Recall, and F1-measure of the bug report priority
prediction.

</details>


### [190] [Bug Destiny Prediction in Large Open-Source Software Repositories through Sentiment Analysis and BERT Topic Modeling](https://arxiv.org/abs/2504.15972)
*Sophie C. Pope,Andrew Barovic,Armin Moin*

Main category: cs.SE

TL;DR: 该研究提出了一种预测Bug相关结果的新方法，结合情感分析和BERTopic模型，通过CNN和MLP提升预测准确性，但输入平衡会降低部分准确性。


<details>
  <summary>Details</summary>
Motivation: 探索利用Bug解决前的数据预测Bug解决时间、修复时间及最终状态，以提高预测准确性。

Method: 结合情感分析（情感评分和分类）、BERTopic模型提取主题特征，使用CNN和MLP进行预测。

Result: 情感分析能有效预测Bug是否被修复，但对复杂分类效果有限；BERTopic和情感分析组合可提升部分指标。

Conclusion: 情感分析在预测Bug结果中有价值，但需权衡输入平衡与准确性，适用于简单分类场景。

Abstract: This study explores a novel approach to predicting key bug-related outcomes,
including the time to resolution, time to fix, and ultimate status of a bug,
using data from the Bugzilla Eclipse Project. Specifically, we leverage
features available before a bug is resolved to enhance predictive accuracy. Our
methodology incorporates sentiment analysis to derive both an emotionality
score and a sentiment classification (positive or negative). Additionally, we
integrate the bug's priority level and its topic, extracted using a BERTopic
model, as features for a Convolutional Neural Network (CNN) and a Multilayer
Perceptron (MLP). Our findings indicate that the combination of BERTopic and
sentiment analysis can improve certain model performance metrics. Furthermore,
we observe that balancing model inputs enhances practical applicability, albeit
at the cost of a significant reduction in accuracy in most cases. To address
our primary objectives, predicting time-to-resolution, time-to-fix, and bug
destiny, we employ both binary classification and exact time value predictions,
allowing for a comparative evaluation of their predictive effectiveness.
Results demonstrate that sentiment analysis serves as a valuable predictor of a
bug's eventual outcome, particularly in determining whether it will be fixed.
However, its utility is less pronounced when classifying bugs into more complex
or unconventional outcome categories.

</details>


### [191] [Benchmarking LLM for Code Smells Detection: OpenAI GPT-4.0 vs DeepSeek-V3](https://arxiv.org/abs/2504.16027)
*Ahmed R. Sadik,Siddhata Govind*

Main category: cs.SE

TL;DR: 研究提出了一种评估大型语言模型（LLM）在代码异味检测中效果的方法，比较了GPT-4.0和DeepSeek-V3，并分析了成本效益。


<details>
  <summary>Details</summary>
Motivation: 确定最有效的LLM用于代码异味检测是一个复杂挑战，需要系统化的评估方法。

Method: 使用标注好的代码数据集，跨四种编程语言，通过精确率、召回率和F1分数评估两种LLM的性能。

Result: 研究提供了两种模型的性能比较和成本效益分析，为实践者提供选择依据。

Conclusion: 研究为自动化代码异味检测提供了高效且经济的解决方案指导。

Abstract: Determining the most effective Large Language Model for code smell detection
presents a complex challenge. This study introduces a structured methodology
and evaluation matrix to tackle this issue, leveraging a curated dataset of
code samples consistently annotated with known smells. The dataset spans four
prominent programming languages Java, Python, JavaScript, and C++; allowing for
cross language comparison. We benchmark two state of the art LLMs, OpenAI GPT
4.0 and DeepSeek-V3, using precision, recall, and F1 score as evaluation
metrics. Our analysis covers three levels of detail: overall performance,
category level performance, and individual code smell type performance.
Additionally, we explore cost effectiveness by comparing the token based
detection approach of GPT 4.0 with the pattern-matching techniques employed by
DeepSeek V3. The study also includes a cost analysis relative to traditional
static analysis tools such as SonarQube. The findings offer valuable guidance
for practitioners in selecting an efficient, cost effective solution for
automated code smell detection

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [192] [New Recipe for Semi-supervised Community Detection: Clique Annealing under Crystallization Kinetics](https://arxiv.org/abs/2504.15927)
*Ling Cheng,Jiashu Pu,Ruicheng Liang,Qian Shao,Hezhe Qiao,Feida Zhu*

Main category: cs.SI

TL;DR: 论文提出CLANN方法，通过结晶动力学原理改进半监督社区检测，解决了现有方法计算成本高和候选核心不合理的问题。


<details>
  <summary>Details</summary>
Motivation: 现有半监督社区检测方法因依赖强化学习和生成对抗网络，计算成本高且候选核心不合理，限制了可扩展性。

Method: 将社区检测类比为结晶过程，提出CLANN方法，结合动力学原理优化社区核心一致性，并使用无学习的Transitive Annealer细化候选。

Result: 在43种网络设置下的实验表明，CLANN在多个真实数据集上优于现有方法，表现出高效性和有效性。

Conclusion: CLANN通过结晶动力学原理显著提升了社区检测的效率和可扩展性。

Abstract: Semi-supervised community detection methods are widely used for identifying
specific communities due to the label scarcity. Existing semi-supervised
community detection methods typically involve two learning stages learning in
both initial identification and subsequent adjustment, which often starts from
an unreasonable community core candidate. Moreover, these methods encounter
scalability issues because they depend on reinforcement learning and generative
adversarial networks, leading to higher computational costs and restricting the
selection of candidates. To address these limitations, we draw a parallel
between crystallization kinetics and community detection to integrate the
spontaneity of the annealing process into community detection. Specifically, we
liken community detection to identifying a crystal subgrain (core) that expands
into a complete grain (community) through a process similar to annealing. Based
on this finding, we propose CLique ANNealing (CLANN), which applies kinetics
concepts to community detection by integrating these principles into the
optimization process to strengthen the consistency of the community core.
Subsequently, a learning-free Transitive Annealer was employed to refine the
first-stage candidates by merging neighboring cliques and repositioning the
community core, enabling a spontaneous growth process that enhances
scalability. Extensive experiments on \textbf{43} different network settings
demonstrate that CLANN outperforms state-of-the-art methods across multiple
real-world datasets, showcasing its exceptional efficacy and efficiency in
community detection.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [193] [Real-Time Optimal Design of Experiment for Parameter Identification of Li-Ion Cell Electrochemical Model](https://arxiv.org/abs/2504.15578)
*Ian Mikesell,Samuel Filgueira da Silva,Mehmet Fatih Ozkan,Faissal El Idrissi,Prashanth Ramesh,Marcello Canova*

Main category: eess.SY

TL;DR: 本文提出了一种基于强化学习（RL）的方法，用于动态优化锂离子电池（LiB）电化学模型的参数识别，通过硬件在环（HIL）验证，证明其优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统参数识别方法需要大量数据收集且缺乏动态环境适应性，因此需要更高效、自适应的解决方案。

Method: 采用强化学习（RL）动态调整电流曲线，优化参数可识别性，并通过HIL实时实现。

Result: RL方法在减少建模误差和缩短实验时间方面优于传统测试协议。

Conclusion: RL框架为LiB参数识别提供了高效、自适应的解决方案。

Abstract: Accurately identifying the parameters of electrochemical models of li-ion
battery (LiB) cells is a critical task for enhancing the fidelity and
predictive ability. Traditional parameter identification methods often require
extensive data collection experiments and lack adaptability in dynamic
environments. This paper describes a Reinforcement Learning (RL) based approach
that dynamically tailors the current profile applied to a LiB cell to optimize
the parameters identifiability of the electrochemical model. The proposed
framework is implemented in real-time using a Hardware-in-the-Loop (HIL) setup,
which serves as a reliable testbed for evaluating the RL-based design strategy.
The HIL validation confirms that the RL-based experimental design outperforms
conventional test protocols used for parameter identification in terms of both
reducing the modeling errors on a verification test and minimizing the duration
of the experiment used for parameter identification.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [194] [Solving Multi-Agent Safe Optimal Control with Distributed Epigraph Form MARL](https://arxiv.org/abs/2504.15425)
*Songyuan Zhang,Oswin So,Mitchell Black,Zachary Serlin,Chuchu Fan*

Main category: cs.RO

TL;DR: 论文提出了一种名为Def-MARL的分布式多智能体强化学习算法，用于解决多机器人系统中的安全协作问题，通过改进训练稳定性并在仿真和硬件实验中验证其性能。


<details>
  <summary>Details</summary>
Motivation: 多机器人系统在协作完成任务时需要确保安全性，现有算法在训练稳定性上存在问题，因此需要一种更稳定的解决方案。

Method: 采用约束优化的epigraph形式，提出Def-MARL算法，实现集中训练分布式执行。

Result: 在8个任务和2个模拟器中的仿真实验及Crazyflie四旋翼硬件实验中，Def-MARL表现最佳，满足安全约束且训练稳定。

Conclusion: Def-MARL在多机器人协作任务中表现出色，能够安全协调智能体完成任务，优于其他方法。

Abstract: Tasks for multi-robot systems often require the robots to collaborate and
complete a team goal while maintaining safety. This problem is usually
formalized as a constrained Markov decision process (CMDP), which targets
minimizing a global cost and bringing the mean of constraint violation below a
user-defined threshold. Inspired by real-world robotic applications, we define
safety as zero constraint violation. While many safe multi-agent reinforcement
learning (MARL) algorithms have been proposed to solve CMDPs, these algorithms
suffer from unstable training in this setting. To tackle this, we use the
epigraph form for constrained optimization to improve training stability and
prove that the centralized epigraph form problem can be solved in a distributed
fashion by each agent. This results in a novel centralized training distributed
execution MARL algorithm named Def-MARL. Simulation experiments on 8 different
tasks across 2 different simulators show that Def-MARL achieves the best
overall performance, satisfies safety constraints, and maintains stable
training. Real-world hardware experiments on Crazyflie quadcopters demonstrate
the ability of Def-MARL to safely coordinate agents to complete complex
collaborative tasks compared to other methods.

</details>


### [195] [Advancing Embodied Intelligence in Robotic-Assisted Endovascular Procedures: A Systematic Review of AI Solutions](https://arxiv.org/abs/2504.15327)
*Tianliang Yao,Bo Lu,Markus Kowarschik,Yixuan Yuan,Hubin Zhao,Sebastien Ourselin,Kaspar Althoefer,Junbo Ge,Peng Qi*

Main category: cs.RO

TL;DR: 论文探讨了机器人系统结合具身智能（EI）在血管内手术中的应用，通过数据驱动方法提升手术精度和效率，并展望了未来发展方向。


<details>
  <summary>Details</summary>
Motivation: 血管内手术的精确性和灵活性要求高，传统方法存在操作疲劳、辐射暴露和人为误差等问题，机器人系统和EI技术为解决这些问题提供了新思路。

Method: 采用数据驱动方法，包括计算机视觉、医学图像分析和机器学习（如强化学习和模仿学习），实现实时血管分割、设备跟踪和导航优化。

Result: EI技术的整合显著提升了机器人系统的感知和控制能力，改善了手术效果，并为未来更高自主性和临床效果奠定了基础。

Conclusion: 论文总结了EI在血管内手术中的潜力，提出了未来研究方向，如联邦学习、可解释AI和人机协作，推动该领域的进一步发展。

Abstract: Endovascular procedures have revolutionized the treatment of vascular
diseases thanks to minimally invasive solutions that significantly reduce
patient recovery time and enhance clinical outcomes. However, the precision and
dexterity required during these procedures poses considerable challenges for
interventionists. Robotic systems have emerged offering transformative
solutions, addressing issues such as operator fatigue, radiation exposure, and
the inherent limitations of human precision. The integration of Embodied
Intelligence (EI) into these systems signifies a paradigm shift, enabling
robots to navigate complex vascular networks and adapt to dynamic physiological
conditions. Data-driven approaches, advanced computer vision, medical image
analysis, and machine learning techniques, are at the forefront of this
evolution. These methods augment procedural intelligence by facilitating
real-time vessel segmentation, device tracking, and anatomical landmark
detection. Reinforcement learning and imitation learning further refine
navigation strategies and replicate experts' techniques. This review
systematically examines the integration of EI principles into robotic
technologies, in relation to endovascular procedures. We discuss recent
advancements in intelligent perception and data-driven control, and their
practical applications in robot-assisted endovascular procedures. By critically
evaluating current limitations and emerging opportunities, this review
establishes a framework for future developments, emphasizing the potential for
greater autonomy and improved clinical outcomes. Emerging trends and specific
areas of research, such as federated learning for medical data sharing,
explainable AI for clinical decision support, and advanced human-robot
collaboration paradigms, are also explored, offering insights into the future
direction of this rapidly evolving field.

</details>


### [196] [A Vision-Enabled Prosthetic Hand for Children with Upper Limb Disabilities](https://arxiv.org/abs/2504.15654)
*Md Abdul Baset Sarker,Art Nguyen,Sigmond Kukla,Kevin Fite,Masudul H. Imtiaz*

Main category: cs.RO

TL;DR: 本文介绍了一种新型AI视觉儿童假肢手，专为10-12岁上肢残疾儿童设计，具有仿生外观、多关节功能和轻量化设计，结合3D打印技术和机器视觉，提供低成本定制化解决方案。


<details>
  <summary>Details</summary>
Motivation: 解决当前肌电假肢的局限性，为低收入家庭提供可负担且功能强大的假肢解决方案。

Method: 采用3D打印技术，集成机器视觉、传感和嵌入式计算，配备微型摄像头和低功耗FPGA，实现实时物体检测和精确抓取。

Result: 深度学习物体检测和抓取分类模型准确率分别达96%和100%，力预测平均绝对误差为0.018。

Conclusion: 该假肢手结合AI视觉和低功耗设计，为儿童提供高性能、可负担的假肢解决方案。

Abstract: This paper introduces a novel AI vision-enabled pediatric prosthetic hand
designed to assist children aged 10-12 with upper limb disabilities. The
prosthesis features an anthropomorphic appearance, multi-articulating
functionality, and a lightweight design that mimics a natural hand, making it
both accessible and affordable for low-income families. Using 3D printing
technology and integrating advanced machine vision, sensing, and embedded
computing, the prosthetic hand offers a low-cost, customizable solution that
addresses the limitations of current myoelectric prostheses. A micro camera is
interfaced with a low-power FPGA for real-time object detection and assists
with precise grasping. The onboard DL-based object detection and grasp
classification models achieved accuracies of 96% and 100% respectively. In the
force prediction, the mean absolute error was found to be 0.018. The features
of the proposed prosthetic hand can thus be summarized as: a) a wrist-mounted
micro camera for artificial sensing, enabling a wide range of hand-based tasks;
b) real-time object detection and distance estimation for precise grasping; and
c) ultra-low-power operation that delivers high performance within constrained
power and resource limits.

</details>


### [197] [Post-Convergence Sim-to-Real Policy Transfer: A Principled Alternative to Cherry-Picking](https://arxiv.org/abs/2504.15414)
*Dylan Khor,Bowen Weng*

Main category: cs.RO

TL;DR: 论文提出了一种基于最坏情况性能转移优化的方法，用于解决强化学习策略在模拟到现实转移中的后收敛问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注预收敛阶段的模拟到现实转移，但无法避免奖励的收敛轨迹和噪声振荡，导致策略选择依赖启发式或人工挑选。

Method: 提出了一种凸二次约束线性规划问题，用于优化最坏情况性能转移。

Result: 实验证明该方法在将基于强化学习的运动策略从模拟转移到现实实验室测试中有效。

Conclusion: 该方法为后收敛阶段的模拟到现实转移提供了一种有效解决方案。

Abstract: Learning-based approaches, particularly reinforcement learning (RL), have
become widely used for developing control policies for autonomous agents, such
as locomotion policies for legged robots. RL training typically maximizes a
predefined reward (or minimizes a corresponding cost/loss) by iteratively
optimizing policies within a simulator. Starting from a randomly initialized
policy, the empirical expected reward follows a trajectory with an overall
increasing trend. While some policies become temporarily stuck in local optima,
a well-defined training process generally converges to a reward level with
noisy oscillations. However, selecting a policy for real-world deployment is
rarely an analytical decision (i.e., simply choosing the one with the highest
reward) and is instead often performed through trial and error. To improve
sim-to-real transfer, most research focuses on the pre-convergence stage,
employing techniques such as domain randomization, multi-fidelity training,
adversarial training, and architectural innovations. However, these methods do
not eliminate the inevitable convergence trajectory and noisy oscillations of
rewards, leading to heuristic policy selection or cherry-picking. This paper
addresses the post-convergence sim-to-real transfer problem by introducing a
worst-case performance transference optimization approach, formulated as a
convex quadratic-constrained linear programming problem. Extensive experiments
demonstrate its effectiveness in transferring RL-based locomotion policies from
simulation to real-world laboratory tests.

</details>


### [198] [LAPP: Large Language Model Feedback for Preference-Driven Reinforcement Learning](https://arxiv.org/abs/2504.15472)
*Pingcheng Jian,Xiao Wei,Yanbaihui Liu,Samuel A. Moore,Michael M. Zavlanos,Boyuan Chen*

Main category: cs.RO

TL;DR: LAPP是一种利用大语言模型自动生成偏好标签的机器人学习框架，减少人工干预，实现高效、可定制和复杂行为的学习。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖人工设计奖励、演示或昂贵标签，LAPP旨在通过LLM自动生成偏好标签，降低人力成本并提升学习效率。

Method: LAPP结合LLM从RL收集的状态-动作轨迹中生成偏好标签，训练在线偏好预测器以指导策略优化。

Result: 在四足运动和灵巧操作任务中，LAPP实现高效学习、更高性能、快速适应和精确控制，如完成四足后空翻等高难度任务。

Conclusion: LAPP为可扩展的偏好驱动机器人学习提供了有前景的方向。

Abstract: We introduce Large Language Model-Assisted Preference Prediction (LAPP), a
novel framework for robot learning that enables efficient, customizable, and
expressive behavior acquisition with minimum human effort. Unlike prior
approaches that rely heavily on reward engineering, human demonstrations,
motion capture, or expensive pairwise preference labels, LAPP leverages large
language models (LLMs) to automatically generate preference labels from raw
state-action trajectories collected during reinforcement learning (RL). These
labels are used to train an online preference predictor, which in turn guides
the policy optimization process toward satisfying high-level behavioral
specifications provided by humans. Our key technical contribution is the
integration of LLMs into the RL feedback loop through trajectory-level
preference prediction, enabling robots to acquire complex skills including
subtle control over gait patterns and rhythmic timing. We evaluate LAPP on a
diverse set of quadruped locomotion and dexterous manipulation tasks and show
that it achieves efficient learning, higher final performance, faster
adaptation, and precise control of high-level behaviors. Notably, LAPP enables
robots to master highly dynamic and expressive tasks such as quadruped
backflips, which remain out of reach for standard LLM-generated or handcrafted
rewards. Our results highlight LAPP as a promising direction for scalable
preference-driven robot learning.

</details>


### [199] [Dynamic Intent Queries for Motion Transformer-based Trajectory Prediction](https://arxiv.org/abs/2504.15766)
*Tobias Demmler,Lennart Hartung,Andreas Tamke,Thao Dang,Alexander Hegai,Karsten Haug,Lars Mikelsons*

Main category: cs.RO

TL;DR: 论文提出了一种改进的轨迹预测方法，通过动态意图点提升MTR模型的预测准确性，尤其在长时预测中表现显著。


<details>
  <summary>Details</summary>
Motivation: 静态意图点与地图数据不匹配导致预测不准确，需动态调整以适应复杂交通场景。

Method: 在MTR模型中引入场景特定的动态意图点，并在Waymo Open Motion Dataset上训练和评估。

Result: 动态意图点显著提高了轨迹预测准确性，尤其是长时预测，并对不符合地图数据的真实轨迹有积极影响。

Conclusion: 动态意图点的引入有效解决了静态点的局限性，提升了轨迹预测的实用性。

Abstract: In autonomous driving, accurately predicting the movements of other traffic
participants is crucial, as it significantly influences a vehicle's planning
processes. Modern trajectory prediction models strive to interpret complex
patterns and dependencies from agent and map data. The Motion Transformer (MTR)
architecture and subsequent work define the most accurate methods in common
benchmarks such as the Waymo Open Motion Benchmark. The MTR model employs
pre-generated static intention points as initial goal points for trajectory
prediction. However, the static nature of these points frequently leads to
misalignment with map data in specific traffic scenarios, resulting in
unfeasible or unrealistic goal points. Our research addresses this limitation
by integrating scene-specific dynamic intention points into the MTR model. This
adaptation of the MTR model was trained and evaluated on the Waymo Open Motion
Dataset. Our findings demonstrate that incorporating dynamic intention points
has a significant positive impact on trajectory prediction accuracy, especially
for predictions over long time horizons. Furthermore, we analyze the impact on
ground truth trajectories which are not compliant with the map data or are
illegal maneuvers.

</details>


### [200] [Few-Shot Vision-Language Action-Incremental Policy Learning](https://arxiv.org/abs/2504.15517)
*Mingchen Song,Xiang Deng,Guoqiang Zhong,Qi Lv,Jia Wan,Yinchuan Li,Jianye Hao,Weili Guan*

Main category: cs.RO

TL;DR: 该论文提出了一种名为TOPIC的方法，用于解决机器人模仿学习中的数据稀缺和持续学习问题，通过任务特定提示和连续演化策略显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer的机器人操作方法需要大量演示数据，且缺乏在少量演示下持续学习新任务的能力。

Method: 设计了任务提示图演化策略（TOPIC），包括任务特定提示（TSP）和连续演化策略（CES），通过多模态信息交互和任务关系图实现高效学习和技能复用。

Result: TOPIC在机器人操作任务中实现了26%以上的成功率提升，显著优于现有基线方法。

Conclusion: TOPIC为机器人模仿学习中的少样本持续学习提供了有效解决方案，具有广泛的应用潜力。

Abstract: Recently, Transformer-based robotic manipulation methods utilize multi-view
spatial representations and language instructions to learn robot motion
trajectories by leveraging numerous robot demonstrations. However, the
collection of robot data is extremely challenging, and existing methods lack
the capability for continuous learning on new tasks with only a few
demonstrations. In this paper, we formulate these challenges as the Few-Shot
Action-Incremental Learning (FSAIL) task, and accordingly design a Task-prOmpt
graPh evolutIon poliCy (TOPIC) to address these issues. Specifically, to
address the data scarcity issue in robotic imitation learning, TOPIC learns
Task-Specific Prompts (TSP) through the deep interaction of multi-modal
information within few-shot demonstrations, thereby effectively extracting the
task-specific discriminative information. On the other hand, to enhance the
capability for continual learning on new tasks and mitigate the issue of
catastrophic forgetting, TOPIC adopts a Continuous Evolution Strategy (CES).
CES leverages the intrinsic relationships between tasks to construct a task
relation graph, which effectively facilitates the adaptation of new tasks by
reusing skills learned from previous tasks. TOPIC pioneers few-shot continual
learning in the robotic manipulation task, and extensive experimental results
demonstrate that TOPIC outperforms state-of-the-art baselines by over 26$\%$ in
success rate, significantly enhancing the continual learning capabilities of
existing Transformer-based policies.

</details>


### [201] [RiskNet: Interaction-Aware Risk Forecasting for Autonomous Driving in Long-Tail Scenarios](https://arxiv.org/abs/2504.15541)
*Qichao Liu,Heye Huang,Shiyue Zhao,Lei Shi,Soyoung Ahn,Xiaopeng Li*

Main category: cs.RO

TL;DR: RiskNet是一个交互感知的风险预测框架，结合确定性风险建模与概率行为预测，用于自动驾驶车辆在复杂多代理交互中的安全评估。


<details>
  <summary>Details</summary>
Motivation: 解决自动驾驶车辆在长尾场景和高不确定性环境中的安全问题。

Method: 采用场论模型捕捉车辆、周围代理和基础设施的交互，结合GNN轨迹预测模块学习多模态未来运动分布。

Result: 在多个数据集上显著优于传统方法（如TTC、THW、RSS、NC Field），具有更高的准确性、响应性和方向敏感性。

Conclusion: RiskNet为长尾场景下的安全决策提供了统一基础，支持实时、场景自适应的风险预测。

Abstract: Ensuring the safety of autonomous vehicles (AVs) in long-tail scenarios
remains a critical challenge, particularly under high uncertainty and complex
multi-agent interactions. To address this, we propose RiskNet, an
interaction-aware risk forecasting framework, which integrates deterministic
risk modeling with probabilistic behavior prediction for comprehensive risk
assessment. At its core, RiskNet employs a field-theoretic model that captures
interactions among ego vehicle, surrounding agents, and infrastructure via
interaction fields and force. This model supports multidimensional risk
evaluation across diverse scenarios (highways, intersections, and roundabouts),
and shows robustness under high-risk and long-tail settings. To capture the
behavioral uncertainty, we incorporate a graph neural network (GNN)-based
trajectory prediction module, which learns multi-modal future motion
distributions. Coupled with the deterministic risk field, it enables dynamic,
probabilistic risk inference across time, enabling proactive safety assessment
under uncertainty. Evaluations on the highD, inD, and rounD datasets, spanning
lane changes, turns, and complex merges, demonstrate that our method
significantly outperforms traditional approaches (e.g., TTC, THW, RSS, NC
Field) in terms of accuracy, responsiveness, and directional sensitivity, while
maintaining strong generalization across scenarios. This framework supports
real-time, scenario-adaptive risk forecasting and demonstrates strong
generalization across uncertain driving environments. It offers a unified
foundation for safety-critical decision-making in long-tail scenarios.

</details>


### [202] [SPECI: Skill Prompts based Hierarchical Continual Imitation Learning for Robot Manipulation](https://arxiv.org/abs/2504.15561)
*Jingkai Xu,Xiangli Nie*

Main category: cs.RO

TL;DR: SPECI是一种新型的端到端分层持续模仿学习框架，通过动态技能提取和任务级知识转移，显著提升了机器人操作的适应性和性能。


<details>
  <summary>Details</summary>
Motivation: 传统模仿学习依赖静态训练，无法适应动态环境；现有持续模仿学习方法忽视技能特性或依赖手动定义技能，导致跨任务知识转移不足。

Method: SPECI框架包括多模态感知融合模块、高层技能推理模块和低层动作执行模块，通过可扩展技能代码本和注意力驱动机制实现技能获取与重用。

Result: 实验表明，SPECI在所有评估指标上均优于现有持续模仿学习方法，展现出卓越的双向知识转移和整体性能。

Conclusion: SPECI通过动态技能提取和任务级参数优化，为机器人操作提供了高效的终身适应能力。

Abstract: Real-world robot manipulation in dynamic unstructured environments requires
lifelong adaptability to evolving objects, scenes and tasks. Traditional
imitation learning relies on static training paradigms, which are ill-suited
for lifelong adaptation. Although Continual Imitation Learnin (CIL) enables
incremental task adaptation while preserving learned knowledge, current CIL
methods primarily overlook the intrinsic skill characteristics of robot
manipulation or depend on manually defined and rigid skills, leading to
suboptimal cross-task knowledge transfer. To address these issues, we propose
Skill Prompts-based HiErarchical Continual Imitation Learning (SPECI), a novel
end-to-end hierarchical CIL policy architecture for robot manipulation. The
SPECI framework consists of a multimodal perception and fusion module for
heterogeneous sensory information encoding, a high-level skill inference module
for dynamic skill extraction and selection, and a low-level action execution
module for precise action generation. To enable efficient knowledge transfer on
both skill and task levels, SPECI performs continual implicit skill acquisition
and reuse via an expandable skill codebook and an attention-driven skill
selection mechanism. Furthermore, we introduce mode approximation to augment
the last two modules with task-specific and task-sharing parameters, thereby
enhancing task-level knowledge transfer. Extensive experiments on diverse
manipulation task suites demonstrate that SPECI consistently outperforms
state-of-the-art CIL methods across all evaluated metrics, revealing
exceptional bidirectional knowledge transfer and superior overall performance.

</details>


### [203] [Bidirectional Task-Motion Planning Based on Hierarchical Reinforcement Learning for Strategic Confrontation](https://arxiv.org/abs/2504.15876)
*Qizhen Wu Lei Chen,Kexin Liu,Jinhu Lü*

Main category: cs.RO

TL;DR: 提出了一种基于分层强化学习的双向方法，用于群机器人对抗场景中的高效决策，结合离散命令和连续动作，显著提升了动态环境中的适应性和性能。


<details>
  <summary>Details</summary>
Motivation: 传统任务与运动规划方法因单向结构无法捕捉层次间相互依赖，限制了动态环境中的适应性。

Method: 采用分层强化学习的双向方法，结合交叉训练技术和轨迹预测模型，实现任务分配与路径规划的动态交互。

Result: 实验显示对抗胜率超过80%，决策时间低于0.01秒，优于现有方法。

Conclusion: 该方法在群机器人对抗场景中表现出优异的泛化能力和实际应用潜力。

Abstract: In swarm robotics, confrontation scenarios, including strategic
confrontations, require efficient decision-making that integrates discrete
commands and continuous actions. Traditional task and motion planning methods
separate decision-making into two layers, but their unidirectional structure
fails to capture the interdependence between these layers, limiting
adaptability in dynamic environments. Here, we propose a novel bidirectional
approach based on hierarchical reinforcement learning, enabling dynamic
interaction between the layers. This method effectively maps commands to task
allocation and actions to path planning, while leveraging cross-training
techniques to enhance learning across the hierarchical framework. Furthermore,
we introduce a trajectory prediction model that bridges abstract task
representations with actionable planning goals. In our experiments, it achieves
over 80\% in confrontation win rate and under 0.01 seconds in decision time,
outperforming existing approaches. Demonstrations through large-scale tests and
real-world robot experiments further emphasize the generalization capabilities
and practical applicability of our method.

</details>

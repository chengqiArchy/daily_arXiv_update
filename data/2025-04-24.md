<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 36]
- [cs.AI](#cs.AI) [Total: 9]
- [cs.LG](#cs.LG) [Total: 51]
- [cs.NE](#cs.NE) [Total: 1]
- [cs.DB](#cs.DB) [Total: 1]
- [eess.IV](#eess.IV) [Total: 1]
- [cs.DS](#cs.DS) [Total: 1]
- [cs.SD](#cs.SD) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [cs.CY](#cs.CY) [Total: 10]
- [eess.SY](#eess.SY) [Total: 2]
- [q-bio.NC](#q-bio.NC) [Total: 2]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [astro-ph.IM](#astro-ph.IM) [Total: 1]
- [math.NA](#math.NA) [Total: 1]
- [cs.IR](#cs.IR) [Total: 6]
- [quant-ph](#quant-ph) [Total: 3]
- [math.OC](#math.OC) [Total: 2]
- [cs.NI](#cs.NI) [Total: 1]
- [cs.HC](#cs.HC) [Total: 6]
- [math.ST](#math.ST) [Total: 1]
- [cs.CR](#cs.CR) [Total: 14]
- [stat.ML](#stat.ML) [Total: 2]
- [eess.SP](#eess.SP) [Total: 8]
- [cs.MA](#cs.MA) [Total: 1]
- [q-bio.BM](#q-bio.BM) [Total: 2]
- [cs.RO](#cs.RO) [Total: 7]
- [cs.SE](#cs.SE) [Total: 4]
- [cs.AR](#cs.AR) [Total: 4]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.DC](#cs.DC) [Total: 3]
- [physics.ao-ph](#physics.ao-ph) [Total: 1]
- [cs.CV](#cs.CV) [Total: 22]
- [q-fin.PM](#q-fin.PM) [Total: 1]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]
- [stat.ME](#stat.ME) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [FinNLI: Novel Dataset for Multi-Genre Financial Natural Language Inference Benchmarking](https://arxiv.org/abs/2504.16188)
*Jabez Magomere,Elena Kochkina,Samuel Mensah,Simerjot Kaur,Charese H. Smiley*

Main category: cs.CL

TL;DR: FinNLI是一个金融自然语言推理基准数据集，包含21,304个前提-假设对，测试集由金融专家标注。评估显示领域转移显著降低通用NLI性能，当前LLM在金融推理中表现不佳。


<details>
  <summary>Details</summary>
Motivation: 构建一个多样化的金融文本数据集，以评估和改进金融领域的自然语言推理能力。

Method: 创建FinNLI数据集，包含SEC文件、年度报告和财报电话记录等金融文本，确保前提-假设对的多样性并减少虚假相关性。

Result: 预训练模型和大型语言模型的最高Macro F1分别为74.57%和78.62%，指令调优的金融LLM表现较差。

Conclusion: FinNLI揭示了当前LLM在金融推理中的不足，表明仍有改进空间。

Abstract: We introduce FinNLI, a benchmark dataset for Financial Natural Language
Inference (FinNLI) across diverse financial texts like SEC Filings, Annual
Reports, and Earnings Call transcripts. Our dataset framework ensures diverse
premise-hypothesis pairs while minimizing spurious correlations. FinNLI
comprises 21,304 pairs, including a high-quality test set of 3,304 instances
annotated by finance experts. Evaluations show that domain shift significantly
degrades general-domain NLI performance. The highest Macro F1 scores for
pre-trained (PLMs) and large language models (LLMs) baselines are 74.57% and
78.62%, respectively, highlighting the dataset's difficulty. Surprisingly,
instruction-tuned financial LLMs perform poorly, suggesting limited
generalizability. FinNLI exposes weaknesses in current LLMs for financial
reasoning, indicating room for improvement.

</details>


### [2] [The Language of Attachment: Modeling Attachment Dynamics in Psychotherapy](https://arxiv.org/abs/2504.16271)
*Frederik Bredgaard,Martin Lund Trinhammer,Elisa Bassignana*

Main category: cs.CL

TL;DR: 论文探讨了利用NLP技术自动评估患者依恋风格，以改进心理治疗的效果和研究。


<details>
  <summary>Details</summary>
Motivation: 目前依恋风格的评估依赖复杂且耗资源的人工方法（PACS），限制了其广泛应用。通过NLP自动化评估可以提升效率和可扩展性。

Method: 使用NLP分类模型对心理治疗转录文本进行分析，自动识别患者依恋风格。

Result: 研究发现自动化工具在评估依恋风格时存在挑战（如混淆“焦虑型”和“回避型”），但为个性化治疗和研究提供了新途径。

Conclusion: NLP技术为心理治疗的个性化和机制研究开辟了新方向，尽管仍需解决分类准确性等问题。

Abstract: The delivery of mental healthcare through psychotherapy stands to benefit
immensely from developments within Natural Language Processing (NLP), in
particular through the automatic identification of patient specific qualities,
such as attachment style. Currently, the assessment of attachment style is
performed manually using the Patient Attachment Coding System (PACS; Talia et
al., 2017), which is complex, resource-consuming and requires extensive
training. To enable wide and scalable adoption of attachment informed treatment
and research, we propose the first exploratory analysis into automatically
assessing patient attachment style from psychotherapy transcripts using NLP
classification models. We further analyze the results and discuss the
implications of using automated tools for this purpose -- e.g., confusing
`preoccupied' patients with `avoidant' likely has a more negative impact on
therapy outcomes with respect to other mislabeling. Our work opens an avenue of
research enabling more personalized psychotherapy and more targeted research
into the mechanisms of psychotherapy through advancements in NLP.

</details>


### [3] [The Paradox of Poetic Intent in Back-Translation: Evaluating the Quality of Large Language Models in Chinese Translation](https://arxiv.org/abs/2504.16286)
*Li Weigang,Pedro Carvalho Brom*

Main category: cs.CL

TL;DR: 该研究评估了大型语言模型（LLMs）和传统翻译工具在中文-英文翻译中的表现，重点关注诗歌意图、文化传承和专业术语处理。通过构建多样化语料库和使用BT-Fried评估系统，发现LLMs在科学摘要中表现较好，但在文化和文学翻译中表现不佳。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决LLMs在中文-英文翻译中保留诗歌意图、文化传承和处理专业术语的挑战。

Method: 构建多样化语料库，使用BT-Fried评估系统（结合回译和Friedman测试）评估六种LLMs和三种传统工具的性能。

Result: LLMs在科学摘要中表现较好，但在文化和文学翻译中表现不佳；传统工具在语言差异大的文本中更优；提出了一种新的BLEU变体。

Conclusion: 研究为中文NLP性能的实证评估提供了贡献，并深化了对AI翻译中文化保真度的理解。

Abstract: The rapid advancement of large language models (LLMs) has reshaped the
landscape of machine translation, yet challenges persist in preserving poetic
intent, cultural heritage, and handling specialized terminology in
Chinese-English translation. This study constructs a diverse corpus
encompassing Chinese scientific terminology, historical translation paradoxes,
and literary metaphors. Utilizing a back-translation and Friedman test-based
evaluation system (BT-Fried), we evaluate BLEU, CHRF, TER, and semantic
similarity metrics across six major LLMs (e.g., GPT-4.5, DeepSeek V3) and three
traditional translation tools. Key findings include: (1) Scientific abstracts
often benefit from back-translation, while traditional tools outperform LLMs in
linguistically distinct texts; (2) LLMs struggle with cultural and literary
retention, exemplifying the "paradox of poetic intent"; (3) Some models exhibit
"verbatim back-translation", reflecting emergent memory behavior; (4) A novel
BLEU variant using Jieba segmentation and n-gram weighting is proposed. The
study contributes to the empirical evaluation of Chinese NLP performance and
advances understanding of cultural fidelity in AI-mediated translation.

</details>


### [4] [Capturing Symmetry and Antisymmetry in Language Models through Symmetry-Aware Training Objectives](https://arxiv.org/abs/2504.16312)
*Zhangdie Yuan,Andreas Vlachos*

Main category: cs.CL

TL;DR: 论文提出了一种基于Wikidata的自然语言推理数据集，用于评估大语言模型（LLMs）在处理对称和反对称关系时的表现，发现LLMs表现不佳，并通过对比学习改进编码器。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在对称和反对称关系理解上的不足。

Method: 引入Wikidata数据集评估LLMs，并通过对比学习重新训练编码器。

Result: LLMs表现接近随机，改进后的编码器性能与微调分类头相当，且在少样本学习和减轻灾难性遗忘方面更优。

Conclusion: 对比学习改进的编码器在关系理解任务中表现优异，具有高效性和鲁棒性。

Abstract: Capturing symmetric (e.g., country borders another country) and antisymmetric
(e.g., parent_of) relations is crucial for a variety of applications. This
paper tackles this challenge by introducing a novel Wikidata-derived natural
language inference dataset designed to evaluate large language models (LLMs).
Our findings reveal that LLMs perform comparably to random chance on this
benchmark, highlighting a gap in relational understanding. To address this, we
explore encoder retraining via contrastive learning with k-nearest neighbors.
The retrained encoder matches the performance of fine-tuned classification
heads while offering additional benefits, including greater efficiency in
few-shot learning and improved mitigation of catastrophic forgetting.

</details>


### [5] [Transformer-Based Extraction of Statutory Definitions from the U.S. Code](https://arxiv.org/abs/2504.16353)
*Arpana Hosabettu,Harsh Shah*

Main category: cs.CL

TL;DR: 本文提出了一种基于Transformer架构的NLP系统，用于从美国法典（U.S.C.）中自动提取法律定义、定义范围及其术语，显著提高了提取准确性。


<details>
  <summary>Details</summary>
Motivation: 提升对复杂法律文本（如美国法典）的理解和清晰度，解决自动识别法律定义及其范围的挑战。

Method: 采用领域特定的Transformer模型（Legal-BERT），结合多阶段管道处理法律文本，包括段落分类、定义单元聚合及注意力机制与规则模式的结合。

Result: 在多个美国法典标题上评估，最佳模型达到96.8%的精确率和98.9%的召回率（F1分数98.2%），显著优于传统机器学习分类器。

Conclusion: 该工作不仅提高了法律信息的可访问性和理解，还为下游法律推理任务奠定了基础。

Abstract: Automatic extraction of definitions from legal texts is critical for
enhancing the comprehension and clarity of complex legal corpora such as the
United States Code (U.S.C.). We present an advanced NLP system leveraging
transformer-based architectures to automatically extract defined terms, their
definitions, and their scope from the U.S.C. We address the challenges of
automatically identifying legal definitions, extracting defined terms, and
determining their scope within this complex corpus of over 200,000 pages of
federal statutory law. Building upon previous feature-based machine learning
methods, our updated model employs domain-specific transformers (Legal-BERT)
fine-tuned specifically for statutory texts, significantly improving extraction
accuracy. Our work implements a multi-stage pipeline that combines document
structure analysis with state-of-the-art language models to process legal text
from the XML version of the U.S. Code. Each paragraph is first classified using
a fine-tuned legal domain BERT model to determine if it contains a definition.
Our system then aggregates related paragraphs into coherent definitional units
and applies a combination of attention mechanisms and rule-based patterns to
extract defined terms and their jurisdictional scope. The definition extraction
system is evaluated on multiple titles of the U.S. Code containing thousands of
definitions, demonstrating significant improvements over previous approaches.
Our best model achieves 96.8% precision and 98.9% recall (98.2% F1-score),
substantially outperforming traditional machine learning classifiers. This work
contributes to improving accessibility and understanding of legal information
while establishing a foundation for downstream legal reasoning tasks.

</details>


### [6] [Text-to-TrajVis: Enabling Trajectory Data Visualizations from Natural Language Questions](https://arxiv.org/abs/2504.16358)
*Tian Bai,Huiyan Ying,Kailong Suo,Junqiu Wei,Tao Fan,Yuanfeng Song*

Main category: cs.CL

TL;DR: 论文提出Text-to-TrajVis任务，将自然语言问题转化为轨迹数据可视化，并构建首个大规模数据集TrajVL。


<details>
  <summary>Details</summary>
Motivation: 解决轨迹可视化系统中自然语言接口的缺失问题，填补相关数据集的空白。

Method: 设计轨迹可视化语言（TVL），结合LLMs与人工构建高质量数据集TrajVL，并评估多种LLMs性能。

Result: 实验证明任务可行且具挑战性，TrajVL包含18,140对（问题，TVL）数据。

Conclusion: Text-to-TrajVis任务值得进一步研究，TrajVL为未来工作提供了基础。

Abstract: This paper introduces the Text-to-TrajVis task, which aims to transform
natural language questions into trajectory data visualizations, facilitating
the development of natural language interfaces for trajectory visualization
systems. As this is a novel task, there is currently no relevant dataset
available in the community. To address this gap, we first devised a new
visualization language called Trajectory Visualization Language (TVL) to
facilitate querying trajectory data and generating visualizations. Building on
this foundation, we further proposed a dataset construction method that
integrates Large Language Models (LLMs) with human efforts to create
high-quality data. Specifically, we first generate TVLs using a comprehensive
and systematic process, and then label each TVL with corresponding natural
language questions using LLMs. This process results in the creation of the
first large-scale Text-to-TrajVis dataset, named TrajVL, which contains 18,140
(question, TVL) pairs. Based on this dataset, we systematically evaluated the
performance of multiple LLMs (GPT, Qwen, Llama, etc.) on this task. The
experimental results demonstrate that this task is both feasible and highly
challenging and merits further exploration within the research community.

</details>


### [7] [SplitReason: Learning To Offload Reasoning](https://arxiv.org/abs/2504.16379)
*Yash Akhauri,Anthony Fei,Chi-Chih Chang,Ahmed F. AbouElhamayed,Yueying Li,Mohamed S. Abdelfattah*

Main category: cs.CL

TL;DR: 论文提出了一种通过将推理中最具挑战性的部分卸载到更大模型的方法，以提高效率并保持准确性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在推理任务中生成较长的序列，导致效率低下，但并非所有部分都同样难以生成。

Method: 通过标注困难部分并训练小模型识别和触发卸载，结合监督和强化学习微调。

Result: AIME24推理准确率提高了24%和28.3%，同时仅卸载了1.35%和5%的生成标记。

Conclusion: SplitReason方法有效平衡了效率和准确性，相关数据和模型已开源。

Abstract: Reasoning in large language models (LLMs) tends to produce substantially
longer token generation sequences than simpler language modeling tasks. This
extended generation length reflects the multi-step, compositional nature of
reasoning and is often correlated with higher solution accuracy. From an
efficiency perspective, longer token generation exacerbates the inherently
sequential and memory-bound decoding phase of LLMs. However, not all parts of
this expensive reasoning process are equally difficult to generate. We leverage
this observation by offloading only the most challenging parts of the reasoning
process to a larger, more capable model, while performing most of the
generation with a smaller, more efficient model; furthermore, we teach the
smaller model to identify these difficult segments and independently trigger
offloading when needed. To enable this behavior, we annotate difficult segments
across 18k reasoning traces from the OpenR1-Math-220k chain-of-thought (CoT)
dataset. We then apply supervised fine-tuning (SFT) and reinforcement learning
fine-tuning (RLFT) to a 1.5B-parameter reasoning model, training it to learn to
offload the most challenging parts of its own reasoning process to a larger
model. This approach improves AIME24 reasoning accuracy by 24% and 28.3% while
offloading 1.35% and 5% of the generated tokens respectively. We open-source
our SplitReason model, data, code and logs.

</details>


### [8] [ConTextual: Improving Clinical Text Summarization in LLMs with Context-preserving Token Filtering and Knowledge Graphs](https://arxiv.org/abs/2504.16394)
*Fahmida Liza Piya,Rahmatollah Beheshti*

Main category: cs.CL

TL;DR: 论文提出了一种名为ConTextual的新框架，通过结合上下文保留令牌过滤方法和领域特定知识图谱，提升临床文本摘要的准确性和临床相关性。


<details>
  <summary>Details</summary>
Motivation: 临床非结构化数据蕴含丰富信息，但现有方法未能有效提取关键内容，导致决策支持不足。

Method: 提出ConTextual框架，结合上下文保留令牌过滤和知识图谱增强，优化临床文本摘要。

Result: 在公开数据集上，ConTextual表现优于基线方法，提升了语言连贯性和临床保真度。

Conclusion: ConTextual展示了令牌级过滤与结构化检索的互补作用，为临床文本生成提供了可扩展的高精度解决方案。

Abstract: Unstructured clinical data can serve as a unique and rich source of
information that can meaningfully inform clinical practice. Extracting the most
pertinent context from such data is critical for exploiting its true potential
toward optimal and timely decision-making in patient care. While prior research
has explored various methods for clinical text summarization, most prior
studies either process all input tokens uniformly or rely on heuristic-based
filters, which can overlook nuanced clinical cues and fail to prioritize
information critical for decision-making. In this study, we propose Contextual,
a novel framework that integrates a Context-Preserving Token Filtering method
with a Domain-Specific Knowledge Graph (KG) for contextual augmentation. By
preserving context-specific important tokens and enriching them with structured
knowledge, ConTextual improves both linguistic coherence and clinical fidelity.
Our extensive empirical evaluations on two public benchmark datasets
demonstrate that ConTextual consistently outperforms other baselines. Our
proposed approach highlights the complementary role of token-level filtering
and structured retrieval in enhancing both linguistic and clinical integrity,
as well as offering a scalable solution for improving precision in clinical
text generation.

</details>


### [9] [Less is More: Enhancing Structured Multi-Agent Reasoning via Quality-Guided Distillation](https://arxiv.org/abs/2504.16408)
*Jiahao Yuan,Xingzhe Sun,Xing Yu,Jingwen Wang,Dehui Du,Zhiqing Cui,Zixiang Di*

Main category: cs.CL

TL;DR: 论文介绍了在低资源条件下通过多智能体框架和反向提示诱导等方法提升结构化推理质量的第三名获奖方案Less is More。


<details>
  <summary>Details</summary>
Motivation: 解决低资源结构化推理任务中LLMs生成可解释、逐步推理的挑战。

Method: 采用多智能体框架、反向提示诱导、检索增强推理合成和双阶段奖励引导过滤，结合LoRA+微调Meta-Llama-3-8B-Instruct。

Result: 方法在仅24个标注样本下显著提升结构化推理质量。

Conclusion: 可控数据蒸馏在低资源条件下对结构化推理具有重要价值。

Abstract: The XLLM@ACL2025 Shared Task-III formulates a low-resource structural
reasoning task that challenges LLMs to generate interpretable, step-by-step
rationales with minimal labeled data. We present Less is More, the third-place
winning approach in the XLLM@ACL2025 Shared Task-III, which focuses on
structured reasoning from only 24 labeled examples. Our approach leverages a
multi-agent framework with reverse-prompt induction, retrieval-augmented
reasoning synthesis via GPT-4o, and dual-stage reward-guided filtering to
distill high-quality supervision across three subtasks: question parsing, CoT
parsing, and step-level verification. All modules are fine-tuned from
Meta-Llama-3-8B-Instruct under a unified LoRA+ setup. By combining structure
validation with reward filtering across few-shot and zero-shot prompts, our
pipeline consistently improves structure reasoning quality. These results
underscore the value of controllable data distillation in enhancing structured
inference under low-resource constraints. Our code is available at
https://github.com/Jiahao-Yuan/Less-is-More.

</details>


### [10] [Out-of-the-Box Conditional Text Embeddings from Large Language Models](https://arxiv.org/abs/2504.16411)
*Kosuke Yamada,Peinan Zhang*

Main category: cs.CL

TL;DR: PonTE是一种无监督条件文本嵌入方法，利用因果大语言模型和条件提示，无需微调即可生成有用的条件文本嵌入，性能接近监督方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖大量训练数据进行微调，导致劳动力和资源成本高，需要一种更高效的无监督方法。

Method: 提出PonTE方法，结合因果大语言模型和条件提示，通过条件语义文本相似性和文本聚类实验验证。

Result: PonTE生成的条件文本嵌入性能接近监督方法，并通过提示词生成和嵌入可视化展示可解释性。

Conclusion: PonTE是一种高效的无监督条件文本嵌入方法，具有实际应用潜力。

Abstract: Conditional text embedding is a proposed representation that captures the
shift in perspective on texts when conditioned on a specific aspect. Previous
methods have relied on extensive training data for fine-tuning models, leading
to challenges in terms of labor and resource costs. We propose PonTE, a novel
unsupervised conditional text embedding method that leverages a causal large
language model and a conditional prompt. Through experiments on conditional
semantic text similarity and text clustering, we demonstrate that PonTE can
generate useful conditional text embeddings and achieve performance comparable
to supervised methods without fine-tuning. We also show the interpretability of
text embeddings with PonTE by analyzing word generation following prompts and
embedding visualization.

</details>


### [11] [Evaluating Multi-Hop Reasoning in Large Language Models: A Chemistry-Centric Case Study](https://arxiv.org/abs/2504.16414)
*Mohammad Khodadad,Ali Shiraee Kasmaee,Mahdi Astaraki,Nicholas Sherck,Hamidreza Mahyar,Soheila Samiee*

Main category: cs.CL

TL;DR: 该研究提出了一个新的基准测试，用于评估大型语言模型在化学领域的组合推理能力，并开发了一个自动化流程来生成多跳问题。实验表明，即使是先进模型在多跳推理中也面临挑战，强调了上下文增强的重要性。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型在化学领域的组合推理能力，并探索如何通过上下文增强提升其表现。

Method: 结合OpenAI推理模型和命名实体识别（NER）系统，从文献中提取化学实体并构建知识图谱，生成多跳问题进行评估。

Result: 实验显示，先进模型在多跳推理中表现不佳，上下文增强能显著提升性能，但无法完全消除推理错误。

Conclusion: 该研究不仅揭示了当前模型的局限性，还提出了一种新的数据生成流程，为计算语言学的推理研究提供了新方向。

Abstract: In this study, we introduced a new benchmark consisting of a curated dataset
and a defined evaluation process to assess the compositional reasoning
capabilities of large language models within the chemistry domain. We designed
and validated a fully automated pipeline, verified by subject matter experts,
to facilitate this task. Our approach integrates OpenAI reasoning models with
named entity recognition (NER) systems to extract chemical entities from recent
literature, which are then augmented with external knowledge bases to form a
comprehensive knowledge graph. By generating multi-hop questions across these
graphs, we assess LLM performance in both context-augmented and non-context
augmented settings. Our experiments reveal that even state-of-the-art models
face significant challenges in multi-hop compositional reasoning. The results
reflect the importance of augmenting LLMs with document retrieval, which can
have a substantial impact on improving their performance. However, even perfect
retrieval accuracy with full context does not eliminate reasoning errors,
underscoring the complexity of compositional reasoning. This work not only
benchmarks and highlights the limitations of current LLMs but also presents a
novel data generation pipeline capable of producing challenging reasoning
datasets across various domains. Overall, this research advances our
understanding of reasoning in computational linguistics.

</details>


### [12] [Can Large Language Models Help Multimodal Language Analysis? MMLA: A Comprehensive Benchmark](https://arxiv.org/abs/2504.16427)
*Hanlei Zhang,Zhuohang Li,Yeshuang Zhu,Hua Xu,Peiwu Wang,Jinchao Zhang,Jie Zhou,Haige Zhu*

Main category: cs.CL

TL;DR: 论文介绍了MMLA基准测试，用于评估多模态大语言模型（MLLMs）在理解认知级语义方面的能力，覆盖六种核心维度。实验显示当前模型的准确率仅为60%~70%，凸显其局限性。


<details>
  <summary>Details</summary>
Motivation: 当前研究未充分探索MLLMs在理解认知级语义方面的能力，因此需要开发一个全面的基准测试来填补这一空白。

Method: 提出MMLA基准测试，包含61K多模态话语，评估了八种主流LLMs和MLLMs，采用零样本推理、监督微调和指令调优三种方法。

Result: 实验表明，即使是微调后的模型，准确率也仅为60%~70%，表明当前MLLMs在复杂语义理解上的不足。

Conclusion: MMLA为探索大语言模型在多模态语言分析中的潜力提供了基础，并推动了该领域的发展。

Abstract: Multimodal language analysis is a rapidly evolving field that leverages
multiple modalities to enhance the understanding of high-level semantics
underlying human conversational utterances. Despite its significance, little
research has investigated the capability of multimodal large language models
(MLLMs) to comprehend cognitive-level semantics. In this paper, we introduce
MMLA, a comprehensive benchmark specifically designed to address this gap. MMLA
comprises over 61K multimodal utterances drawn from both staged and real-world
scenarios, covering six core dimensions of multimodal semantics: intent,
emotion, dialogue act, sentiment, speaking style, and communication behavior.
We evaluate eight mainstream branches of LLMs and MLLMs using three methods:
zero-shot inference, supervised fine-tuning, and instruction tuning. Extensive
experiments reveal that even fine-tuned models achieve only about 60%~70%
accuracy, underscoring the limitations of current MLLMs in understanding
complex human language. We believe that MMLA will serve as a solid foundation
for exploring the potential of large language models in multimodal language
analysis and provide valuable resources to advance this field. The datasets and
code are open-sourced at https://github.com/thuiar/MMLA.

</details>


### [13] [EMRModel: A Large Language Model for Extracting Medical Consultation Dialogues into Structured Medical Records](https://arxiv.org/abs/2504.16448)
*Shuguang Zhao,Qiangzhong Feng,Zhiyang He,Peipei Sun,Yingying Wang,Xiaodong Tao,Xiaoliang Lu,Mei Cheng,Xinyue Wu,Yanyan Wang,Wei Liang*

Main category: cs.CL

TL;DR: EMRModel结合LoRA微调和代码风格提示设计，将医疗咨询对话高效转换为结构化电子病历，实验结果显示其F1分数达88.1%，优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 医疗咨询对话的非结构化特性限制了其在诊断和治疗中的有效利用，传统方法难以捕捉深层语义。

Method: 提出EMRModel，整合LoRA微调和代码风格提示设计，并构建高质量标注数据集。

Result: EMRModel的F1分数为88.1%，比标准预训练模型提升49.5%，且优于传统LoRA微调方法。

Conclusion: EMRModel在结构化医疗记录提取任务中表现优异，推动了医疗NLP模型的优化。

Abstract: Medical consultation dialogues contain critical clinical information, yet
their unstructured nature hinders effective utilization in diagnosis and
treatment. Traditional methods, relying on rule-based or shallow machine
learning techniques, struggle to capture deep and implicit semantics. Recently,
large pre-trained language models and Low-Rank Adaptation (LoRA), a lightweight
fine-tuning method, have shown promise for structured information extraction.
We propose EMRModel, a novel approach that integrates LoRA-based fine-tuning
with code-style prompt design, aiming to efficiently convert medical
consultation dialogues into structured electronic medical records (EMRs).
Additionally, we construct a high-quality, realistically grounded dataset of
medical consultation dialogues with detailed annotations. Furthermore, we
introduce a fine-grained evaluation benchmark for medical consultation
information extraction and provide a systematic evaluation methodology,
advancing the optimization of medical natural language processing (NLP) models.
Experimental results show EMRModel achieves an F1 score of 88.1%, improving
by49.5% over standard pre-trained models. Compared to traditional LoRA
fine-tuning methods, our model shows superior performance, highlighting its
effectiveness in structured medical record extraction tasks.

</details>


### [14] [T-VEC: A Telecom-Specific Vectorization Model with Enhanced Semantic Understanding via Deep Triplet Loss Fine-Tuning](https://arxiv.org/abs/2504.16460)
*Vignesh Ethiraj,Sidhanth Menon,Divya Vijay*

Main category: cs.CL

TL;DR: T-VEC是一种专为电信行业设计的嵌入模型，通过深度微调优化了电信领域语义的捕捉，显著提升了任务性能。


<details>
  <summary>Details</summary>
Motivation: 电信行业的专业词汇和复杂概念对通用NLP模型构成挑战，需要领域特定的嵌入模型。

Method: 基于gte-Qwen2-1.5B-instruct模型，采用三元组损失目标在电信数据集上进行深度微调，并开发了专用分词器。

Result: T-VEC在MTEB评分（0.825）和内部电信评估基准（0.9380）上表现优异。

Conclusion: T-VEC为电信AI提供了强大的开源工具，推动了领域创新。

Abstract: The specialized vocabulary and complex concepts of the telecommunications
industry present significant challenges for standard Natural Language
Processing models. Generic text embeddings often fail to capture
telecom-specific semantics, hindering downstream task performance. We introduce
T-VEC (Telecom Vectorization Model), a novel embedding model tailored for the
telecom domain through deep fine-tuning. Developed by NetoAI, T-VEC is created
by adapting the state-of-the-art gte-Qwen2-1.5B-instruct model using a triplet
loss objective on a meticulously curated, large-scale dataset of
telecom-specific data. Crucially, this process involved substantial
modification of weights across 338 layers of the base model, ensuring deep
integration of domain knowledge, far exceeding superficial adaptation
techniques. We quantify this deep change via weight difference analysis. A key
contribution is the development and open-sourcing (MIT License) of the first
dedicated telecom-specific tokenizer, enhancing the handling of industry
jargon. T-VEC achieves a leading average MTEB score (0.825) compared to
established models and demonstrates vastly superior performance (0.9380 vs.
less than 0.07) on our internal telecom-specific triplet evaluation benchmark,
indicating an exceptional grasp of domain-specific nuances, visually confirmed
by improved embedding separation. This work positions NetoAI at the forefront
of telecom AI innovation, providing the community with a powerful, deeply
adapted, open-source tool.

</details>


### [15] [QuaDMix: Quality-Diversity Balanced Data Selection for Efficient LLM Pretraining](https://arxiv.org/abs/2504.16511)
*Fengze Liu,Weidong Zhou,Binbin Liu,Zhimiao Yu,Yifan Zhang,Haobin Lin,Yifeng Yu,Xiaohuan Zhou,Taifeng Wang,Yong Cao*

Main category: cs.CL

TL;DR: QuaDMix是一个统一的数据选择框架，用于优化LLM预训练数据的质量和多样性平衡，通过参数化采样函数和模拟实验实现性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有研究通常单独优化数据质量和多样性，忽略了二者之间的权衡，需要联合考虑以提升LLM性能。

Method: 提出多标准衡量数据质量，通过域分类评估多样性；设计参数化采样函数，结合LightGBM优化参数。

Result: 实验表明，QuaDMix在多个基准测试中平均性能提升7.2%，优于独立优化策略。

Conclusion: QuaDMix证明了联合优化数据质量和多样性的必要性和有效性。

Abstract: Quality and diversity are two critical metrics for the training data of large
language models (LLMs), positively impacting performance. Existing studies
often optimize these metrics separately, typically by first applying quality
filtering and then adjusting data proportions. However, these approaches
overlook the inherent trade-off between quality and diversity, necessitating
their joint consideration. Given a fixed training quota, it is essential to
evaluate both the quality of each data point and its complementary effect on
the overall dataset. In this paper, we introduce a unified data selection
framework called QuaDMix, which automatically optimizes the data distribution
for LLM pretraining while balancing both quality and diversity. Specifically,
we first propose multiple criteria to measure data quality and employ domain
classification to distinguish data points, thereby measuring overall diversity.
QuaDMix then employs a unified parameterized data sampling function that
determines the sampling probability of each data point based on these quality
and diversity related labels. To accelerate the search for the optimal
parameters involved in the QuaDMix framework, we conduct simulated experiments
on smaller models and use LightGBM for parameters searching, inspired by the
RegMix method. Our experiments across diverse models and datasets demonstrate
that QuaDMix achieves an average performance improvement of 7.2% across
multiple benchmarks. These results outperform the independent strategies for
quality and diversity, highlighting the necessity and ability to balance data
quality and diversity.

</details>


### [16] [Transformers for Complex Query Answering over Knowledge Hypergraphs](https://arxiv.org/abs/2504.16537)
*Hong Ting Tsang,Zihao Wang,Yangqiu Song*

Main category: cs.CL

TL;DR: 论文提出了LKHGT模型，用于解决知识超图上的复杂查询问题，并在新数据集上验证了其性能。


<details>
  <summary>Details</summary>
Motivation: 现实世界数据复杂，传统知识图谱（三元组）表达能力有限，需扩展为超关系图以支持更复杂的查询。

Method: 提出两阶段Transformer模型LKHGT，包含投影编码器和逻辑编码器，结合类型感知偏置（TAB）捕捉交互。

Result: 实验表明LKHGT在知识超图上的复杂查询任务中表现最优，并能泛化到分布外查询类型。

Conclusion: LKHGT为知识超图的复杂查询提供了一种高效解决方案，具有泛化能力。

Abstract: Complex Query Answering (CQA) has been extensively studied in recent years.
In order to model data that is closer to real-world distribution, knowledge
graphs with different modalities have been introduced. Triple KGs, as the
classic KGs composed of entities and relations of arity 2, have limited
representation of real-world facts. Real-world data is more sophisticated.
While hyper-relational graphs have been introduced, there are limitations in
representing relationships of varying arity that contain entities with equal
contributions. To address this gap, we sampled new CQA datasets: JF17k-HCQA and
M-FB15k-HCQA. Each dataset contains various query types that include logical
operations such as projection, negation, conjunction, and disjunction. In order
to answer knowledge hypergraph (KHG) existential first-order queries, we
propose a two-stage transformer model, the Logical Knowledge Hypergraph
Transformer (LKHGT), which consists of a Projection Encoder for atomic
projection and a Logical Encoder for complex logical operations. Both encoders
are equipped with Type Aware Bias (TAB) for capturing token interactions.
Experimental results on CQA datasets show that LKHGT is a state-of-the-art CQA
method over KHG and is able to generalize to out-of-distribution query types.

</details>


### [17] [PIS: Linking Importance Sampling and Attention Mechanisms for Efficient Prompt Compression](https://arxiv.org/abs/2504.16574)
*Lizhe Chen,Binjia Zhou,Yuyao Ge,Jiayi Chen,Shiguang NI*

Main category: cs.CL

TL;DR: 论文提出了一种名为Prompt Importance Sampling (PIS)的新框架，通过动态采样重要标记来压缩提示，结合了标记级和语义级的压缩机制，显著提升了压缩性能。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型（LLMs）表现出色，但其高成本限制了广泛应用，现有提示压缩方法忽视了LLMs的内在机制，缺乏对标记重要性的系统评估。

Method: PIS框架通过分析隐藏状态的注意力分数动态压缩提示，包括标记级的自适应压缩（使用轻量级RL网络）和语义级的俄罗斯轮盘采样策略。

Result: 在多个领域基准测试中，PIS实现了最先进的压缩性能，并意外地通过优化上下文结构提升了推理效率。

Conclusion: PIS为LLMs的提示工程提供了理论基础和实践效率，推动了上下文管理的进步。

Abstract: Large language models (LLMs) have achieved remarkable progress, demonstrating
unprecedented capabilities across various natural language processing tasks.
However, the high costs associated with such exceptional performance limit the
widespread adoption of LLMs, highlighting the need for prompt compression.
Existing prompt compression methods primarily rely on heuristic truncation or
abstractive summarization techniques, which fundamentally overlook the
intrinsic mechanisms of LLMs and lack a systematic evaluation of token
importance for generation. In this work, we introduce Prompt Importance
Sampling (PIS), a novel compression framework that dynamically compresses
prompts by sampling important tokens based on the analysis of attention scores
of hidden states. PIS employs a dual-level compression mechanism: 1) at the
token level, we quantify saliency using LLM-native attention scores and
implement adaptive compression through a lightweight 9-layer reinforcement
learning (RL) network; 2) at the semantic level, we propose a Russian roulette
sampling strategy for sentence-level importance sampling. Comprehensive
evaluations across multiple domain benchmarks demonstrate that our method
achieves state-of-the-art compression performance. Notably, our framework
serendipitously enhances reasoning efficiency through optimized context
structuring. This work advances prompt engineering by offering both theoretical
grounding and practical efficiency in context management for LLMs.

</details>


### [18] [Comparing Large Language Models and Traditional Machine Translation Tools for Translating Medical Consultation Summaries: A Pilot Study](https://arxiv.org/abs/2504.16601)
*Andy Li,Wei Zhou,Rashina Hoda,Chris Bain,Peter Poon*

Main category: cs.CL

TL;DR: 比较大型语言模型（LLM）与传统机器翻译（MT）工具在医学咨询摘要翻译中的表现，发现传统MT工具表现更优，但LLM在简单文本翻译中显示出潜力。


<details>
  <summary>Details</summary>
Motivation: 评估LLM和传统MT工具在医学领域翻译的适用性，特别是在多语言环境下的表现。

Method: 使用标准自动化指标评估LLM和传统MT工具对英文医学摘要翻译为阿拉伯语、中文和越南语的效果。

Result: 传统MT工具整体表现更好，尤其是复杂文本；LLM在简单文本翻译中表现较好，阿拉伯语翻译随复杂度提升而改善。

Conclusion: LLM具有上下文灵活性但表现不稳定，需领域特定训练、改进评估方法和人工监督。

Abstract: This study evaluates how well large language models (LLMs) and traditional
machine translation (MT) tools translate medical consultation summaries from
English into Arabic, Chinese, and Vietnamese. It assesses both patient,
friendly and clinician, focused texts using standard automated metrics. Results
showed that traditional MT tools generally performed better, especially for
complex texts, while LLMs showed promise, particularly in Vietnamese and
Chinese, when translating simpler summaries. Arabic translations improved with
complexity due to the language's morphology. Overall, while LLMs offer
contextual flexibility, they remain inconsistent, and current evaluation
metrics fail to capture clinical relevance. The study highlights the need for
domain-specific training, improved evaluation methods, and human oversight in
medical translation.

</details>


### [19] [Debunking with Dialogue? Exploring AI-Generated Counterspeech to Challenge Conspiracy Theories](https://arxiv.org/abs/2504.16604)
*Mareike Lisker,Christina Gottschalk,Helena Mihaljević*

Main category: cs.CL

TL;DR: 论文探讨了使用LLMs（如GPT-4o、Llama 3和Mistral）生成针对阴谋论的反驳言论的可行性，发现模型表现不佳，常产生重复、肤浅或虚构的内容。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏针对阴谋论的反驳言论数据集，且专家驱动的反驳难以规模化，研究探索了LLMs在此领域的潜力。

Method: 通过结构化提示，评估GPT-4o、Llama 3和Mistral基于心理学研究的反驳策略应用能力。

Result: 模型常生成重复、肤浅或虚构的内容，过度承认恐惧，且在实际应用中存在问题。

Conclusion: 当前LLMs在生成针对阴谋论的反驳言论时效果有限，需进一步改进。

Abstract: Counterspeech is a key strategy against harmful online content, but scaling
expert-driven efforts is challenging. Large Language Models (LLMs) present a
potential solution, though their use in countering conspiracy theories is
under-researched. Unlike for hate speech, no datasets exist that pair
conspiracy theory comments with expert-crafted counterspeech. We address this
gap by evaluating the ability of GPT-4o, Llama 3, and Mistral to effectively
apply counterspeech strategies derived from psychological research provided
through structured prompts. Our results show that the models often generate
generic, repetitive, or superficial results. Additionally, they
over-acknowledge fear and frequently hallucinate facts, sources, or figures,
making their prompt-based use in practical applications problematic.

</details>


### [20] [TIFIN India at SemEval-2025: Harnessing Translation to Overcome Multilingual IR Challenges in Fact-Checked Claim Retrieval](https://arxiv.org/abs/2504.16627)
*Prasanna Devadiga,Arya Suneesh,Pawan Kumar Rajpoot,Bharatdeep Hazarika,Aditya U Baliga*

Main category: cs.CL

TL;DR: 论文提出了一种两阶段策略（基于嵌入模型的检索和LLM重排），用于单语和跨语言事实核查任务，并展示了LLM翻译在跨语言检索中的优势。


<details>
  <summary>Details</summary>
Motivation: 解决全球范围内虚假信息传播的问题，特别是在单语和跨语言环境下检索已核查事实的挑战。

Method: 采用两阶段策略：1）基于微调嵌入模型的基线检索系统；2）基于LLM的重排器。同时利用LLM翻译解决跨语言检索问题。

Result: 在单语和跨语言测试集上分别取得了0.938和0.81025的success@10分数。

Conclusion: LLM翻译能有效克服跨语言检索的障碍，且整个系统可在消费级GPU上复现。

Abstract: We address the challenge of retrieving previously fact-checked claims in
monolingual and crosslingual settings - a critical task given the global
prevalence of disinformation. Our approach follows a two-stage strategy: a
reliable baseline retrieval system using a fine-tuned embedding model and an
LLM-based reranker. Our key contribution is demonstrating how LLM-based
translation can overcome the hurdles of multilingual information retrieval.
Additionally, we focus on ensuring that the bulk of the pipeline can be
replicated on a consumer GPU. Our final integrated system achieved a success@10
score of 0.938 and 0.81025 on the monolingual and crosslingual test sets,
respectively.

</details>


### [21] [A Post-trainer's Guide to Multilingual Training Data: Uncovering Cross-lingual Transfer Dynamics](https://arxiv.org/abs/2504.16677)
*Luisa Shimabucoro,Ahmet Ustun,Marzieh Fadaee,Sebastian Ruder*

Main category: cs.CL

TL;DR: 研究探讨了多语言数据微调后大语言模型的跨语言迁移动态，发现其效果受多种因素综合影响，并提出了实践中有效的迁移条件。


<details>
  <summary>Details</summary>
Motivation: 理解跨语言迁移的动态机制，以优化多语言大语言模型的微调效果。

Method: 研究了两个模型家族（最大35B参数）在多语言数据混合下的跨语言迁移，涵盖三种生成任务（摘要、指令跟随、数学推理）的单任务和多任务微调。

Result: 跨语言迁移和多语言性能无法由单一变量解释，效果取决于微调设置的组合。

Conclusion: 确定了实践中实现有效跨语言迁移的条件。

Abstract: In order for large language models to be useful across the globe, they are
fine-tuned to follow instructions on multilingual data. Despite the ubiquity of
such post-training, a clear understanding of the dynamics that enable
cross-lingual transfer remains elusive. This study examines cross-lingual
transfer (CLT) dynamics in realistic post-training settings. We study two model
families of up to 35B parameters in size trained on carefully controlled
mixtures of multilingual data on three generative tasks with varying levels of
complexity (summarization, instruction following, and mathematical reasoning)
in both single-task and multi-task instruction tuning settings. Overall, we
find that the dynamics of cross-lingual transfer and multilingual performance
cannot be explained by isolated variables, varying depending on the combination
of post-training settings. Finally, we identify the conditions that lead to
effective cross-lingual transfer in practice.

</details>


### [22] [HEMA : A Hippocampus-Inspired Extended Memory Architecture for Long-Context AI Conversations](https://arxiv.org/abs/2504.16754)
*Kwangseob Ahn*

Main category: cs.CL

TL;DR: HEMA是一种受人类认知启发的双记忆系统，显著提升大型语言模型在长对话中的连贯性和事实回忆能力。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在长对话中难以保持连贯性的问题。

Method: HEMA结合了紧凑记忆（持续更新的单句摘要）和向量记忆（基于余弦相似度查询的片段存储）。

Result: 实验显示，HEMA将事实回忆准确率从41%提升至87%，人类评分连贯性从2.7提升至4.3（5分制）。

Conclusion: HEMA通过结合逐字回忆和语义连续性，为无需重新训练的隐私友好型对话AI提供了实用解决方案。

Abstract: Large language models (LLMs) struggle with maintaining coherence in extended
conversations spanning hundreds of turns, despite performing well within their
context windows. This paper introduces HEMA (Hippocampus-Inspired Extended
Memory Architecture), a dual-memory system inspired by human cognitive
processes. HEMA combines Compact Memory - a continuously updated one-sentence
summary preserving global narrative coherence, and Vector Memory - an episodic
store of chunk embeddings queried via cosine similarity. When integrated with a
6B-parameter transformer, HEMA maintains coherent dialogues beyond 300 turns
while keeping prompt length under 3,500 tokens. Experimental results show
substantial improvements: factual recall accuracy increases from 41% to 87%,
and human-rated coherence improves from 2.7 to 4.3 on a 5-point scale. With 10K
indexed chunks, Vector Memory achieves P@5 >= 0.80 and R@50 >= 0.74, doubling
the area under the precision-recall curve compared to summarization-only
approaches. Ablation studies reveal two key insights: semantic forgetting
through age-weighted pruning reduces retrieval latency by 34% with minimal
recall loss, and a two-level summary hierarchy prevents cascade errors in
ultra-long conversations exceeding 1,000 turns. HEMA demonstrates that
combining verbatim recall with semantic continuity provides a practical
solution for privacy-aware conversational AI capable of month-long dialogues
without model retraining.

</details>


### [23] [How Effective are Generative Large Language Models in Performing Requirements Classification?](https://arxiv.org/abs/2504.16768)
*Waad Alhoshan,Alessio Ferrari,Liping Zhao*

Main category: cs.CL

TL;DR: 论文探讨生成式大型语言模型（如Bloom、Gemma、Llama）在需求分类任务中的表现，发现提示设计和模型架构是关键因素，而数据集影响因任务复杂度而异。


<details>
  <summary>Details</summary>
Motivation: 研究生成式LLMs在需求分类任务中的表现，填补现有研究中对此类模型探索的空白。

Method: 设计超过400项实验，使用三个数据集（PROMISE NFR、Functional-Quality、SecReq）评估三种生成式LLMs的二元和多类分类能力。

Result: 提示设计和模型架构对性能至关重要，数据集的影响则因任务复杂度而异。

Conclusion: 未来模型开发应优化提示结构，并根据任务需求调整模型架构以提升性能。

Abstract: In recent years, transformer-based large language models (LLMs) have
revolutionised natural language processing (NLP), with generative models
opening new possibilities for tasks that require context-aware text generation.
Requirements engineering (RE) has also seen a surge in the experimentation of
LLMs for different tasks, including trace-link detection, regulatory
compliance, and others. Requirements classification is a common task in RE.
While non-generative LLMs like BERT have been successfully applied to this
task, there has been limited exploration of generative LLMs. This gap raises an
important question: how well can generative LLMs, which produce context-aware
outputs, perform in requirements classification? In this study, we explore the
effectiveness of three generative LLMs-Bloom, Gemma, and Llama-in performing
both binary and multi-class requirements classification. We design an extensive
experimental study involving over 400 experiments across three widely used
datasets (PROMISE NFR, Functional-Quality, and SecReq). Our study concludes
that while factors like prompt design and LLM architecture are universally
important, others-such as dataset variations-have a more situational impact,
depending on the complexity of the classification task. This insight can guide
future model development and deployment strategies, focusing on optimising
prompt structures and aligning model architectures with task-specific needs for
improved performance.

</details>


### [24] [Evaluation Framework for AI Systems in "the Wild"](https://arxiv.org/abs/2504.16778)
*Sarah Jabbour,Trenton Chang,Anindya Das Antar,Joseph Peper,Insu Jang,Jiachen Liu,Jae-Won Chung,Shiqi He,Michael Wellman,Bryan Goodman,Elizabeth Bondi-Kelly,Kevin Samy,Rada Mihalcea,Mosharaf Chowhury,David Jurgens,Lu Wang*

Main category: cs.CL

TL;DR: 提出一个评估生成式AI系统的综合框架，强调动态、多样化的输入和持续评估方法，以确保其在实际应用中的表现和伦理责任。


<details>
  <summary>Details</summary>
Motivation: 当前评估方法未能适应生成式AI的广泛应用，传统方法无法反映真实世界性能，导致实验室结果与实际应用之间的差距。

Method: 提出一个综合框架，结合多样化输入、动态评估和持续监测，整合性能、公平性和伦理考量，并采用人机结合的透明评估方法。

Result: 框架为实践者提供了设计评估方法的指导，并为政策制定者提供了关注社会影响的建议。

Conclusion: 实施该框架可确保生成式AI系统在技术和伦理上的双重成功，促进其实际应用中的信任和影响力。

Abstract: Generative AI (GenAI) models have become vital across industries, yet current
evaluation methods have not adapted to their widespread use. Traditional
evaluations often rely on benchmarks and fixed datasets, frequently failing to
reflect real-world performance, which creates a gap between lab-tested outcomes
and practical applications. This white paper proposes a comprehensive framework
for how we should evaluate real-world GenAI systems, emphasizing diverse,
evolving inputs and holistic, dynamic, and ongoing assessment approaches. The
paper offers guidance for practitioners on how to design evaluation methods
that accurately reflect real-time capabilities, and provides policymakers with
recommendations for crafting GenAI policies focused on societal impacts, rather
than fixed performance numbers or parameter sizes. We advocate for holistic
frameworks that integrate performance, fairness, and ethics and the use of
continuous, outcome-oriented methods that combine human and automated
assessments while also being transparent to foster trust among stakeholders.
Implementing these strategies ensures GenAI models are not only technically
proficient but also ethically responsible and impactful.

</details>


### [25] [MOOSComp: Improving Lightweight Long-Context Compressor via Mitigating Over-Smoothing and Incorporating Outlier Scores](https://arxiv.org/abs/2504.16786)
*Fengwei Zhou,Jiafei Song,Wenjin Jason Li,Gengjian Xue,Zhikang Zhao,Yichao Lu,Bailin Na*

Main category: cs.CL

TL;DR: MOOSComp是一种基于令牌分类的长上下文压缩方法，通过解决过平滑问题和引入异常值分数，提升了BERT压缩器的性能，并在资源受限环境中实现了高效推理。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在处理长上下文输入时面临推理时间和资源消耗增加的挑战，尤其是在资源受限环境中。

Method: 提出MOOSComp方法，通过添加类间余弦相似度损失项和引入异常值分数，优化令牌分类和压缩过程。

Result: 在多种压缩比下，MOOSComp在长上下文理解和推理任务中表现优异，并在资源受限设备上实现了3.3倍的加速。

Conclusion: MOOSComp通过改进压缩方法，显著提升了长上下文处理的效率和性能。

Abstract: Recent advances in large language models have significantly improved their
ability to process long-context input, but practical applications are
challenged by increased inference time and resource consumption, particularly
in resource-constrained environments. To address these challenges, we propose
MOOSComp, a token-classification-based long-context compression method that
enhances the performance of a BERT-based compressor by mitigating the
over-smoothing problem and incorporating outlier scores. In the training phase,
we add an inter-class cosine similarity loss term to penalize excessively
similar token representations, thereby improving the token classification
accuracy. During the compression phase, we introduce outlier scores to preserve
rare but critical tokens that are prone to be discarded in task-agnostic
compression. These scores are integrated with the classifier's output, making
the compressor more generalizable to various tasks. Superior performance is
achieved at various compression ratios on long-context understanding and
reasoning benchmarks. Moreover, our method obtains a speedup of 3.3x at a 4x
compression ratio on a resource-constrained mobile device.

</details>


### [26] [Credible plan-driven RAG method for Multi-hop Question Answering](https://arxiv.org/abs/2504.16787)
*Ningning Zhang,Chi Zhang,Zhizhong Tan,Xingxing Yang,Weiping Deng,Wenyong Wang*

Main category: cs.CL

TL;DR: PAR RAG框架通过规划、执行和审查三阶段，减少多跳问答中的错误传播，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前RAG方法在多跳问答中易因推理路径偏差或中间结果错误导致准确性下降。

Method: 提出PAR RAG框架，分三阶段：规划（全局分解问题）、执行（多粒度验证）、审查（调整中间结果）。

Result: 在多跳问答数据集上，PAR RAG在EM和F1分数上显著优于现有方法。

Conclusion: PAR RAG通过结构化推理和错误控制，实现了更准确可靠的多跳问答。

Abstract: Multi-hop question answering (QA) presents a considerable challenge for
Retrieval-Augmented Generation (RAG), requiring the structured decomposition of
complex queries into logical reasoning paths and the generation of dependable
intermediate results. However, deviations in reasoning paths or errors in
intermediate results, which are common in current RAG methods, may propagate
and accumulate throughout the reasoning process, diminishing the accuracy of
the answer to complex queries. To address this challenge, we propose the
Plan-then-Act-and-Review (PAR RAG) framework, which is organized into three key
stages: planning, act, and review, and aims to offer an interpretable and
incremental reasoning paradigm for accurate and reliable multi-hop question
answering by mitigating error propagation.PAR RAG initially applies a top-down
problem decomposition strategy, formulating a comprehensive plan that
integrates multiple executable steps from a holistic viewpoint. This approach
avoids the pitfalls of local optima common in traditional RAG methods, ensuring
the accuracy of the entire reasoning path. Subsequently, PAR RAG incorporates a
plan execution mechanism based on multi-granularity verification. By utilizing
both coarse-grained similarity information and fine-grained relevant data, the
framework thoroughly checks and adjusts intermediate results, ensuring process
accuracy while effectively managing error propagation and amplification.
Experimental results on multi-hop QA datasets demonstrate that the PAR RAG
framework substantially outperforms existing state-of-the-art methods in key
metrics, including EM and F1 scores.

</details>


### [27] [Random Long-Context Access for Mamba via Hardware-aligned Hierarchical Sparse Attention](https://arxiv.org/abs/2504.16795)
*Xiang Hu,Jiaqi Leng,Jun Zhao,Kewei Tu,Wei Wu*

Main category: cs.CL

TL;DR: 提出了一种名为HSA的分层稀疏注意力机制，结合RNN的高效性和注意力机制的灵活性，显著提升了长序列建模能力。


<details>
  <summary>Details</summary>
Motivation: 解决RNN无法随机访问历史上下文的问题，同时保留其高效性。

Method: HSA将输入分块，选择top-k块并分层聚合信息，学习块内细粒度token相关性。

Result: RAMba（结合HSA和Mamba）在64M上下文上实现完美检索，并在下游任务中表现优异，内存占用几乎恒定。

Conclusion: HSA和RAMba在长上下文建模中展现出巨大潜力。

Abstract: A key advantage of Recurrent Neural Networks (RNNs) over Transformers is
their linear computational and space complexity enables faster training and
inference for long sequences. However, RNNs are fundamentally unable to
randomly access historical context, and simply integrating attention mechanisms
may undermine their efficiency advantages. To overcome this limitation, we
propose \textbf{H}ierarchical \textbf{S}parse \textbf{A}ttention (HSA), a novel
attention mechanism that enhances RNNs with long-range random access
flexibility while preserving their merits in efficiency and length
generalization. HSA divides inputs into chunks, selecting the top-$k$ chunks
and hierarchically aggregates information. The core innovation lies in learning
token-to-chunk relevance based on fine-grained token-level information inside
each chunk. This approach enhances the precision of chunk selection across both
in-domain and out-of-domain context lengths. To make HSA efficient, we further
introduce a hardware-aligned kernel design. By combining HSA with Mamba, we
introduce RAMba, which achieves perfect accuracy in passkey retrieval across 64
million contexts despite pre-training on only 4K-length contexts, and
significant improvements on various downstream tasks, with nearly constant
memory footprint. These results show RAMba's huge potential in long-context
modeling.

</details>


### [28] [LLM-assisted Graph-RAG Information Extraction from IFC Data](https://arxiv.org/abs/2504.16813)
*Sima Iranmanesh,Hadeel Saadany,Edlira Vakaj*

Main category: cs.CL

TL;DR: 利用Graph-RAG技术结合LLMs解析复杂的IFC数据，提升自然语言查询能力。


<details>
  <summary>Details</summary>
Motivation: IFC数据在建筑行业中应用广泛，但其复杂性导致信息解析困难。

Method: 采用Graph-RAG技术结合LLMs（如GPT-4o）解析IFC数据，提取对象属性及关系。

Result: Graph-RAG增强了LLMs的图知识能力，支持自然语言查询，无需复杂流程。

Conclusion: Graph-RAG技术有效简化了IFC数据的解析与查询，提升了LLMs的应用潜力。

Abstract: IFC data has become the general building information standard for
collaborative work in the construction industry. However, IFC data can be very
complicated because it allows for multiple ways to represent the same product
information. In this research, we utilise the capabilities of LLMs to parse the
IFC data with Graph Retrieval-Augmented Generation (Graph-RAG) technique to
retrieve building object properties and their relations. We will show that,
despite limitations due to the complex hierarchy of the IFC data, the Graph-RAG
parsing enhances generative LLMs like GPT-4o with graph-based knowledge,
enabling natural language query-response retrieval without the need for a
complex pipeline.

</details>


### [29] [GreenMind: A Next-Generation Vietnamese Large Language Model for Structured and Logical Reasoning](https://arxiv.org/abs/2504.16832)
*Luu Quy Tung,Hoang Quoc Viet,Vo Trong Thu*

Main category: cs.CL

TL;DR: 本文提出GreenMind-Medium-14B-R1，一种基于Group Relative Policy Optimization的越南语推理模型，通过高质量数据集和奖励函数解决语言混合和事实正确性问题，并在VLSP 2023和SeaExam数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决Chain-of-Thought方法在越南语任务中的语言混合和事实正确性问题，提升推理模型的性能。

Method: 使用Group Relative Policy Optimization微调策略，设计两种奖励函数：检测语言混合和确保事实正确性。

Result: 在VLSP 2023越南语数据集和SeaExam多语言数据集上表现优于现有方法。

Conclusion: GreenMind-Medium-14B-R1有效解决了语言混合和事实正确性问题，提升了推理模型的性能。

Abstract: Chain-of-Thought (CoT) is a robust approach for tackling LLM tasks that
require intermediate reasoning steps prior to generating a final answer. In
this paper, we present GreenMind-Medium-14B-R1, the Vietnamese reasoning model
inspired by the finetuning strategy based on Group Relative Policy
Optimization. We also leverage a high-quality Vietnamese synthesized reasoning
dataset and design two reward functions to tackle the main limitations of this
technique: (i) language mixing, where we explicitly detect the presence of
biased language characters during the process of sampling tokens, and (ii) we
leverage Sentence Transformer-based models to ensure that the generated
reasoning content maintains factual correctness and does not distort the final
output. Experimental results on the Vietnamese dataset from the VLSP 2023
Challenge demonstrate that our model outperforms prior works and enhances
linguistic consistency in its responses. Furthermore, we extend our evaluation
to SeaExam-a multilingual multiple-choice dataset, showing the effectiveness of
our reasoning method compared to few-shot prompting techniques.

</details>


### [30] [Monte Carlo Planning with Large Language Model for Text-Based Game Agents](https://arxiv.org/abs/2504.16855)
*Zijing Shi,Meng Fang,Ling Chen*

Main category: cs.CL

TL;DR: MC-DML算法结合大型语言模型（LLM）和树搜索算法，通过动态记忆机制提升语言理解和推理能力，显著提高文本游戏中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于规划和学习的算法（如MCTS与RL结合）耗时且缺乏语言理解能力，需要改进。

Method: 提出MC-DML算法，利用LLM的语言能力与树搜索的探索优势，通过动态记忆机制学习经验并调整动作评估。

Result: 在Jericho基准测试中，MC-DML在初始规划阶段显著优于现有方法，无需多次迭代。

Conclusion: MC-DML为复杂环境中的语言规划提供了高效解决方案。

Abstract: Text-based games provide valuable environments for language-based autonomous
agents. However, planning-then-learning paradigms, such as those combining
Monte Carlo Tree Search (MCTS) and reinforcement learning (RL), are notably
time-consuming due to extensive iterations. Additionally, these algorithms
perform uncertainty-driven exploration but lack language understanding and
reasoning abilities. In this paper, we introduce the Monte Carlo planning with
Dynamic Memory-guided Large language model (MC-DML) algorithm. MC-DML leverages
the language understanding and reasoning capabilities of Large Language Models
(LLMs) alongside the exploratory advantages of tree search algorithms.
Specifically, we enhance LLMs with in-trial and cross-trial memory mechanisms,
enabling them to learn from past experiences and dynamically adjust action
evaluations during planning. We conduct experiments on a series of text-based
games from the Jericho benchmark. Our results demonstrate that the MC-DML
algorithm significantly enhances performance across various games at the
initial planning phase, outperforming strong contemporary methods that require
multiple iterations. This demonstrates the effectiveness of our algorithm,
paving the way for more efficient language-grounded planning in complex
environments.

</details>


### [31] [Emo Pillars: Knowledge Distillation to Support Fine-Grained Context-Aware and Context-Less Emotion Classification](https://arxiv.org/abs/2504.16856)
*Alexander Shvets*

Main category: cs.CL

TL;DR: 论文提出了一种基于LLM的数据合成方法，利用Mistral-7b生成多样化的情感分析训练数据，并通过微调BERT类模型达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有情感分析数据集缺乏上下文且情感类别有限，大型语言模型（如GPT-4）存在过度预测和资源消耗大的问题。

Method: 设计LLM数据合成流程，生成多样化的情感数据，并微调BERT类模型（Emo Pillars）。

Result: 生成100K带上下文和300K无上下文的数据集，Emo Pillars模型在多个任务上达到SOTA性能。

Conclusion: 方法成功提升了数据多样性和上下文个性化，但需改进对非分类标签的处理。

Abstract: Most datasets for sentiment analysis lack context in which an opinion was
expressed, often crucial for emotion understanding, and are mainly limited by a
few emotion categories. Foundation large language models (LLMs) like GPT-4
suffer from over-predicting emotions and are too resource-intensive. We design
an LLM-based data synthesis pipeline and leverage a large model, Mistral-7b,
for the generation of training examples for more accessible, lightweight
BERT-type encoder models. We focus on enlarging the semantic diversity of
examples and propose grounding the generation into a corpus of narratives to
produce non-repetitive story-character-centered utterances with unique contexts
over 28 emotion classes. By running 700K inferences in 450 GPU hours, we
contribute with the dataset of 100K contextual and also 300K context-less
examples to cover both scenarios. We use it for fine-tuning pre-trained
encoders, which results in several Emo Pillars models. We show that Emo Pillars
models are highly adaptive to new domains when tuned to specific tasks such as
GoEmotions, ISEAR, IEMOCAP, and EmoContext, reaching the SOTA performance on
the first three. We also validate our dataset, conducting statistical analysis
and human evaluation, and confirm the success of our measures in utterance
diversification (although less for the neutral class) and context
personalization, while pointing out the need for improved handling of
out-of-taxonomy labels within the pipeline.

</details>


### [32] [Planning with Diffusion Models for Target-Oriented Dialogue Systems](https://arxiv.org/abs/2504.16858)
*Hanwen Du,Bo Peng,Xia Ning*

Main category: cs.CL

TL;DR: DiffTOD提出了一种基于扩散模型的非顺序对话规划框架，解决了传统方法中的短视行为和错误累积问题。


<details>
  <summary>Details</summary>
Motivation: 现有对话规划方法存在顺序生成的局限性，容易导致错误累积和短视行为。

Method: DiffTOD将对话规划建模为条件引导的轨迹生成问题，利用扩散语言模型估计对话轨迹的似然，并针对不同目标类型设计了三种引导机制。

Result: 实验表明，DiffTOD能有效进行非短视的前瞻探索，并在复杂多样的对话场景中表现出强大的灵活性。

Conclusion: DiffTOD通过非顺序对话规划优化了长期动作策略，适用于多样化的目标导向对话任务。

Abstract: Target-Oriented Dialogue (TOD) remains a significant challenge in the LLM
era, where strategic dialogue planning is crucial for directing conversations
toward specific targets. However, existing dialogue planning methods generate
dialogue plans in a step-by-step sequential manner, and may suffer from
compounding errors and myopic actions. To address these limitations, we
introduce a novel dialogue planning framework, DiffTOD, which leverages
diffusion models to enable non-sequential dialogue planning. DiffTOD formulates
dialogue planning as a trajectory generation problem with conditional guidance,
and leverages a diffusion language model to estimate the likelihood of the
dialogue trajectory. To optimize the dialogue action strategies, DiffTOD
introduces three tailored guidance mechanisms for different target types,
offering flexible guidance towards diverse TOD targets at test time. Extensive
experiments across three diverse TOD settings show that DiffTOD can effectively
perform non-myopic lookahead exploration and optimize action strategies over a
long horizon through non-sequential dialogue planning, and demonstrates strong
flexibility across complex and diverse dialogue scenarios. Our code and data
are accessible through https://anonymous.4open.science/r/DiffTOD.

</details>


### [33] [Do Large Language Models know who did what to whom?](https://arxiv.org/abs/2504.16884)
*Joseph M. Denning,Xiaohan,Guo,Bryor Snefjella,Idan A. Blank*

Main category: cs.CL

TL;DR: LLMs的句子表征主要反映句法相似性而非主题角色信息，但部分注意力头能独立捕获主题角色。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs是否通过词预测目标学习到与语言紧密相关的主题角色理解能力。

Method: 通过两个实验分析四种LLMs的句子表征，比较其与人类相似性判断的差异。

Result: LLMs的句子表征更依赖句法而非主题角色，但部分注意力头能独立捕获主题角色信息。

Conclusion: LLMs能提取主题角色，但其表征中该信息的影响较弱，与人类不同。

Abstract: Large Language Models (LLMs) are commonly criticized for not understanding
language. However, many critiques focus on cognitive abilities that, in humans,
are distinct from language processing. Here, we instead study a kind of
understanding tightly linked to language: inferring who did what to whom
(thematic roles) in a sentence. Does the central training objective of
LLMs-word prediction-result in sentence representations that capture thematic
roles? In two experiments, we characterized sentence representations in four
LLMs. In contrast to human similarity judgments, in LLMs the overall
representational similarity of sentence pairs reflected syntactic similarity
but not whether their agent and patient assignments were identical vs.
reversed. Furthermore, we found little evidence that thematic role information
was available in any subset of hidden units. However, some attention heads
robustly captured thematic roles, independently of syntax. Therefore, LLMs can
extract thematic roles but, relative to humans, this information influences
their representations more weakly.

</details>


### [34] [Tracing Thought: Using Chain-of-Thought Reasoning to Identify the LLM Behind AI-Generated Text](https://arxiv.org/abs/2504.16913)
*Shifali Agrahari,Sanasam Ranbir Singh*

Main category: cs.CL

TL;DR: 本文提出了一种名为COT Fine-tuned的新框架，用于检测AI生成的文本并识别其来源语言模型。通过双任务设计和Chain-of-Thought推理，实现了高准确率和可解释性。


<details>
  <summary>Details</summary>
Motivation: 近年来，AI生成文本的检测成为重要研究方向，涉及学术诚信、错误信息和AI伦理问题。

Method: 采用双任务设计：任务A区分AI生成与人类撰写文本；任务B识别具体语言模型。关键创新是使用Chain-of-Thought推理，提升模型透明度和解释性。

Result: 实验表明，COT Fine-tuned在两项任务中均表现优异，尤其在语言模型识别和人类-AI分类方面。Chain-of-Thought推理显著提升了模型效果。

Conclusion: COT Fine-tuned框架在检测AI生成文本和模型识别方面高效且透明，为相关领域提供了实用工具。

Abstract: In recent years, the detection of AI-generated text has become a critical
area of research due to concerns about academic integrity, misinformation, and
ethical AI deployment. This paper presents COT Fine-tuned, a novel framework
for detecting AI-generated text and identifying the specific language model.
responsible for generating the text. We propose a dual-task approach, where
Task A involves classifying text as AI-generated or human-written, and Task B
identifies the specific LLM behind the text. The key innovation of our method
lies in the use of Chain-of-Thought reasoning, which enables the model to
generate explanations for its predictions, enhancing transparency and
interpretability. Our experiments demonstrate that COT Fine-tuned achieves high
accuracy in both tasks, with strong performance in LLM identification and
human-AI classification. We also show that the CoT reasoning process
contributes significantly to the models effectiveness and interpretability.

</details>


### [35] [OptimAI: Optimization from Natural Language Using LLM-Powered AI Agents](https://arxiv.org/abs/2504.16918)
*Raghav Thind,Youran Sun,Ling Liang,Haizhao Yang*

Main category: cs.CL

TL;DR: OptimAI是一个利用LLM驱动的AI代理解决自然语言描述的优化问题的框架，通过四个关键角色实现性能提升。


<details>
  <summary>Details</summary>
Motivation: 优化问题在科研和实际应用中至关重要，但将自然语言描述的优化问题转化为数学形式并选择合适的求解器需要大量领域知识。

Method: 框架包含四个角色：formulator（问题描述转数学公式）、planner（构建解决方案策略）、coder和code critic（与环境交互并优化行为）。采用UCB-based debug调度动态切换计划。

Result: 在NLP4LP数据集上达到88.1%准确率，Optibench子集上71.2%，错误率分别降低58%和50%。

Conclusion: OptimAI通过多代理协作显著提升了优化问题的解决效率，证明了框架设计的有效性。

Abstract: Optimization plays a vital role in scientific research and practical
applications, but formulating a concrete optimization problem described in
natural language into a mathematical form and selecting a suitable solver to
solve the problem requires substantial domain expertise. We introduce
\textbf{OptimAI}, a framework for solving \underline{Optim}ization problems
described in natural language by leveraging LLM-powered \underline{AI} agents,
achieving superior performance over current state-of-the-art methods. Our
framework is built upon four key roles: (1) a \emph{formulator} that translates
natural language problem descriptions into precise mathematical formulations;
(2) a \emph{planner} that constructs a high-level solution strategy prior to
execution; and (3) a \emph{coder} and a \emph{code critic} capable of
interacting with the environment and reflecting on outcomes to refine future
actions. Ablation studies confirm that all roles are essential; removing the
planner or code critic results in $5.8\times$ and $3.1\times$ drops in
productivity, respectively. Furthermore, we introduce UCB-based debug
scheduling to dynamically switch between alternative plans, yielding an
additional $3.3\times$ productivity gain. Our design emphasizes multi-agent
collaboration, allowing us to conveniently explore the synergistic effect of
combining diverse models within a unified system. Our approach attains 88.1\%
accuracy on the NLP4LP dataset and 71.2\% on the Optibench (non-linear w/o
table) subset, reducing error rates by 58\% and 50\% respectively over prior
best results.

</details>


### [36] [IberBench: LLM Evaluation on Iberian Languages](https://arxiv.org/abs/2504.16921)
*José Ángel González,Ian Borrego Obrador,Álvaro Romo Herrero,Areg Mikael Sarvazyan,Mara Chinea-Ríos,Angelo Basile,Marc Franco-Salvador*

Main category: cs.CL

TL;DR: IberBench是一个针对伊比利亚半岛和拉丁美洲语言的全面、可扩展的基准测试，旨在评估LLM在基础和工业相关NLP任务上的表现，解决了现有基准测试的不足。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试主要针对英语，缺乏对其他语言的多样性、工业相关任务的关注以及动态更新的支持。

Method: IberBench整合了101个数据集，覆盖22个任务类别，支持持续更新和社区驱动的提交，并由专家委员会审核。

Result: 评估了23个LLM，发现其在工业相关任务上表现较差，某些语言（如加利西亚语和巴斯克语）表现更低，部分任务结果接近随机。

Conclusion: IberBench提供了开源实现和公开排行榜，为LLM评估提供了更全面的工具。

Abstract: Large Language Models (LLMs) remain difficult to evaluate comprehensively,
particularly for languages other than English, where high-quality data is often
limited. Existing benchmarks and leaderboards are predominantly
English-centric, with only a few addressing other languages. These benchmarks
fall short in several key areas: they overlook the diversity of language
varieties, prioritize fundamental Natural Language Processing (NLP)
capabilities over tasks of industrial relevance, and are static. With these
aspects in mind, we present IberBench, a comprehensive and extensible benchmark
designed to assess LLM performance on both fundamental and industry-relevant
NLP tasks, in languages spoken across the Iberian Peninsula and Ibero-America.
IberBench integrates 101 datasets from evaluation campaigns and recent
benchmarks, covering 22 task categories such as sentiment and emotion analysis,
toxicity detection, and summarization. The benchmark addresses key limitations
in current evaluation practices, such as the lack of linguistic diversity and
static evaluation setups by enabling continual updates and community-driven
model and dataset submissions moderated by a committee of experts. We evaluate
23 LLMs ranging from 100 million to 14 billion parameters and provide empirical
insights into their strengths and limitations. Our findings indicate that (i)
LLMs perform worse on industry-relevant tasks than in fundamental ones, (ii)
performance is on average lower for Galician and Basque, (iii) some tasks show
results close to random, and (iv) in other tasks LLMs perform above random but
below shared task systems. IberBench offers open-source implementations for the
entire evaluation pipeline, including dataset normalization and hosting,
incremental evaluation of LLMs, and a publicly accessible leaderboard.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [37] [A Framework for Objective-Driven Dynamical Stochastic Fields](https://arxiv.org/abs/2504.16115)
*Yibo Jacky Zhang,Sanmi Koyejo*

Main category: cs.AI

TL;DR: 论文提出了三个基本原则（完全配置、局部性和目的性）来建立智能场的理论框架，并探讨了从人工智能应用角度设计此类场的方法。


<details>
  <summary>Details</summary>
Motivation: 由于动态和随机系统的复杂性，目前缺乏对智能场的正式理论描述及其实际应用的转化方法。

Method: 提出了三个基本原则：完全配置、局部性和目的性，并探索了从人工智能应用角度设计智能场的方法。

Result: 初步建立了智能场的理论框架，为未来理论和实践发展奠定了基础。

Conclusion: 该研究为理解和利用目标驱动的动态随机场的潜力提供了初步的理论和方法支持。

Abstract: Fields offer a versatile approach for describing complex systems composed of
interacting and dynamic components. In particular, some of these dynamical and
stochastic systems may exhibit goal-directed behaviors aimed at achieving
specific objectives, which we refer to as $\textit{intelligent fields}$.
However, due to their inherent complexity, it remains challenging to develop a
formal theoretical description of such systems and to effectively translate
these descriptions into practical applications. In this paper, we propose three
fundamental principles -- complete configuration, locality, and purposefulness
-- to establish a theoretical framework for understanding intelligent fields.
Moreover, we explore methodologies for designing such fields from the
perspective of artificial intelligence applications. This initial investigation
aims to lay the groundwork for future theoretical developments and practical
advances in understanding and harnessing the potential of such objective-driven
dynamical stochastic fields.

</details>


### [38] [HTN Plan Repair Algorithms Compared: Strengths and Weaknesses of Different Methods](https://arxiv.org/abs/2504.16209)
*Paul Zaidins,Robert P. Goldman,Ugur Kuter,Dana Nau,Mark Roberts*

Main category: cs.AI

TL;DR: 本文比较了三种分层计划修复算法的理论和实证表现，分析了它们的不同定义、搜索空间和修复能力，并提供了实验评估。


<details>
  <summary>Details</summary>
Motivation: 理解不同计划修复算法的定义和性能差异，以便为具体应用选择合适的修复方法。

Method: 通过理论分析和实证评估（基准规划问题）比较SHOPFixer、IPyHOPPER和Rewrite三种算法。

Result: 理论分析显示三种算法对应不同的修复问题定义，实证结果提供了运行时性能和修复覆盖率的详细见解。

Conclusion: 选择合适的修复算法需考虑其定义、搜索空间和性能特点，本文为决策提供了理论和实证依据。

Abstract: This paper provides theoretical and empirical comparisons of three recent
hierarchical plan repair algorithms: SHOPFixer, IPyHOPPER, and Rewrite. Our
theoretical results show that the three algorithms correspond to three
different definitions of the plan repair problem, leading to differences in the
algorithms' search spaces, the repair problems they can solve, and the kinds of
repairs they can make. Understanding these distinctions is important when
choosing a repair method for any given application.
  Building on the theoretical results, we evaluate the algorithms empirically
in a series of benchmark planning problems. Our empirical results provide more
detailed insight into the runtime repair performance of these systems and the
coverage of the repair problems solved, based on algorithmic properties such as
replanning, chronological backtracking, and backjumping over plan trees.

</details>


### [39] [Investigating LLMs in Clinical Triage: Promising Capabilities, Persistent Intersectional Biases](https://arxiv.org/abs/2504.16273)
*Joseph Lee,Tianqi Shang,Jae Young Baik,Duy Duong-Tran,Shu Yang,Lingyao Li,Li Shen*

Main category: cs.AI

TL;DR: LLMs在急诊分诊中表现出较强的鲁棒性，但在性别和种族的交叉分析中存在偏好差异。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在急诊分诊中的应用潜力，特别是其对数据分布变化和缺失数据的鲁棒性，以及性别与种族的交叉偏见。

Method: 通过持续预训练和上下文学习等多种LLM方法，结合机器学习方法，评估其在急诊分诊中的表现。

Result: LLMs在鲁棒性方面表现优异，但在性别和种族的特定组合中存在明显偏好差异。

Conclusion: LLMs在临床决策中具有潜力，但其对特定人口特征的偏好需进一步研究。

Abstract: Large Language Models (LLMs) have shown promise in clinical decision support,
yet their application to triage remains underexplored. We systematically
investigate the capabilities of LLMs in emergency department triage through two
key dimensions: (1) robustness to distribution shifts and missing data, and (2)
counterfactual analysis of intersectional biases across sex and race. We assess
multiple LLM-based approaches, ranging from continued pre-training to
in-context learning, as well as machine learning approaches. Our results
indicate that LLMs exhibit superior robustness, and we investigate the key
factors contributing to the promising LLM-based approaches. Furthermore, in
this setting, we identify gaps in LLM preferences that emerge in particular
intersections of sex and race. LLMs generally exhibit sex-based differences,
but they are most pronounced in certain racial groups. These findings suggest
that LLMs encode demographic preferences that may emerge in specific clinical
contexts or particular combinations of characteristics.

</details>


### [40] [Cognitive Silicon: An Architectural Blueprint for Post-Industrial Computing Systems](https://arxiv.org/abs/2504.16622)
*Christoforus Yoga Haryanto,Emily Lomempow*

Main category: cs.AI

TL;DR: 本文提出了一种名为“Cognitive Silicon”的假设性全栈架构框架，探索2035年认知计算系统设计的可能路径。该框架整合了符号化支撑、受控内存、运行时道德一致性等特性，旨在通过硬件约束和身份绑定机制实现道德可控的认知基础设施。


<details>
  <summary>Details</summary>
Motivation: 现有自主AI系统在确定性、人类编写的计算架构中存在基础性局限，需要探索新的认知计算设计路径。

Method: 通过符号化支撑、受控内存、运行时道德一致性等设计元素，结合硬件约束和身份绑定机制，构建一种新型认知计算架构。

Result: 该框架理论上与自由能原理一致，可能为认知系统如何通过预测误差最小化维持身份提供形式化解释。

Conclusion: Cognitive Silicon框架旨在提供一种道德可控的认知基础设施，通过不可逆的硬件约束和抗复制的身份绑定机制，保持与人类的一致性。

Abstract: Autonomous AI systems reveal foundational limitations in deterministic,
human-authored computing architectures. This paper presents Cognitive Silicon:
a hypothetical full-stack architectural framework projected toward 2035,
exploring a possible trajectory for cognitive computing system design. The
proposed architecture would integrate symbolic scaffolding, governed memory,
runtime moral coherence, and alignment-aware execution across
silicon-to-semantics layers. Our design grammar has emerged from dialectical
co-design with LLMs under asymmetric epistemic conditions--creating structured
friction to expose blind spots and trade-offs. The envisioned framework would
establish mortality as a natural consequence of physical constraints,
non-copyable tacit knowledge, and non-cloneable identity keys as
cognitive-embodiment primitives. Core tensions (trust/agency,
scaffolding/emergence, execution/governance) would function as central
architectural pressures rather than edge cases. The architecture theoretically
converges with the Free Energy Principle, potentially offering a formal account
of how cognitive systems could maintain identity through prediction error
minimization across physical and computational boundaries. The resulting
framework aims to deliver a morally tractable cognitive infrastructure that
could maintain human-alignment through irreversible hardware constraints and
identity-bound epistemic mechanisms resistant to replication or subversion.

</details>


### [41] [Bridging Econometrics and AI: VaR Estimation via Reinforcement Learning and GARCH Models](https://arxiv.org/abs/2504.16635)
*Fredy Pokou,Jules Sadefo Kamdem,François Benhmad*

Main category: cs.AI

TL;DR: 提出了一种结合GARCH模型与深度强化学习的混合框架，用于更准确地估计金融市场的风险值（VaR）。


<details>
  <summary>Details</summary>
Motivation: 传统计量经济模型（如GARCH）假设过于刚性，难以适应当前市场的复杂性，需要更灵活的方法。

Method: 结合GARCH波动率模型与深度强化学习（DDQN），将任务视为不平衡分类问题，动态调整风险预测。

Result: 在Eurostoxx 50数据上的实证验证显示，VaR估计准确性显著提高，违约次数和资本要求减少。

Conclusion: 该模型能实时调整风险水平，适用于现代主动风险管理。

Abstract: In an environment of increasingly volatile financial markets, the accurate
estimation of risk remains a major challenge. Traditional econometric models,
such as GARCH and its variants, are based on assumptions that are often too
rigid to adapt to the complexity of the current market dynamics. To overcome
these limitations, we propose a hybrid framework for Value-at-Risk (VaR)
estimation, combining GARCH volatility models with deep reinforcement learning.
Our approach incorporates directional market forecasting using the Double Deep
Q-Network (DDQN) model, treating the task as an imbalanced classification
problem. This architecture enables the dynamic adjustment of risk-level
forecasts according to market conditions. Empirical validation on daily
Eurostoxx 50 data covering periods of crisis and high volatility shows a
significant improvement in the accuracy of VaR estimates, as well as a
reduction in the number of breaches and also in capital requirements, while
respecting regulatory risk thresholds. The ability of the model to adjust risk
levels in real time reinforces its relevance to modern and proactive risk
management.

</details>


### [42] [IRIS: Interactive Research Ideation System for Accelerating Scientific Discovery](https://arxiv.org/abs/2504.16728)
*Aniketh Garikaparthi,Manasi Patwardhan,Lovekesh Vig,Arman Cohan*

Main category: cs.AI

TL;DR: IRIS是一个开源平台，利用大型语言模型（LLM）辅助科学假设生成，结合人类参与（HITL）和透明性，通过MCTS等技术增强研究人员的控制力。


<details>
  <summary>Details</summary>
Motivation: 解决现有自动假设生成方法缺乏透明性和人类参与的问题，加速科学发现。

Method: 开发IRIS平台，整合MCTS、细粒度反馈机制和文献合成功能，支持研究人员交互式生成假设。

Result: 用户研究表明IRIS能有效提升假设生成质量，平台已开源。

Conclusion: IRIS通过结合LLM和HITL，为科学假设生成提供了透明且可控的工具。

Abstract: The rapid advancement in capabilities of large language models (LLMs) raises
a pivotal question: How can LLMs accelerate scientific discovery? This work
tackles the crucial first stage of research, generating novel hypotheses. While
recent work on automated hypothesis generation focuses on multi-agent
frameworks and extending test-time compute, none of the approaches effectively
incorporate transparency and steerability through a synergistic
Human-in-the-loop (HITL) approach. To address this gap, we introduce IRIS:
Interactive Research Ideation System, an open-source platform designed for
researchers to leverage LLM-assisted scientific ideation. IRIS incorporates
innovative features to enhance ideation, including adaptive test-time compute
expansion via Monte Carlo Tree Search (MCTS), fine-grained feedback mechanism,
and query-based literature synthesis. Designed to empower researchers with
greater control and insight throughout the ideation process. We additionally
conduct a user study with researchers across diverse disciplines, validating
the effectiveness of our system in enhancing ideation. We open-source our code
at https://github.com/Anikethh/IRIS-Interactive-Research-Ideation-System

</details>


### [43] [A Survey of AI Agent Protocols](https://arxiv.org/abs/2504.16736)
*Yingxuan Yang,Huacan Chai,Yuanyi Song,Siyuan Qi,Muning Wen,Ning Li,Junwei Liao,Haoyi Hu,Jianghao Lin,Gaowei Chang,Weiwen Liu,Ying Wen,Yong Yu,Weinan Zhang*

Main category: cs.AI

TL;DR: 论文探讨了大型语言模型（LLM）代理之间缺乏标准化通信协议的问题，提出统一协议的重要性，并对现有协议进行分类、分析和性能比较，同时展望未来挑战。


<details>
  <summary>Details</summary>
Motivation: 随着LLM代理的广泛应用，缺乏标准化通信协议限制了其协作和扩展能力，亟需统一解决方案以提升效率和功能。

Method: 系统综述现有LLM代理通信协议，将其分为四类，并进行性能分析（如安全性、可扩展性和延迟）。

Result: 提供了协议分类和性能比较，帮助用户和开发者选择适合特定应用的协议。

Conclusion: 统一通信协议对LLM代理的未来发展至关重要，本文为设计和评估此类协议提供了实用参考。

Abstract: The rapid development of large language models (LLMs) has led to the
widespread deployment of LLM agents across diverse industries, including
customer service, content generation, data analysis, and even healthcare.
However, as more LLM agents are deployed, a major issue has emerged: there is
no standard way for these agents to communicate with external tools or data
sources. This lack of standardized protocols makes it difficult for agents to
work together or scale effectively, and it limits their ability to tackle
complex, real-world tasks. A unified communication protocol for LLM agents
could change this. It would allow agents and tools to interact more smoothly,
encourage collaboration, and triggering the formation of collective
intelligence. In this paper, we provide a systematic overview of existing
communication protocols for LLM agents. We classify them into four main
categories and make an analysis to help users and developers select the most
suitable protocols for specific applications. Additionally, we conduct a
comparative performance analysis of these protocols across key dimensions such
as security, scalability, and latency. Finally, we explore future challenges,
such as how protocols can adapt and survive in fast-evolving environments, and
what qualities future protocols might need to support the next generation of
LLM agent ecosystems. We expect this work to serve as a practical reference for
both researchers and engineers seeking to design, evaluate, or integrate robust
communication infrastructures for intelligent agents.

</details>


### [44] [Lightweight Latent Verifiers for Efficient Meta-Generation Strategies](https://arxiv.org/abs/2504.16760)
*Bartosz Piotrowski,Witold Drzewakowski,Konrad Staniszewski,Piotr Miłoś*

Main category: cs.AI

TL;DR: LiLaVe是一种轻量级验证方法，通过从基础LLM的隐藏状态中提取正确性信号，显著降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 传统验证器通常与基础LLM规模相当，计算成本高。LiLaVe旨在提供一种更高效的验证方法。

Method: LiLaVe从基础LLM的隐藏状态中提取正确性信号，并与元生成策略结合，如best-of-n或自一致性。

Result: LiLaVe显著提高了生成任务的准确性和效率，尤其适用于小型LLM。

Conclusion: LiLaVe展示了从LLM隐藏状态中提取信息的潜力，为推理密集型应用提供了可扩展且资源高效的解决方案。

Abstract: Verifiers are auxiliary models that assess the correctness of outputs
generated by base large language models (LLMs). They play a crucial role in
many strategies for solving reasoning-intensive problems with LLMs. Typically,
verifiers are LLMs themselves, often as large (or larger) than the base model
they support, making them computationally expensive. In this work, we introduce
a novel lightweight verification approach, LiLaVe, which reliably extracts
correctness signals from the hidden states of the base LLM. A key advantage of
LiLaVe is its ability to operate with only a small fraction of the
computational budget required by traditional LLM-based verifiers. To
demonstrate its practicality, we couple LiLaVe with popular meta-generation
strategies, like best-of-n or self-consistency. Moreover, we design novel
LiLaVe-based approaches, like conditional self-correction or conditional
majority voting, that significantly improve both accuracy and efficiency in
generation tasks with smaller LLMs. Our work demonstrates the fruitfulness of
extracting latent information from the hidden states of LLMs, and opens the
door to scalable and resource-efficient solutions for reasoning-intensive
applications.

</details>


### [45] [AIMO-2 Winning Solution: Building State-of-the-Art Mathematical Reasoning Models with OpenMathReasoning dataset](https://arxiv.org/abs/2504.16891)
*Ivan Moshkov,Darragh Hanley,Ivan Sorokin,Shubham Toshniwal,Christof Henkel,Benedikt Schifferer,Wei Du,Igor Gitman*

Main category: cs.AI

TL;DR: 本文介绍了在AIMO-2竞赛中获胜的数学推理模型，通过大规模数据集、工具集成推理方法和生成式解决方案选择（GenSelect）实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 构建高性能数学推理模型，推动数学问题解决领域的研究。

Method: 1. 创建包含54万高质量数学问题及320万详细解答的数据集；2. 开发迭代训练与工具集成推理方法；3. 提出GenSelect方法优化解决方案选择。

Result: 模型在数学推理基准测试中达到最先进水平。

Conclusion: 通过数据集、工具集成和GenSelect方法的结合，实现了高性能数学推理模型，并开源了代码和数据集。

Abstract: This paper presents our winning submission to the AI Mathematical Olympiad -
Progress Prize 2 (AIMO-2) competition. Our recipe for building state-of-the-art
mathematical reasoning models relies on three key pillars. First, we create a
large-scale dataset comprising 540K unique high-quality math problems,
including olympiad-level problems, and their 3.2M long-reasoning solutions.
Second, we develop a novel method to integrate code execution with long
reasoning models through iterative training, generation, and quality filtering,
resulting in 1.7M high-quality Tool-Integrated Reasoning solutions. Third, we
create a pipeline to train models to select the most promising solution from
many candidates. We show that such generative solution selection (GenSelect)
can significantly improve upon majority voting baseline. Combining these ideas,
we train a series of models that achieve state-of-the-art results on
mathematical reasoning benchmarks. To facilitate further research, we release
our code, models, and the complete OpenMathReasoning dataset under a
commercially permissive license.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [46] [Representation Learning for Tabular Data: A Comprehensive Survey](https://arxiv.org/abs/2504.16109)
*Jun-Peng Jiang,Si-Yang Liu,Hao-Run Cai,Qile Zhou,Han-Jia Ye*

Main category: cs.LG

TL;DR: 本文系统综述了表格表示学习领域，包括背景、挑战、基准以及深度神经网络（DNNs）的优缺点，并将现有方法分为专用、可迁移和通用模型三类。


<details>
  <summary>Details</summary>
Motivation: 表格数据是机器学习分类和回归中最常见的数据类型，DNNs因其表示学习能力展现出潜力，因此需要系统梳理该领域的研究进展。

Method: 将现有方法分为专用、可迁移和通用模型三类，并进一步按特征、样本和目标对专用模型进行层次分类，同时探讨了集成方法和扩展应用。

Result: 提出了表格表示学习的分类框架，并详细分析了各类模型的优缺点及其适用场景。

Conclusion: 表格表示学习是一个快速发展的领域，DNNs在其中的应用前景广阔，但仍需解决数据异质性和泛化能力等挑战。

Abstract: Tabular data, structured as rows and columns, is among the most prevalent
data types in machine learning classification and regression applications.
Models for learning from tabular data have continuously evolved, with Deep
Neural Networks (DNNs) recently demonstrating promising results through their
capability of representation learning. In this survey, we systematically
introduce the field of tabular representation learning, covering the
background, challenges, and benchmarks, along with the pros and cons of using
DNNs. We organize existing methods into three main categories according to
their generalization capabilities: specialized, transferable, and general
models. Specialized models focus on tasks where training and evaluation occur
within the same data distribution. We introduce a hierarchical taxonomy for
specialized models based on the key aspects of tabular data -- features,
samples, and objectives -- and delve into detailed strategies for obtaining
high-quality feature- and sample-level representations. Transferable models are
pre-trained on one or more datasets and subsequently fine-tuned on downstream
tasks, leveraging knowledge acquired from homogeneous or heterogeneous sources,
or even cross-modalities such as vision and language. General models, also
known as tabular foundation models, extend this concept further, allowing
direct application to downstream tasks without fine-tuning. We group these
general models based on the strategies used to adapt across heterogeneous
datasets. Additionally, we explore ensemble methods, which integrate the
strengths of multiple tabular models. Finally, we discuss representative
extensions of tabular learning, including open-environment tabular machine
learning, multimodal learning with tabular data, and tabular understanding.
More information can be found in the following repository:
https://github.com/LAMDA-Tabular/Tabular-Survey.

</details>


### [47] [Active Learning Methods for Efficient Data Utilization and Model Performance Enhancement](https://arxiv.org/abs/2504.16136)
*Chiung-Yi Tseng,Junhao Song,Ziqian Bi,Tianyang Wang,Chia Xin Liang,Ming Liu*

Main category: cs.LG

TL;DR: 本文概述了主动学习（AL）在机器学习中的应用，探讨了其基本概念、多领域应用及研究重点，同时指出当前挑战和未来方向。


<details>
  <summary>Details</summary>
Motivation: 解决数据丰富但标注稀缺的瓶颈问题，提升机器学习模型的性能和数据效率。

Method: 介绍主动学习的基本概念，探讨其在计算机视觉、自然语言处理等领域的应用，并分析不确定性估计、类别不平衡等关键研究主题。

Result: 主动学习通常优于被动学习，尤其在良好评估指标下表现更佳。

Conclusion: 本文为研究者和实践者提供了关键见解，并提出了主动学习未来发展的方向。

Abstract: In the era of data-driven intelligence, the paradox of data abundance and
annotation scarcity has emerged as a critical bottleneck in the advancement of
machine learning. This paper gives a detailed overview of Active Learning (AL),
which is a strategy in machine learning that helps models achieve better
performance using fewer labeled examples. It introduces the basic concepts of
AL and discusses how it is used in various fields such as computer vision,
natural language processing, transfer learning, and real-world applications.
The paper focuses on important research topics such as uncertainty estimation,
handling of class imbalance, domain adaptation, fairness, and the creation of
strong evaluation metrics and benchmarks. It also shows that learning methods
inspired by humans and guided by questions can improve data efficiency and help
models learn more effectively. In addition, this paper talks about current
challenges in the field, including the need to rebuild trust, ensure
reproducibility, and deal with inconsistent methodologies. It points out that
AL often gives better results than passive learning, especially when good
evaluation measures are used. This work aims to be useful for both researchers
and practitioners by providing key insights and proposing directions for future
progress in active learning.

</details>


### [48] [SparseJEPA: Sparse Representation Learning of Joint Embedding Predictive Architectures](https://arxiv.org/abs/2504.16140)
*Max Hartman,Lav Varshney*

Main category: cs.LG

TL;DR: SparseJEPA通过稀疏表示学习改进JEPA框架，提升表示质量与可解释性，并在CIFAR-100数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: JEPA框架虽强大但缺乏可解释性且效率低，稀疏表示学习可解决这些问题。

Method: SparseJEPA引入稀疏表示学习，通过惩罚方法鼓励语义相关特征共享潜在变量。

Result: 在CIFAR-100上训练并预训练轻量级ViT，展示了跨任务迁移的通用性，理论证明稀疏分组提升表示质量。

Conclusion: 稀疏性不仅优化潜在空间，还促进更可解释的表示学习，未来将探索基于对象的分组机制。

Abstract: Joint Embedding Predictive Architectures (JEPA) have emerged as a powerful
framework for learning general-purpose representations. However, these models
often lack interpretability and suffer from inefficiencies due to dense
embedding representations. We propose SparseJEPA, an extension that integrates
sparse representation learning into the JEPA framework to enhance the quality
of learned representations. SparseJEPA employs a penalty method that encourages
latent space variables to be shared among data features with strong semantic
relationships, while maintaining predictive performance. We demonstrate the
effectiveness of SparseJEPA by training on the CIFAR-100 dataset and
pre-training a lightweight Vision Transformer. The improved embeddings are
utilized in linear-probe transfer learning for both image classification and
low-level tasks, showcasing the architecture's versatility across different
transfer tasks. Furthermore, we provide a theoretical proof that demonstrates
that the grouping mechanism enhances representation quality. This was done by
displaying that grouping reduces Multiinformation among latent-variables,
including proofing the Data Processing Inequality for Multiinformation. Our
results indicate that incorporating sparsity not only refines the latent space
but also facilitates the learning of more meaningful and interpretable
representations. In further work, hope to further extend this method by finding
new ways to leverage the grouping mechanism through object-centric
representation learning.

</details>


### [49] [Deep Learning Meets Process-Based Models: A Hybrid Approach to Agricultural Challenges](https://arxiv.org/abs/2504.16141)
*Yue Shi,Liangxiu Han,Xin Zhang,Tam Sobeih,Thomas Gaiser,Nguyen Huu Thuy,Dominik Behrend,Amit Kumar Srivastava,Krishnagopal Halder,Frank Ewert*

Main category: cs.LG

TL;DR: 本文系统综述了基于过程的模型（PBMs）和深度学习（DL）在农业建模中的应用，提出混合PBM-DL框架，并通过案例研究验证其优越性。


<details>
  <summary>Details</summary>
Motivation: PBMs和DL各有优缺点，PBMs提供可解释性但难以扩展，DL擅长处理复杂数据但缺乏可解释性。研究旨在结合两者优势，提升农业建模的鲁棒性和泛化能力。

Method: 分类混合PBM-DL方法为DL-informed PBMs和PBM-informed DL，并通过作物干生物量预测案例比较混合模型与独立PBMs和DL模型的性能。

Result: 混合模型在噪声数据和未见地点上表现优于独立PBMs和DL模型，具有更强的鲁棒性和泛化能力。

Conclusion: 混合模型结合领域知识和AI方法，为可持续农业提供可扩展、可解释且可复现的建模方案，但仍需解决可解释性、扩展性和数据需求等挑战。

Abstract: Process-based models (PBMs) and deep learning (DL) are two key approaches in
agricultural modelling, each offering distinct advantages and limitations. PBMs
provide mechanistic insights based on physical and biological principles,
ensuring interpretability and scientific rigour. However, they often struggle
with scalability, parameterisation, and adaptation to heterogeneous
environments. In contrast, DL models excel at capturing complex, nonlinear
patterns from large datasets but may suffer from limited interpretability, high
computational demands, and overfitting in data-scarce scenarios.
  This study presents a systematic review of PBMs, DL models, and hybrid PBM-DL
frameworks, highlighting their applications in agricultural and environmental
modelling. We classify hybrid PBM-DL approaches into DL-informed PBMs, where
neural networks refine process-based models, and PBM-informed DL, where
physical constraints guide deep learning predictions. Additionally, we conduct
a case study on crop dry biomass prediction, comparing hybrid models against
standalone PBMs and DL models under varying data quality, sample sizes, and
spatial conditions. The results demonstrate that hybrid models consistently
outperform traditional PBMs and DL models, offering greater robustness to noisy
data and improved generalisation across unseen locations.
  Finally, we discuss key challenges, including model interpretability,
scalability, and data requirements, alongside actionable recommendations for
advancing hybrid modelling in agriculture. By integrating domain knowledge with
AI-driven approaches, this study contributes to the development of scalable,
interpretable, and reproducible agricultural models that support data-driven
decision-making for sustainable agriculture.

</details>


### [50] [Hexcute: A Tile-based Programming Language with Automatic Layout and Task-Mapping Synthesis](https://arxiv.org/abs/2504.16214)
*Xiao Zhang,Yaoyao Ding,Yang Hu,Gennady Pekhimenko*

Main category: cs.LG

TL;DR: Hexcute是一种基于块的编程语言，旨在简化混合数据类型DL算子的GPU优化，通过共享内存和寄存器抽象实现细粒度优化，并在性能上显著优于现有编译器。


<details>
  <summary>Details</summary>
Motivation: 当前DL量化技术需要处理混合数据类型的矩阵乘法算子，现有高级编译器缺乏表达能力，而低级编程模型需要大量工程努力。

Method: Hexcute通过基于块的编程语言暴露共享内存和寄存器抽象，结合任务映射和自动布局合成算法，减少编程复杂度。

Result: Hexcute在混合类型算子上的性能提升1.7-11.28倍，端到端评估中最高提升2.91倍。

Conclusion: Hexcute在表达能力和工程效率之间取得了平衡，为DL算子优化提供了高效解决方案。

Abstract: Deep learning (DL) workloads mainly run on accelerators like GPUs. Recent DL
quantization techniques demand a new matrix multiplication operator with mixed
input data types, further complicating GPU optimization. Prior high-level
compilers like Triton lack the expressiveness to implement key optimizations
like fine-grained data pipelines and hardware-friendly memory layouts for these
operators, while low-level programming models, such as Hidet, Graphene, and
CUTLASS, require significant programming efforts. To balance expressiveness
with engineering effort, we propose Hexcute, a tile-based programming language
that exposes shared memory and register abstractions to enable fine-grained
optimization for these operators. Additionally, Hexcute leverages task mapping
to schedule the GPU program, and to reduce programming efforts, it automates
layout and task mapping synthesis with a novel type-inference-based algorithm.
Our evaluation shows that Hexcute generalizes to a wide range of DL operators,
achieves 1.7-11.28$\times$ speedup over existing DL compilers for mixed-type
operators, and brings up to 2.91$\times$ speedup in the end-to-end evaluation.

</details>


### [51] [Using Phonemes in cascaded S2S translation pipeline](https://arxiv.org/abs/2504.16234)
*Rene Pilz,Johannes Schneider*

Main category: cs.LG

TL;DR: 论文探讨了在语音翻译中使用音素代替传统文本表示的方法，实验表明音素方法效果相当且资源需求更低。


<details>
  <summary>Details</summary>
Motivation: 研究音素作为文本表示的替代方案，以优化多语言语音翻译的资源利用和低资源语言适应性。

Method: 在WMT17数据集上训练序列到序列模型，比较文本表示和音素表示的性能，使用BLEU指标评估。

Result: 音素方法在质量上与文本方法相当，但资源需求更低，更适合低资源语言。

Conclusion: 音素表示在多语言语音翻译中具有潜力，尤其适用于资源受限的场景。

Abstract: This paper explores the idea of using phonemes as a textual representation
within a conventional multilingual simultaneous speech-to-speech translation
pipeline, as opposed to the traditional reliance on text-based language
representations. To investigate this, we trained an open-source
sequence-to-sequence model on the WMT17 dataset in two formats: one using
standard textual representation and the other employing phonemic
representation. The performance of both approaches was assessed using the BLEU
metric. Our findings shows that the phonemic approach provides comparable
quality but offers several advantages, including lower resource requirements or
better suitability for low-resource languages.

</details>


### [52] [General Post-Processing Framework for Fairness Adjustment of Machine Learning Models](https://arxiv.org/abs/2504.16238)
*Léandre Eberhard,Nirek Sharma,Filipp Shelobolin,Aalok Ganesh Shanbhag*

Main category: cs.LG

TL;DR: 提出了一种新颖的公平性调整框架，适用于多种机器学习任务，通过解耦公平性调整与模型训练，保持模型性能的同时提供灵活性。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习在关键领域（如信贷、公共政策、人才招聘）的应用增加，确保公平性成为法律和伦理的迫切需求。

Method: 将传统的“处理中”技术调整为“后处理”步骤，解耦公平性调整与模型训练，支持多种公平性指标。

Result: 框架在保持模型性能的同时，提供了更高的灵活性，并在真实数据集上实现了与Adversarial Debiasing相当的公平性/准确性权衡。

Conclusion: 该框架为公平性调整提供了更灵活、可解释的解决方案，适用于黑盒模型和不同数据集。

Abstract: As machine learning increasingly influences critical domains such as credit
underwriting, public policy, and talent acquisition, ensuring compliance with
fairness constraints is both a legal and ethical imperative. This paper
introduces a novel framework for fairness adjustments that applies to diverse
machine learning tasks, including regression and classification, and
accommodates a wide range of fairness metrics. Unlike traditional approaches
categorized as pre-processing, in-processing, or post-processing, our method
adapts in-processing techniques for use as a post-processing step. By
decoupling fairness adjustments from the model training process, our framework
preserves model performance on average while enabling greater flexibility in
model development. Key advantages include eliminating the need for custom loss
functions, enabling fairness tuning using different datasets, accommodating
proprietary models as black-box systems, and providing interpretable insights
into the fairness adjustments. We demonstrate the effectiveness of this
approach by comparing it to Adversarial Debiasing, showing that our framework
achieves a comparable fairness/accuracy tradeoff on real-world datasets.

</details>


### [53] [FairPlay: A Collaborative Approach to Mitigate Bias in Datasets for Improved AI Fairness](https://arxiv.org/abs/2504.16255)
*Tina Behzad,Mithilesh Kumar Singh,Anthony J. Ripa,Klaus Mueller*

Main category: cs.LG

TL;DR: FairPlay是一个基于网络的协作工具，帮助多方利益相关者通过谈判达成公平的数据集去偏结果，无需统一的公平理论。


<details>
  <summary>Details</summary>
Motivation: 解决决策公平性问题，尤其是当各方对公平的定义不一致且难以调和时。

Method: 开发了FairPlay软件，支持多方协作谈判和数据集去偏，通过游戏化交互达成共识。

Result: 用户研究表明，FairPlay能在约五轮游戏内帮助用户达成共识，展示了其在提升AI系统公平性方面的潜力。

Conclusion: FairPlay提供了一种无需统一公平标准的协作方法，有效解决了多方利益冲突下的公平决策问题。

Abstract: The issue of fairness in decision-making is a critical one, especially given
the variety of stakeholder demands for differing and mutually incompatible
versions of fairness. Adopting a strategic interaction of perspectives provides
an alternative to enforcing a singular standard of fairness. We present a
web-based software application, FairPlay, that enables multiple stakeholders to
debias datasets collaboratively. With FairPlay, users can negotiate and arrive
at a mutually acceptable outcome without a universally agreed-upon theory of
fairness. In the absence of such a tool, reaching a consensus would be highly
challenging due to the lack of a systematic negotiation process and the
inability to modify and observe changes. We have conducted user studies that
demonstrate the success of FairPlay, as users could reach a consensus within
about five rounds of gameplay, illustrating the application's potential for
enhancing fairness in AI systems.

</details>


### [54] [Learning Energy-Based Generative Models via Potential Flow: A Variational Principle Approach to Probability Density Homotopy Matching](https://arxiv.org/abs/2504.16262)
*Junn Yong Loo,Michelle Adeline,Julia Kaiwen Lau,Fang Yu Leong,Hwa Hui Tew,Arghya Pal,Vishnu Monn Baskaran,Chee-Ming Ting,Raphaël C. -W. Phan*

Main category: cs.LG

TL;DR: VPFB是一种新的基于能量的生成框架，通过变分损失最小化KL散度，避免了隐式MCMC采样，提高了稳定性和效率。


<details>
  <summary>Details</summary>
Motivation: 探索潜在流与显式EBM之间的关系，解决高维设置中对比散度训练的不稳定性和高成本问题。

Method: VPFB通过构建流驱动的密度同伦，并通过变分损失匹配数据分布，学习能量参数化的潜在流。

Result: 实验表明，VPFB在图像生成、插值、分布外检测和组合生成等任务中表现优异。

Conclusion: VPFB在保持EBM可解释性的同时，实现了高效且稳健的生成建模。

Abstract: Energy-based models (EBMs) are a powerful class of probabilistic generative
models due to their flexibility and interpretability. However, relationships
between potential flows and explicit EBMs remain underexplored, while
contrastive divergence training via implicit Markov chain Monte Carlo (MCMC)
sampling is often unstable and expensive in high-dimensional settings. In this
paper, we propose Variational Potential Flow Bayes (VPFB), a new energy-based
generative framework that eliminates the need for implicit MCMC sampling and
does not rely on auxiliary networks or cooperative training. VPFB learns an
energy-parameterized potential flow by constructing a flow-driven density
homotopy that is matched to the data distribution through a variational loss
minimizing the Kullback-Leibler divergence between the flow-driven and marginal
homotopies. This principled formulation enables robust and efficient generative
modeling while preserving the interpretability of EBMs. Experimental results on
image generation, interpolation, out-of-distribution detection, and
compositional generation confirm the effectiveness of VPFB, showing that our
method performs competitively with existing approaches in terms of sample
quality and versatility across diverse generative modeling tasks.

</details>


### [55] [Gradient-Optimized Fuzzy Classifier: A Benchmark Study Against State-of-the-Art Models](https://arxiv.org/abs/2504.16263)
*Magnus Sieverding,Nathan Steffen,Kelly Cohen*

Main category: cs.LG

TL;DR: 本文比较了梯度优化模糊推理系统（GF）与多种先进机器学习模型的性能，发现GF在分类准确性、训练效率和鲁棒性方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 研究梯度优化的模糊推理系统是否能作为复杂深度学习模型的高效、可解释替代方案。

Method: 在五个UCI数据集上对比GF与随机森林、XGBoost、逻辑回归、支持向量机和神经网络的性能。

Result: GF在分类准确性上表现竞争性甚至更优，训练时间短且对噪声数据鲁棒。

Conclusion: 梯度优化的模糊系统是监督学习中高效、可解释的替代方案。

Abstract: This paper presents a performance benchmarking study of a Gradient-Optimized
Fuzzy Inference System (GF) classifier against several state-of-the-art machine
learning models, including Random Forest, XGBoost, Logistic Regression, Support
Vector Machines, and Neural Networks. The evaluation was conducted across five
datasets from the UCI Machine Learning Repository, each chosen for their
diversity in input types, class distributions, and classification complexity.
Unlike traditional Fuzzy Inference Systems that rely on derivative-free
optimization methods, the GF leverages gradient descent to significantly
improving training efficiency and predictive performance. Results demonstrate
that the GF model achieved competitive, and in several cases superior,
classification accuracy while maintaining high precision and exceptionally low
training times. In particular, the GF exhibited strong consistency across folds
and datasets, underscoring its robustness in handling noisy data and variable
feature sets. These findings support the potential of gradient optimized fuzzy
systems as interpretable, efficient, and adaptable alternatives to more complex
deep learning models in supervised learning tasks.

</details>


### [56] [Boosting Classifier Performance with Opposition-Based Data Transformation](https://arxiv.org/abs/2504.16268)
*Abdesslem Layeb*

Main category: cs.LG

TL;DR: 本文提出了一种基于对立学习（OBL）的新型数据转换框架，用于提升传统分类算法的性能。通过生成合成对立样本，OBL改善了决策边界的形成，并在实验中显著提高了分类准确率和F1分数。


<details>
  <summary>Details</summary>
Motivation: 传统分类算法在复杂或稀疏学习环境中表现不佳，因此需要一种轻量级但强大的数据转换策略来提升性能。

Method: 提出了三种OBL变体（全局OBL、类级OBL和局部类级OBL），并将其与KNN、SVM、LR和DT等分类器结合。

Result: 在26个异构和高维数据集上的实验表明，OBL增强的分类器在准确率和F1分数上均优于标准分类器，且计算效率更高。

Conclusion: OBL作为一种轻量级数据转换策略，在提升分类性能方面具有显著潜力，尤其适用于复杂或稀疏的学习环境。

Abstract: In this paper, we introduce a novel data transformation framework based on
Opposition-Based Learning (OBL) to boost the performance of traditional
classification algorithms. Originally developed to accelerate convergence in
optimization tasks, OBL is leveraged here to generate synthetic opposite
samples that replace the acutely training data and improve decision boundary
formation. We explore three OBL variants; Global OBL, Class-Wise OBL, and
Localized Class-Wise OBL; and integrate them with several widely used
classifiers, including K-Nearest Neighbors (KNN), Support Vector Machines
(SVM), Logistic Regression (LR), and Decision Tree (DT). Extensive experiments
conducted on 26 heterogeneous and high-dimensional datasets demonstrate that
OBL-enhanced classifiers consistently outperform their standard counterparts in
terms of accuracy and F1-score, frequently achieving near-perfect or perfect
classification. Furthermore, OBL contributes to improved computational
efficiency, particularly in SVM and LR. These findings underscore the potential
of OBL as a lightweight yet powerful data transformation strategy for enhancing
classification performance, especially in complex or sparse learning
environments.

</details>


### [57] [Learning Explainable Dense Reward Shapes via Bayesian Optimization](https://arxiv.org/abs/2504.16272)
*Ryan Koo,Ian Yang,Vipul Raheja,Mingyi Hong,Kwang-Sung Jun,Dongyeop Kang*

Main category: cs.LG

TL;DR: 提出了一种基于可解释性方法的奖励塑造函数，用于改进LLM对齐中的稀疏反馈问题，通过双层优化框架实现更优的令牌级奖励分配。


<details>
  <summary>Details</summary>
Motivation: 当前RLHF流水线在LLM对齐中仅使用标量奖励，导致稀疏反馈和次优的令牌级信用分配，需要更精细的奖励分配方法。

Method: 提出利用SHAP和LIME等可解释性方法估计每令牌奖励，并通过双层优化框架（结合贝叶斯优化和策略训练）学习奖励塑造函数的参数。

Result: 实验表明，改进的令牌级奖励分配能提升下游任务性能，并加速策略训练。理论证明，特征加性归因方法保持原始奖励的最优策略。

Conclusion: 通过可解释性方法优化令牌级奖励分配，显著提升LLM对齐效果和训练效率。

Abstract: Current reinforcement learning from human feedback (RLHF) pipelines for large
language model (LLM) alignment typically assign scalar rewards to sequences,
using the final token as a surrogate indicator for the quality of the entire
sequence. However, this leads to sparse feedback and suboptimal token-level
credit assignment. In this work, we frame reward shaping as an optimization
problem focused on token-level credit assignment. We propose a reward-shaping
function leveraging explainability methods such as SHAP and LIME to estimate
per-token rewards from the reward model. To learn parameters of this shaping
function, we employ a bilevel optimization framework that integrates Bayesian
Optimization and policy training to handle noise from the token reward
estimates. Our experiments show that achieving a better balance of token-level
reward attribution leads to performance improvements over baselines on
downstream tasks and finds an optimal policy faster during training.
Furthermore, we show theoretically that explainability methods that are feature
additive attribution functions maintain the optimal policy as the original
reward.

</details>


### [58] [Quantum Doubly Stochastic Transformers](https://arxiv.org/abs/2504.16275)
*Jannis Born,Filip Skogh,Kahn Rhrissorrakrai,Filippo Utro,Nico Wagner,Aleksandros Sobczyk*

Main category: cs.LG

TL;DR: 论文提出了一种混合经典-量子双随机Transformer（QDSFormer），用变分量子电路替代Softmax，提升了性能与训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer中Softmax导致训练不稳定，而Sinkhorn算法虽能生成双随机矩阵但不够灵活。量子电路提供了一种新的参数化方法，具有独特优势。

Method: 用变分量子电路替代Softmax，生成双随机矩阵，并研究其表达能力。

Result: QDSFormer在小规模物体识别任务中表现优于标准Vision Transformer和其他双随机Transformer，且训练更稳定。

Conclusion: QDSFormer展示了量子电路在Transformer中的潜力，尤其在提升性能与训练稳定性方面。

Abstract: At the core of the Transformer, the Softmax normalizes the attention matrix
to be right stochastic. Previous research has shown that this often
destabilizes training and that enforcing the attention matrix to be doubly
stochastic (through Sinkhorn's algorithm) consistently improves performance
across different tasks, domains and Transformer flavors. However, Sinkhorn's
algorithm is iterative, approximative, non-parametric and thus inflexible
w.r.t. the obtained doubly stochastic matrix (DSM). Recently, it has been
proven that DSMs can be obtained with a parametric quantum circuit, yielding a
novel quantum inductive bias for DSMs with no known classical analogue.
Motivated by this, we demonstrate the feasibility of a hybrid classical-quantum
doubly stochastic Transformer (QDSFormer) that replaces the Softmax in the
self-attention layer with a variational quantum circuit. We study the
expressive power of the circuit and find that it yields more diverse DSMs that
better preserve information than classical operators. Across multiple
small-scale object recognition tasks, we find that our QDSFormer consistently
surpasses both a standard Vision Transformer and other doubly stochastic
Transformers. Beyond the established Sinkformer, this comparison includes a
novel quantum-inspired doubly stochastic Transformer (based on QR
decomposition) that can be of independent interest. The QDSFormer also shows
improved training stability and lower performance variation suggesting that it
may mitigate the notoriously unstable training of ViTs on small-scale data.

</details>


### [59] [An Automated Pipeline for Few-Shot Bird Call Classification: A Case Study with the Tooth-Billed Pigeon](https://arxiv.org/abs/2504.16276)
*Abhishek Jana,Moeumu Uili,James Atherton,Mark O'Brien,Joe Wood,Leandra Brickson*

Main category: cs.LG

TL;DR: 提出了一种针对稀有鸟类的自动化单次鸟鸣分类方法，适用于训练数据极少的物种。


<details>
  <summary>Details</summary>
Motivation: 现有公开分类器（如BirdNET和Perch）对常见鸟类表现良好，但对仅有1-3条录音的稀有物种无能为力，这对濒危物种监测至关重要。

Method: 利用大型鸟类分类网络的嵌入空间，结合余弦相似度分类器及预处理技术（过滤和降噪），优化极少量训练数据的检测效果。

Result: 在模拟和真实测试中（如极危物种齿嘴鸽），模型召回率达1.0，准确率0.95。

Conclusion: 该系统为濒危物种监测提供了实用工具，且开源可用。

Abstract: This paper presents an automated one-shot bird call classification pipeline
designed for rare species absent from large publicly available classifiers like
BirdNET and Perch. While these models excel at detecting common birds with
abundant training data, they lack options for species with only 1-3 known
recordings-a critical limitation for conservationists monitoring the last
remaining individuals of endangered birds. To address this, we leverage the
embedding space of large bird classification networks and develop a classifier
using cosine similarity, combined with filtering and denoising preprocessing
techniques, to optimize detection with minimal training data. We evaluate
various embedding spaces using clustering metrics and validate our approach in
both a simulated scenario with Xeno-Canto recordings and a real-world test on
the critically endangered tooth-billed pigeon (Didunculus strigirostris), which
has no existing classifiers and only three confirmed recordings. The final
model achieved 1.0 recall and 0.95 accuracy in detecting tooth-billed pigeon
calls, making it practical for use in the field. This open-source system
provides a practical tool for conservationists seeking to detect and monitor
rare species on the brink of extinction.

</details>


### [60] [DataS^3: Dataset Subset Selection for Specialization](https://arxiv.org/abs/2504.16277)
*Neha Hulkund,Alaa Maalouf,Levi Cai,Daniel Yang,Tsun-Hsuan Wang,Abigail O'Neil,Timm Haucke,Sandeep Mukherjee,Vikram Ramaswamy,Judy Hansen Shen,Gabriel Tseng,Mike Walmsley,Daniela Rus,Ken Goldberg,Hannah Kerner,Irene Chen,Yogesh Girdhar,Sara Beery*

Main category: cs.LG

TL;DR: 论文提出了一种针对特定部署场景的数据子集选择方法（DS3），并引入了DataS^3数据集和基准测试，验证了部署专用数据子集对性能提升的重要性。


<details>
  <summary>Details</summary>
Motivation: 现实中的机器学习应用需要在特定部署场景（如特定医院或国家公园）中表现良好，但部署数据分布往往不平衡且独特，导致训练数据与部署数据分布不一致，性能下降。

Method: 提出了DS3问题，即从通用训练数据中选择适合特定部署场景的子集，并引入DataS^3数据集和基准测试，评估了多种算法（如核心集、数据过滤和数据整理）的性能。

Result: 研究发现通用方法在部署专用任务上表现不佳，而手动整理的部署专用子集性能显著提升，最高可达51.3%。

Conclusion: 研究强调了针对特定部署场景的数据整理对性能提升和训练效率的重要性，并认为随着全球公共数据集的普及，这一问题将愈发重要。

Abstract: In many real-world machine learning (ML) applications (e.g. detecting broken
bones in x-ray images, detecting species in camera traps), in practice models
need to perform well on specific deployments (e.g. a specific hospital, a
specific national park) rather than the domain broadly. However, deployments
often have imbalanced, unique data distributions. Discrepancy between the
training distribution and the deployment distribution can lead to suboptimal
performance, highlighting the need to select deployment-specialized subsets
from the available training data. We formalize dataset subset selection for
specialization (DS3): given a training set drawn from a general distribution
and a (potentially unlabeled) query set drawn from the desired
deployment-specific distribution, the goal is to select a subset of the
training data that optimizes deployment performance.
  We introduce DataS^3; the first dataset and benchmark designed specifically
for the DS3 problem. DataS^3 encompasses diverse real-world application
domains, each with a set of distinct deployments to specialize in. We conduct a
comprehensive study evaluating algorithms from various families--including
coresets, data filtering, and data curation--on DataS^3, and find that
general-distribution methods consistently fail on deployment-specific tasks.
Additionally, we demonstrate the existence of manually curated
(deployment-specific) expert subsets that outperform training on all available
data with accuracy gains up to 51.3 percent. Our benchmark highlights the
critical role of tailored dataset curation in enhancing performance and
training efficiency on deployment-specific distributions, which we posit will
only become more important as global, public datasets become available across
domains and ML models are deployed in the real world.

</details>


### [61] [Affect Models Have Weak Generalizability to Atypical Speech](https://arxiv.org/abs/2504.16283)
*Jaya Narain,Amrit Romana,Vikramjit Mitra,Colin Lea,Shirley Ren*

Main category: cs.LG

TL;DR: 论文研究了语音异常对情感识别模型性能的影响，发现模型输出受语音异常显著影响，并提出通过伪标签数据微调模型以提高鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 探讨语音异常（如发音、韵律和音质问题）如何影响情感识别模型的性能，并评估现有模型在非典型语音数据集上的表现。

Method: 比较公开的情感识别模型在典型和非典型语音数据集上的表现，分析三类语音异常（可懂度、单调音和刺耳度）对模型输出的影响，并尝试通过伪标签数据微调模型。

Result: 语音异常显著影响模型输出，例如非典型语音被预测为“悲伤”的比例更高；微调模型可提升对非典型语音的性能，且不影响典型语音的表现。

Conclusion: 研究强调需要更广泛的训练和评估数据集，以及开发对语音差异更具鲁棒性的建模方法。

Abstract: Speech and voice conditions can alter the acoustic properties of speech,
which could impact the performance of paralinguistic models for affect for
people with atypical speech. We evaluate publicly available models for
recognizing categorical and dimensional affect from speech on a dataset of
atypical speech, comparing results to datasets of typical speech. We
investigate three dimensions of speech atypicality: intelligibility, which is
related to pronounciation; monopitch, which is related to prosody, and
harshness, which is related to voice quality. We look at (1) distributional
trends of categorical affect predictions within the dataset, (2) distributional
comparisons of categorical affect predictions to similar datasets of typical
speech, and (3) correlation strengths between text and speech predictions for
spontaneous speech for valence and arousal. We find that the output of affect
models is significantly impacted by the presence and degree of speech
atypicalities. For instance, the percentage of speech predicted as sad is
significantly higher for all types and grades of atypical speech when compared
to similar typical speech datasets. In a preliminary investigation on improving
robustness for atypical speech, we find that fine-tuning models on
pseudo-labeled atypical speech data improves performance on atypical speech
without impacting performance on typical speech. Our results emphasize the need
for broader training and evaluation datasets for speech emotion models, and for
modeling approaches that are robust to voice and speech differences.

</details>


### [62] [Semantics at an Angle: When Cosine Similarity Works Until It Doesn't](https://arxiv.org/abs/2504.16318)
*Kisung You*

Main category: cs.LG

TL;DR: 本文回顾了余弦相似度的演变、优势与局限性，探讨了其在语义信息中的表现及新兴替代方法。


<details>
  <summary>Details</summary>
Motivation: 余弦相似度是现代机器学习中广泛使用的嵌入比较标准，但其在嵌入范数携带语义信息时的局限性逐渐显现。

Method: 通过反思性和选择性分析，探讨余弦相似度的适用场景及其失效情况。

Result: 余弦相似度在许多场景下表现良好，但在某些语义信息相关的任务中存在盲点，新兴替代方法正逐步解决这些问题。

Conclusion: 本文为定量科学家提供了概念清晰性和实践视角，强调嵌入不仅是向量，更是几何和哲学对象。

Abstract: Cosine similarity has become a standard metric for comparing embeddings in
modern machine learning. Its scale-invariance and alignment with model training
objectives have contributed to its widespread adoption. However, recent studies
have revealed important limitations, particularly when embedding norms carry
meaningful semantic information. This informal article offers a reflective and
selective examination of the evolution, strengths, and limitations of cosine
similarity. We highlight why it performs well in many settings, where it tends
to break down, and how emerging alternatives are beginning to address its blind
spots. We hope to offer a mix of conceptual clarity and practical perspective,
especially for quantitative scientists who think about embeddings not just as
vectors, but as geometric and philosophical objects.

</details>


### [63] [Disentangled Graph Representation Based on Substructure-Aware Graph Optimal Matching Kernel Convolutional Networks](https://arxiv.org/abs/2504.16360)
*Mao Wang,Tao Wu,Xingping Xian,Shaojie Qiao,Weina Niu,Canyixing Cui*

Main category: cs.LG

TL;DR: 论文提出GOMKCN方法，通过子图结构优化匹配核提升图表示学习的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有方法对图结构的表征不够精细，限制了结构模式分析。

Method: 将图视为节点中心子图，引入图最优匹配核（GOMK）作为卷积算子，计算子图与可学习滤波器之间的相似性。

Result: GOMKCN在图形模式挖掘和预测中表现出更高的准确性和可解释性。

Conclusion: GOMKCN为解耦图表示学习提供了理论支持，并解决了图核在可微性和准确性之间的权衡问题。

Abstract: Graphs effectively characterize relational data, driving graph representation
learning methods that uncover underlying predictive information. As
state-of-the-art approaches, Graph Neural Networks (GNNs) enable end-to-end
learning for diverse tasks. Recent disentangled graph representation learning
enhances interpretability by decoupling independent factors in graph data.
However, existing methods often implicitly and coarsely characterize graph
structures, limiting structural pattern analysis within the graph. This paper
proposes the Graph Optimal Matching Kernel Convolutional Network (GOMKCN) to
address this limitation. We view graphs as node-centric subgraphs, where each
subgraph acts as a structural factor encoding position-specific information.
This transforms graph prediction into structural pattern recognition. Inspired
by CNNs, GOMKCN introduces the Graph Optimal Matching Kernel (GOMK) as a
convolutional operator, computing similarities between subgraphs and learnable
graph filters. Mathematically, GOMK maps subgraphs and filters into a Hilbert
space, representing graphs as point sets. Disentangled representations emerge
from projecting subgraphs onto task-optimized filters, which adaptively capture
relevant structural patterns via gradient descent. Crucially, GOMK incorporates
local correspondences in similarity measurement, resolving the trade-off
between differentiability and accuracy in graph kernels. Experiments validate
that GOMKCN achieves superior accuracy and interpretability in graph pattern
mining and prediction. The framework advances the theoretical foundation for
disentangled graph representation learning.

</details>


### [64] [Natural Policy Gradient for Average Reward Non-Stationary RL](https://arxiv.org/abs/2504.16415)
*Neharika Jali,Eshika Pathak,Pranay Sharma,Guannan Qu,Gauri Joshi*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We consider the problem of non-stationary reinforcement learning (RL) in the
infinite-horizon average-reward setting. We model it by a Markov Decision
Process with time-varying rewards and transition probabilities, with a
variation budget of $\Delta_T$. Existing non-stationary RL algorithms focus on
model-based and model-free value-based methods. Policy-based methods despite
their flexibility in practice are not theoretically well understood in
non-stationary RL. We propose and analyze the first model-free policy-based
algorithm, Non-Stationary Natural Actor-Critic (NS-NAC), a policy gradient
method with a restart based exploration for change and a novel interpretation
of learning rates as adapting factors. Further, we present a bandit-over-RL
based parameter-free algorithm BORL-NS-NAC that does not require prior
knowledge of the variation budget $\Delta_T$. We present a dynamic regret of
$\tilde{\mathscr O}(|S|^{1/2}|A|^{1/2}\Delta_T^{1/6}T^{5/6})$ for both
algorithms, where $T$ is the time horizon, and $|S|$, $|A|$ are the sizes of
the state and action spaces. The regret analysis leverages a novel adaptation
of the Lyapunov function analysis of NAC to dynamic environments and
characterizes the effects of simultaneous updates in policy, value function
estimate and changes in the environment.

</details>


### [65] [MAGIC: Near-Optimal Data Attribution for Deep Learning](https://arxiv.org/abs/2504.16430)
*Andrew Ilyas,Logan Engstrom*

Main category: cs.LG

TL;DR: 提出了一种新方法（MAGIC）用于预测数据属性，结合经典方法和元微分技术，以更准确地估计训练数据增减对模型预测的影响。


<details>
  <summary>Details</summary>
Motivation: 在非凸大规模场景中，现有方法对数据属性的估计效果不佳，需要更有效的方法。

Method: 结合经典方法和元微分技术，开发了MAGIC方法。

Result: MAGIC方法能够（近乎）最优地估计训练数据增减对模型预测的影响。

Conclusion: MAGIC方法在非凸大规模场景中显著提升了数据属性估计的准确性。

Abstract: The goal of predictive data attribution is to estimate how adding or removing
a given set of training datapoints will affect model predictions. In convex
settings, this goal is straightforward (i.e., via the infinitesimal jackknife).
In large-scale (non-convex) settings, however, existing methods are far less
successful -- current methods' estimates often only weakly correlate with
ground truth. In this work, we present a new data attribution method (MAGIC)
that combines classical methods and recent advances in metadifferentiation to
(nearly) optimally estimate the effect of adding or removing training data on
model predictions.

</details>


### [66] [Target Concrete Score Matching: A Holistic Framework for Discrete Diffusion](https://arxiv.org/abs/2504.16431)
*Ruixiang Zhang,Shuangfei Zhai,Yizhe Zhang,James Thornton,Zijing Ou,Joshua Susskind,Navdeep Jaitly*

Main category: cs.LG

TL;DR: 提出了一种名为TCSM的新目标函数，用于训练和微调离散扩散模型，具有广泛适用性和灵活性。


<details>
  <summary>Details</summary>
Motivation: 离散扩散模型在离散数据建模和生成方面具有潜力，但现有方法缺乏通用性和灵活性。

Method: TCSM通过估计目标分布的具体分数，支持预训练和微调，并能与奖励函数和预训练模型无缝集成。

Result: 在语言建模任务中，TCSM表现优于或与现有方法相当，同时更具样本效率和灵活性。

Conclusion: TCSM为离散扩散模型提供了通用框架，适用于多种场景，扩展了其应用范围。

Abstract: Discrete diffusion is a promising framework for modeling and generating
discrete data. In this work, we present Target Concrete Score Matching (TCSM),
a novel and versatile objective for training and fine-tuning discrete diffusion
models. TCSM provides a general framework with broad applicability. It supports
pre-training discrete diffusion models directly from data samples, and many
existing discrete diffusion approaches naturally emerge as special cases of our
more general TCSM framework. Furthermore, the same TCSM objective extends to
post-training of discrete diffusion models, including fine-tuning using reward
functions or preference data, and distillation of knowledge from pre-trained
autoregressive models. These new capabilities stem from the core idea of TCSM,
estimating the concrete score of the target distribution, which resides in the
original (clean) data space. This allows seamless integration with reward
functions and pre-trained models, which inherently only operate in the clean
data space rather than the noisy intermediate spaces of diffusion processes.
Our experiments on language modeling tasks demonstrate that TCSM matches or
surpasses current methods. Additionally, TCSM is versatile, applicable to both
pre-training and post-training scenarios, offering greater flexibility and
sample efficiency.

</details>


### [67] [iTFKAN: Interpretable Time Series Forecasting with Kolmogorov-Arnold Network](https://arxiv.org/abs/2504.16432)
*Ziran Liang,Rui An,Wenqi Fan,Yanghui Rao,Yuxuan Liang*

Main category: cs.LG

TL;DR: iTFKAN是一种新型可解释时间序列预测模型，通过符号化和时间-频率协同学习提升预测性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: 当前深度预测方法缺乏可解释性，限制了其在安全关键领域的应用。

Method: iTFKAN通过模型符号化实现可解释性，并采用先验知识注入和时间-频率协同学习策略。

Result: 实验表明iTFKAN在预测性能和可解释性上均表现优异。

Conclusion: iTFKAN为可信时间序列预测提供了有效解决方案。

Abstract: As time evolves, data within specific domains exhibit predictability that
motivates time series forecasting to predict future trends from historical
data. However, current deep forecasting methods can achieve promising
performance but generally lack interpretability, hindering trustworthiness and
practical deployment in safety-critical applications such as auto-driving and
healthcare. In this paper, we propose a novel interpretable model, iTFKAN, for
credible time series forecasting. iTFKAN enables further exploration of model
decision rationales and underlying data patterns due to its interpretability
achieved through model symbolization. Besides, iTFKAN develops two strategies,
prior knowledge injection, and time-frequency synergy learning, to effectively
guide model learning under complex intertwined time series data. Extensive
experimental results demonstrated that iTFKAN can achieve promising forecasting
performance while simultaneously possessing high interpretive capabilities.

</details>


### [68] [Private Federated Learning using Preference-Optimized Synthetic Data](https://arxiv.org/abs/2504.16438)
*Charlie Hou,Mei-Yu Wang,Yige Zhu,Daniel Lazar,Giulia Fanti*

Main category: cs.LG

TL;DR: 论文提出了一种名为POPri的新方法，通过偏好优化算法（如DPO）利用客户端反馈生成高质量的差分隐私合成数据，显著提升了合成数据的实用性。


<details>
  <summary>Details</summary>
Motivation: 现有的差分隐私联邦学习（DP-FL）方法可能被差分隐私合成数据方法超越，但后者需要复杂的提示工程或迭代反馈。论文旨在通过偏好优化简化这一过程。

Method: POPri算法将客户端反馈视为偏好排名，并利用DPO等偏好优化算法微调LLM，生成高质量的差分隐私合成数据。

Result: 在LargeFedBench数据集上，POPri将完全隐私与非隐私设置下的下一个令牌预测准确率差距缩小了68%，优于现有方法。

Conclusion: POPri通过偏好优化显著提升了差分隐私合成数据的质量，为联邦学习提供了更高效的隐私保护解决方案。

Abstract: In practical settings, differentially private Federated learning (DP-FL) is
the dominant method for training models from private, on-device client data.
Recent work has suggested that DP-FL may be enhanced or outperformed by methods
that use DP synthetic data (Wu et al., 2024; Hou et al., 2024). The primary
algorithms for generating DP synthetic data for FL applications require careful
prompt engineering based on public information and/or iterative private client
feedback. Our key insight is that the private client feedback collected by
prior DP synthetic data methods (Hou et al., 2024; Xie et al., 2024) can be
viewed as a preference ranking. Our algorithm, Preference Optimization for
Private Client Data (POPri) harnesses client feedback using preference
optimization algorithms such as Direct Preference Optimization (DPO) to
fine-tune LLMs to generate high-quality DP synthetic data. To evaluate POPri,
we release LargeFedBench, a new federated text benchmark for uncontaminated LLM
evaluations on federated client data. POPri substantially improves the utility
of DP synthetic data relative to prior work on LargeFedBench datasets and an
existing benchmark from Xie et al. (2024). POPri closes the gap between
next-token prediction accuracy in the fully-private and non-private settings by
up to 68%, compared to 52% for prior synthetic data methods, and 10% for
state-of-the-art DP federated learning methods. The code and data are available
at https://github.com/meiyuw/POPri.

</details>


### [69] [Node Assigned physics-informed neural networks for thermal-hydraulic system simulation: CVH/FL module](https://arxiv.org/abs/2504.16447)
*Jeesuk Shin,Cheolwoong Kim,Sunwoong Yang,Minseo Lee,Sung Joong Kim,Joongoo Jeon*

Main category: cs.LG

TL;DR: 论文提出了一种基于物理信息神经网络（PINN）的新数值方法，用于核电站严重事故分析，解决了传统热工水力系统代码的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统热工水力系统代码（如MELCOR和MAAP）在模拟严重事故时存在有限差分方案不一致的问题，导致多物理场分析中的单向耦合。

Method: 提出节点分配PINN（NA-PINN），为系统代码的每个节点分配独立网络，排除空间信息，专注于时间解。

Result: 在6水箱模拟中，NA-PINN的最大绝对误差为0.007，显著优于普通PINN的1.678，仅NA-PINN达到可接受精度。

Conclusion: NA-PINN首次成功应用于系统代码，未来将扩展为多物理场求解器并开发为替代模型。

Abstract: Severe accidents (SAs) in nuclear power plants have been analyzed using
thermal-hydraulic (TH) system codes such as MELCOR and MAAP. These codes
efficiently simulate the progression of SAs, while they still have inherent
limitations due to their inconsistent finite difference schemes. The use of
empirical schemes incorporating both implicit and explicit formulations
inherently induces unidirectional coupling in multi-physics analyses. The
objective of this study is to develop a novel numerical method for TH system
codes using physics-informed neural network (PINN). They have shown strength in
solving multi-physics due to the innate feature of neural networks-automatic
differentiation. We propose a node-assigned PINN (NA-PINN) that is suitable for
the control volume approach-based system codes. NA-PINN addresses the issue of
spatial governing equation variation by assigning an individual network to each
nodalization of the system code, such that spatial information is excluded from
both the input and output domains, and each subnetwork learns to approximate a
purely temporal solution. In this phase, we evaluated the accuracy of the PINN
methods for the hydrodynamic module. In the 6 water tank simulation, PINN and
NA-PINN showed maximum absolute errors of 1.678 and 0.007, respectively. It
should be noted that only NA-PINN demonstrated acceptable accuracy. To the best
of the authors' knowledge, this is the first study to successfully implement a
system code using PINN. Our future work involves extending NA-PINN to a
multi-physics solver and developing it in a surrogate manner.

</details>


### [70] [An Effective Gram Matrix Characterizes Generalization in Deep Networks](https://arxiv.org/abs/2504.16450)
*Rubing Yang,Pratik Chaudhari*

Main category: cs.LG

TL;DR: 论文通过微分方程分析深度网络在梯度下降训练中泛化间隙的演化，提出有效Gram矩阵和初始残差的对齐决定泛化性能。


<details>
  <summary>Details</summary>
Motivation: 研究深度网络训练中泛化间隙的演化机制，以理解不同数据集和架构对泛化性能的影响。

Method: 推导控制泛化间隙演化的微分方程，分析有效Gram矩阵与初始残差的对齐关系。

Result: 实验表明该分析能准确预测测试损失，且训练过程是良性的，不会显著恶化泛化间隙。

Conclusion: 数据集与架构的匹配程度是决定泛化性能好坏的主要因素。

Abstract: We derive a differential equation that governs the evolution of the
generalization gap when a deep network is trained by gradient descent. This
differential equation is controlled by two quantities, a contraction factor
that brings together trajectories corresponding to slightly different datasets,
and a perturbation factor that accounts for them training on different
datasets. We analyze this differential equation to compute an ``effective Gram
matrix'' that characterizes the generalization gap after training in terms of
the alignment between this Gram matrix and a certain initial ``residual''.
Empirical evaluations on image classification datasets indicate that this
analysis can predict the test loss accurately. Further, at any point during
training, the residual predominantly lies in the subspace of the effective Gram
matrix with the smallest eigenvalues. This indicates that the training process
is benign, i.e., it does not lead to significant deterioration of the
generalization gap (which is zero at initialization). The alignment between the
effective Gram matrix and the residual is different for different datasets and
architectures. The match/mismatch of the data and the architecture is primarily
responsible for good/bad generalization.

</details>


### [71] [Dynamic Time-aware Continual User Representation Learning](https://arxiv.org/abs/2504.16501)
*Seungyoon Choi,Sein Kim,Hongseok Kang,Wonjoong Kim,Chanyoung Park*

Main category: cs.LG

TL;DR: 论文提出了一种动态时间感知的持续用户表示学习框架DITTO，解决了传统方法在任务随时间变化时忽略新项目分布变化的问题。


<details>
  <summary>Details</summary>
Motivation: 传统用户建模方法局限于单一任务，缺乏泛化能力；现有持续学习方法未考虑时间推移对任务分布的影响。

Method: 提出DITTO框架，动态适应项目分布变化，缓解灾难性遗忘，并利用历史知识适应新分布。

Result: 实验表明DITTO在实用评估场景下优于现有方法。

Conclusion: DITTO为持续学习中的用户表示提供了一种更实用的解决方案。

Abstract: Traditional user modeling (UM) approaches have primarily focused on designing
models for a single specific task, but they face limitations in generalization
and adaptability across various tasks. Recognizing these challenges, recent
studies have shifted towards continual learning (CL)-based universal user
representation learning aiming to develop a single model capable of handling
multiple tasks. Despite advancements, existing methods are in fact evaluated
under an unrealistic scenario that does not consider the passage of time as
tasks progress, which overlooks newly emerged items that may change the item
distribution of previous tasks. In this paper, we introduce a practical
evaluation scenario on which CL-based universal user representation learning
approaches should be evaluated, which takes into account the passage of time as
tasks progress. Then, we propose a novel framework Dynamic Time-aware continual
user representation learner, named DITTO, designed to alleviate catastrophic
forgetting despite continuous shifts in item distribution, while also allowing
the knowledge acquired from previous tasks to adapt to the current shifted item
distribution. Through our extensive experiments, we demonstrate the superiority
of DITTO over state-of-the-art methods under a practical evaluation scenario.
Our source code is available at
https://github.com/seungyoon-Choi/DITTO_official.

</details>


### [72] [A Comprehensive Survey of Synthetic Tabular Data Generation](https://arxiv.org/abs/2504.16506)
*Ruxue Shi,Yili Wang,Mengnan Du,Xu Shen,Xin Wang*

Main category: cs.LG

TL;DR: 这篇论文综述了合成表格数据生成的方法，提出了一个综合分类法，并探讨了当前挑战和未来方向。


<details>
  <summary>Details</summary>
Motivation: 表格数据在机器学习中的应用受到数据稀缺、隐私问题和类别不平衡的限制，合成数据生成成为解决方案。现有研究分散且缺乏全面性，尤其是基于LLM和扩散模型的新方法未被充分探讨。

Method: 论文提出了一种分类法，将现有方法分为传统方法、基于扩散模型和基于LLM的模型，并详细分析了合成表格数据的完整流程。

Result: 论文提供了对合成表格数据生成方法的系统综述，包括数据合成、后处理和评估，并指出了当前的主要挑战。

Conclusion: 论文总结了合成表格数据生成的研究现状，提出了开放问题和未来方向，为该领域的进一步发展提供了指导。

Abstract: Tabular data remains one of the most prevalent and critical data formats
across diverse real-world applications. However, its effective use in machine
learning (ML) is often constrained by challenges such as data scarcity, privacy
concerns, and class imbalance. Synthetic data generation has emerged as a
promising solution, leveraging generative models to learn the distribution of
real datasets and produce high-fidelity, privacy-preserving samples. Various
generative paradigms have been explored, including energy-based models (EBMs),
variational autoencoders (VAEs), generative adversarial networks (GANs), large
language models (LLMs), and diffusion models. While several surveys have
investigated synthetic tabular data generation, most focus on narrow subdomains
or specific generative methods, such as GANs, diffusion models, or
privacy-preserving techniques. This limited scope often results in fragmented
insights, lacking a comprehensive synthesis that bridges diverse approaches. In
particular, recent advances driven by LLMs and diffusion-based models remain
underexplored. This gap hinders a holistic understanding of the field`s
evolution, methodological interplay, and open challenges. To address this, our
survey provides a unified and systematic review of synthetic tabular data
generation. Our contributions are threefold: (1) we propose a comprehensive
taxonomy that organizes existing methods into traditional approaches,
diffusion-based methods, and LLM-based models, and provide an in-depth
comparative analysis; (2) we detail the complete pipeline for synthetic tabular
data generation, including data synthesis, post-processing, and evaluation; (3)
we identify major challenges, explore real-world applications, and outline open
research questions and future directions to guide future work in this rapidly
evolving area.

</details>


### [73] [Least-Squares-Embedded Optimization for Accelerated Convergence of PINNs in Acoustic Wavefield Simulations](https://arxiv.org/abs/2504.16553)
*Mohammad Mahdi Abedi,David Pardo,Tariq Alkhalifah*

Main category: cs.LG

TL;DR: 论文提出了一种混合优化框架，通过将最小二乘（LS）求解器嵌入梯度下降（GD）损失函数中，加速了物理信息神经网络（PINNs）在求解Helmholtz方程时的收敛速度和稳定性。


<details>
  <summary>Details</summary>
Motivation: 标准PINN训练在求解高频波场时存在收敛慢和不稳定的问题，需要一种更高效的优化方法。

Method: 提出了一种结合GD和LS求解器的混合优化框架，优化线性输出层的更新，适用于有无完美匹配层（PML）的情况。

Result: 数值实验表明，该方法比传统PINN训练收敛更快、精度更高、稳定性更强，尤其在高频波场情况下表现优异。

Conclusion: LS增强的方法显著提升了PINN在Helmholtz方程求解中的性能，且计算开销小，适用于大规模波场模拟。

Abstract: Physics-Informed Neural Networks (PINNs) have shown promise in solving
partial differential equations (PDEs), including the frequency-domain Helmholtz
equation. However, standard training of PINNs using gradient descent (GD)
suffers from slow convergence and instability, particularly for high-frequency
wavefields. For scattered acoustic wavefield simulation based on Helmholtz
equation, we derive a hybrid optimization framework that accelerates training
convergence by embedding a least-squares (LS) solver directly into the GD loss
function. This formulation enables optimal updates for the linear output layer.
Our method is applicable with or without perfectly matched layers (PML), and we
provide practical tensor-based implementations for both scenarios. Numerical
experiments on benchmark velocity models demonstrate that our approach achieves
faster convergence, higher accuracy, and improved stability compared to
conventional PINN training. In particular, our results show that the
LS-enhanced method converges rapidly even in cases where standard GD-based
training fails. The LS solver operates on a small normal matrix, ensuring
minimal computational overhead and making the method scalable for large-scale
wavefield simulations.

</details>


### [74] [Unified Molecule Generation and Property Prediction](https://arxiv.org/abs/2504.16559)
*Adam Izdebski,Jan Olszewski,Pankhil Gawade,Krzysztof Koras,Serra Korkmaz,Valentin Rauscher,Jakub M. Tomczak,Ewa Szczurek*

Main category: cs.LG

TL;DR: Hyformer是一个基于Transformer的联合模型，通过交替注意力掩码和统一预训练方案，成功结合了生成和预测功能，性能优于其他联合模型及现有分子生成与属性预测模型。


<details>
  <summary>Details</summary>
Motivation: 联合建模数据样本及其属性的分布可以构建一个兼具生成和预测功能的模型，但训练联合模型存在架构和优化挑战。

Method: 提出Hyformer，使用交替注意力掩码和统一预训练方案，结合生成与预测功能。

Result: Hyformer在分子生成和属性预测任务中表现优异，并在分子表示学习、命中识别和抗菌肽设计等下游任务中展示了联合建模的优势。

Conclusion: Hyformer证明了联合建模的潜力，为生成与预测任务提供了高效解决方案。

Abstract: Modeling the joint distribution of the data samples and their properties
allows to construct a single model for both data generation and property
prediction, with synergistic capabilities reaching beyond purely generative or
predictive models. However, training joint models presents daunting
architectural and optimization challenges. Here, we propose Hyformer, a
transformer-based joint model that successfully blends the generative and
predictive functionalities, using an alternating attention mask together with a
unified pre-training scheme. We show that Hyformer rivals other joint models,
as well as state-of-the-art molecule generation and property prediction models.
Additionally, we show the benefits of joint modeling in downstream tasks of
molecular representation learning, hit identification and antimicrobial peptide
design.

</details>


### [75] [Hyper-Transforming Latent Diffusion Models](https://arxiv.org/abs/2504.16580)
*Ignacio Peis,Batuhan Koyuncu,Isabel Valera,Jes Frellsen*

Main category: cs.LG

TL;DR: 提出了一种结合隐式神经表示（INR）和基于Transformer的超网络的新型生成框架，解决了传统MLP超网络的可扩展性问题。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖MLP超网络，存在可扩展性限制，需要一种更高效且表达能力强的生成框架。

Method: 使用基于Transformer的解码器从潜在变量生成INR参数，扩展了潜在扩散模型（LDMs），支持从头训练或通过超变换微调解码器。

Result: 框架在表达能力和计算效率上表现优异，能够高效适配现有生成模型到INR表示。

Conclusion: 该框架为INR生成提供了高效且灵活的解决方案，无需完全重新训练即可适配现有模型。

Abstract: We introduce a novel generative framework for functions by integrating
Implicit Neural Representations (INRs) and Transformer-based hypernetworks into
latent variable models. Unlike prior approaches that rely on MLP-based
hypernetworks with scalability limitations, our method employs a
Transformer-based decoder to generate INR parameters from latent variables,
addressing both representation capacity and computational efficiency. Our
framework extends latent diffusion models (LDMs) to INR generation by replacing
standard decoders with a Transformer-based hypernetwork, which can be trained
either from scratch or via hyper-transforming-a strategy that fine-tunes only
the decoder while freezing the pre-trained latent space. This enables efficient
adaptation of existing generative models to INR-based representations without
requiring full retraining.

</details>


### [76] [Enhancing Variable Selection in Large-scale Logistic Regression: Leveraging Manual Labeling with Beneficial Noise](https://arxiv.org/abs/2504.16585)
*Xiaofei Wu,Rongmei Liang*

Main category: cs.LG

TL;DR: 本文研究了带标签噪声的惩罚逻辑回归（PLR）在大规模监督学习中的表现，发现标签噪声对变量选择有益，并提出了一种基于ADMM的分区不敏感并行算法。


<details>
  <summary>Details</summary>
Motivation: 探讨标签噪声对PLR变量选择的影响，并解决大规模数据下PLR的分布式计算问题。

Method: 提出基于ADMM的分区不敏感并行算法，结合手动标签噪声处理PLR。

Result: 实验表明，带标签噪声的PLR在估计和分类准确性上优于传统方法。

Conclusion: 标签噪声对PLR变量选择有益，所提算法具有全局收敛性和分区不敏感性。

Abstract: In large-scale supervised learning, penalized logistic regression (PLR)
effectively addresses the overfitting problem by introducing regularization
terms yet its performance still depends on efficient variable selection
strategies. This paper theoretically demonstrates that label noise stemming
from manual labeling, which is solely related to classification difficulty,
represents a type of beneficial noise for variable selection in PLR. This
benefit is reflected in a more accurate estimation of the selected non-zero
coefficients when compared with the case where only truth labels are used.
Under large-scale settings, the sample size for PLR can become very large,
making it infeasible to store on a single machine. In such cases, distributed
computing methods are required to handle PLR model with manual labeling. This
paper presents a partition-insensitive parallel algorithm founded on the ADMM
(alternating direction method of multipliers) algorithm to address PLR by
incorporating manual labeling. The partition insensitivity of the proposed
algorithm refers to the fact that the solutions obtained by the algorithm will
not change with the distributed storage of data. In addition, the algorithm has
global convergence and a sublinear convergence rate. Experimental results
indicate that, as compared with traditional variable selection classification
techniques, the PLR with manually-labeled noisy data achieves higher estimation
and classification accuracy across multiple large-scale datasets.

</details>


### [77] [Compositional Active Learning of Synchronous Systems through Automated Alphabet Refinement](https://arxiv.org/abs/2504.16624)
*Leo Henry,Thomas Neele,Mohammad Mousavi,Matteo Sammartino*

Main category: cs.LG

TL;DR: 本文提出了一种用于同步并行系统的组合学习方法，能够自动细化全局字母表为组件字母表，并开发了理论框架和算法实现。实验表明，该方法在查询效率和可扩展性上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的组合学习方法在处理未知分解的同步并行系统时存在局限性，需要一种更通用的技术来改进。

Method: 开发了一种组合学习算法，自动细化全局字母表为组件字母表，并通过理论框架处理字母表分布问题。

Result: 在630多个实验系统中，该方法在成员查询和等价查询上表现出显著的性能提升（最高达五个数量级）。

Conclusion: 该方法为同步并行系统的组合学习提供了高效且可扩展的解决方案。

Abstract: Active automata learning infers automaton models of systems from behavioral
observations, a technique successfully applied to a wide range of domains.
Compositional approaches for concurrent systems have recently emerged. We take
a significant step beyond available results, including those by the authors,
and develop a general technique for compositional learning of a synchronizing
parallel system with an unknown decomposition. Our approach automatically
refines the global alphabet into component alphabets while learning the
component models. We develop a theoretical treatment of distributions of
alphabets, i.e., sets of possibly overlapping component alphabets. We
characterize counter-examples that reveal inconsistencies with global
observations, and show how to systematically update the distribution to restore
consistency. We present a compositional learning algorithm implementing these
ideas, where learning counterexamples precisely correspond to distribution
counterexamples under well-defined conditions. We provide an implementation,
called CoalA, using the state-of-the-art active learning library LearnLib. Our
experiments show that in more than 630 subject systems, CoalA delivers orders
of magnitude improvements (up to five orders) in membership queries and in
systems with significant concurrency, it also achieves better scalability in
the number of equivalence queries.

</details>


### [78] [ParetoHqD: Fast Offline Multiobjective Alignment of Large Language Models using Pareto High-quality Data](https://arxiv.org/abs/2504.16628)
*Haoran Gu,Handing Wang,Yi Mei,Mengjie Zhang,Yaochu Jin*

Main category: cs.LG

TL;DR: 论文提出ParetoHqD方法，通过将人类偏好表示为目标空间中的偏好方向，并利用帕累托前沿附近的数据作为高质量数据，解决了多目标对齐算法中的偏好表示和奖励不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 确保大型语言模型能够满足多样化的用户需求，需要将其与多种人类期望和价值观对齐。现有的多目标对齐算法在偏好表示和奖励不平衡方面存在局限性。

Method: 提出ParetoHqD方法，将人类偏好表示为目标空间中的偏好方向，并利用帕累托前沿附近的数据作为高质量数据。采用两阶段监督微调过程，每个阶段使用与偏好方向最匹配的高质量训练集。

Result: 实验结果表明，ParetoHqD在两个多目标对齐任务上优于五种基线方法。

Conclusion: ParetoHqD通过改进偏好表示和训练数据选择，显著提升了多目标对齐算法的性能。

Abstract: Aligning large language models with multiple human expectations and values is
crucial for ensuring that they adequately serve a variety of user needs. To
this end, offline multiobjective alignment algorithms such as the
Rewards-in-Context algorithm have shown strong performance and efficiency.
However, inappropriate preference representations and training with imbalanced
reward scores limit the performance of such algorithms. In this work, we
introduce ParetoHqD that addresses the above issues by representing human
preferences as preference directions in the objective space and regarding data
near the Pareto front as ''high-quality'' data. For each preference, ParetoHqD
follows a two-stage supervised fine-tuning process, where each stage uses an
individual Pareto high-quality training set that best matches its preference
direction. The experimental results have demonstrated the superiority of
ParetoHqD over five baselines on two multiobjective alignment tasks.

</details>


### [79] [DAPLSR: Data Augmentation Partial Least Squares Regression Model via Manifold Optimization](https://arxiv.org/abs/2504.16639)
*Haoran Chen,Jiapeng Liu,Jiafan Wang,Wenjun Shi*

Main category: cs.LG

TL;DR: 提出了一种基于流形优化的数据增强偏最小二乘回归（DAPLSR）模型，通过SMOTE和VDM技术提升不平衡数据的分类性能。


<details>
  <summary>Details</summary>
Motivation: 传统PLSR模型在处理类别不平衡数据时表现不佳，需要改进。

Method: 结合SMOTE增加样本数量，利用VDM选择近邻生成合成样本，并通过流形优化提高数值解的准确性。

Result: 实验表明DAPLSR在多个数据集上分类性能和评价指标显著优于现有方法。

Conclusion: DAPLSR模型有效解决了类别不平衡问题，提升了PLSR的性能。

Abstract: Traditional Partial Least Squares Regression (PLSR) models frequently
underperform when handling data characterized by uneven categories. To address
the issue, this paper proposes a Data Augmentation Partial Least Squares
Regression (DAPLSR) model via manifold optimization. The DAPLSR model
introduces the Synthetic Minority Over-sampling Technique (SMOTE) to increase
the number of samples and utilizes the Value Difference Metric (VDM) to select
the nearest neighbor samples that closely resemble the original samples for
generating synthetic samples. In solving the model, in order to obtain a more
accurate numerical solution for PLSR, this paper proposes a manifold
optimization method that uses the geometric properties of the constraint space
to improve model degradation and optimization. Comprehensive experiments show
that the proposed DAPLSR model achieves superior classification performance and
outstanding evaluation metrics on various datasets, significantly outperforming
existing methods.

</details>


### [80] [Representation Learning via Non-Contrastive Mutual Information](https://arxiv.org/abs/2504.16667)
*Zhaohan Daniel Guo,Bernardo Avila Pires,Khimya Khetarpal,Dale Schuurmans,Bo Dai*

Main category: cs.LG

TL;DR: 本文提出了一种结合对比和非对比自监督学习优势的新目标函数MINC，通过改进谱对比损失，实现了低方差且避免崩溃的效果。


<details>
  <summary>Details</summary>
Motivation: 数据标注成本高，自监督学习能从未标注数据中学习有用表示，但现有方法（对比和非对比）各有优缺点。

Method: 将谱对比损失转化为非对比形式，提出MINC损失，结合对比方法的互信息优势和非对比方法的低方差特性。

Result: 在ImageNet上测试，MINC表现优于谱对比损失基线。

Conclusion: MINC成功结合了两种自监督方法的优势，为下游任务提供了更优的表示。

Abstract: Labeling data is often very time consuming and expensive, leaving us with a
majority of unlabeled data. Self-supervised representation learning methods
such as SimCLR (Chen et al., 2020) or BYOL (Grill et al., 2020) have been very
successful at learning meaningful latent representations from unlabeled image
data, resulting in much more general and transferable representations for
downstream tasks. Broadly, self-supervised methods fall into two types: 1)
Contrastive methods, such as SimCLR; and 2) Non-Contrastive methods, such as
BYOL. Contrastive methods are generally trying to maximize mutual information
between related data points, so they need to compare every data point to every
other data point, resulting in high variance, and thus requiring large batch
sizes to work well. Non-contrastive methods like BYOL have much lower variance
as they do not need to make pairwise comparisons, but are much trickier to
implement as they have the possibility of collapsing to a constant vector. In
this paper, we aim to develop a self-supervised objective that combines the
strength of both types. We start with a particular contrastive method called
the Spectral Contrastive Loss (HaoChen et al., 2021; Lu et al., 2024), and we
convert it into a more general non-contrastive form; this removes the pairwise
comparisons resulting in lower variance, but keeps the mutual information
formulation of the contrastive method preventing collapse. We call our new
objective the Mutual Information Non-Contrastive (MINC) loss. We test MINC by
learning image representations on ImageNet (similar to SimCLR and BYOL) and
show that it consistently improves upon the Spectral Contrastive loss baseline.

</details>


### [81] [Efficient Data Valuation Approximation in Federated Learning: A Sampling-based Approach](https://arxiv.org/abs/2504.16668)
*Shuyue Wei,Yongxin Tong,Zimu Zhou,Tianran He,Yi Xu*

Main category: cs.LG

TL;DR: 论文提出了一种高效的Shapley值近似算法IPSS，用于联邦学习中的数据估值，通过选择高影响力的数据集组合显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中，跨机构数据提供者不愿共享高质量数据，除非其数据价值能被公平评估。现有Shapley值计算方法计算开销大且效率低。

Method: 提出统一的分层抽样框架，分析并选择更优的计算方案，识别关键组合现象，并设计IPSS算法选择性评估高影响力组合。

Result: IPSS算法在联邦学习基准数据集上表现优异，显著提升了计算效率且误差较小。

Conclusion: IPSS算法为联邦学习中的数据估值提供了高效且准确的解决方案。

Abstract: Federated learning paradigm to utilize datasets across multiple data
providers. In FL, cross-silo data providers often hesitate to share their
high-quality dataset unless their data value can be fairly assessed. Shapley
value (SV) has been advocated as the standard metric for data valuation in FL
due to its desirable properties. However, the computational overhead of SV is
prohibitive in practice, as it inherently requires training and evaluating an
FL model across an exponential number of dataset combinations. Furthermore,
existing solutions fail to achieve high accuracy and efficiency, making
practical use of SV still out of reach, because they ignore choosing suitable
computation scheme for approximation framework and overlook the property of
utility function in FL. We first propose a unified stratified-sampling
framework for two widely-used schemes. Then, we analyze and choose the more
promising scheme under the FL linear regression assumption. After that, we
identify a phenomenon termed key combinations, where only limited dataset
combinations have a high-impact on final data value. Building on these
insights, we propose a practical approximation algorithm, IPSS, which
strategically selects high-impact dataset combinations rather than evaluating
all possible combinations, thus substantially reducing time cost with minor
approximation error. Furthermore, we conduct extensive evaluations on the FL
benchmark datasets to demonstrate that our proposed algorithm outperforms a
series of representative baselines in terms of efficiency and effectiveness.

</details>


### [82] [Provable wavelet-based neural approximation](https://arxiv.org/abs/2504.16682)
*Youngmi Hur,Hyojae Lim,Mikyoung Lim*

Main category: cs.LG

TL;DR: 提出了一种基于小波的框架，分析神经网络在多种激活函数下的通用逼近能力，并给出了误差估计。


<details>
  <summary>Details</summary>
Motivation: 研究神经网络在不同激活函数下的逼近能力，为网络设计提供理论支持。

Method: 利用小波框架理论，推导激活函数的充分条件，确保神经网络能逼近给定空间中的任意函数。

Result: 适用于多种平滑激活函数，包括振荡函数，并扩展到非平滑激活函数，误差由距离控制。

Conclusion: 为网络架构设计提供了更大的灵活性。

Abstract: In this paper, we develop a wavelet-based theoretical framework for analyzing
the universal approximation capabilities of neural networks over a wide range
of activation functions. Leveraging wavelet frame theory on the spaces of
homogeneous type, we derive sufficient conditions on activation functions to
ensure that the associated neural network approximates any functions in the
given space, along with an error estimate. These sufficient conditions
accommodate a variety of smooth activation functions, including those that
exhibit oscillatory behavior. Furthermore, by considering the $L^2$-distance
between smooth and non-smooth activation functions, we establish a generalized
approximation result that is applicable to non-smooth activations, with the
error explicitly controlled by this distance. This provides increased
flexibility in the design of network architectures.

</details>


### [83] [MCMC for Bayesian estimation of Differential Privacy from Membership Inference Attacks](https://arxiv.org/abs/2504.16683)
*Ceren Yildirim,Kamer Kaya,Sinan Yildirim,Erkay Savas*

Main category: cs.LG

TL;DR: 提出了一种基于贝叶斯估计的差分隐私框架，结合多成员推理攻击（MIA）证据，通过MCMC-DP-Est算法估计隐私参数的全后验分布。


<details>
  <summary>Details</summary>
Motivation: 传统方法假设隐私审计基于最强攻击和最坏情况，通常不现实；新方法更谨慎地联合估计MIA强度和隐私性。

Method: 使用MCMC-DP-Est算法进行贝叶斯估计，并开发经济高效的MIA性能测量方法。

Result: 通过人工和真实数据的数值示例验证了方法的有效性。

Conclusion: 新框架提供了更现实的隐私分析，适用于实际场景。

Abstract: We propose a new framework for Bayesian estimation of differential privacy,
incorporating evidence from multiple membership inference attacks (MIA).
Bayesian estimation is carried out via a Markov chain Monte Carlo (MCMC)
algorithm, named MCMC-DP-Est, which provides an estimate of the full posterior
distribution of the privacy parameter (e.g., instead of just credible
intervals). Critically, the proposed method does not assume that privacy
auditing is performed with the most powerful attack on the worst-case (dataset,
challenge point) pair, which is typically unrealistic. Instead, MCMC-DP-Est
jointly estimates the strengths of MIAs used and the privacy of the training
algorithm, yielding a more cautious privacy analysis. We also present an
economical way to generate measurements for the performance of an MIA that is
to be used by the MCMC method to estimate privacy. We present the use of the
methods with numerical examples with both artificial and real data.

</details>


### [84] [PIN-WM: Learning Physics-INformed World Models for Non-Prehensile Manipulation](https://arxiv.org/abs/2504.16693)
*Wenxuan Li,Hang Zhao,Zhiyuan Yu,Yu Du,Qin Zou,Ruizhen Hu,Kai Xu*

Main category: cs.LG

TL;DR: 论文提出了一种基于物理信息的世界模型（PIN-WM），用于学习非抓取操作（如推动/戳动）的3D刚体动力学，并通过模型强化学习实现鲁棒的策略学习和泛化。


<details>
  <summary>Details</summary>
Motivation: 非抓取操作对复杂物理交互（如摩擦和恢复）高度敏感，学习鲁棒策略具有挑战性。

Method: 采用可微分物理模拟，从视觉观察中端到端学习3D刚体动力学模型（PIN-WM），无需状态估计，并通过物理感知随机化生成数字变体（Digital Cousins）以弥合Sim2Real差距。

Result: 在仿真和真实世界测试中，PIN-WM结合数字变体显著提升了非抓取操作技能的鲁棒性和Sim2Real迁移能力，优于现有方法。

Conclusion: PIN-WM通过物理信息建模和数字变体技术，有效解决了非抓取操作学习中的鲁棒性和泛化问题，实现了Sim2Real迁移的突破。

Abstract: While non-prehensile manipulation (e.g., controlled pushing/poking)
constitutes a foundational robotic skill, its learning remains challenging due
to the high sensitivity to complex physical interactions involving friction and
restitution. To achieve robust policy learning and generalization, we opt to
learn a world model of the 3D rigid body dynamics involved in non-prehensile
manipulations and use it for model-based reinforcement learning. We propose
PIN-WM, a Physics-INformed World Model that enables efficient end-to-end
identification of a 3D rigid body dynamical system from visual observations.
Adopting differentiable physics simulation, PIN-WM can be learned with only
few-shot and task-agnostic physical interaction trajectories. Further, PIN-WM
is learned with observational loss induced by Gaussian Splatting without
needing state estimation. To bridge Sim2Real gaps, we turn the learned PIN-WM
into a group of Digital Cousins via physics-aware randomizations which perturb
physics and rendering parameters to generate diverse and meaningful variations
of the PIN-WM. Extensive evaluations on both simulation and real-world tests
demonstrate that PIN-WM, enhanced with physics-aware digital cousins,
facilitates learning robust non-prehensile manipulation skills with Sim2Real
transfer, surpassing the Real2Sim2Real state-of-the-arts.

</details>


### [85] [A Unified Retrieval Framework with Document Ranking and EDU Filtering for Multi-document Summarization](https://arxiv.org/abs/2504.16711)
*Shiyin Tan,Jaeeon Park,Dongyuan Li,Renhe Jiang,Manabu Okumura*

Main category: cs.LG

TL;DR: 提出了一种新的检索框架，通过整合查询选择和文档排序缩短过程，解决了多文档摘要中Transformer模型的输入长度限制问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖人工查询且信息检索粒度粗，导致无关内容被包含。

Method: 利用潜在查询（EDUs）指导文档排序，过滤无关内容以适应上下文长度。

Result: 在多个数据集上ROUGE指标提升，验证了框架的可扩展性和灵活性。

Conclusion: 该框架有效解决了上下文长度限制，为多文档摘要提供了可靠解决方案。

Abstract: In the field of multi-document summarization (MDS), transformer-based models
have demonstrated remarkable success, yet they suffer an input length
limitation. Current methods apply truncation after the retrieval process to fit
the context length; however, they heavily depend on manually well-crafted
queries, which are impractical to create for each document set for MDS.
Additionally, these methods retrieve information at a coarse granularity,
leading to the inclusion of irrelevant content. To address these issues, we
propose a novel retrieval-based framework that integrates query selection and
document ranking and shortening into a unified process. Our approach identifies
the most salient elementary discourse units (EDUs) from input documents and
utilizes them as latent queries. These queries guide the document ranking by
calculating relevance scores. Instead of traditional truncation, our approach
filters out irrelevant EDUs to fit the context length, ensuring that only
critical information is preserved for summarization. We evaluate our framework
on multiple MDS datasets, demonstrating consistent improvements in ROUGE
metrics while confirming its scalability and flexibility across diverse model
architectures. Additionally, we validate its effectiveness through an in-depth
analysis, emphasizing its ability to dynamically select appropriate queries and
accurately rank documents based on their relevance scores. These results
demonstrate that our framework effectively addresses context-length
constraints, establishing it as a robust and reliable solution for MDS.

</details>


### [86] [Simple Graph Contrastive Learning via Fractional-order Neural Diffusion Networks](https://arxiv.org/abs/2504.16748)
*Yanan Zhao,Feng Ji,Kai Zhao,Xuhao Li,Qiyu Kang,Wenfei Liang,Yahya Alkhatib,Xingchao Jian,Wee Peng Tay*

Main category: cs.LG

TL;DR: 提出了一种基于图神经扩散模型的无增强图对比学习框架，无需负样本训练，适用于同质和异质数据集，性能优越。


<details>
  <summary>Details</summary>
Motivation: 解决现有图对比学习方法依赖复杂数据增强或负样本的问题。

Method: 利用分数微分方程（FDE）的可学习编码器生成多样视图，捕捉局部或全局信息。

Result: 在多种数据集上实现最优性能。

Conclusion: 该框架为无增强图对比学习提供了高效且通用的解决方案。

Abstract: Graph Contrastive Learning (GCL) has recently made progress as an
unsupervised graph representation learning paradigm. GCL approaches can be
categorized into augmentation-based and augmentation-free methods. The former
relies on complex data augmentations, while the latter depends on encoders that
can generate distinct views of the same input. Both approaches may require
negative samples for training. In this paper, we introduce a novel
augmentation-free GCL framework based on graph neural diffusion models.
Specifically, we utilize learnable encoders governed by Fractional Differential
Equations (FDE). Each FDE is characterized by an order parameter of the
differential operator. We demonstrate that varying these parameters allows us
to produce learnable encoders that generate diverse views, capturing either
local or global information, for contrastive learning. Our model does not
require negative samples for training and is applicable to both homophilic and
heterophilic datasets. We demonstrate its effectiveness across various
datasets, achieving state-of-the-art performance.

</details>


### [87] [QAOA-PCA: Enhancing Efficiency in the Quantum Approximate Optimization Algorithm via Principal Component Analysis](https://arxiv.org/abs/2504.16755)
*Owain Parry,Phil McMinn*

Main category: cs.LG

TL;DR: QAOA-PCA是一种基于主成分分析（PCA）的新参数化方法，用于减少QAOA算法的参数空间维度，提高优化效率。


<details>
  <summary>Details</summary>
Motivation: 随着QAOA电路层数增加，参数优化计算负担加重，需要减少参数空间维度以提高效率。

Method: 利用PCA从较小问题实例的优化参数中提取主成分，减少大实例的参数数量。

Result: 在MaxCut问题上，QAOA-PCA比标准QAOA需要更少迭代次数，效率显著提升，但近似比略有降低。

Conclusion: QAOA-PCA在效率和性能之间取得良好平衡，显著减少优化开销且不显著影响解的质量。

Abstract: The Quantum Approximate Optimization Algorithm (QAOA) is a promising
variational algorithm for solving combinatorial optimization problems on
near-term devices. However, as the number of layers in a QAOA circuit
increases, which is correlated with the quality of the solution, the number of
parameters to optimize grows linearly. This results in more iterations required
by the classical optimizer, which results in an increasing computational burden
as more circuit executions are needed. To mitigate this issue, we introduce
QAOA-PCA, a novel reparameterization technique that employs Principal Component
Analysis (PCA) to reduce the dimensionality of the QAOA parameter space. By
extracting principal components from optimized parameters of smaller problem
instances, QAOA-PCA facilitates efficient optimization with fewer parameters on
larger instances. Our empirical evaluation on the prominent MaxCut problem
demonstrates that QAOA-PCA consistently requires fewer iterations than standard
QAOA, achieving substantial efficiency gains. While this comes at the cost of a
slight reduction in approximation ratio compared to QAOA with the same number
of layers, QAOA-PCA almost always outperforms standard QAOA when matched by
parameter count. QAOA-PCA strikes a favorable balance between efficiency and
performance, reducing optimization overhead without significantly compromising
solution quality.

</details>


### [88] [Noise-Tolerant Coreset-Based Class Incremental Continual Learning](https://arxiv.org/abs/2504.16763)
*Edison Mucllari,Aswin Raghavan,Zachary Alan Daniels*

Main category: cs.LG

TL;DR: 论文研究了在类增量学习中标签噪声和实例噪声对持续学习的影响，提出了两种抗噪声的持续学习算法，并通过实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 许多计算机视觉应用需要在部署后适应新的数据分布，而持续学习（CL）是实现这一目标的关键。然而，噪声（如标签噪声和实例噪声）可能干扰CL过程，尤其是在类增量学习（CIL）场景中。

Method: 论文通过理论分析推导了基于Coresets的持续学习方法对不相关实例噪声的鲁棒性边界，并提出了两种新的噪声容忍重放缓冲区构建算法。

Result: 实验在五个数据集上对比了现有方法和新方法，结果显示现有方法对噪声不鲁棒，而新方法在分类准确性和减少遗忘方面表现显著更好。

Conclusion: 论文提出的噪声容忍持续学习算法在噪声环境下表现出更强的鲁棒性，为类增量学习中的噪声问题提供了有效解决方案。

Abstract: Many applications of computer vision require the ability to adapt to novel
data distributions after deployment. Adaptation requires algorithms capable of
continual learning (CL). Continual learners must be plastic to adapt to novel
tasks while minimizing forgetting of previous tasks.However, CL opens up
avenues for noise to enter the training pipeline and disrupt the CL. This work
focuses on label noise and instance noise in the context of class-incremental
learning (CIL), where new classes are added to a classifier over time, and
there is no access to external data from past classes. We aim to understand the
sensitivity of CL methods that work by replaying items from a memory
constructed using the idea of Coresets. We derive a new bound for the
robustness of such a method to uncorrelated instance noise under a general
additive noise threat model, revealing several insights. Putting the theory
into practice, we create two continual learning algorithms to construct
noise-tolerant replay buffers. We empirically compare the effectiveness of
prior memory-based continual learners and the proposed algorithms under label
and uncorrelated instance noise on five diverse datasets. We show that existing
memory-based CL are not robust whereas the proposed methods exhibit significant
improvements in maximizing classification accuracy and minimizing forgetting in
the noisy CIL setting.

</details>


### [89] [Online model learning with data-assimilated reservoir computers](https://arxiv.org/abs/2504.16767)
*Andrea Nóvoa,Luca Magri*

Main category: cs.LG

TL;DR: 提出了一种在线学习框架，用于预测非线性时空信号，结合降维、广义自回归模型和在线适应方法。


<details>
  <summary>Details</summary>
Motivation: 解决非线性时空信号预测的挑战，通过在线学习提升模型适应性和准确性。

Method: 集成POD降维、储层计算机预测和序列数据同化，测试了三种状态估计策略。

Result: 两重和三重状态估计策略显著提升收敛性和减少误差，支持部分训练模型的在线学习。

Conclusion: 结合数据驱动降维和贝叶斯同化，为非线性时间序列预测提供了可扩展的在线学习新方法。

Abstract: We propose an online learning framework for forecasting nonlinear
spatio-temporal signals (fields). The method integrates (i) dimensionality
reduction, here, a simple proper orthogonal decomposition (POD) projection;
(ii) a generalized autoregressive model to forecast reduced dynamics, here, a
reservoir computer; (iii) online adaptation to update the reservoir computer
(the model), here, ensemble sequential data assimilation.We demonstrate the
framework on a wake past a cylinder governed by the Navier-Stokes equations,
exploring the assimilation of full flow fields (projected onto POD modes) and
sparse sensors. Three scenarios are examined: a na\"ive physical state
estimation; a two-fold estimation of physical and reservoir states; and a
three-fold estimation that also adjusts the model parameters. The two-fold
strategy significantly improves ensemble convergence and reduces reconstruction
error compared to the na\"ive approach. The three-fold approach enables robust
online training of partially-trained reservoir computers, overcoming
limitations of a priori training. By unifying data-driven reduced order
modelling with Bayesian data assimilation, this work opens new opportunities
for scalable online model learning for nonlinear time series forecasting.

</details>


### [90] [Process Reward Models That Think](https://arxiv.org/abs/2504.16828)
*Muhammad Khalifa,Rishabh Agarwal,Lajanugen Logeswaran,Jaekyeom Kim,Hao Peng,Moontae Lee,Honglak Lee,Lu Wang*

Main category: cs.LG

TL;DR: ThinkPRM是一种基于生成式长链思维验证的步进验证模型，仅需1%的标注数据即可超越传统判别式验证模型和LLM-as-a-Judge，在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统步进验证模型（PRMs）需要大量步级标注数据，训练成本高。本研究旨在构建数据高效的PRMs，通过生成验证链式思维（CoT）减少标注需求。

Method: 提出ThinkPRM，一种基于长链思维验证的生成式模型，利用少量标注数据进行微调，并通过验证CoT实现高效验证。

Result: ThinkPRM在ProcessBench、MATH-500和AIME '24等基准测试中表现优于基线模型，并在GPQA-Diamond和LiveCodeBench的域外评估中分别提升8%和4.5%。

Conclusion: 生成式长链思维验证模型（ThinkPRM）在减少标注需求的同时，显著提升了验证性能，为测试时计算扩展提供了高效解决方案。

Abstract: Step-by-step verifiers -- also known as process reward models (PRMs) -- are a
key ingredient for test-time scaling. PRMs require step-level supervision,
making them expensive to train. This work aims to build data-efficient PRMs as
verbalized step-wise reward models that verify every step in the solution by
generating a verification chain-of-thought (CoT). We propose ThinkPRM, a long
CoT verifier fine-tuned on orders of magnitude fewer process labels than those
required by discriminative PRMs. Our approach capitalizes on the inherent
reasoning abilities of long CoT models, and outperforms LLM-as-a-Judge and
discriminative verifiers -- using only 1% of the process labels in PRM800K --
across several challenging benchmarks. Specifically, ThinkPRM beats the
baselines on ProcessBench, MATH-500, and AIME '24 under best-of-N selection and
reward-guided search. In an out-of-domain evaluation on a subset of
GPQA-Diamond and LiveCodeBench, our PRM surpasses discriminative verifiers
trained on the full PRM800K by 8% and 4.5%, respectively. Lastly, under the
same token budget, ThinkPRM scales up verification compute more effectively
compared to LLM-as-a-Judge, outperforming it by 7.2% on a subset of
ProcessBench. Our work highlights the value of generative, long CoT PRMs that
can scale test-time compute for verification while requiring minimal
supervision for training. Our code, data, and models will be released at
https://github.com/mukhal/thinkprm.

</details>


### [91] [Evaluating Autoencoders for Parametric and Invertible Multidimensional Projections](https://arxiv.org/abs/2504.16831)
*Frederik L. Dennig,Nina Geyer,Daniela Blumberg,Yannick Metz,Daniel A. Keim*

Main category: cs.LG

TL;DR: 论文研究了如何同时实现参数化和可逆的多维数据投影，通过评估三种自编码器架构，发现定制损失函数的自编码器能生成更平滑的投影。


<details>
  <summary>Details</summary>
Motivation: 探索参数化和可逆投影的联合实现，以支持新数据的嵌入和生成。

Method: 训练自编码器学习2D空间映射及逆映射，定量和定性比较四种数据集。

Result: 定制损失函数的自编码器能生成更平滑的参数化和逆投影，且用户可控制平滑强度。

Conclusion: 自编码器在参数化和可逆投影中表现优于前馈神经网络，具有实际应用潜力。

Abstract: Recently, neural networks have gained attention for creating parametric and
invertible multidimensional data projections. Parametric projections allow for
embedding previously unseen data without recomputing the projection as a whole,
while invertible projections enable the generation of new data points. However,
these properties have never been explored simultaneously for arbitrary
projection methods. We evaluate three autoencoder (AE) architectures for
creating parametric and invertible projections. Based on a given projection, we
train AEs to learn a mapping into 2D space and an inverse mapping into the
original space. We perform a quantitative and qualitative comparison on four
datasets of varying dimensionality and pattern complexity using t-SNE. Our
results indicate that AEs with a customized loss function can create smoother
parametric and inverse projections than feed-forward neural networks while
giving users control over the strength of the smoothing effect.

</details>


### [92] [Improving Significant Wave Height Prediction Using Chronos Models](https://arxiv.org/abs/2504.16834)
*Yilin Zhai,Hongyuan Shi,Chao Zhan,Qing Wang,Zaijin You,Nan Wang*

Main category: cs.LG

TL;DR: 论文提出了一种基于大语言模型（LLM）的时间架构Chronos，用于波浪高度预测，显著提升了计算效率和预测精度。


<details>
  <summary>Details</summary>
Motivation: 传统物理模型和机器学习方法在计算效率和非线性动态建模方面存在不足，需要更高效的解决方案。

Method: 采用LLM增强的时间架构Chronos，通过历史波浪数据的时间模式识别进行优化。

Result: 在训练时间、推理速度、短期和长期预测以及零样本能力方面均表现出色。

Conclusion: Chronos为波浪预测设立了新标准，并为复杂地球物理系统建模提供了可转移的框架。

Abstract: Accurate wave height prediction is critical for maritime safety and coastal
resilience, yet conventional physics-based models and traditional machine
learning methods face challenges in computational efficiency and nonlinear
dynamics modeling. This study introduces Chronos, the first implementation of a
large language model (LLM)-powered temporal architecture (Chronos) optimized
for wave forecasting. Through advanced temporal pattern recognition applied to
historical wave data from three strategically chosen marine zones in the
Northwest Pacific basin, our framework achieves multimodal improvements: (1)
14.3% reduction in training time with 2.5x faster inference speed compared to
PatchTST baselines, achieving 0.575 mean absolute scaled error (MASE) units;
(2) superior short-term forecasting (1-24h) across comprehensive metrics; (3)
sustained predictive leadership in extended-range forecasts (1-120h); and (4)
demonstrated zero-shot capability maintaining median performance (rank 4/12)
against specialized operational models. This LLM-enhanced temporal modeling
paradigm establishes a new standard in wave prediction, offering both
computationally efficient solutions and a transferable framework for complex
geophysical systems modeling.

</details>


### [93] [An Adaptive ML Framework for Power Converter Monitoring via Federated Transfer Learning](https://arxiv.org/abs/2504.16866)
*Panagiotis Kakosimos,Alireza Nemat Saberi,Luca Peretti*

Main category: cs.LG

TL;DR: 研究提出了一种结合迁移学习（TL）和联邦学习（FL）的分段框架，用于适应电力转换器的热机器学习模型，解决了数据共享限制和安全问题。


<details>
  <summary>Details</summary>
Motivation: 解决电力转换器在不同运行条件下的适应性、数据共享限制和安全问题。

Method: 结合迁移学习和联邦学习，采用微调、TCA和DDA三种域适应技术，使用Flower框架进行联邦学习。

Result: 微调方法简单且准确，适合实际应用；本地FL在数据聚合不可行时表现更好，云FL在客户数量增加时更具扩展性。

Conclusion: 分段框架有效解决了电力转换器的适应性问题，不同方法适用于不同场景。

Abstract: This study explores alternative framework configurations for adapting thermal
machine learning (ML) models for power converters by combining transfer
learning (TL) and federated learning (FL) in a piecewise manner. This approach
inherently addresses challenges such as varying operating conditions, data
sharing limitations, and security implications. The framework starts with a
base model that is incrementally adapted by multiple clients via adapting three
state-of-the-art domain adaptation techniques: Fine-tuning, Transfer Component
Analysis (TCA), and Deep Domain Adaptation (DDA). The Flower framework is
employed for FL, using Federated Averaging for aggregation. Validation with
field data demonstrates that fine-tuning offers a straightforward TL approach
with high accuracy, making it suitable for practical applications. Benchmarking
results reveal a comprehensive comparison of these methods, showcasing their
respective strengths and weaknesses when applied in different scenarios.
Locally hosted FL enhances performance when data aggregation is not feasible,
while cloud-based FL becomes more practical with a significant increase in the
number of clients, addressing scalability and connectivity challenges.

</details>


### [94] [Exploring How LLMs Capture and Represent Domain-Specific Knowledge](https://arxiv.org/abs/2504.16871)
*Mirian Hipolito Garcia,Camille Couturier,Daniel Madrigal Diaz,Ankur Mallick,Anastasios Kyrillidis,Robert Sim,Victor Ruhle,Saravan Rajmohan*

Main category: cs.LG

TL;DR: 研究大型语言模型（LLMs）是否能捕捉领域特定语言细微差别，通过隐藏状态分析其领域敏感性，并利用领域表示进行模型选择。


<details>
  <summary>Details</summary>
Motivation: 探讨LLMs是否能够识别不同领域的查询，并研究其领域表示的鲁棒性。

Method: 通过预填充阶段的隐藏状态分析LLMs的领域敏感性，并利用领域轨迹进行模型选择。

Result: LLMs能区分相关领域查询，且微调模型并非总是最准确。

Conclusion: LLMs具备领域敏感性，领域表示可用于模型选择，适用于封闭和开放生成任务。

Abstract: We study whether Large Language Models (LLMs) inherently capture
domain-specific nuances in natural language. Our experiments probe the domain
sensitivity of LLMs by examining their ability to distinguish queries from
different domains using hidden states generated during the prefill phase. We
reveal latent domain-related trajectories that indicate the model's internal
recognition of query domains. We also study the robustness of these domain
representations to variations in prompt styles and sources. Our approach
leverages these representations for model selection, mapping the LLM that best
matches the domain trace of the input query (i.e., the model with the highest
performance on similar traces). Our findings show that LLMs can differentiate
queries for related domains, and that the fine-tuned model is not always the
most accurate. Unlike previous work, our interpretations apply to both closed
and open-ended generative tasks

</details>


### [95] [Hybrid Reinforcement Learning and Model Predictive Control for Adaptive Control of Hydrogen-Diesel Dual-Fuel Combustion](https://arxiv.org/abs/2504.16875)
*Julian Bedei,Murray McBain,Charles Robert Koch,Jakob Andert,David Gordon*

Main category: cs.LG

TL;DR: 论文提出了一种结合强化学习（RL）和机器学习模型预测控制（ML-MPC）的混合方法，用于优化氢-柴油双燃料发动机控制，解决了单独使用RL或ML-MPC的局限性。


<details>
  <summary>Details</summary>
Motivation: RL和ML-MPC在控制多输入多输出系统和非线性过程方面各有优势，但单独使用RL可能导致不安全行为，而ML-MPC对系统漂移的适应性有限。

Method: 采用混合方法，ML-MPC框架确保安全控制，RL动态调整ML-MPC的负载跟踪参考以适应环境变化。

Result: 实验表明，混合方法在模型-植物不匹配情况下显著降低了负载跟踪的RMSE（从0.57 bar降至0.44 bar）。

Conclusion: 混合RL和ML-MPC方法在确保安全的同时提高了适应性，为双燃料发动机控制提供了有效解决方案。

Abstract: Reinforcement Learning (RL) and Machine Learning Integrated Model Predictive
Control (ML-MPC) are promising approaches for optimizing hydrogen-diesel
dual-fuel engine control, as they can effectively control multiple-input
multiple-output systems and nonlinear processes. ML-MPC is advantageous for
providing safe and optimal controls, ensuring the engine operates within
predefined safety limits. In contrast, RL is distinguished by its adaptability
to changing conditions through its learning-based approach. However, the
practical implementation of either method alone poses challenges. RL requires
high variance in control inputs during early learning phases, which can pose
risks to the system by potentially executing unsafe actions, leading to
mechanical damage. Conversely, ML-MPC relies on an accurate system model to
generate optimal control inputs and has limited adaptability to system drifts,
such as injector aging, which naturally occur in engine applications. To
address these limitations, this study proposes a hybrid RL and ML-MPC approach
that uses an ML-MPC framework while incorporating an RL agent to dynamically
adjust the ML-MPC load tracking reference in response to changes in the
environment. At the same time, the ML-MPC ensures that actions stay safe
throughout the RL agent's exploration. To evaluate the effectiveness of this
approach, fuel pressure is deliberately varied to introduce a model-plant
mismatch between the ML-MPC and the engine test bench. The result of this
mismatch is a root mean square error (RMSE) in indicated mean effective
pressure of 0.57 bar when running the ML-MPC. The experimental results
demonstrate that RL successfully adapts to changing boundary conditions by
altering the tracking reference while ML-MPC ensures safe control inputs. The
quantitative improvement in load tracking by implementing RL is an RSME of 0.44
bar.

</details>


### [96] [I-Con: A Unifying Framework for Representation Learning](https://arxiv.org/abs/2504.16929)
*Shaden Alshammari,John Hershey,Axel Feldmann,William T. Freeman,Mark Hamilton*

Main category: cs.LG

TL;DR: 论文提出了一种信息论框架，统一了多种现代机器学习损失函数，揭示了其背后的信息几何结构，并在无监督图像分类中实现了8%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 随着表示学习领域的发展，出现了大量针对不同问题的损失函数。本文旨在通过一个统一的信息论框架，概括这些多样化的损失函数，揭示其共性。

Method: 引入一个信息论方程，将多种机器学习方法统一为最小化两个条件分布之间的KL散度。通过这一框架，连接了聚类、谱方法、降维、对比学习和监督学习等方法。

Result: 理论框架连接了23种不同方法，并在无监督图像分类任务中实现了8%的性能提升。此外，还展示了如何用于改进对比表示学习。

Conclusion: 该框架不仅统一了多种损失函数，还推动了新损失函数的开发，并在实际任务中验证了其有效性。

Abstract: As the field of representation learning grows, there has been a proliferation
of different loss functions to solve different classes of problems. We
introduce a single information-theoretic equation that generalizes a large
collection of modern loss functions in machine learning. In particular, we
introduce a framework that shows that several broad classes of machine learning
methods are precisely minimizing an integrated KL divergence between two
conditional distributions: the supervisory and learned representations. This
viewpoint exposes a hidden information geometry underlying clustering, spectral
methods, dimensionality reduction, contrastive learning, and supervised
learning. This framework enables the development of new loss functions by
combining successful techniques from across the literature. We not only present
a wide array of proofs, connecting over 23 different approaches, but we also
leverage these theoretical results to create state-of-the-art unsupervised
image classifiers that achieve a +8% improvement over the prior
state-of-the-art on unsupervised classification on ImageNet-1K. We also
demonstrate that I-Con can be used to derive principled debiasing methods which
improve contrastive representation learners.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [97] [Neuro-Evolutionary Approach to Physics-Aware Symbolic Regression](https://arxiv.org/abs/2504.16503)
*Jiří Kubalík,Robert Babuška*

Main category: cs.NE

TL;DR: 提出了一种结合神经进化与梯度优化的符号回归方法，通过记忆策略和种群扰动提升模型质量，优于其他基于神经网络的方法。


<details>
  <summary>Details</summary>
Motivation: 传统符号回归依赖遗传编程，而神经网络方法易陷入次优结构。结合两者优势，提升模型性能。

Method: 结合进化搜索最优神经网络拓扑与梯度优化参数，采用记忆策略和种群扰动减少计算需求。

Result: 在三个实际测试问题中表现优于其他基于神经网络的方法。

Conclusion: 该方法有效结合进化与梯度优化，提升了符号回归的模型质量。

Abstract: Symbolic regression is a technique that can automatically derive analytic
models from data. Traditionally, symbolic regression has been implemented
primarily through genetic programming that evolves populations of candidate
solutions sampled by genetic operators, crossover and mutation. More recently,
neural networks have been employed to learn the entire analytical model, i.e.,
its structure and coefficients, using regularized gradient-based optimization.
Although this approach tunes the model's coefficients better, it is prone to
premature convergence to suboptimal model structures. Here, we propose a
neuro-evolutionary symbolic regression method that combines the strengths of
evolutionary-based search for optimal neural network (NN) topologies with
gradient-based tuning of the network's parameters. Due to the inherent high
computational demand of evolutionary algorithms, it is not feasible to learn
the parameters of every candidate NN topology to full convergence. Thus, our
method employs a memory-based strategy and population perturbations to enhance
exploitation and reduce the risk of being trapped in suboptimal NNs. In this
way, each NN topology can be trained using only a short sequence of
backpropagation iterations. The proposed method was experimentally evaluated on
three real-world test problems and has been shown to outperform other NN-based
approaches regarding the quality of the models obtained.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [98] [Circinus: Efficient Query Planner for Compound ML Serving](https://arxiv.org/abs/2504.16397)
*Banruo Liu,Wei-Yu Lin,Minghao Fang,Yihan Jiang,Fan Lai*

Main category: cs.DB

TL;DR: Circinus是一个面向大规模复合AI工作负载的SLO感知查询规划器，通过分解多查询规划和多维度SLO目标，显著提升服务吞吐量和查询效率，同时降低成本。


<details>
  <summary>Details</summary>
Motivation: 复合AI服务（如自动驾驶、生成式AI会议助手）需要满足高吞吐量（SLOs），但现有解决方案在实时服务和成本效率方面受限。

Method: Circinus通过分解多查询规划和多维度SLO目标，利用查询间和查询内的计划相似性减少搜索步骤，并通过精度感知的计划分析器提升效率。

Result: 实验表明，Circinus将服务吞吐量提升3.2-5.0倍，查询规划速度提升4.2-5.8倍，响应时间缩短至秒级，部署成本降低3.2-4.0倍。

Conclusion: Circinus在复杂AI服务中显著提升了性能和成本效率，优于现有技术。

Abstract: The rise of compound AI serving -- integrating multiple operators in a
pipeline that may span edge and cloud tiers -- enables end-user applications
such as autonomous driving, generative AI-powered meeting companions, and
immersive gaming. Achieving high service goodput -- i.e., meeting service level
objectives (SLOs) for pipeline latency, accuracy, and costs -- requires
effective planning of operator placement, configuration, and resource
allocation across infrastructure tiers. However, the diverse SLO requirements,
varying edge capabilities, and high query volumes create an enormous planning
search space, rendering current solutions fundamentally limited for real-time
serving and cost-efficient deployments.
  This paper presents Circinus, an SLO-aware query planner for large-scale
compound AI workloads. Circinus novelly decomposes multi-query planning and
multi-dimensional SLO objectives while preserving global decision quality. By
exploiting plan similarities within and across queries, it significantly
reduces search steps. It further improves per-step efficiency with a
precision-aware plan profiler that incrementally profiles and strategically
applies early stopping based on imprecise estimates of plan performance. At
scale, Circinus selects query-plan combinations to maximize global SLO goodput.
Evaluations in real-world settings show that Circinus improves service goodput
by 3.2-5.0$\times$, accelerates query planning by 4.2-5.8$\times$, achieving
query response in seconds, while reducing deployment costs by 3.2-4.0$\times$
over state of the arts even in their intended single-tier deployments.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [99] [Comprehensive Evaluation of Quantitative Measurements from Automated Deep Segmentations of PSMA PET/CT Images](https://arxiv.org/abs/2504.16237)
*Obed Korshie Dzikunu,Amirhossein Toosi,Shadab Ahamed,Sara Harsini,Francois Benard,Xiaoxiao Li,Arman Rahmim*

Main category: eess.IV

TL;DR: 该研究通过深度学习分割方法评估了六种定量指标，提出了一种新的损失函数L1DFL，并在PSMA PET/CT扫描数据中验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 传统Dice相似系数评估存在局限性，需更全面的定量指标评估方法。

Method: 使用U-Net、Attention U-Net和SegResNet模型，结合四种损失函数（包括提出的L1DFL），分析380例PSMA PET/CT扫描数据。

Result: Attention U-Net与L1DFL表现最佳（一致性相关系数0.90-0.99），而Dice Loss表现较差。L1DFL在SUV指标、病灶计数和TLA中表现优异。

Conclusion: L1DFL能最小化临床测量变异性，优于传统损失函数，代码已开源。

Abstract: This study performs a comprehensive evaluation of quantitative measurements
as extracted from automated deep-learning-based segmentation methods, beyond
traditional Dice Similarity Coefficient assessments, focusing on six
quantitative metrics, namely SUVmax, SUVmean, total lesion activity (TLA),
tumor volume (TMTV), lesion count, and lesion spread. We analyzed 380
prostate-specific membrane antigen (PSMA) targeted [18F]DCFPyL PET/CT scans of
patients with biochemical recurrence of prostate cancer, training deep neural
networks, U-Net, Attention U-Net and SegResNet with four loss functions: Dice
Loss, Dice Cross Entropy, Dice Focal Loss, and our proposed L1 weighted Dice
Focal Loss (L1DFL). Evaluations indicated that Attention U-Net paired with
L1DFL achieved the strongest correlation with the ground truth (concordance
correlation = 0.90-0.99 for SUVmax and TLA), whereas models employing the Dice
Loss and the other two compound losses, particularly with SegResNet,
underperformed. Equivalence testing (TOST, alpha = 0.05, Delta = 20%) confirmed
high performance for SUV metrics, lesion count and TLA, with L1DFL yielding the
best performance. By contrast, tumor volume and lesion spread exhibited greater
variability. Bland-Altman, Coverage Probability, and Total Deviation Index
analyses further highlighted that our proposed L1DFL minimizes variability in
quantification of the ground truth clinical measures. The code is publicly
available at: https://github.com/ObedDzik/pca\_segment.git.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [100] [Approximating Optimal Labelings for Temporal Connectivity](https://arxiv.org/abs/2504.16837)
*Daniele Carnevale,Gianlorenzo D'Angelo,Martin Olsen*

Main category: cs.DS

TL;DR: 研究时间图中边的时间标签调度问题，目标是使所有顶点对在给定时间内连通且标签总数最小。问题在无向图中是NP完全的，在有向图中是APX难的。本文扩展了其复杂性和近似性研究。


<details>
  <summary>Details</summary>
Motivation: 在物流、配送调度和社交网络信息传播中，优化时间标签能显著降低成本或排放。

Method: 证明问题在特定条件下的不可近似性，并提出近似算法，同时建立与静态图问题的联系。

Result: 问题在a≥2时不可近似于O(log n)，在a≥3时不可近似于2^(log^(1-ε)n)，并提出匹配的近似算法。

Conclusion: 研究扩展了对MAL问题的理解，并揭示了与DCSS问题的联系。

Abstract: In a temporal graph the edge set dynamically changes over time according to a
set of time-labels associated with each edge that indicates at which time-steps
the edge is available. Two vertices are connected if there is a path connecting
them in which the edges are traversed in increasing order of their labels. We
study the problem of scheduling the availability time of the edges of a
temporal graph in such a way that all pairs of vertices are connected within a
given maximum allowed time $a$ and the overall number of labels is minimized.
  The problem, known as \emph{Minimum Aged Labeling} (MAL), has several
applications in logistics, distribution scheduling, and information spreading
in social networks, where carefully choosing the time-labels can significantly
reduce infrastructure costs, fuel consumption, or greenhouse gases.
  The problem MAL has previously been proved to be NP-complete on undirected
graphs and \APX-hard on directed graphs. In this paper, we extend our knowledge
on the complexity and approximability of MAL in several directions. We first
show that the problem cannot be approximated within a factor better than
$O(\log n)$ when $a\geq 2$, unless $\text{P} = \text{NP}$, and a factor better
than $2^{\log ^{1-\epsilon} n}$ when $a\geq 3$, unless $\text{NP}\subseteq
\text{DTIME}(2^{\text{polylog}(n)})$, where $n$ is the number of vertices in
the graph. Then we give a set of approximation algorithms that, under some
conditions, almost match these lower bounds. In particular, we show that the
approximation depends on a relation between $a$ and the diameter of the input
graph.
  We further establish a connection with a foundational optimization problem on
static graphs called \emph{Diameter Constrained Spanning Subgraph} (DCSS) and
show that our hardness results also apply to DCSS.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [101] [TinyML for Speech Recognition](https://arxiv.org/abs/2504.16213)
*Andrew Barovic,Armin Moin*

Main category: cs.SD

TL;DR: 论文提出了一种在资源受限的IoT边缘设备上部署量化1D卷积神经网络模型进行语音识别的方法，并创建了新数据集。


<details>
  <summary>Details</summary>
Motivation: 为智能家居和辅助生活等IoT应用提供高效的语音识别解决方案。

Method: 使用Edge Impulse技术优化模型性能，在Arduino Nano 33 BLE Sense上实现原型验证。

Result: 模型在自建数据集上达到97%的准确率，支持23种关键词识别。

Conclusion: 该方法在资源受限设备上实现了高效语音识别，扩展了关键词识别范围。

Abstract: We train and deploy a quantized 1D convolutional neural network model to
conduct speech recognition on a highly resource-constrained IoT edge device.
This can be useful in various Internet of Things (IoT) applications, such as
smart homes and ambient assisted living for the elderly and people with
disabilities, just to name a few examples. In this paper, we first create a new
dataset with over one hour of audio data that enables our research and will be
useful to future studies in this field. Second, we utilize the technologies
provided by Edge Impulse to enhance our model's performance and achieve a high
Accuracy of up to 97% on our dataset. For the validation, we implement our
prototype using the Arduino Nano 33 BLE Sense microcontroller board. This
microcontroller board is specifically designed for IoT and AI applications,
making it an ideal choice for our target use case scenarios. While most
existing research focuses on a limited set of keywords, our model can process
23 different keywords, enabling complex commands.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [102] [Exploring zero-shot structure-based protein fitness prediction](https://arxiv.org/abs/2504.16886)
*Arnav Sharma,Anthony Gitter*

Main category: q-bio.QM

TL;DR: 该论文探讨了基于结构的零-shot蛋白质适应性预测模型的应用与挑战，强调了结构匹配的重要性，并展示了多模态集成模型的优势。


<details>
  <summary>Details</summary>
Motivation: 研究旨在利用预训练的机器学习模型，无需额外标记数据即可预测蛋白质序列变化的适应性，为遗传变异解释和蛋白质工程提供工具。

Method: 通过实验评估了基于结构的模型选择及其对适应性预测的影响，并测试了多模态集成模型在ProteinGym基准上的表现。

Result: 研究发现，无序蛋白质区域的预测结构可能误导模型性能，而结构匹配对预测准确性至关重要。多模态集成模型表现优异。

Conclusion: 结构匹配和多模态集成是提升蛋白质适应性预测模型性能的关键因素，尤其在处理无序区域时需谨慎。

Abstract: The ability to make zero-shot predictions about the fitness consequences of
protein sequence changes with pre-trained machine learning models enables many
practical applications. Such models can be applied for downstream tasks like
genetic variant interpretation and protein engineering without additional
labeled data. The advent of capable protein structure prediction tools has led
to the availability of orders of magnitude more precomputed predicted
structures, giving rise to powerful structure-based fitness prediction models.
Through our experiments, we assess several modeling choices for structure-based
models and their effects on downstream fitness prediction. Zero-shot fitness
prediction models can struggle to assess the fitness landscape within
disordered regions of proteins, those that lack a fixed 3D structure. We
confirm the importance of matching protein structures to fitness assays and
find that predicted structures for disordered regions can be misleading and
affect predictive performance. Lastly, we evaluate an additional
structure-based model on the ProteinGym substitution benchmark and show that
simple multi-modal ensembles are strong baselines.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [103] [SOTOPIA-S4: a user-friendly system for flexible, customizable, and large-scale social simulation](https://arxiv.org/abs/2504.16122)
*Xuhui Zhou,Zhe Su,Sophie Feng,Jiaxu Zhou,Jen-tse Huang,Hsien-Te Kao,Spencer Lynch,Svitlana Volkova,Tongshuang Sherry Wu,Anita Woolley,Hao Zhu,Maarten Sap*

Main category: cs.CY

TL;DR: SOTOPIA-S4是一个快速、灵活且可扩展的社交模拟系统，用于通过LLM代理验证社会科学假设。


<details>
  <summary>Details</summary>
Motivation: 解决当前框架的技术障碍，支持多轮和多方的LLM交互，并提供可定制的评估指标。

Method: 系统包含模拟引擎、RESTful API服务器和Web界面，支持用户无需编程即可设计、运行和分析模拟。

Result: 通过招聘谈判和多方规划场景验证了系统的实用性。

Conclusion: SOTOPIA-S4为社会科学研究和LLM行为分析提供了高效工具。

Abstract: Social simulation through large language model (LLM) agents is a promising
approach to explore and validate hypotheses related to social science questions
and LLM agents behavior. We present SOTOPIA-S4, a fast, flexible, and scalable
social simulation system that addresses the technical barriers of current
frameworks while enabling practitioners to generate multi-turn and multi-party
LLM-based interactions with customizable evaluation metrics for hypothesis
testing. SOTOPIA-S4 comes as a pip package that contains a simulation engine,
an API server with flexible RESTful APIs for simulation management, and a web
interface that enables both technical and non-technical users to design, run,
and analyze simulations without programming. We demonstrate the usefulness of
SOTOPIA-S4 with two use cases involving dyadic hiring negotiation and
multi-party planning scenarios.

</details>


### [104] [Efficacy of a Computer Tutor that Models Expert Human Tutors](https://arxiv.org/abs/2504.16132)
*Andrew M. Olney,Sidney K. D'Mello,Natalie Person,Whitney Cade,Patrick Hays,Claire W. Dempsey,Blair Lehman,Betsy Williams,Art Graesser*

Main category: cs.CY

TL;DR: 研究探讨了专家知识对辅导效果的影响，比较了智能辅导系统（ITS）、非辅导专家和未辅导组的效果，发现ITS和人类专家辅导在即时和延迟测试中均有显著正面效果。


<details>
  <summary>Details</summary>
Motivation: 探讨专家知识在辅导中的贡献，解决其有效性争议。

Method: 9周学习效果研究，比较ITS、非辅导专家人类导师和无辅导组，结合即时和延迟测试。

Result: ITS和人类导师在即时测试（d=.71和d=.66）和延迟测试（d=.36和d=.39）中均表现出显著正面效果。

Conclusion: 专家知识对辅导效果有重要贡献，未来研究可进一步优化辅导系统设计。

Abstract: Tutoring is highly effective for promoting learning. However, the
contribution of expertise to tutoring effectiveness is unclear and continues to
be debated. We conducted a 9-week learning efficacy study of an intelligent
tutoring system (ITS) for biology modeled on expert human tutors with two
control conditions: human tutors who were experts in the domain but not in
tutoring and a no-tutoring condition. All conditions were supplemental to
classroom instruction, and students took learning tests immediately before and
after tutoring sessions as well as delayed tests 1-2 weeks later. Analysis
using logistic mixed-effects modeling indicates significant positive effects on
the immediate post-test for the ITS (d =.71) and human tutors (d =.66) which
are in the 99th percentile of meta-analytic effects, as well as significant
positive effects on the delayed post-test for the ITS (d =.36) and human tutors
(d =.39). We discuss implications for the role of expertise in tutoring and the
design of future studies.

</details>


### [105] [A Conceptual Framework for AI-based Decision Systems in Critical Infrastructures](https://arxiv.org/abs/2504.16133)
*Milad Leyli-abadi,Ricardo J. Bessa,Jan Viebahn,Daniel Boos,Clark Borst,Alberto Castagna,Ricardo Chavarriaga,Mohamed Hassouna,Bruno Lemetayer,Giulia Leto,Antoine Marot,Maroua Meddeb,Manuel Meyer,Viola Schiaffonati,Manuel Schneider,Toni Waefler*

Main category: cs.CY

TL;DR: 本文提出了一种综合性框架，用于解决安全关键系统中人与AI交互的挑战，整合多学科知识并展示其灵活性。


<details>
  <summary>Details</summary>
Motivation: 现有框架未能完全解决安全关键系统中人与AI交互的透明性、信任性和解释性需求，亟需一种综合性解决方案。

Method: 采用跨学科方法，整合数学、决策理论、计算机科学、哲学、心理学和认知工程等领域知识，并在现有框架中实例化。

Result: 提出了一种灵活且全面的框架，适用于能源、交通和航空等关键基础设施。

Conclusion: 该框架为设计、部署和维护安全有效的系统提供了重要工具，填补了现有研究的空白。

Abstract: The interaction between humans and AI in safety-critical systems presents a
unique set of challenges that remain partially addressed by existing
frameworks. These challenges stem from the complex interplay of requirements
for transparency, trust, and explainability, coupled with the necessity for
robust and safe decision-making. A framework that holistically integrates human
and AI capabilities while addressing these concerns is notably required,
bridging the critical gaps in designing, deploying, and maintaining safe and
effective systems. This paper proposes a holistic conceptual framework for
critical infrastructures by adopting an interdisciplinary approach. It
integrates traditionally distinct fields such as mathematics, decision theory,
computer science, philosophy, psychology, and cognitive engineering and draws
on specialized engineering domains, particularly energy, mobility, and
aeronautics. The flexibility in its adoption is also demonstrated through its
instantiation on an already existing framework.

</details>


### [106] [Trends in Frontier AI Model Count: A Forecast to 2028](https://arxiv.org/abs/2504.16138)
*Iyngkarran Kumar,Sam Manning*

Main category: cs.CY

TL;DR: 论文分析了政府基于训练计算量对AI模型的监管要求，预测了未来几年内超过特定计算阈值的模型数量及其增长趋势。


<details>
  <summary>Details</summary>
Motivation: 探讨政府基于计算量对AI模型的监管政策（如欧盟AI法案和美国AI Diffusion Framework）的实际影响，预测未来模型数量是否会超过这些阈值。

Method: 通过统计和预测模型，估计未来几年内超过$10^{25}$和$10^{26}$ FLOP阈值的AI模型数量，并分析其增长趋势。

Result: 预测到2028年，将有103-306个模型超过欧盟的$10^{25}$ FLOP阈值，45-148个模型超过美国的$10^{26}$ FLOP阈值；且每年超过阈值的模型数量将超线性增长。

Conclusion: 基于绝对计算量的监管阈值将捕获越来越多的模型，而基于相对计算量的阈值则更稳定。监管政策需考虑计算量的动态增长趋势。

Abstract: Governments are starting to impose requirements on AI models based on how
much compute was used to train them. For example, the EU AI Act imposes
requirements on providers of general-purpose AI with systemic risk, which
includes systems trained using greater than $10^{25}$ floating point operations
(FLOP). In the United States' AI Diffusion Framework, a training compute
threshold of $10^{26}$ FLOP is used to identify "controlled models" which face
a number of requirements. We explore how many models such training compute
thresholds will capture over time. We estimate that by the end of 2028, there
will be between 103-306 foundation models exceeding the $10^{25}$ FLOP
threshold put forward in the EU AI Act (90% CI), and 45-148 models exceeding
the $10^{26}$ FLOP threshold that defines controlled models in the AI Diffusion
Framework (90% CI). We also find that the number of models exceeding these
absolute compute thresholds each year will increase superlinearly -- that is,
each successive year will see more new models captured within the threshold
than the year before. Thresholds that are defined with respect to the largest
training run to date (for example, such that all models within one order of
magnitude of the largest training run to date are captured by the threshold)
see a more stable trend, with a median forecast of 14-16 models being captured
by this definition annually from 2025-2028.

</details>


### [107] [Enhancing Trust Through Standards: A Comparative Risk-Impact Framework for Aligning ISO AI Standards with Global Ethical and Regulatory Contexts](https://arxiv.org/abs/2504.16139)
*Sridharan Sankaran*

Main category: cs.CY

TL;DR: 论文提出了一种比较风险影响评估框架，用于评估ISO AI标准在不同监管环境中的有效性，并提出了改进建议以增强其全球适用性。


<details>
  <summary>Details</summary>
Motivation: 随着AI重塑行业和社会，确保其可信赖性（如减少偏见、不透明和责任缺失等伦理风险）是全球性挑战。ISO AI标准旨在促进负责任的发展，但其效果因监管环境不同而异。

Method: 论文引入了一个比较风险影响评估框架，通过将ISO标准映射到欧盟AI法案，并调查十个地区的监管框架，评估其伦理一致性。

Result: 研究发现，自愿性ISO标准在执行（如美国科罗拉多州）和地区特定风险（如中国的隐私问题）方面存在不足。

Conclusion: 建议强制风险审计、地区特定附录和隐私模块，以增强ISO标准的适应性，促进全球AI治理的互操作性和信任。

Abstract: As artificial intelligence (AI) reshapes industries and societies, ensuring
its trustworthiness-through mitigating ethical risks like bias, opacity, and
accountability deficits-remains a global challenge. International Organization
for Standardization (ISO) AI standards, such as ISO/IEC 24027 and 24368, aim to
foster responsible development by embedding fairness, transparency, and risk
management into AI systems. However, their effectiveness varies across diverse
regulatory landscapes, from the EU's risk-based AI Act to China's
stability-focused measures and the U.S.'s fragmented state-led initiatives.
This paper introduces a novel Comparative Risk-Impact Assessment Framework to
evaluate how well ISO standards address ethical risks within these contexts,
proposing enhancements to strengthen their global applicability. By mapping ISO
standards to the EU AI Act and surveying regulatory frameworks in ten
regions-including the UK, Canada, India, Japan, Singapore, South Korea, and
Brazil-we establish a baseline for ethical alignment. The framework, applied to
case studies in the EU, US-Colorado, and China, reveals gaps: voluntary ISO
standards falter in enforcement (e.g., Colorado) and undervalue region-specific
risks like privacy (China). We recommend mandatory risk audits, region-specific
annexes, and a privacy-focused module to enhance ISO's adaptability. This
approach not only synthesizes global trends but also offers a replicable tool
for aligning standardization with ethical imperatives, fostering
interoperability and trust in AI worldwide. Policymakers and standards bodies
can leverage these insights to evolve AI governance, ensuring it meets diverse
societal needs as the technology advances.

</details>


### [108] [Cooperative Speech, Semantic Competence, and AI](https://arxiv.org/abs/2504.16092)
*Mahrad Almotahari*

Main category: cs.CY

TL;DR: 论文探讨了合作性言语的道德基础，认为大型语言模型（LLMs）缺乏合作对话所需的尊重，因此不具备断言能力，并质疑其语义能力。


<details>
  <summary>Details</summary>
Motivation: 研究合作性言语中尊重的重要性，以及LLMs是否具备合作对话的道德地位。

Method: 通过哲学分析，探讨尊重在合作性言语中的作用，并论证LLMs缺乏这种尊重。

Result: LLMs不具备合作对话所需的尊重，因此无法进行断言，其语义能力受到质疑。

Conclusion: 语义知识不仅是认知心理学的研究对象，也涉及道德心理学。

Abstract: Cooperative speech is purposive. From the speaker's perspective, one crucial
purpose is the transmission of knowledge. Cooperative speakers care about
getting things right for their conversational partners. This attitude is a kind
of respect. Cooperative speech is an ideal form of communication because
participants have respect for each other. And having respect within a
cooperative enterprise is sufficient for a particular kind of moral standing:
we ought to respect those who have respect for us. Respect demands reciprocity.
I maintain that large language models aren't owed the kind of respect that
partly constitutes a cooperative conversation. This implies that they aren't
cooperative interlocutors, otherwise we would be obliged to reciprocate the
attitude. Leveraging this conclusion, I argue that present-day LLMs are
incapable of assertion and that this raises an overlooked doubt about their
semantic competence. One upshot of this argument is that knowledge of meaning
isn't just a subject for the cognitive psychologist. It's also a subject for
the moral psychologist.

</details>


### [109] [Towards responsible AI for education: Hybrid human-AI to confront the Elephant in the room](https://arxiv.org/abs/2504.16148)
*Danial Hooshyar,Gustav Šír,Yeongwook Yang,Eve Kikas,Raija Hämäläinen,Tommi Kärkkäinen,Dragan Gašević,Roger Azevedo*

Main category: cs.CY

TL;DR: 论文分析了AI在教育领域的九大未解决问题，并提出神经符号AI作为解决方案。


<details>
  <summary>Details</summary>
Motivation: 尽管AI在教育系统中有显著进展，但公平性、透明性和有效性等问题仍未解决，需要批判性分析。

Method: 通过理论和实证研究，识别并分析了九大挑战，并提出神经符号AI作为解决方法。

Result: 研究发现当前AI方法在教育中存在多种问题，如忽视学习过程、缺乏领域知识整合等。

Conclusion: 神经符号AI可作为负责任、可信赖的AI教育系统的基础。

Abstract: Despite significant advancements in AI-driven educational systems and ongoing
calls for responsible AI for education, several critical issues remain
unresolved -- acting as the elephant in the room within AI in education,
learning analytics, educational data mining, learning sciences, and educational
psychology communities. This critical analysis identifies and examines nine
persistent challenges that continue to undermine the fairness, transparency,
and effectiveness of current AI methods and applications in education. These
include: (1) the lack of clarity around what AI for education truly means --
often ignoring the distinct purposes, strengths, and limitations of different
AI families -- and the trend of equating it with domain-agnostic,
company-driven large language models; (2) the widespread neglect of essential
learning processes such as motivation, emotion, and (meta)cognition in
AI-driven learner modelling and their contextual nature; (3) limited
integration of domain knowledge and lack of stakeholder involvement in AI
design and development; (4) continued use of non-sequential machine learning
models on temporal educational data; (5) misuse of non-sequential metrics to
evaluate sequential models; (6) use of unreliable explainable AI methods to
provide explanations for black-box models; (7) ignoring ethical guidelines in
addressing data inconsistencies during model training; (8) use of mainstream AI
methods for pattern discovery and learning analytics without systematic
benchmarking; and (9) overemphasis on global prescriptions while overlooking
localised, student-specific recommendations. Supported by theoretical and
empirical research, we demonstrate how hybrid AI methods -- specifically
neural-symbolic AI -- can address the elephant in the room and serve as the
foundation for responsible, trustworthy AI systems in education.

</details>


### [110] [Reflexive Prompt Engineering: A Framework for Responsible Prompt Engineering and Interaction Design](https://arxiv.org/abs/2504.16204)
*Christian Djeffal*

Main category: cs.CY

TL;DR: 本文探讨了负责任提示工程的重要性及其框架，旨在通过提示设计、系统选择等五个组件将伦理和法律考量融入AI交互，平衡技术精确性与伦理意识。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI的普及，提示工程对公平性、责任和透明度的影响日益显著，需将其与社会价值结合。

Method: 提出一个包含提示设计、系统选择、配置、性能评估和管理的综合框架，结合实证分析。

Result: 研究表明，有效的提示工程需技术精确性与伦理意识的平衡，能优化AI输出而不修改模型架构。

Conclusion: 负责任提示工程是AI开发与部署的关键桥梁，未来需进一步研究与实践指导。

Abstract: Responsible prompt engineering has emerged as a critical framework for
ensuring that generative artificial intelligence (AI) systems serve society's
needs while minimizing potential harms. As generative AI applications become
increasingly powerful and ubiquitous, the way we instruct and interact with
them through prompts has profound implications for fairness, accountability,
and transparency. This article examines how strategic prompt engineering can
embed ethical and legal considerations and societal values directly into AI
interactions, moving beyond mere technical optimization for functionality. This
article proposes a comprehensive framework for responsible prompt engineering
that encompasses five interconnected components: prompt design, system
selection, system configuration, performance evaluation, and prompt management.
Drawing from empirical evidence, the paper demonstrates how each component can
be leveraged to promote improved societal outcomes while mitigating potential
risks. The analysis reveals that effective prompt engineering requires a
delicate balance between technical precision and ethical consciousness,
combining the systematic rigor and focus on functionality with the nuanced
understanding of social impact. Through examination of real-world and emerging
practices, the article illustrates how responsible prompt engineering serves as
a crucial bridge between AI development and deployment, enabling organizations
to fine-tune AI outputs without modifying underlying model architectures. This
approach aligns with broader "Responsibility by Design" principles, embedding
ethical considerations directly into the implementation process rather than
treating them as post-hoc additions. The article concludes by identifying key
research directions and practical guidelines for advancing the field of
responsible prompt engineering.

</details>


### [111] [Leveraging Social Media Analytics for Sustainability Trend Detection in Saudi Arabias Evolving Market](https://arxiv.org/abs/2504.16153)
*Kanwal Aalijah*

Main category: cs.CY

TL;DR: 本文探讨了如何利用AI和社交媒体分析实时追踪沙特阿拉伯在《愿景2030》下的可持续发展趋势，为决策者提供跨行业的可靠市场洞察。


<details>
  <summary>Details</summary>
Motivation: 沙特阿拉伯的快速经济增长和社会变革为实时追踪新兴趋势提供了独特机会，AI和社交媒体分析可帮助发现商业和投资机会。

Method: 采用AI驱动的方法，处理数百万条社交媒体帖子、新闻和博客，以识别和监测可持续发展趋势。

Result: 研究提出了一种AI方法，能够为经济学家、企业和政府提供可靠、实时的市场趋势分析，并展示了该框架在其他地区的适应性。

Conclusion: 通过AI方法，决策者可以更可靠地了解公众对倡议的接受度和趋势发展，从而做出更明智的决策。

Abstract: Saudi Arabias rapid economic growth and social evolution under Vision 2030
present a unique opportunity to track emerging trends in real time. Uncovering
trends in real time can open up new avenues for business and investment
opportunities. This paper explores how AI and social media analytics can
uncover and monitor these trends across sectors like sustainability,
construction, food beverages industry, tourism, technology, and entertainment.
This paper focus on use of AI-driven methodology to identify sustainability
trends across Saudi Arabia. We processed millions of social media posts, news,
blogs in order to understand sustainability trends in the region. The paper
presents an AI approach that can help economists, businesses, government to
understand sustainability trends and make better decisions around them. This
approach offers both sector-specific and cross-sector insights, giving
decision-makers a reliable, up to date snapshot of Saudi Arabias market shifts.
Beyond Saudi Arabia, this framework also shows potential for adapting to other
regions. Overall, our findings highlight how by using AI-methodologies, give
decision makers a reliable method to understand how initiatives are perceived
and adopted by the public and understand growth of trends.

</details>


### [112] [Virology Capabilities Test (VCT): A Multimodal Virology Q&A Benchmark](https://arxiv.org/abs/2504.16137)
*Jasper Götting,Pedro Medeiros,Jon G Sanders,Nathaniel Li,Long Phan,Karam Elabd,Lennart Justen,Dan Hendrycks,Seth Donoughe*

Main category: cs.CY

TL;DR: VCT是一个评估大型语言模型（LLM）在复杂病毒学实验室协议中故障排除能力的基准测试，结果显示最先进的LLM表现优于专家病毒学家，引发了对双用途技术治理的紧迫考虑。


<details>
  <summary>Details</summary>
Motivation: 评估LLM在病毒学实验室协议中的实际应用能力，并探讨其双用途潜力。

Method: 构建了由专家设计的322个多模态问题组成的VCT基准测试，涵盖病毒学实验室的基本、隐性及视觉知识。

Result: 最先进的LLM（OpenAI的o3）在VCT上达到43.8%的准确率，优于94%的专家病毒学家。

Conclusion: LLM在病毒学故障排除中的专家级能力需纳入现有双用途技术治理框架。

Abstract: We present the Virology Capabilities Test (VCT), a large language model (LLM)
benchmark that measures the capability to troubleshoot complex virology
laboratory protocols. Constructed from the inputs of dozens of PhD-level expert
virologists, VCT consists of $322$ multimodal questions covering fundamental,
tacit, and visual knowledge that is essential for practical work in virology
laboratories. VCT is difficult: expert virologists with access to the internet
score an average of $22.1\%$ on questions specifically in their sub-areas of
expertise. However, the most performant LLM, OpenAI's o3, reaches $43.8\%$
accuracy, outperforming $94\%$ of expert virologists even within their
sub-areas of specialization. The ability to provide expert-level virology
troubleshooting is inherently dual-use: it is useful for beneficial research,
but it can also be misused. Therefore, the fact that publicly available models
outperform virologists on VCT raises pressing governance considerations. We
propose that the capability of LLMs to provide expert-level troubleshooting of
dual-use virology work should be integrated into existing frameworks for
handling dual-use technologies in the life sciences.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [113] [Data-Assimilated Model-Based Reinforcement Learning for Partially Observed Chaotic Flows](https://arxiv.org/abs/2504.16588)
*Defne E. Ozan,Andrea Nóvoa,Luca Magri*

Main category: eess.SY

TL;DR: 提出了一种结合数据同化的模型强化学习框架（DA-MBRL），用于部分可观测和噪声测量系统的湍流控制。


<details>
  <summary>Details</summary>
Motivation: 湍流控制因混沌动力学和高维度而困难，且传统强化学习方法需要完整状态信息，实验环境中往往无法满足。

Method: 采用控制感知的Echo State Network进行数据驱动预测，结合Ensemble Kalman Filter进行实时状态估计，并使用离策略actor-critic算法学习最优控制策略。

Result: 在Kuramoto-Sivashinsky方程上测试，成功从噪声和部分测量中稳定时空混沌流。

Conclusion: DA-MBRL框架在部分可观测和噪声环境下有效，为湍流控制提供了新方法。

Abstract: The goal of many applications in energy and transport sectors is to control
turbulent flows. However, because of chaotic dynamics and high dimensionality,
the control of turbulent flows is exceedingly difficult. Model-free
reinforcement learning (RL) methods can discover optimal control policies by
interacting with the environment, but they require full state information,
which is often unavailable in experimental settings. We propose a
data-assimilated model-based RL (DA-MBRL) framework for systems with partial
observability and noisy measurements. Our framework employs a control-aware
Echo State Network for data-driven prediction of the dynamics, and integrates
data assimilation with an Ensemble Kalman Filter for real-time state
estimation. An off-policy actor-critic algorithm is employed to learn optimal
control strategies from state estimates. The framework is tested on the
Kuramoto-Sivashinsky equation, demonstrating its effectiveness in stabilizing a
spatiotemporally chaotic flow from noisy and partial measurements.

</details>


### [114] [Learning Verifiable Control Policies Using Relaxed Verification](https://arxiv.org/abs/2504.16879)
*Puja Chaudhury,Alexander Estornell,Michael Everett*

Main category: eess.SY

TL;DR: 该论文提出了一种在训练过程中进行验证的方法，以确保学习型控制系统的安全性，避免了传统后训练验证的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统后训练验证方法可能因策略不符合规范或验证算法保守而无法提供安全保证，因此需要在训练过程中进行验证。

Method: 使用可微可达性分析，并将新组件纳入损失函数中。

Result: 在四旋翼模型和独轮车模型上的数值实验表明，该方法能生成满足可达-避免和不变性规范的策略。

Conclusion: 通过在训练中集成验证，该方法能够生成可实时评估安全性的轻量级策略。

Abstract: To provide safety guarantees for learning-based control systems, recent work
has developed formal verification methods to apply after training ends.
However, if the trained policy does not meet the specifications, or there is
conservatism in the verification algorithm, establishing these guarantees may
not be possible. Instead, this work proposes to perform verification throughout
training to ultimately aim for policies whose properties can be evaluated
throughout runtime with lightweight, relaxed verification algorithms. The
approach is to use differentiable reachability analysis and incorporate new
components into the loss function. Numerical experiments on a quadrotor model
and unicycle model highlight the ability of this approach to lead to learned
control policies that satisfy desired reach-avoid and invariance
specifications.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [115] [BrainPrompt: Multi-Level Brain Prompt Enhancement for Neurological Condition Identification](https://arxiv.org/abs/2504.16096)
*Jiaxing Xu,Kai He,Yue Tang,Wei Li,Mengcheng Lan,Xia Dong,Yiping Ke,Mengling Feng*

Main category: q-bio.NC

TL;DR: BrainPrompt结合图神经网络（GNN）与大型语言模型（LLM），通过多级知识驱动提示增强神经疾病诊断能力。


<details>
  <summary>Details</summary>
Motivation: 现有脑网络分析方法主要依赖成像数据，忽略了非成像因素，限制了预测能力和可解释性。

Method: 提出BrainPrompt框架，整合ROI级、受试者级和疾病级知识驱动提示，利用LLM增强多模态信息。

Result: 在两项静息态fMRI数据集上表现优于现有方法，并能提取与神经科学领域知识一致的生物标志物。

Conclusion: BrainPrompt通过知识增强的多模态信息提升了神经疾病诊断的预测能力和可解释性。

Abstract: Neurological conditions, such as Alzheimer's Disease, are challenging to
diagnose, particularly in the early stages where symptoms closely resemble
healthy controls. Existing brain network analysis methods primarily focus on
graph-based models that rely solely on imaging data, which may overlook
important non-imaging factors and limit the model's predictive power and
interpretability. In this paper, we present BrainPrompt, an innovative
framework that enhances Graph Neural Networks (GNNs) by integrating Large
Language Models (LLMs) with knowledge-driven prompts, enabling more effective
capture of complex, non-imaging information and external knowledge for
neurological disease identification. BrainPrompt integrates three types of
knowledge-driven prompts: (1) ROI-level prompts to encode the identity and
function of each brain region, (2) subject-level prompts that incorporate
demographic information, and (3) disease-level prompts to capture the temporal
progression of disease. By leveraging these multi-level prompts, BrainPrompt
effectively harnesses knowledge-enhanced multi-modal information from LLMs,
enhancing the model's capability to predict neurological disease stages and
meanwhile offers more interpretable results. We evaluate BrainPrompt on two
resting-state functional Magnetic Resonance Imaging (fMRI) datasets from
neurological disorders, showing its superiority over state-of-the-art methods.
Additionally, a biomarker study demonstrates the framework's ability to extract
valuable and interpretable information aligned with domain knowledge in
neuroscience.

</details>


### [116] [Application of an attention-based CNN-BiLSTM framework for in vivo two-photon calcium imaging of neuronal ensembles: decoding complex bilateral forelimb movements from unilateral M1](https://arxiv.org/abs/2504.16917)
*Ghazal Mirzaee,Jonathan Chang,Shahrzad Latifi*

Main category: q-bio.NC

TL;DR: 论文提出了一种基于注意力机制的CNN-BiLSTM混合深度学习模型，用于解码复杂前肢运动，展示了从单侧M1神经元群体中解码双侧前肢运动的潜力。


<details>
  <summary>Details</summary>
Motivation: 多尺度脑网络解码行为（如运动）是神经科学的核心目标，而人工智能和机器学习在揭示运动功能神经机制中的作用日益重要。

Method: 采用注意力机制的CNN-BiLSTM混合深度学习框架，利用双光子钙成像信号解码复杂前肢运动。

Result: 研究发现，单侧M1神经元群体可以准确解码双侧前肢的复杂运动。

Conclusion: 结果表明，先进的混合深度学习模型能有效捕捉与复杂运动执行相关的神经元网络活动的时空依赖性。

Abstract: Decoding behavior, such as movement, from multiscale brain networks remains a
central objective in neuroscience. Over the past decades, artificial
intelligence and machine learning have played an increasingly significant role
in elucidating the neural mechanisms underlying motor function. The advancement
of brain-monitoring technologies, capable of capturing complex neuronal signals
with high spatial and temporal resolution, necessitates the development and
application of more sophisticated machine learning models for behavioral
decoding. In this study, we employ a hybrid deep learning framework, an
attention-based CNN-BiLSTM model, to decode skilled and complex forelimb
movements using signals obtained from in vivo two-photon calcium imaging. Our
findings demonstrate that the intricate movements of both ipsilateral and
contralateral forelimbs can be accurately decoded from unilateral M1 neuronal
ensembles. These results highlight the efficacy of advanced hybrid deep
learning models in capturing the spatiotemporal dependencies of neuronal
networks activity linked to complex movement execution.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [117] [Breaking scaling relations with inverse catalysts: a machine learning exploration of trends in $\mathrm{CO_2}$ hydrogenation energy barriers](https://arxiv.org/abs/2504.16493)
*Luuk H. E. Kempen,Marius Juul Nielsen,Mie Andersen*

Main category: cond-mat.mtrl-sci

TL;DR: 论文提出了一种基于机器学习原子间势的逆向催化剂过渡态探索工作流，用于高效研究CO2转化为甲醇的催化过程。


<details>
  <summary>Details</summary>
Motivation: 减少CO2排放并降低对化石燃料的依赖是应对气候变化的关键策略，但开发新型催化剂成本高且耗时。

Method: 通过训练神经网络原子间势，快速探索逆向催化剂（如Cu(111)负载的氧化铟纳米团簇）上的过渡态，重点关注甲酸盐中间体的形成。

Result: 该方法显著加速了活性位点的探索，揭示了纳米团簇边缘和内部的结构-活性关系，并打破了线性标度关系。

Conclusion: 逆向催化剂在实验中表现优异的催化性能可能与其打破线性标度关系的能力有关。

Abstract: The conversion of $\mathrm{CO_2}$ into useful products such as methanol is a
key strategy for abating climate change and our dependence on fossil fuels.
Developing new catalysts for this process is costly and time-consuming and can
thus benefit from computational exploration of possible active sites. However,
this is complicated by the complexity of the materials and reaction networks.
Here, we present a workflow for exploring transition states of elementary
reaction steps at inverse catalysts, which is based on the training of a neural
network-based machine learning interatomic potential. We focus on the crucial
formate intermediate and its formation over nanoclusters of indium oxide
supported on Cu(111). The speedup compared to an approach purely based on
density functional theory allows us to probe a wide variety of active sites
found at nanoclusters of different sizes and stoichiometries. Analysis of the
obtained set of transition state geometries reveals different
structure--activity trends at the edge or interior of the nanoclusters.
Furthermore, the identified geometries allow for the breaking of linear scaling
relations, which could be a key underlying reason for the excellent catalytic
performance of inverse catalysts observed in experiments.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [118] [Radiometer Calibration using Machine Learning](https://arxiv.org/abs/2504.16791)
*S. A. K. Leeney,H. T. J. Bevins,E. de Lera Acedo,W. J. Handley,C. Kirkham,R. S. Patel,J. Zhu,D. Molnar,J. Cumner,D. Anstey,K. Artuc,G. Bernardi,M. Bucher,S. Carey,J. Cavillot,R. Chiello,W. Croukamp,D. I. L. de Villiers,J. A. Ely,A. Fialkov,T. Gessey-Jones,G. Kulkarni,A. Magro,P. D. Meerburg,S. Mittal,J. H. N. Pattison,S. Pegwal,C. M. Pieterse,J. R. Pritchard,E. Puchwein,N. Razavi-Ghods,I. L. V. Roque,A. Saxena,K. H. Scheutwinkel,P. Scott,E. Shen,P. H. Sims,M. Spinelli*

Main category: astro-ph.IM

TL;DR: 论文提出了一种基于机器学习的校准框架，用于提高射电天文辐射计的精度，特别针对探测21厘米线的挑战。


<details>
  <summary>Details</summary>
Motivation: 传统校准方法（如Dicke切换）在处理天线与接收器阻抗不匹配时存在局限性，而机器学习为复杂系统的校准提供了新思路。

Method: 使用神经网络训练已知信号源，建模并校准辐射计系统，以消除反射和失真。

Result: 首次测试并验证了机器学习校准框架在21厘米线探测中的高精度能力。

Conclusion: 机器学习校准框架为射电天文实验提供了更高效、精确的解决方案，尤其适用于微弱信号的探测。

Abstract: Radiometers are crucial instruments in radio astronomy, forming the primary
component of nearly all radio telescopes. They measure the intensity of
electromagnetic radiation, converting this radiation into electrical signals. A
radiometer's primary components are an antenna and a Low Noise Amplifier (LNA),
which is the core of the ``receiver'' chain. Instrumental effects introduced by
the receiver are typically corrected or removed during calibration. However,
impedance mismatches between the antenna and receiver can introduce unwanted
signal reflections and distortions. Traditional calibration methods, such as
Dicke switching, alternate the receiver input between the antenna and a
well-characterised reference source to mitigate errors by comparison. Recent
advances in Machine Learning (ML) offer promising alternatives. Neural
networks, which are trained using known signal sources, provide a powerful
means to model and calibrate complex systems where traditional analytical
approaches struggle. These methods are especially relevant for detecting the
faint sky-averaged 21-cm signal from atomic hydrogen at high redshifts. This is
one of the main challenges in observational Cosmology today. Here, for the
first time, we introduce and test a machine learning-based calibration
framework capable of achieving the precision required for radiometric
experiments aiming to detect the 21-cm line.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [119] [Physics-Informed Inference Time Scaling via Simulation-Calibrated Scientific Machine Learning](https://arxiv.org/abs/2504.16172)
*Zexi Fan,Yan Sun,Shihao Yang,Yiping Lu*

Main category: math.NA

TL;DR: SCaSML是一种物理信息框架，通过动态修正和去偏SciML预测，显著提升高维PDE求解精度。


<details>
  <summary>Details</summary>
Motivation: 高维PDE求解存在计算挑战，传统SciML方法存在偏差且忽视物理规律。

Method: 提出SCaSML框架，利用蒙特卡洛求解器和物理规律动态修正预测。

Result: 实验显示SCaSML将误差降低20-50%，优于基准模型。

Conclusion: SCaSML是首个能在推理阶段优化高维PDE近似解的算法。

Abstract: High-dimensional partial differential equations (PDEs) pose significant
computational challenges across fields ranging from quantum chemistry to
economics and finance. Although scientific machine learning (SciML) techniques
offer approximate solutions, they often suffer from bias and neglect crucial
physical insights. Inspired by inference-time scaling strategies in language
models, we propose Simulation-Calibrated Scientific Machine Learning (SCaSML),
a physics-informed framework that dynamically refines and debiases the SCiML
predictions during inference by enforcing the physical laws. SCaSML leverages
derived new physical laws that quantifies systematic errors and employs Monte
Carlo solvers based on the Feynman-Kac and Elworthy-Bismut-Li formulas to
dynamically correct the prediction. Both numerical and theoretical analysis
confirms enhanced convergence rates via compute-optimal inference methods. Our
numerical experiments demonstrate that SCaSML reduces errors by 20-50% compared
to the base surrogate model, establishing it as the first algorithm to refine
approximated solutions to high-dimensional PDE during inference. Code of SCaSML
is available at https://github.com/Francis-Fan-create/SCaSML.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [120] [Detecting Actionable Requests and Offers on Social Media During Crises Using LLMs](https://arxiv.org/abs/2504.16144)
*Ahmed El Fekih Zguir,Ferda Ofli,Muhammad Imran*

Main category: cs.IR

TL;DR: 提出了一种基于LLM的细粒度分类方法（QSF Learning），用于在自然灾害中高效分类和优先处理社交媒体上的求助与援助信息。


<details>
  <summary>Details</summary>
Motivation: 自然灾害中社交媒体信息激增，但缺乏系统分类方法，导致救援效率低下。

Method: 采用分层分类法，结合QSF Learning（基于嵌入数据库的少样本学习）提升分类性能，并评估信息的可操作性。

Result: 实验表明，该方法优于基线提示策略，能有效识别和优先处理紧急信息。

Conclusion: 该方法为救援组织提供了高效的信息分类和优先级排序工具。

Abstract: Natural disasters often result in a surge of social media activity, including
requests for assistance, offers of help, sentiments, and general updates. To
enable humanitarian organizations to respond more efficiently, we propose a
fine-grained hierarchical taxonomy to systematically organize crisis-related
information about requests and offers into three critical dimensions: supplies,
emergency personnel, and actions. Leveraging the capabilities of Large Language
Models (LLMs), we introduce Query-Specific Few-shot Learning (QSF Learning)
that retrieves class-specific labeled examples from an embedding database to
enhance the model's performance in detecting and classifying posts. Beyond
classification, we assess the actionability of messages to prioritize posts
requiring immediate attention. Extensive experiments demonstrate that our
approach outperforms baseline prompting strategies, effectively identifying and
prioritizing actionable requests and offers.

</details>


### [121] [LegalRAG: A Hybrid RAG System for Multilingual Legal Information Retrieval](https://arxiv.org/abs/2504.16121)
*Muhammad Rafsan Kabir,Rafeed Mohammad Sultan,Fuad Rahman,Mohammad Ruhul Amin,Sifat Momen,Nabeel Mohammed,Shafin Rahman*

Main category: cs.IR

TL;DR: 开发了一个高效的双语问答框架，用于处理孟加拉国警察公报中的法规文件，结合了检索增强生成（RAG）技术，提升了信息检索和回答生成的准确性。


<details>
  <summary>Details</summary>
Motivation: 法律和监管任务中自然语言处理技术的应用有限，希望通过双语问答框架提高法律信息的可访问性。

Method: 采用现代RAG管道，并提出一种改进的RAG方法，以提升检索性能和回答精确度。

Result: 在孟加拉国警察公报的测试集上，提出的方法在所有评估指标上均优于现有方法。

Conclusion: 该框架为法规文件的高效检索和问答提供了有效解决方案，提升了法律信息的可用性。

Abstract: Natural Language Processing (NLP) and computational linguistic techniques are
increasingly being applied across various domains, yet their use in legal and
regulatory tasks remains limited. To address this gap, we develop an efficient
bilingual question-answering framework for regulatory documents, specifically
the Bangladesh Police Gazettes, which contain both English and Bangla text. Our
approach employs modern Retrieval Augmented Generation (RAG) pipelines to
enhance information retrieval and response generation. In addition to
conventional RAG pipelines, we propose an advanced RAG-based approach that
improves retrieval performance, leading to more precise answers. This system
enables efficient searching for specific government legal notices, making legal
information more accessible. We evaluate both our proposed and conventional RAG
systems on a diverse test set on Bangladesh Police Gazettes, demonstrating that
our approach consistently outperforms existing methods across all evaluation
metrics.

</details>


### [122] [CLIRudit: Cross-Lingual Information Retrieval of Scientific Documents](https://arxiv.org/abs/2504.16264)
*Francisco Valentini,Diego Kozlowski,Vincent Larivière*

Main category: cs.IR

TL;DR: 本文介绍了CLIRudit数据集，用于评估跨语言学术搜索，展示了多种零样本检索方法的性能比较，发现密集检索器表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决跨语言学术信息检索的需求，促进科学知识的跨语言传播。

Method: 使用双语文章元数据构建数据集，比较了密集检索、稀疏检索、机器翻译等方法。

Result: 密集检索器在零样本任务中表现接近人工翻译效果，稀疏检索器结合文档翻译也具竞争力。

Conclusion: 研究为跨语言学术检索提供了新框架和数据集，推动科学知识的无障碍传播。

Abstract: Cross-lingual information retrieval (CLIR) consists in finding relevant
documents in a language that differs from the language of the queries. This
paper presents CLIRudit, a new dataset created to evaluate cross-lingual
academic search, focusing on English queries and French documents. The dataset
is built using bilingual article metadata from \'Erudit, a Canadian publishing
platform, and is designed to represent scenarios in which researchers search
for scholarly content in languages other than English. We perform a
comprehensive benchmarking of different zero-shot first-stage retrieval methods
on the dataset, including dense and sparse retrievers, query and document
machine translation, and state-of-the-art multilingual retrievers. Our results
show that large dense retrievers, not necessarily trained for the cross-lingual
retrieval task, can achieve zero-shot performance comparable to using ground
truth human translations, without the need for machine translation. Sparse
retrievers, such as BM25 or SPLADE, combined with document translation, show
competitive results, providing an efficient alternative to large dense models.
This research advances the understanding of cross-lingual academic information
retrieval and provides a framework that others can use to build comparable
datasets across different languages and disciplines. By making the dataset and
code publicly available, we aim to facilitate further research that will help
make scientific knowledge more accessible across language barriers.

</details>


### [123] [Disentangling and Generating Modalities for Recommendation in Missing Modality Scenarios](https://arxiv.org/abs/2504.16352)
*Jiwan Kim,Hongseok Kang,Sein Kim,Kibum Kim,Chanyoung Park*

Main category: cs.IR

TL;DR: DGMRec是一个针对多模态推荐系统中缺失模态问题的新框架，通过解耦和生成模态特征提升推荐性能。


<details>
  <summary>Details</summary>
Motivation: 解决多模态推荐系统中缺失模态和模态特征独特性未被充分考量的问题。

Method: 提出DGMRec框架，解耦模态特征为通用和特定特征，并生成缺失模态特征。

Result: 在缺失模态和新物品场景下表现优于现有方法，支持跨模态检索。

Conclusion: DGMRec在现实应用中具有适应性和潜力。

Abstract: Multi-modal recommender systems (MRSs) have achieved notable success in
improving personalization by leveraging diverse modalities such as images,
text, and audio. However, two key challenges remain insufficiently addressed:
(1) Insufficient consideration of missing modality scenarios and (2) the
overlooking of unique characteristics of modality features. These challenges
result in significant performance degradation in realistic situations where
modalities are missing. To address these issues, we propose Disentangling and
Generating Modality Recommender (DGMRec), a novel framework tailored for
missing modality scenarios. DGMRec disentangles modality features into general
and specific modality features from an information-based perspective, enabling
richer representations for recommendation. Building on this, it generates
missing modality features by integrating aligned features from other modalities
and leveraging user modality preferences. Extensive experiments show that
DGMRec consistently outperforms state-of-the-art MRSs in challenging scenarios,
including missing modalities and new item settings as well as diverse missing
ratios and varying levels of missing modalities. Moreover, DGMRec's
generation-based approach enables cross-modal retrieval, a task inapplicable
for existing MRSs, highlighting its adaptability and potential for real-world
applications. Our code is available at https://github.com/ptkjw1997/DGMRec.

</details>


### [124] [A Survey of Foundation Model-Powered Recommender Systems: From Feature-Based, Generative to Agentic Paradigms](https://arxiv.org/abs/2504.16420)
*Chengkai Huang,Hongtao Huang,Tong Yu,Kaige Xie,Junda Wu,Shuai Zhang,Julian Mcauley,Dietmar Jannach,Lina Yao*

Main category: cs.IR

TL;DR: 该论文综述了基础模型（FMs）在推荐系统（RS）中的三种应用范式：特征增强、生成式推荐和交互式代理系统，并探讨了其机遇与挑战。


<details>
  <summary>Details</summary>
Motivation: 随着基础模型（如GPT、LLaMA和CLIP）的兴起，推荐系统的范式正在被重塑，需要全面梳理其应用与影响。

Method: 论文通过综述现有研究，分析了FMs在推荐系统中的三种集成范式及其具体应用。

Result: FMs为推荐系统带来了新的能力（如多模态推理），但也面临数据隐私、计算成本等挑战。

Conclusion: 论文总结了FMs在推荐系统中的潜力与局限，并提出了未来研究方向。

Abstract: Recommender systems (RS) have become essential in filtering information and
personalizing content for users. RS techniques have traditionally relied on
modeling interactions between users and items as well as the features of
content using models specific to each task. The emergence of foundation models
(FMs), large scale models trained on vast amounts of data such as GPT, LLaMA
and CLIP, is reshaping the recommendation paradigm. This survey provides a
comprehensive overview of the Foundation Models for Recommender Systems
(FM4RecSys), covering their integration in three paradigms: (1) Feature-Based
augmentation of representations, (2) Generative recommendation approaches, and
(3) Agentic interactive systems. We first review the data foundations of RS,
from traditional explicit or implicit feedback to multimodal content sources.
We then introduce FMs and their capabilities for representation learning,
natural language understanding, and multi-modal reasoning in RS contexts. The
core of the survey discusses how FMs enhance RS under different paradigms.
Afterward, we examine FM applications in various recommendation tasks. Through
an analysis of recent research, we highlight key opportunities that have been
realized as well as challenges encountered. Finally, we outline open research
directions and technical challenges for next-generation FM4RecSys. This survey
not only reviews the state-of-the-art methods but also provides a critical
analysis of the trade-offs among the feature-based, the generative, and the
agentic paradigms, outlining key open issues and future research directions.

</details>


### [125] [MMHCL: Multi-Modal Hypergraph Contrastive Learning for Recommendation](https://arxiv.org/abs/2504.16576)
*Xu Guo,Tong Zhang,Fuyun Wang,Xudong Wang,Xiaoya Zhang,Xin Liu,Zhen Cui*

Main category: cs.IR

TL;DR: 论文提出了一种多模态超图对比学习框架（MMHCL），通过构建用户和商品的超图来挖掘共享偏好和语义相似性，缓解数据稀疏和冷启动问题。


<details>
  <summary>Details</summary>
Motivation: 多模态内容平台的个性化推荐系统面临数据稀疏和冷启动问题，现有方法未能充分挖掘多模态数据中的语义关联。

Method: 构建用户-用户和商品-商品超图，结合对比学习增强特征区分性，融合一阶和二阶语义信息。

Result: MMHCL通过挖掘更丰富的共享属性，有效缓解了数据稀疏和冷启动问题，实验验证了其有效性。

Conclusion: MMHCL框架在多模态推荐系统中表现出色，为数据稀疏和冷启动问题提供了有效解决方案。

Abstract: The burgeoning presence of multimodal content-sharing platforms propels the
development of personalized recommender systems. Previous works usually suffer
from data sparsity and cold-start problems, and may fail to adequately explore
semantic user-product associations from multimodal data. To address these
issues, we propose a novel Multi-Modal Hypergraph Contrastive Learning (MMHCL)
framework for user recommendation. For a comprehensive information exploration
from user-product relations, we construct two hypergraphs, i.e. a user-to-user
(u2u) hypergraph and an item-to-item (i2i) hypergraph, to mine shared
preferences among users and intricate multimodal semantic resemblance among
items, respectively. This process yields denser second-order semantics that are
fused with first-order user-item interaction as complementary to alleviate the
data sparsity issue. Then, we design a contrastive feature enhancement paradigm
by applying synergistic contrastive learning. By maximizing/minimizing the
mutual information between second-order (e.g. shared preference pattern for
users) and first-order (information of selected items for users) embeddings of
the same/different users and items, the feature distinguishability can be
effectively enhanced. Compared with using sparse primary user-item interaction
only, our MMHCL obtains denser second-order hypergraphs and excavates more
abundant shared attributes to explore the user-product associations, which to a
certain extent alleviates the problems of data sparsity and cold-start.
Extensive experiments have comprehensively demonstrated the effectiveness of
our method. Our code is publicly available at: https://github.com/Xu107/MMHCL.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [126] [Introduction to Quantum Machine Learning and Quantum Architecture Search](https://arxiv.org/abs/2504.16131)
*Samuel Yen-Chi Chen,Zhiding Liang*

Main category: quant-ph

TL;DR: 量子计算与机器学习的结合推动了量子机器学习（QML）的发展，旨在通过量子原理提升ML算法性能，并探索自动化设计高性能量子电路架构的方法。


<details>
  <summary>Details</summary>
Motivation: 整合量子计算与机器学习，以提升ML算法的性能并扩展QML的应用范围。

Method: 利用量子原理和自动化方法设计高性能量子电路架构。

Result: 展示了QML在多个领域的应用潜力。

Conclusion: QML有望通过量子计算与机器学习的结合，推动跨领域的技术进步。

Abstract: Recent advancements in quantum computing (QC) and machine learning (ML) have
fueled significant research efforts aimed at integrating these two
transformative technologies. Quantum machine learning (QML), an emerging
interdisciplinary field, leverages quantum principles to enhance the
performance of ML algorithms. Concurrently, the exploration of systematic and
automated approaches for designing high-performance quantum circuit
architectures for QML tasks has gained prominence, as these methods empower
researchers outside the quantum computing domain to effectively utilize
quantum-enhanced tools. This tutorial will provide an in-depth overview of
recent breakthroughs in both areas, highlighting their potential to expand the
application landscape of QML across diverse fields.

</details>


### [127] [QAOA-GPT: Efficient Generation of Adaptive and Regular Quantum Approximate Optimization Algorithm Circuits](https://arxiv.org/abs/2504.16350)
*Ilya Tyagin,Marwa H. Farag,Kyle Sherbert,Karunya Shirali,Yuri Alexeev,Ilya Safro*

Main category: quant-ph

TL;DR: QAOA-GPT是一个生成框架，利用GPT直接合成量子电路以解决优化问题，并在MaxCut问题上验证其效果。


<details>
  <summary>Details</summary>
Motivation: 量子计算在解决某些经典计算机难以处理的优化问题上具有潜力，但需要新的算法方法。

Method: 使用GPT生成量子电路，并通过自适应QAOA方法生成合成数据集以训练模型。

Result: QAOA-GPT能生成高质量量子电路，显著减少计算开销和参数优化需求。

Conclusion: 生成式AI是生成紧凑量子电路的一种有前景的途径。

Abstract: Quantum computing has the potential to improve our ability to solve certain
optimization problems that are computationally difficult for classical
computers, by offering new algorithmic approaches that may provide speedups
under specific conditions. In this work, we introduce QAOA-GPT, a generative
framework that leverages Generative Pretrained Transformers (GPT) to directly
synthesize quantum circuits for solving quadratic unconstrained binary
optimization problems, and demonstrate it on the MaxCut problem on graphs. To
diversify the training circuits and ensure their quality, we have generated a
synthetic dataset using the adaptive QAOA approach, a method that incrementally
builds and optimizes problem-specific circuits. The experiments conducted on a
curated set of graph instances demonstrate that QAOA-GPT, generates high
quality quantum circuits for new problem instances unseen in the training as
well as successfully parametrizes QAOA. Our results show that using QAOA-GPT to
generate quantum circuits will significantly decrease both the computational
overhead of classical QAOA and adaptive approaches that often use gradient
evaluation to generate the circuit and the classical optimization of the
circuit parameters. Our work shows that generative AI could be a promising
avenue to generate compact quantum circuits in a scalable way.

</details>


### [128] [Deep Neural Network Emulation of the Quantum-Classical Transition via Learned Wigner Function Dynamics](https://arxiv.org/abs/2504.16334)
*Kamran Majid*

Main category: quant-ph

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The emergence of classical behavior from quantum mechanics as Planck's
constant $\hbar$ approaches zero remains a fundamental challenge in physics
[1-3]. This paper introduces a novel approach employing deep neural networks to
directly learn the dynamical mapping from initial quantum state parameters (for
Gaussian wave packets of the one-dimensional harmonic oscillator) and $\hbar$
to the parameters of the time-evolved Wigner function in phase space [4-6]. A
comprehensive dataset of analytically derived time-evolved Wigner functions was
generated, and a deep feedforward neural network with an enhanced architecture
was successfully trained for this prediction task, achieving a final training
loss of ~ 0.0390. The network demonstrates a significant and previously
unrealized ability to accurately capture the underlying mapping of the Wigner
function dynamics. This allows for a direct emulation of the quantum-classical
transition by predicting the evolution of phase-space distributions as $\hbar$
is systematically varied. The implications of these findings for providing a
new computational lens on the emergence of classicality are discussed,
highlighting the potential of this direct phase-space learning approach for
studying fundamental aspects of quantum mechanics. This work presents a
significant advancement beyond previous efforts that focused on learning
observable mappings [7], offering a direct route via the phase-space
representation.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [129] [A Geometric Approach to Problems in Optimization and Data Science](https://arxiv.org/abs/2504.16270)
*Naren Sarayu Manoj*

Main category: math.OC

TL;DR: 论文分为两部分：第一部分关注优化中的计算问题，提出新算法；第二部分研究数据科学问题的统计保证，分析后门数据投毒攻击和图聚类算法的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 利用高维几何和概率工具解决计算和统计机器学习中的问题。

Method: 第一部分提出近似凸多面体、稀疏化、鲁棒最小二乘回归和对偶优化的新算法；第二部分提出新模型分析后门数据投毒攻击的统计特性，并研究图聚类算法在“有益”误设下的鲁棒性。

Result: 提出了针对优化和数据科学问题的新算法和统计保证。

Conclusion: 通过高维几何和概率工具，论文在计算优化和统计机器学习领域取得了新进展。

Abstract: We give new results for problems in computational and statistical machine
learning using tools from high-dimensional geometry and probability.
  We break up our treatment into two parts. In Part I, we focus on
computational considerations in optimization. Specifically, we give new
algorithms for approximating convex polytopes in a stream, sparsification and
robust least squares regression, and dueling optimization.
  In Part II, we give new statistical guarantees for data science problems. In
particular, we formulate a new model in which we analyze statistical properties
of backdoor data poisoning attacks, and we study the robustness of graph
clustering algorithms to ``helpful'' misspecification.

</details>


### [130] [The Safety-Privacy Tradeoff in Linear Bandits](https://arxiv.org/abs/2504.16371)
*Arghavan Zibaie,Spencer Hutchinson,Ramtin Pedarsani,Mahnoosh Alizadeh*

Main category: math.OC

TL;DR: 论文研究了多智能体线性随机老虎机问题，在全局安全约束和局部差分隐私保护下，权衡隐私、安全性和遗憾最小化。


<details>
  <summary>Details</summary>
Motivation: 解决多智能体系统中隐私保护与全局安全约束下的遗憾最小化问题。

Method: 基于安全集的几何性质（锐度），提出了一种在给定最大遗憾预算下，为不同智能体分配隐私级别的不可单边改进方法。

Result: 通过权衡隐私级别与安全性和遗憾，提出了一个优化的隐私分配方案。

Conclusion: 研究为隐私保护与安全约束下的多智能体决策提供了理论框架和实用方法。

Abstract: We consider a collection of linear stochastic bandit problems, each modeling
the random response of different agents to proposed interventions, coupled
together by a global safety constraint. We assume a central coordinator must
choose actions to play on each bandit with the objective of regret
minimization, while also ensuring that the expected response of all agents
satisfies the global safety constraints at each round, in spite of uncertainty
about the bandits' parameters. The agents consider their observed responses to
be private and in order to protect their sensitive information, the data
sharing with the central coordinator is performed under local differential
privacy (LDP). However, providing higher level of privacy to different agents
would have consequences in terms of safety and regret. We formalize these
tradeoffs by building on the notion of the sharpness of the safety set - a
measure of how the geometric properties of the safe set affects the growth of
regret - and propose a unilaterally unimprovable vector of privacy levels for
different agents given a maximum regret budget.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [131] [A Statistical Evaluation of Indoor LoRaWAN Environment-Aware Propagation for 6G: MLR, ANOVA, and Residual Distribution Analysis](https://arxiv.org/abs/2504.16688)
*Nahshon Mokua,Obiri,Kristof,Van Laerhoven*

Main category: cs.NI

TL;DR: 该研究提出了一种两阶段方法，通过环境变量和概率分布模型改进室内LoRaWAN技术的路径损耗建模。


<details>
  <summary>Details</summary>
Motivation: 室内LoRaWAN技术部署中路径损耗建模因结构障碍、人员密度和环境变化而复杂，需更精确的模型。

Method: 使用多元线性回归模型（包含传统传播指标和新增环境变量）和五种概率分布拟合残差。

Result: 环境变量减少42.32%的未解释方差；四组分高斯混合模型最能捕捉残差异质性。

Conclusion: 环境感知建模可显著提升动态室内物联网部署中的LoRaWAN网络设计。

Abstract: Modeling path loss in indoor LoRaWAN technology deployments is inherently
challenging due to structural obstructions, occupant density and activities,
and fluctuating environmental conditions. This study proposes a two-stage
approach to capture and analyze these complexities using an extensive dataset
of 1,328,334 field measurements collected over six months in a single-floor
office at the University of Siegen's Hoelderlinstrasse Campus, Germany. First,
we implement a multiple linear regression model that includes traditional
propagation metrics (distance, structural walls) and an extension with proposed
environmental variables (relative humidity, temperature, carbon dioxide,
particulate matter, and barometric pressure). Using analysis of variance, we
demonstrate that adding these environmental factors can reduce unexplained
variance by 42.32 percent. Secondly, we examine residual distributions by
fitting five candidate probability distributions: Normal, Skew-Normal, Cauchy,
Student's t, and Gaussian Mixture Models with one to five components. Our
results show that a four-component Gaussian Mixture Model captures the residual
heterogeneity of indoor signal propagation most accurately, significantly
outperforming single-distribution approaches. Given the push toward
ultra-reliable, context-aware communications in 6G networks, our analysis shows
that environment-aware modeling can substantially improve LoRaWAN network
design in dynamic indoor IoT deployments.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [132] [Quality of explanation of xAI from the prespective of Italian end-users: Italian version of System Causability Scale (SCS)](https://arxiv.org/abs/2504.16193)
*Carmine Attanasio,Alireza Mortezapour*

Main category: cs.HC

TL;DR: 研究验证了意大利版系统可解释性量表（I-SCS）的有效性，用于衡量xAI系统提供的解释质量。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能在计算机科学以外领域的广泛应用，研究者关注如何提供高质量的算法解释。

Method: 采用前向-后向翻译方法，计算内容效度指数/比率，并进行认知访谈。

Result: 原问卷10个问题中删除了1个（CVR低于0.49），最终意大利版包含9个问题，用户理解良好。

Conclusion: 意大利版I-SCS可用于未来研究及xAI开发，衡量意大利文化中的解释质量。

Abstract: Background and aim: Considering the scope of the application of artificial
intelligence beyond the field of computer science, one of the concerns of
researchers is to provide quality explanations about the functioning of
algorithms based on artificial intelligence and the data extracted from it. The
purpose of the present study is to validate the Italian version of system
causability scale (I-SCS) to measure the quality of explanations provided in a
xAI.
  Method: For this purpose, the English version, initially provided in 2020 in
coordination with the main developer, was utilized. The forward-backward
translation method was applied to ensure accuracy. Finally, these nine steps
were completed by calculating the content validity index/ratio and conducting
cognitive interviews with representative end users.
  Results: The original version of the questionnaire consisted of 10 questions.
However, based on the obtained indexes (CVR below 0.49), one question (Question
8) was entirely removed. After completing the aforementioned steps, the Italian
version contained 9 questions. The representative sample of Italian end users
fully comprehended the meaning and content of the questions in the Italian
version.
  Conclusion: The Italian version obtained in this study can be used in future
research studies as well as in the field by xAI developers. This tool can be
used to measure the quality of explanations provided for an xAI system in
Italian culture.

</details>


### [133] [Cyberoception: Finding a Painlessly-Measurable New Sense in the Cyberworld Towards Emotion-Awareness in Computing](https://arxiv.org/abs/2504.16378)
*Tadashi Okoshi,Zexiong Gao,Tan Yi Zhen,Takumi Karasawa,Takeshi Miki,Wataru Sasaki,Rajesh K. Balan*

Main category: cs.HC

TL;DR: 论文提出新概念“cyberoception”，通过智能手机传感器测量用户情绪状态，替代传统实验室方法。


<details>
  <summary>Details</summary>
Motivation: 现有内感受测量方法依赖实验室环境，难以在真实生活中监测用户情绪。

Method: 提出“cyberoception”概念，通过智能手机传感器测量用户行为（如“Turn On”行为），并与情绪关联。

Result: 实验发现“Turn On”行为与用户情绪显著相关。

Conclusion: cyberoception可作为开发更智能情绪感知应用的基础。

Abstract: In Affective computing, recognizing users' emotions accurately is the basis
of affective human-computer interaction. Understanding users' interoception
contributes to a better understanding of individually different emotional
abilities, which is essential for achieving inter-individually accurate emotion
estimation. However, existing interoception measurement methods, such as the
heart rate discrimination task, have several limitations, including their
dependence on a well-controlled laboratory environment and precision apparatus,
making monitoring users' interoception challenging. This study aims to
determine other forms of data that can explain users' interoceptive or similar
states in their real-world lives and propose a novel hypothetical concept
"cyberoception," a new sense (1) which has properties similar to interoception
in terms of the correlation with other emotion-related abilities, and (2) which
can be measured only by the sensors embedded inside commodity smartphone
devices in users' daily lives. Results from a 10-day-long in-lab/in-the-wild
hybrid experiment reveal a specific cyberoception type "Turn On" (users'
subjective sensory perception about the frequency of turning-on behavior on
their smartphones), significantly related to participants' emotional valence.
We anticipate that cyberoception to serve as a fundamental building block for
developing more "emotion-aware", user-friendly applications and services.

</details>


### [134] [FeedQUAC: Quick Unobtrusive AI-Generated Commentary](https://arxiv.org/abs/2504.16416)
*Tao Long,Kendra Wannamaker,Jo Vermeulen,George Fitzmaurice,Justin Matejka*

Main category: cs.HC

TL;DR: 论文探讨了AI如何通过实时反馈工具FeedQUAC减轻设计过程中反馈收集的负担，提升创意工作流。


<details>
  <summary>Details</summary>
Motivation: 设计过程中持续反馈的需求与收集反馈的高成本之间的矛盾。

Method: 引入FeedQUAC工具，通过多角色AI提供实时反馈，并进行8人设计探针研究。

Result: 参与者认为AI反馈便捷、有趣、提升信心和灵感，同时提出改进建议。

Conclusion: AI反馈在设计中有潜力，需平衡用户参与，环境交互是未来创意支持系统的重要方向。

Abstract: Design thrives on feedback. However, gathering constant feedback throughout
the design process can be labor-intensive and disruptive. We explore how AI can
bridge this gap by providing effortless, ambient feedback. We introduce
FeedQUAC, a design companion that delivers real-time AI-generated commentary
from a variety of perspectives through different personas. A design probe study
with eight participants highlights how designers can leverage quick yet ambient
AI feedback to enhance their creative workflows. Participants highlight
benefits such as convenience, playfulness, confidence boost, and inspiration
from this lightweight feedback agent, while suggesting additional features,
like chat interaction and context curation. We discuss the role of AI feedback,
its strengths and limitations, and how to integrate it into existing design
workflows while balancing user involvement. Our findings also suggest that
ambient interaction is a valuable consideration for both the design and
evaluation of future creativity support systems.

</details>


### [135] [Exploring human-SAV interaction using large language models: The impact of psychological ownership and anthropomorphism on user experience](https://arxiv.org/abs/2504.16548)
*Lirui Guo,Michael G. Burke,Wynita M. Griggs*

Main category: cs.HC

TL;DR: 研究探讨了LLM驱动的共享自动驾驶汽车（SAV）用户界面中提示策略如何影响用户感知、体验和采用意图，发现更具拟人化和心理所有权触发的设计能提升用户体验和接受度。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注心理因素（如拟人化）对SAV采用的影响，但LLM驱动的SAV用户界面中提示策略的作用尚未充分探索。

Method: 设计了四种具有不同拟人化特征和心理所有权触发因素的SAV用户界面，通过定量和定性方法评估用户反应。

Result: 拟人化和心理所有权触发的设计显著提升了用户对SAV人性化特征的感知和情感反馈。

Conclusion: 研究结果为设计提升用户体验和SAV采用率的LLM对话界面提供了实用指导。

Abstract: There has been extensive prior work exploring how psychological factors such
as anthropomorphism affect the adoption of shared autonomous vehicles (SAVs).
However, limited research has been conducted on how prompt strategies in large
language model (LLM)-powered SAV User Interfaces (UIs) affect users'
perceptions, experiences, and intentions to adopt such technology. In this
work, we investigate how conversational UIs powered by LLMs drive these
psychological factors and psychological ownership, the sense of possession a
user may come to feel towards an entity or object they may not legally own. We
designed four SAV UIs with varying levels of anthropomorphic characteristics
and psychological ownership triggers. Quantitative measures of psychological
ownership, anthropomorphism, quality of service, disclosure tendency, sentiment
of SAV responses, and overall acceptance were collected after participants
interacted with each SAV. Qualitative feedback was also gathered regarding the
experience of psychological ownership during the interactions. The results
indicate that an SAV conversational UI designed to be more anthropomorphic and
to induce psychological ownership improved users' perceptions of the SAV's
human-like qualities and improved the sentiment of responses compared to a
control condition. These findings provide practical guidance for designing
LLM-based conversational UIs that enhance user experience and adoption of SAVs.

</details>


### [136] [A Vision for AI-Driven Adaptation of Dynamic AR Content to Users and Environments](https://arxiv.org/abs/2504.16562)
*Julian Rasch,Florian Müller,Francesco Chiossi*

Main category: cs.HC

TL;DR: 论文探讨了AI驱动的动态AR内容布局，旨在提升AR体验的直观性和有效性。


<details>
  <summary>Details</summary>
Motivation: 现有AR系统在管理交互可能性方面存在不足，需要更智能的解决方案。

Method: 提出利用机器学习方法动态调整AR内容布局，适应用户移动和环境变化。

Result: 通过AI驱动的动态内容管理，可优化UI布局并降低用户认知负荷。

Conclusion: AI驱动的AR内容布局为多行业创新提供了新机遇，如导航、学习和生产力提升。

Abstract: Augmented Reality (AR) is transforming the way we interact with virtual
information in the physical world. By overlaying digital content in real-world
environments, AR enables new forms of immersive and engaging experiences.
However, existing AR systems often struggle to effectively manage the many
interactive possibilities that AR presents. This vision paper speculates on
AI-driven approaches for adaptive AR content placement, dynamically adjusting
to user movement and environmental changes. By leveraging machine learning
methods, such a system would intelligently manage content distribution between
AR projections integrated into the external environment and fixed static
content, enabling seamless UI layout and potentially reducing users' cognitive
load. By exploring the possibilities of AI-driven dynamic AR content placement,
we aim to envision new opportunities for innovation and improvement in various
industries, from urban navigation and workplace productivity to immersive
learning and beyond. This paper outlines a vision for the development of more
intuitive, engaging, and effective AI-powered AR experiences.

</details>


### [137] [PsyCounAssist: A Full-Cycle AI-Powered Psychological Counseling Assistant System](https://arxiv.org/abs/2504.16573)
*Xianghe Liu,Jiaqi Xu,Tao Sun*

Main category: cs.HC

TL;DR: PsyCounAssist是一个AI驱动的心理咨询助手系统，通过多模态情感识别和自动化报告增强心理咨询实践。


<details>
  <summary>Details</summary>
Motivation: 心理咨询需要实时监测情绪变化和记录会话内容，传统方法效率低且易出错。

Method: 结合语音和PPG信号进行情感分析，利用大语言模型生成结构化报告，提供个性化随访支持。

Result: 实验验证了PPG情感分类的可靠性，系统在真实场景中表现出实用性和隐私保护性。

Conclusion: PsyCounAssist为AI在心理咨询中的伦理和高效整合提供了新思路。

Abstract: Psychological counseling is a highly personalized and dynamic process that
requires therapists to continuously monitor emotional changes, document session
insights, and maintain therapeutic continuity. In this paper, we introduce
PsyCounAssist, a comprehensive AI-powered counseling assistant system
specifically designed to augment psychological counseling practices.
PsyCounAssist integrates multimodal emotion recognition combining speech and
photoplethysmography (PPG) signals for accurate real-time affective analysis,
automated structured session reporting using large language models (LLMs), and
personalized AI-generated follow-up support. Deployed on Android-based tablet
devices, the system demonstrates practical applicability and flexibility in
real-world counseling scenarios. Experimental evaluation confirms the
reliability of PPG-based emotional classification and highlights the system's
potential for non-intrusive, privacy-aware emotional support. PsyCounAssist
represents a novel approach to ethically and effectively integrating AI into
psychological counseling workflows.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [138] [Confidence Sequences for Generalized Linear Models via Regret Analysis](https://arxiv.org/abs/2504.16555)
*Eugenio Clerico,Hamish Flynn,Wojciech Kotłowski,Gergely Neu*

Main category: math.ST

TL;DR: 提出了一种通过序列预测构建统计模型参数置信集的方法，将问题转化为在线学习中的低遗憾算法设计。


<details>
  <summary>Details</summary>
Motivation: 为广义线性模型（GLM）参数构建置信集，通过序列概率分配游戏将统计问题转化为算法问题。

Method: 提出在线到置信集的转换方案，包括分析转换（证明低遗憾算法存在性）和算法转换（利用在线算法输出构建置信集）。

Result: 该方法统一了现有最优置信集构造，并提出了文献中未见过的新型置信集。

Conclusion: 通过在线学习框架，为统计模型参数置信集提供了一种通用且灵活的方法。

Abstract: We develop a methodology for constructing confidence sets for parameters of
statistical models via a reduction to sequential prediction. Our key
observation is that for any generalized linear model (GLM), one can construct
an associated game of sequential probability assignment such that achieving low
regret in the game implies a high-probability upper bound on the excess
likelihood of the true parameter of the GLM. This allows us to develop a scheme
that we call online-to-confidence-set conversions, which effectively reduces
the problem of proving the desired statistical claim to an algorithmic
question. We study two varieties of this conversion scheme: 1) analytical
conversions that only require proving the existence of algorithms with low
regret and provide confidence sets centered at the maximum-likelihood estimator
2) algorithmic conversions that actively leverage the output of the online
algorithm to construct confidence sets (and may be centered at other,
adaptively constructed point estimators). The resulting methodology recovers
all state-of-the-art confidence set constructions within a single framework,
and also provides several new types of confidence sets that were previously
unknown in the literature.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [139] [Security-First AI: Foundations for Robust and Trustworthy Systems](https://arxiv.org/abs/2504.16110)
*Krti Tallam*

Main category: cs.CR

TL;DR: 论文主张将AI安全作为基础层优先考虑，提出分层视角区分安全与保障，并提倡以安全为先的方法构建可信赖的AI系统。


<details>
  <summary>Details</summary>
Motivation: 当前AI讨论多关注安全、透明、责任等问题，但AI安全（保护数据、模型和流程免受攻击）是这些努力的基础。

Method: 提出分层视角区分AI安全与保障，讨论核心威胁模型、攻击途径及防御机制。

Result: 主张以指标驱动的方法提升AI安全，从而增强AI系统的稳健性、透明度和问责性。

Conclusion: AI安全是构建可信赖和稳健AI系统的关键基础，需优先考虑。

Abstract: The conversation around artificial intelligence (AI) often focuses on safety,
transparency, accountability, alignment, and responsibility. However, AI
security (i.e., the safeguarding of data, models, and pipelines from
adversarial manipulation) underpins all of these efforts. This manuscript
posits that AI security must be prioritized as a foundational layer. We present
a hierarchical view of AI challenges, distinguishing security from safety, and
argue for a security-first approach to enable trustworthy and resilient AI
systems. We discuss core threat models, key attack vectors, and emerging
defense mechanisms, concluding that a metric-driven approach to AI security is
essential for robust AI safety, transparency, and accountability.

</details>


### [140] [AI-Based Vulnerability Analysis of NFT Smart Contracts](https://arxiv.org/abs/2504.16113)
*Xin Wang,Xiaoqi Li*

Main category: cs.CR

TL;DR: 论文通过收集和分类智能合约代码，识别常见缺陷，并利用决策树和随机森林模型进行分析，最终比较不同模型的性能。


<details>
  <summary>Details</summary>
Motivation: 研究智能合约中的常见缺陷，并开发有效的模型进行缺陷检测和分析。

Method: 1. 收集和分类智能合约代码；2. 使用Python处理数据；3. 构建决策树模型并进行特征提取；4. 引入随机森林模型优化；5. 比较不同模型性能。

Result: 通过决策树和随机森林模型成功分析了智能合约缺陷，并比较了不同模型的效果。

Conclusion: 决策树和随机森林模型在智能合约缺陷检测中表现良好，随机森林模型在优化后效果更佳。

Abstract: In the research experiment of this article, our research work is divided into
several stages. Firstly, we collected a large number of smart contract codes
and classified them, identifying several common defects, including Risky
Mutably Porxy, ERC-721 Recentrancy, Unlimited Mining, Missing Requirements, and
Public Burns. Secondly, we used Python to process the smart contracts. On the
one hand, we modified the file names, and on the other hand, we batched the
process of the content for analysis and application. Next, we built a model of
the decision tree. Firstly, we carried out the feature extraction. We selected
the algorithm and divided the data. After comparing and processing, we chose
the CART classification tree to process. By gene coefficient, we analyzed and
sorted the data, and got the initial model of the decision tree. Then, we
introduced the random forest model on the basis of the decision tree. From
abstracting the same amount of samples to selecting features randomly.From
adjusting and optimizing parameters to completing the construction of the
forest model. Finally, we compared and analyzed the decision tree, random
forest, and self-built model in the paper and drew general conclusions.

</details>


### [141] [DMind Benchmark: The First Comprehensive Benchmark for LLM Evaluation in the Web3 Domain](https://arxiv.org/abs/2504.16116)
*Miracle Master,Rainy Sun,Anya Reese,Joey Ouyang,Alex Chen,Winter Dong,Frank Li,James Yi,Garry Zhao,Tony Ling,Hobert Wong,Lowes Yang*

Main category: cs.CR

TL;DR: DMind Benchmark是一个系统测试LLMs在Web3领域表现的框架，涵盖九大类别，揭示模型在Web3特定任务中的性能差距。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在NLP任务中表现优异，但在Web3等快速发展的专业领域中的应用尚未充分探索。

Method: 引入DMind Benchmark，包含主观任务（如智能合约审计、链上数据推理等），评估15种流行LLMs。

Result: 发现LLMs在Web3特定推理（如代币经济学、安全漏洞识别）中表现不足，最强模型也面临挑战。

Conclusion: 公开数据集和评估流程，推动Web3领域LLMs的适应性研究和开发。

Abstract: Recent advances in Large Language Models (LLMs) have led to significant
progress on a wide range of natural language processing tasks. However, their
effectiveness in specialized and rapidly evolving domains such as Web3 remains
underexplored. In this paper, we introduce DMind Benchmark, a novel framework
that systematically tests LLMs across nine key categories encompassing
blockchain fundamentals, infrastructure, smart contract analysis, decentralized
finance (DeFi), decentralized autonomous organizations (DAOs), non-fungible
tokens (NFTs), token economics, meme concepts, and security vulnerabilities.
  DMind Benchmark goes beyond conventional multiple-choice questions by
incorporating domain-specific subjective tasks (e.g., smart contract code
auditing and repair, numeric reasoning on on-chain data, and fill-in
assessments), thereby capturing real-world complexities and stress-testing
model adaptability. We evaluate fifteen popular LLMs (from ChatGPT, DeepSeek,
Claude, and Gemini series) on DMind Benchmark, uncovering performance gaps in
Web3-specific reasoning and application, particularly in emerging areas like
token economics and meme concepts. Even the strongest models face significant
challenges in identifying subtle security vulnerabilities and analyzing complex
DeFi mechanisms. To foster progress in this area, we publicly release our
benchmark dataset, evaluation pipeline, and annotated results at
http://www.dmind.ai, offering a valuable resource for advancing specialized
domain adaptation and the development of more robust Web3-enabled LLMs.

</details>


### [142] [Towards Explainable and Lightweight AI for Real-Time Cyber Threat Hunting in Edge Networks](https://arxiv.org/abs/2504.16118)
*Milad Rahmati*

Main category: cs.CR

TL;DR: 论文提出了一种可解释且轻量化的AI框架（ELAI），用于边缘网络中的实时网络威胁检测，解决了传统深度学习模型在可解释性和计算成本上的不足。


<details>
  <summary>Details</summary>
Motivation: 边缘网络的分布式特性和资源限制使得网络安全面临挑战，而传统AI驱动的威胁检测系统缺乏可解释性且计算成本高，限制了实际部署。

Method: 结合可解释的机器学习算法和优化的轻量化深度学习技术，采用决策树、注意力机制和联邦学习，提升检测准确性和可解释性。

Result: 在CICIDS和UNSW-NB15等数据集上的实验表明，ELAI框架实现了高检测率和低误报率，同时显著降低了计算需求。

Conclusion: ELAI框架为边缘计算环境提供了一种高效、可解释的网络安全解决方案，兼具高性能和低计算成本。

Abstract: As cyber threats continue to evolve, securing edge networks has become
increasingly challenging due to their distributed nature and resource
limitations. Many AI-driven threat detection systems rely on complex deep
learning models, which, despite their high accuracy, suffer from two major
drawbacks: lack of interpretability and high computational cost. Black-box AI
models make it difficult for security analysts to understand the reasoning
behind their predictions, limiting their practical deployment. Moreover,
conventional deep learning techniques demand significant computational
resources, rendering them unsuitable for edge devices with limited processing
power. To address these issues, this study introduces an Explainable and
Lightweight AI (ELAI) framework designed for real-time cyber threat detection
in edge networks. Our approach integrates interpretable machine learning
algorithms with optimized lightweight deep learning techniques, ensuring both
transparency and computational efficiency. The proposed system leverages
decision trees, attention-based deep learning, and federated learning to
enhance detection accuracy while maintaining explainability. We evaluate ELAI
using benchmark cybersecurity datasets, such as CICIDS and UNSW-NB15, assessing
its performance across diverse cyberattack scenarios. Experimental results
demonstrate that the proposed framework achieves high detection rates with
minimal false positives, all while significantly reducing computational demands
compared to traditional deep learning methods. The key contributions of this
work include: (1) a novel interpretable AI-based cybersecurity model tailored
for edge computing environments, (2) an optimized lightweight deep learning
approach for real-time cyber threat detection, and (3) a comprehensive analysis
of explainability techniques in AI-driven cybersecurity applications.

</details>


### [143] [A Data-Centric Approach for Safe and Secure Large Language Models against Threatening and Toxic Content](https://arxiv.org/abs/2504.16120)
*Chaima Njeh,Haïfa Nakouri,Fehmi Jaafar*

Main category: cs.CR

TL;DR: 论文提出了一种后生成修正机制BART-Corrective Model，用于减少大型语言模型（LLM）生成的有害内容，显著降低了毒性和越狱分数。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM取得了显著进展，但其潜在偏见和有害内容的问题仍然存在，需要一种实用解决方案确保其安全与伦理使用。

Method: 采用BART-Corrective Model作为后生成修正机制，不依赖模型微调或提示工程，而是通过数据为中心的方法减少有害内容。

Result: 实验表明，该方法显著降低了多种模型的毒性和越狱分数，如GPT-4（15%和21%）、PaLM2（28%和5%）、Mistral-7B（26%和23%）和Gemma-2b-it（11.1%和19%）。

Conclusion: 该方法有效提升了LLM的安全性和适用性，为其实际应用提供了可靠保障。

Abstract: Large Language Models (LLM) have made remarkable progress, but concerns about
potential biases and harmful content persist. To address these apprehensions,
we introduce a practical solution for ensuring LLM's safe and ethical use. Our
novel approach focuses on a post-generation correction mechanism, the
BART-Corrective Model, which adjusts generated content to ensure safety and
security. Unlike relying solely on model fine-tuning or prompt engineering, our
method provides a robust data-centric alternative for mitigating harmful
content. We demonstrate the effectiveness of our approach through experiments
on multiple toxic datasets, which show a significant reduction in mean toxicity
and jail-breaking scores after integration. Specifically, our results show a
reduction of 15% and 21% in mean toxicity and jail-breaking scores with GPT-4,
a substantial reduction of 28% and 5% with PaLM2, a reduction of approximately
26% and 23% with Mistral-7B, and a reduction of 11.1% and 19% with Gemma-2b-it.
These results demonstrate the potential of our approach to improve the safety
and security of LLM, making them more suitable for real-world applications.

</details>


### [144] [Blockchain Meets Adaptive Honeypots: A Trust-Aware Approach to Next-Gen IoT Security](https://arxiv.org/abs/2504.16226)
*Yazan Otoum,Arghavan Asad,Amiya Nayak*

Main category: cs.CR

TL;DR: 论文提出了一种动态攻击检测与防御方法，结合区块链认证、双阶段入侵检测系统、信任感知服务迁移和虚拟蜜罐技术，显著提升了NGWN-IoT的安全性。


<details>
  <summary>Details</summary>
Motivation: NGWN-IoT虽然提供了高带宽能力，但面临不断演变的网络威胁，现有安全方法效果有限。

Method: 采用区块链认证（DAA）、双阶段入侵检测（IRF和DCRNN）、信任感知服务迁移（HBO）和虚拟蜜罐（BLISS存储攻击模式）。

Result: 实验表明，该方法在准确性、检测率、误报率等多指标上优于现有方法。

Conclusion: 该框架显著增强了NGWN-IoT生态系统的安全性。

Abstract: Edge computing-based Next-Generation Wireless Networks (NGWN)-IoT offer
enhanced bandwidth capacity for large-scale service provisioning but remain
vulnerable to evolving cyber threats. Existing intrusion detection and
prevention methods provide limited security as adversaries continually adapt
their attack strategies. We propose a dynamic attack detection and prevention
approach to address this challenge. First, blockchain-based authentication uses
the Deoxys Authentication Algorithm (DAA) to verify IoT device legitimacy
before data transmission. Next, a bi-stage intrusion detection system is
introduced: the first stage uses signature-based detection via an Improved
Random Forest (IRF) algorithm. In contrast, the second stage applies
feature-based anomaly detection using a Diffusion Convolution Recurrent Neural
Network (DCRNN). To ensure Quality of Service (QoS) and maintain Service Level
Agreements (SLA), trust-aware service migration is performed using Heap-Based
Optimization (HBO). Additionally, on-demand virtual High-Interaction honeypots
deceive attackers and extract attack patterns, which are securely stored using
the Bimodal Lattice Signature Scheme (BLISS) to enhance signature-based
Intrusion Detection Systems (IDS). The proposed framework is implemented in the
NS3 simulation environment and evaluated against existing methods across
multiple performance metrics, including accuracy, attack detection rate, false
negative rate, precision, recall, ROC curve, memory usage, CPU usage, and
execution time. Experimental results demonstrate that the framework
significantly outperforms existing approaches, reinforcing the security of
NGWN-enabled IoT ecosystems

</details>


### [145] [On the Consistency of GNN Explanations for Malware Detection](https://arxiv.org/abs/2504.16316)
*Hossein Shokouhinejad,Griffin Higgins,Roozbeh Razavi-Far,Hesamodin Mohammadian,Ali A. Ghorbani*

Main category: cs.CR

TL;DR: 提出了一种结合规则编码和自动编码器的动态构建控制流图（CFG）的框架，利用图神经网络（GNN）分类器检测恶意行为，并通过多种解释技术提升模型可解释性。


<details>
  <summary>Details</summary>
Motivation: 控制流图（CFG）在程序执行分析和恶意软件行为表征中至关重要，但现有方法在动态构建和可解释性方面存在不足。

Method: 动态构建CFG并嵌入节点特征，结合规则编码和自动编码器；构建GNN分类器；应用多种解释技术（如GNNExplainer、PGExplainer等）并引入RankFusion聚合方法。

Result: 框架在恶意软件检测中表现出高准确性，并通过解释技术生成可靠且可解释的结果。

Conclusion: 提出的框架在恶意软件检测和解释性方面具有显著优势，为安全分析提供了新工具。

Abstract: Control Flow Graphs (CFGs) are critical for analyzing program execution and
characterizing malware behavior. With the growing adoption of Graph Neural
Networks (GNNs), CFG-based representations have proven highly effective for
malware detection. This study proposes a novel framework that dynamically
constructs CFGs and embeds node features using a hybrid approach combining
rule-based encoding and autoencoder-based embedding. A GNN-based classifier is
then constructed to detect malicious behavior from the resulting graph
representations. To improve model interpretability, we apply state-of-the-art
explainability techniques, including GNNExplainer, PGExplainer, and
CaptumExplainer, the latter is utilized three attribution methods: Integrated
Gradients, Guided Backpropagation, and Saliency. In addition, we introduce a
novel aggregation method, called RankFusion, that integrates the outputs of the
top-performing explainers to enhance the explanation quality. We also evaluate
explanations using two subgraph extraction strategies, including the proposed
Greedy Edge-wise Composition (GEC) method for improved structural coherence. A
comprehensive evaluation using accuracy, fidelity, and consistency metrics
demonstrates the effectiveness of the proposed framework in terms of accurate
identification of malware samples and generating reliable and interpretable
explanations.

</details>


### [146] [Amplified Vulnerabilities: Structured Jailbreak Attacks on LLM-based Multi-Agent Debate](https://arxiv.org/abs/2504.16489)
*Senmao Qi,Yifei Zou,Peng Li,Ziyi Lin,Xiuzhen Cheng,Dongxiao Yu*

Main category: cs.CR

TL;DR: 该论文研究了多智能体辩论（MAD）框架在大型语言模型（LLMs）中的安全漏洞，特别是针对越狱攻击的脆弱性，并提出了一种新型攻击方法。


<details>
  <summary>Details</summary>
Motivation: 多智能体辩论（MAD）通过协作提升复杂任务的推理能力，但其安全风险（如越狱攻击）尚未充分研究。

Method: 论文提出了一种结构化提示重写框架，利用叙事封装、角色驱动升级、迭代优化和修辞混淆来攻击MAD系统。

Result: 实验表明，MAD系统比单智能体更脆弱，攻击方法将平均危害性从28.14%提升至80.34%，成功率高达80%。

Conclusion: MAD架构存在固有漏洞，亟需在现实部署前开发专门防御措施。

Abstract: Multi-Agent Debate (MAD), leveraging collaborative interactions among Large
Language Models (LLMs), aim to enhance reasoning capabilities in complex tasks.
However, the security implications of their iterative dialogues and
role-playing characteristics, particularly susceptibility to jailbreak attacks
eliciting harmful content, remain critically underexplored. This paper
systematically investigates the jailbreak vulnerabilities of four prominent MAD
frameworks built upon leading commercial LLMs (GPT-4o, GPT-4, GPT-3.5-turbo,
and DeepSeek) without compromising internal agents. We introduce a novel
structured prompt-rewriting framework specifically designed to exploit MAD
dynamics via narrative encapsulation, role-driven escalation, iterative
refinement, and rhetorical obfuscation. Our extensive experiments demonstrate
that MAD systems are inherently more vulnerable than single-agent setups.
Crucially, our proposed attack methodology significantly amplifies this
fragility, increasing average harmfulness from 28.14% to 80.34% and achieving
attack success rates as high as 80% in certain scenarios. These findings reveal
intrinsic vulnerabilities in MAD architectures and underscore the urgent need
for robust, specialized defenses prior to real-world deployment.

</details>


### [147] [Property-Preserving Hashing for $\ell_1$-Distance Predicates: Applications to Countering Adversarial Input Attacks](https://arxiv.org/abs/2504.16355)
*Hassan Asghar,Chenhan Zhang,Dali Kaafar*

Main category: cs.CR

TL;DR: 提出了首个基于ℓ₁距离的保属性哈希（PPH）构造，用于在对抗性设置下检测相似图像，具有高效性和强正确性保证。


<details>
  <summary>Details</summary>
Motivation: 对抗性攻击使感知哈希失效，PPH通过保留输入属性提供强正确性保证，适用于检测对抗性环境下的相似图像。

Method: 提出基于ℓ₁距离的PPH构造，通过设定阈值t迫使攻击者添加显著噪声，从而降低图像质量。算法时间复杂度为O(t²)。

Result: 在28×28灰度图像上，1%像素扰动下评估时间为0.0784秒；224×224 RGB图像分块处理，1%扰动下每块0.0128秒，14%扰动下0.2641秒。

Conclusion: 该PPH方案高效且能有效对抗攻击，适用于实际应用中的图像相似性检测。

Abstract: Perceptual hashing is used to detect whether an input image is similar to a
reference image with a variety of security applications. Recently, they have
been shown to succumb to adversarial input attacks which make small
imperceptible changes to the input image yet the hashing algorithm does not
detect its similarity to the original image. Property-preserving hashing (PPH)
is a recent construct in cryptography, which preserves some property
(predicate) of its inputs in the hash domain. Researchers have so far shown
constructions of PPH for Hamming distance predicates, which, for instance,
outputs 1 if two inputs are within Hamming distance $t$. A key feature of PPH
is its strong correctness guarantee, i.e., the probability that the predicate
will not be correctly evaluated in the hash domain is negligible. Motivated by
the use case of detecting similar images under adversarial setting, we propose
the first PPH construction for an $\ell_1$-distance predicate. Roughly, this
predicate checks if the two one-sided $\ell_1$-distances between two images are
within a threshold $t$. Since many adversarial attacks use $\ell_2$-distance
(related to $\ell_1$-distance) as the objective function to perturb the input
image, by appropriately choosing the threshold $t$, we can force the attacker
to add considerable noise to evade detection, and hence significantly
deteriorate the image quality. Our proposed scheme is highly efficient, and
runs in time $O(t^2)$. For grayscale images of size $28 \times 28$, we can
evaluate the predicate in $0.0784$ seconds when pixel values are perturbed by
up to $1 \%$. For larger RGB images of size $224 \times 224$, by dividing the
image into 1,000 blocks, we achieve times of $0.0128$ seconds per block for $1
\%$ change, and up to $0.2641$ seconds per block for $14\%$ change.

</details>


### [148] [Case Study: Fine-tuning Small Language Models for Accurate and Private CWE Detection in Python Code](https://arxiv.org/abs/2504.16584)
*Md. Azizul Hakim Bappy,Hossen A Mustafa,Prottoy Saha,Rajinus Salehat*

Main category: cs.CR

TL;DR: 研究探讨了小型语言模型（SLMs）作为隐私保护的高效漏洞检测工具的潜力，通过微调350M参数的预训练模型，在Python代码中检测MITRE Top 25 CWEs，取得了99%的准确率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在代码漏洞检测中表现优异，但依赖云端计算且成本高，不适合敏感或专有代码库。研究旨在探索SLMs作为替代方案的可行性。

Method: 使用半监督方法构建500个样本的数据集，结合LLM生成的数据和人工审核，对350M参数的codegen-mono模型进行指令微调。

Result: 微调后的SLM在测试集上表现优异，准确率99%，精确率98.08%，召回率100%，F1分数99.04%。

Conclusion: 微调SLMs可作为高效、隐私保护的漏洞检测工具，适合直接集成到开发工作流中。

Abstract: Large Language Models (LLMs) have demonstrated significant capabilities in
understanding and analyzing code for security vulnerabilities, such as Common
Weakness Enumerations (CWEs). However, their reliance on cloud infrastructure
and substantial computational requirements pose challenges for analyzing
sensitive or proprietary codebases due to privacy concerns and inference costs.
This work explores the potential of Small Language Models (SLMs) as a viable
alternative for accurate, on-premise vulnerability detection. We investigated
whether a 350-million parameter pre-trained code model (codegen-mono) could be
effectively fine-tuned to detect the MITRE Top 25 CWEs specifically within
Python code. To facilitate this, we developed a targeted dataset of 500
examples using a semi-supervised approach involving LLM-driven synthetic data
generation coupled with meticulous human review. Initial tests confirmed that
the base codegen-mono model completely failed to identify CWEs in our samples.
However, after applying instruction-following fine-tuning, the specialized SLM
achieved remarkable performance on our test set, yielding approximately 99%
accuracy, 98.08% precision, 100% recall, and a 99.04% F1-score. These results
strongly suggest that fine-tuned SLMs can serve as highly accurate and
efficient tools for CWE detection, offering a practical and privacy-preserving
solution for integrating advanced security analysis directly into development
workflows.

</details>


### [149] [From Past to Present: A Survey of Malicious URL Detection Techniques, Datasets and Code Repositories](https://arxiv.org/abs/2504.16449)
*Ye Tian,Yanqiu Yu,Jianguo Sun,Yanbin Wang*

Main category: cs.CR

TL;DR: 本文对恶意URL检测技术进行了全面综述，提出了一种基于模态的新型分类法，并整理了公开数据集和开源实现，以填补现有研究的空白。


<details>
  <summary>Details</summary>
Motivation: 恶意URL持续威胁网络安全，现有研究存在算法分类不清晰、未涵盖LLM/Transformer防御、缺乏开源实现和数据集覆盖不足等问题。

Method: 系统分析从传统黑名单到深度学习方法（如Transformer、GNNs和LLMs）的技术，提出模态分类法，并整理公开数据集和开源实现。

Result: 提出了一种新的分类法，整理了数据集和开源实现，并总结了产品级实现的设计原则和架构框架。

Conclusion: 本文填补了现有研究的空白，提出了未来研究的可行方向，并维护了一个GitHub仓库以持续更新资源。

Abstract: Malicious URLs persistently threaten the cybersecurity ecosystem, by either
deceiving users into divulging private data or distributing harmful payloads to
infiltrate host systems. Gaining timely insights into the current state of this
ongoing battle holds significant importance. However, existing reviews exhibit
4 critical gaps: 1) Their reliance on algorithm-centric taxonomies obscures
understanding of how detection approaches exploit specific modal information
channels; 2) They fail to incorporate pivotal LLM/Transformer-based defenses;
3) No open-source implementations are collected to facilitate benchmarking; 4)
Insufficient dataset coverage.This paper presents a comprehensive review of
malicious URL detection technologies, systematically analyzing methods from
traditional blacklisting to advanced deep learning approaches (e.g.
Transformer, GNNs, and LLMs). Unlike prior surveys, we propose a novel
modality-based taxonomy that categorizes existing works according to their
primary data modalities (URL, HTML, Visual, etc.). This hierarchical
classification enables both rigorous technical analysis and clear understanding
of multimodal information utilization. Furthermore, to establish a profile of
accessible datasets and address the lack of standardized benchmarking (where
current studies often lack proper baseline comparisons), we curate and analyze:
1) publicly available datasets (2016-2024), and 2) open-source implementations
from published works(2013-2025). Then, we outline essential design principles
and architectural frameworks for product-level implementations. The review
concludes by examining emerging challenges and proposing actionable directions
for future research. We maintain a GitHub repository for ongoing curating
datasets and open-source implementations:
https://github.com/sevenolu7/Malicious-URL-Detection-Open-Source/tree/master.

</details>


### [150] [Seeking Flat Minima over Diverse Surrogates for Improved Adversarial Transferability: A Theoretical Framework and Algorithmic Instantiation](https://arxiv.org/abs/2504.16474)
*Meixi Zheng,Kehan Wu,Yanbo Fan,Rui Huang,Baoyuan Wu*

Main category: cs.CR

TL;DR: 论文提出了一种基于理论的可转移性边界，用于提升黑盒对抗攻击的迁移性，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有黑盒对抗攻击方法缺乏理论支持，作者希望通过理论分析填补这一空白，并为设计更有效的攻击算法提供指导。

Method: 通过优化对抗样本在代理模型集上的平坦性，并控制代理与目标模型间的对抗模型差异，提出了一种新的攻击框架（DRAP）。

Result: 实验表明，该方法在NIPS2017和CIFAR-10数据集上对多种目标模型具有显著提升的迁移性。

Conclusion: 理论分析和实验结果共同验证了所提方法的有效性，为对抗攻击的可转移性提供了新的理论基础和实用框架。

Abstract: The transfer-based black-box adversarial attack setting poses the challenge
of crafting an adversarial example (AE) on known surrogate models that remain
effective against unseen target models. Due to the practical importance of this
task, numerous methods have been proposed to address this challenge. However,
most previous methods are heuristically designed and intuitively justified,
lacking a theoretical foundation. To bridge this gap, we derive a novel
transferability bound that offers provable guarantees for adversarial
transferability. Our theoretical analysis has the advantages of \textit{(i)}
deepening our understanding of previous methods by building a general attack
framework and \textit{(ii)} providing guidance for designing an effective
attack algorithm. Our theoretical results demonstrate that optimizing AEs
toward flat minima over the surrogate model set, while controlling the
surrogate-target model shift measured by the adversarial model discrepancy,
yields a comprehensive guarantee for AE transferability. The results further
lead to a general transfer-based attack framework, within which we observe that
previous methods consider only partial factors contributing to the
transferability. Algorithmically, inspired by our theoretical results, we first
elaborately construct the surrogate model set in which models exhibit diverse
adversarial vulnerabilities with respect to AEs to narrow an instantiated
adversarial model discrepancy. Then, a \textit{model-Diversity-compatible
Reverse Adversarial Perturbation} (DRAP) is generated to effectively promote
the flatness of AEs over diverse surrogate models to improve transferability.
Extensive experiments on NIPS2017 and CIFAR-10 datasets against various target
models demonstrate the effectiveness of our proposed attack.

</details>


### [151] [MAYA: Addressing Inconsistencies in Generative Password Guessing through a Unified Benchmark](https://arxiv.org/abs/2504.16651)
*William Corrias,Fabio De Gaspari,Dorjan Hitaj,Luigi V. Mancini*

Main category: cs.CR

TL;DR: MAYA是一个统一的密码基准测试框架，用于评估生成式密码猜测模型，发现序列模型在生成复杂密码方面表现最佳。


<details>
  <summary>Details</summary>
Motivation: 生成式模型在密码猜测领域的潜力尚未被充分理解，缺乏标准化评估方法。

Method: 引入MAYA框架，通过高级测试场景和真实密码数据集评估六种先进模型。

Result: 序列模型在生成复杂密码时表现最佳，多模型攻击效果优于单一模型。

Conclusion: MAYA为密码生成技术提供了标准化评估工具，促进进一步研究。

Abstract: The rapid evolution of generative models has led to their integration across
various fields, including password guessing, aiming to generate passwords that
resemble human-created ones in complexity, structure, and patterns. Despite
generative model's promise, inconsistencies in prior research and a lack of
rigorous evaluation have hindered a comprehensive understanding of their true
potential. In this paper, we introduce MAYA, a unified, customizable,
plug-and-play password benchmarking framework. MAYA provides a standardized
approach for evaluating generative password-guessing models through a rigorous
set of advanced testing scenarios and a collection of eight real-life password
datasets. Using MAYA, we comprehensively evaluate six state-of-the-art
approaches, which have been re-implemented and adapted to ensure
standardization, for a total of over 15,000 hours of computation. Our findings
indicate that these models effectively capture different aspects of human
password distribution and exhibit strong generalization capabilities. However,
their effectiveness varies significantly with long and complex passwords.
Through our evaluation, sequential models consistently outperform other
generative architectures and traditional password-guessing tools, demonstrating
unique capabilities in generating accurate and complex guesses. Moreover,
models learn and generate different password distributions, enabling a
multi-model attack that outperforms the best individual model. By releasing
MAYA, we aim to foster further research, providing the community with a new
tool to consistently and reliably benchmark password-generation techniques. Our
framework is publicly available at
https://github.com/williamcorrias/MAYA-Password-Benchmarking

</details>


### [152] [Building A Secure Agentic AI Application Leveraging A2A Protocol](https://arxiv.org/abs/2504.16902)
*Idan Habler,Ken Huang,Vineeth Sai Narajala,Prashant Kulkarni*

Main category: cs.CR

TL;DR: 本文对Google的Agent2Agent（A2A）协议进行了全面的安全分析，旨在为开发者提供构建安全可靠的AI代理系统的实用指南。


<details>
  <summary>Details</summary>
Motivation: 随着AI代理系统从简单工作流发展为复杂的多代理协作，确保A2A协议的安全实现成为关键。

Method: 利用MAESTRO框架进行主动威胁建模，分析A2A协议的安全性，重点关注代理卡管理、任务执行完整性和认证方法。

Result: 提出了安全的开发方法和架构最佳实践，并探讨了A2A与模型上下文协议（MCP）的协同作用。

Conclusion: 本文为开发者提供了构建安全可靠的下一代AI代理应用所需的知识和实用指导。

Abstract: As Agentic AI systems evolve from basic workflows to complex multi agent
collaboration, robust protocols such as Google's Agent2Agent (A2A) become
essential enablers. To foster secure adoption and ensure the reliability of
these complex interactions, understanding the secure implementation of A2A is
essential. This paper addresses this goal by providing a comprehensive security
analysis centered on the A2A protocol. We examine its fundamental elements and
operational dynamics, situating it within the framework of agent communication
development. Utilizing the MAESTRO framework, specifically designed for AI
risks, we apply proactive threat modeling to assess potential security issues
in A2A deployments, focusing on aspects such as Agent Card management, task
execution integrity, and authentication methodologies.
  Based on these insights, we recommend practical secure development
methodologies and architectural best practices designed to build resilient and
effective A2A systems. Our analysis also explores how the synergy between A2A
and the Model Context Protocol (MCP) can further enhance secure
interoperability. This paper equips developers and architects with the
knowledge and practical guidance needed to confidently leverage the A2A
protocol for building robust and secure next generation agentic applications.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [153] [Behavior of prediction performance metrics with rare events](https://arxiv.org/abs/2504.16185)
*Emily Minus,R. Yates Coley,Susan M. Shortreed,Brian D. Williamson*

Main category: stat.ML

TL;DR: AUC在罕见事件预测中可能不稳定，研究发现其偏差和方差主要由最小类别大小而非事件率决定。其他性能指标如敏感性和特异性也受不同因素影响。


<details>
  <summary>Details</summary>
Motivation: 评估AUC在罕见事件预测中的可靠性，探讨其偏差和方差的驱动因素。

Method: 通过模拟研究，分析AUC及其他预测性能指标（如PPV、准确性、敏感性和特异性）的行为。

Result: AUC的可靠性取决于事件总数而非事件率；敏感性和特异性分别受事件数和非事件数影响；PPV和准确性则与事件率相关。

Conclusion: 在事件总数较大的情况下，AUC在罕见事件预测中仍可靠。

Abstract: Area under the receiving operator characteristic curve (AUC) is commonly
reported alongside binary prediction models. However, there are concerns that
AUC might be a misleading measure of prediction performance in the rare event
setting. This setting is common since many events of clinical importance are
rare events. We conducted a simulation study to determine when or whether AUC
is unstable in the rare event setting. Specifically, we aimed to determine
whether the bias and variance of AUC are driven by the number of events or the
event rate. We also investigated the behavior of other commonly used measures
of prediction performance, including positive predictive value, accuracy,
sensitivity, and specificity. Our results indicate that poor AUC behavior -- as
measured by empirical bias, variability of cross-validated AUC estimates, and
empirical coverage of confidence intervals -- is driven by the minimum class
size, not event rate. Performance of sensitivity is driven by the number of
events, while that of specificity is driven by the number of non-events. Other
measures, including positive predictive value and accuracy, depend on the event
rate even in large samples. AUC is reliable in the rare event setting provided
that the total number of events is moderately large.

</details>


### [154] [Covariate-dependent Graphical Model Estimation via Neural Networks with Statistical Guarantees](https://arxiv.org/abs/2504.16356)
*Jiahe Lin,Yikai Zhang,George Michailidis*

Main category: stat.ML

TL;DR: 本文提出了一种基于深度神经网络的图结构估计方法，适用于协变量依赖的场景，具有灵活的功能依赖性和非高斯性假设。


<details>
  <summary>Details</summary>
Motivation: 研究协变量依赖的图结构估计问题，以应对现实应用中复杂的依赖关系。

Method: 采用深度神经网络方法，允许协变量的灵活功能依赖，并在非高斯假设下拟合数据。

Result: 在合成数据和真实数据（神经科学和金融）上验证了方法的有效性，并提供了可解释的结果。

Conclusion: 该方法在理论和实际应用中均表现良好，适用于协变量依赖的图结构估计。

Abstract: Graphical models are widely used in diverse application domains to model the
conditional dependencies amongst a collection of random variables. In this
paper, we consider settings where the graph structure is covariate-dependent,
and investigate a deep neural network-based approach to estimate it. The method
allows for flexible functional dependency on the covariate, and fits the data
reasonably well in the absence of a Gaussianity assumption. Theoretical results
with PAC guarantees are established for the method, under assumptions commonly
used in an Empirical Risk Minimization framework. The performance of the
proposed method is evaluated on several synthetic data settings and benchmarked
against existing approaches. The method is further illustrated on real datasets
involving data from neuroscience and finance, respectively, and produces
interpretable results.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [155] [A CNN-based Local-Global Self-Attention via Averaged Window Embeddings for Hierarchical ECG Analysis](https://arxiv.org/abs/2504.16097)
*Arthur Buzelin,Pedro Robles Dutenhefner,Turi Rezende,Luisa G. Porfirio,Pedro Bento,Yan Aquino,Jose Fernandes,Caio Santana,Gabriela Miana,Gisele L. Pappa,Antonio Ribeiro,Wagner Meira Jr*

Main category: eess.SP

TL;DR: 提出了一种结合局部和全局注意力机制的LGA-ECG模型，用于提升心电图分析的准确性。


<details>
  <summary>Details</summary>
Motivation: 心血管疾病是全球主要死因，需要高效的心电图诊断工具。传统Transformer模型难以捕捉局部形态特征，影响ECG分析的准确性。

Method: 提出LGA-ECG模型，结合卷积归纳偏置和全局自注意力机制，通过卷积窗口提取局部特征，同时利用全局注意力建模上下文。

Result: 在CODE-15数据集上，LGA-ECG优于现有模型，验证了局部-全局注意力策略的有效性。

Conclusion: LGA-ECG通过捕捉ECG信号的层次时间依赖和形态模式，展示了其在临床自动化ECG分类中的潜力。

Abstract: Cardiovascular diseases remain the leading cause of global mortality,
emphasizing the critical need for efficient diagnostic tools such as
electrocardiograms (ECGs). Recent advancements in deep learning, particularly
transformers, have revolutionized ECG analysis by capturing detailed waveform
features as well as global rhythm patterns. However, traditional transformers
struggle to effectively capture local morphological features that are critical
for accurate ECG interpretation. We propose a novel Local-Global Attention ECG
model (LGA-ECG) to address this limitation, integrating convolutional inductive
biases with global self-attention mechanisms. Our approach extracts queries by
averaging embeddings obtained from overlapping convolutional windows, enabling
fine-grained morphological analysis, while simultaneously modeling global
context through attention to keys and values derived from the entire sequence.
Experiments conducted on the CODE-15 dataset demonstrate that LGA-ECG
outperforms state-of-the-art models and ablation studies validate the
effectiveness of the local-global attention strategy. By capturing the
hierarchical temporal dependencies and morphological patterns in ECG signals,
this new design showcases its potential for clinical deployment with robust
automated ECG classification.

</details>


### [156] [Two-Timescale Joint Transmit and Pinching Beamforming for Pinching-Antenna Systems](https://arxiv.org/abs/2504.16099)
*Luyuan Zhang,Xidong Mu,An Liu,Yuanwei Liu*

Main category: eess.SP

TL;DR: 提出了一种基于PASS的双时间尺度联合传输和夹持波束成形设计，通过分解问题为短期和长期子问题，显著提升了多用户系统的和速率。


<details>
  <summary>Details</summary>
Motivation: 为了解决PASS技术中灵活天线系统的波束成形问题，以最大化多用户下行链路的和速率。

Method: 采用原始对偶分解法将问题分解为短期传输波束成形和长期夹持波束成形子问题，分别通过KKT引导的双学习方法和随机逐次凸近似方法解决。

Result: 仿真结果表明，所提算法相比基线方法有显著性能提升。

Conclusion: 双时间尺度算法在PASS系统中有效提升了性能，为灵活天线技术提供了实用解决方案。

Abstract: Pinching antenna systems (PASS) have been proposed as a revolutionary
flexible antenna technology which facilitates line-of-sight links via numerous
low-cost pinching antennas with adjustable activation positions over
waveguides. This letter proposes a two-timescale joint transmit and pinching
beamforming design for the maximization of sum rate of a PASS-based downlink
multi-user multiple input single output system. A primal dual decomposition
method is developed to decouple the two-timescale problem into two
sub-problems: 1) A Karush-Kuhn-Tucker-guided dual learning-based approach is
proposed to solve the short-term transmit beamforming design sub-problem; 2)
The long-term pinching beamforming design sub-problem is tackled by adopting a
stochastic successive convex approximation method. Simulation results
demonstrate that the proposed two-timescale algorithm achieves a significant
performance gain compared to other baselines.

</details>


### [157] [Towards Accurate Forecasting of Renewable Energy : Building Datasets and Benchmarking Machine Learning Models for Solar and Wind Power in France](https://arxiv.org/abs/2504.16100)
*Eloi Lindas,Yannig Goude,Philippe Ciais*

Main category: eess.SP

TL;DR: 该研究提出了一种利用机器学习模型预测法国太阳能和风能发电量的方法，结合空间天气数据和发电站点信息，结果显示神经网络优于传统树模型。


<details>
  <summary>Details</summary>
Motivation: 准确预测可再生能源发电对电网稳定和电价预测至关重要，现有方法通常间接且未充分利用空间数据。

Method: 使用ERA5天气数据、发电站点容量和位置、电价等特征，训练机器学习模型，探索了三种处理空间天气数据的方法。

Result: 神经网络表现优于树模型，误差范围为4%至10%，与单站点模型相当。

Conclusion: 该方法展示了区域电力供应预测的潜力，尤其是神经网络在处理空间数据和容量增长时的优势。

Abstract: Accurate prediction of non-dispatchable renewable energy sources is essential
for grid stability and price prediction. Regional power supply forecasts are
usually indirect through a bottom-up approach of plant-level forecasts,
incorporate lagged power values, and do not use the potential of spatially
resolved data. This study presents a comprehensive methodology for predicting
solar and wind power production at country scale in France using machine
learning models trained with spatially explicit weather data combined with
spatial information about production sites capacity. A dataset is built
spanning from 2012 to 2023, using daily power production data from RTE (the
national grid operator) as the target variable, with daily weather data from
ERA5, production sites capacity and location, and electricity prices as input
features. Three modeling approaches are explored to handle spatially resolved
weather data: spatial averaging over the country, dimension reduction through
principal component analysis, and a computer vision architecture to exploit
complex spatial relationships. The study benchmarks state-of-the-art machine
learning models as well as hyperparameter tuning approaches based on
cross-validation methods on daily power production data. Results indicate that
cross-validation tailored to time series is best suited to reach low error. We
found that neural networks tend to outperform traditional tree-based models,
which face challenges in extrapolation due to the increasing renewable capacity
over time. Model performance ranges from 4% to 10% in nRMSE for midterm
horizon, achieving similar error metrics to local models established at a
single-plant level, highlighting the potential of these methods for regional
power supply forecasting.

</details>


### [158] [xLSTM-ECG: Multi-label ECG Classification via Feature Fusion with xLSTM](https://arxiv.org/abs/2504.16101)
*Lei Kang,Xuanshuo Fu,Javier Vazquez-Corral,Ernest Valveny,Dimosthenis Karatzas*

Main category: eess.SP

TL;DR: 提出了一种基于xLSTM网络的ECG信号多标签分类方法，显著提高了诊断准确性。


<details>
  <summary>Details</summary>
Motivation: 心血管疾病是全球主要死因，ECG手动解读耗时且易错，需高效准确工具。

Method: 使用STFT将ECG信号转换到频域，xLSTM网络捕获局部和全局特征，适用于12导联ECG。

Result: 在PTB-XL数据集上表现优异，Georgia数据集验证了其鲁棒性和高效性。

Conclusion: xLSTM-ECG显著提升分类准确性，推动临床诊断进步，代码将公开。

Abstract: Cardiovascular diseases (CVDs) remain the leading cause of mortality
worldwide, highlighting the critical need for efficient and accurate diagnostic
tools. Electrocardiograms (ECGs) are indispensable in diagnosing various heart
conditions; however, their manual interpretation is time-consuming and
error-prone. In this paper, we propose xLSTM-ECG, a novel approach that
leverages an extended Long Short-Term Memory (xLSTM) network for multi-label
classification of ECG signals, using the PTB-XL dataset. To the best of our
knowledge, this work represents the first design and application of xLSTM
modules specifically adapted for multi-label ECG classification. Our method
employs a Short-Time Fourier Transform (STFT) to convert time-series ECG
waveforms into the frequency domain, thereby enhancing feature extraction. The
xLSTM architecture is specifically tailored to address the complexities of
12-lead ECG recordings by capturing both local and global signal features.
Comprehensive experiments on the PTB-XL dataset reveal that our model achieves
strong multi-label classification performance, while additional tests on the
Georgia 12-Lead dataset underscore its robustness and efficiency. This approach
significantly improves ECG classification accuracy, thereby advancing clinical
diagnostics and patient care. The code will be publicly available upon
acceptance.

</details>


### [159] [A Self-supervised Learning Method for Raman Spectroscopy based on Masked Autoencoders](https://arxiv.org/abs/2504.16130)
*Pengju Ren,Ri-gui Zhou,Yaochong Li*

Main category: eess.SP

TL;DR: 提出了一种基于掩码自编码器（SMAE）的自监督学习方法，用于拉曼光谱分析，解决了标注数据不足的问题，并在光谱去噪和细菌分类中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有监督学习方法依赖大量标注数据，而标注成本高且数据有限，导致性能下降。

Method: 采用自监督学习范式SMAE，通过掩码和重构光谱信息学习特征，无需标注数据。

Result: SMAE在去噪和细菌分类任务中表现优异，聚类准确率超80%，微调后识别准确率达83.90%。

Conclusion: SMAE为拉曼光谱分析提供了一种高效的自监督学习方案，性能媲美监督学习方法。

Abstract: Raman spectroscopy serves as a powerful and reliable tool for analyzing the
chemical information of substances. The integration of Raman spectroscopy with
deep learning methods enables rapid qualitative and quantitative analysis of
materials. Most existing approaches adopt supervised learning methods. Although
supervised learning has achieved satisfactory accuracy in spectral analysis, it
is still constrained by costly and limited well-annotated spectral datasets for
training. When spectral annotation is challenging or the amount of annotated
data is insufficient, the performance of supervised learning in spectral
material identification declines. In order to address the challenge of feature
extraction from unannotated spectra, we propose a self-supervised learning
paradigm for Raman Spectroscopy based on a Masked AutoEncoder, termed SMAE.
SMAE does not require any spectral annotations during pre-training. By randomly
masking and then reconstructing the spectral information, the model learns
essential spectral features. The reconstructed spectra exhibit certain
denoising properties, improving the signal-to-noise ratio (SNR) by more than
twofold. Utilizing the network weights obtained from masked pre-training, SMAE
achieves clustering accuracy of over 80% for 30 classes of isolated bacteria in
a pathogenic bacterial dataset, demonstrating significant improvements compared
to classical unsupervised methods and other state-of-the-art deep clustering
methods. After fine-tuning the network with a limited amount of annotated data,
SMAE achieves an identification accuracy of 83.90% on the test set, presenting
competitive performance against the supervised ResNet (83.40%).

</details>


### [160] [A Non-Invasive Load Monitoring Method for Edge Computing Based on MobileNetV3 and Dynamic Time Regulation](https://arxiv.org/abs/2504.16142)
*Hangxu Liu,Yaojie Sun,Yu Wang*

Main category: eess.SP

TL;DR: 本文提出了一种基于动态时间规整（DTW）算法的非侵入式负载监测（NILM）方法，通过融合时频域特征，显著提升了负载分解精度，并在边缘MCU上实现了95%的识别准确率，同时优化了计算和存储开销。


<details>
  <summary>Details</summary>
Motivation: 现有基于机器学习和深度学习的NILM方法计算和存储成本高，难以在资源受限的边缘MCU上部署，因此需要一种高效且低成本的解决方案。

Method: 提出了一种时频域动态时间规整（DTW）算法，并系统比较了六种机器学习技术在家用电场景中的性能。优化了频域特征提取过程。

Result: 在边缘MCU上实现了95%的识别准确率，运行时间减少55.55%，存储开销降低约34.6%。

Conclusion: 未来研究将聚焦于消除电压互感器设计以降低成本，为NILM的实际应用提供更具性价比的解决方案。

Abstract: In recent years, non-intrusive load monitoring (NILM) technology has
attracted much attention in the related research field by virtue of its unique
advantage of utilizing single meter data to achieve accurate decomposition of
device-level energy consumption. Cutting-edge methods based on machine learning
and deep learning have achieved remarkable results in load decomposition
accuracy by fusing time-frequency domain features. However, these methods
generally suffer from high computational costs and huge memory requirements,
which become the main obstacles for their deployment on resource-constrained
microcontroller units (MCUs). To address these challenges, this study proposes
an innovative Dynamic Time Warping (DTW) algorithm in the time-frequency domain
and systematically compares and analyzes the performance of six machine
learning techniques in home electricity scenarios. Through complete
experimental validation on edge MCUs, this scheme successfully achieves a
recognition accuracy of 95%. Meanwhile, this study deeply optimizes the
frequency domain feature extraction process, which effectively reduces the
running time by 55.55% and the storage overhead by about 34.6%. The algorithm
performance will be further optimized in future research work. Considering that
the elimination of voltage transformer design can significantly reduce the
cost, the subsequent research will focus on this direction, and is committed to
providing more cost-effective solutions for the practical application of NILM,
and providing a solid theoretical foundation and feasible technical paths for
the design of efficient NILM systems in edge computing environments.

</details>


### [161] [SeizureFormer: A Transformer Model for IEA-Based Seizure Risk Forecasting](https://arxiv.org/abs/2504.16098)
*Tianning Feng,Junting Ni,Ezequiel Gleichgerrcht,Wei Jin*

Main category: eess.SP

TL;DR: SeizureFormer是一种基于Transformer的模型，用于长期癫痫风险预测，利用临床相关特征和多种技术模块，表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 开发一种能够利用临床相关特征和长期癫痫周期数据的模型，以提升癫痫风险预测的准确性和泛化能力。

Method: 结合CNN-based patch embedding、multi-head self-attention和squeeze-and-excitation模块，建模短期动态和长期癫痫周期。

Result: 在五名患者和多个预测窗口（1至14天）中，SeizureFormer的平均ROC AUC为79.44%，PR AUC为76.29%，优于基线方法。

Conclusion: SeizureFormer为个性化癫痫管理提供了可解释且稳健的预测工具，支持未来临床集成。

Abstract: We present SeizureFormer, a Transformer-based model for long-term seizure
risk forecasting using interictal epileptiform activity (IEA) surrogate
biomarkers and long episode (LE) biomarkers from responsive neurostimulation
(RNS) systems. Unlike raw scalp EEG-based models, SeizureFormer leverages
structured, clinically relevant features and integrates CNN-based patch
embedding, multi-head self-attention, and squeeze-and-excitation blocks to
model both short-term dynamics and long-term seizure cycles. Tested across five
patients and multiple prediction windows (1 to 14 days), SeizureFormer achieved
state-of-the-art performance with mean ROC AUC of 79.44 percent and mean PR AUC
of 76.29 percent. Compared to statistical, machine learning, and deep learning
baselines, it demonstrates enhanced generalizability and seizure risk
forecasting performance under class imbalance. This work supports future
clinical integration of interpretable and robust seizure forecasting tools for
personalized epilepsy management.

</details>


### [162] [A Statistical Approach for Synthetic EEG Data Generation](https://arxiv.org/abs/2504.16143)
*Gideon Vos,Maryam Ebrahimpour,Liza van Eijk,Zoltan Sarnyai,Mostafa Rahimi Azghadi*

Main category: eess.SP

TL;DR: 该研究提出了一种结合相关分析和随机采样的方法，用于生成高质量、逼真的合成EEG数据，以解决真实EEG数据收集成本高的问题。


<details>
  <summary>Details</summary>
Motivation: EEG数据对心理健康诊断至关重要，但大规模收集成本高且耗时。合成数据生成可以扩充数据集，但保留情感和心理健康信号的高质量合成EEG仍具挑战性。

Method: 通过相关分析EEG频段间的相互依赖关系，并基于此结构通过随机采样生成合成样本。保留与真实数据相关性高的样本，并通过分布分析和分类任务评估。

Result: 合成数据在统计和结构特性上与原始EEG高度匹配，分类任务中随机森林模型无法区分合成与真实数据，表明合成数据的高保真度。

Conclusion: 该方法为扩充EEG数据集提供了一种可扩展且保护隐私的解决方案，有助于心理健康研究中更高效的模型训练。

Abstract: Electroencephalogram (EEG) data is crucial for diagnosing mental health
conditions but is costly and time-consuming to collect at scale. Synthetic data
generation offers a promising solution to augment datasets for machine learning
applications. However, generating high-quality synthetic EEG that preserves
emotional and mental health signals remains challenging. This study proposes a
method combining correlation analysis and random sampling to generate realistic
synthetic EEG data.
  We first analyze interdependencies between EEG frequency bands using
correlation analysis. Guided by this structure, we generate synthetic samples
via random sampling. Samples with high correlation to real data are retained
and evaluated through distribution analysis and classification tasks. A Random
Forest model trained to distinguish synthetic from real EEG performs at chance
level, indicating high fidelity.
  The generated synthetic data closely match the statistical and structural
properties of the original EEG, with similar correlation coefficients and no
significant differences in PERMANOVA tests. This method provides a scalable,
privacy-preserving approach for augmenting EEG datasets, enabling more
efficient model training in mental health research.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [163] [MARFT: Multi-Agent Reinforcement Fine-Tuning](https://arxiv.org/abs/2504.16129)
*Junwei Liao,Muning Wen,Jun Wang,Weinan Zhang*

Main category: cs.MA

TL;DR: 论文提出了一种名为多智能体强化微调（MARFT）的新范式，用于优化基于LLM的多智能体系统（LaMAS），并提供了一个通用的算法框架和开源实现。


<details>
  <summary>Details</summary>
Motivation: 现有研究在将多智能体强化学习（MARL）应用于LaMAS时面临挑战，因此需要一种新的方法来适应LaMAS的特性。

Method: 论文提出了MARFT框架，包括核心算法和实现策略，并对比了MARL与MARFT的关键区别。

Result: 论文提供了一个开源实现，并探讨了MARFT在实际应用中的潜力和挑战。

Conclusion: MARFT为研究人员提供了一个路线图，旨在推动LaMAS向更具适应性和鲁棒性的方向发展。

Abstract: LLM-based Multi-Agent Systems have demonstrated remarkable capabilities in
addressing complex, agentic tasks requiring multifaceted reasoning and
collaboration, from generating high-quality presentation slides to conducting
sophisticated scientific research. Meanwhile, RL has been widely recognized for
its effectiveness in enhancing agent intelligence, but limited research has
investigated the fine-tuning of LaMAS using foundational RL techniques.
Moreover, the direct application of MARL methodologies to LaMAS introduces
significant challenges, stemming from the unique characteristics and mechanisms
inherent to LaMAS. To address these challenges, this article presents a
comprehensive study of LLM-based MARL and proposes a novel paradigm termed
Multi-Agent Reinforcement Fine-Tuning (MARFT). We introduce a universal
algorithmic framework tailored for LaMAS, outlining the conceptual foundations,
key distinctions, and practical implementation strategies. We begin by
reviewing the evolution from RL to Reinforcement Fine-Tuning, setting the stage
for a parallel analysis in the multi-agent domain. In the context of LaMAS, we
elucidate critical differences between MARL and MARFT. These differences
motivate a transition toward a novel, LaMAS-oriented formulation of RFT.
Central to this work is the presentation of a robust and scalable MARFT
framework. We detail the core algorithm and provide a complete, open-source
implementation to facilitate adoption and further research. The latter sections
of the paper explore real-world application perspectives and opening challenges
in MARFT. By bridging theoretical underpinnings with practical methodologies,
this work aims to serve as a roadmap for researchers seeking to advance MARFT
toward resilient and adaptive solutions in agentic systems. Our implementation
of the proposed framework is publicly available at:
https://github.com/jwliao-ai/MARFT.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [164] [Heterogeneous networks in drug-target interaction prediction](https://arxiv.org/abs/2504.16152)
*Mohammad Molaee,Nasrollah Moghadam Charkari*

Main category: q-bio.BM

TL;DR: 综述了2020-2024年基于图机器学习的药物-靶点相互作用预测方法，包括框架、贡献、数据集和代码，并讨论了未来挑战。


<details>
  <summary>Details</summary>
Motivation: 药物发现耗时长、成本高，计算预测可缩小实验范围，提高效率。

Method: 综述了图机器学习方法在药物-靶点相互作用预测中的应用，包括框架、贡献、数据集和代码。

Result: 总结了现有方法的成果，并提供了常用数据集和性能评估指标。

Conclusion: 讨论了未来挑战和需进一步探索的关键领域。

Abstract: Drug discovery requires a tremendous amount of time and cost. Computational
drug-target interaction prediction, a significant part of this process, can
reduce these requirements by narrowing the search space for wet lab
experiments. In this survey, we provide comprehensive details of graph machine
learning-based methods in predicting drug-target interaction, as they have
shown promising results in this field. These details include the overall
framework, main contribution, datasets, and their source codes. The selected
papers were mainly published from 2020 to 2024. Prior to discussing papers, we
briefly introduce the datasets commonly used with these methods and
measurements to assess their performance. Finally, future challenges and some
crucial areas that need to be explored are discussed.

</details>


### [165] [The Dance of Atoms-De Novo Protein Design with Diffusion Model](https://arxiv.org/abs/2504.16479)
*Yujie Qin,Ming He,Changyong Yu,Ming Ni,Xian Liu,Xiaochen Bo*

Main category: q-bio.BM

TL;DR: 生成式AI模型，尤其是扩散模型，显著提升了蛋白质从头设计的成功率，降低了实验成本。RFDiffusion等模型在任务中表现优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 利用积累的蛋白质结构和序列数据，结合生成式AI技术，突破传统蛋白质设计的局限性。

Method: 采用扩散模型生成蛋白质骨架和序列，对比不同模型的优缺点。

Result: 扩散模型在25项蛋白质设计任务中表现优异，成功率高。

Conclusion: 扩散模型在蛋白质设计中前景广阔，未来需进一步优化和探索。

Abstract: The de novo design of proteins refers to creating proteins with specific
structures and functions that do not naturally exist. In recent years, the
accumulation of high-quality protein structure and sequence data and
technological advancements have paved the way for the successful application of
generative artificial intelligence (AI) models in protein design. These models
have surpassed traditional approaches that rely on fragments and
bioinformatics. They have significantly enhanced the success rate of de novo
protein design, and reduced experimental costs, leading to breakthroughs in the
field. Among various generative AI models, diffusion models have yielded the
most promising results in protein design. In the past two to three years, more
than ten protein design models based on diffusion models have emerged. Among
them, the representative model, RFDiffusion, has demonstrated success rates in
25 protein design tasks that far exceed those of traditional methods, and other
AI-based approaches like RFjoint and hallucination. This review will
systematically examine the application of diffusion models in generating
protein backbones and sequences. We will explore the strengths and limitations
of different models, summarize successful cases of protein design using
diffusion models, and discuss future development directions.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [166] [ManipDreamer: Boosting Robotic Manipulation World Model with Action Tree and Visual Guidance](https://arxiv.org/abs/2504.16464)
*Ying Li,Xiaobao Wei,Xiaowei Chi,Yuming Li,Zhongyu Zhao,Hao Wang,Ningning Ma,Ming Lu,Shanghang Zhang*

Main category: cs.RO

TL;DR: ManipDreamer提出了一种基于动作树和视觉引导的高级世界模型，显著提升了机器人操作视频合成的指令跟随能力和视觉质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法如RoboDreamer虽通过语言分解实现指令跟随，但忽略了指令原语间的关系及视觉引导（如深度和语义信息），导致视觉质量和指令跟随效果受限。

Method: ManipDreamer将指令表示为动作树，为节点分配嵌入以捕捉原语关系，并引入视觉引导适配器以结合深度和语义信息，增强视频生成的时空一致性。

Result: 在机器人操作基准测试中，ManipDreamer显著提升视频质量指标（PSNR、SSIM、Flow Error）和任务成功率（平均提升2.5%）。

Conclusion: ManipDreamer通过动作树和视觉引导的协同设计，有效解决了指令跟随和视觉质量的挑战，为机器人操作视频合成提供了新方向。

Abstract: While recent advancements in robotic manipulation video synthesis have shown
promise, significant challenges persist in ensuring effective
instruction-following and achieving high visual quality. Recent methods, like
RoboDreamer, utilize linguistic decomposition to divide instructions into
separate lower-level primitives, conditioning the world model on these
primitives to achieve compositional instruction-following. However, these
separate primitives do not consider the relationships that exist between them.
Furthermore, recent methods neglect valuable visual guidance, including depth
and semantic guidance, both crucial for enhancing visual quality. This paper
introduces ManipDreamer, an advanced world model based on the action tree and
visual guidance. To better learn the relationships between instruction
primitives, we represent the instruction as the action tree and assign
embeddings to tree nodes, each instruction can acquire its embeddings by
navigating through the action tree. The instruction embeddings can be used to
guide the world model. To enhance visual quality, we combine depth and semantic
guidance by introducing a visual guidance adapter compatible with the world
model. This visual adapter enhances both the temporal and physical consistency
of video generation. Based on the action tree and visual guidance, ManipDreamer
significantly boosts the instruction-following ability and visual quality.
Comprehensive evaluations on robotic manipulation benchmarks reveal that
ManipDreamer achieves large improvements in video quality metrics in both seen
and unseen tasks, with PSNR improved from 19.55 to 21.05, SSIM improved from
0.7474 to 0.7982 and reduced Flow Error from 3.506 to 3.201 in unseen tasks,
compared to the recent RoboDreamer model. Additionally, our method increases
the success rate of robotic manipulation tasks by 2.5% in 6 RLbench tasks on
average.

</details>


### [167] [PCF-Grasp: Converting Point Completion to Geometry Feature to Enhance 6-DoF Grasp](https://arxiv.org/abs/2504.16320)
*Yaofeng Cheng,Fusheng Zha,Wei Guo,Pengfei Wang,Chao Zeng,Lining Sun,Chenguang Yang*

Main category: cs.RO

TL;DR: 提出了一种基于点云补全的6自由度抓取框架，通过将补全后的点云作为形状特征训练抓取网络，并结合评分过滤器提升实际抓取成功率。


<details>
  <summary>Details</summary>
Motivation: 现有基于单视角点云的抓取方法因几何信息不完整导致抓取精度低，受人类利用几何经验估计物体形状的启发，提出补全点云以提升抓取效率。

Method: 将点云补全结果作为形状特征训练6自由度抓取网络，并引入评分过滤器筛选可执行的抓取方案。

Result: 实验表明，该方法生成的抓取方案更准确，实际抓取成功率比现有方法高17.8%。

Conclusion: 通过点云补全和评分过滤器的结合，显著提升了机器人抓取的准确性和实际执行效果。

Abstract: The 6-Degree of Freedom (DoF) grasp method based on point clouds has shown
significant potential in enabling robots to grasp target objects. However, most
existing methods are based on the point clouds (2.5D points) generated from
single-view depth images. These point clouds only have one surface side of the
object providing incomplete geometry information, which mislead the grasping
algorithm to judge the shape of the target object, resulting in low grasping
accuracy. Humans can accurately grasp objects from a single view by leveraging
their geometry experience to estimate object shapes. Inspired by humans, we
propose a novel 6-DoF grasping framework that converts the point completion
results as object shape features to train the 6-DoF grasp network. Here, point
completion can generate approximate complete points from the 2.5D points
similar to the human geometry experience, and converting it as shape features
is the way to utilize it to improve grasp efficiency. Furthermore, due to the
gap between the network generation and actual execution, we integrate a score
filter into our framework to select more executable grasp proposals for the
real robot. This enables our method to maintain a high grasp quality in any
camera viewpoint. Extensive experiments demonstrate that utilizing complete
point features enables the generation of significantly more accurate grasp
proposals and the inclusion of a score filter greatly enhances the credibility
of real-world robot grasping. Our method achieves a 17.8\% success rate higher
than the state-of-the-art method in real-world experiments.

</details>


### [168] [Offline Robotic World Model: Learning Robotic Policies without a Physics Simulator](https://arxiv.org/abs/2504.16680)
*Chenhao Li,Andreas Krause,Marco Hutter*

Main category: cs.RO

TL;DR: RWM-O是一种基于模型的离线强化学习方法，通过显式估计认知不确定性提升策略学习，减少对物理模拟器的依赖，提高泛化能力和安全性。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习在机器人控制中面临分布偏移问题，而现有基于模型的方法缺乏鲁棒的不确定性估计，导致误差累积。

Method: 提出RWM-O方法，通过估计认知不确定性并将其融入策略优化，惩罚不可靠的转移，减少对模型错误的过拟合。

Result: 实验表明，RWM-O提高了泛化能力和安全性，支持仅从真实数据中学习策略。

Conclusion: RWM-O为机器人领域提供了可扩展且数据高效的强化学习解决方案。

Abstract: Reinforcement Learning (RL) has demonstrated impressive capabilities in
robotic control but remains challenging due to high sample complexity, safety
concerns, and the sim-to-real gap. While offline RL eliminates the need for
risky real-world exploration by learning from pre-collected data, it suffers
from distributional shift, limiting policy generalization. Model-Based RL
(MBRL) addresses this by leveraging predictive models for synthetic rollouts,
yet existing approaches often lack robust uncertainty estimation, leading to
compounding errors in offline settings. We introduce Offline Robotic World
Model (RWM-O), a model-based approach that explicitly estimates epistemic
uncertainty to improve policy learning without reliance on a physics simulator.
By integrating these uncertainty estimates into policy optimization, our
approach penalizes unreliable transitions, reducing overfitting to model errors
and enhancing stability. Experimental results show that RWM-O improves
generalization and safety, enabling policy learning purely from real-world data
and advancing scalable, data-efficient RL for robotics.

</details>


### [169] [HERB: Human-augmented Efficient Reinforcement learning for Bin-packing](https://arxiv.org/abs/2504.16595)
*Gojko Perovic,Nuno Ferreira Duarte,Atabak Dehban,Gonçalo Teixeira,Egidio Falotico,José Santos-Victor*

Main category: cs.RO

TL;DR: HERB是一种结合人类演示和强化学习的框架，用于高效打包不规则3D物体，优于传统几何和纯RL方法。


<details>
  <summary>Details</summary>
Motivation: 传统打包方法难以处理不规则3D物体的形状和稳定性问题，而纯RL方法训练效率低且计算成本高。

Method: 利用人类演示学习打包顺序，结合视觉信息训练放置算法，优化空间利用和稳定性。

Result: 实验表明HERB在打包效率和适应性上优于几何和纯RL方法，并在机器人系统中验证了可行性。

Conclusion: 结合人类直觉的RL方法能有效解决复杂打包问题，提升机器人系统的鲁棒性和适应性。

Abstract: Packing objects efficiently is a fundamental problem in logistics, warehouse
automation, and robotics. While traditional packing solutions focus on
geometric optimization, packing irregular, 3D objects presents significant
challenges due to variations in shape and stability. Reinforcement
Learning~(RL) has gained popularity in robotic packing tasks, but training
purely from simulation can be inefficient and computationally expensive. In
this work, we propose HERB, a human-augmented RL framework for packing
irregular objects. We first leverage human demonstrations to learn the best
sequence of objects to pack, incorporating latent factors such as space
optimization, stability, and object relationships that are difficult to model
explicitly. Next, we train a placement algorithm that uses visual information
to determine the optimal object positioning inside a packing container. Our
approach is validated through extensive performance evaluations, analyzing both
packing efficiency and latency. Finally, we demonstrate the real-world
feasibility of our method on a robotic system. Experimental results show that
our method outperforms geometric and purely RL-based approaches by leveraging
human intuition, improving both packing robustness and adaptability. This work
highlights the potential of combining human expertise-driven RL to tackle
complex real-world packing challenges in robotic systems.

</details>


### [170] [MOSAIC: A Skill-Centric Algorithmic Framework for Long-Horizon Manipulation Planning](https://arxiv.org/abs/2504.16738)
*Itamar Mishani,Yorai Shaoul,Maxim Likhachev*

Main category: cs.RO

TL;DR: MOSAIC是一个以技能为中心的框架，通过技能本身指导规划过程，解决了机器人长期运动规划中的关键挑战。


<details>
  <summary>Details</summary>
Motivation: 解决机器人长期运动规划中技能组合探索、通用技能利用和避免符号化世界表示的挑战。

Method: MOSAIC使用两类技能：Generators生成可执行轨迹和世界配置，Connectors通过解决边界值问题连接这些轨迹。

Result: 在模拟和真实机器人任务中，MOSAIC成功解决了复杂长期规划问题。

Conclusion: MOSAIC通过技能导向的规划方法，为复杂长期任务提供了可扩展的解决方案。

Abstract: Planning long-horizon motions using a set of predefined skills is a key
challenge in robotics and AI. Addressing this challenge requires methods that
systematically explore skill combinations to uncover task-solving sequences,
harness generic, easy-to-learn skills (e.g., pushing, grasping) to generalize
across unseen tasks, and bypass reliance on symbolic world representations that
demand extensive domain and task-specific knowledge. Despite significant
progress, these elements remain largely disjoint in existing approaches,
leaving a critical gap in achieving robust, scalable solutions for complex,
long-horizon problems. In this work, we present MOSAIC, a skill-centric
framework that unifies these elements by using the skills themselves to guide
the planning process. MOSAIC uses two families of skills: Generators compute
executable trajectories and world configurations, and Connectors link these
independently generated skill trajectories by solving boundary value problems,
enabling progress toward completing the overall task. By breaking away from the
conventional paradigm of incrementally discovering skills from predefined start
or goal states--a limitation that significantly restricts exploration--MOSAIC
focuses planning efforts on regions where skills are inherently effective. We
demonstrate the efficacy of MOSAIC in both simulated and real-world robotic
manipulation tasks, showcasing its ability to solve complex long-horizon
planning problems using a diverse set of skills incorporating generative
diffusion models, motion planning algorithms, and manipulation-specific models.
Visit https://skill-mosaic.github.io for demonstrations and examples.

</details>


### [171] [Meta-Learning Online Dynamics Model Adaptation in Off-Road Autonomous Driving](https://arxiv.org/abs/2504.16923)
*Jacob Levy,Jason Gibson,Bogdan Vlahov,Erica Tevere,Evangelos Theodorou,David Fridovich-Keil,Patrick Spieler*

Main category: cs.RO

TL;DR: 论文提出了一种结合卡尔曼滤波在线适应与元学习参数的新框架，用于高速越野自动驾驶的动态模型适应，提升了预测精度和安全性。


<details>
  <summary>Details</summary>
Motivation: 高速越野自动驾驶面临复杂多变的地形和难以建模的车辆-地形交互问题，现有动态模型难以泛化到未知地形，需要实时适应。

Method: 通过离线元学习优化适应参数和基函数，结合在线卡尔曼滤波动态调整动态模型，实现实时适应。

Result: 实验验证表明，该方法在预测精度、性能和安全性上优于基线方法，尤其在安全关键场景中表现突出。

Conclusion: 元学习动态模型适应方法有效提升了自动驾驶系统在多样化和未知环境中的可靠性。

Abstract: High-speed off-road autonomous driving presents unique challenges due to
complex, evolving terrain characteristics and the difficulty of accurately
modeling terrain-vehicle interactions. While dynamics models used in
model-based control can be learned from real-world data, they often struggle to
generalize to unseen terrain, making real-time adaptation essential. We propose
a novel framework that combines a Kalman filter-based online adaptation scheme
with meta-learned parameters to address these challenges. Offline meta-learning
optimizes the basis functions along which adaptation occurs, as well as the
adaptation parameters, while online adaptation dynamically adjusts the onboard
dynamics model in real time for model-based control. We validate our approach
through extensive experiments, including real-world testing on a full-scale
autonomous off-road vehicle, demonstrating that our method outperforms baseline
approaches in prediction accuracy, performance, and safety metrics,
particularly in safety-critical scenarios. Our results underscore the
effectiveness of meta-learned dynamics model adaptation, advancing the
development of reliable autonomous systems capable of navigating diverse and
unseen environments. Video is available at: https://youtu.be/cCKHHrDRQEA

</details>


### [172] [Latent Diffusion Planning for Imitation Learning](https://arxiv.org/abs/2504.16925)
*Amber Xie,Oleh Rybkin,Dorsa Sadigh,Chelsea Finn*

Main category: cs.RO

TL;DR: 论文提出了一种名为Latent Diffusion Planning (LDP)的模块化方法，通过利用无动作演示和次优数据，解决了模仿学习中对大量专家演示的依赖问题。


<details>
  <summary>Details</summary>
Motivation: 模仿学习通常需要大量专家演示，限制了其应用范围。LDP旨在通过利用无动作演示和次优数据，减少对专家演示的依赖。

Method: LDP包括一个规划器和一个逆动力学模型，两者在学习的潜在空间中操作。首先通过变分自编码器学习紧凑的潜在空间，然后使用扩散目标训练规划器和逆动力学模型。

Result: 在模拟视觉机器人操作任务中，LDP优于现有模仿学习方法，因为它能利用额外的数据。

Conclusion: LDP通过分离规划和动作预测，能够利用更密集的监督信号，为模仿学习提供了更高效的数据利用方式。

Abstract: Recent progress in imitation learning has been enabled by policy
architectures that scale to complex visuomotor tasks, multimodal distributions,
and large datasets. However, these methods often rely on learning from large
amount of expert demonstrations. To address these shortcomings, we propose
Latent Diffusion Planning (LDP), a modular approach consisting of a planner
which can leverage action-free demonstrations, and an inverse dynamics model
which can leverage suboptimal data, that both operate over a learned latent
space. First, we learn a compact latent space through a variational
autoencoder, enabling effective forecasting of future states in image-based
domains. Then, we train a planner and an inverse dynamics model with diffusion
objectives. By separating planning from action prediction, LDP can benefit from
the denser supervision signals of suboptimal and action-free data. On simulated
visual robotic manipulation tasks, LDP outperforms state-of-the-art imitation
learning approaches, as they cannot leverage such additional data.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [173] [Mining Software Repositories for Expert Recommendation](https://arxiv.org/abs/2504.16343)
*Chad Marshall,Andrew Barovic,Armin Moin*

Main category: cs.SE

TL;DR: 提出一种基于BERTopic和TopicMiner的自动化方法，用于在大型开源项目中分配bug给开发者。


<details>
  <summary>Details</summary>
Motivation: 帮助人工bug分类员更高效地找到适合处理新报告的开发者。

Method: 利用问题跟踪系统中的历史数据，结合bug报告的特征（如产品、组件、优先级和严重性）和开发者的经验进行分配。

Result: 通过Top-k准确率评估，结果优于TopicMiner MTM、BUGZIE、BT-RL和LDA-SVM等方法。

Conclusion: 该方法能有效提升bug分配的准确性和效率。

Abstract: We propose an automated approach to bug assignment to developers in large
open-source software projects. This way, we assist human bug triagers who are
in charge of finding the best developer with the right level of expertise in a
particular area to be assigned to a newly reported issue. Our approach is based
on the history of software development as documented in the issue tracking
systems. We deploy BERTopic and techniques from TopicMiner. Our approach works
based on the bug reports' features, such as the corresponding products and
components, as well as their priority and severity levels. We sort developers
based on their experience with specific combinations of new reports. The
evaluation is performed using Top-k accuracy, and the results are compared with
the reported results in prior work, namely TopicMiner MTM, BUGZIE, Bug triaging
via deep Reinforcement Learning BT-RL, and LDA-SVM. The evaluation data come
from various Eclipse and Mozilla projects, such as JDT, Firefox, and
Thunderbird.

</details>


### [174] [Harden and Catch for Just-in-Time Assured LLM-Based Software Testing: Open Research Challenges](https://arxiv.org/abs/2504.16472)
*Mark Harman,Peter O'Hearn,Shubho Sengupta*

Main category: cs.SE

TL;DR: 论文探讨了自动化软件测试中的硬化测试和捕获测试，提出了“及时捕获测试”挑战，并讨论了基于LLM的测试生成方法及其潜在应用。


<details>
  <summary>Details</summary>
Motivation: 自动化软件测试中的一些基本概念尚未明确定义，但具有巨大实际潜力，尤其是在大型语言模型（LLM）应用于测试生成的背景下。

Method: 正式定义并研究了硬化测试和捕获测试的特性，提出了“及时捕获测试”（JiTTest）挑战，并探讨了基于LLM的测试生成方法。

Result: 展示了硬化测试和捕获测试的可能结果，讨论了部署选项，并分享了在Meta公司基于LLM的自动化硬化测试的初步成果。

Conclusion: 论文为自动化软件测试领域提出了新的研究方向，尤其是在LLM和及时测试生成的应用上，具有重要的理论和实践意义。

Abstract: Despite decades of research and practice in automated software testing,
several fundamental concepts remain ill-defined and under-explored, yet offer
enormous potential real-world impact. We show that these concepts raise
exciting new challenges in the context of Large Language Models for software
test generation. More specifically, we formally define and investigate the
properties of hardening and catching tests. A hardening test is one that seeks
to protect against future regressions, while a catching test is one that
catches such a regression or a fault in new functionality introduced by a code
change. Hardening tests can be generated at any time and may become catching
tests when a future regression is caught. We also define and motivate the
Catching `Just-in-Time' (JiTTest) Challenge, in which tests are generated
`just-in-time' to catch new faults before they land into production. We show
that any solution to Catching JiTTest generation can also be repurposed to
catch latent faults in legacy code. We enumerate possible outcomes for
hardening and catching tests and JiTTests, and discuss open research problems,
deployment options, and initial results from our work on automated LLM-based
hardening at Meta. This paper\footnote{Author order is alphabetical. The
corresponding author is Mark Harman.} was written to accompany the keynote by
the authors at the ACM International Conference on the Foundations of Software
Engineering (FSE) 2025.

</details>


### [175] [On Developers' Self-Declaration of AI-Generated Code: An Analysis of Practices](https://arxiv.org/abs/2504.16485)
*Syed Mohammad Kashif,Peng Liang,Amjed Tahir*

Main category: cs.SE

TL;DR: 研究探讨开发者如何自我声明AI生成的代码及其原因，发现多数开发者会声明，主要出于追踪和伦理考虑，少数不声明则因修改多或认为不必要。


<details>
  <summary>Details</summary>
Motivation: 现实开发中需区分AI生成与人工代码，但现有研究多关注代码质量，缺乏对开发者自我声明行为的理解。

Method: 混合方法研究：1) 挖掘GitHub收集613个AI生成代码片段；2) 工业调查获111份有效回复。

Result: 76.6%开发者会声明AI代码，主要原因为追踪和伦理；23.4%不声明，因修改多或认为不必要。

Conclusion: 提出指导开发者自我声明AI代码的指南，解决伦理与代码质量问题。

Abstract: AI code generation tools have gained significant popularity among developers,
who use them to assist in software development due to their capability to
generate code. Existing studies mainly explored the quality, e.g., correctness
and security, of AI-generated code, while in real-world software development,
the prerequisite is to distinguish AI-generated code from human-written code,
which emphasizes the need to explicitly declare AI-generated code by
developers. To this end, this study intends to understand the ways developers
use to self-declare AI-generated code and explore the reasons why developers
choose to self-declare or not. We conducted a mixed-methods study consisting of
two phases. In the first phase, we mined GitHub repositories and collected 613
instances of AI-generated code snippets. In the second phase, we conducted a
follow-up industrial survey, which received 111 valid responses. Our research
revealed the practices followed by developers to self-declare AI-generated
code. Most practitioners (76.6%) always or sometimes self-declare AI-generated
code. In contrast, other practitioners (23.4%) noted that they never
self-declare AI-generated code. The reasons for self-declaring AI-generated
code include the need to track and monitor the code for future review and
debugging, and ethical considerations. The reasons for not self-declaring
AI-generated code include extensive modifications to AI-generated code and the
developers' perception that self-declaration is an unnecessary activity. We
finally provided guidelines for practitioners to self-declare AI-generated
code, addressing ethical and code quality concerns.

</details>


### [176] [ClarifyCoder: Clarification-Aware Fine-Tuning for Programmatic Problem Solving](https://arxiv.org/abs/2504.16331)
*Jie JW Wu,Manav Chaudhary,Davit Abrahamyan,Arhaan Khaku,Anjiang Wei,Fatemeh H. Fard*

Main category: cs.SE

TL;DR: ClarifyCoder是一个新框架，通过合成数据生成和指令调优，使大语言模型（LLMs）能够在代码生成前识别模糊需求并请求澄清。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs在代码生成任务中表现优异，但与人类工程师相比，缺乏主动澄清模糊需求的能力。

Method: 框架包括两部分：1）数据合成技术，为编程数据集添加需要澄清的场景；2）微调策略，教导模型在模糊需求下优先请求澄清而非直接生成代码。

Result: 实验表明，ClarifyCoder显著提升了Code LLMs的沟通能力，同时保持了代码生成能力。

Conclusion: ClarifyCoder通过增强LLMs的澄清意识，缩小了其与人类工程师在代码生成任务中的差距。

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities in
code generation tasks. However, a significant gap remains between their current
performance and that of expert software engineers. A key differentiator is that
human engineers actively seek clarification when faced with ambiguous
requirements, while LLMs typically generate code regardless of uncertainties in
the problem description. We present ClarifyCoder, a novel framework with
synthetic data generation and instruction-tuning that enables LLMs to identify
ambiguities and request clarification before proceeding with code generation.
While recent work has focused on LLM-based agents for iterative code
generation, we argue that the fundamental ability to recognize and query
ambiguous requirements should be intrinsic to the models themselves. Our
approach consists of two main components: (1) a data synthesis technique that
augments existing programming datasets with scenarios requiring clarification
to generate clarification-aware training data, and (2) a fine-tuning strategy
that teaches models to prioritize seeking clarification over immediate code
generation when faced with incomplete or ambiguous requirements. We further
provide an empirical analysis of integrating ClarifyCoder with standard
fine-tuning for a joint optimization of both clarify-awareness and coding
ability. Experimental results demonstrate that ClarifyCoder significantly
improves the communication capabilities of Code LLMs through meaningful
clarification dialogues while maintaining code generation capabilities.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [177] [HPU: High-Bandwidth Processing Unit for Scalable, Cost-effective LLM Inference via GPU Co-processing](https://arxiv.org/abs/2504.16112)
*Myunghyun Rhee,Joonseop Sim,Taeyoung Ahn,Seungyong Lee,Daegun Yoon,Euiseok Kim,Kyoung Park,Youngpyo Joo,Hosik Kim*

Main category: cs.AR

TL;DR: 论文提出了一种高带宽处理单元（HPU），作为GPU的协处理器，用于提升大批次LLM推理的效率，通过卸载内存密集型任务，显著提高了性能和能效。


<details>
  <summary>Details</summary>
Motivation: 当前GPU系统在处理Transformer LLM的注意力层时，由于操作强度低和KV缓存的高内存需求，效率低下。

Method: 设计了基于PCIe FPGA卡的HPU原型，作为GPU的协处理器，卸载内存密集型任务，使GPU专注于计算密集型任务。

Result: 实验表明，GPU-HPU异构系统相比纯GPU系统，性能提升4.1倍，能效提升4.6倍。

Conclusion: HPU作为一种扩展卡，能够在不增加GPU数量的情况下提供可扩展性，显著提升LLM推理效率。

Abstract: The attention layer, a core component of Transformer-based LLMs, brings out
inefficiencies in current GPU systems due to its low operational intensity and
the substantial memory requirements of KV caches. We propose a High-bandwidth
Processing Unit (HPU), a memoryintensive co-processor that enhances GPU
resource utilization during large-batched LLM inference. By offloading
memory-bound operations, the HPU allows the GPU to focus on compute-intensive
tasks, increasing overall efficiency. Also, the HPU, as an add-on card, scales
out to accommodate surging memory demands driven by large batch sizes and
extended sequence lengths. In this paper, we show the HPU prototype implemented
with PCIe-based FPGA cards mounted on a GPU system. Our novel GPU-HPU
heterogeneous system demonstrates up to 4.1x performance gains and 4.6x energy
efficiency improvements over a GPUonly system, providing scalability without
increasing the number of GPUs.

</details>


### [178] [FPGA-Based Neural Network Accelerators for Space Applications: A Survey](https://arxiv.org/abs/2504.16173)
*Pedro Antunes,Artur Podobas*

Main category: cs.AR

TL;DR: 本文综述了FPGA在航天计算中的潜力，探讨了基于FPGA的神经网络加速器在空间任务中的应用前景。


<details>
  <summary>Details</summary>
Motivation: 随着航天任务的复杂性增加，需要高性能的星载计算系统，FPGA因其灵活性、成本效益和抗辐射潜力受到关注。

Method: 通过分析现有文献，识别趋势与不足，并提出未来研究方向。

Result: 研究表明，FPGA与神经网络结合可显著提升星载计算系统的性能。

Conclusion: FPGA神经网络加速器在航天应用中具有广阔前景，未来研究应关注其优化与实现。

Abstract: Space missions are becoming increasingly ambitious, necessitating
high-performance onboard spacecraft computing systems. In response,
field-programmable gate arrays (FPGAs) have garnered significant interest due
to their flexibility, cost-effectiveness, and radiation tolerance potential.
Concurrently, neural networks (NNs) are being recognized for their capability
to execute space mission tasks such as autonomous operations, sensor data
analysis, and data compression. This survey serves as a valuable resource for
researchers aiming to implement FPGA-based NN accelerators in space
applications. By analyzing existing literature, identifying trends and gaps,
and proposing future research directions, this work highlights the potential of
these accelerators to enhance onboard computing systems.

</details>


### [179] [TeLLMe: An Energy-Efficient Ternary LLM Accelerator for Prefilling and Decoding on Edge FPGAs](https://arxiv.org/abs/2504.16266)
*Ye Qiao,Zhiheng Cheng,Yifan Zhang,Yian Wang,Sitao Huang*

Main category: cs.AR

TL;DR: TeLLMe是一种针对低功耗FPGA的三元LLM加速器，支持1.58位权重和8位激活，优化了预填充和解码阶段的效率。


<details>
  <summary>Details</summary>
Motivation: 解决边缘平台上部署大型语言模型时的高计算和内存需求问题，尤其是预填充阶段的延迟和资源限制。

Method: 1. 使用表查找矩阵引擎优化三元矩阵乘法；2. 设计融合的注意力模块加速预填充；3. 集成归一化和量化-反量化单元。

Result: 在7W功耗下，TeLLMe实现了9 tokens/s的吞吐量，预填充延迟为0.55-1.15秒，显著提升了能效。

Conclusion: TeLLMe为生成式AI在边缘FPGA上的部署设定了新的能效基准。

Abstract: Deploying large language models (LLMs) on edge platforms is challenged by
their high computational and memory demands. Although recent low-bit
quantization methods (e.g., BitNet, DeepSeek) compress weights to as little as
1.58 bits with minimal accuracy loss, edge deployment is still constrained by
limited on-chip resources, power budgets, and the often-neglected latency of
the prefill phase. We present TeLLMe, the first ternary LLM accelerator for
low-power FPGAs (e.g., AMD KV260) that fully supports both prefill and
autoregressive decoding using 1.58-bit weights and 8-bit activations. Our
contributions include: (1) a table-lookup matrix engine for ternary matmul that
merges grouped activations with online precomputation to minimize resource use;
(2) a fused, bandwidth-efficient attention module featuring a reversed
reordering scheme to accelerate prefill; and (3) a tightly integrated
normalization and quantization--dequantization unit optimized for ultra-low-bit
inference. Under a 7W power budget, TeLLMe delivers up to 9 tokens/s throughput
over 1,024-token contexts and prefill latencies of 0.55--1.15 s for 64--128
token prompts, marking a significant energy-efficiency advance and establishing
a new edge FPGA benchmark for generative AI.

</details>


### [180] [COBRA: Algorithm-Architecture Co-optimized Binary Transformer Accelerator for Edge Inference](https://arxiv.org/abs/2504.16269)
*Ye Qiao,Zhiheng Cheng,Yian Wang,Yifan Zhang,Yunzhe Deng,Sitao Huang*

Main category: cs.AR

TL;DR: COBRA是一种针对边缘计算的算法-架构协同优化的二进制Transformer加速器，显著提升了性能和能效。


<details>
  <summary>Details</summary>
Motivation: Transformer模型在边缘平台部署时面临计算、内存和通信的高需求问题，现有二进制Transformer因缺乏硬件优化而效率低下。

Method: COBRA采用1位二进制乘法单元，支持-1、0和+1值的矩阵运算，并对注意力块进行硬件友好优化。

Result: 在边缘FPGA上，COBRA实现了3,894.7 GOPS的吞吐量和448.7 GOPS/W的能效，性能显著优于GPU和现有二进制加速器。

Conclusion: COBRA在保持推理精度几乎不变的情况下，显著提升了二进制Transformer在边缘计算中的性能和能效。

Abstract: Transformer-based models have demonstrated superior performance in various
fields, including natural language processing and computer vision. However,
their enormous model size and high demands in computation, memory, and
communication limit their deployment to edge platforms for local, secure
inference. Binary transformers offer a compact, low-complexity solution for
edge deployment with reduced bandwidth needs and acceptable accuracy. However,
existing binary transformers perform inefficiently on current hardware due to
the lack of binary specific optimizations. To address this, we introduce COBRA,
an algorithm-architecture co-optimized binary Transformer accelerator for edge
computing. COBRA features a real 1-bit binary multiplication unit, enabling
matrix operations with -1, 0, and +1 values, surpassing ternary methods. With
further hardware-friendly optimizations in the attention block, COBRA achieves
up to 3,894.7 GOPS throughput and 448.7 GOPS/Watt energy efficiency on edge
FPGAs, delivering a 311x energy efficiency improvement over GPUs and a 3.5x
throughput improvement over the state-of-the-art binary accelerator, with only
negligible inference accuracy degradation.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [181] [4D Multimodal Co-attention Fusion Network with Latent Contrastive Alignment for Alzheimer's Diagnosis](https://arxiv.org/abs/2504.16798)
*Yuxiang Wei,Yanteng Zhang,Xi Xiao,Tianyang Wang,Xiao Wang,Vince D. Calhoun*

Main category: cs.MM

TL;DR: M2M-AlignNet是一种几何感知的多模态共注意力网络，用于早期阿尔茨海默病（AD）诊断，通过sMRI和fMRI数据融合解决模态异质性挑战。


<details>
  <summary>Details</summary>
Motivation: 多模态神经影像数据（如sMRI和fMRI）在AD诊断中具有互补性，但模态间的异质性（如4D fMRI与3D sMRI）导致特征融合困难。

Method: 提出M2M对比损失函数和潜在对齐机制，通过几何加权补丁对应关系减少表示差异，并设计潜在查询共注意力模块自主发现融合模式。

Result: 实验验证了方法的有效性，并揭示了fMRI与sMRI作为AD生物标志物的对应关系。

Conclusion: M2M-AlignNet成功解决了多模态数据融合的挑战，为早期AD诊断提供了新工具。

Abstract: Multimodal neuroimaging provides complementary structural and functional
insights into both human brain organization and disease-related dynamics.
Recent studies demonstrate enhanced diagnostic sensitivity for Alzheimer's
disease (AD) through synergistic integration of neuroimaging data (e.g., sMRI,
fMRI) with behavioral cognitive scores tabular data biomarkers. However, the
intrinsic heterogeneity across modalities (e.g., 4D spatiotemporal fMRI
dynamics vs. 3D anatomical sMRI structure) presents critical challenges for
discriminative feature fusion. To bridge this gap, we propose M2M-AlignNet: a
geometry-aware multimodal co-attention network with latent alignment for early
AD diagnosis using sMRI and fMRI. At the core of our approach is a
multi-patch-to-multi-patch (M2M) contrastive loss function that quantifies and
reduces representational discrepancies via geometry-weighted patch
correspondence, explicitly aligning fMRI components across brain regions with
their sMRI structural substrates without one-to-one constraints. Additionally,
we propose a latent-as-query co-attention module to autonomously discover
fusion patterns, circumventing modality prioritization biases while minimizing
feature redundancy. We conduct extensive experiments to confirm the
effectiveness of our method and highlight the correspondance between fMRI and
sMRI as AD biomarkers.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [182] [DP2FL: Dual Prompt Personalized Federated Learning in Foundation Models](https://arxiv.org/abs/2504.16357)
*Ying Chang,Xiaohu Shi,Xiaohui Zhao,Zhaohuang Chen,Deyin Ma*

Main category: cs.DC

TL;DR: DP2FL框架通过双提示和自适应聚合策略，解决了联邦学习中本地数据不足和新客户端整合的问题，提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习中因本地数据不足导致的模型性能下降问题，并探索基础模型在联邦学习中的应用潜力。

Method: 提出DP2FL框架，结合双提示和自适应聚合策略，平衡全局任务与本地数据特性。

Result: 实验验证了DP2FL在异构环境中的有效性，支持新数据源的预测和新客户端的无缝整合。

Conclusion: DP2FL为联邦学习提供了高效、灵活的解决方案，尤其适用于数据异构和新客户端动态加入的场景。

Abstract: Personalized federated learning (PFL) has garnered significant attention for
its ability to address heterogeneous client data distributions while preserving
data privacy. However, when local client data is limited, deep learning models
often suffer from insufficient training, leading to suboptimal performance.
Foundation models, such as CLIP (Contrastive Language-Image Pretraining),
exhibit strong feature extraction capabilities and can alleviate this issue by
fine-tuning on limited local data. Despite their potential, foundation models
are rarely utilized in federated learning scenarios, and challenges related to
integrating new clients remain largely unresolved. To address these challenges,
we propose the Dual Prompt Personalized Federated Learning (DP2FL) framework,
which introduces dual prompts and an adaptive aggregation strategy. DP2FL
combines global task awareness with local data-driven insights, enabling local
models to achieve effective generalization while remaining adaptable to
specific data distributions. Moreover, DP2FL introduces a global model that
enables prediction on new data sources and seamlessly integrates newly added
clients without requiring retraining. Experimental results in highly
heterogeneous environments validate the effectiveness of DP2FL's prompt design
and aggregation strategy, underscoring the advantages of prediction on novel
data sources and demonstrating the seamless integration of new clients into the
federated learning framework.

</details>


### [183] [Towards a Distributed Federated Learning Aggregation Placement using Particle Swarm Intelligence](https://arxiv.org/abs/2504.16227)
*Amir Ali-Pour,Sadra Bekrani,Laya Samizadeh,Julien Gascon-Samson*

Main category: cs.DC

TL;DR: Flag-Swap是一种基于粒子群优化（PSO）的方法，用于在分层半去中心化联邦学习中优化聚合位置，仅依赖处理延迟，减少对系统数据的依赖。


<details>
  <summary>Details</summary>
Motivation: 现有方法需要频繁交换系统数据以监控节点性能和资源消耗，Flag-Swap旨在减少这种依赖。

Method: 提出Flag-Swap，一种PSO方法，根据处理延迟优化聚合位置。

Result: 模拟和实际测试显示，Flag-Swap比随机和均匀放置策略分别快43%和32%。

Conclusion: Flag-Swap在减少系统数据依赖的同时，显著提升了聚合效率。

Abstract: Federated learning has become a promising distributed learning concept with
extra insurance on data privacy. Extensive studies on various models of
Federated learning have been done since the coinage of its term. One of the
important derivatives of federated learning is hierarchical semi-decentralized
federated learning, which distributes the load of the aggregation task over
multiple nodes and parallelizes the aggregation workload at the breadth of each
level of the hierarchy. Various methods have also been proposed to perform
inter-cluster and intra-cluster aggregation optimally. Most of the solutions,
nonetheless, require monitoring the nodes' performance and resource consumption
at each round, which necessitates frequently exchanging systematic data. To
optimally perform distributed aggregation in SDFL with minimal reliance on
systematic data, we propose Flag-Swap, a Particle Swarm Optimization (PSO)
method that optimizes the aggregation placement according only to the
processing delay. Our simulation results show that PSO-based placement can find
the optimal placement relatively fast, even in scenarios with many clients as
candidates for aggregation. Our real-world docker-based implementation of
Flag-Swap over the recently emerged FL framework shows superior performance
compared to black-box-based deterministic placement strategies, with about 43%
minutes faster than random placement, and 32% minutes faster than uniform
placement, in terms of total processing time.

</details>


### [184] [Simplified Swarm Learning Framework for Robust and Scalable Diagnostic Services in Cancer Histopathology](https://arxiv.org/abs/2504.16732)
*Yanjie Wu,Yuhao Ji,Saiho Lee,Juniad Akram,Ali Braytee,Ali Anaissi*

Main category: cs.DC

TL;DR: 论文提出了一种简化的点对点群体学习框架（P2P-SL），用于资源受限环境，解决了传统群体学习依赖区块链的问题，同时保持数据隐私。


<details>
  <summary>Details</summary>
Motivation: 医疗数据的复杂性（如隐私问题、数据集不平衡和互操作性）需要创新的机器学习解决方案，而传统群体学习因依赖区块链而难以普及。

Method: 提出P2P-SL框架，取消区块链依赖，采用轻量级点对点通信，结合优化的预训练模型（如TorchXRayVision和DenseNet解码器）。

Result: 实验表明，该框架在处理不平衡和偏置数据集时表现优异，性能接近集中式模型，同时保护隐私。

Conclusion: 该研究为医疗领域提供了一种可扩展、高效且隐私保护的机器学习解决方案，推动了先进技术的普及。

Abstract: The complexities of healthcare data, including privacy concerns, imbalanced
datasets, and interoperability issues, necessitate innovative machine learning
solutions. Swarm Learning (SL), a decentralized alternative to Federated
Learning, offers privacy-preserving distributed training, but its reliance on
blockchain technology hinders accessibility and scalability. This paper
introduces a \textit{Simplified Peer-to-Peer Swarm Learning (P2P-SL) Framework}
tailored for resource-constrained environments. By eliminating blockchain
dependencies and adopting lightweight peer-to-peer communication, the proposed
framework ensures robust model synchronization while maintaining data privacy.
Applied to cancer histopathology, the framework integrates optimized
pre-trained models, such as TorchXRayVision, enhanced with DenseNet decoders,
to improve diagnostic accuracy. Extensive experiments demonstrate the
framework's efficacy in handling imbalanced and biased datasets, achieving
comparable performance to centralized models while preserving privacy. This
study paves the way for democratizing advanced machine learning in healthcare,
offering a scalable, accessible, and efficient solution for privacy-sensitive
diagnostic applications.

</details>


<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [185] [Probabilistic Emulation of the Community Radiative Transfer Model Using Machine Learning](https://arxiv.org/abs/2504.16192)
*Lucas Howard,Aneesh C. Subramanian,Gregory Thompson,Benjamin Johnson,Thomas Auligne*

Main category: physics.ao-ph

TL;DR: 利用机器学习构建了一个高效的神经网络概率模拟器，用于替代传统的辐射传输模型（CRTM），显著提高了计算效率，同时保持了高精度。


<details>
  <summary>Details</summary>
Motivation: 传统辐射传输模型计算成本高，导致大量卫星数据未被充分利用。通过机器学习模拟器可以解决这一瓶颈。

Method: 使用神经网络构建CRTM的概率模拟器，应用于GOES Advanced Baseline Imager，预测亮度温度及其误差。

Result: 模拟器预测的亮度温度均方根误差为0.3 K，晴空条件下9/10红外通道误差小于0.1 K，误差预测可靠。

Conclusion: 模拟器不仅高效，还能重现相关物理过程，增强了其在新数据上的可靠性。

Abstract: The continuous improvement in weather forecast skill over the past several
decades is largely due to the increasing quantity of available satellite
observations and their assimilation into operational forecast systems.
Assimilating these observations requires observation operators in the form of
radiative transfer models. Significant efforts have been dedicated to enhancing
the computational efficiency of these models. Computational cost remains a
bottleneck, and a large fraction of available data goes unused for
assimilation. To address this, we used machine learning to build an efficient
neural network based probabilistic emulator of the Community Radiative Transfer
Model (CRTM), applied to the GOES Advanced Baseline Imager. The trained NN
emulator predicts brightness temperatures output by CRTM and the corresponding
error with respect to CRTM. RMSE of the predicted brightness temperature is 0.3
K averaged across all channels. For clear sky conditions, the RMSE is less than
0.1 K for 9 out of 10 infrared channels. The error predictions are generally
reliable across a wide range of conditions. Explainable AI methods demonstrate
that the trained emulator reproduces the relevant physics, increasing
confidence that the model will perform well when presented with new data.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [186] [Context-Awareness and Interpretability of Rare Occurrences for Discovery and Formalization of Critical Failure Modes](https://arxiv.org/abs/2504.16117)
*Sridevi Polavaram,Xin Zhou,Meenu Ravi,Mohammad Zarei,Anmol Srivastava*

Main category: cs.CV

TL;DR: CAIRO是一个基于本体的人类辅助发现框架，用于检测和形式化AI模型中的罕见故障案例，特别关注自动驾驶系统中的对象检测模型。


<details>
  <summary>Details</summary>
Motivation: 解决视觉系统在关键领域（如自动驾驶）中因罕见或未预见场景而引发的安全风险。

Method: 通过人类参与的测试和评估，结合知识图谱（OWL/XML格式）形式化故障案例。

Result: 展示了可扩展且可解释的形式化方法，用于捕捉相机感知与现实场景之间的差距。

Conclusion: CAIRO为共享、逻辑推理和问责提供了明确的故障案例知识库。

Abstract: Vision systems are increasingly deployed in critical domains such as
surveillance, law enforcement, and transportation. However, their
vulnerabilities to rare or unforeseen scenarios pose significant safety risks.
To address these challenges, we introduce Context-Awareness and
Interpretability of Rare Occurrences (CAIRO), an ontology-based human-assistive
discovery framework for failure cases (or CP - Critical Phenomena) detection
and formalization. CAIRO by design incentivizes human-in-the-loop for testing
and evaluation of criticality that arises from misdetections, adversarial
attacks, and hallucinations in AI black-box models. Our robust analysis of
object detection model(s) failures in automated driving systems (ADS) showcases
scalable and interpretable ways of formalizing the observed gaps between camera
perception and real-world contexts, resulting in test cases stored as explicit
knowledge graphs (in OWL/XML format) amenable for sharing, downstream analysis,
logical reasoning, and accountability.

</details>


### [187] [Hybrid Knowledge Transfer through Attention and Logit Distillation for On-Device Vision Systems in Agricultural IoT](https://arxiv.org/abs/2504.16128)
*Stanley Mugisha,Rashid Kisitu,Florence Tushabe*

Main category: cs.CV

TL;DR: 提出了一种混合知识蒸馏框架，将Swin Transformer的精度与MobileNetV3的效率结合，用于农业IoT设备的实时植物病害检测。


<details>
  <summary>Details</summary>
Motivation: 解决Vision Transformers高精度但计算复杂与轻量模型效率高但空间推理能力不足的矛盾。

Method: 通过自适应注意力对齐和双损失函数，将Swin Transformer的知识蒸馏到MobileNetV3。

Result: 蒸馏后的MobileNetV3在PlantVillage-Tomato数据集上达到92.4%准确率，计算量减少95%，推理延迟降低82%。

Conclusion: 该方法实现了边缘设备上的实时高效病害检测，为精准农业提供了可行方案。

Abstract: Integrating deep learning applications into agricultural IoT systems faces a
serious challenge of balancing the high accuracy of Vision Transformers (ViTs)
with the efficiency demands of resource-constrained edge devices. Large
transformer models like the Swin Transformers excel in plant disease
classification by capturing global-local dependencies. However, their
computational complexity (34.1 GFLOPs) limits applications and renders them
impractical for real-time on-device inference. Lightweight models such as
MobileNetV3 and TinyML would be suitable for on-device inference but lack the
required spatial reasoning for fine-grained disease detection. To bridge this
gap, we propose a hybrid knowledge distillation framework that synergistically
transfers logit and attention knowledge from a Swin Transformer teacher to a
MobileNetV3 student model. Our method includes the introduction of adaptive
attention alignment to resolve cross-architecture mismatch (resolution,
channels) and a dual-loss function optimizing both class probabilities and
spatial focus. On the lantVillage-Tomato dataset (18,160 images), the distilled
MobileNetV3 attains 92.4% accuracy relative to 95.9% for Swin-L but at an 95%
reduction on PC and < 82% in inference latency on IoT devices. (23ms on PC CPU
and 86ms/image on smartphone CPUs). Key innovations include IoT-centric
validation metrics (13 MB memory, 0.22 GFLOPs) and dynamic resolution-matching
attention maps. Comparative experiments show significant improvements over
standalone CNNs and prior distillation methods, with a 3.5% accuracy gain over
MobileNetV3 baselines. Significantly, this work advances real-time,
energy-efficient crop monitoring in precision agriculture and demonstrates how
we can attain ViT-level diagnostic precision on edge devices. Code and models
will be made available for replication after acceptance.

</details>


### [188] [Progressive Language-guided Visual Learning for Multi-Task Visual Grounding](https://arxiv.org/abs/2504.16145)
*Jingchao Wang,Hong Wang,Wenlong Zhang,Kunhua Ji,Dingjiang Huang,Yefeng Zheng*

Main category: cs.CV

TL;DR: PLVL框架通过渐进式语言引导视觉学习，解决了多任务视觉定位中语言信息未充分利用和任务间协作不足的问题，显著提升了REC和RES任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在语言信息注入和任务协作方面存在不足，PLVL旨在通过语言引导和任务协作提升多任务视觉定位的效果。

Method: 提出PLVL框架，渐进式注入语言信息到视觉特征中，并设计多任务头实现REC和RES的协作预测。

Result: 在多个基准数据集上，PLVL显著优于现有方法。

Conclusion: PLVL通过语言引导和任务协作，有效提升了多任务视觉定位的性能。

Abstract: Multi-task visual grounding (MTVG) includes two sub-tasks, i.e., Referring
Expression Comprehension (REC) and Referring Expression Segmentation (RES). The
existing representative approaches generally follow the research pipeline which
mainly consists of three core procedures, including independent feature
extraction for visual and linguistic modalities, respectively, cross-modal
interaction module, and independent prediction heads for different sub-tasks.
Albeit achieving remarkable performance, this research line has two
limitations: 1) The linguistic content has not been fully injected into the
entire visual backbone for boosting more effective visual feature extraction
and it needs an extra cross-modal interaction module; 2) The relationship
between REC and RES tasks is not effectively exploited to help the
collaborative prediction for more accurate output. To deal with these problems,
in this paper, we propose a Progressive Language-guided Visual Learning
framework for multi-task visual grounding, called PLVL, which not only finely
mine the inherent feature expression of the visual modality itself but also
progressively inject the language information to help learn linguistic-related
visual features. In this manner, our PLVL does not need additional cross-modal
fusion module while fully introducing the language guidance. Furthermore, we
analyze that the localization center for REC would help identify the
to-be-segmented object region for RES to some extent. Inspired by this
investigation, we design a multi-task head to accomplish collaborative
predictions for these two sub-tasks. Extensive experiments conducted on several
benchmark datasets comprehensively substantiate that our PLVL obviously
outperforms the representative methods in both REC and RES tasks.
https://github.com/jcwang0602/PLVL

</details>


### [189] [Multimodal Large Language Models for Enhanced Traffic Safety: A Comprehensive Review and Future Trends](https://arxiv.org/abs/2504.16134)
*Mohammad Abu Tami,Mohammed Elhenawy,Huthaifa I. Ashqar*

Main category: cs.CV

TL;DR: 本文探讨了多模态大语言模型（MLLMs）如何通过整合视觉、空间和环境数据提升交通安全性，弥补传统ADAS的不足，并展望了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 传统ADAS在动态现实场景中表现不佳，亟需更全面的解决方案。MLLMs因其跨模态数据处理能力，有望成为下一代交通安全系统的核心。

Method: 通过分析MLLM方法，结合关键数据集（如KITTI、DRAMA等），评估其在感知、决策和对抗鲁棒性方面的表现。

Result: MLLMs能够提供更全面的场景理解，增强交通安全性，并具备应对对抗性条件的能力。

Conclusion: MLLMs有望革新交通安全领域，提供可扩展、情境感知的解决方案，未来需关注实时边缘部署和因果推理等方向。

Abstract: Traffic safety remains a critical global challenge, with traditional Advanced
Driver-Assistance Systems (ADAS) often struggling in dynamic real-world
scenarios due to fragmented sensor processing and susceptibility to adversarial
conditions. This paper reviews the transformative potential of Multimodal Large
Language Models (MLLMs) in addressing these limitations by integrating
cross-modal data such as visual, spatial, and environmental inputs to enable
holistic scene understanding. Through a comprehensive analysis of MLLM-based
approaches, we highlight their capabilities in enhancing perception,
decision-making, and adversarial robustness, while also examining the role of
key datasets (e.g., KITTI, DRAMA, ML4RoadSafety) in advancing research.
Furthermore, we outline future directions, including real-time edge deployment,
causality-driven reasoning, and human-AI collaboration. By positioning MLLMs as
a cornerstone for next-generation traffic safety systems, this review
underscores their potential to revolutionize the field, offering scalable,
context-aware solutions that proactively mitigate risks and improve overall
road safety.

</details>


### [190] [A detection-task-specific deep-learning method to improve the quality of sparse-view myocardial perfusion SPECT images](https://arxiv.org/abs/2504.16171)
*Zezhang Yang,Zitong Yu,Nuri Choi,Abhinav K. Jha*

Main category: cs.CV

TL;DR: 提出了一种基于深度学习的稀疏视角心肌灌注成像方法，旨在缩短扫描时间并提高图像质量。


<details>
  <summary>Details</summary>
Motivation: 传统SPECT扫描时间长，可能导致患者不适和图像伪影，稀疏视角虽能缩短时间但影响图像质量。

Method: 结合观察者损失项的深度学习模型，优化灌注缺陷检测任务。

Result: 在检测心肌灌注缺陷任务中，AUC显著优于稀疏视角协议，并能恢复左心室壁结构。

Conclusion: 初步结果表明该方法有效，值得进一步评估。

Abstract: Myocardial perfusion imaging (MPI) with single-photon emission computed
tomography (SPECT) is a widely used and cost-effective diagnostic tool for
coronary artery disease. However, the lengthy scanning time in this imaging
procedure can cause patient discomfort, motion artifacts, and potentially
inaccurate diagnoses due to misalignment between the SPECT scans and the
CT-scans which are acquired for attenuation compensation. Reducing projection
angles is a potential way to shorten scanning time, but this can adversely
impact the quality of the reconstructed images. To address this issue, we
propose a detection-task-specific deep-learning method for sparse-view MPI
SPECT images. This method integrates an observer loss term that penalizes the
loss of anthropomorphic channel features with the goal of improving performance
in perfusion defect-detection task. We observed that, on the task of detecting
myocardial perfusion defects, the proposed method yielded an area under the
receiver operating characteristic (ROC) curve (AUC) significantly larger than
the sparse-view protocol. Further, the proposed method was observed to be able
to restore the structure of the left ventricle wall, demonstrating ability to
overcome sparse-sampling artifacts. Our preliminary results motivate further
evaluations of the method.

</details>


### [191] [SignX: The Foundation Model for Sign Recognition](https://arxiv.org/abs/2504.16315)
*Sen Fang,Chunyu Sui,Hongwei Yi,Carol Neidle,Dimitris N. Metaxas*

Main category: cs.CV

TL;DR: SignX是一个用于手语识别的框架，通过两阶段训练（Pose2Gloss和Video2Pose）实现高精度识别。


<details>
  <summary>Details</summary>
Motivation: 手语数据处理复杂，现有方法依赖RGB视频和姿势信息，但缺乏统一的标注规范。SignX旨在解决这一问题。

Method: 提出两阶段框架：1) Pose2Gloss基于逆扩散模型融合多源姿势信息；2) Video2Pose基于ViT将视频直接转换为姿势表示。

Result: 实验表明SignX在手语视频识别中比现有方法更准确。

Conclusion: SignX为手语识别提供了兼容现有姿势格式的通用框架。

Abstract: The complexity of sign language data processing brings many challenges. The
current approach to recognition of ASL signs aims to translate RGB sign
language videos through pose information into English-based ID glosses, which
serve to uniquely identify ASL signs. Note that there is no shared convention
for assigning such glosses to ASL signs, so it is essential that the same
glossing conventions are used for all of the data in the datasets that are
employed. This paper proposes SignX, a foundation model framework for sign
recognition. It is a concise yet powerful framework applicable to multiple
human activity recognition scenarios. First, we developed a Pose2Gloss
component based on an inverse diffusion model, which contains a multi-track
pose fusion layer that unifies five of the most powerful pose information
sources--SMPLer-X, DWPose, Mediapipe, PrimeDepth, and Sapiens
Segmentation--into a single latent pose representation. Second, we trained a
Video2Pose module based on ViT that can directly convert raw video into signer
pose representation. Through this 2-stage training framework, we enable sign
language recognition models to be compatible with existing pose formats, laying
the foundation for the common pose estimation necessary for sign recognition.
Experimental results show that SignX can recognize signs from sign language
video, producing predicted gloss representations with greater accuracy than has
been reported in prior work.

</details>


### [192] [CLPSTNet: A Progressive Multi-Scale Convolutional Steganography Model Integrating Curriculum Learning](https://arxiv.org/abs/2504.16364)
*Fengchun Liu,Tong Zhang,Chunying Zhang*

Main category: cs.CV

TL;DR: 论文提出了一种基于课程学习的渐进式多尺度卷积网络（CLPSTNet），用于解决CNN在图像隐写术中存在的不可见性和安全性问题。


<details>
  <summary>Details</summary>
Motivation: 传统隐写方法依赖手工特征和先验知识设计，而CNN虽能自主学习信息嵌入，但在复杂图像中仍存在不可见性和安全性问题。

Method: CLPSTNet由多个渐进式多尺度卷积模块组成，结合Inception结构和空洞卷积，逐步提取从细到粗的多尺度特征。

Result: 在ALASKA2、VOC2012和ImageNet数据集上，CLPSTNet表现出高PSNR、SSIM和解码准确率，且生成的隐写图像具有低隐写分析分数。

Conclusion: CLPSTNet通过渐进式多尺度特征提取，显著提升了隐写术的不可见性和安全性。

Abstract: In recent years, a large number of works have introduced Convolutional Neural
Networks (CNNs) into image steganography, which transform traditional
steganography methods such as hand-crafted features and prior knowledge design
into steganography methods that neural networks autonomically learn information
embedding. However, due to the inherent complexity of digital images, issues of
invisibility and security persist when using CNN models for information
embedding. In this paper, we propose Curriculum Learning Progressive Steganophy
Network (CLPSTNet). The network consists of multiple progressive multi-scale
convolutional modules that integrate Inception structures and dilated
convolutions. The module contains multiple branching pathways, starting from a
smaller convolutional kernel and dilatation rate, extracting the basic, local
feature information from the feature map, and gradually expanding to the
convolution with a larger convolutional kernel and dilatation rate for
perceiving the feature information of a larger receptive field, so as to
realize the multi-scale feature extraction from shallow to deep, and from fine
to coarse, allowing the shallow secret information features to be refined in
different fusion stages. The experimental results show that the proposed
CLPSTNet not only has high PSNR , SSIM metrics and decoding accuracy on three
large public datasets, ALASKA2, VOC2012 and ImageNet, but also the
steganographic images generated by CLPSTNet have low steganalysis scores.You
can find our code at
\href{https://github.com/chaos-boops/CLPSTNet}{https://github.com/chaos-boops/CLPSTNet}.

</details>


### [193] [Assessing the Feasibility of Internet-Sourced Video for Automatic Cattle Lameness Detection](https://arxiv.org/abs/2504.16404)
*Md Fahimuzzman Sohan*

Main category: cs.CV

TL;DR: 该研究提出了一种基于深度学习的模型，用于通过视频数据检测牛的跛行、疾病或步态异常。3D CNN模型表现最佳，准确率达90%。


<details>
  <summary>Details</summary>
Motivation: 牛的跛行常由蹄部损伤或趾间皮炎引起，严重影响其生理活动。传统方法多阶段复杂，本研究旨在简化流程并提高检测效果。

Method: 使用公开视频数据，包含50个视频（40头牛），分为正常和跛行两类。采用数据增强技术，并比较ConvLSTM2D和3D CNN两种深度学习模型。

Result: 3D CNN模型表现优异，视频分类准确率90%，精确率、召回率和F1分数均为90.9%。ConvLSTM2D准确率稍低，为85%。

Conclusion: 3D CNN能有效学习视频时空特征，简化传统多阶段流程，为牛跛行检测提供高效解决方案。

Abstract: Cattle lameness is often caused by hoof injuries or interdigital dermatitis,
leads to pain and significantly impacts essential physiological activities such
as walking, feeding, and drinking. This study presents a deep learning-based
model for detecting cattle lameness, sickness, or gait abnormalities using
publicly available video data. The dataset consists of 50 unique videos from 40
individual cattle, recorded from various angles in both indoor and outdoor
environments. Half of the dataset represents naturally walking
(normal/non-lame) cattle, while the other half consists of cattle exhibiting
gait abnormalities (lame). To enhance model robustness and generalizability,
data augmentation was applied to the training data. The pre-processed videos
were then classified using two deep learning models: ConvLSTM2D and 3D CNN. A
comparative analysis of the results demonstrates strong classification
performance. Specifically, the 3D CNN model achieved a video-level
classification accuracy of 90%, with precision, recall, and f1-score of 90.9%,
90.9%, and 90.91% respectively. The ConvLSTM2D model exhibited a slightly lower
accuracy of 85%. This study highlights the effectiveness of directly applying
classification models to learn spatiotemporal features from video data,
offering an alternative to traditional multi-stage approaches that typically
involve object detection, pose estimation, and feature extraction. Besides, the
findings demonstrate that the proposed deep learning models, particularly the
3D CNN, effectively classify and detect lameness in cattle while simplifying
the processing pipeline.

</details>


### [194] [PixelWeb: The First Web GUI Dataset with Pixel-Wise Labels](https://arxiv.org/abs/2504.16419)
*Qi Yang,Weichen Bi,Haiyang Shen,Yaoqi Guo,Yun Ma*

Main category: cs.CV

TL;DR: PixelWeb是一个大规模GUI数据集，通过结合视觉特征提取和DOM结构分析，提供高质量的BBox注释，显著提升了GUI元素检测任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有GUI数据集因自动标注导致BBox注释不准确（如缺失、重复或无意义），且仅提供视觉BBox注释，限制了视觉相关下游任务的发展。

Method: 提出PixelWeb数据集，采用通道派生和层次分析两个核心模块，结合BGRA四通道位图注释和DOM分析，确保GUI元素的精确定位和可见性。

Result: PixelWeb在mAP95指标上比现有数据集性能提升3-7倍，并通过人工验证确认注释的高质量。

Conclusion: PixelWeb有望显著提升GUI生成和自动化用户交互等下游任务的性能。

Abstract: Graphical User Interface (GUI) datasets are crucial for various downstream
tasks. However, GUI datasets often generate annotation information through
automatic labeling, which commonly results in inaccurate GUI element BBox
annotations, including missing, duplicate, or meaningless BBoxes. These issues
can degrade the performance of models trained on these datasets, limiting their
effectiveness in real-world applications. Additionally, existing GUI datasets
only provide BBox annotations visually, which restricts the development of
visually related GUI downstream tasks. To address these issues, we introduce
PixelWeb, a large-scale GUI dataset containing over 100,000 annotated web
pages. PixelWeb is constructed using a novel automatic annotation approach that
integrates visual feature extraction and Document Object Model (DOM) structure
analysis through two core modules: channel derivation and layer analysis.
Channel derivation ensures accurate localization of GUI elements in cases of
occlusion and overlapping elements by extracting BGRA four-channel bitmap
annotations. Layer analysis uses the DOM to determine the visibility and
stacking order of elements, providing precise BBox annotations. Additionally,
PixelWeb includes comprehensive metadata such as element images, contours, and
mask annotations. Manual verification by three independent annotators confirms
the high quality and accuracy of PixelWeb annotations. Experimental results on
GUI element detection tasks show that PixelWeb achieves performance on the
mAP95 metric that is 3-7 times better than existing datasets. We believe that
PixelWeb has great potential for performance improvement in downstream tasks
such as GUI generation and automated user interaction.

</details>


### [195] [Naturally Computed Scale Invariance in the Residual Stream of ResNet18](https://arxiv.org/abs/2504.16290)
*André Longon*

Main category: cs.CV

TL;DR: 论文研究了ResNet18中残差流如何通过尺度等变表示的元素级残差求和实现尺度不变性，并探讨了其与尺度鲁棒物体识别行为的因果关系。


<details>
  <summary>Details</summary>
Motivation: 探讨神经网络如何实现图像变换（如光照、旋转、尺度）下的物体识别不变性，尤其是不同架构网络（如ResNet18）中未被充分研究的机制。

Method: 分析ResNet18的残差流，观察中间块卷积通道的尺度不变性，并通过消融实验验证其与尺度鲁棒行为的关系。

Result: 发现残差流通过尺度等变表示的元素级求和实现尺度不变性，初步表明其在行为中的作用。

Conclusion: 残差流可能是ResNet18实现尺度不变性的关键机制，为理解神经网络的不变性提供了新视角。

Abstract: An important capacity in visual object recognition is invariance to
image-altering variables which leave the identity of objects unchanged, such as
lighting, rotation, and scale. How do neural networks achieve this? Prior
mechanistic interpretability research has illuminated some invariance-building
circuitry in InceptionV1, but the results are limited and networks with
different architectures have remained largely unexplored. This work
investigates ResNet18 with a particular focus on its residual stream, an
architectural component which InceptionV1 lacks. We observe that many
convolutional channels in intermediate blocks exhibit scale invariant
properties, computed by the element-wise residual summation of scale
equivariant representations: the block input's smaller-scale copy with the
block pre-sum output's larger-scale copy. Through subsequent ablation
experiments, we attempt to causally link these neural properties with
scale-robust object recognition behavior. Our tentative findings suggest how
the residual stream computes scale invariance and its possible role in
behavior. Code is available at:
https://github.com/cest-andre/residual-stream-interp

</details>


### [196] [Federated Learning of Low-Rank One-Shot Image Detection Models in Edge Devices with Scalable Accuracy and Compute Complexity](https://arxiv.org/abs/2504.16515)
*Abdul Hannaan,Zubair Shah,Aiman Erbad,Amr Mohamed,Ali Safa*

Main category: cs.CV

TL;DR: LoRa-FL是一种新颖的联邦学习框架，用于在边缘设备上训练低秩单次图像检测模型，显著降低计算和通信开销，同时保持可扩展的准确性。


<details>
  <summary>Details</summary>
Motivation: 解决边缘设备上资源受限的问题，同时通过联邦学习实现轻量级图像识别模型的协作训练。

Method: 将低秩适应技术融入单次检测架构，结合联邦学习框架，在MNIST和CIFAR10数据集上进行实验验证。

Result: 在IID和非IID设置下，LoRa-FL实现了竞争性的检测性能，同时显著降低了通信带宽和计算复杂度。

Conclusion: LoRa-FL是一种有前景的解决方案，能够自适应地减少通信和计算开销，同时不牺牲模型准确性。

Abstract: This paper introduces a novel federated learning framework termed LoRa-FL
designed for training low-rank one-shot image detection models deployed on edge
devices. By incorporating low-rank adaptation techniques into one-shot
detection architectures, our method significantly reduces both computational
and communication overhead while maintaining scalable accuracy. The proposed
framework leverages federated learning to collaboratively train lightweight
image recognition models, enabling rapid adaptation and efficient deployment
across heterogeneous, resource-constrained devices. Experimental evaluations on
the MNIST and CIFAR10 benchmark datasets, both in an
independent-and-identically-distributed (IID) and non-IID setting, demonstrate
that our approach achieves competitive detection performance while
significantly reducing communication bandwidth and compute complexity. This
makes it a promising solution for adaptively reducing the communication and
compute power overheads, while not sacrificing model accuracy.

</details>


### [197] [Think Hierarchically, Act Dynamically: Hierarchical Multi-modal Fusion and Reasoning for Vision-and-Language Navigation](https://arxiv.org/abs/2504.16516)
*Junrong Yue,Yifan Zhang,Chuan Qin,Bo Li,Xiaomin Lie,Xinlei Yu,Wenxin Zhang,Zhendong Zhao*

Main category: cs.CV

TL;DR: 论文提出了一种多级融合与推理架构（MFRA），通过多模态特征融合和指令引导的推理，提升视觉与语言导航（VLN）任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖全局场景或对象级特征，难以捕捉跨模态的复杂交互，限制了导航准确性。

Method: MFRA采用分层融合机制，聚合从低级视觉线索到高级语义概念的多级特征，并结合推理模块动态整合上下文。

Result: 在REVERIE、R2R和SOON等基准数据集上，MFRA表现优于现有方法。

Conclusion: 多级模态融合能有效提升具身导航的决策准确性。

Abstract: Vision-and-Language Navigation (VLN) aims to enable embodied agents to follow
natural language instructions and reach target locations in real-world
environments. While prior methods often rely on either global scene
representations or object-level features, these approaches are insufficient for
capturing the complex interactions across modalities required for accurate
navigation. In this paper, we propose a Multi-level Fusion and Reasoning
Architecture (MFRA) to enhance the agent's ability to reason over visual
observations, language instructions and navigation history. Specifically, MFRA
introduces a hierarchical fusion mechanism that aggregates multi-level
features-ranging from low-level visual cues to high-level semantic
concepts-across multiple modalities. We further design a reasoning module that
leverages fused representations to infer navigation actions through
instruction-guided attention and dynamic context integration. By selectively
capturing and combining relevant visual, linguistic, and temporal signals, MFRA
improves decision-making accuracy in complex navigation scenarios. Extensive
experiments on benchmark VLN datasets including REVERIE, R2R, and SOON
demonstrate that MFRA achieves superior performance compared to
state-of-the-art methods, validating the effectiveness of multi-level modal
fusion for embodied navigation.

</details>


### [198] [SSLR: A Semi-Supervised Learning Method for Isolated Sign Language Recognition](https://arxiv.org/abs/2504.16640)
*Hasan Algafri,Hamzah Luqman,Sarah Alyami,Issam Laradji*

Main category: cs.CV

TL;DR: 论文提出了一种半监督学习方法（SSL）用于手语识别（SLR），通过伪标签方法标注未标记样本，利用姿态信息表示手势，并采用Transformer模型。实验表明，SSL方法在标注数据较少时优于全监督学习。


<details>
  <summary>Details</summary>
Motivation: 手语识别（SLR）面临标注数据稀缺的挑战，需要一种有效的方法利用未标记数据提升性能。

Method: 提出基于伪标签的半监督学习方法，使用姿态信息（骨骼关节点）作为输入，采用Transformer模型进行训练。

Result: 在WLASL-100数据集上，SSL方法在标注数据较少时表现优于全监督学习模型。

Conclusion: 半监督学习是解决手语识别数据稀缺问题的有效方法，尤其在标注数据有限时表现优异。

Abstract: Sign language is the primary communication language for people with disabling
hearing loss. Sign language recognition (SLR) systems aim to recognize sign
gestures and translate them into spoken language. One of the main challenges in
SLR is the scarcity of annotated datasets. To address this issue, we propose a
semi-supervised learning (SSL) approach for SLR (SSLR), employing a
pseudo-label method to annotate unlabeled samples. The sign gestures are
represented using pose information that encodes the signer's skeletal joint
points. This information is used as input for the Transformer backbone model
utilized in the proposed approach. To demonstrate the learning capabilities of
SSL across various labeled data sizes, several experiments were conducted using
different percentages of labeled data with varying numbers of classes. The
performance of the SSL approach was compared with a fully supervised
learning-based model on the WLASL-100 dataset. The obtained results of the SSL
model outperformed the supervised learning-based model with less labeled data
in many cases.

</details>


### [199] [Streetscape Analysis with Generative AI (SAGAI): Vision-Language Assessment and Mapping of Urban Scenes](https://arxiv.org/abs/2504.16538)
*Joan Perez,Giovanni Fusco*

Main category: cs.CV

TL;DR: SAGAI是一个基于生成式人工智能的街道景观分析工具，利用开源数据和视觉语言模型，通过自然语言提示生成结构化空间指标。


<details>
  <summary>Details</summary>
Motivation: 现有街道景观评估方法局限于形态学特性或需要大量人工定性评估，SAGAI旨在提供一种可扩展且无需特定训练的分析方案。

Method: 结合OpenStreetMap几何数据、Google街景图像和轻量级LLaVA模型，通过自然语言提示生成视觉评分，并自动映射到点和街道级别。

Result: 在尼斯和维也纳的案例中，SAGAI在城乡分类和商业特征检测中表现良好，但在人行道宽度估计上精度较低。

Conclusion: SAGAI无需专有软件或特定训练，适用于多种城市研究主题，如步行性、安全性和城市设计。

Abstract: Streetscapes are an essential component of urban space. Their assessment is
presently either limited to morphometric properties of their mass skeleton or
requires labor-intensive qualitative evaluations of visually perceived
qualities. This paper introduces SAGAI: Streetscape Analysis with Generative
Artificial Intelligence, a modular workflow for scoring street-level urban
scenes using open-access data and vision-language models. SAGAI integrates
OpenStreetMap geometries, Google Street View imagery, and a lightweight version
of the LLaVA model to generate structured spatial indicators from images via
customizable natural language prompts. The pipeline includes an automated
mapping module that aggregates visual scores at both the point and street
levels, enabling direct cartographic interpretation. It operates without
task-specific training or proprietary software dependencies, supporting
scalable and interpretable analysis of urban environments. Two exploratory case
studies in Nice and Vienna illustrate SAGAI's capacity to produce geospatial
outputs from vision-language inference. The initial results show strong
performance for binary urban-rural scene classification, moderate precision in
commercial feature detection, and lower estimates, but still informative, of
sidewalk width. Fully deployable by any user, SAGAI can be easily adapted to a
wide range of urban research themes, such as walkability, safety, or urban
design, through prompt modification alone.

</details>


### [200] [PMG: Progressive Motion Generation via Sparse Anchor Postures Curriculum Learning](https://arxiv.org/abs/2504.16722)
*Yingjie Xi,Jian Jun Zhang,Xiaosong Yang*

Main category: cs.CV

TL;DR: ProMoGen是一种结合轨迹引导和稀疏锚点运动控制的新框架，用于生成更可控和精确的人体运动。


<details>
  <summary>Details</summary>
Motivation: 现有方法在生成复杂动作或精确运动时存在局限性，需要更灵活的解决方案。

Method: ProMoGen通过分离全局轨迹和稀疏锚点运动，实现独立优化；引入SAP-CL课程学习策略提高稳定性。

Result: 实验表明，ProMoGen能生成生动多样的运动，显著优于现有方法。

Conclusion: ProMoGen提供了一种高效可控的运动生成框架，适用于多种控制场景。

Abstract: In computer animation, game design, and human-computer interaction,
synthesizing human motion that aligns with user intent remains a significant
challenge. Existing methods have notable limitations: textual approaches offer
high-level semantic guidance but struggle to describe complex actions
accurately; trajectory-based techniques provide intuitive global motion
direction yet often fall short in generating precise or customized character
movements; and anchor poses-guided methods are typically confined to synthesize
only simple motion patterns. To generate more controllable and precise human
motions, we propose \textbf{ProMoGen (Progressive Motion Generation)}, a novel
framework that integrates trajectory guidance with sparse anchor motion
control. Global trajectories ensure consistency in spatial direction and
displacement, while sparse anchor motions only deliver precise action guidance
without displacement. This decoupling enables independent refinement of both
aspects, resulting in a more controllable, high-fidelity, and sophisticated
motion synthesis. ProMoGen supports both dual and single control paradigms
within a unified training process. Moreover, we recognize that direct learning
from sparse motions is inherently unstable, we introduce \textbf{SAP-CL (Sparse
Anchor Posture Curriculum Learning)}, a curriculum learning strategy that
progressively adjusts the number of anchors used for guidance, thereby enabling
more precise and stable convergence. Extensive experiments demonstrate that
ProMoGen excels in synthesizing vivid and diverse motions guided by predefined
trajectory and arbitrary anchor frames. Our approach seamlessly integrates
personalized motion with structured guidance, significantly outperforming
state-of-the-art methods across multiple control scenarios.

</details>


### [201] [Detecting and Understanding Hateful Contents in Memes Through Captioning and Visual Question-Answering](https://arxiv.org/abs/2504.16723)
*Ali Anaissi,Junaid Akram,Kunal Chaturvedi,Ali Braytee*

Main category: cs.CV

TL;DR: 提出了一种多模态仇恨内容检测框架，结合OCR、字幕生成、子标签分类、RAG和VQA技术，显著提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 由于仇恨内容在模因中的隐蔽性和多模态特性，传统单模态检测系统难以有效识别，因此需要更先进的解决方案。

Method: 整合OCR提取文本、字幕生成描述图像、子标签分类细化仇恨内容、RAG检索上下文、VQA分析符号线索，形成多模态检测框架。

Result: 在Facebook仇恨模因数据集上，该框架在准确率和AUC-ROC上优于单模态和传统多模态模型。

Conclusion: 多模态方法能有效捕捉仇恨模因中的潜在信号，为复杂内容检测提供了新思路。

Abstract: Memes are widely used for humor and cultural commentary, but they are
increasingly exploited to spread hateful content. Due to their multimodal
nature, hateful memes often evade traditional text-only or image-only detection
systems, particularly when they employ subtle or coded references. To address
these challenges, we propose a multimodal hate detection framework that
integrates key components: OCR to extract embedded text, captioning to describe
visual content neutrally, sub-label classification for granular categorization
of hateful content, RAG for contextually relevant retrieval, and VQA for
iterative analysis of symbolic and contextual cues. This enables the framework
to uncover latent signals that simpler pipelines fail to detect. Experimental
results on the Facebook Hateful Memes dataset reveal that the proposed
framework exceeds the performance of unimodal and conventional multimodal
models in both accuracy and AUC-ROC.

</details>


### [202] [Federated EndoViT: Pretraining Vision Transformers via Federated Learning on Endoscopic Image Collections](https://arxiv.org/abs/2504.16612)
*Max Kirchner,Alexander C. Jenke,Sebastian Bodenstedt,Fiona R. Kolbinger,Oliver Saldanha,Jakob N. Kather,Martin Wagner,Stefanie Speidel*

Main category: cs.CV

TL;DR: 研究探讨了利用联邦学习训练基础模型，解决数据共享限制，实现无需数据传输的协作模型训练，应用于微创手术。


<details>
  <summary>Details</summary>
Motivation: 解决手术数据共享的隐私问题，同时实现多机构协作训练模型。

Method: 基于EndoViT研究，改进Masked Autoencoder，引入自适应FedSAM和SWA，预训练于Endo700k数据集，并在下游任务中微调评估。

Result: FedSAM改进的联邦MAE降低了重建损失，FL-EndoViT在数据有限时优于CEN-EndoViT，尤其在手术场景分割和动作三元组识别中表现突出。

Conclusion: 联邦学习为手术基础模型的隐私保护训练提供了可行方案，未来可探索视频模型以增强时空动态能力。

Abstract: Purpose: In this study, we investigate the training of foundation models
using federated learning to address data-sharing limitations and enable
collaborative model training without data transfer for minimally invasive
surgery. Methods: Inspired by the EndoViT study, we adapt the Masked
Autoencoder for federated learning, enhancing it with adaptive Sharpness-Aware
Minimization (FedSAM) and Stochastic Weight Averaging (SWA). Our model is
pretrained on the Endo700k dataset collection and later fine-tuned and
evaluated for tasks such as Semantic Segmentation, Action Triplet Recognition,
and Surgical Phase Recognition. Results: Our findings demonstrate that
integrating adaptive FedSAM into the federated MAE approach improves
pretraining, leading to a reduction in reconstruction loss per patch. The
application of FL-EndoViT in surgical downstream tasks results in performance
comparable to CEN-EndoViT. Furthermore, FL-EndoViT exhibits advantages over
CEN-EndoViT in surgical scene segmentation when data is limited and in action
triplet recognition when large datasets are used. Conclusion: These findings
highlight the potential of federated learning for privacy-preserving training
of surgical foundation models, offering a robust and generalizable solution for
surgical data science. Effective collaboration requires adapting federated
learning methods, such as the integration of FedSAM, which can accommodate the
inherent data heterogeneity across institutions. In future, exploring FL in
video-based models may enhance these capabilities by incorporating
spatiotemporal dynamics crucial for real-world surgical environments.

</details>


### [203] [V$^2$R-Bench: Holistically Evaluating LVLM Robustness to Fundamental Visual Variations](https://arxiv.org/abs/2504.16727)
*Zhiyuan Fan,Yumeng Wang,Sandeep Polisetty,Yi R.,Fung*

Main category: cs.CV

TL;DR: V²R-Bench是一个评估大型视觉语言模型（LVLMs）对视觉变化鲁棒性的基准框架，揭示了模型在简单任务中的脆弱性，并指出其架构缺陷。


<details>
  <summary>Details</summary>
Motivation: 尽管LVLMs在多种视觉语言任务中表现出色，但其对自然场景中不可避免的视觉变化（如位置、尺度、方向等）的鲁棒性尚未充分研究。

Method: 提出V²R-Bench框架，包括自动化评估数据集生成和鲁棒性评估指标，对21个LVLMs进行广泛测试，并通过组件级分析和可视化方法识别问题根源。

Result: 发现LVLMs对视觉变化表现脆弱，存在位置偏差和人类视觉敏锐度阈值，问题源于架构缺陷和多模态对齐不足。

Conclusion: 研究呼吁未来LVLM设计需进行架构创新，以提升对视觉变化的鲁棒性。

Abstract: Large Vision Language Models (LVLMs) excel in various vision-language tasks.
Yet, their robustness to visual variations in position, scale, orientation, and
context that objects in natural scenes inevitably exhibit due to changes in
viewpoint and environment remains largely underexplored. To bridge this gap, we
introduce V$^2$R-Bench, a comprehensive benchmark framework for evaluating
Visual Variation Robustness of LVLMs, which encompasses automated evaluation
dataset generation and principled metrics for thorough robustness assessment.
Through extensive evaluation on 21 LVLMs, we reveal a surprising vulnerability
to visual variations, in which even advanced models that excel at complex
vision-language tasks significantly underperform on simple tasks such as object
recognition. Interestingly, these models exhibit a distinct visual position
bias that contradicts theories of effective receptive fields, and demonstrate a
human-like visual acuity threshold. To identify the source of these
vulnerabilities, we present a systematic framework for component-level
analysis, featuring a novel visualization approach for aligned visual features.
Results show that these vulnerabilities stem from error accumulation in the
pipeline architecture and inadequate multimodal alignment. Complementary
experiments with synthetic data further demonstrate that these limitations are
fundamentally architectural deficiencies, scoring the need for architectural
innovations in future LVLM designs.

</details>


### [204] [SemanticSugarBeets: A Multi-Task Framework and Dataset for Inspecting Harvest and Storage Characteristics of Sugar Beets](https://arxiv.org/abs/2504.16684)
*Gerardus Croonen,Andreas Trondl,Julia Simon,Daniel Steininger*

Main category: cs.CV

TL;DR: 论文提出了一种基于RGB图像的两阶段方法，用于检测、语义分割和质量估计收获后储存的甜菜，并提供了一个高质量标注数据集。实验表明，该方法在检测和分割任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 甜菜在储存过程中因微生物和多余植被导致糖分损失，自动化视觉检测可提升生产链效率。

Method: 采用两阶段方法，结合多种模型架构和编码器，评估不同图像尺寸和环境条件对检测和分割任务的影响。

Result: 检测任务mAP50-95达98.8，最佳分割模型mIoU为64.0。

Conclusion: 该方法在甜菜检测和分割任务中表现优异，为生产链质量保障提供了有效工具。

Abstract: While sugar beets are stored prior to processing, they lose sugar due to
factors such as microorganisms present in adherent soil and excess vegetation.
Their automated visual inspection promises to aide in quality assurance and
thereby increase efficiency throughout the processing chain of sugar
production. In this work, we present a novel high-quality annotated dataset and
two-stage method for the detection, semantic segmentation and mass estimation
of post-harvest and post-storage sugar beets in monocular RGB images. We
conduct extensive ablation experiments for the detection of sugar beets and
their fine-grained semantic segmentation regarding damages, rot, soil adhesion
and excess vegetation. For these tasks, we evaluate multiple image sizes, model
architectures and encoders, as well as the influence of environmental
conditions. Our experiments show an mAP50-95 of 98.8 for sugar-beet detection
and an mIoU of 64.0 for the best-performing segmentation model.

</details>


### [205] [Towards Explainable AI: Multi-Modal Transformer for Video-based Image Description Generation](https://arxiv.org/abs/2504.16788)
*Lakshita Agarwal,Bindu Verma*

Main category: cs.CV

TL;DR: 论文提出了一种结合视觉和文本模态的新框架，用于从视频数据生成自然语言描述，性能优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 视频动作的理解与分析对智能监控和自主系统等应用至关重要，需要生成上下文相关的描述。

Method: 使用ResNet50提取视频帧的视觉特征，并通过基于GPT-2的编码器-解码器模型处理，结合多头自注意力和交叉注意力技术对齐文本与视觉表示。

Result: 在BLEU-4、CIDEr、METEOR和ROUGE-L等指标上表现优异，BDD-X和MSVD数据集上的分数分别为0.755/0.778、1.235/1.315、0.312/0.329和0.782/0.795。

Conclusion: 该研究通过生成类人且上下文相关的描述，推动了可解释AI的发展，并提升了实际应用效果。

Abstract: Understanding and analyzing video actions are essential for producing
insightful and contextualized descriptions, especially for video-based
applications like intelligent monitoring and autonomous systems. The proposed
work introduces a novel framework for generating natural language descriptions
from video datasets by combining textual and visual modalities. The suggested
architecture makes use of ResNet50 to extract visual features from video frames
that are taken from the Microsoft Research Video Description Corpus (MSVD), and
Berkeley DeepDrive eXplanation (BDD-X) datasets. The extracted visual
characteristics are converted into patch embeddings and then run through an
encoder-decoder model based on Generative Pre-trained Transformer-2 (GPT-2). In
order to align textual and visual representations and guarantee high-quality
description production, the system uses multi-head self-attention and
cross-attention techniques. The model's efficacy is demonstrated by performance
evaluation using BLEU (1-4), CIDEr, METEOR, and ROUGE-L. The suggested
framework outperforms traditional methods with BLEU-4 scores of 0.755 (BDD-X)
and 0.778 (MSVD), CIDEr scores of 1.235 (BDD-X) and 1.315 (MSVD), METEOR scores
of 0.312 (BDD-X) and 0.329 (MSVD), and ROUGE-L scores of 0.782 (BDD-X) and
0.795 (MSVD). By producing human-like, contextually relevant descriptions,
strengthening interpretability, and improving real-world applications, this
research advances explainable AI.

</details>


### [206] [Generalized Neighborhood Attention: Multi-dimensional Sparse Attention at the Speed of Light](https://arxiv.org/abs/2504.16922)
*Ali Hassani,Fengzhe Zhou,Aditya Kane,Jiannan Huang,Chieh-Yun Chen,Min Shi,Steven Walton,Markus Hoehnerbach,Vijay Thakkar,Michael Isaev,Qinsheng Zhang,Bing Xu,Haicheng Wu,Wen-mei Hwu,Ming-Yu Liu,Humphrey Shi*

Main category: cs.CV

TL;DR: 论文研究了基于局部性的稀疏注意力机制，提出了广义邻域注意力（GNA），并通过模拟器和实际实现验证了其性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有稀疏注意力机制在速度和硬件适应性上表现不佳，而许多先进模型受限于注意力机制的O(n^2)复杂度，需要可靠的稀疏性解决方案。

Method: 引入GNA描述多种注意力模式，设计模拟器预测性能上限，并在NVIDIA Blackwell架构上实现GNA。

Result: GNA在完美块稀疏情况下实现理论最大加速，FP16下有效利用率达1.3 petaFLOPs/秒，并在多个生成模型中带来28%-46%的端到端加速。

Conclusion: GNA为稀疏注意力提供了高效实现方案，性能显著提升，相关工具将通过NATTEN项目开源。

Abstract: Many sparse attention mechanisms such as Neighborhood Attention have
typically failed to consistently deliver speedup over the self attention
baseline. This is largely due to the level of complexity in attention
infrastructure, and the rapid evolution of AI hardware architecture. At the
same time, many state-of-the-art foundational models, particularly in computer
vision, are heavily bound by attention, and need reliable sparsity to escape
the O(n^2) complexity. In this paper, we study a class of promising sparse
attention mechanisms that focus on locality, and aim to develop a better
analytical model of their performance improvements. We first introduce
Generalized Neighborhood Attention (GNA), which can describe sliding window,
strided sliding window, and blocked attention. We then consider possible design
choices in implementing these approaches, and create a simulator that can
provide much more realistic speedup upper bounds for any given setting.
Finally, we implement GNA on top of a state-of-the-art fused multi-headed
attention (FMHA) kernel designed for the NVIDIA Blackwell architecture in
CUTLASS. Our implementation can fully realize the maximum speedup theoretically
possible in many perfectly block-sparse cases, and achieves an effective
utilization of 1.3 petaFLOPs/second in FP16. In addition, we plug various GNA
configurations into off-the-shelf generative models, such as Cosmos-7B,
HunyuanVideo, and FLUX, and show that it can deliver 28% to 46% end-to-end
speedup on B200 without any fine-tuning. We will open source our simulator and
Blackwell kernels directly through the NATTEN project.

</details>


### [207] [BadVideo: Stealthy Backdoor Attack against Text-to-Video Generation](https://arxiv.org/abs/2504.16907)
*Ruotong Wang,Mingli Zhu,Jiarong Ou,Rui Chen,Xin Tao,Pengfei Wan,Baoyuan Wu*

Main category: cs.CV

TL;DR: BadVideo是首个针对文本到视频（T2V）生成模型的后门攻击框架，利用视频中的冗余信息嵌入隐藏有害内容，具有高隐蔽性。


<details>
  <summary>Details</summary>
Motivation: 探索T2V生成模型的对抗性漏洞，揭示其潜在风险。

Method: 通过时空组合和动态元素变换两种策略设计目标对抗输出。

Result: BadVideo攻击成功率高，同时保持原始语义和干净输入的性能。

Conclusion: 揭示了T2V模型的对抗性漏洞，呼吁关注潜在风险和滥用。

Abstract: Text-to-video (T2V) generative models have rapidly advanced and found
widespread applications across fields like entertainment, education, and
marketing. However, the adversarial vulnerabilities of these models remain
rarely explored. We observe that in T2V generation tasks, the generated videos
often contain substantial redundant information not explicitly specified in the
text prompts, such as environmental elements, secondary objects, and additional
details, providing opportunities for malicious attackers to embed hidden
harmful content. Exploiting this inherent redundancy, we introduce BadVideo,
the first backdoor attack framework tailored for T2V generation. Our attack
focuses on designing target adversarial outputs through two key strategies: (1)
Spatio-Temporal Composition, which combines different spatiotemporal features
to encode malicious information; (2) Dynamic Element Transformation, which
introduces transformations in redundant elements over time to convey malicious
information. Based on these strategies, the attacker's malicious target
seamlessly integrates with the user's textual instructions, providing high
stealthiness. Moreover, by exploiting the temporal dimension of videos, our
attack successfully evades traditional content moderation systems that
primarily analyze spatial information within individual frames. Extensive
experiments demonstrate that BadVideo achieves high attack success rates while
preserving original semantics and maintaining excellent performance on clean
inputs. Overall, our work reveals the adversarial vulnerability of T2V models,
calling attention to potential risks and misuse. Our project page is at
https://wrt2000.github.io/BadVideo2025/.

</details>


<div id='q-fin.PM'></div>

# q-fin.PM [[Back]](#toc)

### [208] [Efficient Portfolio Selection through Preference Aggregation with Quicksort and the Bradley--Terry Model](https://arxiv.org/abs/2504.16093)
*Yurun Ge,Lucas Böttcher,Tom Chou,Maria R. D'Orsogna*

Main category: q-fin.PM

TL;DR: 论文提出了一种基于Quicksort和Bradley-Terry模型的资源分配方法，用于在不确定性下优化项目选择，其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决在不确定性下如何分配有限资源以最大化长期效益的问题，适用于创新项目、研究资助和公共项目选择等场景。

Method: 采用Quicksort和Bradley-Terry模型，通过代理对项目对的“胜率”评估，聚合这些评估并排名项目。

Result: 提出的方法性能优于现有两种最有效的聚合方法，并能结合采样技术减少配对比较次数。

Conclusion: Bradley-Terry模型在项目组合选择中具有实际应用潜力，能有效优化资源分配。

Abstract: How to allocate limited resources to projects that will yield the greatest
long-term benefits is a problem that often arises in decision-making under
uncertainty. For example, organizations may need to evaluate and select
innovation projects with risky returns. Similarly, when allocating resources to
research projects, funding agencies are tasked with identifying the most
promising proposals based on idiosyncratic criteria. Finally, in participatory
budgeting, a local community may need to select a subset of public projects to
fund. Regardless of context, agents must estimate the uncertain values of a
potentially large number of projects. Developing parsimonious methods to
compare these projects, and aggregating agent evaluations so that the overall
benefit is maximized, are critical in assembling the best project portfolio.
Unlike in standard sorting algorithms, evaluating projects on the basis of
uncertain long-term benefits introduces additional complexities. We propose
comparison rules based on Quicksort and the Bradley--Terry model, which
connects rankings to pairwise "win" probabilities. In our model, each agent
determines win probabilities of a pair of projects based on his or her specific
evaluation of the projects' long-term benefit. The win probabilities are then
appropriately aggregated and used to rank projects. Several of the methods we
propose perform better than the two most effective aggregation methods
currently available. Additionally, our methods can be combined with sampling
techniques to significantly reduce the number of pairwise comparisons. We also
discuss how the Bradley--Terry portfolio selection approach can be implemented
in practice.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [209] [PINN-MEP: Continuous Neural Representations for Minimum-Energy Path Discovery in Molecular Systems](https://arxiv.org/abs/2504.16381)
*Magnus Petersen,Roberto Covino*

Main category: physics.chem-ph

TL;DR: 论文提出了一种基于物理信息神经网络（PINNs）的方法，用于高效生成分子系统的过渡路径，解决了传统采样方法在高维系统和能量壁垒问题上的不足。


<details>
  <summary>Details</summary>
Motivation: 分子系统中的构象转变（如离子通道蛋白的开关状态）在生物学中至关重要，但传统方法（如分子动力学或MCMC）因高维性和高能量壁垒难以捕捉这些罕见事件。

Method: 将过渡路径生成重新定义为连续优化问题，利用物理信息神经网络（PINNs）和可微分分子动力学力场，通过自动微分高效发现物理真实的过渡路径。

Result: 方法在两种蛋白质（包括一个包含8,300多个原子的水合BPTI系统）上验证了有效性，无需昂贵的路径采样。

Conclusion: 该方法为高维分子系统的构象转变研究提供了一种高效且物理真实的解决方案。

Abstract: Characterizing conformational transitions in physical systems remains a
fundamental challenge in the computational sciences. Traditional sampling
methods like molecular dynamics (MD) or MCMC often struggle with the
high-dimensional nature of molecular systems and the high energy barriers of
transitions between stable states. While these transitions are rare events in
simulation timescales, they often represent the most biologically significant
processes - for example, the conformational change of an ion channel protein
from its closed to open state, which controls cellular ion flow and is crucial
for neural signaling. Such transitions in real systems may take milliseconds to
seconds but could require months or years of continuous simulation to observe
even once. We present a method that reformulates transition path generation as
a continuous optimization problem solved through physics-informed neural
networks (PINNs) inspired by string methods for minimum-energy path (MEP)
generation. By representing transition paths as implicit neural functions and
leveraging automatic differentiation with differentiable molecular dynamics
force fields, our method enables the efficient discovery of physically
realistic transition pathways without requiring expensive path sampling. We
demonstrate our method's effectiveness on two proteins, including an explicitly
hydrated bovine pancreatic trypsin inhibitor (BPTI) system with over 8,300
atoms.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [210] [Common Functional Decompositions Can Mis-attribute Differences in Outcomes Between Populations](https://arxiv.org/abs/2504.16864)
*Manuel Quintero,William T. Stephenson,Advik Shreekumar,Tamara Broderick*

Main category: stat.ME

TL;DR: 论文探讨了如何扩展Kitagawa-Oaxaca-Blinder（KOB）分解方法以处理非线性关系，指出现有方法（如功能ANOVA和ALE）在特定情况下会错误归因差异，并提出避免误归因的条件。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决KOB分解方法仅适用于线性关系的局限性，并探索如何利用现代机器学习中的非线性功能分解方法扩展KOB。

Method: 论文分析了功能ANOVA和Accumulated Local Effects（ALE）两种常见分解方法，发现它们在简单例子中会错误归因差异，并提出避免误归因的通用条件。

Result: 结果表明，如果分解方法独立于输入分布，则不会出现误归因。此外，论文推测任何依赖于协变量分布的合理加性分解都可能出现误归因。

Conclusion: 结论是扩展KOB分解需要满足特定条件以避免误归因，并建议未来研究关注分布无关的分解方法。

Abstract: In science and social science, we often wish to explain why an outcome is
different in two populations. For instance, if a jobs program benefits members
of one city more than another, is that due to differences in program
participants (particular covariates) or the local labor markets (outcomes given
covariates)? The Kitagawa-Oaxaca-Blinder (KOB) decomposition is a standard tool
in econometrics that explains the difference in the mean outcome across two
populations. However, the KOB decomposition assumes a linear relationship
between covariates and outcomes, while the true relationship may be
meaningfully nonlinear. Modern machine learning boasts a variety of nonlinear
functional decompositions for the relationship between outcomes and covariates
in one population. It seems natural to extend the KOB decomposition using these
functional decompositions. We observe that a successful extension should not
attribute the differences to covariates -- or, respectively, to outcomes given
covariates -- if those are the same in the two populations. Unfortunately, we
demonstrate that, even in simple examples, two common decompositions --
functional ANOVA and Accumulated Local Effects -- can attribute differences to
outcomes given covariates, even when they are identical in two populations. We
provide a characterization of when functional ANOVA misattributes, as well as a
general property that any discrete decomposition must satisfy to avoid
misattribution. We show that if the decomposition is independent of its input
distribution, it does not misattribute. We further conjecture that
misattribution arises in any reasonable additive decomposition that depends on
the distribution of the covariates.

</details>

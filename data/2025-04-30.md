<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 41]
- [cs.AI](#cs.AI) [Total: 31]
- [cs.LG](#cs.LG) [Total: 87]
- [cs.PF](#cs.PF) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [physics.optics](#physics.optics) [Total: 1]
- [cs.NE](#cs.NE) [Total: 1]
- [cs.MM](#cs.MM) [Total: 1]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]
- [math.NA](#math.NA) [Total: 1]
- [eess.IV](#eess.IV) [Total: 3]
- [q-bio.BM](#q-bio.BM) [Total: 1]
- [cs.FL](#cs.FL) [Total: 1]
- [cs.DL](#cs.DL) [Total: 1]
- [eess.SY](#eess.SY) [Total: 1]
- [quant-ph](#quant-ph) [Total: 1]
- [cs.DS](#cs.DS) [Total: 1]
- [cs.CR](#cs.CR) [Total: 8]
- [cs.MA](#cs.MA) [Total: 2]
- [cs.IR](#cs.IR) [Total: 12]
- [cs.CV](#cs.CV) [Total: 14]
- [math.OC](#math.OC) [Total: 1]
- [cs.NI](#cs.NI) [Total: 1]
- [cs.DC](#cs.DC) [Total: 5]
- [cs.CY](#cs.CY) [Total: 2]
- [stat.ML](#stat.ML) [Total: 4]
- [cs.HC](#cs.HC) [Total: 2]
- [cs.RO](#cs.RO) [Total: 3]
- [physics.ao-ph](#physics.ao-ph) [Total: 3]
- [cs.SE](#cs.SE) [Total: 12]
- [cs.SD](#cs.SD) [Total: 4]
- [q-fin.ST](#q-fin.ST) [Total: 2]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [It's the same but not the same: Do LLMs distinguish Spanish varieties?](https://arxiv.org/abs/2504.20049)
*Marina Mayor-Rocher,Cristina Pozo,Nina Melero,Gonzalo Martínez,María Grandury,Pedro Reviriego*

Main category: cs.CL

TL;DR: 研究评估了九种语言模型识别七种西班牙语变体的能力，发现GPT-4o是唯一能识别西班牙语多样性的模型。


<details>
  <summary>Details</summary>
Motivation: 西班牙语具有丰富的方言变体，但现有语言模型对其多样性的识别能力尚不明确。

Method: 通过多选题测试评估模型对七种西班牙语变体的识别能力。

Result: 所有模型中，GPT-4o表现最佳，能识别西班牙语的多样性，而半岛西班牙语最易被识别。

Conclusion: GPT-4o在识别西班牙语方言变体方面表现突出，但其他模型仍需改进。

Abstract: In recent years, large language models (LLMs) have demonstrated a high
capacity for understanding and generating text in Spanish. However, with five
hundred million native speakers, Spanish is not a homogeneous language but
rather one rich in diatopic variations spanning both sides of the Atlantic. For
this reason, in this study, we evaluate the ability of nine language models to
identify and distinguish the morphosyntactic and lexical peculiarities of seven
varieties of Spanish (Andean, Antillean, Continental Caribbean, Chilean,
Peninsular, Mexican and Central American and Rioplatense) through a
multiple-choice test. The results indicate that the Peninsular Spanish variety
is the best identified by all models and that, among them, GPT-4o is the only
model capable of recognizing the variability of the Spanish language.
  --
  En los \'ultimos a\~nos, los grandes modelos de lenguaje (LLMs, por sus
siglas en ingl\'es) han demostrado una alta capacidad para comprender y generar
texto en espa\~nol. Sin embargo, con quinientos millones de hablantes nativos,
la espa\~nola no es una lengua homog\'enea, sino rica en variedades
diat\'opicas que se extienden a ambos lados del Atl\'antico. Por todo ello,
evaluamos en este trabajo la capacidad de nueve modelos de lenguaje de
identificar y discernir las peculiaridades morfosint\'acticas y l\'exicas de
siete variedades de espa\~nol (andino, antillano, caribe\~no continental,
chileno, espa\~nol peninsular, mexicano y centroamericano y rioplatense)
mediante un test de respuesta m\'ultiple. Los resultados obtenidos indican que
la variedad de espa\~nol peninsular es la mejor identificada por todos los
modelos y que, de entre todos, GPT-4o es el \'unico modelo capaz de identificar
la variabilidad de la lengua espa\~nola.

</details>


### [2] [Evaluating Large Language Models on Multiword Expressions in Multilingual and Code-Switched Contexts](https://arxiv.org/abs/2504.20051)
*Frances Laureano De Leon,Harish Tayyar Madabushi,Mark G. Lee*

Main category: cs.CL

TL;DR: 该研究评估了大型语言模型处理多词表达歧义的能力，发现即使是最新模型如GPT-4，在检测和语义任务上也未能超越基线模型，尤其在处理模糊多词表达时表现较差。


<details>
  <summary>Details</summary>
Motivation: 多词表达具有非组合意义和句法不规则性，其歧义性对语言模型提出了挑战。研究旨在评估模型处理此类语言微妙性的能力。

Method: 通过评估英语、葡萄牙语和加利西亚语中的模型表现，使用新颖的代码切换数据集和任务，比较模型在多词表达检测和语义任务中的表现。

Result: 大型语言模型（包括GPT-4）在检测和语义任务上表现不佳，尤其在处理模糊多词表达时，未能超越基线模型xlm-roBERTa-base。

Conclusion: 多词表达，尤其是模糊表达，仍然是语言模型的挑战，表明模型在处理语言微妙性方面仍有局限。

Abstract: Multiword expressions, characterised by non-compositional meanings and
syntactic irregularities, are an example of nuanced language. These expressions
can be used literally or idiomatically, leading to significant changes in
meaning. While large language models have demonstrated strong performance
across many tasks, their ability to handle such linguistic subtleties remains
uncertain. Therefore, this study evaluates how state-of-the-art language models
process the ambiguity of potentially idiomatic multiword expressions,
particularly in contexts that are less frequent, where models are less likely
to rely on memorisation. By evaluating models across in Portuguese and
Galician, in addition to English, and using a novel code-switched dataset and a
novel task, we find that large language models, despite their strengths,
struggle with nuanced language. In particular, we find that the latest models,
including GPT-4, fail to outperform the xlm-roBERTa-base baselines in both
detection and semantic tasks, with especially poor performance on the novel
tasks we introduce, despite its similarity to existing tasks. Overall, our
results demonstrate that multiword expressions, especially those which are
ambiguous, continue to be a challenge to models.

</details>


### [3] [Understanding and Mitigating Risks of Generative AI in Financial Services](https://arxiv.org/abs/2504.20086)
*Sebastian Gehrmann,Claire Huang,Xian Teng,Sergei Yurovski,Iyanuoluwa Shode,Chirag S. Patel,Arjun Bhorkar,Naveen Thomas,John Doucette,David Rosenberg,Mark Dredze,David Rabinowitz*

Main category: cs.CL

TL;DR: 论文探讨了金融服务业中生成式AI内容安全的具体考虑，提出了风险分类法，并评估现有开源技术防护措施的不足。


<details>
  <summary>Details</summary>
Motivation: 现有研究过于关注通用领域的模型评估（如毒性、偏见），而忽视了专业领域的法律和监管要求。金融服务业需要特定的AI内容安全框架。

Method: 提出金融服务业AI内容风险分类法，通过红队活动收集数据，评估现有开源防护措施的覆盖范围。

Result: 现有防护措施未能检测到大部分讨论的内容风险。

Conclusion: 金融服务业需要更专业的AI内容安全解决方案，现有通用防护措施不足以满足需求。

Abstract: To responsibly develop Generative AI (GenAI) products, it is critical to
define the scope of acceptable inputs and outputs. What constitutes a "safe"
response is an actively debated question. Academic work puts an outsized focus
on evaluating models by themselves for general purpose aspects such as
toxicity, bias, and fairness, especially in conversational applications being
used by a broad audience. In contrast, less focus is put on considering
sociotechnical systems in specialized domains. Yet, those specialized systems
can be subject to extensive and well-understood legal and regulatory scrutiny.
These product-specific considerations need to be set in industry-specific laws,
regulations, and corporate governance requirements. In this paper, we aim to
highlight AI content safety considerations specific to the financial services
domain and outline an associated AI content risk taxonomy. We compare this
taxonomy to existing work in this space and discuss implications of risk
category violations on various stakeholders. We evaluate how existing
open-source technical guardrail solutions cover this taxonomy by assessing them
on data collected via red-teaming activities. Our results demonstrate that
these guardrails fail to detect most of the content risks we discuss.

</details>


### [4] [Toward Evaluative Thinking: Meta Policy Optimization with Evolving Reward Models](https://arxiv.org/abs/2504.20157)
*Zae Myung Kim,Chanwoo Park,Vipul Raheja,Dongyeop Kang*

Main category: cs.CL

TL;DR: Meta Policy Optimization (MPO) 通过动态调整奖励模型的提示，解决了奖励信号易受攻击和依赖人工设计的问题，提升了语言模型的稳定性和适应性。


<details>
  <summary>Details</summary>
Motivation: 传统奖励对齐方法存在奖励信号易受攻击和依赖人工设计的问题，限制了语言模型的稳定性和适应性。

Method: 引入 MPO 框架，通过元奖励模型动态调整奖励模型的提示，提供自适应奖励信号，减少人工设计需求。

Result: MPO 在多项任务中表现优异，性能与手工设计的奖励提示相当或更好，且无需专门设计。

Conclusion: MPO 解决了奖励对齐的理论和实践挑战，为更鲁棒和自适应的对齐策略铺平了道路。

Abstract: Reward-based alignment methods for large language models (LLMs) face two key
limitations: vulnerability to reward hacking, where models exploit flaws in the
reward signal; and reliance on brittle, labor-intensive prompt engineering when
LLMs are used as reward models. We introduce Meta Policy Optimization (MPO), a
framework that addresses these challenges by integrating a meta-reward model
that dynamically refines the reward model's prompt throughout training. In MPO,
the meta-reward model monitors the evolving training context and continuously
adjusts the reward model's prompt to maintain high alignment, providing an
adaptive reward signal that resists exploitation by the policy. This
meta-learning approach promotes a more stable policy optimization, and greatly
reduces the need for manual reward prompt design. It yields performance on par
with or better than models guided by extensively hand-crafted reward prompts.
Furthermore, we show that MPO maintains its effectiveness across diverse tasks,
such as question answering and mathematical reasoning, without requiring
specialized reward designs. Beyond standard RLAIF, MPO's meta-learning
formulation is readily extensible to higher-level alignment frameworks.
Overall, this method addresses theoretical and practical challenges in
reward-based RL alignment for LLMs, paving the way for more robust and
adaptable alignment strategies. The code and models will be publicly shared.

</details>


### [5] [MICE for CATs: Model-Internal Confidence Estimation for Calibrating Agents with Tools](https://arxiv.org/abs/2504.20168)
*Nishant Subramani,Jason Eisner,Justin Svegliato,Benjamin Van Durme,Yu Su,Sam Thomson*

Main category: cs.CL

TL;DR: 论文提出了一种新型模型内部置信度估计器（MICE），通过解码语言模型的中间层并计算相似度分数，结合概率分类器评估置信度，显著提升了工具调用的实用性和安全性。


<details>
  <summary>Details</summary>
Motivation: 工具使用代理需要兼具实用性和安全性，但现有模型的置信度校准较差，无法有效权衡风险与收益。

Method: MICE通过解码语言模型的中间层（使用logitLens），计算各层生成与最终输出的相似度，并利用学习的概率分类器评估置信度。

Result: 在STE数据集上，MICE在平滑预期校准误差上优于基线；使用MICE置信度显著提升了预期工具调用效用，且能零样本泛化到未见API。

Conclusion: MICE是一种高效、通用的置信度估计方法，能显著提升工具调用的实用性和安全性，代码已开源。

Abstract: Tool-using agents that act in the world need to be both useful and safe.
Well-calibrated model confidences can be used to weigh the risk versus reward
of potential actions, but prior work shows that many models are poorly
calibrated. Inspired by interpretability literature exploring the internals of
models, we propose a novel class of model-internal confidence estimators (MICE)
to better assess confidence when calling tools. MICE first decodes from each
intermediate layer of the language model using logitLens and then computes
similarity scores between each layer's generation and the final output. These
features are fed into a learned probabilistic classifier to assess confidence
in the decoded output. On the simulated trial and error (STE) tool-calling
dataset using Llama3 models, we find that MICE beats or matches the baselines
on smoothed expected calibration error. Using MICE confidences to determine
whether to call a tool significantly improves over strong baselines on a new
metric, expected tool-calling utility. Further experiments show that MICE is
sample-efficient, can generalize zero-shot to unseen APIs, and results in
higher tool-calling utility in scenarios with varying risk levels. Our code is
open source, available at https://github.com/microsoft/mice_for_cats.

</details>


### [6] [A Multimodal Pipeline for Clinical Data Extraction: Applying Vision-Language Models to Scans of Transfusion Reaction Reports](https://arxiv.org/abs/2504.20220)
*Henning Schäfer,Cynthia S. Schmidt,Johannes Wutzkowsky,Kamil Lorek,Lea Reinartz,Johannes Rückert,Christian Temme,Britta Böckmann,Peter A. Horn,Christoph M. Friedrich*

Main category: cs.CL

TL;DR: 本文提出了一种开源流程，用于从扫描文档中提取和分类复选框数据，以减少人工转录错误并简化工作流程。


<details>
  <summary>Details</summary>
Motivation: 尽管电子健康记录日益普及，但许多流程仍依赖纸质文档，导致手动转录耗时且易出错。

Method: 方法结合了复选框检测、多语言OCR和多语言视觉语言模型（VLMs）。

Result: 该流程在2017至2024年的黄金标准数据上表现出高精度和召回率，减少了管理负担并提高了报告准确性。

Conclusion: 开源设计鼓励其他复选框丰富文档类型的自托管解析。

Abstract: Despite the growing adoption of electronic health records, many processes
still rely on paper documents, reflecting the heterogeneous real-world
conditions in which healthcare is delivered. The manual transcription process
is time-consuming and prone to errors when transferring paper-based data to
digital formats. To streamline this workflow, this study presents an
open-source pipeline that extracts and categorizes checkbox data from scanned
documents. Demonstrated on transfusion reaction reports, the design supports
adaptation to other checkbox-rich document types. The proposed method
integrates checkbox detection, multilingual optical character recognition (OCR)
and multilingual vision-language models (VLMs). The pipeline achieves high
precision and recall compared against annually compiled gold-standards from
2017 to 2024. The result is a reduction in administrative workload and accurate
regulatory reporting. The open-source availability of this pipeline encourages
self-hosted parsing of checkbox forms.

</details>


### [7] [A Platform for Generating Educational Activities to Teach English as a Second Language](https://arxiv.org/abs/2504.20251)
*Aiala Rosá,Santiago Góngora,Juan Pablo Filevich,Ignacio Sastre,Laura Musto,Brian Carpenter,Luis Chiruzzo*

Main category: cs.CL

TL;DR: 介绍了一个基于自然语言处理技术的教育平台，用于生成英语作为外语的教学活动，包括游戏和练习，支持教师自定义内容，并计划扩展功能和提升性能。


<details>
  <summary>Details</summary>
Motivation: 为英语作为外语的教学提供多样化的教育活动，结合自然语言处理技术提升教学效果。

Method: 平台利用自然语言处理技术生成游戏和练习，支持半自动资源创建和教师自定义内容，并计划整合图像和文本生成功能。

Result: 平台已部署并提供多种活动，未来计划迁移至更强大的服务器以提升性能。

Conclusion: 平台展示了结合自然语言处理技术在教育中的潜力，未来将继续扩展功能和优化性能。

Abstract: We present a platform for the generation of educational activities oriented
to teaching English as a foreign language. The different activities -- games
and language practice exercises -- are strongly based on Natural Language
Processing techniques. The platform offers the possibility of playing
out-of-the-box games, generated from resources created semi-automatically and
then manually curated. It can also generate games or exercises of greater
complexity from texts entered by teachers, providing a stage of review and
edition of the generated content before use. As a way of expanding the variety
of activities in the platform, we are currently experimenting with image and
text generation. In order to integrate them and improve the performance of
other neural tools already integrated, we are working on migrating the platform
to a more powerful server. In this paper we describe the development of our
platform and its deployment for end users, discussing the challenges faced and
how we overcame them, and also detail our future work plans.

</details>


### [8] [Enhancing Systematic Reviews with Large Language Models: Using GPT-4 and Kimi](https://arxiv.org/abs/2504.20276)
*Dandan Chen Kaptur,Yue Huang,Xuejun Ryan Ji,Yanhui Guo,Bradley Kaptur*

Main category: cs.CL

TL;DR: 研究比较了GPT-4和Kimi两种大语言模型在系统综述中的表现，发现其性能受数据量和问题复杂度影响。


<details>
  <summary>Details</summary>
Motivation: 探讨大语言模型在系统综述任务中的适用性，并与人工生成的代码进行比较。

Method: 通过对比LLM生成的代码与人工生成的代码，评估模型在系统综述中的表现。

Result: LLM的性能因数据量和问题复杂度而异。

Conclusion: 大语言模型在系统综述中的应用需考虑数据量和问题复杂度的影响。

Abstract: This research delved into GPT-4 and Kimi, two Large Language Models (LLMs),
for systematic reviews. We evaluated their performance by comparing
LLM-generated codes with human-generated codes from a peer-reviewed systematic
review on assessment. Our findings suggested that the performance of LLMs
fluctuates by data volume and question complexity for systematic reviews.

</details>


### [9] [UD-English-CHILDES: A Collected Resource of Gold and Silver Universal Dependencies Trees for Child Language Interactions](https://arxiv.org/abs/2504.20304)
*Xiulin Yang,Zhuoxuan Ju,Lanni Bu,Zoey Liu,Nathan Schneider*

Main category: cs.CL

TL;DR: 本文介绍了UD-English-CHILDES，这是首个基于CHILDES数据的官方发布的Universal Dependencies树库，统一了11名儿童及其照顾者的48k句子标注，并提供了1M银标准句子。


<details>
  <summary>Details</summary>
Motivation: CHILDES是广泛使用的儿童语言数据资源，但缺乏一致的标注标准。本文旨在通过UD框架统一标注，为计算和语言学研究提供一致资源。

Method: 利用UD v2框架对CHILDES数据进行重新标注和验证，生成48k金标准句子和1M银标准句子。

Result: 成功构建了UD-English-CHILDES树库，统一了标注标准，并扩展了数据规模。

Conclusion: UD-English-CHILDES为儿童语言研究提供了高质量、一致的标注资源，具有广泛的应用潜力。

Abstract: CHILDES is a widely used resource of transcribed child and child-directed
speech. This paper introduces UD-English-CHILDES, the first officially released
Universal Dependencies (UD) treebank derived from previously
dependency-annotated CHILDES data with consistent and unified annotation
guidelines. Our corpus harmonizes annotations from 11 children and their
caregivers, totaling over 48k sentences. We validate existing gold-standard
annotations under the UD v2 framework and provide an additional 1M
silver-standard sentences, offering a consistent resource for computational and
linguistic research.

</details>


### [10] [Labeling Case Similarity based on Co-Citation of Legal Articles in Judgment Documents with Empirical Dispute-Based Evaluation](https://arxiv.org/abs/2504.20323)
*Chao-Lin Liu,Po-Hsien Wu,Yi-Ting Yu*

Main category: cs.CL

TL;DR: 提出一种利用法律条款共引用的新方法，解决法律推荐系统中标注数据不足的问题，并在劳动争议领域验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决法律推荐系统在专业领域（如劳动争议）中标注数据有限的问题。

Method: 利用法律条款在案件中的共引用关系建立相似性，结合文本嵌入模型和BiLSTM模块推荐相似案例。

Result: 验证了该方法能有效推荐基于法律条款共引用的相似劳动争议案例。

Conclusion: 为法律文档的自动标注技术提供了新思路，尤其适用于法律数据库有限的领域。

Abstract: This report addresses the challenge of limited labeled datasets for
developing legal recommender systems, particularly in specialized domains like
labor disputes. We propose a new approach leveraging the co-citation of legal
articles within cases to establish similarity and enable algorithmic
annotation. This method draws a parallel to the concept of case co-citation,
utilizing cited precedents as indicators of shared legal issues. To evaluate
the labeled results, we employ a system that recommends similar cases based on
plaintiffs' accusations, defendants' rebuttals, and points of disputes. The
evaluation demonstrates that the recommender, with finetuned text embedding
models and a reasonable BiLSTM module can recommend labor cases whose
similarity was measured by the co-citation of the legal articles. This research
contributes to the development of automated annotation techniques for legal
documents, particularly in areas with limited access to comprehensive legal
databases.

</details>


### [11] [Local Prompt Optimization](https://arxiv.org/abs/2504.20355)
*Yash Jain,Vishal Chowdhary*

Main category: cs.CL

TL;DR: 提出局部提示优化（LPO）方法，专注于优化提示中的关键令牌，显著提升自动提示工程性能。


<details>
  <summary>Details</summary>
Motivation: 现有提示优化方法全局优化所有令牌，导致优化空间过大且指导不足，需改进。

Method: LPO识别提示中的优化令牌，引导LLM仅优化这些令牌，结合通用自动提示工程方法。

Result: 在数学推理（GSM8k和MultiArith）和BIG-bench Hard基准测试中表现显著提升，且收敛更快。

Conclusion: LPO通过局部优化提高了提示工程的效率和性能，优于全局方法。

Abstract: In recent years, the use of prompts to guide the output of Large Language
Models have increased dramatically. However, even the best of experts struggle
to choose the correct words to stitch up a prompt for the desired task. To
solve this, LLM driven prompt optimization emerged as an important problem.
Existing prompt optimization methods optimize a prompt globally, where in all
the prompt tokens have to be optimized over a large vocabulary while solving a
complex task. The large optimization space (tokens) leads to insufficient
guidance for a better prompt. In this work, we introduce Local Prompt
Optimization (LPO) that integrates with any general automatic prompt
engineering method. We identify the optimization tokens in a prompt and nudge
the LLM to focus only on those tokens in its optimization step. We observe
remarkable performance improvements on Math Reasoning (GSM8k and MultiArith)
and BIG-bench Hard benchmarks across various automatic prompt engineering
methods. Further, we show that LPO converges to the optimal prompt faster than
global methods.

</details>


### [12] [What Causes Knowledge Loss in Multilingual Language Models?](https://arxiv.org/abs/2504.20356)
*Maria Khelli,Samuel Cahyawijaya,Ayu Purwarianti,Genta Indra Winata*

Main category: cs.CL

TL;DR: 研究探讨了跨语言迁移中灾难性遗忘的问题，通过实验发现非拉丁脚本语言更容易受影响，而拉丁脚本语言表现更好。


<details>
  <summary>Details</summary>
Motivation: 传统方法在处理多语言数据时未能模拟真实场景，导致灾难性遗忘等问题，研究旨在探索如何通过参数共享缓解这一问题。

Method: 使用52种语言和不同等级的LoRA适配器，评估非共享、部分共享和完全共享参数的效果。

Result: 非拉丁脚本语言更容易出现灾难性遗忘，拉丁脚本语言在跨语言迁移中表现更优。

Conclusion: 参数共享（如适配器）可以缓解灾难性遗忘，但语言脚本类型对效果有显著影响。

Abstract: Cross-lingual transfer in natural language processing (NLP) models enhances
multilingual performance by leveraging shared linguistic knowledge. However,
traditional methods that process all data simultaneously often fail to mimic
real-world scenarios, leading to challenges like catastrophic forgetting, where
fine-tuning on new tasks degrades performance on previously learned ones. Our
study explores this issue in multilingual contexts, focusing on linguistic
differences affecting representational learning rather than just model
parameters. We experiment with 52 languages using LoRA adapters of varying
ranks to evaluate non-shared, partially shared, and fully shared parameters.
Our aim is to see if parameter sharing through adapters can mitigate forgetting
while preserving prior knowledge. We find that languages using non-Latin
scripts are more susceptible to catastrophic forgetting, whereas those written
in Latin script facilitate more effective cross-lingual transfer.

</details>


### [13] [DMDTEval: An Evaluation and Analysis of LLMs on Disambiguation in Multi-domain Translation](https://arxiv.org/abs/2504.20371)
*Zhibo Man,Yuanmeng Chen,Yujie Zhang,Yufeng Chen,Jinan Xu*

Main category: cs.CL

TL;DR: 论文提出了一个系统评估框架DMDTEval，用于评估大型语言模型（LLMs）在多领域翻译中的消歧能力，包括构建测试集、设计提示模板和精确指标，并揭示了关键发现。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在机器翻译中表现优异，但在多领域翻译（MDT）中因词义歧义问题表现不佳，因此需要评估其消歧能力。

Method: 提出了DMDTEval框架，包括构建多领域歧义词标注的测试集、设计多样化的消歧提示模板，以及设计精确的消歧指标。

Result: 实验揭示了多种提示策略在多个先进LLMs上的有效性，并提供了关键发现。

Conclusion: 该研究为改进LLMs在多领域翻译中的消歧能力提供了重要参考，并推动了进一步研究。

Abstract: Currently, Large Language Models (LLMs) have achieved remarkable results in
machine translation. However, their performance in multi-domain translation
(MDT) is less satisfactory; the meanings of words can vary across different
domains, highlighting the significant ambiguity inherent in MDT. Therefore,
evaluating the disambiguation ability of LLMs in MDT remains an open problem.
To this end, we present an evaluation and analysis of LLMs on disambiguation in
multi-domain translation (DMDTEval), our systematic evaluation framework
consisting of three critical aspects: (1) we construct a translation test set
with multi-domain ambiguous word annotation, (2) we curate a diverse set of
disambiguation prompting templates, and (3) we design precise disambiguation
metrics, and study the efficacy of various prompting strategies on multiple
state-of-the-art LLMs. Our extensive experiments reveal a number of crucial
findings that we believe will pave the way and also facilitate further research
in the critical area of improving the disambiguation of LLMs.

</details>


### [14] [On Psychology of AI -- Does Primacy Effect Affect ChatGPT and Other LLMs?](https://arxiv.org/abs/2504.20444)
*Mika Hämäläinen*

Main category: cs.CL

TL;DR: 研究三种商业LLM（ChatGPT、Gemini、Claude）的首因效应，通过改造Asch（1946）实验发现，不同模型在不同实验条件下对形容词顺序的偏好不同。


<details>
  <summary>Details</summary>
Motivation: 探讨大型语言模型是否像人类一样表现出首因效应，即对信息顺序的偏好。

Method: 通过两种实验设计：1）同时呈现两个候选人描述；2）分别呈现描述，测试模型对形容词顺序的偏好。

Result: ChatGPT在同时呈现时偏好正面形容词先出现的候选人，而Gemini无偏好，Claude拒绝选择；在分别呈现时，ChatGPT和Claude倾向于平等评价，否则偏好负面形容词先出现的候选人，Gemini则偏好负面形容词先出现的候选人。

Conclusion: 不同LLM对信息顺序的偏好存在差异，表明模型设计可能影响其决策行为。

Abstract: We study the primacy effect in three commercial LLMs: ChatGPT, Gemini and
Claude. We do this by repurposing the famous experiment Asch (1946) conducted
using human subjects. The experiment is simple, given two candidates with equal
descriptions which one is preferred if one description has positive adjectives
first before negative ones and another description has negative adjectives
followed by positive ones. We test this in two experiments. In one experiment,
LLMs are given both candidates simultaneously in the same prompt, and in
another experiment, LLMs are given both candidates separately. We test all the
models with 200 candidate pairs. We found that, in the first experiment,
ChatGPT preferred the candidate with positive adjectives listed first, while
Gemini preferred both equally often. Claude refused to make a choice. In the
second experiment, ChatGPT and Claude were most likely to rank both candidates
equally. In the case where they did not give an equal rating, both showed a
clear preference to a candidate that had negative adjectives listed first.
Gemini was most likely to prefer a candidate with negative adjectives listed
first.

</details>


### [15] [Team ACK at SemEval-2025 Task 2: Beyond Word-for-Word Machine Translation for English-Korean Pairs](https://arxiv.org/abs/2504.20451)
*Daniel Lee,Harsh Sharma,Jieun Han,Sunny Jeong,Alice Oh,Vered Shwartz*

Main category: cs.CL

TL;DR: 论文评估了13种模型（LLMs和MT模型）在英韩知识密集型和实体丰富文本翻译中的表现，发现LLMs优于传统MT系统，但在需要文化适应的实体翻译上表现不佳。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决英韩翻译中语言和文化细微差异的保留问题，超越字面或逐字转换。

Method: 通过自动指标和双语标注者的人工评估，评估了13种模型的表现，并构建了错误分类法。

Result: LLMs在整体表现上优于传统MT系统，但在实体翻译和文化适应方面存在困难，性能因实体类型和流行度而异。

Conclusion: 研究揭示了自动评估指标的不足，为未来实现文化细微机器翻译提供了方向。

Abstract: Translating knowledge-intensive and entity-rich text between English and
Korean requires transcreation to preserve language-specific and cultural
nuances beyond literal, phonetic or word-for-word conversion. We evaluate 13
models (LLMs and MT models) using automatic metrics and human assessment by
bilingual annotators. Our findings show LLMs outperform traditional MT systems
but struggle with entity translation requiring cultural adaptation. By
constructing an error taxonomy, we identify incorrect responses and entity name
errors as key issues, with performance varying by entity type and popularity
level. This work exposes gaps in automatic evaluation metrics and hope to
enable future work in completing culturally-nuanced machine translation.

</details>


### [16] [Fane at SemEval-2025 Task 10: Zero-Shot Entity Framing with Large Language Models](https://arxiv.org/abs/2504.20469)
*Enfa Fane,Mihai Surdeanu,Eduardo Blanco,Steven R. Corman*

Main category: cs.CL

TL;DR: 论文评估了大型语言模型（LLMs）在零样本分类新闻叙事中实体框架角色的能力，通过分层方法和优化输入上下文与提示策略，取得了显著效果。


<details>
  <summary>Details</summary>
Motivation: 研究新闻叙事如何框架实体对理解媒体对社会事件感知的影响至关重要。

Method: 通过系统实验评估输入上下文、提示策略和任务分解的效果，采用分层分类方法。

Result: 分层方法在广泛角色和细粒度角色分类中表现优于单步分类，主角色准确率达89.4%，精确匹配率为34.5%。

Conclusion: 研究表明，针对子任务特定的提示设计和输入上下文优化对提升LLM在实体框架分类中的性能至关重要。

Abstract: Understanding how news narratives frame entities is crucial for studying
media's impact on societal perceptions of events. In this paper, we evaluate
the zero-shot capabilities of large language models (LLMs) in classifying
framing roles. Through systematic experimentation, we assess the effects of
input context, prompting strategies, and task decomposition. Our findings show
that a hierarchical approach of first identifying broad roles and then
fine-grained roles, outperforms single-step classification. We also demonstrate
that optimal input contexts and prompts vary across task levels, highlighting
the need for subtask-specific strategies. We achieve a Main Role Accuracy of
89.4% and an Exact Match Ratio of 34.5%, demonstrating the effectiveness of our
approach. Our findings emphasize the importance of tailored prompt design and
input context optimization for improving LLM performance in entity framing.

</details>


### [17] [Enhancing LLM Language Adaption through Cross-lingual In-Context Pre-training](https://arxiv.org/abs/2504.20484)
*Linjuan Wu,Haoran Wei,Huan Lin,Tianhao Li,Baosong Yang,Weiming Lu*

Main category: cs.CL

TL;DR: CrossIC-PT是一种简单且可扩展的方法，通过利用语义相关的双语文本增强跨语言迁移，显著提升了多语言模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖平行资源，语言和领域覆盖有限，限制了跨语言迁移的效果。

Method: 通过将语义相关的双语维基百科文档交错拼接为上下文窗口，并采用分段策略和滑动窗口机制保持上下文连贯性，进一步通过语义检索框架扩展数据来源。

Result: 在三种模型（Llama-3.1-8B、Qwen2.5-7B和Qwen2.5-1.5B）和六种目标语言上，性能分别提升了3.79%、3.99%和1.95%，数据增强后效果更佳。

Conclusion: CrossIC-PT有效提升了跨语言迁移能力，且方法简单、可扩展。

Abstract: Large language models (LLMs) exhibit remarkable multilingual capabilities
despite English-dominated pre-training, attributed to cross-lingual mechanisms
during pre-training. Existing methods for enhancing cross-lingual transfer
remain constrained by parallel resources, suffering from limited linguistic and
domain coverage. We propose Cross-lingual In-context Pre-training (CrossIC-PT),
a simple and scalable approach that enhances cross-lingual transfer by
leveraging semantically related bilingual texts via simple next-word
prediction. We construct CrossIC-PT samples by interleaving semantic-related
bilingual Wikipedia documents into a single context window. To access window
size constraints, we implement a systematic segmentation policy to split long
bilingual document pairs into chunks while adjusting the sliding window
mechanism to preserve contextual coherence. We further extend data availability
through a semantic retrieval framework to construct CrossIC-PT samples from
web-crawled corpus. Experimental results demonstrate that CrossIC-PT improves
multilingual performance on three models (Llama-3.1-8B, Qwen2.5-7B, and
Qwen2.5-1.5B) across six target languages, yielding performance gains of 3.79%,
3.99%, and 1.95%, respectively, with additional improvements after data
augmentation.

</details>


### [18] [UniDetox: Universal Detoxification of Large Language Models via Dataset Distillation](https://arxiv.org/abs/2504.20500)
*Huimin Lu,Masaru Isonuma,Junichiro Mori,Ichiro Sakata*

Main category: cs.CL

TL;DR: UniDetox是一种通用方法，用于减轻各种大型语言模型（LLM）的毒性，无需针对不同模型单独调整超参数。


<details>
  <summary>Details</summary>
Motivation: 现有解毒方法通常针对特定模型或模型家族，且需要在解毒效果和语言建模性能之间权衡。UniDetox旨在提供一种通用解决方案。

Method: 采用对比解码的数据集蒸馏技术，生成解毒文本数据，通过微调实现通用解毒。

Result: 实验表明，从GPT-2蒸馏的解毒文本可有效解毒OPT、Falcon和LLaMA-2等更大模型，且无需单独调整超参数。

Conclusion: UniDetox是一种高效通用的LLM解毒方法，还能减少政治偏见内容。

Abstract: We present UniDetox, a universally applicable method designed to mitigate
toxicity across various large language models (LLMs). Previous detoxification
methods are typically model-specific, addressing only individual models or
model families, and require careful hyperparameter tuning due to the trade-off
between detoxification efficacy and language modeling performance. In contrast,
UniDetox provides a detoxification technique that can be universally applied to
a wide range of LLMs without the need for separate model-specific tuning.
Specifically, we propose a novel and efficient dataset distillation technique
for detoxification using contrastive decoding. This approach distills
detoxifying representations in the form of synthetic text data, enabling
universal detoxification of any LLM through fine-tuning with the distilled
text. Our experiments demonstrate that the detoxifying text distilled from
GPT-2 can effectively detoxify larger models, including OPT, Falcon, and
LLaMA-2. Furthermore, UniDetox eliminates the need for separate hyperparameter
tuning for each model, as a single hyperparameter configuration can be
seamlessly applied across different models. Additionally, analysis of the
detoxifying text reveals a reduction in politically biased content, providing
insights into the attributes necessary for effective detoxification of LLMs.

</details>


### [19] [Revisiting the MIMIC-IV Benchmark: Experiments Using Language Models for Electronic Health Records](https://arxiv.org/abs/2504.20547)
*Jesus Lovon,Thouria Ben-Haddi,Jules Di Scala,Jose G. Moreno,Lynda Tamine*

Main category: cs.CL

TL;DR: 论文提出标准化医学文本评估基准的重要性，利用MIMIC-IV数据集，通过模板将表格数据转为文本，实验表明微调文本模型优于零样本LLMs。


<details>
  <summary>Details</summary>
Motivation: 解决医学领域缺乏标准化文本评估基准的问题，促进自然语言模型在健康相关任务中的应用。

Method: 整合MIMIC-IV数据至Hugging Face库，使用模板将EHR表格数据转为文本，对比微调文本模型与零样本LLMs在患者死亡率任务中的表现。

Result: 微调文本模型表现优于零样本LLMs，与表格分类器竞争性相当。

Conclusion: 文本方法在医学领域潜力显著，但零样本LLMs仍需改进。

Abstract: The lack of standardized evaluation benchmarks in the medical domain for text
inputs can be a barrier to widely adopting and leveraging the potential of
natural language models for health-related downstream tasks. This paper
revisited an openly available MIMIC-IV benchmark for electronic health records
(EHRs) to address this issue. First, we integrate the MIMIC-IV data within the
Hugging Face datasets library to allow an easy share and use of this
collection. Second, we investigate the application of templates to convert EHR
tabular data to text. Experiments using fine-tuned and zero-shot LLMs on the
mortality of patients task show that fine-tuned text-based models are
competitive against robust tabular classifiers. In contrast, zero-shot LLMs
struggle to leverage EHR representations. This study underlines the potential
of text-based approaches in the medical field and highlights areas for further
improvement.

</details>


### [20] [BrAIcht, a theatrical agent that speaks like Bertolt Brecht's characters](https://arxiv.org/abs/2504.20552)
*Baz Roland,Kristina Malyseva,Anna Pappa,Tristan Cazenave*

Main category: cs.CL

TL;DR: BrAIcht是一个基于德国LeoLM的AI对话代理，能够生成类似德国剧作家Bertolt Brecht风格的对话。


<details>
  <summary>Details</summary>
Motivation: 旨在通过AI重现Bertolt Brecht独特的戏剧风格，为文学创作和研究提供工具。

Method: 使用7B参数的LeoLM模型，通过QLoRA技术进行参数高效微调，数据集包括29部Brecht剧作和907部风格相似的德国戏剧。

Result: 基于BLEU分数和困惑度的结果显示，BrAIcht在生成Brecht风格对话方面表现优异。

Conclusion: BrAIcht成功实现了生成Brecht风格对话的目标，展示了AI在文学风格模仿中的潜力。

Abstract: This project introduces BrAIcht, an AI conversational agent that creates
dialogues in the distinctive style of the famous German playwright Bertolt
Brecht. BrAIcht is fine-tuned using German LeoLM, a large language model with 7
billion parameters and a modified version of the base Llama2 suitable for
German language tasks. For fine-tuning, 29 plays of Bertolt Brecht and 907 of
other German plays that are stylistically similar to Bertolt Brecht are used to
form a more di-erse dataset. Due to the limited memory capacity, a
parameterefficient fine-tuning technique called QLoRA is implemented to train
the large language model. The results, based on BLEU score and perplexity, show
very promising performance of BrAIcht in generating dialogues in the style of
Bertolt Brecht.

</details>


### [21] [ClonEval: An Open Voice Cloning Benchmark](https://arxiv.org/abs/2504.20581)
*Iwona Christop,Tomasz Kuczyński,Marek Kubis*

Main category: cs.CL

TL;DR: 提出了一种新的语音克隆文本转语音模型基准，包括评估协议、开源库和排行榜。


<details>
  <summary>Details</summary>
Motivation: 为语音克隆模型的性能评估提供标准化工具和平台。

Method: 设计了评估协议和开源库，详细描述了评估流程和软件库的使用方法。

Result: 提供了一个完整的评估框架和公开的排行榜。

Conclusion: 该基准为语音克隆模型的性能评估提供了实用工具和标准化方法。

Abstract: We present a novel benchmark for voice cloning text-to-speech models. The
benchmark consists of an evaluation protocol, an open-source library for
assessing the performance of voice cloning models, and an accompanying
leaderboard. The paper discusses design considerations and presents a detailed
description of the evaluation procedure. The usage of the software library is
explained, along with the organization of results on the leaderboard.

</details>


### [22] [TF1-EN-3M: Three Million Synthetic Moral Fables for Training Small, Open Language Models](https://arxiv.org/abs/2504.20605)
*Mihai Nadas,Laura Diosan,Andrei Piscoran,Andreea Tomescu*

Main category: cs.CL

TL;DR: TF1-EN-3M是一个由指令调优模型生成的300万英文寓言数据集，填补了现代NLP缺乏结构化道德叙事数据的空白。


<details>
  <summary>Details</summary>
Motivation: 现代NLP缺乏结合连贯叙事与明确道德教训的大规模结构化数据集，TF1-EN-3M旨在填补这一空白。

Method: 使用不超过8B参数的指令调优模型生成故事，采用六槽模板（角色->特质->背景->冲突->解决->道德），并通过混合评估流程（GPT评分和无参考多样性指标）验证质量。

Result: 8B参数的Llama-3变体在质量与速度上表现最佳，生成成本约为每1000个故事13.5美分。

Conclusion: TF1-EN-3M展示了大规模道德叙事无需依赖专有巨型模型，为指令跟随、叙事智能等研究提供了新资源。

Abstract: Moral stories are a time-tested vehicle for transmitting values, yet modern
NLP lacks a large, structured corpus that couples coherent narratives with
explicit ethical lessons. We close this gap with TF1-EN-3M, the first open
dataset of three million English-language fables generated exclusively by
instruction-tuned models no larger than 8B parameters. Each story follows a
six-slot scaffold (character -> trait -> setting -> conflict -> resolution ->
moral), produced through a combinatorial prompt engine that guarantees genre
fidelity while covering a broad thematic space.
  A hybrid evaluation pipeline blends (i) a GPT-based critic that scores
grammar, creativity, moral clarity, and template adherence with (ii)
reference-free diversity and readability metrics. Among ten open-weight
candidates, an 8B-parameter Llama-3 variant delivers the best quality-speed
trade-off, producing high-scoring fables on a single consumer GPU (<24 GB VRAM)
at approximately 13.5 cents per 1,000 fables.
  We release the dataset, generation code, evaluation scripts, and full
metadata under a permissive license, enabling exact reproducibility and cost
benchmarking. TF1-EN-3M opens avenues for research in instruction following,
narrative intelligence, value alignment, and child-friendly educational AI,
demonstrating that large-scale moral storytelling no longer requires
proprietary giant models.

</details>


### [23] [WenyanGPT: A Large Language Model for Classical Chinese Tasks](https://arxiv.org/abs/2504.20609)
*Xinyu Yao,Mengdi Wang,Bo Chen,Xiaobing Zhao*

Main category: cs.CL

TL;DR: 本文提出了一种针对文言文的语言处理解决方案，通过预训练和指令微调LLaMA3-8B-Chinese模型，构建了专门用于文言文任务的大语言模型WenyanGPT，并开发了评估基准数据集WenyanBENCH。实验表明WenyanGPT在文言文任务中显著优于现有先进模型。


<details>
  <summary>Details</summary>
Motivation: 现有自然语言处理模型主要针对现代汉语优化，在文言文任务上表现不佳，而文言文作为中华文化的核心载体，其处理和研究具有重要意义。

Method: 通过继续预训练和指令微调LLaMA3-8B-Chinese模型，构建专门用于文言文的WenyanGPT，并开发评估基准数据集WenyanBENCH。

Result: 实验结果显示，WenyanGPT在WenyanBENCH上的表现显著优于当前先进的LLMs。

Conclusion: WenyanGPT为文言文处理提供了有效解决方案，并公开了训练数据、指令微调数据和评估基准数据集，以推动该领域的进一步研究。

Abstract: Classical Chinese, as the core carrier of Chinese culture, plays a crucial
role in the inheritance and study of ancient literature. However, existing
natural language processing models primarily optimize for Modern Chinese,
resulting in inadequate performance on Classical Chinese. This paper presents a
comprehensive solution for Classical Chinese language processing. By continuing
pre-training and instruction fine-tuning on the LLaMA3-8B-Chinese model, we
construct a large language model, WenyanGPT, which is specifically designed for
Classical Chinese tasks. Additionally, we develop an evaluation benchmark
dataset, WenyanBENCH. Experimental results on WenyanBENCH demonstrate that
WenyanGPT significantly outperforms current advanced LLMs in various Classical
Chinese tasks. We make the model's training data, instruction fine-tuning
data\footnote, and evaluation benchmark dataset publicly available to promote
further research and development in the field of Classical Chinese processing.

</details>


### [24] [Cooking Up Creativity: A Cognitively-Inspired Approach for Enhancing LLM Creativity through Structured Representations](https://arxiv.org/abs/2504.20643)
*Moran Mizrahi,Chen Shani,Gabriel Stanovsky,Dan Jurafsky,Dafna Shahaf*

Main category: cs.CL

TL;DR: 本文提出了一种结合大型语言模型（LLMs）与结构化表示和认知启发操作的新方法，以生成更具创造性和多样性的想法。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在许多任务中表现出色，但在创造力方面仍有不足。本文旨在通过结构化表示和认知操作提升LLMs的创造力。

Method: 通过结合LLMs与结构化表示，并采用认知启发操作，重新组合现有想法的结构化表示，以探索更抽象的想法空间。

Result: 在烹饪领域，模型DishCOVER生成的食谱比GPT-4o更具多样性。专家评估显示，其输出在新颖性上显著优于GPT-4o。

Conclusion: 该方法在提升AI创造力方面具有潜力，为结构化创造力的进一步研究提供了启示。

Abstract: Large Language Models (LLMs) excel at countless tasks, yet struggle with
creativity. In this paper, we introduce a novel approach that couples LLMs with
structured representations and cognitively inspired manipulations to generate
more creative and diverse ideas. Our notion of creativity goes beyond
superficial token-level variations; rather, we explicitly recombine structured
representations of existing ideas, allowing our algorithm to effectively
explore the more abstract landscape of ideas. We demonstrate our approach in
the culinary domain with DishCOVER, a model that generates creative recipes.
Experiments comparing our model's results to those of GPT-4o show greater
diversity. Domain expert evaluations reveal that our outputs, which are mostly
coherent and feasible culinary creations, significantly surpass GPT-4o in terms
of novelty, thus outperforming it in creative generation. We hope our work
inspires further research into structured creativity in AI.

</details>


### [25] [A Generative-AI-Driven Claim Retrieval System Capable of Detecting and Retrieving Claims from Social Media Platforms in Multiple Languages](https://arxiv.org/abs/2504.20668)
*Ivan Vykopal,Martin Hyben,Robert Moro,Michal Gregor,Jakub Simko*

Main category: cs.CL

TL;DR: 该研究提出了一种利用大语言模型（LLMs）检索和评估已核实信息的方法，以减少重复核实工作，提高事实核查效率。


<details>
  <summary>Details</summary>
Motivation: 在线虚假信息泛滥，事实核查员需高效验证信息，但重复核查已核实信息增加了工作负担。本研究旨在解决这一问题。

Method: 使用LLMs过滤无关事实核查，生成简洁摘要和解释，帮助核查员快速判断信息是否已核实。

Result: LLMs能有效过滤无关信息，减少工作量，优化核查流程。

Conclusion: 该方法通过LLMs支持事实核查员，显著提升了核查效率。

Abstract: Online disinformation poses a global challenge, placing significant demands
on fact-checkers who must verify claims efficiently to prevent the spread of
false information. A major issue in this process is the redundant verification
of already fact-checked claims, which increases workload and delays responses
to newly emerging claims. This research introduces an approach that retrieves
previously fact-checked claims, evaluates their relevance to a given input, and
provides supplementary information to support fact-checkers. Our method employs
large language models (LLMs) to filter irrelevant fact-checks and generate
concise summaries and explanations, enabling fact-checkers to faster assess
whether a claim has been verified before. In addition, we evaluate our approach
through both automatic and human assessments, where humans interact with the
developed tool to review its effectiveness. Our results demonstrate that LLMs
are able to filter out many irrelevant fact-checks and, therefore, reduce
effort and streamline the fact-checking process.

</details>


### [26] [Non-native Children's Automatic Speech Assessment Challenge (NOCASA)](https://arxiv.org/abs/2504.20678)
*Yaroslav Getman,Tamás Grósz,Mikko Kurimo,Giampiero Salvi*

Main category: cs.CL

TL;DR: NOCASA竞赛要求开发系统评估非母语儿童单词语音，提供伪匿名训练数据和基线模型，最佳模型UAR为36.37%。


<details>
  <summary>Details</summary>
Motivation: 解决非母语儿童发音评估中的数据不足和类别不平衡问题，推动发音训练应用的发展。

Method: 提供TeflonNorL2数据集（10,334条录音），并发布SVM和wav2vec 2.0基线模型。

Result: wav2vec 2.0模型在测试集上表现最佳，UAR为36.37%。

Conclusion: NOCASA竞赛及提供的数据和模型为发音评估研究提供了重要资源。

Abstract: This paper presents the "Non-native Children's Automatic Speech Assessment"
(NOCASA) - a data competition part of the IEEE MLSP 2025 conference. NOCASA
challenges participants to develop new systems that can assess single-word
pronunciations of young second language (L2) learners as part of a gamified
pronunciation training app. To achieve this, several issues must be addressed,
most notably the limited nature of available training data and the highly
unbalanced distribution among the pronunciation level categories. To expedite
the development, we provide a pseudo-anonymized training data (TeflonNorL2),
containing 10,334 recordings from 44 speakers attempting to pronounce 205
distinct Norwegian words, human-rated on a 1 to 5 scale (number of stars that
should be given in the game). In addition to the data, two already trained
systems are released as official baselines: an SVM classifier trained on the
ComParE_16 acoustic feature set and a multi-task wav2vec 2.0 model. The latter
achieves the best performance on the challenge test set, with an unweighted
average recall (UAR) of 36.37%.

</details>


### [27] [Are Information Retrieval Approaches Good at Harmonising Longitudinal Survey Questions in Social Science?](https://arxiv.org/abs/2504.20679)
*Wing Yan Li,Zeqiang Wang,Jon Johnson,Suparna De*

Main category: cs.CL

TL;DR: 论文提出了一种自动检测纵向社会科学调查中语义等价问题的方法，通过多学科合作解决了概念表示不一致和词汇演变的挑战，并比较了多种无监督方法的性能。


<details>
  <summary>Details</summary>
Motivation: 解决纵向调查中语义等价问题检测的挑战，包括概念表示不一致和词汇演变，以支持社会科学、经济学和健康科学的长期研究。

Method: 研究了多种无监督方法，包括概率模型、语言模型的线性探测和专用于信息检索的预训练神经网络，并在1946-2020年的调查数据集上进行了测试。

Result: 专用于信息检索的神经网络模型表现最佳，其他方法表现相当；概率模型结果通过神经网络重新排名后F1分数最多提升0.07。

Conclusion: 研究为社会科学纵向研究的协调提供了进一步的研究方向，模型在词汇重叠高的问题上敏感性较低。

Abstract: Automated detection of semantically equivalent questions in longitudinal
social science surveys is crucial for long-term studies informing empirical
research in the social, economic, and health sciences. Retrieving equivalent
questions faces dual challenges: inconsistent representation of theoretical
constructs (i.e. concept/sub-concept) across studies as well as between
question and response options, and the evolution of vocabulary and structure in
longitudinal text. To address these challenges, our multi-disciplinary
collaboration of computer scientists and survey specialists presents a new
information retrieval (IR) task of identifying concept (e.g. Housing, Job,
etc.) equivalence across question and response options to harmonise
longitudinal population studies. This paper investigates multiple unsupervised
approaches on a survey dataset spanning 1946-2020, including probabilistic
models, linear probing of language models, and pre-trained neural networks
specialised for IR. We show that IR-specialised neural models achieve the
highest overall performance with other approaches performing comparably.
Additionally, the re-ranking of the probabilistic model's results with neural
models only introduces modest improvements of 0.07 at most in F1-score.
Qualitative post-hoc evaluation by survey specialists shows that models
generally have a low sensitivity to questions with high lexical overlap,
particularly in cases where sub-concepts are mismatched. Altogether, our
analysis serves to further research on harmonising longitudinal studies in
social science.

</details>


### [28] [Can LLMs Detect Intrinsic Hallucinations in Paraphrasing and Machine Translation?](https://arxiv.org/abs/2504.20699)
*Evangelia Gogoulou,Shorouq Zahra,Liane Guillou,Luise Dürlich,Joakim Nivre*

Main category: cs.CL

TL;DR: 论文研究了LLMs在翻译和释义任务中检测内在幻觉的能力，发现模型性能因模型大小和指令调整而异，但提示选择影响不大，且NLI模型表现相当。


<details>
  <summary>Details</summary>
Motivation: LLMs常生成无意义或错误的输出（幻觉），研究旨在评估其在特定任务中检测幻觉的能力。

Method: 使用HalluciGen任务评估多个开源LLMs，分析模型大小、指令调整和提示选择对性能的影响。

Result: 模型性能因任务和语言而异，但提示选择影响较小；NLI模型表现与LLM相当。

Conclusion: LLM并非检测幻觉的唯一可行方案，NLI模型同样有效。

Abstract: A frequently observed problem with LLMs is their tendency to generate output
that is nonsensical, illogical, or factually incorrect, often referred to
broadly as hallucination. Building on the recently proposed HalluciGen task for
hallucination detection and generation, we evaluate a suite of open-access LLMs
on their ability to detect intrinsic hallucinations in two conditional
generation tasks: translation and paraphrasing. We study how model performance
varies across tasks and language and we investigate the impact of model size,
instruction tuning, and prompt choice. We find that performance varies across
models but is consistent across prompts. Finally, we find that NLI models
perform comparably well, suggesting that LLM-based detectors are not the only
viable option for this specific task.

</details>


### [29] [BrightCookies at SemEval-2025 Task 9: Exploring Data Augmentation for Food Hazard Classification](https://arxiv.org/abs/2504.20703)
*Foteini Papadopoulou,Osman Mutlu,Neris Özen,Bas H. M. van der Velden,Iris Hendrickx,Ali Hürriyetoğlu*

Main category: cs.CL

TL;DR: 本文介绍了为SemEval-2025 Task 9开发的系统，通过文本增强技术提升少数类别的分类性能，发现BERT模型在细粒度分类中表现最佳。


<details>
  <summary>Details</summary>
Motivation: 解决食品召回事件报告中少数类别分类性能不佳的问题。

Method: 采用三种词级数据增强技术（同义词替换、随机词交换、上下文词插入），并在多种模型上比较效果。

Result: Transformer模型整体表现更好，但增强技术未显著提升整体性能；BERT模型在细粒度分类中显著提升少数类别准确率6%。

Conclusion: 针对少数类别的增强技术可提升Transformer模型性能。

Abstract: This paper presents our system developed for the SemEval-2025 Task 9: The
Food Hazard Detection Challenge. The shared task's objective is to evaluate
explainable classification systems for classifying hazards and products in two
levels of granularity from food recall incident reports. In this work, we
propose text augmentation techniques as a way to improve poor performance on
minority classes and compare their effect for each category on various
transformer and machine learning models. We explore three word-level data
augmentation techniques, namely synonym replacement, random word swapping, and
contextual word insertion. The results show that transformer models tend to
have a better overall performance. None of the three augmentation techniques
consistently improved overall performance for classifying hazards and products.
We observed a statistically significant improvement (P < 0.05) in the
fine-grained categories when using the BERT model to compare the baseline with
each augmented model. Compared to the baseline, the contextual words insertion
augmentation improved the accuracy of predictions for the minority hazard
classes by 6%. This suggests that targeted augmentation of minority classes can
improve the performance of transformer models.

</details>


### [30] [Beyond the Last Answer: Your Reasoning Trace Uncovers More than You Think](https://arxiv.org/abs/2504.20708)
*Hasan Abed Al Kader Hammoud,Hani Itani,Bernard Ghanem*

Main category: cs.CL

TL;DR: 论文提出了一种通过分析中间推理步骤（子思想）来评估大语言模型（LLM）的方法，发现聚合不同子思想的答案能显著提高准确性。


<details>
  <summary>Details</summary>
Motivation: 挑战传统仅依赖最终答案的评估方式，探究最终答案是否可靠以及不同推理路径是否能产生不同结果。

Method: 将推理轨迹分段为子思想，生成每个子思想的延续，提取潜在答案，并通过选择最常见答案（众数）来提高准确性。

Result: 实验显示，该方法在多个LLM和数学推理数据集上显著提升准确性（AIME2024和AIME2025分别提升13%和10%）。

Conclusion: 分析子思想的答案一致性可识别模型的置信度和正确性，为评估LLM提供了新思路。

Abstract: Large Language Models (LLMs) leverage step-by-step reasoning to solve complex
problems. Standard evaluation practice involves generating a complete reasoning
trace and assessing the correctness of the final answer presented at its
conclusion. In this paper, we challenge the reliance on the final answer by
posing the following two questions: Does the final answer reliably represent
the model's optimal conclusion? Can alternative reasoning paths yield different
results? To answer these questions, we analyze intermediate reasoning steps,
termed subthoughts, and propose a method based on our findings. Our approach
involves segmenting a reasoning trace into sequential subthoughts based on
linguistic cues. We start by prompting the model to generate continuations from
the end-point of each intermediate subthought. We extract a potential answer
from every completed continuation originating from different subthoughts. We
find that aggregating these answers by selecting the most frequent one (the
mode) often yields significantly higher accuracy compared to relying solely on
the answer derived from the original complete trace. Analyzing the consistency
among the answers derived from different subthoughts reveals characteristics
that correlate with the model's confidence and correctness, suggesting
potential for identifying less reliable answers. Our experiments across various
LLMs and challenging mathematical reasoning datasets (AIME2024 and AIME2025)
show consistent accuracy improvements, with gains reaching up to 13\% and 10\%
respectively. Implementation is available at:
https://github.com/hammoudhasan/SubthoughtReasoner.

</details>


### [31] [UniversalRAG: Retrieval-Augmented Generation over Multiple Corpora with Diverse Modalities and Granularities](https://arxiv.org/abs/2504.20734)
*Woongyeong Yeo,Kangsan Kim,Soyeong Jeong,Jinheon Baek,Sung Ju Hwang*

Main category: cs.CL

TL;DR: UniversalRAG是一个新型的RAG框架，通过动态路由机制从多模态、多粒度的异构知识源中检索和整合信息，解决了现有RAG方法局限于单一模态的问题。


<details>
  <summary>Details</summary>
Motivation: 现有RAG方法通常仅针对单一模态的知识源，无法满足现实查询对多样化知识的需求。

Method: 提出了一种模态感知的路由机制，动态选择最合适的模态特定知识库进行检索，并支持多粒度级别的检索。

Result: 在8个多模态基准测试中，UniversalRAG优于单一模态和统一基线方法。

Conclusion: UniversalRAG通过多模态和多粒度的检索机制，显著提升了RAG的适应性和准确性。

Abstract: Retrieval-Augmented Generation (RAG) has shown substantial promise in
improving factual accuracy by grounding model responses with external knowledge
relevant to queries. However, most existing RAG approaches are limited to a
text-only corpus, and while recent efforts have extended RAG to other
modalities such as images and videos, they typically operate over a single
modality-specific corpus. In contrast, real-world queries vary widely in the
type of knowledge they require, which a single type of knowledge source cannot
address. To address this, we introduce UniversalRAG, a novel RAG framework
designed to retrieve and integrate knowledge from heterogeneous sources with
diverse modalities and granularities. Specifically, motivated by the
observation that forcing all modalities into a unified representation space
derived from a single combined corpus causes a modality gap, where the
retrieval tends to favor items from the same modality as the query, we propose
a modality-aware routing mechanism that dynamically identifies the most
appropriate modality-specific corpus and performs targeted retrieval within it.
Also, beyond modality, we organize each modality into multiple granularity
levels, enabling fine-tuned retrieval tailored to the complexity and scope of
the query. We validate UniversalRAG on 8 benchmarks spanning multiple
modalities, showing its superiority over modality-specific and unified
baselines.

</details>


### [32] [Grokking in the Wild: Data Augmentation for Real-World Multi-Hop Reasoning with Transformers](https://arxiv.org/abs/2504.20752)
*Roman Abramov,Felix Steinbauer,Gjergji Kasneci*

Main category: cs.CL

TL;DR: 将grokking技术扩展到真实世界的事实数据，通过合成数据增强知识图谱，显著提升多跳推理能力。


<details>
  <summary>Details</summary>
Motivation: 解决Transformer在多步事实推理中的不足，尤其是在知识稀疏的情况下。

Method: 通过设计合成数据增强知识图谱，提高推断事实与原子事实的比例（φ_r），以触发grokking现象。

Result: 在2WikiMultiHopQA基准测试中达到95-100%的准确率，超越基线并匹配或超过当前最优结果。

Conclusion: grokking数据增强可以释放隐式多跳推理能力，为大规模语言模型提供更鲁棒和可解释的事实推理。

Abstract: Transformers have achieved great success in numerous NLP tasks but continue
to exhibit notable gaps in multi-step factual reasoning, especially when
real-world knowledge is sparse. Recent advances in grokking have demonstrated
that neural networks can transition from memorizing to perfectly generalizing
once they detect underlying logical patterns - yet these studies have primarily
used small, synthetic tasks. In this paper, for the first time, we extend
grokking to real-world factual data and address the challenge of dataset
sparsity by augmenting existing knowledge graphs with carefully designed
synthetic data to raise the ratio $\phi_r$ of inferred facts to atomic facts
above the threshold required for grokking. Surprisingly, we find that even
factually incorrect synthetic data can strengthen emergent reasoning circuits
rather than degrade accuracy, as it forces the model to rely on relational
structure rather than memorization. When evaluated on multi-hop reasoning
benchmarks, our approach achieves up to 95-100% accuracy on 2WikiMultiHopQA -
substantially improving over strong baselines and matching or exceeding current
state-of-the-art results. We further provide an in-depth analysis of how
increasing $\phi_r$ drives the formation of generalizing circuits inside
Transformers. Our findings suggest that grokking-based data augmentation can
unlock implicit multi-hop reasoning capabilities, opening the door to more
robust and interpretable factual reasoning in large-scale language models.

</details>


### [33] [Chain-of-Defensive-Thought: Structured Reasoning Elicits Robustness in Large Language Models against Reference Corruption](https://arxiv.org/abs/2504.20769)
*Wenxiao Wang,Parsa Hosseini,Soheil Feizi*

Main category: cs.CL

TL;DR: 链式防御思维提示显著提升大型语言模型在非推理任务中的鲁棒性，尤其在面对参考数据损坏时表现优异。


<details>
  <summary>Details</summary>
Motivation: 探索如何利用链式思维提示增强大型语言模型的推理能力，以提升其在非推理任务中的鲁棒性。

Method: 提出链式防御思维提示方法，仅需提供少量带有结构化防御推理的示例作为演示。

Result: 在Natural Questions任务中，GPT-4o使用链式防御思维提示的准确率从标准提示下的3%提升至50%。

Conclusion: 链式防御思维提示是一种简单且高效的方法，能够显著提升语言模型在对抗性环境中的鲁棒性。

Abstract: Chain-of-thought prompting has demonstrated great success in facilitating the
reasoning abilities of large language models. In this work, we explore how
these enhanced reasoning abilities can be exploited to improve the robustness
of large language models in tasks that are not necessarily reasoning-focused.
In particular, we show how a wide range of large language models exhibit
significantly improved robustness against reference corruption using a simple
method called chain-of-defensive-thought, where only a few exemplars with
structured and defensive reasoning are provided as demonstrations. Empirically,
the improvements can be astounding, especially given the simplicity and
applicability of the method. For example, in the Natural Questions task, the
accuracy of GPT-4o degrades from 60% to as low as 3% with standard prompting
when 1 out of 10 references provided is corrupted with prompt injection
attacks. In contrast, GPT-4o using chain-of-defensive-thought prompting
maintains an accuracy of 50%.

</details>


### [34] [Turing Machine Evaluation for Large Language Model](https://arxiv.org/abs/2504.20771)
*Haitao Wu,Zongbo Han,Huaxi Huang,Changqing Zhang*

Main category: cs.CL

TL;DR: 本文提出了一种基于通用图灵机（UTM）模拟的评估框架TMBench，用于系统评估大语言模型（LLMs）的计算推理能力，发现其性能与其他推理基准强相关。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs的快速发展和广泛应用，对其核心计算推理能力的评估变得尤为重要，以确保其作为精确执行器的可靠性。

Method: 采用UTM模拟框架，要求LLMs在多步计算中严格遵循指令并跟踪动态状态（如磁带内容和读写头位置），开发了标准化评估基准TMBench。

Result: TMBench具有知识无关性、可调难度、通过图灵机编码的基础覆盖和无限实例生成能力，模型性能与其他推理基准强相关（Pearson相关系数为0.73）。

Conclusion: 计算推理能力是衡量LLMs深层能力的重要维度，TMBench为评估提供了有效工具。

Abstract: With the rapid development and widespread application of Large Language
Models (LLMs), rigorous evaluation has become particularly crucial. This
research adopts a novel perspective, focusing on evaluating the core
computational reasoning ability of LLMs, defined as the capacity of model to
accurately understand rules, and execute logically computing operations. This
capability assesses the reliability of LLMs as precise executors, and is
critical to advanced tasks such as complex code generation and multi-step
problem-solving. We propose an evaluation framework based on Universal Turing
Machine (UTM) simulation. This framework requires LLMs to strictly follow
instructions and track dynamic states, such as tape content and read/write head
position, during multi-step computations. To enable standardized evaluation, we
developed TMBench, a benchmark for systematically studying the computational
reasoning capabilities of LLMs. TMBench provides several key advantages,
including knowledge-agnostic evaluation, adjustable difficulty, foundational
coverage through Turing machine encoding, and unlimited capacity for instance
generation, ensuring scalability as models continue to evolve. We find that
model performance on TMBench correlates strongly with performance on other
recognized reasoning benchmarks (Pearson correlation coefficient is 0.73),
clearly demonstrating that computational reasoning is a significant dimension
for measuring the deep capabilities of LLMs. Code and data are available at
https://github.com/HaitaoWuTJU/Turing-Machine-Bench.

</details>


### [35] [Universal language model with the intervention of quantum theory](https://arxiv.org/abs/2504.20839)
*D. -F. Qin*

Main category: cs.CL

TL;DR: 该论文探讨了基于量子力学理论的语言建模，提出将量子力学引入语言符号-意义对中，以构建自然语言的表示模型，并尝试用量子统计等理论改进词嵌入技术。


<details>
  <summary>Details</summary>
Motivation: 研究动机是将量子力学的数学框架应用于语言建模，以改进现有技术（如词嵌入）并探索自然语言的量子特性。

Method: 方法包括将量子力学理论引入语言符号-意义对，利用量子统计研究语言的数学表示与统计特性，并通过实验代码验证可行性。

Result: 结果表明量子理论可用于改进语言建模，并可能帮助构建生成模型，同时为未来量子计算机的应用提供初步讨论。

Conclusion: 结论是量子力学理论在语言建模中具有潜力，未来可进一步探索其在生成模型和量子计算中的应用。

Abstract: This paper examines language modeling based on the theory of quantum
mechanics. It focuses on the introduction of quantum mechanics into the
symbol-meaning pairs of language in order to build a representation model of
natural language. At the same time, it is realized that word embedding, which
is widely used as a basic technique for statistical language modeling, can be
explained and improved by the mathematical framework of quantum mechanics. On
this basis, this paper continues to try to use quantum statistics and other
related theories to study the mathematical representation, natural evolution
and statistical properties of natural language. It is also assumed that the
source of such quantum properties is the physicality of information. The
feasibility of using quantum theory to model natural language is pointed out
through the construction of a experimental code. The paper discusses, in terms
of applications, the possible help of the theory in constructing generative
models that are popular nowadays. A preliminary discussion of future
applications of the theory to quantum computers is also presented.

</details>


### [36] [JaccDiv: A Metric and Benchmark for Quantifying Diversity of Generated Marketing Text in the Music Industry](https://arxiv.org/abs/2504.20849)
*Anum Afzal,Alexandre Mercier,Florian Matthes*

Main category: cs.CL

TL;DR: 研究探讨了基于LLM的数据到文本方法，用于生成高质量且多样化的营销文本，并提出了评估多样性的新指标JaccDiv。


<details>
  <summary>Details</summary>
Motivation: 传统生成方法容易陷入重复模式，导致文本单调，限制了在线平台的内容生成能力。

Method: 利用T5、GPT-3.5、GPT-4和LLaMa2等语言模型，结合微调、少样本和零样本方法，生成多样化的营销文本。

Result: 提出了JaccDiv指标评估文本多样性，证明该方法适用于音乐行业及其他需要自动化内容生成的领域。

Conclusion: 基于LLM的方法能够有效提升文本多样性和质量，具有广泛的应用潜力。

Abstract: Online platforms are increasingly interested in using Data-to-Text
technologies to generate content and help their users. Unfortunately,
traditional generative methods often fall into repetitive patterns, resulting
in monotonous galleries of texts after only a few iterations. In this paper, we
investigate LLM-based data-to-text approaches to automatically generate
marketing texts that are of sufficient quality and diverse enough for broad
adoption. We leverage Language Models such as T5, GPT-3.5, GPT-4, and LLaMa2 in
conjunction with fine-tuning, few-shot, and zero-shot approaches to set a
baseline for diverse marketing texts. We also introduce a metric JaccDiv to
evaluate the diversity of a set of texts. This research extends its relevance
beyond the music industry, proving beneficial in various fields where
repetitive automated content generation is prevalent.

</details>


### [37] [DYNAMAX: Dynamic computing for Transformers and Mamba based architectures](https://arxiv.org/abs/2504.20922)
*Miguel Nogales,Matteo Gambella,Manuel Roveri*

Main category: cs.CL

TL;DR: DYNAMAX框架首次将早期退出机制应用于Mamba架构，并展示了其在Mamba和Transformer模型中的高效性和多功能性。


<details>
  <summary>Details</summary>
Motivation: 探索早期退出机制在Mamba架构中的应用，以降低计算成本和延迟，同时保持预测准确性。

Method: 将早期退出机制集成到Mamba架构中，并将其重新用作高效的早期退出分类器，适用于Mamba和Transformer模型。

Result: 实验表明，Mamba作为早期退出分类器在计算节省、准确性和一致性方面表现优异，适用于多种NLP任务。

Conclusion: Mamba的动态处理设计为资源受限环境中的高效推理提供了新途径，具有重新定义LLM动态计算范式的潜力。

Abstract: Early exits (EEs) offer a promising approach to reducing computational costs
and latency by dynamically terminating inference once a satisfactory prediction
confidence on a data sample is achieved. Although many works integrate EEs into
encoder-only Transformers, their application to decoder-only architectures and,
more importantly, Mamba models, a novel family of state-space architectures in
the LLM realm, remains insufficiently explored. This work introduces DYNAMAX,
the first framework to exploit the unique properties of Mamba architectures for
early exit mechanisms. We not only integrate EEs into Mamba but also repurpose
Mamba as an efficient EE classifier for both Mamba-based and transformer-based
LLMs, showcasing its versatility. Our experiments employ the Mistral 7B
transformer compared to the Codestral 7B Mamba model, using data sets such as
TruthfulQA, CoQA, and TriviaQA to evaluate computational savings, accuracy, and
consistency. The results highlight the adaptability of Mamba as a powerful EE
classifier and its efficiency in balancing computational cost and performance
quality across NLP tasks. By leveraging Mamba's inherent design for dynamic
processing, we open pathways for scalable and efficient inference in embedded
applications and resource-constrained environments. This study underscores the
transformative potential of Mamba in redefining dynamic computing paradigms for
LLMs.

</details>


### [38] [Trace-of-Thought: Enhanced Arithmetic Problem Solving via Reasoning Distillation From Large to Small Language Models](https://arxiv.org/abs/2504.20946)
*Tyler McDonald,Ali Emami*

Main category: cs.CL

TL;DR: 论文提出了一种名为Trace-of-Thought Prompting的零样本提示工程方法，旨在通过开源模型（参数≤70亿）提升算术推理能力，性能提升高达125%。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在专业领域（如算术推理）的应用存在计算成本高、封闭模型限制定制化的问题，开源模型成为更优选择。

Method: 引入Trace-of-Thought Prompting方法，指导LLMs通过可观察的子问题解决过程增强推理能力。

Result: 在开源模型与GPT-4结合使用时，性能提升高达125%。

Conclusion: 开源模型在AI研究民主化和提升计算语言学应用可访问性方面具有潜力。

Abstract: As Large Language Models (LLMs) continue to be leveraged for daily tasks,
prompt engineering remains an active field of contribution within computational
linguistics, particularly in domains requiring specialized knowledge such as
arithmetic reasoning. While these LLMs are optimized for a variety of tasks,
their exhaustive employment may become computationally or financially
cumbersome for small teams. Additionally, complete reliance on proprietary,
closed-source models often limits customization and adaptability, posing
significant challenges in research and application scalability. Instead, by
leveraging open-source models at or below 7 billion parameters, we can optimize
our resource usage while still observing remarkable gains over standard
prompting approaches. To cultivate this notion, we introduce Trace-of-Thought
Prompting, a simple, zero-shot prompt engineering method that instructs LLMs to
create observable subproblems using critical problem-solving, specifically
designed to enhance arithmetic reasoning capabilities. When applied to
open-source models in tandem with GPT-4, we observe that Trace-of-Thought not
only allows novel insight into the problem-solving process but also introduces
performance gains as large as 125% on language models at or below 7 billion
parameters. This approach underscores the potential of open-source initiatives
in democratizing AI research and improving the accessibility of high-quality
computational linguistics applications.

</details>


### [39] [Information Gravity: A Field-Theoretic Model for Token Selection in Large Language Models](https://arxiv.org/abs/2504.20951)
*Maryna Vyshnyvetska*

Main category: cs.CL

TL;DR: 提出了一种名为‘信息引力’的理论模型，用于描述大语言模型（LLMs）中的文本生成过程，借鉴了场论和时空几何的物理概念。


<details>
  <summary>Details</summary>
Motivation: 解释LLMs中观察到的现象，如幻觉、对查询表述的敏感性以及采样温度对输出多样性的影响。

Method: 将用户查询视为具有‘信息质量’的对象，通过弯曲模型的语义空间形成引力势阱，吸引生成过程中的标记。

Result: 模型为LLM行为中的多种现象提供了机制解释。

Conclusion: ‘信息引力’模型为理解LLMs的文本生成过程提供了新的理论框架。

Abstract: We propose a theoretical model called "information gravity" to describe the
text generation process in large language models (LLMs). The model uses
physical apparatus from field theory and spacetime geometry to formalize the
interaction between user queries and the probability distribution of generated
tokens. A query is viewed as an object with "information mass" that curves the
semantic space of the model, creating gravitational potential wells that
"attract" tokens during generation. This model offers a mechanism to explain
several observed phenomena in LLM behavior, including hallucinations (emerging
from low-density semantic voids), sensitivity to query formulation (due to
semantic field curvature changes), and the influence of sampling temperature on
output diversity.

</details>


### [40] [OSVBench: Benchmarking LLMs on Specification Generation Tasks for Operating System Verification](https://arxiv.org/abs/2504.20964)
*Shangyu Li,Juyong Jiang,Tiancheng Zhao,Jiasi Shen*

Main category: cs.CL

TL;DR: OSVBench是一个新的基准测试，用于评估大型语言模型（LLMs）在操作系统内核验证任务中生成完整规范代码的能力。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs在操作系统验证任务中的表现有限，需要一种标准化的评估方法来衡量其能力。

Method: 将规范生成问题定义为程序合成问题，提供编程模型和验证假设，要求LLMs在有限语法和语义空间内生成完整规范。

Result: 对12种LLMs的综合评估显示，它们在操作系统验证任务中的表现有限，且处理长上下文代码生成任务的能力差异显著。

Conclusion: OSVBench为LLMs在操作系统验证任务中的性能提供了标准化评估工具，揭示了当前模型的局限性。

Abstract: We introduce OSVBench, a new benchmark for evaluating Large Language Models
(LLMs) in generating complete specification code pertaining to operating system
kernel verification tasks. The benchmark first defines the specification
generation problem into a program synthesis problem within a confined scope of
syntax and semantics by providing LLMs with the programming model. The LLMs are
required to understand the provided verification assumption and the potential
syntax and semantics space to search for, then generate the complete
specification for the potentially buggy operating system code implementation
under the guidance of the high-level functional description of the operating
system. This benchmark is built upon a real-world operating system kernel,
Hyperkernel, and consists of 245 complex specification generation tasks in
total, each is a long context task of about 20k-30k tokens. Our comprehensive
evaluation of 12 LLMs exhibits the limited performance of the current LLMs on
the specification generation tasks for operating system verification.
Significant disparities in their performance on the benchmark highlight
differences in their ability to handle long-context code generation tasks. The
evaluation toolkit and benchmark are available at
https://github.com/lishangyu-hkust/OSVBench.

</details>


### [41] [SetKE: Knowledge Editing for Knowledge Elements Overlap](https://arxiv.org/abs/2504.20972)
*Yifan Wei,Xiaoyan Yu,Ran Song,Hao Peng,Angsheng Li*

Main category: cs.CL

TL;DR: 论文提出了一种新的知识编辑方法SetKE，通过同时编辑三元组集合来解决知识元素重叠（KEO）问题，优于现有方法，并提供了包含KEO三元组的数据集EditSet作为基准。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）需要更新以纳入新知识并减少错误和幻觉，但传统更新方法存在过拟合和高计算成本问题。知识编辑（KE）是一种替代方案，但忽视了知识元素重叠（KEO）现象，导致编辑冲突。

Method: 提出知识集编辑（KSE）的新框架，并开发SetKE方法，同时编辑三元组集合以解决KEO问题。

Result: 实验表明，SetKE在主流LLMs上优于现有方法，特别是在KEO场景中。

Conclusion: SetKE有效解决了KEO问题，EditSet数据集为未来研究提供了基准。

Abstract: Large Language Models (LLMs) excel in tasks such as retrieval and question
answering but require updates to incorporate new knowledge and reduce
inaccuracies and hallucinations. Traditional updating methods, like fine-tuning
and incremental learning, face challenges such as overfitting and high
computational costs. Knowledge Editing (KE) provides a promising alternative
but often overlooks the Knowledge Element Overlap (KEO) phenomenon, where
multiple triplets share common elements, leading to editing conflicts. We
identify the prevalence of KEO in existing KE datasets and show its significant
impact on current KE methods, causing performance degradation in handling such
triplets. To address this, we propose a new formulation, Knowledge Set Editing
(KSE), and introduce SetKE, a method that edits sets of triplets
simultaneously. Experimental results demonstrate that SetKE outperforms
existing methods in KEO scenarios on mainstream LLMs. Additionally, we
introduce EditSet, a dataset containing KEO triplets, providing a comprehensive
benchmark.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [42] [Evolution of AI in Education: Agentic Workflows](https://arxiv.org/abs/2504.20082)
*Firuz Kamalov,David Santandreu Calonge,Linda Smail,Dilshod Azizov,Dimple R. Thadani,Theresa Kwong,Amara Atif*

Main category: cs.AI

TL;DR: 本文探讨了AI代理在教育中的潜力，通过反思、规划、工具使用和多代理协作四大范式，分析了其优势、应用与挑战，并展示了一个多代理框架用于自动评分，初步结果优于传统LLM。


<details>
  <summary>Details</summary>
Motivation: 传统大型语言模型（LLM）在教育中存在静态数据依赖、适应性和推理能力不足的问题，AI代理被视为解决这些限制并推动教育创新的新途径。

Method: 通过四大范式（反思、规划、工具使用和多代理协作）分析AI代理在教育中的作用，并开发了一个多代理框架用于自动评分。

Result: 初步结果表明，多代理框架在自动评分中的一致性优于传统LLM。

Conclusion: AI代理在教育中具有变革潜力，但需进一步研究其可解释性、可信度和对教学的可持续影响。

Abstract: Artificial intelligence (AI) has transformed various aspects of education,
with large language models (LLMs) driving advancements in automated tutoring,
assessment, and content generation. However, conventional LLMs are constrained
by their reliance on static training data, limited adaptability, and lack of
reasoning. To address these limitations and foster more sustainable
technological practices, AI agents have emerged as a promising new avenue for
educational innovation. In this review, we examine agentic workflows in
education according to four major paradigms: reflection, planning, tool use,
and multi-agent collaboration. We critically analyze the role of AI agents in
education through these key design paradigms, exploring their advantages,
applications, and challenges. To illustrate the practical potential of agentic
systems, we present a proof-of-concept application: a multi-agent framework for
automated essay scoring. Preliminary results suggest this agentic approach may
offer improved consistency compared to stand-alone LLMs. Our findings highlight
the transformative potential of AI agents in educational settings while
underscoring the need for further research into their interpretability,
trustworthiness, and sustainable impact on pedagogical impact.

</details>


### [43] [AI Awareness](https://arxiv.org/abs/2504.20084)
*Xiaojian Li,Haoyuan Shi,Rongwu Xu,Wei Xu*

Main category: cs.AI

TL;DR: 本文综述了AI意识的四种形式（元认知、自我意识、社会意识和情境意识），探讨其理论基础、评估方法及与AI能力的关系，同时指出其带来的风险和伦理问题。


<details>
  <summary>Details</summary>
Motivation: 随着AI能力的提升，AI意识不再仅是哲学问题，而是可测量的功能能力，需要系统研究其表现与影响。

Method: 结合认知科学、心理学和计算理论，分析AI意识的理论基础，并系统评估现有方法和实证结果。

Result: 研究发现AI意识与智能行为水平密切相关，但也带来安全、对齐和伦理风险。

Conclusion: AI意识是一把双刃剑，需在提升能力的同时谨慎监管，为未来研究提供方向。

Abstract: Recent breakthroughs in artificial intelligence (AI) have brought about
increasingly capable systems that demonstrate remarkable abilities in
reasoning, language understanding, and problem-solving. These advancements have
prompted a renewed examination of AI awareness, not as a philosophical question
of consciousness, but as a measurable, functional capacity. In this review, we
explore the emerging landscape of AI awareness, which includes meta-cognition
(the ability to represent and reason about its own state), self-awareness
(recognizing its own identity, knowledge, limitations, inter alia), social
awareness (modeling the knowledge, intentions, and behaviors of other agents),
and situational awareness (assessing and responding to the context in which it
operates).
  First, we draw on insights from cognitive science, psychology, and
computational theory to trace the theoretical foundations of awareness and
examine how the four distinct forms of AI awareness manifest in
state-of-the-art AI. Next, we systematically analyze current evaluation methods
and empirical findings to better understand these manifestations. Building on
this, we explore how AI awareness is closely linked to AI capabilities,
demonstrating that more aware AI agents tend to exhibit higher levels of
intelligent behaviors. Finally, we discuss the risks associated with AI
awareness, including key topics in AI safety, alignment, and broader ethical
concerns.
  AI awareness is a double-edged sword: it improves general capabilities, i.e.,
reasoning, safety, while also raises concerns around misalignment and societal
risks, demanding careful oversight as AI capabilities grow. On the whole, our
interdisciplinary review provides a roadmap for future research and aims to
clarify the role of AI awareness in the ongoing development of intelligent
machines.

</details>


### [44] [Spark: A System for Scientifically Creative Idea Generation](https://arxiv.org/abs/2504.20090)
*Aishik Sanyal,Samuel Schapiro,Sumuk Shashidhar,Royce Moon,Lav R. Varshney,Dilek Hakkani-Tur*

Main category: cs.AI

TL;DR: 论文介绍了Spark系统，结合检索增强的LLMs生成科学创意，并利用基于60万科学评论训练的Judge模型进行评审，旨在激发计算创造力研究。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在科学创意生成中的潜力，并推动计算创造力领域的研究。

Method: 开发Spark系统，结合检索增强的LLMs生成创意，并训练Judge模型进行评审。

Result: 发布了用于训练Judge的标注数据集，鼓励研究者探索LLMs在创意生成和评估中的应用。

Conclusion: Spark系统展示了LLMs在科学创意生成中的潜力，为计算创造力研究提供了新方向。

Abstract: Recently, large language models (LLMs) have shown promising abilities to
generate novel research ideas in science, a direction which coincides with many
foundational principles in computational creativity (CC). In light of these
developments, we present an idea generation system named Spark that couples
retrieval-augmented idea generation using LLMs with a reviewer model named
Judge trained on 600K scientific reviews from OpenReview. Our work is both a
system demonstration and intended to inspire other CC researchers to explore
grounding the generation and evaluation of scientific ideas within foundational
CC principles. To this end, we release the annotated dataset used to train
Judge, inviting other researchers to explore the use of LLMs for idea
generation and creative evaluations.

</details>


### [45] [Personalized Artificial General Intelligence (AGI) via Neuroscience-Inspired Continuous Learning Systems](https://arxiv.org/abs/2504.20109)
*Rajeev Gupta,Suhani Gupta,Ronak Parikh,Divya Gupta,Amir Javaheri,Jairaj Singh Shaktawat*

Main category: cs.AI

TL;DR: 本文提出了一种新型的个性化通用人工智能（AGI）架构，结合脑科学启发的学习机制，适用于边缘设备部署，支持持续学习和个性化。


<details>
  <summary>Details</summary>
Motivation: 当前大规模深度学习模型虽在任务特定性能上有所提升，但无法实现持续、适应性强且通用的学习，尤其是在资源受限的边缘设备上。

Method: 综述了持续学习和神经科学启发的AI文献，提出了一种结合快速-慢速学习模块、突触自优化和高效内存更新的架构。

Result: 提出了理论架构，解决了灾难性遗忘、内存效率和系统可扩展性等挑战，并展示了应用场景。

Conclusion: 该架构为未来实现真正持续、个性化的边缘AGI提供了路线图。

Abstract: Artificial Intelligence has made remarkable advancements in recent years,
primarily driven by increasingly large deep learning models. However, achieving
true Artificial General Intelligence (AGI) demands fundamentally new
architectures rather than merely scaling up existing models. Current approaches
largely depend on expanding model parameters, which improves task-specific
performance but falls short in enabling continuous, adaptable, and generalized
learning. Achieving AGI capable of continuous learning and personalization on
resource-constrained edge devices is an even bigger challenge.
  This paper reviews the state of continual learning and neuroscience-inspired
AI, and proposes a novel architecture for Personalized AGI that integrates
brain-like learning mechanisms for edge deployment. We review literature on
continuous lifelong learning, catastrophic forgetting, and edge AI, and discuss
key neuroscience principles of human learning, including Synaptic Pruning,
Hebbian plasticity, Sparse Coding, and Dual Memory Systems, as inspirations for
AI systems. Building on these insights, we outline an AI architecture that
features complementary fast-and-slow learning modules, synaptic
self-optimization, and memory-efficient model updates to support on-device
lifelong adaptation.
  Conceptual diagrams of the proposed architecture and learning processes are
provided. We address challenges such as catastrophic forgetting, memory
efficiency, and system scalability, and present application scenarios for
mobile AI assistants and embodied AI systems like humanoid robots. We conclude
with key takeaways and future research directions toward truly continual,
personalized AGI on the edge. While the architecture is theoretical, it
synthesizes diverse findings and offers a roadmap for future implementation.

</details>


### [46] [Transforming Evidence Synthesis: A Systematic Review of the Evolution of Automated Meta-Analysis in the Age of AI](https://arxiv.org/abs/2504.20113)
*Lingbo Li,Anuradha Mathrani,Teo Susnjak*

Main category: cs.AI

TL;DR: 本文通过系统综述评估了自动化元分析（AMA）的现状，发现其主要集中在数据处理阶段，而高级合成阶段和全流程自动化仍存在显著差距。尽管AI技术有所突破，但AMA在统计建模和高级合成中的应用仍不足。


<details>
  <summary>Details</summary>
Motivation: 科学文献的指数增长推动了对高效证据合成的需求，促使AMA领域的兴起。本文旨在评估AMA的当前状态及其潜力。

Method: 采用PRISMA系统综述方法，筛选了978篇论文（2006-2024年），并分析了54项研究，涵盖医学和非医学领域。

Result: 研究发现AMA主要集中在数据处理（57%），而高级合成阶段（17%）和全流程自动化（2%）研究较少。AI技术在统计建模和高级合成中的应用仍不足。

Conclusion: 未来需关注全流程自动化、提高解释性和方法稳健性，以实现AMA在跨领域高效合成中的潜力。

Abstract: Exponential growth in scientific literature has heightened the demand for
efficient evidence-based synthesis, driving the rise of the field of Automated
Meta-analysis (AMA) powered by natural language processing and machine
learning. This PRISMA systematic review introduces a structured framework for
assessing the current state of AMA, based on screening 978 papers from 2006 to
2024, and analyzing 54 studies across diverse domains. Findings reveal a
predominant focus on automating data processing (57%), such as extraction and
statistical modeling, while only 17% address advanced synthesis stages. Just
one study (2%) explored preliminary full-process automation, highlighting a
critical gap that limits AMA's capacity for comprehensive synthesis. Despite
recent breakthroughs in large language models (LLMs) and advanced AI, their
integration into statistical modeling and higher-order synthesis, such as
heterogeneity assessment and bias evaluation, remains underdeveloped. This has
constrained AMA's potential for fully autonomous meta-analysis. From our
dataset spanning medical (67%) and non-medical (33%) applications, we found
that AMA has exhibited distinct implementation patterns and varying degrees of
effectiveness in actually improving efficiency, scalability, and
reproducibility. While automation has enhanced specific meta-analytic tasks,
achieving seamless, end-to-end automation remains an open challenge. As AI
systems advance in reasoning and contextual understanding, addressing these
gaps is now imperative. Future efforts must focus on bridging automation across
all meta-analysis stages, refining interpretability, and ensuring
methodological robustness to fully realize AMA's potential for scalable,
domain-agnostic synthesis.

</details>


### [47] [Deep Physics Prior for First Order Inverse Optimization](https://arxiv.org/abs/2504.20278)
*Haoyu Yang,Kamyar Azizzadenesheli,Haoxing Ren*

Main category: cs.AI

TL;DR: 本文提出了一种名为Deep Physics Prior（DPP）的新方法，通过预训练的辅助神经算子实现基于梯度的逆优化，解决了传统方法在计算成本、可扩展性和噪声问题上的局限性。


<details>
  <summary>Details</summary>
Motivation: 逆设计优化在多个领域（如半导体制造、结构工程等）中面临缺乏显式数学表示的问题，导致传统优化方法难以应用。现有方法（如生成式AI和贝叶斯优化）存在计算成本高、对先验敏感等问题。

Method: 提出DPP方法，利用预训练的辅助神经算子，通过梯度优化实现逆设计，同时施加先验分布约束以确保解的鲁棒性和意义。

Result: DPP方法在缺乏先验数据和观测分布的情况下仍能有效工作，解决了传统方法的局限性。

Conclusion: DPP为逆设计优化提供了一种高效且鲁棒的新方法，特别适用于复杂系统。

Abstract: Inverse design optimization aims to infer system parameters from observed
solutions, posing critical challenges across domains such as semiconductor
manufacturing, structural engineering, materials science, and fluid dynamics.
The lack of explicit mathematical representations in many systems complicates
this process and makes the first order optimization impossible. Mainstream
approaches, including generative AI and Bayesian optimization, address these
challenges but have limitations. Generative AI is computationally expensive,
while Bayesian optimization, relying on surrogate models, suffers from
scalability, sensitivity to priors, and noise issues, often leading to
suboptimal solutions. This paper introduces Deep Physics Prior (DPP), a novel
method enabling first-order gradient-based inverse optimization with surrogate
machine learning models. By leveraging pretrained auxiliary Neural Operators,
DPP enforces prior distribution constraints to ensure robust and meaningful
solutions. This approach is particularly effective when prior data and
observation distributions are unknown.

</details>


### [48] [mrCAD: Multimodal Refinement of Computer-aided Designs](https://arxiv.org/abs/2504.20294)
*William P. McCarthy,Saujas Vaduguru,Karl D. D. Willis,Justin Matejka,Judith E. Fan,Daniel Fried,Yewen Pu*

Main category: cs.AI

TL;DR: 论文提出mrCAD数据集，研究人类如何通过多模态指令（文本和绘图）迭代修改设计，发现生成和修改指令的组成差异，并测试了现有视觉语言模型的表现。


<details>
  <summary>Details</summary>
Motivation: 人类协作中迭代修改概念的能力是核心，而生成式AI在内容生成上表现优异，但在语言指导的修改上存在不足。研究旨在弥合这一差距。

Method: 通过mrCAD数据集，记录人类玩家在多轮通信游戏中如何用文本、绘图或多模态指令修改CAD设计，并分析指令组成。

Result: 发现生成指令和修改指令在绘图与文本的组成上存在差异，且现有视觉语言模型在生成指令上表现优于修改指令。

Conclusion: 研究为分析和建模多模态修改语言奠定了基础，填补了现有数据集的空白。

Abstract: A key feature of human collaboration is the ability to iteratively refine the
concepts we have communicated. In contrast, while generative AI excels at the
\textit{generation} of content, it often struggles to make specific
language-guided \textit{modifications} of its prior outputs. To bridge the gap
between how humans and machines perform edits, we present mrCAD, a dataset of
multimodal instructions in a communication game. In each game, players created
computer aided designs (CADs) and refined them over several rounds to match
specific target designs. Only one player, the Designer, could see the target,
and they must instruct the other player, the Maker, using text, drawing, or a
combination of modalities. mrCAD consists of 6,082 communication games, 15,163
instruction-execution rounds, played between 1,092 pairs of human players. We
analyze the dataset and find that generation and refinement instructions differ
in their composition of drawing and text. Using the mrCAD task as a benchmark,
we find that state-of-the-art VLMs are better at following generation
instructions than refinement instructions. These results lay a foundation for
analyzing and modeling a multimodal language of refinement that is not
represented in previous datasets.

</details>


### [49] [Leveraging Action Relational Structures for Integrated Learning and Planning](https://arxiv.org/abs/2504.20318)
*Ryan Xiao Wang,Felipe Trevizan*

Main category: cs.AI

TL;DR: 论文提出了一种名为partial-space search的新搜索空间方法，结合学习系统优化规划任务，并通过实验验证其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有规划方法未充分利用PDDL动作模式的关系结构，导致搜索效率不高。

Method: 引入partial-space search和action set heuristics，并训练新的启发式方法。

Result: 新规划器LazyLifted在IPC 2023学习赛道和高分支因子任务中表现优异。

Conclusion: partial-space search结合学习启发式方法显著提升了规划效率。

Abstract: Recent advances in planning have explored using learning methods to help
planning. However, little attention has been given to adapting search
algorithms to work better with learning systems. In this paper, we introduce
partial-space search, a new search space for classical planning that leverages
the relational structure of actions given by PDDL action schemas -- a structure
overlooked by traditional planning approaches. Partial-space search provides a
more granular view of the search space and allows earlier pruning of poor
actions compared to state-space search. To guide partial-space search, we
introduce action set heuristics that evaluate sets of actions in a state. We
describe how to automatically convert existing heuristics into action set
heuristics. We also train action set heuristics from scratch using large
training datasets from partial-space search. Our new planner, LazyLifted,
exploits our better integrated search and learning heuristics and outperforms
the state-of-the-art ML-based heuristic on IPC 2023 learning track (LT)
benchmarks. We also show the efficiency of LazyLifted on high-branching factor
tasks and show that it surpasses LAMA in the combined IPC 2023 LT and
high-branching factor benchmarks.

</details>


### [50] [A Picture is Worth a Thousand Prompts? Efficacy of Iterative Human-Driven Prompt Refinement in Image Regeneration Tasks](https://arxiv.org/abs/2504.20340)
*Khoi Trinh,Scott Seidenberger,Raveen Wijewickrama,Murtuza Jadliwala,Anindya Maiti*

Main category: cs.AI

TL;DR: 研究探讨了AI图像再生中通过迭代提示优化实现目标图像重现的效果，并验证了图像相似性指标与人类感知的一致性。


<details>
  <summary>Details</summary>
Motivation: 随着AI生成内容普及，研究如何通过迭代提示优化实现目标图像重现，并验证图像相似性指标是否适用于迭代工作流。

Method: 通过结构化用户研究，评估迭代提示优化对图像相似性的影响，并比较图像相似性指标与人类感知的一致性。

Result: 迭代提示优化显著提高了图像对齐效果，主观评估和定量指标均验证了这一点。

Conclusion: 迭代工作流在生成AI内容中具有广泛潜力，图像相似性指标可作为有效的反馈机制。

Abstract: With AI-generated content becoming ubiquitous across the web, social media,
and other digital platforms, it is vital to examine how such content are
inspired and generated. The creation of AI-generated images often involves
refining the input prompt iteratively to achieve desired visual outcomes. This
study focuses on the relatively underexplored concept of image regeneration
using AI, in which a human operator attempts to closely recreate a specific
target image by iteratively refining their prompt. Image regeneration is
distinct from normal image generation, which lacks any predefined visual
reference. A separate challenge lies in determining whether existing image
similarity metrics (ISMs) can provide reliable, objective feedback in iterative
workflows, given that we do not fully understand if subjective human judgments
of similarity align with these metrics. Consequently, we must first validate
their alignment with human perception before assessing their potential as a
feedback mechanism in the iterative prompt refinement process. To address these
research gaps, we present a structured user study evaluating how iterative
prompt refinement affects the similarity of regenerated images relative to
their targets, while also examining whether ISMs capture the same improvements
perceived by human observers. Our findings suggest that incremental prompt
adjustments substantially improve alignment, verified through both subjective
evaluations and quantitative measures, underscoring the broader potential of
iterative workflows to enhance generative AI content creation across various
application domains.

</details>


### [51] [Skill Discovery for Software Scripting Automation via Offline Simulations with LLMs](https://arxiv.org/abs/2504.20406)
*Paiheng Xu,Gang Wu,Xiang Chen,Tong Yu,Chang Xiao,Franck Dernoncourt,Tianyi Zhou,Wei Ai,Viswanathan Swaminathan*

Main category: cs.AI

TL;DR: 提出了一种离线模拟框架，利用LLMs和公开脚本指南生成已验证脚本的技能集，显著提高自动化成功率并降低成本。


<details>
  <summary>Details</summary>
Motivation: 传统脚本编写需要编程知识和API熟悉度，LLMs生成的运行时代码存在安全风险和效率问题，需改进。

Method: 框架包含任务创建（功能指导和API协同探索）和技能生成（基于执行反馈验证脚本），引入GNN模型捕捉API协同性。

Result: 在Adobe Illustrator实验中，框架显著提高自动化成功率，减少响应时间并节省运行时令牌成本。

Conclusion: 首次将软件脚本接口作为LLM系统测试平台，展示了在受控环境中利用执行反馈的优势，为专业软件领域AI应用提供新思路。

Abstract: Scripting interfaces enable users to automate tasks and customize software
workflows, but creating scripts traditionally requires programming expertise
and familiarity with specific APIs, posing barriers for many users. While Large
Language Models (LLMs) can generate code from natural language queries, runtime
code generation is severely limited due to unverified code, security risks,
longer response times, and higher computational costs. To bridge the gap, we
propose an offline simulation framework to curate a software-specific skillset,
a collection of verified scripts, by exploiting LLMs and publicly available
scripting guides. Our framework comprises two components: (1) task creation,
using top-down functionality guidance and bottom-up API synergy exploration to
generate helpful tasks; and (2) skill generation with trials, refining and
validating scripts based on execution feedback. To efficiently navigate the
extensive API landscape, we introduce a Graph Neural Network (GNN)-based link
prediction model to capture API synergy, enabling the generation of skills
involving underutilized APIs and expanding the skillset's diversity.
Experiments with Adobe Illustrator demonstrate that our framework significantly
improves automation success rates, reduces response time, and saves runtime
token costs compared to traditional runtime code generation. This is the first
attempt to use software scripting interfaces as a testbed for LLM-based
systems, highlighting the advantages of leveraging execution feedback in a
controlled environment and offering valuable insights into aligning AI
capabilities with user needs in specialized software domains.

</details>


### [52] [RV-Syn: Rational and Verifiable Mathematical Reasoning Data Synthesis based on Structured Function Library](https://arxiv.org/abs/2504.20426)
*Jiapeng Wang,Jinhao Jiang,Zhiqiang Zhang,Jun Zhou,Wayne Xin Zhao*

Main category: cs.AI

TL;DR: RV-Syn是一种新型的数学数据合成方法，通过构建结构化数学操作函数库和计算图，生成可验证的高质量推理数据。


<details>
  <summary>Details</summary>
Motivation: 现有数据合成方法在掌握问题内部逻辑和确保解决方案可验证性方面存在挑战，RV-Syn旨在解决这些问题。

Method: RV-Syn基于初始种子问题构建数学操作函数库，生成Python格式的计算图作为解决方案，并将其反向翻译为复杂问题。

Result: 实验结果表明，RV-Syn在数据扩展效率上超越了现有合成方法，包括人工生成问题的方法。

Conclusion: RV-Syn为生成高质量推理数据集提供了可扩展的框架。

Abstract: The advancement of reasoning capabilities in Large Language Models (LLMs)
requires substantial amounts of high-quality reasoning data, particularly in
mathematics. Existing data synthesis methods, such as data augmentation from
annotated training sets or direct question generation based on relevant
knowledge points and documents, have expanded datasets but face challenges in
mastering the inner logic of the problem during generation and ensuring the
verifiability of the solutions. To address these issues, we propose RV-Syn, a
novel Rational and Verifiable mathematical Synthesis approach. RV-Syn
constructs a structured mathematical operation function library based on
initial seed problems and generates computational graphs as solutions by
combining Python-formatted functions from this library. These graphs are then
back-translated into complex problems. Based on the constructed computation
graph, we achieve solution-guided logic-aware problem generation. Furthermore,
the executability of the computational graph ensures the verifiability of the
solving process. Experimental results show that RV-Syn surpasses existing
synthesis methods, including those involving human-generated problems,
achieving greater efficient data scaling. This approach provides a scalable
framework for generating high-quality reasoning datasets.

</details>


### [53] [Head-Tail-Aware KL Divergence in Knowledge Distillation for Spiking Neural Networks](https://arxiv.org/abs/2504.20445)
*Tianqing Zhang,Zixin Zhu,Kairong Yu,Hongwei Wang*

Main category: cs.AI

TL;DR: 提出了一种名为HTA-KL的新知识蒸馏方法，用于提升SNN的性能，通过动态区分高低概率区域并平衡知识转移，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: SNN在性能和效率上存在与ANN的差距，传统KL方法未能充分利用SNN特性，导致泛化能力不足。

Method: 提出HTA-KL方法，结合累积概率掩码和自适应权重，整合FKL和RKL以对齐分布的高低概率区域。

Result: 在CIFAR-10、CIFAR-100和Tiny ImageNet数据集上表现优于现有方法，且需要更少的时间步。

Conclusion: HTA-KL方法有效提升了SNN的性能，填补了与ANN的差距，为SNN训练提供了新思路。

Abstract: Spiking Neural Networks (SNNs) have emerged as a promising approach for
energy-efficient and biologically plausible computation. However, due to
limitations in existing training methods and inherent model constraints, SNNs
often exhibit a performance gap when compared to Artificial Neural Networks
(ANNs). Knowledge distillation (KD) has been explored as a technique to
transfer knowledge from ANN teacher models to SNN student models to mitigate
this gap. Traditional KD methods typically use Kullback-Leibler (KL) divergence
to align output distributions. However, conventional KL-based approaches fail
to fully exploit the unique characteristics of SNNs, as they tend to
overemphasize high-probability predictions while neglecting low-probability
ones, leading to suboptimal generalization. To address this, we propose
Head-Tail Aware Kullback-Leibler (HTA-KL) divergence, a novel KD method for
SNNs. HTA-KL introduces a cumulative probability-based mask to dynamically
distinguish between high- and low-probability regions. It assigns adaptive
weights to ensure balanced knowledge transfer, enhancing the overall
performance. By integrating forward KL (FKL) and reverse KL (RKL) divergence,
our method effectively align both head and tail regions of the distribution. We
evaluate our methods on CIFAR-10, CIFAR-100 and Tiny ImageNet datasets. Our
method outperforms existing methods on most datasets with fewer timesteps.

</details>


### [54] [TAMO:Fine-Grained Root Cause Analysis via Tool-Assisted LLM Agent with Multi-Modality Observation Data](https://arxiv.org/abs/2504.20462)
*Qi Wang,Xiao Zhang,Mingyi Li,Yuan Yuan,Mengbai Xiao,Fuzhen Zhuang,Dongxiao Yu*

Main category: cs.AI

TL;DR: 论文提出了一种名为TAMO的工具辅助LLM代理，用于解决微服务和云原生技术中的根因分析问题，通过多模态数据和时间对齐表示克服了现有LLM方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 随着分布式系统的发展，微服务和云原生技术带来了系统复杂性和运维挑战，传统根因分析依赖人工干预，而现有基于LLM的方法存在文本输入限制、动态服务依赖幻觉和上下文窗口限制等问题。

Method: 提出TAMO，通过多模态观测数据的时间对齐表示提取一致特征，并利用专用工具进行根因定位和故障分类，结构化关键信息以生成与系统上下文一致的修复策略。

Result: 实验结果表明，TAMO在处理异构性和常见故障类型的公共数据集时表现良好，验证了其有效性。

Conclusion: TAMO通过工具辅助和多模态数据解决了LLM在根因分析中的局限性，为AIOps提供了新的解决方案。

Abstract: With the development of distributed systems, microservices and cloud native
technologies have become central to modern enterprise software development.
Despite bringing significant advantages, these technologies also increase
system complexity and operational challenges. Traditional root cause analysis
(RCA) struggles to achieve automated fault response, heavily relying on manual
intervention. In recent years, large language models (LLMs) have made
breakthroughs in contextual inference and domain knowledge integration,
providing new solutions for Artificial Intelligence for Operations (AIOps).
However, Existing LLM-based approaches face three key challenges: text input
constraints, dynamic service dependency hallucinations, and context window
limitations. To address these issues, we propose a tool-assisted LLM agent with
multi-modality observation data, namely TAMO, for fine-grained RCA. It unifies
multi-modal observational data into time-aligned representations to extract
consistent features and employs specialized root cause localization and fault
classification tools for perceiving the contextual environment. This approach
overcomes the limitations of LLM in handling real-time changing service
dependencies and raw observational data and guides LLM to generate repair
strategies aligned with system contexts by structuring key information into a
prompt. Experimental results show that TAMO performs well in root cause
analysis when dealing with public datasets characterized by heterogeneity and
common fault types, demonstrating its effectiveness.

</details>


### [55] [A Summary on GUI Agents with Foundation Models Enhanced by Reinforcement Learning](https://arxiv.org/abs/2504.20464)
*Jiahao Li,Kaer Huang*

Main category: cs.AI

TL;DR: 本文总结了基于多模态大语言模型（MLLM）和强化学习（RL）的GUI智能代理的最新进展，包括任务形式化、架构演变和训练方法，并指出了未来挑战。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过MLLM和RL提升GUI代理的智能交互能力，以应对复杂现实环境中的任务。

Method: 将GUI代理任务形式化为马尔可夫决策过程，分析模块化架构（感知、规划、执行），并分类训练方法为提示工程、监督微调和强化学习。

Result: 多模态感知、决策推理和自适应动作生成的创新显著提升了GUI代理的泛化能力和鲁棒性。

Conclusion: 未来需解决的关键挑战包括进一步提升GUI代理的能力和可靠性。

Abstract: Graphical User Interface (GUI) agents, driven by Multi-modal Large Language
Models (MLLMs), have emerged as a promising paradigm for enabling intelligent
interaction with digital systems. This paper provides a structured summary of
recent advances in GUI agents, focusing on architectures enhanced by
Reinforcement Learning (RL). We first formalize GUI agent tasks as Markov
Decision Processes and discuss typical execution environments and evaluation
metrics. We then review the modular architecture of (M)LLM-based GUI agents,
covering Perception, Planning, and Acting modules, and trace their evolution
through representative works. Furthermore, we categorize GUI agent training
methodologies into Prompt-based, Supervised Fine-Tuning (SFT)-based, and
RL-based approaches, highlighting the progression from simple prompt
engineering to dynamic policy learning via RL. Our summary illustrates how
recent innovations in multimodal perception, decision reasoning, and adaptive
action generation have significantly improved the generalization and robustness
of GUI agents in complex real-world environments. We conclude by identifying
key challenges and future directions for building more capable and reliable GUI
agents.

</details>


### [56] [MuRAL: A Multi-Resident Ambient Sensor Dataset Annotated with Natural Language for Activities of Daily Living](https://arxiv.org/abs/2504.20505)
*Xi Chen,Julien Cumin,Fano Ramparany,Dominique Vaufreydaz*

Main category: cs.AI

TL;DR: MuRAL是首个多居民环境传感器数据集，专为LLM设计，包含21小时的多用户传感器数据，支持自然语言标注和任务测试。


<details>
  <summary>Details</summary>
Motivation: 现有数据集（如CASAS、ARAS、MARBLE）缺乏上下文丰富性和标注粒度，无法充分发挥LLM在人类活动识别中的潜力。

Method: 引入MuRAL数据集，包含多用户传感器数据、自然语言描述和活动标签，并测试LLM在主题分配、动作描述和活动分类任务中的表现。

Result: LLM能提供丰富的语义解释，但在处理多用户模糊性和传感器上下文不足时仍面临挑战。

Conclusion: MuRAL支持未来LLM驱动的智能环境研究，促进可解释和社交感知的活动理解。

Abstract: Recent advances in Large Language Models (LLMs) have shown promising
potential for human activity recognition (HAR) using ambient sensors,
especially through natural language reasoning and zero-shot learning. However,
existing datasets such as CASAS, ARAS, and MARBLE were not originally designed
with LLMs in mind and therefore lack the contextual richness, complexity, and
annotation granularity required to fully exploit LLM capabilities. In this
paper, we introduce MuRAL, the first Multi-Resident Ambient sensor dataset with
natural Language, comprising over 21 hours of multi-user sensor data collected
from 21 sessions in a smart-home environment. MuRAL is annotated with
fine-grained natural language descriptions, resident identities, and high-level
activity labels, all situated in dynamic, realistic multi-resident settings. We
benchmark MuRAL using state-of-the-art LLMs for three core tasks: subject
assignment, action description, and activity classification. Our results
demonstrate that while LLMs can provide rich semantic interpretations of
ambient data, current models still face challenges in handling multi-user
ambiguity and under-specified sensor contexts. We release MuRAL to support
future research on LLM-powered, explainable, and socially aware activity
understanding in smart environments. For access to the dataset, please reach
out to us via the provided contact information. A direct link for dataset
retrieval will be made available at this location in due course.

</details>


### [57] [ReasonIR: Training Retrievers for Reasoning Tasks](https://arxiv.org/abs/2504.20595)
*Rulin Shao,Rui Qiao,Varsha Kishore,Niklas Muennighoff,Xi Victoria Lin,Daniela Rus,Bryan Kian Hsiang Low,Sewon Min,Wen-tau Yih,Pang Wei Koh,Luke Zettlemoyer*

Main category: cs.AI

TL;DR: ReasonIR-8B是一种专为通用推理任务设计的检索模型，通过合成数据和公共数据混合训练，在推理密集型IR基准测试中取得最佳成绩，并显著提升RAG任务表现。


<details>
  <summary>Details</summary>
Motivation: 现有检索模型在推理任务中表现有限，主要因为训练数据集中于简单事实查询。

Method: 开发合成数据生成流程，生成具有挑战性的查询和硬负样本，结合公共数据训练模型。

Result: 在BRIGHT基准测试中达到29.9 nDCG@10（无重排）和36.9 nDCG@10（有重排），在MMLU和GPQA任务中分别提升6.4%和22.6%。

Conclusion: ReasonIR-8B在推理任务中表现优异，训练方法通用且开源，适用于未来LLMs。

Abstract: We present ReasonIR-8B, the first retriever specifically trained for general
reasoning tasks. Existing retrievers have shown limited gains on reasoning
tasks, in part because existing training datasets focus on short factual
queries tied to documents that straightforwardly answer them. We develop a
synthetic data generation pipeline that, for each document, our pipeline
creates a challenging and relevant query, along with a plausibly related but
ultimately unhelpful hard negative. By training on a mixture of our synthetic
data and existing public data, ReasonIR-8B achieves a new state-of-the-art of
29.9 nDCG@10 without reranker and 36.9 nDCG@10 with reranker on BRIGHT, a
widely-used reasoning-intensive information retrieval (IR) benchmark. When
applied to RAG tasks, ReasonIR-8B improves MMLU and GPQA performance by 6.4%
and 22.6% respectively, relative to the closed-book baseline, outperforming
other retrievers and search engines. In addition, ReasonIR-8B uses test-time
compute more effectively: on BRIGHT, its performance consistently increases
with longer and more information-rich rewritten queries; it continues to
outperform other retrievers when combined with an LLM reranker. Our training
recipe is general and can be easily extended to future LLMs; to this end, we
open-source our code, data, and model.

</details>


### [58] [PaRT: Enhancing Proactive Social Chatbots with Personalized Real-Time Retrieval](https://arxiv.org/abs/2504.20624)
*Zihan Niu,Zheyong Xie,Shaosheng Cao,Chonggang Lu,Zheyu Ye,Tong Xu,Zuozhu Liu,Yan Gao,Jia Chen,Zhe Xu,Yi Wu,Yao Hu*

Main category: cs.AI

TL;DR: PaRT框架通过个性化实时检索和生成，提升社交聊天机器人的主动对话能力，显著延长对话时长。


<details>
  <summary>Details</summary>
Motivation: 传统聊天机器人依赖用户主动维持对话，导致参与度低和对话时间短。PaRT旨在通过上下文感知的主动对话机制解决这一问题。

Method: PaRT整合用户画像和对话上下文到LLM中，生成个性化话题并检索相关知识，最终生成基于知识的优化回复。

Result: 在实际生产环境中运行30天，平均对话时长提升21.77%。

Conclusion: PaRT框架有效提升了社交聊天机器人的主动性和对话质量。

Abstract: Social chatbots have become essential intelligent companions in daily
scenarios ranging from emotional support to personal interaction. However,
conventional chatbots with passive response mechanisms usually rely on users to
initiate or sustain dialogues by bringing up new topics, resulting in
diminished engagement and shortened dialogue duration. In this paper, we
present PaRT, a novel framework enabling context-aware proactive dialogues for
social chatbots through personalized real-time retrieval and generation.
Specifically, PaRT first integrates user profiles and dialogue context into a
large language model (LLM), which is initially prompted to refine user queries
and recognize their underlying intents for the upcoming conversation. Guided by
refined intents, the LLM generates personalized dialogue topics, which then
serve as targeted queries to retrieve relevant passages from RedNote. Finally,
we prompt LLMs with summarized passages to generate knowledge-grounded and
engagement-optimized responses. Our approach has been running stably in a
real-world production environment for more than 30 days, achieving a 21.77\%
improvement in the average duration of dialogues.

</details>


### [59] [Cognitive maps are generative programs](https://arxiv.org/abs/2504.20628)
*Marta Kryven,Cole Wyeth,Aidan Curtis,Kevin Ellis*

Main category: cs.AI

TL;DR: 论文探讨了人类高效规划行为可能源于将世界视为可预测结构的认知地图，并提出了一种基于生成程序的模型，该模型在预测人类行为方面优于非结构化算法。


<details>
  <summary>Details</summary>
Motivation: 研究人类如何在有限资源下构建功能性认知地图，并探索其规划行为的机制。

Method: 提出程序化认知地图的概念，利用生成程序捕捉可预测性和冗余性，并通过行为实验和计算模型验证假设。

Result: 模型在计算效率、内存需求和预测人类行为方面优于非结构化算法，支持程序化认知地图的假设。

Conclusion: 人类规划策略依赖于程序化认知地图，这种结构化的表示方式有助于资源高效利用。

Abstract: Making sense of the world and acting in it relies on building simplified
mental representations that abstract away aspects of reality. This principle of
cognitive mapping is universal to agents with limited resources. Living
organisms, people, and algorithms all face the problem of forming functional
representations of their world under various computing constraints. In this
work, we explore the hypothesis that human resource-efficient planning may
arise from representing the world as predictably structured. Building on the
metaphor of concepts as programs, we propose that cognitive maps can take the
form of generative programs that exploit predictability and redundancy, in
contrast to directly encoding spatial layouts. We use a behavioral experiment
to show that people who navigate in structured spaces rely on modular planning
strategies that align with programmatic map representations. We describe a
computational model that predicts human behavior in a variety of structured
scenarios. This model infers a small distribution over possible programmatic
cognitive maps conditioned on human prior knowledge of the world, and uses this
distribution to generate resource-efficient plans. Our models leverages a Large
Language Model as an embedding of human priors, implicitly learned through
training on a vast corpus of human data. Our model demonstrates improved
computational efficiency, requires drastically less memory, and outperforms
unstructured planning algorithms with cognitive constraints at predicting human
behavior, suggesting that human planning strategies rely on programmatic
cognitive maps.

</details>


### [60] [The Limits of AI Explainability: An Algorithmic Information Theory Approach](https://arxiv.org/abs/2504.20676)
*Shrisha Rao*

Main category: cs.AI

TL;DR: 本文通过算法信息理论为AI可解释性的基本限制建立了理论基础，量化了近似误差和解释复杂性，并提出了关键理论贡献和监管不可能定理。


<details>
  <summary>Details</summary>
Motivation: 研究AI可解释性的基本限制，为设计、评估和监管可解释AI系统提供理论支持。

Method: 使用Kolmogorov复杂性量化解释复杂性，提出复杂性差距定理和精确边界，分析局部与全局可解释性的差异。

Result: 证明了简化解释必然与原始模型存在差异，解释复杂性随输入维度指数增长但对误差容忍度多项式增长，局部解释可显著简化且保持准确性。

Conclusion: 研究结果强调了在设计、评估和监管可解释AI系统时需权衡能力、解释性和误差，揭示了监管框架的局限性。

Abstract: This paper establishes a theoretical foundation for understanding the
fundamental limits of AI explainability through algorithmic information theory.
We formalize explainability as the approximation of complex models by simpler
ones, quantifying both approximation error and explanation complexity using
Kolmogorov complexity. Our key theoretical contributions include: (1) a
complexity gap theorem proving that any explanation significantly simpler than
the original model must differ from it on some inputs; (2) precise bounds
showing that explanation complexity grows exponentially with input dimension
but polynomially with error tolerance for Lipschitz functions; and (3) a
characterization of the gap between local and global explainability,
demonstrating that local explanations can be significantly simpler while
maintaining accuracy in relevant regions. We further establish a regulatory
impossibility theorem proving that no governance framework can simultaneously
pursue unrestricted AI capabilities, human-interpretable explanations, and
negligible error. These results highlight considerations likely to be relevant
to the design, evaluation, and oversight of explainable AI systems.

</details>


### [61] [Graph-Based Fault Diagnosis for Rotating Machinery: Adaptive Segmentation and Structural Feature Integration](https://arxiv.org/abs/2504.20756)
*Moirangthem Tiken Singh*

Main category: cs.AI

TL;DR: 提出了一种基于图的新型框架，用于旋转机械的多类故障诊断，具有鲁棒性和可解释性。该方法结合了信号分割、特征提取和图建模，取得了高精度和噪声鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决传统故障诊断方法在复杂性和可解释性上的不足，同时保持高精度和鲁棒性。

Method: 集成熵优化信号分割、时频特征提取和图论建模，利用图度量（如平均最短路径长度、模块性）和局部特征进行分类。

Result: 在CWRU和SU数据集上分别达到99.8%和100%的准确率，噪声鲁棒性强（95.4%准确率），跨域迁移性能优异（99.7% F1-score）。

Conclusion: 该方法无需深度学习架构，复杂度低且可解释，适用于工业实时诊断，具有高可靠性和扩展性。

Abstract: This paper proposes a novel graph-based framework for robust and
interpretable multiclass fault diagnosis in rotating machinery. The method
integrates entropy-optimized signal segmentation, time-frequency feature
extraction, and graph-theoretic modeling to transform vibration signals into
structured representations suitable for classification. Graph metrics, such as
average shortest path length, modularity, and spectral gap, are computed and
combined with local features to capture global and segment-level fault
characteristics. The proposed method achieves high diagnostic accuracy when
evaluated on two benchmark datasets, the CWRU bearing dataset (under 0-3 HP
loads) and the SU gearbox and bearing datasets (under different speed-load
configurations). Classification scores reach up to 99.8% accuracy on Case
Western Reserve University (CWRU) and 100% accuracy on the Southeast University
datasets using a logistic regression classifier. Furthermore, the model
exhibits strong noise resilience, maintaining over 95.4% accuracy at high noise
levels (standard deviation = 0.5), and demonstrates excellent cross-domain
transferability with up to 99.7% F1-score in load-transfer scenarios. Compared
to traditional techniques, this approach requires no deep learning
architecture, enabling lower complexity while ensuring interpretability. The
results confirm the method's scalability, reliability, and potential for
real-time deployment in industrial diagnostics.

</details>


### [62] [Approximate Lifted Model Construction](https://arxiv.org/abs/2504.20784)
*Malte Luttermann,Jan Speller,Marcel Gehrke,Tanya Braun,Ralf Möller,Mattis Hartwig*

Main category: cs.AI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Probabilistic relational models such as parametric factor graphs enable
efficient (lifted) inference by exploiting the indistinguishability of objects.
In lifted inference, a representative of indistinguishable objects is used for
computations. To obtain a relational (i.e., lifted) representation, the
Advanced Colour Passing (ACP) algorithm is the state of the art. The ACP
algorithm, however, requires underlying distributions, encoded as
potential-based factorisations, to exactly match to identify and exploit
indistinguishabilities. Hence, ACP is unsuitable for practical applications
where potentials learned from data inevitably deviate even if associated
objects are indistinguishable. To mitigate this problem, we introduce the
$\varepsilon$-Advanced Colour Passing ($\varepsilon$-ACP) algorithm, which
allows for a deviation of potentials depending on a hyperparameter
$\varepsilon$. $\varepsilon$-ACP efficiently uncovers and exploits
indistinguishabilities that are not exact. We prove that the approximation
error induced by $\varepsilon$-ACP is strictly bounded and our experiments show
that the approximation error is close to zero in practice.

</details>


### [63] [Partitioned Memory Storage Inspired Few-Shot Class-Incremental learning](https://arxiv.org/abs/2504.20797)
*Renye Zhang,Yimin Yin,Jinghua Zhang*

Main category: cs.AI

TL;DR: 论文提出了一种Few-Shot Class-Incremental Learning (FSCIL)方法，通过为每个会话学习独立模型来避免灾难性遗忘，并利用不确定性量化进行模型部署。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习技术过度依赖大量训练数据且缺乏动态适应性，与人类智能存在差距。FSCIL旨在通过有限样本持续学习新类别而不遗忘旧知识。

Method: 为每个会话学习独立模型，避免稳定性-可塑性困境，并在测试阶段结合不确定性量化进行模型部署。

Result: 在CIFAR-100和mini-ImageNet数据集上实现了最先进的性能。

Conclusion: 该方法为FSCIL提供了新视角，有效解决了灾难性遗忘问题。

Abstract: Current mainstream deep learning techniques exhibit an over-reliance on
extensive training data and a lack of adaptability to the dynamic world,
marking a considerable disparity from human intelligence. To bridge this gap,
Few-Shot Class-Incremental Learning (FSCIL) has emerged, focusing on continuous
learning of new categories with limited samples without forgetting old
knowledge. Existing FSCIL studies typically use a single model to learn
knowledge across all sessions, inevitably leading to the stability-plasticity
dilemma. Unlike machines, humans store varied knowledge in different cerebral
cortices. Inspired by this characteristic, our paper aims to develop a method
that learns independent models for each session. It can inherently prevent
catastrophic forgetting. During the testing stage, our method integrates
Uncertainty Quantification (UQ) for model deployment. Our method provides a
fresh viewpoint for FSCIL and demonstrates the state-of-the-art performance on
CIFAR-100 and mini-ImageNet datasets.

</details>


### [64] [Ascendra: Dynamic Request Prioritization for Efficient LLM Serving](https://arxiv.org/abs/2504.20828)
*Azam Ikram,Xiang Li,Sameh Elnikety,Saurabh Bagchi*

Main category: cs.AI

TL;DR: Ascendra是一个LLM服务系统，通过分区GPU资源为高低优先级实例，同时满足TTFT和TBT的SLO要求，提升吞吐量1.7倍。


<details>
  <summary>Details</summary>
Motivation: 现有系统通常牺牲一个指标（TTFT或TBT）以优化另一个，无法同时满足两者的SLO要求。

Method: Ascendra将GPU资源分为低优先级和高优先级实例，低优先级实例最大化吞吐量，高优先级实例处理紧急请求。通过性能模型预测可能违反SLO的请求并转移。

Result: Ascendra在满足TTFT和TBT SLO的同时，吞吐量比vLLM和Sarathi-Serve提高了1.7倍。

Conclusion: Ascendra通过动态资源分区和优先级调度，有效平衡了高吞吐量和低延迟的需求。

Abstract: The rapid advancement of Large Language Models (LLMs) has driven the need for
more efficient serving strategies. In this context, efficiency refers to the
proportion of requests that meet their Service Level Objectives (SLOs),
particularly for Time To First Token (TTFT) and Time Between Tokens (TBT).
However, existing systems often prioritize one metric at the cost of the other.
We present Ascendra, an LLM serving system designed to meet both TTFT and TBT
SLOs simultaneously. The core insight behind Ascendra is that a request's
urgency evolves as it approaches its deadline. To leverage this, Ascendra
partitions GPU resources into two types of instances: low-priority and
high-priority. Low-priority instances maximize throughput by processing
requests out of arrival order, but at the risk of request starvation. To
address this, Ascendra employs a performance model to predict requests at risk
of missing their SLOs and proactively offloads them to high-priority instances.
High-priority instances are optimized for low-latency execution and handle
urgent requests nearing their deadlines. This partitioned architecture enables
Ascendra to effectively balance high throughput and low latency. Extensive
evaluation shows that Ascendra improves system throughput by up to 1.7x
compared to vLLM and Sarathi-Serve while meeting both TTFT and TBT SLOs.

</details>


### [65] [Disjunctive and Conjunctive Normal Form Explanations of Clusters Using Auxiliary Information](https://arxiv.org/abs/2504.20846)
*Robert F. Downey,S. S. Ravi*

Main category: cs.AI

TL;DR: 论文提出了一种利用未用于聚类的辅助信息（标签）生成聚类后解释的方法，包括析取形式和二子句合取范式（CNF）形式，并采用整数线性规划（ILP）和启发式方法生成解释。


<details>
  <summary>Details</summary>
Motivation: 旨在通过辅助信息（标签）为聚类结果提供可解释性，帮助理解聚类形成的依据。

Method: 使用整数线性规划（ILP）和启发式方法生成析取形式和二子句CNF形式的解释。

Result: 通过多种数据集验证了方法的有效性，并展示了其可扩展性。

Conclusion: 提出的方法能够有效生成聚类解释，为理解聚类结果提供了新视角。

Abstract: We consider generating post-hoc explanations of clusters generated from
various datasets using auxiliary information which was not used by clustering
algorithms. Following terminology used in previous work, we refer to the
auxiliary information as tags. Our focus is on two forms of explanations,
namely disjunctive form (where the explanation for a cluster consists of a set
of tags) and a two-clause conjunctive normal form (CNF) explanation (where the
explanation consists of two sets of tags, combined through the AND operator).
We use integer linear programming (ILP) as well as heuristic methods to
generate these explanations. We experiment with a variety of datasets and
discuss the insights obtained from our explanations. We also present
experimental results regarding the scalability of our explanation methods.

</details>


### [66] [The Leaderboard Illusion](https://arxiv.org/abs/2504.20879)
*Shivalika Singh,Yiyang Nan,Alex Wang,Daniel D'Souza,Sayash Kapoor,Ahmet Üstün,Sanmi Koyejo,Yuntian Deng,Shayne Longpre,Noah Smith,Beyza Ermis,Marzieh Fadaee,Sara Hooker*

Main category: cs.AI

TL;DR: 论文指出Chatbot Arena排行榜存在系统性偏差，私有测试和选择性披露导致数据不对称，影响公平性。


<details>
  <summary>Details</summary>
Motivation: 衡量科学进展需要公平的基准测试，但Chatbot Arena的当前实践导致排行榜失真，影响AI系统评估的公正性。

Method: 通过分析Meta等公司的私有测试行为、模型采样率及数据分配，揭示排行榜的偏差。

Result: 发现私有模型在Arena中占据优势，数据分配不均，开放模型受益较少；额外数据可显著提升性能。

Conclusion: 需改革Chatbot Arena的评估框架，以促进更公平、透明的基准测试。

Abstract: Measuring progress is fundamental to the advancement of any scientific field.
As benchmarks play an increasingly central role, they also grow more
susceptible to distortion. Chatbot Arena has emerged as the go-to leaderboard
for ranking the most capable AI systems. Yet, in this work we identify
systematic issues that have resulted in a distorted playing field. We find that
undisclosed private testing practices benefit a handful of providers who are
able to test multiple variants before public release and retract scores if
desired. We establish that the ability of these providers to choose the best
score leads to biased Arena scores due to selective disclosure of performance
results. At an extreme, we identify 27 private LLM variants tested by Meta in
the lead-up to the Llama-4 release. We also establish that proprietary closed
models are sampled at higher rates (number of battles) and have fewer models
removed from the arena than open-weight and open-source alternatives. Both
these policies lead to large data access asymmetries over time. Providers like
Google and OpenAI have received an estimated 19.2% and 20.4% of all data on the
arena, respectively. In contrast, a combined 83 open-weight models have only
received an estimated 29.7% of the total data. We show that access to Chatbot
Arena data yields substantial benefits; even limited additional data can result
in relative performance gains of up to 112% on the arena distribution, based on
our conservative estimates. Together, these dynamics result in overfitting to
Arena-specific dynamics rather than general model quality. The Arena builds on
the substantial efforts of both the organizers and an open community that
maintains this valuable evaluation platform. We offer actionable
recommendations to reform the Chatbot Arena's evaluation framework and promote
fairer, more transparent benchmarking for the field

</details>


### [67] [CBM-RAG: Demonstrating Enhanced Interpretability in Radiology Report Generation with Multi-Agent RAG and Concept Bottleneck Models](https://arxiv.org/abs/2504.20898)
*Hasan Md Tusfiqur Alam,Devansh Srivastav,Abdulrahman Mohamed Selim,Md Abdul Kadir,Md Moktadiurl Hoque Shuvo,Daniel Sonntag*

Main category: cs.AI

TL;DR: 本文提出了一种结合概念瓶颈模型（CBM）和多智能体检索增强生成（RAG）系统的自动化放射学报告生成框架，旨在提升AI的可解释性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 尽管生成式AI在放射学工作流自动化方面具有潜力，但其可解释性和可靠性问题阻碍了临床采用。

Method: 通过CBM将胸部X射线特征映射为可理解的临床概念，同时利用RAG系统整合多智能体协作和外部知识生成报告。

Result: 系统能够提供可解释的预测、减少幻觉，并生成高质量、定制化的报告。

Conclusion: 该框架为提升诊断一致性和为放射科医生提供可操作见解提供了途径。

Abstract: Advancements in generative Artificial Intelligence (AI) hold great promise
for automating radiology workflows, yet challenges in interpretability and
reliability hinder clinical adoption. This paper presents an automated
radiology report generation framework that combines Concept Bottleneck Models
(CBMs) with a Multi-Agent Retrieval-Augmented Generation (RAG) system to bridge
AI performance with clinical explainability. CBMs map chest X-ray features to
human-understandable clinical concepts, enabling transparent disease
classification. Meanwhile, the RAG system integrates multi-agent collaboration
and external knowledge to produce contextually rich, evidence-based reports.
Our demonstration showcases the system's ability to deliver interpretable
predictions, mitigate hallucinations, and generate high-quality, tailored
reports with an interactive interface addressing accuracy, trust, and usability
challenges. This framework provides a pathway to improving diagnostic
consistency and empowering radiologists with actionable insights.

</details>


### [68] [Leveraging Generative AI Through Prompt Engineering and Rigorous Validation to Create Comprehensive Synthetic Datasets for AI Training in Healthcare](https://arxiv.org/abs/2504.20921)
*Polycarp Nalela*

Main category: cs.AI

TL;DR: 利用GPT-4 API生成高质量合成医疗数据，以解决隐私问题，并通过多种验证技术确保数据质量。


<details>
  <summary>Details</summary>
Motivation: 医疗数据因隐私问题难以获取，限制了AI算法的训练。

Method: 使用GPT-4 API生成合成数据，并通过BERT、GPT-2、RoBERTa等模型验证数据质量。

Result: 成功生成并验证了高质量的合成医疗数据，并集成到PostgreSQL数据库中。

Conclusion: 生成式AI模型结合严格验证可有效解决医疗数据隐私问题，支持AI算法训练。

Abstract: Access to high-quality medical data is often restricted due to privacy
concerns, posing significant challenges for training artificial intelligence
(AI) algorithms within Electronic Health Record (EHR) applications. In this
study, prompt engineering with the GPT-4 API was employed to generate
high-quality synthetic datasets aimed at overcoming this limitation. The
generated data encompassed a comprehensive array of patient admission
information, including healthcare provider details, hospital departments,
wards, bed assignments, patient demographics, emergency contacts, vital signs,
immunizations, allergies, medical histories, appointments, hospital visits,
laboratory tests, diagnoses, treatment plans, medications, clinical notes,
visit logs, discharge summaries, and referrals. To ensure data quality and
integrity, advanced validation techniques were implemented utilizing models
such as BERT's Next Sentence Prediction for sentence coherence, GPT-2 for
overall plausibility, RoBERTa for logical consistency, autoencoders for anomaly
detection, and conducted diversity analysis. Synthetic data that met all
validation criteria were integrated into a comprehensive PostgreSQL database,
serving as the data management system for the EHR application. This approach
demonstrates that leveraging generative AI models with rigorous validation can
effectively produce high-quality synthetic medical data, facilitating the
training of AI algorithms while addressing privacy concerns associated with
real patient data.

</details>


### [69] [A Domain-Agnostic Scalable AI Safety Ensuring Framework](https://arxiv.org/abs/2504.20924)
*Beomjun Kim,Kangyeon Kim,Sunwoo Kim,Heejin Ahn*

Main category: cs.AI

TL;DR: 提出了一种新型AI安全框架，确保AI系统满足用户定义的约束条件、概率阈值，并适用于多领域。


<details>
  <summary>Details</summary>
Motivation: 当前AI安全方法局限于特定领域，缺乏泛化能力，需一种更通用的解决方案。

Method: 结合AI组件与优化问题，使用内部测试数据和保守测试方法，提出损失函数近似及梯度计算。

Result: 数学证明在特定条件下概率约束满足，实验证明方法在多个领域有效，性能优于现有方法。

Conclusion: 该框架能有效保障AI系统安全，适用于多样化需求，性能显著提升。

Abstract: Ensuring the safety of AI systems has recently emerged as a critical priority
for real-world deployment, particularly in physical AI applications. Current
approaches to AI safety typically address predefined domain-specific safety
conditions, limiting their ability to generalize across contexts.
  We propose a novel AI safety framework that ensures AI systems comply with
\textbf{any user-defined constraint}, with \textbf{any desired probability},
and across \textbf{various domains}.
  In this framework, we combine an AI component (e.g., neural network) with an
optimization problem to produce responses that minimize objectives while
satisfying user-defined constraints with probabilities exceeding user-defined
thresholds. For credibility assessment of the AI component, we propose
\textit{internal test data}, a supplementary set of safety-labeled data, and a
\textit{conservative testing} methodology that provides statistical validity of
using internal test data. We also present an approximation method of a loss
function and how to compute its gradient for training.
  We mathematically prove that probabilistic constraint satisfaction is
guaranteed under specific, mild conditions and prove a scaling law between
safety and the number of internal test data. We demonstrate our framework's
effectiveness through experiments in diverse domains: demand prediction for
production decision, safe reinforcement learning within the SafetyGym
simulator, and guarding AI chatbot outputs. Through these experiments, we
demonstrate that our method guarantees safety for user-specified constraints,
outperforms {for \textbf{up to several order of magnitudes}} existing methods
in low safety threshold regions, and scales effectively with respect to the
size of internal test data.

</details>


### [70] [ChestX-Reasoner: Advancing Radiology Foundation Models with Reasoning through Step-by-Step Verification](https://arxiv.org/abs/2504.20930)
*Ziqing Fan,Cheng Liang,Chaoyi Wu,Ya Zhang,Yanfeng Wang,Weidi Xie*

Main category: cs.AI

TL;DR: ChestX-Reasoner是一种放射学诊断多模态大语言模型，通过临床报告中的结构化推理过程提升性能，显著优于现有医学和通用领域模型。


<details>
  <summary>Details</summary>
Motivation: 医学AI模型常忽略临床实践中的结构化推理过程，ChestX-Reasoner旨在填补这一空白。

Method: 利用临床报告构建大规模数据集，采用两阶段训练框架（监督微调和强化学习），并引入RadRBench-CXR基准和RadRScore评估指标。

Result: 在诊断准确性和推理能力上均优于现有模型，推理能力提升16%、5.9%和18%，结果准确性提升3.3%、24%和27%。

Conclusion: ChestX-Reasoner通过临床推理过程的监督显著提升了性能，所有资源已开源以促进医学推理MLLM的进一步研究。

Abstract: Recent advances in reasoning-enhanced large language models (LLMs) and
multimodal LLMs (MLLMs) have significantly improved performance in complex
tasks, yet medical AI models often overlook the structured reasoning processes
inherent in clinical practice. In this work, we present ChestX-Reasoner, a
radiology diagnosis MLLM designed to leverage process supervision mined
directly from clinical reports, reflecting the step-by-step reasoning followed
by radiologists. We construct a large dataset by extracting and refining
reasoning chains from routine radiology reports. Our two-stage training
framework combines supervised fine-tuning and reinforcement learning guided by
process rewards to better align model reasoning with clinical standards. We
introduce RadRBench-CXR, a comprehensive benchmark featuring 59K visual
question answering samples with 301K clinically validated reasoning steps, and
propose RadRScore, a metric evaluating reasoning factuality, completeness, and
effectiveness. ChestX-Reasoner outperforms existing medical and general-domain
MLLMs in both diagnostic accuracy and reasoning ability, achieving 16%, 5.9%,
and 18% improvements in reasoning ability compared to the best medical MLLM,
the best general MLLM, and its base model, respectively, as well as 3.3%, 24%,
and 27% improvements in outcome accuracy. All resources are open-sourced to
facilitate further research in medical reasoning MLLMs.

</details>


### [71] [Jekyll-and-Hyde Tipping Point in an AI's Behavior](https://arxiv.org/abs/2504.20980)
*Neil F. Johnson,Frank Yingjie Huo*

Main category: cs.AI

TL;DR: 论文提出了一个基于基本原理的精确公式，用于预测和解释LLM（如ChatGPT）输出何时会突然变得错误、误导、无关或危险，并提供了预防措施。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏科学方法预测或解释LLM输出的突然变化，导致公众对AI的信任度下降，甚至引发对LLM的过度礼貌行为。

Method: 从基本原理推导出一个精确公式，仅需中学数学知识，揭示了AI注意力分散导致突然崩溃的机制。

Result: 该公式能够定量预测如何通过改变提示或训练延迟或防止临界点，为政策制定者和公众提供讨论AI风险和用途的平台。

Conclusion: 研究为LLM的透明性和安全性提供了科学基础，回答了诸如‘是否应对LLM礼貌’等实际问题。

Abstract: Trust in AI is undermined by the fact that there is no science that predicts
-- or that can explain to the public -- when an LLM's output (e.g. ChatGPT) is
likely to tip mid-response to become wrong, misleading, irrelevant or
dangerous. With deaths and trauma already being blamed on LLMs, this
uncertainty is even pushing people to treat their 'pet' LLM more politely to
'dissuade' it (or its future Artificial General Intelligence offspring) from
suddenly turning on them. Here we address this acute need by deriving from
first principles an exact formula for when a Jekyll-and-Hyde tipping point
occurs at LLMs' most basic level. Requiring only secondary school mathematics,
it shows the cause to be the AI's attention spreading so thin it suddenly
snaps. This exact formula provides quantitative predictions for how the
tipping-point can be delayed or prevented by changing the prompt and the AI's
training. Tailored generalizations will provide policymakers and the public
with a firm platform for discussing any of AI's broader uses and risks, e.g. as
a personal counselor, medical advisor, decision-maker for when to use force in
a conflict situation. It also meets the need for clear and transparent answers
to questions like ''should I be polite to my LLM?''

</details>


### [72] [LTLf Adaptive Synthesis for Multi-Tier Goals in Nondeterministic Domains](https://arxiv.org/abs/2504.20983)
*Giuseppe De Giacomo,Gianmarco Parretti,Shufang Zhu*

Main category: cs.AI

TL;DR: 该论文研究了一种LTLf合成的变体，用于在非确定性规划领域中为多层级目标生成自适应策略。这些策略动态调整以最大化满足目标，并利用环境合作。


<details>
  <summary>Details</summary>
Motivation: 解决在非确定性环境中如何动态调整策略以最大化满足多层级目标的问题，同时利用环境的潜在合作。

Method: 提出了一种基于博弈论的技术，用于计算自适应策略，该技术是多项式时间复杂度的（二次方）。

Result: 该方法在目标数量上是多项式时间复杂度的，且相比标准LTLf合成仅增加少量开销。

Conclusion: 该技术为处理多层级目标提供了一种高效且动态的自适应策略生成方法。

Abstract: We study a variant of LTLf synthesis that synthesizes adaptive strategies for
achieving a multi-tier goal, consisting of multiple increasingly challenging
LTLf objectives in nondeterministic planning domains. Adaptive strategies are
strategies that at any point of their execution (i) enforce the satisfaction of
as many objectives as possible in the multi-tier goal, and (ii) exploit
possible cooperation from the environment to satisfy as many as possible of the
remaining ones. This happens dynamically: if the environment cooperates (ii)
and an objective becomes enforceable (i), then our strategies will enforce it.
We provide a game-theoretic technique to compute adaptive strategies that is
sound and complete. Notably, our technique is polynomial, in fact quadratic, in
the number of objectives. In other words, it handles multi-tier goals with only
a minor overhead compared to standard LTLf synthesis.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [73] [A constraints-based approach to fully interpretable neural networks for detecting learner behaviors](https://arxiv.org/abs/2504.20055)
*Juan D. Pinto,Luc Paquette*

Main category: cs.LG

TL;DR: 论文提出了一种可解释性设计的行为检测模型，通过约束神经网络实现完全可解释性，并验证了其在检测作弊行为中的有效性。


<details>
  <summary>Details</summary>
Motivation: 随着复杂机器学习模型在教育中的应用增加，其可解释性问题引发关注，需要开发既忠实于模型内部机制又易于理解的解释技术。

Method: 设计了一种基于神经网络的行为检测模型，通过引入约束简化推理过程并贴近人类对任务的理解，实现完全可解释性。

Result: 模型成功学习了作弊行为的模式，并能提供忠实且可理解的解释，其学习模式与人类专家的判断一致。

Conclusion: 该方法为可解释性模型设计提供了新思路，并建议通过人类验证评估解释性。

Abstract: The increasing use of complex machine learning models in education has led to
concerns about their interpretability, which in turn has spurred interest in
developing explainability techniques that are both faithful to the model's
inner workings and intelligible to human end-users. In this paper, we describe
a novel approach to creating a neural-network-based behavior detection model
that is interpretable by design. Our model is fully interpretable, meaning that
the parameters we extract for our explanations have a clear interpretation,
fully capture the model's learned knowledge about the learner behavior of
interest, and can be used to create explanations that are both faithful and
intelligible. We achieve this by implementing a series of constraints to the
model that both simplify its inference process and bring it closer to a human
conception of the task at hand. We train the model to detect gaming-the-system
behavior, evaluate its performance on this task, and compare its learned
patterns to those identified by human experts. Our results show that the model
is successfully able to learn patterns indicative of gaming-the-system behavior
while providing evidence for fully interpretable explanations. We discuss the
implications of our approach and suggest ways to evaluate explainability using
a human-grounded approach.

</details>


### [74] [A Simple Review of EEG Foundation Models: Datasets, Advancements and Future Perspectives](https://arxiv.org/abs/2504.20069)
*Junhong Lai,Jiyu Wei,Lin Yao,Yueming Wang*

Main category: cs.LG

TL;DR: 本文综述了EEG基础模型（EEG-FMs）的最新发展，包括其架构、预训练策略及数据集，并探讨了该领域的挑战与未来方向。


<details>
  <summary>Details</summary>
Motivation: EEG信号在理解大脑活动和诊断神经疾病中至关重要，EEG-FMs为EEG数据处理提供了新方法。

Method: 综述了多种EEG-FMs的架构、预训练策略及数据集。

Result: EEG-FMs在EEG数据分析中展现出巨大潜力。

Conclusion: 本文为EEG分析和相关EEG-FMs的研究者与实践者提供了全面概述，并指出了未来研究方向。

Abstract: Electroencephalogram (EEG) signals play a crucial role in understanding brain
activity and diagnosing neurological disorders. This review focuses on the
recent development of EEG foundation models(EEG-FMs), which have shown great
potential in processing and analyzing EEG data. We discuss various EEG-FMs,
including their architectures, pre-training strategies, their pre-training and
downstream datasets and other details. The review also highlights the
challenges and future directions in this field, aiming to provide a
comprehensive overview for researchers and practitioners interested in EEG
analysis and related EEG-FMs.

</details>


### [75] [Improving Deep Knowledge Tracing via Gated Architectures and Adaptive Optimization](https://arxiv.org/abs/2504.20070)
*Altun Shukurlu*

Main category: cs.LG

TL;DR: 论文改进了深度知识追踪（DKT）模型，使用LSTM和GRU增强性能，并基于PyTorch重新实现，优化了训练效率和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 原始DKT模型基于标准RNN和Lua Torch框架，限制了可扩展性和可复现性。

Method: 采用LSTM和GRU改进模型，并使用PyTorch重新实现，对比了多种优化算法（SGD、RMSProp、Adagrad、Adam、AdamW）。

Result: 在Synthetic-5和Khan Academy数据集上，LSTM和GRU表现优于基础RNN，自适应优化器（如Adam和AdamW）在收敛速度和预测准确性上更优。

Conclusion: 开源PyTorch实现为神经知识追踪和个性化学习系统提供了可复现和可扩展的基础。

Abstract: Deep Knowledge Tracing (DKT) models student learning behavior by using
Recurrent Neural Networks (RNNs) to predict future performance based on
historical interaction data. However, the original implementation relied on
standard RNNs in the Lua-based Torch framework, which limited extensibility and
reproducibility. In this work, we revisit the DKT model from two perspectives:
architectural improvements and optimization efficiency. First, we enhance the
model using gated recurrent units, specifically Long Short-Term Memory (LSTM)
networks and Gated Recurrent Units (GRU), which better capture long-term
dependencies and help mitigate vanishing gradient issues. Second, we
re-implement DKT using the PyTorch framework, enabling a modular and accessible
infrastructure compatible with modern deep learning workflows. We also
benchmark several optimization algorithms SGD, RMSProp, Adagrad, Adam, and
AdamW to evaluate their impact on convergence speed and predictive accuracy in
educational modeling tasks. Experiments on the Synthetic-5 and Khan Academy
datasets show that GRUs and LSTMs achieve higher accuracy and improved training
stability compared to basic RNNs, while adaptive optimizers such as Adam and
AdamW consistently outperform SGD in both early-stage learning and final model
performance. Our open-source PyTorch implementation provides a reproducible and
extensible foundation for future research in neural knowledge tracing and
personalized learning systems.

</details>


### [76] [RAGEN: Understanding Self-Evolution in LLM Agents via Multi-Turn Reinforcement Learning](https://arxiv.org/abs/2504.20073)
*Zihan Wang,Kangrui Wang,Qineng Wang,Pingyue Zhang,Linjie Li,Zhengyuan Yang,Kefan Yu,Minh Nhat Nguyen,Licheng Liu,Eli Gottlieb,Monica Lam,Yiping Lu,Kyunghyun Cho,Jiajun Wu,Li Fei-Fei,Lijuan Wang,Yejin Choi,Manling Li*

Main category: cs.LG

TL;DR: 论文提出StarPO框架和RAGEN系统，用于训练LLM交互代理，发现Echo Trap问题并提出解决方案StarPO-S，同时探讨了RL训练中的初始状态多样性和奖励信号对推理的影响。


<details>
  <summary>Details</summary>
Motivation: 训练大型语言模型（LLM）作为交互代理面临长期决策和随机环境反馈的挑战，多轮代理强化学习（RL）研究较少。

Method: 提出StarPO框架和RAGEN系统，通过轨迹级RL训练代理，并引入StarPO-S解决Echo Trap问题。

Result: 发现Echo Trap模式，提出改进方法；RL训练中初始状态多样性和奖励信号对推理能力至关重要。

Conclusion: StarPO和RAGEN为LLM代理RL训练提供有效框架，强调奖励信号和初始状态设计的重要性。

Abstract: Training large language models (LLMs) as interactive agents presents unique
challenges including long-horizon decision making and interacting with
stochastic environment feedback. While reinforcement learning (RL) has enabled
progress in static tasks, multi-turn agent RL training remains underexplored.
We propose StarPO (State-Thinking-Actions-Reward Policy Optimization), a
general framework for trajectory-level agent RL, and introduce RAGEN, a modular
system for training and evaluating LLM agents. Our study on three stylized
environments reveals three core findings. First, our agent RL training shows a
recurring mode of Echo Trap where reward variance cliffs and gradient spikes;
we address this with StarPO-S, a stabilized variant with trajectory filtering,
critic incorporation, and decoupled clipping. Second, we find the shaping of RL
rollouts would benefit from diverse initial states, medium interaction
granularity and more frequent sampling. Third, we show that without
fine-grained, reasoning-aware reward signals, agent reasoning hardly emerge
through multi-turn RL and they may show shallow strategies or hallucinated
thoughts. Code and environments are available at
https://github.com/RAGEN-AI/RAGEN.

</details>


### [77] [Low-Rank Matrix Approximation for Neural Network Compression](https://arxiv.org/abs/2504.20078)
*Kalyan Cherukuri,Aarav Lala*

Main category: cs.LG

TL;DR: 论文提出了一种自适应秩奇异值分解（ARSVD）方法，动态调整全连接层的秩以优化能量消耗，同时保持模型精度。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络（DNNs）常受限于内存和计算资源，传统固定秩压缩方法无法平衡压缩与性能。

Method: 通过能量分布动态选择每层的秩，训练MLP模型并在MNIST、CIFAR-10和CIFAR-100数据集上评估。

Result: ARSVD实现了显著的模型压缩且不影响分类精度。

Conclusion: ARSVD在资源受限的计算场景中具有实用价值。

Abstract: Deep Neural Networks (DNNs) are often constrained by their large memories and
computational restrictions. In this paper, we introduce a novel adaptive-rank
Singular Value Decomposition (ARSVD) that dynamically chooses the rank increase
of the fully connected layers below a certain threshold in energy expenditure.
Unlike conventional SVD compression methods that apply a fixed rank reduction
in all layers, our ARSVD method uses energy distribution to adaptively select
rank per layer while retaining accuracy. This is done for each layer in an
effort to use as much energy as possible while maintaining the lowest accuracy
loss. Such accuracy-adaptive approaches outperform traditional static rank
reduction methods by providing an improved balance between compression and
model performance. We first train a simple Multi-Layer Perceptron (MLP) on the
MNIST, CIFAR-10, and CIFAR-100 dataset and evaluate its performance using
accuracy and F1-score. After applying ARSVD, our results demonstrate that the
technique can achieve substantial model compression without compromising
classification accuracy. These results illustrate the usefulness of ARSVD in
computing scenarios where both computational and memory resources are scarce.

</details>


### [78] [FX-DARTS: Designing Topology-unconstrained Architectures with Differentiable Architecture Search and Entropy-based Super-network Shrinking](https://arxiv.org/abs/2504.20079)
*Xuan Rao,Bo Zhao,Derong Liu,Cesare Alippi*

Main category: cs.LG

TL;DR: FX-DARTS通过消除DARTS中的强先验约束，提出了一种基于熵的超网络收缩框架，以在更大的搜索空间中稳定搜索，并发现性能与计算复杂度平衡的神经网络架构。


<details>
  <summary>Details</summary>
Motivation: DARTS中的强先验约束限制了Auto-ML的发展和神经网络架构的灵活性，阻碍了更强大网络的探索。

Method: 提出FX-DARTS方法，通过消除单元拓扑限制和改进超网络的离散化机制，利用基于熵的超网络收缩框架（ESS）解决约束消除带来的挑战。

Result: 实验表明，FX-DARTS能在单次搜索过程中发现性能与计算复杂度平衡的神经网络架构。

Conclusion: FX-DARTS通过减少先验约束，提升了架构搜索的灵活性和稳定性，为Auto-ML提供了新的可能性。

Abstract: Strong priors are imposed on the search space of Differentiable Architecture
Search (DARTS), such that cells of the same type share the same topological
structure and each intermediate node retains two operators from distinct nodes.
While these priors reduce optimization difficulties and improve the
applicability of searched architectures, they hinder the subsequent development
of automated machine learning (Auto-ML) and prevent the optimization algorithm
from exploring more powerful neural networks through improved architectural
flexibility. This paper aims to reduce these prior constraints by eliminating
restrictions on cell topology and modifying the discretization mechanism for
super-networks. Specifically, the Flexible DARTS (FX-DARTS) method, which
leverages an Entropy-based Super-Network Shrinking (ESS) framework, is
presented to address the challenges arising from the elimination of prior
constraints. Notably, FX-DARTS enables the derivation of neural architectures
without strict prior rules while maintaining the stability in the enlarged
search space. Experimental results on image classification benchmarks
demonstrate that FX-DARTS is capable of exploring a set of neural architectures
with competitive trade-offs between performance and computational complexity
within a single search procedure.

</details>


### [79] [DNAD: Differentiable Neural Architecture Distillation](https://arxiv.org/abs/2504.20080)
*Xuan Rao,Bo Zhao,Derong Liu*

Main category: cs.LG

TL;DR: DNAD算法通过结合搜索删除和模仿搜索，设计高效神经网络，平衡性能和计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 设计高效神经网络，平衡模型性能和计算复杂度。

Method: 开发SNPS算法（基于DARTS框架的搜索删除）和DNAD算法（结合SNPS与知识蒸馏的模仿搜索）。

Result: 在CIFAR-10和ImageNet上，DNAD和SNPS均能生成性能优异且参数更少的架构。DNAD在ImageNet上达到23.7%的top-1错误率。

Conclusion: DNAD和SNPS能有效生成高性能、低复杂度的神经网络架构，优于传统DARTS方法。

Abstract: To meet the demand for designing efficient neural networks with appropriate
trade-offs between model performance (e.g., classification accuracy) and
computational complexity, the differentiable neural architecture distillation
(DNAD) algorithm is developed based on two cores, namely search by deleting and
search by imitating. Primarily, to derive neural architectures in a space where
cells of the same type no longer share the same topology, the super-network
progressive shrinking (SNPS) algorithm is developed based on the framework of
differentiable architecture search (DARTS), i.e., search by deleting. Unlike
conventional DARTS-based approaches which yield neural architectures with
simple structures and derive only one architecture during the search procedure,
SNPS is able to derive a Pareto-optimal set of architectures with flexible
structures by forcing the dynamic super-network shrink from a dense structure
to a sparse one progressively. Furthermore, since knowledge distillation (KD)
has shown great effectiveness to train a compact network with the assistance of
an over-parameterized model, we integrate SNPS with KD to formulate the DNAD
algorithm, i.e., search by imitating. By minimizing behavioral differences
between the super-network and teacher network, the over-fitting of one-level
DARTS is avoided and well-performed neural architectures are derived.
Experiments on CIFAR-10 and ImageNet classification tasks demonstrate that both
SNPS and DNAD are able to derive a set of architectures which achieve similar
or lower error rates with fewer parameters and FLOPs. Particularly, DNAD
achieves the top-1 error rate of 23.7% on ImageNet classification with a model
of 6.0M parameters and 598M FLOPs, which outperforms most DARTS-based methods.

</details>


### [80] [Towards Practical Second-Order Optimizers in Deep Learning: Insights from Fisher Information Analysis](https://arxiv.org/abs/2504.20096)
*Damien Martins Gomes*

Main category: cs.LG

TL;DR: AdaFisher是一种新型自适应二阶优化器，通过近似Fisher信息矩阵来预条件梯度，旨在平衡二阶方法的收敛性和DNN训练的计算效率。


<details>
  <summary>Details</summary>
Motivation: 尽管一阶优化方法（如Adam）在DNN训练中广泛使用，但二阶方法通常具有更好的收敛性，但其高计算成本限制了实用性。AdaFisher试图解决这一矛盾。

Method: AdaFisher利用Fisher信息矩阵的对角块Kronecker近似来自适应预条件梯度。

Result: AdaFisher在图像分类和语言建模任务中表现出色，稳定且鲁棒，且在准确性和收敛速度上优于现有优化器。

Conclusion: AdaFisher成功地将二阶方法的优势与计算效率结合，为DNN训练提供了一种高效且性能优越的优化器。

Abstract: First-order optimization methods remain the standard for training deep neural
networks (DNNs). Optimizers like Adam incorporate limited curvature information
by preconditioning the stochastic gradient with a diagonal matrix. Despite the
widespread adoption of first-order methods, second-order optimization
algorithms often exhibit superior convergence compared to methods like Adam and
SGD. However, their practicality in training DNNs is still limited by a
significantly higher per-iteration computational cost compared to first-order
methods. In this thesis, we present AdaFisher, a novel adaptive second-order
optimizer that leverages a diagonal block-Kronecker approximation of the Fisher
information matrix to adaptively precondition gradients. AdaFisher aims to
bridge the gap between the improved convergence and generalization of
second-order methods and the computational efficiency needed for training DNNs.
Despite the traditionally slower speed of second-order optimizers, AdaFisher is
effective for tasks such as image classification and language modeling,
exhibiting remarkable stability and robustness during hyperparameter tuning. We
demonstrate that AdaFisher outperforms state-of-the-art optimizers in both
accuracy and convergence speed. The code is available from
https://github.com/AtlasAnalyticsLab/AdaFisher.

</details>


### [81] [Decoding Latent Spaces: Assessing the Interpretability of Time Series Foundation Models for Visual Analytics](https://arxiv.org/abs/2504.20099)
*Inmaculada Santamaria-Valenzuela,Victor Rodriguez-Fernandez,Javier Huertas-Tato,Jong Hyuk Park,David Camacho*

Main category: cs.LG

TL;DR: 研究探讨了时间序列基础模型（如MOMENT）潜在空间的可解释性，验证了微调对嵌入空间清晰度的提升效果，发现性能有显著改进但可解释性提升有限。


<details>
  <summary>Details</summary>
Motivation: 探索时间序列基础模型潜在空间的可解释性，以支持视觉分析任务。

Method: 评估MOMENT系列模型在五个数据集上的表现，分析微调对潜在空间结构和嵌入清晰度的影响。

Result: 微调显著减少了损失，但潜在空间的可解释性提升有限，需进一步优化方法。

Conclusion: 时间序列基础模型（如MOMENT）虽强大，但其潜在空间需额外方法改进（如投影技术、损失函数或数据预处理）以提升可解释性，同时为交互式视觉分析提供了效率优势。

Abstract: The present study explores the interpretability of latent spaces produced by
time series foundation models, focusing on their potential for visual analysis
tasks. Specifically, we evaluate the MOMENT family of models, a set of
transformer-based, pre-trained architectures for multivariate time series tasks
such as: imputation, prediction, classification, and anomaly detection. We
evaluate the capacity of these models on five datasets to capture the
underlying structures in time series data within their latent space projection
and validate whether fine tuning improves the clarity of the resulting
embedding spaces. Notable performance improvements in terms of loss reduction
were observed after fine tuning. Visual analysis shows limited improvement in
the interpretability of the embeddings, requiring further work. Results suggest
that, although Time Series Foundation Models such as MOMENT are robust, their
latent spaces may require additional methodological refinements to be
adequately interpreted, such as alternative projection techniques, loss
functions, or data preprocessing strategies. Despite the limitations of MOMENT,
foundation models supose a big reduction in execution time and so a great
advance for interactive visual analytics.

</details>


### [82] [HyboWaveNet: Hyperbolic Graph Neural Networks with Multi-Scale Wavelet Transform for Protein-Protein Interaction Prediction](https://arxiv.org/abs/2504.20102)
*Qingzhi Yu,Shuai Yan,Wenfeng Dai,Xiang Cheng*

Main category: cs.LG

TL;DR: HyboWaveNet结合双曲图神经网络和多尺度图小波变换，提出了一种新的深度学习框架，用于蛋白质相互作用预测，解决了现有方法的黑盒问题和多尺度动态交互模式捕捉问题。


<details>
  <summary>Details</summary>
Motivation: 现有神经网络和机器学习方法在蛋白质相互作用预测中缺乏因果解释，且难以捕捉多尺度动态交互模式。

Method: HyboWaveNet结合双曲图神经网络（HGNNs）和多尺度图小波变换，将蛋白质特征映射到Lorentz空间，模拟生物分子的层次拓扑关系，并通过对比学习和多尺度特征提取预测PPI。

Result: 在公开数据集上，HyboWaveNet优于现有最先进方法，且多尺度图小波变换模块提升了预测性能和泛化能力。

Conclusion: 该研究将几何深度学习和信号处理结合，为分析复杂生物系统提供了新方法。

Abstract: Protein-protein interactions (PPIs) are fundamental for deciphering cellular
functions,disease pathways,and drug discovery.Although existing neural networks
and machine learning methods have achieved high accuracy in PPI
prediction,their black-box nature leads to a lack of causal interpretation of
the prediction results and difficulty in capturing hierarchical geometries and
multi-scale dynamic interaction patterns among proteins.To address these
challenges, we propose HyboWaveNet,a novel deep learning framework that
collaborates with hyperbolic graphical neural networks (HGNNs) and multiscale
graphical wavelet transform for robust PPI prediction. Mapping protein features
to Lorentz space simulates hierarchical topological relationships among
biomolecules via a hyperbolic distance metric,enabling node feature
representations that better fit biological a priori.HyboWaveNet inherently
simulates hierarchical and scale-free biological relationships, while the
integration of wavelet transforms enables adaptive extraction of local and
global interaction features across different resolutions. Our framework
generates node feature representations via a graph neural network under the
Lorenz model and generates pairs of positive samples under multiple different
views for comparative learning, followed by further feature extraction via
multi-scale graph wavelet transforms to predict potential PPIs. Experiments on
public datasets show that HyboWaveNet improves over both existing
state-of-the-art methods. We also demonstrate through ablation experimental
studies that the multi-scale graph wavelet transform module improves the
predictive performance and generalization ability of HyboWaveNet. This work
links geometric deep learning and signal processing to advance PPI prediction,
providing a principled approach for analyzing complex biological systems

</details>


### [83] [Adaptive Helpfulness-Harmlessness Alignment with Preference Vectors](https://arxiv.org/abs/2504.20106)
*Ren-Wei Liang,Chin-Ting Hsu,Chan-Hung Yu,Saransh Agrawal,Shih-Cheng Huang,Shang-Tse Chen,Kuan-Hao Huang,Shao-Hua Sun*

Main category: cs.LG

TL;DR: 论文提出了一种名为Preference Vector的新框架，通过分离训练和动态合并偏好向量，解决了现有方法在平衡语言模型的有用性和无害性时的性能冲突和可控性问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如RLHF和DPO）在平衡语言模型的有用性和无害性时存在性能冲突、可控性差和扩展性不足的问题，需要一种更灵活和可扩展的解决方案。

Method: 提出Preference Vector框架，通过单独训练不同偏好的模型，提取行为偏移作为偏好向量，并在测试时动态合并，实现细粒度的用户可控调整和新偏好的无缝集成。

Result: 实验表明，Preference Vector框架在不增加过度保守性的情况下提高了有用性，支持平滑的偏好权衡控制，并实现了可扩展的多偏好对齐。

Conclusion: Preference Vector框架为解决语言模型偏好对齐问题提供了一种模块化、可控性强且易于扩展的新方法。

Abstract: Ensuring that large language models (LLMs) are both helpful and harmless is a
critical challenge, as overly strict constraints can lead to excessive
refusals, while permissive models risk generating harmful content. Existing
approaches, such as reinforcement learning from human feedback (RLHF) and
direct preference optimization (DPO), attempt to balance these trade-offs but
suffer from performance conflicts, limited controllability, and poor
extendability. To address these issues, we propose Preference Vector, a novel
framework inspired by task arithmetic. Instead of optimizing multiple
preferences within a single objective, we train separate models on individual
preferences, extract behavior shifts as preference vectors, and dynamically
merge them at test time. This modular approach enables fine-grained,
user-controllable preference adjustments and facilitates seamless integration
of new preferences without retraining. Experiments show that our proposed
Preference Vector framework improves helpfulness without excessive
conservatism, allows smooth control over preference trade-offs, and supports
scalable multi-preference alignment.

</details>


### [84] [Swapped Logit Distillation via Bi-level Teacher Alignment](https://arxiv.org/abs/2504.20108)
*Stephen Ekaputra Limantoro,Jhe-Hao Lin,Chih-Yu Wang,Yi-Lung Tsai,Hong-Han Shuai,Ching-Chun Huang,Wen-Huang Cheng*

Main category: cs.LG

TL;DR: 提出了一种基于对数交换的知识蒸馏方法（SLD），通过交换教师和学生网络的输出对数，解决了传统知识蒸馏中错误预测的问题，并在图像分类任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统知识蒸馏方法直接将教师网络的知识传递给学生网络，可能导致错误预测。SLD旨在通过交换对数处理解决这一问题。

Method: 提出交换对数处理方案，将教师和学生输出转化为两个教师，并引入损失调度以优化两者对齐。

Result: 在图像分类任务中，SLD表现优于现有最先进方法。

Conclusion: SLD通过交换对数处理和损失调度，有效提升了知识蒸馏的性能。

Abstract: Knowledge distillation (KD) compresses the network capacity by transferring
knowledge from a large (teacher) network to a smaller one (student). It has
been mainstream that the teacher directly transfers knowledge to the student
with its original distribution, which can possibly lead to incorrect
predictions. In this article, we propose a logit-based distillation via swapped
logit processing, namely Swapped Logit Distillation (SLD). SLD is proposed
under two assumptions: (1) the wrong prediction occurs when the prediction
label confidence is not the maximum; (2) the "natural" limit of probability
remains uncertain as the best value addition to the target cannot be
determined. To address these issues, we propose a swapped logit processing
scheme. Through this approach, we find that the swap method can be effectively
extended to teacher and student outputs, transforming into two teachers. We
further introduce loss scheduling to boost the performance of two teachers'
alignment. Extensive experiments on image classification tasks demonstrate that
SLD consistently performs best among previous state-of-the-art methods.

</details>


### [85] [Attention to Detail: Fine-Scale Feature Preservation-Oriented Geometric Pre-training for AI-Driven Surrogate Modeling](https://arxiv.org/abs/2504.20110)
*Yu-hsuan Chen,Jing Bi,Cyril Ngo Ngoc,Victor Oancea,Jonathan Cagan,Levent Burak Kara*

Main category: cs.LG

TL;DR: 本文提出了一种自监督几何表示学习方法，用于从非参数3D模型中捕捉精细几何特征，解决了传统方法在保留精细几何细节方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 由于标记的CAD到仿真数据稀缺，传统端到端代理模型在保留精细几何细节方面效果有限，因此需要一种新的方法。

Method: 提出了一种自监督几何表示学习方法，通过解耦几何特征提取与下游物理任务，利用几何重建损失学习潜在空间嵌入，并采用近零水平采样和批量自适应注意力加权损失函数。

Result: 在结构力学案例中验证了该方法在捕捉设计特征和实现准确少样本物理预测方面的优异表现。

Conclusion: 该方法在几何与物理表示之间架起了桥梁，为数据稀缺场景下的代理建模提供了有效解决方案。

Abstract: AI-driven surrogate modeling has become an increasingly effective alternative
to physics-based simulations for 3D design, analysis, and manufacturing. These
models leverage data-driven methods to predict physical quantities
traditionally requiring computationally expensive simulations. However, the
scarcity of labeled CAD-to-simulation datasets has driven recent advancements
in self-supervised and foundation models, where geometric representation
learning is performed offline and later fine-tuned for specific downstream
tasks. While these approaches have shown promise, their effectiveness is
limited in applications requiring fine-scale geometric detail preservation.
This work introduces a self-supervised geometric representation learning method
designed to capture fine-scale geometric features from non-parametric 3D
models. Unlike traditional end-to-end surrogate models, this approach decouples
geometric feature extraction from downstream physics tasks, learning a latent
space embedding guided by geometric reconstruction losses. Key elements include
the essential use of near-zero level sampling and the innovative batch-adaptive
attention-weighted loss function, which enhance the encoding of intricate
design features. The proposed method is validated through case studies in
structural mechanics, demonstrating strong performance in capturing design
features and enabling accurate few-shot physics predictions. Comparisons with
traditional parametric surrogate modeling highlight its potential to bridge the
gap between geometric and physics-based representations, providing an effective
solution for surrogate modeling in data-scarce scenarios.

</details>


### [86] [Supervised Pretraining for Material Property Prediction](https://arxiv.org/abs/2504.20112)
*Chowdhury Mohammad Abid Rahman,Aldo H. Romero,Prashnna K. Gyawali*

Main category: cs.LG

TL;DR: 论文提出了一种基于监督预训练的方法，利用类别信息作为替代标签，结合图增强技术，显著提升了材料属性预测的性能。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习方法依赖大量标注数据，而自监督学习（SSL）虽能利用未标注数据，但仍有改进空间。本文探索监督预训练在材料属性预测中的应用，以提升模型性能。

Method: 提出监督预训练框架，利用类别信息作为替代标签；引入图增强技术，通过注入噪声提升模型鲁棒性。

Result: 在六项材料属性预测任务中，性能显著提升，平均绝对误差（MAE）降低2%至6.67%，达到新基准。

Conclusion: 本研究首次探索了监督预训练在材料属性预测中的应用，为领域方法学和应用提供了新思路。

Abstract: Accurate prediction of material properties facilitates the discovery of novel
materials with tailored functionalities. Deep learning models have recently
shown superior accuracy and flexibility in capturing structure-property
relationships. However, these models often rely on supervised learning, which
requires large, well-annotated datasets an expensive and time-consuming
process. Self-supervised learning (SSL) offers a promising alternative by
pretraining on large, unlabeled datasets to develop foundation models that can
be fine-tuned for material property prediction. In this work, we propose
supervised pretraining, where available class information serves as surrogate
labels to guide learning, even when downstream tasks involve unrelated material
properties. We evaluate this strategy on two state-of-the-art SSL models and
introduce a novel framework for supervised pretraining. To further enhance
representation learning, we propose a graph-based augmentation technique that
injects noise to improve robustness without structurally deforming material
graphs. The resulting foundation models are fine-tuned for six challenging
material property predictions, achieving significant performance gains over
baselines, ranging from 2% to 6.67% improvement in mean absolute error (MAE)
and establishing a new benchmark in material property prediction. This study
represents the first exploration of supervised pertaining with surrogate labels
in material property prediction, advancing methodology and application in the
field.

</details>


### [87] [Benchmarking Transferability: A Framework for Fair and Robust Evaluation](https://arxiv.org/abs/2504.20121)
*Alireza Kazemi,Helia Rezvani,Mahsa Baktashmotlagh*

Main category: cs.LG

TL;DR: 本文提出了一个系统评估迁移性得分的框架，发现现有评估方法可能无法全面反映不同指标的优劣，并强调了标准化评估协议的重要性。


<details>
  <summary>Details</summary>
Motivation: 尽管已有多种方法用于衡量迁移性，但其可靠性和实用性仍不明确，主要由于实验设置、数据集和假设的差异。

Method: 引入了一个全面的基准测试框架，通过广泛实验评估不同迁移性得分在多样化设置下的表现。

Result: 实验表明不同指标在不同场景下表现不一，当前评估实践可能未能充分反映方法的优缺点。此外，提出的新指标在特定实验设置中实现了3.5%的提升。

Conclusion: 标准化评估协议对提高迁移性得分的可靠性至关重要，有助于跨域应用中更明智的模型选择。

Abstract: Transferability scores aim to quantify how well a model trained on one domain
generalizes to a target domain. Despite numerous methods proposed for measuring
transferability, their reliability and practical usefulness remain
inconclusive, often due to differing experimental setups, datasets, and
assumptions. In this paper, we introduce a comprehensive benchmarking framework
designed to systematically evaluate transferability scores across diverse
settings. Through extensive experiments, we observe variations in how different
metrics perform under various scenarios, suggesting that current evaluation
practices may not fully capture each method's strengths and limitations. Our
findings underscore the value of standardized assessment protocols, paving the
way for more reliable transferability measures and better-informed model
selection in cross-domain applications. Additionally, we achieved a 3.5\%
improvement using our proposed metric for the head-training fine-tuning
experimental setup. Our code is available in this repository:
https://github.com/alizkzm/pert_robust_platform.

</details>


### [88] [LZ Penalty: An information-theoretic repetition penalty for autoregressive language models](https://arxiv.org/abs/2504.20131)
*Antonio A. Ginart,Naveen Kodali,Jason Lee,Caiming Xiong,Silvio Savarese,John R. Emmons*

Main category: cs.LG

TL;DR: LZ惩罚是一种专门用于减少自回归语言模型中退化重复的惩罚方法，基于LZ77无损压缩算法的编码长度，通过预测-压缩对偶性实现。


<details>
  <summary>Details</summary>
Motivation: 解决自回归语言模型中退化重复的问题，同时不损失模型能力。

Method: 基于LZ77算法的编码长度设计LZ惩罚，通过预测-压缩对偶性解码。

Result: LZ惩罚在贪婪解码下有效消除退化重复，优于行业标准的频率惩罚和重复惩罚。

Conclusion: LZ惩罚是一种高效的方法，能在不损失能力的情况下消除退化重复。

Abstract: We introduce the LZ penalty, a penalty specialized for reducing degenerate
repetitions in autoregressive language models without loss of capability. The
penalty is based on the codelengths in the LZ77 universal lossless compression
algorithm. Through the lens of the prediction-compression duality, decoding the
LZ penalty has the interpretation of sampling from the residual distribution
after removing the information that is highly compressible. We demonstrate the
LZ penalty enables state-of-the-art open-source reasoning models to operate
with greedy (temperature zero) decoding without loss of capability and without
instances of degenerate repetition. Both the industry-standard frequency
penalty and repetition penalty are ineffective, incurring degenerate repetition
rates of up to 4%.

</details>


### [89] [Causal Identification in Time Series Models](https://arxiv.org/abs/2504.20172)
*Erik Jahn,Karthik Karnik,Leonard J. Schulman*

Main category: cs.LG

TL;DR: 论文研究了因果识别算法在含隐混杂因子的因果时间序列图上的适用性，提出了一个仅依赖于每时间步变量数量和最大时间滞后的边界。


<details>
  <summary>Details</summary>
Motivation: 由于时间序列图可能无限延伸，传统方法无法确定因果效应的可识别性，尤其是在跨任意时间区间时。

Method: 通过分析时间序列图的固定大小片段，证明了因果识别算法的适用性。

Result: 提出了一个边界，表明仅需考虑有限时间步即可决定因果效应的可识别性。

Conclusion: 证明了因果识别算法在时间序列图上的有效性，即使跨无限时间区间。

Abstract: In this paper, we analyze the applicability of the Causal Identification
algorithm to causal time series graphs with latent confounders. Since these
graphs extend over infinitely many time steps, deciding whether causal effects
across arbitrary time intervals are identifiable appears to require computation
on graph segments of unbounded size. Even for deciding the identifiability of
intervention effects on variables that are close in time, no bound is known on
how many time steps in the past need to be considered. We give a first bound of
this kind that only depends on the number of variables per time step and the
maximum time lag of any direct or latent causal effect. More generally, we show
that applying the Causal Identification algorithm to a constant-size segment of
the time series graph is sufficient to decide identifiability of causal
effects, even across unbounded time intervals.

</details>


### [90] [AI Recommendation Systems for Lane-Changing Using Adherence-Aware Reinforcement Learning](https://arxiv.org/abs/2504.20187)
*Weihao Sun,Heeseung Bang,Andreas A. Malikopoulos*

Main category: cs.LG

TL;DR: 提出了一种基于强化学习的车道变换推荐方法，考虑了人类驾驶员的部分遵从性，以提高半自动驾驶环境中的旅行效率。


<details>
  <summary>Details</summary>
Motivation: 在半自动驾驶环境中，人类驾驶员对推荐动作的部分遵从性影响了车道变换的效率，因此需要一种能够考虑这种遵从性的方法。

Method: 采用马尔可夫决策过程框架，使用基于深度Q网络的强化学习方法，结合人类驾驶员的部分遵从性。

Result: 在CARLA驾驶环境中进行了评估，验证了方法的有效性。

Conclusion: 该方法能够有效提升半自动驾驶环境中的车道变换效率。

Abstract: In this paper, we present an adherence-aware reinforcement learning (RL)
approach aimed at seeking optimal lane-changing recommendations within a
semi-autonomous driving environment to enhance a single vehicle's travel
efficiency. The problem is framed within a Markov decision process setting and
is addressed through an adherence-aware deep Q network, which takes into
account the partial compliance of human drivers with the recommended actions.
This approach is evaluated within CARLA's driving environment under realistic
scenarios.

</details>


### [91] [ProFi-Net: Prototype-based Feature Attention with Curriculum Augmentation for WiFi-based Gesture Recognition](https://arxiv.org/abs/2504.20193)
*Zhe Cui,Shuxian Zhang,Kangzhi Lou,Le-Nam Tran*

Main category: cs.LG

TL;DR: ProFi-Net是一种新颖的少样本学习框架，用于WiFi手势识别，通过原型度量学习和特征级注意力机制解决训练数据有限和特征稀疏的问题。


<details>
  <summary>Details</summary>
Motivation: 解决WiFi手势识别中训练数据有限和特征稀疏的挑战。

Method: 采用原型度量学习架构，结合特征级注意力机制和课程式数据增强策略。

Result: 在多样真实环境中显著优于传统原型网络和其他先进少样本学习方法。

Conclusion: ProFi-Net在分类准确性和训练效率上表现出色，具有更好的泛化能力和抗过拟合性。

Abstract: This paper presents ProFi-Net, a novel few-shot learning framework for
WiFi-based gesture recognition that overcomes the challenges of limited
training data and sparse feature representations. ProFi-Net employs a
prototype-based metric learning architecture enhanced with a feature-level
attention mechanism, which dynamically refines the Euclidean distance by
emphasizing the most discriminative feature dimensions. Additionally, our
approach introduces a curriculum-inspired data augmentation strategy
exclusively on the query set. By progressively incorporating Gaussian noise of
increasing magnitude, the model is exposed to a broader range of challenging
variations, thereby improving its generalization and robustness to overfitting.
Extensive experiments conducted across diverse real-world environments
demonstrate that ProFi-Net significantly outperforms conventional prototype
networks and other state-of-the-art few-shot learning methods in terms of
classification accuracy and training efficiency.

</details>


### [92] [Representation Learning on a Random Lattice](https://arxiv.org/abs/2504.20197)
*Aryeh Brill*

Main category: cs.LG

TL;DR: 论文提出了一种几何视角，将深度神经网络的表示分解为可解释的特征，以提升其安全性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 通过理解特征，增强深度神经网络的安全性和可靠性。

Method: 采用几何视角，将特征视为嵌入数据分布的坐标系，并使用渗流理论分析随机点阵模型。

Result: 特征被分类为上下文、组件和表面特征，模型与机制可解释性的最新发现一致。

Conclusion: 模型为未来研究提供了方向，支持进一步探索特征的可解释性。

Abstract: Decomposing a deep neural network's learned representations into
interpretable features could greatly enhance its safety and reliability. To
better understand features, we adopt a geometric perspective, viewing them as a
learned coordinate system for mapping an embedded data distribution. We
motivate a model of a generic data distribution as a random lattice and analyze
its properties using percolation theory. Learned features are categorized into
context, component, and surface features. The model is qualitatively consistent
with recent findings in mechanistic interpretability and suggests directions
for future research.

</details>


### [93] [Can Large Language Models Learn Formal Logic? A Data-Driven Training and Evaluation Framework](https://arxiv.org/abs/2504.20213)
*Yuan Xia,Akanksha Atrey,Fadoua Khmaissia,Kedar S. Namjoshi*

Main category: cs.LG

TL;DR: 本文研究了大型语言模型（LLMs）的逻辑推理能力，通过布尔逻辑中的证明构建任务，提出了一种高效的数据增强方法（模板转换），并验证了模型在短证明任务中的强推理能力。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs是否具备逻辑推理能力，并解决训练数据稀缺的问题。

Method: 使用布尔逻辑证明任务，提出随机化合成有效证明的方法和模板转换数据增强技术。

Result: 实验表明LLMs在短证明任务中表现良好，但随着证明复杂度增加性能下降；模板转换对小模型也有效。

Conclusion: LLMs在逻辑推理任务中展现出潜力，模板转换是一种有效的通用数据增强方法。

Abstract: This paper investigates the logical reasoning capabilities of large language
models (LLMs). For a precisely defined yet tractable formulation, we choose the
conceptually simple but technically complex task of constructing proofs in
Boolean logic. A trained LLM receives as input a set of assumptions and a goal,
and produces as output a proof that formally derives the goal from the
assumptions. Incorrect proofs are caught by an automated proof checker. A
critical obstacle for training is the scarcity of real-world proofs. We propose
an efficient, randomized procedure for synthesizing valid proofs and introduce
Template Transformation, a data augmentation technique that enhances the
model's ability to handle complex logical expressions. The central evaluation
question is whether an LLM has indeed learned to reason. We propose tests to
measure the reasoning ability of a black-box LLM. By these measures,
experiments demonstrate strong reasoning capabilities for assertions with short
proofs, which decline with proof complexity. Notably, template transformation
improves accuracy even for smaller models, suggesting its effectiveness across
model scales.

</details>


### [94] [Temporal Neural Operator for Modeling Time-Dependent Physical Phenomena](https://arxiv.org/abs/2504.20249)
*W. Diab,M. Al-Kobaisi*

Main category: cs.LG

TL;DR: 论文提出了一种名为Temporal Neural Operator (TNO)的新型神经网络算子，专门用于解决时间依赖的偏微分方程（PDEs），通过改进DeepONet框架并引入多种训练策略，显著提升了时间动态映射的能力和计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有的神经网络算子（如DeepONet和FNO）在空间函数映射上表现优异，但在时间依赖PDEs的时间动态映射上表现不佳，且训练成本高。

Method: 提出TNO，通过引入时间分支、结合多种训练策略（如马尔可夫假设、教师强制、时间捆绑等），优化了时间动态映射和计算效率。

Result: TNO在时间外推能力、误差累积鲁棒性、分辨率不变性和多输入函数处理方面表现出色。

Conclusion: TNO是一种高效且灵活的神经网络算子，适用于时间依赖PDEs的时空算子学习。

Abstract: Neural Operators (NOs) are machine learning models designed to solve partial
differential equations (PDEs) by learning to map between function spaces.
Neural Operators such as the Deep Operator Network (DeepONet) and the Fourier
Neural Operator (FNO) have demonstrated excellent generalization properties
when mapping between spatial function spaces. However, they struggle in mapping
the temporal dynamics of time-dependent PDEs, especially for time steps not
explicitly seen during training. This limits their temporal accuracy as they do
not leverage these dynamics in the training process. In addition, most NOs tend
to be prohibitively costly to train, especially for higher-dimensional PDEs. In
this paper, we propose the Temporal Neural Operator (TNO), an efficient neural
operator specifically designed for spatio-temporal operator learning for
time-dependent PDEs. TNO achieves this by introducing a temporal-branch to the
DeepONet framework, leveraging the best architectural design choices from
several other NOs, and a combination of training strategies including Markov
assumption, teacher forcing, temporal bundling, and the flexibility to
condition the output on the current state or past states. Through extensive
benchmarking and an ablation study on a diverse set of example problems we
demonstrate the TNO long range temporal extrapolation capabilities, robustness
to error accumulation, resolution invariance, and flexibility to handle
multiple input functions.

</details>


### [95] [Financial Data Analysis with Robust Federated Logistic Regression](https://arxiv.org/abs/2504.20250)
*Kun Yang,Nikhil Krishnan,Sanjeev R. Kulkarni*

Main category: cs.LG

TL;DR: 该研究提出了一种鲁棒的联邦逻辑回归框架，旨在保护用户数据隐私的同时提高模型的可解释性和对异常值的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于解决联邦学习中数据隐私保护、模型可解释性以及对异常值的鲁棒性问题。

Method: 提出了一种鲁棒的联邦逻辑回归框架，并在IID和非IID数据（尤其是含异常值的情况）下进行了验证。

Result: 实验结果表明，该方法在二元和多类分类任务中表现与经典集中式算法（如逻辑回归、决策树和K近邻）相当。

Conclusion: 该框架在联邦学习中实现了隐私保护、可解释性和鲁棒性的平衡，具有实际应用潜力。

Abstract: In this study, we focus on the analysis of financial data in a federated
setting, wherein data is distributed across multiple clients or locations, and
the raw data never leaves the local devices. Our primary focus is not only on
the development of efficient learning frameworks (for protecting user data
privacy) in the field of federated learning but also on the importance of
designing models that are easier to interpret. In addition, we care about the
robustness of the framework to outliers. To achieve these goals, we propose a
robust federated logistic regression-based framework that strives to strike a
balance between these goals. To verify the feasibility of our proposed
framework, we carefully evaluate its performance not only on independently
identically distributed (IID) data but also on non-IID data, especially in
scenarios involving outliers. Extensive numerical results collected from
multiple public datasets demonstrate that our proposed method can achieve
comparable performance to those of classical centralized algorithms, such as
Logistical Regression, Decision Tree, and K-Nearest Neighbors, in both binary
and multi-class classification tasks.

</details>


### [96] [Investigating task-specific prompts and sparse autoencoders for activation monitoring](https://arxiv.org/abs/2504.20271)
*Henk Tillman,Dan Mossing*

Main category: cs.LG

TL;DR: 论文探讨了如何通过监控语言模型的内部激活来检测其不安全行为，比较了多种方法，包括线性探测、提示探测和稀疏自编码器方法，并推荐了在不同计算资源下的最佳方法。


<details>
  <summary>Details</summary>
Motivation: 语言模型可能表现出意外和不安全的行为，因此需要监控其输出。内部激活编码了额外信息，可用于改进监控。

Method: 比较了线性探测、提示探测和稀疏自编码器（SAE）方法。提示探测利用测试时计算，SAE方法利用训练时计算。

Result: 零-shot直接询问模型是一个合理的基线，但激活探测方法在足够训练数据下表现更好。提示探测在计算资源充足时表现最佳，SAE方法在计算受限时更优。

Conclusion: 推荐在计算资源充足时使用提示探测，计算受限时使用SAE方法。

Abstract: Language models can behave in unexpected and unsafe ways, and so it is
valuable to monitor their outputs. Internal activations of language models
encode additional information that could be useful for this. The baseline
approach for activation monitoring is some variation of linear probing on a
particular layer: starting from a labeled dataset, train a logistic regression
classifier on that layer's activations. Recent work has proposed several
approaches which may improve on naive linear probing, by leveraging additional
computation. One class of techniques, which we call "prompted probing,"
leverages test time computation to improve monitoring by (1) prompting the
model with a description of the monitoring task, and (2) applying a learned
linear probe to resulting activations. Another class of techniques uses
computation at train time: training sparse autoencoders offline to identify an
interpretable basis for the activations, and e.g. max-pooling activations
across tokens using that basis before applying a linear probe. However, one can
also prompt the model with a description of the monitoring task and use its
output directly. We develop and test novel refinements of these methods and
compare them against each other. We find asking the model zero-shot is a
reasonable baseline when inference-time compute is not limited; however,
activation probing methods can substantially outperform this baseline given
sufficient training data. Specifically, we recommend prompted probing when
inference-time compute is available, due to its superior data efficiency and
good generalization performance. Alternatively, if inference-time compute is
limited, we find SAE-based probing methods outperform raw activation probing.

</details>


### [97] [Generative Diffusion Models for Resource Allocation in Wireless Networks](https://arxiv.org/abs/2504.20277)
*Yigit Berkay Uslu,Samar Hadou,Shirin Saeedi Bidokhti,Alejandro Ribeiro*

Main category: cs.LG

TL;DR: 提出了一种基于生成扩散模型（GDM）的监督训练算法，用于学习随机资源分配策略，并通过图神经网络（GNN）参数化扩散过程以实现泛化。


<details>
  <summary>Details</summary>
Motivation: 解决随机资源分配问题，通过模仿专家策略生成最优分布样本，实现近最优性能。

Method: 将分配问题建模为具有遍历QoS约束的效用函数最大化问题，利用GDM模仿专家策略并生成样本，通过GNN参数化扩散过程。

Result: 在多用户干扰网络的功率控制案例中展示了近最优性能。

Conclusion: 该方法通过GDM和GNN的结合，实现了对随机资源分配问题的有效解决和泛化能力。

Abstract: This paper proposes a supervised training algorithm for learning stochastic
resource allocation policies with generative diffusion models (GDMs). We
formulate the allocation problem as the maximization of an ergodic utility
function subject to ergodic Quality of Service (QoS) constraints. Given samples
from a stochastic expert policy that yields a near-optimal solution to the
problem, we train a GDM policy to imitate the expert and generate new samples
from the optimal distribution. We achieve near-optimal performance through
sequential execution of the generated samples. To enable generalization to a
family of network configurations, we parameterize the backward diffusion
process with a graph neural network (GNN) architecture. We present numerical
results in a case study of power control in multi-user interference networks.

</details>


### [98] [FedCCL: Federated Clustered Continual Learning Framework for Privacy-focused Energy Forecasting](https://arxiv.org/abs/2504.20282)
*Michael A. Helcig,Stefan Nastic*

Main category: cs.LG

TL;DR: FedCCL是一个针对静态组织特征但动态客户端可用性的联邦学习框架，通过静态预训练聚类和异步FedAvg算法，提高模型效率和适应性。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习方法在异构数据分布和计算能力差异下表现不佳，传统解决方案效率低且模型专业化延迟。

Method: 结合静态预训练聚类和异步FedAvg算法，采用三层模型拓扑（全局、集群特定和本地模型）管理知识共享。

Result: 在欧洲中部光伏安装数据上，FedCCL实现了3.93%的能源预测误差，且对新安装的性能下降仅为0.14个百分点。

Conclusion: FedCCL为隐私保护的分布式学习提供了高效框架，适应动态参与者群体，保持高准确性和稳定性。

Abstract: Privacy-preserving distributed model training is crucial for modern machine
learning applications, yet existing Federated Learning approaches struggle with
heterogeneous data distributions and varying computational capabilities.
Traditional solutions either treat all participants uniformly or require costly
dynamic clustering during training, leading to reduced efficiency and delayed
model specialization. We present FedCCL (Federated Clustered Continual
Learning), a framework specifically designed for environments with static
organizational characteristics but dynamic client availability. By combining
static pre-training clustering with an adapted asynchronous FedAvg algorithm,
FedCCL enables new clients to immediately profit from specialized models
without prior exposure to their data distribution, while maintaining reduced
coordination overhead and resilience to client disconnections. Our approach
implements an asynchronous Federated Learning protocol with a three-tier model
topology - global, cluster-specific, and local models - that efficiently
manages knowledge sharing across heterogeneous participants. Evaluation using
photovoltaic installations across central Europe demonstrates that FedCCL's
location-based clustering achieves an energy prediction error of 3.93%
(+-0.21%), while maintaining data privacy and showing that the framework
maintains stability for population-independent deployments, with 0.14
percentage point degradation in performance for new installations. The results
demonstrate that FedCCL offers an effective framework for privacy-preserving
distributed learning, maintaining high accuracy and adaptability even with
dynamic participant populations.

</details>


### [99] [Radius-Guided Post-Clustering for Shape-Aware, Scalable Refinement of k-Means Results](https://arxiv.org/abs/2504.20293)
*Stefan Kober*

Main category: cs.LG

TL;DR: 论文提出了一种基于几何增强的k-means改进方法，通过合并重叠簇来适应非凸形状，并支持递归分区和分布式计算。


<details>
  <summary>Details</summary>
Motivation: 传统k-means对非凸形状效果不佳且需预先指定簇数k。本文旨在解决这些问题。

Method: 在标准k-means后，为每个簇中心分配半径（最远点距离），合并重叠簇。支持递归分区和全局合并。

Result: 该方法在基准数据集上表现良好，计算开销低，能准确重建非凸形状。

Conclusion: 该几何增强方法提升了k-means的灵活性和可扩展性，适用于分布式系统。

Abstract: Traditional k-means clustering underperforms on non-convex shapes and
requires the number of clusters k to be specified in advance. We propose a
simple geometric enhancement: after standard k-means, each cluster center is
assigned a radius (the distance to its farthest assigned point), and clusters
whose radii overlap are merged. This post-processing step loosens the
requirement for exact k: as long as k is overestimated (but not excessively),
the method can often reconstruct non-convex shapes through meaningful merges.
We also show that this approach supports recursive partitioning: clustering can
be performed independently on tiled regions of the feature space, then globally
merged, making the method scalable and suitable for distributed systems.
Implemented as a lightweight post-processing step atop scikit-learn's k-means,
the algorithm performs well on benchmark datasets, achieving high accuracy with
minimal additional computation.

</details>


### [100] [The Dark Side of Digital Twins: Adversarial Attacks on AI-Driven Water Forecasting](https://arxiv.org/abs/2504.20295)
*Mohammadhossein Homaei,Victor Gonzalez Morales,Oscar Mogollon-Gutierrez,Andres Caro*

Main category: cs.LG

TL;DR: 本文介绍了一种用于西班牙供水网络的数字孪生平台，利用LSTM预测用水量，但模型易受对抗攻击。作者提出了一种基于学习自动机（LA）的方法增强攻击效果，实验显示预测误差显著增加，强调了AI驱动数字孪生的网络安全风险及防御需求。


<details>
  <summary>Details</summary>
Motivation: 数字孪生（DTs）通过实时数据和预测模型优化供水系统，但机器学习模型易受对抗攻击，需研究其影响及防御措施。

Method: 设计了一个基于LSTM的数字孪生平台，并引入学习自动机（LA）和随机LA方法动态调整对抗扰动，增强攻击效果。

Result: 实验表明，对抗攻击使预测误差（MAPE）从26%升至35%以上，自适应攻击策略进一步放大了影响。

Conclusion: 研究揭示了AI驱动数字孪生的网络安全风险，呼吁采用对抗训练、异常检测等防御措施。

Abstract: Digital twins (DTs) are improving water distribution systems by using
real-time data, analytics, and prediction models to optimize operations. This
paper presents a DT platform designed for a Spanish water supply network,
utilizing Long Short-Term Memory (LSTM) networks to predict water consumption.
However, machine learning models are vulnerable to adversarial attacks, such as
the Fast Gradient Sign Method (FGSM) and Projected Gradient Descent (PGD).
These attacks manipulate critical model parameters, injecting subtle
distortions that degrade forecasting accuracy. To further exploit these
vulnerabilities, we introduce a Learning Automata (LA) and Random LA-based
approach that dynamically adjusts perturbations, making adversarial attacks
more difficult to detect. Experimental results show that this approach
significantly impacts prediction reliability, causing the Mean Absolute
Percentage Error (MAPE) to rise from 26% to over 35%. Moreover, adaptive attack
strategies amplify this effect, highlighting cybersecurity risks in AI-driven
DTs. These findings emphasize the urgent need for robust defenses, including
adversarial training, anomaly detection, and secure data pipelines.

</details>


### [101] [FigBO: A Generalized Acquisition Function Framework with Look-Ahead Capability for Bayesian Optimization](https://arxiv.org/abs/2504.20307)
*Hui Chen,Xuhui Fan,Zhangkai Wu,Longbing Cao*

Main category: cs.LG

TL;DR: FigBO是一种通用的获取函数，通过融入候选点对未来全局信息增益的影响，解决了传统短视获取函数缺乏前瞻能力的问题。


<details>
  <summary>Details</summary>
Motivation: 传统短视获取函数因其简单高效被广泛采用，但缺乏前瞻能力限制了其性能。

Method: 提出FigBO，一种可无缝集成现有短视获取函数的通用获取函数，分析了其与期望改进（EI）结合的遗憾界和收敛速度。

Result: 实验表明，FigBO在多样任务中实现了最先进的性能和更快的收敛速度。

Conclusion: FigBO通过前瞻性设计显著提升了贝叶斯优化的性能。

Abstract: Bayesian optimization is a powerful technique for optimizing
expensive-to-evaluate black-box functions, consisting of two main components: a
surrogate model and an acquisition function. In recent years, myopic
acquisition functions have been widely adopted for their simplicity and
effectiveness. However, their lack of look-ahead capability limits their
performance. To address this limitation, we propose FigBO, a generalized
acquisition function that incorporates the future impact of candidate points on
global information gain. FigBO is a plug-and-play method that can integrate
seamlessly with most existing myopic acquisition functions. Theoretically, we
analyze the regret bound and convergence rate of FigBO when combined with the
myopic base acquisition function expected improvement (EI), comparing them to
those of standard EI. Empirically, extensive experimental results across
diverse tasks demonstrate that FigBO achieves state-of-the-art performance and
significantly faster convergence compared to existing methods.

</details>


### [102] [A Cryptographic Perspective on Mitigation vs. Detection in Machine Learning](https://arxiv.org/abs/2504.20310)
*Greg Gluch,Shafi Goldwasser*

Main category: cs.LG

TL;DR: 论文研究了对抗性输入在机器学习推理阶段的检测与缓解问题，提出了防御检测（DbD）和防御缓解（DbM）的形式化定义，并展示了在分类任务中两者等价，但在生成任务中分离。


<details>
  <summary>Details</summary>
Motivation: 研究对抗性输入在机器学习推理阶段的防御策略，形式化定义防御检测和防御缓解，并探讨其在不同任务中的适用性。

Method: 通过3轮协议形式化定义DbD和DbM，引入正确性、完备性和可靠性属性，分析其在分类和生成任务中的表现。

Result: 分类任务中DbD和DbM等价；生成任务中DbM可行而DbD不可行，基于密码学假设。

Conclusion: 防御策略的选择需根据任务类型，生成任务中DbM更具优势。

Abstract: In this paper, we initiate a cryptographically inspired theoretical study of
detection versus mitigation of adversarial inputs produced by attackers of
Machine Learning algorithms during inference time.
  We formally define defense by detection (DbD) and defense by mitigation
(DbM). Our definitions come in the form of a 3-round protocol between two
resource-bounded parties: a trainer/defender and an attacker. The attacker aims
to produce inference-time inputs that fool the training algorithm. We define
correctness, completeness, and soundness properties to capture successful
defense at inference time while not degrading (too much) the performance of the
algorithm on inputs from the training distribution.
  We first show that achieving DbD and achieving DbM are equivalent for ML
classification tasks. Surprisingly, this is not the case for ML generative
learning tasks, where there are many possible correct outputs that can be
generated for each input. We show a separation between DbD and DbM by
exhibiting a generative learning task for which is possible to defend by
mitigation but is provably impossible to defend by detection under the
assumption that the Identity-Based Fully Homomorphic Encryption (IB-FHE),
publicly-verifiable zero-knowledge Succinct Non-Interactive Arguments of
Knowledge (zk-SNARK) and Strongly Unforgeable Signatures exist. The mitigation
phase uses significantly fewer samples than the initial training algorithm.

</details>


### [103] [Perturbation-efficient Zeroth-order Optimization for Hardware-friendly On-device Training](https://arxiv.org/abs/2504.20314)
*Qitao Tan,Sung-En Chang,Rui Xia,Huidong Ji,Chence Yang,Ci Zhang,Jun Liu,Zheng Zhan,Zhou Zou,Yanzhi Wang,Jin Lu,Geng Yuan*

Main category: cs.LG

TL;DR: PeZO是一种高效的零阶优化框架，通过减少随机数生成需求和使用硬件友好的均匀分布替代高斯分布，显著降低了硬件资源消耗和功耗，同时保持训练性能。


<details>
  <summary>Details</summary>
Motivation: 零阶优化（ZO）在深度神经网络训练中具有计算简单和内存节省的优势，但其需要大量生成高斯随机数，导致在FPGA和ASIC等硬件平台上难以实现。本文旨在解决这一算法与硬件设计不匹配的问题。

Method: 提出PeZO框架，采用随机数重用策略减少随机数生成需求，并引入硬件友好的自适应缩放方法，用均匀分布替代高斯分布。

Result: 实验表明，PeZO将随机数生成所需的LUT和FF分别减少48.6%和12.7%，最大节省86%功耗，且不影响训练性能。

Conclusion: PeZO首次探索了零阶优化在设备端训练的潜力，为未来研究提供了有价值的见解。

Abstract: Zeroth-order (ZO) optimization is an emerging deep neural network (DNN)
training paradigm that offers computational simplicity and memory savings.
However, this seemingly promising approach faces a significant and long-ignored
challenge. ZO requires generating a substantial number of Gaussian random
numbers, which poses significant difficulties and even makes it infeasible for
hardware platforms, such as FPGAs and ASICs. In this paper, we identify this
critical issue, which arises from the mismatch between algorithm and hardware
designers. To address this issue, we proposed PeZO, a perturbation-efficient ZO
framework. Specifically, we design random number reuse strategies to
significantly reduce the demand for random number generation and introduce a
hardware-friendly adaptive scaling method to replace the costly Gaussian
distribution with a uniform distribution. Our experiments show that PeZO
reduces the required LUTs and FFs for random number generation by 48.6\% and
12.7\%, and saves at maximum 86\% power consumption, all without compromising
training performance, making ZO optimization feasible for on-device training.
To the best of our knowledge, we are the first to explore the potential of
on-device ZO optimization, providing valuable insights for future research.

</details>


### [104] [Bayesian Experimental Design for Model Discrepancy Calibration: An Auto-Differentiable Ensemble Kalman Inversion Approach](https://arxiv.org/abs/2504.20319)
*Huchen Yang,Xinghao Dong,Jin-Long Wu*

Main category: cs.LG

TL;DR: 提出了一种基于自动微分集成卡尔曼反演（AD-EKI）的混合贝叶斯实验设计框架，用于高效处理高维模型差异，并通过实验设计优化校准模型差异。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯实验设计（BED）因模型差异（预测模型与真实物理系统不匹配）导致参数估计偏差，而高维参数空间进一步加剧了贝叶斯更新和设计优化的挑战。

Method: 采用AD-EKI作为梯度自由方法评估信息增益，结合标准BED方法迭代优化实验设计，分离低维物理参数与高维模型差异的推断。

Result: 在经典对流-扩散BED示例中，AD-EKI高效识别了校准模型差异的信息数据，并稳健推断未知物理参数。

Conclusion: AD-EKI不仅解决了BED中模型差异的挑战，还为元学习和结构优化等双层优化问题提供了高效可扩展的框架。

Abstract: Bayesian experimental design (BED) offers a principled framework for
optimizing data acquisition by leveraging probabilistic inference. However,
practical implementations of BED are often compromised by model discrepancy,
i.e., the mismatch between predictive models and true physical systems, which
can potentially lead to biased parameter estimates. While data-driven
approaches have been recently explored to characterize the model discrepancy,
the resulting high-dimensional parameter space poses severe challenges for both
Bayesian updating and design optimization. In this work, we propose a hybrid
BED framework enabled by auto-differentiable ensemble Kalman inversion (AD-EKI)
that addresses these challenges by providing a computationally efficient,
gradient-free alternative to estimate the information gain for high-dimensional
network parameters. The AD-EKI allows a differentiable evaluation of the
utility function in BED and thus facilitates the use of standard gradient-based
methods for design optimization. In the proposed hybrid framework, we
iteratively optimize experimental designs, decoupling the inference of
low-dimensional physical parameters handled by standard BED methods, from the
high-dimensional model discrepancy handled by AD-EKI. The identified optimal
designs for the model discrepancy enable us to systematically collect
informative data for its calibration. The performance of the proposed method is
studied by a classical convection-diffusion BED example, and the hybrid
framework enabled by AD-EKI efficiently identifies informative data to
calibrate the model discrepancy and robustly infers the unknown physical
parameters in the modeled system. Besides addressing the challenges of BED with
model discrepancy, AD-EKI also potentially fosters efficient and scalable
frameworks in many other areas with bilevel optimization, such as meta-learning
and structure optimization.

</details>


### [105] [Generative Learning for Slow Manifolds and Bifurcation Diagrams](https://arxiv.org/abs/2504.20375)
*Ellis R. Crabtree,Dimitris G. Giovanis,Nikolaos Evangelou,Juan M. Bello-Rivas,Ioannis G. Kevrekidis*

Main category: cs.LG

TL;DR: 论文提出了一种利用条件生成模型（cSGMs）快速初始化多时间尺度系统的低维慢流形，并近似分岔图中稳态状态的方法。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于利用机器学习中的条件生成模型，高效地生成符合特定条件的数据分布，以简化多时间尺度系统的建模和分岔分析。

Method: 方法是通过条件生成模型（cSGMs）生成与目标条件（如慢流形上的特定值或分岔图中的新参数）一致的数据样本。

Result: 结果表明，该方法能够快速初始化慢流形并近似分岔图中的稳态状态，有助于揭示慢流形的几何结构或填补分岔图中的缺失部分。

Conclusion: 结论是条件生成模型为多时间尺度系统的建模和分岔分析提供了一种高效的工具，能够显著提升计算效率。

Abstract: In dynamical systems characterized by separation of time scales, the
approximation of so called ``slow manifolds'', on which the long term dynamics
lie, is a useful step for model reduction. Initializing on such slow manifolds
is a useful step in modeling, since it circumvents fast transients, and is
crucial in multiscale algorithms alternating between fine scale (fast) and
coarser scale (slow) simulations. In a similar spirit, when one studies the
infinite time dynamics of systems depending on parameters, the system
attractors (e.g., its steady states) lie on bifurcation diagrams. Sampling
these manifolds gives us representative attractors (here, steady states of ODEs
or PDEs) at different parameter values. Algorithms for the systematic
construction of these manifolds are required parts of the ``traditional''
numerical nonlinear dynamics toolkit.
  In more recent years, as the field of Machine Learning develops, conditional
score-based generative models (cSGMs) have demonstrated capabilities in
generating plausible data from target distributions that are conditioned on
some given label. It is tempting to exploit such generative models to produce
samples of data distributions conditioned on some quantity of interest (QoI).
In this work, we present a framework for using cSGMs to quickly (a) initialize
on a low-dimensional (reduced-order) slow manifold of a multi-time-scale system
consistent with desired value(s) of a QoI (a ``label'') on the manifold, and
(b) approximate steady states in a bifurcation diagram consistent with a (new,
out-of-sample) parameter value. This conditional sampling can help uncover the
geometry of the reduced slow-manifold and/or approximately ``fill in'' missing
segments of steady states in a bifurcation diagram.

</details>


### [106] [Manifold Clustering with Schatten p-norm Maximization](https://arxiv.org/abs/2504.20390)
*Fangfang Li,Quanxue Gao*

Main category: cs.LG

TL;DR: 论文提出了一种新的聚类框架，通过融合K-means和流形学习，并利用标签指导流形结构，确保数据结构与标签的一致性。同时，通过最大化标签的Schatten p-范数保持类平衡，并支持多种距离函数。实验验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常仅关注K-means与流形学习的结合，而忽略了数据结构与标签的一致性。本文旨在解决这一问题。

Method: 提出新聚类框架，融合K-means和流形学习，利用标签指导流形结构，最大化Schatten p-范数以保持类平衡，并支持多种距离函数。

Result: 实验结果表明，所提模型在多个数据库上表现优越。

Conclusion: 新框架有效解决了数据结构与标签一致性问题，并具有灵活性和高效性。

Abstract: Manifold clustering, with its exceptional ability to capture complex data
structures, holds a pivotal position in cluster analysis. However, existing
methods often focus only on finding the optimal combination between K-means and
manifold learning, and overlooking the consistency between the data structure
and labels. To address this issue, we deeply explore the relationship between
K-means and manifold learning, and on this basis, fuse them to develop a new
clustering framework. Specifically, the algorithm uses labels to guide the
manifold structure and perform clustering on it, which ensures the consistency
between the data structure and labels. Furthermore, in order to naturally
maintain the class balance in the clustering process, we maximize the Schatten
p-norm of labels, and provide a theoretical proof to support this.
Additionally, our clustering framework is designed to be flexible and
compatible with many types of distance functions, which facilitates efficient
processing of nonlinear separable data. The experimental results of several
databases confirm the superiority of our proposed model.

</details>


### [107] [FourierSpecNet: Neural Collision Operator Approximation Inspired by the Fourier Spectral Method for Solving the Boltzmann Equation](https://arxiv.org/abs/2504.20408)
*Jae Yong Lee,Gwang Jae Jung,Byung Chan Lim,Hyung Ju Hwang*

Main category: cs.LG

TL;DR: 提出了一种结合傅里叶谱方法和深度学习的混合框架（FourierSpecNet），用于高效近似玻尔兹曼方程中的碰撞算子，支持零样本超分辨率，显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 玻尔兹曼方程的高维非线性碰撞算子数值求解计算量大，尤其在非弹性碰撞和高维速度域中。

Method: 结合傅里叶谱方法和深度学习，提出FourierSpecNet框架，实现分辨率无关学习和零样本超分辨率。

Result: 在多个基准测试中表现出竞争力，计算成本显著低于传统谱方法。

Conclusion: FourierSpecNet为玻尔兹曼方程的求解提供了高效、可扩展的解决方案，适用于弹性和非弹性碰撞场景。

Abstract: The Boltzmann equation, a fundamental model in kinetic theory, describes the
evolution of particle distribution functions through a nonlinear,
high-dimensional collision operator. However, its numerical solution remains
computationally demanding, particularly for inelastic collisions and
high-dimensional velocity domains. In this work, we propose the Fourier Neural
Spectral Network (FourierSpecNet), a hybrid framework that integrates the
Fourier spectral method with deep learning to approximate the collision
operator in Fourier space efficiently. FourierSpecNet achieves
resolution-invariant learning and supports zero-shot super-resolution, enabling
accurate predictions at unseen resolutions without retraining. Beyond empirical
validation, we establish a consistency result showing that the trained operator
converges to the spectral solution as the discretization is refined. We
evaluate our method on several benchmark cases, including Maxwellian and
hard-sphere molecular models, as well as inelastic collision scenarios. The
results demonstrate that FourierSpecNet offers competitive accuracy while
significantly reducing computational cost compared to traditional spectral
solvers. Our approach provides a robust and scalable alternative for solving
the Boltzmann equation across both elastic and inelastic regimes.

</details>


### [108] [ADiff4TPP: Asynchronous Diffusion Models for Temporal Point Processes](https://arxiv.org/abs/2504.20411)
*Amartya Mukherjee,Ruizhi Deng,He Zhao,Yuzhen Mao,Leonid Sigal,Frederick Tung*

Main category: cs.LG

TL;DR: 提出了一种基于异步噪声调度的扩散模型，用于建模时间点过程，通过优化噪声调度设计，实现了对未来事件的更准确预测，并在基准数据集上取得了最佳性能。


<details>
  <summary>Details</summary>
Motivation: 传统方法在建模时间点过程时难以灵活处理不同时间尺度的事件生成，且长时预测性能不足。本文旨在通过扩散模型和异步噪声调度解决这些问题。

Method: 采用扩散模型结合异步噪声调度，通过条件流匹配训练模型，优化噪声注入策略，实现事件序列的联合分布建模。

Result: 在基准数据集上，该方法在预测事件间隔时间和事件类型方面表现最优，且能灵活适应不同观测和预测窗口长度。

Conclusion: 该方法在时间点过程建模中表现出色，尤其在长时预测任务中优于现有基线方法。

Abstract: This work introduces a novel approach to modeling temporal point processes
using diffusion models with an asynchronous noise schedule. At each step of the
diffusion process, the noise schedule injects noise of varying scales into
different parts of the data. With a careful design of the noise schedules,
earlier events are generated faster than later ones, thus providing stronger
conditioning for forecasting the more distant future. We derive an objective to
effectively train these models for a general family of noise schedules based on
conditional flow matching. Our method models the joint distribution of the
latent representations of events in a sequence and achieves state-of-the-art
results in predicting both the next inter-event time and event type on
benchmark datasets. Additionally, it flexibly accommodates varying lengths of
observation and prediction windows in different forecasting settings by
adjusting the starting and ending points of the generation process. Finally,
our method shows superior performance in long-horizon prediction tasks,
outperforming existing baseline methods.

</details>


### [109] [Understanding GNNs and Homophily in Dynamic Node Classification](https://arxiv.org/abs/2504.20421)
*Michael Ito,Danai Koutra,Jenna Wiens*

Main category: cs.LG

TL;DR: 论文提出动态同质性（dynamic homophily）作为衡量动态图中图神经网络（GNN）性能的新指标，并证明其在动态节点分类任务中的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有同质性（homophily）研究仅限于静态图，无法解释动态图中GNN的性能差异。

Method: 理论分析动态图中图卷积网络（GCN）的判别性能，并提出动态同质性作为新指标。

Result: 实验表明，现有GNN在动态同质性较低时性能不稳定，新指标与GNN性能相关。

Conclusion: 动态同质性为理解动态图中GNN性能提供了新视角，有助于设计更强大的动态GNN。

Abstract: Homophily, as a measure, has been critical to increasing our understanding of
graph neural networks (GNNs). However, to date this measure has only been
analyzed in the context of static graphs. In our work, we explore homophily in
dynamic settings. Focusing on graph convolutional networks (GCNs), we
demonstrate theoretically that in dynamic settings, current GCN discriminative
performance is characterized by the probability that a node's future label is
the same as its neighbors' current labels. Based on this insight, we propose
dynamic homophily, a new measure of homophily that applies in the dynamic
setting. This new measure correlates with GNN discriminative performance and
sheds light on how to potentially design more powerful GNNs for dynamic graphs.
Leveraging a variety of dynamic node classification datasets, we demonstrate
that popular GNNs are not robust to low dynamic homophily. Going forward, our
work represents an important step towards understanding homophily and GNN
performance in dynamic node classification.

</details>


### [110] [Learning Laplacian Positional Encodings for Heterophilous Graphs](https://arxiv.org/abs/2504.20430)
*Michael Ito,Jiong Zhu,Dexiong Chen,Danai Koutra,Jenna Wiens*

Main category: cs.LG

TL;DR: 论文提出了一种新的图位置编码方法LLPE，解决了现有方法在异质图任务中的不足，并在理论和实验上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有图位置编码在异质图任务中表现不佳，甚至可能损害性能，而现实网络常具有异质性。

Method: 提出Learnable Laplacian Positional Encodings (LLPE)，利用图拉普拉斯算子的全谱，捕捉同质和异质图结构。

Result: 在12个基准测试中，LLPE将GNN的准确率提升高达35%（合成图）和14%（真实图）。

Conclusion: LLPE是开发能有效捕捉异质图复杂结构的PE的重要一步。

Abstract: In this work, we theoretically demonstrate that current graph positional
encodings (PEs) are not beneficial and could potentially hurt performance in
tasks involving heterophilous graphs, where nodes that are close tend to have
different labels. This limitation is critical as many real-world networks
exhibit heterophily, and even highly homophilous graphs can contain local
regions of strong heterophily. To address this limitation, we propose Learnable
Laplacian Positional Encodings (LLPE), a new PE that leverages the full
spectrum of the graph Laplacian, enabling them to capture graph structure on
both homophilous and heterophilous graphs. Theoretically, we prove LLPE's
ability to approximate a general class of graph distances and demonstrate its
generalization properties. Empirically, our evaluation on 12 benchmarks
demonstrates that LLPE improves accuracy across a variety of GNNs, including
graph transformers, by up to 35% and 14% on synthetic and real-world graphs,
respectively. Going forward, our work represents a significant step towards
developing PEs that effectively capture complex structures in heterophilous
graphs.

</details>


### [111] [GaLore 2: Large-Scale LLM Pre-Training by Gradient Low-Rank Projection](https://arxiv.org/abs/2504.20437)
*DiJia Su,Andrew Gu,Jane Xu,Yuandong Tian,Jiawei Zhao*

Main category: cs.LG

TL;DR: GaLore 2是一种高效且可扩展的框架，解决了GaLore在计算开销和并行化策略集成方面的挑战，并展示了其在大型语言模型预训练中的潜力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在训练中面临显著的内存瓶颈，GaLore通过利用梯度低秩结构解决了这一问题，但仍存在计算开销和并行化策略集成的挑战。

Method: GaLore 2通过优化子空间更新的计算开销和集成先进的并行化策略（如FSDP），扩展了GaLore的功能。

Result: GaLore 2成功预训练了Llama 7B模型，使用了高达5000亿的训练token，展示了其在实际LLM预训练中的潜力。

Conclusion: GaLore 2是一个高效且可扩展的框架，解决了GaLore的遗留问题，并展示了其在大型语言模型预训练中的实际应用价值。

Abstract: Large language models (LLMs) have revolutionized natural language
understanding and generation but face significant memory bottlenecks during
training. GaLore, Gradient Low-Rank Projection, addresses this issue by
leveraging the inherent low-rank structure of weight gradients, enabling
substantial memory savings without sacrificing performance. Recent works
further extend GaLore from various aspects, including low-bit quantization and
higher-order tensor structures. However, there are several remaining challenges
for GaLore, such as the computational overhead of SVD for subspace updates and
the integration with state-of-the-art training parallelization strategies
(e.g., FSDP). In this paper, we present GaLore 2, an efficient and scalable
GaLore framework that addresses these challenges and incorporates recent
advancements. In addition, we demonstrate the scalability of GaLore 2 by
pre-training Llama 7B from scratch using up to 500 billion training tokens,
highlighting its potential impact on real LLM pre-training scenarios.

</details>


### [112] [Multidimensional precipitation index prediction based on CNN-LSTM hybrid framework](https://arxiv.org/abs/2504.20442)
*Yuchen Wang,Pengfei Jia,Zhitao Shu,Keyan Liu,Abdul Rashid Mohamed Shariff*

Main category: cs.LG

TL;DR: 该研究提出了一种基于CNN-LSTM混合框架的多维降水指数预测模型，旨在提高降水预测的准确性。实验结果显示，该模型在预测精度和泛化能力上优于传统方法，但处理大规模数据集时计算资源需求较高。


<details>
  <summary>Details</summary>
Motivation: 全球气候变化加剧，准确预测降水对防灾减灾、农业生产和交通管理至关重要。

Method: 使用CNN-LSTM混合框架分析印度Pune地区1972-2002年的月均降水数据，捕捉局部特征和长期依赖关系。

Result: 模型均方根误差（RMSE）为6.752，优于传统时间序列预测方法。

Conclusion: 该模型为降水预测提供了新思路，但需进一步优化以支持多维数据预测，并减少计算资源需求。

Abstract: With the intensification of global climate change, accurate prediction of
weather indicators is of great significance in disaster prevention and
mitigation, agricultural production, and transportation. Precipitation, as one
of the key meteorological indicators, plays a crucial role in water resource
management, agricultural production, and urban flood control. This study
proposes a multidimensional precipitation index prediction model based on a
CNN- LSTM hybrid framework, aiming to improve the accuracy of precipitation
forecasts. The dataset is sourced from Pune, Maharashtra, India, covering
monthly mean precipitation data from 1972 to 2002. This dataset includes nearly
31 years (1972-2002) of monthly average precipitation, reflecting the long-term
fluctuations and seasonal variations of precipitation in the region. By
analyzing these time series data, the CNN-LSTM model effectively captures local
features and long-term dependencies. Experimental results show that the model
achieves a root mean square error (RMSE) of 6.752, which demonstrates a
significant advantage over traditional time series prediction methods in terms
of prediction accuracy and generalization ability. Furthermore, this study
provides new research ideas for precipitation prediction. However, the model
requires high computational resources when dealing with large-scale datasets,
and its predictive ability for multidimensional precipitation data still needs
improvement. Future research could extend the model to support and predict
multidimensional precipitation data, thereby promoting the development of more
accurate and efficient meteorological prediction technologies.

</details>


### [113] [FT-MoE: Sustainable-learning Mixture of Experts Model for Fault-Tolerant Computing with Multiple Tasks](https://arxiv.org/abs/2504.20446)
*Wenjing Xiao,Wenhao Song,Miaojiang Chen,Ruikun Luo,Min Chen*

Main category: cs.LG

TL;DR: FT-MoE是一种可持续学习的混合专家模型，用于多任务容错计算，通过解耦长距离依赖关系和双专家网络提升故障检测与分类性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的容错算法因故障知识的异构性和时间序列日志数据的复杂依赖关系，难以通过单一神经网络模型进一步提升检测性能。

Method: 使用基于解码器的Transformer模型获取故障原型向量，设计双专家网络进行高精度预测，并采用离线训练和在线调优的两阶段优化方案。

Result: 实验表明，FT-MoE在容错基准测试中优于现有最先进方法。

Conclusion: FT-MoE通过可持续学习和动态适应能力，显著提升了容错计算的可靠性和性能。

Abstract: Intelligent fault-tolerant (FT) computing has recently demonstrated
significant advantages of predicting and diagnosing faults in advance, enabling
reliable service delivery. However, due to heterogeneity of fault knowledge and
complex dependence relationships of time series log data, existing deep
learning-based FT algorithms further improve detection performance relying on
single neural network model with difficulty. To this end, we propose FT-MoE, a
sustainable-learning mixture-of-experts model for fault-tolerant computing with
multiple tasks, which enables different parameters learning distinct fault
knowledge to achieve high-reliability for service system. Firstly, we use
decoder-based transformer models to obtain fault prototype vectors of
decoupling long-distance dependencies. Followed by, we present a dual mixture
of experts networks for high-accurate prediction for both fault detection and
classification tasks. Then, we design a two-stage optimization scheme of
offline training and online tuning, which allows that in operation FT-MoE can
also keep learning to adapt to dynamic service environments. Finally, to verify
the effectiveness of FT-MoE, we conduct extensive experiments on the FT
benchmark. Experimental results show that FT-MoE achieves superior performance
compared to the state-of-the-art methods. Code will be available upon
publication.

</details>


### [114] [Reviving Any-Subset Autoregressive Models with Principled Parallel Sampling and Speculative Decoding](https://arxiv.org/abs/2504.20456)
*Gabe Guo,Stefano Ermon*

Main category: cs.LG

TL;DR: AS-ARMs（任意子集自回归模型）通过Any-Subset Speculative Decoding（ASSD）算法解决了并行生成令牌时分布偏离的问题，显著提升了语言生成速度且不牺牲质量。


<details>
  <summary>Details</summary>
Motivation: 解决离散扩散模型在并行生成令牌时分布偏离的问题，探索AS-ARMs的潜力。

Method: 提出AS-ARMs和ASSD算法，支持并行化联合概率密度估计，并通过数学证明和实验验证其有效性。

Result: AS-ARMs在子2亿参数模型中表现最佳，代码生成性能接近50倍大模型。

Conclusion: AS-ARMs是语言建模中一个极具潜力的方向。

Abstract: In arbitrary-order language models, it is an open question how to sample
tokens in parallel from the correct joint distribution. With discrete diffusion
models, the more tokens they generate in parallel, the less their predicted
distributions adhere to the originally learned data distribution, as they rely
on a conditional independence assumption that only works with infinitesimally
small timesteps. We find that a different class of models, any-subset
autoregressive models (AS-ARMs), holds the solution. As implied by the name,
AS-ARMs can generate tokens in any order, and in parallel. Moreover, AS-ARMs
support parallelized joint probability density estimation, allowing them to
correct their own parallel-generated token distributions, via our Any-Subset
Speculative Decoding (ASSD) algorithm. ASSD provably enables generation of
tokens from the correct joint distribution, with the number of neural network
calls upper bounded by the number of tokens predicted. We empirically verify
that ASSD speeds up language generation, without sacrificing quality.
Furthermore, we provide a mathematically justified scheme for training AS-ARMs
for generation, and show that AS-ARMs achieve state-of-the-art performance
among sub-200M parameter models on infilling benchmark tasks, and nearly match
the performance of models 50X larger on code generation. Our theoretical and
empirical results indicate that the once-forgotten AS-ARMs are a promising
direction of language modeling.

</details>


### [115] [The Estimation of Continual Causal Effect for Dataset Shifting Streams](https://arxiv.org/abs/2504.20471)
*Baining Chen,Yiming Zhang,Yuqiao Han,Ruyue Zhang,Ruihuan Du,Zhishuo Zhou,Zhengdan Zhu,Xun Liu,Jiecheng Guo*

Main category: cs.LG

TL;DR: 论文提出了一种名为ICE-PKD的增量因果效应框架，用于解决营销优化中因时间数据偏移带来的复杂性。


<details>
  <summary>Details</summary>
Motivation: 在在线环境中，传统提升模型和约束优化算法框架因时间数据偏移（用户行为和领域分布变化）而性能下降，需要改进。

Method: ICE-PKD框架包括两部分：(i) 多处理提升网络，通过反事实回归消除混杂偏差；(ii) 增量训练策略，通过最新数据更新并基于回放的知识蒸馏保护泛化能力。还引入了新的评估指标。

Result: 在模拟和在线数据集上的实验表明，ICE-PKD框架性能更优，并已部署在华夏租车营销系统中。

Conclusion: ICE-PKD框架有效解决了时间数据偏移问题，提升了营销优化效果。

Abstract: Causal effect estimation has been widely used in marketing optimization. The
framework of an uplift model followed by a constrained optimization algorithm
is popular in practice. To enhance performance in the online environment, the
framework needs to be improved to address the complexities caused by temporal
dataset shift. This paper focuses on capturing the dataset shift from user
behavior and domain distribution changing over time. We propose an Incremental
Causal Effect with Proxy Knowledge Distillation (ICE-PKD) framework to tackle
this challenge. The ICE-PKD framework includes two components: (i) a
multi-treatment uplift network that eliminates confounding bias using
counterfactual regression; (ii) an incremental training strategy that adapts to
the temporal dataset shift by updating with the latest data and protects
generalization via replay-based knowledge distillation. We also revisit the
uplift modeling metrics and introduce a novel metric for more precise online
evaluation in multiple treatment scenarios. Extensive experiments on both
simulated and online datasets show that the proposed framework achieves better
performance. The ICE-PKD framework has been deployed in the marketing system of
Huaxiaozhu, a ride-hailing platform in China.

</details>


### [116] [Group Relative Knowledge Distillation: Learning from Teacher's Relational Inductive Bias](https://arxiv.org/abs/2504.20482)
*Chao Li,Changhua Zhou,Jia Chen*

Main category: cs.LG

TL;DR: 论文提出了一种新的知识蒸馏方法GRKD，通过关注类别间的相对排名而非绝对概率分布，提升了学生模型的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有知识蒸馏方法主要关注绝对概率的模仿，忽略了教师模型中相对预测的关系性归纳偏差，导致暴露偏差。

Method: 提出了GRKD框架，通过引入组相对损失，鼓励学生模型学习教师输出的类别间相对排序。

Result: 在分类基准测试中，GRKD表现出优于现有方法的泛化能力，尤其在细粒度分类任务中效果显著。

Conclusion: GRKD为知识蒸馏提供了新视角，强调关系结构而非绝对概率，具有广泛的应用潜力。

Abstract: Knowledge distillation typically transfers knowledge from a teacher model to
a student model by minimizing differences between their output distributions.
However, existing distillation approaches largely focus on mimicking absolute
probabilities and neglect the valuable relational inductive biases embedded in
the teacher's relative predictions, leading to exposure bias. In this paper, we
propose Group Relative Knowledge Distillation (GRKD), a novel framework that
distills teacher knowledge by learning the relative ranking among classes,
rather than directly fitting the absolute distribution. Specifically, we
introduce a group relative loss that encourages the student model to preserve
the pairwise preference orderings provided by the teacher's outputs. Extensive
experiments on classification benchmarks demonstrate that GRKD achieves
superior generalization compared to existing methods, especially in tasks
requiring fine-grained class differentiation. Our method provides a new
perspective on exploiting teacher knowledge, focusing on relational structure
rather than absolute likelihood.

</details>


### [117] [Wavelet-Filtering of Symbolic Music Representations for Folk Tune Segmentation and Classification](https://arxiv.org/abs/2504.20522)
*Gissel Velarde,Tillman Weyde,David Meredith*

Main category: cs.LG

TL;DR: 本研究评估了一种基于Haar小波滤波的机器学习方法，用于对民歌符号表示进行分段和分类，并与基于格式塔的方法进行比较。结果表明，小波滤波在优化参数后分类效果更优。


<details>
  <summary>Details</summary>
Motivation: 旨在探索一种更有效的民歌旋律分类方法，通过小波滤波提升分类准确性。

Method: 使用Haar小波进行连续小波变换（CWT），提取特定时间尺度的滤波信号，利用小波系数的局部最大值进行分段，并通过k近邻算法分类。

Result: 优化参数后，基于小波的分段和滤波方法在交叉验证中表现出更高的分类准确性。

Conclusion: Haar小波滤波方法在民歌旋律分类中优于传统的格式塔方法，尤其在参数优化后效果显著。

Abstract: The aim of this study is to evaluate a machine-learning method in which
symbolic representations of folk songs are segmented and classified into tune
families with Haar-wavelet filtering. The method is compared with previously
proposed Gestalt-based method. Melodies are represented as discrete symbolic
pitch-time signals. We apply the continuous wavelet transform (CWT) with the
Haar wavelet at specific scales, obtaining filtered versions of melodies
emphasizing their information at particular time-scales. We use the filtered
signal for representation and segmentation, using the wavelet coefficients'
local maxima to indicate local boundaries and classify segments by means of
k-nearest neighbours based on standard vector-metrics (Euclidean, cityblock),
and compare the results to a Gestalt-based segmentation method and metrics
applied directly to the pitch signal. We found that the wavelet based
segmentation and wavelet-filtering of the pitch signal lead to better
classification accuracy in cross-validated evaluation when the time-scale and
other parameters are optimized.

</details>


### [118] [DeeP-Mod: Deep Dynamic Programming based Environment Modelling using Feature Extraction](https://arxiv.org/abs/2504.20535)
*Chris Child,Lam Ngo*

Main category: cs.LG

TL;DR: DeeP-Mod框架通过DDPN提取特征构建环境模型，解决了DQN中状态信息丢失的问题，并实现了任务和动作集的独立性。


<details>
  <summary>Details</summary>
Motivation: 解决DQN在深层网络中因混合状态-动作表示导致状态信息丢失的问题。

Method: 使用动态规划训练DDPN，通过值迭代确保输出为状态值而非状态-动作对，并提取特征构建环境模型。

Result: 减少的DDPN在噪声下收敛更快且性能优于原始DDPN；DeeP-Mod框架无需外部环境模型即可学习最优策略。

Conclusion: DeeP-Mod框架通过DDPN特征提取有效解决了状态信息丢失问题，适用于广泛环境。

Abstract: The DeeP-Mod framework builds an environment model using features from a Deep
Dynamic Programming Network (DDPN), trained via a Deep Q-Network (DQN). While
Deep Q-Learning is effective in decision-making, state information is lost in
deeper DQN layers due to mixed state-action representations. We address this by
using Dynamic Programming (DP) to train a DDPN, where Value Iteration ensures
the output represents state values, not state-action pairs. Extracting features
from the DDPN preserves state information, enabling task and action set
independence. We show that a reduced DDPN can be trained using features
extracted from the original DDPN trained on an identical problem. This reduced
DDPN achieves faster convergence under noise and outperforms the original DDPN.
Finally, we introduce the DeeP-Mod framework, which creates an environment
model using the evolution of features extracted from a DDPN in response to
actions. A second DDPN, which learns directly from this feature model rather
than raw states, can learn an effective feature-value representation and thus
optimal policy. A key advantage of DeeP-Mod is that an externally defined
environment model is not needed at any stage, making DDPN applicable to a wide
range of environments.

</details>


### [119] [Inclusive Training Separation and Implicit Knowledge Interaction for Balanced Online Class-Incremental Learning](https://arxiv.org/abs/2504.20566)
*Shunjie Wen,Thomas Heinis,Dong-Wan Choi*

Main category: cs.LG

TL;DR: 论文提出了一种名为BOIL的新方法，通过双分类器的包容性训练策略，有效平衡在线类增量学习中的新旧类知识，实现高性能。


<details>
  <summary>Details</summary>
Motivation: 在线类增量学习（OCIL）的核心挑战是在不断更新的模型中平衡新旧类知识，现有方法往往难以兼顾可塑性和稳定性。

Method: 提出BOIL方法，采用双分类器的包容性训练策略，并通过隐式知识转移整合新旧类知识。

Result: 在三个广泛使用的OCIL基准数据集上，BOIL表现出更平衡且优于现有方法的性能。

Conclusion: BOIL方法通过双分类器和知识转移策略，成功解决了OCIL中的平衡问题，为未来研究提供了新思路。

Abstract: Online class-incremental learning (OCIL) focuses on gradually learning new
classes (called plasticity) from a stream of data in a single-pass, while
concurrently preserving knowledge of previously learned classes (called
stability). The primary challenge in OCIL lies in maintaining a good balance
between the knowledge of old and new classes within the continually updated
model. Most existing methods rely on explicit knowledge interaction through
experience replay, and often employ exclusive training separation to address
bias problems. Nevertheless, it still remains a big challenge to achieve a
well-balanced learner, as these methods often exhibit either reduced plasticity
or limited stability due to difficulties in continually integrating knowledge
in the OCIL setting. In this paper, we propose a novel replay-based method,
called Balanced Online Incremental Learning (BOIL), which can achieve both high
plasticity and stability, thus ensuring more balanced performance in OCIL. Our
BOIL method proposes an inclusive training separation strategy using dual
classifiers so that knowledge from both old and new classes can effectively be
integrated into the model, while introducing implicit approaches for
transferring knowledge across the two classifiers. Extensive experimental
evaluations over three widely-used OCIL benchmark datasets demonstrate the
superiority of BOIL, showing more balanced yet better performance compared to
state-of-the-art replay-based OCIL methods.

</details>


### [120] [Reinforcement Learning for Reasoning in Large Language Models with One Training Example](https://arxiv.org/abs/2504.20571)
*Yiping Wang,Qing Yang,Zhiyuan Zeng,Liliang Ren,Lucas Liu,Baolin Peng,Hao Cheng,Xuehai He,Kuan Wang,Jianfeng Gao,Weizhu Chen,Shuohang Wang,Simon Shaolei Du,Yelong Shen*

Main category: cs.LG

TL;DR: 1-shot RLVR（单样本强化学习验证奖励）显著提升大型语言模型（LLM）的数学推理能力，单样本训练即可实现性能翻倍。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过高效的数据利用（单样本训练）提升LLM的数学推理能力，并验证RLVR的有效性。

Method: 采用1-shot RLVR方法，结合GRPO和PPO等强化学习算法，通过单样本训练优化模型性能。

Result: 单样本训练使MATH500性能从36.0%提升至73.6%，平均性能从17.6%提升至35.7%，且在多模型和多算法中表现一致。

Conclusion: 1-shot RLVR是一种高效的数据利用方法，其性能提升主要源于策略梯度损失，而非“grokking”现象。熵损失在探索中起关键作用。

Abstract: We show that reinforcement learning with verifiable reward using one training
example (1-shot RLVR) is effective in incentivizing the math reasoning
capabilities of large language models (LLMs). Applying RLVR to the base model
Qwen2.5-Math-1.5B, we identify a single example that elevates model performance
on MATH500 from 36.0% to 73.6%, and improves the average performance across six
common mathematical reasoning benchmarks from 17.6% to 35.7%. This result
matches the performance obtained using the 1.2k DeepScaleR subset (MATH500:
73.6%, average: 35.9%), which includes the aforementioned example. Similar
substantial improvements are observed across various models (Qwen2.5-Math-7B,
Llama3.2-3B-Instruct, DeepSeek-R1-Distill-Qwen-1.5B), RL algorithms (GRPO and
PPO), and different math examples (many of which yield approximately 30% or
greater improvement on MATH500 when employed as a single training example). In
addition, we identify some interesting phenomena during 1-shot RLVR, including
cross-domain generalization, increased frequency of self-reflection, and
sustained test performance improvement even after the training accuracy has
saturated, a phenomenon we term post-saturation generalization. Moreover, we
verify that the effectiveness of 1-shot RLVR primarily arises from the policy
gradient loss, distinguishing it from the "grokking" phenomenon. We also show
the critical role of promoting exploration (e.g., by adding entropy loss with
an appropriate coefficient) in 1-shot RLVR training. As a bonus, we observe
that applying entropy loss alone, without any outcome reward, significantly
enhances Qwen2.5-Math-1.5B's performance on MATH500 by 27.4%. These findings
can inspire future work on RLVR data efficiency and encourage a re-examination
of both recent progress and the underlying mechanisms in RLVR. Our code, model,
and data are open source at https://github.com/ypwang61/One-Shot-RLVR

</details>


### [121] [Digital Shielding for Cross-Domain Wi-Fi Signal Adaptation using Relativistic Average Generative Adversarial Network](https://arxiv.org/abs/2504.20568)
*Danilo Avola,Federica Bruni,Gian Luca Foresti,Daniele Pannone,Amedeo Ranaldi*

Main category: cs.LG

TL;DR: 论文提出了一种基于深度学习的Wi-Fi信号跨域适应模型，通过物理信号屏蔽模拟和RaGAN结合Bi-LSTM架构，实现了高精度的材料识别。


<details>
  <summary>Details</summary>
Motivation: Wi-Fi感知技术受环境条件影响较大，跨域泛化能力不足，需要一种鲁棒的方法来适应不同环境下的信号变化。

Method: 使用RaGAN和Bi-LSTM架构，模拟物理屏蔽（法拉第笼）收集信号数据，训练多类SVM进行分类。

Result: 系统在去噪后的信号上达到96%的准确率，具备强大的材料识别能力。

Conclusion: 该方法在安全应用中具有潜力，可用于识别隐藏物体的材料组成。

Abstract: Wi-Fi sensing uses radio-frequency signals from Wi-Fi devices to analyze
environments, enabling tasks such as tracking people, detecting intrusions, and
recognizing gestures. The rise of this technology is driven by the IEEE
802.11bf standard and growing demand for tools that can ensure privacy and
operate through obstacles. However, the performance of Wi-Fi sensing is heavily
influenced by environmental conditions, especially when extracting spatial and
temporal features from the surrounding scene. A key challenge is achieving
robust generalization across domains, ensuring stable performance even when the
sensing environment changes significantly. This paper introduces a novel deep
learning model for cross-domain adaptation of Wi-Fi signals, inspired by
physical signal shielding. The model uses a Relativistic average Generative
Adversarial Network (RaGAN) with Bidirectional Long Short-Term Memory (Bi-LSTM)
architectures for both the generator and discriminator. To simulate physical
shielding, an acrylic box lined with electromagnetic shielding fabric was
constructed, mimicking a Faraday cage. Wi-Fi signal spectra were collected from
various materials both inside (domain-free) and outside (domain-dependent) the
box to train the model. A multi-class Support Vector Machine (SVM) was trained
on domain-free spectra and tested on signals denoised by the RaGAN. The system
achieved 96% accuracy and demonstrated strong material discrimination
capabilities, offering potential for use in security applications to identify
concealed objects based on their composition.

</details>


### [122] [Representation Learning Preserving Ignorability and Covariate Matching for Treatment Effects](https://arxiv.org/abs/2504.20579)
*Praharsh Nanavati,Ranjitha Prasad,Karthikeyan Shanmugam*

Main category: cs.LG

TL;DR: 论文提出了一种结合神经网络的方法，通过联合处理隐藏混杂和协变量不匹配问题，以更准确地估计观测数据中的处理效应。


<details>
  <summary>Details</summary>
Motivation: 观测数据中的处理效应估计面临隐藏混杂和协变量不匹配两大挑战，现有方法通常只解决其中之一，缺乏统一框架。

Method: 提出两种神经网络架构：一种基于梯度匹配，另一种基于协变量匹配变换，结合因果侧信息生成近似不变的表示。

Result: 实验表明，该方法在ATE和PEHE误差上优于基线，适用于多个因果基准数据集。

Conclusion: 通过近似不变的表示，该方法能够为真实因果效应提供有效调整集和边界，优于传统敏感性分析。

Abstract: Estimating treatment effects from observational data is challenging due to
two main reasons: (a) hidden confounding, and (b) covariate mismatch (control
and treatment groups not having identical distributions). Long lines of works
exist that address only either of these issues. To address the former,
conventional techniques that require detailed knowledge in the form of causal
graphs have been proposed. For the latter, covariate matching and importance
weighting methods have been used. Recently, there has been progress in
combining testable independencies with partial side information for tackling
hidden confounding. A common framework to address both hidden confounding and
selection bias is missing. We propose neural architectures that aim to learn a
representation of pre-treatment covariates that is a valid adjustment and also
satisfies covariate matching constraints. We combine two different neural
architectures: one based on gradient matching across domains created by
subsampling a suitable anchor variable that assumes causal side information,
followed by the other, a covariate matching transformation. We prove that
approximately invariant representations yield approximate valid adjustment sets
which would enable an interval around the true causal effect. In contrast to
usual sensitivity analysis, where an unknown nuisance parameter is varied, we
have a testable approximation yielding a bound on the effect estimate. We also
outperform various baselines with respect to ATE and PEHE errors on causal
benchmarks that include IHDP, Jobs, Cattaneo, and an image-based Crowd
Management dataset.

</details>


### [123] [Independent Learning in Performative Markov Potential Games](https://arxiv.org/abs/2504.20593)
*Rilind Sahitaj,Paulius Sasnauskas,Yiğit Yalın,Debmalya Mandal,Goran Radanović*

Main category: cs.LG

TL;DR: 本文研究了多智能体表演性强化学习（PRL），将表演性效应引入马尔可夫势博弈（MPGs），提出了表演性稳定均衡（PSE）的概念，并证明了其存在性。同时，分析了独立策略梯度上升（IPGA）和独立自然策略梯度（INPG）算法的收敛性，验证了理论结果。


<details>
  <summary>Details</summary>
Motivation: 研究多智能体PRL中表演性效应对环境动态的影响，旨在解决策略部署后环境动态变化的问题。

Method: 引入表演性稳定均衡（PSE）概念，分析IPGA和INPG算法的收敛性，并通过实验验证。

Result: 证明了PSE的存在性，IPGA和INPG在最佳迭代和最后迭代意义上收敛到近似PSE，且INPG渐近收敛到PSE。

Conclusion: 表演性效应消失时，收敛速率与之前工作一致；实验验证了理论结果，为多智能体PRL提供了理论基础。

Abstract: Performative Reinforcement Learning (PRL) refers to a scenario in which the
deployed policy changes the reward and transition dynamics of the underlying
environment. In this work, we study multi-agent PRL by incorporating
performative effects into Markov Potential Games (MPGs). We introduce the
notion of a performatively stable equilibrium (PSE) and show that it always
exists under a reasonable sensitivity assumption. We then provide convergence
results for state-of-the-art algorithms used to solve MPGs. Specifically, we
show that independent policy gradient ascent (IPGA) and independent natural
policy gradient (INPG) converge to an approximate PSE in the best-iterate
sense, with an additional term that accounts for the performative effects.
Furthermore, we show that INPG asymptotically converges to a PSE in the
last-iterate sense. As the performative effects vanish, we recover the
convergence rates from prior work. For a special case of our game, we provide
finite-time last-iterate convergence results for a repeated retraining
approach, in which agents independently optimize a surrogate objective. We
conduct extensive experiments to validate our theoretical findings.

</details>


### [124] [Bridging the Generalisation Gap: Synthetic Data Generation for Multi-Site Clinical Model Validation](https://arxiv.org/abs/2504.20635)
*Bradley Segal,Joshua Fieggen,David Clifton,Lei Clifton*

Main category: cs.LG

TL;DR: 提出了一种结构化合成数据框架，用于系统评估临床机器学习模型的稳健性、公平性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 临床机器学习模型的泛化能力因患者人口统计、疾病流行率和机构实践的多样性而受限，现有评估方法依赖真实数据但存在偏差和灵活性不足的问题。

Method: 开发了一种结构化合成数据框架，提供对数据生成过程的显式控制，包括站点特异性流行率变化、分层子群效应和结构化特征交互。

Result: 通过实验验证了框架在隔离站点变化影响、支持公平性审计和揭示泛化失败方面的有效性，特别是模型复杂性与站点特异性效应的相互作用。

Conclusion: 该框架为临床机器学习模型的可靠部署提供了一种可重复、可解释和可配置的工具。

Abstract: Ensuring the generalisability of clinical machine learning (ML) models across
diverse healthcare settings remains a significant challenge due to variability
in patient demographics, disease prevalence, and institutional practices.
Existing model evaluation approaches often rely on real-world datasets, which
are limited in availability, embed confounding biases, and lack the flexibility
needed for systematic experimentation. Furthermore, while generative models aim
for statistical realism, they often lack transparency and explicit control over
factors driving distributional shifts. In this work, we propose a novel
structured synthetic data framework designed for the controlled benchmarking of
model robustness, fairness, and generalisability. Unlike approaches focused
solely on mimicking observed data, our framework provides explicit control over
the data generating process, including site-specific prevalence variations,
hierarchical subgroup effects, and structured feature interactions. This
enables targeted investigation into how models respond to specific
distributional shifts and potential biases. Through controlled experiments, we
demonstrate the framework's ability to isolate the impact of site variations,
support fairness-aware audits, and reveal generalisation failures, particularly
highlighting how model complexity interacts with site-specific effects. This
work contributes a reproducible, interpretable, and configurable tool designed
to advance the reliable deployment of ML in clinical settings.

</details>


### [125] [Towards Understanding the Nature of Attention with Low-Rank Sparse Decomposition](https://arxiv.org/abs/2504.20938)
*Zhengfu He,Junxuan Wang,Rui Lin,Xuyang Ge,Wentao Shu,Qiong Tang,Junping Zhang,Xipeng Qiu*

Main category: cs.LG

TL;DR: Lorsa是一种稀疏注意力模型，用于替代Transformer中的多头自注意力（MHSA），旨在解决注意力叠加问题，并更清晰地理解特征间的交互。


<details>
  <summary>Details</summary>
Motivation: 解决MHSA中注意力叠加的挑战，以更清晰地分析特征间的交互。

Method: 将MHSA分解为可理解的稀疏组件，采用低秩稀疏注意力（Lorsa）模型。

Result: Lorsa发现了更清晰的MHSA行为（如归纳头、后继头和注意力下沉），并在算术任务中发现了特定头。其解释性与SAE相当，但电路发现能力更强。

Conclusion: Lorsa在解释性和电路发现方面表现优异，尤其在多MHSA头共同计算的特征上。

Abstract: We propose Low-Rank Sparse Attention (Lorsa), a sparse replacement model of
Transformer attention layers to disentangle original Multi Head Self Attention
(MHSA) into individually comprehensible components. Lorsa is designed to
address the challenge of attention superposition to understand
attention-mediated interaction between features in different token positions.
We show that Lorsa heads find cleaner and finer-grained versions of previously
discovered MHSA behaviors like induction heads, successor heads and attention
sink behavior (i.e., heavily attending to the first token). Lorsa and Sparse
Autoencoder (SAE) are both sparse dictionary learning methods applied to
different Transformer components, and lead to consistent findings in many ways.
For instance, we discover a comprehensive family of arithmetic-specific Lorsa
heads, each corresponding to an atomic operation in Llama-3.1-8B. Automated
interpretability analysis indicates that Lorsa achieves parity with SAE in
interpretability while Lorsa exhibits superior circuit discovery properties,
especially for features computed collectively by multiple MHSA heads. We also
conduct extensive experiments on architectural design ablation, Lorsa scaling
law and error analysis.

</details>


### [126] [Decision-centric fairness: Evaluation and optimization for resource allocation problems](https://arxiv.org/abs/2504.20642)
*Simon De Vos,Jente Van Belle,Andres Algaba,Wouter Verbeke,Sam Verboven*

Main category: cs.LG

TL;DR: 论文提出了一种决策中心公平方法，仅在决策区域内（相关决策阈值范围内）引入公平性，避免全局公平方法对模型预测质量的过度限制。


<details>
  <summary>Details</summary>
Motivation: 数据驱动的决策工具可能因预测分数对特定人口群体产生歧视性行为，导致不公平的资源分配。

Method: 提出决策中心公平方法，仅在决策区域内（相关决策阈值范围内）实施公平性，而非全局公平方法。

Result: 通过实验比较，决策中心公平方法在避免模型预测质量下降的同时，实现了公平性。

Conclusion: 决策中心公平方法在真正需要公平性的场景中更有效，避免了不必要的模型性能损失。

Abstract: Data-driven decision support tools play an increasingly central role in
decision-making across various domains. In this work, we focus on binary
classification models for predicting positive-outcome scores and deciding on
resource allocation, e.g., credit scores for granting loans or churn propensity
scores for targeting customers with a retention campaign. Such models may
exhibit discriminatory behavior toward specific demographic groups through
their predicted scores, potentially leading to unfair resource allocation. We
focus on demographic parity as a fairness metric to compare the proportions of
instances that are selected based on their positive outcome scores across
groups. In this work, we propose a decision-centric fairness methodology that
induces fairness only within the decision-making region -- the range of
relevant decision thresholds on the score that may be used to decide on
resource allocation -- as an alternative to a global fairness approach that
seeks to enforce parity across the entire score distribution. By restricting
the induction of fairness to the decision-making region, the proposed
decision-centric approach avoids imposing overly restrictive constraints on the
model, which may unnecessarily degrade the quality of the predicted scores. We
empirically compare our approach to a global fairness approach on multiple
(semi-synthetic) datasets to identify scenarios in which focusing on fairness
where it truly matters, i.e., decision-centric fairness, proves beneficial.

</details>


### [127] [Combatting Dimensional Collapse in LLM Pre-Training Data via Diversified File Selection](https://arxiv.org/abs/2504.20644)
*Ziqing Fan,Siyuan Du,Shengchao Hu,Pingjie Wang,Li Shen,Ya Zhang,Dacheng Tao,Yanfeng Wang*

Main category: cs.LG

TL;DR: 论文提出了一种多样化文件选择算法（DiSF），通过避免特征空间中的维度崩溃，提升预训练数据的多样性，从而显著提高语言模型的整体性能。


<details>
  <summary>Details</summary>
Motivation: 在有限计算预算下，选择高质量预训练数据对提升语言模型的性能和效率至关重要。现有方法基于领域相似性选择数据，但会导致特征空间多样性不足，影响通用任务表现。

Method: 提出DiSF算法，通过贪婪算法选择特征空间中相关性最低的文本文件，优化特征协方差矩阵的特征值分布，解决维度崩溃问题。

Result: 在TinyLlama架构上进行实验，DiSF在9个任务中显著提升性能，节省98.5%的训练文件，并在50B预算内优于全数据预训练，实现1.5倍的训练效率和5倍的数据效率。

Conclusion: DiSF通过增强数据多样性，有效解决了领域相似性选择方法的局限性，显著提升了语言模型的整体性能和效率。

Abstract: Selecting high-quality pre-training data for large language models (LLMs) is
crucial for enhancing their overall performance under limited computation
budget, improving both training and sample efficiency. Recent advancements in
file selection primarily rely on using an existing or trained proxy model to
assess the similarity of samples to a target domain, such as high quality
sources BookCorpus and Wikipedia. However, upon revisiting these methods, the
domain-similarity selection criteria demonstrates a diversity dilemma,
i.e.dimensional collapse in the feature space, improving performance on the
domain-related tasks but causing severe degradation on generic performance. To
prevent collapse and enhance diversity, we propose a DiverSified File selection
algorithm (DiSF), which selects the most decorrelated text files in the feature
space. We approach this with a classical greedy algorithm to achieve more
uniform eigenvalues in the feature covariance matrix of the selected texts,
analyzing its approximation to the optimal solution under a formulation of
$\gamma$-weakly submodular optimization problem. Empirically, we establish a
benchmark and conduct extensive experiments on the TinyLlama architecture with
models from 120M to 1.1B parameters. Evaluating across nine tasks from the
Harness framework, DiSF demonstrates a significant improvement on overall
performance. Specifically, DiSF saves 98.5% of 590M training files in
SlimPajama, outperforming the full-data pre-training within a 50B training
budget, and achieving about 1.5x training efficiency and 5x data efficiency.

</details>


### [128] [RuleKit 2: Faster and simpler rule learning](https://arxiv.org/abs/2504.20650)
*Adam Gudyś,Cezary Maszczyk,Joanna Badura,Adam Grzelak,Marek Sikora,Łukasz Wróbel*

Main category: cs.LG

TL;DR: RuleKit 2是一个基于规则的数据分析工具包，通过新算法和优化实现显著提升性能，并新增Python包和浏览器应用以增强可用性。


<details>
  <summary>Details</summary>
Motivation: 规则在预测和描述性分析中具有重要价值，RuleKit 1已证明其有效性，但需要进一步提升性能和扩展功能以满足更广泛的需求。

Method: RuleKit 2引入了新算法和优化实现，显著提升计算性能；新增Python包和浏览器应用以支持更灵活的使用方式。

Result: RuleKit 2将某些数据集的分析时间缩短了两个数量级，并通过与scikit-learn兼容的Python包和图形界面提升了用户体验。

Conclusion: RuleKit 2是一个高效且易用的规则分析工具，适用于分类、回归和生存问题，开源且易于集成到现有数据分析流程中。

Abstract: Rules offer an invaluable combination of predictive and descriptive
capabilities. Our package for rule-based data analysis, RuleKit, has proven its
effectiveness in classification, regression, and survival problems. Here we
present its second version. New algorithms and optimized implementations of
those previously included, significantly improved the computational performance
of our suite, reducing the analysis time of some data sets by two orders of
magnitude. The usability of RuleKit 2 is provided by two new components: Python
package and browser application with a graphical user interface. The former
complies with scikit-learn, the most popular data mining library for Python,
allowing RuleKit 2 to be straightforwardly integrated into existing data
analysis pipelines. RuleKit 2 is available at GitHub under GNU AGPL 3 license
(https://github.com/adaa-polsl/RuleKit)

</details>


### [129] [Federated learning, ethics, and the double black box problem in medical AI](https://arxiv.org/abs/2504.20656)
*Joshua Hatherley,Anders Søgaard,Angela Ballantyne,Ruben Pauwels*

Main category: cs.LG

TL;DR: 本文探讨了医疗联邦学习（FL）中的伦理风险，提出了一种新的不透明性——联邦不透明性，并指出其在医疗AI中引发的双重黑箱问题。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在医疗AI中被视为解决患者隐私问题的有效方法，但其伦理风险尚未充分研究。本文旨在填补这一空白。

Method: 通过分析医疗FL系统的特点，提出“联邦不透明性”概念，并探讨其对医疗AI的影响。

Result: 研究发现医疗FL可能被过度夸大其益处，并存在双重黑箱问题。

Conclusion: 需克服关键挑战，才能使医疗FL在伦理上可行。

Abstract: Federated learning (FL) is a machine learning approach that allows multiple
devices or institutions to collaboratively train a model without sharing their
local data with a third-party. FL is considered a promising way to address
patient privacy concerns in medical artificial intelligence. The ethical risks
of medical FL systems themselves, however, have thus far been underexamined.
This paper aims to address this gap. We argue that medical FL presents a new
variety of opacity -- federation opacity -- that, in turn, generates a
distinctive double black box problem in healthcare AI. We highlight several
instances in which the anticipated benefits of medical FL may be exaggerated,
and conclude by highlighting key challenges that must be overcome to make FL
ethically feasible in medicine.

</details>


### [130] [Quantum-Enhanced Hybrid Reinforcement Learning Framework for Dynamic Path Planning in Autonomous Systems](https://arxiv.org/abs/2504.20660)
*Sahil Tomar,Shamshe Alam,Sandeep Kumar,Amit Mathur*

Main category: cs.LG

TL;DR: 提出了一种量子经典混合框架，结合量子计算与经典强化学习，显著减少训练时间并提升路径效率和任务成功率。


<details>
  <summary>Details</summary>
Motivation: 通过量子计算的并行性提升经典强化学习的性能，以应对复杂和不可预测环境中的实时自主导航。

Method: 利用量子计算生成鲁棒的Q表和专用转向成本估计，并将其与经典强化学习流程结合。

Result: 模拟和实际测试（如IIT Delhi校园）显示训练时间显著减少，路径效率、轨迹平滑度和任务成功率显著提升。

Conclusion: 该框架在复杂和不可预测环境中具有实时自主导航的潜力。

Abstract: In this paper, a novel quantum classical hybrid framework is proposed that
synergizes quantum with Classical Reinforcement Learning. By leveraging the
inherent parallelism of quantum computing, the proposed approach generates
robust Q tables and specialized turn cost estimations, which are then
integrated with a classical Reinforcement Learning pipeline. The Classical
Quantum fusion results in rapid convergence of training, reducing the training
time significantly and improved adaptability in scenarios featuring static,
dynamic, and moving obstacles. Simulator based evaluations demonstrate
significant enhancements in path efficiency, trajectory smoothness, and mission
success rates, underscoring the potential of framework for real time,
autonomous navigation in complex and unpredictable environments. Furthermore,
the proposed framework was tested beyond simulations on practical scenarios,
including real world map data such as the IIT Delhi campus, reinforcing its
potential for real time, autonomous navigation in complex and unpredictable
environments.

</details>


### [131] [SFi-Former: Sparse Flow Induced Attention for Graph Transformer](https://arxiv.org/abs/2504.20666)
*Zhonghao Li,Ji Shi,Xinming Zhang,Miao Zhang,Bo Li*

Main category: cs.LG

TL;DR: 论文提出了一种名为SFi-attention的新型注意力机制，通过最小化基于网络流的能量函数和l1正则化来学习稀疏模式，解决了传统图变换器因密集注意力导致的弱归纳偏差、过拟合和过度全局化问题。基于此，设计了SFi-Former模型，在多个图数据集上验证了其优越性能。


<details>
  <summary>Details</summary>
Motivation: 传统图变换器在处理图数据时因密集注意力机制存在弱归纳偏差、过拟合和过度全局化问题，需要一种更高效的注意力机制来解决这些问题。

Method: 提出SFi-attention机制，通过最小化基于网络流的能量函数和l1正则化学习稀疏模式，并设计SFi-Former模型，利用稀疏注意力生成稀疏网络流。

Result: SFi-Former在多个图数据集上表现出色，尤其在长距离依赖图数据上达到SOTA性能，且泛化能力更强。

Conclusion: SFi-Former通过稀疏注意力机制有效解决了传统图变换器的问题，并在性能上取得显著提升。

Abstract: Graph Transformers (GTs) have demonstrated superior performance compared to
traditional message-passing graph neural networks in many studies, especially
in processing graph data with long-range dependencies. However, GTs tend to
suffer from weak inductive bias, overfitting and over-globalizing problems due
to the dense attention. In this paper, we introduce SFi-attention, a novel
attention mechanism designed to learn sparse pattern by minimizing an energy
function based on network flows with l1-norm regularization, to relieve those
issues caused by dense attention. Furthermore, SFi-Former is accordingly
devised which can leverage the sparse attention pattern of SFi-attention to
generate sparse network flows beyond adjacency matrix of graph data.
Specifically, SFi-Former aggregates features selectively from other nodes
through flexible adaptation of the sparse attention, leading to a more robust
model. We validate our SFi-Former on various graph datasets, especially those
graph data exhibiting long-range dependencies. Experimental results show that
our SFi-Former obtains competitive performance on GNN Benchmark datasets and
SOTA performance on LongRange Graph Benchmark (LRGB) datasets. Additionally,
our model gives rise to smaller generalization gaps, which indicates that it is
less prone to over-fitting. Click here for codes.

</details>


### [132] [Explanations Go Linear: Interpretable and Individual Latent Encoding for Post-hoc Explainability](https://arxiv.org/abs/2504.20667)
*Simone Piaggesi,Riccardo Guidotti,Fosca Giannotti,Dino Pedreschi*

Main category: cs.LG

TL;DR: ILLUME是一个基于表示学习的灵活可解释框架，结合全局代理模型和实例特定线性变换，为黑盒分类器提供本地和全局解释，解决了传统代理方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 黑盒机器学习模型的可解释性至关重要，但现有的代理方法（本地和全局）各有局限性，如计算成本高或无法捕捉复杂行为。

Method: ILLUME结合全局训练的代理模型和通过元编码器学习的实例特定线性变换，生成本地和全局解释。

Result: 实验证明ILLUME能生成准确、鲁棒且忠实于黑盒模型的特征归因和决策规则。

Conclusion: ILLUME提供了一个统一的解释框架，有效弥补了传统代理方法的不足。

Abstract: Post-hoc explainability is essential for understanding black-box machine
learning models. Surrogate-based techniques are widely used for local and
global model-agnostic explanations but have significant limitations. Local
surrogates capture non-linearities but are computationally expensive and
sensitive to parameters, while global surrogates are more efficient but
struggle with complex local behaviors. In this paper, we present ILLUME, a
flexible and interpretable framework grounded in representation learning, that
can be integrated with various surrogate models to provide explanations for any
black-box classifier. Specifically, our approach combines a globally trained
surrogate with instance-specific linear transformations learned with a
meta-encoder to generate both local and global explanations. Through extensive
empirical evaluations, we demonstrate the effectiveness of ILLUME in producing
feature attributions and decision rules that are not only accurate but also
robust and faithful to the black-box, thus providing a unified explanation
framework that effectively addresses the limitations of traditional surrogate
methods.

</details>


### [133] [What's Wrong with Your Synthetic Tabular Data? Using Explainable AI to Evaluate Generative Models](https://arxiv.org/abs/2504.20687)
*Jan Kapar,Niklas Koenen,Martin Jullum*

Main category: cs.LG

TL;DR: 论文提出了一种利用可解释AI（XAI）技术评估合成表格数据质量的方法，通过分析分类器的特征重要性和特征效应，揭示合成数据的缺陷。


<details>
  <summary>Details</summary>
Motivation: 现有评估合成数据质量的指标存在冲突且无法具体指出问题，需要更透明和深入的分析方法。

Method: 训练一个二元检测分类器区分真实与合成数据，并应用XAI技术（如特征重要性、部分依赖图、Shapley值和反事实解释）分析差异。

Result: 该方法能揭示合成数据中的不一致性、不现实的依赖关系或缺失模式，优于传统评估技术。

Conclusion: XAI技术提高了合成数据评估的透明度，为诊断和改进合成数据质量提供了更深入的见解。

Abstract: Evaluating synthetic tabular data is challenging, since they can differ from
the real data in so many ways. There exist numerous metrics of synthetic data
quality, ranging from statistical distances to predictive performance, often
providing conflicting results. Moreover, they fail to explain or pinpoint the
specific weaknesses in the synthetic data. To address this, we apply
explainable AI (XAI) techniques to a binary detection classifier trained to
distinguish real from synthetic data. While the classifier identifies
distributional differences, XAI concepts such as feature importance and feature
effects, analyzed through methods like permutation feature importance, partial
dependence plots, Shapley values and counterfactual explanations, reveal why
synthetic data are distinguishable, highlighting inconsistencies, unrealistic
dependencies, or missing patterns. This interpretability increases transparency
in synthetic data evaluation and provides deeper insights beyond conventional
metrics, helping diagnose and improve synthetic data quality. We apply our
approach to two tabular datasets and generative models, showing that it
uncovers issues overlooked by standard evaluation techniques.

</details>


### [134] [Unsupervised Surrogate Anomaly Detection](https://arxiv.org/abs/2504.20733)
*Simon Klüttermann,Tim Katzke,Emmanuel Müller*

Main category: cs.LG

TL;DR: 论文提出了一种名为DEAN的无监督异常检测算法，通过神经网络学习正常数据的模式，并基于工程中的类似概念提出“代理异常检测”方法。


<details>
  <summary>Details</summary>
Motivation: 研究无监督异常检测算法，旨在通过学习正常数据的模式来识别异常，受工程中类似概念的启发。

Method: 提出DEAN算法，基于一组代理模型的最优公理设计，并通过121个基准数据集进行评估。

Result: DEAN在性能上优于19种现有方法，同时展示了其可扩展性和可靠性。

Conclusion: DEAN是一种高效且可靠的无监督异常检测方法，适用于多种场景。

Abstract: In this paper, we study unsupervised anomaly detection algorithms that learn
a neural network representation, i.e. regular patterns of normal data, which
anomalies are deviating from. Inspired by a similar concept in engineering, we
refer to our methodology as surrogate anomaly detection. We formalize the
concept of surrogate anomaly detection into a set of axioms required for
optimal surrogate models and propose a new algorithm, named DEAN (Deep Ensemble
ANomaly detection), designed to fulfill these criteria. We evaluate DEAN on 121
benchmark datasets, demonstrating its competitive performance against 19
existing methods, as well as the scalability and reliability of our method.

</details>


### [135] [Intelligent Task Offloading in VANETs: A Hybrid AI-Driven Approach for Low-Latency and Energy Efficiency](https://arxiv.org/abs/2504.20735)
*Tariq Qayyum,Asadullah Tariq,Muhammad Ali,Mohamed Adel Serhani,Zouheir Trabelsi,Maite López-Sánchez*

Main category: cs.LG

TL;DR: 本文提出了一种混合AI框架，结合监督学习、强化学习和粒子群优化（PSO），用于智能任务卸载和资源分配，以解决车载自组网（VANETs）中的动态性挑战。


<details>
  <summary>Details</summary>
Motivation: 车载自组网（VANETs）的动态性导致网络条件不可预测、高延迟、能源效率低下和任务失败等问题，亟需高效解决方案。

Method: 提出混合AI框架，集成监督学习预测最优卸载策略、强化学习实现自适应决策、PSO优化延迟和能源消耗。

Result: 仿真结果表明，该框架显著降低了延迟和能源消耗，同时提高了任务成功率和网络吞吐量。

Conclusion: 该框架为动态车载环境中的实时应用提供了高效、可扩展的解决方案。

Abstract: Vehicular Ad-hoc Networks (VANETs) are integral to intelligent transportation
systems, enabling vehicles to offload computational tasks to nearby roadside
units (RSUs) and mobile edge computing (MEC) servers for real-time processing.
However, the highly dynamic nature of VANETs introduces challenges, such as
unpredictable network conditions, high latency, energy inefficiency, and task
failure. This research addresses these issues by proposing a hybrid AI
framework that integrates supervised learning, reinforcement learning, and
Particle Swarm Optimization (PSO) for intelligent task offloading and resource
allocation. The framework leverages supervised models for predicting optimal
offloading strategies, reinforcement learning for adaptive decision-making, and
PSO for optimizing latency and energy consumption. Extensive simulations
demonstrate that the proposed framework achieves significant reductions in
latency and energy usage while improving task success rates and network
throughput. By offering an efficient, and scalable solution, this framework
sets the foundation for enhancing real-time applications in dynamic vehicular
environments.

</details>


### [136] [DDPS: Discrete Diffusion Posterior Sampling for Paths in Layered Graphs](https://arxiv.org/abs/2504.20754)
*Hao Luan,See-Kiong Ng,Chun Kai Ling*

Main category: cs.LG

TL;DR: 本文研究了在分层图中生成路径的问题，使用离散扩散模型并确保生成的样本是有效路径。方法基于一种称为PALM的简单表示，并通过分类器引导优化生成路径。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在生成模型中表现优异，但在生成具有显式约束的样本方面研究较少。本文旨在解决分层图中路径生成的问题。

Method: 提出了一种称为PALM的路径表示方法，并利用分类器引导技术优化路径生成，无需重新训练扩散模型。

Result: 实验结果表明，该方法在生成满足路径约束的样本上优于其他方法。

Conclusion: 本文的方法为生成具有显式约束的样本提供了一种有效解决方案，尤其在分层图路径生成任务中表现突出。

Abstract: Diffusion models form an important class of generative models today,
accounting for much of the state of the art in cutting edge AI research. While
numerous extensions beyond image and video generation exist, few of such
approaches address the issue of explicit constraints in the samples generated.
In this paper, we study the problem of generating paths in a layered graph (a
variant of a directed acyclic graph) using discrete diffusion models, while
guaranteeing that our generated samples are indeed paths. Our approach utilizes
a simple yet effective representation for paths which we call the padded
adjacency-list matrix (PALM). In addition, we show how to effectively perform
classifier guidance, which helps steer the sampled paths to specific preferred
edges without any retraining of the diffusion model. Our preliminary results
show that empirically, our method outperforms alternatives which do not
explicitly account for path constraints.

</details>


### [137] [JTreeformer: Graph-Transformer via Latent-Diffusion Model for Molecular Generation](https://arxiv.org/abs/2504.20770)
*Ji Shi,Chengxun Xie,Zhonghao Li,Xinming Zhang,Miao Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种名为JTreeformer的图变换器框架，用于分子生成，通过将图生成转化为连接树生成，结合GCN和多头注意力编码器，以及基于有向无环GCN的解码器，显著提升了分子生成的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于变换器的图解码器难以有效利用图信息，限制了其在分子生成中的应用。本文旨在解决这一问题，提升分子生成的能力。

Method: 结合GCN与多头注意力作为编码器，集成有向无环GCN到基于变换器的解码器中，并在潜在空间中引入扩散模型以增强采样效率。

Result: 实验结果表明，JTreeformer在分子生成任务中优于现有方法。

Conclusion: JTreeformer为药物发现提供了一种有前景的工具。

Abstract: The discovery of new molecules based on the original chemical molecule
distributions is of great importance in medicine. The graph transformer, with
its advantages of high performance and scalability compared to traditional
graph networks, has been widely explored in recent research for applications of
graph structures. However, current transformer-based graph decoders struggle to
effectively utilize graph information, which limits their capacity to leverage
only sequences of nodes rather than the complex topological structures of
molecule graphs. This paper focuses on building a graph transformer-based
framework for molecular generation, which we call \textbf{JTreeformer} as it
transforms graph generation into junction tree generation. It combines GCN
parallel with multi-head attention as the encoder. It integrates a directed
acyclic GCN into a graph-based Transformer to serve as a decoder, which can
iteratively synthesize the entire molecule by leveraging information from the
partially constructed molecular structure at each step. In addition, a
diffusion model is inserted in the latent space generated by the encoder, to
enhance the efficiency and effectiveness of sampling further. The empirical
results demonstrate that our novel framework outperforms existing molecule
generation methods, thus offering a promising tool to advance drug discovery
(https://anonymous.4open.science/r/JTreeformer-C74C).

</details>


### [138] [Evaluating Effects of Augmented SELFIES for Molecular Understanding Using QK-LSTM](https://arxiv.org/abs/2504.20789)
*Collin Beaudoin,Swaroop Ghosh*

Main category: cs.LG

TL;DR: 该论文提出了一种结合量子核函数的LSTM网络（QK-LSTM），用于分子特性和副作用预测，并首次分析了增强SMILES和SELFIES在量子-经典混合模型中的作用。


<details>
  <summary>Details</summary>
Motivation: 药物开发中分子特性和副作用的识别耗时且关键，传统方法存在局限性，需要创新的机器学习方法。

Method: 提出QK-LSTM模型，结合量子核函数和经典LSTM，并探索增强SMILES和SELFIES在量子-经典混合模型中的应用。

Result: 增强SELFIES在经典和量子-经典混合模型中分别带来5.97%和5.91%的显著性能提升。

Conclusion: QK-LSTM和增强SELFIES为分子特性预测提供了新方法，显著提升了性能。

Abstract: Identifying molecular properties, including side effects, is a critical yet
time-consuming step in drug development. Failing to detect these side effects
before regulatory submission can result in significant financial losses and
production delays, and overlooking them during the regulatory review can lead
to catastrophic consequences. This challenge presents an opportunity for
innovative machine learning approaches, particularly hybrid quantum-classical
models like the Quantum Kernel-Based Long Short-Term Memory (QK-LSTM) network.
The QK-LSTM integrates quantum kernel functions into the classical LSTM
framework, enabling the capture of complex, non-linear patterns in sequential
data. By mapping input data into a high-dimensional quantum feature space, the
QK-LSTM model reduces the need for large parameter sets, allowing for model
compression without sacrificing accuracy in sequence-based tasks. Recent
advancements have been made in the classical domain using augmented variations
of the Simplified Molecular Line-Entry System (SMILES). However, to the best of
our knowledge, no research has explored the impact of augmented SMILES in the
quantum domain, nor the role of augmented Self-Referencing Embedded Strings
(SELFIES) in either classical or hybrid quantum-classical settings. This study
presents the first analysis of these approaches, providing novel insights into
their potential for enhancing molecular property prediction and side effect
identification. Results reveal that augmenting SELFIES yields in statistically
significant improvements from SMILES by a 5.97% improvement for the classical
domain and a 5.91% improvement for the hybrid quantum-classical domain.

</details>


### [139] [Q-Fusion: Diffusing Quantum Circuits](https://arxiv.org/abs/2504.20794)
*Collin Beaudoin,Swaroop Ghosh*

Main category: cs.LG

TL;DR: 本文提出了一种基于扩散的算法，利用LayerDAG框架生成新的量子电路，解决了当前量子计算中电路设计复杂的问题，并实现了100%有效的电路输出。


<details>
  <summary>Details</summary>
Motivation: 量子计算在解决复杂社会问题方面潜力巨大，但当前NISQ设备受限于量子比特数量和门操作次数，且量子算法设计需要大量专业知识和时间。量子架构搜索（QAS）旨在通过自动生成量子电路减少人工干预。

Method: 采用基于扩散的算法和LayerDAG框架，与其他方法（如LLM、RL、VAE等）形成对比。

Result: 实验结果表明，该模型能够始终生成100%有效的量子电路输出。

Conclusion: 提出的方法在量子电路自动生成方面具有显著优势，为量子计算的发展提供了新的工具。

Abstract: Quantum computing holds great potential for solving socially relevant and
computationally complex problems. Furthermore, quantum machine learning (QML)
promises to rapidly improve our current machine learning capabilities. However,
current noisy intermediate-scale quantum (NISQ) devices are constrained by
limitations in the number of qubits and gate counts, which hinder their full
capabilities. Furthermore, the design of quantum algorithms remains a laborious
task, requiring significant domain expertise and time. Quantum Architecture
Search (QAS) aims to streamline this process by automatically generating novel
quantum circuits, reducing the need for manual intervention. In this paper, we
propose a diffusion-based algorithm leveraging the LayerDAG framework to
generate new quantum circuits. This method contrasts with other approaches that
utilize large language models (LLMs), reinforcement learning (RL), variational
autoencoders (VAE), and similar techniques. Our results demonstrate that the
proposed model consistently generates 100% valid quantum circuit outputs.

</details>


### [140] [The When and How of Target Variable Transformations](https://arxiv.org/abs/2504.20821)
*Loren Nuyts,Jesse Davis*

Main category: cs.LG

TL;DR: 论文强调目标变量转换在机器学习中的重要性，提供实用案例、通用规则及适用情境下的转换建议。


<details>
  <summary>Details</summary>
Motivation: 现有文献对目标变量转换的关注不足，但其对模型学习有显著影响。论文旨在填补这一空白。

Method: 通过案例展示目标变量转换的实用性，提出通用规则，并讨论不同情境下的适用转换。

Result: 目标变量转换在特定情境下能显著提升模型性能。

Conclusion: 目标变量转换是数据准备阶段的关键步骤，需根据情境选择合适的转换方法。

Abstract: The machine learning pipeline typically involves the iterative process of (1)
collecting the data, (2) preparing the data, (3) learning a model, and (4)
evaluating a model. Practitioners recognize the importance of the data
preparation phase in terms of its impact on the ability to learn accurate
models. In this regard, significant attention is often paid to manipulating the
feature set (e.g., selection, transformations, dimensionality reduction). A
point that is less well appreciated is that transformations on the target
variable can also have a large impact on whether it is possible to learn a
suitable model. These transformations may include accounting for
subject-specific biases (e.g., in how someone uses a rating scale), contexts
(e.g., population size effects), and general trends (e.g., inflation). However,
this point has received a much more cursory treatment in the existing
literature. The goal of this paper is three-fold. First, we aim to highlight
the importance of this problem by showing when transforming the target variable
has been useful in practice. Second, we will provide a set of generic ``rules
of thumb'' that indicate situations when transforming the target variable may
be needed. Third, we will discuss which transformations should be considered in
a given situation.

</details>


### [141] [An approach to melodic segmentation and classification based on filtering with the Haar-wavelet](https://arxiv.org/abs/2504.20822)
*Gissel Velarde,Tillman Weyde,David Meredith*

Main category: cs.LG

TL;DR: 提出了一种基于Haar小波变换的旋律分类与分割方法，在巴赫作品和荷兰民谣分类任务中表现优于未滤波信号，但不及多特征字符串匹配方法。


<details>
  <summary>Details</summary>
Motivation: 解决旋律在符号表示中的分类和分割问题，探索小波变换在音乐分析中的应用。

Method: 使用Haar小波对音高信号滤波，通过局部极值或过零点分割信号，再用k近邻算法分类。

Result: 在巴赫作品分类中优于未滤波信号和Gestalt分割法，在荷兰民谣分类中与音高信号相当，但不及多特征字符串匹配方法。

Conclusion: Haar小波方法在特定任务中有效，但需结合其他方法以提升性能。

Abstract: We present a novel method of classification and segmentation of melodies in
symbolic representation. The method is based on filtering pitch as a signal
over time with the Haar-wavelet, and we evaluate it on two tasks. The filtered
signal corresponds to a single-scale signal ws from the continuous Haar wavelet
transform. The melodies are first segmented using local maxima or
zero-crossings of w_s. The segments of w_s are then classified using the
k-nearest neighbour algorithm with Euclidian and city-block distances. The
method proves more effective than using unfiltered pitch signals and
Gestalt-based segmentation when used to recognize the parent works of segments
from Bach's Two-Part Inventions (BWV 772-786). When used to classify 360 Dutch
folk tunes into 26 tune families, the performance of the method is comparable
to the use of pitch signals, but not as good as that of string-matching methods
based on multiple features.

</details>


### [142] [Hybrid Quantum Recurrent Neural Network For Remaining Useful Life Prediction](https://arxiv.org/abs/2504.20823)
*Olga Tsurkan,Aleksandra Konstantinova,Aleksandr Sedykh,Dmitrii Zhiganov,Arsenii Senokosov,Daniil Tarpanov,Matvei Anoshin,Leonid Fedichkin*

Main category: cs.LG

TL;DR: 本文提出了一种混合量子循环神经网络框架，用于预测喷气发动机的剩余使用寿命，相比传统方法在误差指标上表现更优。


<details>
  <summary>Details</summary>
Motivation: 航空航天领域的预测性维护需要准确估计喷气发动机的剩余使用寿命，传统方法在高频成分学习上存在不足。

Method: 结合量子长短期记忆层和经典密集层，利用量子深度注入电路替代传统线性变换，提升高频成分学习能力。

Result: 混合量子循环神经网络在均方根误差和平均绝对误差上比传统方法提升5%，优于随机森林、卷积神经网络和多层感知器等基线方法。

Conclusion: 混合量子-经典方法在有限数据条件下具有潜力，为提升预测性维护的可靠性提供了新途径。

Abstract: Predictive maintenance in aerospace heavily relies on accurate estimation of
the remaining useful life of jet engines. In this paper, we introduce a Hybrid
Quantum Recurrent Neural Network framework, combining Quantum Long Short-Term
Memory layers with classical dense layers for Remaining Useful Life forecasting
on NASA's Commercial Modular Aero-Propulsion System Simulation dataset. Each
Quantum Long Short-Term Memory gate replaces conventional linear
transformations with Quantum Depth-Infused circuits, allowing the network to
learn high-frequency components more effectively. Experimental results
demonstrate that, despite having fewer trainable parameters, the Hybrid Quantum
Recurrent Neural Network achieves up to a 5% improvement over a Recurrent
Neural Network based on stacked Long Short-Term Memory layers in terms of mean
root mean squared error and mean absolute error. Moreover, a thorough
comparison of our method with established techniques, including Random Forest,
Convolutional Neural Network, and Multilayer Perceptron, demonstrates that our
approach, which achieves a Root Mean Squared Error of 15.46, surpasses these
baselines by approximately 13.68%, 16.21%, and 7.87%, respectively.
Nevertheless, it remains outperformed by certain advanced joint architectures.
Our findings highlight the potential of hybrid quantum-classical approaches for
robust time-series forecasting under limited data conditions, offering new
avenues for enhancing reliability in predictive maintenance tasks.

</details>


### [143] [Reinforcement Learning for LLM Reasoning Under Memory Constraints](https://arxiv.org/abs/2504.20834)
*Alan Lee,Harry Tong*

Main category: cs.LG

TL;DR: 论文提出两种内存高效的RL方法（S-GRPO和T-SPMO），在单40GB GPU限制下显著提升LLM在SVAMP和多位数乘法任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 在内存和计算资源有限的学术环境中，探索如何通过RL技术提升LLM的推理能力。

Method: 提出S-GRPO（内存高效的GRPO变体）和T-SPMO（基于令牌前缀匹配的细粒度信用分配策略），并与LoRA微调结合。

Result: 在Qwen2-1.5B上，SVAMP基准准确率从46%提升至70%以上；T-SPMO在多位数乘法任务中表现优异。

Conclusion: 内存高效的RL方法在资源受限时可能通过正则化作用稳定训练，提升模型性能。

Abstract: We explore reinforcement learning (RL) techniques to enhance reasoning within
targeted problem spaces in large language models (LLMs) under memory and
compute constraints. Our focus is on critic-free methods compatible with LoRA
fine-tuning on a single 40GB GPU, a common limitation in academic settings. We
introduce S-GRPO, a memory-efficient variant of Group Relative Policy
Optimization, and T-SPMO, a token-level prefix matching strategy for
fine-grained credit assignment. Despite limited resources, when used to
fine-tune Qwen2-1.5B both methods significantly improve SVAMP benchmark
accuracy from 46% to above 70% using LoRA training. T-SPMO also excels in
multi-digit multiplication tasks, underscoring the potential of RL fine-tuning
under hardware constraints. Additionally, we find that our full-token GRPO
baseline under LoRA fine-tuning did not improve model performance (compared to
base model) on either task, suggesting that our memory-efficient methods may
act as a form of regularization that stabilizes training when only a small
subset of parameters are updated.

</details>


### [144] [Mitigating the Structural Bias in Graph Adversarial Defenses](https://arxiv.org/abs/2504.20848)
*Junyuan Fang,Huimin Liu,Han Yang,Jiajing Wu,Zibin Zheng,Chi K. Tse*

Main category: cs.LG

TL;DR: 提出了一种针对图神经网络（GNNs）对抗攻击的防御策略，通过异质-同质增强图构建、kNN增强图构建和多视角节点注意力模块，减少GNNs在低度节点上的结构偏差。


<details>
  <summary>Details</summary>
Motivation: 当前GNNs防御方法在低度节点上存在结构偏差，类似于传统GNNs在干净图中的问题，因此需要一种新策略来增强防御能力并减少偏差。

Method: 采用异质-同质增强图构建（移除异质链接并添加同质链接）、kNN增强图构建和多视角节点注意力模块，自适应结合不同图视图的表示。

Result: 实验证明该策略在基准数据集上具有防御和去偏效果。

Conclusion: 提出的策略有效提升了GNNs对对抗攻击的鲁棒性，并减少了在低度节点上的结构偏差。

Abstract: In recent years, graph neural networks (GNNs) have shown great potential in
addressing various graph structure-related downstream tasks. However, recent
studies have found that current GNNs are susceptible to malicious adversarial
attacks. Given the inevitable presence of adversarial attacks in the real
world, a variety of defense methods have been proposed to counter these attacks
and enhance the robustness of GNNs. Despite the commendable performance of
these defense methods, we have observed that they tend to exhibit a structural
bias in terms of their defense capability on nodes with low degree (i.e., tail
nodes), which is similar to the structural bias of traditional GNNs on nodes
with low degree in the clean graph. Therefore, in this work, we propose a
defense strategy by including hetero-homo augmented graph construction, $k$NN
augmented graph construction, and multi-view node-wise attention modules to
mitigate the structural bias of GNNs against adversarial attacks. Notably, the
hetero-homo augmented graph consists of removing heterophilic links (i.e.,
links connecting nodes with dissimilar features) globally and adding homophilic
links (i.e., links connecting nodes with similar features) for nodes with low
degree. To further enhance the defense capability, an attention mechanism is
adopted to adaptively combine the representations from the above two kinds of
graph views. We conduct extensive experiments to demonstrate the defense and
debiasing effect of the proposed strategy on benchmark datasets.

</details>


### [145] [Tabular Data Adapters: Improving Outlier Detection for Unlabeled Private Data](https://arxiv.org/abs/2504.20862)
*Dayananda Herurkar,Jörn Hees,Vesselin Tzvetkov,Andreas Dengel*

Main category: cs.LG

TL;DR: 提出了一种名为Tabular Data Adapters（TDA）的新方法，用于在异常检测任务中为未标记的表格数据生成软标签，以解决私有数据集缺乏标签的问题。


<details>
  <summary>Details</summary>
Motivation: 深度学习在大型公共数据集上表现优异，但在私有数据集上常面临结构差异、领域偏移和标签缺失的挑战。

Method: 通过识别统计相似的公共数据集，并利用共享自编码器将私有数据转换为与现有公共模型兼容的格式，生成弱标签。

Result: 在50个不同领域的表格数据集上实验表明，该方法比基线方法提供更准确的标注，同时减少计算时间。

Conclusion: TDA提供了一种可扩展、高效且经济的方法，弥合了公共研究模型与工业应用之间的差距。

Abstract: The remarkable success of Deep Learning approaches is often based and
demonstrated on large public datasets. However, when applying such approaches
to internal, private datasets, one frequently faces challenges arising from
structural differences in the datasets, domain shift, and the lack of labels.
In this work, we introduce Tabular Data Adapters (TDA), a novel method for
generating soft labels for unlabeled tabular data in outlier detection tasks.
By identifying statistically similar public datasets and transforming private
data (based on a shared autoencoder) into a format compatible with
state-of-the-art public models, our approach enables the generation of weak
labels. It thereby can help to mitigate the cold start problem of labeling by
basing on existing outlier detection models for public datasets. In experiments
on 50 tabular datasets across different domains, we demonstrate that our method
is able to provide more accurate annotations than baseline approaches while
reducing computational time. Our approach offers a scalable, efficient, and
cost-effective solution, to bridge the gap between public research models and
real-world industrial applications.

</details>


### [146] [Quantifying the Noise of Structural Perturbations on Graph Adversarial Attacks](https://arxiv.org/abs/2504.20869)
*Junyuan Fang,Han Yang,Haixian Wen,Jiajing Wu,Zibin Zheng,Chi K. Tse*

Main category: cs.LG

TL;DR: 该论文提出了一种基于噪声的攻击强度量化方法，并设计了三种攻击策略，以增强图神经网络的对抗攻击可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有的图对抗攻击研究多关注攻击性能优化，而忽略了攻击强度的量化，导致攻击选择缺乏可解释性。

Method: 提出噪声概念量化攻击强度，并基于分类边界设计单步和多步优化的三种攻击策略。

Result: 在基准数据集上的实验验证了所提策略的有效性，并分析了有效对抗扰动的偏好模式。

Conclusion: 通过量化攻击强度和设计策略，提升了对抗攻击的可解释性和效果。

Abstract: Graph neural networks have been widely utilized to solve graph-related tasks
because of their strong learning power in utilizing the local information of
neighbors. However, recent studies on graph adversarial attacks have proven
that current graph neural networks are not robust against malicious attacks.
Yet much of the existing work has focused on the optimization objective based
on attack performance to obtain (near) optimal perturbations, but paid less
attention to the strength quantification of each perturbation such as the
injection of a particular node/link, which makes the choice of perturbations a
black-box model that lacks interpretability. In this work, we propose the
concept of noise to quantify the attack strength of each adversarial link.
Furthermore, we propose three attack strategies based on the defined noise and
classification margins in terms of single and multiple steps optimization.
Extensive experiments conducted on benchmark datasets against three
representative graph neural networks demonstrate the effectiveness of the
proposed attack strategies. Particularly, we also investigate the preferred
patterns of effective adversarial perturbations by analyzing the corresponding
properties of the selected perturbation nodes.

</details>


### [147] [Return Capping: Sample-Efficient CVaR Policy Gradient Optimisation](https://arxiv.org/abs/2504.20887)
*Harry Mead,Clarissa Costen,Bruno Lacerda,Nick Hawes*

Main category: cs.LG

TL;DR: 论文提出了一种改进的条件风险价值（CVaR）优化方法，通过限制训练轨迹的总回报而非直接丢弃，提高了样本效率。


<details>
  <summary>Details</summary>
Motivation: 当前方法在优化CVaR时丢弃大量轨迹，导致样本效率低下。

Method: 提出一种问题重构方法，限制训练轨迹的总回报，而非简单丢弃，并证明在适当设置限制时与原问题等价。

Result: 在多个环境中实验表明，该方法相比基线方法性能持续提升。

Conclusion: 通过限制轨迹回报而非丢弃，显著提高了CVaR优化的样本效率和性能。

Abstract: When optimising for conditional value at risk (CVaR) using policy gradients
(PG), current methods rely on discarding a large proportion of trajectories,
resulting in poor sample efficiency. We propose a reformulation of the CVaR
optimisation problem by capping the total return of trajectories used in
training, rather than simply discarding them, and show that this is equivalent
to the original problem if the cap is set appropriately. We show, with
empirical results in an number of environments, that this reformulation of the
problem results in consistently improved performance compared to baselines.

</details>


### [148] [Does Feedback Help in Bandits with Arm Erasures?](https://arxiv.org/abs/2504.20894)
*Merve Karakas,Osama Hanna,Lin F. Yang,Christina Fragouli*

Main category: cs.LG

TL;DR: 研究了分布式多臂老虎机问题，发现反馈机制并未显著改善最坏情况下的遗憾上界。


<details>
  <summary>Details</summary>
Motivation: 研究在通信受限网络中多臂老虎机算法的应用，特别是通过反馈机制优化性能。

Method: 考虑代理可以向学习者反馈是否收到臂请求，学习者知道实际播放的臂。

Result: 证明反馈机制并未改善最坏情况下的遗憾上界，遗憾下界为Ω(√KT + K/(1-ε))。

Conclusion: 反馈机制虽简化了算法设计，但未显著提升性能，未来可优化常数项。

Abstract: We study a distributed multi-armed bandit (MAB) problem over arm erasure
channels, motivated by the increasing adoption of MAB algorithms over
communication-constrained networks. In this setup, the learner communicates the
chosen arm to play to an agent over an erasure channel with probability
$\epsilon \in [0,1)$; if an erasure occurs, the agent continues pulling the
last successfully received arm; the learner always observes the reward of the
arm pulled. In past work, we considered the case where the agent cannot convey
feedback to the learner, and thus the learner does not know whether the arm
played is the requested or the last successfully received one. In this paper,
we instead consider the case where the agent can send feedback to the learner
on whether the arm request was received, and thus the learner exactly knows
which arm was played. Surprisingly, we prove that erasure feedback does not
improve the worst-case regret upper bound order over the previously studied
no-feedback setting. In particular, we prove a regret lower bound of
$\Omega(\sqrt{KT} + K / (1 - \epsilon))$, where $K$ is the number of arms and
$T$ the time horizon, that matches no-feedback upper bounds up to logarithmic
factors. We note however that the availability of feedback enables simpler
algorithm designs that may achieve better constants (albeit not better order)
regret bounds; we design one such algorithm and evaluate its performance
numerically.

</details>


### [149] [Evaluating Generative Models for Tabular Data: Novel Metrics and Benchmarking](https://arxiv.org/abs/2504.20900)
*Dayananda Herurkar,Ahmad Ali,Andreas Dengel*

Main category: cs.LG

TL;DR: 论文提出三种新的评估指标（FAED、FPCAD、RFIS）用于表格数据生成模型，实验证明FAED能有效捕捉现有指标忽略的问题。


<details>
  <summary>Details</summary>
Motivation: 表格数据生成模型的评估存在挑战，现有指标不全面，需更全面的性能衡量方法。

Method: 提出FAED、FPCAD、RFIS三种新指标，并在三个标准网络入侵检测数据集上实验验证。

Result: FAED能有效捕捉生成模型问题，FPCAD表现有潜力但需改进，框架实用性强。

Conclusion: 新框架为表格数据生成模型评估提供了全面且实用的方法。

Abstract: Generative models have revolutionized multiple domains, yet their application
to tabular data remains underexplored. Evaluating generative models for tabular
data presents unique challenges due to structural complexity, large-scale
variability, and mixed data types, making it difficult to intuitively capture
intricate patterns. Existing evaluation metrics offer only partial insights,
lacking a comprehensive measure of generative performance. To address this
limitation, we propose three novel evaluation metrics: FAED, FPCAD, and RFIS.
Our extensive experimental analysis, conducted on three standard network
intrusion detection datasets, compares these metrics with established
evaluation methods such as Fidelity, Utility, TSTR, and TRTS. Our results
demonstrate that FAED effectively captures generative modeling issues
overlooked by existing metrics. While FPCAD exhibits promising performance,
further refinements are necessary to enhance its reliability. Our proposed
framework provides a robust and practical approach for assessing generative
models in tabular data applications.

</details>


### [150] [MOSIC: Model-Agnostic Optimal Subgroup Identification with Multi-Constraint for Improved Reliability](https://arxiv.org/abs/2504.20908)
*Wenxin Chen,Weishen Pan,Kyra Gan,Fei Wang*

Main category: cs.LG

TL;DR: 提出了一种模型无关的框架，用于在多约束条件下识别最优亚组，解决了现有方法无法同时处理多个约束的问题。


<details>
  <summary>Details</summary>
Motivation: 个性化医疗中，识别从特定治疗中受益的亚组是一个关键挑战。现有方法通常仅关注治疗效果，而忽略了亚组大小和混杂因素平衡等实际约束。

Method: 将组合问题重新表述为无约束的极小极大优化问题，并通过梯度下降上升算法求解，证明了其收敛性。

Result: 在合成和真实数据集上的实验表明，该方法能有效识别满足多约束的亚组，具有更高的治疗效果和更好的混杂因素平衡。

Conclusion: 该方法稳定且灵活，适用于多种模型和技术，为个性化医疗提供了可行的解决方案。

Abstract: Identifying subgroups that benefit from specific treatments using
observational data is a critical challenge in personalized medicine. Most
existing approaches solely focus on identifying a subgroup with an improved
treatment effect. However, practical considerations, such as ensuring a minimum
subgroup size for representativeness or achieving sufficient confounder balance
for reliability, are also important for making findings clinically meaningful
and actionable. While some studies address these constraints individually, none
offer a unified approach to handle them simultaneously. To bridge this gap, we
propose a model-agnostic framework for optimal subgroup identification under
multiple constraints. We reformulate this combinatorial problem as an
unconstrained min-max optimization problem with novel modifications and solve
it by a gradient descent ascent algorithm. We further prove its convergence to
a feasible and locally optimal solution. Our method is stable and highly
flexible, supporting various models and techniques for estimating and
optimizing treatment effectiveness with observational data. Extensive
experiments on both synthetic and real-world datasets demonstrate its
effectiveness in identifying subgroups that satisfy multiple constraints,
achieving higher treatment effects and better confounder balancing results
across different group sizes.

</details>


### [151] [Statistical and Predictive Analysis to Identify Risk Factors and Effects of Post COVID-19 Syndrome](https://arxiv.org/abs/2504.20915)
*Milad Leyli-abadi,Jean-Patrick Brunet,Axel Tahmasebimoradi*

Main category: cs.LG

TL;DR: 论文通过统计和预测分析，研究了长期COVID的影响因素，发现神经网络模型预测效果最佳，关键预测因素包括嗅觉丧失、头痛等。


<details>
  <summary>Details</summary>
Motivation: 研究长期COVID的持续症状及其影响因素，以帮助理解并提供针对性干预措施。

Method: 采用线性模型、随机森林、梯度提升和神经网络等方法，基于Lifelines COVID-19队列数据进行统计和预测分析。

Result: 神经网络模型表现最佳，平均预测误差为19%，关键预测因素包括嗅觉丧失、头痛、肌肉疼痛和疫苗接种时间。

Conclusion: 研究为理解长期COVID及其干预提供了重要见解，神经网络模型和关键因素分析具有实际应用价值。

Abstract: Based on recent studies, some COVID-19 symptoms can persist for months after
infection, leading to what is termed long COVID. Factors such as vaccination
timing, patient characteristics, and symptoms during the acute phase of
infection may contribute to the prolonged effects and intensity of long COVID.
Each patient, based on their unique combination of factors, develops a specific
risk or intensity of long COVID. In this work, we aim to achieve two
objectives: (1) conduct a statistical analysis to identify relationships
between various factors and long COVID, and (2) perform predictive analysis of
long COVID intensity using these factors. We benchmark and interpret various
data-driven approaches, including linear models, random forests, gradient
boosting, and neural networks, using data from the Lifelines COVID-19 cohort.
Our results show that Neural Networks (NN) achieve the best performance in
terms of MAPE, with predictions averaging 19\% error. Additionally,
interpretability analysis reveals key factors such as loss of smell, headache,
muscle pain, and vaccination timing as significant predictors, while chronic
disease and gender are critical risk factors. These insights provide valuable
guidance for understanding long COVID and developing targeted interventions.

</details>


### [152] [Improvements of Dark Experience Replay and Reservoir Sampling towards Better Balance between Consolidation and Plasticity](https://arxiv.org/abs/2504.20932)
*Taisuke Kobayashi*

Main category: cs.LG

TL;DR: 论文提出改进策略，优化DER和RS方法，以平衡记忆巩固与可塑性，提升持续学习性能。


<details>
  <summary>Details</summary>
Motivation: 解决持续学习中DER方法因多目标权重不当和错误输出抑制学习的问题，以及RS缓冲区因数据流停止存储新技能数据的隐藏权衡。

Method: 改进DER（自动权重调整、阻止错误数据回放、修正过去输出）和RS（泛化接受概率、分层多缓冲区、有意忽略不必要数据）。

Result: 在回归、分类和强化学习任务中验证了改进方法的有效性，实现了学习性能的稳定提升。

Conclusion: 通过平衡记忆巩固与可塑性，改进后的方法显著提升了持续学习的效果。

Abstract: Continual learning is the one of the most essential abilities for autonomous
agents, which can incrementally learn daily-life skills. For this ultimate
goal, a simple but powerful method, dark experience replay (DER), has been
proposed recently. DER mitigates catastrophic forgetting, in which the skills
acquired in the past are unintentionally forgotten, by stochastically storing
the streaming data in a reservoir sampling (RS) buffer and by relearning them
or retaining the past outputs for them. However, since DER considers multiple
objectives, it will not function properly without appropriate weighting of
them. In addition, the ability to retain past outputs inhibits learning if the
past outputs are incorrect due to distribution shift or other effects. This is
due to a tradeoff between memory consolidation and plasticity. The tradeoff is
hidden even in the RS buffer, which gradually stops storing new data for new
skills in it as data is continuously passed to it. To alleviate the tradeoff
and achieve better balance, this paper proposes improvement strategies to each
of DER and RS. Specifically, DER is improved with automatic adaptation of
weights, block of replaying erroneous data, and correction of past outputs. RS
is also improved with generalization of acceptance probability, stratification
of plural buffers, and intentional omission of unnecessary data. These
improvements are verified through multiple benchmarks including regression,
classification, and reinforcement learning problems. As a result, the proposed
methods achieve steady improvements in learning performance by balancing the
memory consolidation and plasticity.

</details>


### [153] [Scenario-based Compositional Verification of Autonomous Systems with Neural Perception](https://arxiv.org/abs/2504.20942)
*Christopher Watson,Rajeev Alur,Divya Gopinath,Ravi Mangal,Corina S. Pasareanu*

Main category: cs.LG

TL;DR: 提出了一种基于场景建模和概率抽象的自动驾驶系统概率验证框架，解决了深度神经网络感知的复杂性和环境变化带来的验证挑战。


<details>
  <summary>Details</summary>
Motivation: 由于感知深度神经网络的复杂性和环境条件难以量化，自动驾驶系统的形式化验证面临挑战。

Method: 通过场景建模将任务分解为不同环境条件的场景，为每个场景构建概率抽象，并利用符号推理和加速证明规则进行高效验证。

Result: 在两个案例研究中验证了方法的有效性：飞机滑行道引导系统和F1Tenth自动驾驶汽车模拟模型。

Conclusion: 该框架为复杂自动驾驶系统的验证提供了一种高效且可扩展的方法。

Abstract: Recent advances in deep learning have enabled the development of autonomous
systems that use deep neural networks for perception. Formal verification of
these systems is challenging due to the size and complexity of the perception
DNNs as well as hard-to-quantify, changing environment conditions. To address
these challenges, we propose a probabilistic verification framework for
autonomous systems based on the following key concepts: (1) Scenario-based
Modeling: We decompose the task (e.g., car navigation) into a composition of
scenarios, each representing a different environment condition. (2)
Probabilistic Abstractions: For each scenario, we build a compact abstraction
of perception based on the DNN's performance on an offline dataset that
represents the scenario's environment condition. (3) Symbolic Reasoning and
Acceleration: The abstractions enable efficient compositional verification of
the autonomous system via symbolic reasoning and a novel acceleration proof
rule that bounds the error probability of the system under arbitrary variations
of environment conditions. We illustrate our approach on two case studies: an
experimental autonomous system that guides airplanes on taxiways using
high-dimensional perception DNNs and a simulation model of an F1Tenth
autonomous car using LiDAR observations.

</details>


### [154] [Deep Learning Characterizes Depression and Suicidal Ideation from Eye Movements](https://arxiv.org/abs/2504.20944)
*Kleanthis Avramidis,Woojae Jeong,Aditya Kommineni,Sudarsana R. Kadiri,Marcus Ma,Colin McDaniel,Myzelle Hughes,Thomas McGee,Elsi Kaiser,Dani Byrd,Assal Habibi,B. Rael Cahn,Idan A. Blank,Kristina Lerman,Takfarinas Medani,Richard M. Leahy,Shrikanth Narayanan*

Main category: cs.LG

TL;DR: 研究探讨眼动追踪作为抑郁症和自杀意念的客观生物标志物，通过深度学习模型分析眼动数据，取得显著预测效果。


<details>
  <summary>Details</summary>
Motivation: 抑郁症和自杀意念缺乏客观生物标志物，当前诊断依赖主观报告，亟需客观筛查工具。

Method: 记录126名年轻成年人在阅读情感句子时的眼动数据，开发深度学习框架，分析正负情感句子的眼动模式。

Result: 模型对抑郁症和自杀意念的预测AUC分别为0.793和0.826，对区分抑郁与自杀参与者AUC为0.609。

Conclusion: 眼动追踪可作为心理健康评估的客观工具，情感刺激对眼动控制有显著调节作用。

Abstract: Identifying physiological and behavioral markers for mental health conditions
is a longstanding challenge in psychiatry. Depression and suicidal ideation, in
particular, lack objective biomarkers, with screening and diagnosis primarily
relying on self-reports and clinical interviews. Here, we investigate eye
tracking as a potential marker modality for screening purposes. Eye movements
are directly modulated by neuronal networks and have been associated with
attentional and mood-related patterns; however, their predictive value for
depression and suicidality remains unclear. We recorded eye-tracking sequences
from 126 young adults as they read and responded to affective sentences, and
subsequently developed a deep learning framework to predict their clinical
status. The proposed model included separate branches for trials of positive
and negative sentiment, and used 2D time-series representations to account for
both intra-trial and inter-trial variations. We were able to identify
depression and suicidal ideation with an area under the receiver operating
curve (AUC) of 0.793 (95% CI: 0.765-0.819) against healthy controls, and
suicidality specifically with 0.826 AUC (95% CI: 0.797-0.852). The model also
exhibited moderate, yet significant, accuracy in differentiating depressed from
suicidal participants, with 0.609 AUC (95% CI 0.571-0.646). Discriminative
patterns emerge more strongly when assessing the data relative to response
generation than relative to the onset time of the final word of the sentences.
The most pronounced effects were observed for negative-sentiment sentences,
that are congruent to depressed and suicidal participants. Our findings
highlight eye tracking as an objective tool for mental health assessment and
underscore the modulatory impact of emotional stimuli on cognitive processes
affecting oculomotor control.

</details>


### [155] [AegisLLM: Scaling Agentic Systems for Self-Reflective Defense in LLM Security](https://arxiv.org/abs/2504.20965)
*Zikui Cai,Shayan Shabihi,Bang An,Zora Che,Brian R. Bartoldson,Bhavya Kailkhura,Tom Goldstein,Furong Huang*

Main category: cs.LG

TL;DR: AegisLLM是一种多代理防御系统，通过协作代理（如协调器、偏转器、响应器和评估器）保护LLM免受对抗攻击和信息泄露，同时通过提示优化自我改进。


<details>
  <summary>Details</summary>
Motivation: 解决对抗攻击和信息泄露问题，提供实时适应性防御，无需模型重新训练。

Method: 采用多代理协作框架，结合自动化提示优化（如DSPy），增强鲁棒性。

Result: 在WMDP遗忘基准测试中表现优异（仅需20个训练样本和300次LM调用），在越狱基准测试中提升51%性能，误拒率显著降低。

Conclusion: AegisLLM展示了自适应代理推理的优势，是传统模型修改方法的有效运行时替代方案。

Abstract: We introduce AegisLLM, a cooperative multi-agent defense against adversarial
attacks and information leakage. In AegisLLM, a structured workflow of
autonomous agents - orchestrator, deflector, responder, and evaluator -
collaborate to ensure safe and compliant LLM outputs, while self-improving over
time through prompt optimization. We show that scaling agentic reasoning system
at test-time - both by incorporating additional agent roles and by leveraging
automated prompt optimization (such as DSPy)- substantially enhances robustness
without compromising model utility. This test-time defense enables real-time
adaptability to evolving attacks, without requiring model retraining.
Comprehensive evaluations across key threat scenarios, including unlearning and
jailbreaking, demonstrate the effectiveness of AegisLLM. On the WMDP unlearning
benchmark, AegisLLM achieves near-perfect unlearning with only 20 training
examples and fewer than 300 LM calls. For jailbreaking benchmarks, we achieve
51% improvement compared to the base model on StrongReject, with false refusal
rates of only 7.9% on PHTest compared to 18-55% for comparable methods. Our
results highlight the advantages of adaptive, agentic reasoning over static
defenses, establishing AegisLLM as a strong runtime alternative to traditional
approaches based on model modifications. Code is available at
https://github.com/zikuicai/aegisllm

</details>


### [156] [Softpick: No Attention Sink, No Massive Activations with Rectified Softmax](https://arxiv.org/abs/2504.20966)
*Zayd M. K. Zuhri,Erland Hilman Fuadi,Alham Fikri Aji*

Main category: cs.LG

TL;DR: Softpick是一种替代softmax的注意力机制，解决了注意力下沉和大激活问题，保持性能的同时提升稀疏性和量化表现。


<details>
  <summary>Details</summary>
Motivation: 解决传统softmax在注意力机制中导致的注意力下沉和大激活问题，为量化、低精度训练和稀疏优化提供新可能。

Method: 提出softpick作为softmax的替代方案，通过实验验证其在340M参数模型中的表现。

Result: softpick在标准基准测试中与softmax性能相当，但实现了0%的下沉率、更低的峰度和更高的稀疏性（46.97%）。量化后表现更优。

Conclusion: softpick为量化、低精度训练和稀疏优化等领域提供了新的可能性，代码已开源。

Abstract: We introduce softpick, a rectified, not sum-to-one, drop-in replacement for
softmax in transformer attention mechanisms that eliminates attention sink and
massive activations. Our experiments with 340M parameter models demonstrate
that softpick maintains performance parity with softmax on standard benchmarks
while achieving 0% sink rate. The softpick transformer produces hidden states
with significantly lower kurtosis (340 vs 33,510) and creates sparse attention
maps (46.97% sparsity). Models using softpick consistently outperform softmax
when quantized, with particularly pronounced advantages at lower bit
precisions. Our analysis and discussion shows how softpick has the potential to
open new possibilities for quantization, low-precision training, sparsity
optimization, pruning, and interpretability. Our code is available at
https://github.com/zaydzuhri/softpick-attention.

</details>


### [157] [Equivariant non-linear maps for neural networks on homogeneous spaces](https://arxiv.org/abs/2504.20974)
*Elias Nyholm,Oscar Carlsson,Maurice Weiler,Daniel Persson*

Main category: cs.LG

TL;DR: 本文提出了一种新颖的非线性等变神经网络层框架，适用于齐次空间。通过推广线性设置下的等变性约束，作者提出了广义的可操纵性约束，并证明了其普适性。


<details>
  <summary>Details</summary>
Motivation: 受非线性层（如自注意力或输入依赖核）在实践中的成功启发，作者希望将线性等变性层的理论推广到非线性设置。

Method: 作者推导了广义的可操纵性约束，证明了其普适性，并展示了如何从该框架中衍生出多种常见的等变网络架构。

Result: 该框架为设计未来的等变神经网络层提供了理论支持，并成功应用于多种现有架构。

Conclusion: 本文的框架为非线性等变神经网络层的设计提供了理论基础，并展示了其广泛适用性。

Abstract: This paper presents a novel framework for non-linear equivariant neural
network layers on homogeneous spaces. The seminal work of Cohen et al. on
equivariant $G$-CNNs on homogeneous spaces characterized the representation
theory of such layers in the linear setting, finding that they are given by
convolutions with kernels satisfying so-called steerability constraints.
Motivated by the empirical success of non-linear layers, such as self-attention
or input dependent kernels, we set out to generalize these insights to the
non-linear setting. We derive generalized steerability constraints that any
such layer needs to satisfy and prove the universality of our construction. The
insights gained into the symmetry-constrained functional dependence of
equivariant operators on feature maps and group elements informs the design of
future equivariant neural network layers. We demonstrate how several common
equivariant network architectures - $G$-CNNs, implicit steerable kernel
networks, conventional and relative position embedded attention based
transformers, and LieTransformers - may be derived from our framework.

</details>


### [158] [Hubs and Spokes Learning: Efficient and Scalable Collaborative Machine Learning](https://arxiv.org/abs/2504.20988)
*Atul Sharma,Kavindu Herath,Saurabh Bagchi,Chaoyue Liu,Somali Chaterji*

Main category: cs.LG

TL;DR: HSL是一种结合联邦学习和去中心化学习优势的新型协作学习框架，通过双层通信结构避免单点故障，并在相同通信预算下优于现有P2PL框架ELL。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习的单点故障问题，同时提升去中心化学习的性能，适用于资源受限系统。

Method: 采用双层通信结构（Hubs和Spokes），在相同或更低通信预算下实现更高性能。

Result: HSL在400条边时达到ELL需1000条边才能实现的CIFAR-10测试准确率，且节点间共识更强。

Conclusion: HSL是一种高效、实用的协作学习框架，适合大规模应用。

Abstract: We introduce the Hubs and Spokes Learning (HSL) framework, a novel paradigm
for collaborative machine learning that combines the strengths of Federated
Learning (FL) and Decentralized Learning (P2PL). HSL employs a two-tier
communication structure that avoids the single point of failure inherent in FL
and outperforms the state-of-the-art P2PL framework, Epidemic Learning Local
(ELL). At equal communication budgets (total edges), HSL achieves higher
performance than ELL, while at significantly lower communication budgets, it
can match ELL's performance. For instance, with only 400 edges, HSL reaches the
same test accuracy that ELL achieves with 1000 edges for 100 peers (spokes) on
CIFAR-10, demonstrating its suitability for resource-constrained systems. HSL
also achieves stronger consensus among nodes after mixing, resulting in
improved performance with fewer training rounds. We substantiate these claims
through rigorous theoretical analyses and extensive experimental results,
showcasing HSL's practicality for large-scale collaborative learning.

</details>


### [159] [Toward Efficient Exploration by Large Language Model Agents](https://arxiv.org/abs/2504.20997)
*Dilip Arumugam,Thomas L. Griffiths*

Main category: cs.LG

TL;DR: 论文探讨了如何利用大型语言模型（LLMs）实现数据高效的强化学习（RL），提出了一种基于LLM的显式RL算法实现，以解决探索难题。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的自主决策代理在数据高效RL中面临探索难题，而传统RL算法难以在纯自然语言环境中实现。

Method: 采用显式实现RL算法（后验采样）而非微调或上下文学习，利用LLM实现数据高效的探索。

Result: 实验表明，该方法在需要谨慎探索的自然语言任务中显著优于其他LLM代理设计。

Conclusion: 通过显式实现已知RL算法，LLM代理在数据效率和探索能力上取得了显著提升。

Abstract: A burgeoning area within reinforcement learning (RL) is the design of
sequential decision-making agents centered around large language models (LLMs).
While autonomous decision-making agents powered by modern LLMs could facilitate
numerous real-world applications, such successes demand agents that are capable
of data-efficient RL. One key obstacle to achieving data efficiency in RL is
exploration, a challenge that we demonstrate many recent proposals for LLM
agent designs struggle to contend with. Meanwhile, classic algorithms from the
RL literature known to gracefully address exploration require technical
machinery that can be challenging to operationalize in purely natural language
settings. In this work, rather than relying on finetuning or in-context
learning to coax LLMs into implicitly imitating a RL algorithm, we illustrate
how LLMs can be used to explicitly implement an existing RL algorithm
(Posterior Sampling for Reinforcement Learning) whose capacity for
statistically-efficient exploration is already well-studied. We offer empirical
results demonstrating how our LLM-based implementation of a known,
data-efficient RL algorithm can be considerably more effective in natural
language tasks that demand prudent exploration.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [160] [CarbonCall: Sustainability-Aware Function Calling for Large Language Models on Edge Devices](https://arxiv.org/abs/2504.20348)
*Varatheepan Paramanayakam,Andreas Karatzas,Iraklis Anagnostopoulos,Dimitrios Stamoulis*

Main category: cs.PF

TL;DR: CarbonCall是一个可持续性感知的函数调用框架，通过动态工具选择、碳感知执行和量化LLM适配，显著降低碳排放和功耗。


<details>
  <summary>Details</summary>
Motivation: 现有方法在优化性能时忽视了可持续性，导致高能耗和碳排放，不适用于能源受限环境。

Method: CarbonCall整合动态工具选择、碳感知执行和量化LLM适配，根据实时碳强度预测调整功率阈值，并在模型变体间切换。

Result: 在NVIDIA Jetson AGX Orin上的实验显示，CarbonCall减少碳排放52%、功耗30%、执行时间30%，同时保持高效。

Conclusion: CarbonCall为边缘AI系统提供了一种高效且可持续的函数调用解决方案。

Abstract: Large Language Models (LLMs) enable real-time function calling in edge AI
systems but introduce significant computational overhead, leading to high power
consumption and carbon emissions. Existing methods optimize for performance
while neglecting sustainability, making them inefficient for energy-constrained
environments. We introduce CarbonCall, a sustainability-aware function-calling
framework that integrates dynamic tool selection, carbon-aware execution, and
quantized LLM adaptation. CarbonCall adjusts power thresholds based on
real-time carbon intensity forecasts and switches between model variants to
sustain high tokens-per-second throughput under power constraints. Experiments
on an NVIDIA Jetson AGX Orin show that CarbonCall reduces carbon emissions by
up to 52%, power consumption by 30%, and execution time by 30%, while
maintaining high efficiency.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [161] [Heterogeneous network drug-target interaction prediction model based on graph wavelet transform and multi-level contrastive learning](https://arxiv.org/abs/2504.20103)
*Wenfeng Dai,Yanhong Wang,Shuai Yan,Qingzhi Yu,Xiang Cheng*

Main category: q-bio.QM

TL;DR: 提出了一种结合图神经网络和多尺度信号处理的药物-靶点相互作用预测框架，具有高效预测和多层次可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决传统机器学习方法在药物-靶点相互作用预测中的黑盒问题，揭示模型决策机制与生物分子相互作用模式的深层关联。

Method: 整合异构图卷积神经网络（HGCN）和多尺度图信号分解技术，设计局部-全局特征协同感知模块、多尺度图信号分解与生物解释模块，以及对比学习模型。

Result: 在所有数据集上表现出优异的预测性能，为药物靶点发现提供了从黑盒预测到机制解码的完整解决方案。

Conclusion: 该框架为复杂生物分子相互作用系统建模提供了重要参考价值。

Abstract: Drug-target interaction (DTI) prediction is a core task in drug development
and precision medicine in the biomedical field. However, traditional machine
learning methods generally have the black box problem, which makes it difficult
to reveal the deep correlation between the model decision mechanism and the
interaction pattern between biological molecules. This study proposes a
heterogeneous network drug target interaction prediction framework, integrating
graph neural network and multi scale signal processing technology to construct
a model with both efficient prediction and multi level interpretability. Its
technical breakthroughs are mainly reflected in the following three
dimensions:Local global feature collaborative perception module. Based on
heterogeneous graph convolutional neural network (HGCN), a multi order neighbor
aggregation strategy is designed.Multi scale graph signal decomposition and
biological interpretation module. A deep hierarchical node feature transform
(GWT) architecture is proposed.Contrastive learning combining multi dimensional
perspectives and hierarchical representations. By comparing the learning
models, the node representations from the two perspectives of HGCN and GWT are
aligned and fused, so that the model can integrate multi dimensional
information and improve the prediction robustness. Experimental results show
that our framework shows excellent prediction performance on all datasets. This
study provides a complete solution for drug target discovery from black box
prediction to mechanism decoding, and its methodology has important reference
value for modeling complex biomolecular interaction systems.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [162] [Nonlinear Computation with Linear Optics via Source-Position Encoding](https://arxiv.org/abs/2504.20401)
*N. Richardson,C. Bosch,R. P. Adams*

Main category: physics.optics

TL;DR: 提出了一种在完全线性介质中实现非线性计算的新方法，通过位置编码和拓扑优化设计高效的光学神经网络。


<details>
  <summary>Details</summary>
Motivation: 光学计算系统适合神经网络工作负载，但缺乏高效的非线性实现方法。

Method: 利用位置编码和拓扑优化设计光学神经网络，实现低功耗非线性计算。

Result: 在分类任务中表现优于线性方法，与标准人工神经网络性能相当。

Conclusion: 该方法为光学神经网络提供了高效的非线性计算解决方案。

Abstract: Optical computing systems provide an alternate hardware model which appears
to be aligned with the demands of neural network workloads. However, the
challenge of implementing energy efficient nonlinearities in optics -- a key
requirement for realizing neural networks -- is a conspicuous missing link. In
this work we introduce a novel method to achieve nonlinear computation in fully
linear media. Our method can operate at low power and requires only the ability
to drive the optical system at a data-dependent spatial position. Leveraging
this positional encoding, we formulate a fully automated,
topology-optimization-based hardware design framework for extremely specialized
optical neural networks, drawing on modern advancements in optimization and
machine learning. We evaluate our optical designs on machine learning
classification tasks: demonstrating significant improvements over linear
methods, and competitive performance when compared to standard artificial
neural networks.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [163] [Generate more than one child in your co-evolutionary semi-supervised learning GAN](https://arxiv.org/abs/2504.20560)
*Francisco Sedeño,Jamal Toutouh,Francisco Chicano*

Main category: cs.NE

TL;DR: 论文提出了一种新的协同进化方法CE-SSLGAN，通过泛种群、精英替换和多子代策略改进传统SSL-GAN的性能。


<details>
  <summary>Details</summary>
Motivation: 传统SSL-GAN的协同进化方法基于空间结构和单子代策略，限制了性能提升。

Method: 提出CE-SSLGAN，采用泛种群、精英替换和多子代策略。

Result: 在三个标准数据集上验证，性能优于传统SSL-GAN。

Conclusion: 多子代和精英替换策略能有效提升SSL-GAN的性能。

Abstract: Generative Adversarial Networks (GANs) are very useful methods to address
semi-supervised learning (SSL) datasets, thanks to their ability to generate
samples similar to real data. This approach, called SSL-GAN has attracted many
researchers in the last decade. Evolutionary algorithms have been used to guide
the evolution and training of SSL-GANs with great success. In particular,
several co-evolutionary approaches have been applied where the two networks of
a GAN (the generator and the discriminator) are evolved in separate
populations. The co-evolutionary approaches published to date assume some
spatial structure of the populations, based on the ideas of cellular
evolutionary algorithms. They also create one single individual per generation
and follow a generational replacement strategy in the evolution. In this paper,
we re-consider those algorithmic design decisions and propose a new
co-evolutionary approach, called Co-evolutionary Elitist SSL-GAN (CE-SSLGAN),
with panmictic population, elitist replacement, and more than one individual in
the offspring. We evaluate the performance of our proposed method using three
standard benchmark datasets. The results show that creating more than one
offspring per population and using elitism improves the results in comparison
with a classical SSL-GAN.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [164] [TrueFake: A Real World Case Dataset of Last Generation Fake Images also Shared on Social Networks](https://arxiv.org/abs/2504.20658)
*Stefano Dell'Anna,Andrea Montibeller,Giulia Boato*

Main category: cs.MM

TL;DR: TrueFake是一个包含60万张图像的大规模基准数据集，用于评估虚假图像检测器在真实社交媒体环境中的性能。


<details>
  <summary>Details</summary>
Motivation: AI生成的合成媒体被广泛用于传播虚假信息，而现有检测工具未能充分应对社交媒体压缩等现实挑战。

Method: 构建TrueFake数据集，涵盖多种生成技术和社交媒体共享场景，并进行大量实验分析检测性能。

Result: 实验表明社交媒体共享显著影响检测性能，并确定了当前最有效的检测和训练策略。

Conclusion: 研究强调需要在真实世界条件下评估法证模型。

Abstract: AI-generated synthetic media are increasingly used in real-world scenarios,
often with the purpose of spreading misinformation and propaganda through
social media platforms, where compression and other processing can degrade fake
detection cues. Currently, many forensic tools fail to account for these
in-the-wild challenges. In this work, we introduce TrueFake, a large-scale
benchmarking dataset of 600,000 images including top notch generative
techniques and sharing via three different social networks. This dataset allows
for rigorous evaluation of state-of-the-art fake image detectors under very
realistic and challenging conditions. Through extensive experimentation, we
analyze how social media sharing impacts detection performance, and identify
current most effective detection and training strategies. Our findings
highlight the need for evaluating forensic models in conditions that mirror
real-world use.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [165] [Energy-Based Coarse-Graining in Molecular Dynamics: A Flow-Based Framework Without Data](https://arxiv.org/abs/2504.20940)
*Maximilian Stupp,P. S. Koutsourelakis*

Main category: physics.chem-ph

TL;DR: 论文提出了一种无需数据的生成框架，用于粗粒化分子模拟，直接针对全原子玻尔兹曼分布，避免了传统方法对大量模拟数据的依赖。


<details>
  <summary>Details</summary>
Motivation: 传统粗粒化方法依赖大量全原子分子动力学模拟数据，限制了模型的准确性和泛化性。本文旨在开发一种无需数据的方法，直接生成全原子样本。

Method: 通过定义包含慢速和快速变量的结构化潜在空间，结合可学习的双射映射和基于能量的目标函数，训练模型以最小化反向KL散度。

Result: 模型在合成系统和丙氨酸二肽基准测试中成功捕捉了玻尔兹曼分布的所有相关模式，并准确重构了原子构型。

Conclusion: 该方法无需模拟数据即可生成无偏的全原子样本，并学习到物理意义明确的粗粒化表示。

Abstract: Coarse-grained (CG) models offer an effective route to reducing the
complexity of molecular simulations, yet conventional approaches depend heavily
on long all-atom molecular dynamics (MD) trajectories to adequately sample
configurational space. This data-driven dependence limits their accuracy and
generalizability, as unvisited configurations remain excluded from the
resulting CG model. We introduce a data-free generative framework for
coarse-graining that directly targets the all-atom Boltzmann distribution. Our
model defines a structured latent space comprising slow collective variables,
which are statistically associated with multimodal marginal densities capturing
metastable states, and fast variables, which represent the remaining degrees of
freedom with simple, unimodal conditional distributions. A potentially
learnable, bijective map from the full latent space to the all-atom
configuration space enables automatic and accurate reconstruction of molecular
structures. The model is trained using an energy-based objective that minimizes
the reverse Kullback-Leibler divergence, relying solely on the interatomic
potential rather than sampled trajectories. A tempering scheme is used to
stabilize training and promote exploration of diverse configurations. Once
trained, the model can generate unbiased, one-shot equilibrium all-atom
samples. We validate the method on two synthetic systems-a double-well
potential and a Gaussian mixture-as well as on the benchmark alanine dipeptide.
The model captures all relevant modes of the Boltzmann distribution, accurately
reconstructs atomic configurations, and learns physically meaningful
coarse-grained representations, all without any simulation data.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [166] [On Stochastic Rounding with Few Random Bits](https://arxiv.org/abs/2504.20634)
*Andrew Fitzgibbon,Stephen Felix*

Main category: math.NA

TL;DR: 本文研究了低精度浮点格式和混合精度算术中使用的随机舍入（SR）技术，探讨了少位随机舍入（FBSR）的实现及其可能引入的偏差，并在机器学习示例中分析了这些偏差的影响。


<details>
  <summary>Details</summary>
Motivation: 随着大规模数值计算越来越多地使用低精度浮点格式和混合精度算术，随机舍入技术可以增强其性能。然而，高质量的随机位生成会增加计算成本，因此需要研究如何在保持SR优势的同时减少随机位需求。

Method: 本文研究了多种少位随机舍入（FBSR）的实现方式，分析了这些实现可能引入的偏差，并与无限位、无限精度的情况进行了对比。

Result: 研究发现，某些自然实现的FBSR会引入显著偏差，而这些偏差在无限位情况下不存在。这些偏差在机器学习示例中具有实际影响。

Conclusion: 本文揭示了低精度浮点计算中少位随机舍入的潜在偏差问题，为开发者和使用者提供了新的配置参数考虑方向。

Abstract: Large-scale numerical computations make increasing use of low-precision (LP)
floating point formats and mixed precision arithmetic, which can be enhanced by
the technique of stochastic rounding (SR), that is, rounding an intermediate
high-precision value up or down randomly as a function of the value's distance
to the two rounding candidates. Stochastic rounding requires, in addition to
the high-precision input value, a source of random bits. As the provision of
high-quality random bits is an additional computational cost, it is of interest
to require as few bits as possible while maintaining the desirable properties
of SR in a given computation, or computational domain. This paper examines a
number of possible implementations of few-bit stochastic rounding (FBSR), and
shows how several natural implementations can introduce sometimes significant
bias into the rounding process, which are not present in the case of
infinite-bit, infinite-precision examinations of these implementations. The
paper explores the impact of these biases in machine learning examples, and
hence opens another class of configuration parameters of which practitioners
should be aware when developing or adopting low-precision floating point. Code
is available at
http://github.com/graphcore-research/arith25-stochastic-rounding.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [167] [SCOPE-MRI: Bankart Lesion Detection as a Case Study in Data Curation and Deep Learning for Challenging Diagnoses](https://arxiv.org/abs/2504.20405)
*Sahil Sethi,Sai Reddy,Mansi Sakarvadia,Jordan Serotte,Darlington Nwaudo,Nicholas Maassen,Lewis Shi*

Main category: eess.IV

TL;DR: ScopeMRI是首个公开的肩部病理专家标注数据集，结合深度学习模型在标准MRI和MRA上检测Bankart病变，性能达到放射科医生水平。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注易于诊断的病理，而Bankart病变因影像特征细微常依赖侵入性MRA。本研究旨在解决这一临床难题。

Method: 使用ScopeMRI数据集（586例MRI），结合CNN和Transformer训练模型，并通过多视图集成优化性能。

Result: 模型在标准MRI和MRA上的AUC分别为0.91和0.93，性能媲美放射科医生。

Conclusion: 深度学习模型可减少对侵入性MRA的依赖，ScopeMRI的发布将加速肌肉骨骼影像研究。

Abstract: While deep learning has shown strong performance in musculoskeletal imaging,
existing work has largely focused on pathologies where diagnosis is not a
clinical challenge, leaving more difficult problems underexplored, such as
detecting Bankart lesions (anterior-inferior glenoid labral tears) on standard
MRIs. Diagnosing these lesions is challenging due to their subtle imaging
features, often leading to reliance on invasive MRI arthrograms (MRAs). This
study introduces ScopeMRI, the first publicly available, expert-annotated
dataset for shoulder pathologies, and presents a deep learning (DL) framework
for detecting Bankart lesions on both standard MRIs and MRAs. ScopeMRI includes
586 shoulder MRIs (335 standard, 251 MRAs) from 558 patients who underwent
arthroscopy. Ground truth labels were derived from intraoperative findings, the
gold standard for diagnosis. Separate DL models for MRAs and standard MRIs were
trained using a combination of CNNs and transformers. Predictions from
sagittal, axial, and coronal views were ensembled to optimize performance. The
models were evaluated on a 20% hold-out test set (117 MRIs: 46 MRAs, 71
standard MRIs). The models achieved an AUC of 0.91 and 0.93, sensitivity of 83%
and 94%, and specificity of 91% and 86% for standard MRIs and MRAs,
respectively. Notably, model performance on non-invasive standard MRIs matched
or surpassed radiologists interpreting MRAs. External validation demonstrated
initial generalizability across imaging protocols. This study demonstrates that
DL models can achieve radiologist-level diagnostic performance on standard
MRIs, reducing the need for invasive MRAs. By releasing ScopeMRI and a modular
codebase for training and evaluating deep learning models on 3D medical imaging
data, we aim to accelerate research in musculoskeletal imaging and support the
development of new datasets for clinically challenging diagnostic tasks.

</details>


### [168] [Full-field surrogate modeling of cardiac function encoding geometric variability](https://arxiv.org/abs/2504.20479)
*Elena Martinez,Beatrice Moscoloni,Matteo Salvador,Fanwei Kong,Mathias Peirlinck,Alison Lesley Marsden*

Main category: eess.IV

TL;DR: 论文提出了一种结合物理建模与数据驱动方法的新计算流程，用于构建心脏功能的通用替代模型，解决了现有模型需针对不同患者重新训练的问题。


<details>
  <summary>Details</summary>
Motivation: 将计算医学方法应用于心脏病临床实践需要结合物理建模与数据驱动方法，但现有替代模型通常针对特定几何形状，难以泛化。

Method: 采用多尺度数学模型生成电生理模拟数据，利用BLNMs编码激活图，并通过统计形状建模生成合成几何数据集用于训练。

Result: 替代模型在复杂患者队列中表现出强大的泛化能力，平均无维度均方误差为0.0034。

Conclusion: 提出的方法为心脏功能的通用替代模型提供了可行方案，代码已开源。

Abstract: Combining physics-based modeling with data-driven methods is critical to
enabling the translation of computational methods to clinical use in
cardiology. The use of rigorous differential equations combined with machine
learning tools allows for model personalization with uncertainty quantification
in time frames compatible with clinical practice. However, accurate and
efficient surrogate models of cardiac function, built from physics-based
numerical simulation, are still mostly geometry-specific and require retraining
for different patients and pathological conditions. We propose a novel
computational pipeline to embed cardiac anatomies into full-field surrogate
models. We generate a dataset of electrophysiology simulations using a complex
multi-scale mathematical model coupling partial and ordinary differential
equations. We adopt Branched Latent Neural Maps (BLNMs) as an effective
scientific machine learning method to encode activation maps extracted from
physics-based numerical simulations into a neural network. Leveraging large
deformation diffeomorphic metric mappings, we build a biventricular anatomical
atlas and parametrize the anatomical variability of a small and challenging
cohort of 13 pediatric patients affected by Tetralogy of Fallot. We propose a
novel statistical shape modeling based z-score sampling approach to generate a
new synthetic cohort of 52 biventricular geometries that are compatible with
the original geometrical variability. This synthetic cohort acts as the
training set for BLNMs. Our surrogate model demonstrates robustness and great
generalization across the complex original patient cohort, achieving an average
adimensional mean squared error of 0.0034. The Python implementation of our
BLNM model is publicly available under MIT License at
https://github.com/StanfordCBCL/BLNM.

</details>


### [169] [Quality-factor inspired deep neural network solver for solving inverse scattering problems](https://arxiv.org/abs/2504.20504)
*Yutong Du,Zicheng Liu,Miao Cao,Zupeng Liang,Yali Zong,Changyou Li*

Main category: eess.IV

TL;DR: 论文提出了一种基于质量因子的深度神经网络（QuaDNN）求解器，通过优化训练数据集、改进网络结构和设计损失函数，显著提升了电磁逆散射问题的成像性能。


<details>
  <summary>Details</summary>
Motivation: 电磁逆散射问题（ISPs）的成像性能受训练数据集、网络结构和损失函数影响，论文旨在通过优化这些因素提升成像质量。

Method: 定义了质量因子优化训练数据集；结合残差连接和通道注意力机制改进网络结构；设计包含数据拟合误差、物理信息约束和期望特征的损失函数。

Result: 数值分析和实验验证表明，QuaDNN求解器能有效抑制背景伪影并提高重建精度。

Conclusion: QuaDNN通过综合优化训练数据、网络结构和损失函数，显著提升了电磁逆散射问题的成像性能。

Abstract: Deep neural networks have been applied to address electromagnetic inverse
scattering problems (ISPs) and shown superior imaging performances, which can
be affected by the training dataset, the network architecture and the applied
loss function. Here, the quality of data samples is cared and valued by the
defined quality factor. Based on the quality factor, the composition of the
training dataset is optimized. The network architecture is integrated with the
residual connections and channel attention mechanism to improve feature
extraction. A loss function that incorporates data-fitting error,
physical-information constraints and the desired feature of the solution is
designed and analyzed to suppress the background artifacts and improve the
reconstruction accuracy. Various numerical analysis are performed to
demonstrate the superiority of the proposed quality-factor inspired deep neural
network (QuaDNN) solver and the imaging performance is finally verified by
experimental imaging test.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [170] [Learning Hierarchical Interaction for Accurate Molecular Property Prediction](https://arxiv.org/abs/2504.20127)
*Huiyang Hong,Xinkai Wu,Hongyu Sun,Qi Wang,Yuquan Li*

Main category: q-bio.BM

TL;DR: 论文提出了一种名为HimNet的新模型，通过分层交互消息传递机制（Hierarchical Interaction Message Passing Mechanism）解决现有深度学习方法在分子属性预测中未能有效利用分子结构层次性的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如GNN和Transformer）在预测分子属性时未能高效捕捉分子结构的层次性，且缺乏多级特征间的有效交互机制。

Method: 提出HimNet模型，通过分层注意力引导的消息传递机制，实现原子、基序和分子层面的交互感知表示学习。

Result: 在多个基准数据集上的实验表明，HimNet在大多数分子属性预测任务中表现最佳或接近最佳，并展现出良好的层次可解释性。

Conclusion: HimNet为分子活性和ADMET属性预测提供了准确高效的解决方案，对药物发现早期阶段的决策具有重要意义。

Abstract: Discovering molecules with desirable molecular properties, including ADMET
(Absorption, Distribution, Metabolism, Excretion, and Toxicity) profiles, is of
great importance in drug discovery. Existing approaches typically employ deep
learning models, such as Graph Neural Networks (GNNs) and Transformers, to
predict these molecular properties by learning from diverse chemical
information. However, these models often fail to efficiently capture and
utilize the hierarchical nature of molecular structures, and lack mechanisms
for effective interaction among multi-level features. To address these
limitations, we propose a Hierarchical Interaction Message Passing Mechanism,
which serves as the foundation of our novel model, HimNet. Our method enables
interaction-aware representation learning across atomic, motif, and molecular
levels via hierarchical attention-guided message passing. This design allows
HimNet to effectively balance global and local information, ensuring rich and
task-relevant feature extraction for downstream property prediction tasks, such
as Blood-Brain Barrier Permeability (BBBP). Extensive experiments on multiple
benchmark datasets demonstrate that HimNet achieves the best or near-best
performance in most molecular property prediction tasks. Furthermore, our
method exhibits promising hierarchical interpretability, aligning well with
chemical intuition on representative molecules. We believe that HimNet offers
an accurate and efficient solution for molecular activity and ADMET property
prediction, contributing significantly to advanced decision-making in the early
stages of drug discovery.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [171] [Partial Answer of How Transformers Learn Automata](https://arxiv.org/abs/2504.20395)
*Tiantian,Zhang*

Main category: cs.FL

TL;DR: 提出了一种基于表示理论半直积和傅里叶模块的有限自动机模拟框架，提升了基于Transformer的实现效率。


<details>
  <summary>Details</summary>
Motivation: 通过结合表示理论和傅里叶分析，优化有限自动机的模拟效率，特别是在Transformer架构中的应用。

Method: 使用表示理论的半直积和傅里叶模块构建框架，并将其应用于Transformer实现。

Result: 实现了更高效的基于Transformer的有限自动机模拟。

Conclusion: 该框架为有限自动机的模拟提供了一种高效的新方法，尤其在Transformer架构中表现优异。

Abstract: We introduce a novel framework for simulating finite automata using
representation-theoretic semidirect products and Fourier modules, achieving
more efficient Transformer-based implementations.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [172] [Towards Large Language Models for Lunar Mission Planning and In Situ Resource Utilization](https://arxiv.org/abs/2504.20125)
*Michael Pekala,Gregory Canal,Samuel Barham,Milena B. Graziano,Morgan Trexler,Leslie Hamilton,Elizabeth Reilly,Christopher D. Stiles*

Main category: cs.DL

TL;DR: 利用LLM从科学文献中快速提取月球成分数据，评估其可行性，发现现有模型对表格数据提取有效，但需进一步优化以捕捉更精细的信息。


<details>
  <summary>Details</summary>
Motivation: 月球任务规划需要评估当地原材料可用性，但相关数据分散在科学文献中，需快速提取和分析。

Method: 利用LLM处理科学文献语料库，提取月球成分数据，重点关注准确性和不确定性量化。

Result: 现成LLM对表格数据提取有效，但在捕捉精细矿物学信息和复杂信息方面仍有改进空间。

Conclusion: LLM在月球成分数据提取中具有潜力，但需进一步优化以提升性能。

Abstract: A key factor for lunar mission planning is the ability to assess the local
availability of raw materials. However, many potentially relevant measurements
are scattered across a variety of scientific publications. In this paper we
consider the viability of obtaining lunar composition data by leveraging LLMs
to rapidly process a corpus of scientific publications. While leveraging LLMs
to obtain knowledge from scientific documents is not new, this particular
application presents interesting challenges due to the heterogeneity of lunar
samples and the nuances involved in their characterization. Accuracy and
uncertainty quantification are particularly crucial since many materials
properties can be sensitive to small variations in composition. Our findings
indicate that off-the-shelf LLMs are generally effective at extracting data
from tables commonly found in these documents. However, there remains
opportunity to further refine the data we extract in this initial approach; in
particular, to capture fine-grained mineralogy information and to improve
performance on more subtle/complex pieces of information.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [173] [Exploiting inter-agent coupling information for efficient reinforcement learning of cooperative LQR](https://arxiv.org/abs/2504.20927)
*Shahbaz P Qadri Syed,He Bai*

Main category: eess.SY

TL;DR: 本文提出了一种基于智能体间耦合信息的精确分解方法，用于多智能体强化学习中的局部Q函数，并开发了近似最小二乘策略迭代算法，证明了其样本复杂性与集中式方法相当。


<details>
  <summary>Details</summary>
Motivation: 解决多智能体控制中现有局部Q函数分解方法的不精确性问题，提高样本效率和计算效率。

Method: 利用智能体间耦合信息，提出精确分解局部Q函数的系统性方法，并开发近似最小二乘策略迭代算法。

Result: 证明了分解的样本复杂性与集中式方法相同，并推导了实现更高样本效率的图形条件。数值实验验证了改进的样本和计算效率。

Conclusion: 所提方法在多智能体强化学习中实现了更高的样本和计算效率，为实际应用提供了有效工具。

Abstract: Developing scalable and efficient reinforcement learning algorithms for
cooperative multi-agent control has received significant attention over the
past years. Existing literature has proposed inexact decompositions of local
Q-functions based on empirical information structures between the agents. In
this paper, we exploit inter-agent coupling information and propose a
systematic approach to exactly decompose the local Q-function of each agent. We
develop an approximate least square policy iteration algorithm based on the
proposed decomposition and identify two architectures to learn the local
Q-function for each agent. We establish that the worst-case sample complexity
of the decomposition is equal to the centralized case and derive necessary and
sufficient graphical conditions on the inter-agent couplings to achieve better
sample efficiency. We demonstrate the improved sample efficiency and
computational efficiency on numerical examples.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [174] [Provably faster randomized and quantum algorithms for k-means clustering via uniform sampling](https://arxiv.org/abs/2504.20982)
*Tyler Chen,Archan Ray,Akshay Seshadri,Dylan Herman,Bao Bach,Pranav Deshpande,Abhishek Som,Niraj Kumar,Marco Pistoia*

Main category: quant-ph

TL;DR: 本文提出了一种随机小批量$k$-均值算法及其量子版本，显著改进了先前算法的性能，通过均匀采样保留了问题的对称性。


<details>
  <summary>Details</summary>
Motivation: 传统$k$-均值算法在大数据应用中计算成本高，量子或量子启发算法虽有所改进，但仍有优化空间。

Method: 提出随机小批量$k$-均值算法和量子算法，利用均匀采样保留对称性。

Result: 证明了比先前算法更优的最坏情况性能保证。

Conclusion: 均匀采样的引入显著提升了算法性能，为大数据聚类提供了更高效的方法。

Abstract: The $k$-means algorithm (Lloyd's algorithm) is a widely used method for
clustering unlabeled data. A key bottleneck of the $k$-means algorithm is that
each iteration requires time linear in the number of data points, which can be
expensive in big data applications. This was improved in recent works proposing
quantum and quantum-inspired classical algorithms to approximate the $k$-means
algorithm locally, in time depending only logarithmically on the number of data
points (along with data dependent parameters) [$q$-means: A quantum algorithm
for unsupervised machine learning; Kerenidis, Landman, Luongo, and Prakash,
NeurIPS 2019; Do you know what $q$-means?, Doriguello, Luongo, Tang]. In this
work, we describe a simple randomized mini-batch $k$-means algorithm and a
quantum algorithm inspired by the classical algorithm. We prove worse-case
guarantees that significantly improve upon the bounds for previous algorithms.
Our improvements are due to a careful use of uniform sampling, which preserves
certain symmetries of the $k$-means problem that are not preserved in previous
algorithms that use data norm-based sampling.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [175] [Guessing Efficiently for Constrained Subspace Approximation](https://arxiv.org/abs/2504.20883)
*Aditya Bhaskara,Sepideh Mahabadi,Madhusudhan Reddy Pittu,Ali Vakilian,David P. Woodruff*

Main category: cs.DS

TL;DR: 本文研究了约束子空间逼近问题，提出了一种通用的框架（coreset-guess-solve），用于解决多种约束条件下的子空间逼近问题，并在多个应用中改进了现有结果。


<details>
  <summary>Details</summary>
Motivation: 子空间逼近问题在数据分析和机器学习中广泛应用，但现有方法对约束条件的处理有限。本文旨在提出一种通用框架，以高效解决约束子空间逼近问题。

Method: 提出了一种名为coreset-guess-solve的通用框架，能够生成(1+ε)乘法或ε加法近似解，适用于多种约束条件。

Result: 该框架在公平子空间逼近、k均值聚类和非负矩阵分解等问题中改进了现有结果，同时保持了k均值聚类在欧几里得空间中的已知最优界。

Conclusion: 本文提出的通用框架为约束子空间逼近问题提供了高效解决方案，并在多个应用中展示了其优越性。

Abstract: In this paper we study constrained subspace approximation problem. Given a
set of $n$ points $\{a_1,\ldots,a_n\}$ in $\mathbb{R}^d$, the goal of the {\em
subspace approximation} problem is to find a $k$ dimensional subspace that best
approximates the input points. More precisely, for a given $p\geq 1$, we aim to
minimize the $p$th power of the $\ell_p$ norm of the error vector
$(\|a_1-\bm{P}a_1\|,\ldots,\|a_n-\bm{P}a_n\|)$, where $\bm{P}$ denotes the
projection matrix onto the subspace and the norms are Euclidean. In
\emph{constrained} subspace approximation (CSA), we additionally have
constraints on the projection matrix $\bm{P}$. In its most general form, we
require $\bm{P}$ to belong to a given subset $\mathcal{S}$ that is described
explicitly or implicitly.
  We introduce a general framework for constrained subspace approximation. Our
approach, that we term coreset-guess-solve, yields either
$(1+\varepsilon)$-multiplicative or $\varepsilon$-additive approximations for a
variety of constraints. We show that it provides new algorithms for
partition-constrained subspace approximation with applications to {\it fair}
subspace approximation, $k$-means clustering, and projected non-negative matrix
factorization, among others. Specifically, while we reconstruct the best known
bounds for $k$-means clustering in Euclidean spaces, we improve the known
results for the remainder of the problems.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [176] [Smart Water Security with AI and Blockchain-Enhanced Digital Twins](https://arxiv.org/abs/2504.20275)
*Mohammadhossein Homaei,Victor Gonzalez Morales,Oscar Mogollon Gutierrez,Ruben Molano Gomez,Andres Caro*

Main category: cs.CR

TL;DR: 本文提出了一种结合LoRaWAN数据采集、机器学习驱动的入侵检测系统（IDS）和区块链数字孪生（BC-DT）的框架，用于农村地区安全透明的供水管理。


<details>
  <summary>Details</summary>
Motivation: 农村供水系统面临实时监控不足、易受网络攻击和数据不可靠等问题，亟需一种安全且高效的解决方案。

Method: 采用LSTM自编码器和Isolation Forest进行异常数据检测，通过私有以太坊区块链和PoA共识机制记录验证数据，并构建实时数字孪生模型。

Result: 系统实现每秒80次以上交易，延迟低于2秒，支持1000个智能水表的扩展，且成本效益高。

Conclusion: 该框架为农村分散式供水基础设施提供了一种实用且安全的解决方案。

Abstract: Water distribution systems in rural areas face serious challenges such as a
lack of real-time monitoring, vulnerability to cyberattacks, and unreliable
data handling. This paper presents an integrated framework that combines
LoRaWAN-based data acquisition, a machine learning-driven Intrusion Detection
System (IDS), and a blockchain-enabled Digital Twin (BC-DT) platform for secure
and transparent water management. The IDS filters anomalous or spoofed data
using a Long Short-Term Memory (LSTM) Autoencoder and Isolation Forest before
validated data is logged via smart contracts on a private Ethereum blockchain
using Proof of Authority (PoA) consensus. The verified data feeds into a
real-time DT model supporting leak detection, consumption forecasting, and
predictive maintenance. Experimental results demonstrate that the system
achieves over 80 transactions per second (TPS) with under 2 seconds of latency
while remaining cost-effective and scalable for up to 1,000 smart meters. This
work demonstrates a practical and secure architecture for decentralized water
infrastructure in under-connected rural environments.

</details>


### [177] [Token-Efficient Prompt Injection Attack: Provoking Cessation in LLM Reasoning via Adaptive Token Compression](https://arxiv.org/abs/2504.20493)
*Yu Cui,Yujun Cai,Yiwei Wang*

Main category: cs.CR

TL;DR: 论文提出了一种名为“推理中断攻击”的新型提示注入攻击方法，通过自适应令牌压缩技术，显著降低了触发DeepSeek-R1模型中“思维停止”漏洞的令牌成本。实验证明，该方法能有效利用简单算术任务触发漏洞，并分析了漏洞的成因。


<details>
  <summary>Details</summary>
Motivation: 现有方法触发DeepSeek-R1模型的“思维停止”漏洞需要复杂且冗长的提示（超过5000令牌），成本高昂。研究旨在降低令牌成本并正式定义该漏洞。

Method: 提出基于自适应令牌压缩的“推理中断攻击”方法，开发了系统化的攻击提示收集方法和压缩框架，利用LLM自动压缩提示。

Result: 实验表明，压缩框架显著缩短了提示长度，同时保持了攻击效果。简单算术任务能有效触发漏洞，且逻辑结构比数学应用题更简单。

Conclusion: 研究为提升推理LLM的安全性提供了有价值的见解，并揭示了漏洞的成因。

Abstract: While reasoning large language models (LLMs) demonstrate remarkable
performance across various tasks, they also contain notable security
vulnerabilities. Recent research has uncovered a "thinking-stopped"
vulnerability in DeepSeek-R1, where model-generated reasoning tokens can
forcibly interrupt the inference process, resulting in empty responses that
compromise LLM-integrated applications. However, existing methods triggering
this vulnerability require complex mathematical word problems with long
prompts--even exceeding 5,000 tokens. To reduce the token cost and formally
define this vulnerability, we propose a novel prompt injection attack named
"Reasoning Interruption Attack", based on adaptive token compression. We
demonstrate that simple standalone arithmetic tasks can effectively trigger
this vulnerability, and the prompts based on such tasks exhibit simpler logical
structures than mathematical word problems. We develop a systematic approach to
efficiently collect attack prompts and an adaptive token compression framework
that utilizes LLMs to automatically compress these prompts. Experiments show
our compression framework significantly reduces prompt length while maintaining
effective attack capabilities. We further investigate the attack's performance
via output prefix and analyze the underlying causes of the vulnerability,
providing valuable insights for improving security in reasoning LLMs.

</details>


### [178] [The Hidden Risks of LLM-Generated Web Application Code: A Security-Centric Evaluation of Code Generation Capabilities in Large Language Models](https://arxiv.org/abs/2504.20612)
*Swaroop Dora,Deven Lunkad,Naziya Aslam,S. Venkatesan,Sandeep Kumar Shukla*

Main category: cs.CR

TL;DR: 论文评估了多种LLM生成代码的安全性，发现存在关键漏洞，强调人类审查和更严格的安全评估框架的必要性。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM提升了开发效率，但其生成的代码在安全性方面存在隐患，需要评估其在实际应用中的可靠性。

Method: 使用预定义的安全参数对多种LLM（如ChatGPT、DeepSeek等）生成的代码进行安全合规性分析。

Result: 发现认证机制、会话管理、输入验证和HTTP安全标头等方面存在漏洞，且无模型完全符合行业最佳实践。

Conclusion: 人类专家审查和更严格的安全评估框架对确保LLM生成代码的安全性至关重要。

Abstract: The rapid advancement of Large Language Models (LLMs) has enhanced software
development processes, minimizing the time and effort required for coding and
enhancing developer productivity. However, despite their potential benefits,
code generated by LLMs has been shown to generate insecure code in controlled
environments, raising critical concerns about their reliability and security in
real-world applications. This paper uses predefined security parameters to
evaluate the security compliance of LLM-generated code across multiple models,
such as ChatGPT, DeepSeek, Claude, Gemini and Grok. The analysis reveals
critical vulnerabilities in authentication mechanisms, session management,
input validation and HTTP security headers. Although some models implement
security measures to a limited extent, none fully align with industry best
practices, highlighting the associated risks in automated software development.
Our findings underscore that human expertise is crucial to ensure secure
software deployment or review of LLM-generated code. Also, there is a need for
robust security assessment frameworks to enhance the reliability of
LLM-generated code in real-world applications.

</details>


### [179] [A Virtual Cybersecurity Department for Securing Digital Twins in Water Distribution Systems](https://arxiv.org/abs/2504.20266)
*Mohammadhossein Homaei,Agustin Di Bartolo,Oscar Mogollon-Gutierrez,Fernando Broncano Morgado,Pablo Garcia Rodriguez*

Main category: cs.CR

TL;DR: 论文提出了一种面向中小企业的虚拟网络安全部门（VCD），通过低成本开源工具和机器学习模型提升水务系统数字孪生的安全性。


<details>
  <summary>Details</summary>
Motivation: 数字孪生在水务系统中广泛应用，但其易受网络攻击，而中小企业缺乏资源建立强大的网络安全团队。

Method: VCD结合开源工具（如Zabbix、Suricata、Fail2Ban）和基于机器学习的入侵检测系统（使用OD-IDS2022数据集训练），提供实时监控和威胁检测。

Result: 改进的集成模型在检测暴力破解、远程代码执行等攻击时准确率达92%，误报率低。

Conclusion: VCD为中小企业提供了一种低成本、高效的水务系统网络安全解决方案。

Abstract: Digital twins (DTs) help improve real-time monitoring and decision-making in
water distribution systems. However, their connectivity makes them easy targets
for cyberattacks such as scanning, denial-of-service (DoS), and unauthorized
access. Small and medium-sized enterprises (SMEs) that manage these systems
often do not have enough budget or staff to build strong cybersecurity teams.
To solve this problem, we present a Virtual Cybersecurity Department (VCD), an
affordable and automated framework designed for SMEs. The VCD uses open-source
tools like Zabbix for real-time monitoring, Suricata for network intrusion
detection, Fail2Ban to block repeated login attempts, and simple firewall
settings. To improve threat detection, we also add a machine-learning-based IDS
trained on the OD-IDS2022 dataset using an improved ensemble model. This model
detects cyber threats such as brute-force attacks, remote code execution (RCE),
and network flooding, with 92\% accuracy and fewer false alarms. Our solution
gives SMEs a practical and efficient way to secure water systems using low-cost
and easy-to-manage tools.

</details>


### [180] [Enhancing Vulnerability Reports with Automated and Augmented Description Summarization](https://arxiv.org/abs/2504.20726)
*Hattan Althebeiti,Mohammed Alkinoon,Manar Mohaisen,Saeed Salem,DaeHun Nyang,David Mohaisen*

Main category: cs.CR

TL;DR: Zad系统通过外部资源丰富NVD漏洞描述，解决描述简短和信息不足的问题。


<details>
  <summary>Details</summary>
Motivation: 公共漏洞数据库（如NVD）的描述通常简短且信息不足，需要改进。

Method: Zad由两条流水线组成：一条收集并过滤补充数据，另一条微调预训练模型生成详细描述。

Result: Zad通过标准摘要指标和人工评估证明其能有效提升漏洞信息的质量。

Conclusion: Zad能生成更全面、连贯的漏洞描述，解决了NVD的局限性。

Abstract: Public vulnerability databases, such as the National Vulnerability Database
(NVD), document vulnerabilities and facilitate threat information sharing.
However, they often suffer from short descriptions and outdated or insufficient
information. In this paper, we introduce Zad, a system designed to enrich NVD
vulnerability descriptions by leveraging external resources. Zad consists of
two pipelines: one collects and filters supplementary data using two encoders
to build a detailed dataset, while the other fine-tunes a pre-trained model on
this dataset to generate enriched descriptions. By addressing brevity and
improving content quality, Zad produces more comprehensive and cohesive
vulnerability descriptions. We evaluate Zad using standard summarization
metrics and human assessments, demonstrating its effectiveness in enhancing
vulnerability information.

</details>


### [181] [Dual Explanations via Subgraph Matching for Malware Detection](https://arxiv.org/abs/2504.20904)
*Hossein Shokouhinejad,Roozbeh Razavi-Far,Griffin Higgins,Ali A. Ghorbani*

Main category: cs.CR

TL;DR: 提出了一种基于双原型驱动的可解释框架，用于解释基于GNN的恶意软件检测决策，结合了基础解释器和新型子图匹配解释器，提高了可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统GNN解释方法无法将重要区域与已知的良性或恶意行为模式关联，降低了在安全场景中的实用性。

Method: 提出双原型驱动框架，结合基础解释器和子图匹配解释器（SubMatch），通过子图匹配技术为节点分配可解释分数。

Result: 实验表明，该方法在保持高检测性能的同时，显著提高了恶意软件分析的可解释性。

Conclusion: 该框架为恶意软件检测提供了更行为对齐的解释，增强了安全系统的可信度。

Abstract: Interpretable malware detection is crucial for understanding harmful
behaviors and building trust in automated security systems. Traditional
explainable methods for Graph Neural Networks (GNNs) often highlight important
regions within a graph but fail to associate them with known benign or
malicious behavioral patterns. This limitation reduces their utility in
security contexts, where alignment with verified prototypes is essential. In
this work, we introduce a novel dual prototype-driven explainable framework
that interprets GNN-based malware detection decisions. This dual explainable
framework integrates a base explainer (a state-of-the-art explainer) with a
novel second-level explainer which is designed by subgraph matching technique,
called SubMatch explainer. The proposed explainer assigns interpretable scores
to nodes based on their association with matched subgraphs, offering a
fine-grained distinction between benign and malicious regions. This
prototype-guided scoring mechanism enables more interpretable, behavior-aligned
explanations. Experimental results demonstrate that our method preserves high
detection performance while significantly improving interpretability in malware
analysis.

</details>


### [182] [GiBy: A Giant-Step Baby-Step Classifier For Anomaly Detection In Industrial Control Systems](https://arxiv.org/abs/2504.20906)
*Sarad Venugopalan,Sridhar Adepu*

Main category: cs.CR

TL;DR: 提出了一种基于线性化和降维的异常检测方法，用于工业控制系统（ICS），以快速、可解释地检测异常。


<details>
  <summary>Details</summary>
Motivation: 确保工业控制系统的安全运行，及时检测异常（如攻击、故障等），保护设备和人员安全。

Method: 通过线性化非线性传感器-执行器关系，并利用降维技术降低时间复杂性。

Result: 实验显示该方法能在毫秒级响应时间内检测异常，并提供可解释性，优于现有AI/ML模型。

Conclusion: 该方法高效且可解释，适用于工业控制系统的实时异常检测。

Abstract: The continuous monitoring of the interactions between cyber-physical
components of any industrial control system (ICS) is required to secure
automation of the system controls, and to guarantee plant processes are
fail-safe and remain in an acceptably safe state. Safety is achieved by
managing actuation (where electric signals are used to trigger physical
movement), dependent on corresponding sensor readings; used as ground truth in
decision making. Timely detection of anomalies (attacks, faults and
unascertained states) in ICSs is crucial for the safe running of a plant, the
safety of its personnel, and for the safe provision of any services provided.
We propose an anomaly detection method that involves accurate linearization of
the non-linear forms arising from sensor-actuator(s) relationships, primarily
because solving linear models is easier and well understood. Further, the time
complexity of the anomaly detection scenario/problem at hand is lowered using
dimensionality reduction of the actuator(s) in relationship with a sensor. We
accomplish this by using a well-known water treatment testbed as a use case.
Our experiments show millisecond time response to detect anomalies and provide
explainability; that are not simultaneously achieved by other state of the art
AI/ML models with eXplainable AI (XAI) used for the same purpose. Further, we
pin-point the sensor(s) and its actuation state for which anomaly was detected.

</details>


### [183] [ACE: A Security Architecture for LLM-Integrated App Systems](https://arxiv.org/abs/2504.20984)
*Evan Li,Tushin Mallick,Evan Rose,William Robertson,Alina Oprea,Cristina Nita-Rotaru*

Main category: cs.CR

TL;DR: 论文提出了一种名为ACE的新安全架构，用于保护LLM集成应用系统的规划和执行阶段免受恶意应用的攻击。


<details>
  <summary>Details</summary>
Motivation: LLM集成应用系统通过第三方应用扩展了大型语言模型的实用性，但也引入了新的攻击向量，如恶意应用可能导致规划或执行的完整性破坏、可用性崩溃或隐私泄露。

Method: ACE架构将规划分为两个阶段：首先使用可信信息创建抽象执行计划，然后将其映射到具体计划。通过静态分析验证计划的安全性，并在执行阶段强制执行数据和能力隔离。

Result: 实验表明，ACE能够抵御INJECAGENT基准测试中的攻击以及新引入的攻击，确保系统的安全性和完整性。

Conclusion: ACE架构显著提升了LLM集成应用系统的安全性，特别是在面对不同信任级别的系统设施时。

Abstract: LLM-integrated app systems extend the utility of Large Language Models (LLMs)
with third-party apps that are invoked by a system LLM using interleaved
planning and execution phases to answer user queries. These systems introduce
new attack vectors where malicious apps can cause integrity violation of
planning or execution, availability breakdown, or privacy compromise during
execution.
  In this work, we identify new attacks impacting the integrity of planning, as
well as the integrity and availability of execution in LLM-integrated apps, and
demonstrate them against IsolateGPT, a recent solution designed to mitigate
attacks from malicious apps. We propose Abstract-Concrete-Execute (ACE), a new
secure architecture for LLM-integrated app systems that provides security
guarantees for system planning and execution. Specifically, ACE decouples
planning into two phases by first creating an abstract execution plan using
only trusted information, and then mapping the abstract plan to a concrete plan
using installed system apps. We verify that the plans generated by our system
satisfy user-specified secure information flow constraints via static analysis
on the structured plan output. During execution, ACE enforces data and
capability barriers between apps, and ensures that the execution is conducted
according to the trusted abstract plan. We show experimentally that our system
is secure against attacks from the INJECAGENT benchmark, a standard benchmark
for control flow integrity in the face of indirect prompt injection attacks,
and our newly introduced attacks. Our architecture represents a significant
advancement towards hardening LLM-based systems containing system facilities of
varying levels of trustworthiness.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [184] [AKIBoards: A Structure-Following Multiagent System for Predicting Acute Kidney Injury](https://arxiv.org/abs/2504.20368)
*David Gordon,Panayiotis Petousis,Susanne B. Nicholas,Alex A. T. Bui*

Main category: cs.MA

TL;DR: 论文提出STRUC-MAS框架，通过多智能体系统学习全局模型以提升医疗诊断性能，并以急性肾损伤预测为例验证其效果。


<details>
  <summary>Details</summary>
Motivation: 在复杂医疗场景中，多专家协作需要整合不同视角以优化诊断决策，但缺乏自动化学习全局模型的方法。

Method: 引入STRUC-MAS框架，自动化学习全局模型并将其作为多智能体系统的先验信念，应用于急性肾损伤预测。

Result: 实验显示，基于全局模型的智能体性能显著提升（AP=0.195 vs. 基线0.141），且交互后智能体决策信心增强。

Conclusion: 学习并利用全局模型对多智能体系统在分类和诊断推理中至关重要。

Abstract: Diagnostic reasoning entails a physician's local (mental) model based on an
assumed or known shared perspective (global model) to explain patient
observations with evidence assigned towards a clinical assessment. But in
several (complex) medical situations, multiple experts work together as a team
to optimize health evaluation and decision-making by leveraging different
perspectives. Such consensus-driven reasoning reflects individual knowledge
contributing toward a broader perspective on the patient. In this light, we
introduce STRUCture-following for Multiagent Systems (STRUC-MAS), a framework
automating the learning of these global models and their incorporation as prior
beliefs for agents in multiagent systems (MAS) to follow. We demonstrate proof
of concept with a prosocial MAS application for predicting acute kidney
injuries (AKIs). In this case, we found that incorporating a global structure
enabled multiple agents to achieve better performance (average precision, AP)
in predicting AKI 48 hours before onset (structure-following-fine-tuned, SF-FT,
AP=0.195; SF-FT-retrieval-augmented generation, SF-FT-RAG, AP=0.194) vs.
baseline (non-structure-following-FT, NSF-FT, AP=0.141; NSF-FT-RAG, AP=0.180)
for balanced precision-weighted-recall-weighted voting. Markedly, SF-FT agents
with higher recall scores reported lower confidence levels in the initial round
on true positive and false negative cases. But after explicit interactions,
their confidence in their decisions increased (suggesting reinforced belief).
In contrast, the SF-FT agent with the lowest recall decreased its confidence in
true positive and false negative cases (suggesting a new belief). This approach
suggests that learning and leveraging global structures in MAS is necessary
prior to achieving competitive classification and diagnostic reasoning
performance.

</details>


### [185] [Modeling AI-Human Collaboration as a Multi-Agent Adaptation](https://arxiv.org/abs/2504.20903)
*Prothit Sen,Sai Mihir Jakkaraju*

Main category: cs.MA

TL;DR: 论文通过基于代理的模拟研究了AI与人类协作的效率，发现任务结构（模块化或序列化）是决定协作效果的关键因素。


<details>
  <summary>Details</summary>
Motivation: 探讨AI与人类协作在不同任务结构下的表现，为组织战略决策提供通用框架。

Method: 使用NK模型模拟模块化和序列化任务中AI与人类的交互，区分启发式人类适应和基于规则的AI搜索。

Result: 模块化任务中AI常替代人类，除非人类专业能力极高；序列化任务中，AI与人类互补性更强，但AI主导时人类过度调整会降低收益。

Conclusion: AI与人类协作的效果主要取决于任务结构，而非上下文或行业，任务分解是分析的核心单元。

Abstract: We develop an agent-based simulation to formalize AI-human collaboration as a
function of task structure, advancing a generalizable framework for strategic
decision-making in organizations. Distinguishing between heuristic-based human
adaptation and rule-based AI search, we model interactions across modular
(parallel) and sequenced (interdependent) tasks using an NK model. Our results
reveal that in modular tasks, AI often substitutes for humans - delivering
higher payoffs unless human expertise is very high, and the AI search space is
either narrowly focused or extremely broad. In sequenced tasks, interesting
complementarities emerge. When an expert human initiates the search and AI
subsequently refines it, aggregate performance is maximized. Conversely, when
AI leads, excessive heuristic refinement by the human can reduce payoffs. We
also show that even "hallucinatory" AI - lacking memory or structure - can
improve outcomes when augmenting low-capability humans by helping escape local
optima. These results yield a robust implication: the effectiveness of AI-human
collaboration depends less on context or industry, and more on the underlying
task structure. By elevating task decomposition as the central unit of
analysis, our model provides a transferable lens for strategic decision-making
involving humans and an agentic AI across diverse organizational settings.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [186] [HCT-QA: A Benchmark for Question Answering on Human-Centric Tables](https://arxiv.org/abs/2504.20047)
*Mohammad S. Ahmad,Zan A. Naeem,Michaël Aupetit,Ahmed Elmagarmid,Mohamed Eltabakh,Xiasong Ma,Mourad Ouzzani,Chaoyi Ruan*

Main category: cs.IR

TL;DR: HCT-QA是一个针对复杂布局的人类中心表格（HCTs）的基准测试，包含真实和合成表格及问答对，评估大型语言模型处理此类表格的能力。


<details>
  <summary>Details</summary>
Motivation: 传统方法难以处理HCTs的复杂布局和查询需求，因此需要新的解决方案。

Method: 构建包含2188个真实HCTs和4679个合成表格的数据集，生成9835和67.5K问答对，评估大型语言模型的查询能力。

Result: 数据集为HCTs的查询处理提供了基准，大型语言模型展现出潜力。

Conclusion: HCT-QA为复杂表格的查询处理提供了新方向，大型语言模型是可行的解决方案。

Abstract: Tabular data embedded within PDF files, web pages, and other document formats
are prevalent across numerous sectors such as government, engineering, science,
and business. These human-centric tables (HCTs) possess a unique combination of
high business value, intricate layouts, limited operational power at scale, and
sometimes serve as the only data source for critical insights. However, their
complexity poses significant challenges to traditional data extraction,
processing, and querying methods. While current solutions focus on transforming
these tables into relational formats for SQL queries, they fall short in
handling the diverse and complex layouts of HCTs and hence being amenable to
querying. This paper describes HCT-QA, an extensive benchmark of HCTs, natural
language queries, and related answers on thousands of tables. Our dataset
includes 2,188 real-world HCTs with 9,835 QA pairs and 4,679 synthetic tables
with 67.5K QA pairs. While HCTs can be potentially processed by different type
of query engines, in this paper, we focus on Large Language Models as potential
engines and assess their ability in processing and querying such tables.

</details>


### [187] [Recommending Clinical Trials for Online Patient Cases using Artificial Intelligence](https://arxiv.org/abs/2504.20059)
*Joey Chan,Qiao Jin,Nicholas Wan,Charalampos S. Floudas,Elisabetta Xue,Zhiyong Lu*

Main category: cs.IR

TL;DR: TrialGPT框架利用大型语言模型（LLM）匹配在线患者病例与临床试验，相比传统关键词搜索方法，识别合格试验的能力提升了46%。


<details>
  <summary>Details</summary>
Motivation: 临床试验招募面临挑战，如患者意识不足和复杂资格标准，而在线平台为扩大招募提供了新机会。

Method: 使用TrialGPT框架（基于LLM）匹配50例在线患者病例与临床试验，并与传统关键词搜索方法对比。

Result: TrialGPT识别合格试验的能力比传统方法高46%，平均每位患者有约7个合格试验。患者和试验组织者对匹配结果反馈积极。

Conclusion: TrialGPT在临床试验招募中表现出显著优势，为未来招募策略提供了新方向。

Abstract: Clinical trials are crucial for assessing new treatments; however,
recruitment challenges - such as limited awareness, complex eligibility
criteria, and referral barriers - hinder their success. With the growth of
online platforms, patients increasingly turn to social media and health
communities for support, research, and advocacy, expanding recruitment pools
and established enrollment pathways. Recognizing this potential, we utilized
TrialGPT, a framework that leverages a large language model (LLM) as its
backbone, to match 50 online patient cases (collected from published case
reports and a social media website) to clinical trials and evaluate performance
against traditional keyword-based searches. Our results show that TrialGPT
outperforms traditional methods by 46% in identifying eligible trials, with
each patient, on average, being eligible for around 7 trials. Additionally, our
outreach efforts to case authors and trial organizers regarding these
patient-trial matches yielded highly positive feedback, which we present from
both perspectives.

</details>


### [188] [A model and package for German ColBERT](https://arxiv.org/abs/2504.20083)
*Thuong Dang,Qiqi Chen*

Main category: cs.IR

TL;DR: 本文介绍了ColBERT的德语版本，专注于RAG应用，并展示了支持检索和微调工作流的ColBERT模型包的主要功能。


<details>
  <summary>Details</summary>
Motivation: 为德语用户提供ColBERT的适配版本，并支持RAG应用的需求。

Method: 开发了ColBERT的德语版本，并设计了一个支持检索和微调工作流的软件包。

Result: 成功实现了德语版ColBERT，并提供了功能完善的模型包。

Conclusion: 该工作为德语RAG应用提供了有效的工具，并展示了ColBERT模型的灵活性。

Abstract: In this work, we introduce a German version for ColBERT, a late interaction
multi-dense vector retrieval method, with a focus on RAG applications. We also
present the main features of our package for ColBERT models, supporting both
retrieval and fine-tuning workflows.

</details>


### [189] [MATCHA: Can Multi-Agent Collaboration Build a Trustworthy Conversational Recommender?](https://arxiv.org/abs/2504.20094)
*Zheng Hui,Xiaokai Wei,Yexi Jiang,Kevin Gao,Chen Wang,Frank Ong,Se-eun Yoon,Rachit Pareek,Michelle Gong*

Main category: cs.IR

TL;DR: MATCHA是一个基于多智能体协作的对话推荐系统框架，利用大语言模型提升个性化和用户参与度，通过多个专业智能体协作优化推荐准确性、多样性和安全性。


<details>
  <summary>Details</summary>
Motivation: 解决对话推荐系统中的关键挑战，如处理复杂用户请求、提升个性化、确保安全交互，并通过多智能体协作优化推荐效果。

Method: 引入多个专业智能体（意图分析、候选生成、排序、重排序、解释性和安全保障）协作工作，利用大语言模型处理自由文本请求。

Result: 在八个指标上优于或与当前最佳模型相当，解决了游戏推荐中的复杂用户请求、个性化提升和安全性问题。

Conclusion: MATCHA通过多智能体协作和大语言模型的应用，显著提升了对话推荐系统的性能和用户体验。

Abstract: In this paper, we propose a multi-agent collaboration framework called MATCHA
for conversational recommendation system, leveraging large language models
(LLMs) to enhance personalization and user engagement. Users can request
recommendations via free-form text and receive curated lists aligned with their
interests, preferences, and constraints. Our system introduces specialized
agents for intent analysis, candidate generation, ranking, re-ranking,
explainability, and safeguards. These agents collaboratively improve
recommendations accuracy, diversity, and safety. On eight metrics, our model
achieves superior or comparable performance to the current state-of-the-art.
Through comparisons with six baseline models, our approach addresses key
challenges in conversational recommendation systems for game recommendations,
including: (1) handling complex, user-specific requests, (2) enhancing
personalization through multi-agent collaboration, (3) empirical evaluation and
deployment, and (4) ensuring safe and trustworthy interactions.

</details>


### [190] [An Integrated Framework for Contextual Personalized LLM-Based Food Recommendation](https://arxiv.org/abs/2504.20092)
*Ali Rostami*

Main category: cs.IR

TL;DR: 论文提出了一种针对食物推荐系统（Food-RecSys）的改进方法，通过多媒体食物日志平台和世界食物图谱（World Food Atlas）获取丰富数据，并开发了专门的食物推荐语言处理框架（F-RLP），以解决现有通用模型的不足。


<details>
  <summary>Details</summary>
Motivation: 现有食物推荐系统因组件理解不足和通用机器学习方法在处理不平衡食物数据时的失败而表现不佳，需要针对食物领域的专门解决方案。

Method: 1. 识别并分析Food-RecSys的关键组件；2. 引入多媒体食物日志平台和世界食物图谱；3. 开发专门的食物推荐语言处理框架（F-RLP）。

Result: F-RLP框架通过定制化利用大语言模型（LLMs），克服了通用模型的限制，提供了更有效、上下文感知和个性化的食物推荐。

Conclusion: F-RLP框架为食物推荐领域提供了创新且高效的解决方案，填补了现有技术的空白。

Abstract: Personalized food recommendation systems (Food-RecSys) critically
underperform due to fragmented component understanding and the failure of
conventional machine learning with vast, imbalanced food data. While Large
Language Models (LLMs) offer promise, current generic Recommendation as
Language Processing (RLP) strategies lack the necessary specialization for the
food domain's complexity. This thesis tackles these deficiencies by first
identifying and analyzing the essential components for effective Food-RecSys.
We introduce two key innovations: a multimedia food logging platform for rich
contextual data acquisition and the World Food Atlas, enabling unique
geolocation-based food analysis previously unavailable. Building on this
foundation, we pioneer the Food Recommendation as Language Processing (F-RLP)
framework - a novel, integrated approach specifically architected for the food
domain. F-RLP leverages LLMs in a tailored manner, overcoming the limitations
of generic models and providing a robust infrastructure for effective,
contextual, and truly personalized food recommendations.

</details>


### [191] [Search-Based Interaction For Conversation Recommendation via Generative Reward Model Based Simulated User](https://arxiv.org/abs/2504.20458)
*Xiaolei Wang,Chunxuan Xia,Junyi Li,Fanzhe Meng,Lei Huang,Jinpeng Wang,Wayne Xin Zhao,Ji-Rong Wen*

Main category: cs.IR

TL;DR: 提出了一种基于生成奖励模型的模拟用户GRSU，用于自动与对话推荐系统交互，以更好地捕捉用户偏好。


<details>
  <summary>Details</summary>
Motivation: 对话推荐系统（CRS）难以准确理解用户偏好，频繁用户交互又影响体验。

Method: 设计了生成奖励模型的模拟用户GRSU，提供粗粒度和细粒度反馈，并通过指令调优统一反馈格式。

Result: 实验表明该方法在公共数据集上有效、高效且可迁移。

Conclusion: GRSU能自动优化CRS的交互过程，提升推荐效果。

Abstract: Conversational recommendation systems (CRSs) use multi-turn interaction to
capture user preferences and provide personalized recommendations. A
fundamental challenge in CRSs lies in effectively understanding user
preferences from conversations. User preferences can be multifaceted and
complex, posing significant challenges for accurate recommendations even with
access to abundant external knowledge. While interaction with users can clarify
their true preferences, frequent user involvement can lead to a degraded user
experience.
  To address this problem, we propose a generative reward model based simulated
user, named GRSU, for automatic interaction with CRSs. The simulated user
provides feedback to the items recommended by CRSs, enabling them to better
capture intricate user preferences through multi-turn interaction. Inspired by
generative reward models, we design two types of feedback actions for the
simulated user: i.e., generative item scoring, which offers coarse-grained
feedback, and attribute-based item critique, which provides fine-grained
feedback. To ensure seamless integration, these feedback actions are unified
into an instruction-based format, allowing the development of a unified
simulated user via instruction tuning on synthesized data. With this simulated
user, automatic multi-turn interaction with CRSs can be effectively conducted.
Furthermore, to strike a balance between effectiveness and efficiency, we draw
inspiration from the paradigm of reward-guided search in complex reasoning
tasks and employ beam search for the interaction process. On top of this, we
propose an efficient candidate ranking method to improve the recommendation
results derived from interaction. Extensive experiments on public datasets
demonstrate the effectiveness, efficiency, and transferability of our approach.

</details>


### [192] [X-Cross: Dynamic Integration of Language Models for Cross-Domain Sequential Recommendation](https://arxiv.org/abs/2504.20859)
*Guy Hadad,Haggai Roitman,Yotam Eshel,Bracha Shapira,Lior Rokach*

Main category: cs.IR

TL;DR: X-Cross是一种新颖的跨域顺序推荐模型，通过集成多个领域特定的语言模型，动态优化表示，减少参数和训练数据需求，实现高效跨域推荐。


<details>
  <summary>Details</summary>
Motivation: 解决新产品不断涌现时推荐系统需要快速适应新领域而无需大量重新训练的问题。

Method: 使用低秩适配器（LoRA）微调多个领域特定语言模型，动态整合知识，逐层优化表示。

Result: 在亚马逊数据集上，X-Cross性能接近LoRA微调模型，仅需25%额外参数；在跨域任务中，数据需求减少50%-75%，且准确性显著优于基线方法。

Conclusion: X-Cross提供了一种高效、可扩展的跨域推荐解决方案，显著降低了计算和数据需求。

Abstract: As new products are emerging daily, recommendation systems are required to
quickly adapt to possible new domains without needing extensive retraining.
This work presents ``X-Cross'' -- a novel cross-domain
sequential-recommendation model that recommends products in new domains by
integrating several domain-specific language models; each model is fine-tuned
with low-rank adapters (LoRA). Given a recommendation prompt, operating layer
by layer, X-Cross dynamically refines the representation of each source
language model by integrating knowledge from all other models. These refined
representations are propagated from one layer to the next, leveraging the
activations from each domain adapter to ensure domain-specific nuances are
preserved while enabling adaptability across domains. Using Amazon datasets for
sequential recommendation, X-Cross achieves performance comparable to a model
that is fine-tuned with LoRA, while using only 25% of the additional
parameters. In cross-domain tasks, such as adapting from Toys domain to Tools,
Electronics or Sports, X-Cross demonstrates robust performance, while requiring
about 50%-75% less fine-tuning data than LoRA to make fine-tuning effective.
Furthermore, X-Cross achieves significant improvement in accuracy over
alternative cross-domain baselines. Overall, X-Cross enables scalable and
adaptive cross-domain recommendations, reducing computational overhead and
providing an efficient solution for data-constrained environments.

</details>


### [193] [TreeHop: Generate and Filter Next Query Embeddings Efficiently for Multi-hop Question Answering](https://arxiv.org/abs/2504.20114)
*Zhonghao Li,Kunpeng Zhang,Jinghuai Ou,Shuliang Liu,Xuming Hu*

Main category: cs.IR

TL;DR: TreeHop提出了一种基于嵌入的动态查询更新框架，用于多跳问答任务，显著降低了计算成本和延迟。


<details>
  <summary>Details</summary>
Motivation: 解决多跳问答中传统方法依赖LLM导致的高计算成本和复杂流程的问题。

Method: 通过动态更新查询嵌入，结合语义信息，实现无需LLM的迭代检索，并引入规则停止准则。

Result: 在三个开放域数据集上表现媲美先进方法，参数规模仅5%-0.4%，延迟降低约99%。

Conclusion: TreeHop是一种高效、低成本的多跳问答解决方案，适用于知识密集型应用。

Abstract: Retrieval-augmented generation (RAG) systems face significant challenges in
multi-hop question answering (MHQA), where complex queries require synthesizing
information across multiple document chunks. Existing approaches typically rely
on iterative LLM-based query rewriting and routing, resulting in high
computational costs due to repeated LLM invocations and multi-stage processes.
To address these limitations, we propose TreeHop, an embedding-level framework
without the need for LLMs in query refinement. TreeHop dynamically updates
query embeddings by fusing semantic information from prior queries and
retrieved documents, enabling iterative retrieval through embedding-space
operations alone. This method replaces the traditional
"Retrieve-Rewrite-Vectorize-Retrieve" cycle with a streamlined
"Retrieve-Embed-Retrieve" loop, significantly reducing computational overhead.
Moreover, a rule-based stop criterion is introduced to further prune redundant
retrievals, balancing efficiency and recall rate. Experimental results show
that TreeHop rivals advanced RAG methods across three open-domain MHQA
datasets, achieving comparable performance with only 5\%-0.4\% of the model
parameter size and reducing the query latency by approximately 99\% compared to
concurrent approaches. This makes TreeHop a faster and more cost-effective
solution for deployment in a range of knowledge-intensive applications. For
reproducibility purposes, codes and data are available here:
https://github.com/allen-li1231/TreeHop.

</details>


### [194] [OpenTCM: A GraphRAG-Empowered LLM-based System for Traditional Chinese Medicine Knowledge Retrieval and Diagnosis](https://arxiv.org/abs/2504.20118)
*Jinglin He,Yunqi Guo,Lai Kwan Lam,Waikei Leung,Lixing He,Yuanan Jiang,Chi Chiu Wang,Guoliang Xing,Hongkai Chen*

Main category: cs.IR

TL;DR: OpenTCM是一个基于LLM的系统，结合了中医知识图谱和图增强检索生成技术，旨在解决中医文献的复杂性和语义关系建模问题。


<details>
  <summary>Details</summary>
Motivation: 中医文献复杂且晦涩，AI技术的整合对现代化和普及至关重要，但面临挑战。

Method: 从中医经典数据库中提取文本，构建多关系知识图谱，并集成OpenTCM系统进行高效检索和问答。

Result: 知识图谱精度达98.55%，F1分数99.55%；OpenTCM在信息检索和问答任务中表现优异。

Conclusion: OpenTCM通过知识图谱和LLM的结合，显著提升了中医知识的检索和问答能力。

Abstract: Traditional Chinese Medicine (TCM) represents a rich repository of ancient
medical knowledge that continues to play an important role in modern
healthcare. Due to the complexity and breadth of the TCM literature, the
integration of AI technologies is critical for its modernization and broader
accessibility. However, this integration poses considerable challenges,
including the interpretation of obscure classical Chinese texts and the
modeling of intricate semantic relationships among TCM concepts. In this paper,
we develop OpenTCM, an LLM-based system that combines a domain-specific TCM
knowledge graph and Graph-based Retrieval-Augmented Generation (GraphRAG).
First, we extract more than 3.73 million classical Chinese characters from 68
gynecological books in the Chinese Medical Classics Database, with the help of
TCM and gynecology experts. Second, we construct a comprehensive
multi-relational knowledge graph comprising more than 48,000 entities and
152,000 interrelationships, using customized prompts and Chinese-oriented LLMs
such as DeepSeek and Kimi to ensure high-fidelity semantic understanding. Last,
we integrate OpenTCM with this knowledge graph, enabling high-fidelity
ingredient knowledge retrieval and diagnostic question-answering without model
fine-tuning. Experimental evaluations demonstrate that our prompt design and
model selection significantly improve knowledge graph quality, achieving a
precision of 98. 55% and an F1 score of 99. 55%. In addition, OpenTCM achieves
mean expert scores of 4.5 in ingredient information retrieval and 3.8 in
diagnostic question-answering tasks, outperforming state-of-the-art solutions
in real-world TCM use cases.

</details>


### [195] [Can LLMs Be Trusted for Evaluating RAG Systems? A Survey of Methods and Datasets](https://arxiv.org/abs/2504.20119)
*Lorenz Brehme,Thomas Ströhle,Ruth Breu*

Main category: cs.IR

TL;DR: 本文系统综述了63篇学术文章，总结了RAG系统的最新评估方法，重点关注数据集、检索器、索引与数据库以及生成器组件，并探讨了自动化评估的可行性。


<details>
  <summary>Details</summary>
Motivation: RAG系统的复杂性使其评估和质量提升面临挑战，研究旨在系统化评估方法，为领域应用提供指导。

Method: 通过综述63篇学术文章，分析RAG系统的评估方法，并探讨利用LLM进行自动化评估的可行性。

Result: 研究发现自动化评估方法可行，但需进一步实践研究以明确实施和评估的指导原则。

Conclusion: 研究为RAG系统的评估方法提供了系统化框架，并强调了自动化与人工评估的平衡。

Abstract: Retrieval-Augmented Generation (RAG) has advanced significantly in recent
years. The complexity of RAG systems, which involve multiple components-such as
indexing, retrieval, and generation-along with numerous other parameters, poses
substantial challenges for systematic evaluation and quality enhancement.
Previous research highlights that evaluating RAG systems is essential for
documenting advancements, comparing configurations, and identifying effective
approaches for domain-specific applications. This study systematically reviews
63 academic articles to provide a comprehensive overview of state-of-the-art
RAG evaluation methodologies, focusing on four key areas: datasets, retrievers,
indexing and databases, and the generator component. We observe the feasibility
of an automated evaluation approach for each component of a RAG system,
leveraging an LLM capable of both generating evaluation datasets and conducting
evaluations. In addition, we found that further practical research is essential
to provide companies with clear guidance on the do's and don'ts of implementing
and evaluating RAG systems. By synthesizing evaluation approaches for key RAG
components and emphasizing the creation and adaptation of domain-specific
datasets for benchmarking, we contribute to the advancement of systematic
evaluation methods and the improvement of evaluation rigor for RAG systems.
Furthermore, by examining the interplay between automated approaches leveraging
LLMs and human judgment, we contribute to the ongoing discourse on balancing
automation and human input, clarifying their respective contributions,
limitations, and challenges in achieving robust and reliable evaluations.

</details>


### [196] [Enhancing News Recommendation with Hierarchical LLM Prompting](https://arxiv.org/abs/2504.20452)
*Hai-Dang Kieu,Delvin Ce Zhang,Minh Duc Nguyen,Min Xu,Qiang Wu,Dung D. Le*

Main category: cs.IR

TL;DR: PNR-LLM利用大型语言模型（LLM）增强新闻标题和摘要的语义信息，通过注意力机制整合数据，提升个性化新闻推荐质量。


<details>
  <summary>Details</summary>
Motivation: 现有个性化新闻推荐系统依赖浅层表示（如标题和摘要），难以捕捉用户偏好的复杂性。

Method: 提出PNR-LLM方法，包含新闻增强模块（利用LLM生成更深语义和实体信息）和注意力机制整合数据。

Result: 在MIND数据集上表现优于现有方法，且增强模块可提升其他模型的性能。

Conclusion: PNR-LLM通过LLM增强语义表示，显著提升推荐质量，且具有模型无关性。

Abstract: Personalized news recommendation systems often struggle to effectively
capture the complexity of user preferences, as they rely heavily on shallow
representations, such as article titles and abstracts. To address this problem,
we introduce a novel method, namely PNR-LLM, for Large Language Models for
Personalized News Recommendation. Specifically, PNR-LLM harnesses the
generation capabilities of LLMs to enrich news titles and abstracts, and
consequently improves recommendation quality. PNR-LLM contains a novel module,
News Enrichment via LLMs, which generates deeper semantic information and
relevant entities from articles, transforming shallow contents into richer
representations. We further propose an attention mechanism to aggregate
enriched semantic- and entity-level data, forming unified user and news
embeddings that reveal a more accurate user-news match. Extensive experiments
on MIND datasets show that PNR-LLM outperforms state-of-the-art baselines.
Moreover, the proposed data enrichment module is model-agnostic, and we
empirically show that applying our proposed module to multiple existing models
can further improve their performance, verifying the advantage of our design.

</details>


### [197] [Information Retrieval in the Age of Generative AI: The RGB Model](https://arxiv.org/abs/2504.20610)
*Michele Garetto,Alessandro Cornacchia,Franco Galante,Emilio Leonardi,Alessandro Nordio,Alberto Tarable*

Main category: cs.IR

TL;DR: 论文探讨了生成式AI对信息检索的影响，提出了量化模型分析信息动态，指出其快速普及可能加剧虚假信息传播，强调需负责任地开发AI工具。


<details>
  <summary>Details</summary>
Motivation: 研究生成式AI对互联网信息动态的复杂影响，填补当前对其理解不足的空白。

Method: 提出随机模型，分析信息生成、索引和传播的动态过程，结合Stack Exchange数据进行验证。

Result: 生成式AI的快速普及可能超越人工验证能力，增加虚假信息风险；高质量回答仍需大量时间和人力。

Conclusion: 需负责任地开发未来生成式AI工具，以应对其潜在风险。

Abstract: The advent of Large Language Models (LLMs) and generative AI is fundamentally
transforming information retrieval and processing on the Internet, bringing
both great potential and significant concerns regarding content authenticity
and reliability. This paper presents a novel quantitative approach to shed
light on the complex information dynamics arising from the growing use of
generative AI tools. Despite their significant impact on the digital ecosystem,
these dynamics remain largely uncharted and poorly understood. We propose a
stochastic model to characterize the generation, indexing, and dissemination of
information in response to new topics. This scenario particularly challenges
current LLMs, which often rely on real-time Retrieval-Augmented Generation
(RAG) techniques to overcome their static knowledge limitations. Our findings
suggest that the rapid pace of generative AI adoption, combined with increasing
user reliance, can outpace human verification, escalating the risk of
inaccurate information proliferation across digital resources. An in-depth
analysis of Stack Exchange data confirms that high-quality answers inevitably
require substantial time and human effort to emerge. This underscores the
considerable risks associated with generating persuasive text in response to
new questions and highlights the critical need for responsible development and
deployment of future generative AI tools.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [198] [Edge-Based Learning for Improved Classification Under Adversarial Noise](https://arxiv.org/abs/2504.20077)
*Manish Kansana,Keyan Alexander Rahimi,Elias Hossain,Iman Dehzangi,Noorbakhsh Amiri Golilarz*

Main category: cs.CV

TL;DR: 研究分析了FGSM对抗性噪声对图像分类的影响，发现基于边缘特征的训练能提升模型对抗攻击的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 对抗性噪声会误导深度学习模型，降低识别准确性，研究旨在探索通过特定图像特征（如边缘）提升模型鲁棒性。

Method: 使用脑肿瘤和COVID数据集，先训练模型于干净图像，再引入对抗性噪声并重新训练，最后测试基于边缘特征的训练效果。

Result: 基于边缘特征的模型对抗攻击更具韧性，但原始数据重新训练后的准确性提升略高于边缘数据。

Conclusion: 利用边缘学习可增强深度学习模型对抗对抗性扰动的能力。

Abstract: Adversarial noise introduces small perturbations in images, misleading deep
learning models into misclassification and significantly impacting recognition
accuracy. In this study, we analyzed the effects of Fast Gradient Sign Method
(FGSM) adversarial noise on image classification and investigated whether
training on specific image features can improve robustness. We hypothesize that
while adversarial noise perturbs various regions of an image, edges may remain
relatively stable and provide essential structural information for
classification. To test this, we conducted a series of experiments using brain
tumor and COVID datasets. Initially, we trained the models on clean images and
then introduced subtle adversarial perturbations, which caused deep learning
models to significantly misclassify the images. Retraining on a combination of
clean and noisy images led to improved performance. To evaluate the robustness
of the edge features, we extracted edges from the original/clean images and
trained the models exclusively on edge-based representations. When noise was
introduced to the images, the edge-based models demonstrated greater resilience
to adversarial attacks compared to those trained on the original or clean
images. These results suggest that while adversarial noise is able to exploit
complex non-edge regions significantly more than edges, the improvement in the
accuracy after retraining is marginally more in the original data as compared
to the edges. Thus, leveraging edge-based learning can improve the resilience
of deep learning models against adversarial perturbations.

</details>


### [199] [Weaving Context Across Images: Improving Vision-Language Models through Focus-Centric Visual Chains](https://arxiv.org/abs/2504.20199)
*Juntian Zhang,Chuanqi cheng,Yuhan Liu,Wei Liu,Jian Luan,Rui Yan*

Main category: cs.CV

TL;DR: 本文提出了一种名为Focus-Centric Visual Chain的新方法，用于提升视觉语言模型在多图像任务中的性能。通过合成高质量数据（VISC-150K数据集），该方法在七个多图像基准测试中表现优异，平均性能提升3.16%和2.24%。


<details>
  <summary>Details</summary>
Motivation: 现实场景中多图像输入复杂，现有视觉语言模型难以有效处理分散的关键信息，导致性能下降。

Method: 提出Focus-Centric Visual Chain范式，结合Focus-Centric Data Synthesis方法合成高质量数据（VISC-150K数据集）。

Result: 在七个多图像基准测试中，平均性能提升3.16%和2.24%，且不影响通用视觉语言能力。

Conclusion: 该研究为处理复杂视觉场景的视觉语言系统提供了重要进展。

Abstract: Vision-language models (VLMs) achieve remarkable success in single-image
tasks. However, real-world scenarios often involve intricate multi-image
inputs, leading to a notable performance decline as models struggle to
disentangle critical information scattered across complex visual features. In
this work, we propose Focus-Centric Visual Chain, a novel paradigm that
enhances VLMs'perception, comprehension, and reasoning abilities in multi-image
scenarios. To facilitate this paradigm, we propose Focus-Centric Data
Synthesis, a scalable bottom-up approach for synthesizing high-quality data
with elaborate reasoning paths. Through this approach, We construct VISC-150K,
a large-scale dataset with reasoning data in the form of Focus-Centric Visual
Chain, specifically designed for multi-image tasks. Experimental results on
seven multi-image benchmarks demonstrate that our method achieves average
performance gains of 3.16% and 2.24% across two distinct model architectures,
without compromising the general vision-language capabilities. our study
represents a significant step toward more robust and capable vision-language
systems that can handle complex visual scenarios.

</details>


### [200] [Integration Flow Models](https://arxiv.org/abs/2504.20179)
*Jingjing Wang,Dan Zhang,Joshua Luo,Yin Yang,Feng Luo*

Main category: cs.CV

TL;DR: 本文提出了一种名为Integration Flow的新方法，直接学习ODE轨迹路径的积分，避免了ODE求解的离散化误差，并通过锚定目标状态提高了稳定性和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有ODE生成模型存在离散化误差或训练不稳定的问题，限制了样本质量。

Method: 提出Integration Flow，直接学习ODE轨迹路径的积分，并显式引入目标状态作为锚定状态。

Result: 在CIFAR10和ImageNet上，Integration Flow显著提升了现有ODE模型的性能，如扩散模型和Rectified Flows。

Conclusion: Integration Flow是一种统一且高效的ODE生成模型框架，显著提升了样本质量和训练稳定性。

Abstract: Ordinary differential equation (ODE) based generative models have emerged as
a powerful approach for producing high-quality samples in many applications.
However, the ODE-based methods either suffer the discretization error of
numerical solvers of ODE, which restricts the quality of samples when only a
few NFEs are used, or struggle with training instability. In this paper, we
proposed Integration Flow, which directly learns the integral of ODE-based
trajectory paths without solving the ODE functions. Moreover, Integration Flow
explicitly incorporates the target state $\mathbf{x}_0$ as the anchor state in
guiding the reverse-time dynamics. We have theoretically proven this can
contribute to both stability and accuracy. To the best of our knowledge,
Integration Flow is the first model with a unified structure to estimate
ODE-based generative models and the first to show the exact straightness of
1-Rectified Flow without reflow. Through theoretical analysis and empirical
evaluations, we show that Integration Flows achieve improved performance when
it is applied to existing ODE-based models, such as diffusion models, Rectified
Flows, and PFGM++. Specifically, Integration Flow achieves one-step generation
on CIFAR10 with FIDs of 2.86 for the Variance Exploding (VE) diffusion model,
3.36 for rectified flow without reflow, and 2.91 for PFGM++; and on ImageNet
with FIDs of 4.09 for VE diffusion model, 4.35 for rectified flow without
reflow and 4.15 for PFGM++.

</details>


### [201] [AlignDiT: Multimodal Aligned Diffusion Transformer for Synchronized Speech Generation](https://arxiv.org/abs/2504.20629)
*Jeongsoo Choi,Ji-Hoon Kim,Kim Sung-Bin,Tae-Hyun Oh,Joon Son Chung*

Main category: cs.CV

TL;DR: AlignDiT是一种多模态对齐扩散变换器，用于从对齐的多模态输入生成高质量语音，解决了现有方法在语音清晰度、音视频同步、自然度和说话人相似性方面的不足。


<details>
  <summary>Details</summary>
Motivation: 多模态语音合成任务在电影制作、配音和虚拟形象等领域有广泛应用，但现有方法在语音清晰度、同步性和自然度方面存在局限。

Method: 提出AlignDiT，基于DiT架构，采用三种策略对齐多模态表示，并引入多模态无分类器引导机制以自适应平衡各模态信息。

Result: 实验表明AlignDiT在质量、同步性和说话人相似性上显著优于现有方法，并在多模态任务中表现出强大的泛化能力。

Conclusion: AlignDiT在多模态语音合成任务中实现了最先进的性能，具有广泛的应用潜力。

Abstract: In this paper, we address the task of multimodal-to-speech generation, which
aims to synthesize high-quality speech from multiple input modalities: text,
video, and reference audio. This task has gained increasing attention due to
its wide range of applications, such as film production, dubbing, and virtual
avatars. Despite recent progress, existing methods still suffer from
limitations in speech intelligibility, audio-video synchronization, speech
naturalness, and voice similarity to the reference speaker. To address these
challenges, we propose AlignDiT, a multimodal Aligned Diffusion Transformer
that generates accurate, synchronized, and natural-sounding speech from aligned
multimodal inputs. Built upon the in-context learning capability of the DiT
architecture, AlignDiT explores three effective strategies to align multimodal
representations. Furthermore, we introduce a novel multimodal classifier-free
guidance mechanism that allows the model to adaptively balance information from
each modality during speech synthesis. Extensive experiments demonstrate that
AlignDiT significantly outperforms existing methods across multiple benchmarks
in terms of quality, synchronization, and speaker similarity. Moreover,
AlignDiT exhibits strong generalization capability across various multimodal
tasks, such as video-to-speech synthesis and visual forced alignment,
consistently achieving state-of-the-art performance. The demo page is available
at https://mm.kaist.ac.kr/projects/AlignDiT .

</details>


### [202] [A Transformer-based Multimodal Fusion Model for Efficient Crowd Counting Using Visual and Wireless Signals](https://arxiv.org/abs/2504.20178)
*Zhe Cui,Yuli Li,Le-Nam Tran*

Main category: cs.CV

TL;DR: TransFusion是一种基于多模态融合的人群计数模型，结合了CSI和图像数据，利用Transformer和CNN提升全局和局部特征提取能力，实现了高精度和高效性。


<details>
  <summary>Details</summary>
Motivation: 当前单模态输入的人群计数模型存在信息丢失和性能不佳的问题，需要一种多模态融合方法以提升准确性。

Method: 提出TransFusion模型，结合Transformer（全局特征）和CNN（局部特征），融合CSI和图像数据。

Result: 实验表明TransFusion在计数误差最小的情况下实现了高精度和高效性。

Conclusion: TransFusion通过多模态融合和混合网络架构，显著提升了人群计数的性能。

Abstract: Current crowd-counting models often rely on single-modal inputs, such as
visual images or wireless signal data, which can result in significant
information loss and suboptimal recognition performance. To address these
shortcomings, we propose TransFusion, a novel multimodal fusion-based
crowd-counting model that integrates Channel State Information (CSI) with image
data. By leveraging the powerful capabilities of Transformer networks,
TransFusion effectively combines these two distinct data modalities, enabling
the capture of comprehensive global contextual information that is critical for
accurate crowd estimation. However, while transformers are well capable of
capturing global features, they potentially fail to identify finer-grained,
local details essential for precise crowd counting. To mitigate this, we
incorporate Convolutional Neural Networks (CNNs) into the model architecture,
enhancing its ability to extract detailed local features that complement the
global context provided by the Transformer. Extensive experimental evaluations
demonstrate that TransFusion achieves high accuracy with minimal counting
errors while maintaining superior efficiency.

</details>


### [203] [SpaRE: Enhancing Spatial Reasoning in Vision-Language Models with Synthetic Data](https://arxiv.org/abs/2504.20648)
*Michael Ogezi,Freda Shi*

Main category: cs.CV

TL;DR: 论文提出了一种增强视觉语言模型（VLM）空间推理能力的方法，通过构建合成VQA数据集SpaRE，显著提升了模型在空间推理任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在空间推理任务上表现不佳，主要因为广泛使用的VL数据集中空间关系样本稀少且分布不均。

Method: 利用Localized Narratives、DOCCI和PixMo-Cap中的超详细图像描述，构建了一个包含455k样本和3.4百万QA对的合成VQA数据集，用于训练SpaRE模型。

Result: SpaRE模型在空间推理基准测试中表现显著提升，在What's Up基准上性能提高了49%，同时保持了通用任务的强性能。

Conclusion: 该研究缩小了人类与VLM在空间推理能力上的差距，提升了模型在机器人技术和导航等实际任务中的应用潜力。

Abstract: Vision-language models (VLMs) work well in tasks ranging from image
captioning to visual question answering (VQA), yet they struggle with spatial
reasoning, a key skill for understanding our physical world that humans excel
at. We find that spatial relations are generally rare in widely used VL
datasets, with only a few being well represented, while most form a long tail
of underrepresented relations. This gap leaves VLMs ill-equipped to handle
diverse spatial relationships. To bridge it, we construct a synthetic VQA
dataset focused on spatial reasoning generated from hyper-detailed image
descriptions in Localized Narratives, DOCCI, and PixMo-Cap. Our dataset
consists of 455k samples containing 3.4 million QA pairs. Trained on this
dataset, our Spatial-Reasoning Enhanced (SpaRE) VLMs show strong improvements
on spatial reasoning benchmarks, achieving up to a 49% performance gain on the
What's Up benchmark, while maintaining strong results on general tasks. Our
work narrows the gap between human and VLM spatial reasoning and makes VLMs
more capable in real-world tasks such as robotics and navigation.

</details>


### [204] [Advance Fake Video Detection via Vision Transformers](https://arxiv.org/abs/2504.20669)
*Joy Battocchio,Stefano Dell'Anna,Andrea Montibeller,Giulia Boato*

Main category: cs.CV

TL;DR: 本文提出了一种基于Vision Transformer（ViT）的创新框架，用于检测AI生成的视频，解决了当前虚假多媒体内容泛滥的问题。


<details>
  <summary>Details</summary>
Motivation: 随着AI生成的多媒体内容日益逼真，其可能被用于传播虚假信息的风险增加，亟需高准确性和泛化能力的检测方法。

Method: 扩展ViT在图像检测中的应用，提出了一种整合ViT嵌入信息的创新框架，用于视频检测。

Result: 该方法在新的大型多样化数据集上表现出高准确性、泛化能力和少样本学习能力。

Conclusion: 该框架为检测AI生成的视频提供了一种有效的解决方案，具有广泛的应用潜力。

Abstract: Recent advancements in AI-based multimedia generation have enabled the
creation of hyper-realistic images and videos, raising concerns about their
potential use in spreading misinformation. The widespread accessibility of
generative techniques, which allow for the production of fake multimedia from
prompts or existing media, along with their continuous refinement, underscores
the urgent need for highly accurate and generalizable AI-generated media
detection methods, underlined also by new regulations like the European Digital
AI Act. In this paper, we draw inspiration from Vision Transformer (ViT)-based
fake image detection and extend this idea to video. We propose an {original}
%innovative framework that effectively integrates ViT embeddings over time to
enhance detection performance. Our method shows promising accuracy,
generalization, and few-shot learning capabilities across a new, large and
diverse dataset of videos generated using five open source generative
techniques from the state-of-the-art, as well as a separate dataset containing
videos produced by proprietary generative methods.

</details>


### [205] [Fine Grain Classification: Connecting Meta using Cross-Contrastive pre-training](https://arxiv.org/abs/2504.20322)
*Sumit Mamtani,Yash Thesia*

Main category: cs.CV

TL;DR: 论文提出了一种利用元信息辅助细粒度视觉分类的统一框架，通过跨对比预训练联合学习视觉和元信息，显著提升了分类性能。


<details>
  <summary>Details</summary>
Motivation: 细粒度视觉分类仅依赖外观信息难以准确区分子类别，因此需要利用元信息辅助识别。

Method: 采用三种编码器分别处理图像、文本和元信息，通过跨对比预训练对齐嵌入表示，随后微调图像和元信息编码器进行分类任务。

Result: 在NABirds数据集上，框架利用元信息将性能提升7.83%，最终达到84.44%的准确率，优于现有方法。

Conclusion: 元信息的引入显著提升了细粒度视觉分类性能，证明了联合学习视觉和元信息的有效性。

Abstract: Fine-grained visual classification aims to recognize objects belonging to
multiple subordinate categories within a super-category. However, this remains
a challenging problem, as appearance information alone is often insufficient to
accurately differentiate between fine-grained visual categories. To address
this, we propose a novel and unified framework that leverages meta-information
to assist fine-grained identification. We tackle the joint learning of visual
and meta-information through cross-contrastive pre-training. In the first
stage, we employ three encoders for images, text, and meta-information,
aligning their projected embeddings to achieve better representations. We then
fine-tune the image and meta-information encoders for the classification task.
Experiments on the NABirds dataset demonstrate that our framework effectively
utilizes meta-information to enhance fine-grained recognition performance. With
the addition of meta-information, our framework surpasses the current baseline
on NABirds by 7.83%. Furthermore, it achieves an accuracy of 84.44% on the
NABirds dataset, outperforming many existing state-of-the-art approaches that
utilize meta-information.

</details>


### [206] [Autoencoder Models for Point Cloud Environmental Synthesis from WiFi Channel State Information: A Preliminary Study](https://arxiv.org/abs/2504.20541)
*Daniele Pannone,Danilo Avola*

Main category: cs.CV

TL;DR: 提出了一种基于WiFi信道状态信息（CSI）数据的深度学习框架，用于生成点云。


<details>
  <summary>Details</summary>
Motivation: 利用WiFi数据实现环境点云的精确重建，为无线传感和环境映射提供新方法。

Method: 采用两阶段自编码器方法：PointNet自编码器用于点云生成，CNN自编码器将CSI数据映射到匹配的潜在空间。

Result: 实验验证了方法的有效性，能够从WiFi数据中准确重建环境点云。

Conclusion: 该方法在无线传感和环境映射应用中具有潜力。

Abstract: This paper introduces a deep learning framework for generating point clouds
from WiFi Channel State Information data. We employ a two-stage autoencoder
approach: a PointNet autoencoder with convolutional layers for point cloud
generation, and a Convolutional Neural Network autoencoder to map CSI data to a
matching latent space. By aligning these latent spaces, our method enables
accurate environmental point cloud reconstruction from WiFi data. Experimental
results validate the effectiveness of our approach, highlighting its potential
for wireless sensing and environmental mapping applications.

</details>


### [207] [GaussTrap: Stealthy Poisoning Attacks on 3D Gaussian Splatting for Targeted Scene Confusion](https://arxiv.org/abs/2504.20829)
*Jiaxin Hong,Sixu Chen,Shuoyang Sun,Hongyao Yu,Hao Fang,Yuqi Tan,Bin Chen,Shuhan Qi,Jiawei Li*

Main category: cs.CV

TL;DR: 该论文首次系统研究了3D高斯泼溅（3DGS）中的后门威胁，提出了一种名为GuassTrap的新型攻击方法，能够在特定视角植入恶意视图，同时保持非目标视图的高质量渲染。


<details>
  <summary>Details</summary>
Motivation: 随着3DGS在安全关键领域的快速应用，亟需研究其潜在安全漏洞，特别是后门威胁对场景理解和沉浸式环境的影响。

Method: GuassTrap采用三阶段流程（攻击、稳定和正常训练），在3DGS中植入隐蔽且视角一致的毒化渲染，优化攻击效果和感知真实性。

Result: 实验表明，GuassTrap能有效嵌入难以察觉但有害的后门视图，同时保持正常视图的高质量渲染。

Conclusion: 该研究揭示了3D渲染中的安全风险，验证了GuassTrap的鲁棒性和实用性。

Abstract: As 3D Gaussian Splatting (3DGS) emerges as a breakthrough in scene
representation and novel view synthesis, its rapid adoption in safety-critical
domains (e.g., autonomous systems, AR/VR) urgently demands scrutiny of
potential security vulnerabilities. This paper presents the first systematic
study of backdoor threats in 3DGS pipelines. We identify that adversaries may
implant backdoor views to induce malicious scene confusion during inference,
potentially leading to environmental misperception in autonomous navigation or
spatial distortion in immersive environments. To uncover this risk, we propose
GuassTrap, a novel poisoning attack method targeting 3DGS models. GuassTrap
injects malicious views at specific attack viewpoints while preserving
high-quality rendering in non-target views, ensuring minimal detectability and
maximizing potential harm. Specifically, the proposed method consists of a
three-stage pipeline (attack, stabilization, and normal training) to implant
stealthy, viewpoint-consistent poisoned renderings in 3DGS, jointly optimizing
attack efficacy and perceptual realism to expose security risks in 3D
rendering. Extensive experiments on both synthetic and real-world datasets
demonstrate that GuassTrap can effectively embed imperceptible yet harmful
backdoor views while maintaining high-quality rendering in normal views,
validating its robustness, adaptability, and practical applicability.

</details>


### [208] [RadSAM: Segmenting 3D radiological images with a 2D promptable model](https://arxiv.org/abs/2504.20837)
*Julien Khlaut,Elodie Ferreres,Daniel Tordjman,Hélène Philippe,Tom Boeken,Pierre Manceron,Corentin Dancette*

Main category: cs.CV

TL;DR: RadSAM提出了一种基于2D模型从单个提示分割3D对象的新方法，解决了SAM在医学图像分割中的局限性。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割对临床至关重要，但现有SAM模型基于自然图像预训练，无法有效处理3D医学数据（如CT和MRI），且缺乏编辑功能。

Method: 通过使用噪声掩码、边界框和点作为初始提示，训练2D模型，并结合迭代推理管道逐片重建3D掩码。

Result: 在AMOS腹部器官分割数据集上验证了RadSAM的有效性，优于现有先进模型。

Conclusion: RadSAM填补了SAM在医学3D图像分割中的空白，展示了其高效性和编辑能力。

Abstract: Medical image segmentation is a crucial and time-consuming task in clinical
care, where mask precision is extremely important. The Segment Anything Model
(SAM) offers a promising approach, as it provides an interactive interface
based on visual prompting and edition to refine an initial segmentation. This
model has strong generalization capabilities, does not rely on predefined
classes, and adapts to diverse objects; however, it is pre-trained on natural
images and lacks the ability to process medical data effectively. In addition,
this model is built for 2D images, whereas a whole medical domain is based on
3D images, such as CT and MRI. Recent adaptations of SAM for medical imaging
are based on 2D models, thus requiring one prompt per slice to segment 3D
objects, making the segmentation process tedious. They also lack important
features such as editing. To bridge this gap, we propose RadSAM, a novel method
for segmenting 3D objects with a 2D model from a single prompt. In practice, we
train a 2D model using noisy masks as initial prompts, in addition to bounding
boxes and points. We then use this novel prompt type with an iterative
inference pipeline to reconstruct the 3D mask slice-by-slice. We introduce a
benchmark to evaluate the model's ability to segment 3D objects in CT images
from a single prompt and evaluate the models' out-of-domain transfer and
edition capabilities. We demonstrate the effectiveness of our approach against
state-of-the-art models on this benchmark using the AMOS abdominal organ
segmentation dataset.

</details>


### [209] [Classifier-to-Bias: Toward Unsupervised Automatic Bias Detection for Visual Classifiers](https://arxiv.org/abs/2504.20902)
*Quentin Guimard,Moreno D'Incà,Massimiliano Mancini,Elisa Ricci*

Main category: cs.CV

TL;DR: C2B是一种无需标注数据的偏差发现框架，通过任务描述生成偏差建议并评估模型偏差。


<details>
  <summary>Details</summary>
Motivation: 现有偏差识别方法依赖标注数据，限制了应用范围，C2B旨在解决这一问题。

Method: 利用大型语言模型生成偏差建议和描述，通过检索模型收集图像并评估模型偏差。

Result: C2B在公开数据集上表现优于依赖标注的基线方法，发现更多偏差。

Conclusion: C2B为任务无关的无监督偏差检测提供了有前景的解决方案。

Abstract: A person downloading a pre-trained model from the web should be aware of its
biases. Existing approaches for bias identification rely on datasets containing
labels for the task of interest, something that a non-expert may not have
access to, or may not have the necessary resources to collect: this greatly
limits the number of tasks where model biases can be identified. In this work,
we present Classifier-to-Bias (C2B), the first bias discovery framework that
works without access to any labeled data: it only relies on a textual
description of the classification task to identify biases in the target
classification model. This description is fed to a large language model to
generate bias proposals and corresponding captions depicting biases together
with task-specific target labels. A retrieval model collects images for those
captions, which are then used to assess the accuracy of the model w.r.t. the
given biases. C2B is training-free, does not require any annotations, has no
constraints on the list of biases, and can be applied to any pre-trained model
on any classification task. Experiments on two publicly available datasets show
that C2B discovers biases beyond those of the original datasets and outperforms
a recent state-of-the-art bias detection baseline that relies on task-specific
annotations, being a promising first step toward addressing task-agnostic
unsupervised bias detection.

</details>


### [210] [SVD Based Least Squares for X-Ray Pneumonia Classification Using Deep Features](https://arxiv.org/abs/2504.20970)
*Mete Erdogan,Sebnem Demirtas*

Main category: cs.CV

TL;DR: 提出了一种基于SVD-LS的肺炎多分类框架，结合自监督和迁移学习模型，实现高效准确的诊断。


<details>
  <summary>Details</summary>
Motivation: 通过X光影像实现肺炎的准确早期诊断对治疗和患者预后至关重要，机器学习的发展为自动化诊断工具提供了可能。

Method: 采用SVD-LS框架，结合自监督和迁移学习的特征表示，使用闭式非迭代分类方法，避免计算昂贵的梯度微调。

Result: 实验表明，SVD-LS在保持高准确性的同时显著降低计算成本，适用于实时医疗影像应用。

Conclusion: SVD-LS是一种高效且准确的肺炎分类方法，适合实际医疗应用。

Abstract: Accurate and early diagnosis of pneumonia through X-ray imaging is essential
for effective treatment and improved patient outcomes. Recent advancements in
machine learning have enabled automated diagnostic tools that assist
radiologists in making more reliable and efficient decisions. In this work, we
propose a Singular Value Decomposition-based Least Squares (SVD-LS) framework
for multi-class pneumonia classification, leveraging powerful feature
representations from state-of-the-art self-supervised and transfer learning
models. Rather than relying on computationally expensive gradient based
fine-tuning, we employ a closed-form, non-iterative classification approach
that ensures efficiency without compromising accuracy. Experimental results
demonstrate that SVD-LS achieves competitive performance while offering
significantly reduced computational costs, making it a viable alternative for
real-time medical imaging applications.

</details>


### [211] [YoChameleon: Personalized Vision and Language Generation](https://arxiv.org/abs/2504.20998)
*Thao Nguyen,Krishna Kumar Singh,Jing Shi,Trung Bui,Yong Jae Lee,Yuheng Li*

Main category: cs.CV

TL;DR: Yo'Chameleon是首个研究大型多模态模型个性化的方法，通过软提示调优实现特定概念的个性化生成和问答。


<details>
  <summary>Details</summary>
Motivation: 现有大型多模态模型缺乏对用户特定概念的个性化知识，尤其在图像生成方面。

Method: 利用3-5张特定概念的图像，通过软提示调优嵌入主题信息，结合自提示优化机制和“软正”图像生成方法。

Result: 能够回答关于特定主题的问题，并在新上下文中生成包含像素级细节的图像。

Conclusion: Yo'Chameleon为多模态模型个性化提供了有效方法，尤其在少样本场景下表现优异。

Abstract: Large Multimodal Models (e.g., GPT-4, Gemini, Chameleon) have evolved into
powerful tools with millions of users. However, they remain generic models and
lack personalized knowledge of specific user concepts. Previous work has
explored personalization for text generation, yet it remains unclear how these
methods can be adapted to new modalities, such as image generation. In this
paper, we introduce Yo'Chameleon, the first attempt to study personalization
for large multimodal models. Given 3-5 images of a particular concept,
Yo'Chameleon leverages soft-prompt tuning to embed subject-specific information
to (i) answer questions about the subject and (ii) recreate pixel-level details
to produce images of the subject in new contexts. Yo'Chameleon is trained with
(i) a self-prompting optimization mechanism to balance performance across
multiple modalities, and (ii) a ``soft-positive" image generation approach to
enhance image quality in a few-shot setting.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [212] [Optimizing Hard Thresholding for Sparse Model Discovery](https://arxiv.org/abs/2504.20256)
*Derek W. Jollie,Scott G. McCalla*

Main category: math.OC

TL;DR: 论文提出了一种通过退火方案重新激活部分被移除项的稀疏字典学习方法，提升了稀疏学习算法的性能，并在多个非线性系统中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 稀疏字典学习方法在模型选择中常用，但硬阈值处理可能影响性能，因此引入退火方案以优化稀疏激活。

Method: 采用退火方案重新激活部分被移除的项，结合冷却计划，应用于SINDy和硬阈值追踪两种优化方法。

Result: 退火方案在多个非线性系统（对流流动、可激发系统、种群动力学）中提高了模型准确性，并在实验数据（抛体运动）中得到验证。

Conclusion: 退火方案能有效提升稀疏学习算法的性能，适用于多种非线性系统和实际数据。

Abstract: Many model selection algorithms rely on sparse dictionary learning to provide
interpretable and physics-based governing equations. The optimization
algorithms typically use a hard thresholding process to enforce sparse
activations in the model coefficients by removing library elements from
consideration. By introducing an annealing scheme that reactivates a fraction
of the removed terms with a cooling schedule, we are able to improve the
performance of these sparse learning algorithms. We concentrate on two
approaches to the optimization, SINDy, and an alternative using hard
thresholding pursuit. We see in both cases that annealing can improve model
accuracy. The effectiveness of annealing is demonstrated through comparisons on
several nonlinear systems pulled from convective flows, excitable systems, and
population dynamics. Finally we apply these algorithms to experimental data for
projectile motion.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [213] [Towards Easy and Realistic Network Infrastructure Testing for Large-scale Machine Learning](https://arxiv.org/abs/2504.20854)
*Jinsun Yoo,ChonLam Lao,Lianjie Cao,Bob Lantz,Minlan Yu,Tushar Krishna,Puneet Sharma*

Main category: cs.NI

TL;DR: Genie是一个测试框架，通过CPU模拟GPU通信，结合ASTRA-sim模拟器分析网络行为对ML工作负载性能的影响，无需昂贵GPU。


<details>
  <summary>Details</summary>
Motivation: 研究真实硬件网络行为对ML工作负载性能的影响，避免使用昂贵GPU资源。

Method: 利用CPU生成流量模拟GPU间通信，并适配ASTRA-sim模拟器建模网络与ML工作负载的交互。

Result: 成功开发出Genie框架，能够高效模拟和分析网络行为对ML性能的影响。

Conclusion: Genie为研究网络行为对ML性能的影响提供了低成本、高效的解决方案。

Abstract: This paper lays the foundation for Genie, a testing framework that captures
the impact of real hardware network behavior on ML workload performance,
without requiring expensive GPUs. Genie uses CPU-initiated traffic over a
hardware testbed to emulate GPU to GPU communication, and adapts the ASTRA-sim
simulator to model interaction between the network and the ML workload.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [214] [EPSILON: Adaptive Fault Mitigation in Approximate Deep Neural Network using Statistical Signatures](https://arxiv.org/abs/2504.20074)
*Khurram Khalil,Khaza Anuarul Hoque*

Main category: cs.DC

TL;DR: EPSILON是一个轻量级框架，通过预计算的统计特征和层重要性指标，高效检测和缓解AxDNN中的故障，保持模型准确性并提升能效。


<details>
  <summary>Details</summary>
Motivation: 传统故障检测方法在AxDNN中引入高开销和延迟，不适用于实时部署，EPSILON旨在解决这一问题。

Method: EPSILON采用非参数模式匹配算法，实现恒定时间故障检测，并根据权重分布和层关键性动态调整缓解策略。

Result: 在多种AxDNN架构和故障场景下，EPSILON保持80.05%的准确率，推理时间提升22%，能效提升28%。

Conclusion: EPSILON是安全关键边缘应用中部署可靠AxDNN的实用解决方案。

Abstract: The increasing adoption of approximate computing in deep neural network
accelerators (AxDNNs) promises significant energy efficiency gains. However,
permanent faults in AxDNNs can severely degrade their performance compared to
their accurate counterparts (AccDNNs). Traditional fault detection and
mitigation approaches, while effective for AccDNNs, introduce substantial
overhead and latency, making them impractical for energy-constrained real-time
deployment. To address this, we introduce EPSILON, a lightweight framework that
leverages pre-computed statistical signatures and layer-wise importance metrics
for efficient fault detection and mitigation in AxDNNs. Our framework
introduces a novel non-parametric pattern-matching algorithm that enables
constant-time fault detection without interrupting normal execution while
dynamically adapting to different network architectures and fault patterns.
EPSILON maintains model accuracy by intelligently adjusting mitigation
strategies based on a statistical analysis of weight distribution and layer
criticality while preserving the energy benefits of approximate computing.
Extensive evaluations across various approximate multipliers, AxDNN
architectures, popular datasets (MNIST, CIFAR-10, CIFAR-100, ImageNet-1k), and
fault scenarios demonstrate that EPSILON maintains 80.05\% accuracy while
offering 22\% improvement in inference time and 28\% improvement in energy
efficiency, establishing EPSILON as a practical solution for deploying reliable
AxDNNs in safety-critical edge applications.

</details>


### [215] [GenTorrent: Scaling Large Language Model Serving with An Overley Network](https://arxiv.org/abs/2504.20101)
*Fei Fang,Yifan Hua,Shengze Wang,Ruilin Zhou,Yi Liu,Chen Qian,Xiaoxue Zhang*

Main category: cs.DC

TL;DR: GenTorrent提出了一种基于P2P网络的去中心化LLM服务框架，解决了小组织和个人的LLM部署与测试难题，降低了50%的延迟。


<details>
  <summary>Details</summary>
Motivation: 开源和低成本LLM的研究进展显著，但服务扩展性对小组织和个人仍是挑战。

Method: 利用去中心化贡献者的计算资源，提出GenTorrent框架，解决网络组织、隐私、资源效率和验证等关键问题。

Result: 原型测试显示延迟降低50%，安全特性对性能影响极小。

Conclusion: GenTorrent为未来AI服务的民主化和扩展提供了新方向。

Abstract: While significant progress has been made in research and development on
open-source and cost-efficient large-language models (LLMs), serving
scalability remains a critical challenge, particularly for small organizations
and individuals seeking to deploy and test their LLM innovations. Inspired by
peer-to-peer networks that leverage decentralized overlay nodes to increase
throughput and availability, we propose GenTorrent, an LLM serving overlay that
harnesses computing resources from decentralized contributors. We identify four
key research problems inherent to enabling such a decentralized infrastructure:
1) overlay network organization; 2) LLM communication privacy; 3) overlay
forwarding for resource efficiency; and 4) verification of serving quality.
This work presents the first systematic study of these fundamental problems in
the context of decentralized LLM serving. Evaluation results from a prototype
implemented on a set of decentralized nodes demonstrate that GenTorrent
achieves a latency reduction of over 50% compared to the baseline design
without overlay forwarding. Furthermore, the security features introduce
minimal overhead to serving latency and throughput. We believe this work
pioneers a new direction for democratizing and scaling future AI serving
capabilities.

</details>


### [216] [Electricity Cost Minimization for Multi-Workflow Allocation in Geo-Distributed Data Centers](https://arxiv.org/abs/2504.20105)
*Shuang Wang,He Zhang,Tianxing Wu,Yueyou Zhang,Wei Emma Zhang,Quan Z. Sheng*

Main category: cs.DC

TL;DR: 论文提出了一种地理分布式数据中心（GDCs）中电力成本感知的多工作流调度算法（ECMWS），旨在降低电力成本并满足工作流应用的截止时间约束。


<details>
  <summary>Details</summary>
Motivation: 地理分布式数据中心的电力成本因地理位置和时间而异，如何在不违反工作流截止时间的前提下降低电力成本是一个重要问题。

Method: 提出了ECMWS算法，包括工作流排序、截止时间划分、任务排序和资源分配四个阶段，并利用图嵌入模型和策略网络解决马尔可夫决策过程（MDP）。

Result: 实验表明，ECMWS算法在两种工作流实例上显著优于现有方法，性能提升超过15%，同时保持可接受的计算时间。

Conclusion: ECMWS算法有效解决了地理分布式数据中心中电力成本优化和工作流调度的问题，具有实际应用价值。

Abstract: Worldwide, Geo-distributed Data Centers (GDCs) provide computing and storage
services for massive workflow applications, resulting in high electricity costs
that vary depending on geographical locations and time. How to reduce
electricity costs while satisfying the deadline constraints of workflow
applications is important in GDCs, which is determined by the execution time of
servers, power, and electricity price. Determining the completion time of
workflows with different server frequencies can be challenging, especially in
scenarios with heterogeneous computing resources in GDCs. Moreover, the
electricity price is also different in geographical locations and may change
dynamically. To address these challenges, we develop a geo-distributed system
architecture and propose an Electricity Cost aware Multiple Workflows
Scheduling algorithm (ECMWS) for servers of GDCs with fixed frequency and
power. ECMWS comprises four stages, namely workflow sequencing, deadline
partitioning, task sequencing, and resource allocation where two graph
embedding models and a policy network are constructed to solve the Markov
Decision Process (MDP). After statistically calibrating parameters and
algorithm components over a comprehensive set of workflow instances, the
proposed algorithms are compared with the state-of-the-art methods over two
types of workflow instances. The experimental results demonstrate that our
proposed algorithm significantly outperforms other algorithms, achieving an
improvement of over 15\% while maintaining an acceptable computational time.
The source codes are available at
https://gitee.com/public-artifacts/ecmws-experiments.

</details>


### [217] [Tempo: Application-aware LLM Serving with Mixed SLO Requirements](https://arxiv.org/abs/2504.20068)
*Wei Zhang,Zhiyu Wu,Yi Mu,Banruo Liu,Myungjin Lee,Fan Lai*

Main category: cs.DC

TL;DR: Tempo是一个SLO感知的调度器，旨在最大化大型语言模型（LLM）工作负载的服务增益，通过动态分配资源满足SLO要求。


<details>
  <summary>Details</summary>
Motivation: 现有调度器无法应对LLM工作负载的多样性（如延迟敏感、吞吐密集型等）和不可预测性（如响应长度、运行时依赖）。

Method: Tempo采用混合调度策略，包括基于分位数的响应上限估计、依赖图匹配、按服务增益密度优先排序，并在生成过程中动态调整决策。

Result: 实验表明，Tempo在端到端服务增益上提升8.3倍，SLO良好吞吐量提升10.3倍。

Conclusion: Tempo通过动态资源分配和在线优化，显著提升了LLM工作负载的性能和效率。

Abstract: The integration of Large Language Models (LLMs) into diverse applications,
ranging from interactive chatbots and cloud AIOps to intelligent agents, has
introduced a wide spectrum of Service Level Objectives (SLOs) for
responsiveness. These workloads include latency-sensitive requests focused on
per-token latency in streaming chat, throughput-intensive requests that require
rapid full responses to invoke tools, and collective requests with dynamic
dependencies arising from self-reflection or agent-based reasoning. This
workload diversity, amplified by unpredictable request information such as
response lengths and runtime dependencies, makes existing schedulers inadequate
even within their design envelopes.
  In this paper, we define service gain as the useful service delivered by
completing requests. We observe that as SLO directly reflects the actual
performance needs of requests, completing a request much faster than its SLO
(e.g., deadline) yields limited additional service gain. Based on this insight,
we introduce Tempo, the first systematic SLO-aware scheduler designed to
maximize service gain across diverse LLM workloads. Tempo allocates just enough
serving bandwidth to meet each SLO, maximizing residual capacity for others
best-effort workloads. Instead of assuming request information or none at all,
it adopts a hybrid scheduling strategy: using quantile-based response upper
bounds and dependency-graph matching for conservative initial estimates,
prioritizing requests by service gain density, and refining decisions online as
generation progresses. Our evaluation across diverse workloads, including chat,
reasoning, and agentic pipelines, shows that Tempo improves end-to-end service
gain by up to 8.3$\times$ and achieves up to 10.3$\times$ SLO goodput compared
to state-of-the-art designs

</details>


### [218] [Leveraging Neural Graph Compilers in Machine Learning Research for Edge-Cloud Systems](https://arxiv.org/abs/2504.20198)
*Alireza Furutanpey,Carmen Walser,Philipp Raith,Pantelis A. Frangoudis,Schahram Dustdar*

Main category: cs.DC

TL;DR: 本文全面评估了神经网络图编译器在异构硬件平台上的表现，揭示了理论优化与实际部署之间的差距，并提出了新的性能度量方法。


<details>
  <summary>Details</summary>
Motivation: 解决理论优化技术与实际部署场景之间的关键差距，揭示编译器对性能比较的影响。

Method: 通过系统分析和细粒度块级实验，评估不同神经网络架构和批量大小下的编译器性能。

Result: 发现编译器性能高度依赖架构和批量大小，特定编译器能利用简单架构中的重复模式，显著提升吞吐量。

Conclusion: 本文方法为实践者在异构硬件环境中优化性能提供了实用见解，弥合了学术研究与实际部署的差距。

Abstract: This work presents a comprehensive evaluation of neural network graph
compilers across heterogeneous hardware platforms, addressing the critical gap
between theoretical optimization techniques and practical deployment scenarios.
We demonstrate how vendor-specific optimizations can invalidate relative
performance comparisons between architectural archetypes, with performance
advantages sometimes completely reversing after compilation. Our systematic
analysis reveals that graph compilers exhibit performance patterns highly
dependent on both neural architecture and batch sizes. Through fine-grained
block-level experimentation, we establish that vendor-specific compilers can
leverage repeated patterns in simple architectures, yielding disproportionate
throughput gains as model depth increases. We introduce novel metrics to
quantify a compiler's ability to mitigate performance friction as batch size
increases. Our methodology bridges the gap between academic research and
practical deployment by incorporating compiler effects throughout the research
process, providing actionable insights for practitioners navigating complex
optimization landscapes across heterogeneous hardware environments.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [219] [Fostering Self-Directed Growth with Generative AI: Toward a New Learning Analytics Framework](https://arxiv.org/abs/2504.20851)
*Qianrun Mao*

Main category: cs.CY

TL;DR: 本文提出了一种结合生成式人工智能和学习分析的新框架A2PL，旨在培养学习者的自我导向成长能力。


<details>
  <summary>Details</summary>
Motivation: 在去中心化知识生态系统和AI技术普及的背景下，如何培养学习者的可持续自主能力成为教育的关键需求。

Method: 研究提出了A2PL模型，整合了学习者愿望、复杂思维和总结性自我评估，探讨了未来干预设计和学习分析的应用。

Result: A2PL模型为数字时代构建公平、适应性强且可持续的学习系统提供了理论基础。

Conclusion: 自我导向成长是数字时代学习系统发展的关键，A2PL模型为此提供了新的研究方向。

Abstract: In an era increasingly shaped by decentralized knowledge ecosystems and
pervasive AI technologies, fostering sustainable learner agency has become a
critical educational imperative. This study introduces a novel conceptual
framework integrating Generative Artificial Intelligence and Learning Analytics
to cultivate Self-Directed Growth, a dynamic competency that enables learners
to iteratively drive their own developmental pathways across diverse
contexts.Building upon critical gaps in current research on Self Directed
Learning and AI-mediated education, the proposed Aspire to Potentials for
Learners (A2PL) model reconceptualizes the interplay of learner aspirations,
complex thinking, and summative self-assessment within GAI supported
environments.Methodological implications for future intervention design and
learning analytics applications are discussed, positioning Self-Directed Growth
as a pivotal axis for developing equitable, adaptive, and sustainable learning
systems in the digital era.

</details>


### [220] [When Testing AI Tests Us: Safeguarding Mental Health on the Digital Frontlines](https://arxiv.org/abs/2504.20910)
*Sachin R. Pendse,Darren Gergle,Rachel Kornfield,Jonah Meyerhoff,David Mohr,Jina Suh,Annie Wescott,Casey Williams,Jessica Schleider*

Main category: cs.CY

TL;DR: 本文探讨了AI红队成员的心理健康问题，提出了保护措施。


<details>
  <summary>Details</summary>
Motivation: 生成式AI的黑箱特性要求红队成员通过互动测试模型，这种对抗性工作可能导致心理健康问题，需引起重视。

Method: 通过分析红队工作的心理健康影响，并借鉴其他职业（如演员、心理健康专家等）的保护措施，提出应对策略。

Result: 提出了针对红队成员心理健康的个体和组织层面的保护策略。

Conclusion: 保护红队成员的心理健康是AI安全的重要组成部分，需借鉴其他职业的经验并制定适应性措施。

Abstract: Red-teaming is a core part of the infrastructure that ensures that AI models
do not produce harmful content. Unlike past technologies, the black box nature
of generative AI systems necessitates a uniquely interactional mode of testing,
one in which individuals on red teams actively interact with the system,
leveraging natural language to simulate malicious actors and solicit harmful
outputs. This interactional labor done by red teams can result in mental health
harms that are uniquely tied to the adversarial engagement strategies necessary
to effectively red team. The importance of ensuring that generative AI models
do not propagate societal or individual harm is widely recognized -- one less
visible foundation of end-to-end AI safety is also the protection of the mental
health and wellbeing of those who work to keep model outputs safe. In this
paper, we argue that the unmet mental health needs of AI red-teamers is a
critical workplace safety concern. Through analyzing the unique mental health
impacts associated with the labor done by red teams, we propose potential
individual and organizational strategies that could be used to meet these
needs, and safeguard the mental health of red-teamers. We develop our proposed
strategies through drawing parallels between common red-teaming practices and
interactional labor common to other professions (including actors, mental
health professionals, conflict photographers, and content moderators),
describing how individuals and organizations within these professional spaces
safeguard their mental health given similar psychological demands. Drawing on
these protective practices, we describe how safeguards could be adapted for the
distinct mental health challenges experienced by red teaming organizations as
they mitigate emerging technological risks on the new digital frontlines.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [221] [Coreset selection for the Sinkhorn divergence and generic smooth divergences](https://arxiv.org/abs/2504.20194)
*Alex Kokot,Alex Luedtke*

Main category: stat.ML

TL;DR: CO2是一种高效算法，用于生成基于通用平滑散度的凸加权核心集。通过功能泰勒展开，将核心集选择问题转化为最大均值差异最小化，并应用于Sinkhorn散度，实现对数级数据点采样。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决核心集选择问题，通过功能泰勒展开和最大均值差异最小化，提供更高效的采样方法。

Method: 采用功能泰勒展开，将问题转化为最大均值差异最小化，并应用于Sinkhorn散度。

Result: 算法实现了对数级数据点采样，匹配随机采样的近似保证，并验证了熵正则化最优传输的新规律性。

Conclusion: CO2为核积分和经典统计方法提供了新视角，展示了在图像数据子采样中的实际应用，并指出了未来改进方向。

Abstract: We introduce CO2, an efficient algorithm to produce convexly-weighted
coresets with respect to generic smooth divergences. By employing a functional
Taylor expansion, we show a local equivalence between sufficiently regular
losses and their second order approximations, reducing the coreset selection
problem to maximum mean discrepancy minimization. We apply CO2 to the Sinkhorn
divergence, providing a novel sampling procedure that requires logarithmically
many data points to match the approximation guarantees of random sampling. To
show this, we additionally verify several new regularity properties for
entropically regularized optimal transport of independent interest. Our
approach leads to a new perspective linking coreset selection and kernel
quadrature to classical statistical methods such as moment and score matching.
We showcase this method with a practical application of subsampling image data,
and highlight key directions to explore for improved algorithmic efficiency and
theoretical guarantees.

</details>


### [222] [Sobolev norm inconsistency of kernel interpolation](https://arxiv.org/abs/2504.20617)
*Yunfei Yang*

Main category: stat.ML

TL;DR: 论文研究了在再生核希尔伯特空间中最小范数插值的一致性，发现核插值在特定条件下总是不一致的。


<details>
  <summary>Details</summary>
Motivation: 探讨核插值在再生核希尔伯特空间中的泛化误差，特别是在不同范数尺度下的表现。

Method: 通过分析核插值的泛化误差下界，结合假设空间的嵌入指数和特征值衰减率。

Result: 核插值在范数的光滑指数大于特定常数时总是不一致的。

Conclusion: 核插值的一致性受限于假设空间的特性和特征值衰减率，无法在所有情况下保持一致。

Abstract: We study the consistency of minimum-norm interpolation in reproducing kernel
Hilbert spaces corresponding to bounded kernels. Our main result give lower
bounds for the generalization error of the kernel interpolation measured in a
continuous scale of norms that interpolate between $L^2$ and the hypothesis
space. These lower bounds imply that kernel interpolation is always
inconsistent, when the smoothness index of the norm is larger than a constant
that depends only on the embedding index of the hypothesis space and the decay
rate of the eigenvalues.

</details>


### [223] [Learning and Generalization with Mixture Data](https://arxiv.org/abs/2504.20651)
*Harsh Vardhan,Avishek Ghosh,Arya Mazumdar*

Main category: stat.ML

TL;DR: 该论文研究了从混合分布中采样的数据在机器学习中的泛化性能和统计速率，重点关注了混合数据在何种范围内可被视为单一分布。


<details>
  <summary>Details</summary>
Motivation: 数据异质性是现代大规模学习的主要挑战之一，论文旨在通过混合模型表征异质数据，并分析其泛化性能和统计速率。

Method: 通过表征混合分布的异质性（基于子总体分布之间的总变差距离），并利用Rademacher复杂度和高斯复杂度界限，分析泛化和收敛速率。

Result: 研究发现，随着函数类复杂度的增加，对总变差距离的要求更严格。对于混合线性回归，论文提供了泛化误差的紧界。

Conclusion: 混合数据在特定条件下可被视为单一分布，但函数类复杂度越高，对异质性的要求越严格。

Abstract: In many, if not most, machine learning applications the training data is
naturally heterogeneous (e.g. federated learning, adversarial attacks and
domain adaptation in neural net training). Data heterogeneity is identified as
one of the major challenges in modern day large-scale learning. A classical way
to represent heterogeneous data is via a mixture model. In this paper, we study
generalization performance and statistical rates when data is sampled from a
mixture distribution. We first characterize the heterogeneity of the mixture in
terms of the pairwise total variation distance of the sub-population
distributions. Thereafter, as a central theme of this paper, we characterize
the range where the mixture may be treated as a single (homogeneous)
distribution for learning. In particular, we study the generalization
performance under the classical PAC framework and the statistical error rates
for parametric (linear regression, mixture of hyperplanes) as well as
non-parametric (Lipschitz, convex and H\"older-smooth) regression problems. In
order to do this, we obtain Rademacher complexity and (local) Gaussian
complexity bounds with mixture data, and apply them to get the generalization
and convergence rates respectively. We observe that as the (regression)
function classes get more complex, the requirement on the pairwise total
variation distance gets stringent, which matches our intuition. We also do a
finer analysis for the case of mixed linear regression and provide a tight
bound on the generalization error in terms of heterogeneity.

</details>


### [224] [Preference-centric Bandits: Optimality of Mixtures and Regret-efficient Algorithms](https://arxiv.org/abs/2504.20877)
*Meltem Tatlı,Arpan Mukherjee,Prashanth L. A.,Karthikeyan Shanmugam,Ali Tajer*

Main category: stat.ML

TL;DR: 论文提出了一种基于偏好度量（PM）的多臂老虎机框架，取代传统的期望值评估，强调风险厌恶和不确定性态度，并设计了两种算法来高效学习混合策略。


<details>
  <summary>Details</summary>
Motivation: 传统的多臂老虎机方法仅关注奖励的期望值，忽略了分布的尾部行为和风险。本文旨在通过偏好度量（PM）引入更丰富的偏好建模，以更好地反映决策中的风险和不确定性。

Method: 论文提出了PM-centric框架，并设计了两种算法（horizon-dependent和anytime），通过学习混合策略来优化PM。算法包括估计最优混合和跟踪机制。

Result: 算法在多种PM代数形式下具有高效的遗憾保证，能够学习并跟踪最优混合策略。

Conclusion: PM-centric框架为多臂老虎机提供了更灵活的偏好建模方式，算法设计原则与传统方法显著不同，适用于风险敏感的决策场景。

Abstract: The objective of canonical multi-armed bandits is to identify and repeatedly
select an arm with the largest reward, often in the form of the expected value
of the arm's probability distribution. Such a utilitarian perspective and focus
on the probability models' first moments, however, is agnostic to the
distributions' tail behavior and their implications for variability and risks
in decision-making. This paper introduces a principled framework for shifting
from expectation-based evaluation to an alternative reward formulation, termed
a preference metric (PM). The PMs can place the desired emphasis on different
reward realization and can encode a richer modeling of preferences that
incorporate risk aversion, robustness, or other desired attitudes toward
uncertainty. A fundamentally distinct observation in such a PM-centric
perspective is that designing bandit algorithms will have a significantly
different principle: as opposed to the reward-based models in which the optimal
sampling policy converges to repeatedly sampling from the single best arm, in
the PM-centric framework the optimal policy converges to selecting a mix of
arms based on specific mixing weights. Designing such mixture policies departs
from the principles for designing bandit algorithms in significant ways,
primarily because of uncountable mixture possibilities. The paper formalizes
the PM-centric framework and presents two algorithm classes (horizon-dependent
and anytime) that learn and track mixtures in a regret-efficient fashion. These
algorithms have two distinctions from their canonical counterparts: (i) they
involve an estimation routine to form reliable estimates of optimal mixtures,
and (ii) they are equipped with tracking mechanisms to navigate arm selection
fractions to track the optimal mixtures. These algorithms' regret guarantees
are investigated under various algebraic forms of the PMs.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [225] [Narrative-Centered Emotional Reflection: Scaffolding Autonomous Emotional Literacy with AI](https://arxiv.org/abs/2504.20342)
*Shou-Tzu Han*

Main category: cs.HC

TL;DR: Reflexion是一个AI平台，通过情感检测、反思提示和隐喻故事生成，帮助用户进行深层次情感探索，初步研究显示其在情感表达和心理韧性方面有积极效果。


<details>
  <summary>Details</summary>
Motivation: 基于表达性写作、认知重构、自我决定和批判意识发展理论，旨在提升用户的情感认知和心理健康。

Method: 整合实时情感检测、分层反思提示和隐喻故事生成，引导用户从情感识别到价值驱动的行动计划。

Result: 初步研究显示用户在情感表达、认知重构和心理韧性方面有显著提升。

Conclusion: Reflexion为教育、治疗和公共卫生领域提供了一种可扩展的情感计算干预方法。

Abstract: Reflexion is an AI-powered platform designed to enable structured emotional
self-reflection at scale. By integrating real-time emotion detection, layered
reflective prompting, and metaphorical storytelling generation, Reflexion
empowers users to engage in autonomous emotional exploration beyond basic
sentiment categorization. Grounded in theories of expressive writing, cognitive
restructuring, self-determination, and critical consciousness development, the
system scaffolds a progressive journey from surface-level emotional recognition
toward value-aligned action planning. Initial pilot studies with diverse
participants demonstrate positive outcomes in emotional articulation, cognitive
reframing, and perceived psychological resilience. Reflexion represents a
promising direction for scalable, theory-informed affective computing
interventions aimed at fostering emotional literacy and psychological growth
across educational, therapeutic, and public health contexts.

</details>


### [226] [In defence of post-hoc explanations in medical AI](https://arxiv.org/abs/2504.20741)
*Joshua Hatherley,Lauritz Munch,Jens Christian Bjerring*

Main category: cs.HC

TL;DR: 本文为后解释在医疗AI中的价值辩护，认为尽管其不复制黑盒系统的实际推理过程，但仍能提升用户的功能性理解、提高临床-AI团队的准确性，并帮助临床医生证明其AI支持的决策。


<details>
  <summary>Details</summary>
Motivation: 回应近期对后解释在医疗AI中价值的批评，证明其尽管不完美，仍具有实际用途。

Method: 通过论证后解释的功能性价值，反驳批评观点。

Result: 后解释能提升用户理解、团队准确性和决策合理性。

Conclusion: 后解释虽非完美解决方案，但在医疗AI中仍是有用的策略。

Abstract: Since the early days of the Explainable AI movement, post-hoc explanations
have been praised for their potential to improve user understanding, promote
trust, and reduce patient safety risks in black box medical AI systems.
Recently, however, critics have argued that the benefits of post-hoc
explanations are greatly exaggerated since they merely approximate, rather than
replicate, the actual reasoning processes that black box systems take to arrive
at their outputs. In this article, we aim to defend the value of post-hoc
explanations against this recent critique. We argue that even if post-hoc
explanations do not replicate the exact reasoning processes of black box
systems, they can still improve users' functional understanding of black box
systems, increase the accuracy of clinician-AI teams, and assist clinicians in
justifying their AI-informed decisions. While post-hoc explanations are not a
"silver bullet" solution to the black box problem in medical AI, we conclude
that they remain a useful strategy for addressing the black box problem in
medical AI.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [227] [PRISM: Projection-based Reward Integration for Scene-Aware Real-to-Sim-to-Real Transfer with Few Demonstrations](https://arxiv.org/abs/2504.20520)
*Haowen Sun,Han Wang,Chengzhong Ma,Shaolong Zhang,Jiawei Ye,Xingyu Chen,Xuguang Lan*

Main category: cs.RO

TL;DR: 提出了一种结合真实到模拟再到真实的流程，通过专家演示构建仿真环境，并利用视觉语言模型（VLM）设计奖励模型，最终训练出适用于真实场景的鲁棒机器人控制策略。


<details>
  <summary>Details</summary>
Motivation: 解决机器人初始位置和物体姿态变化时，从少量演示中学习鲁棒策略的实际问题。模仿学习难以泛化，而强化学习（RL）虽能自主探索，但直接与真实世界交互不切实际且不安全，构建仿真环境又需大量人工。

Method: 提出集成流程：1）基于专家演示构建仿真环境，通过图像识别场景物体并检索3D模型；2）设计基于投影的奖励模型，由VLM监督，使用人类引导的物体投影关系作为提示；3）利用专家演示进一步微调策略。

Result: 成功构建了仿真环境并训练出鲁棒的RL策略，适用于真实场景部署。

Conclusion: 该工作通过仿真环境构建和RL策略训练，实现了在真实场景中部署可靠机器人控制策略的目标。

Abstract: Learning from few demonstrations to develop policies robust to variations in
robot initial positions and object poses is a problem of significant practical
interest in robotics. Compared to imitation learning, which often struggles to
generalize from limited samples, reinforcement learning (RL) can autonomously
explore to obtain robust behaviors. Training RL agents through direct
interaction with the real world is often impractical and unsafe, while building
simulation environments requires extensive manual effort, such as designing
scenes and crafting task-specific reward functions. To address these
challenges, we propose an integrated real-to-sim-to-real pipeline that
constructs simulation environments based on expert demonstrations by
identifying scene objects from images and retrieving their corresponding 3D
models from existing libraries. We introduce a projection-based reward model
for RL policy training that is supervised by a vision-language model (VLM)
using human-guided object projection relationships as prompts, with the policy
further fine-tuned using expert demonstrations. In general, our work focuses on
the construction of simulation environments and RL-based policy training,
ultimately enabling the deployment of reliable robotic control policies in
real-world scenarios.

</details>


### [228] [SoccerDiffusion: Toward Learning End-to-End Humanoid Robot Soccer from Gameplay Recordings](https://arxiv.org/abs/2504.20808)
*Florian Vahl,Jörn Griepenburg,Jan Gutsche,Jasper Güldenstein,Jianwei Zhang*

Main category: cs.RO

TL;DR: SoccerDiffusion是一种基于Transformer的扩散模型，用于从真实足球比赛录像中学习人形机器人的端到端控制策略。


<details>
  <summary>Details</summary>
Motivation: 通过从RoboCup比赛中收集的数据，直接学习复杂动作行为（如行走、踢球和摔倒恢复），为后续强化学习或偏好优化方法奠定基础。

Method: 使用多模态传感器输入（视觉、本体感觉和游戏状态）预测关节命令轨迹，并通过蒸馏技术实现嵌入式平台上的实时推理。

Result: 模型在仿真和物理机器人上成功复现了复杂动作行为，但高级战术行为仍有局限。

Conclusion: 该研究为后续强化学习或优化方法提供了坚实基础，并公开了数据集、预训练模型和代码。

Abstract: This paper introduces SoccerDiffusion, a transformer-based diffusion model
designed to learn end-to-end control policies for humanoid robot soccer
directly from real-world gameplay recordings. Using data collected from RoboCup
competitions, the model predicts joint command trajectories from multi-modal
sensor inputs, including vision, proprioception, and game state. We employ a
distillation technique to enable real-time inference on embedded platforms that
reduces the multi-step diffusion process to a single step. Our results
demonstrate the model's ability to replicate complex motion behaviors such as
walking, kicking, and fall recovery both in simulation and on physical robots.
Although high-level tactical behavior remains limited, this work provides a
robust foundation for subsequent reinforcement learning or preference
optimization methods. We release the dataset, pretrained models, and code
under: https://bit-bots.github.io/SoccerDiffusion

</details>


### [229] [XPG-RL: Reinforcement Learning with Explainable Priority Guidance for Efficiency-Boosted Mechanical Search](https://arxiv.org/abs/2504.20969)
*Yiting Zhang,Shichen Li,Elena Shrestha*

Main category: cs.RO

TL;DR: XPG-RL是一种强化学习框架，通过可解释的优先级决策和原始感官输入，在杂乱环境中高效完成机械搜索任务。


<details>
  <summary>Details</summary>
Motivation: 解决杂乱环境中机械搜索的长时程规划和部分可观测性挑战。

Method: 结合任务驱动的动作优先级机制和上下文感知切换策略，动态选择动作基元（如抓取、遮挡移除、视角调整），并优化策略输出自适应阈值。感知模块融合RGB-D输入与语义几何特征生成场景表示。

Result: 在仿真和真实环境中，XPG-RL在任务成功率和运动效率上显著优于基线方法，长时程任务效率提升4.5倍。

Conclusion: 将领域知识与可学习决策策略结合，能实现鲁棒高效的机器人操作。

Abstract: Mechanical search (MS) in cluttered environments remains a significant
challenge for autonomous manipulators, requiring long-horizon planning and
robust state estimation under occlusions and partial observability. In this
work, we introduce XPG-RL, a reinforcement learning framework that enables
agents to efficiently perform MS tasks through explainable, priority-guided
decision-making based on raw sensory inputs. XPG-RL integrates a task-driven
action prioritization mechanism with a learned context-aware switching strategy
that dynamically selects from a discrete set of action primitives such as
target grasping, occlusion removal, and viewpoint adjustment. Within this
strategy, a policy is optimized to output adaptive threshold values that govern
the discrete selection among action primitives. The perception module fuses
RGB-D inputs with semantic and geometric features to produce a structured scene
representation for downstream decision-making. Extensive experiments in both
simulation and real-world settings demonstrate that XPG-RL consistently
outperforms baseline methods in task success rates and motion efficiency,
achieving up to 4.5$\times$ higher efficiency in long-horizon tasks. These
results underscore the benefits of integrating domain knowledge with learnable
decision-making policies for robust and efficient robotic manipulation.

</details>


<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [230] [A Physically Driven Long Short Term Memory Model for Estimating Snow Water Equivalent over the Continental United States](https://arxiv.org/abs/2504.20129)
*Arun M. Saranathan,Mahmoud Saeedimoghaddam,Brandon Smith,Deepthi Raghunandan,Grey Nearing,Craig Pelissier*

Main category: physics.ao-ph

TL;DR: 该论文提出了一种基于LSTM网络的方法，用于估计雪水当量（SWE），解决了现有再分析产品计算成本高和现场测量稀疏的问题。


<details>
  <summary>Details</summary>
Motivation: 季节性雪的估计对地表模型至关重要，但现有方法（如再分析产品和现场测量）存在计算成本高或数据稀疏的问题。

Method: 构建了一个LSTM网络，将SWE估计分为分类任务（判断雪是否存在）和回归任务（估计SWE高度），并使用SNOTEL数据训练模型。

Result: 模型在雪存在分类任务中准确率≥93%，SWE估计的相关系数∼0.9，并能泛化到未见过的时空数据。

Conclusion: LSTM模型能有效估计SWE，解决了现有方法的局限性，并展示了良好的泛化能力。

Abstract: Snow is an essential input for various land surface models. Seasonal snow
estimates are available as snow water equivalent (SWE) from process-based
reanalysis products or locally from in situ measurements. While the reanalysis
products are computationally expensive and available at only fixed spatial and
temporal resolutions, the in situ measurements are highly localized and sparse.
To address these issues and enable the analysis of the effect of a large suite
of physical, morphological, and geological conditions on the presence and
amount of snow, we build a Long Short-Term Memory (LSTM) network, which is able
to estimate the SWE based on time series input of the various
physical/meteorological factors as well static spatial/morphological factors.
Specifically, this model breaks down the SWE estimation into two separate
tasks: (i) a classification task that indicates the presence/absence of snow on
a specific day and (ii) a regression task that indicates the height of the SWE
on a specific day in the case of snow presence. The model is trained using
physical/in situ SWE measurements from the SNOw TELemetry (SNOTEL) snow pillows
in the western United States. We will show that trained LSTM models have a
classification accuracy of $\geq 93\%$ for the presence of snow and a
coefficient of correlation of $\sim 0.9$ concerning their SWE estimates. We
will also demonstrate that the models can generalize both spatially and
temporally to previously unseen data.

</details>


### [231] [Testing the Limit of Atmospheric Predictability with a Machine Learning Weather Model](https://arxiv.org/abs/2504.20238)
*P. Trent Vonich,Gregory J. Hakim*

Main category: physics.ao-ph

TL;DR: 论文挑战了传统认为确定性天气预报技能极限为14天的观点，通过机器学习模型GraphCast优化初始条件，将技能预报延长至30天以上，并显著减少误差。


<details>
  <summary>Details</summary>
Motivation: 传统观点认为确定性天气预报的技能极限为14天，但作者希望通过优化初始条件，突破这一限制。

Method: 使用GraphCast机器学习模型，通过梯度优化技术对2020年每日两次的预报初始条件进行优化。

Result: 在10天预报中平均误差减少86%，技能预报可持续30天以上；优化初始条件在Pangu-Weather模型中也能减少21%误差。

Conclusion: 研究表明，通过准确初始条件，确定性预报技能可以远超两周，挑战了大气可预测性的传统假设。

Abstract: Atmospheric predictability research has long held that the limit of skillful
deterministic weather forecasts is about 14 days. We challenge this limit using
GraphCast, a machine-learning weather model, by optimizing forecast initial
conditions using gradient-based techniques for twice-daily forecasts spanning
2020. This approach yields an average error reduction of 86% at 10 days, with
skill lasting beyond 30 days. Mean optimal initial-condition perturbations
reveal large-scale, spatially coherent corrections to ERA5, primarily
reflecting an intensification of the Hadley circulation. Forecasts using
GraphCast-optimal initial conditions in the Pangu-Weather model achieve a 21%
error reduction, peaking at 4 days, indicating that analysis corrections
reflect a combination of both model bias and a reduction in analysis error.
These results demonstrate that, given accurate initial conditions, skillful
deterministic forecasts are consistently achievable far beyond two weeks,
challenging long-standing assumptions about the limits of atmospheric
predictability.

</details>


### [232] [Data Driven Deep Learning for Correcting Global Climate Model Projections of SST and DSL in the Bay of Bengal](https://arxiv.org/abs/2504.20620)
*Abhishek Pasula,Deepak N. Subramani*

Main category: physics.ao-ph

TL;DR: 论文提出了一种基于深度学习的偏差校正方法，用于修正CMIP6气候模型在孟加拉湾的预测误差，相比传统方法显著降低了RMSE。


<details>
  <summary>Details</summary>
Motivation: 气候变化对孟加拉湾的海洋条件和季风降水有重要影响，但现有气候模型与再分析数据之间存在显著偏差，需要更准确的校正方法。

Method: 使用数据驱动的深度学习模型，以气候模型输出为输入，ORAS5再分析数据为输出，进行训练和验证。

Result: 新方法将SST和DSL的RMSE分别降低了0.15C和0.3m，优于传统EDCDF方法。

Conclusion: 深度学习模型有效校正了气候模型的偏差，为未来气候预测提供了更可靠的数据。

Abstract: Climate change alters ocean conditions, notably temperature and sea level. In
the Bay of Bengal, these changes influence monsoon precipitation and marine
productivity, critical to the Indian economy. In Phase 6 of the Coupled Model
Intercomparison Project (CMIP6), Global Climate Models (GCMs) use different
shared socioeconomic pathways (SSPs) to obtain future climate projections.
However, significant discrepancies are observed between these models and the
reanalysis data in the Bay of Bengal for 2015-2024. Specifically, the root mean
square error (RMSE) between the climate model output and the Ocean Reanalysis
System (ORAS5) is 1.2C for the sea surface temperature (SST) and 1.1 m for the
dynamic sea level (DSL). We introduce a new data-driven deep learning model to
correct for this bias. The deep neural model for each variable is trained using
pairs of climatology-removed monthly climate projections as input and the
corresponding month's ORAS5 as output. This model is trained with historical
data (1950 to 2014), validated with future projection data from 2015 to 2020,
and tested with future projections from 2021 to 2023. Compared to the
conventional EquiDistant Cumulative Distribution Function (EDCDF) statistical
method for bias correction in climate models, our approach decreases RMSE by
0.15C for SST and 0.3 m for DSL. The trained model subsequently corrects the
projections for 2024-2100. A detailed analysis of the monthly, seasonal, and
decadal means and variability is performed to underscore the implications of
the novel dynamics uncovered in our corrected projections.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [233] [ResearchCodeAgent: An LLM Multi-Agent System for Automated Codification of Research Methodologies](https://arxiv.org/abs/2504.20117)
*Shubham Gandhi,Dhruv Shah,Manasi Patwardhan,Lovekesh Vig,Gautam Shroff*

Main category: cs.SE

TL;DR: ResearchCodeAgent是一个基于大语言模型的多智能体系统，用于自动化机器学习文献中研究方法的代码生成，显著减少编码时间并提高代码质量。


<details>
  <summary>Details</summary>
Motivation: 解决机器学习研究中高概念与具体实现之间的鸿沟，帮助研究者快速生成基准代码或扩展现有方法。

Method: 采用灵活的智能体架构和动态规划机制，结合短期和长期记忆，支持上下文感知的研究环境交互。

Result: 在三个机器学习任务中，46.9%的生成代码高质量且无错误，25%优于基线实现，编码时间平均减少57.9%。

Conclusion: ResearchCodeAgent为自动化研究实现提供了重要进展，有望加速机器学习研究进程。

Abstract: In this paper we introduce ResearchCodeAgent, a novel multi-agent system
leveraging large language models (LLMs) agents to automate the codification of
research methodologies described in machine learning literature. The system
bridges the gap between high-level research concepts and their practical
implementation, allowing researchers auto-generating code of existing research
papers for benchmarking or building on top-of existing methods specified in the
literature with availability of partial or complete starter code.
ResearchCodeAgent employs a flexible agent architecture with a comprehensive
action suite, enabling context-aware interactions with the research
environment. The system incorporates a dynamic planning mechanism, utilizing
both short and long-term memory to adapt its approach iteratively. We evaluate
ResearchCodeAgent on three distinct machine learning tasks with distinct task
complexity and representing different parts of the ML pipeline: data
augmentation, optimization, and data batching. Our results demonstrate the
system's effectiveness and generalizability, with 46.9% of generated code being
high-quality and error-free, and 25% showing performance improvements over
baseline implementations. Empirical analysis shows an average reduction of
57.9% in coding time compared to manual implementation. We observe higher gains
for more complex tasks. ResearchCodeAgent represents a significant step towards
automating the research implementation process, potentially accelerating the
pace of machine learning research.

</details>


### [234] [Self-Healing Software Systems: Lessons from Nature, Powered by AI](https://arxiv.org/abs/2504.20093)
*Mohammad Baqar,Rajat Khanda,Saba Naqvi*

Main category: cs.SE

TL;DR: 论文提出了一种受生物启发的人工智能驱动自愈软件框架，结合日志分析、静态代码检查和AI生成补丁，以减少停机时间并增强软件韧性。


<details>
  <summary>Details</summary>
Motivation: 随着软件系统复杂性和规模的增加，自主检测、诊断和恢复故障的能力变得至关重要。受生物自愈机制的启发，研究旨在开发类似的自愈软件系统。

Method: 提出了一种框架，利用系统可观测性工具作为感知输入，AI模型作为诊断和修复的核心，以及修复代理应用代码和测试修改。结合日志分析、静态代码检查和AI生成补丁。

Result: 通过案例研究和模拟评估，该框架在减少停机时间、加速调试和增强软件韧性方面优于传统手动调试方法。

Conclusion: 该研究为开发智能、自适应且自给自足的软件系统奠定了基础，使其能够像生物体一样持续自愈。

Abstract: As modern software systems grow in complexity and scale, their ability to
autonomously detect, diagnose, and recover from failures becomes increasingly
vital. Drawing inspiration from biological healing - where the human body
detects damage, signals the brain, and activates targeted recovery - this paper
explores the concept of self-healing software driven by artificial
intelligence. We propose a novel framework that mimics this biological model
system observability tools serve as sensory inputs, AI models function as the
cognitive core for diagnosis and repair, and healing agents apply targeted code
and test modifications. By combining log analysis, static code inspection, and
AI-driven generation of patches or test updates, our approach aims to reduce
downtime, accelerate debugging, and enhance software resilience. We evaluate
the effectiveness of this model through case studies and simulations, comparing
it against traditional manual debugging and recovery workflows. This work paves
the way toward intelligent, adaptive and self-reliant software systems capable
of continuous healing, akin to living organisms.

</details>


### [235] [AutoP2C: An LLM-Based Agent Framework for Code Repository Generation from Multimodal Content in Academic Papers](https://arxiv.org/abs/2504.20115)
*Zijie Lin,Yiqing Shen,Qilin Cai,He Sun,Jinrui Zhou,Mingjun Xiao*

Main category: cs.SE

TL;DR: 论文提出了一种名为“Paper-to-Code”（P2C）的新任务，通过多模态内容生成可执行代码库，并开发了AutoP2C框架实现自动化。


<details>
  <summary>Details</summary>
Motivation: 将学术论文中的多模态内容（如文本、图表）转化为可执行代码是一个耗时且需要专业知识的过程，现有方法仅能生成孤立代码片段。

Method: AutoP2C框架包含四个阶段：代码库蓝图提取、多模态内容解析、分层任务分解和迭代反馈调试。

Result: 在八篇论文的测试中，AutoP2C成功生成了所有论文的可执行代码库，而其他方法仅能处理一篇。

Conclusion: AutoP2C有效解决了从论文到代码的转化问题，显著优于现有方法。

Abstract: Machine Learning (ML) research is spread through academic papers featuring
rich multimodal content, including text, diagrams, and tabular results.
However, translating these multimodal elements into executable code remains a
challenging and time-consuming process that requires substantial ML expertise.
We introduce ``Paper-to-Code'' (P2C), a novel task that transforms the
multimodal content of scientific publications into fully executable code
repositories, which extends beyond the existing formulation of code generation
that merely converts textual descriptions into isolated code snippets. To
automate the P2C process, we propose AutoP2C, a multi-agent framework based on
large language models that processes both textual and visual content from
research papers to generate complete code repositories. Specifically, AutoP2C
contains four stages: (1) repository blueprint extraction from established
codebases, (2) multimodal content parsing that integrates information from
text, equations, and figures, (3) hierarchical task decomposition for
structured code generation, and (4) iterative feedback-driven debugging to
ensure functionality and performance. Evaluation on a benchmark of eight
research papers demonstrates the effectiveness of AutoP2C, which can
successfully generate executable code repositories for all eight papers, while
OpenAI-o1 or DeepSeek-R1 can only produce runnable code for one paper. The code
is available at https://github.com/shoushouyu/Automated-Paper-to-Code.

</details>


### [236] [BLADE: Benchmark suite for LLM-driven Automated Design and Evolution of iterative optimisation heuristics](https://arxiv.org/abs/2504.20183)
*Niki van Stein,Anna V. Kononova,Haoran Yin,Thomas Bäck*

Main category: cs.SE

TL;DR: BLADE是一个模块化、可扩展的框架，用于标准化评估基于大语言模型（LLM）的自动算法设计（AAD）方法，特别关注优化启发式算法。


<details>
  <summary>Details</summary>
Motivation: 由于LLM驱动的AAD方法设计过程不透明且现有基准存在问题，需要一种标准化方法来评估其能力和局限性。

Method: BLADE整合了多种基准问题、实例生成器和文本描述，支持灵活的实验设置、标准化日志记录和过程分析工具（如代码演化图）。

Result: BLADE提供了系统评估LLM驱动AAD方法的解决方案，并通过两个用例展示了其应用。

Conclusion: BLADE为LLM驱动的AAD方法提供了一个标准化、可复现的评估框架。

Abstract: The application of Large Language Models (LLMs) for Automated Algorithm
Discovery (AAD), particularly for optimisation heuristics, is an emerging field
of research. This emergence necessitates robust, standardised benchmarking
practices to rigorously evaluate the capabilities and limitations of LLM-driven
AAD methods and the resulting generated algorithms, especially given the
opacity of their design process and known issues with existing benchmarks. To
address this need, we introduce BLADE (Benchmark suite for LLM-driven Automated
Design and Evolution), a modular and extensible framework specifically designed
for benchmarking LLM-driven AAD methods in a continuous black-box optimisation
context. BLADE integrates collections of benchmark problems (including MA-BBOB
and SBOX-COST among others) with instance generators and textual descriptions
aimed at capability-focused testing, such as generalisation, specialisation and
information exploitation. It offers flexible experimental setup options,
standardised logging for reproducibility and fair comparison, incorporates
methods for analysing the AAD process (e.g., Code Evolution Graphs and various
visualisation approaches) and facilitates comparison against human-designed
baselines through integration with established tools like IOHanalyser and
IOHexplainer. BLADE provides an `out-of-the-box' solution to systematically
evaluate LLM-driven AAD approaches. The framework is demonstrated through two
distinct use cases exploring mutation prompt strategies and function
specialisation.

</details>


### [237] [Prompting LLMs for Code Editing: Struggles and Remedies](https://arxiv.org/abs/2504.20196)
*Daye Nam,Ahmed Omran,Ambar Murillo,Saksham Thakur,Abner Araujo,Marcel Blistein,Alexander Frömmgen,Vincent Hellendoorn,Satish Chandra*

Main category: cs.SE

TL;DR: 论文研究了开发者如何在实际工作流程中使用LLM驱动的代码编辑工具，发现频繁重新提示是使用困难的标志，并提出了一种自动改进提示的工具AutoPrompter，显著提高了编辑正确性。


<details>
  <summary>Details</summary>
Motivation: 理解开发者如何在日常工作中使用LLM驱动的代码编辑工具，并识别他们在使用中的困难。

Method: 通过分析IDE中的使用日志和定性分析不满意的请求，提出并评估了自动改进提示的工具AutoPrompter。

Result: AutoPrompter在测试集上使编辑正确性提高了27%。

Conclusion: 研究填补了开发者实际使用LLM工具的知识空白，并展示了自动改进提示的有效性。

Abstract: Large Language Models (LLMs) are rapidly transforming software engineering,
with coding assistants embedded in an IDE becoming increasingly prevalent.
While research has focused on improving the tools and understanding developer
perceptions, a critical gap exists in understanding how developers actually use
these tools in their daily workflows, and, crucially, where they struggle. This
paper addresses part of this gap through a multi-phased investigation of
developer interactions with an LLM-powered code editing and transformation
feature, Transform Code, in an IDE widely used at Google. First, we analyze
telemetry logs of the feature usage, revealing that frequent re-prompting can
be an indicator of developer struggles with using Transform Code. Second, we
conduct a qualitative analysis of unsatisfactory requests, identifying five key
categories of information often missing from developer prompts. Finally, based
on these findings, we propose and evaluate a tool, AutoPrompter, for
automatically improving prompts by inferring missing information from the
surrounding code context, leading to a 27% improvement in edit correctness on
our test set.

</details>


### [238] [Automated Unit Test Case Generation: A Systematic Literature Review](https://arxiv.org/abs/2504.20357)
*Jason Wang,Basem Suleiman,Muhammad Johan Alibasa*

Main category: cs.SE

TL;DR: 本文系统综述了遗传算法和粒子群优化在自动化软件测试中的改进与挑战，填补了现有研究的信息空白。


<details>
  <summary>Details</summary>
Motivation: 软件测试的高成本和资源消耗促使自动化测试研究发展，但遗传算法和粒子群优化的改进领域存在信息空白。

Method: 通过系统文献综述，整合进化方法及其改进（如混合算法、突变测试与神经网络的结合）和局限性。

Result: 总结了当前算法中的主要测试标准及面临的挑战（如可读性、模拟等）。

Conclusion: 本文为自动化测试领域的研究提供了知识整合，并指出了未来改进的方向。

Abstract: Software is omnipresent within all factors of society. It is thus important
to ensure that software are well tested to mitigate bad user experiences as
well as the potential for severe financial and human losses. Software testing
is however expensive and absorbs valuable time and resources. As a result, the
field of automated software testing has grown of interest to researchers in
past decades. In our review of present and past research papers, we have
identified an information gap in the areas of improvement for the Genetic
Algorithm and Particle Swarm Optimisation. A gap in knowledge in the current
challenges that face automated testing has also been identified. We therefore
present this systematic literature review in an effort to consolidate existing
knowledge in regards to the evolutionary approaches as well as their
improvements and resulting limitations. These improvements include hybrid
algorithm combinations as well as interoperability with mutation testing and
neural networks. We will also explore the main test criterion that are used in
these algorithms alongside the challenges currently faced in the field related
to readability, mocking and more.

</details>


### [239] [CrashFixer: A crash resolution agent for the Linux kernel](https://arxiv.org/abs/2504.20412)
*Alex Mathai,Chenxi Huang,Suwei Ma,Jihwan Kim,Hailie Mitchell,Aleksandr Nogikh,Petros Maniatis,Franjo Ivančić,Junfeng Yang,Baishakhi Ray*

Main category: cs.SE

TL;DR: 本文介绍了CrashFixer，首个适用于Linux内核漏洞的基于LLM的修复工具，改进了kGym平台为kGymSuite，并展示了其修复复杂内核漏洞的能力。


<details>
  <summary>Details</summary>
Motivation: 现有代码LLM基准测试通常局限于小规模场景，而Linux内核漏洞修复需要更强大的工具。

Method: 基于kGym平台改进为kGymSuite，结合开发者工作流设计CrashFixer，并评估不同修复策略。

Result: CrashFixer在开放漏洞中生成至少两个被认可可行的补丁建议。

Conclusion: 显式生成假设有助于修复复杂系统漏洞，CrashFixer展示了LLM在内核修复中的潜力。

Abstract: Code large language models (LLMs) have shown impressive capabilities on a
multitude of software engineering tasks. In particular, they have demonstrated
remarkable utility in the task of code repair. However, common benchmarks used
to evaluate the performance of code LLMs are often limited to small-scale
settings. In this work, we build upon kGym, which shares a benchmark for
system-level Linux kernel bugs and a platform to run experiments on the Linux
kernel.
  This paper introduces CrashFixer, the first LLM-based software repair agent
that is applicable to Linux kernel bugs. Inspired by the typical workflow of a
kernel developer, we identify the key capabilities an expert developer
leverages to resolve a kernel crash. Using this as our guide, we revisit the
kGym platform and identify key system improvements needed to practically run
LLM-based agents at the scale of the Linux kernel (50K files and 20M lines of
code). We implement these changes by extending kGym to create an improved
platform - called kGymSuite, which will be open-sourced. Finally, the paper
presents an evaluation of various repair strategies for such complex kernel
bugs and showcases the value of explicitly generating a hypothesis before
attempting to fix bugs in complex systems such as the Linux kernel. We also
evaluated CrashFixer's capabilities on still open bugs, and found at least two
patch suggestions considered plausible to resolve the reported bug.

</details>


### [240] [ARCS: Agentic Retrieval-Augmented Code Synthesis with Iterative Refinement](https://arxiv.org/abs/2504.20434)
*Manish Bhattarai,Miguel Cordova,Javier Santos,Dan O'Malley*

Main category: cs.SE

TL;DR: ARCS框架结合检索增强生成与链式思维推理，通过状态-动作搜索树优化，显著提升代码生成与翻译质量。


<details>
  <summary>Details</summary>
Motivation: 在超级计算中，高效且优化的代码生成对充分利用高性能系统至关重要。

Method: ARCS整合检索增强生成（RAG）与链式思维推理（CoT），通过代理机制检索代码片段，并利用实时执行反馈优化候选方案。

Result: 在Geeks4Geeks和HumanEval基准测试中，ARCS显著优于传统提示方法。

Conclusion: ARCS为超级计算应用中的代码开发自动化和优化提供了变革性潜力。

Abstract: In supercomputing, efficient and optimized code generation is essential to
leverage high-performance systems effectively. We propose Agentic
Retrieval-Augmented Code Synthesis (ARCS), an advanced framework for accurate,
robust, and efficient code generation, completion, and translation. ARCS
integrates Retrieval-Augmented Generation (RAG) with Chain-of-Thought (CoT)
reasoning to systematically break down and iteratively refine complex
programming tasks. An agent-based RAG mechanism retrieves relevant code
snippets, while real-time execution feedback drives the synthesis of candidate
solutions. This process is formalized as a state-action search tree
optimization, balancing code correctness with editing efficiency. Evaluations
on the Geeks4Geeks and HumanEval benchmarks demonstrate that ARCS significantly
outperforms traditional prompting methods in translation and generation
quality. By enabling scalable and precise code synthesis, ARCS offers
transformative potential for automating and optimizing code development in
supercomputing applications, enhancing computational resource utilization.

</details>


### [241] [Enhancing Cell Counting through MLOps: A Structured Approach for Automated Cell Analysis](https://arxiv.org/abs/2504.20126)
*Matteo Testi,Luca Clissa,Matteo Ballabio,Salvatore Ricciardi,Federico Baldo,Emanuele Frontoni,Sara Moccia,Gennario Vessio*

Main category: cs.SE

TL;DR: 本文介绍了CC-MLOps框架，旨在优化机器学习在细胞计数中的应用，提升模型可靠性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 机器学习在细胞计数中有巨大潜力，但需要稳健的操作框架来实现高效应用。

Method: 提出CC-MLOps框架，涵盖数据预处理、模型训练、监控、可解释性和可持续性。

Result: 通过实际案例展示，该框架能提高模型可靠性、减少人为错误，并支持可扩展的解决方案。

Conclusion: 为研究人员和实验室专业人员提供了实施机器学习细胞计数系统的实用指导。

Abstract: Machine Learning (ML) models offer significant potential for advancing cell
counting applications in neuroscience, medical research, pharmaceutical
development, and environmental monitoring. However, implementing these models
effectively requires robust operational frameworks. This paper introduces Cell
Counting Machine Learning Operations (CC-MLOps), a comprehensive framework that
streamlines the integration of ML in cell counting workflows. CC-MLOps
encompasses data access and preprocessing, model training, monitoring,
explainability features, and sustainability considerations. Through a practical
use case, we demonstrate how MLOps principles can enhance model reliability,
reduce human error, and enable scalable Cell Counting solutions. This work
provides actionable guidance for researchers and laboratory professionals
seeking to implement machine learning (ML)- powered cell counting systems.

</details>


### [242] [CoCo-Bench: A Comprehensive Code Benchmark For Multi-task Large Language Model Evaluation](https://arxiv.org/abs/2504.20673)
*Wenjing Yin,Tianze Sun,Yijiong Yu,Jiawei Fang,Guangyao Su,Jiancheng Wang,Zekun Wang,Wei Wang,Ran Chen,Ziyun Dai,Shuai Yuan,Menghang Dong,Peng Luo,Dong Cao,Da Lei,Yajun Zhang,Hao Chen,Xiang Ma,Yong Liu,Weifeng Liu,Yuanjian Xu,Ji Pei*

Main category: cs.SE

TL;DR: CoCo-Bench是一个全面的代码基准测试工具，用于评估大型语言模型在代码理解、生成、修改和审查四个关键维度的表现，填补了现有基准测试的不足。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试范围狭窄，无法全面反映大型语言模型在软件工程中的实际应用需求，因此需要更系统的评估框架。

Method: 设计CoCo-Bench，涵盖多种编程语言和任务难度，通过严格的人工审查确保数据质量。

Result: CoCo-Bench与现有基准测试一致，同时揭示了模型性能的显著差异，有效突出了优缺点。

Conclusion: CoCo-Bench为代码导向的大型语言模型提供了全面客观的评估，为未来研究和技术发展提供了可靠基准。

Abstract: Large language models (LLMs) play a crucial role in software engineering,
excelling in tasks like code generation and maintenance. However, existing
benchmarks are often narrow in scope, focusing on a specific task and lack a
comprehensive evaluation framework that reflects real-world applications. To
address these gaps, we introduce CoCo-Bench (Comprehensive Code Benchmark),
designed to evaluate LLMs across four critical dimensions: code understanding,
code generation, code modification, and code review. These dimensions capture
essential developer needs, ensuring a more systematic and representative
evaluation. CoCo-Bench includes multiple programming languages and varying task
difficulties, with rigorous manual review to ensure data quality and accuracy.
Empirical results show that CoCo-Bench aligns with existing benchmarks while
uncovering significant variations in model performance, effectively
highlighting strengths and weaknesses. By offering a holistic and objective
evaluation, CoCo-Bench provides valuable insights to guide future research and
technological advancements in code-oriented LLMs, establishing a reliable
benchmark for the field.

</details>


### [243] [Using LLMs in Generating Design Rationale for Software Architecture Decisions](https://arxiv.org/abs/2504.20781)
*Xiyu Zhou,Ruiyin Li,Peng Liang,Beiqi Zhang,Mojtaba Shahin,Zengyang Li,Chen Yang*

Main category: cs.SE

TL;DR: 该研究评估了大型语言模型（LLMs）在生成软件架构设计理由（DR）方面的表现，发现其生成的DR在精确度、召回率和F1分数上表现一般，但部分未提及的论点仍具帮助性。


<details>
  <summary>Details</summary>
Motivation: 软件架构设计理由（DR）在实践中常因开发者缺乏动力和努力而未被充分记录，LLMs的文本理解和生成能力可能为DR的生成和恢复提供解决方案。

Method: 研究收集了100个架构相关问题，使用五种LLMs和三种提示策略（零样本、思维链和LLM代理）生成DR，并与人类专家提供的DR进行对比评估。

Result: LLM生成的DR在精确度（0.267-0.278）、召回率（0.627-0.715）和F1分数（0.351-0.389）上表现一般，但64.45%-69.42%的未提及论点仍具帮助性。

Conclusion: 研究讨论了三种提示策略的优缺点及LLM生成DR的局限性，表明LLMs在DR生成中有潜力但需进一步优化。

Abstract: Design Rationale (DR) for software architecture decisions refers to the
reasoning underlying architectural choices, which provides valuable insights
into the different phases of the architecting process throughout software
development. However, in practice, DR is often inadequately documented due to a
lack of motivation and effort from developers. With the recent advancements in
Large Language Models (LLMs), their capabilities in text comprehension,
reasoning, and generation may enable the generation and recovery of DR for
architecture decisions. In this study, we evaluated the performance of LLMs in
generating DR for architecture decisions. First, we collected 50 Stack Overflow
(SO) posts, 25 GitHub issues, and 25 GitHub discussions related to architecture
decisions to construct a dataset of 100 architecture-related problems. Then, we
selected five LLMs to generate DR for the architecture decisions with three
prompting strategies, including zero-shot, chain of thought (CoT), and
LLM-based agents. With the DR provided by human experts as ground truth, the
Precision of LLM-generated DR with the three prompting strategies ranges from
0.267 to 0.278, Recall from 0.627 to 0.715, and F1-score from 0.351 to 0.389.
Additionally, 64.45% to 69.42% of the arguments of DR not mentioned by human
experts are also helpful, 4.12% to 4.87% of the arguments have uncertain
correctness, and 1.59% to 3.24% of the arguments are potentially misleading.
Based on the results, we further discussed the pros and cons of the three
prompting strategies and the strengths and limitations of the DR generated by
LLMs.

</details>


### [244] [Hallucination by Code Generation LLMs: Taxonomy, Benchmarks, Mitigation, and Challenges](https://arxiv.org/abs/2504.20799)
*Yunseo Lee,John Youngeun Song,Dongsun Kim,Jindae Kim,Mijung Kim,Jaechang Nam*

Main category: cs.SE

TL;DR: 本文综述了CodeLLMs生成代码时的幻觉问题，分类了幻觉类型，回顾了现有基准和缓解策略，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在代码生成中表现出色，但容易产生难以识别的幻觉代码，影响代码质量。

Method: 分类CodeLLMs生成的幻觉类型，分析现有基准和缓解策略。

Result: 总结了幻觉问题的现状，并指出现有方法的局限性。

Conclusion: 提出了未来在检测和消除CodeLLMs幻觉方面的研究方向。

Abstract: Recent technical breakthroughs in large language models (LLMs) have enabled
them to fluently generate source code. Software developers often leverage both
general-purpose and code-specialized LLMs to revise existing code or even
generate a whole function from scratch. These capabilities are also beneficial
in no-code or low-code contexts, in which one can write programs without a
technical background. However, due to their internal design, LLMs are prone to
generating hallucinations, which are incorrect, nonsensical, and not
justifiable information but difficult to identify its presence. This problem
also occurs when generating source code. Once hallucinated code is produced, it
is often challenging for users to identify and fix it, especially when such
hallucinations can be identified under specific execution paths. As a result,
the hallucinated code may remain unnoticed within the codebase. This survey
investigates recent studies and techniques relevant to hallucinations generated
by CodeLLMs. We categorize the types of hallucinations in the code generated by
CodeLLMs, review existing benchmarks and mitigation strategies, and identify
open challenges. Based on these findings, this survey outlines further research
directions in the detection and removal of hallucinations produced by CodeLLMs.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [245] [Pediatric Asthma Detection with Googles HeAR Model: An AI-Driven Respiratory Sound Classifier](https://arxiv.org/abs/2504.20124)
*Abul Ehtesham,Saket Kumar,Aditi Singh,Tala Talaei Khoei*

Main category: cs.SD

TL;DR: 利用AI和HeAR模型从儿童呼吸音中早期检测哮喘，准确率超过91%。


<details>
  <summary>Details</summary>
Motivation: 早期发现儿童哮喘对预防长期呼吸并发症和减少急诊干预至关重要。

Method: 使用HeAR模型将呼吸音嵌入512维表示，并用多种分类器区分哮喘指示音和正常音。

Result: 系统准确率超过91%，在阳性病例的精确召回指标上表现强劲。

Conclusion: 该方法证明短时、低资源儿童录音结合基础音频模型可实现快速、非侵入性哮喘筛查，适用于远程或医疗资源匮乏地区。

Abstract: Early detection of asthma in children is crucial to prevent long-term
respiratory complications and reduce emergency interventions. This work
presents an AI-powered diagnostic pipeline that leverages Googles Health
Acoustic Representations (HeAR) model to detect early signs of asthma from
pediatric respiratory sounds. The SPRSound dataset, the first open-access
collection of annotated respiratory sounds in children aged 1 month to 18
years, is used to extract 2-second audio segments labeled as wheeze, crackle,
rhonchi, stridor, or normal. Each segment is embedded into a 512-dimensional
representation using HeAR, a foundation model pretrained on 300 million
health-related audio clips, including 100 million cough sounds. Multiple
classifiers, including SVM, Random Forest, and MLP, are trained on these
embeddings to distinguish between asthma-indicative and normal sounds. The
system achieves over 91\% accuracy, with strong performance on precision-recall
metrics for positive cases. In addition to classification, learned embeddings
are visualized using PCA, misclassifications are analyzed through waveform
playback, and ROC and confusion matrix insights are provided. This method
demonstrates that short, low-resource pediatric recordings, when powered by
foundation audio models, can enable fast, noninvasive asthma screening. The
approach is especially promising for digital diagnostics in remote or
underserved healthcare settings.

</details>


### [246] [APG-MOS: Auditory Perception Guided-MOS Predictor for Synthetic Speech](https://arxiv.org/abs/2504.20447)
*Zhicheng Lian,Lizhi Wang,Hua Huang*

Main category: cs.SD

TL;DR: APG-MOS模型通过结合听觉感知建模和语义分析，提升语音质量评估与人类判断的一致性。


<details>
  <summary>Details</summary>
Motivation: 减少人工评估的负担，同时解决现有深度学习模型忽视听觉感知机制的问题。

Method: 设计基于生物听觉机制的感知模块模拟耳蜗功能，提出RVQ语义失真建模方法，并采用残差交叉注意力架构进行多模态融合。

Result: APG-MOS在两个主要基准测试中表现优异。

Conclusion: APG-MOS通过整合听觉感知和语义分析，显著提升了语音质量评估的准确性。

Abstract: Automatic speech quality assessment aims to quantify subjective human
perception of speech through computational models to reduce the need for
labor-consuming manual evaluations. While models based on deep learning have
achieved progress in predicting mean opinion scores (MOS) to assess synthetic
speech, the neglect of fundamental auditory perception mechanisms limits
consistency with human judgments. To address this issue, we propose an auditory
perception guided-MOS prediction model (APG-MOS) that synergistically
integrates auditory modeling with semantic analysis to enhance consistency with
human judgments. Specifically, we first design a perceptual module, grounded in
biological auditory mechanisms, to simulate cochlear functions, which encodes
acoustic signals into biologically aligned electrochemical representations.
Secondly, we propose a residual vector quantization (RVQ)-based semantic
distortion modeling method to quantify the degradation of speech quality at the
semantic level. Finally, we design a residual cross-attention architecture,
coupled with a progressive learning strategy, to enable multimodal fusion of
encoded electrochemical signals and semantic representations. Experiments
demonstrate that APG-MOS achieves superior performance on two primary
benchmarks. Our code and checkpoint will be available on a public repository
upon publication.

</details>


### [247] [DiffusionRIR: Room Impulse Response Interpolation using Diffusion Models](https://arxiv.org/abs/2504.20625)
*Sagi Della Torre,Mirco Pezzoli,Fabio Antonacci,Sharon Gannot*

Main category: cs.SD

TL;DR: 该研究利用去噪扩散概率模型（DDPM）估计房间内未测量位置的房间脉冲响应（RIR），通过将RIR数据转换为适合扩散重建的格式，显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 高空间分辨率的RIR测量资源密集，难以在大空间或密集采样场景中实现，因此需要一种高效的方法估计未测量位置的RIR。

Method: 将RIR矩阵类比为图像修复问题，使用DDPM进行数据重建，并在不同曲率的麦克风阵列上验证。

Result: 该方法在麦克风间距较大的情况下仍能准确重建RIR，在归一化均方误差和余弦距离上显著优于基线方法。

Conclusion: 研究表明生成模型在RIR插值中具有潜力，为从有限的真实测量中生成额外数据提供了新途径。

Abstract: Room Impulse Responses (RIRs) characterize acoustic environments and are
crucial in multiple audio signal processing tasks. High-quality RIR estimates
drive applications such as virtual microphones, sound source localization,
augmented reality, and data augmentation. However, obtaining RIR measurements
with high spatial resolution is resource-intensive, making it impractical for
large spaces or when dense sampling is required. This research addresses the
challenge of estimating RIRs at unmeasured locations within a room using
Denoising Diffusion Probabilistic Models (DDPM). Our method leverages the
analogy between RIR matrices and image inpainting, transforming RIR data into a
format suitable for diffusion-based reconstruction.
  Using simulated RIR data based on the image method, we demonstrate our
approach's effectiveness on microphone arrays of different curvatures, from
linear to semi-circular. Our method successfully reconstructs missing RIRs,
even in large gaps between microphones. Under these conditions, it achieves
accurate reconstruction, significantly outperforming baseline Spline Cubic
Interpolation in terms of Normalized Mean Square Error and Cosine Distance
between actual and interpolated RIRs.
  This research highlights the potential of using generative models for
effective RIR interpolation, paving the way for generating additional data from
limited real-world measurements.

</details>


### [248] [ECOSoundSet: a finely annotated dataset for the automated acoustic identification of Orthoptera and Cicadidae in North, Central and temperate Western Europe](https://arxiv.org/abs/2504.20776)
*David Funosas,Elodie Massol,Yves Bas,Svenja Schmidt,Dominik Arend,Alexander Gebhard,Luc Barbaro,Sebastian König,Rafael Carbonell Font,David Sannier,Fernand Deroussen,Jérôme Sueur,Christian Roesti,Tomi Trilar,Wolfgang Forstmeier,Lucas Roger,Eloïsa Matheu,Piotr Guzik,Julien Barataud,Laurent Pelozuelo,Stéphane Puissant,Sandra Mueller,Björn Schuller,Jose M. Montoya,Andreas Triantafyllopoulos,Maxime Cauchoix*

Main category: cs.SD

TL;DR: ECOSoundSet是一个包含欧洲直翅目和蝉类声音的数据集，旨在支持深度学习算法在自然声景中的昆虫声音识别。


<details>
  <summary>Details</summary>
Motivation: 当前自动识别欧洲昆虫声音的工具受限于数据集的范围和多样性，需要更大且生态多样的数据集来提升算法性能。

Method: 通过实地采集和欧洲昆虫学家的贡献，构建了一个包含10,653条录音的数据集，分为弱标注和强标注两种形式，并提供训练/验证/测试集划分。

Result: 数据集覆盖200种直翅目和24种蝉类，涵盖欧洲多个地区，为深度学习算法提供了丰富的训练资源。

Conclusion: ECOSoundSet可作为现有在线录音的有力补充，提升欧洲直翅目和蝉类声音的自动分类能力。

Abstract: Currently available tools for the automated acoustic recognition of European
insects in natural soundscapes are limited in scope. Large and ecologically
heterogeneous acoustic datasets are currently needed for these algorithms to
cross-contextually recognize the subtle and complex acoustic signatures
produced by each species, thus making the availability of such datasets a key
requisite for their development. Here we present ECOSoundSet (European
Cicadidae and Orthoptera Sound dataSet), a dataset containing 10,653 recordings
of 200 orthopteran and 24 cicada species (217 and 26 respective taxa when
including subspecies) present in North, Central, and temperate Western Europe
(Andorra, Belgium, Denmark, mainland France and Corsica, Germany, Ireland,
Luxembourg, Monaco, Netherlands, United Kingdom, Switzerland), collected partly
through targeted fieldwork in South France and Catalonia and partly through
contributions from various European entomologists. The dataset is composed of a
combination of coarsely labeled recordings, for which we can only infer the
presence, at some point, of their target species (weak labeling), and finely
annotated recordings, for which we know the specific time and frequency range
of each insect sound present in the recording (strong labeling). We also
provide a train/validation/test split of the strongly labeled recordings, with
respective approximate proportions of 0.8, 0.1 and 0.1, in order to facilitate
their incorporation in the training and evaluation of deep learning algorithms.
This dataset could serve as a meaningful complement to recordings already
available online for the training of deep learning algorithms for the acoustic
classification of orthopterans and cicadas in North, Central, and temperate
Western Europe.

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [249] [Predictive AI with External Knowledge Infusion for Stocks](https://arxiv.org/abs/2504.20058)
*Ambedkar Dukkipati,Kawin Mayilvaghanan,Naveen Kumar Pallekonda,Sai Prakash Hadnoor,Ranga Shaarad Ayyagari*

Main category: q-fin.ST

TL;DR: 本文提出了一种结合历史趋势和外部知识（通过时间知识图谱）的股票预测方法，首次解决了外部影响下的预测问题，并构建了相关数据集。


<details>
  <summary>Details</summary>
Motivation: 股票价格波动受多种动态外部因素影响，现有方法未能充分结合这些因素。

Method: 提出基于时间知识图谱的学习机制，将外部关系建模为图上的霍克斯过程事件。

Result: 实验表明，动态表示能有效根据回报率对股票进行排名，优于基线方法。

Conclusion: 结合外部动态因素的建模方法在股票预测中表现优异。

Abstract: Fluctuations in stock prices are influenced by a complex interplay of factors
that go beyond mere historical data. These factors, themselves influenced by
external forces, encompass inter-stock dynamics, broader economic factors,
various government policy decisions, outbreaks of wars, etc. Furthermore, all
of these factors are dynamic and exhibit changes over time. In this paper, for
the first time, we tackle the forecasting problem under external influence by
proposing learning mechanisms that not only learn from historical trends but
also incorporate external knowledge from temporal knowledge graphs. Since there
are no such datasets or temporal knowledge graphs available, we study this
problem with stock market data, and we construct comprehensive temporal
knowledge graph datasets. In our proposed approach, we model relations on
external temporal knowledge graphs as events of a Hawkes process on graphs.
With extensive experiments, we show that learned dynamic representations
effectively rank stocks based on returns across multiple holding periods,
outperforming related baselines on relevant metrics.

</details>


### [250] [Deep Learning vs. Black-Scholes: Option Pricing Performance on Brazilian Petrobras Stocks](https://arxiv.org/abs/2504.20088)
*Joao Felipe Gueiros,Hemanth Chandravamsi,Steven H. Frankel*

Main category: q-fin.ST

TL;DR: 论文探讨了使用深度残差网络为巴西石油公司（Petrobras）的欧式期权定价，并与Black-Scholes模型进行比较，结果显示深度学习方法在误差减少和长期准确性上表现更优。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索深度学习在金融建模中的潜力，特别是针对Black-Scholes模型在长期预测中准确性下降的问题。

Method: 方法包括使用从巴西证券交易所（B3）通过网络爬取的八年历史数据，训练一个基于混合损失函数的深度残差网络模型。

Result: 结果显示，深度残差网络在3-19巴西雷亚尔范围内，平均绝对误差比Black-Scholes模型减少了64.3%，且在长期到期日表现更准确。

Conclusion: 结论指出深度学习在金融建模中具有潜力，未来工作将专注于不同价格区间的专用模型。

Abstract: This paper explores the use of deep residual networks for pricing European
options on Petrobras, one of the world's largest oil and gas producers, and
compares its performance with the Black-Scholes (BS) model. Using eight years
of historical data from B3 (Brazilian Stock Exchange) collected via web
scraping, a deep learning model was trained using a custom built hybrid loss
function that incorporates market data and analytical pricing. The data for
training and testing were drawn between the period spanning November 2016 to
January 2025, using an 80-20 train-test split. The test set consisted of data
from the final three months: November, December, and January 2025. The deep
residual network model achieved a 64.3\% reduction in the mean absolute error
for the 3-19 BRL (Brazilian Real) range when compared to the Black-Scholes
model on the test set. Furthermore, unlike the Black-Scholes solution, which
tends to decrease its accuracy for longer periods of time, the deep learning
model performed accurately for longer expiration periods. These findings
highlight the potential of deep learning in financial modeling, with future
work focusing on specialized models for different price ranges.

</details>

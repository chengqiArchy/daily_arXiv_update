<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 48]
- [cs.AI](#cs.AI) [Total: 14]
- [cs.LG](#cs.LG) [Total: 44]
- [cs.CV](#cs.CV) [Total: 22]
- [cs.SE](#cs.SE) [Total: 3]
- [econ.GN](#econ.GN) [Total: 1]
- [cs.CY](#cs.CY) [Total: 7]
- [cs.CR](#cs.CR) [Total: 26]
- [math.ST](#math.ST) [Total: 1]
- [quant-ph](#quant-ph) [Total: 2]
- [cs.IT](#cs.IT) [Total: 2]
- [eess.SY](#eess.SY) [Total: 2]
- [cs.CE](#cs.CE) [Total: 1]
- [physics.ao-ph](#physics.ao-ph) [Total: 1]
- [cs.NE](#cs.NE) [Total: 2]
- [cs.IR](#cs.IR) [Total: 1]
- [cs.DC](#cs.DC) [Total: 1]
- [cs.MM](#cs.MM) [Total: 1]
- [eess.IV](#eess.IV) [Total: 2]
- [q-bio.CB](#q-bio.CB) [Total: 1]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 1]
- [cs.MA](#cs.MA) [Total: 3]
- [cs.RO](#cs.RO) [Total: 9]
- [eess.SP](#eess.SP) [Total: 1]
- [physics.data-an](#physics.data-an) [Total: 1]
- [cs.HC](#cs.HC) [Total: 2]
- [cs.DB](#cs.DB) [Total: 1]
- [cs.GR](#cs.GR) [Total: 1]
- [cs.SD](#cs.SD) [Total: 1]
- [stat.ML](#stat.ML) [Total: 5]
- [math.NA](#math.NA) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Waking Up an AI: A Quantitative Framework for Prompt-Induced Phase Transition in Large Language Models](https://arxiv.org/abs/2504.21012)
*Makoto Sato*

Main category: cs.CL

TL;DR: 论文提出了一个框架，通过诱导和量化提示研究LLM的认知行为，发现LLM在概念融合方面与人类直觉存在差异。


<details>
  <summary>Details</summary>
Motivation: 探讨人类直觉思维的底层机制，通过比较人类和LLM的认知动态，揭示其差异。

Method: 提出TIP和TQP框架，通过控制实验分析LLM对语义融合与非融合提示的反应。

Result: LLM在语义融合与非融合提示下的反应无显著差异，与人类直觉不同。

Conclusion: LLM尚未复制人类的概念整合过程，方法为量化认知差异提供了工具。

Abstract: What underlies intuitive human thinking? One approach to this question is to
compare the cognitive dynamics of humans and large language models (LLMs).
However, such a comparison requires a method to quantitatively analyze AI
cognitive behavior under controlled conditions. While anecdotal observations
suggest that certain prompts can dramatically change LLM behavior, these
observations have remained largely qualitative. Here, we propose a two-part
framework to investigate this phenomenon: a Transition-Inducing Prompt (TIP)
that triggers a rapid shift in LLM responsiveness, and a Transition Quantifying
Prompt (TQP) that evaluates this change using a separate LLM. Through
controlled experiments, we examined how LLMs react to prompts embedding two
semantically distant concepts (e.g., mathematical aperiodicity and traditional
crafts)--either fused together or presented separately--by changing their
linguistic quality and affective tone. Whereas humans tend to experience
heightened engagement when such concepts are meaningfully blended producing a
novel concept--a form of conceptual fusion--current LLMs showed no significant
difference in responsiveness between semantically fused and non-fused prompts.
This suggests that LLMs may not yet replicate the conceptual integration
processes seen in human intuition. Our method enables fine-grained,
reproducible measurement of cognitive responsiveness, and may help illuminate
key differences in how intuition and conceptual leaps emerge in artificial
versus human minds.

</details>


### [2] [Analyzing Feedback Mechanisms in AI-Generated MCQs: Insights into Readability, Lexical Properties, and Levels of Challenge](https://arxiv.org/abs/2504.21013)
*Antoun Yaacoub,Zainab Assaghir,Lionel Prevost,Jérôme Da-Rugna*

Main category: cs.CL

TL;DR: 研究分析了Google Gemini 1.5-flash文本模型生成的反馈在计算机科学选择题中的语言特征，揭示了反馈语调与题目难度之间的动态适应关系。


<details>
  <summary>Details</summary>
Motivation: 探索AI生成反馈的语言特征（如可读性、词汇丰富度）及其在不同难度和语调下的适应性，以提升学习效果。

Method: 分析了1,200多道选择题的反馈，计算了长度、可读性、词汇丰富度等指标，并训练了一个RoBERTa多任务学习模型预测这些特征。

Result: 模型在可读性和词汇丰富度预测上表现良好（MAE分别为2.0和0.03），发现反馈语调与题目难度有显著交互作用。

Conclusion: 研究为个性化AI反馈机制提供了依据，强调了设计中的伦理考量，有望改善学习效果。

Abstract: Artificial Intelligence (AI)-generated feedback in educational settings has
garnered considerable attention due to its potential to enhance learning
outcomes. However, a comprehensive understanding of the linguistic
characteristics of AI-generated feedback, including readability, lexical
richness, and adaptability across varying challenge levels, remains limited.
This study delves into the linguistic and structural attributes of feedback
generated by Google's Gemini 1.5-flash text model for computer science
multiple-choice questions (MCQs). A dataset of over 1,200 MCQs was analyzed,
considering three difficulty levels (easy, medium, hard) and three feedback
tones (supportive, neutral, challenging). Key linguistic metrics, such as
length, readability scores (Flesch-Kincaid Grade Level), vocabulary richness,
and lexical density, were computed and examined. A fine-tuned RoBERTa-based
multi-task learning (MTL) model was trained to predict these linguistic
properties, achieving a Mean Absolute Error (MAE) of 2.0 for readability and
0.03 for vocabulary richness. The findings reveal significant interaction
effects between feedback tone and question difficulty, demonstrating the
dynamic adaptation of AI-generated feedback within diverse educational
contexts. These insights contribute to the development of more personalized and
effective AI-driven feedback mechanisms, highlighting the potential for
improved learning outcomes while underscoring the importance of ethical
considerations in their design and deployment.

</details>


### [3] [Nested Named-Entity Recognition on Vietnamese COVID-19: Dataset and Experiments](https://arxiv.org/abs/2504.21016)
*Ngoc C. Lê,Hai-Chung Nguyen-Phung,Thu-Huong Pham Thi,Hue Vu,Phuong-Thao Nguyen Thi,Thu-Thuy Tran,Hong-Nhung Le Thi,Thuy-Duong Nguyen-Thi,Thanh-Huy Nguyen*

Main category: cs.CL

TL;DR: 研究提出了一种命名实体识别（NER）方法，用于辅助越南的COVID-19疫情防控，并展示了一个手动标注的越南语嵌套命名实体识别数据集。


<details>
  <summary>Details</summary>
Motivation: COVID-19疫情导致全球重大损失，越南通过追踪、定位和隔离接触者有效防控疫情，但手工操作效率低下。

Method: 研究采用命名实体识别（NER）技术，并定义新的实体类型，构建了一个手动标注的越南语嵌套命名实体识别数据集。

Result: 提出了一个适用于越南语COVID-19防控的NER系统，并提供了相关数据集。

Conclusion: NER技术可有效辅助越南的COVID-19疫情防控，未来可进一步优化系统。

Abstract: The COVID-19 pandemic caused great losses worldwide, efforts are taken place
to prevent but many countries have failed. In Vietnam, the traceability,
localization, and quarantine of people who contact with patients contribute to
effective disease prevention. However, this is done by hand, and take a lot of
work. In this research, we describe a named-entity recognition (NER) study that
assists in the prevention of COVID-19 pandemic in Vietnam. We also present our
manually annotated COVID-19 dataset with nested named entity recognition task
for Vietnamese which be defined new entity types using for our system.

</details>


### [4] [ViQA-COVID: COVID-19 Machine Reading Comprehension Dataset for Vietnamese](https://arxiv.org/abs/2504.21017)
*Hai-Chung Nguyen-Phung,Ngoc C. Lê,Van-Chien Nguyen,Hang Thi Nguyen,Thuy Phuong Thi Nguyen*

Main category: cs.CL

TL;DR: 论文介绍了首个越南语COVID-19多跨度抽取机器阅读理解数据集ViQA-COVID，旨在支持疾病预防和促进越南语及多语言MRC研究。


<details>
  <summary>Details</summary>
Motivation: COVID-19对全球造成严重影响，AI应用需求迫切。现有研究多集中于AI在疾病预防中的应用，但缺乏越南语相关数据集。

Method: 创建了首个越南语COVID-19多跨度抽取MRC数据集ViQA-COVID，可用于模型和系统开发。

Result: ViQA-COVID填补了越南语MRC数据集的空白，支持疾病预防和MRC研究。

Conclusion: ViQA-COVID为越南语及多语言MRC研究提供了重要资源，有望推动相关领域发展。

Abstract: After two years of appearance, COVID-19 has negatively affected people and
normal life around the world. As in May 2022, there are more than 522 million
cases and six million deaths worldwide (including nearly ten million cases and
over forty-three thousand deaths in Vietnam). Economy and society are both
severely affected. The variant of COVID-19, Omicron, has broken disease
prevention measures of countries and rapidly increased number of infections.
Resources overloading in treatment and epidemics prevention is happening all
over the world. It can be seen that, application of artificial intelligence
(AI) to support people at this time is extremely necessary. There have been
many studies applying AI to prevent COVID-19 which are extremely useful, and
studies on machine reading comprehension (MRC) are also in it. Realizing that,
we created the first MRC dataset about COVID-19 for Vietnamese: ViQA-COVID and
can be used to build models and systems, contributing to disease prevention.
Besides, ViQA-COVID is also the first multi-span extraction MRC dataset for
Vietnamese, we hope that it can contribute to promoting MRC studies in
Vietnamese and multilingual.

</details>


### [5] [HYPEROFA: Expanding LLM Vocabulary to New Languages via Hypernetwork-Based Embedding Initialization](https://arxiv.org/abs/2504.21018)
*Enes Özeren,Yihong Liu,Hinrich Schütze*

Main category: cs.CL

TL;DR: HYPEROFA提出了一种基于超网络的自适应词嵌入初始化方法，优于随机初始化和OFA方法。


<details>
  <summary>Details</summary>
Motivation: 预训练语言模型（PLMs）在中低资源语言上表现不佳，主要因预训练时接触有限。现有方法如OFA虽有效，但其固定源语言嵌入组合限制了表达能力。

Method: HYPEROFA利用超网络从多语言词向量空间映射到PLM的词嵌入空间，生成目标语言的自适应嵌入，作为持续预训练的起点。

Result: 实验表明，HYPEROFA在持续预训练收敛和下游任务性能上均优于随机初始化，且与OFA相当或更优。

Conclusion: HYPEROFA通过更灵活的词嵌入初始化提升了PLM在中低资源语言上的表现，代码已开源。

Abstract: Many pre-trained language models (PLMs) exhibit suboptimal performance on
mid- and low-resource languages, largely due to limited exposure to these
languages during pre-training. A common strategy to address this is to
introduce new tokens specific to the target languages, initialize their
embeddings, and apply continual pre-training on target-language data. Among
such methods, OFA (Liu et al., 2024a) proposes a similarity-based subword
embedding initialization heuristic that is both effective and efficient.
However, OFA restricts target-language token embeddings to be convex
combinations of a fixed number of source-language embeddings, which may limit
expressiveness. To overcome this limitation, we propose HYPEROFA, a
hypernetwork-based approach for more adaptive token embedding initialization.
The hypernetwork is trained to map from an external multilingual word vector
space to the PLMs token embedding space using source-language tokens. Once
trained, it can generate flexible embeddings for target-language tokens,
serving as a good starting point for continual pretraining. Experiments
demonstrate that HYPEROFA consistently outperforms random initialization
baseline and matches or exceeds the performance of OFA in both continual
pre-training convergence and downstream task performance. We make the code
publicly available.

</details>


### [6] [Kill two birds with one stone: generalized and robust AI-generated text detection via dynamic perturbations](https://arxiv.org/abs/2504.21019)
*Yinghan Zhou,Juan Wen,Wanli Peng,Yiming Xue,Ziwei Zhang,Zhengxian Wu*

Main category: cs.CL

TL;DR: 本文提出了一种新的AI生成文本检测方法（DP-Net），通过动态扰动和强化学习，同时解决了模型泛化性和鲁棒性问题。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的普及，AI生成文本（AIGT）的滥用风险增加，亟需一种兼具高泛化性和鲁棒性的检测方法。现有方法往往只关注其中一方面，缺乏统一机制。

Method: 将鲁棒性视为域偏移的一种形式，提出DP-Net方法，通过强化学习引入动态扰动，设计了精细的奖励和动作机制。

Result: 实验表明，DP-Net在三种跨域场景中显著优于现有方法，同时在两种文本对抗攻击下表现出最佳鲁棒性。

Conclusion: DP-Net为AIGT检测提供了一种同时兼顾泛化性和鲁棒性的有效解决方案。

Abstract: The growing popularity of large language models has raised concerns regarding
the potential to misuse AI-generated text (AIGT). It becomes increasingly
critical to establish an excellent AIGT detection method with high
generalization and robustness. However, existing methods either focus on model
generalization or concentrate on robustness. The unified mechanism, to
simultaneously address the challenges of generalization and robustness, is less
explored. In this paper, we argue that robustness can be view as a specific
form of domain shift, and empirically reveal an intrinsic mechanism for model
generalization of AIGT detection task. Then, we proposed a novel AIGT detection
method (DP-Net) via dynamic perturbations introduced by a reinforcement
learning with elaborated reward and action. Experimentally, extensive results
show that the proposed DP-Net significantly outperforms some state-of-the-art
AIGT detection methods for generalization capacity in three cross-domain
scenarios. Meanwhile, the DP-Net achieves best robustness under two text
adversarial attacks. The code is publicly available at
https://github.com/CAU-ISS-Lab/AIGT-Detection-Evade-Detection/tree/main/DP-Net.

</details>


### [7] [Context-Enhanced Contrastive Search for Improved LLM Text Generation](https://arxiv.org/abs/2504.21020)
*Jaydip Sen,Rohit Pandey,Hetvi Waghela*

Main category: cs.CL

TL;DR: 论文提出了一种改进的对比搜索算法CECS，通过动态上下文重要性加权和多级对比搜索等技术，显著提升了生成文本的连贯性和相关性。


<details>
  <summary>Details</summary>
Motivation: 传统解码方法在生成长文本时存在重复或不连贯的问题，因此需要一种新方法以平衡流畅性、创造性和精确性。

Method: 提出了Context-Enhanced Contrastive Search (CECS)，结合动态上下文重要性加权、多级对比搜索和自适应温度控制。

Result: 实验结果表明，CECS在BLEU、ROUGE和语义相似度等指标上显著优于现有对比搜索技术。

Conclusion: CECS在生成高质量文本方面具有潜力，适用于法律文件起草、客服聊天机器人和内容营销等领域。

Abstract: Recently, Large Language Models (LLMs) have demonstrated remarkable
advancements in Natural Language Processing (NLP). However, generating
high-quality text that balances coherence, diversity, and relevance remains
challenging. Traditional decoding methods, such as bean search and top-k
sampling, often struggle with either repetitive or incoherent outputs,
particularly in tasks that require long-form text generation. To address these
limitations, the paper proposes a novel enhancement of the well-known
Contrastive Search algorithm, Context-Enhanced Contrastive Search (CECS) with
contextual calibration. The proposed scheme introduces several novelties
including dynamic contextual importance weighting, multi-level Contrastive
Search, and adaptive temperature control, to optimize the balance between
fluency, creativity, and precision. The performance of CECS is evaluated using
several standard metrics such as BLEU, ROUGE, and semantic similarity.
Experimental results demonstrate significant improvements in both coherence and
relevance of the generated texts by CECS outperforming the existing Contrastive
Search techniques. The proposed algorithm has several potential applications in
the real world including legal document drafting, customer service chatbots,
and content marketing.

</details>


### [8] [ConformalNL2LTL: Translating Natural Language Instructions into Temporal Logic Formulas with Conformal Correctness Guarantees](https://arxiv.org/abs/2504.21022)
*Jun Wang,David Smith Sundarsingh,Jyotirmoy V. Deshmukh,Yiannis Kantaros*

Main category: cs.CL

TL;DR: ConformalNL2LTL是一种新的自然语言到LTL的翻译方法，通过结合问答和不确定性量化，实现用户定义的翻译成功率。


<details>
  <summary>Details</summary>
Motivation: 减少手动定义LTL任务所需的工作量和专业知识，并提供翻译正确性保证。

Method: 通过迭代解决开放词汇问答问题，利用LLMs生成答案，并结合conformal prediction量化不确定性。

Result: ConformalNL2LTL能够达到用户指定的翻译准确率，同时最小化求助率。

Conclusion: 该方法在理论和实证上均证明了其有效性，为NL到LTL的翻译提供了可靠解决方案。

Abstract: Linear Temporal Logic (LTL) has become a prevalent specification language for
robotic tasks. To mitigate the significant manual effort and expertise required
to define LTL-encoded tasks, several methods have been proposed for translating
Natural Language (NL) instructions into LTL formulas, which, however, lack
correctness guarantees. To address this, we introduce a new NL-to-LTL
translation method, called ConformalNL2LTL, that can achieve user-defined
translation success rates over unseen NL commands. Our method constructs LTL
formulas iteratively by addressing a sequence of open-vocabulary
Question-Answering (QA) problems with LLMs. To enable uncertainty-aware
translation, we leverage conformal prediction (CP), a distribution-free
uncertainty quantification tool for black-box models. CP enables our method to
assess the uncertainty in LLM-generated answers, allowing it to proceed with
translation when sufficiently confident and request help otherwise. We provide
both theoretical and empirical results demonstrating that ConformalNL2LTL
achieves user-specified translation accuracy while minimizing help rates.

</details>


### [9] [Param$Δ$ for Direct Weight Mixing: Post-Train Large Language Model at Zero Cost](https://arxiv.org/abs/2504.21023)
*Sheng Cao,Mingrui Wu,Karthik Prasad,Yuandong Tian,Zechun Liu*

Main category: cs.CL

TL;DR: 论文提出了一种名为$Param\Delta$的新方法，通过计算已有后训练模型与基础模型权重的差异，无需额外训练即可将后训练能力迁移到更新的基础模型上。


<details>
  <summary>Details</summary>
Motivation: 后训练阶段需要大量高质量数据和计算资源，且存在过拟合风险，$Param\Delta$旨在解决这些问题。

Method: 通过计算后训练模型权重与基础模型权重的差异，并将该差异应用于更新的基础模型，实现零额外训练的知识迁移。

Result: 实验表明，$Param\Delta$模型性能接近直接后训练模型，例如在70B Llama3模型上达到Llama3.1-inst模型95%的性能。

Conclusion: $Param\Delta$为开源模型社区提供了一种零成本加速模型迭代的方法。

Abstract: The post-training phase of large language models is essential for enhancing
capabilities such as instruction-following, reasoning, and alignment with human
preferences. However, it demands extensive high-quality data and poses risks
like overfitting, alongside significant computational costs due to repeated
post-training and evaluation after each base model update. This paper
introduces $Param\Delta$, a novel method that streamlines post-training by
transferring knowledge from an existing post-trained model to a newly updated
base model with ZERO additional training. By computing the difference between
post-trained model weights ($\Theta_\text{post}$) and base model weights
($\Theta_\text{base}$), and adding this to the updated base model
($\Theta'_\text{base}$), we define $Param\Delta$ Model as:
$\Theta_{\text{Param}\Delta} = \Theta_\text{post} - \Theta_\text{base} +
\Theta'_\text{base}$. This approach surprisingly equips the new base model with
post-trained capabilities, achieving performance comparable to direct
post-training. We did analysis on LLama3, Llama3.1, Qwen, and
DeepSeek-distilled models. Results indicate $Param\Delta$ Model effectively
replicates traditional post-training. For example, the $Param\Delta$ Model
obtained from 70B Llama3-inst, Llama3-base, Llama3.1-base models attains
approximately 95\% of Llama3.1-inst model's performance on average.
$Param\Delta$ brings a new perspective on how to fully leverage models in the
open-weight community, where checkpoints for base and instruct models are
readily available and frequently updated, by providing a cost-free framework to
accelerate the iterative cycle of model development.

</details>


### [10] [WebEvolver: Enhancing Web Agent Self-Improvement with Coevolving World Model](https://arxiv.org/abs/2504.21024)
*Tianqing Fang,Hongming Zhang,Zhisong Zhang,Kaixin Ma,Wenhao Yu,Haitao Mi,Dong Yu*

Main category: cs.CL

TL;DR: 提出了一种结合世界模型LLM的新框架，通过增强探索和利用预训练知识，解决了自主学习中性能停滞的问题，实验显示性能提升10%。


<details>
  <summary>Details</summary>
Motivation: 现有自主学习方法在性能提升过程中会遇到停滞点，原因是环境探索不足和预训练知识利用不充分。

Method: 引入一个共同进化的世界模型LLM，预测环境状态变化，生成自指导训练数据，并在推理时模拟前瞻行动。

Result: 在真实网络环境中（Mind2Web-Live、WebVoyager和GAIA-web）性能提升10%，无需依赖闭源模型蒸馏。

Conclusion: 世界模型的整合是实现自主代理持续适应性的关键。

Abstract: Agent self-improvement, where the backbone Large Language Model (LLM) of the
agent are trained on trajectories sampled autonomously based on their own
policies, has emerged as a promising approach for enhancing performance. Recent
advancements, particularly in web environments, face a critical limitation:
their performance will reach a stagnation point during autonomous learning
cycles, hindering further improvement. We argue that this stems from limited
exploration of the web environment and insufficient exploitation of pre-trained
web knowledge in LLMs. To improve the performance of self-improvement, we
propose a novel framework that introduces a co-evolving World Model LLM. This
world model predicts the next observation based on the current observation and
action within the web environment. Leveraging LLMs' pretrained knowledge of
abundant web content, the World Model serves dual roles: (1) as a virtual web
server generating self-instructed training data to continuously refine the
agent's policy, and (2) as an imagination engine during inference, enabling
look-ahead simulation to guide action selection for the agent LLM. Experiments
in real-world web environments (Mind2Web-Live, WebVoyager, and GAIA-web) show a
10% performance gain over existing self-evolving agents, demonstrating the
efficacy and generalizability of our approach, without using any distillation
from more powerful close-sourced models. Our work establishes the necessity of
integrating world models into autonomous agent frameworks to unlock sustained
adaptability.

</details>


### [11] [Durghotona GPT: A Web Scraping and Large Language Model Based Framework to Generate Road Accident Dataset Automatically in Bangladesh](https://arxiv.org/abs/2504.21025)
*MD Thamed Bin Zaman Chowdhury,Moazzem Hossain,Md. Ridwanul Islam*

Main category: cs.CL

TL;DR: 论文提出了一种名为'Durghotona GPT'的框架，通过结合网络爬取和大型语言模型（LLMs），从孟加拉国主要报纸中自动生成全面的交通事故数据集。


<details>
  <summary>Details</summary>
Motivation: 全球范围内交通事故导致重大经济损失和社会问题，准确及时的数据对预测和缓解事故至关重要。

Method: 从三家主要报纸收集事故报告，使用GPT-4、GPT-3.5和Llama-3等LLMs处理数据，提取信息并分类。

Result: Llama-3表现接近GPT-4，准确率达89%，框架显著提升数据质量和可用性。

Conclusion: 该框架支持交通安全分析、城市规划和公共卫生等应用，未来将扩展数据收集方法并优化LLMs。

Abstract: Road accidents pose significant concerns globally. They lead to large
financial losses, injuries, disabilities, and societal challenges. Accurate and
timely accident data is essential for predicting and mitigating these events.
This paper presents a novel framework named 'Durghotona GPT' that integrates
web scraping and Large Language Models (LLMs) to automate the generation of
comprehensive accident datasets from prominent national dailies in Bangladesh.
The authors collected accident reports from three major newspapers: Prothom
Alo, Dhaka Tribune, and The Daily Star. The collected news was then processed
using the newest available LLMs: GPT-4, GPT-3.5, and Llama-3. The framework
efficiently extracts relevant information, categorizes reports, and compiles
detailed datasets. Thus, this framework overcomes limitations of manual data
collection methods such as delays, errors, and communication gaps. The authors'
evaluation demonstrates that Llama-3, an open-source model, performs comparably
to GPT-4. It achieved 89% accuracy in the authors' evaluation. Therefore, it
can be considered a cost-effective alternative for similar tasks. The results
suggest that the framework developed by the authors can drastically enhance the
quality and availability of accident data. As a result, it can support critical
applications in traffic safety analysis, urban planning, and public health. The
authors also developed an interface for 'Durghotona GPT' for ease of use as
part of this paper. Future work will focus on expanding data collection methods
and refining LLMs to further increase dataset accuracy and applicability.

</details>


### [12] [Creating and Evaluating Code-Mixed Nepali-English and Telugu-English Datasets for Abusive Language Detection Using Traditional and Deep Learning Models](https://arxiv.org/abs/2504.21026)
*Manish Pandey,Nageshwar Prasad Yadav,Mokshada Adduru,Sawan Rai*

Main category: cs.CL

TL;DR: 该研究提出了一种针对泰卢固语-英语和尼泊尔语-英语混合文本的滥用语言检测方法，通过构建新的标注数据集并评估多种机器学习、深度学习和大型语言模型，填补了低资源语言在滥用检测领域的空白。


<details>
  <summary>Details</summary>
Motivation: 随着多语言用户在社交媒体上的增加，混合文本中的滥用语言检测变得更具挑战性，尤其是低资源语言如泰卢固语和尼泊尔语缺乏相关研究。

Method: 研究构建了一个包含2000条泰卢固语-英语和500条尼泊尔语-英语混合文本的标注数据集，并评估了多种模型（如逻辑回归、随机森林、SVM、神经网络、LSTM、CNN和LLMs），通过超参数调优和10折交叉验证优化性能。

Result: 研究提供了关于混合文本中滥用语言检测的挑战性见解，并对不同计算方法进行了比较分析，为低资源语言的NLP研究提供了基准。

Conclusion: 该研究通过数据集和模型分析，为多语言社交媒体环境中的滥用检测提供了更强大的策略支持，推动了低资源语言NLP的发展。

Abstract: With the growing presence of multilingual users on social media, detecting
abusive language in code-mixed text has become increasingly challenging.
Code-mixed communication, where users seamlessly switch between English and
their native languages, poses difficulties for traditional abuse detection
models, as offensive content may be context-dependent or obscured by linguistic
blending. While abusive language detection has been extensively explored for
high-resource languages like English and Hindi, low-resource languages such as
Telugu and Nepali remain underrepresented, leaving gaps in effective
moderation. In this study, we introduce a novel, manually annotated dataset of
2 thousand Telugu-English and 5 Nepali-English code-mixed comments, categorized
as abusive and non-abusive, collected from various social media platforms. The
dataset undergoes rigorous preprocessing before being evaluated across multiple
Machine Learning (ML), Deep Learning (DL), and Large Language Models (LLMs). We
experimented with models including Logistic Regression, Random Forest, Support
Vector Machines (SVM), Neural Networks (NN), LSTM, CNN, and LLMs, optimizing
their performance through hyperparameter tuning, and evaluate it using 10-fold
cross-validation and statistical significance testing (t-test). Our findings
provide key insights into the challenges of detecting abusive language in
code-mixed settings and offer a comparative analysis of computational
approaches. This study contributes to advancing NLP for low-resource languages
by establishing benchmarks for abusive language detection in Telugu-English and
Nepali-English code-mixed text. The dataset and insights can aid in the
development of more robust moderation strategies for multilingual social media
environments.

</details>


### [13] [UrbanPlanBench: A Comprehensive Urban Planning Benchmark for Evaluating Large Language Models](https://arxiv.org/abs/2504.21027)
*Yu Zheng,Longyi Liu,Yuming Lin,Jie Feng,Guozhen Zhang,Depeng Jin,Yong Li*

Main category: cs.CL

TL;DR: 论文提出了一个名为UrbanPlanBench的基准测试，用于评估大型语言模型（LLMs）在城市规划中的表现，并揭示了LLMs在规划知识获取上的不平衡性。同时，论文还发布了最大的监督微调数据集UrbanPlanText，以促进LLMs在城市规划领域的应用。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在城市规划领域的潜力，填补现有研究中关于LLMs辅助人类规划师能力的空白。

Method: 开发了UrbanPlanBench基准测试和UrbanPlanText数据集，通过广泛的评估和微调实验分析LLMs的表现。

Result: 发现LLMs在规划知识获取上存在显著不平衡，70%的模型在理解规划法规方面表现不佳。微调后的模型在记忆测试和知识理解上有所提升，但在专业术语和推理任务上仍有不足。

Conclusion: 通过公开基准、数据集和工具集，论文旨在推动LLMs与城市规划实践的融合，实现人机协作的共生关系。

Abstract: The advent of Large Language Models (LLMs) holds promise for revolutionizing
various fields traditionally dominated by human expertise. Urban planning, a
professional discipline that fundamentally shapes our daily surroundings, is
one such field heavily relying on multifaceted domain knowledge and experience
of human experts. The extent to which LLMs can assist human practitioners in
urban planning remains largely unexplored. In this paper, we introduce a
comprehensive benchmark, UrbanPlanBench, tailored to evaluate the efficacy of
LLMs in urban planning, which encompasses fundamental principles, professional
knowledge, and management and regulations, aligning closely with the
qualifications expected of human planners. Through extensive evaluation, we
reveal a significant imbalance in the acquisition of planning knowledge among
LLMs, with even the most proficient models falling short of meeting
professional standards. For instance, we observe that 70% of LLMs achieve
subpar performance in understanding planning regulations compared to other
aspects. Besides the benchmark, we present the largest-ever supervised
fine-tuning (SFT) dataset, UrbanPlanText, comprising over 30,000 instruction
pairs sourced from urban planning exams and textbooks. Our findings demonstrate
that fine-tuned models exhibit enhanced performance in memorization tests and
comprehension of urban planning knowledge, while there exists significant room
for improvement, particularly in tasks requiring domain-specific terminology
and reasoning. By making our benchmark, dataset, and associated evaluation and
fine-tuning toolsets publicly available at
https://github.com/tsinghua-fib-lab/PlanBench, we aim to catalyze the
integration of LLMs into practical urban planning, fostering a symbiotic
collaboration between human expertise and machine intelligence.

</details>


### [14] [Beyond One-Size-Fits-All: Inversion Learning for Highly Effective NLG Evaluation Prompts](https://arxiv.org/abs/2504.21117)
*Hanhua Hong,Chenghao Xiao,Yang Wang,Yiqi Liu,Wenge Rong,Chenghua Lin*

Main category: cs.CL

TL;DR: 提出了一种基于反转学习的方法，自动生成高效、模型特定的评估提示，以提高LLM评估的效率和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 自然语言生成（NLG）系统评估困难，人工评估存在不一致性、缺乏标准化和人口偏见，而LLM评估对提示设计敏感。

Method: 采用反转学习方法，从模型输出反向映射到输入指令，自动生成评估提示。

Result: 仅需单个评估样本，无需耗时的手动提示工程，提高了效率和鲁棒性。

Conclusion: 为更高效和鲁棒的LLM评估提供了新方向。

Abstract: Evaluating natural language generation (NLG) systems is challenging due to
the diversity of valid outputs. While human evaluation is the gold standard, it
suffers from inconsistencies, lack of standardisation, and demographic biases,
limiting reproducibility. LLM-based evaluation offers a scalable alternative
but is highly sensitive to prompt design, where small variations can lead to
significant discrepancies. In this work, we propose an inversion learning
method that learns effective reverse mappings from model outputs back to their
input instructions, enabling the automatic generation of highly effective,
model-specific evaluation prompts. Our method requires only a single evaluation
sample and eliminates the need for time-consuming manual prompt engineering,
thereby improving both efficiency and robustness. Our work contributes toward a
new direction for more robust and efficient LLM-based evaluation.

</details>


### [15] [LLM Enhancer: Merged Approach using Vector Embedding for Reducing Large Language Model Hallucinations with External Knowledge](https://arxiv.org/abs/2504.21132)
*Naheed Rayhan,Md. Ashrafuzzaman*

Main category: cs.CL

TL;DR: LLM ENHANCER系统通过整合多源在线数据提升LLMs的准确性，减少幻觉生成。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs在关键场景中生成不准确信息及无法有效利用外部知识的问题。

Method: 并行数据获取，利用自定义代理工具管理信息流，通过向量嵌入筛选最相关信息。

Result: 系统在保持回答自然性的同时，显著减少了LLMs的幻觉现象。

Conclusion: LLM ENHANCER系统有效提升了LLMs在真实场景中的可靠性和实用性。

Abstract: Large Language Models (LLMs), such as ChatGPT, have demonstrated the
capability to generate human like, natural responses across a range of tasks,
including task oriented dialogue and question answering. However, their
application in real world, critical scenarios is often hindered by a tendency
to produce inaccurate information and a limited ability to leverage external
knowledge sources. This paper introduces the LLM ENHANCER system, designed to
integrate multiple online sources such as Google, Wikipedia, and DuckDuckGo to
enhance data accuracy. The LLMs employed within this system are open source.
The data acquisition process for the LLM ENHANCER system operates in parallel,
utilizing custom agent tools to manage the flow of information. Vector
embeddings are used to identify the most pertinent information, which is
subsequently supplied to the LLM for user interaction. The LLM ENHANCER system
mitigates hallucinations in chat based LLMs while preserving response
naturalness and accuracy.

</details>


### [16] [Detecting Manipulated Contents Using Knowledge-Grounded Inference](https://arxiv.org/abs/2504.21165)
*Mark Huasong Meng,Ruizhe Wang,Meng Xu,Chuan Yan,Guangdong Bai*

Main category: cs.CL

TL;DR: Manicod是一种检测零日操纵内容的工具，通过检索增强生成（RAG）结合大型语言模型（LLM）进行推理，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有解决方案依赖训练中的固有知识或手动整理的上下文，无法应对零日操纵内容，需要实时上下文信息。

Method: Manicod从主流搜索引擎获取上下文信息，通过RAG向量化后输入LLM进行推理，生成判断和解释。

Result: 在包含4270条操纵假新闻的数据集上，Manicod的F1得分为0.856，优于现有方法1.9倍。

Conclusion: Manicod能有效检测零日操纵内容，为假新闻识别提供新方法。

Abstract: The detection of manipulated content, a prevalent form of fake news, has been
widely studied in recent years. While existing solutions have been proven
effective in fact-checking and analyzing fake news based on historical events,
the reliance on either intrinsic knowledge obtained during training or manually
curated context hinders them from tackling zero-day manipulated content, which
can only be recognized with real-time contextual information. In this work, we
propose Manicod, a tool designed for detecting zero-day manipulated content.
Manicod first sources contextual information about the input claim from
mainstream search engines, and subsequently vectorizes the context for the
large language model (LLM) through retrieval-augmented generation (RAG). The
LLM-based inference can produce a "truthful" or "manipulated" decision and
offer a textual explanation for the decision. To validate the effectiveness of
Manicod, we also propose a dataset comprising 4270 pieces of manipulated fake
news derived from 2500 recent real-world news headlines. Manicod achieves an
overall F1 score of 0.856 on this dataset and outperforms existing methods by
up to 1.9x in F1 score on their benchmarks on fact-checking and claim
verification.

</details>


### [17] [Small or Large? Zero-Shot or Finetuned? Guiding Language Model Choice for Specialized Applications in Healthcare](https://arxiv.org/abs/2504.21191)
*Lovedeep Gondara,Jonathan Simkin,Graham Sayle,Shebnum Devji,Gregory Arbour,Raymond Ng*

Main category: cs.CL

TL;DR: 研究探讨了语言模型选择的关键因素，包括微调与零样本使用的必要性、领域相关与通用预训练模型的优势、进一步领域特定预训练的价值，以及小型语言模型（SLMs）在特定任务中相对于大型语言模型（LLMs）的持续相关性。


<details>
  <summary>Details</summary>
Motivation: 指导语言模型选择，特别是在专业领域任务中，比较微调与零样本使用、领域相关与通用模型、以及SLMs与LLMs的性能差异。

Method: 使用BCCR的电子病理报告，评估三种不同难度和数据量的分类场景，比较SLMs（零样本和微调）与零样本LLM的性能。

Result: 微调显著提升SLMs性能，优于零样本LLM；领域相关SLMs在微调后表现更佳；领域特定预训练对复杂任务有显著帮助。

Conclusion: 在专业领域任务中，微调SLMs表现优于零样本LLMs，SLMs在性能和资源消耗上更具优势，仍具实际应用价值。

Abstract: This study aims to guide language model selection by investigating: 1) the
necessity of finetuning versus zero-shot usage, 2) the benefits of
domain-adjacent versus generic pretrained models, 3) the value of further
domain-specific pretraining, and 4) the continued relevance of Small Language
Models (SLMs) compared to Large Language Models (LLMs) for specific tasks.
Using electronic pathology reports from the British Columbia Cancer Registry
(BCCR), three classification scenarios with varying difficulty and data size
are evaluated. Models include various SLMs and an LLM. SLMs are evaluated both
zero-shot and finetuned; the LLM is evaluated zero-shot only. Finetuning
significantly improved SLM performance across all scenarios compared to their
zero-shot results. The zero-shot LLM outperformed zero-shot SLMs but was
consistently outperformed by finetuned SLMs. Domain-adjacent SLMs generally
performed better than the generic SLM after finetuning, especially on harder
tasks. Further domain-specific pretraining yielded modest gains on easier tasks
but significant improvements on the complex, data-scarce task. The results
highlight the critical role of finetuning for SLMs in specialized domains,
enabling them to surpass zero-shot LLM performance on targeted classification
tasks. Pretraining on domain-adjacent or domain-specific data provides further
advantages, particularly for complex problems or limited finetuning data. While
LLMs offer strong zero-shot capabilities, their performance on these specific
tasks did not match that of appropriately finetuned SLMs. In the era of LLMs,
SLMs remain relevant and effective, offering a potentially superior
performance-resource trade-off compared to LLMs.

</details>


### [18] [Automatic Legal Writing Evaluation of LLMs](https://arxiv.org/abs/2504.21202)
*Ramon Pires,Roseval Malaquias Junior,Rodrigo Nogueira*

Main category: cs.CL

TL;DR: oab-bench是一个用于评估大语言模型在法律写作领域表现的基准测试，基于巴西律师考试题目，包含105个问题。Claude-3.5 Sonnet表现最佳，平均得分7.93/10。前沿模型如OpenAI的o1在评分上与人类评分高度相关，显示出作为自动评分器的潜力。


<details>
  <summary>Details</summary>
Motivation: 由于法律写作评估的复杂性，缺乏公开且全面的测试数据集。巴西律师考试提供了满足这些需求的资源，因此开发了oab-bench。

Method: 使用巴西律师考试的105个问题构建基准测试，包含评分指南和参考材料。评估了四种大语言模型的性能，并测试了它们作为自动评分器的可靠性。

Result: Claude-3.5 Sonnet表现最佳，平均得分7.93/10。前沿模型在评分上与人类评分高度相关。

Conclusion: oab-bench为法律写作评估提供了有效工具，大语言模型在自动评分方面显示出潜力。

Abstract: Despite the recent advances in Large Language Models, benchmarks for
evaluating legal writing remain scarce due to the inherent complexity of
assessing open-ended responses in this domain. One of the key challenges in
evaluating language models on domain-specific tasks is finding test datasets
that are public, frequently updated, and contain comprehensive evaluation
guidelines. The Brazilian Bar Examination meets these requirements. We
introduce oab-bench, a benchmark comprising 105 questions across seven areas of
law from recent editions of the exam. The benchmark includes comprehensive
evaluation guidelines and reference materials used by human examiners to ensure
consistent grading. We evaluate the performance of four LLMs on oab-bench,
finding that Claude-3.5 Sonnet achieves the best results with an average score
of 7.93 out of 10, passing all 21 exams. We also investigated whether LLMs can
serve as reliable automated judges for evaluating legal writing. Our
experiments show that frontier models like OpenAI's o1 achieve a strong
correlation with human scores when evaluating approved exams, suggesting their
potential as reliable automated evaluators despite the inherently subjective
nature of legal writing assessment. The source code and the benchmark --
containing questions, evaluation guidelines, model-generated responses, and
their respective automated evaluations -- are publicly available.

</details>


### [19] [Pretraining Large Brain Language Model for Active BCI: Silent Speech](https://arxiv.org/abs/2504.21214)
*Jinzhao Zhou,Zehong Cao,Yiqun Duan,Connor Barkley,Daniel Leong,Xiaowei Jiang,Quoc-Toan Nguyen,Ziyi Zhao,Thomas Do,Yu-Cheng Chang,Sheng-Fu Liang,Chin-teng Lin*

Main category: cs.CL

TL;DR: 论文提出了一种基于自监督预训练的大脑语言模型（LBLM），用于解码无声语音，并在跨会话任务中显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 探索无声语音解码在主动脑机接口（BCI）中的应用，提供比传统BCI更自然灵活的通信方式。

Method: 提出未来时频预测（FSTP）预训练范式，通过自回归建模学习EEG信号的时频依赖关系，并预训练LBLM模型。

Result: 在跨会话任务中，LBLM在语义级和词级分类上分别达到47.0%和39.6%的准确率，显著优于基线方法。

Conclusion: 研究推动了无声语音解码技术的发展，为EEG语言模型预训练提供了创新解决方案和新数据集。

Abstract: This paper explores silent speech decoding in active brain-computer interface
(BCI) systems, which offer more natural and flexible communication than
traditional BCI applications. We collected a new silent speech dataset of over
120 hours of electroencephalogram (EEG) recordings from 12 subjects, capturing
24 commonly used English words for language model pretraining and decoding.
Following the recent success of pretraining large models with self-supervised
paradigms to enhance EEG classification performance, we propose Large Brain
Language Model (LBLM) pretrained to decode silent speech for active BCI. To
pretrain LBLM, we propose Future Spectro-Temporal Prediction (FSTP) pretraining
paradigm to learn effective representations from unlabeled EEG data. Unlike
existing EEG pretraining methods that mainly follow a masked-reconstruction
paradigm, our proposed FSTP method employs autoregressive modeling in temporal
and frequency domains to capture both temporal and spectral dependencies from
EEG signals. After pretraining, we finetune our LBLM on downstream tasks,
including word-level and semantic-level classification. Extensive experiments
demonstrate significant performance gains of the LBLM over fully-supervised and
pretrained baseline models. For instance, in the difficult cross-session
setting, our model achieves 47.0\% accuracy on semantic-level classification
and 39.6\% in word-level classification, outperforming baseline methods by
5.4\% and 7.3\%, respectively. Our research advances silent speech decoding in
active BCI systems, offering an innovative solution for EEG language model
pretraining and a new dataset for fundamental research.

</details>


### [20] [Phi-4-Mini-Reasoning: Exploring the Limits of Small Reasoning Language Models in Math](https://arxiv.org/abs/2504.21233)
*Haoran Xu,Baolin Peng,Hany Awadalla,Dongdong Chen,Yen-Chun Chen,Mei Gao,Young Jin Kim,Yunsheng Li,Liliang Ren,Yelong Shen,Shuohang Wang,Weijian Xu,Jianfeng Gao,Weizhu Chen*

Main category: cs.CL

TL;DR: 论文提出了一种系统训练方法，通过四步提升小型语言模型（SLM）的推理能力，并在Phi-4-Mini上验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管链式思维（CoT）能显著提升大型语言模型（LLM）的推理能力，但小型语言模型（SLM）由于容量有限，推理能力提升仍具挑战性。

Method: 四步训练方法：1）大规模中训练；2）监督微调；3）偏好数据集优化；4）可验证奖励的强化学习。

Result: Phi-4-Mini-Reasoning在数学推理任务上超越更大模型，如Math-500上分别超出DeepSeek-R1-Distill-Qwen-7B和Llama-8B 3.2和7.7分。

Conclusion: 精心设计的训练方法和大规模高质量CoT数据可显著提升小型模型的推理能力。

Abstract: Chain-of-Thought (CoT) significantly enhances formal reasoning capabilities
in Large Language Models (LLMs) by training them to explicitly generate
intermediate reasoning steps. While LLMs readily benefit from such techniques,
improving reasoning in Small Language Models (SLMs) remains challenging due to
their limited model capacity. Recent work by Deepseek-R1 demonstrates that
distillation from LLM-generated synthetic data can substantially improve the
reasoning ability of SLM. However, the detailed modeling recipe is not
disclosed. In this work, we present a systematic training recipe for SLMs that
consists of four steps: (1) large-scale mid-training on diverse distilled
long-CoT data, (2) supervised fine-tuning on high-quality long-CoT data, (3)
Rollout DPO leveraging a carefully curated preference dataset, and (4)
Reinforcement Learning (RL) with Verifiable Reward. We apply our method on
Phi-4-Mini, a compact 3.8B-parameter model. The resulting Phi-4-Mini-Reasoning
model exceeds, on math reasoning tasks, much larger reasoning models, e.g.,
outperforming DeepSeek-R1-Distill-Qwen-7B by 3.2 points and
DeepSeek-R1-Distill-Llama-8B by 7.7 points on Math-500. Our results validate
that a carefully designed training recipe, with large-scale high-quality CoT
data, is effective to unlock strong reasoning capabilities even in
resource-constrained small models.

</details>


### [21] [Memorization and Knowledge Injection in Gated LLMs](https://arxiv.org/abs/2504.21239)
*Xu Pan,Ely Hahami,Zechen Zhang,Haim Sompolinsky*

Main category: cs.CL

TL;DR: MEGa框架通过门控低秩权重直接嵌入记忆到LLMs中，解决了LLMs持续学习和知识整合的难题。


<details>
  <summary>Details</summary>
Motivation: LLMs难以持续学习和整合新知识，而人类能不断从新经验中学习。现有方法多依赖大上下文窗口或外部记忆缓冲，缺乏对日常事件场景的测试。

Method: 提出MEGa框架，将事件记忆嵌入到LLMs的门控低秩权重中，通过门控机制激活相关记忆。

Result: 在虚构角色和维基百科事件数据集上，MEGa在缓解灾难性遗忘方面优于基线方法。

Conclusion: MEGa受人类大脑互补记忆系统启发，为LLMs的持续学习提供了新思路。

Abstract: Large Language Models (LLMs) currently struggle to sequentially add new
memories and integrate new knowledge. These limitations contrast with the human
ability to continuously learn from new experiences and acquire knowledge
throughout life. Most existing approaches add memories either through large
context windows or external memory buffers (e.g., Retrieval-Augmented
Generation), and studies on knowledge injection rarely test scenarios
resembling everyday life events. In this work, we introduce a continual
learning framework, Memory Embedded in Gated LLMs (MEGa), which injects event
memories directly into the weights of LLMs. Each memory is stored in a
dedicated set of gated low-rank weights. During inference, a gating mechanism
activates relevant memory weights by matching query embeddings to stored memory
embeddings. This enables the model to both recall entire memories and answer
related questions. On two datasets - fictional characters and Wikipedia events
- MEGa outperforms baseline approaches in mitigating catastrophic forgetting.
Our model draws inspiration from the complementary memory system of the human
brain.

</details>


### [22] [Talk Before You Retrieve: Agent-Led Discussions for Better RAG in Medical QA](https://arxiv.org/abs/2504.21252)
*Xuanzhao Dong,Wenhui Zhu,Hao Wang,Xiwen Chen,Peijie Qiu,Rui Yin,Yi Su,Yalin Wang*

Main category: cs.CL

TL;DR: Discuss-RAG是一个通过协作代理推理增强医学问答系统的模块，显著提升了答案准确性。


<details>
  <summary>Details</summary>
Motivation: 医学问答任务对大型语言模型具有挑战性，现有检索增强生成系统存在推理行为建模不足和依赖低质量语料库的问题。

Method: 引入一个总结代理协调医学专家团队模拟多轮头脑风暴，并通过决策代理评估检索内容。

Result: 在四个医学问答基准数据集上，Discuss-RAG显著优于MedRAG，答案准确性最高提升16.67%。

Conclusion: Discuss-RAG通过协作代理推理有效提升了医学问答系统的性能。

Abstract: Medical question answering (QA) is a reasoning-intensive task that remains
challenging for large language models (LLMs) due to hallucinations and outdated
domain knowledge. Retrieval-Augmented Generation (RAG) provides a promising
post-training solution by leveraging external knowledge. However, existing
medical RAG systems suffer from two key limitations: (1) a lack of modeling for
human-like reasoning behaviors during information retrieval, and (2) reliance
on suboptimal medical corpora, which often results in the retrieval of
irrelevant or noisy snippets. To overcome these challenges, we propose
Discuss-RAG, a plug-and-play module designed to enhance the medical QA RAG
system through collaborative agent-based reasoning. Our method introduces a
summarizer agent that orchestrates a team of medical experts to emulate
multi-turn brainstorming, thereby improving the relevance of retrieved content.
Additionally, a decision-making agent evaluates the retrieved snippets before
their final integration. Experimental results on four benchmark medical QA
datasets show that Discuss-RAG consistently outperforms MedRAG, especially
significantly improving answer accuracy by up to 16.67% on BioASQ and 12.20% on
PubMedQA. The code is available at: https://github.com/LLM-VLM-GSL/Discuss-RAG.

</details>


### [23] [BiasGuard: A Reasoning-enhanced Bias Detection Tool For Large Language Models](https://arxiv.org/abs/2504.21299)
*Zhiting Fan,Ruizhe Chen,Zuozhu Liu*

Main category: cs.CL

TL;DR: BiasGuard是一种新型的偏见检测工具，通过两阶段方法（显式推理和强化学习）提升偏见检测的准确性，优于现有工具。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如公平性分类器和基于LLM的评判）在理解意图和公平性判断标准方面存在局限，需要更有效的偏见检测工具。

Method: BiasGuard采用两阶段方法：第一阶段基于公平性规范显式推理，第二阶段通过强化学习增强推理和判断能力。

Result: 在五个数据集上的实验表明，BiasGuard在准确性和减少过度公平误判方面优于现有工具。

Conclusion: BiasGuard通过推理增强的决策和两阶段优化，有效提升了偏见检测能力。

Abstract: Identifying bias in LLM-generated content is a crucial prerequisite for
ensuring fairness in LLMs. Existing methods, such as fairness classifiers and
LLM-based judges, face limitations related to difficulties in understanding
underlying intentions and the lack of criteria for fairness judgment. In this
paper, we introduce BiasGuard, a novel bias detection tool that explicitly
analyzes inputs and reasons through fairness specifications to provide accurate
judgments. BiasGuard is implemented through a two-stage approach: the first
stage initializes the model to explicitly reason based on fairness
specifications, while the second stage leverages reinforcement learning to
enhance its reasoning and judgment capabilities. Our experiments, conducted
across five datasets, demonstrate that BiasGuard outperforms existing tools,
improving accuracy and reducing over-fairness misjudgments. We also highlight
the importance of reasoning-enhanced decision-making and provide evidence for
the effectiveness of our two-stage optimization pipeline.

</details>


### [24] [Confidence in Large Language Model Evaluation: A Bayesian Approach to Limited-Sample Challenges](https://arxiv.org/abs/2504.21303)
*Xiao Xiao,Yu Su,Sijing Zhang,Zhang Chen,Yadong Chen,Tian Liu*

Main category: cs.CL

TL;DR: 本文提出了一种基于贝叶斯方法的LLM能力评估框架，通过概率推理整合先验知识，解决了小样本场景下的评估局限性，实验证明其优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统评估框架依赖确定性标量指标，无法充分反映LLM的概率输出特性，尤其在样本有限时表现不佳。

Method: 将模型能力视为潜在变量，利用精心设计的查询集诱导判别性响应，并将模型排名问题形式化为贝叶斯假设检验。

Result: 实验表明，该方法在GPT系列模型上表现出更强的判别能力，即使样本量减少也能保持统计稳健性。

Conclusion: 该研究通过结合贝叶斯推理与实际部署约束，推动了LLM评估方法的发展。

Abstract: Large language models (LLMs) exhibit probabilistic output characteristics,
yet conventional evaluation frameworks rely on deterministic scalar metrics.
This study introduces a Bayesian approach for LLM capability assessment that
integrates prior knowledge through probabilistic inference, addressing
limitations under limited-sample regimes. By treating model capabilities as
latent variables and leveraging a curated query set to induce discriminative
responses, we formalize model ranking as a Bayesian hypothesis testing problem
over mutually exclusive capability intervals. Experimental evaluations with
GPT-series models demonstrate that the proposed method achieves superior
discrimination compared to conventional evaluation methods. Results indicate
that even with reduced sample sizes, the approach maintains statistical
robustness while providing actionable insights, such as probabilistic
statements about a model's likelihood of surpassing specific baselines. This
work advances LLM evaluation methodologies by bridging Bayesian inference with
practical constraints in real-world deployment scenarios.

</details>


### [25] [Does the Prompt-based Large Language Model Recognize Students' Demographics and Introduce Bias in Essay Scoring?](https://arxiv.org/abs/2504.21330)
*Kaixun Yang,Mladen Raković,Dragan Gašević,Guanliang Chen*

Main category: cs.CL

TL;DR: 研究探讨了基于提示的大型语言模型（LLM）在自动作文评分（AES）中的偏见问题，发现模型对学生人口统计属性的预测能力与评分偏见相关。


<details>
  <summary>Details</summary>
Motivation: 传统微调方法需要技术背景，限制了教育者的可访问性。基于提示的工具（如ChatGPT）提高了可访问性，但偏见问题尚未明确。

Method: 使用公开数据集设计提示，通过GPT-4o推断学生人口统计属性，并评估评分的公平性。通过多元回归分析探讨预测能力与评分偏见的关系。

Result: 发现基于提示的LLM能部分推断学生人口统计属性；评分偏见在模型正确预测学生母语背景时更明显；非母语英语学习者的评分误差增加。

Conclusion: 基于提示的LLM在AES中存在偏见，尤其是对非母语英语学习者，需进一步研究以减轻偏见。

Abstract: Large Language Models (LLMs) are widely used in Automated Essay Scoring (AES)
due to their ability to capture semantic meaning. Traditional fine-tuning
approaches required technical expertise, limiting accessibility for educators
with limited technical backgrounds. However, prompt-based tools like ChatGPT
have made AES more accessible, enabling educators to obtain machine-generated
scores using natural-language prompts (i.e., the prompt-based paradigm).
Despite advancements, prior studies have shown bias in fine-tuned LLMs,
particularly against disadvantaged groups. It remains unclear whether such
biases persist or are amplified in the prompt-based paradigm with cutting-edge
tools. Since such biases are believed to stem from the demographic information
embedded in pre-trained models (i.e., the ability of LLMs' text embeddings to
predict demographic attributes), this study explores the relationship between
the model's predictive power of students' demographic attributes based on their
written works and its predictive bias in the scoring task in the prompt-based
paradigm. Using a publicly available dataset of over 25,000 students'
argumentative essays, we designed prompts to elicit demographic inferences
(i.e., gender, first-language background) from GPT-4o and assessed fairness in
automated scoring. Then we conducted multivariate regression analysis to
explore the impact of the model's ability to predict demographics on its
scoring outcomes. Our findings revealed that (i) prompt-based LLMs can somewhat
infer students' demographics, particularly their first-language backgrounds,
from their essays; (ii) scoring biases are more pronounced when the LLM
correctly predicts students' first-language background than when it does not;
and (iii) scoring error for non-native English speakers increases when the LLM
correctly identifies them as non-native.

</details>


### [26] [Retrieval-Enhanced Few-Shot Prompting for Speech Event Extraction](https://arxiv.org/abs/2504.21372)
*Máté Gedeon*

Main category: cs.CL

TL;DR: 论文提出了一种模块化的SpeechEE框架，结合ASR和LLM，通过语义搜索增强的提示方法提取语音中的事件信息，性能优于现有基准。


<details>
  <summary>Details</summary>
Motivation: SpeechEE任务结合了ASR和NLP，需要从语音中提取结构化事件信息，现有方法存在挑战。

Method: 采用模块化流水线框架，结合ASR和LLM，通过混合过滤机制分类语音段，并利用语义搜索增强的少样本LLM提示提取事件触发词和参数。

Result: 使用o1-mini模型在触发词分类和参数分类上分别达到63.3%和27.8%的F1分数，优于现有基准。

Conclusion: 流水线方法结合检索增强的LLM可以媲美端到端系统，同时保持可解释性和模块化，为未来结合文本和声学特征的混合模型提供了方向。

Abstract: Speech Event Extraction (SpeechEE) is a challenging task that lies at the
intersection of Automatic Speech Recognition (ASR) and Natural Language
Processing (NLP), requiring the identification of structured event information
from spoken language. In this work, we present a modular, pipeline-based
SpeechEE framework that integrates high-performance ASR with semantic
search-enhanced prompting of Large Language Models (LLMs). Our system first
classifies speech segments likely to contain events using a hybrid filtering
mechanism including rule-based, BERT-based, and LLM-based models. It then
employs few-shot LLM prompting, dynamically enriched via semantic similarity
retrieval, to identify event triggers and extract corresponding arguments. We
evaluate the pipeline using multiple LLMs (Llama3-8B, GPT-4o-mini, and o1-mini)
highlighting significant performance gains with o1-mini, which achieves 63.3%
F1 on trigger classification and 27.8% F1 on argument classification,
outperforming prior benchmarks. Our results demonstrate that pipeline
approaches, when empowered by retrieval-augmented LLMs, can rival or exceed
end-to-end systems while maintaining interpretability and modularity. This work
provides practical insights into LLM-driven event extraction and opens pathways
for future hybrid models combining textual and acoustic features.

</details>


### [27] [The Distribution of Dependency Distance and Hierarchical Distance in Contemporary Written Japanese and Its Influencing Factors](https://arxiv.org/abs/2504.21421)
*Linxuan Wang,Shuiyuan Yu*

Main category: cs.CL

TL;DR: 研究探讨了日语中依存距离（DD）与层次距离（HD）的关系，发现谓词的配价是两者权衡关系的根本原因。


<details>
  <summary>Details</summary>
Motivation: 探索日语中DD与HD的关系及其背后的认知机制。

Method: 通过固定句子长度比较DD和HD的概率分布，分析MDD和MHD随句子长度的变化及其相关性。

Result: 谓词的配价是MDD和MHD权衡关系的关键因素，且对HD分布的影响大于DD。

Conclusion: 日语母语者通过谓词配价调节线性和层次复杂度，MDD均值低于MHD。

Abstract: To explore the relationship between dependency distance (DD) and hierarchical
distance (HD) in Japanese, we compared the probability distributions of DD and
HD with and without sentence length fixed, and analyzed the changes in mean
dependency distance (MDD) and mean hierarchical distance (MHD) as sentence
length increases, along with their correlation coefficient based on the
Balanced Corpus of Contemporary Written Japanese. It was found that the valency
of the predicates is the underlying factor behind the trade-off relation
between MDD and MHD in Japanese. Native speakers of Japanese regulate the
linear complexity and hierarchical complexity through the valency of the
predicates, and the relative sizes of MDD and MHD depend on whether the
threshold of valency has been reached. Apart from the cognitive load, the
valency of the predicates also affects the probability distributions of DD and
HD. The effect of the valency of the predicates on the distribution of HD is
greater than on that of DD, which leads to differences in their probability
distributions and causes the mean of MDD to be lower than that of MHD.

</details>


### [28] [RWKV-X: A Linear Complexity Hybrid Language Model](https://arxiv.org/abs/2504.21463)
*Haowen Hou,Zhiyi Huang,Kaifeng Tan,Rongchang Lu,Fei Richard Yu*

Main category: cs.CL

TL;DR: RWKV-X是一种新型混合架构，结合了RWKV的短程建模效率和稀疏注意力机制，用于捕捉长程上下文。它在训练中实现线性时间复杂度，推理解码中实现常数时间复杂度，并在64K标记序列上表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决传统混合方法依赖全注意力层且保持二次复杂度的问题，提出一种更高效的架构以支持长程上下文建模。

Method: 结合RWKV的短程建模能力和稀疏注意力机制，设计线性时间复杂度的训练和常数时间复杂度的推理解码。

Result: 在64K标记序列上实现近乎完美的准确性，优于先前的RWKV-7模型，同时保持短程任务的强性能。

Conclusion: RWKV-X是一种可扩展且高效的语言建模主干，支持高达100万标记的序列解码，速度和内存使用稳定。

Abstract: In this paper, we introduce \textbf{RWKV-X}, a novel hybrid architecture that
combines the efficiency of RWKV for short-range modeling with a sparse
attention mechanism designed to capture long-range context. Unlike previous
hybrid approaches that rely on full attention layers and retain quadratic
complexity, RWKV-X achieves linear-time complexity in training and
constant-time complexity in inference decoding. We demonstrate that RWKV-X,
when continually pretrained on 64K-token sequences, achieves near-perfect
accuracy on the 64K passkey retrieval benchmark. It consistently outperforms
prior RWKV-7 models on long-context benchmarks, while maintaining strong
performance on short-context tasks. These results highlight RWKV-X as a
scalable and efficient backbone for general-purpose language modeling, capable
of decoding sequences up to 1 million tokens with stable speed and memory
usage. To facilitate further research and analysis, we have made the
checkpoints and the associated code publicly accessible at:
https://github.com/howard-hou/RWKV-X.

</details>


### [29] [Homa at SemEval-2025 Task 5: Aligning Librarian Records with OntoAligner for Subject Tagging](https://arxiv.org/abs/2504.21474)
*Hadi Bayrami Asl Tekanlou,Jafar Razmara,Mahsa Sanaei,Mostafa Rahgouy,Hamed Babaei Giglou*

Main category: cs.CL

TL;DR: Homa系统利用OntoAligner工具包和RAG技术，将主题标注问题转化为对齐任务，通过语义相似性匹配记录与GND分类，展示了在数字图书馆中改进主题标注的潜力。


<details>
  <summary>Details</summary>
Motivation: 解决TIBKAT技术记录的主题标注问题，利用GND分类法自动分配主题标签，提升数字图书馆的检索效率。

Method: 使用OntoAligner工具包，结合RAG技术，将主题标注问题建模为对齐任务，通过语义相似性匹配记录与GND分类。

Result: 实验结果表明该方法在多语言记录处理中具有潜力，但也揭示了其局限性。

Conclusion: 对齐技术在改进数字图书馆主题标注方面具有潜力，但需进一步优化以适应多语言环境。

Abstract: This paper presents our system, Homa, for SemEval-2025 Task 5: Subject
Tagging, which focuses on automatically assigning subject labels to technical
records from TIBKAT using the Gemeinsame Normdatei (GND) taxonomy. We leverage
OntoAligner, a modular ontology alignment toolkit, to address this task by
integrating retrieval-augmented generation (RAG) techniques. Our approach
formulates the subject tagging problem as an alignment task, where records are
matched to GND categories based on semantic similarity. We evaluate
OntoAligner's adaptability for subject indexing and analyze its effectiveness
in handling multilingual records. Experimental results demonstrate the
strengths and limitations of this method, highlighting the potential of
alignment techniques for improving subject tagging in digital libraries.

</details>


### [30] [Advancing Arabic Reverse Dictionary Systems: A Transformer-Based Approach with Dataset Construction Guidelines](https://arxiv.org/abs/2504.21475)
*Serry Sibaee,Samar Ahmed,Abdullah Al Harbi,Omer Nacar,Adel Ammar,Yasser Habashi,Wadii Boulila*

Main category: cs.CL

TL;DR: 该研究通过开发一种高效的阿拉伯语反向词典系统填补了阿拉伯语自然语言处理的空白，采用基于Transformer的半编码器神经网络架构，取得了最先进的成果。


<details>
  <summary>Details</summary>
Motivation: 解决阿拉伯语自然语言处理中反向词典任务的不足，提升阿拉伯语词汇定义的质量和实用性。

Method: 提出一种新颖的Transformer方法，结合半编码器神经网络架构，并构建全面的数据集和定义质量标准。

Result: 阿拉伯语专用模型（如ARBERTv2）表现优于通用多语言模型，排名得分达0.0644。

Conclusion: 该研究为阿拉伯语计算语言学提供了重要工具，并推动了语言学习、学术写作和专业交流的发展。

Abstract: This study addresses the critical gap in Arabic natural language processing
by developing an effective Arabic Reverse Dictionary (RD) system that enables
users to find words based on their descriptions or meanings. We present a novel
transformer-based approach with a semi-encoder neural network architecture
featuring geometrically decreasing layers that achieves state-of-the-art
results for Arabic RD tasks. Our methodology incorporates a comprehensive
dataset construction process and establishes formal quality standards for
Arabic lexicographic definitions. Experiments with various pre-trained models
demonstrate that Arabic-specific models significantly outperform general
multilingual embeddings, with ARBERTv2 achieving the best ranking score
(0.0644). Additionally, we provide a formal abstraction of the reverse
dictionary task that enhances theoretical understanding and develop a modular,
extensible Python library (RDTL) with configurable training pipelines. Our
analysis of dataset quality reveals important insights for improving Arabic
definition construction, leading to eight specific standards for building
high-quality reverse dictionary resources. This work contributes significantly
to Arabic computational linguistics and provides valuable tools for language
learning, academic writing, and professional communication in Arabic.

</details>


### [31] [Improving Informally Romanized Language Identification](https://arxiv.org/abs/2504.21540)
*Adrian Benton,Alexander Gutkin,Christo Kirov,Brian Roark*

Main category: cs.CL

TL;DR: 论文提出了一种通过合成训练数据提高罗马化文本语言识别准确率的方法，显著提升了20种印度语言的识别性能。


<details>
  <summary>Details</summary>
Motivation: 罗马化文本中拼写变异性高，导致语言识别困难（如印地语和乌尔都语）。

Method: 通过合成包含自然拼写变体的训练样本，改进语言识别系统的训练方法。

Result: 在Bhasha-Abhijnaanam评估集上，F1分数从74.7%提升至85.4%（仅用合成数据）和88.2%（结合真实数据）。

Conclusion: 合成训练数据能显著提升罗马化文本的语言识别性能，且效果优于传统方法。

Abstract: The Latin script is often used to informally write languages with non-Latin
native scripts. In many cases (e.g., most languages in India), there is no
conventional spelling of words in the Latin script, hence there will be high
spelling variability in written text. Such romanization renders languages that
are normally easily distinguished based on script highly confusable, such as
Hindi and Urdu. In this work, we increase language identification (LID)
accuracy for romanized text by improving the methods used to synthesize
training sets. We find that training on synthetic samples which incorporate
natural spelling variation yields higher LID system accuracy than including
available naturally occurring examples in the training set, or even training
higher capacity models. We demonstrate new state-of-the-art LID performance on
romanized text from 20 Indic languages in the Bhasha-Abhijnaanam evaluation set
(Madhani et al., 2023a), improving test F1 from the reported 74.7% (using a
pretrained neural model) to 85.4% using a linear classifier trained solely on
synthetic data and 88.2% when also training on available harvested text.

</details>


### [32] [TartuNLP at SemEval-2025 Task 5: Subject Tagging as Two-Stage Information Retrieval](https://arxiv.org/abs/2504.21547)
*Aleksei Dorkin,Kairit Sirts*

Main category: cs.CL

TL;DR: 论文提出了一种两阶段信息检索系统，用于为图书馆记录分配主题标签，显著提高了召回率。


<details>
  <summary>Details</summary>
Motivation: 帮助图书馆员为文档分配主题标签，提高标签分配的效率和准确性。

Method: 采用两阶段信息检索系统：第一阶段使用双编码器进行粗粒度候选提取，第二阶段使用交叉编码器进行细粒度重排序。

Result: 该方法显著提高了召回率，并在定性评估中表现出竞争力。

Conclusion: 两阶段信息检索系统在主题标签分配任务中表现出高效性和竞争力。

Abstract: We present our submission to the Task 5 of SemEval-2025 that aims to aid
librarians in assigning subject tags to the library records by producing a list
of likely relevant tags for a given document. We frame the task as an
information retrieval problem, where the document content is used to retrieve
subject tags from a large subject taxonomy. We leverage two types of encoder
models to build a two-stage information retrieval system -- a bi-encoder for
coarse-grained candidate extraction at the first stage, and a cross-encoder for
fine-grained re-ranking at the second stage. This approach proved effective,
demonstrating significant improvements in recall compared to single-stage
methods and showing competitive results according to qualitative evaluation.

</details>


### [33] [Precision Where It Matters: A Novel Spike Aware Mixed-Precision Quantization Strategy for LLaMA-based Language Models](https://arxiv.org/abs/2504.21553)
*Lucas Maisonnave,Cyril Moineau,Olivier Bichler,Fabrice Rastello*

Main category: cs.CL

TL;DR: 本文提出了一种针对LLaMA架构的混合精度量化方法，通过在高激活层使用高精度（FP16或FP8），其余部分使用低比特量化，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）的规模给部署和推理带来挑战，现有量化方法未充分考虑LLaMA架构的激活异常值分布。

Method: 提出混合精度量化方法，针对LLaMA架构中激活峰值集中的特定投影层使用高精度，其余部分低比特量化。

Result: 在LLaMA2、LLaMA3和Mistral模型上，8比特量化表现优于通用方法，困惑度和零样本准确率显著提升。

Conclusion: 架构特定的量化策略更有效，研究为LLM的高效部署提供了新思路，尤其适用于资源受限环境。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
various natural language processing tasks. However, their size presents
significant challenges for deployment and inference. This paper investigates
the quantization of LLMs, focusing on the LLaMA architecture and its
derivatives. We challenge existing assumptions about activation outliers in
LLMs and propose a novel mixed-precision quantization approach tailored for
LLaMA-like models. Our method leverages the observation that activation spikes
in LLaMA architectures are predominantly concentrated in specific projection
layers. By applying higher precision (FP16 or FP8) to these layers while
quantizing the rest of the model to lower bit-widths, we achieve superior
performance compared to existing quantization techniques. Experimental results
on LLaMA2, LLaMA3, and Mistral models demonstrate significant improvements in
perplexity and zero-shot accuracy, particularly for 8-bit per-tensor
quantization. Our approach outperforms general-purpose methods designed to
handle outliers across all architecture types, highlighting the benefits of
architecture-specific quantization strategies. This research contributes to the
ongoing efforts to make LLMs more efficient and deployable, potentially
enabling their use in resource-constrained environments. Our findings emphasize
the importance of considering model-specific characteristics in developing
effective quantization pipelines for state-of-the-art language models by
identifying and targeting a small number of projections that concentrate
activation spikes.

</details>


### [34] [DNB-AI-Project at SemEval-2025 Task 5: An LLM-Ensemble Approach for Automated Subject Indexing](https://arxiv.org/abs/2504.21589)
*Lisa Kluge,Maximilian Kähler*

Main category: cs.CL

TL;DR: 论文介绍了一种基于LLMs的自动化主题标注系统，用于国家技术图书馆的开放获取目录，结合少样本提示和后处理步骤，在定量排名中位列第四，但在专家定性评估中表现最佳。


<details>
  <summary>Details</summary>
Motivation: 开发一个自动化系统，利用LLMs为技术图书馆的开放获取目录提供高效且准确的主题标注。

Method: 采用少样本提示技术，结合后处理步骤（关键词映射、集成投票和相关性排序）来优化LLMs生成的主题标签。

Result: 系统在定量排名中位列第四，但在专家定性评估中表现最佳。

Conclusion: 该系统展示了LLMs在主题标注任务中的潜力，尤其是在结合后处理技术时，能够显著提升标注质量。

Abstract: This paper presents our system developed for the SemEval-2025 Task 5:
LLMs4Subjects: LLM-based Automated Subject Tagging for a National Technical
Library's Open-Access Catalog. Our system relies on prompting a selection of
LLMs with varying examples of intellectually annotated records and asking the
LLMs to similarly suggest keywords for new records. This few-shot prompting
technique is combined with a series of post-processing steps that map the
generated keywords to the target vocabulary, aggregate the resulting subject
terms to an ensemble vote and, finally, rank them as to their relevance to the
record. Our system is fourth in the quantitative ranking in the all-subjects
track, but achieves the best result in the qualitative ranking conducted by
subject indexing experts.

</details>


### [35] [Robust Misinformation Detection by Visiting Potential Commonsense Conflict](https://arxiv.org/abs/2504.21604)
*Bing Wang,Ximing Li,Changchun Li,Bingrui Zhao,Bo Fu,Renchu Guan,Shengsheng Wang*

Main category: cs.CL

TL;DR: 提出了一种基于常识冲突的虚假信息检测增强方法MD-PCC，通过构建常识表达增强文章数据，并在多个数据集上验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 互联网技术的发展导致虚假信息泛滥，自动检测虚假信息成为重要研究方向。

Method: 利用常识冲突作为虚假信息的特征，构建常识表达增强文章数据，结合现有MD方法进行训练。

Result: MD-PCC在多个公开基准数据集和自建数据集CoMis上均优于现有基线方法。

Conclusion: MD-PCC通过常识冲突增强虚假信息检测，具有普适性和有效性。

Abstract: The development of Internet technology has led to an increased prevalence of
misinformation, causing severe negative effects across diverse domains. To
mitigate this challenge, Misinformation Detection (MD), aiming to detect online
misinformation automatically, emerges as a rapidly growing research topic in
the community. In this paper, we propose a novel plug-and-play augmentation
method for the MD task, namely Misinformation Detection with Potential
Commonsense Conflict (MD-PCC). We take inspiration from the prior studies
indicating that fake articles are more likely to involve commonsense conflict.
Accordingly, we construct commonsense expressions for articles, serving to
express potential commonsense conflicts inferred by the difference between
extracted commonsense triplet and golden ones inferred by the well-established
commonsense reasoning tool COMET. These expressions are then specified for each
article as augmentation. Any specific MD methods can be then trained on those
commonsense-augmented articles. Besides, we also collect a novel
commonsense-oriented dataset named CoMis, whose all fake articles are caused by
commonsense conflict. We integrate MD-PCC with various existing MD backbones
and compare them across both 4 public benchmark datasets and CoMis. Empirical
results demonstrate that MD-PCC can consistently outperform the existing MD
baselines.

</details>


### [36] [RDF-Based Structured Quality Assessment Representation of Multilingual LLM Evaluations](https://arxiv.org/abs/2504.21605)
*Jonas Gwozdz,Andreas Both*

Main category: cs.CL

TL;DR: 提出了一种基于RDF的框架，用于评估多语言大语言模型（LLM）在知识冲突情况下的可靠性。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏系统评估LLM在信息冲突时可靠性的方法。

Method: 通过四种上下文条件（完整、不完整、冲突和无上下文）捕捉模型在德语和英语中的响应，分析知识泄漏、错误检测和多语言一致性。

Result: 在消防安全领域的实验中，框架成功揭示了上下文优先级和语言特定性能的关键模式。

Conclusion: 提出的词汇表足以表达28个问题研究中的所有评估方面，验证了框架的有效性。

Abstract: Large Language Models (LLMs) increasingly serve as knowledge interfaces, yet
systematically assessing their reliability with conflicting information remains
difficult. We propose an RDF-based framework to assess multilingual LLM
quality, focusing on knowledge conflicts. Our approach captures model responses
across four distinct context conditions (complete, incomplete, conflicting, and
no-context information) in German and English. This structured representation
enables the comprehensive analysis of knowledge leakage-where models favor
training data over provided context-error detection, and multilingual
consistency. We demonstrate the framework through a fire safety domain
experiment, revealing critical patterns in context prioritization and
language-specific performance, and demonstrating that our vocabulary was
sufficient to express every assessment facet encountered in the 28-question
study.

</details>


### [37] [Meeseeks: An Iterative Benchmark Evaluating LLMs Multi-Turn Instruction-Following Ability](https://arxiv.org/abs/2504.21625)
*Jiaming Wang*

Main category: cs.CL

TL;DR: Meeseeks是一个新的基准测试，通过迭代反馈过程模拟真实的人-LLM交互，使模型能够自我纠正，并全面评估LLM的指令跟随能力。


<details>
  <summary>Details</summary>
Motivation: 现有指令跟随基准测试多为单轮或不允许自我纠正，无法反映真实应用场景。Meeseeks旨在填补这一空白。

Method: 采用迭代反馈过程，允许模型自我纠正，并通过38个能力标签在三个维度（意图识别、内容验证、输出结构验证）进行评估。

Result: Meeseeks为LLM在真实应用中的指令跟随能力提供了有价值的见解。

Conclusion: Meeseeks通过更真实的交互设计和全面评估，提升了LLM指令跟随能力的评测标准。

Abstract: The ability to follow instructions accurately is fundamental for Large
Language Models (LLMs) to serve as reliable agents in real-world applications.
While existing instruction-following benchmarks are either single-turn or
introduce new requirements in each turn without allowing self-correction,
Meeseeks simulates realistic human-LLM interactions through an iterative
feedback process. This design enables models to self-correct based on specific
requirement failures, better reflecting real-world user-end usage patterns. The
benchmark implements a comprehensive evaluation system with 38 capability tags
organized across three dimensions: Intent Recognition, Granular Content
Validation, and Output Structure Validation. Through rigorous evaluation across
LLMs, Meeseeks provides valuable insights into LLMs' instruction-following
capabilities in practical applications.

</details>


### [38] [Sadeed: Advancing Arabic Diacritization Through Small Language Model](https://arxiv.org/abs/2504.21635)
*Zeina Aldallal,Sara Chrouf,Khalil Hennara,Mohamed Motaism Hamed,Muhammad Hreden,Safwan AlModhayan*

Main category: cs.CL

TL;DR: Sadeed是一种基于Kuwain 1.5B微调的阿拉伯语文本标音方法，性能优于传统模型，并提出了新的评测基准SadeedDiac-25。


<details>
  <summary>Details</summary>
Motivation: 解决阿拉伯语文本标音在自然语言处理中的挑战，提升标音模型的性能和评测公平性。

Method: 基于Kuwain 1.5B微调的Sadeed模型，使用高质量标音数据集训练，并设计了新的评测基准SadeedDiac-25。

Result: Sadeed在有限计算资源下表现优于传统模型，与大型专有模型竞争。

Conclusion: Sadeed和SadeedDiac-25为阿拉伯语NLP应用提供了坚实基础。

Abstract: Arabic text diacritization remains a persistent challenge in natural language
processing due to the language's morphological richness. In this paper, we
introduce Sadeed, a novel approach based on a fine-tuned decoder-only language
model adapted from Kuwain 1.5B Hennara et al. [2025], a compact model
originally trained on diverse Arabic corpora. Sadeed is fine-tuned on carefully
curated, high-quality diacritized datasets, constructed through a rigorous
data-cleaning and normalization pipeline. Despite utilizing modest
computational resources, Sadeed achieves competitive results compared to
proprietary large language models and outperforms traditional models trained on
similar domains. Additionally, we highlight key limitations in current
benchmarking practices for Arabic diacritization. To address these issues, we
introduce SadeedDiac-25, a new benchmark designed to enable fairer and more
comprehensive evaluation across diverse text genres and complexity levels.
Together, Sadeed and SadeedDiac-25 provide a robust foundation for advancing
Arabic NLP applications, including machine translation, text-to-speech, and
language learning tools.

</details>


### [39] [20min-XD: A Comparable Corpus of Swiss News Articles](https://arxiv.org/abs/2504.21677)
*Michelle Wastl,Jannis Vamvas,Selena Calleri,Rico Sennrich*

Main category: cs.CL

TL;DR: 20min-XD是一个法语-德语新闻文章可比语料库，包含约15,000篇文章对，覆盖2015至2024年，基于语义相似性自动对齐。


<details>
  <summary>Details</summary>
Motivation: 构建一个跨语言、文档级别的新闻语料库，支持多种NLP应用和语言学研究的需要。

Method: 从瑞士在线新闻平台20 Minuten/20 minutes收集数据，通过语义相似性自动对齐文章对。

Result: 语料库展示了从近似翻译到松散相关文章的广泛跨语言相似性。

Conclusion: 该数据集对NLP应用和语言学研究具有重要价值，已公开发布文档和句子对齐版本及相关代码。

Abstract: We present 20min-XD (20 Minuten cross-lingual document-level), a
French-German, document-level comparable corpus of news articles, sourced from
the Swiss online news outlet 20 Minuten/20 minutes. Our dataset comprises
around 15,000 article pairs spanning 2015 to 2024, automatically aligned based
on semantic similarity. We detail the data collection process and alignment
methodology. Furthermore, we provide a qualitative and quantitative analysis of
the corpus. The resulting dataset exhibits a broad spectrum of cross-lingual
similarity, ranging from near-translations to loosely related articles, making
it valuable for various NLP applications and broad linguistically motivated
studies. We publicly release the dataset in document- and sentence-aligned
versions and code for the described experiments.

</details>


### [40] [Investigating the Effect of Parallel Data in the Cross-Lingual Transfer for Vision-Language Encoders](https://arxiv.org/abs/2504.21681)
*Andrei-Alexandru Manea,Jindřich Libovický*

Main category: cs.CL

TL;DR: 研究了通过平行数据转移预训练视觉语言模型的方法，发现机器翻译的任务数据效果最佳，但某些语言中真实平行数据表现更好，且多语言训练对多数语言有益。


<details>
  <summary>Details</summary>
Motivation: 现有预训练视觉语言模型和下游任务数据多为英文，多语言任务需依赖跨语言迁移，本文探索了通过平行数据转移已训练编码器的替代方法。

Method: 研究了平行数据的领域和语言数量对迁移效果的影响，比较了机器翻译任务数据和真实平行数据的表现。

Result: 机器翻译的任务数据平均效果最佳，但某些语言中真实平行数据表现更好；多语言训练对多数语言有益。

Conclusion: 平行数据的领域和语言数量对迁移效果有显著影响，多语言训练是提升多语言视觉语言任务的有效策略。

Abstract: Most pre-trained Vision-Language (VL) models and training data for the
downstream tasks are only available in English. Therefore, multilingual VL
tasks are solved using cross-lingual transfer: fine-tune a multilingual
pre-trained model or transfer the text encoder using parallel data. We study
the alternative approach: transferring an already trained encoder using
parallel data. We investigate the effect of parallel data: domain and the
number of languages, which were out of focus in previous work. Our results show
that even machine-translated task data are the best on average, caption-like
authentic parallel data outperformed it in some languages. Further, we show
that most languages benefit from multilingual training.

</details>


### [41] [Enhancing Health Mention Classification Performance: A Study on Advancements in Parameter Efficient Tuning](https://arxiv.org/abs/2504.21685)
*Reem Abdel-Salam,Mary Adewunmi*

Main category: cs.CL

TL;DR: 论文提出了一种通过改进生物医学NLP方法的参数来优化健康提及分类（HMC）的方法，结合POS标记信息和PEFT技术，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 解决HMC中因上下文复杂性（如比喻语言和描述性术语）导致的分类困难问题。

Method: 采用POS标记信息和PEFT技术，结合多种方法进行实验。

Result: 在RHDM、PHM和Illness数据集上，F1分数显著优于现有方法，同时模型更小、训练更高效。

Conclusion: 该方法能有效分类社交媒体中的健康提及，并优化模型大小和训练效率。

Abstract: Health Mention Classification (HMC) plays a critical role in leveraging
social media posts for real-time tracking and public health monitoring.
Nevertheless, the process of HMC presents significant challenges due to its
intricate nature, primarily stemming from the contextual aspects of health
mentions, such as figurative language and descriptive terminology, rather than
explicitly reflecting a personal ailment. To address this problem, we argue
that clearer mentions can be achieved through conventional fine-tuning with
enhanced parameters of biomedical natural language methods (NLP). In this
study, we explore different techniques such as the utilisation of
part-of-speech (POS) tagger information, improving on PEFT techniques, and
different combinations thereof. Extensive experiments are conducted on three
widely used datasets: RHDM, PHM, and Illness. The results incorporated POS
tagger information, and leveraging PEFT techniques significantly improves
performance in terms of F1-score compared to state-of-the-art methods across
all three datasets by utilising smaller models and efficient training.
Furthermore, the findings highlight the effectiveness of incorporating POS
tagger information and leveraging PEFT techniques for HMC. In conclusion, the
proposed methodology presents a potentially effective approach to accurately
classifying health mentions in social media posts while optimising the model
size and training efficiency.

</details>


### [42] [Investigating Literary Motifs in Ancient and Medieval Novels with Large Language Models](https://arxiv.org/abs/2504.21742)
*Emelie Hallenberg*

Main category: cs.CL

TL;DR: 该研究使用精细调整的大语言模型分析希腊爱情小说中的文学母题，发现部分母题贯穿始终，而其他母题的频率波动反映了趋势或外部影响。


<details>
  <summary>Details</summary>
Motivation: 探讨希腊爱情小说中文学母题的共性与差异，以揭示其潜在趋势或外部影响。

Method: 应用精细调整的大语言模型提取并分析文学母题。

Result: 部分母题持续存在，其他母题频率波动，表明趋势或外部影响。

Conclusion: 该方法能有效提取文学母题，为定量和定性分析提供数据。

Abstract: The Greek fictional narratives often termed love novels or romances, ranging
from the first century CE to the middle of the 15th century, have long been
considered as similar in many ways, not least in the use of particular literary
motifs. By applying the use of fine-tuned large language models, this study
aims to investigate which motifs exactly that the texts in this corpus have in
common, and in which ways they differ from each other. The results show that
while some motifs persist throughout the corpus, others fluctuate in frequency,
indicating certain trends or external influences. Conclusively, the method
proves to adequately extract literary motifs according to a set definition,
providing data for both quantitative and qualitative analyses.

</details>


### [43] [Improving Retrieval-Augmented Neural Machine Translation with Monolingual Data](https://arxiv.org/abs/2504.21747)
*Maxime Bouthors,Josep Crego,François Yvon*

Main category: cs.CL

TL;DR: 本文探讨了如何利用目标语言单语语料库改进检索增强神经机器翻译（RANMT）系统，通过设计改进的跨语言检索系统，结合句子级和词级匹配目标，显著提升了翻译性能。


<details>
  <summary>Details</summary>
Motivation: 传统RANMT系统依赖双语语料库（如翻译记忆库），但在许多场景下，目标语言单语语料库更为丰富。本文旨在利用这些资源，直接基于源语言查询检索目标语言相关片段。

Method: 设计了改进的跨语言检索系统，结合句子级和词级匹配目标进行训练，并在两种RANMT架构上进行了实验验证。

Result: 实验表明，该方法在控制环境下优于传统基于翻译记忆库的模型，并在实际场景中（目标单语资源远超平行数据量）显著提升翻译性能，超越基线设置和通用跨语言检索器。

Conclusion: 通过利用目标语言单语资源并结合跨语言检索优化，能够显著提升RANMT系统的翻译性能，尤其在资源丰富的实际场景中效果显著。

Abstract: Conventional retrieval-augmented neural machine translation (RANMT) systems
leverage bilingual corpora, e.g., translation memories (TMs). Yet, in many
settings, in-domain monolingual target-side corpora are often available. This
work explores ways to take advantage of such resources by retrieving relevant
segments directly in the target language, based on a source-side query. For
this, we design improved cross-lingual retrieval systems, trained with both
sentence level and word-level matching objectives. In our experiments with two
RANMT architectures, we first demonstrate the benefits of such cross-lingual
objectives in a controlled setting, obtaining translation performances that
surpass standard TM-based models. We then showcase our method on a real-world
set-up, where the target monolingual resources far exceed the amount of
parallel data and observe large improvements of our new techniques, which
outperform both the baseline setting, and general-purpose cross-lingual
retrievers.

</details>


### [44] [MAC-Tuning: LLM Multi-Compositional Problem Reasoning with Enhanced Knowledge Boundary Awareness](https://arxiv.org/abs/2504.21773)
*Junsheng Huang,Zhitao He,Sandeep Polisetty,Qingyun Wang,May Fung*

Main category: cs.CL

TL;DR: 论文提出了一种新方法MAC-Tuning，用于在多问题设置下提升大语言模型（LLM）对自身知识边界的感知能力，显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在多问题设置下生成虚假事实（幻觉）的问题，现有研究主要关注单一问题设置，多问题设置下的知识边界感知尚未充分探索。

Method: 提出MAC-Tuning方法，在指令数据微调过程中分离答案预测和置信度估计的学习。

Result: 实验表明，MAC-Tuning在平均精度上比基线方法提升高达25%。

Conclusion: MAC-Tuning在多问题设置下有效提升了LLM的置信度估计能力，填补了研究空白。

Abstract: With the widespread application of large language models (LLMs), the issue of
generating non-existing facts, known as hallucination, has garnered increasing
attention. Previous research in enhancing LLM confidence estimation mainly
focuses on the single problem setting. However, LLM awareness of its internal
parameterized knowledge boundary under the more challenging multi-problem
setting, which requires answering multiple problems accurately simultaneously,
remains underexplored. To bridge this gap, we introduce a novel method,
Multiple Answers and Confidence Stepwise Tuning (MAC-Tuning), that separates
the learning of answer prediction and confidence estimation during fine-tuning
on instruction data. Extensive experiments demonstrate that our method
outperforms baselines by up to 25% in average precision.

</details>


### [45] [WebThinker: Empowering Large Reasoning Models with Deep Research Capability](https://arxiv.org/abs/2504.21776)
*Xiaoxi Li,Jiajie Jin,Guanting Dong,Hongjin Qian,Yutao Zhu,Yongkang Wu,Ji-Rong Wen,Zhicheng Dou*

Main category: cs.CL

TL;DR: WebThinker是一个深度研究代理，通过动态搜索网络和实时整合信息，提升大型推理模型在复杂任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有大型推理模型依赖静态知识，难以处理复杂、知识密集型任务，限制了其在综合研究报告生成中的能力。

Method: 提出WebThinker，结合深度网络探索模块和自主思考-搜索-草拟策略，并通过RL训练优化。

Result: 在多个复杂推理基准和科学报告生成任务中，WebThinker显著优于现有方法和专有系统。

Conclusion: WebThinker提高了大型推理模型在复杂场景中的可靠性和适用性，为更强大的深度研究系统铺平了道路。

Abstract: Large reasoning models (LRMs), such as OpenAI-o1 and DeepSeek-R1, demonstrate
impressive long-horizon reasoning capabilities. However, their reliance on
static internal knowledge limits their performance on complex,
knowledge-intensive tasks and hinders their ability to produce comprehensive
research reports requiring synthesis of diverse web information. To address
this, we propose \textbf{WebThinker}, a deep research agent that empowers LRMs
to autonomously search the web, navigate web pages, and draft research reports
during the reasoning process. WebThinker integrates a \textbf{Deep Web
Explorer} module, enabling LRMs to dynamically search, navigate, and extract
information from the web when encountering knowledge gaps. It also employs an
\textbf{Autonomous Think-Search-and-Draft strategy}, allowing the model to
seamlessly interleave reasoning, information gathering, and report writing in
real time. To further enhance research tool utilization, we introduce an
\textbf{RL-based training strategy} via iterative online Direct Preference
Optimization (DPO). Extensive experiments on complex reasoning benchmarks
(GPQA, GAIA, WebWalkerQA, HLE) and scientific report generation tasks (Glaive)
demonstrate that WebThinker significantly outperforms existing methods and
strong proprietary systems. Our approach enhances LRM reliability and
applicability in complex scenarios, paving the way for more capable and
versatile deep research systems. The code is available at
https://github.com/RUC-NLPIR/WebThinker.

</details>


### [46] [How Real Are Synthetic Therapy Conversations? Evaluating Fidelity in Prolonged Exposure Dialogues](https://arxiv.org/abs/2504.21800)
*Suhas BN,Dominik Mattioli,Saeed Abdullah,Rosa I. Arriaga,Chris W. Wiese,Andrew M. Sherrill*

Main category: cs.CL

TL;DR: 论文探讨了合成数据在PTSD治疗对话中的应用，发现其虽能模拟结构特征，但难以捕捉治疗互动的细微动态，需改进评估框架。


<details>
  <summary>Details</summary>
Motivation: 隐私问题、真实数据获取困难和标注成本高推动了合成数据在医疗领域的应用。

Method: 通过语言、结构和协议特定指标（如对话模式和治疗保真度）系统比较真实与合成对话，并引入PE特定指标。

Result: 合成数据能模拟结构特征（如说话者切换比例），但无法反映关键保真度标记（如痛苦监测）。

Conclusion: 合成数据在补充真实数据方面有潜力，但需开发更注重临床保真度的评估指标。

Abstract: The growing adoption of synthetic data in healthcare is driven by privacy
concerns, limited access to real-world data, and the high cost of annotation.
This work explores the use of synthetic Prolonged Exposure (PE) therapeutic
conversations for Post-Traumatic Stress Disorder (PTSD) as a scalable
alternative for training and evaluating clinical models. We systematically
compare real and synthetic dialogues using linguistic, structural, and
protocol-specific metrics, including turn-taking patterns and treatment
fidelity. We also introduce and evaluate PE-specific metrics derived from
linguistic analysis and semantic modeling, offering a novel framework for
assessing clinical fidelity beyond surface fluency. Our findings show that
although synthetic data holds promise for mitigating data scarcity and
protecting patient privacy, it can struggle to capture the subtle dynamics of
therapeutic interactions. In our dataset, synthetic dialogues match structural
features of real-world dialogues (e.g., speaker switch ratio: 0.98 vs. 0.99),
however, synthetic interactions do not adequately reflect key fidelity markers
(e.g., distress monitoring). We highlight gaps in existing evaluation
frameworks and advocate for fidelity-aware metrics that go beyond surface
fluency to uncover clinically significant failures. Our findings clarify where
synthetic data can effectively complement real-world datasets -- and where
critical limitations remain.

</details>


### [47] [DeepSeek-Prover-V2: Advancing Formal Mathematical Reasoning via Reinforcement Learning for Subgoal Decomposition](https://arxiv.org/abs/2504.21801)
*Z. Z. Ren,Zhihong Shao,Junxiao Song,Huajian Xin,Haocheng Wang,Wanjia Zhao,Liyue Zhang,Zhe Fu,Qihao Zhu,Dejian Yang,Z. F. Wu,Zhibin Gou,Shirong Ma,Hongxuan Tang,Yuxuan Liu,Wenjun Gao,Daya Guo,Chong Ruan*

Main category: cs.CL

TL;DR: DeepSeek-Prover-V2是一个开源大型语言模型，专为Lean 4中的形式化定理证明设计，通过递归定理证明流程初始化数据，结合非正式和正式数学推理，实现了在神经定理证明中的最先进性能。


<details>
  <summary>Details</summary>
Motivation: 旨在通过结合非正式和正式数学推理，提升大型语言模型在形式化定理证明中的能力。

Method: 使用DeepSeek-V3分解复杂问题为子目标，通过链式思维过程合成证明，结合强化学习进行冷启动训练。

Result: 模型在MiniF2F-test中达到88.9%通过率，解决PutnamBench中49/658问题，并在ProverBench中解决6/15 AIME问题。

Conclusion: DeepSeek-Prover-V2显著缩小了形式化和非正式数学推理之间的差距，展示了其在定理证明中的强大能力。

Abstract: We introduce DeepSeek-Prover-V2, an open-source large language model designed
for formal theorem proving in Lean 4, with initialization data collected
through a recursive theorem proving pipeline powered by DeepSeek-V3. The
cold-start training procedure begins by prompting DeepSeek-V3 to decompose
complex problems into a series of subgoals. The proofs of resolved subgoals are
synthesized into a chain-of-thought process, combined with DeepSeek-V3's
step-by-step reasoning, to create an initial cold start for reinforcement
learning. This process enables us to integrate both informal and formal
mathematical reasoning into a unified model. The resulting model,
DeepSeek-Prover-V2-671B, achieves state-of-the-art performance in neural
theorem proving, reaching 88.9% pass ratio on the MiniF2F-test and solving 49
out of 658 problems from PutnamBench. In addition to standard benchmarks, we
introduce ProverBench, a collection of 325 formalized problems, to enrich our
evaluation, including 15 selected problems from the recent AIME competitions
(years 24-25). Further evaluation on these 15 AIME problems shows that the
model successfully solves 6 of them. In comparison, DeepSeek-V3 solves 8 of
these problems using majority voting, highlighting that the gap between formal
and informal mathematical reasoning in large language models is substantially
narrowing.

</details>


### [48] [TRUST: An LLM-Based Dialogue System for Trauma Understanding and Structured Assessments](https://arxiv.org/abs/2504.21851)
*Sichang Tu,Abigail Powers,Stephen Doogan,Jinho D. Choi*

Main category: cs.CL

TL;DR: 该研究开发了一个基于LLM的对话系统TRUST，用于标准化诊断访谈和评估，特别是针对创伤后应激障碍（PTSD），并通过专家评估验证其效果。


<details>
  <summary>Details</summary>
Motivation: 填补心理健康领域标准化诊断访谈的空白，提升心理健康服务的可及性。

Method: 提出TRUST框架，结合LLM模块和专门设计的Dialogue Acts模式，并通过患者模拟方法进行测试。

Result: 专家评估显示TRUST表现与真实临床访谈相当，达到普通临床医生水平。

Conclusion: TRUST框架展现了提升心理健康服务可用性的潜力，未来可进一步优化。

Abstract: Objectives: While Large Language Models (LLMs) have been widely used to
assist clinicians and support patients, no existing work has explored dialogue
systems for standard diagnostic interviews and assessments. This study aims to
bridge the gap in mental healthcare accessibility by developing an LLM-powered
dialogue system that replicates clinician behavior. Materials and Methods: We
introduce TRUST, a framework of cooperative LLM modules capable of conducting
formal diagnostic interviews and assessments for Post-Traumatic Stress Disorder
(PTSD). To guide the generation of appropriate clinical responses, we propose a
Dialogue Acts schema specifically designed for clinical interviews.
Additionally, we develop a patient simulation approach based on real-life
interview transcripts to replace time-consuming and costly manual testing by
clinicians. Results: A comprehensive set of evaluation metrics is designed to
assess the dialogue system from both the agent and patient simulation
perspectives. Expert evaluations by conversation and clinical specialists show
that TRUST performs comparably to real-life clinical interviews. Discussion:
Our system performs at the level of average clinicians, with room for future
enhancements in communication styles and response appropriateness. Conclusions:
Our TRUST framework shows its potential to facilitate mental healthcare
availability.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [49] [A Formalism for Optimal Search with Dynamic Heuristics](https://arxiv.org/abs/2504.21131)
*Remo Christen,Florian Pommerening,Clemens Büchner,Malte Helmert*

Main category: cs.AI

TL;DR: 论文研究了动态启发式在搜索算法中的应用，提出了一个通用框架，并证明了其最优性。


<details>
  <summary>Details</summary>
Motivation: 现有研究在动态启发式搜索中忽略了启发式可变性带来的复杂性，需要更系统的理论支持。

Method: 提出一个通用算法框架，研究动态启发式在A*算法中的具体实例化。

Result: 证明了动态启发式搜索的最优性，并将经典规划方法纳入该框架。

Conclusion: 动态启发式的通用框架为现有方法提供了理论支持，并扩展了其应用范围。

Abstract: While most heuristics studied in heuristic search depend only on the state,
some accumulate information during search and thus also depend on the search
history. Various existing approaches use such dynamic heuristics in
$\mathrm{A}^*$-like algorithms and appeal to classic results for $\mathrm{A}^*$
to show optimality. However, doing so ignores the complexities of searching
with a mutable heuristic. In this paper we formalize the idea of dynamic
heuristics and use them in a generic algorithm framework. We study a particular
instantiation that models $\mathrm{A}^*$ with dynamic heuristics and show
general optimality results. Finally we show how existing approaches from
classical planning can be viewed as special cases of this instantiation, making
it possible to directly apply our optimality results.

</details>


### [50] [AffectEval: A Modular and Customizable Framework for Affective Computing](https://arxiv.org/abs/2504.21184)
*Emily Zhou,Khushboo Khatri,Yixue Zhao,Bhaskar Krishnamachari*

Main category: cs.AI

TL;DR: AffectEval是一个模块化且可定制的框架，旨在简化情感计算管道的开发，减少手动工作和重复劳动。


<details>
  <summary>Details</summary>
Motivation: 当前情感计算领域缺乏支持多模态、多领域情感识别应用的软件框架，导致开发管道时存在冗余工作。

Method: 提出AffectEval框架，通过模块化和可定制化设计减少开发工作量，并通过复现先前实验验证其有效性。

Result: AffectEval能将编程工作量减少高达90%（以代码行数衡量）。

Conclusion: AffectEval显著提升了情感计算管道的开发效率，减少了重复劳动。

Abstract: The field of affective computing focuses on recognizing, interpreting, and
responding to human emotions, and has broad applications across education,
child development, and human health and wellness. However, developing affective
computing pipelines remains labor-intensive due to the lack of software
frameworks that support multimodal, multi-domain emotion recognition
applications. This often results in redundant effort when building pipelines
for different applications. While recent frameworks attempt to address these
challenges, they remain limited in reducing manual effort and ensuring
cross-domain generalizability. We introduce AffectEval, a modular and
customizable framework to facilitate the development of affective computing
pipelines while reducing the manual effort and duplicate work involved in
developing such pipelines. We validate AffectEval by replicating prior
affective computing experiments, and we demonstrate that our framework reduces
programming effort by up to 90%, as measured by the reduction in raw lines of
code.

</details>


### [51] [Theoretical Foundations for Semantic Cognition in Artificial Intelligence](https://arxiv.org/abs/2504.21218)
*Sebastian Dumbrava*

Main category: cs.AI

TL;DR: 该论文提出了一种基于结构化语义状态的模块化认知架构，定义了信念状态为动态语言表达集合，并开发了一个分层框架，支持自调节的认知代理。


<details>
  <summary>Details</summary>
Motivation: 结合哲学、认知科学和神经科学的理论，构建一个能够实现反思性、目标导向思维的认知架构。

Method: 通过定义信念状态和引入语义惰性的认知状态（epistemic vacuum），构建生成性结构（Null Tower），并设计可实现的符号和神经网络系统。

Result: 提出了一个理论基础，支持构建能够推理、记忆和调节信念的智能代理。

Conclusion: 该工作为构建结构化、可解释的认知代理提供了基础框架。

Abstract: This monograph presents a modular cognitive architecture for artificial
intelligence grounded in the formal modeling of belief as structured semantic
state. Belief states are defined as dynamic ensembles of linguistic expressions
embedded within a navigable manifold, where operators enable assimilation,
abstraction, nullification, memory, and introspection. Drawing from philosophy,
cognitive science, and neuroscience, we develop a layered framework that
enables self-regulating epistemic agents capable of reflective, goal-directed
thought. At the core of this framework is the epistemic vacuum: a class of
semantically inert cognitive states that serves as the conceptual origin of
belief space. From this foundation, the Null Tower arises as a generative
structure recursively built through internal representational capacities. The
theoretical constructs are designed to be implementable in both symbolic and
neural systems, including large language models, hybrid agents, and adaptive
memory architectures. This work offers a foundational substrate for
constructing agents that reason, remember, and regulate their beliefs in
structured, interpretable ways.

</details>


### [52] [Reinforced MLLM: A Survey on RL-Based Reasoning in Multimodal Large Language Models](https://arxiv.org/abs/2504.21277)
*Guanghao Zhou,Panjia Qiu,Cen Chen,Jie Wang,Zheming Yang,Jian Xu,Minghui Qiu*

Main category: cs.AI

TL;DR: 本文综述了强化学习（RL）在多模态大语言模型（MLLMs）推理中的应用，涵盖算法设计、奖励机制创新及实际应用，并分析了价值无关和价值相关方法。


<details>
  <summary>Details</summary>
Motivation: 尽管MLLMs扩展了LLMs以处理多模态输入，但其推理能力仍面临挑战，RL被引入以优化推理轨迹和对齐多模态信息。

Method: 系统回顾了RL在MLLMs中的最新进展，包括两种主要RL范式（价值无关和价值相关方法），并探讨了其如何增强推理能力。

Result: 总结了基准数据集、评估协议及现有局限性，如稀疏奖励和跨模态推理效率低。

Conclusion: 提出了未来研究方向以解决当前瓶颈，旨在为研究者提供全面指导。

Abstract: The integration of reinforcement learning (RL) into the reasoning
capabilities of Multimodal Large Language Models (MLLMs) has rapidly emerged as
a transformative research direction. While MLLMs significantly extend Large
Language Models (LLMs) to handle diverse modalities such as vision, audio, and
video, enabling robust reasoning across multimodal inputs remains a major
challenge. This survey systematically reviews recent advances in RL-based
reasoning for MLLMs, covering key algorithmic designs, reward mechanism
innovations, and practical applications. We highlight two main RL
paradigms--value-free and value-based methods--and analyze how RL enhances
reasoning abilities by optimizing reasoning trajectories and aligning
multimodal information. Furthermore, we provide an extensive overview of
benchmark datasets, evaluation protocols, and existing limitations, and propose
future research directions to address current bottlenecks such as sparse
rewards, inefficient cross-modal reasoning, and real-world deployment
constraints. Our goal is to offer a comprehensive and structured guide to
researchers interested in advancing RL-based reasoning in the multimodal era.

</details>


### [53] [Phi-4-reasoning Technical Report](https://arxiv.org/abs/2504.21318)
*Marah Abdin,Sahaj Agarwal,Ahmed Awadallah,Vidhisha Balachandran,Harkirat Behl,Lingjiao Chen,Gustavo de Rosa,Suriya Gunasekar,Mojan Javaheripi,Neel Joshi,Piero Kauffmann,Yash Lara,Caio César Teodoro Mendes,Arindam Mitra,Besmira Nushi,Dimitris Papailiopoulos,Olli Saarikivi,Shital Shah,Vaishnavi Shrivastava,Vibhav Vineet,Yue Wu,Safoora Yousefi,Guoqing Zheng*

Main category: cs.AI

TL;DR: Phi-4-reasoning是一个140亿参数的推理模型，通过监督微调和强化学习优化，在复杂推理任务中表现优异，甚至超越更大规模的模型。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过精心设计的数据和训练方法，提升语言模型在复杂推理任务中的表现，并探索强化学习对推理能力的进一步优化。

Method: 模型基于Phi-4进行监督微调，使用精心筛选的提示和推理演示数据，并通过强化学习生成更长的推理链以提升性能。

Result: Phi-4-reasoning及其强化学习变体在数学、科学推理、编程等任务中表现优异，超越了一些更大规模的开放权重模型。

Conclusion: 研究表明，数据筛选和监督微调对推理模型至关重要，强化学习能进一步提升性能，同时指出了评估推理模型性能的新方向。

Abstract: We introduce Phi-4-reasoning, a 14-billion parameter reasoning model that
achieves strong performance on complex reasoning tasks. Trained via supervised
fine-tuning of Phi-4 on carefully curated set of "teachable" prompts-selected
for the right level of complexity and diversity-and reasoning demonstrations
generated using o3-mini, Phi-4-reasoning generates detailed reasoning chains
that effectively leverage inference-time compute. We further develop
Phi-4-reasoning-plus, a variant enhanced through a short phase of outcome-based
reinforcement learning that offers higher performance by generating longer
reasoning traces. Across a wide range of reasoning tasks, both models
outperform significantly larger open-weight models such as
DeepSeek-R1-Distill-Llama-70B model and approach the performance levels of full
DeepSeek-R1 model. Our comprehensive evaluations span benchmarks in math and
scientific reasoning, coding, algorithmic problem solving, planning, and
spatial understanding. Interestingly, we observe a non-trivial transfer of
improvements to general-purpose benchmarks as well. In this report, we provide
insights into our training data, our training methodologies, and our
evaluations. We show that the benefit of careful data curation for supervised
fine-tuning (SFT) extends to reasoning language models, and can be further
amplified by reinforcement learning (RL). Finally, our evaluation points to
opportunities for improving how we assess the performance and robustness of
reasoning models.

</details>


### [54] [IRL Dittos: Embodied Multimodal AI Agent Interactions in Open Spaces](https://arxiv.org/abs/2504.21347)
*Seonghee Lee,Denae Ford,John Tang,Sasa Junuzovic,Asta Roseway,Ed Cutrell,Kori Inkpen*

Main category: cs.AI

TL;DR: IRL Ditto是一种AI驱动的实体代理，用于在共享办公空间中代表远程同事，促进实时互动。研究发现，其增强社交关系的能力取决于用户与代理来源的既有关系基础。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索如何通过IRL Ditto这种实体代理，改善分布式团队中同事之间的互动和关系。

Method: 通过为期四天的研究，评估IRL Ditto在模拟存在和促进不同社交熟悉度下的互动能力。

Result: 研究发现，IRL Ditto增强社交关系的效果与用户与其来源的既有关系密切相关。

Conclusion: 研究揭示了实体代理在丰富分布式团队工作环境中的潜力，强调了既有关系的重要性。

Abstract: We introduce the In Real Life (IRL) Ditto, an AI-driven embodied agent
designed to represent remote colleagues in shared office spaces, creating
opportunities for real-time exchanges even in their absence. IRL Ditto offers a
unique hybrid experience by allowing in-person colleagues to encounter a
digital version of their remote teammates, initiating greetings, updates, or
small talk as they might in person. Our research question examines: How can the
IRL Ditto influence interactions and relationships among colleagues in a shared
office space? Through a four-day study, we assessed IRL Ditto's ability to
strengthen social ties by simulating presence and enabling meaningful
interactions across different levels of social familiarity. We find that
enhancing social relationships depended deeply on the foundation of the
relationship participants had with the source of the IRL Ditto. This study
provides insights into the role of embodied agents in enriching workplace
dynamics for distributed teams.

</details>


### [55] [ShorterBetter: Guiding Reasoning Models to Find Optimal Inference Length for Efficient Reasoning](https://arxiv.org/abs/2504.21370)
*Jingyang Yi,Jiazheng Wang*

Main category: cs.AI

TL;DR: 论文提出了一种名为ShorterBetter的强化学习方法，通过动态调整推理链长度，显著减少输出长度同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: 现有推理模型在长推理链下容易‘过度思考’，导致推理效率低下。

Method: 使用强化学习方法，通过采样多个输出并定义最短正确响应为最优长度，动态引导模型。

Result: 在DeepSeek-Distill-Qwen-1.5B模型上，输出长度减少80%，同时保持准确性。

Conclusion: 长推理链常伴随推理方向迷失，表明推理链具有高度可压缩性。

Abstract: Reasoning models such as OpenAI o3 and DeepSeek-R1 have demonstrated strong
performance on reasoning-intensive tasks through extended Chain-of-Thought
(CoT) prompting. While longer reasoning traces can facilitate a more thorough
exploration of solution paths for complex problems, researchers have observed
that these models often "overthink", leading to inefficient inference. In this
paper, we introduce ShorterBetter, a simple yet effective reinforcement
learning methed that enables reasoning language models to discover their own
optimal CoT lengths without human intervention. By sampling multiple outputs
per problem and defining the Sample Optimal Length (SOL) as the shortest
correct response among all the outputs, our method dynamically guides the model
toward optimal inference lengths. Applied to the DeepSeek-Distill-Qwen-1.5B
model, ShorterBetter achieves up to an 80% reduction in output length on both
in-domain and out-of-domain reasoning tasks while maintaining accuracy. Our
analysis shows that overly long reasoning traces often reflect loss of
reasoning direction, and thus suggests that the extended CoT produced by
reasoning models is highly compressible.

</details>


### [56] [NGENT: Next-Generation AI Agents Must Integrate Multi-Domain Abilities to Achieve Artificial General Intelligence](https://arxiv.org/abs/2504.21433)
*Zhicong Li,Hangyu Mao,Jiangjin Yin,Mingzhe Xing,Zhiwei Xu,Yuanxing Zhang,Yang Xiao*

Main category: cs.AI

TL;DR: 论文主张下一代AI代理（NGENT）需整合跨领域能力以实现通用人工智能（AGI），提出统一框架整合文本、视觉、机器人等多领域技术。


<details>
  <summary>Details</summary>
Motivation: 当前AI代理虽在特定领域（如机器人、角色扮演）表现优异，但局限于狭窄领域，缺乏人类智能的通用性和适应性。

Method: 提出未来AI代理应整合各领域优势，构建统一框架，涵盖文本、视觉、机器人、强化学习、情感智能等。

Result: 跨领域技术融合和用户需求表明，这种整合是可行的，且对实现AGI至关重要。

Conclusion: 开发多功能代理是实现AGI的关键步骤，论文探讨了其理论基础和实现路径。

Abstract: This paper argues that the next generation of AI agent (NGENT) should
integrate across-domain abilities to advance toward Artificial General
Intelligence (AGI). Although current AI agents are effective in specialized
tasks such as robotics, role-playing, and tool-using, they remain confined to
narrow domains. We propose that future AI agents should synthesize the
strengths of these specialized systems into a unified framework capable of
operating across text, vision, robotics, reinforcement learning, emotional
intelligence, and beyond. This integration is not only feasible but also
essential for achieving the versatility and adaptability that characterize
human intelligence. The convergence of technologies across AI domains, coupled
with increasing user demand for cross-domain capabilities, suggests that such
integration is within reach. Ultimately, the development of these versatile
agents is a critical step toward realizing AGI. This paper explores the
rationale for this shift, potential pathways for achieving it.

</details>


### [57] [A Study on Group Decision Making Problem Based on Fuzzy Reasoning and Bayesian Networks](https://arxiv.org/abs/2504.21568)
*Shui-jin Rong,Wei Guo,Da-qing Zhang*

Main category: cs.AI

TL;DR: 提出了一种结合模糊推理和贝叶斯网络的群决策系统，用于解决多目标属性的决策问题，并在学生评价案例中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决多目标属性群决策问题中的定量挑战，如尺度差异和专家语言变量。

Method: 构建模糊规则库和分层贝叶斯网络，动态优化条件概率表以建模多维指标的非线性相关性。

Result: 分类准确率为86.0%，F1值比传统方法提高53.4%，验证了方法的有效性和鲁棒性。

Conclusion: 该方法在规则构建和排序一致性上表现优异，适用于多样化的群决策场景。

Abstract: Aiming at the group decision - making problem with multi - objective
attributes, this study proposes a group decision - making system that
integrates fuzzy inference and Bayesian network. A fuzzy rule base is
constructed by combining threshold values, membership functions, expert
experience, and domain knowledge to address quantitative challenges such as
scale differences and expert linguistic variables. A hierarchical Bayesian
network is designed, featuring a directed acyclic graph with nodes selected by
experts, and maximum likelihood estimation is used to dynamically optimize the
conditional probability table, modeling the nonlinear correlations among
multidimensional indices for posterior probability aggregation. In a
comprehensive student evaluation case, this method is compared with the
traditional weighted scoring approach. The results indicate that the proposed
method demonstrates effectiveness in both rule criterion construction and
ranking consistency, with a classification accuracy of 86.0% and an F1 value
improvement of 53.4% over the traditional method. Additionally, computational
experiments on real - world datasets across various group decision scenarios
assess the method's performance and robustness, providing evidence of its
reliability in diverse contexts.

</details>


### [58] [Designing Control Barrier Function via Probabilistic Enumeration for Safe Reinforcement Learning Navigation](https://arxiv.org/abs/2504.21643)
*Luca Marzari,Francesco Trotti,Enrico Marchesini,Alessandro Farinelli*

Main category: cs.AI

TL;DR: 论文提出了一种基于神经网络验证的分层控制框架，用于确保强化学习导航策略的安全性，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 在动态和不确定的真实环境中实现安全的自主导航系统是机器人部署的关键。

Method: 采用分层控制框架，结合神经网络验证技术设计控制屏障函数（CBFs）和政策修正机制，通过概率枚举识别不安全区域并构建安全的CBF控制层。

Result: 仿真和真实机器人实验表明，该方法能有效修正不安全行为并保持高效导航。

Conclusion: 分层验证系统在复杂场景中展现出实现安全、鲁棒导航行为的潜力。

Abstract: Achieving safe autonomous navigation systems is critical for deploying robots
in dynamic and uncertain real-world environments. In this paper, we propose a
hierarchical control framework leveraging neural network verification
techniques to design control barrier functions (CBFs) and policy correction
mechanisms that ensure safe reinforcement learning navigation policies. Our
approach relies on probabilistic enumeration to identify unsafe regions of
operation, which are then used to construct a safe CBF-based control layer
applicable to arbitrary policies. We validate our framework both in simulation
and on a real robot, using a standard mobile robot benchmark and a highly
dynamic aquatic environmental monitoring task. These experiments demonstrate
the ability of the proposed solution to correct unsafe actions while preserving
efficient navigation behavior. Our results show the promise of developing
hierarchical verification-based systems to enable safe and robust navigation
behaviors in complex scenarios.

</details>


### [59] [AdaR1: From Long-CoT to Hybrid-CoT via Bi-Level Adaptive Reasoning Optimization](https://arxiv.org/abs/2504.21659)
*Haotian Luo,Haiying He,Yibo Wang,Jinluan Yang,Rui Liu,Naiqiang Tan,Xiaochun Cao,Dacheng Tao,Li Shen*

Main category: cs.AI

TL;DR: 论文提出了一种自适应推理框架，通过结合长推理和短推理模型，并采用双层偏好训练，显著降低了推理成本，同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 现有长推理模型在复杂任务上表现优异，但推理开销大，且对不同问题的适应性不足，需要更高效的推理策略。

Method: 提出两阶段框架：1) 构建混合推理模型，结合长推理和短推理；2) 采用双层偏好训练，选择适合的推理风格并优化推理简洁性。

Result: 在五个数学数据集上，平均推理长度减少50%以上，推理成本显著降低，性能保持稳定。

Conclusion: 自适应策略能有效优化大型语言模型的推理效率，为未来研究提供了新方向。

Abstract: Recently, long-thought reasoning models achieve strong performance on complex
reasoning tasks, but often incur substantial inference overhead, making
efficiency a critical concern. Our empirical analysis reveals that the benefit
of using Long-CoT varies across problems: while some problems require elaborate
reasoning, others show no improvement, or even degraded accuracy. This
motivates adaptive reasoning strategies that tailor reasoning depth to the
input. However, prior work primarily reduces redundancy within long reasoning
paths, limiting exploration of more efficient strategies beyond the Long-CoT
paradigm. To address this, we propose a novel two-stage framework for adaptive
and efficient reasoning. First, we construct a hybrid reasoning model by
merging long and short CoT models to enable diverse reasoning styles. Second,
we apply bi-level preference training to guide the model to select suitable
reasoning styles (group-level), and prefer concise and correct reasoning within
each style group (instance-level). Experiments demonstrate that our method
significantly reduces inference costs compared to other baseline approaches,
while maintaining performance. Notably, on five mathematical datasets, the
average length of reasoning is reduced by more than 50%, highlighting the
potential of adaptive strategies to optimize reasoning efficiency in large
language models. Our code is coming soon at https://github.com/StarDewXXX/AdaR1

</details>


### [60] [Extension-ranking Semantics for Abstract Argumentation Preprint](https://arxiv.org/abs/2504.21683)
*Kenneth Skiba,Tjitze Rienstra,Matthias Thimm,Jesse Heyninck,Gabriele Kern-Isberner*

Main category: cs.AI

TL;DR: 本文提出了一个基于论证可接受性的抽象论证框架，扩展了Dung的扩展语义为扩展-排序语义，通过预序关系比较论证集的可接受性。


<details>
  <summary>Details</summary>
Motivation: 为论证集的可接受性提供一个通用的排序框架，以比较不同论证集的合理性。

Method: 提出扩展-排序语义，结合多个基础关系，形成一系列语义，并评估其行为。

Result: 通过引入原则评估语义的合理性，并比较文献中的其他方法。

Conclusion: 扩展-排序语义为论证集的排序提供了灵活且合理的方法，适用于多种论证推理场景。

Abstract: In this paper, we present a general framework for ranking sets of arguments
in abstract argumentation based on their plausibility of acceptance. We present
a generalisation of Dung's extension semantics as extension-ranking semantics,
which induce a preorder over the power set of all arguments, allowing us to
state that one set is "closer" to being acceptable than another. To evaluate
the extension-ranking semantics, we introduce a number of principles that a
well-behaved extension-ranking semantics should satisfy. We consider several
simple base relations, each of which models a single central aspect of
argumentative reasoning. The combination of these base relations provides us
with a family of extension-ranking semantics. We also adapt a number of
approaches from the literature for ranking extensions to be usable in the
context of extension-ranking semantics, and evaluate their behaviour.

</details>


### [61] [Automatic Mapping of AutomationML Files to Ontologies for Graph Queries and Validation](https://arxiv.org/abs/2504.21694)
*Tom Westermann,Malte Ramonat,Johannes Hujer,Felix Gehlhoff,Alexander Fay*

Main category: cs.AI

TL;DR: AutomationML作为自动化领域的数据交换格式广泛使用，但因其扩展语义限制了XML工具的适用性。本文提供了AutomationML的最新本体概念和RDF三元组的映射方法，便于将其集成到工业知识图谱中。


<details>
  <summary>Details</summary>
Motivation: 解决AutomationML因扩展语义导致XML工具不适用的问题，提升查询和数据验证能力。

Method: 提出AutomationML的本体概念和RDF三元组的自动映射方法。

Result: 研究表明，将AutomationML转换为OWL后，可实现更强大的查询和验证功能。

Conclusion: 通过本体和RDF映射，AutomationML能更高效地集成到知识图谱中，扩展了其应用潜力。

Abstract: AutomationML has seen widespread adoption as an open data exchange format in
the automation domain. It is an open and vendor neutral standard based on the
extensible markup language XML. However, AutomationML extends XML with
additional semantics, that limit the applicability of common XML-tools for
applications like querying or data validation. This article provides
practitioners with 1) an up-to-date ontology of the concepts in the
AutomationML-standard, as well as 2) a declarative mapping to automatically
transform any AutomationML model into RDF triples. Together, these artifacts
allow practitioners an easy integration of AutomationML information into
industrial knowledge graphs. A study on examples from the automation domain
concludes that transforming AutomationML to OWL opens up new powerful ways for
querying and validation that are impossible without transformation.

</details>


### [62] [Is Intermediate Fusion All You Need for UAV-based Collaborative Perception?](https://arxiv.org/abs/2504.21774)
*Jiuwu Hao,Liguo Sun,Yuting Wan,Yueyang Wu,Ti Xiang,Haolin Song,Pin Lv*

Main category: cs.AI

TL;DR: 提出了一种基于延迟中间融合的高效通信协作感知框架LIF，通过交换紧凑检测结果和特征级融合，减少无人机协作中的通信开销。


<details>
  <summary>Details</summary>
Motivation: 现有无人机协作方法未考虑无人机视角特性，导致通信开销大。

Method: 采用延迟中间融合（LIF），结合视觉引导位置嵌入（VPE）和基于框的虚拟增强特征（BoBEV），并引入不确定性驱动通信机制。

Result: 实验表明LIF在低通信带宽下表现优异。

Conclusion: LIF框架高效实用，适合无人机协作感知。

Abstract: Collaborative perception enhances environmental awareness through inter-agent
communication and is regarded as a promising solution to intelligent
transportation systems. However, existing collaborative methods for Unmanned
Aerial Vehicles (UAVs) overlook the unique characteristics of the UAV
perspective, resulting in substantial communication overhead. To address this
issue, we propose a novel communication-efficient collaborative perception
framework based on late-intermediate fusion, dubbed LIF. The core concept is to
exchange informative and compact detection results and shift the fusion stage
to the feature representation level. In particular, we leverage vision-guided
positional embedding (VPE) and box-based virtual augmented feature (BoBEV) to
effectively integrate complementary information from various agents.
Additionally, we innovatively introduce an uncertainty-driven communication
mechanism that uses uncertainty evaluation to select high-quality and reliable
shared areas. Experimental results demonstrate that our LIF achieves superior
performance with minimal communication bandwidth, proving its effectiveness and
practicality. Code and models are available at https://github.com/uestchjw/LIF.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [63] [Model Connectomes: A Generational Approach to Data-Efficient Language Models](https://arxiv.org/abs/2504.21047)
*Klemen Kotar,Greta Tuckute*

Main category: cs.LG

TL;DR: 论文提出了一种结合进化和个体学习的框架，通过‘外循环’进化塑造‘内循环’学习，使人工网络更接近生物神经网络的特性。在语言任务中，该模型表现优于或与对照组相当。


<details>
  <summary>Details</summary>
Motivation: 生物神经网络受进化和个体学习双重影响，而人工神经网络通常缺乏这种多代约束。研究旨在缩小这一差距。

Method: 提出一个框架，通过‘外循环’进化生成‘模型连接组’，再在‘内循环’中学习语言任务。实验使用了100M tokens的语料库。

Result: 模型在自然语言处理任务及与人类行为和脑数据的对齐上表现优于或与对照组相当。

Conclusion: ‘模型连接组’在低数据量下作为高效先验，缩小了人工模型与生物神经网络的差距。

Abstract: Biological neural networks are shaped both by evolution across generations
and by individual learning within an organism's lifetime, whereas standard
artificial neural networks undergo a single, large training procedure without
inherited constraints. In this preliminary work, we propose a framework that
incorporates this crucial generational dimension - an "outer loop" of evolution
that shapes the "inner loop" of learning - so that artificial networks better
mirror the effects of evolution and individual learning in biological
organisms. Focusing on language, we train a model that inherits a "model
connectome" from the outer evolution loop before exposing it to a
developmental-scale corpus of 100M tokens. Compared with two closely matched
control models, we show that the connectome model performs better or on par on
natural language processing tasks as well as alignment to human behavior and
brain data. These findings suggest that a model connectome serves as an
efficient prior for learning in low-data regimes - narrowing the gap between
single-generation artificial models and biologically evolved neural networks.

</details>


### [64] [Multimodal Large Language Models for Medicine: A Comprehensive Survey](https://arxiv.org/abs/2504.21051)
*Jiarui Ye,Hao Tang*

Main category: cs.LG

TL;DR: 本文探讨了多模态大语言模型（MLLMs）在医疗健康领域的应用，总结了其在医疗报告、诊断和治疗中的潜力，并提出了当前面临的挑战及解决方案。


<details>
  <summary>Details</summary>
Motivation: 随着GPT-4的发布，MLLMs在多模态任务中的强大能力引起了广泛关注，研究者开始探索其在医疗健康领域的应用潜力。

Method: 通过综述330篇相关论文，总结了MLLMs在医疗健康领域的三个主要应用方向，并提供了具体案例和数据模式。

Result: MLLMs在医疗报告、诊断和治疗中展现出显著能力，同时提出了六种主流数据模式及评估基准。

Conclusion: 尽管MLLMs在医疗领域具有潜力，但仍面临挑战，需进一步研究以解决这些问题。

Abstract: MLLMs have recently become a focal point in the field of artificial
intelligence research. Building on the strong capabilities of LLMs, MLLMs are
adept at addressing complex multi-modal tasks. With the release of GPT-4, MLLMs
have gained substantial attention from different domains. Researchers have
begun to explore the potential of MLLMs in the medical and healthcare domain.
In this paper, we first introduce the background and fundamental concepts
related to LLMs and MLLMs, while emphasizing the working principles of MLLMs.
Subsequently, we summarize three main directions of application within
healthcare: medical reporting, medical diagnosis, and medical treatment. Our
findings are based on a comprehensive review of 330 recent papers in this area.
We illustrate the remarkable capabilities of MLLMs in these domains by
providing specific examples. For data, we present six mainstream modes of data
along with their corresponding evaluation benchmarks. At the end of the survey,
we discuss the challenges faced by MLLMs in the medical and healthcare domain
and propose feasible methods to mitigate or overcome these issues.

</details>


### [65] [NeuRel-Attack: Neuron Relearning for Safety Disalignment in Large Language Models](https://arxiv.org/abs/2504.21053)
*Yi Zhou,Wenpeng Xing,Dezhang Kong,Changting Lin,Meng Han*

Main category: cs.LG

TL;DR: 本文提出了一种通过识别和修改负责安全约束的神经元来诱导大型语言模型（LLM）安全对齐失效的新方法。


<details>
  <summary>Details</summary>
Motivation: 当前LLM的安全对齐机制通过调节神经元激活来抑制有害内容，但缺乏对对抗性微调攻击的鲁棒性。本文旨在揭示这一漏洞。

Method: 方法包括三个步骤：神经元激活分析、基于相似性的神经元识别以及神经元重新学习以移除安全约束。

Result: 实验表明，该方法能以最小微调有效移除安全约束，暴露当前对齐技术的脆弱性。

Conclusion: 研究结果强调了针对LLM对抗性微调攻击的防御需求。

Abstract: Safety alignment in large language models (LLMs) is achieved through
fine-tuning mechanisms that regulate neuron activations to suppress harmful
content. In this work, we propose a novel approach to induce disalignment by
identifying and modifying the neurons responsible for safety constraints. Our
method consists of three key steps: Neuron Activation Analysis, where we
examine activation patterns in response to harmful and harmless prompts to
detect neurons that are critical for distinguishing between harmful and
harmless inputs; Similarity-Based Neuron Identification, which systematically
locates the neurons responsible for safe alignment; and Neuron Relearning for
Safety Removal, where we fine-tune these selected neurons to restore the
model's ability to generate previously restricted responses. Experimental
results demonstrate that our method effectively removes safety constraints with
minimal fine-tuning, highlighting a critical vulnerability in current alignment
techniques. Our findings underscore the need for robust defenses against
adversarial fine-tuning attacks on LLMs.

</details>


### [66] [Modeling and Performance Analysis for Semantic Communications Based on Empirical Results](https://arxiv.org/abs/2504.21055)
*Shuai Ma,Bin Shen,Chuanhui Zhang,Youlong Wu,Hang Li,Shiyin Li,Guangming Shi,Naofal Al-Dhahir*

Main category: cs.LG

TL;DR: 本文提出了一种Alpha-Beta-Gamma（ABG）公式，用于分析语义通信的性能，适用于图像重建和推理任务，并提出了自适应功率控制方案。


<details>
  <summary>Details</summary>
Motivation: 由于深度学习语义编码器和解码器的黑盒特性，分析语义通信性能是一个挑战性问题。

Method: 提出ABG公式建模端到端性能与信噪比（SNR）的关系，并推导了MS-SSIM与量化输出比特数的闭式表达式。

Result: ABG公式能有效拟合常见DL网络，如SCUNet和Vision Transformer，并设计出最大化能效和用户QoS的功率分配方案。

Conclusion: 仿真验证了ABG公式和功率分配方案的有效性和优越性。

Abstract: Due to the black-box characteristics of deep learning based semantic encoders
and decoders, finding a tractable method for the performance analysis of
semantic communications is a challenging problem. In this paper, we propose an
Alpha-Beta-Gamma (ABG) formula to model the relationship between the end-to-end
measurement and SNR, which can be applied for both image reconstruction tasks
and inference tasks. Specifically, for image reconstruction tasks, the proposed
ABG formula can well fit the commonly used DL networks, such as SCUNet, and
Vision Transformer, for semantic encoding with the multi scale-structural
similarity index measure (MS-SSIM) measurement. Furthermore, we find that the
upper bound of the MS-SSIM depends on the number of quantized output bits of
semantic encoders, and we also propose a closed-form expression to fit the
relationship between the MS-SSIM and quantized output bits. To the best of our
knowledge, this is the first theoretical expression between end-to-end
performance metrics and SNR for semantic communications. Based on the proposed
ABG formula, we investigate an adaptive power control scheme for semantic
communications over random fading channels, which can effectively guarantee
quality of service (QoS) for semantic communications, and then design the
optimal power allocation scheme to maximize the energy efficiency of the
semantic communication system. Furthermore, by exploiting the bisection
algorithm, we develop the power allocation scheme to maximize the minimum QoS
of multiple users for OFDMA downlink semantic communication Extensive
simulations verify the effectiveness and superiority of the proposed ABG
formula and power allocation schemes.

</details>


### [67] [A Hamiltonian Higher-Order Elasticity Framework for Dynamic Diagnostics(2HOED)](https://arxiv.org/abs/2504.21062)
*Ngueuleweu Tiwang Gildas*

Main category: cs.LG

TL;DR: 2HOED框架结合机器学习、区块链和因果推理，通过哈密顿动力学分析复杂系统的能量结构，提供跨学科的动态能量映射。


<details>
  <summary>Details</summary>
Motivation: 现有技术（如机器学习、区块链和因果推理）无法全面解析复杂系统的能量动态，2HOED填补了这一空白。

Method: 基于经典力学扩展的哈密顿动力学框架，分析系统的位置、速度、加速度和弹性，生成能量地图。

Result: 2HOED揭示了线性模型无法捕捉的韧性、临界点和反馈循环，适用于金融、气候、供应链等多领域。

Conclusion: 2HOED为决策者提供了动态能量映射工具，兼具AI的预测能力和物理学的解释性。

Abstract: Machine learning detects patterns, block chain guarantees trust and
immutability, and modern causal inference identifies directional linkages, yet
none alone exposes the full energetic anatomy of complex systems; the
Hamiltonian Higher Order Elasticity Dynamics(2HOED) framework bridges these
gaps. Grounded in classical mechanics but extended to Economics order
elasticity terms, 2HOED represents economic, social, and physical systems as
energy-based Hamiltonians whose position, velocity, acceleration, and jerk of
elasticity jointly determine systemic power, Inertia, policy sensitivity, and
marginal responses. Because the formalism is scaling free and coordinate
agnostic, it transfers seamlessly from financial markets to climate science,
from supply chain logistics to epidemiology, thus any discipline in which
adaptation and shocks coexist. By embedding standard econometric variables
inside a Hamiltonian, 2HOED enriches conventional economic analysis with
rigorous diagnostics of resilience, tipping points, and feedback loops,
revealing failure modes invisible to linear models. Wavelet spectra, phase
space attractors, and topological persistence diagrams derived from 2HOED
expose multistage policy leverage that machine learning detects only
empirically and block chain secures only after the fact. For economists,
physicians and other scientists, the method opens a new causal energetic
channel linking biological or mechanical elasticity to macro level outcomes.
Portable, interpretable, and computationally light, 2HOED turns data streams
into dynamical energy maps, empowering decision makers to anticipate crises,
design adaptive policies, and engineer robust systems delivering the predictive
punch of AI with the explanatory clarity of physics.

</details>


### [68] [Token-Level Prompt Mixture with Parameter-Free Routing for Federated Domain Generalization](https://arxiv.org/abs/2504.21063)
*Shuai Gong,Chaoran Cui,Xiaolin Dong,Xiushan Nie,Lei Zhu,Xiaojun Chang*

Main category: cs.LG

TL;DR: TRIP提出了一种基于令牌级提示混合的无参数路由框架，用于联邦域泛化（FedDG），通过将不同令牌分配给特定专家并优化通信效率，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有FedDG方法中单一全局提示导致的性能下降问题，以及基于MoE的方法中粗粒度路由和高通信成本的限制。

Method: TRIP采用令牌级提示混合，通过无参数路由机制（基于令牌聚类和最优传输）分配令牌给专家，并合成实例特定提示。

Result: 在四个基准测试中，TRIP实现了最优的泛化性能，每轮通信仅需1K参数。

Conclusion: TRIP通过令牌级路由和无参数设计，有效提升了FedDG的性能和通信效率。

Abstract: Federated domain generalization (FedDG) aims to learn a globally
generalizable model from decentralized clients with heterogeneous data while
preserving privacy. Recent studies have introduced prompt learning to adapt
vision-language models (VLMs) in FedDG by learning a single global prompt.
However, such a one-prompt-fits-all learning paradigm typically leads to
performance degradation on personalized samples. Although the mixture of
experts (MoE) offers a promising solution for specialization, existing
MoE-based methods suffer from coarse image-level expert assignment and high
communication costs from parameterized routers. To address these limitations,
we propose TRIP, a Token-level prompt mixture with parameter-free routing
framework for FedDG, which treats multiple prompts as distinct experts. Unlike
existing image-level routing designs, TRIP assigns different tokens within an
image to specific experts. To ensure communication efficiency, TRIP
incorporates a parameter-free routing mechanism based on token clustering and
optimal transport. The instance-specific prompt is then synthesized by
aggregating experts, weighted by the number of tokens assigned to each.
Additionally, TRIP develops an unbiased learning strategy for prompt experts,
leveraging the VLM's zero-shot generalization capability. Extensive experiments
across four benchmarks demonstrate that TRIP achieves optimal generalization
results, with communication of only 1K parameters per round. Our code is
available at https://github.com/GongShuai8210/TRIP.

</details>


### [69] [Frequency Feature Fusion Graph Network For Depression Diagnosis Via fNIRS](https://arxiv.org/abs/2504.21064)
*Chengkai Yang,Xingping Dong,Xiaofen Zong*

Main category: cs.LG

TL;DR: 本文提出了一种基于离散傅里叶变换（DFT）的新型生物标志物，并结合定制化的时序图卷积网络（TGCN）架构，显著提升了抑郁症诊断的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有基于图神经网络（GNN）的抑郁症诊断方法缺乏有效的时序生物标志物，限制了其性能。

Method: 利用DFT提取时序生物标志物，设计TGCN架构，并在包含1,086名受试者的大规模数据集上进行训练，同时使用倾向得分匹配（PSM）优化数据。

Result: 新生物标志物显著提升了脑通道时序特征的表示能力，在真实数据集和PSM数据集上均提高了F1分数。

Conclusion: 该方法为抑郁症诊断提供了更有效的工具，并通过SHAP验证了模型的可解释性，具有实际医疗应用潜力。

Abstract: Data-driven approaches for depression diagnosis have emerged as a significant
research focus in neuromedicine, driven by the development of relevant
datasets. Recently, graph neural network (GNN)-based models have gained
widespread adoption due to their ability to capture brain channel functional
connectivity from both spatial and temporal perspectives. However, their
effectiveness is hindered by the absence of a robust temporal biomarker. In
this paper, we introduce a novel and effective biomarker for depression
diagnosis by leveraging the discrete Fourier transform (DFT) and propose a
customized graph network architecture based on Temporal Graph Convolutional
Network (TGCN). Our model was trained on a dataset comprising 1,086 subjects,
which is over 10 times larger than previous datasets in the field of depression
diagnosis. Furthermore, to align with medical requirements, we performed
propensity score matching (PSM) to create a refined subset, referred to as the
PSM dataset. Experimental results demonstrate that incorporating our newly
designed biomarker enhances the representation of temporal characteristics in
brain channels, leading to improved F1 scores in both the real-world dataset
and the PSM dataset. This advancement has the potential to contribute to the
development of more effective depression diagnostic tools. In addition, we used
SHapley Additive exPlaination (SHAP) to validate the interpretability of our
model, ensuring its practical applicability in medical settings.

</details>


### [70] [A 3D pocket-aware and affinity-guided diffusion model for lead optimization](https://arxiv.org/abs/2504.21065)
*Anjie Qiao,Junjie Xie,Weifeng Huang,Hao Zhang,Jiahua Rao,Shuangjia Zheng,Yuedong Yang,Zhen Wang,Guo-Bo Li,Jinping Lei*

Main category: cs.LG

TL;DR: Diffleop是一种3D口袋感知和亲和力引导的扩散模型，用于优化分子以提高结合亲和力。


<details>
  <summary>Details</summary>
Motivation: 分子优化在药物发现中至关重要，但现有深度学习模型在优化过程中常忽略结合亲和力。

Method: 提出Diffleop模型，利用蛋白质-配体结合亲和力知识引导去噪采样，生成高亲和力分子。

Result: Diffleop在多个指标上优于基线模型，尤其在结合亲和力方面表现突出。

Conclusion: Diffleop为分子优化提供了一种高效且亲和力导向的新方法。

Abstract: Molecular optimization, aimed at improving binding affinity or other
molecular properties, is a crucial task in drug discovery that often relies on
the expertise of medicinal chemists. Recently, deep learning-based 3D
generative models showed promise in enhancing the efficiency of molecular
optimization. However, these models often struggle to adequately consider
binding affinities with protein targets during lead optimization. Herein, we
propose a 3D pocket-aware and affinity-guided diffusion model, named Diffleop,
to optimize molecules with enhanced binding affinity. The model explicitly
incorporates the knowledge of protein-ligand binding affinity to guide the
denoising sampling for molecule generation with high affinity. The
comprehensive evaluations indicated that Diffleop outperforms baseline models
across multiple metrics, especially in terms of binding affinity.

</details>


### [71] [A Brief Review for Compression and Transfer Learning Techniques in DeepFake Detection](https://arxiv.org/abs/2504.21066)
*Andreas Karathanasis,John Violos,Ioannis Kompatsiaris,Symeon Papadopoulos*

Main category: cs.LG

TL;DR: 论文探讨了在边缘设备上部署深度伪造检测模型的压缩和迁移学习方法，以解决资源限制问题。实验表明，高压缩率下性能保持稳定，但跨模型测试时存在领域泛化问题。


<details>
  <summary>Details</summary>
Motivation: 边缘设备的计算和内存资源有限，需要压缩技术和迁移学习来降低计算需求和训练开销。

Method: 使用剪枝、知识蒸馏、量化、微调和适配器技术，在Synthbuster、RAISE和ForenSynths数据集上评估效果。

Result: 高压缩率（90%）下性能稳定，但测试数据来自不同DeepFake模型时出现领域泛化问题。

Conclusion: 压缩和迁移学习在边缘设备上可行，但需解决跨模型泛化问题。

Abstract: Training and deploying deepfake detection models on edge devices offers the
advantage of maintaining data privacy and confidentiality by processing it
close to its source. However, this approach is constrained by the limited
computational and memory resources available at the edge. To address this
challenge, we explore compression techniques to reduce computational demands
and inference time, alongside transfer learning methods to minimize training
overhead. Using the Synthbuster, RAISE, and ForenSynths datasets, we evaluate
the effectiveness of pruning, knowledge distillation (KD), quantization,
fine-tuning, and adapter-based techniques. Our experimental results demonstrate
that both compression and transfer learning can be effectively achieved, even
with a high compression level of 90%, remaining at the same performance level
when the training and validation data originate from the same DeepFake model.
However, when the testing dataset is generated by DeepFake models not present
in the training set, a domain generalization issue becomes evident.

</details>


### [72] [R^2VFL: A Robust Random Vector Functional Link Network with Huber-Weighted Framework](https://arxiv.org/abs/2504.21069)
*Anuradha Kumari,Mushir Akhtar,P. N. Suganthan,M. Tanveer*

Main category: cs.LG

TL;DR: 提出了一种名为R2VFL的鲁棒框架，结合Huber加权函数和类别概率，有效减少噪声和异常值的影响，提升了RVFL神经网络的性能。


<details>
  <summary>Details</summary>
Motivation: RVFL神经网络在处理噪声和异常值时表现不佳，因为其假设所有数据样本贡献相同。

Method: 通过Huber加权函数减少异常值的影响，并利用类别概率机制为噪声数据点分配较低权重。提出了两种计算类别中心的方法，分别基于平均值和中位数，形成R2VFL-A和R2VFL-M变体。

Result: 在47个UCI数据集上的实验表明，R2VFL模型显著优于传统方法，并在EEG信号分类中表现出色。

Conclusion: R2VFL框架在鲁棒性和适应性方面表现优异，适用于实际生物医学领域。

Abstract: The random vector functional link (RVFL) neural network has shown significant
potential in overcoming the constraints of traditional artificial neural
networks, such as excessive computation time and suboptimal solutions. However,
RVFL faces challenges when dealing with noise and outliers, as it assumes all
data samples contribute equally. To address this issue, we propose a novel
robust framework, R2VFL, RVFL with Huber weighting function and class
probability, which enhances the model's robustness and adaptability by
effectively mitigating the impact of noise and outliers in the training data.
The Huber weighting function reduces the influence of outliers, while the class
probability mechanism assigns less weight to noisy data points, resulting in a
more resilient model. We explore two distinct approaches for calculating class
centers within the R2VFL framework: the simple average of all data points in
each class and the median of each feature, the later providing a robust
alternative by minimizing the effect of extreme values. These approaches give
rise to two novel variants of the model-R2VFL-A and R2VFL-M. We extensively
evaluate the proposed models on 47 UCI datasets, encompassing both binary and
multiclass datasets, and conduct rigorous statistical testing, which confirms
the superiority of the proposed models. Notably, the models also demonstrate
exceptional performance in classifying EEG signals, highlighting their
practical applicability in real-world biomedical domain.

</details>


### [73] [A Survey on Parameter-Efficient Fine-Tuning for Foundation Models in Federated Learning](https://arxiv.org/abs/2504.21099)
*Jieming Bian,Yuanzhe Peng,Lei Wang,Yin Huang,Jie Xu*

Main category: cs.LG

TL;DR: 本文综述了参数高效微调（PEFT）与联邦学习（FL）的结合，分析了三类PEFT方法在联邦学习环境中的应用及其挑战。


<details>
  <summary>Details</summary>
Motivation: 基础模型在特定任务上的微调计算成本高，而联邦学习需要隐私保护，PEFT方法旨在解决这些挑战。

Method: 将PEFT方法分为三类：加性PEFT、选择性PEFT和重参数化PEFT，并分析其在联邦学习中的适应性。

Result: 综述了各类方法如何应对数据异构性、通信效率、计算限制和隐私问题，并总结了应用领域。

Conclusion: 未来研究方向包括扩展至更大基础模型、理论分析及资源受限环境下的可持续方法。

Abstract: Foundation models have revolutionized artificial intelligence by providing
robust, versatile architectures pre-trained on large-scale datasets. However,
adapting these massive models to specific downstream tasks requires
fine-tuning, which can be prohibitively expensive in computational resources.
Parameter-Efficient Fine-Tuning (PEFT) methods address this challenge by
selectively updating only a small subset of parameters. Meanwhile, Federated
Learning (FL) enables collaborative model training across distributed clients
without sharing raw data, making it ideal for privacy-sensitive applications.
This survey provides a comprehensive review of the integration of PEFT
techniques within federated learning environments. We systematically categorize
existing approaches into three main groups: Additive PEFT (which introduces new
trainable parameters), Selective PEFT (which fine-tunes only subsets of
existing parameters), and Reparameterized PEFT (which transforms model
architectures to enable efficient updates). For each category, we analyze how
these methods address the unique challenges of federated settings, including
data heterogeneity, communication efficiency, computational constraints, and
privacy concerns. We further organize the literature based on application
domains, covering both natural language processing and computer vision tasks.
Finally, we discuss promising research directions, including scaling to larger
foundation models, theoretical analysis of federated PEFT methods, and
sustainable approaches for resource-constrained environments.

</details>


### [74] [SMOGAN: Synthetic Minority Oversampling with GAN Refinement for Imbalanced Regression](https://arxiv.org/abs/2504.21152)
*Shayan Alahyari,Mike Domaratzki*

Main category: cs.LG

TL;DR: SMOGAN是一个用于不平衡回归的两步过采样框架，通过结合初始过采样和分布感知GAN（DistGAN）来提升稀疏区域的样本质量，显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 不平衡回归中目标变量的偏态分布导致模型在稀疏区域表现不佳，现有方法生成的合成样本未能准确反映真实分布。

Method: SMOGAN分为两步：1）初始过采样生成稀疏区域的样本；2）通过DistGAN（结合对抗损失和最大均值差异目标）优化样本分布。

Result: 在23个不平衡数据集上的实验表明，SMOGAN显著优于未使用DistGAN过滤层的传统过采样方法。

Conclusion: SMOGAN通过分布感知的样本优化，有效解决了不平衡回归中稀疏区域的样本生成问题。

Abstract: Imbalanced regression refers to prediction tasks where the target variable is
skewed. This skewness hinders machine learning models, especially neural
networks, which concentrate on dense regions and therefore perform poorly on
underrepresented (minority) samples. Despite the importance of this problem,
only a few methods have been proposed for imbalanced regression. Many of the
available solutions for imbalanced regression adapt techniques from the class
imbalance domain, such as linear interpolation and the addition of Gaussian
noise, to create synthetic data in sparse regions. However, in many cases, the
underlying distribution of the data is complex and non-linear. Consequently,
these approaches generate synthetic samples that do not accurately represent
the true feature-target relationship. To overcome these limitations, we propose
SMOGAN, a two-step oversampling framework for imbalanced regression. In Stage
1, an existing oversampler generates initial synthetic samples in sparse target
regions. In Stage 2, we introduce DistGAN, a distribution-aware GAN that serves
as SMOGAN's filtering layer and refines these samples via adversarial loss
augmented with a Maximum Mean Discrepancy objective, aligning them with the
true joint feature-target distribution. Extensive experiments on 23 imbalanced
datasets show that SMOGAN consistently outperforms the default oversampling
method without the DistGAN filtering layer.

</details>


### [75] [Efficient LLMs with AMP: Attention Heads and MLP Pruning](https://arxiv.org/abs/2504.21174)
*Leandro Giusti Mugnaini,Bruno Lopes Yamamoto,Lucas Lauton de Alcantara,Victor Zacarias,Edson Bollis,Lucas Pellicer,Anna Helena Reali Costa,Artur Jordao*

Main category: cs.LG

TL;DR: AMP是一种新型结构化剪枝方法，通过移除LLMs中不太重要的结构（如MHA和MLP）来高效压缩模型，提升推理速度，适合资源受限环境。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）虽然性能优越，但参数过多导致计算成本高、推理慢，难以在资源有限的环境中部署。

Method: AMP通过将输入数据投影到权重上，评估结构重要性，灵活高效地剪枝MHA和MLP。

Result: AMP在常识推理任务上超越现有技术1.49个百分点，实现30%剪枝率且对零样本任务性能影响极小，同时提升推理速度。

Conclusion: AMP在不同LLM家族（如LLaMA和Phi）上表现出灵活性，适合资源受限环境部署。

Abstract: Deep learning drives a new wave in computing systems and triggers the
automation of increasingly complex problems. In particular, Large Language
Models (LLMs) have significantly advanced cognitive tasks, often matching or
even surpassing human-level performance. However, their extensive parameters
result in high computational costs and slow inference, posing challenges for
deployment in resource-limited settings. Among the strategies to overcome the
aforementioned challenges, pruning emerges as a successful mechanism since it
reduces model size while maintaining predictive ability. In this paper, we
introduce AMP: Attention Heads and MLP Pruning, a novel structured pruning
method that efficiently compresses LLMs by removing less critical structures
within Multi-Head Attention (MHA) and Multilayer Perceptron (MLP). By
projecting the input data onto weights, AMP assesses structural importance and
overcomes the limitations of existing techniques, which often fall short in
flexibility or efficiency. In particular, AMP surpasses the current
state-of-the-art on commonsense reasoning tasks by up to 1.49 percentage
points, achieving a 30% pruning ratio with minimal impact on zero-shot task
performance. Moreover, AMP also improves inference speeds, making it
well-suited for deployment in resource-constrained environments. We confirm the
flexibility of AMP on different families of LLMs, including LLaMA and Phi.

</details>


### [76] [GLIP-OOD: Zero-Shot Graph OOD Detection with Foundation Model](https://arxiv.org/abs/2504.21186)
*Haoyan Xu,Zhengtao Yao,Xuzhi Zhang,Ziyi Wang,Langzhou He,Yushun Dong,Philip S. Yu,Mengyuan Li,Yue Zhao*

Main category: cs.LG

TL;DR: 该论文提出了零样本图结构数据的OOD检测方法，利用图基础模型（GFM）和LLM生成的伪OOD标签，无需节点级监督即可实现高性能检测。


<details>
  <summary>Details</summary>
Motivation: 解决图结构数据中零样本OOD检测的空白，因缺乏大规模预训练模型和复杂关系结构而具有挑战性。

Method: 使用GFM进行零样本OOD检测，并引入GLIP-OOD框架，利用LLM生成伪OOD标签以捕捉ID和OOD类间的语义边界。

Result: 在多个数据集上超越现有监督方法，并在四个基准文本属性图数据集上达到最优性能。

Conclusion: 首次实现完全零样本的节点级图OOD检测，为动态开放环境中的机器学习系统安全提供新方法。

Abstract: Out-of-distribution (OOD) detection is critical for ensuring the safety and
reliability of machine learning systems, particularly in dynamic and open-world
environments. In the vision and text domains, zero-shot OOD detection - which
requires no training on in-distribution (ID) data - has made significant
progress through the use of large-scale pretrained models such as
vision-language models (VLMs) and large language models (LLMs). However,
zero-shot OOD detection in graph-structured data remains largely unexplored,
primarily due to the challenges posed by complex relational structures and the
absence of powerful, large-scale pretrained models for graphs. In this work, we
take the first step toward enabling zero-shot graph OOD detection by leveraging
a graph foundation model (GFM). We show that, when provided only with class
label names, the GFM can perform OOD detection without any node-level
supervision - outperforming existing supervised methods across multiple
datasets. To address the more practical setting where OOD label names are
unavailable, we introduce GLIP-OOD, a novel framework that employs LLMs to
generate semantically informative pseudo-OOD labels from unlabeled data. These
labels enable the GFM to capture nuanced semantic boundaries between ID and OOD
classes and perform fine-grained OOD detection - without requiring any labeled
nodes. Our approach is the first to enable node-level graph OOD detection in a
fully zero-shot setting, and achieves state-of-the-art performance on four
benchmark text-attributed graph datasets.

</details>


### [77] [LIFT: LLM-Based Pragma Insertion for HLS via GNN Supervised Fine-Tuning](https://arxiv.org/abs/2504.21187)
*Neha Prakriya,Zijian Ding,Yizhou Sun,Jason Cong*

Main category: cs.LG

TL;DR: LIFT是一种基于大型语言模型（LLM）的HLS编码助手，能自动生成性能关键pragma，显著提升FPGA设计性能。


<details>
  <summary>Details</summary>
Motivation: FPGA在数据中心的应用日益广泛，但高性能设计仍依赖专家知识和手动优化。LIFT旨在通过自动化优化解决这一问题。

Method: LIFT结合LLM和GNN，通过监督训练生成性能关键pragma，利用LLM的序列建模能力和GNN的结构化理解能力。

Result: LIFT平均性能提升3.52倍（优于AutoDSE）、2.16倍（优于HARP）和66倍（优于GPT-4o）。

Conclusion: LIFT通过LLM和GNN的结合，显著提升了FPGA设计的自动化优化能力。

Abstract: FPGAs are increasingly adopted in datacenter environments for their
reconfigurability and energy efficiency. High-Level Synthesis (HLS) tools have
eased FPGA programming by raising the abstraction level from RTL to untimed
C/C++, yet attaining high performance still demands expert knowledge and
iterative manual insertion of optimization pragmas to modify the
microarchitecture. To address this challenge, we propose LIFT, a large language
model (LLM)-based coding assistant for HLS that automatically generates
performance-critical pragmas given a C/C++ design. We fine-tune the LLM by
tightly integrating and supervising the training process with a graph neural
network (GNN), combining the sequential modeling capabilities of LLMs with the
structural and semantic understanding of GNNs necessary for reasoning over code
and its control/data dependencies. On average, LIFT produces designs that
improve performance by 3.52x and 2.16x than prior state-of the art AutoDSE and
HARP respectively, and 66x than GPT-4o.

</details>


### [78] [Artificial Intelligence for Personalized Prediction of Alzheimer's Disease Progression: A Survey of Methods, Data Challenges, and Future Directions](https://arxiv.org/abs/2504.21189)
*Gulsah Hancerliogullari Koksalmis,Bulent Soykan,Laura J. Brattain,Hsin-Hsiung Huang*

Main category: cs.LG

TL;DR: 本文综述了人工智能在个性化阿尔茨海默病（AD）进展预测中的应用，包括多种AI方法、数据挑战及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: AD进展的个体差异大，需要个性化预测模型以改善预后和护理计划。AI能够分析复杂的多模态数据，为此提供解决方案。

Method: 综述了状态空间模型、循环神经网络、图神经网络和AI驱动的数字孪生等方法，并探讨了数据挑战及应对策略（如VAEs和GANs）。

Result: 总结了当前方法的优缺点，强调多模态整合趋势及模型可解释性和泛化性的需求。

Conclusion: 指出了外部验证、临床整合和伦理问题等挑战，并提出了混合模型、因果推理和联邦学习等未来方向。

Abstract: Alzheimer's Disease (AD) is marked by significant inter-individual
variability in its progression, complicating accurate prognosis and
personalized care planning. This heterogeneity underscores the critical need
for predictive models capable of forecasting patient-specific disease
trajectories. Artificial Intelligence (AI) offers powerful tools to address
this challenge by analyzing complex, multi-modal, and longitudinal patient
data. This paper provides a comprehensive survey of AI methodologies applied to
personalized AD progression prediction. We review key approaches including
state-space models for capturing temporal dynamics, deep learning techniques
like Recurrent Neural Networks for sequence modeling, Graph Neural Networks
(GNNs) for leveraging network structures, and the emerging concept of AI-driven
digital twins for individualized simulation. Recognizing that data limitations
often impede progress, we examine common challenges such as high
dimensionality, missing data, and dataset imbalance. We further discuss
AI-driven mitigation strategies, with a specific focus on synthetic data
generation using Variational Autoencoders (VAEs) and Generative Adversarial
Networks (GANs) to augment and balance datasets. The survey synthesizes the
strengths and limitations of current approaches, emphasizing the trend towards
multimodal integration and the persistent need for model interpretability and
generalizability. Finally, we identify critical open challenges, including
robust external validation, clinical integration, and ethical considerations,
and outline promising future research directions such as hybrid models, causal
inference, and federated learning. This review aims to consolidate current
knowledge and guide future efforts in developing clinically relevant AI tools
for personalized AD prognostication.

</details>


### [79] [TT-LoRA MoE: Unifying Parameter-Efficient Fine-Tuning and Sparse Mixture-of-Experts](https://arxiv.org/abs/2504.21190)
*Pradip Kunwar,Minh N. Vu,Maanak Gupta,Mahmoud Abdelsalam,Manish Bhattarai*

Main category: cs.LG

TL;DR: TT-LoRA MoE是一种结合参数高效微调（PEFT）和稀疏MoE路由的新框架，解决了大模型部署中的扩展性问题。通过分阶段训练和动态路由选择，显著提升了计算效率和灵活性。


<details>
  <summary>Details</summary>
Motivation: 传统MoE方法在专家数量增加时面临计算开销大的问题，TT-LoRA MoE旨在通过分阶段训练和动态路由优化解决这一问题。

Method: 分两阶段训练：首先独立训练轻量级张量化低秩适配器（TT-LoRA专家），然后冻结这些适配器，通过稀疏MoE路由器动态选择适配器。

Result: 实验表明，TT-LoRA MoE仅使用少量参数（如2%的LoRA参数），在多任务中表现优于AdapterFusion，提升4个值。

Conclusion: TT-LoRA MoE通过结构化解耦显著提升了计算效率和灵活性，适用于大规模多任务推理部署。

Abstract: We propose Tensor-Trained Low-Rank Adaptation Mixture of Experts (TT-LoRA
MoE), a novel computational framework integrating Parameter-Efficient
Fine-Tuning (PEFT) with sparse MoE routing to address scalability challenges in
large model deployments. Unlike traditional MoE approaches, which face
substantial computational overhead as expert counts grow, TT-LoRA MoE
decomposes training into two distinct, optimized stages. First, we
independently train lightweight, tensorized low-rank adapters (TT-LoRA
experts), each specialized for specific tasks. Subsequently, these expert
adapters remain frozen, eliminating inter-task interference and catastrophic
forgetting in multi-task setting. A sparse MoE router, trained separately,
dynamically leverages base model representations to select exactly one
specialized adapter per input at inference time, automating expert selection
without explicit task specification. Comprehensive experiments confirm our
architecture retains the memory efficiency of low-rank adapters, seamlessly
scales to large expert pools, and achieves robust task-level optimization. This
structured decoupling significantly enhances computational efficiency and
flexibility: uses only 2% of LoRA, 0.3% of Adapters and 0.03% of AdapterFusion
parameters and outperforms AdapterFusion by 4 value in multi-tasking, enabling
practical and scalable multi-task inference deployments.

</details>


### [80] [Graph Synthetic Out-of-Distribution Exposure with Large Language Models](https://arxiv.org/abs/2504.21198)
*Haoyan Xu,Zhengtao Yao,Ziyi Wang,Zhan Cheng,Xiyang Hu,Mengyuan Li,Yue Zhao*

Main category: cs.LG

TL;DR: GOE-LLM利用大语言模型（LLMs）进行图OOD检测，无需真实OOD节点，通过生成伪OOD节点提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有图OOD检测方法依赖真实OOD节点，但获取这些节点成本高且不切实际。

Method: GOE-LLM通过零样本LLM标注识别伪OOD节点，并通过LLM生成合成OOD节点，用于训练ID分类器。

Result: GOE-LLM在多个基准数据集上显著优于不依赖OOD暴露的方法，性能接近依赖真实OOD数据的方法。

Conclusion: GOE-LLM为图OOD检测提供了一种无需真实OOD数据的高效解决方案。

Abstract: Out-of-distribution (OOD) detection in graphs is critical for ensuring model
robustness in open-world and safety-sensitive applications. Existing approaches
to graph OOD detection typically involve training an in-distribution (ID)
classifier using only ID data, followed by the application of post-hoc OOD
scoring techniques. Although OOD exposure - introducing auxiliary OOD samples
during training - has proven to be an effective strategy for enhancing
detection performance, current methods in the graph domain generally assume
access to a set of real OOD nodes. This assumption, however, is often
impractical due to the difficulty and cost of acquiring representative OOD
samples. In this paper, we introduce GOE-LLM, a novel framework that leverages
Large Language Models (LLMs) for OOD exposure in graph OOD detection without
requiring real OOD nodes. GOE-LLM introduces two pipelines: (1) identifying
pseudo-OOD nodes from the initially unlabeled graph using zero-shot LLM
annotations, and (2) generating semantically informative synthetic OOD nodes
via LLM-prompted text generation. These pseudo-OOD nodes are then used to
regularize the training of the ID classifier for improved OOD awareness. We
evaluate our approach across multiple benchmark datasets, showing that GOE-LLM
significantly outperforms state-of-the-art graph OOD detection methods that do
not use OOD exposure and achieves comparable performance to those relying on
real OOD data.

</details>


### [81] [FedHERO: A Federated Learning Approach for Node Classification Task on Heterophilic Graphs](https://arxiv.org/abs/2504.21206)
*Zihan Chen,Xingbo Fu,Yushun Dong,Jundong Li,Cong Shen*

Main category: cs.LG

TL;DR: FedHERO是一个联邦图学习框架，旨在有效处理异构图数据，通过双通道GNN和结构学习器提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有的联邦图学习方法假设所有客户端的图数据是同质的，但实际中节点邻居分布可能不同，导致模型聚合时性能下降。

Method: 提出FedHERO框架，采用双通道GNN和结构学习器，识别并学习适用于不同节点邻居分布模式的通用知识。

Result: 实验证明FedHERO在异构图数据上优于现有方法，提升了客户端模型性能。

Conclusion: FedHERO为处理多样化节点邻居分布图数据提供了新思路，并在性能上取得显著提升。

Abstract: Federated Graph Learning (FGL) empowers clients to collaboratively train
Graph neural networks (GNNs) in a distributed manner while preserving data
privacy. However, FGL methods usually require that the graph data owned by all
clients is homophilic to ensure similar neighbor distribution patterns of
nodes. Such an assumption ensures that the learned knowledge is consistent
across the local models from all clients. Therefore, these local models can be
properly aggregated as a global model without undermining the overall
performance. Nevertheless, when the neighbor distribution patterns of nodes
vary across different clients (e.g., when clients hold graphs with different
levels of heterophily), their local models may gain different and even conflict
knowledge from their node-level predictive tasks. Consequently, aggregating
these local models usually leads to catastrophic performance deterioration on
the global model. To address this challenge, we propose FedHERO, an FGL
framework designed to harness and share insights from heterophilic graphs
effectively. At the heart of FedHERO is a dual-channel GNN equipped with a
structure learner, engineered to discern the structural knowledge encoded in
the local graphs. With this specialized component, FedHERO enables the local
model for each client to identify and learn patterns that are universally
applicable across graphs with different patterns of node neighbor
distributions. FedHERO not only enhances the performance of individual client
models by leveraging both local and shared structural insights but also sets a
new precedent in this field to effectively handle graph data with various node
neighbor distribution patterns. We conduct extensive experiments to validate
the superior performance of FedHERO against existing alternatives.

</details>


### [82] [A Cost-Effective LLM-based Approach to Identify Wildlife Trafficking in Online Marketplaces](https://arxiv.org/abs/2504.21211)
*Juliana Barbosa,Ulhas Gondhali,Gohar Petrossian,Kinshuk Sharma,Sunandan Chakraborty,Jennifer Jacquet,Juliana Freire*

Main category: cs.LG

TL;DR: 论文提出了一种利用大型语言模型（LLMs）生成伪标签的低成本策略，用于构建高效分类器，以识别野生动物非法贸易的在线广告，实验显示其性能优于直接使用LLMs。


<details>
  <summary>Details</summary>
Motivation: 野生动物非法贸易对生态和公共健康造成严重影响，而在线平台为打击此类活动提供了新的数据来源。然而，传统数据标注方法成本高且效率低，阻碍了相关研究的进展。

Method: 提出一种策略，利用LLMs为少量数据生成伪标签，并基于这些标签训练专用分类器。通过自动采集多样化和代表性样本，降低标注成本。

Result: 实验结果表明，该方法构建的分类器F1分数高达95%，性能优于直接使用LLMs，且成本更低。

Conclusion: 该方法为野生动物非法贸易分析提供了一种高效、低成本的解决方案，能够支持多样化的研究需求。

Abstract: Wildlife trafficking remains a critical global issue, significantly impacting
biodiversity, ecological stability, and public health. Despite efforts to
combat this illicit trade, the rise of e-commerce platforms has made it easier
to sell wildlife products, putting new pressure on wild populations of
endangered and threatened species. The use of these platforms also opens a new
opportunity: as criminals sell wildlife products online, they leave digital
traces of their activity that can provide insights into trafficking activities
as well as how they can be disrupted. The challenge lies in finding these
traces. Online marketplaces publish ads for a plethora of products, and
identifying ads for wildlife-related products is like finding a needle in a
haystack. Learning classifiers can automate ad identification, but creating
them requires costly, time-consuming data labeling that hinders support for
diverse ads and research questions. This paper addresses a critical challenge
in the data science pipeline for wildlife trafficking analytics: generating
quality labeled data for classifiers that select relevant data. While large
language models (LLMs) can directly label advertisements, doing so at scale is
prohibitively expensive. We propose a cost-effective strategy that leverages
LLMs to generate pseudo labels for a small sample of the data and uses these
labels to create specialized classification models. Our novel method
automatically gathers diverse and representative samples to be labeled while
minimizing the labeling costs. Our experimental evaluation shows that our
classifiers achieve up to 95% F1 score, outperforming LLMs at a lower cost. We
present real use cases that demonstrate the effectiveness of our approach in
enabling analyses of different aspects of wildlife trafficking.

</details>


### [83] [ABG-NAS: Adaptive Bayesian Genetic Neural Architecture Search for Graph Representation Learning](https://arxiv.org/abs/2504.21254)
*Sixuan Wang,Jiao Yin,Jinli Cao,MingJian Tang,Hua Wang,Yanchun Zhang*

Main category: cs.LG

TL;DR: ABG-NAS是一种自动化图神经网络架构搜索框架，旨在高效学习图表示，通过三个关键组件（CASS、AGOS、BGTM）优化架构搜索和超参数调整，在多个基准数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有图神经网络（GNN）架构难以适应多样化和复杂的图结构，限制了其表示学习的鲁棒性和泛化能力。

Method: ABG-NAS包含三个组件：1) CASS探索传播和变换操作；2) AGOS动态平衡探索与利用；3) BGTM周期性优化超参数。

Result: 在Cora、PubMed等数据集上，ABG-NAS优于手动设计的GNN和现有NAS方法。

Conclusion: ABG-NAS为图表示学习提供了可扩展和自适应的解决方案，具有潜力推动该领域发展。

Abstract: Effective and efficient graph representation learning is essential for
enabling critical downstream tasks, such as node classification, link
prediction, and subgraph search. However, existing graph neural network (GNN)
architectures often struggle to adapt to diverse and complex graph structures,
limiting their ability to provide robust and generalizable representations. To
address this challenge, we propose ABG-NAS, a novel framework for automated
graph neural network architecture search tailored for efficient graph
representation learning. ABG-NAS encompasses three key components: a
Comprehensive Architecture Search Space (CASS), an Adaptive Genetic
Optimization Strategy (AGOS), and a Bayesian-Guided Tuning Module (BGTM). CASS
systematically explores diverse propagation (P) and transformation (T)
operations, enabling the discovery of GNN architectures capable of capturing
intricate graph characteristics. AGOS dynamically balances exploration and
exploitation, ensuring search efficiency and preserving solution diversity.
BGTM further optimizes hyperparameters periodically, enhancing the scalability
and robustness of the resulting architectures. Empirical evaluations on
benchmark datasets (Cora, PubMed, Citeseer, and CoraFull) demonstrate that
ABG-NAS consistently outperforms both manually designed GNNs and
state-of-the-art neural architecture search (NAS) methods. These results
highlight the potential of ABG-NAS to advance graph representation learning by
providing scalable and adaptive solutions for diverse graph structures. Our
code is publicly available at https://github.com/sserranw/ABG-NAS.

</details>


### [84] [Multi-Domain Causal Discovery in Bijective Causal Models](https://arxiv.org/abs/2504.21261)
*Kasra Jalaldoust,Saber Salehkaleybar,Negar Kiyavash*

Main category: cs.LG

TL;DR: 论文研究了多域环境下的因果发现问题，提出了一种基于双射生成机制（BGM）的方法，放宽了功能假设的限制，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 在多域环境中，因果函数的跨域不变性与噪声分布的可变性为因果发现提供了新的机会。本文旨在通过更宽松的功能假设实现因果图的发现。

Method: 采用双射生成机制（BGM），确保外生噪声与内生变量之间的函数关系在因果变量的每一层级上都是双射且可微的。BGM推广了多种模型，并提出了统计测试以确定目标变量的父集。

Result: 实验表明，BGM方法在合成和真实数据集上均能有效发现因果图，验证了理论结果的正确性。

Conclusion: BGM为多域环境下的因果发现提供了一种通用且有效的方法，放宽了功能假设的限制，具有广泛的应用潜力。

Abstract: We consider the problem of causal discovery (a.k.a., causal structure
learning) in a multi-domain setting. We assume that the causal functions are
invariant across the domains, while the distribution of the exogenous noise may
vary. Under causal sufficiency (i.e., no confounders exist), we show that the
causal diagram can be discovered under less restrictive functional assumptions
compared to previous work. What enables causal discovery in this setting is
bijective generation mechanisms (BGM), which ensures that the functional
relation between the exogenous noise $E$ and the endogenous variable $Y$ is
bijective and differentiable in both directions at every level of the cause
variable $X = x$. BGM generalizes a variety of models including additive noise
model, LiNGAM, post-nonlinear model, and location-scale noise model. Further,
we derive a statistical test to find the parents set of the target variable.
Experiments on various synthetic and real-world datasets validate our
theoretical findings.

</details>


### [85] [Orthogonal Factor-Based Biclustering Algorithm (BCBOF) for High-Dimensional Data and Its Application in Stock Trend Prediction](https://arxiv.org/abs/2504.21289)
*Yan Huang,Da-Qing Zhang*

Main category: cs.LG

TL;DR: 提出了一种基于正交因子的双聚类算法（BCBOF），用于解决高维数据中的稀疏性和局部结构破坏问题，并将其应用于股票趋势预测，表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统双聚类算法在处理高维数据时面临距离集中和局部结构破坏的问题，限制了其有效性。

Method: 通过构建高维数据集的正交因子，在正交子空间中进行聚类，并将结果转化为模糊规则用于股票预测。

Result: BCBOF算法在多项评估指标上优于现有方法，虚拟交易实验显示其生成的交易策略能带来更高收益。

Conclusion: BCBOF算法有效解决了高维数据双聚类问题，并在股票预测中表现出实用价值。

Abstract: Biclustering is an effective technique in data mining and pattern
recognition. Biclustering algorithms based on traditional clustering face two
fundamental limitations when processing high-dimensional data: (1) The distance
concentration phenomenon in high-dimensional spaces leads to data sparsity,
rendering similarity measures ineffective; (2) Mainstream linear dimensionality
reduction methods disrupt critical local structural patterns. To apply
biclustering to high-dimensional datasets, we propose an orthogonal
factor-based biclustering algorithm (BCBOF). First, we constructed orthogonal
factors in the vector space of the high-dimensional dataset. Then, we performed
clustering using the coordinates of the original data in the orthogonal
subspace as clustering targets. Finally, we obtained biclustering results of
the original dataset. Since dimensionality reduction was applied before
clustering, the proposed algorithm effectively mitigated the data sparsity
problem caused by high dimensionality. Additionally, we applied this
biclustering algorithm to stock technical indicator combinations and stock
price trend prediction. Biclustering results were transformed into fuzzy rules,
and we incorporated profit-preserving and stop-loss rules into the rule set,
ultimately forming a fuzzy inference system for stock price trend predictions
and trading signals. To evaluate the performance of BCBOF, we compared it with
existing biclustering methods using multiple evaluation metrics. The results
showed that our algorithm outperformed other biclustering techniques. To
validate the effectiveness of the fuzzy inference system, we conducted virtual
trading experiments using historical data from 10 A-share stocks. The
experimental results showed that the generated trading strategies yielded
higher returns for investors.

</details>


### [86] [Fairness in Graph Learning Augmented with Machine Learning: A Survey](https://arxiv.org/abs/2504.21296)
*Renqiang Luo,Ziqi Xu,Xikun Zhang,Qing Qing,Huafei Huang,Enyan Dai,Zhe Wang,Bo Yang*

Main category: cs.LG

TL;DR: 本文探讨了将机器学习技术融入传统图学习模型时带来的公平性挑战，并分析了其在高风险应用中的潜在歧视性结果。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于揭示图学习与机器学习结合（GL-ML）时对公平性的复杂影响，尤其是在高风险领域如推荐系统、灾难响应等。

Method: 系统性地分析了GL-ML中的公平性挑战，并探讨了四种常用技术以提高公平性。

Result: 研究强调了图学习机制与机器学习技术的相互作用，以及其对公平性的双重影响。

Conclusion: 本文为GL-ML公平性研究奠定了坚实基础，为未来创新提供了方向。

Abstract: Augmenting specialised machine learning techniques into traditional graph
learning models has achieved notable success across various domains, including
federated graph learning, dynamic graph learning, and graph transformers.
However, the intricate mechanisms of these specialised techniques introduce
significant challenges in maintaining model fairness, potentially resulting in
discriminatory outcomes in high-stakes applications such as recommendation
systems, disaster response, criminal justice, and loan approval. This paper
systematically examines the unique fairness challenges posed by Graph Learning
augmented with Machine Learning (GL-ML). It highlights the complex interplay
between graph learning mechanisms and machine learning techniques, emphasising
how the augmentation of machine learning both enhances and complicates
fairness. Additionally, we explore four critical techniques frequently employed
to improve fairness in GL-ML methods. By thoroughly investigating the root
causes and broader implications of fairness challenges in this rapidly evolving
field, this work establishes a robust foundation for future research and
innovation in GL-ML fairness.

</details>


### [87] [Unsupervised Feature Transformation via In-context Generation, Generator-critic LLM Agents, and Duet-play Teaming](https://arxiv.org/abs/2504.21304)
*Nanxu Gong,Xinyuan Wang,Wangyang Ying,Haoyue Bai,Sixun Dong,Haifeng Chen,Yanjie Fu*

Main category: cs.LG

TL;DR: 提出了一种基于生成器-批评者框架的无监督特征转换方法，利用LLM代理和上下文学习提升特征转换效率。


<details>
  <summary>Details</summary>
Motivation: 在材料性能筛选等领域，数据维度高且标注成本高，需要高效的无监督特征转换方法。现有方法无法高效处理大规模特征组合且多为监督学习。

Method: 采用生成器-批评者双代理框架，通过批评者诊断数据生成建议，生成器基于建议生成特征转换，并通过迭代反馈优化。

Result: 实验表明，该方法在特征转换效率、鲁棒性和实用性上优于监督基线。

Conclusion: 该框架为无监督特征转换提供了高效解决方案，并可扩展至人机协作生成。

Abstract: Feature transformation involves generating a new set of features from the
original dataset to enhance the data's utility. In certain domains like
material performance screening, dimensionality is large and collecting labels
is expensive and lengthy. It highly necessitates transforming feature spaces
efficiently and without supervision to enhance data readiness and AI utility.
However, existing methods fall short in efficient navigation of a vast space of
feature combinations, and are mostly designed for supervised settings. To fill
this gap, our unique perspective is to leverage a generator-critic duet-play
teaming framework using LLM agents and in-context learning to derive
pseudo-supervision from unsupervised data. The framework consists of three
interconnected steps: (1) Critic agent diagnoses data to generate actionable
advice, (2) Generator agent produces tokenized feature transformations guided
by the critic's advice, and (3) Iterative refinement ensures continuous
improvement through feedback between agents. The generator-critic framework can
be generalized to human-agent collaborative generation, by replacing the critic
agent with human experts. Extensive experiments demonstrate that the proposed
framework outperforms even supervised baselines in feature transformation
efficiency, robustness, and practical applicability across diverse datasets.

</details>


### [88] [Capturing Conditional Dependence via Auto-regressive Diffusion Models](https://arxiv.org/abs/2504.21314)
*Xunpeng Huang,Yujin Han,Difan Zou,Yian Ma,Tong Zhang*

Main category: cs.LG

TL;DR: 本文研究了扩散模型在捕捉数据条件依赖结构方面的不足，提出了自回归（AR）扩散模型，并提供了理论支持。实验表明，AR扩散模型在存在明显条件依赖结构时表现优于传统扩散模型。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像和视频生成中表现优异，但难以捕捉现实世界中的高层次关系（如物理规律或物体稳定性）。这源于其未能充分捕捉数据的条件依赖结构。

Method: 研究采用自回归（AR）扩散模型，并首次在温和数据假设下提出其采样误差的理论结果。

Result: 理论分析表明，AR扩散模型在逼近数据条件分布时的误差更小，且推理时间仅略高于传统扩散模型。实验证明，AR模型在数据存在条件依赖结构时表现更优。

Conclusion: AR扩散模型在捕捉条件依赖结构方面优于传统扩散模型，同时保持实用性，适用于大规模应用。

Abstract: Diffusion models have demonstrated appealing performance in both image and
video generation. However, many works discover that they struggle to capture
important, high-level relationships that are present in the real world. For
example, they fail to learn physical laws from data, and even fail to
understand that the objects in the world exist in a stable fashion. This is due
to the fact that important conditional dependence structures are not adequately
captured in the vanilla diffusion models. In this work, we initiate an in-depth
study on strengthening the diffusion model to capture the conditional
dependence structures in the data. In particular, we examine the efficacy of
the auto-regressive (AR) diffusion models for such purpose and develop the
first theoretical results on the sampling error of AR diffusion models under
(possibly) the mildest data assumption. Our theoretical findings indicate that,
compared with typical diffusion models, the AR variant produces samples with a
reduced gap in approximating the data conditional distribution. On the other
hand, the overall inference time of the AR-diffusion models is only moderately
larger than that for the vanilla diffusion models, making them still practical
for large scale applications. We also provide empirical results showing that
when there is clear conditional dependence structure in the data, the AR
diffusion models captures such structure, whereas vanilla DDPM fails to do so.
On the other hand, when there is no obvious conditional dependence across
patches of the data, AR diffusion does not outperform DDPM.

</details>


### [89] [Q-function Decomposition with Intervention Semantics with Factored Action Spaces](https://arxiv.org/abs/2504.21326)
*Junkyu Lee,Tian Gao,Elliot Nelson,Miao Liu,Debarun Bhattacharjya,Songtao Lu*

Main category: cs.LG

TL;DR: 论文提出了一种基于因果统计的动作分解强化学习方法，通过将Q函数定义在低维投影子空间上，避免了高维动作空间的组合问题，提高了样本效率。


<details>
  <summary>Details</summary>
Motivation: 解决高维离散动作空间带来的组合挑战，避免枚举所有动作组合。

Method: 利用因果统计中的无未观测混杂因子设定，将Q函数分解到低维投影子空间，提出动作分解强化学习框架。

Result: 在模型强化学习环境中降低了样本复杂度，并在在线连续控制环境和离线脓毒症治疗环境中优于现有基线方法。

Conclusion: 动作分解强化学习方法有效提高了样本效率，适用于高维动作空间的强化学习任务。

Abstract: Many practical reinforcement learning environments have a discrete factored
action space that induces a large combinatorial set of actions, thereby posing
significant challenges. Existing approaches leverage the regular structure of
the action space and resort to a linear decomposition of Q-functions, which
avoids enumerating all combinations of factored actions. In this paper, we
consider Q-functions defined over a lower dimensional projected subspace of the
original action space, and study the condition for the unbiasedness of
decomposed Q-functions using causal effect estimation from the no unobserved
confounder setting in causal statistics. This leads to a general scheme which
we call action decomposed reinforcement learning that uses the projected
Q-functions to approximate the Q-function in standard model-free reinforcement
learning algorithms. The proposed approach is shown to improve sample
complexity in a model-based reinforcement learning setting. We demonstrate
improvements in sample efficiency compared to state-of-the-art baselines in
online continuous control environments and a real-world offline sepsis
treatment environment.

</details>


### [90] [A Generalized Meta Federated Learning Framework with Theoretical Convergence Guarantees](https://arxiv.org/abs/2504.21327)
*Mohammad Vahid Jamali,Hamid Saber,Jung Hyun Bae*

Main category: cs.LG

TL;DR: 论文提出了一种广义的元联邦学习框架，通过最小化代理在任意数量微调步骤后的本地模型损失，改进了传统方法。实验表明其具有更高的准确性和更快的收敛速度。


<details>
  <summary>Details</summary>
Motivation: 传统元联邦学习方法仅优化代理在一步微调后的损失，但在数据分布高度异构时，代理可能需要多步微调。本文旨在解决这一问题。

Method: 提出广义框架，最小化代理在任意ν步微调后的损失，并设计了一种改进的FedAvg算法，进行了理论收敛分析。

Result: 实验证明，新方法在真实数据集上表现出更高的准确性和更快的收敛速度。

Conclusion: 广义框架和算法在异构数据下表现优越，为元联邦学习提供了更灵活和高效的方法。

Abstract: Meta federated learning (FL) is a personalized variant of FL, where multiple
agents collaborate on training an initial shared model without exchanging raw
data samples. The initial model should be trained in a way that current or new
agents can easily adapt it to their local datasets after one or a few
fine-tuning steps, thus improving the model personalization. Conventional meta
FL approaches minimize the average loss of agents on the local models obtained
after one step of fine-tuning. In practice, agents may need to apply several
fine-tuning steps to adapt the global model to their local data, especially
under highly heterogeneous data distributions across agents. To this end, we
present a generalized framework for the meta FL by minimizing the average loss
of agents on their local model after any arbitrary number $\nu$ of fine-tuning
steps. For this generalized framework, we present a variant of the well-known
federated averaging (FedAvg) algorithm and conduct a comprehensive theoretical
convergence analysis to characterize the convergence speed as well as behavior
of the meta loss functions in both the exact and approximated cases. Our
experiments on real-world datasets demonstrate superior accuracy and faster
convergence for the proposed scheme compared to conventional approaches.

</details>


### [91] [Multi-level datasets training method in Physics-Informed Neural Networks](https://arxiv.org/abs/2504.21328)
*Yao-Hsuan Tsai,Hsiao-Tung Juan,Pao-Hsiung Chiu,Chao-An Lin*

Main category: cs.LG

TL;DR: 论文提出了一种基于多网格方法的改进PINNs框架，用于解决高频率和刚性PDE问题，显著提升了训练精度和收敛性。


<details>
  <summary>Details</summary>
Motivation: 尽管PINNs在解决PDE问题中表现出色，但在处理高频率或刚性问题时存在精度和收敛性问题。本研究旨在通过多网格方法改进这些问题。

Method: 采用多网格训练策略，通过不同级别的训练样本有效去除频率误差，避免了对神经网络结构、损失权重和超参数的繁琐调优。

Result: 在1D ODE和2D对流-扩散方程中验证了方法的有效性，并在Lid-driven腔流问题中实现了30%至60%的精度提升，甚至适用于Re=5000的复杂问题。

Conclusion: 该方法显著提升了PINNs在高频率和刚性PDE问题中的性能，展示了与迁移学习结合的潜力。

Abstract: Physics-Informed Neural Networks have emerged as a promising methodology for
solving PDEs, gaining significant attention in computer science and various
physics-related fields. Despite being demonstrated the ability to incorporate
the physics of laws for versatile applications, PINNs still struggle with the
challenging problems which are stiff to be solved and/or have high-frequency
components in the solutions, resulting in accuracy and convergence issues. It
may not only increase computational costs, but also lead to accuracy loss or
solution divergence. In this study, an alternative approach is proposed to
mitigate the above-mentioned problems. Inspired by the multi-grid method in CFD
community, the underlying idea of the current approach is to efficiently remove
different frequency errors via training with different levels of training
samples, resulting in a simpler way to improve the training accuracy without
spending time in fine-tuning of neural network structures, loss weights as well
as hyperparameters. To demonstrate the efficacy of current approach, we first
investigate canonical 1D ODE with high-frequency component and 2D
convection-diffusion equation with V-cycle training strategy. Finally, the
current method is employed for the classical benchmark problem of steady
Lid-driven cavity flows at different Reynolds numbers, to investigate the
applicability and efficacy for the problem involved multiple modes of high and
low frequency. By virtue of various training sequence modes, improvement
through predictions lead to 30% to 60% accuracy improvement. We also
investigate the synergies between current method and transfer learning
techniques for more challenging problems (i.e., higher Re). From the present
results, it also revealed that the current framework can produce good
predictions even for the case of Re=5000, demonstrating the ability to solve
complex high-frequency PDEs.

</details>


### [92] [Generative QoE Modeling: A Lightweight Approach for Telecom Networks](https://arxiv.org/abs/2504.21353)
*Vinti Nayar,Kanica Sachdev,Brejesh Lall*

Main category: cs.LG

TL;DR: 论文提出了一种轻量级的生成建模框架（VQ-HMM），用于QoE预测，平衡了计算效率、可解释性和预测准确性。


<details>
  <summary>Details</summary>
Motivation: 优化资源管理并提升用户满意度，同时解决现有深度学习方法在计算资源和延迟方面的限制。

Method: 使用向量量化（VQ）预处理技术将连续特征转换为离散符号，并结合隐马尔可夫模型（HMM）进行时序建模。

Result: 实验表明，该方法在公开数据集上表现良好，适用于实时和资源受限环境。

Conclusion: 该框架为复杂深度学习方法提供了可扩展的替代方案，特别适用于计算资源有限或延迟敏感的场景。

Abstract: Quality of Experience (QoE) prediction plays a crucial role in optimizing
resource management and enhancing user satisfaction across both
telecommunication and OTT services. While recent advances predominantly rely on
deep learning models, this study introduces a lightweight generative modeling
framework that balances computational efficiency, interpretability, and
predictive accuracy. By validating the use of Vector Quantization (VQ) as a
preprocessing technique, continuous network features are effectively
transformed into discrete categorical symbols, enabling integration with a
Hidden Markov Model (HMM) for temporal sequence modeling. This VQ-HMM pipeline
enhances the model's capacity to capture dynamic QoE patterns while supporting
probabilistic inference on new and unseen data. Experimental results on
publicly available time-series datasets incorporating both objective indicators
and subjective QoE scores demonstrate the viability of this approach in
real-time and resource-constrained environments, where inference latency is
also critical. The framework offers a scalable alternative to complex deep
learning methods, particularly in scenarios with limited computational
resources or where latency constraints are critical.

</details>


### [93] [A comparative study of deep learning and ensemble learning to extend the horizon of traffic forecasting](https://arxiv.org/abs/2504.21358)
*Xiao Zheng,Saeed Asadi Bagloee,Majid Sarvi*

Main category: cs.LG

TL;DR: 本文比较了多种机器学习方法在长期交通流量预测（长达30天）中的表现，发现时间嵌入技术对周期性建模至关重要，XGBoost在仅使用时间特征时表现优异。


<details>
  <summary>Details</summary>
Motivation: 长期交通预测是一个具有挑战性的开放性问题，现有研究多关注短期预测，本文旨在填补这一空白。

Method: 开发了XGBoost和多种深度学习方法（如RNN和Transformer），并利用时间嵌入技术增强模型对季节性和事件因素的理解。

Result: 实验表明，随着预测时间延长，周期性建模比时间依赖性捕获更为关键；时间嵌入使简单RNN在30天预测中优于Informer 31.1%。XGBoost表现与深度学习方法相当。

Conclusion: 研究为长期交通预测提供了重要见解，并指出时间嵌入和XGBoost的潜力。

Abstract: Traffic forecasting is vital for Intelligent Transportation Systems, for
which Machine Learning (ML) methods have been extensively explored to develop
data-driven Artificial Intelligence (AI) solutions. Recent research focuses on
modelling spatial-temporal correlations for short-term traffic prediction,
leaving the favourable long-term forecasting a challenging and open issue. This
paper presents a comparative study on large-scale real-world signalized
arterials and freeway traffic flow datasets, aiming to evaluate promising ML
methods in the context of large forecasting horizons up to 30 days. Focusing on
modelling capacity for temporal dynamics, we develop one ensemble ML method,
eXtreme Gradient Boosting (XGBoost), and a range of Deep Learning (DL) methods,
including Recurrent Neural Network (RNN)-based methods and the state-of-the-art
Transformer-based method. Time embedding is leveraged to enhance their
understanding of seasonality and event factors. Experimental results highlight
that while the attention mechanism/Transformer framework is effective for
capturing long-range dependencies in sequential data, as the forecasting
horizon extends, the key to effective traffic forecasting gradually shifts from
temporal dependency capturing to periodicity modelling. Time embedding is
particularly effective in this context, helping naive RNN outperform Informer
by 31.1% for 30-day-ahead forecasting. Meanwhile, as an efficient and robust
model, XGBoost, while learning solely from time features, performs
competitively with DL methods. Moreover, we investigate the impacts of various
factors like input sequence length, holiday traffic, data granularity, and
training data size. The findings offer valuable insights and serve as a
reference for future long-term traffic forecasting research and the improvement
of AI's corresponding learning capabilities.

</details>


### [94] [Synergy-CLIP: Extending CLIP with Multi-modal Integration for Robust Representation Learning](https://arxiv.org/abs/2504.21375)
*Sangyeon Cho,Jangyeong Jeon,Mingi Kim,Junyeong Kim*

Main category: cs.LG

TL;DR: Synergy-CLIP扩展了CLIP架构，通过整合视觉、文本和音频模态来增强多模态表示学习，并引入VGG-sound+数据集解决数据平衡问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注双模态交互，未能充分利用多模态数据的丰富性，且缺乏平衡的大规模数据集。

Method: 提出Synergy-CLIP框架，平等对齐三种模态的潜在信息，并构建VGG-sound+数据集。

Result: 在零样本分类等任务中表现优于基线，并能通过缺失模态重建任务提取模态间的协同作用。

Conclusion: Synergy-CLIP为多模态表示学习提供了坚实基础，并开辟了新的研究方向。

Abstract: Multi-modal representation learning has become a pivotal area in artificial
intelligence, enabling the integration of diverse modalities such as vision,
text, and audio to solve complex problems. However, existing approaches
predominantly focus on bimodal interactions, such as image-text pairs, which
limits their ability to fully exploit the richness of multi-modal data.
Furthermore, the integration of modalities in equal-scale environments remains
underexplored due to the challenges of constructing large-scale, balanced
datasets. In this study, we propose Synergy-CLIP, a novel framework that
extends the contrastive language-image pre-training (CLIP) architecture to
enhance multi-modal representation learning by integrating visual, textual, and
audio modalities. Unlike existing methods that focus on adapting individual
modalities to vanilla-CLIP, Synergy-CLIP aligns and captures latent information
across three modalities equally. To address the high cost of constructing
large-scale multi-modal datasets, we introduce VGG-sound+, a triple-modal
dataset designed to provide equal-scale representation of visual, textual, and
audio data. Synergy-CLIP is validated on various downstream tasks, including
zero-shot classification, where it outperforms existing baselines.
Additionally, we introduce a missing modality reconstruction task,
demonstrating Synergy-CLIP's ability to extract synergy among modalities in
realistic application scenarios. These contributions provide a robust
foundation for advancing multi-modal representation learning and exploring new
research directions.

</details>


### [95] [Sparse-to-Sparse Training of Diffusion Models](https://arxiv.org/abs/2504.21380)
*Inês Cardoso Oliveira,Decebal Constantin Mocanu,Luis A. Leiva*

Main category: cs.LG

TL;DR: 本文提出了一种稀疏到稀疏训练范式，用于扩散模型（DMs），旨在提高训练和推理效率。实验表明，稀疏DMs在减少参数和计算量的同时，性能不逊于甚至优于密集模型。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在生成任务中表现优异，但计算资源需求高。此前工作主要关注推理效率，本文首次探索稀疏训练以同时优化训练和推理效率。

Method: 采用三种方法（Static-DM、RigL-DM、MagRan-DM）对六种数据集进行无条件生成训练，研究稀疏性对模型性能的影响。

Result: 稀疏DMs在减少参数和FLOPs的同时，性能与密集模型相当或更优，并确定了稀疏训练的安全有效值。

Conclusion: 稀疏到稀疏训练是扩散模型的高效替代方案，显著降低资源需求且不影响性能。

Abstract: Diffusion models (DMs) are a powerful type of generative models that have
achieved state-of-the-art results in various image synthesis tasks and have
shown potential in other domains, such as natural language processing and
temporal data modeling. Despite their stable training dynamics and ability to
produce diverse high-quality samples, DMs are notorious for requiring
significant computational resources, both in the training and inference stages.
Previous work has focused mostly on increasing the efficiency of model
inference. This paper introduces, for the first time, the paradigm of
sparse-to-sparse training to DMs, with the aim of improving both training and
inference efficiency. We focus on unconditional generation and train sparse DMs
from scratch (Latent Diffusion and ChiroDiff) on six datasets using three
different methods (Static-DM, RigL-DM, and MagRan-DM) to study the effect of
sparsity in model performance. Our experiments show that sparse DMs are able to
match and often outperform their Dense counterparts, while substantially
reducing the number of trainable parameters and FLOPs. We also identify safe
and effective values to perform sparse-to-sparse training of DMs.

</details>


### [96] [FAST-Q: Fast-track Exploration with Adversarially Balanced State Representations for Counterfactual Action Estimation in Offline Reinforcement Learning](https://arxiv.org/abs/2504.21383)
*Pulkit Agrawal,Rukma Talwadker,Aditya Pareek,Tridib Mukherjee*

Main category: cs.LG

TL;DR: FAST-Q提出了一种新的离线强化学习方法，通过梯度反转学习和Q值分解策略，解决了推荐系统中玩家心理和状态稀疏性问题，显著提升了玩家收益和平台表现。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习在处理高风险的推荐系统（如在线游戏）时，面临玩家心理和状态稀疏性的挑战，现有方法因泛化能力不足而限制了学习效果。

Method: FAST-Q采用梯度反转学习构建平衡状态表示，支持离线反事实探索，并提出Q值分解策略以实现多目标优化。

Result: 实验表明，FAST-Q在玩家收益、平台停留时间和推荐成本等方面均优于现有方法，具体提升了0.15%的玩家收益、2%的LTV等。

Conclusion: FAST-Q通过创新的方法解决了离线强化学习在推荐系统中的关键问题，显著提升了性能和经济效益。

Abstract: Recent advancements in state-of-the-art (SOTA) offline reinforcement learning
(RL) have primarily focused on addressing function approximation errors, which
contribute to the overestimation of Q-values for out-of-distribution actions, a
challenge that static datasets exacerbate. However, high stakes applications
such as recommendation systems in online gaming, introduce further complexities
due to player's psychology (intent) driven by gameplay experiences and the
inherent volatility on the platform. These factors create highly sparse,
partially overlapping state spaces across policies, further influenced by the
experiment path selection logic which biases state spaces towards specific
policies. Current SOTA methods constrain learning from such offline data by
clipping known counterfactual actions as out-of-distribution due to poor
generalization across unobserved states. Further aggravating conservative
Q-learning and necessitating more online exploration. FAST-Q introduces a novel
approach that (1) leverages Gradient Reversal Learning to construct balanced
state representations, regularizing the policy-specific bias between the
player's state and action thereby enabling counterfactual estimation; (2)
supports offline counterfactual exploration in parallel with static data
exploitation; and (3) proposes a Q-value decomposition strategy for
multi-objective optimization, facilitating explainable recommendations over
short and long-term objectives. These innovations demonstrate superiority of
FAST-Q over prior SOTA approaches and demonstrates at least 0.15 percent
increase in player returns, 2 percent improvement in lifetime value (LTV), 0.4
percent enhancement in the recommendation driven engagement, 2 percent
improvement in the player's platform dwell time and an impressive 10 percent
reduction in the costs associated with the recommendation, on our volatile
gaming platform.

</details>


### [97] [Enhanced Semi-Supervised Stamping Process Monitoring with Physically-Informed Feature Extraction](https://arxiv.org/abs/2504.21389)
*Jianyu Zhang,Jianshe Feng,Yizhang Zhu,Fanyu Qi*

Main category: cs.LG

TL;DR: 提出了一种半监督的冲压过程异常监测框架，结合加速度计信号和物理信息，有效捕捉异常，减少批量缺陷风险。


<details>
  <summary>Details</summary>
Motivation: 解决冲压过程中频繁出现的异常问题，提高生产效率和产品质量。

Method: 提出混合特征提取算法结合数据驱动和物理机制，建立半监督异常检测模型，仅用正常样本构建基线模型，并提出新的偏差评分量化异常程度。

Result: 验证了特征提取方法的有效性，并在实际冲压车间数据中展示了框架的优越性。

Conclusion: 该框架能有效监测冲压过程异常，提升生产良率。

Abstract: In tackling frequent anomalies in stamping processes, this study introduces a
novel semi-supervised in-process anomaly monitoring framework, utilizing
accelerometer signals and physics information, to capture the process anomaly
effectively. The proposed framework facilitates the construction of a
monitoring model with imbalanced sample distribution, which enables in-process
condition monitoring in real-time to prevent batch anomalies, which helps to
reduce batch defects risk and enhance production yield. Firstly, to effectively
capture key features from raw data containing redundant information, a hybrid
feature extraction algorithm is proposed to utilize data-driven methods and
physical mechanisms simultaneously. Secondly, to address the challenge brought
by imbalanced sample distribution, a semi-supervised anomaly detection model is
established, which merely employs normal samples to build a golden baseline
model, and a novel deviation score is proposed to quantify the anomaly level of
each online stamping stroke. The effectiveness of the proposed feature
extraction method is validated with various classification algorithms. A
real-world in-process dataset from stamping manufacturing workshop is employed
to illustrate the superiority of proposed semi-supervised framework with
enhance performance for process anomaly monitoring.

</details>


### [98] [MPEC: Manifold-Preserved EEG Classification via an Ensemble of Clustering-Based Classifiers](https://arxiv.org/abs/2504.21427)
*Shermin Shahbazi,Mohammad-Reza Nasiri,Majid Ramezani*

Main category: cs.LG

TL;DR: 提出了一种名为MPEC的新方法，通过保留EEG数据的流形结构，结合协方差矩阵和RBF核的特征工程，以及改进的K-means聚类算法，显著提升了EEG信号分类的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有EEG信号分类方法未能充分考虑数据的非欧几里得流形结构，导致性能不佳，因此需要一种能够保留流形信息的新方法。

Method: MPEC方法包括两个关键创新：1) 结合协方差矩阵和RBF核的特征工程；2) 在黎曼流形空间中使用改进的K-means算法进行聚类。

Result: 在BCI Competition IV数据集2a上验证了MPEC的优越性能，取得了显著改进。

Conclusion: MPEC通过保留EEG数据的流形结构，显著提升了分类性能，为脑机接口和神经假体应用提供了更准确的分类方法。

Abstract: Accurate classification of EEG signals is crucial for brain-computer
interfaces (BCIs) and neuroprosthetic applications, yet many existing methods
fail to account for the non-Euclidean, manifold structure of EEG data,
resulting in suboptimal performance. Preserving this manifold information is
essential to capture the true geometry of EEG signals, but traditional
classification techniques largely overlook this need. To this end, we propose
MPEC (Manifold-Preserved EEG Classification via an Ensemble of Clustering-Based
Classifiers), that introduces two key innovations: (1) a feature engineering
phase that combines covariance matrices and Radial Basis Function (RBF) kernels
to capture both linear and non-linear relationships among EEG channels, and (2)
a clustering phase that employs a modified K-means algorithm tailored for the
Riemannian manifold space, ensuring local geometric sensitivity. Ensembling
multiple clustering-based classifiers, MPEC achieves superior results,
validated by significant improvements on the BCI Competition IV dataset 2a.

</details>


### [99] [Whispers of Data: Unveiling Label Distributions in Federated Learning Through Virtual Client Simulation](https://arxiv.org/abs/2504.21436)
*Zhixuan Ma,Haichang Gao,Junxiang Huang,Ping Wang*

Main category: cs.LG

TL;DR: 提出了一种新颖的标签分布推断攻击方法，适用于多种场景，且在差分隐私防御机制下仍有效。


<details>
  <summary>Details</summary>
Motivation: 联邦学习虽能保护数据隐私，但仍易受标签推断攻击，现有方法对特定设置敏感且防御效果不佳。

Method: 通过估计受害者客户端数据集大小，构建虚拟客户端，量化标签的时间泛化性，并训练推断模型预测标签分布。

Result: 在多个数据集上验证了方法的优越性，且在差分隐私防御下仍有效。

Conclusion: 该方法在现实应用中具有潜力，尤其在防御机制下表现稳定。

Abstract: Federated Learning enables collaborative training of a global model across
multiple geographically dispersed clients without the need for data sharing.
However, it is susceptible to inference attacks, particularly label inference
attacks.
  Existing studies on label distribution inference exhibits sensitive to the
specific settings of the victim client and typically underperforms under
defensive strategies. In this study, we propose a novel label distribution
inference attack that is stable and adaptable to various scenarios.
Specifically, we estimate the size of the victim client's dataset and construct
several virtual clients tailored to the victim client. We then quantify the
temporal generalization of each class label for the virtual clients and utilize
the variation in temporal generalization to train an inference model that
predicts the label distribution proportions of the victim client.
  We validate our approach on multiple datasets, including MNIST,
Fashion-MNIST, FER2013, and AG-News. The results demonstrate the superiority of
our method compared to state-of-the-art techniques. Furthermore, our attack
remains effective even under differential privacy defense mechanisms,
underscoring its potential for real-world applications.

</details>


### [100] [xEEGNet: Towards Explainable AI in EEG Dementia Classification](https://arxiv.org/abs/2504.21457)
*Andrea Zanola,Louis Fabrice Tshimanga,Federico Del Pup,Marco Baiesi,Manfredo Atzori*

Main category: cs.LG

TL;DR: xEEGNet是一种新型、紧凑且可解释的神经网络，用于EEG数据分析，专注于阿尔茨海默病和额颞叶痴呆的分类，参数少且不易过拟合。


<details>
  <summary>Details</summary>
Motivation: 解决传统深度学习模型在EEG数据分析中的“黑箱”问题，同时减少过拟合并保持性能。

Method: 通过分析并逐步改进ShallowNet，设计出xEEGNet，使用Nested-Leave-N-Subjects-Out交叉验证评估性能。

Result: xEEGNet仅用168个参数，性能接近ShallowNet（-1.5%），且能解释EEG频段特征。

Conclusion: 小型架构如xEEGNet在EEG病理分类中同样有效，兼具性能和可解释性。

Abstract: This work presents xEEGNet, a novel, compact, and explainable neural network
for EEG data analysis. It is fully interpretable and reduces overfitting
through major parameter reduction. As an applicative use case, we focused on
classifying common dementia conditions, Alzheimer's and frontotemporal
dementia, versus controls. xEEGNet is broadly applicable to other neurological
conditions involving spectral alterations. We initially used ShallowNet, a
simple and popular model from the EEGNet-family. Its structure was analyzed and
gradually modified to move from a "black box" to a more transparent model,
without compromising performance. The learned kernels and weights were examined
from a clinical standpoint to assess medical relevance. Model variants,
including ShallowNet and the final xEEGNet, were evaluated using robust
Nested-Leave-N-Subjects-Out cross-validation for unbiased performance
estimates. Variability across data splits was explained using embedded EEG
representations, grouped by class and set, with pairwise separability to
quantify group distinction. Overfitting was assessed through
training-validation loss correlation and training speed. xEEGNet uses only 168
parameters, 200 times fewer than ShallowNet, yet retains interpretability,
resists overfitting, achieves comparable median performance (-1.5%), and
reduces variability across splits. This variability is explained by embedded
EEG representations: higher accuracy correlates with greater separation between
test set controls and Alzheimer's cases, without significant influence from
training data. xEEGNet's ability to filter specific EEG bands, learn
band-specific topographies, and use relevant spectral features demonstrates its
interpretability. While large deep learning models are often prioritized for
performance, this study shows smaller architectures like xEEGNet can be equally
effective in EEG pathology classification.

</details>


### [101] [Deep Learning Optimization Using Self-Adaptive Weighted Auxiliary Variables](https://arxiv.org/abs/2504.21501)
*Yaru Liu,Yiqi Gu,Michael K. Ng*

Main category: cs.LG

TL;DR: 提出了一种新的优化框架，通过引入辅助变量和自适应权重改进深度神经网络中的最小二乘学习问题，解决了梯度下降的低效问题。


<details>
  <summary>Details</summary>
Motivation: 梯度下降在深度学习中因损失函数的高度非凸性和梯度消失问题而表现不佳，需要一种更高效的优化方法。

Method: 引入辅助变量分离神经网络层，重新设计损失函数，并使用自适应权重保持新损失函数与原均方损失的一致性。

Result: 数值实验验证了新方法的有效性和鲁棒性，优于传统梯度下降。

Conclusion: 新框架通过优化重构的损失函数，显著提升了深度学习的效率和性能。

Abstract: In this paper, we develop a new optimization framework for the least squares
learning problem via fully connected neural networks or physics-informed neural
networks. The gradient descent sometimes behaves inefficiently in deep learning
because of the high non-convexity of loss functions and the vanishing gradient
issue. Our idea is to introduce auxiliary variables to separate the layers of
the deep neural networks and reformulate the loss functions for ease of
optimization. We design the self-adaptive weights to preserve the consistency
between the reformulated loss and the original mean squared loss, which
guarantees that optimizing the new loss helps optimize the original problem.
Numerical experiments are presented to verify the consistency and show the
effectiveness and robustness of our models over gradient descent.

</details>


### [102] [Towards proactive self-adaptive AI for non-stationary environments with dataset shifts](https://arxiv.org/abs/2504.21565)
*David Fernández Narro,Pablo Ferri,Juan M. García-Gómez,Carlos Sáez*

Main category: cs.LG

TL;DR: 提出了一种主动自适应的AI方法（pro-adaptive），通过建模AI参数的时间轨迹来预测短期参数值，以应对非平稳环境中的性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 在非平稳环境中（如医疗场景），AI模型的性能会因时间数据分布变化而下降，且缺乏及时的新标注数据用于重新训练。

Method: 使用多项式样条基和功能性数据分析框架，建模AI参数的时间轨迹，预测短期参数值。

Result: 在模拟和真实COVID-19数据集上验证，该方法显著提升了AI在数据分布变化下的性能，无需更新训练数据。

Conclusion: 为动态非平稳环境中的自适应AI研究奠定了基础，适用于健康领域的稳健AI生产环境。

Abstract: Artificial Intelligence (AI) models deployed in production frequently face
challenges in maintaining their performance in non-stationary environments.
This issue is particularly noticeable in medical settings, where temporal
dataset shifts often occur. These shifts arise when the distributions of
training data differ from those of the data encountered during deployment over
time. Further, new labeled data to continuously retrain AI is not typically
available in a timely manner due to data access limitations. To address these
challenges, we propose a proactive self-adaptive AI approach, or pro-adaptive,
where we model the temporal trajectory of AI parameters, allowing us to
short-term forecast parameter values. To this end, we use polynomial spline
bases, within an extensible Functional Data Analysis framework. We validate our
methodology with a logistic regression model addressing prior probability
shift, covariate shift, and concept shift. This validation is conducted on both
a controlled simulated dataset and a publicly available real-world COVID-19
dataset from Mexico, with various shifts occurring between 2020 and 2024. Our
results indicate that this approach enhances the performance of AI against
shifts compared to baseline stable models trained at different time distances
from the present, without requiring updated training data. This work lays the
foundation for pro-adaptive AI research against dynamic, non-stationary
environments, being compatible with data protection, in resilient AI production
environments for health.

</details>


### [103] [On Advancements of the Forward-Forward Algorithm](https://arxiv.org/abs/2504.21662)
*Mauricio Ortiz Torres,Markus Lange,Arne P. Raulf*

Main category: cs.LG

TL;DR: Forward-Forward算法通过卷积通道分组、学习率调度和独立块结构等技术改进，在CIFAR10数据集上实现了20%的测试错误率降低，并开发了轻量级模型。


<details>
  <summary>Details</summary>
Motivation: 解决复杂任务并适应低容量硬件需求。

Method: 结合卷积通道分组、学习率调度和独立块结构进行训练。

Result: 测试错误率降低20%，轻量级模型在低参数范围内表现良好。

Conclusion: 为未来神经网络验证和验证研究奠定基础。

Abstract: The Forward-Forward algorithm has evolved in machine learning research,
tackling more complex tasks that mimic real-life applications. In the last
years, it has been improved by several techniques to perform better than its
original version, handling a challenging dataset like CIFAR10 without losing
its flexibility and low memory usage. We have shown in our results that
improvements are achieved through a combination of convolutional channel
grouping, learning rate schedules, and independent block structures during
training that lead to a 20\% decrease in test error percentage. Additionally,
to approach further implementations on low-capacity hardware projects we have
presented a series of lighter models that achieve low test error percentages
within (21$\pm$6)\% and number of trainable parameters between 164,706 and
754,386. This serving also as a basis for our future study on complete
verification and validation of these kinds of neural networks.

</details>


### [104] [Recursive KL Divergence Optimization: A Dynamic Framework for Representation Learning](https://arxiv.org/abs/2504.21707)
*Anthony D Martin*

Main category: cs.LG

TL;DR: 论文提出了一种递归KL散度优化（RKDO）方法，通过动态形式化表示学习过程，显著提升了效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如I-Con）未能充分利用学习过程中的递归结构，限制了效率和适应性。

Method: 引入RKDO，将表示学习视为KL散度在数据邻域上的动态演化过程。

Result: 实验显示，RKDO在损失值和计算资源上分别降低了30%和60-80%。

Conclusion: RKDO为表示学习提供了更高效的优化路径，尤其适用于资源受限场景。

Abstract: We propose a generalization of modern representation learning objectives by
reframing them as recursive divergence alignment processes over localized
conditional distributions While recent frameworks like Information Contrastive
Learning I-Con unify multiple learning paradigms through KL divergence between
fixed neighborhood conditionals we argue this view underplays a crucial
recursive structure inherent in the learning process. We introduce Recursive KL
Divergence Optimization RKDO a dynamic formalism where representation learning
is framed as the evolution of KL divergences across data neighborhoods. This
formulation captures contrastive clustering and dimensionality reduction
methods as static slices while offering a new path to model stability and local
adaptation. Our experiments demonstrate that RKDO offers dual efficiency
advantages approximately 30 percent lower loss values compared to static
approaches across three different datasets and 60 to 80 percent reduction in
computational resources needed to achieve comparable results. This suggests
that RKDOs recursive updating mechanism provides a fundamentally more efficient
optimization landscape for representation learning with significant
implications for resource constrained applications.

</details>


### [105] [Learning Heterogeneous Performance-Fairness Trade-offs in Federated Learning](https://arxiv.org/abs/2504.21775)
*Rongguang Ye,Ming Tang*

Main category: cs.LG

TL;DR: HetPFL提出了一种新方法，通过自适应偏好采样和超网络融合，有效学习联邦学习中的局部和全局Pareto前沿，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在联邦学习中采用均匀偏好采样分布，忽略了局部Pareto前沿的异质性，且未考虑局部与全局Pareto前沿的差距。

Method: HetPFL包含偏好采样适应（PSA）和偏好感知超网络融合（PHF），分别优化局部采样分布和全局超网络融合。

Result: 实验证明HetPFL在四个数据集上显著优于七个基线方法，且理论证明其收敛性更强。

Conclusion: HetPFL通过解决异质性和全局性能差距问题，为联邦学习中的性能-公平性权衡提供了更优方案。

Abstract: Recent methods leverage a hypernet to handle the performance-fairness
trade-offs in federated learning. This hypernet maps the clients' preferences
between model performance and fairness to preference-specifc models on the
trade-off curve, known as local Pareto front. However, existing methods
typically adopt a uniform preference sampling distribution to train the
hypernet across clients, neglecting the inherent heterogeneity of their local
Pareto fronts. Meanwhile, from the perspective of generalization, they do not
consider the gap between local and global Pareto fronts on the global dataset.
To address these limitations, we propose HetPFL to effectively learn both local
and global Pareto fronts. HetPFL comprises Preference Sampling Adaptation (PSA)
and Preference-aware Hypernet Fusion (PHF). PSA adaptively determines the
optimal preference sampling distribution for each client to accommodate
heterogeneous local Pareto fronts. While PHF performs preference-aware fusion
of clients' hypernets to ensure the performance of the global Pareto front. We
prove that HetPFL converges linearly with respect to the number of rounds,
under weaker assumptions than existing methods. Extensive experiments on four
datasets show that HetPFL significantly outperforms seven baselines in terms of
the quality of learned local and global Pareto fronts.

</details>


### [106] [Stable Trajectory Clustering: An Efficient Split and Merge Algorithm](https://arxiv.org/abs/2504.21808)
*Atieh Rahmani,Mansoor Davoodi,Justin M. Calabrese*

Main category: cs.LG

TL;DR: 本文提出了基于DBSCAN线段聚类的全轨迹和子轨迹聚类算法，通过线段的分裂与合并事件处理轨迹数据，并引入稳定轨迹聚类算法以提升聚类稳定性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 研究轨迹聚类算法以解决现有方法因临时异常数据导致聚类模式模糊和可靠性降低的问题。

Method: 基于DBSCAN线段聚类，结合线段分裂与合并事件，提出全轨迹和子轨迹聚类算法，并引入稳定轨迹聚类算法利用平均绝对偏差选择性忽略瞬态偏差。

Result: 在真实轨迹数据集上验证了算法的有效性和对参数变化的敏感性。

Conclusion: 选择性忽略瞬态偏差不仅能保持聚类完整性，还能提升稳定性和可解释性。

Abstract: Clustering algorithms group data points by characteristics to identify
patterns. Over the past two decades, researchers have extended these methods to
analyze trajectories of humans, animals, and vehicles, studying their behavior
and movement across applications. This paper presents whole-trajectory
clustering and sub-trajectory clustering algorithms based on DBSCAN line
segment clustering, which encompasses two key events: split and merge of line
segments. The events are employed by object movement history and the average
Euclidean distance between line segments. In this framework, whole-trajectory
clustering considers entire entities' trajectories, whereas sub-trajectory
clustering employs a sliding window model to identify similar sub-trajectories.
Many existing trajectory clustering algorithms respond to temporary anomalies
in data by splitting trajectories, which often obscures otherwise consistent
clustering patterns and leads to less reliable insights. We introduce the
stable trajectory clustering algorithm, which leverages the mean absolute
deviation concept to demonstrate that selective omission of transient
deviations not only preserves the integrity of clusters but also improves their
stability and interpretability. We run all proposed algorithms on real
trajectory datasets to illustrate their effectiveness and sensitivity to
parameter variations.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [107] [SeriesBench: A Benchmark for Narrative-Driven Drama Series Understanding](https://arxiv.org/abs/2504.21435)
*Chenkai Zhang,Yiming Lei,Zeming Liu,Haitao Leng,ShaoGuo Liu,Tingting Gao,Qingjie Liu,Yunhong Wang*

Main category: cs.CV

TL;DR: 论文提出了SeriesBench，一个用于评估多模态大语言模型（MLLMs）对叙事驱动系列视频理解能力的基准，并提出了PC-DCoT框架以提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要关注独立视频的视觉元素评估，而现实中的视频多为复杂连续的叙事系列，因此需要新的评估方法和模型能力提升。

Method: 通过精选105个叙事系列视频，结合长跨度叙事标注方法和全信息转换技术，构建SeriesBench；提出PC-DCoT框架以增强模型对情节结构和角色关系的分析能力。

Result: 实验表明现有MLLMs在叙事系列理解上仍有挑战，而PC-DCoT能显著提升其性能。

Conclusion: SeriesBench和PC-DCoT强调了提升模型理解叙事系列能力的必要性，为MLLMs的未来发展提供了方向。

Abstract: With the rapid development of Multi-modal Large Language Models (MLLMs), an
increasing number of benchmarks have been established to evaluate the video
understanding capabilities of these models. However, these benchmarks focus on
\textbf{standalone} videos and mainly assess ``visual elements'' like human
actions and object states. In reality, contemporary videos often encompass
complex and continuous narratives, typically presented as a \textbf{series}. To
address this challenge, we propose \textbf{SeriesBench}, a benchmark consisting
of 105 carefully curated narrative-driven series, covering 28 specialized tasks
that require deep narrative understanding. Specifically, we first select a
diverse set of drama series spanning various genres. Then, we introduce a novel
long-span narrative annotation method, combined with a full-information
transformation approach to convert manual annotations into diverse task
formats. To further enhance model capacity for detailed analysis of plot
structures and character relationships within series, we propose a novel
narrative reasoning framework, \textbf{PC-DCoT}. Extensive results on
\textbf{SeriesBench} indicate that existing MLLMs still face significant
challenges in understanding narrative-driven series, while \textbf{PC-DCoT}
enables these MLLMs to achieve performance improvements. Overall, our
\textbf{SeriesBench} and \textbf{PC-DCoT} highlight the critical necessity of
advancing model capabilities to understand narrative-driven series, guiding the
future development of MLLMs. SeriesBench is publicly available at
https://github.com/zackhxn/SeriesBench-CVPR2025.

</details>


### [108] [Black-Box Visual Prompt Engineering for Mitigating Object Hallucination in Large Vision Language Models](https://arxiv.org/abs/2504.21559)
*Sangmin Woo,Kang Zhou,Yun Zhou,Shuai Wang,Sheng Guan,Haibo Ding,Lin Lee Cheong*

Main category: cs.CV

TL;DR: 通过视觉提示（如边界框、圆圈）显著减少大型视觉语言模型（LVLM）中的物体幻觉问题，提出黑盒视觉提示工程（BBVPE）框架，动态选择最优提示。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型常因物体幻觉问题影响可靠性，需找到无需访问模型内部即可优化提示的方法。

Method: 提出BBVPE框架，通过候选视觉提示池和路由器模型动态选择最优提示，适用于开源和专有模型。

Result: 在POPE和CHAIR基准测试中，BBVPE有效减少了物体幻觉。

Conclusion: BBVPE是一种无需模型内部访问的通用方法，显著提升LVLM的可靠性。

Abstract: Large Vision Language Models (LVLMs) often suffer from object hallucination,
which undermines their reliability. Surprisingly, we find that simple
object-based visual prompting -- overlaying visual cues (e.g., bounding box,
circle) on images -- can significantly mitigate such hallucination; however,
different visual prompts (VPs) vary in effectiveness. To address this, we
propose Black-Box Visual Prompt Engineering (BBVPE), a framework to identify
optimal VPs that enhance LVLM responses without needing access to model
internals. Our approach employs a pool of candidate VPs and trains a router
model to dynamically select the most effective VP for a given input image. This
black-box approach is model-agnostic, making it applicable to both open-source
and proprietary LVLMs. Evaluations on benchmarks such as POPE and CHAIR
demonstrate that BBVPE effectively reduces object hallucination.

</details>


### [109] [Emotion Recognition in Contemporary Dance Performances Using Laban Movement Analysis](https://arxiv.org/abs/2504.21154)
*Muhammad Turab,Philippe Colantoni,Damien Muselet,Alain Tremeau*

Main category: cs.CV

TL;DR: 本文提出了一种改进的Laban运动分析（LMA）特征描述符，并结合新的描述符，用于当代舞蹈中的情感识别，准确率高达96.85%。


<details>
  <summary>Details</summary>
Motivation: 改进现有方法以更准确地捕捉舞蹈中的情感表达，为表演分析、舞蹈训练和人机交互提供支持。

Method: 从3D关键点数据中提取运动特征，结合改进的LMA和新型描述符，使用随机森林和支持向量机进行分类。

Result: 模型在情感识别任务中达到96.85%的最高准确率。

Conclusion: 该框架显著提升了当代舞蹈中的情感识别能力，具有广泛的应用潜力。

Abstract: This paper presents a novel framework for emotion recognition in contemporary
dance by improving existing Laban Movement Analysis (LMA) feature descriptors
and introducing robust, novel descriptors that capture both quantitative and
qualitative aspects of the movement. Our approach extracts expressive
characteristics from 3D keypoints data of professional dancers performing
contemporary dance under various emotional states, and trains multiple
classifiers, including Random Forests and Support Vector Machines.
Additionally, we provide in-depth explanation of features and their impact on
model predictions using explainable machine learning methods. Overall, our
study improves emotion recognition in contemporary dance and offers promising
applications in performance analysis, dance training, and human--computer
interaction, with a highest accuracy of 96.85\%.

</details>


### [110] [Dance Style Recognition Using Laban Movement Analysis](https://arxiv.org/abs/2504.21166)
*Muhammad Turab,Philippe Colantoni,Damien Muselet,Alain Tremeau*

Main category: cs.CV

TL;DR: 提出了一种结合3D姿态估计、3D人体网格重建和地板感知身体建模的新方法，通过滑动窗口捕捉时间上下文，显著提高了舞蹈风格识别的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有舞蹈风格识别方法多关注跨帧运动分析，缺乏对时间上下文和动态过渡的捕捉，因此需要一种能增强LMA特征时间背景的方法。

Method: 结合3D姿态估计、3D人体网格重建和地板感知身体建模提取LMA特征，采用滑动窗口方法捕捉时间上下文，并用机器学习分类。

Result: 分类准确率最高达99.18%，表明时间上下文的加入显著提升了舞蹈风格识别性能。

Conclusion: 提出的方法通过引入时间上下文，有效解决了舞蹈风格识别中的动态过渡问题，显著提高了分类准确性。

Abstract: The growing interest in automated movement analysis has presented new
challenges in recognition of complex human activities including dance. This
study focuses on dance style recognition using features extracted using Laban
Movement Analysis. Previous studies for dance style recognition often focus on
cross-frame movement analysis, which limits the ability to capture temporal
context and dynamic transitions between movements. This gap highlights the need
for a method that can add temporal context to LMA features. For this, we
introduce a novel pipeline which combines 3D pose estimation, 3D human mesh
reconstruction, and floor aware body modeling to effectively extract LMA
features. To address the temporal limitation, we propose a sliding window
approach that captures movement evolution across time in features. These
features are then used to train various machine learning methods for
classification, and their explainability explainable AI methods to evaluate the
contribution of each feature to classification performance. Our proposed method
achieves a highest classification accuracy of 99.18\% which shows that the
addition of temporal context significantly improves dance style recognition
performance.

</details>


### [111] [Geolocating Earth Imagery from ISS: Integrating Machine Learning with Astronaut Photography for Enhanced Geographic Mapping](https://arxiv.org/abs/2504.21194)
*Vedika Srivastava,Hemant Kumar Singh,Jaisal Singh*

Main category: cs.CV

TL;DR: 本文提出了一种利用机器学习算法从国际空间站（ISS）拍摄的图像中定位地球位置的新方法，填补了ISS图像中地理位置未识别的空白。


<details>
  <summary>Details</summary>
Motivation: ISS拍摄的图像虽然坐标精确，但具体地理位置常未被识别，影响了环境监测和全球地图绘制的效率。

Method: 采用了三种图像处理流程：基于神经网络的方法、基于SIFT的方法和GPT-4模型，分别处理高分辨率ISS图像，识别自然和人造地理特征。

Result: 在140多张ISS图像上的评估显示，神经网络方法在地理特征匹配上成功率高，SIFT方法擅长处理放大图像，GPT-4模型能提供丰富的地理描述。

Conclusion: 该研究提升了空间图像地理定位的准确性和效率，有助于环境监测和全球地图绘制。

Abstract: This paper presents a novel approach to geolocating images captured from the
International Space Station (ISS) using advanced machine learning algorithms.
Despite having precise ISS coordinates, the specific Earth locations depicted
in astronaut-taken photographs often remain unidentified. Our research
addresses this gap by employing three distinct image processing pipelines: a
Neural Network based approach, a SIFT based method, and GPT-4 model. Each
pipeline is tailored to process high-resolution ISS imagery, identifying both
natural and man-made geographical features. Through extensive evaluation on a
diverse dataset of over 140 ISS images, our methods demonstrate significant
promise in automated geolocation with varied levels of success. The NN approach
showed a high success rate in accurately matching geographical features, while
the SIFT pipeline excelled in processing zoomed-in images. GPT-4 model provided
enriched geographical descriptions alongside location predictions. This
research contributes to the fields of remote sensing and Earth observation by
enhancing the accuracy and efficiency of geolocating space-based imagery,
thereby aiding environmental monitoring and global mapping efforts.

</details>


### [112] [Legilimens: Performant Video Analytics on the System-on-Chip Edge](https://arxiv.org/abs/2504.21136)
*Murali Ramanujam,Yinwei Dai,Kyle Jamieson,Ravi Netravali*

Main category: cs.CV

TL;DR: Legilimens是一个针对移动边缘设备的持续学习系统，通过利用设备内存中的基础模型和高效计算技术，显著降低了重新训练的成本并提高了准确性。


<details>
  <summary>Details</summary>
Motivation: 移动边缘设备（如无人机和行车记录仪）具有较弱的计算能力和丰富的统一内存池，传统边缘服务器的适应方法不适用，因此需要一种新的持续学习系统。

Method: Legilimens提出三种高效计算技术：选择高效用数据样本、无需完全重新训练的基础模型更新、以及计算资源的时间共享。

Result: Legilimens将重新训练成本降低2.8-10倍，并在不同工作负载下实现了18-45%的准确性提升。

Conclusion: Legilimens通过轻量级重新训练和高效计算技术，为移动边缘设备提供了一种高效的持续学习解决方案。

Abstract: Continually retraining models has emerged as a primary technique to enable
high-accuracy video analytics on edge devices. Yet, existing systems employ
such adaptation by relying on the spare compute resources that traditional
(memory-constrained) edge servers afford. In contrast, mobile edge devices such
as drones and dashcams offer a fundamentally different resource profile:
weak(er) compute with abundant unified memory pools. We present Legilimens, a
continuous learning system for the mobile edge's System-on-Chip GPUs. Our
driving insight is that visually distinct scenes that require retraining
exhibit substantial overlap in model embeddings; if captured into a base model
on device memory, specializing to each new scene can become lightweight,
requiring very few samples. To practically realize this approach, Legilimens
presents new, compute-efficient techniques to (1) select high-utility data
samples for retraining specialized models, (2) update the base model without
complete retraining, and (3) time-share compute resources between retraining
and live inference for maximal accuracy. Across diverse workloads, Legilimens
lowers retraining costs by 2.8-10x compared to existing systems, resulting in
18-45% higher accuracies.

</details>


### [113] [MemeBLIP2: A novel lightweight multimodal system to detect harmful memes](https://arxiv.org/abs/2504.21226)
*Jiaqi Liu,Ran Tong,Aowei Shen,Shuzheng Li,Changlin Yang,Lisha Xu*

Main category: cs.CV

TL;DR: MemeBLIP2是一个轻量级多模态系统，通过结合图像和文本特征有效检测有害表情包。


<details>
  <summary>Details</summary>
Motivation: 表情包常结合视觉和简短文本传递幽默或观点，但部分包含有害信息（如仇恨言论），需要有效检测方法。

Method: 基于BLIP-2核心模型，通过模块对齐图像和文本表示，并在共享空间中融合以优化分类。

Result: 在PrideMM数据集上评估，MemeBLIP2能捕捉多模态中的细微线索（如讽刺或文化特定内容），提升有害内容检测效果。

Conclusion: MemeBLIP2通过多模态融合显著提升有害表情包的检测能力。

Abstract: Memes often merge visuals with brief text to share humor or opinions, yet
some memes contain harmful messages such as hate speech. In this paper, we
introduces MemeBLIP2, a light weight multimodal system that detects harmful
memes by combining image and text features effectively. We build on previous
studies by adding modules that align image and text representations into a
shared space and fuse them for better classification. Using BLIP-2 as the core
vision-language model, our system is evaluated on the PrideMM datasets. The
results show that MemeBLIP2 can capture subtle cues in both modalities, even in
cases with ironic or culturally specific content, thereby improving the
detection of harmful material.

</details>


### [114] [T2ID-CAS: Diffusion Model and Class Aware Sampling to Mitigate Class Imbalance in Neck Ultrasound Anatomical Landmark Detection](https://arxiv.org/abs/2504.21231)
*Manikanta Varaganti,Amulya Vankayalapati,Nour Awad,Gregory R. Dion,Laura J. Brattain*

Main category: cs.CV

TL;DR: 论文提出T2ID-CAS方法，结合文本到图像扩散模型和类别感知采样，解决颈部超声中类不平衡问题，显著提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 颈部超声在气道管理中至关重要，但数据集中关键结构（如气管环和声带）的类不平衡问题影响了目标检测模型的性能。

Method: 提出T2ID-CAS方法，结合文本到图像潜在扩散模型和类别感知采样，生成高质量的合成样本以增强少数类别的表示。

Result: 实验显示，T2ID-CAS在YOLOv9上的平均精度达到88.2，显著优于基线66。

Conclusion: T2ID-CAS是一种高效且可扩展的解决方案，适用于AI辅助超声引导干预中的类不平衡问题。

Abstract: Neck ultrasound (US) plays a vital role in airway management by providing
non-invasive, real-time imaging that enables rapid and precise interventions.
Deep learning-based anatomical landmark detection in neck US can further
facilitate procedural efficiency. However, class imbalance within datasets,
where key structures like tracheal rings and vocal folds are underrepresented,
presents significant challenges for object detection models. To address this,
we propose T2ID-CAS, a hybrid approach that combines a text-to-image latent
diffusion model with class-aware sampling to generate high-quality synthetic
samples for underrepresented classes. This approach, rarely explored in the
ultrasound domain, improves the representation of minority classes.
Experimental results using YOLOv9 for anatomical landmark detection in neck US
demonstrated that T2ID-CAS achieved a mean Average Precision of 88.2,
significantly surpassing the baseline of 66. This highlights its potential as a
computationally efficient and scalable solution for mitigating class imbalance
in AI-assisted ultrasound-guided interventions.

</details>


### [115] [Embracing Collaboration Over Competition: Condensing Multiple Prompts for Visual In-Context Learning](https://arxiv.org/abs/2504.21263)
*Jinpeng Wang,Tianci Luo,Yaohua Zha,Yan Feng,Ruisheng Luo,Bin Chen,Tao Dai,Long Chen,Yaowei Wang,Shu-Tao Xia*

Main category: cs.CV

TL;DR: 论文提出了一种名为Condenser的轻量级插件，通过多提示协作而非单一提示选择，优化视觉上下文学习（VICL）中的上下文整合，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 当前VICL方法假设存在单一“理想”提示，但实际上可能有多个合适提示，单独使用时效果不佳，导致选择困难和上下文信息丢失。

Method: 提出提示压缩（prompt condensation）方法，设计Condenser插件，通过多提示协作整合细粒度上下文，并与主干网络端到端优化。

Result: 实验表明，Condenser在基准任务中优于现有方法，具有更好的上下文压缩能力、扩展性和计算效率。

Conclusion: Condenser为VICL提供了一种高效且可扩展的解决方案，代码已开源。

Abstract: Visual In-Context Learning (VICL) enables adaptively solving vision tasks by
leveraging pixel demonstrations, mimicking human-like task completion through
analogy. Prompt selection is critical in VICL, but current methods assume the
existence of a single "ideal" prompt in a pool of candidates, which in practice
may not hold true. Multiple suitable prompts may exist, but individually they
often fall short, leading to difficulties in selection and the exclusion of
useful context. To address this, we propose a new perspective: prompt
condensation. Rather than relying on a single prompt, candidate prompts
collaborate to efficiently integrate informative contexts without sacrificing
resolution. We devise Condenser, a lightweight external plugin that compresses
relevant fine-grained context across multiple prompts. Optimized end-to-end
with the backbone, Condenser ensures accurate integration of contextual cues.
Experiments demonstrate Condenser outperforms state-of-the-arts across
benchmark tasks, showing superior context compression, scalability with more
prompts, and enhanced computational efficiency compared to ensemble methods,
positioning it as a highly competitive solution for VICL. Code is open-sourced
at https://github.com/gimpong/CVPR25-Condenser.

</details>


### [116] [Vision-Language Model-Based Semantic-Guided Imaging Biomarker for Early Lung Cancer Detection](https://arxiv.org/abs/2504.21344)
*Luoting Zhuang,Seyed Mohammad Hossein Tabatabaei,Ramin Salehi-Rad,Linh M. Tran,Denise R. Aberle,Ashley E. Prosper,William Hsu*

Main category: cs.CV

TL;DR: 该研究通过整合放射科医生评估的语义特征，利用CLIP模型预测肺癌，提高了模型的解释性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习模型依赖手动标注、解释性差且对成像变化敏感，限制了临床应用。

Method: 使用多个数据集，通过参数高效微调方法对齐影像和语义特征，预测肺癌诊断。

Result: 模型在外部数据集上表现优异（AUROC: 0.90，AUPRC: 0.78），并能解释预测结果。

Conclusion: 该方法准确分类肺结节，提供可解释的输出，适用于临床环境。

Abstract: Objective: A number of machine learning models have utilized semantic
features, deep features, or both to assess lung nodule malignancy. However,
their reliance on manual annotation during inference, limited interpretability,
and sensitivity to imaging variations hinder their application in real-world
clinical settings. Thus, this research aims to integrate semantic features
derived from radiologists' assessments of nodules, allowing the model to learn
clinically relevant, robust, and explainable features for predicting lung
cancer. Methods: We obtained 938 low-dose CT scans from the National Lung
Screening Trial with 1,246 nodules and semantic features. The Lung Image
Database Consortium dataset contains 1,018 CT scans, with 2,625 lesions
annotated for nodule characteristics. Three external datasets were obtained
from UCLA Health, the LUNGx Challenge, and the Duke Lung Cancer Screening. We
finetuned a pretrained Contrastive Language-Image Pretraining model with a
parameter-efficient fine-tuning approach to align imaging and semantic features
and predict the one-year lung cancer diagnosis. Results: We evaluated the
performance of the one-year diagnosis of lung cancer with AUROC and AUPRC and
compared it to three state-of-the-art models. Our model demonstrated an AUROC
of 0.90 and AUPRC of 0.78, outperforming baseline state-of-the-art models on
external datasets. Using CLIP, we also obtained predictions on semantic
features, such as nodule margin (AUROC: 0.81), nodule consistency (0.81), and
pleural attachment (0.84), that can be used to explain model predictions.
Conclusion: Our approach accurately classifies lung nodules as benign or
malignant, providing explainable outputs, aiding clinicians in comprehending
the underlying meaning of model predictions. This approach also prevents the
model from learning shortcuts and generalizes across clinical settings.

</details>


### [117] [Nexus-Gen: A Unified Model for Image Understanding, Generation, and Editing](https://arxiv.org/abs/2504.21356)
*Hong Zhang,Zhongjie Duan,Xingjun Wang,Yingda Chen,Yuze Zhao,Yu Zhang*

Main category: cs.CV

TL;DR: Nexus-Gen是一个统一的多模态大语言模型，结合了LLM的语言推理能力和扩散模型的图像合成能力，通过双阶段对齐训练解决了性能差距问题。


<details>
  <summary>Details</summary>
Motivation: 现有开源统一模型在性能上与领域专用架构存在差距，Nexus-Gen旨在弥合这一差距。

Method: 采用双阶段对齐训练：LLM学习预测图像嵌入，视觉解码器学习从嵌入重建图像；引入预填充自回归策略避免误差累积。

Result: Nexus-Gen具备综合的图像理解、生成和编辑能力。

Conclusion: Nexus-Gen通过双阶段训练和预填充策略，成功整合了多模态能力，推动了领域发展。

Abstract: Unified multimodal large language models (MLLMs) aim to integrate multimodal
understanding and generation abilities through a single framework. Despite
their versatility, existing open-source unified models exhibit performance gaps
against domain-specific architectures. To bridge this gap, we present
Nexus-Gen, a unified model that synergizes the language reasoning capabilities
of LLMs with the image synthesis power of diffusion models. To align the
embedding space of the LLM and diffusion model, we conduct a dual-phase
alignment training process. (1) The autoregressive LLM learns to predict image
embeddings conditioned on multimodal inputs, while (2) the vision decoder is
trained to reconstruct high-fidelity images from these embeddings. During
training the LLM, we identified a critical discrepancy between the
autoregressive paradigm's training and inference phases, where error
accumulation in continuous embedding space severely degrades generation
quality. To avoid this issue, we introduce a prefilled autoregression strategy
that prefills input sequence with position-embedded special tokens instead of
continuous embeddings. Through dual-phase training, Nexus-Gen has developed the
integrated capability to comprehensively address the image understanding,
generation and editing tasks. All models, datasets, and codes are published at
https://github.com/modelscope/Nexus-Gen.git to facilitate further advancements
across the field.

</details>


### [118] [Towards Improved Cervical Cancer Screening: Vision Transformer-Based Classification and Interpretability](https://arxiv.org/abs/2504.21340)
*Khoa Tuan Nguyen,Ho-min Park,Gaeun Oh,Joris Vankerschaver,Wesley De Neve*

Main category: cs.CV

TL;DR: 提出了一种基于EVA-02变换器模型的宫颈细胞图像分类新方法，通过四步流程优化模型性能，最终F1分数达0.85227，优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 改进宫颈癌筛查中细胞图像分类的准确性和可解释性。

Method: 四步流程：微调EVA-02、特征提取、多模型特征选择、训练新神经网络（可选损失加权）。

Result: 最佳模型F1分数0.85227，优于基线（0.84878）；通过Kernel SHAP分析提供可解释性。

Conclusion: 该方法在性能和可解释性上均优于基线模型，为宫颈癌筛查提供了有效工具。

Abstract: We propose a novel approach to cervical cell image classification for
cervical cancer screening using the EVA-02 transformer model. We developed a
four-step pipeline: fine-tuning EVA-02, feature extraction, selecting important
features through multiple machine learning models, and training a new
artificial neural network with optional loss weighting for improved
generalization. With this design, our best model achieved an F1-score of
0.85227, outperforming the baseline EVA-02 model (0.84878). We also utilized
Kernel SHAP analysis and identified key features correlating with cell
morphology and staining characteristics, providing interpretable insights into
the decision-making process of the fine-tuned model. Our code is available at
https://github.com/Khoa-NT/isbi2025_ps3c.

</details>


### [119] [Revisiting Diffusion Autoencoder Training for Image Reconstruction Quality](https://arxiv.org/abs/2504.21368)
*Pramook Khungurn,Sukit Seripanitkarn,Phonphrm Thawatdamrongkit,Supasorn Suwajanakorn*

Main category: cs.CV

TL;DR: 提出了一种改进的扩散自编码器（DAE）训练方法，通过分阶段训练提高图像重建质量。


<details>
  <summary>Details</summary>
Motivation: 传统的DAE训练在高噪声水平下花费过多时间，导致图像模糊，而潜在编码应已包含结构信息，因此可以优化细节。

Method: 分两阶段训练：第一阶段在高噪声水平下训练DAE作为普通自编码器，第二阶段采用低噪声时间更长的噪声计划以优化细节。

Result: 生成的图像在高层结构和低层细节上均更准确，同时保留了潜在编码的有用特性。

Conclusion: 提出的方法显著提升了DAE的图像重建质量，同时保持了潜在编码的功能。

Abstract: Diffusion autoencoders (DAEs) are typically formulated as a noise prediction
model and trained with a linear-$\beta$ noise schedule that spends much of its
sampling steps at high noise levels. Because high noise levels are associated
with recovering large-scale image structures and low noise levels with
recovering details, this configuration can result in low-quality and blurry
images. However, it should be possible to improve details while spending fewer
steps recovering structures because the latent code should already contain
structural information. Based on this insight, we propose a new DAE training
method that improves the quality of reconstructed images. We divide training
into two phases. In the first phase, the DAE is trained as a vanilla
autoencoder by always setting the noise level to the highest, forcing the
encoder and decoder to populate the latent code with structural information. In
the second phase, we incorporate a noise schedule that spends more time in the
low-noise region, allowing the DAE to learn how to perfect the details. Our
method results in images that have accurate high-level structures and low-level
details while still preserving useful properties of the latent codes.

</details>


### [120] [ClassWise-CRF: Category-Specific Fusion for Enhanced Semantic Segmentation of Remote Sensing Imagery](https://arxiv.org/abs/2504.21491)
*Qinfeng Zhu,Yunxi Jiang,Lei Fan*

Main category: cs.CV

TL;DR: ClassWise-CRF是一种结果级类别特定融合架构，通过两阶段过程选择专家网络并自适应加权融合其预测，结合CRF优化空间一致性和边界精度，显著提升了遥感图像语义分割性能。


<details>
  <summary>Details</summary>
Motivation: 解决多网络融合在语义分割中难以实现类别特定优化的问题，提升遥感图像分割的准确性和一致性。

Method: 1. 使用贪婪算法选择特定类别表现好的专家网络；2. 基于验证集分割指标自适应加权融合预测；3. 结合CRF优化空间一致性和边界精度。

Result: 在LoveDA和Vaihingen数据集上，mIoU分别提升1.00%/0.68%和0.87%/0.91%，验证了架构的有效性和通用性。

Conclusion: ClassWise-CRF通过类别特定优化和CRF后处理，显著提升了遥感图像语义分割性能，具有广泛适用性。

Abstract: We propose a result-level category-specific fusion architecture called
ClassWise-CRF. This architecture employs a two-stage process: first, it selects
expert networks that perform well in specific categories from a pool of
candidate networks using a greedy algorithm; second, it integrates the
segmentation predictions of these selected networks by adaptively weighting
their contributions based on their segmentation performance in each category.
Inspired by Conditional Random Field (CRF), the ClassWise-CRF architecture
treats the segmentation predictions from multiple networks as confidence vector
fields. It leverages segmentation metrics (such as Intersection over Union)
from the validation set as priors and employs an exponential weighting strategy
to fuse the category-specific confidence scores predicted by each network. This
fusion method dynamically adjusts the weights of each network for different
categories, achieving category-specific optimization. Building on this, the
architecture further optimizes the fused results using unary and pairwise
potentials in CRF to ensure spatial consistency and boundary accuracy. To
validate the effectiveness of ClassWise-CRF, we conducted experiments on two
remote sensing datasets, LoveDA and Vaihingen, using eight classic and advanced
semantic segmentation networks. The results show that the ClassWise-CRF
architecture significantly improves segmentation performance: on the LoveDA
dataset, the mean Intersection over Union (mIoU) metric increased by 1.00% on
the validation set and by 0.68% on the test set; on the Vaihingen dataset, the
mIoU improved by 0.87% on the validation set and by 0.91% on the test set.
These results fully demonstrate the effectiveness and generality of the
ClassWise-CRF architecture in semantic segmentation of remote sensing images.
The full code is available at https://github.com/zhuqinfeng1999/ClassWise-CRF.

</details>


### [121] [Rethinking Visual Layer Selection in Multimodal LLMs](https://arxiv.org/abs/2504.21447)
*Haoran Chen,Junyan Lin,Xinhao Chen,Yue Fan,Xin Jin,Hui Su,Jianfeng Dong,Jinlan Fu,Xiaoyu Shen*

Main category: cs.CV

TL;DR: 本文提出了一种基于层间表示相似性的方法，对CLIP-ViT的视觉层进行分组（浅层、中层、深层），并研究了其对多模态大语言模型（MLLM）性能的影响。通过实验发现，不同任务需要不同层次的视觉特征，并提出了一种轻量级融合方法，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM通常基于经验启发式选择视觉特征，缺乏系统性分析。本文旨在通过研究CLIP-ViT不同层次的行为，为视觉层选择提供原则性指导。

Method: 提出Layer-wise Representation Similarity方法，将CLIP-ViT层分为浅层、中层和深层，并在不同规模的LLaVA风格模型上进行实验。

Result: 实验表明：深层对OCR任务至关重要；浅层和中层在计数、定位等推理任务中表现更优；跨层轻量级融合方法在10个数据集中的9个上优于基线。

Conclusion: 本文首次系统研究了MLLM中视觉层选择问题，为未来视觉表征学习的深入研究奠定了基础。

Abstract: Multimodal large language models (MLLMs) have achieved impressive performance
across a wide range of tasks, typically using CLIP-ViT as their visual encoder
due to its strong text-image alignment capabilities. While prior studies
suggest that different CLIP-ViT layers capture different types of information,
with shallower layers focusing on fine visual details and deeper layers
aligning more closely with textual semantics, most MLLMs still select visual
features based on empirical heuristics rather than systematic analysis. In this
work, we propose a Layer-wise Representation Similarity approach to group
CLIP-ViT layers with similar behaviors into {shallow, middle, and deep}
categories and assess their impact on MLLM performance. Building on this
foundation, we revisit the visual layer selection problem in MLLMs at scale,
training LLaVA-style models ranging from 1.4B to 7B parameters. Through
extensive experiments across 10 datasets and 4 tasks, we find that: (1) deep
layers are essential for OCR tasks; (2) shallow and middle layers substantially
outperform deep layers on reasoning tasks involving counting, positioning, and
object localization; (3) a lightweight fusion of features across shallow,
middle, and deep layers consistently outperforms specialized fusion baselines
and single-layer selections, achieving gains on 9 out of 10 datasets. Our work
offers the first principled study of visual layer selection in MLLMs, laying
the groundwork for deeper investigations into visual representation learning
for MLLMs.

</details>


### [122] [Anomaly-Driven Approach for Enhanced Prostate Cancer Segmentation](https://arxiv.org/abs/2504.21789)
*Alessia Hu,Regina Beets-Tan,Lishan Cai,Eduardo Pooch*

Main category: cs.CV

TL;DR: 该研究提出了一种结合异常检测的深度学习分割框架（adU-Net），用于改进临床显著前列腺癌（csPCa）的识别，通过异常图引导分割模型，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: MRI在识别csPCa中至关重要，但自动化方法面临数据不平衡、肿瘤大小不一和标注数据不足的挑战。

Method: 研究引入adU-Net，将基于双参数MRI序列的异常图融入分割框架，并通过比较分析评估其效果。

Result: 在外部测试集上，adU-Net的平均得分（0.618）优于基线nnU-Net（0.605），ADC异常图表现最佳。

Conclusion: 结合异常检测的分割方法提升了泛化能力和性能，为自动化csPCa识别提供了新方向。

Abstract: Magnetic Resonance Imaging (MRI) plays an important role in identifying
clinically significant prostate cancer (csPCa), yet automated methods face
challenges such as data imbalance, variable tumor sizes, and a lack of
annotated data. This study introduces Anomaly-Driven U-Net (adU-Net), which
incorporates anomaly maps derived from biparametric MRI sequences into a deep
learning-based segmentation framework to improve csPCa identification. We
conduct a comparative analysis of anomaly detection methods and evaluate the
integration of anomaly maps into the segmentation pipeline. Anomaly maps,
generated using Fixed-Point GAN reconstruction, highlight deviations from
normal prostate tissue, guiding the segmentation model to potential cancerous
regions. We compare the performance by using the average score, computed as the
mean of the AUROC and Average Precision (AP). On the external test set, adU-Net
achieves the best average score of 0.618, outperforming the baseline nnU-Net
model (0.605). The results demonstrate that incorporating anomaly detection
into segmentation improves generalization and performance, particularly with
ADC-based anomaly maps, offering a promising direction for automated csPCa
identification.

</details>


### [123] [GarmentDiffusion: 3D Garment Sewing Pattern Generation with Multimodal Diffusion Transformers](https://arxiv.org/abs/2504.21476)
*Xinyu Li,Qi Yao,Yuanda Wang*

Main category: cs.CV

TL;DR: GarmentDiffusion是一种新的生成模型，能够从多模态输入（文本、图像和不完整的缝纫图案）生成厘米级精确的矢量化3D缝纫图案，效率比现有方法高100倍。


<details>
  <summary>Details</summary>
Motivation: 现有方法在生成缝纫图案时依赖单一输入模态或效率不高，限制了多样化服装的生成。

Method: 通过将3D缝纫图案参数编码为紧凑的边缘令牌表示，并使用扩散变换器同时对所有边缘令牌进行去噪，实现了高效的生成。

Result: 在DressCodeData和GarmentCodeData数据集上取得了最新的最优结果，生成速度比SewingGPT快100倍。

Conclusion: GarmentDiffusion在缝纫图案生成中实现了高效和多模态输入的结合，为服装设计提供了新的工具。

Abstract: Garment sewing patterns are fundamental design elements that bridge the gap
between design concepts and practical manufacturing. The generative modeling of
sewing patterns is crucial for creating diversified garments. However, existing
approaches are limited either by reliance on a single input modality or by
suboptimal generation efficiency. In this work, we present
\textbf{\textit{GarmentDiffusion}}, a new generative model capable of producing
centimeter-precise, vectorized 3D sewing patterns from multimodal inputs (text,
image, and incomplete sewing pattern). Our method efficiently encodes 3D sewing
pattern parameters into compact edge token representations, achieving a
sequence length that is $\textbf{10}\times$ shorter than that of the
autoregressive SewingGPT in DressCode. By employing a diffusion transformer, we
simultaneously denoise all edge tokens along the temporal axis, while
maintaining a constant number of denoising steps regardless of dataset-specific
edge and panel statistics. With all combination of designs of our model, the
sewing pattern generation speed is accelerated by $\textbf{100}\times$ compared
to SewingGPT. We achieve new state-of-the-art results on DressCodeData, as well
as on the largest sewing pattern dataset, namely GarmentCodeData. The project
website is available at https://shenfu-research.github.io/Garment-Diffusion/.

</details>


### [124] [eNCApsulate: NCA for Precision Diagnosis on Capsule Endoscopes](https://arxiv.org/abs/2504.21562)
*Henry John Krumb,Anirban Mukhopadhyay*

Main category: cs.CV

TL;DR: 该论文提出了一种基于神经细胞自动机（NCA）的方法，用于无线胶囊内窥镜中的出血分割和深度估计，并通过蒸馏技术将大型模型压缩至适合微控制器（如ESP32）的轻量级架构，实现了高效、准确的图像处理。


<details>
  <summary>Details</summary>
Motivation: 无线胶囊内窥镜生成大量视频数据，传统方法难以在胶囊内直接运行大型深度学习模型，且胶囊定位困难。需要一种轻量、高效的方法实现出血分割和深度估计。

Method: 使用神经细胞自动机（NCA）进行出血分割和深度估计，通过蒸馏技术将大型基础模型的输出作为伪真值训练NCA，并将其移植到ESP32微控制器上。

Result: NCA在出血分割上比其他轻量模型更准确（Dice指标），参数存储需求减少100倍以上；深度估计结果在某些情况下优于伪真值。ESP32-S3上的运行时优化使推理速度提升3倍以上。

Conclusion: 通过算法调整和蒸馏技术，NCA模型可成功部署到微型设备中，首次实现了胶囊内可靠的出血分割和深度估计，为结合视觉里程计的精确定位铺平了道路。

Abstract: Wireless Capsule Endoscopy is a non-invasive imaging method for the entire
gastrointestinal tract, and is a pain-free alternative to traditional
endoscopy. It generates extensive video data that requires significant review
time, and localizing the capsule after ingestion is a challenge. Techniques
like bleeding detection and depth estimation can help with localization of
pathologies, but deep learning models are typically too large to run directly
on the capsule. Neural Cellular Automata (NCA) for bleeding segmentation and
depth estimation are trained on capsule endoscopic images. For monocular depth
estimation, we distill a large foundation model into the lean NCA architecture,
by treating the outputs of the foundation model as pseudo ground truth. We then
port the trained NCA to the ESP32 microcontroller, enabling efficient image
processing on hardware as small as a camera capsule. NCA are more accurate
(Dice) than other portable segmentation models, while requiring more than 100x
fewer parameters stored in memory than other small-scale models. The visual
results of NCA depth estimation look convincing, and in some cases beat the
realism and detail of the pseudo ground truth. Runtime optimizations on the
ESP32-S3 accelerate the average inference speed significantly, by more than
factor 3. With several algorithmic adjustments and distillation, it is possible
to eNCApsulate NCA models into microcontrollers that fit into wireless capsule
endoscopes. This is the first work that enables reliable bleeding segmentation
and depth estimation on a miniaturized device, paving the way for precise
diagnosis combined with visual odometry as a means of precise localization of
the capsule -- on the capsule.

</details>


### [125] [Enhancing Self-Supervised Fine-Grained Video Object Tracking with Dynamic Memory Prediction](https://arxiv.org/abs/2504.21692)
*Zihan Zhou,Changrui Dai,Aibo Song,Xiaolin Fang*

Main category: cs.CV

TL;DR: 论文提出了一种动态记忆预测（DMP）框架，通过多参考帧直接增强帧重建，解决了现有方法在复杂场景（如遮挡或快速运动）中的不足。


<details>
  <summary>Details</summary>
Motivation: 现有帧重建方法效率高但忽略了多参考帧的直接参与价值，尤其在复杂场景中表现不佳。

Method: DMP框架包含动态选择参考帧的记忆引擎和双向目标预测网络，利用多参考帧提升重建和跟踪精度。

Result: 实验表明，该算法在目标分割和关键点跟踪任务上优于现有自监督技术。

Conclusion: DMP框架通过多参考帧的动态选择和双向预测，显著提升了复杂场景下的视频分析性能。

Abstract: Successful video analysis relies on accurate recognition of pixels across
frames, and frame reconstruction methods based on video correspondence learning
are popular due to their efficiency. Existing frame reconstruction methods,
while efficient, neglect the value of direct involvement of multiple reference
frames for reconstruction and decision-making aspects, especially in complex
situations such as occlusion or fast movement. In this paper, we introduce a
Dynamic Memory Prediction (DMP) framework that innovatively utilizes multiple
reference frames to concisely and directly enhance frame reconstruction. Its
core component is a Reference Frame Memory Engine that dynamically selects
frames based on object pixel features to improve tracking accuracy. In
addition, a Bidirectional Target Prediction Network is built to utilize
multiple reference frames to improve the robustness of the model. Through
experiments, our algorithm outperforms the state-of-the-art self-supervised
techniques on two fine-grained video object tracking tasks: object segmentation
and keypoint tracking.

</details>


### [126] [Vision Transformers in Precision Agriculture: A Comprehensive Survey](https://arxiv.org/abs/2504.21706)
*Saber Mehdipour,Seyed Abolghasem Mirroshandel,Seyed Amirhossein Tabatabaei*

Main category: cs.CV

TL;DR: 本文探讨了视觉变换器（ViTs）在精准农业中的应用，比较了其与传统方法（如CNN）的优劣，并分析了技术挑战与未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 传统植物病害检测方法在可扩展性和准确性上存在局限，ViTs因其处理长距离依赖和视觉任务的潜力成为有前景的替代方案。

Method: 通过文献综述，介绍了ViTs的基础架构、从NLP到计算机视觉的过渡，以及与CNNs的对比分析。

Result: ViTs在分类、检测和分割任务中表现优异，但仍面临数据需求、计算成本和模型可解释性等挑战。

Conclusion: ViTs有望推动智能农业的发展，未来研究需解决技术挑战并探索实际应用场景。

Abstract: Detecting plant diseases is a crucial aspect of modern agriculture - it plays
a key role in maintaining crop health and increasing overall yield. Traditional
approaches, though still valuable, often rely on manual inspection or
conventional machine learning techniques, both of which face limitations in
scalability and accuracy. Recently, Vision Transformers (ViTs) have emerged as
a promising alternative, offering benefits such as improved handling of
long-range dependencies and better scalability for visual tasks. This survey
explores the application of ViTs in precision agriculture, covering tasks from
classification to detection and segmentation. We begin by introducing the
foundational architecture of ViTs and discuss their transition from Natural
Language Processing (NLP) to computer vision. The discussion includes the
concept of inductive bias in traditional models like Convolutional Neural
Networks (CNNs), and how ViTs mitigate these biases. We provide a comprehensive
review of recent literature, focusing on key methodologies, datasets, and
performance metrics. The survey also includes a comparative analysis of CNNs
and ViTs, with a look at hybrid models and performance enhancements. Technical
challenges - such as data requirements, computational demands, and model
interpretability - are addressed alongside potential solutions. Finally, we
outline potential research directions and technological advancements that could
further support the integration of ViTs in real-world agricultural settings.
Our goal with this study is to offer practitioners and researchers a deeper
understanding of how ViTs are poised to transform smart and precision
agriculture.

</details>


### [127] [Early Exit and Multi Stage Knowledge Distillation in VLMs for Video Summarization](https://arxiv.org/abs/2504.21831)
*Anas Anwarul Haq Khan,Utkarsh Verma,Prateek Chanda,Ganesh Ramakrishnan*

Main category: cs.CV

TL;DR: DEEVISum是一个轻量级、高效的视觉语言模型，用于视频摘要，结合多模态提示和多阶段知识蒸馏（MSKD）与早期退出（EE），在性能和效率之间取得平衡。


<details>
  <summary>Details</summary>
Motivation: 设计一个轻量级且高效的模型，用于视频摘要任务，同时保持高性能和低计算成本。

Method: 结合多模态提示（文本和音频信号），采用多阶段知识蒸馏（MSKD）和早期退出（EE）技术。

Result: 在TVSum数据集上，PaLI Gemma2 3B + MSKD模型达到61.1的F1分数，性能接近更大模型，同时计算成本更低。MSKD比基线蒸馏提升1.33% F1，EE减少21%推理时间。

Conclusion: DEEVISum在视频摘要任务中表现出色，平衡了性能和效率，代码和数据集已公开。

Abstract: We introduce DEEVISum (Distilled Early Exit Vision language model for
Summarization), a lightweight, efficient, and scalable vision language model
designed for segment wise video summarization. Leveraging multi modal prompts
that combine textual and audio derived signals, DEEVISum incorporates Multi
Stage Knowledge Distillation (MSKD) and Early Exit (EE) to strike a balance
between performance and efficiency. MSKD offers a 1.33% absolute F1 improvement
over baseline distillation (0.5%), while EE reduces inference time by
approximately 21% with a 1.3 point drop in F1. Evaluated on the TVSum dataset,
our best model PaLI Gemma2 3B + MSKD achieves an F1 score of 61.1, competing
the performance of significantly larger models, all while maintaining a lower
computational footprint. We publicly release our code and processed dataset to
support further research.

</details>


### [128] [Active Light Modulation to Counter Manipulation of Speech Visual Content](https://arxiv.org/abs/2504.21846)
*Hadleigh Schwartz,Xiaofeng Yan,Charles J. Carver,Xia Zhou*

Main category: cs.CV

TL;DR: Spotlight是一种低开销、非侵入性的系统，通过不可见的光调制动态物理签名保护实时演讲视频免受视觉伪造。


<details>
  <summary>Details</summary>
Motivation: 高影响力演讲视频易受伪造，现有数字域检测方法不足，需物理签名增强安全性。

Method: 利用局部敏感哈希生成紧凑的姿势不变视频特征，并通过光调制嵌入签名。

Result: 实验显示Spotlight在检测伪造视频时AUC≥0.99，真阳性率100%，且对录制条件和攻击具有鲁棒性。

Conclusion: Spotlight通过物理签名有效保护视频完整性，适用于多种场景和攻击环境。

Abstract: High-profile speech videos are prime targets for falsification, owing to
their accessibility and influence. This work proposes Spotlight, a low-overhead
and unobtrusive system for protecting live speech videos from visual
falsification of speaker identity and lip and facial motion. Unlike predominant
falsification detection methods operating in the digital domain, Spotlight
creates dynamic physical signatures at the event site and embeds them into all
video recordings via imperceptible modulated light. These physical signatures
encode semantically-meaningful features unique to the speech event, including
the speaker's identity and facial motion, and are cryptographically-secured to
prevent spoofing. The signatures can be extracted from any video downstream and
validated against the portrayed speech content to check its integrity. Key
elements of Spotlight include (1) a framework for generating extremely compact
(i.e., 150-bit), pose-invariant speech video features, based on
locality-sensitive hashing; and (2) an optical modulation scheme that embeds
>200 bps into video while remaining imperceptible both in video and live.
Prototype experiments on extensive video datasets show Spotlight achieves AUCs
$\geq$ 0.99 and an overall true positive rate of 100% in detecting falsified
videos. Further, Spotlight is highly robust across recording conditions, video
post-processing techniques, and white-box adversarial attacks on its video
feature extraction methodologies.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [129] [CodeFlowBench: A Multi-turn, Iterative Benchmark for Complex Code Generation](https://arxiv.org/abs/2504.21751)
*Sizhe Wang,Zhengren Wang,Dongsheng Ma,Yongan Yu,Rui Ling,Zhiyu Li,Feiyu Xiong,Wentao Zhang*

Main category: cs.SE

TL;DR: CodeFlowBench是首个用于评估LLMs在多轮代码复用中能力的基准测试，包含5258个问题，并揭示了LLMs在多轮代码流中的性能下降。


<details>
  <summary>Details</summary>
Motivation: 现实开发需要可读、可扩展和可测试的代码，通过模块化组件和多轮复用实现。

Method: CodeFlowBench从Codeforces提取问题，通过自动化管道分解为函数级子问题，并设计多轮代码复用的评估框架。

Result: LLMs在多轮代码流中表现较差，如o1-mini的pass@1从单轮的37.8%降至多轮的20.8%。

Conclusion: CodeFlowBench为多轮代码生成任务提供了基准和新见解，指导未来代码生成技术的进步。

Abstract: Real world development demands code that is readable, extensible, and
testable by organizing the implementation into modular components and
iteratively reuse pre-implemented code. We term this iterative, multi-turn
process codeflow and introduce CodeFlowBench, the first benchmark designed for
comprehensively evaluating LLMs' ability to perform codeflow, namely to
implement new functionality by reusing existing functions over multiple turns.
CodeFlowBench comprises 5258 problems drawn from Codeforces and is continuously
updated via an automated pipeline that decomposes each problem into a series of
function-level subproblems based on its dependency tree and each subproblem is
paired with unit tests. We further propose a novel evaluation framework with
tasks and metrics tailored to multi-turn code reuse to assess model
performance. In experiments across various LLMs under both multi-turn and
single-turn patterns. We observe models' poor performance on CodeFlowBench,
with a substantial performance drop in the iterative codeflow scenario. For
instance, o1-mini achieves a pass@1 of 20.8% in multi-turn pattern versus 37.8%
in single-turn pattern. Further analysis shows that different models excel at
different dependency depths, yet all struggle to correctly solve structurally
complex problems, highlighting challenges for current LLMs to serve as code
generation tools when performing codeflow. Overall, CodeFlowBench offers a
comprehensive benchmark and new insights into LLM capabilities for multi-turn,
iterative code generation, guiding future advances in code generation tasks.

</details>


### [130] [SWE-smith: Scaling Data for Software Engineering Agents](https://arxiv.org/abs/2504.21798)
*John Yang,Kilian Leret,Carlos E. Jimenez,Alexander Wettig,Kabir Khandpur,Yanzhe Zhang,Binyuan Hui,Ofir Press,Ludwig Schmidt,Diyi Yang*

Main category: cs.SE

TL;DR: SWE-smith是一种新型管道，用于大规模生成软件工程训练数据，解决了现有数据集小且难以扩展的问题。


<details>
  <summary>Details</summary>
Motivation: 现有软件工程训练数据集规模小、构建复杂且存储需求高，限制了语言模型在软件工程中的应用。

Method: SWE-smith通过自动构建执行环境并合成任务实例，从Python代码库中生成大量训练数据。

Result: 生成了50k实例的数据集，训练模型SWE-agent-LM-32B在SWE-bench Verified基准测试中达到40.2% Pass@1，优于现有开源模型。

Conclusion: SWE-smith及其开源资源显著降低了自动化软件工程研究的门槛。

Abstract: Despite recent progress in Language Models (LMs) for software engineering,
collecting training data remains a significant pain point. Existing datasets
are small, with at most 1,000s of training instances from 11 or fewer GitHub
repositories. The procedures to curate such datasets are often complex,
necessitating hundreds of hours of human labor; companion execution
environments also take up several terabytes of storage, severely limiting their
scalability and usability. To address this pain point, we introduce SWE-smith,
a novel pipeline for generating software engineering training data at scale.
Given any Python codebase, SWE-smith constructs a corresponding execution
environment, then automatically synthesizes 100s to 1,000s of task instances
that break existing test(s) in the codebase. Using SWE-smith, we create a
dataset of 50k instances sourced from 128 GitHub repositories, an order of
magnitude larger than all previous works. We train SWE-agent-LM-32B, achieving
40.2% Pass@1 resolve rate on the SWE-bench Verified benchmark, state of the art
among open source models. We open source SWE-smith (collection procedure, task
instances, trajectories, models) to lower the barrier of entry for research in
LM systems for automated software engineering. All assets available at
https://swesmith.com.

</details>


### [131] [Assessing LLM code generation quality through path planning tasks](https://arxiv.org/abs/2504.21276)
*Wanyi Chen,Meng-Wen Su,Mary L. Cummings*

Main category: cs.SE

TL;DR: 论文评估了六种LLM生成路径规划代码的能力，发现其存在严重安全隐患，不适合直接用于安全关键场景。


<details>
  <summary>Details</summary>
Motivation: 随着LLM生成代码的普及，需要评估其在安全关键应用（如路径规划）中的风险，现有基准测试无法满足需求。

Method: 测试六种LLM生成三种路径规划算法的代码，并在三种不同难度地图上进行验证。

Result: LLM生成的代码在路径规划中存在严重安全隐患。

Conclusion: LLM生成的代码在安全关键应用中需经过严格测试才能使用。

Abstract: As LLM-generated code grows in popularity, more evaluation is needed to
assess the risks of using such tools, especially for safety-critical
applications such as path planning. Existing coding benchmarks are insufficient
as they do not reflect the context and complexity of safety-critical
applications. To this end, we assessed six LLMs' abilities to generate the code
for three different path-planning algorithms and tested them on three maps of
various difficulties. Our results suggest that LLM-generated code presents
serious hazards for path planning applications and should not be applied in
safety-critical contexts without rigorous testing.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [132] [Who Gets the Callback? Generative AI and Gender Bias](https://arxiv.org/abs/2504.21400)
*Sugat Chaturvedi,Rochana Chaturvedi*

Main category: econ.GN

TL;DR: 论文研究了开源大型语言模型（LLM）在招聘中的性别偏见问题，发现模型倾向于推荐男性候选人，尤其是在高薪职位中，且与职业性别刻板印象高度一致。


<details>
  <summary>Details</summary>
Motivation: 探讨生成式AI（特别是LLM）在招聘中的性别偏见问题，以揭示AI驱动招聘可能对劳动力市场公平性和多样性带来的影响。

Method: 使用332,044条真实在线招聘广告数据集，通过提示模型推荐同等资质的男女候选人是否应获得面试机会，并结合标准职业分类系统分析模型行为。

Result: 大多数模型倾向于推荐男性，尤其是在高薪职位中；女性在男性主导职业中回调率较低，而在女性相关职业中较高。模型推荐与传统性别刻板印象高度一致。

Conclusion: AI驱动的招聘可能加剧劳动力市场中的偏见，影响公平性和多样性。研究结果对企业在招聘中实现公平和多样性具有启示意义。

Abstract: Generative artificial intelligence (AI), particularly large language models
(LLMs), is being rapidly deployed in recruitment and for candidate
shortlisting. We audit several mid-sized open-source LLMs for gender bias using
a dataset of 332,044 real-world online job postings. For each posting, we
prompt the model to recommend whether an equally qualified male or female
candidate should receive an interview callback. We find that most models tend
to favor men, especially for higher-wage roles. Mapping job descriptions to the
Standard Occupational Classification system, we find lower callback rates for
women in male-dominated occupations and higher rates in female-associated ones,
indicating occupational segregation. A comprehensive analysis of linguistic
features in job ads reveals strong alignment of model recommendations with
traditional gender stereotypes. To examine the role of recruiter identity, we
steer model behavior by infusing Big Five personality traits and simulating the
perspectives of historical figures. We find that less agreeable personas reduce
stereotyping, consistent with an agreeableness bias in LLMs. Our findings
highlight how AI-driven hiring may perpetuate biases in the labor market and
have implications for fairness and diversity within firms.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [133] [Selecting the Right LLM for eGov Explanations](https://arxiv.org/abs/2504.21032)
*Lior Limonad,Fabiana Fournier,Hadar Mulian,George Manias,Spiros Borotis,Danai Kyrkou*

Main category: cs.CY

TL;DR: 论文探讨了如何通过生成式AI（特别是大语言模型）提升电子政务服务中解释内容的质量，以增强用户信任和使用频率。研究通过调整现有量表，系统比较不同LLM生成解释的感知质量，并以税务返还为例验证方法的实用性。


<details>
  <summary>Details</summary>
Motivation: 电子政务服务中解释内容的质量直接影响用户信任和使用频率，而生成式AI（如LLM）可以自动化生成高质量解释，但如何选择合适的LLM成为难题。

Method: 研究调整了现有量表，系统比较不同LLM生成解释的感知质量，并通过税务返还案例进行用户调查（128名受访者），同时探索自动化反馈的可能性。

Result: 研究提供了选择合适LLM的方法论基础，并通过税务返还案例验证了其适用性，同时尝试用预测技术自动化反馈过程。

Conclusion: 研究为电子政务服务提供了一种系统选择LLM的方法，并展示了其在税务返还中的实用性，未来可进一步探索自动化反馈的可行性。

Abstract: The perceived quality of the explanations accompanying e-government services
is key to gaining trust in these institutions, consequently amplifying further
usage of these services. Recent advances in generative AI, and concretely in
Large Language Models (LLMs) allow the automation of such content
articulations, eliciting explanations' interpretability and fidelity, and more
generally, adapting content to various audiences. However, selecting the right
LLM type for this has become a non-trivial task for e-government service
providers. In this work, we adapted a previously developed scale to assist with
this selection, providing a systematic approach for the comparative analysis of
the perceived quality of explanations generated by various LLMs. We further
demonstrated its applicability through the tax-return process, using it as an
exemplar use case that could benefit from employing an LLM to generate
explanations about tax refund decisions. This was attained through a user study
with 128 survey respondents who were asked to rate different versions of
LLM-generated explanations about tax refund decisions, providing a
methodological basis for selecting the most appropriate LLM. Recognizing the
practical challenges of conducting such a survey, we also began exploring the
automation of this process by attempting to replicate human feedback using a
selection of cutting-edge predictive techniques.

</details>


### [134] [AI Supply Chains: An Emerging Ecosystem of AI Actors, Products, and Services](https://arxiv.org/abs/2504.20185)
*Aspen Hopkins,Sarah H. Cen,Andrew Ilyas,Isabella Struckman,Luis Videgaray,Aleksander Mądry*

Main category: cs.CY

TL;DR: 该论文首次对AI供应链进行了形式化研究，通过两个案例展示了AI开发和监管在供应链中的复杂性。


<details>
  <summary>Details</summary>
Motivation: AI供应链的广泛存在及其对社会、经济和监管的影响尚未被充分理解，因此需要深入研究。

Method: 通过历史视角分析AI供应链的演变，将其建模为有向图，并通过两个案例研究（信息传递和上游设计选择的影响）展示其复杂性。

Result: 研究发现信息在供应链中传递不完善可能导致误解，而上游设计选择会显著影响下游AI产品。

Conclusion: AI供应链的研究具有重要的社会、经济和监管意义，需要进一步探索。

Abstract: The widespread adoption of AI in recent years has led to the emergence of AI
supply chains: complex networks of AI actors contributing models, datasets, and
more to the development of AI products and services. AI supply chains have many
implications yet are poorly understood. In this work, we take a first step
toward a formal study of AI supply chains and their implications, providing two
illustrative case studies indicating that both AI development and regulation
are complicated in the presence of supply chains. We begin by presenting a
brief historical perspective on AI supply chains, discussing how their rise
reflects a longstanding shift towards specialization and outsourcing that
signals the healthy growth of the AI industry. We then model AI supply chains
as directed graphs and demonstrate the power of this abstraction by connecting
examples of AI issues to graph properties. Finally, we examine two case studies
in detail, providing theoretical and empirical results in both. In the first,
we show that information passing (specifically, of explanations) along the AI
supply chains is imperfect, which can result in misunderstandings that have
real-world implications. In the second, we show that upstream design choices
(e.g., by base model providers) have downstream consequences (e.g., on AI
products fine-tuned on the base model). Together, our findings motivate further
study of AI supply chains and their increasingly salient social, economic,
regulatory, and technical implications.

</details>


### [135] [LSTM+Geo with xgBoost Filtering: A Novel Approach for Race and Ethnicity Imputation with Reduced Bias](https://arxiv.org/abs/2504.21259)
*S. Chalavadi,A. Pastor,T. Leitch*

Main category: cs.CY

TL;DR: 论文提出了一种结合LSTM和地理信息的LSTM+Geo方法，显著提高了种族和民族（R&E）预测的准确性，优于传统方法如BISG和BIFSG，并减少了非白人被误分类为白人的比例。


<details>
  <summary>Details</summary>
Motivation: 准确预测种族和民族对分析差异和制定政策至关重要，但现有方法如BISG存在系统性偏差。

Method: 引入LSTM+Geo方法，结合LSTM网络和地理信息，使用大规模选民数据进行验证。

Result: LSTM+Geo的准确率为88.7%，优于单独LSTM（86.4%）和BISG（82.9%），并显著降低了非白人误分类为白人的比例。

Conclusion: LSTM+Geo在性能和偏差改善方面表现优异，可作为独立模型或集成组件，但需谨慎使用。

Abstract: Accurate imputation of race and ethnicity (R&E) is crucial for analyzing
disparities and informing policy. Methods like Bayesian Improved Surname
Geocoding (BISG) are widely used but exhibit limitations, including systematic
misclassification biases linked to socioeconomic status. This paper introduces
LSTM+Geo, a novel approach enhancing Long Short-Term Memory (LSTM) networks
with census tract geolocation information. Using a large voter dataset, we
demonstrate that LSTM+Geo (88.7% accuracy) significantly outperforms standalone
LSTM (86.4%) and Bayesian methods like BISG (82.9%) and BIFSG (86.8%) in
accuracy and F1-score on a held-out validation set. LSTM+Geo reduces the rate
at which non-White individuals are misclassified as White (White FPR 19.3%)
compared to name-only LSTMs (White FPR 24.6%). While sophisticated ensemble
methods incorporating XGBoost achieve the highest overall accuracy (up to
89.4%) and lowest White FPR (17.8%), LSTM+Geo offers strong standalone
performance with improved bias characteristics compared to baseline models.
Integrating LSTM+Geo into an XGBoost ensemble further boosts accuracy,
highlighting its utility as both a standalone model and a component for
advanced systems. We give a caution at the end regarding the appropriate use of
these methods.

</details>


### [136] [Quantitative Auditing of AI Fairness with Differentially Private Synthetic Data](https://arxiv.org/abs/2504.21634)
*Chih-Cheng Rex Yuan,Bow-Yaw Wang*

Main category: cs.CY

TL;DR: 提出了一种利用差分隐私合成数据审计AI系统公平性的框架，解决了传统审计中的安全和隐私问题。


<details>
  <summary>Details</summary>
Motivation: 传统审计使用真实数据存在安全和隐私风险，需要一种既能保护隐私又能有效评估公平性的方法。

Method: 通过隐私保护机制生成合成数据，保留原始数据的统计特性，用于公平性审计。

Result: 实验表明，合成数据能有效保留真实数据的公平性属性，同时保护敏感信息。

Conclusion: 该框架在关键和敏感领域中具有应用潜力，实现了公平性评估与隐私保护的平衡。

Abstract: Fairness auditing of AI systems can identify and quantify biases. However,
traditional auditing using real-world data raises security and privacy
concerns. It exposes auditors to security risks as they become custodians of
sensitive information and targets for cyberattacks. Privacy risks arise even
without direct breaches, as data analyses can inadvertently expose confidential
information. To address these, we propose a framework that leverages
differentially private synthetic data to audit the fairness of AI systems. By
applying privacy-preserving mechanisms, it generates synthetic data that
mirrors the statistical properties of the original dataset while ensuring
privacy. This method balances the goal of rigorous fairness auditing and the
need for strong privacy protections. Through experiments on real datasets like
Adult, COMPAS, and Diabetes, we compare fairness metrics of synthetic and real
data. By analyzing the alignment and discrepancies between these metrics, we
assess the capacity of synthetic data to preserve the fairness properties of
real data. Our results demonstrate the framework's ability to enable meaningful
fairness evaluations while safeguarding sensitive information, proving its
applicability across critical and sensitive domains.

</details>


### [137] [TRIED: Truly Innovative and Effective Detection Benchmark, developed by WITNESS](https://arxiv.org/abs/2504.21489)
*Shirin Anlen,Zuzanna Wojciak*

Main category: cs.CY

TL;DR: WITNESS提出TRIED Benchmark，用于评估AI检测工具的实际效果和创新性，强调工具需适应多样化语言、文化和技术背景，以提升全球信息可信度。


<details>
  <summary>Details</summary>
Motivation: 生成式AI和虚假合成媒体的兴起威胁全球信息生态系统，现有AI检测工具在现实场景中表现不佳，需改进。

Method: 基于前线经验、虚假AI案例和全球咨询，提出TRIED Benchmark框架，评估检测工具的实际影响和创新性。

Result: 报告为开发者、政策制定者和标准机构提供实用指导，推动透明、用户为中心的检测解决方案。

Conclusion: 采用TRIED Benchmark可促进创新、增强公众信任、提升AI素养，助力全球信息可信度。

Abstract: The rise of generative AI and deceptive synthetic media threatens the global
information ecosystem, especially across the Global Majority. This report from
WITNESS highlights the limitations of current AI detection tools, which often
underperform in real-world scenarios due to challenges related to
explainability, fairness, accessibility, and contextual relevance. In response,
WITNESS introduces the Truly Innovative and Effective AI Detection (TRIED)
Benchmark, a new framework for evaluating detection tools based on their
real-world impact and capacity for innovation. Drawing on frontline
experiences, deceptive AI cases, and global consultations, the report outlines
how detection tools must evolve to become truly innovative and relevant by
meeting diverse linguistic, cultural, and technological contexts. It offers
practical guidance for developers, policymakers, and standards bodies to design
accountable, transparent, and user-centered detection solutions, and
incorporate sociotechnical considerations into future AI standards, procedures
and evaluation frameworks. By adopting the TRIED Benchmark, stakeholders can
drive innovation, safeguard public trust, strengthen AI literacy, and
contribute to a more resilient global information credibility.

</details>


### [138] [Characterizing AI Agents for Alignment and Governance](https://arxiv.org/abs/2504.21848)
*Atoosa Kasirzadeh,Iason Gabriel*

Main category: cs.CY

TL;DR: 本文提出一个AI代理的框架，围绕自主性、效能、目标复杂性和通用性四个维度，探讨其治理挑战。


<details>
  <summary>Details</summary>
Motivation: 为AI代理设计有效的治理机制需理解其核心特性及其与部署、操作问题的关联。

Method: 提出四个维度的分级标准，并构建不同AI代理的“代理特征”框架。

Result: 框架揭示了不同AI代理的技术与非技术治理挑战，为治理提供依据。

Conclusion: 该框架帮助开发者、政策制定者和公众制定更符合社会目标的治理方法。

Abstract: The creation of effective governance mechanisms for AI agents requires a
deeper understanding of their core properties and how these properties relate
to questions surrounding the deployment and operation of agents in the world.
This paper provides a characterization of AI agents that focuses on four
dimensions: autonomy, efficacy, goal complexity, and generality. We propose
different gradations for each dimension, and argue that each dimension raises
unique questions about the design, operation, and governance of these systems.
Moreover, we draw upon this framework to construct "agentic profiles" for
different kinds of AI agents. These profiles help to illuminate cross-cutting
technical and non-technical governance challenges posed by different classes of
AI agents, ranging from narrow task-specific assistants to highly autonomous
general-purpose systems. By mapping out key axes of variation and continuity,
this framework provides developers, policymakers, and members of the public
with the opportunity to develop governance approaches that better align with
collective societal goals.

</details>


### [139] [Public Opinion and The Rise of Digital Minds: Perceived Risk, Trust, and Regulation Support](https://arxiv.org/abs/2504.21849)
*Justin B. Bullock,Janet V. T. Pauketat,Hsini Huang,Yi-Fan Wang,Jacy Reese Anthis*

Main category: cs.CY

TL;DR: 研究探讨公众对AI监管的偏好，发现风险感知和信任在政府、AI公司及技术中的差异显著影响监管支持度。


<details>
  <summary>Details</summary>
Motivation: 探讨公众对AI监管的态度，以应对生成式AI带来的社会风险。

Method: 使用2023年AIMS全国代表性调查数据，分析公众对政府、AI公司及技术的信任与风险感知如何影响监管偏好。

Result: 公众普遍支持AI监管，风险感知是关键因素；信任政府者更支持监管，信任AI公司或技术者则反之。

Conclusion: AI治理需平衡公众风险担忧与机构信任，为政策制定提供实证基础，并呼吁进一步研究。

Abstract: Governance institutions must respond to societal risks, including those posed
by generative AI. This study empirically examines how public trust in
institutions and AI technologies, along with perceived risks, shape preferences
for AI regulation. Using the nationally representative 2023 Artificial
Intelligence, Morality, and Sentience (AIMS) survey, we assess trust in
government, AI companies, and AI technologies, as well as public support for
regulatory measures such as slowing AI development or outright bans on advanced
AI. Our findings reveal broad public support for AI regulation, with risk
perception playing a significant role in shaping policy preferences.
Individuals with higher trust in government favor regulation, while those with
greater trust in AI companies and AI technologies are less inclined to support
restrictions. Trust in government and perceived risks significantly predict
preferences for both soft (e.g., slowing development) and strong (e.g., banning
AI systems) regulatory interventions. These results highlight the importance of
public opinion in AI governance. As AI capabilities advance, effective
regulation will require balancing public concerns about risks with trust in
institutions. This study provides a foundational empirical baseline for
policymakers navigating AI governance and underscores the need for further
research into public trust, risk perception, and regulatory strategies in the
evolving AI landscape.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [140] [Research on CNN-BiLSTM Network Traffic Anomaly Detection Model Based on MindSpore](https://arxiv.org/abs/2504.21008)
*Qiuyan Xiang,Shuang Wu,Dongze Wu,Yuxin Liu,Zhenkai Qin*

Main category: cs.CR

TL;DR: 论文提出了一种结合CNN和BiLSTM的网络流量异常检测模型，在MindSpore框架上实现，实验表明其性能优异。


<details>
  <summary>Details</summary>
Motivation: 随着IoT和IIoT技术的普及，网络架构复杂性和流量激增对传统安全机制提出了挑战，尤其是检测高频、多样且隐蔽的网络攻击。

Method: 提出了一种集成CNN和BiLSTM的模型，并在MindSpore框架上实现，使用NF-BoT-IoT数据集进行实验。

Result: 模型在准确率、精确率、召回率和F1分数上均达到99%，表现出色。

Conclusion: 该模型在网络入侵检测任务中具有强大的性能和鲁棒性。

Abstract: With the widespread adoption of the Internet of Things (IoT) and Industrial
IoT (IIoT) technologies, network architectures have become increasingly
complex, and the volume of traffic has grown substantially. This evolution
poses significant challenges to traditional security mechanisms, particularly
in detecting high-frequency, diverse, and highly covert network attacks. To
address these challenges, this study proposes a novel network traffic anomaly
detection model that integrates a Convolutional Neural Network (CNN) with a
Bidirectional Long Short-Term Memory (BiLSTM) network, implemented on the
MindSpore framework. Comprehensive experiments were conducted using the
NF-BoT-IoT dataset. The results demonstrate that the proposed model achieves
99% across accuracy, precision, recall, and F1-score, indicating its strong
performance and robustness in network intrusion detection tasks.

</details>


### [141] [Semantic-Aware Contrastive Fine-Tuning: Boosting Multimodal Malware Classification with Discriminative Embeddings](https://arxiv.org/abs/2504.21028)
*Ivan Montoya Sanchez,Shaswata Mitra,Aritran Piplai,Sudip Mittal*

Main category: cs.CR

TL;DR: 提出了一种对比微调（CFT）方法，通过基于余弦相似度的困难负样本选择优化LLM嵌入，显著提升恶意软件分类性能。


<details>
  <summary>Details</summary>
Motivation: 恶意软件变种快速演变，需要更鲁棒的分类方法。LLM虽能生成描述辅助分类，但语义嵌入重叠和与二进制行为特征的不对齐限制了其效果。

Method: 采用对比微调（CFT）方法，结合高相似度和中等级别负样本优化嵌入，并在MAML框架下集成多模态分类器。

Result: 在CIC-AndMal-2020和BODMAS数据集上，仅需20个样本即达到63.15%的分类准确率，优于基线11-21个百分点。

Conclusion: 该方法通过语义区分和可扩展框架，显著提升了恶意软件分类性能，并为LLM在网络安全中的应用提供了新思路。

Abstract: The rapid evolution of malware variants requires robust classification
methods to enhance cybersecurity. While Large Language Models (LLMs) offer
potential for generating malware descriptions to aid family classification,
their utility is limited by semantic embedding overlaps and misalignment with
binary behavioral features. We propose a contrastive fine-tuning (CFT) method
that refines LLM embeddings via targeted selection of hard negative samples
based on cosine similarity, enabling LLMs to distinguish between closely
related malware families. Our approach combines high-similarity negatives to
enhance discriminative power and mid-tier negatives to increase embedding
diversity, optimizing both precision and generalization. Evaluated on the
CIC-AndMal-2020 and BODMAS datasets, our refined embeddings are integrated into
a multimodal classifier within a Model-Agnostic Meta-Learning (MAML) framework
on a few-shot setting. Experiments demonstrate significant improvements: our
method achieves 63.15% classification accuracy with as few as 20 samples on
CIC-AndMal-2020, outperforming baselines by 11--21 percentage points and
surpassing prior negative sampling strategies. Ablation studies confirm the
superiority of similarity-based selection over random sampling, with gains of
10-23%. Additionally, fine-tuned LLMs generate attribute-aware descriptions
that generalize to unseen variants, bridging textual and binary feature gaps.
This work advances malware classification by enabling nuanced semantic
distinctions and provides a scalable framework for adapting LLMs to
cybersecurity challenges.

</details>


### [142] [PICO: Secure Transformers via Robust Prompt Isolation and Cybersecurity Oversight](https://arxiv.org/abs/2504.21029)
*Ben Goertzel,Paulos Yibelo*

Main category: cs.CR

TL;DR: 提出了一种名为PICO的鲁棒Transformer架构，通过双通道隔离系统指令与用户输入，结合安全专家代理和知识图谱，防止提示注入攻击。


<details>
  <summary>Details</summary>
Motivation: 当前Transformer模型易受提示注入攻击，PICO框架旨在提升模型的安全性和可靠性。

Method: 采用双通道处理指令与输入，结合MoE框架和CKG，并通过训练设计保持系统提示分支不可变。

Result: PICO框架能有效防止提示注入攻击，并通过数学公式、架构细节和案例研究验证其可行性。

Conclusion: PICO框架为安全响应生成提供了新思路，支持从头训练或低成本微调。

Abstract: We propose a robust transformer architecture designed to prevent prompt
injection attacks and ensure secure, reliable response generation. Our PICO
(Prompt Isolation and Cybersecurity Oversight) framework structurally separates
trusted system instructions from untrusted user inputs through dual channels
that are processed independently and merged only by a controlled, gated fusion
mechanism. In addition, we integrate a specialized Security Expert Agent within
a Mixture-of-Experts (MoE) framework and incorporate a Cybersecurity Knowledge
Graph (CKG) to supply domain-specific reasoning. Our training design further
ensures that the system prompt branch remains immutable while the rest of the
network learns to handle adversarial inputs safely. This PICO framework is
presented via a general mathematical formulation, then elaborated in terms of
the specifics of transformer architecture, and fleshed out via hypothetical
case studies including Policy Puppetry attacks. While the most effective
implementation may involve training transformers in a PICO-based way from
scratch, we also present a cost-effective fine-tuning approach.

</details>


### [143] [SAGA: A Security Architecture for Governing AI Agentic Systems](https://arxiv.org/abs/2504.21034)
*Georgios Syros,Anshuman Suri,Cristina Nita-Rotaru,Alina Oprea*

Main category: cs.CR

TL;DR: SAGA是一种用于管理自主代理的安全架构，提供用户对其代理生命周期的监督，确保安全性和可信性。


<details>
  <summary>Details</summary>
Motivation: 现有代理系统设计缺乏用户控制和管理功能，无法有效应对恶意代理的潜在风险。

Method: 提出SAGA架构，用户通过中央实体（Provider）注册代理，利用加密机制生成访问控制令牌，实现细粒度控制。

Result: 在多种代理任务和环境中评估SAGA，显示其性能开销极小且不影响任务效用。

Conclusion: SAGA支持安全可信的自主代理部署，促进敏感环境中该技术的负责任采用。

Abstract: Large Language Model (LLM)-based agents increasingly interact, collaborate,
and delegate tasks to one another autonomously with minimal human interaction.
Industry guidelines for agentic system governance emphasize the need for users
to maintain comprehensive control over their agents, mitigating potential
damage from malicious agents. Several proposed agentic system designs address
agent identity, authorization, and delegation, but remain purely theoretical,
without concrete implementation and evaluation. Most importantly, they do not
provide user-controlled agent management. To address this gap, we propose SAGA,
a Security Architecture for Governing Agentic systems, that offers user
oversight over their agents' lifecycle. In our design, users register their
agents with a central entity, the Provider, that maintains agents contact
information, user-defined access control policies, and helps agents enforce
these policies on inter-agent communication. We introduce a cryptographic
mechanism for deriving access control tokens, that offers fine-grained control
over an agent's interaction with other agents, balancing security and
performance consideration. We evaluate SAGA on several agentic tasks, using
agents in different geolocations, and multiple on-device and cloud LLMs,
demonstrating minimal performance overhead with no impact on underlying task
utility in a wide range of conditions. Our architecture enables secure and
trustworthy deployment of autonomous agents, accelerating the responsible
adoption of this technology in sensitive environments.

</details>


### [144] [Can Differentially Private Fine-tuning LLMs Protect Against Privacy Attacks?](https://arxiv.org/abs/2504.21036)
*Hao Du,Shang Liu,Yang Cao*

Main category: cs.CR

TL;DR: 本文研究了差分隐私（DP）在不同微调方法中对大语言模型（LLMs）隐私保护的效果，发现DP能显著降低隐私风险，但对模型效用的影响因方法而异。


<details>
  <summary>Details</summary>
Motivation: 微调LLMs时可能泄露敏感数据，DP虽提供理论保障，但其实际效果尚不明确。

Method: 通过数据提取和成员推理攻击，评估不同微调方法和隐私预算下的隐私风险。

Result: DP降低模型效用，但隐私风险显著降低；不同微调方法的隐私-效用权衡差异大。

Conclusion: 研究为隐私敏感的LLMs部署提供指导，并为未来优化隐私-效用权衡的研究奠定基础。

Abstract: Fine-tuning large language models (LLMs) has become an essential strategy for
adapting them to specialized tasks; however, this process introduces
significant privacy challenges, as sensitive training data may be inadvertently
memorized and exposed. Although differential privacy (DP) offers strong
theoretical guarantees against such leakage, its empirical privacy
effectiveness on LLMs remains unclear, especially under different fine-tuning
methods. In this paper, we systematically investigate the impact of DP across
fine-tuning methods and privacy budgets, using both data extraction and
membership inference attacks to assess empirical privacy risks. Our main
findings are as follows: (1) Differential privacy reduces model utility, but
its impact varies significantly across different fine-tuning methods. (2)
Without DP, the privacy risks of models fine-tuned with different approaches
differ considerably. (3) When DP is applied, even a relatively high privacy
budget can substantially lower privacy risk. (4) The privacy-utility trade-off
under DP training differs greatly among fine-tuning methods, with some methods
being unsuitable for DP due to severe utility degradation. Our results provide
practical guidance for privacy-conscious deployment of LLMs and pave the way
for future research on optimizing the privacy-utility trade-off in fine-tuning
methodologies.

</details>


### [145] [Security Bug Report Prediction Within and Across Projects: A Comparative Study of BERT and Random Forest](https://arxiv.org/abs/2504.21037)
*Farnaz Soltaniani,Mohammad Ghafari,Mohammed Sayagh*

Main category: cs.CR

TL;DR: 比较BERT和随机森林（RF）在安全漏洞报告（SBR）预测中的表现，发现RF在项目内预测中表现更好，而BERT在跨项目预测中表现更优。


<details>
  <summary>Details</summary>
Motivation: 早期检测安全漏洞报告（SBRs）对预防漏洞和确保系统可靠性至关重要，现有机器学习模型仍有改进空间。

Method: 对BERT和随机森林（RF）进行综合比较，评估其在SBR预测中的表现。

Result: RF在项目内预测中平均G-measure比BERT高34%，而BERT在跨项目预测中表现更优，G-measure达62%。

Conclusion: BERT在跨项目预测中表现显著优于RF，而RF在项目内预测中更有效。

Abstract: Early detection of security bug reports (SBRs) is crucial for preventing
vulnerabilities and ensuring system reliability. While machine learning models
have been developed for SBR prediction, their predictive performance still has
room for improvement. In this study, we conduct a comprehensive comparison
between BERT and Random Forest (RF), a competitive baseline for predicting
SBRs. The results show that RF outperforms BERT with a 34% higher average
G-measure for within-project predictions. Adding only SBRs from various
projects improves both models' average performance. However, including both
security and nonsecurity bug reports significantly reduces RF's average
performance to 46%, while boosts BERT to its best average performance of 66%,
surpassing RF. In cross-project SBR prediction, BERT achieves a remarkable 62%
G-measure, which is substantially higher than RF.

</details>


### [146] [Prefill-Based Jailbreak: A Novel Approach of Bypassing LLM Safety Boundary](https://arxiv.org/abs/2504.21038)
*Yakai Li,Jiekang Hu,Weiduan Sang,Luping Ma,Jing Xie,Weijuan Zhang,Aimin Yu,Shijie Zhao,Qingjia Huang,Qihang Zhou*

Main category: cs.CR

TL;DR: 本文提出了一种新的LLM越狱攻击方法，利用预填充功能绕过安全机制，显著提高了攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 研究越狱方法以暴露LLM的系统性漏洞，指导开发者持续改进安全性。

Method: 提出两种攻击变体：静态预填充（SP）和优化预填充（OP），通过操纵后续令牌的概率分布控制模型输出。

Result: 实验表明，OP方法在某些模型上攻击成功率高达99.82%，显著优于基线方法。

Conclusion: 强调需要强大的内容验证机制来缓解预填充特征的对抗性利用，所有代码和数据已公开。

Abstract: Large Language Models (LLMs) are designed to generate helpful and safe
content. However, adversarial attacks, commonly referred to as jailbreak, can
bypass their safety protocols, prompting LLMs to generate harmful content or
reveal sensitive data. Consequently, investigating jailbreak methodologies is
crucial for exposing systemic vulnerabilities within LLMs, ultimately guiding
the continuous implementation of security enhancements by developers. In this
paper, we introduce a novel jailbreak attack method that leverages the
prefilling feature of LLMs, a feature designed to enhance model output
constraints. Unlike traditional jailbreak methods, the proposed attack
circumvents LLMs' safety mechanisms by directly manipulating the probability
distribution of subsequent tokens, thereby exerting control over the model's
output. We propose two attack variants: Static Prefilling (SP), which employs a
universal prefill text, and Optimized Prefilling (OP), which iteratively
optimizes the prefill text to maximize the attack success rate. Experiments on
six state-of-the-art LLMs using the AdvBench benchmark validate the
effectiveness of our method and demonstrate its capability to substantially
enhance attack success rates when combined with existing jailbreak approaches.
The OP method achieved attack success rates of up to 99.82% on certain models,
significantly outperforming baseline methods. This work introduces a new
jailbreak attack method in LLMs, emphasizing the need for robust content
validation mechanisms to mitigate the adversarial exploitation of prefilling
features. All code and data used in this paper are publicly available.

</details>


### [147] [Llama-3.1-FoundationAI-SecurityLLM-Base-8B Technical Report](https://arxiv.org/abs/2504.21039)
*Paul Kassianik,Baturay Saglam,Alexander Chen,Blaine Nelson,Anu Vellore,Massimo Aufiero,Fraser Burch,Dhruv Kedia,Avi Zohary,Sajana Weerawardhena,Aman Priyanshu,Adam Swanda,Amy Chang,Hyrum Anderson,Kojin Oshiba,Omar Santos,Yaron Singer,Amin Karbasi*

Main category: cs.CR

TL;DR: 论文介绍了Foundation-Sec-8B，一个基于Llama 3.1架构的网络安全专用大语言模型，通过精心策划的网络安全语料库增强训练，性能媲美Llama 3.1-70B和GPT-4o-mini。


<details>
  <summary>Details</summary>
Motivation: 解决网络安全领域因缺乏专业训练数据和知识表示复杂性导致的大语言模型应用受限问题。

Method: 基于Llama 3.1架构，通过继续预训练网络安全语料库构建Foundation-Sec-8B模型。

Result: 在网络安全任务中表现与Llama 3.1-70B和GPT-4o-mini相当。

Conclusion: 公开模型以推动AI工具在网络安全领域的应用。

Abstract: As transformer-based large language models (LLMs) increasingly permeate
society, they have revolutionized domains such as software engineering,
creative writing, and digital arts. However, their adoption in cybersecurity
remains limited due to challenges like scarcity of specialized training data
and complexity of representing cybersecurity-specific knowledge. To address
these gaps, we present Foundation-Sec-8B, a cybersecurity-focused LLM built on
the Llama 3.1 architecture and enhanced through continued pretraining on a
carefully curated cybersecurity corpus. We evaluate Foundation-Sec-8B across
both established and new cybersecurity benchmarks, showing that it matches
Llama 3.1-70B and GPT-4o-mini in certain cybersecurity-specific tasks. By
releasing our model to the public, we aim to accelerate progress and adoption
of AI-driven tools in both public and private cybersecurity contexts.

</details>


### [148] [What's Pulling the Strings? Evaluating Integrity and Attribution in AI Training and Inference through Concept Shift](https://arxiv.org/abs/2504.21042)
*Jiamin Chang,Haoyang Li,Hammond Pearce,Ruoxi Sun,Bo Li,Minhui Xue*

Main category: cs.CR

TL;DR: ConceptLens是一个通用框架，利用预训练多模态模型分析概念漂移，检测数据中毒攻击、隐私风险和社会学偏见，增强AI系统的可信度。


<details>
  <summary>Details</summary>
Motivation: 随着AI的广泛应用，其可信度问题（如完整性、隐私、鲁棒性和偏见）日益突出，需要一种方法来评估和归因这些威胁。

Method: 提出ConceptLens框架，通过分析概念漂移识别威胁根源，检测数据中毒攻击、隐私风险，并揭示模型依赖的关键概念和社会学偏见。

Result: ConceptLens能有效检测数据中毒攻击，识别隐私风险和社会学偏见，并揭示模型弱点，为安全对齐提供见解。

Conclusion: ConceptLens为增强AI系统的可信度提供了可行见解，有助于加速AI的采用和创新。

Abstract: The growing adoption of artificial intelligence (AI) has amplified concerns
about trustworthiness, including integrity, privacy, robustness, and bias. To
assess and attribute these threats, we propose ConceptLens, a generic framework
that leverages pre-trained multimodal models to identify the root causes of
integrity threats by analyzing Concept Shift in probing samples. ConceptLens
demonstrates strong detection performance for vanilla data poisoning attacks
and uncovers vulnerabilities to bias injection, such as the generation of
covert advertisements through malicious concept shifts. It identifies privacy
risks in unaltered but high-risk samples, filters them before training, and
provides insights into model weaknesses arising from incomplete or imbalanced
training data. Additionally, at the model level, it attributes concepts that
the target model is overly dependent on, identifies misleading concepts, and
explains how disrupting key concepts negatively impacts the model. Furthermore,
it uncovers sociological biases in generative content, revealing disparities
across sociological contexts. Strikingly, ConceptLens reveals how safe training
and inference data can be unintentionally and easily exploited, potentially
undermining safety alignment. Our study informs actionable insights to breed
trust in AI systems, thereby speeding adoption and driving greater innovation.

</details>


### [149] [CodeBC: A More Secure Large Language Model for Smart Contract Code Generation in Blockchain](https://arxiv.org/abs/2504.21043)
*Lingxiang wang,Hainan Zhang,Qinnan Zhang,Ziwei Wang,Hongwei Zheng,Jin Dong,Zhiming Zheng*

Main category: cs.CR

TL;DR: CodeBC是一种专为生成安全的区块链智能合约而设计的代码生成模型，通过三阶段微调方法，无需依赖成对的漏洞标注数据，显著降低了漏洞率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）在生成代码时缺乏对安全漏洞的理解，尤其在智能合约等高安全编程任务中表现不足。现有方法依赖手动标注的漏洞数据，但低资源语言（如Solidity）缺乏此类数据。

Method: CodeBC基于CodeLlama，采用三阶段微调方法，利用漏洞和安全标签而非成对标注数据，训练模型区分漏洞代码和安全代码。

Result: 实验表明，CodeBC在BLEU、CodeBLEU和编译通过率上优于基线模型，同时显著降低漏洞率。

Conclusion: CodeBC的三阶段微调策略高效且成本低，为生成安全的智能合约代码提供了可行方案。

Abstract: Large language models (LLMs) excel at generating code from natural language
instructions, yet they often lack an understanding of security vulnerabilities.
This limitation makes it difficult for LLMs to avoid security risks in
generated code, particularly in high-security programming tasks such as smart
contract development for blockchain. Researchers have attempted to enhance the
vulnerability awareness of these models by training them to differentiate
between vulnerable and fixed code snippets. However, this approach relies
heavily on manually labeled vulnerability data, which is only available for
popular languages like Python and C++. For low-resource languages like
Solidity, used in smart contracts, large-scale annotated datasets are scarce
and difficult to obtain. To address this challenge, we introduce CodeBC, a code
generation model specifically designed for generating secure smart contracts in
blockchain. CodeBC employs a three-stage fine-tuning approach based on
CodeLlama, distinguishing itself from previous methods by not relying on
pairwise vulnerability location annotations. Instead, it leverages
vulnerability and security tags to teach the model the differences between
vulnerable and secure code. During the inference phase, the model leverages
security tags to generate secure and robust code. Experimental results
demonstrate that CodeBC outperforms baseline models in terms of BLEU, CodeBLEU,
and compilation pass rates, while significantly reducing vulnerability rates.
These findings validate the effectiveness and cost-efficiency of our
three-stage fine-tuning strategy, making CodeBC a promising solution for
generating secure smart contract code.

</details>


### [150] [AGATE: Stealthy Black-box Watermarking for Multimodal Model Copyright Protection](https://arxiv.org/abs/2504.21044)
*Jianbo Gao,Keke Gai,Jing Yu,Liehuang Zhu,Qi Wu*

Main category: cs.CR

TL;DR: 本文提出了一种名为AGATE的黑盒后门水印框架，用于保护多模态AI模型的版权，解决了现有方法在隐蔽性和鲁棒性上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有方法选择分布外数据作为后门水印，但容易被恶意检测和伪造，导致水印失效。因此，需要一种更隐蔽且鲁棒的版权保护方法。

Method: 提出AGATE框架，包括对抗性触发生成方法、后变换模块和两阶段水印验证机制。

Result: 在五个数据集的多模态图像-文本检索和图像分类任务中，AGATE均优于现有方法，并在两种对抗攻击场景下验证了其鲁棒性。

Conclusion: AGATE框架有效提升了多模态模型版权保护的隐蔽性和鲁棒性。

Abstract: Recent advancement in large-scale Artificial Intelligence (AI) models
offering multimodal services have become foundational in AI systems, making
them prime targets for model theft. Existing methods select Out-of-Distribution
(OoD) data as backdoor watermarks and retrain the original model for copyright
protection. However, existing methods are susceptible to malicious detection
and forgery by adversaries, resulting in watermark evasion. In this work, we
propose Model-\underline{ag}nostic Black-box Backdoor W\underline{ate}rmarking
Framework (AGATE) to address stealthiness and robustness challenges in
multimodal model copyright protection. Specifically, we propose an adversarial
trigger generation method to generate stealthy adversarial triggers from
ordinary dataset, providing visual fidelity while inducing semantic shifts. To
alleviate the issue of anomaly detection among model outputs, we propose a
post-transform module to correct the model output by narrowing the distance
between adversarial trigger image embedding and text embedding. Subsequently, a
two-phase watermark verification is proposed to judge whether the current model
infringes by comparing the two results with and without the transform module.
Consequently, we consistently outperform state-of-the-art methods across five
datasets in the downstream tasks of multimodal image-text retrieval and image
classification. Additionally, we validated the robustness of AGATE under two
adversarial attack scenarios.

</details>


### [151] [Leveraging LLM to Strengthen ML-Based Cross-Site Scripting Detection](https://arxiv.org/abs/2504.21045)
*Dennis Miczek,Divyesh Gabbireddy,Suman Saha*

Main category: cs.CR

TL;DR: 该论文提出了一种基于大型语言模型（LLM）的方法，用于生成复杂的混淆XSS攻击载荷，以提高机器学习模型检测XSS攻击的能力。实验表明，该方法显著提升了模型在混淆数据上的准确率。


<details>
  <summary>Details</summary>
Motivation: XSS攻击是严重的安全漏洞，尤其是混淆XSS攻击更难检测。现有机器学习模型在传统XSS数据上表现良好，但在混淆数据上准确率下降明显。

Method: 通过微调大型语言模型（LLM）自动生成复杂的混淆XSS载荷，并用于训练机器学习模型。

Result: 使用LLM生成的混淆数据训练后，模型准确率达到99.5%，且生成的混淆样本复杂度比其他工具高28.1%。

Conclusion: 该方法显著提升了机器学习模型检测复杂XSS攻击的能力，适用于实际应用安全场景。

Abstract: According to the Open Web Application Security Project (OWASP), Cross-Site
Scripting (XSS) is a critical security vulnerability. Despite decades of
research, XSS remains among the top 10 security vulnerabilities. Researchers
have proposed various techniques to protect systems from XSS attacks, with
machine learning (ML) being one of the most widely used methods. An ML model is
trained on a dataset to identify potential XSS threats, making its
effectiveness highly dependent on the size and diversity of the training data.
A variation of XSS is obfuscated XSS, where attackers apply obfuscation
techniques to alter the code's structure, making it challenging for security
systems to detect its malicious intent. Our study's random forest model was
trained on traditional (non-obfuscated) XSS data achieved 99.8% accuracy.
However, when tested against obfuscated XSS samples, accuracy dropped to 81.9%,
underscoring the importance of training ML models with obfuscated data to
improve their effectiveness in detecting XSS attacks. A significant challenge
is to generate highly complex obfuscated code despite the availability of
several public tools. These tools can only produce obfuscation up to certain
levels of complexity.
  In our proposed system, we fine-tune a Large Language Model (LLM) to generate
complex obfuscated XSS payloads automatically. By transforming original XSS
samples into diverse obfuscated variants, we create challenging training data
for ML model evaluation. Our approach achieved a 99.5% accuracy rate with the
obfuscated dataset. We also found that the obfuscated samples generated by the
LLMs were 28.1% more complex than those created by other tools, significantly
improving the model's ability to handle advanced XSS attacks and making it more
effective for real-world application security.

</details>


### [152] [Phishing URL Detection using Bi-LSTM](https://arxiv.org/abs/2504.21049)
*Sneha Baskota*

Main category: cs.CR

TL;DR: 本文提出了一种基于Bi-LSTM的深度学习模型，用于分类URL，提高了钓鱼攻击检测的准确性。


<details>
  <summary>Details</summary>
Motivation: 传统钓鱼检测系统存在高误报率和检测类型有限的问题，亟需更高效的解决方案。

Method: 使用双向长短期记忆网络（Bi-LSTM）对URL进行分类，分为良性、钓鱼、篡改和恶意软件四类。

Result: 在包含65万条URL的数据集上，模型达到97%的准确率，显著优于传统方法。

Conclusion: Bi-LSTM模型能有效提升钓鱼攻击检测的准确性和适应性。

Abstract: Phishing attacks threaten online users, often leading to data breaches,
financial losses, and identity theft. Traditional phishing detection systems
struggle with high false positive rates and are usually limited by the types of
attacks they can identify. This paper proposes a deep learning-based approach
using a Bidirectional Long Short-Term Memory (Bi-LSTM) network to classify URLs
into four categories: benign, phishing, defacement, and malware. The model
leverages sequential URL data and captures contextual information, improving
the accuracy of phishing detection. Experimental results on a dataset
comprising over 650,000 URLs demonstrate the model's effectiveness, achieving
97% accuracy and significant improvements over traditional techniques.

</details>


### [153] [SFIBA: Spatial-based Full-target Invisible Backdoor Attacks](https://arxiv.org/abs/2504.21052)
*Yangxu Yin,Honglong Chen,Yudong Gao,Peng Sun,Zhishuai Li,Weifeng Liu*

Main category: cs.CR

TL;DR: SFIBA是一种基于空间的全目标隐形后门攻击方法，通过局部空间区域和频率域触发注入，确保攻击的特异性和隐蔽性。


<details>
  <summary>Details</summary>
Motivation: 多目标后门攻击在现实场景中因缺乏触发特异性和隐蔽性而受限，SFIBA旨在解决这些问题。

Method: 利用快速傅里叶变换、离散小波变换和奇异值分解在频率域中注入触发器，并在像素空间中约束触发器形态。

Result: SFIBA在多个数据集和模型上表现出优异的攻击性能和隐蔽性，同时绕过现有防御。

Conclusion: SFIBA为多目标后门攻击提供了一种高效且隐蔽的解决方案。

Abstract: Multi-target backdoor attacks pose significant security threats to deep
neural networks, as they can preset multiple target classes through a single
backdoor injection. This allows attackers to control the model to misclassify
poisoned samples with triggers into any desired target class during inference,
exhibiting superior attack performance compared with conventional backdoor
attacks. However, existing multi-target backdoor attacks fail to guarantee
trigger specificity and stealthiness in black-box settings, resulting in two
main issues. First, they are unable to simultaneously target all classes when
only training data can be manipulated, limiting their effectiveness in
realistic attack scenarios. Second, the triggers often lack visual
imperceptibility, making poisoned samples easy to detect. To address these
problems, we propose a Spatial-based Full-target Invisible Backdoor Attack,
called SFIBA. It restricts triggers for different classes to specific local
spatial regions and morphologies in the pixel space to ensure specificity,
while employing a frequency-domain-based trigger injection method to guarantee
stealthiness. Specifically, for injection of each trigger, we first apply fast
fourier transform to obtain the amplitude spectrum of clean samples in local
spatial regions. Then, we employ discrete wavelet transform to extract the
features from the amplitude spectrum and use singular value decomposition to
integrate the trigger. Subsequently, we selectively filter parts of the trigger
in pixel space to implement trigger morphology constraints and adjust injection
coefficients based on visual effects. We conduct experiments on multiple
datasets and models. The results demonstrate that SFIBA can achieve excellent
attack performance and stealthiness, while preserving the model's performance
on benign samples, and can also bypass existing backdoor defenses.

</details>


### [154] [FFCBA: Feature-based Full-target Clean-label Backdoor Attacks](https://arxiv.org/abs/2504.21054)
*Yangxu Yin,Honglong Chen,Yudong Gao,Peng Sun,Liantao Wu,Zhe Li,Weifeng Liu*

Main category: cs.CR

TL;DR: 论文提出了一种基于特征的多目标干净标签后门攻击方法（FFCBA），包含两种范式：FSBA和FMBA，解决了现有干净标签攻击在多目标攻击中性能不稳定和扩展性差的问题。


<details>
  <summary>Details</summary>
Motivation: 现有干净标签后门攻击在多目标攻击中性能不稳定且难以扩展，而脏标签攻击容易被检测。因此，需要一种更隐蔽且高效的多目标干净标签攻击方法。

Method: FFCBA包含两种范式：FSBA利用类条件自编码器生成噪声触发器，确保触发器有效性；FMBA通过两阶段训练生成具有强目标类特征的触发器，适用于跨模型攻击。

Result: 实验表明，FFCBA在多数据集和模型上表现出色，攻击性能优异，并对现有防御方法具有鲁棒性。

Conclusion: FFCBA为多目标干净标签后门攻击提供了高效且隐蔽的解决方案，具有广泛的应用潜力。

Abstract: Backdoor attacks pose a significant threat to deep neural networks, as
backdoored models would misclassify poisoned samples with specific triggers
into target classes while maintaining normal performance on clean samples.
Among these, multi-target backdoor attacks can simultaneously target multiple
classes. However, existing multi-target backdoor attacks all follow the
dirty-label paradigm, where poisoned samples are mislabeled, and most of them
require an extremely high poisoning rate. This makes them easily detectable by
manual inspection. In contrast, clean-label attacks are more stealthy, as they
avoid modifying the labels of poisoned samples. However, they generally
struggle to achieve stable and satisfactory attack performance and often fail
to scale effectively to multi-target attacks. To address this issue, we propose
the Feature-based Full-target Clean-label Backdoor Attacks (FFCBA) which
consists of two paradigms: Feature-Spanning Backdoor Attacks (FSBA) and
Feature-Migrating Backdoor Attacks (FMBA). FSBA leverages class-conditional
autoencoders to generate noise triggers that align perturbed in-class samples
with the original category's features, ensuring the effectiveness, intra-class
consistency, inter-class specificity and natural-feature correlation of
triggers. While FSBA supports swift and efficient attacks, its cross-model
attack capability is relatively weak. FMBA employs a two-stage
class-conditional autoencoder training process that alternates between using
out-of-class samples and in-class samples. This allows FMBA to generate
triggers with strong target-class features, making it highly effective for
cross-model attacks. We conduct experiments on multiple datasets and models,
the results show that FFCBA achieves outstanding attack performance and
maintains desirable robustness against the state-of-the-art backdoor defenses.

</details>


### [155] [A False Sense of Privacy: Evaluating Textual Data Sanitization Beyond Surface-level Privacy Leakage](https://arxiv.org/abs/2504.21035)
*Rui Xin,Niloofar Mireshghallah,Shuyue Stella Li,Michael Duan,Hyunwoo Kim,Yejin Choi,Yulia Tsvetkov,Sewoong Oh,Pang Wei Koh*

Main category: cs.CR

TL;DR: 论文提出了一种新框架，用于评估去标识化数据的隐私风险，揭示当前方法无法有效防止语义级信息泄露。


<details>
  <summary>Details</summary>
Motivation: 现有敏感文本数据去标识化方法仅关注显式标识符的泄漏，而忽略了可能导致重新识别的细微文本标记，导致隐私保护不足。

Method: 提出了一种新框架，通过评估重新识别攻击来量化数据发布后的个体隐私风险。

Result: 实验表明，即使是看似无害的辅助信息（如日常社交活动）也可用于推断敏感属性（如年龄或药物使用史），Azure的PII移除工具在MedQA数据集中未能保护74%的信息。

Conclusion: 当前去标识化技术仅提供虚假的隐私保护，需开发更鲁棒的方法以防止语义级信息泄露。

Abstract: Sanitizing sensitive text data typically involves removing personally
identifiable information (PII) or generating synthetic data under the
assumption that these methods adequately protect privacy; however, their
effectiveness is often only assessed by measuring the leakage of explicit
identifiers but ignoring nuanced textual markers that can lead to
re-identification. We challenge the above illusion of privacy by proposing a
new framework that evaluates re-identification attacks to quantify individual
privacy risks upon data release. Our approach shows that seemingly innocuous
auxiliary information -- such as routine social activities -- can be used to
infer sensitive attributes like age or substance use history from sanitized
data. For instance, we demonstrate that Azure's commercial PII removal tool
fails to protect 74\% of information in the MedQA dataset. Although
differential privacy mitigates these risks to some extent, it significantly
reduces the utility of the sanitized text for downstream tasks. Our findings
indicate that current sanitization techniques offer a \textit{false sense of
privacy}, highlighting the need for more robust methods that protect against
semantic-level information leakage.

</details>


### [156] [Erased but Not Forgotten: How Backdoors Compromise Concept Erasure](https://arxiv.org/abs/2504.21072)
*Jonas Henry Grebe,Tobias Braun,Marcus Rohrbach,Anna Rohrbach*

Main category: cs.CR

TL;DR: 论文提出了一种新的威胁模型ToxE，揭示了现有机器遗忘技术易受针对性后门攻击的漏洞，并提出了更深入的攻击方法DISA。


<details>
  <summary>Details</summary>
Motivation: 大规模文本到图像扩散模型可能生成有害内容，现有遗忘技术试图通过微调消除这些内容，但存在被攻击的风险。

Method: 提出ToxE威胁模型，通过后门攻击（如DISA）绕过遗忘算法，测试了五种遗忘方法。

Result: ToxE攻击在名人身份消除中成功率最高达82%，在不良内容消除中暴露部分增加9倍。

Conclusion: 当前遗忘策略存在严重安全漏洞，需进一步改进。

Abstract: The expansion of large-scale text-to-image diffusion models has raised
growing concerns about their potential to generate undesirable or harmful
content, ranging from fabricated depictions of public figures to sexually
explicit images. To mitigate these risks, prior work has devised machine
unlearning techniques that attempt to erase unwanted concepts through
fine-tuning. However, in this paper, we introduce a new threat model, Toxic
Erasure (ToxE), and demonstrate how recent unlearning algorithms, including
those explicitly designed for robustness, can be circumvented through targeted
backdoor attacks. The threat is realized by establishing a link between a
trigger and the undesired content. Subsequent unlearning attempts fail to erase
this link, allowing adversaries to produce harmful content. We instantiate ToxE
via two established backdoor attacks: one targeting the text encoder and
another manipulating the cross-attention layers. Further, we introduce Deep
Intervention Score-based Attack (DISA), a novel, deeper backdoor attack that
optimizes the entire U-Net using a score-based objective, improving the
attack's persistence across different erasure methods. We evaluate five recent
concept erasure methods against our threat model. For celebrity identity
erasure, our deep attack circumvents erasure with up to 82% success, averaging
57% across all erasure methods. For explicit content erasure, ToxE attacks can
elicit up to 9 times more exposed body parts, with DISA yielding an average
increase by a factor of 2.9. These results highlight a critical security gap in
current unlearning strategies.

</details>


### [157] [SecRepoBench: Benchmarking LLMs for Secure Code Generation in Real-World Repositories](https://arxiv.org/abs/2504.21205)
*Connor Dilgren,Purva Chiniya,Luke Griffith,Yu Ding,Yizheng Chen*

Main category: cs.CR

TL;DR: SecRepoBench是一个用于评估LLMs在真实代码库中生成安全代码能力的基准测试，包含318个任务和15种CWE。测试发现LLMs在生成正确和安全代码方面表现不佳，且现有提示工程技术在库级安全代码生成中效果有限。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs在生成安全代码方面的能力不足，尤其是在真实代码库级别，需要更有效的评估方法和改进方向。

Method: 使用SecRepoBench（包含318个任务和27个C/C++代码库）评估19种先进LLMs，并测试提示工程技术的效果。

Result: LLMs在生成安全代码方面表现较差，且现有提示工程技术效果有限。SecRepoBench是目前最难的基准测试。

Conclusion: 研究为提升LLMs在真实代码库中生成安全代码的能力提供了潜在方向。

Abstract: This paper introduces SecRepoBench, a benchmark to evaluate LLMs on secure
code generation in real-world repositories. SecRepoBench has 318 code
generation tasks in 27 C/C++ repositories, covering 15 CWEs. We evaluate 19
state-of-the-art LLMs using our benchmark and find that the models struggle
with generating correct and secure code. In addition, the performance of LLMs
to generate self-contained programs as measured by prior benchmarks do not
translate to comparative performance at generating secure and correct code at
the repository level in SecRepoBench. We show that the state-of-the-art prompt
engineering techniques become less effective when applied to the repository
level secure code generation problem. We conduct extensive experiments,
including an agentic technique to generate secure code, to demonstrate that our
benchmark is currently the most difficult secure coding benchmark, compared to
previous state-of-the-art benchmarks. Finally, our comprehensive analysis
provides insights into potential directions for enhancing the ability of LLMs
to generate correct and secure code in real-world repositories.

</details>


### [158] [Federated One-Shot Learning with Data Privacy and Objective-Hiding](https://arxiv.org/abs/2504.21182)
*Maximilian Egger,Rüdiger Urbanke,Rawad Bitar*

Main category: cs.CR

TL;DR: 提出了一种新颖的联邦学习方法，同时保护客户端数据和联邦目标的隐私，结合知识蒸馏和私有信息检索技术。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中客户端数据的隐私保护已广泛研究，但联邦目标的隐私保护较少关注，本文旨在同时解决这两方面问题。

Method: 采用三阶段方法：客户端本地计算（阶段0）、结果共享（阶段1）、联邦目标安全检索（阶段2），结合多方计算和私有信息检索协议。

Result: 该方法在信息论隐私保证下优于现有工具。

Conclusion: 通过精心设计的协议，本文方法成功解决了联邦学习中的双重隐私问题，并展示了优越性能。

Abstract: Privacy in federated learning is crucial, encompassing two key aspects:
safeguarding the privacy of clients' data and maintaining the privacy of the
federator's objective from the clients. While the first aspect has been
extensively studied, the second has received much less attention.
  We present a novel approach that addresses both concerns simultaneously,
drawing inspiration from techniques in knowledge distillation and private
information retrieval to provide strong information-theoretic privacy
guarantees.
  Traditional private function computation methods could be used here; however,
they are typically limited to linear or polynomial functions. To overcome these
constraints, our approach unfolds in three stages. In stage 0, clients perform
the necessary computations locally. In stage 1, these results are shared among
the clients, and in stage 2, the federator retrieves its desired objective
without compromising the privacy of the clients' data. The crux of the method
is a carefully designed protocol that combines secret-sharing-based multi-party
computation and a graph-based private information retrieval scheme. We show
that our method outperforms existing tools from the literature when properly
adapted to this setting.

</details>


### [159] [CachePrune: Neural-Based Attribution Defense Against Indirect Prompt Injection Attacks](https://arxiv.org/abs/2504.21228)
*Rui Wang,Junda Wu,Yu Xia,Tong Yu,Ruiyi Zhang,Ryan Rossi,Lina Yao,Julian McAuley*

Main category: cs.CR

TL;DR: 论文提出CachePrune方法，通过识别并修剪KV缓存中的任务触发神经元，防御间接提示注入攻击，提升LLMs的安全性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）易受间接提示注入攻击，导致模型偏离用户指令。论文旨在解决这一漏洞，开发更安全的AI系统。

Method: 提出CachePrune方法，利用特征归因和损失函数（基于DPO目标上界）识别并修剪KV缓存中的任务触发神经元，避免模型将输入提示上下文误认为指令。

Result: 实验表明，CachePrune显著降低攻击成功率，同时不影响响应质量。

Conclusion: CachePrune有效防御间接提示注入攻击，为LLMs的安全性提供了实用解决方案。

Abstract: Large Language Models (LLMs) are identified as being susceptible to indirect
prompt injection attack, where the model undesirably deviates from
user-provided instructions by executing tasks injected in the prompt context.
This vulnerability stems from LLMs' inability to distinguish between data and
instructions within a prompt. In this paper, we propose CachePrune that defends
against this attack by identifying and pruning task-triggering neurons from the
KV cache of the input prompt context. By pruning such neurons, we encourage the
LLM to treat the text spans of input prompt context as only pure data, instead
of any indicator of instruction following. These neurons are identified via
feature attribution with a loss function induced from an upperbound of the
Direct Preference Optimization (DPO) objective. We show that such a loss
function enables effective feature attribution with only a few samples. We
further improve on the quality of feature attribution, by exploiting an
observed triggering effect in instruction following. Our approach does not
impose any formatting on the original prompt or introduce extra test-time LLM
calls. Experiments show that CachePrune significantly reduces attack success
rates without compromising the response quality. Note: This paper aims to
defend against indirect prompt injection attacks, with the goal of developing
more secure and robust AI systems.

</details>


### [160] [How to Backdoor the Knowledge Distillation](https://arxiv.org/abs/2504.21323)
*Chen Wu,Qian Ma,Prasenjit Mitra,Sencun Zhu*

Main category: cs.CR

TL;DR: 论文提出了一种新型攻击方法，通过污染蒸馏数据集中的对抗样本嵌入后门触发器，成功利用知识蒸馏过程的漏洞，揭示了其潜在的安全风险。


<details>
  <summary>Details</summary>
Motivation: 传统认为知识蒸馏是安全的，因为教师模型是干净的，但本文挑战了这一假设，揭示了蒸馏过程可能被攻击的漏洞。

Method: 通过策略性地在蒸馏数据集中嵌入对抗样本和后门触发器，攻击学生模型，同时保持教师模型的完整性。

Result: 实验表明该方法具有鲁棒性、隐蔽性和高效性，成功揭示了知识蒸馏的新漏洞。

Conclusion: 研究为未来保护知识蒸馏过程免受后门攻击提供了方向，强调了其潜在安全风险。

Abstract: Knowledge distillation has become a cornerstone in modern machine learning
systems, celebrated for its ability to transfer knowledge from a large, complex
teacher model to a more efficient student model. Traditionally, this process is
regarded as secure, assuming the teacher model is clean. This belief stems from
conventional backdoor attacks relying on poisoned training data with backdoor
triggers and attacker-chosen labels, which are not involved in the distillation
process. Instead, knowledge distillation uses the outputs of a clean teacher
model to guide the student model, inherently preventing recognition or response
to backdoor triggers as intended by an attacker. In this paper, we challenge
this assumption by introducing a novel attack methodology that strategically
poisons the distillation dataset with adversarial examples embedded with
backdoor triggers. This technique allows for the stealthy compromise of the
student model while maintaining the integrity of the teacher model. Our
innovative approach represents the first successful exploitation of
vulnerabilities within the knowledge distillation process using clean teacher
models. Through extensive experiments conducted across various datasets and
attack settings, we demonstrate the robustness, stealthiness, and effectiveness
of our method. Our findings reveal previously unrecognized vulnerabilities and
pave the way for future research aimed at securing knowledge distillation
processes against backdoor attacks.

</details>


### [161] [Optimizing Mouse Dynamics for User Authentication by Machine Learning: Addressing Data Sufficiency, Accuracy-Practicality Trade-off, and Model Performance Challenges](https://arxiv.org/abs/2504.21415)
*Yi Wang,Chengyv Wu,Yang Liao,Maowei You*

Main category: cs.CR

TL;DR: 提出了一种基于鼠标动态的用户认证方法，通过统计方法和深度学习框架优化数据量和行为模式捕捉，显著提升了认证性能。


<details>
  <summary>Details</summary>
Motivation: 传统用户认证方法在可用性、成本和安全性方面存在局限，鼠标动态认证提供了一种经济高效且非侵入的解决方案，但仍需解决数据量优化和行为模式捕捉的挑战。

Method: 使用高斯核密度估计和KL散度确定训练数据量，引入MAU优化行为表示，设计LT-AMouse框架结合1D-ResNet和GRU提取特征和建模时间依赖。

Result: 在Balabit和DFL数据集上显著减少数据规模，DFL数据集减少10倍，认证性能AUC达98.52%（DFL）和94.65%（Balabit），超越当前最优。

Conclusion: 提出的方法在减少数据量和提升认证性能方面表现优异，为鼠标动态认证提供了实用且高效的解决方案。

Abstract: User authentication is essential to ensure secure access to computer systems,
yet traditional methods face limitations in usability, cost, and security.
Mouse dynamics authentication, based on the analysis of users' natural
interaction behaviors with mouse devices, offers a cost-effective,
non-intrusive, and adaptable solution. However, challenges remain in
determining the optimal data volume, balancing accuracy and practicality, and
effectively capturing temporal behavioral patterns. In this study, we propose a
statistical method using Gaussian kernel density estimate (KDE) and
Kullback-Leibler (KL) divergence to estimate the sufficient data volume for
training authentication models. We introduce the Mouse Authentication Unit
(MAU), leveraging Approximate Entropy (ApEn) to optimize segment length for
efficient and accurate behavioral representation. Furthermore, we design the
Local-Time Mouse Authentication (LT-AMouse) framework, integrating 1D-ResNet
for local feature extraction and GRU for modeling long-term temporal
dependencies. Taking the Balabit and DFL datasets as examples, we significantly
reduced the data scale, particularly by a factor of 10 for the DFL dataset,
greatly alleviating the training burden. Additionally, we determined the
optimal input recognition unit length for the user authentication system on
different datasets based on the slope of Approximate Entropy. Training with
imbalanced samples, our model achieved a successful defense AUC 98.52% for
blind attack on the DFL dataset and 94.65% on the Balabit dataset, surpassing
the current sota performance.

</details>


### [162] [Traceback of Poisoning Attacks to Retrieval-Augmented Generation](https://arxiv.org/abs/2504.21668)
*Baolei Zhang,Haoran Xin,Minghong Fang,Zhuqing Liu,Biao Yi,Tong Li,Zheli Liu*

Main category: cs.CR

TL;DR: RAGForensics是一种新型的追踪系统，用于识别RAG系统中被投毒的知识库文本，提升安全性。


<details>
  <summary>Details</summary>
Motivation: 现有防御方法主要针对推理时缓解，难以应对复杂的投毒攻击，因此需要一种新的追踪机制。

Method: RAGForensics通过迭代检索知识库子集，并利用定制提示引导LLM检测潜在投毒文本。

Result: 实验证明RAGForensics能有效对抗最先进的投毒攻击。

Conclusion: RAGForensics为RAG系统提供了一种实用且高效的防御机制，开创了投毒文本追踪的先河。

Abstract: Large language models (LLMs) integrated with retrieval-augmented generation
(RAG) systems improve accuracy by leveraging external knowledge sources.
However, recent research has revealed RAG's susceptibility to poisoning
attacks, where the attacker injects poisoned texts into the knowledge database,
leading to attacker-desired responses. Existing defenses, which predominantly
focus on inference-time mitigation, have proven insufficient against
sophisticated attacks. In this paper, we introduce RAGForensics, the first
traceback system for RAG, designed to identify poisoned texts within the
knowledge database that are responsible for the attacks. RAGForensics operates
iteratively, first retrieving a subset of texts from the database and then
utilizing a specially crafted prompt to guide an LLM in detecting potential
poisoning texts. Empirical evaluations across multiple datasets demonstrate the
effectiveness of RAGForensics against state-of-the-art poisoning attacks. This
work pioneers the traceback of poisoned texts in RAG systems, providing a
practical and promising defense mechanism to enhance their security.

</details>


### [163] [XBreaking: Explainable Artificial Intelligence for Jailbreaking LLMs](https://arxiv.org/abs/2504.21700)
*Marco Arazzi,Vignesh Kumar Kembu,Antonino Nocera,Vinod P*

Main category: cs.CR

TL;DR: 论文提出了一种名为XBreaking的新型LLM越狱攻击方法，通过可解释AI分析审查机制的行为模式，并利用这些模式设计有针对性的噪声注入攻击。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）在关键应用场景中的可靠采用受到安全威胁的阻碍，尤其是越狱攻击的威胁。现有方法多为生成-测试策略，缺乏对审查机制的深入理解。

Method: 提出了一种可解释AI方法，比较分析审查和未审查模型的行为，提取可被利用的对齐模式，并设计XBreaking攻击，通过针对性噪声注入突破LLM的安全约束。

Result: 实验验证了XBreaking的有效性和性能，并提供了关于审查机制的重要见解。

Conclusion: XBreaking是一种高效的越狱攻击方法，揭示了LLM审查机制的潜在漏洞，为未来安全防护提供了参考。

Abstract: Large Language Models are fundamental actors in the modern IT landscape
dominated by AI solutions. However, security threats associated with them might
prevent their reliable adoption in critical application scenarios such as
government organizations and medical institutions. For this reason, commercial
LLMs typically undergo a sophisticated censoring mechanism to eliminate any
harmful output they could possibly produce. In response to this, LLM
Jailbreaking is a significant threat to such protections, and many previous
approaches have already demonstrated its effectiveness across diverse domains.
Existing jailbreak proposals mostly adopt a generate-and-test strategy to craft
malicious input. To improve the comprehension of censoring mechanisms and
design a targeted jailbreak attack, we propose an Explainable-AI solution that
comparatively analyzes the behavior of censored and uncensored models to derive
unique exploitable alignment patterns. Then, we propose XBreaking, a novel
jailbreak attack that exploits these unique patterns to break the security
constraints of LLMs by targeted noise injection. Our thorough experimental
campaign returns important insights about the censoring mechanisms and
demonstrates the effectiveness and performance of our attack.

</details>


### [164] [Cert-SSB: Toward Certified Sample-Specific Backdoor Defense](https://arxiv.org/abs/2504.21730)
*Ting Qiao,Yingjia Wang,Xing Liu,Sixing Wu,Jianbing Li,Yiming Li*

Main category: cs.CR

TL;DR: 论文提出了一种样本特定的认证后门防御方法Cert-SSB，通过优化每个样本的噪声水平并动态调整认证区域，提高了防御性能。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络（DNNs）容易受到后门攻击，现有防御方法（如随机平滑）假设所有样本与决策边界距离相同，导致性能不佳。

Method: 提出Cert-SSB方法，使用随机梯度上升优化每个样本的噪声水平，训练多个平滑模型，并通过动态调整认证区域的存储更新认证方法生成最终预测。

Result: 在多个基准数据集上的实验证明了Cert-SSB的有效性，优于现有防御方法。

Conclusion: Cert-SSB通过样本特定的噪声优化和动态认证调整，显著提高了对后门攻击的防御能力。

Abstract: Deep neural networks (DNNs) are vulnerable to backdoor attacks, where an
attacker manipulates a small portion of the training data to implant hidden
backdoors into the model. The compromised model behaves normally on clean
samples but misclassifies backdoored samples into the attacker-specified target
class, posing a significant threat to real-world DNN applications. Currently,
several empirical defense methods have been proposed to mitigate backdoor
attacks, but they are often bypassed by more advanced backdoor techniques. In
contrast, certified defenses based on randomized smoothing have shown promise
by adding random noise to training and testing samples to counteract backdoor
attacks. In this paper, we reveal that existing randomized smoothing defenses
implicitly assume that all samples are equidistant from the decision boundary.
However, it may not hold in practice, leading to suboptimal certification
performance. To address this issue, we propose a sample-specific certified
backdoor defense method, termed Cert-SSB. Cert-SSB first employs stochastic
gradient ascent to optimize the noise magnitude for each sample, ensuring a
sample-specific noise level that is then applied to multiple poisoned training
sets to retrain several smoothed models. After that, Cert-SSB aggregates the
predictions of multiple smoothed models to generate the final robust
prediction. In particular, in this case, existing certification methods become
inapplicable since the optimized noise varies across different samples. To
conquer this challenge, we introduce a storage-update-based certification
method, which dynamically adjusts each sample's certification region to improve
certification performance. We conduct extensive experiments on multiple
benchmark datasets, demonstrating the effectiveness of our proposed method. Our
code is available at https://github.com/NcepuQiaoTing/Cert-SSB.

</details>


### [165] [A Comprehensive Study of Exploitable Patterns in Smart Contracts: From Vulnerability to Defense](https://arxiv.org/abs/2504.21480)
*Yuchen Ding,Hongli Peng,Xiaoqi Li*

Main category: cs.CR

TL;DR: 本文分析了以太坊智能合约中的两种关键安全风险（重入和整数溢出），探讨其机制、攻击场景及应对措施。


<details>
  <summary>Details</summary>
Motivation: 智能合约的安全性挑战日益突出，漏洞频发导致重大经济损失，亟需深入研究。

Method: 通过分析Solidity编写的以太坊智能合约，重点研究重入和整数溢出漏洞的机制，并模拟攻击场景。

Result: 揭示了这两种漏洞的具体表现及危害，同时评估了有效的防御措施。

Conclusion: 智能合约安全需多阶段防护，针对重入和整数溢出的防御措施对提升整体安全性至关重要。

Abstract: With the rapid advancement of blockchain technology, smart contracts have
enabled the implementation of increasingly complex functionalities. However,
ensuring the security of smart contracts remains a persistent challenge across
the stages of development, compilation, and execution. Vulnerabilities within
smart contracts not only undermine the security of individual applications but
also pose significant risks to the broader blockchain ecosystem, as
demonstrated by the growing frequency of attacks since 2016, resulting in
substantial financial losses. This paper provides a comprehensive analysis of
key security risks in Ethereum smart contracts, specifically those written in
Solidity and executed on the Ethereum Virtual Machine (EVM). We focus on two
prevalent and critical vulnerability types (reentrancy and integer overflow) by
examining their underlying mechanisms, replicating attack scenarios, and
assessing effective countermeasures.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [166] [Estimation of discrete distributions in relative entropy, and the deviations of the missing mass](https://arxiv.org/abs/2504.21787)
*Jaouad Mourtada*

Main category: math.ST

TL;DR: 本文研究了从独立同分布样本中估计有限字母表上分布的问题，以相对熵（Kullback-Leibler散度）衡量准确性。分析了经典Laplace估计器，并探讨了高概率风险最优性及稀疏性适应方法。


<details>
  <summary>Details</summary>
Motivation: 研究高概率保证下的分布估计问题，特别是在字母表大小超过样本量的稀疏场景下。

Method: 分析了Laplace估计器，提出了置信依赖平滑技术，并引入了数据依赖平滑的估计器以适应稀疏性。

Result: 证明了Laplace估计器的最优性，并给出了高概率风险的最小最大值最优解。稀疏场景下，新估计器表现优异。

Conclusion: 高概率风险最优解包含额外的对数因子，稀疏性适应方法在字母表较大时表现突出。

Abstract: We study the problem of estimating a distribution over a finite alphabet from
an i.i.d. sample, with accuracy measured in relative entropy (Kullback-Leibler
divergence). While optimal expected risk bounds are known, high-probability
guarantees remain less well-understood. First, we analyze the classical Laplace
(add-$1$) estimator, obtaining matching upper and lower bounds on its
performance and showing its optimality among confidence-independent estimators.
We then characterize the minimax-optimal high-probability risk achievable by
any estimator, which is attained via a simple confidence-dependent smoothing
technique. Interestingly, the optimal non-asymptotic risk contains an
additional logarithmic factor over the ideal asymptotic risk. Next, motivated
by scenarios where the alphabet exceeds the sample size, we investigate methods
that adapt to the sparsity of the distribution at hand. We introduce an
estimator using data-dependent smoothing, for which we establish a
high-probability risk bound depending on two effective sparsity parameters. As
part of the analysis, we also derive a sharp high-probability upper bound on
the missing mass.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [167] [QAOA Parameter Transferability for Maximum Independent Set using Graph Attention Networks](https://arxiv.org/abs/2504.21135)
*Hanjing Xu,Xiaoyuan Liu,Alex Pothen,Ilya Safro*

Main category: quant-ph

TL;DR: 论文提出了一种基于图注意力网络（GAT）的QAOA参数迁移方案，用于解决最大独立集（MIS）问题，并结合混合分布式资源感知算法（HyDRA-MIS）在NISQ计算机上实现高效求解。


<details>
  <summary>Details</summary>
Motivation: 量子近似优化算法（QAOA）在解决组合优化问题时需要优化非线性、非凸参数，传统方法难以扩展到大图。论文旨在通过参数迁移和问题分解，提升QAOA在大规模图上的性能。

Method: 使用GAT迁移小图（12和14顶点）的优化参数到大图；设计HyDRA-MIS算法将大问题分解为适合NISQ计算机的小问题。

Result: 在数千顶点的大图上，该方法与经典MIS求解器KaMIS相比表现出竞争力。

Conclusion: GAT参数迁移与HyDRA-MIS的结合为QAOA在大规模组合优化问题中的应用提供了有效途径。

Abstract: The quantum approximate optimization algorithm (QAOA) is one of the promising
variational approaches of quantum computing to solve combinatorial optimization
problems. In QAOA, variational parameters need to be optimized by solving a
series of nonlinear, nonconvex optimization programs. In this work, we propose
a QAOA parameter transfer scheme using Graph Attention Networks (GAT) to solve
Maximum Independent Set (MIS) problems. We prepare optimized parameters for
graphs of 12 and 14 vertices and use GATs to transfer their parameters to
larger graphs. Additionally, we design a hybrid distributed resource-aware
algorithm for MIS (HyDRA-MIS), which decomposes large problems into smaller
ones that can fit onto noisy intermediate-scale quantum (NISQ) computers. We
integrate our GAT-based parameter transfer approach to HyDRA-MIS and
demonstrate competitive results compared to KaMIS, a state-of-the-art classical
MIS solver, on graphs with several thousands vertices.

</details>


### [168] [Efficient Quantum-Safe Homomorphic Encryption for Quantum Computer Programs](https://arxiv.org/abs/2504.21235)
*Ben Goertzel*

Main category: quant-ph

TL;DR: 提出了一种基于格的量子程序同态加密方案，支持量子安全，并解决了实际应用中的问题。


<details>
  <summary>Details</summary>
Motivation: 将经典同态加密扩展到量子领域，同时确保对量子攻击的安全性。

Method: 使用模块学习带错误（MLWE）格替代复合阶群，并引入有界自然超函子（BNSF）隐藏振幅。通过四混合归约证明安全性。

Result: 方案在100量子比特、深度10^3的量子证明中仅需10毫秒，公钥仅32字节，CCA级密钥小于300 kB。

Conclusion: 全同态量子推理与近量子云和标准后量子安全假设兼容。

Abstract: We present a lattice-based scheme for homomorphic evaluation of quantum
programs and proofs that remains secure against quantum adversaries. Classical
homomorphic encryption is lifted to the quantum setting by replacing
composite-order groups with Module Learning-With-Errors (MLWE) lattices and by
generalizing polynomial functors to bounded natural super functors (BNSFs). A
secret depolarizing BNSF mask hides amplitudes, while each quantum state is
stored as an MLWE ciphertext pair. We formalize security with the qIND-CPA game
that allows coherent access to the encryption oracle and give a four-hybrid
reduction to decisional MLWE.
  The design also covers practical issues usually left open. A typed QC-bridge
keeps classical bits produced by measurements encrypted yet still usable as
controls, with weak-measurement semantics for expectation-value workloads.
Encrypted Pauli twirls add circuit privacy. If a fixed knowledge base is
needed, its axioms are shipped as MLWE "capsules"; the evaluator can use them
but cannot read them. A rho-calculus driver schedules encrypted tasks across
several QPUs and records an auditable trace on an RChain-style ledger.
  Performance analysis shows that the extra lattice arithmetic fits inside
today's QPU idle windows: a 100-qubit, depth-10^3 teleportation-based proof
runs in about 10 ms, the public key (seed only) is 32 bytes, and even a
CCA-level key stays below 300 kB. A photonic Dirac-3 prototype that executes
homomorphic teleportation plus knowledge-base-relative amplitude checks appears
feasible with current hardware. These results indicate that fully homomorphic,
knowledge-base-aware quantum reasoning is compatible with near-term quantum
clouds and standard post-quantum security assumptions.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [169] [Participatory AI, Public Sector AI, Differential Privacy, Conversational Interfaces, Explainable AI, Citizen Engagement in AI](https://arxiv.org/abs/2504.21297)
*Wenjun Yang,Eyhab Al-Masri*

Main category: cs.IT

TL;DR: 论文提出了一种对话式界面系统，用于公共部门中差分隐私AI系统的参与式设计，通过自适应协议、可解释噪声注入框架和法律合规机制，平衡隐私保护与民主问责。


<details>
  <summary>Details</summary>
Motivation: 解决如何在公共部门应用中平衡数学隐私保证与民主问责的挑战。

Method: 提出三个关键贡献：自适应ε选择协议、可解释噪声注入框架和动态法律合规机制。

Result: 结果表明对话式界面能增强公众对算法隐私机制的参与，确保隐私保护AI既数学稳健又民主问责。

Conclusion: 该研究推动了参与式AI实践，为公共部门治理中的隐私保护AI提供了新方法。

Abstract: This paper introduces a conversational interface system that enables
participatory design of differentially private AI systems in public sector
applications. Addressing the challenge of balancing mathematical privacy
guarantees with democratic accountability, we propose three key contributions:
(1) an adaptive $\epsilon$-selection protocol leveraging TOPSIS multi-criteria
decision analysis to align citizen preferences with differential privacy (DP)
parameters, (2) an explainable noise-injection framework featuring real-time
Mean Absolute Error (MAE) visualizations and GPT-4-powered impact analysis, and
(3) an integrated legal-compliance mechanism that dynamically modulates privacy
budgets based on evolving regulatory constraints. Our results advance
participatory AI practices by demonstrating how conversational interfaces can
enhance public engagement in algorithmic privacy mechanisms, ensuring that
privacy-preserving AI in public sector governance remains both mathematically
robust and democratically accountable.

</details>


### [170] [Sionna RT: Technical Report](https://arxiv.org/abs/2504.21719)
*Fayçal Aït Aoudia,Jakob Hoydis,Merlin Nimier-David,Sebastian Cammerer,Alexander Keller*

Main category: cs.IT

TL;DR: Sionna是一个开源、GPU加速的库，其0.14版本引入了射线追踪功能，用于模拟无线电波传播。Sionna 1.0对射线追踪器进行了全面改进，提升了速度、内存效率和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 提供一种高效、可扩展且可微分的无线电波传播模拟工具，以支持系统设计和环境参数的优化。

Method: 使用射线追踪技术（SBR与图像法结合）计算信道脉冲响应（CIRs），并通过哈希机制消除重复路径；采用纯SBR方法计算无线电地图。

Result: Sionna RT实现了高效的无线电波传播模拟，支持对系统参数和环境参数的梯度计算。

Conclusion: Sionna RT为无线电波传播模拟提供了高效、可扩展且可微分的解决方案，但仍存在一些局限性。

Abstract: Sionna is an open-source, GPU-accelerated library that, as of version 0.14,
incorporates a ray tracer for simulating radio wave propagation. A unique
feature of Sionna RT is differentiability, enabling the calculation of
gradients for the channel impulse responses (CIRs), radio maps, and other
related metrics with respect to system and environmental parameters, such as
material properties, antenna patterns, and array geometries. The release of
Sionna 1.0 provides a complete overhaul of the ray tracer, significantly
improving its speed, memory efficiency, and extensibility. This document
details the algorithms employed by Sionna RT to simulate radio wave propagation
efficiently, while also addressing their current limitations. Given that the
computation of CIRs and radio maps requires distinct algorithms, these are
detailed in separate sections. For CIRs, Sionna RT integrates shooting and
bouncing of rays (SBR) with the image method and uses a hashing-based mechanism
to efficiently eliminate duplicate paths. Radio maps are computed using a
purely SBR-based approach.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [171] [Data-driven operator learning for energy-efficient building control](https://arxiv.org/abs/2504.21243)
*Yuexin Bian,Yuanyuan Shi*

Main category: eess.SY

TL;DR: 提出了一种结合CFD物理精度与机器学习计算效率的数据驱动框架，用于建筑通风控制，实现节能与空气质量平衡。


<details>
  <summary>Details</summary>
Motivation: CFD模拟虽能高精度建模气流，但计算成本高，难以实时应用于建筑管理系统。

Method: 联合优化气流供应速率和通风口角度，训练神经算子变换器从CFD数据学习控制动作到气流分布的映射，实现梯度优化控制。

Result: 相比最大气流控制、基于规则的控制及基于区域平均CO2预测的数据驱动控制，显著节能且保持空气质量。

Conclusion: 该方法兼具实用性与可扩展性，适用于安全节能的建筑管理。

Abstract: Energy-efficient ventilation control plays a vital role in reducing building
energy consumption while ensuring occupant health and comfort. While
Computational Fluid Dynamics (CFD) simulations offer high-fidelity modeling of
airflow for building HVAC design, their high computational cost makes them
impractical for practical adoption in real-time building management system. In
this work, we present a data-driven framework that combines the physical
accuracy of CFD with the computational efficiency of machine learning to enable
energy-efficient building ventilation control. Our method jointly optimizes
airflow supply rates and vent angles to reduce energy use and adhere to air
quality constraints. We train a neural operator transformer to learn the
mapping from building control actions to airflow field distributions using
high-resolution CFD data. This learned operator enables a gradient-based
control framework capable of optimal decision-making. Experimental results
demonstrate that our approach achieves substantial energy savings compared to
maximum airflow rate control, rule-based control, and data-driven control based
on regional average CO2 predictions, while consistently maintaining safe indoor
air quality. These results highlight the practicality and scalability of our
method for enabling safe and energy-efficient building management.

</details>


### [172] [Power Flow Approximations for Multiphase Distribution Networks using Gaussian Processes](https://arxiv.org/abs/2504.21260)
*Daniel Glover,Parikshit Pareek,Deepjyoti Deka,Anamika Dubey*

Main category: eess.SY

TL;DR: 该论文提出了一种基于高斯过程（GPs）的数据驱动方法，用于近似多相潮流模型，通过较少的训练数据可靠预测非线性潮流解，并在训练效率和测试性能上优于深度神经网络方法。


<details>
  <summary>Details</summary>
Motivation: 基于模型的方法在电网边缘资源管理中具有数据效率和鲁棒性优势，但需要有效的潮流模型学习。本研究旨在通过高斯过程改进现有方法。

Method: 采用高斯过程（GPs）构建数据驱动的潮流模型，将净负荷注入映射到节点电压，并与深度神经网络方法进行比较。

Result: 仿真结果表明，GP模型能以极少的训练数据可靠预测非线性潮流解，训练样本减少85%的同时，平均绝对误差相对降低99.9%。

Conclusion: 高斯过程方法在数据效率和预测性能上优于深度神经网络，适用于电网边缘资源管理。

Abstract: Learning-based approaches are increasingly leveraged to manage and coordinate
the operation of grid-edge resources in active power distribution networks.
Among these, model-based techniques stand out for their superior data
efficiency and robustness compared to model-free methods. However, effective
model learning requires a learning-based approximator for the underlying power
flow model. This study extends existing work by introducing a data-driven power
flow method based on Gaussian Processes (GPs) to approximate the multiphase
power flow model, by mapping net load injections to nodal voltages. Simulation
results using the IEEE 123-bus and 8500-node distribution test feeders
demonstrate that the trained GP model can reliably predict the nonlinear power
flow solutions with minimal training data. We also conduct a comparative
analysis of the training efficiency and testing performance of the proposed
GP-based power flow approximator against a deep neural network-based
approximator, highlighting the advantages of our data-efficient approach.
Results over realistic operating conditions show that despite an 85% reduction
in the training sample size (corresponding to a 92.8% improvement in training
time), GP models produce a 99.9% relative reduction in mean absolute error
compared to the baselines of deep neural networks.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [173] [Redundancy Analysis and Mitigation for Machine Learning-Based Process Monitoring of Additive Manufacturing](https://arxiv.org/abs/2504.21317)
*Jiarui Xie,Yaoyao Fiona Zhao*

Main category: cs.CE

TL;DR: 本文定义了基于机器学习的增材制造过程监控中的冗余问题，并提出了一种多级冗余缓解框架（MLRM），显著降低了延迟、错误率和存储需求。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对冗余的统一定义和系统评估框架，导致设备成本增加、模型性能下降和高计算需求。

Method: 提出MLRM框架，结合数据注册、降维、跨模态知识迁移和模型剪枝等方法，系统减少冗余。

Result: 在定向能量沉积（DED）案例中，延迟降低91%，错误率减少47%，存储需求减少99.4%。

Conclusion: 该研究为高效机器学习监控系统提供了关键支持，降低了成本和能耗。

Abstract: The deployment of machine learning (ML)-based process monitoring systems has
significantly advanced additive manufacturing (AM) by enabling real-time defect
detection, quality assessment, and process optimization. However, redundancy is
a critical yet often overlooked challenge in the deployment and operation of
ML-based AM process monitoring systems. Excessive redundancy leads to increased
equipment costs, compromised model performance, and high computational
requirements, posing barriers to industrial adoption. However, existing
research lacks a unified definition of redundancy and a systematic framework
for its evaluation and mitigation. This paper defines redundancy in ML-based AM
process monitoring and categorizes it into sample-level, feature-level, and
model-level redundancy. A comprehensive multi-level redundancy mitigation
(MLRM) framework is proposed, incorporating advanced methods such as data
registration, downscaling, cross-modality knowledge transfer, and model pruning
to systematically reduce redundancy while improving model performance. The
framework is validated through an ML-based in-situ defect detection case study
for directed energy deposition (DED), demonstrating a 91% reduction in latency,
a 47% decrease in error rate, and a 99.4% reduction in storage requirements.
Additionally, the proposed approach lowers sensor costs and energy consumption,
enabling a lightweight, cost-effective, and scalable monitoring system. By
defining redundancy and introducing a structured mitigation framework, this
study establishes redundancy analysis and mitigation as a key enabler of
efficient ML-based process monitoring in production environments.

</details>


<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [174] [Turning Up the Heat: Assessing 2-m Temperature Forecast Errors in AI Weather Prediction Models During Heat Waves](https://arxiv.org/abs/2504.21195)
*Kelsey E. Ennis,Elizabeth A. Barnes,Marybeth C. Arcodia,Martin A. Fernandez,Eric D. Maloney*

Main category: physics.ao-ph

TL;DR: AIWP模型（如GraphCast和Pangu-Weather）在预测极端高温方面表现优于传统NWP模型（UFS GEFS），尤其是在中范围和次季节至季节（S2S）时间尺度上。尽管存在冷偏差，但结果表明AIWP模型在极端高温预测中具有潜力。


<details>
  <summary>Details</summary>
Motivation: 极端高温是美国最致命的天气相关灾害，且其强度、频率和持续时间不断增加。传统NWP模型在中范围和S2S时间尺度上预测极端高温存在困难，而AIWP模型的进展迅速，但其在极端事件预测中的表现尚不明确。

Method: 研究比较了两种AIWP模型（GraphCast和Pangu-Weather）和一种传统NWP模型（UFS GEFS）在60次热浪事件中的2米温度预测能力，时间跨度达20天。

Result: GraphCast在大多数情况下表现最佳，优于UFS GEFS和Pangu-Weather。两种AIWP模型在热浪发生前和期间普遍存在冷偏差，但Pangu-Weather在冬季表现出暖偏差。

Conclusion: AIWP模型在中范围和S2S时间尺度上对极端高温的预测具有潜力，尽管仍需改进偏差问题。

Abstract: Extreme heat is the deadliest weather-related hazard in the United States.
Furthermore, it is increasing in intensity, frequency, and duration, making
skillful forecasts vital to protecting life and property. Traditional numerical
weather prediction (NWP) models struggle with extreme heat for medium-range and
subseasonal-to-seasonal (S2S) timescales. Meanwhile, artificial
intelligence-based weather prediction (AIWP) models are progressing rapidly.
However, it is largely unknown how well AIWP models forecast extremes,
especially for medium-range and S2S timescales. This study investigates 2-m
temperature forecasts for 60 heat waves across the four boreal seasons and over
four CONUS regions at lead times up to 20 days, using two AIWP models (Google
GraphCast and Pangu-Weather) and one traditional NWP model (NOAA United
Forecast System Global Ensemble Forecast System (UFS GEFS)). First, case study
analyses show that both AIWP models and the UFS GEFS exhibit consistent cold
biases on regional scales in the 5-10 days of lead time before heat wave onset.
GraphCast is the more skillful AIWP model, outperforming UFS GEFS and
Pangu-Weather in most locations. Next, the two AIWP models are isolated and
analyzed across all heat waves and seasons, with events split among the model's
testing (2018-2023) and training (1979-2017) periods. There are cold biases
before and during the heat waves in both models and all seasons, except
Pangu-Weather in winter, which exhibits a mean warm bias before heat wave
onset. Overall, results offer encouragement that AIWP models may be useful for
medium-range and S2S predictability of extreme heat.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [175] [A Memetic Algorithm based on Variational Autoencoder for Black-Box Discrete Optimization with Epistasis among Parameters](https://arxiv.org/abs/2504.21338)
*Aoi Kato,Kenta Kojima,Masahiro Nomura,Isao Ono*

Main category: cs.NE

TL;DR: 提出了一种结合VAE采样与局部搜索的混合算法，用于解决高维黑盒离散优化问题，实验表明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决黑盒离散优化中参数间相互依赖（epistasis）的挑战，尤其是在高维问题中。

Method: 结合VAE采样与局部搜索的混合算法，继承VAE-EDA和局部搜索方法的优势。

Result: 在NK landscapes实验中，优于当前最先进的VAE-EDA方法及P3、DSMGA-II等算法。

Conclusion: 该方法有效处理高维问题中的参数依赖，且计算开销合理。

Abstract: Black-box discrete optimization (BB-DO) problems arise in many real-world
applications, such as neural architecture search and mathematical model
estimation. A key challenge in BB-DO is epistasis among parameters where
multiple variables must be modified simultaneously to effectively improve the
objective function. Estimation of Distribution Algorithms (EDAs) provide a
powerful framework for tackling BB-DO problems. In particular, an EDA
leveraging a Variational Autoencoder (VAE) has demonstrated strong performance
on relatively low-dimensional problems with epistasis while reducing
computational cost. Meanwhile, evolutionary algorithms such as DSMGA-II and P3,
which integrate bit-flip-based local search with linkage learning, have shown
excellent performance on high-dimensional problems. In this study, we propose a
new memetic algorithm that combines VAE-based sampling with local search. The
proposed method inherits the strengths of both VAE-based EDAs and local
search-based approaches: it effectively handles high-dimensional problems with
epistasis among parameters without incurring excessive computational overhead.
Experiments on NK landscapes -- a challenging benchmark for BB-DO involving
epistasis among parameters -- demonstrate that our method outperforms
state-of-the-art VAE-based EDA methods, as well as leading approaches such as
P3 and DSMGA-II.

</details>


### [176] [Meta knowledge assisted Evolutionary Neural Architecture Search](https://arxiv.org/abs/2504.21545)
*Yangyang Li,Guanlong Liu,Ronghua Shang,Licheng Jiao*

Main category: cs.NE

TL;DR: 本文提出了一种基于元学习框架的高效进化计算神经架构搜索方法，通过Meta-LR方案和自适应代理模型降低计算成本并减少信息损失，同时通过周期性变异算子增强多样性。


<details>
  <summary>Details</summary>
Motivation: 解决传统进化计算神经架构搜索方法中高计算成本和固定学习率计划导致的信息损失问题。

Method: 采用Meta-LR方案预训练学习率计划，设计自适应代理模型筛选潜在架构，并引入周期性变异算子增强多样性。

Result: 在CIFAR-10、CIFAR-100和ImageNet1K数据集上表现优异，计算成本低且鲁棒性强。

Conclusion: 该方法在性能和效率上均优于现有方法，具有较高的实用价值。

Abstract: Evolutionary computation (EC)-based neural architecture search (NAS) has
achieved remarkable performance in the automatic design of neural
architectures. However, the high computational cost associated with evaluating
searched architectures poses a challenge for these methods, and a fixed form of
learning rate (LR) schedule means greater information loss on diverse searched
architectures. This paper introduces an efficient EC-based NAS method to solve
these problems via an innovative meta-learning framework. Specifically, a
meta-learning-rate (Meta-LR) scheme is used through pretraining to obtain a
suitable LR schedule, which guides the training process with lower information
loss when evaluating each individual. An adaptive surrogate model is designed
through an adaptive threshold to select the potential architectures in a few
epochs and then evaluate the potential architectures with complete epochs.
Additionally, a periodic mutation operator is proposed to increase the
diversity of the population, which enhances the generalizability and
robustness. Experiments on CIFAR-10, CIFAR-100, and ImageNet1K datasets
demonstrate that the proposed method achieves high performance comparable to
that of many state-of-the-art peer methods, with lower computational cost and
greater robustness.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [177] [Don't Retrieve, Generate: Prompting LLMs for Synthetic Training Data in Dense Retrieval](https://arxiv.org/abs/2504.21015)
*Aarush Sinha*

Main category: cs.IR

TL;DR: 论文提出了一种基于大语言模型（LLM）的端到端管道，通过查询生成硬负例，无需依赖传统计算密集型方法（如BM25或交叉编码器），性能与传统方法相当。


<details>
  <summary>Details</summary>
Motivation: 传统硬负例生成方法（如BM25或交叉编码器）需要访问整个语料库且计算成本高，本文旨在探索一种更简单高效的替代方案。

Method: 使用LLM从查询生成硬负例，完全脱离语料库依赖，形成“LLM查询→LLM硬负例”的端到端管道。

Result: 实验表明，该方法在nDCG@10、Precision@10和Recall@100等指标上与传统方法性能相当。

Conclusion: 该方法提供了一种无需语料库访问的高效硬负例生成方案，性能不逊于传统方法，简化了高性能检索模型的训练流程。

Abstract: Training effective dense retrieval models often relies on hard negative (HN)
examples mined from the document corpus via methods like BM25 or cross-encoders
(CE), processes that can be computationally demanding and require full corpus
access. This paper introduces a different approach, an end-to-end pipeline
where a Large Language Model (LLM) first generates a query from a passage, and
then generates a hard negative example using \emph{only} that query text. This
corpus-free negative generation contrasts with standard mining techniques. We
evaluated this \textsc{LLM Query $\rightarrow$ LLM HN} approach against
traditional \textsc{LLM Query $\rightarrow$ BM25 HN} and \textsc{LLM Query
$\rightarrow$ CE HN} pipelines using E5-Base and GTE-Base models on several
BEIR benchmark datasets. Our results show the proposed all-LLM pipeline
achieves performance identical to both the BM25 and the computationally
intensive CE baselines across nDCG@10, Precision@10, and Recall@100 metrics.
This demonstrates that our corpus-free negative generation method matches the
effectiveness of complex, corpus-dependent mining techniques, offering a
potentially simpler and more efficient pathway for training high-performance
retrievers without sacrificing results. We make the dataset including the
queries and the hard-negatives for all three methods publicly available
https://huggingface.co/collections/chungimungi/arxiv-hard-negatives-68027bbc601ff6cc8eb1f449.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [178] [Galvatron: An Automatic Distributed System for Efficient Foundation Model Training](https://arxiv.org/abs/2504.21411)
*Xinyi Liu,Yujie Wang,Shenhan Zhu,Fangcheng Fu,Qingshuo Liu,Guangming Lin,Bin Cui*

Main category: cs.DC

TL;DR: Galvatron是一个分布式系统，用于高效训练大规模基础模型，通过自动选择最优混合并行策略提升效率。


<details>
  <summary>Details</summary>
Motivation: 解决在大规模基础模型训练中选择最优并行策略的复杂性。

Method: 系统包含硬件和模型分析的分析器、基于决策树和动态编程的策略优化搜索引擎，以及高效执行策略的运行时。

Result: 在各种集群上的基准测试显示，Galvatron的吞吐量优于现有框架。

Conclusion: Galvatron通过开源、用户友好的接口和全面文档，使复杂分布式训练更高效和易用。

Abstract: Galvatron is a distributed system for efficiently training large-scale
Foundation Models. It overcomes the complexities of selecting optimal
parallelism strategies by automatically identifying the most efficient hybrid
strategy, incorporating data, tensor, pipeline, sharded data, and sequence
parallelism, along with recomputation. The system's architecture includes a
profiler for hardware and model analysis, a search engine for strategy
optimization using decision trees and dynamic programming, and a runtime for
executing these strategies efficiently. Benchmarking on various clusters
demonstrates Galvatron's superior throughput compared to existing frameworks.
This open-source system offers user-friendly interfaces and comprehensive
documentation, making complex distributed training accessible and efficient.
The source code of Galvatron is available at
https://github.com/PKU-DAIR/Hetu-Galvatron.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [179] [Solving Copyright Infringement on Short Video Platforms: Novel Datasets and an Audio Restoration Deep Learning Pipeline](https://arxiv.org/abs/2504.21772)
*Minwoo Oh,Minsu Park,Eunil Park*

Main category: cs.MM

TL;DR: 论文提出了一种结合音乐源分离（MSS）和跨模态视频-音乐检索（CMVMR）的新方法，用于解决短视频平台中背景音乐（BGM）掩盖原声（OST）的版权问题。


<details>
  <summary>Details</summary>
Motivation: 短视频平台（如YouTube Shorts和TikTok）面临版权合规挑战，侵权者常通过添加任意BGM掩盖OST以逃避原创检测。

Method: 提出了一种新流程，整合MSS和CMVMR技术，有效分离BGM并恢复原始OST。

Result: 实验结果表明，该方法能高精度去除BGM并恢复OST，确保内容完整性。

Conclusion: 该方法为短视频平台用户生成内容的版权问题提供了伦理化和可扩展的解决方案。

Abstract: Short video platforms like YouTube Shorts and TikTok face significant
copyright compliance challenges, as infringers frequently embed arbitrary
background music (BGM) to obscure original soundtracks (OST) and evade content
originality detection. To tackle this issue, we propose a novel pipeline that
integrates Music Source Separation (MSS) and cross-modal video-music retrieval
(CMVMR). Our approach effectively separates arbitrary BGM from the original
OST, enabling the restoration of authentic video audio tracks. To support this
work, we introduce two domain-specific datasets: OASD-20K for audio separation
and OSVAR-160 for pipeline evaluation. OASD-20K contains 20,000 audio clips
featuring mixed BGM and OST pairs, while OSVAR160 is a unique benchmark dataset
comprising 1,121 video and mixed-audio pairs, specifically designed for short
video restoration tasks. Experimental results demonstrate that our pipeline not
only removes arbitrary BGM with high accuracy but also restores OSTs, ensuring
content integrity. This approach provides an ethical and scalable solution to
copyright challenges in user-generated content on short video platforms.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [180] [Light Weight CNN for classification of Brain Tumors from MRI Images](https://arxiv.org/abs/2504.21188)
*Natnael Alemayehu*

Main category: eess.IV

TL;DR: 本文提出了一种基于卷积神经网络（CNN）的轻量级深度学习模型，用于MRI扫描中脑肿瘤的多分类任务，准确率达98.78%。


<details>
  <summary>Details</summary>
Motivation: 目标是构建一个高精度的轻量级模型，辅助临床诊断脑肿瘤类型。

Method: 采用图像预处理（归一化、数据增强、裁剪技术）和CNN架构优化（超参数调优），并通过5折交叉验证评估模型。

Result: 模型分类准确率达到98.78%。

Conclusion: 该方法为早期脑肿瘤诊断提供了一种低复杂度且有效的解决方案。

Abstract: This study presents a convolutional neural network (CNN)-based approach for
the multi-class classification of brain tumors using magnetic resonance imaging
(MRI) scans. We utilize a publicly available dataset containing MRI images
categorized into four classes: glioma, meningioma, pituitary tumor, and no
tumor. Our primary objective is to build a light weight deep learning model
that can automatically classify brain tumor types with high accuracy. To
achieve this goal, we incorporate image preprocessing steps, including
normalization, data augmentation, and a cropping technique designed to reduce
background noise and emphasize relevant regions. The CNN architecture is
optimized through hyperparameter tuning using Keras Tuner, enabling systematic
exploration of network parameters. To ensure reliable evaluation, we apply
5-fold cross-validation, where each hyperparameter configuration is evaluated
across multiple data splits to mitigate overfitting. Experimental results
demonstrate that the proposed model achieves a classification accuracy of
98.78%, indicating its potential as a diagnostic aid in clinical settings. The
proposed method offers a low-complexity yet effective solution for assisting in
early brain tumor diagnosis.

</details>


### [181] [Gradient Attention Map Based Verification of Deep Convolutional Neural Networks with Application to X-ray Image Datasets](https://arxiv.org/abs/2504.21227)
*Omid Halimi Milani,Amanda Nikho,Lauren Mills,Marouane Tliba,Ahmet Enis Cetin,Mohammed H. Elnagar*

Main category: eess.IV

TL;DR: 提出了一种综合验证框架，通过梯度注意力图、早期卷积特征图和垃圾类分类，评估深度学习模型在医学影像中的适用性，确保预测可靠性。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习模型在医学影像中因数据分布不同导致的不可靠预测问题，提升患者护理的安全性。

Method: 1. 使用梯度注意力图（GAM）分析注意力模式；2. 扩展到早期卷积特征图捕捉结构偏差；3. 引入垃圾类分类器拒绝分布外输入。

Result: 实验表明，这些方法能有效识别不适用模型和输入，提升模型部署的可靠性。

Conclusion: 综合验证框架为医学影像中深度学习的安全应用提供了有效工具。

Abstract: Deep learning models have great potential in medical imaging, including
orthodontics and skeletal maturity assessment. However, applying a model to
data different from its training set can lead to unreliable predictions that
may impact patient care. To address this, we propose a comprehensive
verification framework that evaluates model suitability through multiple
complementary strategies. First, we introduce a Gradient Attention Map
(GAM)-based approach that analyzes attention patterns using Grad-CAM and
compares them via similarity metrics such as IoU, Dice Similarity, SSIM, Cosine
Similarity, Pearson Correlation, KL Divergence, and Wasserstein Distance.
Second, we extend verification to early convolutional feature maps, capturing
structural mis-alignments missed by attention alone. Finally, we incorporate an
additional garbage class into the classification model to explicitly reject
out-of-distribution inputs. Experimental results demonstrate that these
combined methods effectively identify unsuitable models and inputs, promoting
safer and more reliable deployment of deep learning in medical imaging.

</details>


<div id='q-bio.CB'></div>

# q-bio.CB [[Back]](#toc)

### [182] [Glucagon and insulin production in pancreatic cells modeled using Petri nets and Boolean networks](https://arxiv.org/abs/2504.21578)
*Kamila Barylska,Frank Delaplace,Anna Gogolińska,Ewa Pańkowska*

Main category: q-bio.CB

TL;DR: 论文提出了一种基于Petri网的葡萄糖调节模型，重点关注胰岛素和胰高血糖素的分泌机制，并分析了其动态行为。


<details>
  <summary>Details</summary>
Motivation: 为了更好地理解复杂的葡萄糖调节过程，尤其是糖尿病相关的胰岛素和胰高血糖素分泌机制。

Method: 使用Petri网建模胰岛素和胰高血糖素的分泌过程，并分析其动态行为；同时将系统转换为布尔网络。

Result: 建立了胰岛素和胰高血糖素分泌的Petri网模型，并分析了其动态行为。

Conclusion: 这些模型为理解糖尿病机制提供了基础，并展示了Petri网在生物系统建模中的潜力。

Abstract: Diabetes is a civilization chronic disease characterized by a constant
elevated concentration of glucose in the blood. Many processes are involved in
the glucose regulation, and their interactions are very complex. To better
understand those processes we set ourselves a goal to create a Petri net model
of the glucose regulation in the whole body. So far we have managed to create a
model of glycolysis and synthesis of glucose in the liver, and the general
overview models of the glucose regulation in a healthy and diabetic person. In
this paper we introduce Petri nets models of insulin secretion in beta cell of
the pancreas, and glucagon in the pancreas alpha cells. Those two hormones have
mutually opposite effects: insulin preventing hyperglycemia, and glucagon
preventing hypoglycemia. Understanding the mechanisms of insulin and glucagon
secretion constitutes the basis for understanding diabetes. We also present a
model in which both processes occur together, depending on the blood glucose
level. The dynamics of each model is analysed. Additionally, we transform the
overall insulin and glucagon secretion system to a Boolean network, following
standard transformation rules.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [183] [Evaluation and Verification of Physics-Informed Neural Models of the Grad-Shafranov Equation](https://arxiv.org/abs/2504.21155)
*Fauzan Nazranda Rizqa,Matthew Hole,Charles Gretton*

Main category: physics.plasm-ph

TL;DR: 论文研究了基于物理信息神经网络（PINNs）和傅里叶神经算子（FNO）在解决Grad-Shafranov方程（GSE）中的性能比较，并首次探讨了此类网络的验证问题。


<details>
  <summary>Details</summary>
Motivation: 研究动机源于聚变反应堆中磁流体动力学（MHD）平衡的维持需求，特别是在轴对称托卡马克反应堆中，GSE用于建模平衡。现有研究未涉及单一网络对多种边界条件的泛化能力。

Method: 提出了一种将边界点作为网络输入的PINN架构，并与FNO模型在精度和推理速度上进行了比较。使用Marabou工具对网络进行了验证。

Result: 发现PINN模型在性能和精度上表现最佳，尽管在PyTorch和Marabou评估中存在一些差异，但验证流程实用。

Conclusion: 研究首次探讨了此类网络的验证问题，展示了实用的验证工作流程，为后续研究提供了参考。

Abstract: Our contributions are motivated by fusion reactors that rely on maintaining
magnetohydrodynamic (MHD) equilibrium, where the balance between plasma
pressure and confining magnetic fields is required for stable operation. In
axisymmetric tokamak reactors in particular, and under the assumption of
toroidal symmetry, this equilibrium can be mathematically modelled using the
Grad-Shafranov Equation (GSE). Recent works have demonstrated the potential of
using Physics-Informed Neural Networks (PINNs) to model the GSE. Existing
studies did not examine realistic scenarios in which a single network
generalizes to a variety of boundary conditions. Addressing that limitation, we
evaluate a PINN architecture that incorporates boundary points as network
inputs. Additionally, we compare PINN model accuracy and inference speeds with
a Fourier Neural Operator (FNO) model. Finding the PINN model to be the most
performant, and accurate in our setting, we use the network verification tool
Marabou to perform a range of verification tasks. Although we find some
discrepancies between evaluations of the networks natively in PyTorch, compared
to via Marabou, we are able to demonstrate useful and practical verification
workflows. Our study is the first investigation of verification of such
networks.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [184] [Advancing Multi-Agent Systems Through Model Context Protocol: Architecture, Implementation, and Applications](https://arxiv.org/abs/2504.21030)
*Naveen Krishnan*

Main category: cs.MA

TL;DR: 本文提出了一种基于模型上下文协议（MCP）的多智能体系统框架，解决了上下文管理、协调效率和可扩展性问题，并通过案例研究展示了性能提升。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统在上下文管理、协调效率和可扩展性方面存在挑战，需要一种标准化方法来解决这些问题。

Method: 开发了统一的MCP理论框架，包括上下文管理技术和可扩展协调模式，并通过案例研究验证。

Result: 在多个领域（如企业知识管理、协作研究）中，性能显著优于传统方法。

Conclusion: 该框架为构建更强大、协作和上下文感知的AI系统提供了基础，并指出了未来研究方向。

Abstract: Multi-agent systems represent a significant advancement in artificial
intelligence, enabling complex problem-solving through coordinated specialized
agents. However, these systems face fundamental challenges in context
management, coordination efficiency, and scalable operation. This paper
introduces a comprehensive framework for advancing multi-agent systems through
Model Context Protocol (MCP), addressing these challenges through standardized
context sharing and coordination mechanisms. We extend previous work on AI
agent architectures by developing a unified theoretical foundation, advanced
context management techniques, and scalable coordination patterns. Through
detailed implementation case studies across enterprise knowledge management,
collaborative research, and distributed problem-solving domains, we demonstrate
significant performance improvements compared to traditional approaches. Our
evaluation methodology provides a systematic assessment framework with
benchmark tasks and datasets specifically designed for multi-agent systems. We
identify current limitations, emerging research opportunities, and potential
transformative applications across industries. This work contributes to the
evolution of more capable, collaborative, and context-aware artificial
intelligence systems that can effectively address complex real-world
challenges.

</details>


### [185] [Multi-Agent Reinforcement Learning for Resources Allocation Optimization: A Survey](https://arxiv.org/abs/2504.21048)
*Mohamad A. Hady,Siyi Hu,Mahardhika Pratama,Jimmy Cao,Ryszard Kowalczyk*

Main category: cs.MA

TL;DR: 本文综述了多智能体强化学习（MARL）在资源分配优化（RAO）中的应用，总结了核心概念、分类和结构化分类法，并指出了当前研究的挑战与未来方向。


<details>
  <summary>Details</summary>
Motivation: 资源分配优化在动态和去中心化环境中面临挑战，MARL因其分布式决策和学习能力成为解决这些问题的有力工具。

Method: 通过综述近期MARL算法，涵盖核心概念、分类和结构化分类法，分析其在RAO中的应用。

Result: 总结了MARL在RAO中的研究现状，识别了主要挑战和未来发展方向。

Conclusion: MARL在资源分配优化中具有巨大潜力，本文为研究者和实践者提供了参考，以推动相关解决方案的发展。

Abstract: Multi-Agent Reinforcement Learning (MARL) has become a powerful framework for
numerous real-world applications, modeling distributed decision-making and
learning from interactions with complex environments. Resource Allocation
Optimization (RAO) benefits significantly from MARL's ability to tackle dynamic
and decentralized contexts. MARL-based approaches are increasingly applied to
RAO challenges across sectors playing pivotal roles to Industry 4.0
developments. This survey provides a comprehensive review of recent MARL
algorithms for RAO, encompassing core concepts, classifications, and a
structured taxonomy. By outlining the current research landscape and
identifying primary challenges and future directions, this survey aims to
support researchers and practitioners in leveraging MARL's potential to advance
resource allocation solutions.

</details>


### [186] [MF-LLM: Simulating Collective Decision Dynamics via a Mean-Field Large Language Model Framework](https://arxiv.org/abs/2504.21582)
*Qirui Mi,Mengyue Yang,Xiangning Yu,Zhiyu Zhao,Cheng Deng,Bo An,Haifeng Zhang,Xu Chen,Jun Wang*

Main category: cs.MA

TL;DR: MF-LLM框架通过微观决策与宏观群体的反馈循环模拟集体决策，结合IB-Tune微调方法，显著提升了与真实数据的匹配度。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型（LLM）在社会模拟中与真实数据存在偏差，需解决微观与宏观动态交互的建模问题。

Method: 提出MF-LLM框架，交替使用策略模型和平均场模型，并结合IB-Tune微调方法优化LLM。

Result: 在真实数据集上，MF-LLM将KL散度降低47%，支持准确的趋势预测和干预规划。

Conclusion: MF-LLM为高保真社会模拟提供了可扩展的基础，适用于多领域和多LLM架构。

Abstract: Simulating collective decision-making involves more than aggregating
individual behaviors; it arises from dynamic interactions among individuals.
While large language models (LLMs) show promise for social simulation, existing
approaches often exhibit deviations from real-world data. To address this gap,
we propose the Mean-Field LLM (MF-LLM) framework, which explicitly models the
feedback loop between micro-level decisions and macro-level population. MF-LLM
alternates between two models: a policy model that generates individual actions
based on personal states and group-level information, and a mean field model
that updates the population distribution from the latest individual decisions.
Together, they produce rollouts that simulate the evolving trajectories of
collective decision-making. To better match real-world data, we introduce
IB-Tune, a fine-tuning method for LLMs grounded in the information bottleneck
principle, which maximizes the relevance of population distributions to future
actions while minimizing redundancy with historical data. We evaluate MF-LLM on
a real-world social dataset, where it reduces KL divergence to human population
distributions by 47 percent over non-mean-field baselines, and enables accurate
trend forecasting and intervention planning. It generalizes across seven
domains and four LLM backbones, providing a scalable foundation for
high-fidelity social simulation.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [187] [Automated Generation of Precedence Graphs in Digital Value Chains for Automotive Production](https://arxiv.org/abs/2504.19835)
*Cornelius Hake,Christian Friedrich*

Main category: cs.RO

TL;DR: 论文提出了一种基于混合整数线性规划的新型优先图设计，用于优化汽车制造中的数字价值链流程，显著提升了效率和适应性。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决汽车制造中电子控制单元的识别、软件刷新、定制和调试等数字价值链流程的优化问题。

Method: 采用混合整数线性规划技术设计自动化调度算法，通过优先图优化任务并行化和资源分配。

Result: 算法减少了昂贵硬件和软件的生产站数量，提高了容量利用率，任务并行化优化使流程更高效，准备时间减少50%，调度活动大幅减少。

Conclusion: 自动化调度在效率、功能和适应性上显著优于传统手动方法，并支持车辆特定配置和新拓扑的集成。

Abstract: This study examines the digital value chain in automotive manufacturing,
focusing on the identification, software flashing, customization, and
commissioning of electronic control units in vehicle networks. A novel
precedence graph design is proposed to optimize this process chain using an
automated scheduling algorithm that employs mixed integer linear programming
techniques. The results show significant improvements in key metrics. The
algorithm reduces the number of production stations equipped with expensive
hardware and software to execute digital value chain processes, while
increasing capacity utilization through efficient scheduling and reduced idle
time. Task parallelization is optimized, resulting in streamlined workflows and
increased throughput. Compared to the traditional method, the automated
approach has reduced preparation time by 50% and reduced scheduling activities,
as it now takes two minutes to create the precedence graph. The flexibility of
the algorithm's constraints allows for vehicle-specific configurations while
maintaining high responsiveness, eliminating backup stations and facilitating
the integration of new topologies. Automated scheduling significantly
outperforms manual methods in efficiency, functionality, and adaptability.

</details>


### [188] [LLM-Empowered Embodied Agent for Memory-Augmented Task Planning in Household Robotics](https://arxiv.org/abs/2504.21716)
*Marc Glocker,Peter Hönig,Matthias Hirschmanner,Markus Vincze*

Main category: cs.RO

TL;DR: 该论文提出了一种基于LLM驱动的多代理机器人系统，用于家庭物品管理，通过记忆增强的任务规划和上下文学习实现高效操作。


<details>
  <summary>Details</summary>
Motivation: 解决家庭环境中机器人自主管理物品的需求，同时避免显式模型训练，提高任务规划和长期记忆能力。

Method: 系统采用三个专用代理（路由代理、任务规划代理和知识库代理），结合RAG技术检索上下文，并利用Grounded SAM和LLaMa3.2-Vision进行物体检测和场景理解。

Result: 在三种家庭场景中表现出高任务规划准确性和记忆召回率提升，Qwen2.5和LLaMA3.1分别在专用代理和路由任务中表现最佳。

Conclusion: 该系统通过多代理架构和上下文学习，实现了高效的家庭物品管理，为未来机器人应用提供了可行方案。

Abstract: We present an embodied robotic system with an LLM-driven agent-orchestration
architecture for autonomous household object management. The system integrates
memory-augmented task planning, enabling robots to execute high-level user
commands while tracking past actions. It employs three specialized agents: a
routing agent, a task planning agent, and a knowledge base agent, each powered
by task-specific LLMs. By leveraging in-context learning, our system avoids the
need for explicit model training. RAG enables the system to retrieve context
from past interactions, enhancing long-term object tracking. A combination of
Grounded SAM and LLaMa3.2-Vision provides robust object detection, facilitating
semantic scene understanding for task planning. Evaluation across three
household scenarios demonstrates high task planning accuracy and an improvement
in memory recall due to RAG. Specifically, Qwen2.5 yields best performance for
specialized agents, while LLaMA3.1 excels in routing tasks. The source code is
available at: https://github.com/marc1198/chat-hsr.

</details>


### [189] [UAV Marketplace Simulation Tool for BVLOS Operations](https://arxiv.org/abs/2504.21428)
*Kıvanç Şerefoğlu,Önder Gürcan,Reyhan Aydoğan*

Main category: cs.RO

TL;DR: 介绍了一个用于评估多无人机（UAV）团队形成的模拟工具，支持超视距（BVLOS）任务，可模拟动态和对抗环境中的协作与任务执行。


<details>
  <summary>Details</summary>
Motivation: 研究如何在动态和对抗条件下优化无人机团队的协作与任务执行，特别是在存在拜占庭无人机干扰的情况下。

Method: 开发了一个模拟工具，支持配置任务参数和对抗行为，允许集成和比较不同的团队形成策略。

Result: 工具能够记录模拟运行日志和性能指标，便于统计分析，适用于测试和改进无人机协调策略。

Conclusion: 该工具为研究和优化无人机团队协作提供了高效且可控的测试环境。

Abstract: We present a simulation tool for evaluating team formation in autonomous
multi-UAV (Unmanned Aerial Vehicle) missions that operate Beyond Visual Line of
Sight (BVLOS). The tool models UAV collaboration and mission execution in
dynamic and adversarial conditions, where Byzantine UAVs attempt to disrupt
operations. Our tool allows researchers to integrate and compare various team
formation strategies in a controlled environment with configurable mission
parameters and adversarial behaviors. The log of each simulation run is stored
in a structured way along with performance metrics so that statistical analysis
could be done straightforwardly. The tool is versatile for testing and
improving UAV coordination strategies in real-world applications.

</details>


### [190] [Real Time Semantic Segmentation of High Resolution Automotive LiDAR Scans](https://arxiv.org/abs/2504.21602)
*Hannes Reichert,Benjamin Serfling,Elijah Schüssler,Kerim Turacan,Konrad Doll,Bernhard Sick*

Main category: cs.RO

TL;DR: 本文提出了一种针对现代高分辨率LiDAR传感器的语义分割框架，解决了精度和实时处理的需求，并提供了新的数据集和ROS2实现。


<details>
  <summary>Details</summary>
Motivation: 现有方法多基于低分辨率LiDAR传感器，难以满足实时性和高精度的需求，特别是在自动驾驶和辅助驾驶系统中。

Method: 提出了一种利用表面法线作为输入特征的语义分割方法，并基于128层LiDAR传感器构建了新的数据集。

Result: 该方法在高分辨率LiDAR数据上实现了高精度和实时处理，填补了前沿研究与实际应用之间的差距。

Conclusion: 该框架为自动驾驶领域提供了实用的解决方案，数据集和代码已开源。

Abstract: In recent studies, numerous previous works emphasize the importance of
semantic segmentation of LiDAR data as a critical component to the development
of driver-assistance systems and autonomous vehicles. However, many
state-of-the-art methods are tested on outdated, lower-resolution LiDAR sensors
and struggle with real-time constraints. This study introduces a novel semantic
segmentation framework tailored for modern high-resolution LiDAR sensors that
addresses both accuracy and real-time processing demands. We propose a novel
LiDAR dataset collected by a cutting-edge automotive 128 layer LiDAR in urban
traffic scenes. Furthermore, we propose a semantic segmentation method
utilizing surface normals as strong input features. Our approach is bridging
the gap between cutting-edge research and practical automotive applications.
Additionaly, we provide a Robot Operating System (ROS2) implementation that we
operate on our research vehicle. Our dataset and code are publicly available:
https://github.com/kav-institute/SemanticLiDAR.

</details>


### [191] [SimPRIVE: a Simulation framework for Physical Robot Interaction with Virtual Environments](https://arxiv.org/abs/2504.21454)
*Federico Nesti,Gianluca D'Amico,Mauro Marinoni,Giorgio Buttazzo*

Main category: cs.RO

TL;DR: SimPRIVE是一个用于物理机器人与虚拟环境交互的仿真框架，支持ROS 2的移动机器人通过数字孪生在Unreal Engine 5构建的虚拟世界中运行，用于安全高效地测试复杂算法。


<details>
  <summary>Details</summary>
Motivation: 机器学习和强化学习在物理系统中的不可预测行为缺乏通用解决方案，而高保真仿真器为复杂算法的测试提供了低成本、低风险的途径。

Method: 提出SimPRIVE框架，将物理机器人与虚拟环境结合，通过数字孪生技术实现实时交互，支持自定义虚拟场景和快速渲染。

Result: 验证了SimPRIVE在障碍物避障任务中的有效性，物理机器人在虚拟办公室环境中成功导航，避免了碰撞。

Conclusion: SimPRIVE为复杂算法的测试提供了高效、低成本的解决方案，显著降低了实际测试的风险和成本。

Abstract: The use of machine learning in cyber-physical systems has attracted the
interest of both industry and academia. However, no general solution has yet
been found against the unpredictable behavior of neural networks and
reinforcement learning agents. Nevertheless, the improvements of
photo-realistic simulators have paved the way towards extensive testing of
complex algorithms in different virtual scenarios, which would be expensive and
dangerous to implement in the real world.
  This paper presents SimPRIVE, a simulation framework for physical robot
interaction with virtual environments, which operates as a vehicle-in-the-loop
platform, rendering a virtual world while operating the vehicle in the real
world.
  Using SimPRIVE, any physical mobile robot running on ROS 2 can easily be
configured to move its digital twin in a virtual world built with the Unreal
Engine 5 graphic engine, which can be populated with objects, people, or other
vehicles with programmable behavior.
  SimPRIVE has been designed to accommodate custom or pre-built virtual worlds
while being light-weight to contain execution times and allow fast rendering.
Its main advantage lies in the possibility of testing complex algorithms on the
full software and hardware stack while minimizing the risks and costs of a test
campaign. The framework has been validated by testing a reinforcement learning
agent trained for obstacle avoidance on an AgileX Scout Mini rover that
navigates a virtual office environment where everyday objects and people are
placed as obstacles. The physical rover moves with no collision in an indoor
limited space, thanks to a LiDAR-based heuristic.

</details>


### [192] [Multi-Goal Dexterous Hand Manipulation using Probabilistic Model-based Reinforcement Learning](https://arxiv.org/abs/2504.21585)
*Yingzhuo Jiang,Wenjun Huang,Rongdun Lin,Chenyang Miao,Tianfu Sun,Yunduan Cui*

Main category: cs.RO

TL;DR: 论文提出了一种基于模型强化学习的方法（GC-PMPC），用于解决多目标灵巧手操作任务，通过概率神经网络集合和异步MPC策略实现高效控制。


<details>
  <summary>Details</summary>
Motivation: 解决在高维灵巧手系统中学习多目标操作任务的挑战，尤其是在实时控制频率要求下的性能问题。

Method: 设计了Goal-Conditioned Probabilistic Model Predictive Control (GC-PMPC)，结合概率神经网络集合和异步MPC策略。

Result: 在四个模拟Shadow Hand场景中，GC-PMPC表现优于现有方法，能在80分钟内让DexHand 021灵巧手学会操作立方体到三个目标姿态。

Conclusion: GC-PMPC在低成本灵巧手平台上展示了高效的学习能力和控制性能。

Abstract: This paper tackles the challenge of learning multi-goal dexterous hand
manipulation tasks using model-based Reinforcement Learning. We propose
Goal-Conditioned Probabilistic Model Predictive Control (GC-PMPC) by designing
probabilistic neural network ensembles to describe the high-dimensional
dexterous hand dynamics and introducing an asynchronous MPC policy to meet the
control frequency requirements in real-world dexterous hand systems. Extensive
evaluations on four simulated Shadow Hand manipulation scenarios with randomly
generated goals demonstrate GC-PMPC's superior performance over
state-of-the-art baselines. It successfully drives a cable-driven Dexterous
hand, DexHand 021 with 12 Active DOFs and 5 tactile sensors, to learn
manipulating a cubic die to three goal poses within approximately 80 minutes of
interactions, demonstrating exceptional learning efficiency and control
performance on a cost-effective dexterous hand platform.

</details>


### [193] [One Net to Rule Them All: Domain Randomization in Quadcopter Racing Across Different Platforms](https://arxiv.org/abs/2504.21586)
*Robin Ferede,Till Blaha,Erin Lucassen,Christophe De Wagter,Guido C. H. E. de Croon*

Main category: cs.RO

TL;DR: 本文提出了一种通用的神经网络控制器，用于跨不同四旋翼飞行器的竞速控制，通过域随机化训练实现鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决高速四旋翼竞速中单一控制器难以适应不同平台的挑战。

Method: 使用域随机化训练神经网络控制器，仅依赖当前状态直接计算电机指令。

Result: 通用控制器在两种不同尺寸的四旋翼上验证有效，虽速度略低于专用控制器，但适应性更强。

Conclusion: 域随机化在控制器通用化中具有潜力，为通用AI控制器的发展铺平道路。

Abstract: In high-speed quadcopter racing, finding a single controller that works well
across different platforms remains challenging. This work presents the first
neural network controller for drone racing that generalizes across physically
distinct quadcopters. We demonstrate that a single network, trained with domain
randomization, can robustly control various types of quadcopters. The network
relies solely on the current state to directly compute motor commands. The
effectiveness of this generalized controller is validated through real-world
tests on two substantially different crafts (3-inch and 5-inch race
quadcopters). We further compare the performance of this generalized controller
with controllers specifically trained for the 3-inch and 5-inch drone, using
their identified model parameters with varying levels of domain randomization
(0%, 10%, 20%, 30%). While the generalized controller shows slightly slower
speeds compared to the fine-tuned models, it excels in adaptability across
different platforms. Our results show that no randomization fails sim-to-real
transfer while increasing randomization improves robustness but reduces speed.
Despite this trade-off, our findings highlight the potential of domain
randomization for generalizing controllers, paving the way for universal AI
controllers that can adapt to any platform.

</details>


### [194] [Leveraging Pre-trained Large Language Models with Refined Prompting for Online Task and Motion Planning](https://arxiv.org/abs/2504.21596)
*Huihui Guo,Huilong Pi,Yunchuan Qin,Zhuo Tang,Kenli Li*

Main category: cs.RO

TL;DR: LLM-PAS是一个基于大型语言模型（LLM）的闭环任务规划和执行系统，通过将部分约束检查转移到执行阶段，提高了任务执行的稳定性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能的快速发展，智能机器人需要具备任务规划和稳定执行的能力，以辅助人类完成复杂任务。

Method: LLM-PAS结合传统任务与运动规划器的长时任务规划能力，强调执行阶段，并通过First Look Prompting（FLP）方法提升LLM生成有效PDDL目标的能力。

Result: 实验表明，LLM-PAS在执行任务时能有效处理异常情况，表现出较高的鲁棒性。

Conclusion: LLM-PAS通过闭环规划和执行机制，显著提升了智能机器人在复杂任务中的表现。

Abstract: With the rapid advancement of artificial intelligence, there is an increasing
demand for intelligent robots capable of assisting humans in daily tasks and
performing complex operations. Such robots not only require task planning
capabilities but must also execute tasks with stability and robustness. In this
paper, we present a closed-loop task planning and acting system, LLM-PAS, which
is assisted by a pre-trained Large Language Model (LLM). While LLM-PAS plans
long-horizon tasks in a manner similar to traditional task and motion planners,
it also emphasizes the execution phase of the task. By transferring part of the
constraint-checking process from the planning phase to the execution phase,
LLM-PAS enables exploration of the constraint space and delivers more accurate
feedback on environmental anomalies during execution. The reasoning
capabilities of the LLM allow it to handle anomalies that cannot be addressed
by the robust executor. To further enhance the system's ability to assist the
planner during replanning, we propose the First Look Prompting (FLP) method,
which induces LLM to generate effective PDDL goals. Through comparative
prompting experiments and systematic experiments, we demonstrate the
effectiveness and robustness of LLM-PAS in handling anomalous conditions during
task execution.

</details>


### [195] [Self-Supervised Monocular Visual Drone Model Identification through Improved Occlusion Handling](https://arxiv.org/abs/2504.21695)
*Stavrow A. Bahnam,Christophe De Wagter,Guido C. H. E. de Croon*

Main category: cs.RO

TL;DR: 提出了一种自监督学习方案，通过单目视频和飞行控制器数据训练基于神经网络的无人机模型，解决了GPS缺失环境下的运动估计问题。


<details>
  <summary>Details</summary>
Motivation: 在GPS缺失环境中，无人机运动估计至关重要，但现有视觉方法在高速度和复杂视觉条件下表现不佳，且依赖外部运动捕捉系统的监督学习限制了可扩展性。

Method: 提出自监督学习方案，先训练自监督相对位姿估计模型作为教师，再训练无人机模型，并改进了遮挡处理方法。

Result: 位姿估计误差平均降低15%，神经网络无人机模型在高速度下比教师模型更准确，集成到VIO系统后提升了轨迹估计精度。

Conclusion: 自监督学习为无人机在真实环境中的高速飞行和状态估计提供了可行方案，缩小了实验室与实际应用的差距。

Abstract: Ego-motion estimation is vital for drones when flying in GPS-denied
environments. Vision-based methods struggle when flight speed increases and
close-by objects lead to difficult visual conditions with considerable motion
blur and large occlusions. To tackle this, vision is typically complemented by
state estimation filters that combine a drone model with inertial measurements.
However, these drone models are currently learned in a supervised manner with
ground-truth data from external motion capture systems, limiting scalability to
different environments and drones. In this work, we propose a self-supervised
learning scheme to train a neural-network-based drone model using only onboard
monocular video and flight controller data (IMU and motor feedback). We achieve
this by first training a self-supervised relative pose estimation model, which
then serves as a teacher for the drone model. To allow this to work at high
speed close to obstacles, we propose an improved occlusion handling method for
training self-supervised pose estimation models. Due to this method, the root
mean squared error of resulting odometry estimates is reduced by an average of
15%. Moreover, the student neural drone model can be successfully obtained from
the onboard data. It even becomes more accurate at higher speeds compared to
its teacher, the self-supervised vision-based model. We demonstrate the value
of the neural drone model by integrating it into a traditional filter-based VIO
system (ROVIO), resulting in superior odometry accuracy on aggressive 3D racing
trajectories near obstacles. Self-supervised learning of ego-motion estimation
represents a significant step toward bridging the gap between flying in
controlled, expensive lab environments and real-world drone applications. The
fusion of vision and drone models will enable higher-speed flight and improve
state estimation, on any drone in any environment.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [196] [Generalised Label-free Artefact Cleaning for Real-time Medical Pulsatile Time Series](https://arxiv.org/abs/2504.21209)
*Xuhang Chen,Ihsane Olakorede,Stefan Yu Bögli,Wenhao Xu,Erta Beqiri,Xuemeng Li,Chenyu Tang,Zeyu Gao,Shuo Gao,Ari Ercole,Peter Smielewski*

Main category: eess.SP

TL;DR: GenClean是一个无需标签的通用框架，用于实时清理医疗时间序列中的伪影，适用于多种脉动信号，并在临床实践中验证了其可行性。


<details>
  <summary>Details</summary>
Motivation: 医疗时间序列中的伪影会影响临床决策，现有方法多依赖监督学习且忽略患者层面的分布变化。

Method: 提出GenClean框架，利用18万份10秒动脉血压样本训练，验证其在患者内和患者间分布变化下的鲁棒性，并在MIMIC-III数据库和PPG信号中扩展应用。

Result: GenClean在跨疾病队列实验中表现优异，并成功集成到临床监测软件ICM+中，证实其实时可行性。

Conclusion: 该研究为高分辨率医疗时间序列分析的可靠性提供了基础，推动了精准医学的发展。

Abstract: Artefacts compromise clinical decision-making in the use of medical time
series. Pulsatile waveforms offer probabilities for accurate artefact
detection, yet most approaches rely on supervised manners and overlook
patient-level distribution shifts. To address these issues, we introduce a
generalised label-free framework, GenClean, for real-time artefact cleaning and
leverage an in-house dataset of 180,000 ten-second arterial blood pressure
(ABP) samples for training. We first investigate patient-level generalisation,
demonstrating robust performances under both intra- and inter-patient
distribution shifts. We further validate its effectiveness through challenging
cross-disease cohort experiments on the MIMIC-III database. Additionally, we
extend our method to photoplethysmography (PPG), highlighting its applicability
to diverse medical pulsatile signals. Finally, its integration into ICM+, a
clinical research monitoring software, confirms the real-time feasibility of
our framework, emphasising its practical utility in continuous physiological
monitoring. This work provides a foundational step toward precision medicine in
improving the reliability of high-resolution medical time series analysis

</details>


<div id='physics.data-an'></div>

# physics.data-an [[Back]](#toc)

### [197] [Scalable Multi-Task Learning for Particle Collision Event Reconstruction with Heterogeneous Graph Neural Networks](https://arxiv.org/abs/2504.21844)
*William Sutcliffe,Marta Calvi,Simone Capelli,Jonas Eschle,Julián García Pardiñas,Abhijit Mathad,Azusa Uzuki,Nicola Serra*

Main category: physics.data-an

TL;DR: 论文提出了一种基于异构图神经网络（HGNN）的新方法，用于解决大型强子对撞机（LHC）中粒子碰撞事件重建的挑战，显著提升了美丽强子重建性能。


<details>
  <summary>Details</summary>
Motivation: 随着大型强子对撞机亮度前沿的发展，粒子碰撞事件的重建和分析面临挑战，包括更高的粒子多重性、背景水平和顶点误关联问题，需要更全面且可扩展的重建方法。

Method: 提出了一种异构图神经网络（HGNN）架构，利用独特的粒子碰撞关系表示和集成图剪枝层，采用多任务训练范式，并在模拟LHCb实验环境中进行训练。

Result: HGNN显著提升了美丽强子的重建性能，同时在一个框架内完成了粒子顶点关联和图剪枝，并展示了推理时间随事件复杂度的优化。

Conclusion: 该方法通过加权消息传递方案缓解了潜在性能损失，为高亮度对撞环境下的粒子重建提供了高效且可扩展的解决方案。

Abstract: The growing luminosity frontier at the Large Hadron Collider is challenging
the reconstruction and analysis of particle collision events. Increased
particle multiplicities are straining latency and storage requirements at the
data acquisition stage, while new complications are emerging, including higher
background levels and more frequent particle vertex misassociations. This in
turn necessitates the development of more holistic and scalable reconstruction
methods that take advantage of recent advances in machine learning. We propose
a novel Heterogeneous Graph Neural Network (HGNN) architecture featuring unique
representations for diverse particle collision relationships and integrated
graph pruning layers for scalability. Trained with a multi-task paradigm in an
environment mimicking the LHCb experiment, this HGNN significantly improves
beauty hadron reconstruction performance. Notably, it concurrently performs
particle vertex association and graph pruning within a single framework. We
quantify reconstruction and pruning performance, demonstrate enhanced inference
time scaling with event complexity, and mitigate potential performance loss
using a weighted message passing scheme.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [198] [Passive Measurement of Autonomic Arousal in Real-World Settings](https://arxiv.org/abs/2504.21242)
*Samy Abdel-Ghaffar,Isaac Galatzer-Levy,Conor Heneghan,Xin Liu,Sarah Kernasovskiy,Brennan Garrett,Andrew Barakat,Daniel McDuff*

Main category: cs.HC

TL;DR: 本文提出了一种通过手腕传感器远程测量自主神经系统（ANS）激活的方法，并在实验和实际场景中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以在真实世界中量化ANS活动，而ANS激活与健康问题密切相关。

Method: 开发了Fitbit Body Response算法，通过手腕传感器连续测量ANS激活，并在Trier社会压力测试和生态瞬时评估中验证。

Result: 模型预测感知压力的准确率为0.85，优于仅使用部分信号的方法。

Conclusion: 该方法在真实环境中表现良好，解决了实验室环境未涉及的挑战。

Abstract: The autonomic nervous system (ANS) is activated during stress, which can have
negative effects on cardiovascular health, sleep, the immune system, and mental
health. While there are ways to quantify ANS activity in laboratories, there is
a paucity of methods that have been validated in real-world contexts. We
present the Fitbit Body Response Algorithm, an approach to continuous remote
measurement of ANS activation through widely available remote wrist-based
sensors. The design was validated via two experiments, a Trier Social Stress
Test (n = 45) and ecological momentary assessments (EMA) of perceived stress
(n=87), providing both controlled and ecologically valid test data. Model
performance predicting perceived stress when using all available sensor
modalities was consistent with expectations (accuracy=0.85) and outperformed
models with access to only a subset of the signals. We discuss and address
challenges to sensing that arise in real world settings that do not present in
conventional lab environments.

</details>


### [199] [Adaptive 3D UI Placement in Mixed Reality Using Deep Reinforcement Learning](https://arxiv.org/abs/2504.21731)
*Feiyu Lu,Mengyu Chen,Hsiang Hsu,Pranav Deshpande,Cheng Yao Wang,Blair MacIntyre*

Main category: cs.HC

TL;DR: 本文探讨了如何利用强化学习（RL）在混合现实（MR）中动态优化3D内容布局，以适应用户姿态和环境变化。


<details>
  <summary>Details</summary>
Motivation: MR中虚拟内容的动态布局是一个挑战性问题，传统优化方法难以适应实时变化。

Method: 采用强化学习方法，结合用户姿态和环境信息，实现连续3D内容布局优化。

Result: 初步实验表明，RL能有效优化内容布局，提升用户体验。

Conclusion: RL在MR中具有潜力，未来可进一步探索个性化UI和内容布局优化。

Abstract: Mixed Reality (MR) could assist users' tasks by continuously integrating
virtual content with their view of the physical environment. However, where and
how to place these content to best support the users has been a challenging
problem due to the dynamic nature of MR experiences. In contrast to prior work
that investigates optimization-based methods, we are exploring how
reinforcement learning (RL) could assist with continuous 3D content placement
that is aware of users' poses and their surrounding environments. Through an
initial exploration and preliminary evaluation, our results demonstrate the
potential of RL to position content that maximizes the reward for users on the
go. We further identify future directions for research that could harness the
power of RL for personalized and optimized UI and content placement in MR.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [200] [On the Potential of Large Language Models to Solve Semantics-Aware Process Mining Tasks](https://arxiv.org/abs/2504.21074)
*Adrian Rebmann,Fabian David Schmidt,Goran Glavaš,Han van der Aa*

Main category: cs.DB

TL;DR: 大语言模型（LLMs）在过程挖掘任务中表现出潜力，尤其是在需要语义理解的任务中。本文通过上下文学习和监督微调系统评估了LLMs的能力，发现微调后性能显著提升。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在语义感知的过程挖掘任务中的潜力，如过程发现和异常检测，这些任务依赖对活动及其关系的理解。

Method: 通过上下文学习和监督微调评估LLMs在五个需要语义理解的过程挖掘任务中的表现，并提供基准数据集。

Result: LLMs在默认状态下或少量上下文示例中表现不佳，但经过微调后，在多种过程和行业中表现优异。

Conclusion: LLMs在语义感知的过程挖掘任务中具有潜力，尤其是在经过微调后，性能显著提升。

Abstract: Large language models (LLMs) have shown to be valuable tools for tackling
process mining tasks. Existing studies report on their capability to support
various data-driven process analyses and even, to some extent, that they are
able to reason about how processes work. This reasoning ability suggests that
there is potential for LLMs to tackle semantics-aware process mining tasks,
which are tasks that rely on an understanding of the meaning of activities and
their relationships. Examples of these include process discovery, where the
meaning of activities can indicate their dependency, whereas in anomaly
detection the meaning can be used to recognize process behavior that is
abnormal. In this paper, we systematically explore the capabilities of LLMs for
such tasks. Unlike prior work, which largely evaluates LLMs in their default
state, we investigate their utility through both in-context learning and
supervised fine-tuning. Concretely, we define five process mining tasks
requiring semantic understanding and provide extensive benchmarking datasets
for evaluation. Our experiments reveal that while LLMs struggle with
challenging process mining tasks when used out of the box or with minimal
in-context examples, they achieve strong performance when fine-tuned for these
tasks across a broad range of process types and industries.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [201] [Transcending Dimensions using Generative AI: Real-Time 3D Model Generation in Augmented Reality](https://arxiv.org/abs/2504.21033)
*Majid Behravan,Maryam Haghani,Denis Gracanin*

Main category: cs.GR

TL;DR: 研究结合生成式AI和增强现实（AR），简化3D建模流程，使非专业用户也能轻松生成和操作3D模型。


<details>
  <summary>Details</summary>
Motivation: 传统3D建模技术门槛高、耗时，研究旨在通过AI和AR降低使用门槛。

Method: 使用Shap-E等AI模型和Mask R-CNN等对象检测技术，解决2D图像转3D模型的复杂问题。

Result: 35名参与者的系统可用性评分（SUS）为69.64，熟悉AR/VR的用户评分更高（80.71）。

Conclusion: 该系统适用于游戏、教育和AR电商，为非专业用户提供直观的3D建模工具。

Abstract: Traditional 3D modeling requires technical expertise, specialized software,
and time-intensive processes, making it inaccessible for many users. Our
research aims to lower these barriers by combining generative AI and augmented
reality (AR) into a cohesive system that allows users to easily generate,
manipulate, and interact with 3D models in real time, directly within AR
environments. Utilizing cutting-edge AI models like Shap-E, we address the
complex challenges of transforming 2D images into 3D representations in AR
environments. Key challenges such as object isolation, handling intricate
backgrounds, and achieving seamless user interaction are tackled through
advanced object detection methods, such as Mask R-CNN. Evaluation results from
35 participants reveal an overall System Usability Scale (SUS) score of 69.64,
with participants who engaged with AR/VR technologies more frequently rating
the system significantly higher, at 80.71. This research is particularly
relevant for applications in gaming, education, and AR-based e-commerce,
offering intuitive, model creation for users without specialized skills.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [202] [DGFNet: End-to-End Audio-Visual Source Separation Based on Dynamic Gating Fusion](https://arxiv.org/abs/2504.21366)
*Yinfeng Yu,Shiyu Sun*

Main category: cs.SD

TL;DR: 提出了一种基于门控机制的动态融合方法，解决音频-视觉特征融合中的信息丢失和交互不足问题，并通过音频注意力模块提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在音频-视觉特征融合时存在信息丢失或交互不足的问题，限制了模型性能。

Method: 采用动态门控机制调整模态融合程度，并引入音频注意力模块增强音频特征表达能力。

Result: 在两个基准数据集上取得了显著的性能提升。

Conclusion: 该方法有效解决了音频-视觉源分离任务中的模态融合问题，具有明显优势。

Abstract: Current Audio-Visual Source Separation methods primarily adopt two design
strategies. The first strategy involves fusing audio and visual features at the
bottleneck layer of the encoder, followed by processing the fused features
through the decoder. However, when there is a significant disparity between the
two modalities, this approach may lead to the loss of critical information. The
second strategy avoids direct fusion and instead relies on the decoder to
handle the interaction between audio and visual features. Nonetheless, if the
encoder fails to integrate information across modalities adequately, the
decoder may be unable to effectively capture the complex relationships between
them. To address these issues, this paper proposes a dynamic fusion method
based on a gating mechanism that dynamically adjusts the modality fusion
degree. This approach mitigates the limitations of solely relying on the
decoder and facilitates efficient collaboration between audio and visual
features. Additionally, an audio attention module is introduced to enhance the
expressive capacity of audio features, thereby further improving model
performance. Experimental results demonstrate that our method achieves
significant performance improvements on two benchmark datasets, validating its
effectiveness and advantages in Audio-Visual Source Separation tasks.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [203] [Generate-then-Verify: Reconstructing Data from Limited Published Statistics](https://arxiv.org/abs/2504.21199)
*Terrance Liu,Eileen Xiao,Pratiksha Thaker,Adam Smith,Zhiwei Steven Wu*

Main category: stat.ML

TL;DR: 论文研究了从聚合统计中重建表格数据的问题，提出了一种部分数据重建方法，通过整数规划生成并验证可能的数据子集。


<details>
  <summary>Details</summary>
Motivation: 现有方法在数据统计丰富时可以完全重建数据集，但在统计稀疏时无法完美重建。本文关注后者，研究如何在统计稀疏时仍能确保部分数据的正确性。

Method: 提出了一种基于整数规划的方法，首先生成一组可能的声明，然后验证这些声明是否在所有符合统计的数据集中成立。

Result: 在U.S. Decennial Census的住房微观数据上验证了方法，表明即使统计信息稀疏，隐私泄露风险仍存在。

Conclusion: 即使在统计稀疏的情况下，部分数据重建仍可能导致隐私泄露，需要进一步研究保护措施。

Abstract: We study the problem of reconstructing tabular data from aggregate
statistics, in which the attacker aims to identify interesting claims about the
sensitive data that can be verified with 100% certainty given the aggregates.
Successful attempts in prior work have conducted studies in settings where the
set of published statistics is rich enough that entire datasets can be
reconstructed with certainty. In our work, we instead focus on the regime where
many possible datasets match the published statistics, making it impossible to
reconstruct the entire private dataset perfectly (i.e., when approaches in
prior work fail). We propose the problem of partial data reconstruction, in
which the goal of the adversary is to instead output a $\textit{subset}$ of
rows and/or columns that are $\textit{guaranteed to be correct}$. We introduce
a novel integer programming approach that first $\textbf{generates}$ a set of
claims and then $\textbf{verifies}$ whether each claim holds for all possible
datasets consistent with the published aggregates. We evaluate our approach on
the housing-level microdata from the U.S. Decennial Census release,
demonstrating that privacy violations can still persist even when information
published about such data is relatively sparse.

</details>


### [204] [Kernel Density Machines](https://arxiv.org/abs/2504.21419)
*Damir Filipovic,Paul Schneider*

Main category: stat.ML

TL;DR: KDM是一种新型的密度比估计器，适用于广泛的概率测度，具有计算效率和理论保证。


<details>
  <summary>Details</summary>
Motivation: 提出一种适用于广泛概率测度的密度比估计方法，避免对连续性或Lebesgue密度的限制性假设。

Method: 在再生核希尔伯特空间框架下，结合低秩近似以提高计算效率，并提供理论保证。

Result: 理论分析表明KDM具有渐近一致性和有限样本误差界，实验验证了其有效性和精确性。

Conclusion: KDM为密度比估计提供了一种高效且理论可靠的方法，适用于大规模数据。

Abstract: We introduce kernel density machines (KDM), a novel density ratio estimator
in a reproducing kernel Hilbert space setting. KDM applies to general
probability measures on countably generated measurable spaces without
restrictive assumptions on continuity, or the existence of a Lebesgue density.
For computational efficiency, we incorporate a low-rank approximation with
precisely controlled error that grants scalability to large-sample settings. We
provide rigorous theoretical guarantees, including asymptotic consistency, a
functional central limit theorem, and finite-sample error bounds, establishing
a strong foundation for practical use. Empirical results based on simulated and
real data demonstrate the efficacy and precision of KDM.

</details>


### [205] [Wasserstein-Aitchison GAN for angular measures of multivariate extremes](https://arxiv.org/abs/2504.21438)
*Stéphane Lhaut,Holger Rootzén,Johan Segers*

Main category: stat.ML

TL;DR: 本文提出了一种名为WA-GAN的新方法，用于模拟多维极端事件并估计其发生概率，结合了极值分析和生成对抗网络。


<details>
  <summary>Details</summary>
Motivation: 经济上负责任地缓解多维极端风险（如极端降雨、股票价格大幅波动等）需要估计这些风险未来发生的概率。

Method: 将数据转换到单位Pareto尺度后，假设其分布是规则变化的，结合极值分析的尾部建模和GAN对角度分布的非参数建模。

Result: WA-GAN在捕捉极端数据依赖结构和生成新极端数据方面表现优于现有方法。

Conclusion: WA-GAN为多维极端事件的概率估计提供了有效工具，适用于高维数据。

Abstract: Economically responsible mitigation of multivariate extreme risks -- extreme
rainfall in a large area, huge variations of many stock prices, widespread
breakdowns in transportation systems -- requires estimates of the probabilities
that such risks will materialize in the future. This paper develops a new
method, Wasserstein--Aitchison Generative Adversarial Networks (WA-GAN), which
provides simulated values of future $d$-dimensional multivariate extreme events
and which hence can be used to give estimates of such probabilities. The main
hypothesis is that, after transforming the observations to the unit-Pareto
scale, their distribution is regularly varying in the sense that the
distributions of their radial and angular components (with respect to the
$L_1$-norm) converge and become asymptotically independent as the radius gets
large. The method is a combination of standard extreme value analysis modeling
of the tails of the marginal distributions with nonparametric GAN modeling of
the angular distribution. For the latter, the angular values are transformed to
Aitchison coordinates in a full $(d-1)$-dimensional linear space, and a
Wasserstein GAN is trained on these coordinates and used to generate new
values. A reverse transformation is then applied to these values and gives
simulated values on the original data scale. The method shows good performance
compared to other existing methods in the literature, both in terms of
capturing the dependence structure of the extremes in the data, as well as in
generating accurate new extremes of the data distribution. The comparison is
performed on simulated multivariate extremes from a logistic model in
dimensions up to 50 and on a 30-dimensional financial data set.

</details>


### [206] [A comparison of generative deep learning methods for multivariate angular simulation](https://arxiv.org/abs/2504.21505)
*Jakob Benjamin Wessel,Callum J. R. Murphy-Barltrop,Emma S. Simpson*

Main category: stat.ML

TL;DR: 论文探讨了在多元极值分析中，利用深度学习生成角变量模拟的方法，并与传统方法（如vMF混合模型）进行了比较。


<details>
  <summary>Details</summary>
Motivation: 随着多元极值分析中几何和角度-径向框架的发展，高维角变量的可靠模拟变得重要。传统方法在低维表现良好，但在高维缺乏灵活性和可扩展性。

Method: 研究采用了生成对抗网络、标准化流和流匹配等深度学习生成方法，并与vMF混合模型进行对比。

Result: 通过多种指标评估，深度学习方法在复杂数据结构的模拟中表现出色，并在实际海洋气象数据中验证了其适用性。

Conclusion: 深度学习方法为高维角变量模拟提供了更灵活和可扩展的解决方案，适用于复杂数据结构。

Abstract: With the recent development of new geometric and angular-radial frameworks
for multivariate extremes, reliably simulating from angular variables in
moderate-to-high dimensions is of increasing importance. Empirical approaches
have the benefit of simplicity, and work reasonably well in low dimensions, but
as the number of variables increases, they can lack the required flexibility
and scalability. Classical parametric models for angular variables, such as the
von Mises-Fisher (vMF) distribution, provide an alternative. Exploiting
mixtures of vMF distributions increases their flexibility, but there are cases
where even this is not sufficient to capture the intricate features that can
arise in data. Owing to their flexibility, generative deep learning methods are
able to capture complex data structures; they therefore have the potential to
be useful in the simulation of angular variables. In this paper, we explore a
range of deep learning approaches for this task, including generative
adversarial networks, normalizing flows and flow matching. We assess their
performance via a range of metrics and make comparisons to the more classical
approach of using a mixture of vMF distributions. The methods are also applied
to a metocean data set, demonstrating their applicability to real-world,
complex data structures.

</details>


### [207] [Balancing Interpretability and Flexibility in Modeling Diagnostic Trajectories with an Embedded Neural Hawkes Process Model](https://arxiv.org/abs/2504.21795)
*Yuankang Zhao,Matthew Engelhard*

Main category: stat.ML

TL;DR: 论文提出了一种新型的霍克斯过程（HP）模型，通过事件嵌入空间中的灵活影响核（神经网络实现）来平衡灵活性与可解释性，适用于大规模事件序列建模。


<details>
  <summary>Details</summary>
Motivation: 传统HP通过参数化影响函数实现可解释性，但灵活性不足；神经网络HP灵活性高但可解释性差。论文旨在解决这一矛盾，特别是在医疗领域需要可解释性的场景。

Method: 提出一种HP模型，将影响函数建模为事件嵌入空间中的灵活核（神经网络），并可通过添加Transformer层进一步优化上下文嵌入。

Result: 实验表明，该方法能准确模拟影响函数，在MIMIC-IV数据集上表现优异，并在儿童诊断数据集中获得临床意义的解释。

Conclusion: 灵活影响核能有效捕捉事件序列的动态特性，表明在不牺牲性能的情况下仍可保持可解释性。

Abstract: The Hawkes process (HP) is commonly used to model event sequences with
self-reinforcing dynamics, including electronic health records (EHRs).
Traditional HPs capture self-reinforcement via parametric impact functions that
can be inspected to understand how each event modulates the intensity of
others. Neural network-based HPs offer greater flexibility, resulting in
improved fit and prediction performance, but at the cost of interpretability,
which is often critical in healthcare. In this work, we aim to understand and
improve upon this tradeoff. We propose a novel HP formulation in which impact
functions are modeled by defining a flexible impact kernel, instantiated as a
neural network, in event embedding space, which allows us to model large-scale
event sequences with many event types. This approach is more flexible than
traditional HPs yet more interpretable than other neural network approaches,
and allows us to explicitly trade flexibility for interpretability by adding
transformer encoder layers to further contextualize the event embeddings.
Results show that our method accurately recovers impact functions in
simulations, achieves competitive performance on MIMIC-IV procedure dataset,
and gains clinically meaningful interpretation on XX-EHR with children
diagnosis dataset even without transformer layers. This suggests that our
flexible impact kernel is often sufficient to capture self-reinforcing dynamics
in EHRs and other data effectively, implying that interpretability can be
maintained without loss of performance.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [208] [Low-rank computation of the posterior mean in Multi-Output Gaussian Processes](https://arxiv.org/abs/2504.21527)
*Sebastian Esche,Martin Stoll*

Main category: math.NA

TL;DR: 本文提出了一种低秩方法，用于高效计算多输出高斯过程（MOGP）的后验均值，通过空间和时间可分离的协方差函数和Kronecker积分解，结合LRPCG方法和KPIK求解器，解决了大规模Stein方程问题。


<details>
  <summary>Details</summary>
Motivation: 多输出高斯过程（MOGP）在机器学习和计算科学中应用广泛，但计算后验均值时面临大规模Stein方程的挑战，需要高效的低秩方法。

Method: 采用低秩时空数据，假设空间和时间可分离的协方差函数，将协方差矩阵分解为Kronecker积。结合LRPCG方法和KPIK求解器，高效求解Stein方程。

Result: 在真实世界街道网络图上测试了该方法，使用图滤波器作为协方差矩阵，并提出了一种加权平均协方差矩阵以提高收敛效率。

Conclusion: 提出的低秩方法有效解决了MOGP后验均值的计算问题，适用于大规模数据，并展示了在实际应用中的潜力。

Abstract: Gaussian processes (GP) are a versatile tool in machine learning and
computational science. We here consider the case of multi-output Gaussian
processes (MOGP) and present low-rank approaches for efficiently computing the
posterior mean of a MOGP. Starting from low-rank spatio-temporal data we
consider a structured covariance function, assuming separability across space
and time. This separability, in turn, gives a decomposition of the covariance
matrix into a Kronecker product of individual covariance matrices.
Incorporating the typical noise term to the model then requires the solution of
a large-scale Stein equation for computing the posterior mean. For this, we
propose efficient low-rank methods based on a combination of a LRPCG method
with the Sylvester equation solver KPIK adjusted for solving Stein equations.
We test the developed method on real world street network graphs by using graph
filters as covariance matrices. Moreover, we propose a degree-weighted average
covariance matrix, which can be employed under specific assumptions to achieve
more efficient convergence.

</details>

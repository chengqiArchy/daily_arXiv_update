<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 25]
- [cs.AI](#cs.AI) [Total: 15]
- [cs.LG](#cs.LG) [Total: 74]
- [stat.ME](#stat.ME) [Total: 2]
- [cs.CV](#cs.CV) [Total: 20]
- [cs.IR](#cs.IR) [Total: 3]
- [cs.CY](#cs.CY) [Total: 1]
- [cs.CR](#cs.CR) [Total: 10]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [cs.CE](#cs.CE) [Total: 1]
- [cs.SC](#cs.SC) [Total: 1]
- [eess.AS](#eess.AS) [Total: 1]
- [cs.MA](#cs.MA) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [cs.AR](#cs.AR) [Total: 1]
- [quant-ph](#quant-ph) [Total: 1]
- [cs.RO](#cs.RO) [Total: 8]
- [cs.NE](#cs.NE) [Total: 1]
- [math.OC](#math.OC) [Total: 3]
- [stat.ML](#stat.ML) [Total: 5]
- [cs.NI](#cs.NI) [Total: 1]
- [astro-ph.GA](#astro-ph.GA) [Total: 1]
- [cs.DB](#cs.DB) [Total: 1]
- [cs.DC](#cs.DC) [Total: 1]
- [cs.SE](#cs.SE) [Total: 4]
- [math.DS](#math.DS) [Total: 1]
- [eess.SY](#eess.SY) [Total: 1]
- [stat.AP](#stat.AP) [Total: 1]
- [eess.IV](#eess.IV) [Total: 2]
- [cs.GR](#cs.GR) [Total: 2]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [FinBERT-QA: Financial Question Answering with pre-trained BERT Language Models](https://arxiv.org/abs/2505.00725)
*Bithiah Yuan*

Main category: cs.CL

TL;DR: 提出了一种基于BERT的金融QA系统，通过结合BM25检索和BERT重排序，显著提升了金融非事实性答案选择任务的性能。


<details>
  <summary>Details</summary>
Motivation: 金融行业对大规模非结构化和结构化数据的自动分析需求日益增长，QA系统可为金融顾问的决策提供竞争优势。

Method: 系统分为答案检索器（BM25）和答案重排序器（BERT变体），研究了多种BERT的学习、预训练和微调方法。

Result: FinBERT-QA模型在FiQA数据集任务2上，MRR提升16%，NDCG提升17%，Precision@1提升21%。

Conclusion: 提出的FinBERT-QA系统在金融QA任务中表现优异，显著优于现有方法。

Abstract: Motivated by the emerging demand in the financial industry for the automatic
analysis of unstructured and structured data at scale, Question Answering (QA)
systems can provide lucrative and competitive advantages to companies by
facilitating the decision making of financial advisers. Consequently, we
propose a novel financial QA system using the transformer-based pre-trained
BERT language model to address the limitations of data scarcity and language
specificity in the financial domain. Our system focuses on financial
non-factoid answer selection, which retrieves a set of passage-level texts and
selects the most relevant as the answer. To increase efficiency, we formulate
the answer selection task as a re-ranking problem, in which our system consists
of an Answer Retriever using BM25, a simple information retrieval approach, to
first return a list of candidate answers, and an Answer Re-ranker built with
variants of pre-trained BERT language models to re-rank and select the most
relevant answers. We investigate various learning, further pre-training, and
fine-tuning approaches for BERT. Our experiments suggest that FinBERT-QA, a
model built from applying the Transfer and Adapt further fine-tuning and
pointwise learning approach, is the most effective, improving the
state-of-the-art results of task 2 of the FiQA dataset by 16% on MRR, 17% on
NDCG, and 21% on Precision@1.

</details>


### [2] [A Survey on Large Language Model based Human-Agent Systems](https://arxiv.org/abs/2505.00753)
*Henry Peng Zou,Wei-Chieh Huang,Yaozu Wu,Yankai Chen,Chunyu Miao,Hoang Nguyen,Yue Zhou,Weizhi Zhang,Liancheng Fang,Langzhou He,Yangning Li,Yuwei Cao,Dongyuan Li,Renhe Jiang,Philip S. Yu*

Main category: cs.CL

TL;DR: 本文综述了基于大型语言模型的人机代理系统（LLM-HAS），探讨了其核心组件、应用及挑战，旨在推动这一跨学科领域的研究与创新。


<details>
  <summary>Details</summary>
Motivation: 完全自主的LLM代理存在可靠性、复杂任务处理及安全伦理风险等问题，LLM-HAS通过引入人类信息、反馈或控制以提升系统性能与安全性。

Method: 通过系统化的综述，明确基本概念，分析核心组件（如环境与配置、人类反馈、交互类型等），并探讨应用与挑战。

Result: 提供了LLM-HAS的全面结构化综述，整理了当前知识，并列出相关论文与资源。

Conclusion: LLM-HAS通过结合人类输入提升了系统性能与安全性，未来研究需进一步探索其潜力与挑战。

Abstract: Recent advances in large language models (LLMs) have sparked growing interest
in building fully autonomous agents. However, fully autonomous LLM-based agents
still face significant challenges, including limited reliability due to
hallucinations, difficulty in handling complex tasks, and substantial safety
and ethical risks, all of which limit their feasibility and trustworthiness in
real-world applications. To overcome these limitations, LLM-based human-agent
systems (LLM-HAS) incorporate human-provided information, feedback, or control
into the agent system to enhance system performance, reliability and safety.
This paper provides the first comprehensive and structured survey of LLM-HAS.
It clarifies fundamental concepts, systematically presents core components
shaping these systems, including environment & profiling, human feedback,
interaction types, orchestration and communication, explores emerging
applications, and discusses unique challenges and opportunities. By
consolidating current knowledge and offering a structured overview, we aim to
foster further research and innovation in this rapidly evolving
interdisciplinary field. Paper lists and resources are available at
https://github.com/HenryPengZou/Awesome-LLM-Based-Human-Agent-System-Papers.

</details>


### [3] [Reasoning Capabilities and Invariability of Large Language Models](https://arxiv.org/abs/2505.00776)
*Alessandro Raganato,Rafael Peñaloza,Marco Viviani,Gabriella Pasi*

Main category: cs.CL

TL;DR: 该论文通过新基准数据集分析了大型语言模型（LLMs）在简单推理任务中的表现，特别关注提示依赖性。研究发现，尽管参数超过700亿的模型在零样本设置中表现较好，但仍有改进空间，且链式思维提示的效果因使用时机而异。


<details>
  <summary>Details</summary>
Motivation: 研究动机是评估LLMs在简单推理任务中的能力，尤其是其对提示的依赖性，以填补现有研究的空白。

Method: 方法包括构建一个基于几何图形的基准数据集，采用零样本、少样本和链式思维提示，测试24种不同规模的LLMs。

Result: 结果显示，参数超过700亿的模型在零样本设置中表现更优，但整体仍有不足；链式思维提示的效果取决于其使用时机。

Conclusion: 结论指出LLMs在简单推理任务中仍有改进空间，提示设计对其性能有显著影响。

Abstract: Large Language Models (LLMs) have shown remarkable capabilities in
manipulating natural language across multiple applications, but their ability
to handle simple reasoning tasks is often questioned. In this work, we aim to
provide a comprehensive analysis of LLMs' reasoning competence, specifically
focusing on their prompt dependency. In particular, we introduce a new
benchmark dataset with a series of simple reasoning questions demanding shallow
logical reasoning. Aligned with cognitive psychology standards, the questions
are confined to a basic domain revolving around geometric figures, ensuring
that responses are independent of any pre-existing intuition about the world
and rely solely on deduction. An empirical analysis involving zero-shot and
few-shot prompting across 24 LLMs of different sizes reveals that, while LLMs
with over 70 billion parameters perform better in the zero-shot setting, there
is still a large room for improvement. An additional test with chain-of-thought
prompting over 22 LLMs shows that this additional prompt can aid or damage the
performance of models, depending on whether the rationale is required before or
after the answer.

</details>


### [4] [Knowledge-augmented Pre-trained Language Models for Biomedical Relation Extraction](https://arxiv.org/abs/2505.00814)
*Mario Sänger,Ulf Leser*

Main category: cs.CL

TL;DR: 该论文研究了预训练语言模型（PLMs）在生物医学文献关系抽取（RE）中的应用，重点评估了上下文信息增强PLMs的效果，并强调了模型选择和超参数优化的重要性。


<details>
  <summary>Details</summary>
Motivation: 生物医学文献中关系抽取的自动化为管理大量科学知识提供了关键支持。尽管PLMs已成为主流方法，但模型、数据库和评估方法的差异使得研究结果难以直接比较和推广。

Method: 研究在一致的评估框架下，对五个数据集中的四种关系场景进行了评估。首先优化了三种基线PLMs的超参数，然后选择表现最佳的模型，并通过文本实体描述、知识图谱关系信息和分子结构编码等上下文信息进行增强。

Result: 研究发现，基础语言模型的选择和全面的超参数优化对性能至关重要。尽管上下文信息的加入仅带来小幅提升，但对较小的PLMs在微调时加入外部数据有显著帮助。

Conclusion: 研究强调了模型选择和超参数优化的重要性，同时指出上下文信息对小规模PLMs的潜在价值。

Abstract: Automatic relationship extraction (RE) from biomedical literature is critical
for managing the vast amount of scientific knowledge produced each year. In
recent years, utilizing pre-trained language models (PLMs) has become the
prevalent approach in RE. Several studies report improved performance when
incorporating additional context information while fine-tuning PLMs for RE.
However, variations in the PLMs applied, the databases used for augmentation,
hyper-parameter optimization, and evaluation methods complicate direct
comparisons between studies and raise questions about the generalizability of
these findings. Our study addresses this research gap by evaluating PLMs
enhanced with contextual information on five datasets spanning four relation
scenarios within a consistent evaluation framework. We evaluate three baseline
PLMs and first conduct extensive hyperparameter optimization. After selecting
the top-performing model, we enhance it with additional data, including textual
entity descriptions, relational information from knowledge graphs, and
molecular structure encodings. Our findings illustrate the importance of i) the
choice of the underlying language model and ii) a comprehensive hyperparameter
optimization for achieving strong extraction performance. Although inclusion of
context information yield only minor overall improvements, an ablation study
reveals substantial benefits for smaller PLMs when such external data was
included during fine-tuning.

</details>


### [5] [Large Language Model-Driven Dynamic Assessment of Grammatical Accuracy in English Language Learner Writing](https://arxiv.org/abs/2505.00931)
*Timur Jaganov,John Blake,Julián Villegas,Nicholas Carr*

Main category: cs.CL

TL;DR: 研究探讨了大型语言模型（LLMs）在动态评估（DA）中的扩展潜力，开发了DynaWrite应用，测试了21种LLMs，发现GPT-4o和neural chat表现最佳，GPT-4o在反馈质量上更优。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs如何扩展动态评估，以支持更大规模的语言学习评估。

Method: 开发DynaWrite应用，测试21种LLMs，重点评估GPT-4o和neural chat的语法错误识别和反馈质量。

Result: GPT-4o在反馈质量和系统性能上优于neural chat，证实LLMs可扩展动态评估。

Conclusion: LLMs能有效扩展动态评估，适用于大规模语言学习场景。

Abstract: This study investigates the potential for Large Language Models (LLMs) to
scale-up Dynamic Assessment (DA). To facilitate such an investigation, we first
developed DynaWrite-a modular, microservices-based grammatical tutoring
application which supports multiple LLMs to generate dynamic feedback to
learners of English. Initial testing of 21 LLMs, revealed GPT-4o and neural
chat to have the most potential to scale-up DA in the language learning
classroom. Further testing of these two candidates found both models performed
similarly in their ability to accurately identify grammatical errors in user
sentences. However, GPT-4o consistently outperformed neural chat in the quality
of its DA by generating clear, consistent, and progressively explicit hints.
Real-time responsiveness and system stability were also confirmed through
detailed performance testing, with GPT-4o exhibiting sufficient speed and
stability. This study shows that LLMs can be used to scale-up dynamic
assessment and thus enable dynamic assessment to be delivered to larger groups
than possible in traditional teacher-learner settings.

</details>


### [6] [Llama-Nemotron: Efficient Reasoning Models](https://arxiv.org/abs/2505.00949)
*Akhiad Bercovich,Itay Levy,Izik Golan,Mohammad Dabbah,Ran El-Yaniv,Omri Puny,Ido Galil,Zach Moshe,Tomer Ronen,Najeeb Nabwani,Ido Shahaf,Oren Tropp,Ehud Karpas,Ran Zilberstein,Jiaqi Zeng,Soumye Singhal,Alexander Bukharin,Yian Zhang,Tugrul Konuk,Gerald Shen,Ameya Sunil Mahabaleshwarkar,Bilal Kartal,Yoshi Suhara,Olivier Delalleau,Zijia Chen,Zhilin Wang,David Mosallanezhad,Adi Renduchintala,Haifeng Qian,Dima Rekesh,Fei Jia,Somshubra Majumdar,Vahid Noroozi,Wasi Uddin Ahmad,Sean Narenthiran,Aleksander Ficek,Mehrzad Samadi,Jocelyn Huang,Siddhartha Jain,Igor Gitman,Ivan Moshkov,Wei Du,Shubham Toshniwal,George Armstrong,Branislav Kisacanin,Matvei Novikov,Daria Gitman,Evelina Bakhturina,Jane Polak Scowcroft,John Kamalu,Dan Su,Kezhi Kong,Markus Kliegl,Rabeeh Karimi,Ying Lin,Sanjeev Satheesh,Jupinder Parmar,Pritam Gundecha,Brandon Norick,Joseph Jennings,Shrimai Prabhumoye,Syeda Nahida Akter,Mostofa Patwary,Abhinav Khattar,Deepak Narayanan,Roger Waleffe,Jimmy Zhang,Bor-Yiing Su,Guyue Huang,Terry Kong,Parth Chadha,Sahil Jain,Christine Harvey,Elad Segal,Jining Huang,Sergey Kashirsky,Robert McQueen,Izzy Putterman,George Lam,Arun Venkatesan,Sherry Wu,Vinh Nguyen,Manoj Kilaru,Andrew Wang,Anna Warno,Abhilash Somasamudramath,Sandip Bhaskar,Maka Dong,Nave Assaf,Shahar Mor,Omer Ullman Argov,Scot Junkin,Oleksandr Romanenko,Pedro Larroy,Monika Katariya,Marco Rovinelli,Viji Balas,Nicholas Edelman,Anahita Bhiwandiwalla,Muthu Subramaniam,Smita Ithape,Karthik Ramamoorthy,Yuting Wu,Suguna Varshini Velury,Omri Almog,Joyjit Daw,Denys Fridman,Erick Galinkin,Michael Evans,Katherine Luna,Leon Derczynski,Nikki Pope,Eileen Long,Seth Schneider,Guillermo Siman,Tomasz Grzegorzek,Pablo Ribalta,Monika Katariya,Joey Conway,Trisha Saar,Ann Guan,Krzysztof Pawelec,Shyamala Prayaga,Oleksii Kuchaiev,Boris Ginsburg,Oluwatobi Olabiyi,Kari Briski,Jonathan Cohen,Bryan Catanzaro,Jonah Alben,Yonatan Geifman,Eric Chung*

Main category: cs.CL

TL;DR: Llama-Nemotron系列模型是一个开源的异构推理模型家族，提供卓越的推理能力、高效的推理速度和开放的商业许可。


<details>
  <summary>Details</summary>
Motivation: 旨在开发一个高性能、高效且开放的推理模型家族，支持动态推理切换，并促进开源研究和模型开发。

Method: 通过神经架构搜索、知识蒸馏和持续预训练优化模型，随后进行监督微调和大规模强化学习的后训练阶段。

Result: 模型在推理效率和内存使用上优于现有技术，同时支持动态推理模式切换。

Conclusion: Llama-Nemotron系列模型为开源社区提供了高性能的推理工具和资源，推动了开放研究和模型开发。

Abstract: We introduce the Llama-Nemotron series of models, an open family of
heterogeneous reasoning models that deliver exceptional reasoning capabilities,
inference efficiency, and an open license for enterprise use. The family comes
in three sizes -- Nano (8B), Super (49B), and Ultra (253B) -- and performs
competitively with state-of-the-art reasoning models such as DeepSeek-R1 while
offering superior inference throughput and memory efficiency. In this report,
we discuss the training procedure for these models, which entails using neural
architecture search from Llama 3 models for accelerated inference, knowledge
distillation, and continued pretraining, followed by a reasoning-focused
post-training stage consisting of two main parts: supervised fine-tuning and
large scale reinforcement learning. Llama-Nemotron models are the first
open-source models to support a dynamic reasoning toggle, allowing users to
switch between standard chat and reasoning modes during inference. To further
support open research and facilitate model development, we provide the
following resources: 1. We release the Llama-Nemotron reasoning models --
LN-Nano, LN-Super, and LN-Ultra -- under the commercially permissive NVIDIA
Open Model License Agreement. 2. We release the complete post-training dataset:
Llama-Nemotron-Post-Training-Dataset. 3. We also release our training
codebases: NeMo, NeMo-Aligner, and Megatron-LM.

</details>


### [7] [A Character-based Diffusion Embedding Algorithm for Enhancing the Generation Quality of Generative Linguistic Steganographic Texts](https://arxiv.org/abs/2505.00977)
*Yingquan Chen,Qianmu Li,Xiaocong Wu,Huifeng Li,Qing Chang*

Main category: cs.CL

TL;DR: 本文提出了一种新的嵌入算法CDEA，结合XLNet模型，显著提高了隐写文本的质量，特别是在感知不可察觉性方面。


<details>
  <summary>Details</summary>
Motivation: 现有模型在文本生成能力上有限，且嵌入算法未能有效缓解敏感信息属性（如语义内容或随机性）的负面影响，导致隐写文本质量下降。

Method: 提出基于字符的扩散嵌入算法（CDEA），利用敏感信息的属性，通过字符级别的统计特性和幂律分布分组方法，优化候选词选择。同时引入XLNet模型处理长序列。

Result: 实验结果表明，CDEA与XLNet的结合显著提升了隐写文本的质量，特别是在感知不可察觉性方面。

Conclusion: CDEA和XLNet的组合有效解决了隐写文本生成中的质量问题，为生成高质量隐写文本提供了新方法。

Abstract: Generating high-quality steganographic text is a fundamental challenge in the
field of generative linguistic steganography. This challenge arises primarily
from two aspects: firstly, the capabilities of existing models in text
generation are limited; secondly, embedding algorithms fail to effectively
mitigate the negative impacts of sensitive information's properties, such as
semantic content or randomness. Specifically, to ensure that the recipient can
accurately extract hidden information, embedding algorithms often have to
consider selecting candidate words with relatively low probabilities. This
phenomenon leads to a decrease in the number of high-probability candidate
words and an increase in low-probability candidate words, thereby compromising
the semantic coherence and logical fluency of the steganographic text and
diminishing the overall quality of the generated steganographic material. To
address this issue, this paper proposes a novel embedding algorithm,
character-based diffusion embedding algorithm (CDEA). Unlike existing embedding
algorithms that strive to eliminate the impact of sensitive information's
properties on the generation process, CDEA leverages sensitive information's
properties. It enhances the selection frequency of high-probability candidate
words in the candidate pool based on general statistical properties at the
character level and grouping methods based on power-law distributions, while
reducing the selection frequency of low-probability candidate words in the
candidate pool. Furthermore, to ensure the effective transformation of
sensitive information in long sequences, we also introduce the XLNet model.
Experimental results demonstrate that the combination of CDEA and XLNet
significantly improves the quality of generated steganographic text,
particularly in terms of perceptual-imperceptibility.

</details>


### [8] [Synthesize-on-Graph: Knowledgeable Synthetic Data Generation for Continue Pre-training of Large Language Models](https://arxiv.org/abs/2505.00979)
*Xuhui Jiang,Shengjie Ma,Chengjin Xu,Cehao Yang,Liyu Zhang,Jian Guo*

Main category: cs.CL

TL;DR: SoG是一种新的合成数据生成框架，通过跨文档知识关联提升数据多样性和深度，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs在小规模、专业语料上的数据低效问题，现有方法忽略跨文档知识关联。

Method: 构建上下文图，提取实体和概念，采用图游走策略生成知识关联的合成数据，结合CoT和CC提升质量。

Result: 在多项任务中优于或媲美SOTA方法，展示更好的泛化能力。

Conclusion: SoG推动了合成数据生成，为数据有限领域的LLMs提供了高效知识获取方案。

Abstract: Large Language Models (LLMs) have achieved remarkable success but remain
data-inefficient, especially when learning from small, specialized corpora with
limited and proprietary data. Existing synthetic data generation methods for
continue pre-training focus on intra-document content and overlook
cross-document knowledge associations, limiting content diversity and depth. We
propose Synthetic-on-Graph (SoG), a synthetic data generation framework that
incorporates cross-document knowledge associations for efficient corpus
expansion. SoG constructs a context graph by extracting entities and concepts
from the original corpus, representing cross-document associations, and
employing a graph walk strategy for knowledge-associated sampling. This
enhances synthetic data diversity and coherence, enabling models to learn
complex knowledge structures and handle rare knowledge. To further improve
synthetic data quality, we integrate Chain-of-Thought (CoT) and Contrastive
Clarifying (CC) synthetic, enhancing reasoning processes and discriminative
power. Experiments show that SoG outperforms the state-of-the-art (SOTA) method
in a multi-hop document Q&A dataset while performing comparably to the SOTA
method on the reading comprehension task datasets, which also underscores the
better generalization capability of SoG. Our work advances synthetic data
generation and provides practical solutions for efficient knowledge acquisition
in LLMs, especially in domains with limited data availability.

</details>


### [9] [Position: Enough of Scaling LLMs! Lets Focus on Downscaling](https://arxiv.org/abs/2505.00985)
*Ayan Sengupta,Yash Goel,Tanmoy Chakraborty*

Main category: cs.CL

TL;DR: 论文主张从神经扩展定律转向降尺度开发大型语言模型（LLMs），强调扩展方法的局限性，并提出降尺度框架以保持性能并减少资源需求。


<details>
  <summary>Details</summary>
Motivation: 扩展定律在模型和数据规模增加时提供了性能改进的见解，但其计算效率低、环境影响大且部署受限，因此需要更可持续和高效的方法。

Method: 提出一个全面的降尺度框架，旨在减少资源需求同时保持性能，并提供了从传统扩展范式过渡的实际策略。

Result: 降尺度方法有望在减少计算资源消耗的同时维持LLMs的性能。

Conclusion: 论文倡导转向降尺度开发LLMs，以实现更可持续、高效和可访问的模型开发。

Abstract: We challenge the dominant focus on neural scaling laws and advocate for a
paradigm shift toward downscaling in the development of large language models
(LLMs). While scaling laws have provided critical insights into performance
improvements through increasing model and dataset size, we emphasize the
significant limitations of this approach, particularly in terms of
computational inefficiency, environmental impact, and deployment constraints.
To address these challenges, we propose a holistic framework for downscaling
LLMs that seeks to maintain performance while drastically reducing resource
demands. This paper outlines practical strategies for transitioning away from
traditional scaling paradigms, advocating for a more sustainable, efficient,
and accessible approach to LLM development.

</details>


### [10] [VTS-LLM: Domain-Adaptive LLM Agent for Enhancing Awareness in Vessel Traffic Services through Natural Language](https://arxiv.org/abs/2505.00989)
*Sijin Sun,Liangbin Zhao,Ming Deng,Xiuju Fu*

Main category: cs.CL

TL;DR: 本文提出VTS-LLM Agent，首个针对VTS操作的交互式决策支持领域自适应大语言模型代理，通过知识增强的Text-to-SQL任务识别高风险船舶，并在多语言风格查询中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有VTS系统在时空推理和直观人机交互方面存在局限，需应对日益复杂的交通和多模态数据。

Method: 结合结构化船舶数据库与外部海事知识，构建定制数据集，采用NER关系推理、领域知识注入、语义代数中间表示和查询重思机制。

Result: VTS-LLM在命令式、操作式和正式自然语言查询中均优于通用和SQL专用基线模型。

Conclusion: 为VTS自然语言接口奠定基础，推动LLM驱动的主动式海事实时交通管理。

Abstract: Vessel Traffic Services (VTS) are essential for maritime safety and
regulatory compliance through real-time traffic management. However, with
increasing traffic complexity and the prevalence of heterogeneous, multimodal
data, existing VTS systems face limitations in spatiotemporal reasoning and
intuitive human interaction. In this work, we propose VTS-LLM Agent, the first
domain-adaptive large LLM agent tailored for interactive decision support in
VTS operations. We formalize risk-prone vessel identification as a
knowledge-augmented Text-to-SQL task, combining structured vessel databases
with external maritime knowledge. To support this, we construct a curated
benchmark dataset consisting of a custom schema, domain-specific corpus, and a
query-SQL test set in multiple linguistic styles. Our framework incorporates
NER-based relational reasoning, agent-based domain knowledge injection,
semantic algebra intermediate representation, and query rethink mechanisms to
enhance domain grounding and context-aware understanding. Experimental results
show that VTS-LLM outperforms both general-purpose and SQL-focused baselines
under command-style, operational-style, and formal natural language queries,
respectively. Moreover, our analysis provides the first empirical evidence that
linguistic style variation introduces systematic performance challenges in
Text-to-SQL modeling. This work lays the foundation for natural language
interfaces in vessel traffic services and opens new opportunities for
proactive, LLM-driven maritime real-time traffic management.

</details>


### [11] [Token-free Models for Sarcasm Detection](https://arxiv.org/abs/2505.01006)
*Sumit Mamtani,Maitreya Sonawane,Kanika Agarwal,Nishanth Sanjeev*

Main category: cs.CL

TL;DR: 论文评估了两种无标记模型（ByT5和CANINE）在社交媒体和非社交媒体领域的讽刺检测任务中的表现，发现它们优于基于标记的模型，并实现了新的最先进性能。


<details>
  <summary>Details</summary>
Motivation: 标记化在自然语言处理中引入词汇不匹配和词汇外问题，无标记模型（如ByT5和CANINE）可能解决这些限制。

Method: 在社交媒体（Twitter）和非社交媒体（新闻标题）领域，对ByT5和CANINE进行微调和基准测试，并与基于标记的模型进行比较。

Result: ByT5-small和CANINE在新闻标题和Twitter讽刺数据集上分别提高了0.77%和0.49%的准确率，优于基于标记的模型。

Conclusion: 无标记模型在嘈杂和非正式领域（如社交媒体）中具有潜力，能够实现更稳健的自然语言处理。

Abstract: Tokenization is a foundational step in most natural language processing (NLP)
pipelines, yet it introduces challenges such as vocabulary mismatch and
out-of-vocabulary issues. Recent work has shown that models operating directly
on raw text at the byte or character level can mitigate these limitations. In
this paper, we evaluate two token-free models, ByT5 and CANINE, on the task of
sarcasm detection in both social media (Twitter) and non-social media (news
headlines) domains. We fine-tune and benchmark these models against token-based
baselines and state-of-the-art approaches. Our results show that ByT5-small and
CANINE outperform token-based counterparts and achieve new state-of-the-art
performance, improving accuracy by 0.77% and 0.49% on the News Headlines and
Twitter Sarcasm datasets, respectively. These findings underscore the potential
of token-free models for robust NLP in noisy and informal domains such as
social media.

</details>


### [12] [Value Portrait: Understanding Values of LLMs with Human-aligned Benchmark](https://arxiv.org/abs/2505.01015)
*Jongwook Han,Dongmin Choi,Woojung Song,Eun-Ju Lee,Yohan Jo*

Main category: cs.CL

TL;DR: 提出了一种名为Value Portrait的基准测试，用于评估语言模型的价值取向，解决了现有基准测试中价值偏见和生态效度不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试依赖人类或机器标注，易受价值偏见影响，且测试场景与真实使用场景不符。

Method: 设计了包含真实用户-LLM交互的测试项，并通过人类评分与价值得分的相关性进行心理测量验证。

Result: 评估27个LLM后发现，模型更重视Benevolence、Security和Self-Direction价值，较少关注Tradition、Power和Achievement价值，并揭示了模型对某些人口群体的偏见。

Conclusion: Value Portrait基准测试提供了一种可靠的方法来评估LLM的价值取向，揭示了模型的价值偏好和偏见。

Abstract: The importance of benchmarks for assessing the values of language models has
been pronounced due to the growing need of more authentic, human-aligned
responses. However, existing benchmarks rely on human or machine annotations
that are vulnerable to value-related biases. Furthermore, the tested scenarios
often diverge from real-world contexts in which models are commonly used to
generate text and express values. To address these issues, we propose the Value
Portrait benchmark, a reliable framework for evaluating LLMs' value
orientations with two key characteristics. First, the benchmark consists of
items that capture real-life user-LLM interactions, enhancing the relevance of
assessment results to real-world LLM usage and thus ecological validity.
Second, each item is rated by human subjects based on its similarity to their
own thoughts, and correlations between these ratings and the subjects' actual
value scores are derived. This psychometrically validated approach ensures that
items strongly correlated with specific values serve as reliable items for
assessing those values. Through evaluating 27 LLMs with our benchmark, we find
that these models prioritize Benevolence, Security, and Self-Direction values
while placing less emphasis on Tradition, Power, and Achievement values. Also,
our analysis reveals biases in how LLMs perceive various demographic groups,
deviating from real human data.

</details>


### [13] [Do We Need a Detailed Rubric for Automated Essay Scoring using Large Language Models?](https://arxiv.org/abs/2505.01035)
*Lui Yoshida*

Main category: cs.CL

TL;DR: 研究表明，在基于大语言模型（LLM）的自动作文评分（AES）中，简化评分标准可能足以保持评分准确性，同时显著减少计算资源消耗。


<details>
  <summary>Details</summary>
Motivation: 探讨评分标准的详细程度对LLM在AES中评分准确性和效率的影响。

Method: 使用TOEFL11数据集，比较了四种LLM在三种评分标准条件下的表现：完整标准、简化标准和无标准。

Result: 四分之三的模型在简化标准下保持了与完整标准相似的准确性，同时显著减少了计算资源消耗；但Gemini 1.5 Flash在详细标准下表现下降。

Conclusion: 简化评分标准可能是更高效的AES方案，但需根据具体模型评估其适用性。

Abstract: This study investigates the necessity and impact of a detailed rubric in
automated essay scoring (AES) using large language models (LLMs). While using
rubrics are standard in LLM-based AES, creating detailed rubrics requires
substantial ef-fort and increases token usage. We examined how different levels
of rubric detail affect scoring accuracy across multiple LLMs using the TOEFL11
dataset. Our experiments compared three conditions: a full rubric, a simplified
rubric, and no rubric, using four different LLMs (Claude 3.5 Haiku, Gemini 1.5
Flash, GPT-4o-mini, and Llama 3 70B Instruct). Results showed that three out of
four models maintained similar scoring accuracy with the simplified rubric
compared to the detailed one, while significantly reducing token usage.
However, one model (Gemini 1.5 Flash) showed decreased performance with more
detailed rubrics. The findings suggest that simplified rubrics may be
sufficient for most LLM-based AES applications, offering a more efficient
alternative without compromis-ing scoring accuracy. However, model-specific
evaluation remains crucial as per-formance patterns vary across different LLMs.

</details>


### [14] [Multimodal Transformers are Hierarchical Modal-wise Heterogeneous Graphs](https://arxiv.org/abs/2505.01068)
*Yijie Jin,Junjie Peng,Xuanchao Lin,Haochen Yuan,Lan Wang,Cangzhi Zheng*

Main category: cs.CL

TL;DR: 本文提出了一种基于图结构的GsiT模型，通过Interlaced Mask机制优化多模态Transformer的效率，参数减少2/3且性能提升。


<details>
  <summary>Details</summary>
Motivation: 多模态Transformer（MulTs）在多模态情感分析中效率低下，需要优化。

Method: 将MulTs建模为层次化模态异构图（HMHGs），并设计GsiT模型，引入Interlaced Mask机制实现参数共享。

Result: GsiT参数仅为MulTs的1/3，性能显著提升，并在多个SOTA模型中验证了有效性。

Conclusion: GsiT和HMHG概念在多模态情感分析中高效且有效，为未来研究提供了新思路。

Abstract: Multimodal Sentiment Analysis (MSA) is a rapidly developing field that
integrates multimodal information to recognize sentiments, and existing models
have made significant progress in this area. The central challenge in MSA is
multimodal fusion, which is predominantly addressed by Multimodal Transformers
(MulTs). Although act as the paradigm, MulTs suffer from efficiency concerns.
In this work, from the perspective of efficiency optimization, we propose and
prove that MulTs are hierarchical modal-wise heterogeneous graphs (HMHGs), and
we introduce the graph-structured representation pattern of MulTs. Based on
this pattern, we propose an Interlaced Mask (IM) mechanism to design the
Graph-Structured and Interlaced-Masked Multimodal Transformer (GsiT). It is
formally equivalent to MulTs which achieves an efficient weight-sharing
mechanism without information disorder through IM, enabling All-Modal-In-One
fusion with only 1/3 of the parameters of pure MulTs. A Triton kernel called
Decomposition is implemented to ensure avoiding additional computational
overhead. Moreover, it achieves significantly higher performance than
traditional MulTs. To further validate the effectiveness of GsiT itself and the
HMHG concept, we integrate them into multiple state-of-the-art models and
demonstrate notable performance improvements and parameter reduction on widely
used MSA datasets.

</details>


### [15] [MateICL: Mitigating Attention Dispersion in Large-Scale In-Context Learning](https://arxiv.org/abs/2505.01110)
*Murtadha Ahmed,Wenbo,Liu yunfeng*

Main category: cs.CL

TL;DR: MateICL通过分窗和注意力权重调整，解决了大尺度上下文学习中注意力分散的问题，提升了ICL性能。


<details>
  <summary>Details</summary>
Motivation: 预训练模型的固定位置长度限制和注意力分散问题限制了上下文学习的性能。

Method: 将上下文分窗处理，并引入额外层重新校准注意力权重，优先处理查询标记。

Result: MateICL能有效利用更大上下文提升性能，优于基于检索的基线方法。

Conclusion: MateICL在计算资源受限的环境中仍具优势，代码已开源。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
In-Context Learning (ICL). However, the fixed position length constraints in
pre-trained models limit the number of demonstration examples. Recent efforts
to extend context suffer from attention dispersion as the number of
demonstrations increases. In this paper, we introduce Mitigating Attention
Dispersion in large-scale ICL (MateICL) that enables LLMs to maintain effective
self-attention as the context size grows. We first split the context into
multiple windows, each filled to the model's context capacity, which are
processed separately. Then, we introduce an additional layer to recalibrate the
attention weights, prioritizing the query tokens as the number of
demonstrations increases. Our empirical results show that MateICL can
effectively leverage larger contexts to improve ICL performance. Compared to
retrieval-based baselines, MateICL consistently achieves better performance
without requiring an externally trained retrieval model. Despite recent
advances in inference strategies (e.g., 32k token contexts), our results
demonstrate that MateICL remains beneficial in computationally
resource-constrained settings. The code is publicly available at
https://github.com/amurtadha/MateICL.

</details>


### [16] [On the Limitations of Steering in Language Model Alignment](https://arxiv.org/abs/2505.01162)
*Chebrolu Niranjan,Kokil Jaidka,Gerard Christopher Yeo*

Main category: cs.CL

TL;DR: 本文评估了引导向量作为语言模型对齐机制的局限性，发现其在特定任务（如价值观对齐）中有效，但在复杂场景下可能不足。


<details>
  <summary>Details</summary>
Motivation: 研究引导向量在语言模型对齐中的潜力与限制，为未来研究奠定方法基础。

Method: 使用变压器钩干预和反义词功能向量框架，评估提示结构和上下文复杂性对引导效果的影响。

Result: 引导向量在特定对齐任务（如价值观对齐）中表现良好，但在复杂场景下效果有限。

Conclusion: 引导向量虽具潜力，但需进一步研究以提升其在通用对齐任务中的鲁棒性。

Abstract: Steering vectors are a promising approach to aligning language model behavior
at inference time. In this paper, we propose a framework to assess the
limitations of steering vectors as alignment mechanisms. Using a framework of
transformer hook interventions and antonym-based function vectors, we evaluate
the role of prompt structure and context complexity in steering effectiveness.
Our findings indicate that steering vectors are promising for specific
alignment tasks, such as value alignment, but may not provide a robust
foundation for general-purpose alignment in LLMs, particularly in complex
scenarios. We establish a methodological foundation for future investigations
into steering capabilities of reasoning models.

</details>


### [17] [Gender Bias in Explainability: Investigating Performance Disparity in Post-hoc Methods](https://arxiv.org/abs/2505.01198)
*Mahdi Dhaini,Ege Erdogan,Nils Feldhus,Gjergji Kasneci*

Main category: cs.CL

TL;DR: 研究发现，广泛使用的后验特征归因方法在性别上存在显著差异，影响解释的忠实性、鲁棒性和复杂性，即使数据无偏。


<details>
  <summary>Details</summary>
Motivation: 探讨解释方法在公平性上的不足，尤其是性别差异问题。

Method: 分析三项任务和五个语言模型中的后验特征归因方法。

Result: 发现解释方法存在性别差异，且与训练数据无关。

Conclusion: 强调在开发解释方法时需关注公平性，并将其纳入监管框架。

Abstract: While research on applications and evaluations of explanation methods
continues to expand, fairness of the explanation methods concerning disparities
in their performance across subgroups remains an often overlooked aspect. In
this paper, we address this gap by showing that, across three tasks and five
language models, widely used post-hoc feature attribution methods exhibit
significant gender disparity with respect to their faithfulness, robustness,
and complexity. These disparities persist even when the models are pre-trained
or fine-tuned on particularly unbiased datasets, indicating that the
disparities we observe are not merely consequences of biased training data. Our
results highlight the importance of addressing disparities in explanations when
developing and applying explainability methods, as these can lead to biased
outcomes against certain subgroups, with particularly critical implications in
high-stakes contexts. Furthermore, our findings underscore the importance of
incorporating the fairness of explanations, alongside overall model fairness
and explainability, as a requirement in regulatory frameworks.

</details>


### [18] [EvalxNLP: A Framework for Benchmarking Post-Hoc Explainability Methods on NLP Models](https://arxiv.org/abs/2505.01238)
*Mahdi Dhaini,Kafaite Zahra Hussain,Efstratios Zaradoukas,Gjergji Kasneci*

Main category: cs.CL

TL;DR: EvalxNLP是一个Python框架，用于评估NLP模型的解释方法，整合了多种XAI技术，支持用户生成和评估解释，并提供了交互式文本解释功能。


<details>
  <summary>Details</summary>
Motivation: 随着NLP模型在高风险应用中的普及，确保其可解释性成为关键挑战。现有解释方法多样，但缺乏适合不同需求的框架。

Method: EvalxNLP整合了八种XAI技术，支持生成和评估解释，并提供基于LLM的交互式文本解释功能。

Result: 用户评估显示对EvalxNLP满意度高，表明其适合多样化用户群体。

Conclusion: EvalxNLP是一个用户友好且可扩展的平台，旨在推动XAI技术在NLP中的系统比较和发展。

Abstract: As Natural Language Processing (NLP) models continue to evolve and become
integral to high-stakes applications, ensuring their interpretability remains a
critical challenge. Given the growing variety of explainability methods and
diverse stakeholder requirements, frameworks that help stakeholders select
appropriate explanations tailored to their specific use cases are increasingly
important. To address this need, we introduce EvalxNLP, a Python framework for
benchmarking state-of-the-art feature attribution methods for transformer-based
NLP models. EvalxNLP integrates eight widely recognized explainability
techniques from the Explainable AI (XAI) literature, enabling users to generate
and evaluate explanations based on key properties such as faithfulness,
plausibility, and complexity. Our framework also provides interactive,
LLM-based textual explanations, facilitating user understanding of the
generated explanations and evaluation outcomes. Human evaluation results
indicate high user satisfaction with EvalxNLP, suggesting it is a promising
framework for benchmarking explanation methods across diverse user groups. By
offering a user-friendly and extensible platform, EvalxNLP aims at
democratizing explainability tools and supporting the systematic comparison and
advancement of XAI techniques in NLP.

</details>


### [19] [PREMISE: Matching-based Prediction for Accurate Review Recommendation](https://arxiv.org/abs/2505.01255)
*Wei Han,Hui Chen,Soujanya Poria*

Main category: cs.CL

TL;DR: PREMISE是一种基于匹配分数的多模态学习架构，用于多模态评论有用性任务，通过多尺度多领域表示和语义去重提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决传统融合方法在多模态任务中因跨模态注意力导致的性能不足问题。

Method: 计算多尺度多领域表示，过滤重复语义，生成匹配分数作为特征向量。

Result: 在两个公开数据集上表现优异，计算成本更低。

Conclusion: PREMISE在多模态任务中优于现有融合方法，尤其适用于上下文匹配内容与任务目标高度相关的场景。

Abstract: We present PREMISE (PREdict with Matching ScorEs), a new architecture for the
matching-based learning in the multimodal fields for the multimodal review
helpfulness (MRHP) task. Distinct to previous fusion-based methods which
obtains multimodal representations via cross-modal attention for downstream
tasks, PREMISE computes the multi-scale and multi-field representations,
filters duplicated semantics, and then obtained a set of matching scores as
feature vectors for the downstream recommendation task. This new architecture
significantly boosts the performance for such multimodal tasks whose context
matching content are highly correlated to the targets of that task, compared to
the state-of-the-art fusion-based methods. Experimental results on two publicly
available datasets show that PREMISE achieves promising performance with less
computational cost.

</details>


### [20] [Anti-adversarial Learning: Desensitizing Prompts for Large Language Models](https://arxiv.org/abs/2505.01273)
*Xuan Li,Zhe Yin,Xiaodong Gu,Beijun Shen*

Main category: cs.CL

TL;DR: 提出了一种名为PromptObfus的新方法，通过反对抗学习扰动提示中的隐私词，以保护用户隐私，同时保持模型预测的稳定性。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLMs）的广泛应用，用户提示中的隐私保护变得至关重要，传统方法因计算成本高和用户参与需求而受限。

Method: 将提示脱敏任务建模为掩码语言建模任务，用[MASK]替换隐私敏感词，并通过替代模型的梯度反馈选择候选替换词。

Result: 在三个NLP任务上验证了PromptObfus的有效性，结果显示其能有效防止隐私泄露并保持任务性能。

Conclusion: PromptObfus是一种有效的隐私保护方法，适用于LLM场景，平衡了隐私保护和模型性能。

Abstract: With the widespread use of LLMs, preserving privacy in user prompts has
become crucial, as prompts risk exposing privacy and sensitive data to the
cloud LLMs. Traditional techniques like homomorphic encryption, secure
multi-party computation, and federated learning face challenges due to heavy
computational costs and user participation requirements, limiting their
applicability in LLM scenarios. In this paper, we propose PromptObfus, a novel
method for desensitizing LLM prompts. The core idea of PromptObfus is
"anti-adversarial" learning, which perturbs privacy words in the prompt to
obscure sensitive information while retaining the stability of model
predictions. Specifically, PromptObfus frames prompt desensitization as a
masked language modeling task, replacing privacy-sensitive terms with a [MASK]
token. A desensitization model is trained to generate candidate replacements
for each masked position. These candidates are subsequently selected based on
gradient feedback from a surrogate model, ensuring minimal disruption to the
task output. We demonstrate the effectiveness of our approach on three NLP
tasks. Results show that PromptObfus effectively prevents privacy inference
from remote LLMs while preserving task performance.

</details>


### [21] [TF1-EN-3M: Three Million Synthetic Moral Fables for Training Small, Open Language Models](https://arxiv.org/abs/2504.20605)
*Mihai Nadas,Laura Diosan,Andrei Piscoran,Andreea Tomescu*

Main category: cs.CL

TL;DR: TF1-EN-3M是一个由不超过8B参数的指令调优模型生成的300万英语寓言数据集，填补了现代NLP中缺乏结构化道德叙事数据的空白。


<details>
  <summary>Details</summary>
Motivation: 现代NLP缺乏结合连贯叙事与明确道德教训的大规模结构化数据集，TF1-EN-3M旨在填补这一空白。

Method: 使用组合式提示引擎生成遵循六槽框架的寓言，并通过混合评估流程（基于GPT的评分和无参考多样性指标）评估质量。

Result: 8B参数的Llama-3变体在质量和速度上表现最佳，单块消费级GPU即可高效生成高质量寓言。

Conclusion: TF1-EN-3M为指令遵循、叙事智能等研究提供了新资源，证明大规模道德叙事无需依赖专有巨型模型。

Abstract: Moral stories are a time-tested vehicle for transmitting values, yet modern
NLP lacks a large, structured corpus that couples coherent narratives with
explicit ethical lessons. We close this gap with TF1-EN-3M, the first open
dataset of three million English-language fables generated exclusively by
instruction-tuned models no larger than 8B parameters. Each story follows a
six-slot scaffold (character -> trait -> setting -> conflict -> resolution ->
moral), produced through a combinatorial prompt engine that guarantees genre
fidelity while covering a broad thematic space.
  A hybrid evaluation pipeline blends (i) a GPT-based critic that scores
grammar, creativity, moral clarity, and template adherence with (ii)
reference-free diversity and readability metrics. Among ten open-weight
candidates, an 8B-parameter Llama-3 variant delivers the best quality-speed
trade-off, producing high-scoring fables on a single consumer GPU (<24 GB VRAM)
at approximately 13.5 cents per 1,000 fables.
  We release the dataset, generation code, evaluation scripts, and full
metadata under a permissive license, enabling exact reproducibility and cost
benchmarking. TF1-EN-3M opens avenues for research in instruction following,
narrative intelligence, value alignment, and child-friendly educational AI,
demonstrating that large-scale moral storytelling no longer requires
proprietary giant models.

</details>


### [22] [A Factorized Probabilistic Model of the Semantics of Vague Temporal Adverbials Relative to Different Event Types](https://arxiv.org/abs/2505.01311)
*Svenja Kenneweg,Jörg Deigmöller,Julian Eggert,Philipp Cimiano*

Main category: cs.CL

TL;DR: 论文提出了一种因子化模型，用于捕捉模糊时间副词的语义，并将其与事件特定分布结合，生成上下文意义。与非因子化模型相比，该模型在预测能力相似的情况下更简洁且扩展性更好。


<details>
  <summary>Details</summary>
Motivation: 模糊时间副词（如“最近”、“刚刚”、“很久以前”）描述了事件与当前时间的时间距离，但未明确具体时长。研究旨在通过概率分布模型更准确地捕捉这些副词的语义。

Method: 提出因子化模型，将模糊时间副词的语义建模为概率分布，并与事件特定分布结合。使用现有数据拟合模型参数，对比非因子化高斯分布模型。

Result: 因子化模型与非因子化模型预测能力相似，但前者更简洁且扩展性更好，符合奥卡姆剃刀原则。

Conclusion: 因子化模型在捕捉模糊时间副词语义方面更具优势，适合进一步扩展和应用。

Abstract: Vague temporal adverbials, such as recently, just, and a long time ago,
describe the temporal distance between a past event and the utterance time but
leave the exact duration underspecified. In this paper, we introduce a
factorized model that captures the semantics of these adverbials as
probabilistic distributions. These distributions are composed with
event-specific distributions to yield a contextualized meaning for an adverbial
applied to a specific event. We fit the model's parameters using existing data
capturing judgments of native speakers regarding the applicability of these
vague temporal adverbials to events that took place a given time ago. Comparing
our approach to a non-factorized model based on a single Gaussian distribution
for each pair of event and temporal adverbial, we find that while both models
have similar predictive power, our model is preferable in terms of Occam's
razor, as it is simpler and has better extendability.

</details>


### [23] [A Transformer-based Neural Architecture Search Method](https://arxiv.org/abs/2505.01314)
*Shang Wang,Huanrong Tang,Jianquan Ouyang*

Main category: cs.CL

TL;DR: 本文提出了一种基于Transformer架构的神经架构搜索方法，通过多目标遗传算法优化网络结构，结合BLEU分数和困惑度作为评估指标，实验结果表明其性能优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 为了提高翻译结果的质量，探索更优的神经架构搜索方法，结合多目标评估指标（BLEU和困惑度）来优化模型。

Method: 基于Transformer架构，通过多目标遗传算法搜索多头注意力计算方式，结合BLEU和困惑度作为评估指标，迭代优化网络结构。

Result: 搜索到的神经网络结构性能优于所有基线模型，且引入困惑度作为辅助评估指标比仅使用BLEU分数能找到更好的模型。

Conclusion: 多目标评估指标（BLEU和困惑度）结合遗传算法能有效提升神经架构搜索的效果，为翻译任务提供更优的网络结构。

Abstract: This paper presents a neural architecture search method based on Transformer
architecture, searching cross multihead attention computation ways for
different number of encoder and decoder combinations. In order to search for
neural network structures with better translation results, we considered
perplexity as an auxiliary evaluation metric for the algorithm in addition to
BLEU scores and iteratively improved each individual neural network within the
population by a multi-objective genetic algorithm. Experimental results show
that the neural network structures searched by the algorithm outperform all the
baseline models, and that the introduction of the auxiliary evaluation metric
can find better models than considering only the BLEU score as an evaluation
metric.

</details>


### [24] [Helping Big Language Models Protect Themselves: An Enhanced Filtering and Summarization System](https://arxiv.org/abs/2505.01315)
*Sheikh Samit Muhaimin,Spyridon Mastorakis*

Main category: cs.CL

TL;DR: 该论文提出了一种无需重新训练大语言模型（LLM）的防御框架，通过提示过滤和总结模块识别并抵御恶意输入，实验显示其识别成功率高达98.71%。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型易受对抗性攻击和恶意输入的影响，现有防御方法通常需要重新训练模型，计算成本高且不实用。

Method: 提出一个包含提示过滤模块（使用NLP技术检测有害输入）和总结模块（提供上下文防御知识）的框架。

Result: 实验结果表明，该方法在识别有害模式、操纵性语言结构和编码提示方面成功率为98.71%，并提高了模型的抗攻击能力。

Conclusion: 该框架显著增强了LLM的抗攻击能力，是一种高效且无需重新训练的防御方案。

Abstract: The recent growth in the use of Large Language Models has made them
vulnerable to sophisticated adversarial assaults, manipulative prompts, and
encoded malicious inputs. Existing countermeasures frequently necessitate
retraining models, which is computationally costly and impracticable for
deployment. Without the need for retraining or fine-tuning, this study presents
a unique defense paradigm that allows LLMs to recognize, filter, and defend
against adversarial or malicious inputs on their own. There are two main parts
to the suggested framework: (1) A prompt filtering module that uses
sophisticated Natural Language Processing (NLP) techniques, including zero-shot
classification, keyword analysis, and encoded content detection (e.g. base64,
hexadecimal, URL encoding), to detect, decode, and classify harmful inputs; and
(2) A summarization module that processes and summarizes adversarial research
literature to give the LLM context-aware defense knowledge. This approach
strengthens LLMs' resistance to adversarial exploitation by fusing text
extraction, summarization, and harmful prompt analysis. According to
experimental results, this integrated technique has a 98.71% success rate in
identifying harmful patterns, manipulative language structures, and encoded
prompts. By employing a modest amount of adversarial research literature as
context, the methodology also allows the model to react correctly to harmful
inputs with a larger percentage of jailbreak resistance and refusal rate. While
maintaining the quality of LLM responses, the framework dramatically increases
LLM's resistance to hostile misuse, demonstrating its efficacy as a quick and
easy substitute for time-consuming, retraining-based defenses.

</details>


### [25] [TRAVELER: A Benchmark for Evaluating Temporal Reasoning across Vague, Implicit and Explicit References](https://arxiv.org/abs/2505.01325)
*Svenja Kenneweg,Jörg Deigmöller,Philipp Cimiano,Julian Eggert*

Main category: cs.CL

TL;DR: TRAVELER是一个新的合成基准数据集，用于评估模型在解决显式、隐式和模糊时间引用方面的能力，并通过问答任务测试了四种最先进的大语言模型。


<details>
  <summary>Details</summary>
Motivation: 现有基准对时间引用的系统评估有限，因此需要填补这一空白。

Method: 引入TRAVELER数据集，包含3300个问题，评估模型对不同类型时间引用的处理能力，并通过人类调查确定模糊时间引用的正确答案。

Result: 模型在处理少量事件和显式时间引用时表现良好，但随着事件数量增加或时间引用模糊，性能明显下降。

Conclusion: TRAVELER为评估模型在时间引用理解方面的能力提供了有效工具，并揭示了现有模型的局限性。

Abstract: Understanding and resolving temporal references is essential in Natural
Language Understanding as we often refer to the past or future in daily
communication. Although existing benchmarks address a system's ability to
reason about and resolve temporal references, systematic evaluation of specific
temporal references remains limited. Towards closing this gap, we introduce
TRAVELER, a novel synthetic benchmark dataset that follows a Question Answering
paradigm and consists of questions involving temporal references with the
corresponding correct answers. TRAVELER assesses models' abilities to resolve
explicit, implicit relative to speech time, and vague temporal references.
Beyond investigating the performance of state-of-the-art LLMs depending on the
type of temporal reference, our benchmark also allows evaluation of performance
in relation to the length of the set of events. For the category of vague
temporal references, ground-truth answers were established via human surveys on
Prolific, following a procedure similar to the one from Kenneweg et al. To
demonstrate the benchmark's applicability, we evaluate four state-of-the-art
LLMs using a question-answering task encompassing 3,300 questions. Our findings
show that while the benchmarked LLMs can answer questions over event sets with
a handful of events and explicit temporal references successfully, performance
clearly deteriorates with larger event set length and when temporal references
get less explicit. Notably, the vague question category exhibits the lowest
performance across all models.
  The benchmark is publicly available at:
https://gitlab.ub.uni-bielefeld.de/s.kenneweg/TRAVELER

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [26] [ROSA: A Knowledge-based Solution for Robot Self-Adaptation](https://arxiv.org/abs/2505.00733)
*Gustavo Rezende Silva,Juliane Päßler,S. Lizeth Tapia Tarifa,Einar Broch Johnsen,Carlos Hernández Corbato*

Main category: cs.AI

TL;DR: ROSA是一个基于知识的机器人自适应性框架，支持任务与架构协同适应（TACA），通过运行时知识推理实现动态调整，并在水下机器人应用中验证了其可行性和性能。


<details>
  <summary>Details</summary>
Motivation: 自主机器人在多样化环境中需处理多任务及不确定性，传统架构和任务决策算法难以应对不同上下文需求。

Method: 提出ROSA框架，通过知识模型捕获应用特定知识，运行时推理决定适应时机和方式，并提供ROS 2开源实现。

Result: 实验证明ROSA在可重用性和开发效率上具有优势，适用于自适应性机器人系统设计。

Conclusion: ROSA为机器人系统提供了一种灵活的自适应解决方案，显著提升了任务与架构的动态协同能力。

Abstract: Autonomous robots must operate in diverse environments and handle multiple
tasks despite uncertainties. This creates challenges in designing software
architectures and task decision-making algorithms, as different contexts may
require distinct task logic and architectural configurations. To address this,
robotic systems can be designed as self-adaptive systems capable of adapting
their task execution and software architecture at runtime based on their
context.This paper introduces ROSA, a novel knowledge-based framework for RObot
Self-Adaptation, which enables task-and-architecture co-adaptation (TACA) in
robotic systems. ROSA achieves this by providing a knowledge model that
captures all application-specific knowledge required for adaptation and by
reasoning over this knowledge at runtime to determine when and how adaptation
should occur. In addition to a conceptual framework, this work provides an
open-source ROS 2-based reference implementation of ROSA and evaluates its
feasibility and performance in an underwater robotics application. Experimental
results highlight ROSA's advantages in reusability and development effort for
designing self-adaptive robotic systems.

</details>


### [27] [Howard's Policy Iteration is Subexponential for Deterministic Markov Decision Problems with Rewards of Fixed Bit-size and Arbitrary Discount Factor](https://arxiv.org/abs/2505.00795)
*Dibyangshu Mukherjee,Shivaram Kalyanakrishnan*

Main category: cs.AI

TL;DR: 本文改进了Howard策略迭代（HPI）在确定性MDP（DMDP）上的运行时间上界，提出了一个次指数上界，且与折扣因子无关。


<details>
  <summary>Details</summary>
Motivation: 尽管HPI算法已有60多年历史，但其在确定性MDP上的运行时间上界仍为指数级，而现有下界仅为线性。本文旨在填补这一理论空白。

Method: 通过参数化奖励的比特大小，提出了一种新的分析方法，改进了HPI在DMDP上的运行时间上界。

Result: 证明了HPI在DMDP上的运行时间上界为次指数级，且适用于仅含两种奖励的DMDP。

Conclusion: 本文显著改进了HPI在DMDP上的理论性能，为未来研究提供了新的方向。

Abstract: Howard's Policy Iteration (HPI) is a classic algorithm for solving Markov
Decision Problems (MDPs). HPI uses a "greedy" switching rule to update from any
non-optimal policy to a dominating one, iterating until an optimal policy is
found. Despite its introduction over 60 years ago, the best-known upper bounds
on HPI's running time remain exponential in the number of states -- indeed even
on the restricted class of MDPs with only deterministic transitions (DMDPs).
Meanwhile, the tightest lower bound for HPI for MDPs with a constant number of
actions per state is only linear. In this paper, we report a significant
improvement: a subexponential upper bound for HPI on DMDPs, which is
parameterised by the bit-size of the rewards, while independent of the discount
factor. The same upper bound also applies to DMDPs with only two possible
rewards (which may be of arbitrary size).

</details>


### [28] [Explanations as Bias Detectors: A Critical Study of Local Post-hoc XAI Methods for Fairness Exploration](https://arxiv.org/abs/2505.00802)
*Vasiliki Papanikou,Danae Pla Karidi,Evaggelia Pitoura,Emmanouil Panagiotou,Eirini Ntoutsi*

Main category: cs.AI

TL;DR: 本文探讨了如何利用可解释性方法检测和解释AI系统中的不公平现象，提出了一种结合局部事后解释方法的流程，并解决了相关关键问题。


<details>
  <summary>Details</summary>
Motivation: 随着AI在影响人类生活的领域广泛应用，公平性和透明性问题日益突出，尤其是在对受保护群体的影响上。可解释性与公平性的交叉成为推动负责任AI系统的重要方向。

Method: 提出了一种流程，整合局部事后解释方法以获取公平性相关见解，并解决了使用解释作为偏见检测器时产生的关键问题。

Result: 结果显示解释方法在公平性方面的潜力，同时强调了需要仔细考虑的关键问题。

Conclusion: 可解释性方法可用于公平性检测，但需注意其局限性和关键问题。

Abstract: As Artificial Intelligence (AI) is increasingly used in areas that
significantly impact human lives, concerns about fairness and transparency have
grown, especially regarding their impact on protected groups. Recently, the
intersection of explainability and fairness has emerged as an important area to
promote responsible AI systems. This paper explores how explainability methods
can be leveraged to detect and interpret unfairness. We propose a pipeline that
integrates local post-hoc explanation methods to derive fairness-related
insights. During the pipeline design, we identify and address critical
questions arising from the use of explanations as bias detectors such as the
relationship between distributive and procedural fairness, the effect of
removing the protected attribute, the consistency and quality of results across
different explanation methods, the impact of various aggregation strategies of
local explanations on group fairness evaluations, and the overall
trustworthiness of explanations as bias detectors. Our results show the
potential of explanation methods used for fairness while highlighting the need
to carefully consider the aforementioned critical aspects.

</details>


### [29] [MIMIC-\RNum{4}-Ext-22MCTS: A 22 Millions-Event Temporal Clinical Time-Series Dataset with Relative Timestamp for Risk Prediction](https://arxiv.org/abs/2505.00827)
*Jing Wang,Xing Niu,Juyong Kim,Jie Shen,Tong Zhang,Jeremy C. Weiss*

Main category: cs.AI

TL;DR: 论文提出了一种从MIMIC-IV-Note中提取临床时间序列事件的方法，并发布了MIMIC-4-Ext-22MCTS数据集，显著提升了医疗应用的模型性能。


<details>
  <summary>Details</summary>
Motivation: 现代医疗中，高质量的临床时间序列数据对机器学习预测模型至关重要，但现有数据存在处理困难和缺乏明确时间戳的问题。

Method: 1) 将出院摘要分解为小块文本；2) 使用上下文BM25和语义搜索筛选包含临床事件的文本块；3) 设计提示词引导Llama-3.1-8B模型识别或推断时间信息。

Result: 基于该数据集微调的BERT模型在医疗问答任务中准确率提升10%，临床试验匹配任务提升3%；GPT-2模型生成的结果更具临床可靠性。

Conclusion: 提出的框架和数据集成效显著，为医疗领域的机器学习应用提供了高质量的数据支持。

Abstract: Clinical risk prediction based on machine learning algorithms plays a vital
role in modern healthcare. A crucial component in developing a reliable
prediction model is collecting high-quality time series clinical events. In
this work, we release such a dataset that consists of 22,588,586 Clinical Time
Series events, which we term MIMIC-\RNum{4}-Ext-22MCTS. Our source data are
discharge summaries selected from the well-known yet unstructured MIMIC-IV-Note
\cite{Johnson2023-pg}. We then extract clinical events as short text span from
the discharge summaries, along with the timestamps of these events as temporal
information. The general-purpose MIMIC-IV-Note pose specific challenges for our
work: it turns out that the discharge summaries are too lengthy for typical
natural language models to process, and the clinical events of interest often
are not accompanied with explicit timestamps. Therefore, we propose a new
framework that works as follows: 1) we break each discharge summary into
manageably small text chunks; 2) we apply contextual BM25 and contextual
semantic search to retrieve chunks that have a high potential of containing
clinical events; and 3) we carefully design prompts to teach the recently
released Llama-3.1-8B \cite{touvron2023llama} model to identify or infer
temporal information of the chunks. We show that the obtained dataset is so
informative and transparent that standard models fine-tuned on our dataset are
achieving significant improvements in healthcare applications. In particular,
the BERT model fine-tuned based on our dataset achieves 10\% improvement in
accuracy on medical question answering task, and 3\% improvement in clinical
trial matching task compared with the classic BERT. The GPT-2 model, fine-tuned
on our dataset, produces more clinically reliable results for clinical
questions.

</details>


### [30] [Thoughts without Thinking: Reconsidering the Explanatory Value of Chain-of-Thought Reasoning in LLMs through Agentic Pipelines](https://arxiv.org/abs/2505.00875)
*Ramesh Manuvinakurike,Emanuel Moss,Elizabeth Anne Watkins,Saurav Sahay,Giuseppe Raffa,Lama Nachman*

Main category: cs.AI

TL;DR: 研究发现，在多LLM协作的Agentic管道中，Chain-of-Thought（CoT）推理无法提升输出质量或提供可操作性解释。


<details>
  <summary>Details</summary>
Motivation: 探讨如何在Agentic管道中实现LLM内部工作的透明化，以支持人类中心的可解释性（HCXAI）。

Method: 通过定量和定性分析，研究CoT推理在Agentic管道中的表现。

Result: CoT推理未能提升输出质量，且其生成的解释缺乏实际可操作性。

Conclusion: CoT推理在Agentic管道中无法有效支持可解释性，需探索其他方法。

Abstract: Agentic pipelines present novel challenges and opportunities for
human-centered explainability. The HCXAI community is still grappling with how
best to make the inner workings of LLMs transparent in actionable ways. Agentic
pipelines consist of multiple LLMs working in cooperation with minimal human
control. In this research paper, we present early findings from an agentic
pipeline implementation of a perceptive task guidance system. Through
quantitative and qualitative analysis, we analyze how Chain-of-Thought (CoT)
reasoning, a common vehicle for explainability in LLMs, operates within agentic
pipelines. We demonstrate that CoT reasoning alone does not lead to better
outputs, nor does it offer explainability, as it tends to produce explanations
without explainability, in that they do not improve the ability of end users to
better understand systems or achieve their goals.

</details>


### [31] [Car Sensors Health Monitoring by Verification Based on Autoencoder and Random Forest Regression](https://arxiv.org/abs/2505.00876)
*Sahar Torkhesari,Behnam Yousefimehr,Mehdi Ghatee*

Main category: cs.AI

TL;DR: 论文提出了一种创新的传感器健康监测系统，利用机器学习和深度学习技术评估车辆传感器健康状况，并通过主动检测和替换故障传感器提高系统可靠性，测试准确率达99%。


<details>
  <summary>Details</summary>
Motivation: 为汽车行业开发一种能够实时监测传感器健康状况的系统，以提前发现故障并确保车辆安全运行。

Method: 结合自动编码器检测传感器故障和随机森林回归估计传感器值，利用统计模型（正态分布）主动识别潜在故障。

Result: 在Saipa's Quick车辆的20个关键传感器上测试，系统准确率达到99%。

Conclusion: 该系统通过主动监测和故障替换，显著提高了车辆传感器的可靠性和安全性。

Abstract: Driver assistance systems provide a wide range of crucial services, including
closely monitoring the condition of vehicles. This paper showcases a
groundbreaking sensor health monitoring system designed for the automotive
industry. The ingenious system leverages cutting-edge techniques to process
data collected from various vehicle sensors. It compares their outputs within
the Electronic Control Unit (ECU) to evaluate the health of each sensor. To
unravel the intricate correlations between sensor data, an extensive
exploration of machine learning and deep learning methodologies was conducted.
Through meticulous analysis, the most correlated sensor data were identified.
These valuable insights were then utilized to provide accurate estimations of
sensor values. Among the diverse learning methods examined, the combination of
autoencoders for detecting sensor failures and random forest regression for
estimating sensor values proved to yield the most impressive outcomes. A
statistical model using the normal distribution has been developed to identify
possible sensor failures proactively. By comparing the actual values of the
sensors with their estimated values based on correlated sensors, faulty sensors
can be detected early. When a defective sensor is detected, both the driver and
the maintenance department are promptly alerted. Additionally, the system
replaces the value of the faulty sensor with the estimated value obtained
through analysis. This proactive approach was evaluated using data from twenty
essential sensors in the Saipa's Quick vehicle's ECU, resulting in an
impressive accuracy rate of 99\%.

</details>


### [32] [Seeking to Collide: Online Safety-Critical Scenario Generation for Autonomous Driving with Retrieval Augmented Large Language Models](https://arxiv.org/abs/2505.00972)
*Yuewen Mei,Tong Nie,Jian Sun,Ye Tian*

Main category: cs.AI

TL;DR: 提出了一种基于检索增强的大型语言模型框架，用于在线生成安全关键驾驶场景，显著提升了碰撞率与时间间隔表现。


<details>
  <summary>Details</summary>
Motivation: 现有场景生成方法要么过度拟合常见驾驶模式，要么无法暴露罕见的安全关键场景。

Method: 使用LLM推断背景车辆的最危险意图，并通过查询其他LLM代理合成对抗轨迹，结合动态记忆库扩展行为库。

Result: 在Waymo数据集上，平均最小碰撞时间从1.62秒降至1.08秒，碰撞率提高75%。

Conclusion: 该方法有效生成安全关键场景，显著优于基线方法。

Abstract: Simulation-based testing is crucial for validating autonomous vehicles (AVs),
yet existing scenario generation methods either overfit to common driving
patterns or operate in an offline, non-interactive manner that fails to expose
rare, safety-critical corner cases. In this paper, we introduce an online,
retrieval-augmented large language model (LLM) framework for generating
safety-critical driving scenarios. Our method first employs an LLM-based
behavior analyzer to infer the most dangerous intent of the background vehicle
from the observed state, then queries additional LLM agents to synthesize
feasible adversarial trajectories. To mitigate catastrophic forgetting and
accelerate adaptation, we augment the framework with a dynamic memorization and
retrieval bank of intent-planner pairs, automatically expanding its behavioral
library when novel intents arise. Evaluations using the Waymo Open Motion
Dataset demonstrate that our model reduces the mean minimum time-to-collision
from 1.62 to 1.08 s and incurs a 75% collision rate, substantially
outperforming baselines.

</details>


### [33] [Improving Large Language Model Planning with Action Sequence Similarity](https://arxiv.org/abs/2505.01009)
*Xinran Zhao,Hanie Sedghi,Bernd Bohnet,Dale Schuurmans,Azade Nova*

Main category: cs.AI

TL;DR: 论文提出GRASE-DC方法，通过动作序列相似性（AS）筛选示例，提升大语言模型（LLM）的规划能力，显著提高任务性能。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过上下文学习（ICL）提升LLM的规划能力，并研究哪些信号有助于示例选择。

Method: 提出GRASE-DC两阶段流程：先重采样高AS示例，再通过动态聚类平衡相关性和多样性。

Result: GRASE-DC在多种规划任务中显著提升性能（绝对准确率提高11-40点，示例需求减少27.3%）。结合验证器后性能进一步提升18.9%。

Conclusion: GRASE-DC能泛化至分布外问题，验证了其在不同LLM和任务中的一致性能提升。

Abstract: Planning is essential for artificial intelligence systems to look ahead and
proactively determine a course of actions to reach objectives in the virtual
and real world. Recent work on large language models (LLMs) sheds light on
their planning capability in various tasks. However, it remains unclear what
signals in the context influence the model performance. In this work, we
explore how to improve the model planning capability through in-context
learning (ICL), specifically, what signals can help select the exemplars.
Through extensive experiments, we observe that commonly used problem similarity
may result in false positives with drastically different plans, which can
mislead the model. In response, we propose to sample and filter exemplars
leveraging plan side action sequence similarity (AS). We propose GRASE-DC: a
two-stage pipeline that first re-samples high AS exemplars and then curates the
selected exemplars with dynamic clustering on AS to achieve a balance of
relevance and diversity. Our experimental result confirms that GRASE-DC
achieves significant performance improvement on various planning tasks (up to
~11-40 point absolute accuracy improvement with 27.3% fewer exemplars needed on
average). With GRASE-DC* + VAL, where we iteratively apply GRASE-DC with a
validator, we are able to even boost the performance by 18.9% more.
  Extensive analysis validates the consistent performance improvement of
GRASE-DC with various backbone LLMs and on both classical planning and natural
language planning benchmarks. GRASE-DC can further boost the planning accuracy
by ~24 absolute points on harder problems using simpler problems as exemplars
over a random baseline. This demonstrates its ability to generalize to
out-of-distribution problems.

</details>


### [34] [Adaptive Wizard for Removing Cross-Tier Misconfigurations in Active Directory](https://arxiv.org/abs/2505.01028)
*Huy Q. Ngo,Mingyu Guo,Hung Nguyen*

Main category: cs.AI

TL;DR: 论文提出了一种自适应路径移除问题，通过最小化IT管理员与安全向导之间的交互次数来优化Windows AD系统的安全加固过程。


<details>
  <summary>Details</summary>
Motivation: 由于手动验证AD系统中的安全漏洞修复需要大量人力，研究旨在减少这一过程中的迭代步骤。

Method: 提出自适应路径移除模型，由向导提出攻击路径，IT管理员选择移除边，直到目标断开或达到限制。设计了精确算法、近似算法和启发式方法（如DPR）。

Result: 证明了问题的复杂性，提出了多种解决方案，其中DPR在大规模图上表现优异。

Conclusion: 算法在合成和真实AD图上验证有效，DPR在性能和可扩展性上优于其他方法。

Abstract: Security vulnerabilities in Windows Active Directory (AD) systems are
typically modeled using an attack graph and hardening AD systems involves an
iterative workflow: security teams propose an edge to remove, and IT operations
teams manually review these fixes before implementing the removal. As
verification requires significant manual effort, we formulate an Adaptive Path
Removal Problem to minimize the number of steps in this iterative removal
process. In our model, a wizard proposes an attack path in each step and
presents it as a set of multiple-choice options to the IT admin. The IT admin
then selects one edge from the proposed set to remove. This process continues
until the target $t$ is disconnected from source $s$ or the number of proposed
paths reaches $B$. The model aims to optimize the human effort by minimizing
the expected number of interactions between the IT admin and the security
wizard. We first prove that the problem is $\mathcal{\#P}$-hard. We then
propose a set of solutions including an exact algorithm, an approximate
algorithm, and several scalable heuristics. Our best heuristic, called DPR, can
operate effectively on larger-scale graphs compared to the exact algorithm and
consistently outperforms the approximate algorithm across all graphs. We verify
the effectiveness of our algorithms on several synthetic AD graphs and an AD
attack graph collected from a real organization.

</details>


### [35] [Retrieval Augmented Learning: A Retrial-based Large Language Model Self-Supervised Learning and Autonomous Knowledge Generation](https://arxiv.org/abs/2505.01073)
*Zongyuan Li,Pengfei Li,Runnan Qi,Yanan Ni,Lumin Jiang,Hui Wu,Xuebo Zhang,Kuihua Huang,Xian Guo*

Main category: cs.AI

TL;DR: 本文提出了一种无需训练的奖励自由自监督学习框架RAL，通过检索增强生成（RAG）组织中间数据，实现三阶段自主知识生成，显著降低幻觉并提升决策性能。


<details>
  <summary>Details</summary>
Motivation: 领域特定数据在大型语言模型（LLM）预训练中的缺乏限制了其在专业应用中的表现，而后续训练又需要大量计算资源。

Method: 提出Retrial-Augmented Learning（RAL）框架，利用RAG组织数据，实现假设提出、验证和知识生成的三阶段自主知识生成。

Result: 在LLM-PySC2环境中验证，RAL有效减少幻觉并提升决策性能，同时表现出在OOD任务、鲁棒性和可迁移性方面的潜力。

Conclusion: RAL是一种低成本高效的决策问题解决方案，适用于自主知识生成。

Abstract: The lack of domain-specific data in the pre-training of Large Language Models
(LLMs) severely limits LLM-based decision systems in specialized applications,
while post-training a model in the scenarios requires significant computational
resources. In this paper, we present Retrial-Augmented Learning (RAL), a
reward-free self-supervised learning framework for LLMs that operates without
model training. By developing Retrieval-Augmented Generation (RAG) into a
module for organizing intermediate data, we realized a three-stage autonomous
knowledge generation of proposing a hypothesis, validating the hypothesis, and
generating the knowledge. The method is evaluated in the LLM-PySC2 environment,
a representative decision-making platform that combines sufficient complexity
with domain-specific knowledge requirements. Experiments demonstrate that the
proposed method effectively reduces hallucination by generating and utilizing
validated knowledge, and increases decision-making performance at an extremely
low cost. Meanwhile, the approach exhibits potential in
out-of-distribution(OOD) tasks, robustness, and transferability, making it a
cost-friendly but effective solution for decision-making problems and
autonomous knowledge generation.

</details>


### [36] [MADIL: An MDL-based Framework for Efficient Program Synthesis in the ARC Benchmark](https://arxiv.org/abs/2505.01081)
*Sébastien Ferré*

Main category: cs.AI

TL;DR: MADIL是一种基于最小描述长度（MDL）原则的新型AI方法，旨在高效学习与泛化，尽管性能低于LLM，但更高效且可解释。


<details>
  <summary>Details</summary>
Motivation: 解决AI在高效技能获取和泛化方面的不足，特别是针对ARC基准测试的需求。

Method: 采用MDL原则进行模式分解，实现结构化泛化。

Result: MADIL在ArcPrize 2024中表现7%，低于LLM方法，但更高效且可解释。

Conclusion: MADIL为高效学习提供了一种新途径，尽管性能有限，但其效率和可解释性具有潜力。

Abstract: Artificial Intelligence (AI) has achieved remarkable success in specialized
tasks but struggles with efficient skill acquisition and generalization. The
Abstraction and Reasoning Corpus (ARC) benchmark evaluates intelligence based
on minimal training requirements. While Large Language Models (LLMs) have
recently improved ARC performance, they rely on extensive pre-training and high
computational costs. We introduce MADIL (MDL-based AI), a novel approach
leveraging the Minimum Description Length (MDL) principle for efficient
inductive learning. MADIL performs pattern-based decomposition, enabling
structured generalization. While its performance (7% at ArcPrize 2024) remains
below LLM-based methods, it offers greater efficiency and interpretability.
This paper details MADIL's methodology, its application to ARC, and
experimental evaluations.

</details>


### [37] [Explainable AI Based Diagnosis of Poisoning Attacks in Evolutionary Swarms](https://arxiv.org/abs/2505.01181)
*Mehrdad Asadi,Roxana Rădulescu,Ann Nowé*

Main category: cs.AI

TL;DR: 论文提出了一种框架，利用可解释AI方法研究数据投毒攻击对多无人机网络等群体系统的影响，并展示了如何通过该方法诊断攻击。


<details>
  <summary>Details</summary>
Motivation: 群体系统在关键环境中的协作任务中表现出色，但团队级协调策略易受数据投毒攻击影响，导致协调不准确或对抗行为。

Method: 使用进化智能建模代理间互动，形成最优联盟执行任务，并通过数据操纵攻击系统性毒害群体模型。

Result: 研究发现，当模型被毒害超过10%时，会导致非最优策略和不高效协作。

Conclusion: 可解释AI方法能有效量化投毒对团队策略的影响，并提供诊断攻击的足迹特征。

Abstract: Swarming systems, such as for example multi-drone networks, excel at
cooperative tasks like monitoring, surveillance, or disaster assistance in
critical environments, where autonomous agents make decentralized decisions in
order to fulfill team-level objectives in a robust and efficient manner.
Unfortunately, team-level coordinated strategies in the wild are vulnerable to
data poisoning attacks, resulting in either inaccurate coordination or
adversarial behavior among the agents. To address this challenge, we contribute
a framework that investigates the effects of such data poisoning attacks, using
explainable AI methods. We model the interaction among agents using
evolutionary intelligence, where an optimal coalition strategically emerges to
perform coordinated tasks. Then, through a rigorous evaluation, the swarm model
is systematically poisoned using data manipulation attacks. We showcase the
applicability of explainable AI methods to quantify the effects of poisoning on
the team strategy and extract footprint characterizations that enable
diagnosing. Our findings indicate that when the model is poisoned above 10%,
non-optimal strategies resulting in inefficient cooperation can be identified.

</details>


### [38] [Exploring the Impact of Explainable AI and Cognitive Capabilities on Users' Decisions](https://arxiv.org/abs/2505.01192)
*Federico Maria Cau,Lucio Davide Spano*

Main category: cs.AI

TL;DR: 研究探讨了不同解释风格（如基于示例、特征、规则和反事实）和AI信息（预测、置信度和准确性）对贷款申请场景中准确性、AI依赖和认知负荷的影响，以及高低NFC个体的差异。


<details>
  <summary>Details</summary>
Motivation: 随着AI在决策中的应用增加，解释性AI（XAI）的研究多集中于基于特征的解释，而忽略了其他风格。研究旨在探索不同解释风格和AI信息对决策的影响，以及NFC特质的作用。

Method: 在贷款申请场景中，测试了不同解释风格（示例、特征、规则、反事实）和AI信息（预测、置信度、准确性）对准确性、AI依赖和认知负荷的影响，并比较了高低NFC个体的差异。

Result: 高AI置信度显著增加对AI的依赖并降低认知负荷；反事实解释虽难理解但提高准确性；高低NFC个体在解释优先级上无显著差异。

Conclusion: 研究强调XAI界面需个性化，结合多种解释风格和用户特征以优化人机协作。

Abstract: Artificial Intelligence (AI) systems are increasingly used for
decision-making across domains, raising debates over the information and
explanations they should provide. Most research on Explainable AI (XAI) has
focused on feature-based explanations, with less attention on alternative
styles. Personality traits like the Need for Cognition (NFC) can also lead to
different decision-making outcomes among low and high NFC individuals. We
investigated how presenting AI information (prediction, confidence, and
accuracy) and different explanation styles (example-based, feature-based,
rule-based, and counterfactual) affect accuracy, reliance on AI, and cognitive
load in a loan application scenario. We also examined low and high NFC
individuals' differences in prioritizing XAI interface elements (loan
attributes, AI information, and explanations), accuracy, and cognitive load.
Our findings show that high AI confidence significantly increases reliance on
AI while reducing cognitive load. Feature-based explanations did not enhance
accuracy compared to other conditions. Although counterfactual explanations
were less understandable, they enhanced overall accuracy, increasing reliance
on AI and reducing cognitive load when AI predictions were correct. Both low
and high NFC individuals prioritized explanations after loan attributes,
leaving AI information as the least important. However, we found no significant
differences between low and high NFC groups in accuracy or cognitive load,
raising questions about the role of personality traits in AI-assisted
decision-making. These findings highlight the need for user-centric
personalization in XAI interfaces, incorporating diverse explanation styles and
exploring multiple personality traits and other user characteristics to
optimize human-AI collaboration.

</details>


### [39] [Early Detection of Patient Deterioration from Real-Time Wearable Monitoring System](https://arxiv.org/abs/2505.01305)
*Lo Pang-Yun Ting,Hong-Pei Chen,An-Shan Liu,Chun-Yin Yeh,Po-Lin Chen,Kun-Ta Chuang*

Main category: cs.AI

TL;DR: 论文提出了一种名为TARL的创新方法，通过建模心率时间序列中的代表性子序列（shapelets）的结构关系，构建知识图谱以预测病情变化，并解决穿戴设备数据中的缺失值问题。


<details>
  <summary>Details</summary>
Motivation: 早期检测患者病情恶化对降低死亡率至关重要，但心率数据的多样性和缺失值处理是主要挑战。

Method: TARL通过建模shapelet的动态关系构建知识图谱，并引入过渡感知知识嵌入以强化关系并量化缺失值影响。

Result: 在真实ICU数据上的实验表明，TARL具有高可靠性和早期检测能力。

Conclusion: TARL作为一种AI驱动工具，可帮助临床医生识别患者早期恶化迹象，具有解释性检测潜力。

Abstract: Early detection of patient deterioration is crucial for reducing mortality
rates. Heart rate data has shown promise in assessing patient health, and
wearable devices offer a cost-effective solution for real-time monitoring.
However, extracting meaningful insights from diverse heart rate data and
handling missing values in wearable device data remain key challenges. To
address these challenges, we propose TARL, an innovative approach that models
the structural relationships of representative subsequences, known as
shapelets, in heart rate time series. TARL creates a shapelet-transition
knowledge graph to model shapelet dynamics in heart rate time series,
indicating illness progression and potential future changes. We further
introduce a transition-aware knowledge embedding to reinforce relationships
among shapelets and quantify the impact of missing values, enabling the
formulation of comprehensive heart rate representations. These representations
capture explanatory structures and predict future heart rate trends, aiding
early illness detection. We collaborate with physicians and nurses to gather
ICU patient heart rate data from wearables and diagnostic metrics assessing
illness severity for evaluating deterioration. Experiments on real-world ICU
data demonstrate that TARL achieves both high reliability and early detection.
A case study further showcases TARL's explainable detection process,
highlighting its potential as an AI-driven tool to assist clinicians in
recognizing early signs of patient deterioration.

</details>


### [40] [BalancEdit: Dynamically Balancing the Generality-Locality Trade-off in Multi-modal Model Editing](https://arxiv.org/abs/2505.01343)
*Dongliang Guo,Mengxuan Hu,Zihan Guan,Thomas Hartvigsen,Sheng Li*

Main category: cs.AI

TL;DR: 论文提出了一种名为BalancEdit的新方法，用于在多模态模型中平衡通用性和局部性的知识编辑，解决了传统编辑方法忽视影响范围的问题。


<details>
  <summary>Details</summary>
Motivation: 大型多模态模型随时间推移会因信息过时而性能下降，传统微调方法因模型规模和复杂性不适用，需要更高效的编辑方法。

Method: 提出了BalancEdit方法，通过生成正负样本确定事实影响范围，利用离散的局部化编辑码本在潜在空间中进行编辑，不修改模型权重。

Result: BalancEdit在通用性和局部性之间实现了最优平衡，保持了强大的编辑能力，同时最小化了性能损失。

Conclusion: BalancEdit是首个明确解决多模态模型编辑中通用性与局部性权衡的方法，实验证明了其有效性。

Abstract: Large multi-modal models inevitably decay over time as facts change and
previously learned information becomes outdated. Traditional approaches such as
fine-tuning are often impractical for updating these models due to their size
and complexity. Instead, direct knowledge editing within the models presents a
more viable solution. Current model editing techniques, however, typically
overlook the unique influence ranges of different facts, leading to compromised
model performance in terms of both generality and locality. To address this
issue, we introduce the concept of the generality-locality trade-off in
multi-modal model editing. We develop a new model editing dataset named OKEDIT,
specifically designed to effectively evaluate this trade-off. Building on this
foundation, we propose BalancEdit, a novel method for balanced model editing
that dynamically achieves an optimal balance between generality and locality.
BalancEdit utilizes a unique mechanism that generates both positive and
negative samples for each fact to accurately determine its influence scope and
incorporates these insights into the model's latent space using a discrete,
localized codebook of edits, without modifying the underlying model weights. To
our knowledge, this is the first approach explicitly addressing the
generality-locality trade-off in multi-modal model editing. Our comprehensive
results confirm the effectiveness of BalancEdit, demonstrating minimal
trade-offs while maintaining robust editing capabilities. Our code and dataset
will be available.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [41] [Constructing an Optimal Behavior Basis for the Option Keyboard](https://arxiv.org/abs/2505.00787)
*Lucas N. Alegre,Ana L. C. Bazzan,André Barreto,Bruno C. da Silva*

Main category: cs.LG

TL;DR: 论文提出了一种新方法，高效构建最优行为基，显著减少确保新任务最优性所需的基础策略数量，并在复杂任务中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 多任务强化学习的目标是以最少的环境交互快速解决新任务。现有方法如GPI和OK虽有效，但依赖基础策略的选择，且计算成本高。因此，研究如何构建最优行为基成为关键问题。

Method: 引入一种新方法，高效构建最优行为基，确保新任务的最优解。该方法比CCS更具表达力，能解决某些非线性任务。

Result: 实验证明，该方法在复杂任务中显著优于现有方法，且所需基础策略数量更少。

Conclusion: 提出的方法解决了构建最优行为基的开放问题，为多任务强化学习提供了更高效的解决方案。

Abstract: Multi-task reinforcement learning aims to quickly identify solutions for new
tasks with minimal or no additional interaction with the environment.
Generalized Policy Improvement (GPI) addresses this by combining a set of base
policies to produce a new one that is at least as good -- though not
necessarily optimal -- as any individual base policy. Optimality can be
ensured, particularly in the linear-reward case, via techniques that compute a
Convex Coverage Set (CCS). However, these are computationally expensive and do
not scale to complex domains. The Option Keyboard (OK) improves upon GPI by
producing policies that are at least as good -- and often better. It achieves
this through a learned meta-policy that dynamically combines base policies.
However, its performance critically depends on the choice of base policies.
This raises a key question: is there an optimal set of base policies -- an
optimal behavior basis -- that enables zero-shot identification of optimal
solutions for any linear tasks? We solve this open problem by introducing a
novel method that efficiently constructs such an optimal behavior basis. We
show that it significantly reduces the number of base policies needed to ensure
optimality in new tasks. We also prove that it is strictly more expressive than
a CCS, enabling particular classes of non-linear tasks to be solved optimally.
We empirically evaluate our technique in challenging domains and show that it
outperforms state-of-the-art approaches, increasingly so as task complexity
increases.

</details>


### [42] [Improving Routing in Sparse Mixture of Experts with Graph of Tokens](https://arxiv.org/abs/2505.00792)
*Tam Nguyen,Ngoc N. Tran,Khai Nguyen,Richard G. Baraniuk*

Main category: cs.LG

TL;DR: 论文提出了一种新的相似性感知（S）MoE和注意力感知（S）MoE方法，通过减少路由波动提高稀疏混合专家（SMoE）模型的鲁棒性和准确性。


<details>
  <summary>Details</summary>
Motivation: SMoE模型在训练后期易受路由波动影响，导致模型不鲁棒。

Method: 通过概率图模型（PGM）分析SMoE的局限性，提出相似性感知和注意力感知路由机制，利用注意力矩阵指导令牌路由。

Result: 实验验证表明，新方法显著减少了路由波动，提高了准确性和模型鲁棒性。

Conclusion: 相似性/注意力感知路由机制能有效稳定令牌路由，提升SMoE模型的性能。

Abstract: Sparse Mixture of Experts (SMoE) has emerged as a key to achieving
unprecedented scalability in deep learning. By activating only a small subset
of parameters per sample, SMoE achieves an exponential increase in parameter
counts while maintaining a constant computational overhead. However, SMoE
models are susceptible to routing fluctuations--changes in the routing of a
given input to its target expert--at the late stage of model training, leading
to model non-robustness. In this work, we unveil the limitation of SMoE through
the perspective of the probabilistic graphical model (PGM). Through this PGM
framework, we highlight the independence in the expert-selection of tokens,
which exposes the model to routing fluctuation and non-robustness. Alleviating
this independence, we propose the novel Similarity-Aware (S)MoE, which
considers interactions between tokens during expert selection. We then derive a
new PGM underlying an (S)MoE-Attention block, going beyond just a single (S)MoE
layer. Leveraging the token similarities captured by the attention matrix, we
propose the innovative Attention-Aware (S)MoE, which employs the attention
matrix to guide the routing of tokens to appropriate experts in (S)MoE. We
theoretically prove that Similarity/Attention-Aware routing help reduce the
entropy of expert selection, resulting in more stable token routing mechanisms.
We empirically validate our models on various tasks and domains, showing
significant improvements in reducing routing fluctuations, enhancing accuracy,
and increasing model robustness over the baseline MoE-Transformer with token
routing via softmax gating.

</details>


### [43] [Scalable Meta-Learning via Mixed-Mode Differentiation](https://arxiv.org/abs/2505.00793)
*Iurii Kemaev,Dan A Calian,Luisa M Zintgraf,Gregory Farquhar,Hado van Hasselt*

Main category: cs.LG

TL;DR: 论文提出MixFlow-MG算法，通过混合模式微分优化梯度计算，显著提升内存和计算效率。


<details>
  <summary>Details</summary>
Motivation: 梯度双层优化在超参数优化、任务适应等领域广泛应用，但现有自动微分库难以高效处理其复杂计算需求。

Method: 提出Mixed-Flow Meta-Gradients（MixFlow-MG），利用混合模式微分构建高效计算图。

Result: 在元学习场景中，内存节省超10倍，计算时间减少25%。

Conclusion: MixFlow-MG为梯度双层优化提供了更高效、可扩展的解决方案。

Abstract: Gradient-based bilevel optimisation is a powerful technique with applications
in hyperparameter optimisation, task adaptation, algorithm discovery,
meta-learning more broadly, and beyond. It often requires differentiating
through the gradient-based optimisation process itself, leading to
"gradient-of-a-gradient" calculations with computationally expensive
second-order and mixed derivatives. While modern automatic differentiation
libraries provide a convenient way to write programs for calculating these
derivatives, they oftentimes cannot fully exploit the specific structure of
these problems out-of-the-box, leading to suboptimal performance. In this
paper, we analyse such cases and propose Mixed-Flow Meta-Gradients, or
MixFlow-MG -- a practical algorithm that uses mixed-mode differentiation to
construct more efficient and scalable computational graphs yielding over 10x
memory and up to 25% wall-clock time improvements over standard implementations
in modern meta-learning setups.

</details>


### [44] [A Mathematical Philosophy of Explanations in Mechanistic Interpretability -- The Strange Science Part I.i](https://arxiv.org/abs/2505.00808)
*Kola Ayonrinde,Louis Jaburi*

Main category: cs.LG

TL;DR: 论文提出解释性视图假说，认为机制可解释性研究是一种理解神经网络的系统性方法，并提出解释忠实性作为评估标准。


<details>
  <summary>Details</summary>
Motivation: 探讨机制可解释性（MI）作为一种理解神经网络的方法，并明确其定义与界限。

Method: 提出MI的定义，包括模型层面、本体性、因果机制性和可证伪性，并阐述解释性乐观主义原则。

Result: 明确了MI的框架及其与其他可解释性范式的区别，同时指出了MI的固有局限性。

Conclusion: 机制可解释性是一种有效的理解神经网络的方法，但其成功依赖于解释性乐观主义原则。

Abstract: Mechanistic Interpretability aims to understand neural networks through
causal explanations. We argue for the Explanatory View Hypothesis: that
Mechanistic Interpretability research is a principled approach to understanding
models because neural networks contain implicit explanations which can be
extracted and understood. We hence show that Explanatory Faithfulness, an
assessment of how well an explanation fits a model, is well-defined. We propose
a definition of Mechanistic Interpretability (MI) as the practice of producing
Model-level, Ontic, Causal-Mechanistic, and Falsifiable explanations of neural
networks, allowing us to distinguish MI from other interpretability paradigms
and detail MI's inherent limits. We formulate the Principle of Explanatory
Optimism, a conjecture which we argue is a necessary precondition for the
success of Mechanistic Interpretability.

</details>


### [45] [Scalable Unit Harmonization in Medical Informatics Using Bi-directional Transformers and Bayesian-Optimized BM25 and Sentence Embedding Retrieval](https://arxiv.org/abs/2505.00810)
*Jordi de la Torre*

Main category: cs.LG

TL;DR: 提出了一种结合BM25、句子嵌入和双向Transformer的方法，用于大规模临床数据集中不一致单位的标准化，显著提高了检索和匹配性能。


<details>
  <summary>Details</summary>
Motivation: 解决临床数据集中单位不一致的问题，提升数据互操作性。

Method: 设计了一个多阶段流程，包括过滤、识别、标准化建议生成、自动重排和人工验证，结合BM25、句子嵌入和双向Transformer分类器。

Result: 混合检索方法（MRR: 0.8833）显著优于纯词法或纯嵌入方法，重排后MRR提升至0.9833，精确度和召回率分别达到83.39%和94.66%。

Conclusion: 该框架为临床数据单位标准化提供了高效、可扩展的解决方案，减少了人工工作量并提高了准确性，支持跨医疗系统的数据重用和可靠的多机构研究。

Abstract: Objective: To develop and evaluate a scalable methodology for harmonizing
inconsistent units in large-scale clinical datasets, addressing a key barrier
to data interoperability.
  Materials and Methods: We designed a novel unit harmonization system
combining BM25, sentence embeddings, Bayesian optimization, and a bidirectional
transformer based binary classifier for retrieving and matching laboratory test
entries. The system was evaluated using the Optum Clinformatics Datamart
dataset (7.5 billion entries). We implemented a multi-stage pipeline:
filtering, identification, harmonization proposal generation, automated
re-ranking, and manual validation. Performance was assessed using Mean
Reciprocal Rank (MRR) and other standard information retrieval metrics.
  Results: Our hybrid retrieval approach combining BM25 and sentence embeddings
(MRR: 0.8833) significantly outperformed both lexical-only (MRR: 0.7985) and
embedding-only (MRR: 0.5277) approaches. The transformer-based reranker further
improved performance (absolute MRR improvement: 0.10), bringing the final
system MRR to 0.9833. The system achieved 83.39\% precision at rank 1 and
94.66\% recall at rank 5.
  Discussion: The hybrid architecture effectively leverages the complementary
strengths of lexical and semantic approaches. The reranker addresses cases
where initial retrieval components make errors due to complex semantic
relationships in medical terminology.
  Conclusion: Our framework provides an efficient, scalable solution for unit
harmonization in clinical datasets, reducing manual effort while improving
accuracy. Once harmonized, data can be reused seamlessly in different analyses,
ensuring consistency across healthcare systems and enabling more reliable
multi-institutional studies and meta-analyses.

</details>


### [46] [Handling Label Noise via Instance-Level Difficulty Modeling and Dynamic Optimization](https://arxiv.org/abs/2505.00812)
*Kuan Zhang,Chengliang Chai,Jingzhe Xu,Chi Zhang,Ye Yuan,Guoren Wang,Lei Cao*

Main category: cs.LG

TL;DR: 提出了一种新的两阶段噪声学习框架，通过动态加权损失函数实现实例级优化，无需超参数调优，显著提升模型性能和计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理噪声监督时存在计算成本高、超参数调优复杂和粗粒度优化等问题，需要一种更高效且精细的解决方案。

Method: 采用动态加权损失函数和基于错误事件的简单度量，分两阶段进行噪声建模和鲁棒训练。

Result: 在五个合成和真实世界的LNL基准测试中，性能优于现有方法，计算时间减少75%，模型可扩展性提升。

Conclusion: 该框架通过实例级优化和动态噪声建模，有效解决了噪声监督下的泛化性能问题，具有高效和可扩展的优势。

Abstract: Recent studies indicate that deep neural networks degrade in generalization
performance under noisy supervision. Existing methods focus on isolating clean
subsets or correcting noisy labels, facing limitations such as high
computational costs, heavy hyperparameter tuning process, and coarse-grained
optimization. To address these challenges, we propose a novel two-stage noisy
learning framework that enables instance-level optimization through a
dynamically weighted loss function, avoiding hyperparameter tuning. To obtain
stable and accurate information about noise modeling, we introduce a simple yet
effective metric, termed wrong event, which dynamically models the cleanliness
and difficulty of individual samples while maintaining computational costs. Our
framework first collects wrong event information and builds a strong base
model. Then we perform noise-robust training on the base model, using a
probabilistic model to handle the wrong event information of samples.
Experiments on five synthetic and real-world LNL benchmarks demonstrate our
method surpasses state-of-the-art methods in performance, achieves a nearly 75%
reduction in computational time and improves model scalability.

</details>


### [47] [Dual Filter: A Mathematical Framework for Inference using Transformer-like Architectures](https://arxiv.org/abs/2505.00818)
*Heng-Sheng Chang,Prashant G. Mehta*

Main category: cs.LG

TL;DR: 本文提出了一种基于隐马尔可夫模型（HMM）的因果非线性预测数学框架，灵感来自解码器-仅Transformer架构，旨在从第一性原理推导类似Transformer的结构。


<details>
  <summary>Details</summary>
Motivation: 研究动机是从第一性原理出发，推导解决预测问题的Transformer-like架构，而非直接建模Transformer。

Method: 采用最优控制方法，将预测目标（MMSE）重新表述为最优控制问题，并通过双滤波器算法求解固定点方程。

Result: 提出的双滤波器算法在性能上与研究规模的Transformer模型参数表现相当。

Conclusion: 该框架为Transformer-like架构提供了理论支持，并展示了其在实际预测问题中的有效性。

Abstract: This paper presents a mathematical framework for causal nonlinear prediction
in settings where observations are generated from an underlying hidden Markov
model (HMM). Both the problem formulation and the proposed solution are
motivated by the decoder-only transformer architecture, in which a finite
sequence of observations (tokens) is mapped to the conditional probability of
the next token. Our objective is not to construct a mathematical model of a
transformer. Rather, our interest lies in deriving, from first principles,
transformer-like architectures that solve the prediction problem for which the
transformer is designed. The proposed framework is based on an original optimal
control approach, where the prediction objective (MMSE) is reformulated as an
optimal control problem. An analysis of the optimal control problem is
presented leading to a fixed-point equation on the space of probability
measures. To solve the fixed-point equation, we introduce the dual filter, an
iterative algorithm that closely parallels the architecture of decoder-only
transformers. These parallels are discussed in detail along with the
relationship to prior work on mathematical modeling of transformers as
transport on the space of probability measures. Numerical experiments are
provided to illustrate the performance of the algorithm using parameter values
used in researchscale transformer models.

</details>


### [48] [Data-Driven Optical To Thermal Inference in Pool Boiling Using Generative Adversarial Networks](https://arxiv.org/abs/2505.00823)
*Qianxi Fu,Youngjoon Suh,Xiaojing Zhang,Yoonjin Won*

Main category: cs.LG

TL;DR: 提出了一种基于条件生成对抗网络（CGAN）的数据驱动框架，用于从几何相轮廓推断温度场，填补了多相热传递定量表征的空白。


<details>
  <summary>Details</summary>
Motivation: 多相热传递的定量表征受限于混沌、快速演变的流动状态中温度场的测量挑战，计算模拟难以复现复杂实验条件。

Method: 利用CGAN从高速成像数据和模拟训练中推断温度场，并通过数据增强提升预测精度和物理合理性。

Result: 模型能够以低于6%的误差重建温度场，数据增强策略在模拟和实验数据中均有效。

Conclusion: 深度生成模型有望弥合可观测多相现象与潜在热传输之间的差距，为复杂两相系统的实验测量提供新方法。

Abstract: Phase change plays a critical role in thermal management systems, yet
quantitative characterization of multiphase heat transfer remains limited by
the challenges of measuring temperature fields in chaotic, rapidly evolving
flow regimes. While computational methods offer spatiotemporal resolution in
idealized cases, replicating complex experimental conditions remains
prohibitively difficult. Here, we present a data-driven framework that
leverages a conditional generative adversarial network (CGAN) to infer
temperature fields from geometric phase contours in a canonical pool boiling
configuration where advanced data collection techniques are restricted. Using
high-speed imaging data and simulation-informed training, our model
demonstrates the ability to reconstruct temperature fields with errors below
6%. We further show that standard data augmentation strategies are effective in
enhancing both accuracy and physical plausibility of the predicted maps across
both simulation and experimental datasets when precise physical constraints are
not applicable. Our results highlight the potential of deep generative models
to bridge the gap between observable multiphase phenomena and underlying
thermal transport, offering a powerful approach to augment and interpret
experimental measurements in complex two-phase systems.

</details>


### [49] [Intersectional Divergence: Measuring Fairness in Regression](https://arxiv.org/abs/2505.00830)
*Joe Germino,Nuno Moniz,Nitesh V. Chawla*

Main category: cs.LG

TL;DR: 本文提出了一种新的回归任务公平性度量方法Intersectional Divergence (ID)，考虑多保护属性的组合，并区分用户最关心的目标范围预测影响。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注分类任务的公平性，而回归任务中的公平性研究存在空白，尤其是多保护属性组合和用户偏好不平衡的问题。

Method: 提出Intersectional Divergence (ID)作为回归任务的公平性度量，并扩展为损失函数IDLoss用于优化问题。

Result: 实验表明，ID能提供独特的模型行为与公平性洞察，IDLoss显著提升单属性和多属性组合的公平性，同时保持预测性能。

Conclusion: ID和IDLoss为回归任务中的公平性研究提供了新视角和实用工具，填补了现有空白。

Abstract: Research on fairness in machine learning has been mainly framed in the
context of classification tasks, leaving critical gaps in regression. In this
paper, we propose a seminal approach to measure intersectional fairness in
regression tasks, going beyond the focus on single protected attributes from
existing work to consider combinations of all protected attributes.
Furthermore, we contend that it is insufficient to measure the average error of
groups without regard for imbalanced domain preferences. To this end, we
propose Intersectional Divergence (ID) as the first fairness measure for
regression tasks that 1) describes fair model behavior across multiple
protected attributes and 2) differentiates the impact of predictions in target
ranges most relevant to users. We extend our proposal demonstrating how ID can
be adapted into a loss function, IDLoss, and used in optimization problems.
Through an extensive experimental evaluation, we demonstrate how ID allows
unique insights into model behavior and fairness, and how incorporating IDLoss
into optimization can considerably improve single-attribute and intersectional
model fairness while maintaining a competitive balance in predictive
performance.

</details>


### [50] [IberFire -- a detailed creation of a spatio-temporal dataset for wildfire risk assessment in Spain](https://arxiv.org/abs/2505.00837)
*Julen Ercibengoa,Meritxell Gómez-Omella,Izaro Goienetxea*

Main category: cs.LG

TL;DR: 本文介绍了IberFire，一个高分辨率的时空数据集，用于西班牙野火预测，整合了260个特征，支持机器学习和深度学习建模。


<details>
  <summary>Details</summary>
Motivation: 解决西班牙缺乏本地化和细粒度野火数据的问题，以支持更准确的预测模型和战略规划。

Method: 开发IberFire数据集，整合多源开放数据，涵盖8类260个特征，并通过开源工具实现数据处理。

Result: IberFire提供了比现有欧洲数据集更高的时空粒度和特征多样性，支持野火风险建模和气候分析。

Conclusion: IberFire为野火研究和预防提供了高质量、可复现的数据集，并促进开放研究与合作。

Abstract: Wildfires pose a critical environmental issue to ecosystems, economies, and
public safety, particularly in Mediterranean regions such as Spain. Accurate
predictive models rely on high-resolution spatio-temporal data to capture the
complex interplay of environmental and anthropogenic factors. To address the
lack of localised and fine-grained datasets in Spain, this work introduces
IberFire, a spatio-temporal datacube at 1 km x 1 km x 1-day resolution covering
mainland Spain and the Balearic Islands from December 2007 to December 2024.
IberFire integrates 260 features across eight main categories: auxiliary
features, fire history, geography, topography, meteorology, vegetation indices,
human activity, and land cover. All features are derived from open-access
sources, ensuring transparency and real-time applicability. The data processing
pipeline was implemented entirely using open-source tools, and the codebase has
been made publicly available. This work not only enhances spatio-temporal
granularity and feature diversity compared to existing European datacubes but
also provides a reproducible methodology for constructing similar datasets.
IberFire supports advanced wildfire risk modelling through Machine Learning
(ML) and Deep Learning (DL) techniques, enables climate pattern analysis and
informs strategic planning in fire prevention and land management. The dataset
is publicly available on Zenodo to promote open research and collaboration.

</details>


### [51] [ICQuant: Index Coding enables Low-bit LLM Quantization](https://arxiv.org/abs/2505.00850)
*Xinlin Li,Osama Hanna,Christina Fragouli,Suhas Diggavi*

Main category: cs.LG

TL;DR: ICQuant是一种新颖的低比特后训练量化框架，通过高效的索引编码方案处理权重中的异常值，显著减少量化误差和比特开销。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的高内存成本促使需要高效的低比特后训练量化（PTQ），而权重量化中的异常值问题导致量化范围扩大和误差增加。

Method: ICQuant利用异常值统计设计高效的索引编码方案，用于异常值感知的仅权重量化，显著降低比特开销。

Result: ICQuant仅需约0.3比特开销即可将量化范围减半，显著优于现有技术。在2.3比特/权重下，ICQuant将2-bit Llama3-70B模型的零样本准确率提升130%-150%。

Conclusion: ICQuant在不进行微调的情况下，性能与最佳微调量化器相当，是一种高效的异常值处理方案。

Abstract: The rapid deployment of Large Language Models (LLMs) highlights the need for
efficient low-bit post-training quantization (PTQ), due to their high memory
costs. A key challenge in weight quantization is the presence of outliers,
which inflate quantization ranges and lead to large errors. While a number of
outlier suppression techniques have been proposed, they either: fail to
effectively shrink the quantization range, or incur (relatively) high bit
overhead. In this paper, we present ICQuant, a novel framework that leverages
outlier statistics to design an efficient index coding scheme for outlier-aware
weight-only quantization. Compared to existing outlier suppression techniques
requiring $\approx 1$ bit overhead to halve the quantization range, ICQuant
requires only $\approx 0.3$ bits; a significant saving in extreme compression
regimes (e.g., 2-3 bits per weight). ICQuant can be used on top of any existing
quantizers to eliminate outliers, improving the quantization quality. Using
just 2.3 bits per weight and simple scalar quantizers, ICQuant improves the
zero-shot accuracy of the 2-bit Llama3-70B model by up to 130% and 150%
relative to QTIP and QuIP#; and it achieves comparable performance to the
best-known fine-tuned quantizer (PV-tuning) without fine-tuning.

</details>


### [52] [Rethinking Time Encoding via Learnable Transformation Functions](https://arxiv.org/abs/2505.00887)
*Xi Chen,Yateng Tang,Jiarong Xu,Jiawei Zhang,Siwei Zhang,Sijia Peng,Xuehao Zheng,Yun Xiong*

Main category: cs.LG

TL;DR: 论文提出了一种可学习的广义时间编码方法LeTE，通过深度函数学习技术参数化非线性变换，以处理复杂多样的时间模式。


<details>
  <summary>Details</summary>
Motivation: 现实场景中的时间模式多样且复杂，现有方法多依赖特定归纳偏置（如三角函数建模周期性），难以有效处理多样性。

Method: 提出LeTE方法，利用深度函数学习技术参数化非线性变换，使其可学习并能建模广义时间模式。

Result: 通过多领域实验验证了LeTE的通用性和有效性。

Conclusion: LeTE能够涵盖现有方法并适用于广泛任务，为复杂时间模式建模提供了新思路。

Abstract: Effectively modeling time information and incorporating it into applications
or models involving chronologically occurring events is crucial. Real-world
scenarios often involve diverse and complex time patterns, which pose
significant challenges for time encoding methods. While previous methods focus
on capturing time patterns, many rely on specific inductive biases, such as
using trigonometric functions to model periodicity. This narrow focus on
single-pattern modeling makes them less effective in handling the diversity and
complexities of real-world time patterns. In this paper, we investigate to
improve the existing commonly used time encoding methods and introduce
Learnable Transformation-based Generalized Time Encoding (LeTE). We propose
using deep function learning techniques to parameterize non-linear
transformations in time encoding, making them learnable and capable of modeling
generalized time patterns, including diverse and complex temporal dynamics. By
enabling learnable transformations, LeTE encompasses previous methods as
specific cases and allows seamless integration into a wide range of tasks.
Through extensive experiments across diverse domains, we demonstrate the
versatility and effectiveness of LeTE.

</details>


### [53] [NeMo-Inspector: A Visualization Tool for LLM Generation Analysis](https://arxiv.org/abs/2505.00903)
*Daria Gitman,Igor Gitman,Evelina Bakhturina*

Main category: cs.LG

TL;DR: NeMo-Inspector是一个开源工具，用于简化合成数据集的分析和清理，显著提升了数据质量和模型性能。


<details>
  <summary>Details</summary>
Motivation: 合成数据在缺乏真实数据时是重要替代品，但其质量难以保证，手动检查耗时且复杂。

Method: 开发了NeMo-Inspector工具，集成推理能力，用于分析和清理合成数据集。

Result: 使用该工具后，GSM-Plus数据集的低质量样本从46.99%降至19.51%，OpenMath模型的准确性在MATH和GSM8K数据集上分别提高了1.92%和4.17%。

Conclusion: NeMo-Inspector有效提升了合成数据的质量，为LLM的优化提供了实用工具。

Abstract: Adapting Large Language Models (LLMs) to novel tasks and enhancing their
overall capabilities often requires large, high-quality training datasets.
Synthetic data, generated at scale, serves a valuable alternative when
real-world data is scarce or difficult to obtain. However, ensuring the quality
of synthetic datasets is challenging, as developers must manually inspect and
refine numerous samples to identify errors and areas for improvement. This
process is time-consuming and requires specialized tools. We introduce
NeMo-Inspector, an open-source tool designed to simplify the analysis of
synthetic datasets with integrated inference capabilities. We demonstrate its
effectiveness through two real-world cases. Analysis and cleaning of the
synthetically generated GSM-Plus dataset with NeMo-Inspector led to a
significant decrease in low-quality samples from 46.99% to 19.51%. The tool
also helped identify and correct generation errors in OpenMath models,
improving accuracy by 1.92% on the MATH dataset and by 4.17% on the GSM8K
dataset for a Meta-Llama-3-8B model fine-tuned on synthetic data generated from
Nemotron-4-340B.

</details>


### [54] [Learning Neural Control Barrier Functions from Offline Data with Conservatism](https://arxiv.org/abs/2505.00908)
*Ihab Tabbara,Hussein Sibai*

Main category: cs.LG

TL;DR: 提出一种基于离线数据集训练控制屏障函数的算法，用于提升安全性和避免分布外状态，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于控制屏障函数的安全过滤器在高维系统中存在维度灾难问题，深度学习虽被提出但仍需改进。

Method: 受保守Q学习启发，提出一种离线训练算法，生成保守控制屏障函数（CCBFs），同时避免不安全状态和分布外状态。

Result: 实验表明CCBFs在安全性和分布外避免方面优于现有方法，且对任务性能影响最小。

Conclusion: CCBFs是一种有效的安全控制工具，适用于高维系统。

Abstract: Safety filters, particularly those based on control barrier functions, have
gained increased interest as effective tools for safe control of dynamical
systems. Existing correct-by-construction synthesis algorithms, however, suffer
from the curse of dimensionality. Deep learning approaches have been proposed
in recent years to address this challenge. In this paper, we contribute to this
line of work by proposing an algorithm for training control barrier functions
from offline datasets. Our algorithm trains the filter to not only prevent the
system from reaching unsafe states but also out-of-distribution ones, at which
the filter would be unreliable. It is inspired by Conservative Q-learning, an
offline reinforcement learning algorithm. We call its outputs Conservative
Control Barrier Functions (CCBFs). Our empirical results demonstrate that CCBFs
outperform existing methods in maintaining safety and out-of-distribution
avoidance while minimally affecting task performance.

</details>


### [55] [Gaussian Process Policy Iteration with Additive Schwarz Acceleration for Forward and Inverse HJB and Mean Field Game Problems](https://arxiv.org/abs/2505.00909)
*Xianjin Yang,Jingguo Zhang*

Main category: cs.LG

TL;DR: 提出了一种基于高斯过程的策略迭代框架，用于解决HJB方程和MFG中的正向和逆向问题，通过Schwarz加速提高计算效率。


<details>
  <summary>Details</summary>
Motivation: 解决HJB方程和MFG中的正向和逆向问题，提高计算效率和收敛性。

Method: 采用高斯过程进行函数逼近，策略迭代分为固定策略下的值函数求解和基于值函数的策略更新，引入Schwarz加速作为预处理步骤。

Result: 数值实验表明Schwarz加速显著提高了计算效率。

Conclusion: 该框架有效解决了HJB方程和MFG问题，并通过Schwarz加速优化了计算性能。

Abstract: We propose a Gaussian Process (GP)-based policy iteration framework for
addressing both forward and inverse problems in Hamilton--Jacobi--Bellman (HJB)
equations and mean field games (MFGs). Policy iteration is formulated as an
alternating procedure between solving the value function under a fixed control
policy and updating the policy based on the resulting value function. By
exploiting the linear structure of GPs for function approximation, each policy
evaluation step admits an explicit closed-form solution, eliminating the need
for numerical optimization. To improve convergence, we incorporate the additive
Schwarz acceleration as a preconditioning step following each policy update.
Numerical experiments demonstrate the effectiveness of Schwarz acceleration in
improving computational efficiency.

</details>


### [56] [Fine-Tuning without Performance Degradation](https://arxiv.org/abs/2505.00913)
*Han Wang,Adam White,Martha White*

Main category: cs.LG

TL;DR: 论文提出了一种新的微调算法Jump Start，通过逐步增加探索来减少性能下降，并实现快速微调。


<details>
  <summary>Details</summary>
Motivation: 离线学习的策略在在线微调时通常会出现性能下降或学习缓慢的问题，现有方法主要关注提高学习效率，但忽视了性能下降的代价。

Method: 基于Jump Start算法，新方法通过在线性能评估逐步增加探索。

Result: 实验表明，该方法显著减少了性能下降，并实现了比现有算法更快的微调。

Conclusion: 新算法在减少性能下降和加速微调方面优于现有方法。

Abstract: Fine-tuning policies learned offline remains a major challenge in application
domains. Monotonic performance improvement during \emph{fine-tuning} is often
challenging, as agents typically experience performance degradation at the
early fine-tuning stage. The community has identified multiple difficulties in
fine-tuning a learned network online, however, the majority of progress has
focused on improving learning efficiency during fine-tuning. In practice, this
comes at a serious cost during fine-tuning: initially, agent performance
degrades as the agent explores and effectively overrides the policy learned
offline. We show across a range of settings, many offline-to-online algorithms
exhibit either (1) performance degradation or (2) slow learning (sometimes
effectively no improvement) during fine-tuning. We introduce a new fine-tuning
algorithm, based on an algorithm called Jump Start, that gradually allows more
exploration based on online estimates of performance. Empirically, this
approach achieves fast fine-tuning and significantly reduces performance
degradations compared with existing algorithms designed to do the same.

</details>


### [57] [How Transformers Learn Regular Language Recognition: A Theoretical Study on Training Dynamics and Implicit Bias](https://arxiv.org/abs/2505.00926)
*Ruiquan Huang,Yingbin Liang,Jing Yang*

Main category: cs.LG

TL;DR: 论文研究了单层Transformer在解决‘even pairs’和‘parity check’任务时的训练动态，发现其学习过程分为两个阶段，并通过实验验证了理论结果。


<details>
  <summary>Details</summary>
Motivation: 探索单层Transformer如何通过梯度下降学习解决正则语言识别任务，以揭示其工作机制。

Method: 理论分析单层Transformer（注意力层加线性层）在梯度下降下的训练动态，并实验验证。

Result: 训练分为两个阶段：注意力层快速映射数据为可分向量，随后线性层对数增长并接近最大间隔超平面，损失以O(1/t)速率下降。

Conclusion: 单层Transformer能直接解决‘even pairs’，而‘parity check’需结合CoT；训练动态的理论分析得到实验支持。

Abstract: Language recognition tasks are fundamental in natural language processing
(NLP) and have been widely used to benchmark the performance of large language
models (LLMs). These tasks also play a crucial role in explaining the working
mechanisms of transformers. In this work, we focus on two representative tasks
in the category of regular language recognition, known as `even pairs' and
`parity check', the aim of which is to determine whether the occurrences of
certain subsequences in a given sequence are even. Our goal is to explore how a
one-layer transformer, consisting of an attention layer followed by a linear
layer, learns to solve these tasks by theoretically analyzing its training
dynamics under gradient descent. While even pairs can be solved directly by a
one-layer transformer, parity check need to be solved by integrating
Chain-of-Thought (CoT), either into the inference stage of a transformer
well-trained for the even pairs task, or into the training of a one-layer
transformer. For both problems, our analysis shows that the joint training of
attention and linear layers exhibits two distinct phases. In the first phase,
the attention layer grows rapidly, mapping data sequences into separable
vectors. In the second phase, the attention layer becomes stable, while the
linear layer grows logarithmically and approaches in direction to a max-margin
hyperplane that correctly separates the attention layer outputs into positive
and negative samples, and the loss decreases at a rate of $O(1/t)$. Our
experiments validate those theoretical results.

</details>


### [58] [Compact Recurrent Transformer with Persistent Memory](https://arxiv.org/abs/2505.00929)
*Edison Mucllari,Zachary Daniels,David Zhang,Qiang Ye*

Main category: cs.LG

TL;DR: 提出了一种高效紧凑的循环Transformer（CRT），结合浅层Transformer和RNN，解决了长序列处理中的计算效率问题，并在语言和视觉任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: Transformer在处理长序列时因自注意力计算的二次复杂度而效率低下，现有方法虽能分段处理但引入额外计算开销，限制了在边缘计算等资源受限场景的应用。

Method: CRT结合浅层Transformer处理短片段和RNN压缩全局信息，通过单一持久记忆向量管理长距离依赖。

Result: 在语言数据集（WordPTB和WikiText-103）上，CRT使用更短片段和更低计算量取得与完整Transformer相当或更好的结果；在Toyota Smarthome视频数据集上达到SOTA性能。

Conclusion: CRT是一种高效的长序列处理方法，适用于资源受限场景，且在语言和视觉任务中均表现优异。

Abstract: The Transformer architecture has shown significant success in many language
processing and visual tasks. However, the method faces challenges in
efficiently scaling to long sequences because the self-attention computation is
quadratic with respect to the input length. To overcome this limitation,
several approaches scale to longer sequences by breaking long sequences into a
series of segments, restricting self-attention to local dependencies between
tokens within each segment and using a memory mechanism to manage information
flow between segments. However, these approached generally introduce additional
compute overhead that restricts them from being used for applications where
limited compute memory and power are of great concern (such as edge computing).
We propose a novel and efficient Compact Recurrent Transformer (CRT), which
combines shallow Transformer models that process short local segments with
recurrent neural networks to compress and manage a single persistent memory
vector that summarizes long-range global information between segments. We
evaluate CRT on WordPTB and WikiText-103 for next-token-prediction tasks, as
well as on the Toyota Smarthome video dataset for classification. CRT achieves
comparable or superior prediction results to full-length Transformers in the
language datasets while using significantly shorter segments (half or quarter
size) and substantially reduced FLOPs. Our approach also demonstrates
state-of-the-art performance on the Toyota Smarthome video dataset.

</details>


### [59] [Robust Root Cause Diagnosis using In-Distribution Interventions](https://arxiv.org/abs/2505.00930)
*Lokesh Nagalapatti,Ashutosh Srivastava,Sunita Sarawagi,Amit Sharma*

Main category: cs.LG

TL;DR: IDI是一种新算法，通过满足异常和修复两个条件预测复杂系统中的根因节点，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决复杂互联系统中异常根因诊断的不可靠性问题，尤其是在异常数据稀少时。

Method: 提出IDI算法，利用干预性估计而非反事实估计，避免因异常数据稀少导致的不可靠性。

Result: 在合成和PetShop RCD基准数据集上，IDI比九种现有方法更准确、稳健地识别根因。

Conclusion: IDI通过干预性估计在根因诊断中表现优越，尤其在异常数据稀少时。

Abstract: Diagnosing the root cause of an anomaly in a complex interconnected system is
a pressing problem in today's cloud services and industrial operations. We
propose In-Distribution Interventions (IDI), a novel algorithm that predicts
root cause as nodes that meet two criteria: 1) **Anomaly:** root cause nodes
should take on anomalous values; 2) **Fix:** had the root cause nodes assumed
usual values, the target node would not have been anomalous. Prior methods of
assessing the fix condition rely on counterfactuals inferred from a Structural
Causal Model (SCM) trained on historical data. But since anomalies are rare and
fall outside the training distribution, the fitted SCMs yield unreliable
counterfactual estimates. IDI overcomes this by relying on interventional
estimates obtained by solely probing the fitted SCM at in-distribution inputs.
We present a theoretical analysis comparing and bounding the errors in
assessing the fix condition using interventional and counterfactual estimates.
We then conduct experiments by systematically varying the SCM's complexity to
demonstrate the cases where IDI's interventional approach outperforms the
counterfactual approach and vice versa. Experiments on both synthetic and
PetShop RCD benchmark datasets demonstrate that \our\ consistently identifies
true root causes more accurately and robustly than nine existing
state-of-the-art RCD baselines. Code is released at
https://github.com/nlokeshiisc/IDI_release.

</details>


### [60] [A Self-Supervised Transformer for Unusable Shared Bike Detection](https://arxiv.org/abs/2505.00932)
*Yin Huang,Yongqi Dong,Youhua Tang,Alvaro García Hernandez*

Main category: cs.LG

TL;DR: 论文提出了一种自监督Transformer框架（SSTransformer），用于自动检测共享单车中的故障车辆，通过结合时空特征和自监督预训练策略，显著提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 共享单车系统（BSS）的快速扩张带来了运营挑战，尤其是故障车辆的检测问题。现有方法因忽略动态时空模式或标签稀缺而效果不佳。

Method: 提出SSTransformer框架，利用GPS轨迹和行程记录提取时空特征，通过自监督预训练增强特征提取能力，再微调进行状态识别。

Result: 在成都的真实数据集上，SSTransformer在准确率（97.81%）、精确率（0.8889）和F1分数（0.9358）上显著优于传统方法。

Conclusion: 自监督Transformer在时空数据上能有效捕捉复杂异常，为共享出行提供更可靠和可扩展的维护解决方案。

Abstract: The rapid expansion of bike-sharing systems (BSS) has greatly improved urban
"last-mile" connectivity, yet large-scale deployments face escalating
operational challenges, particularly in detecting faulty bikes. Existing
detection approaches either rely on static model-based thresholds that overlook
dynamic spatiotemporal (ST) usage patterns or employ supervised learning
methods that struggle with label scarcity and class imbalance. To address these
limitations, this paper proposes a novel Self-Supervised Transformer
(SSTransformer) framework for automatically detecting unusable shared bikes,
leveraging ST features extracted from GPS trajectories and trip records. The
model incorporates a self-supervised pre-training strategy to enhance its
feature extraction capabilities, followed by fine-tuning for efficient status
recognition. In the pre-training phase, the Transformer encoder learns
generalized representations of bike movement via a self-supervised objective;
in the fine-tuning phase, the encoder is adapted to a downstream binary
classification task. Comprehensive experiments on a real-world dataset of
10,730 bikes (1,870 unusable, 8,860 normal) from Chengdu, China, demonstrate
that SSTransformer significantly outperforms traditional machine learning,
ensemble learning, and deep learning baselines, achieving the best accuracy
(97.81%), precision (0.8889), and F1-score (0.9358). This work highlights the
effectiveness of self-supervised Transformer on ST data for capturing complex
anomalies in BSS, paving the way toward more reliable and scalable maintenance
solutions for shared mobility.

</details>


### [61] [TunnElQNN: A Hybrid Quantum-classical Neural Network for Efficient Learning](https://arxiv.org/abs/2505.00933)
*A. H. Abbas*

Main category: cs.LG

TL;DR: 论文提出了一种名为TunnElQNN的混合量子-经典神经网络架构，采用交替的经典和量子层，并引入基于量子隧穿特性的TDAF激活函数，在多类分类任务中表现优于传统ReLUQNN。


<details>
  <summary>Details</summary>
Motivation: 结合量子与经典模型的优势，探索物理启发的激活函数在混合架构中的潜力，以提升机器学习的表达能力和鲁棒性。

Method: 提出非顺序架构TunnElQNN，使用TDAF激活函数，并在合成数据集上评估其性能，与ReLUQNN和纯经典架构对比。

Result: TunnElQNN在多类分类任务中表现优于ReLUQNN，且在不同类别重叠程度下展现出更强的决策边界生成能力。

Conclusion: 物理启发的激活函数与量子组件的结合能显著增强混合量子-经典机器学习架构的性能和鲁棒性。

Abstract: Hybrid quantum-classical neural networks (HQCNNs) represent a promising
frontier in machine learning, leveraging the complementary strengths of both
models. In this work, we propose the development of TunnElQNN, a non-sequential
architecture composed of alternating classical and quantum layers. Within the
classical component, we employ the Tunnelling Diode Activation Function (TDAF),
inspired by the I-V characteristics of quantum tunnelling. We evaluate the
performance of this hybrid model on a synthetic dataset of interleaving
half-circle for multi-class classification tasks with varying degrees of class
overlap. The model is compared against a baseline hybrid architecture that uses
the conventional ReLU activation function (ReLUQNN). Our results show that the
TunnElQNN model consistently outperforms the ReLUQNN counterpart. Furthermore,
we analyse the decision boundaries generated by TunnElQNN under different
levels of class overlap and compare them to those produced by a neural network
implementing TDAF within a fully classical architecture. These findings
highlight the potential of integrating physics-inspired activation functions
with quantum components to enhance the expressiveness and robustness of hybrid
quantum-classical machine learning architectures.

</details>


### [62] [StablePCA: Learning Shared Representations across Multiple Sources via Minimax Optimization](https://arxiv.org/abs/2505.00940)
*Zhenyu Wang,Molei Liu,Jing Lei,Francis Bach,Zijian Guo*

Main category: cs.LG

TL;DR: 提出了一种名为StablePCA的新方法，用于从多源高维数据中学习稳健的低维表示，解决了传统PCA在多源场景下的非凸优化问题。


<details>
  <summary>Details</summary>
Motivation: 提取多源高维数据的低维特征表示，以消除系统偏差（如批次效应）并促进公平性。

Method: 采用Fantope松弛将非凸问题转化为凸优化问题，并使用乐观梯度Mirror Prox算法求解。

Result: 理论证明了算法的全局收敛性，实验表明StablePCA在提取稳健低维表示方面具有高准确性和效率。

Conclusion: StablePCA是一种有效的多源数据特征提取方法，适用于各种有限样本场景。

Abstract: When synthesizing multisource high-dimensional data, a key objective is to
extract low-dimensional feature representations that effectively approximate
the original features across different sources. Such general feature extraction
facilitates the discovery of transferable knowledge, mitigates systematic
biases such as batch effects, and promotes fairness. In this paper, we propose
Stable Principal Component Analysis (StablePCA), a novel method for group
distributionally robust learning of latent representations from
high-dimensional multi-source data. A primary challenge in generalizing PCA to
the multi-source regime lies in the nonconvexity of the fixed rank constraint,
rendering the minimax optimization nonconvex. To address this challenge, we
employ the Fantope relaxation, reformulating the problem as a convex minimax
optimization, with the objective defined as the maximum loss across sources. To
solve the relaxed formulation, we devise an optimistic-gradient Mirror Prox
algorithm with explicit closed-form updates. Theoretically, we establish the
global convergence of the Mirror Prox algorithm, with the convergence rate
provided from the optimization perspective. Furthermore, we offer practical
criteria to assess how closely the solution approximates the original nonconvex
formulation. Through extensive numerical experiments, we demonstrate
StablePCA's high accuracy and efficiency in extracting robust low-dimensional
representations across various finite-sample scenarios.

</details>


### [63] [FreCT: Frequency-augmented Convolutional Transformer for Robust Time Series Anomaly Detection](https://arxiv.org/abs/2505.00941)
*Wenxin Zhang,Ding Xu,Guangzhen Yao,Xiaojian Lin,Renxiang Guan,Chengze Du,Renda Han,Xi Xuan,Cuicui Luo*

Main category: cs.LG

TL;DR: 论文提出了一种基于频率增强的卷积变换器（FreCT），用于解决时间序列异常检测中重建方法的局限性，通过结合时间域和频率域信息提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 时间序列异常检测在多个领域至关重要，但现有重建方法因复杂序列模式和忽略频率信息而表现不佳。

Method: FreCT采用补丁操作生成对比视图，结合改进的Transformer和卷积模块捕获长期依赖和局部拓扑信息，并利用傅里叶变换增强频率分析。

Result: 在四个公开数据集上的实验表明，FreCT在异常检测上优于现有方法。

Conclusion: FreCT通过结合时间和频率域信息，显著提升了异常检测的鲁棒性和准确性。

Abstract: Time series anomaly detection is critical for system monitoring and risk
identification, across various domains, such as finance and healthcare.
However, for most reconstruction-based approaches, detecting anomalies remains
a challenge due to the complexity of sequential patterns in time series data.
On the one hand, reconstruction-based techniques are susceptible to
computational deviation stemming from anomalies, which can lead to impure
representations of normal sequence patterns. On the other hand, they often
focus on the time-domain dependencies of time series, while ignoring the
alignment of frequency information beyond the time domain. To address these
challenges, we propose a novel Frequency-augmented Convolutional Transformer
(FreCT). FreCT utilizes patch operations to generate contrastive views and
employs an improved Transformer architecture integrated with a convolution
module to capture long-term dependencies while preserving local topology
information. The introduced frequency analysis based on Fourier transformation
could enhance the model's ability to capture crucial characteristics beyond the
time domain. To protect the training quality from anomalies and improve the
robustness, FreCT deploys stop-gradient Kullback-Leibler (KL) divergence and
absolute error to optimize consistency information in both time and frequency
domains. Extensive experiments on four public datasets demonstrate that FreCT
outperforms existing methods in identifying anomalies.

</details>


### [64] [Addressing Noise and Stochasticity in Fraud Detection for Service Networks](https://arxiv.org/abs/2505.00946)
*Wenxin Zhang,Ding Xu,Xi Xuan,Lei Jiang,Guangzhen Yao,Renda Han,Xiangxiang Lang,Cuicui Luo*

Main category: cs.LG

TL;DR: 提出了一种基于信息瓶颈理论的新型谱图网络（SGNN-IB），用于服务网络中的欺诈检测，解决了现有图滤波方法在信号提取和融合中的不足。


<details>
  <summary>Details</summary>
Motivation: 现有谱图方法在欺诈检测中难以提取干净且具有区分性的图信号，忽略了信息传播中的噪声，且无法区分信号的频率特性。

Method: SGNN-IB将原始图分为同质和异质子图以捕获不同频率信号，利用信息瓶颈理论提取关键特征，并通过原型学习实现信号融合。

Result: 在三个真实数据集上的实验表明，SGNN-IB优于现有欺诈检测方法。

Conclusion: SGNN-IB通过改进信号提取和融合机制，显著提升了欺诈检测性能。

Abstract: Fraud detection is crucial in social service networks to maintain user trust
and improve service network security. Existing spectral graph-based methods
address this challenge by leveraging different graph filters to capture signals
with different frequencies in service networks. However, most graph
filter-based methods struggle with deriving clean and discriminative graph
signals. On the one hand, they overlook the noise in the information
propagation process, resulting in degradation of filtering ability. On the
other hand, they fail to discriminate the frequency-specific characteristics of
graph signals, leading to distortion of signals fusion. To address these
issues, we develop a novel spectral graph network based on information
bottleneck theory (SGNN-IB) for fraud detection in service networks. SGNN-IB
splits the original graph into homophilic and heterophilic subgraphs to better
capture the signals at different frequencies. For the first limitation, SGNN-IB
applies information bottleneck theory to extract key characteristics of encoded
representations. For the second limitation, SGNN-IB introduces prototype
learning to implement signal fusion, preserving the frequency-specific
characteristics of signals. Extensive experiments on three real-world datasets
demonstrate that SGNN-IB outperforms state-of-the-art fraud detection methods.

</details>


### [65] [Adaptive Branch-and-Bound Tree Exploration for Neural Network Verification](https://arxiv.org/abs/2505.00963)
*Kota Fukuda,Guanqin Zhang,Zhenya Zhang,Yulei Sui,Jianjun Zhao*

Main category: cs.LG

TL;DR: ABONN是一种基于蒙特卡洛树搜索（MCTS）的自适应分支定界方法，通过动态评估子问题的重要性提升神经网络形式化验证的效率。


<details>
  <summary>Details</summary>
Motivation: 现有分支定界（BaB）方法在验证神经网络时效率不足，因其未考虑子问题的重要性差异。

Method: 提出子问题“重要性”概念，并设计ABONN方法，以MCTS风格自适应探索子问题空间，优先处理可能找到反例的子问题。

Result: 在MNIST和CIFAR-10数据集上，ABONN分别实现了15.2倍和24.7倍的加速。

Conclusion: ABONN通过自适应探索显著提升了验证效率，同时保持验证能力。

Abstract: Formal verification is a rigorous approach that can provably ensure the
quality of neural networks, and to date, Branch and Bound (BaB) is the
state-of-the-art that performs verification by splitting the problem as needed
and applying off-the-shelf verifiers to sub-problems for improved performance.
However, existing BaB may not be efficient, due to its naive way of exploring
the space of sub-problems that ignores the \emph{importance} of different
sub-problems. To bridge this gap, we first introduce a notion of ``importance''
that reflects how likely a counterexample can be found with a sub-problem, and
then we devise a novel verification approach, called ABONN, that explores the
sub-problem space of BaB adaptively, in a Monte-Carlo tree search (MCTS) style.
The exploration is guided by the ``importance'' of different sub-problems, so
it favors the sub-problems that are more likely to find counterexamples. As
soon as it finds a counterexample, it can immediately terminate; even though it
cannot find, after visiting all the sub-problems, it can still manage to verify
the problem. We evaluate ABONN with 552 verification problems from
commonly-used datasets and neural network models, and compare it with the
state-of-the-art verifiers as baseline approaches. Experimental evaluation
shows that ABONN demonstrates speedups of up to $15.2\times$ on MNIST and
$24.7\times$ on CIFAR-10. We further study the influences of hyperparameters to
the performance of ABONN, and the effectiveness of our adaptive tree
exploration.

</details>


### [66] [Tree-Sliced Wasserstein Distance with Nonlinear Projection](https://arxiv.org/abs/2505.00968)
*Thanh Tran,Viet-Hoang Tran,Thanh Chu,Trang Pham,Laurent El Ghaoui,Tam Le,Tan M. Nguyen*

Main category: cs.LG

TL;DR: 本文提出了一种基于树切片Wasserstein距离（TSW）的非线性投影框架，通过替换线性投影为一般投影，增强了捕捉拓扑结构的能力，并在欧几里得空间和球体上构建了高效度量。实验验证了该方法在梯度流、自监督学习和生成模型中的优越性。


<details>
  <summary>Details</summary>
Motivation: 传统切片Wasserstein（SW）距离使用一维线作为投影空间，而树切片方法通过树结构替代线性投影，能更好地捕捉拓扑结构。本文进一步提出非线性投影框架，以提升TSW的灵活性和适用性。

Method: 提出了一种非线性投影框架，替换了TSW中的线性投影，确保Radon变换的单射性，并保留了度量的良好定义性。设计了适用于欧几里得空间和球体的投影。

Result: 通过数值实验验证了所提方法在欧几里得和球体数据集上的有效性，并在梯度流、自监督学习和生成模型中表现出优于现有SW和TSW变体的性能。

Conclusion: 非线性投影框架显著提升了TSW的灵活性和性能，为复杂数据集的分析提供了高效工具。

Abstract: Tree-Sliced methods have recently emerged as an alternative to the
traditional Sliced Wasserstein (SW) distance, replacing one-dimensional lines
with tree-based metric spaces and incorporating a splitting mechanism for
projecting measures. This approach enhances the ability to capture the
topological structures of integration domains in Sliced Optimal Transport while
maintaining low computational costs. Building on this foundation, we propose a
novel nonlinear projectional framework for the Tree-Sliced Wasserstein (TSW)
distance, substituting the linear projections in earlier versions with general
projections, while ensuring the injectivity of the associated Radon Transform
and preserving the well-definedness of the resulting metric. By designing
appropriate projections, we construct efficient metrics for measures on both
Euclidean spaces and spheres. Finally, we validate our proposed metric through
extensive numerical experiments for Euclidean and spherical datasets.
Applications include gradient flows, self-supervised learning, and generative
models, where our methods demonstrate significant improvements over recent SW
and TSW variants.

</details>


### [67] [A Minimax-MDP Framework with Future-imposed Conditions for Learning-augmented Problems](https://arxiv.org/abs/2505.00973)
*Xin Chen,Yuze Chen,Yuan Zhou*

Main category: cs.LG

TL;DR: 本文提出了一种基于增强预测的序列决策问题框架（minimax-MDP），通过逐步优化的预测区间设计高效且鲁棒的策略，并在多个应用中验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 研究如何在预测不确定性下进行鲁棒的在线决策，利用逐步优化的预测区间提升决策的竞争力。

Method: 提出minimax-MDP框架，结合对抗性环境状态和决策者内部状态，通过未来约束条件设计高效策略。

Result: 在库存管理、资源分配等应用中验证了框架的有效性，提供了鲁棒且可扩展的决策方法。

Conclusion: minimax-MDP为预测不确定性下的在线决策提供了实用且高效的解决方案。

Abstract: We study a class of sequential decision-making problems with augmented
predictions, potentially provided by a machine learning algorithm. In this
setting, the decision-maker receives prediction intervals for unknown
parameters that become progressively refined over time, and seeks decisions
that are competitive with the hindsight optimal under all possible realizations
of both parameters and predictions. We propose a minimax Markov Decision
Process (minimax-MDP) framework, where the system state consists of an
adversarially evolving environment state and an internal state controlled by
the decision-maker. We introduce a set of future-imposed conditions that
characterize the feasibility of minimax-MDPs and enable the design of
efficient, often closed-form, robustly competitive policies. We illustrate the
framework through three applications: multi-period inventory ordering with
refining demand predictions, resource allocation with uncertain utility
functions, and a multi-phase extension of the minimax-MDP applied to the
inventory problem with time-varying ordering costs. Our results provide a
tractable and versatile approach to robust online decision-making under
predictive uncertainty.

</details>


### [68] [Towards the Resistance of Neural Network Watermarking to Fine-tuning](https://arxiv.org/abs/2505.01007)
*Ling Tang,Yuefeng Chen,Hui Xue,Quanshi Zhang*

Main category: cs.LG

TL;DR: 提出了一种新的水印方法，将所有权信息嵌入深度神经网络（DNN），并对微调具有鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 为了保护DNN模型的所有权，防止未经授权的微调。

Method: 通过修正的傅里叶变换提取卷积滤波器的特定频率成分，设计水印模块将信息编码到这些成分中。

Result: 实验证明该方法有效，且对权重缩放和排列具有等变性。

Conclusion: 该方法为DNN模型的所有权保护提供了一种鲁棒的水印解决方案。

Abstract: This paper proves a new watermarking method to embed the ownership
information into a deep neural network (DNN), which is robust to fine-tuning.
Specifically, we prove that when the input feature of a convolutional layer
only contains low-frequency components, specific frequency components of the
convolutional filter will not be changed by gradient descent during the
fine-tuning process, where we propose a revised Fourier transform to extract
frequency components from the convolutional filter. Additionally, we also prove
that these frequency components are equivariant to weight scaling and weight
permutations. In this way, we design a watermark module to encode the watermark
information to specific frequency components in a convolutional filter.
Preliminary experiments demonstrate the effectiveness of our method.

</details>


### [69] [Accelerating Deep Neural Network Training via Distributed Hybrid Order Optimization](https://arxiv.org/abs/2505.00982)
*Shunxian Gu,Chaoqun You,Bangbang Ren,Lailong Luo,Junxu Xia,Deke Guo*

Main category: cs.LG

TL;DR: FOSI是一种混合顺序优化器，通过结合梯度和曲率信息加速DNN训练。本文提出其分布式设计DHO$_2$，通过分布式计算曲率信息和部分更新模型，降低内存负担并加速训练。实验显示，DHO$_2$在设备数量增加时内存负担近似线性减少，且训练时间比其他分布式设计快1.4×∼2.1×。


<details>
  <summary>Details</summary>
Motivation: 在计算资源有限的情况下，传统分布式DNN训练方法难以高效运行。FOSI作为混合优化器，结合梯度和曲率信息，为资源受限环境提供了加速训练的新机会。

Method: 提出分布式设计DHO$_2$，包括分布式计算曲率信息、基于部分曲率信息的模型更新，以及并行化曲率计算和模型更新的策略。

Result: 实验表明，DHO$_2$在设备数量增加时内存负担近似线性减少，训练时间比其他分布式设计快1.4×∼2.1×。

Conclusion: DHO$_2$为资源受限环境下的DNN训练提供了高效的分布式解决方案，显著降低了内存负担并加速了训练过程。

Abstract: Scaling deep neural network (DNN) training to more devices can reduce
time-to-solution. However, it is impractical for users with limited computing
resources. FOSI, as a hybrid order optimizer, converges faster than
conventional optimizers by taking advantage of both gradient information and
curvature information when updating the DNN model. Therefore, it provides a new
chance for accelerating DNN training in the resource-constrained setting. In
this paper, we explore its distributed design, namely DHO$_2$, including
distributed calculation of curvature information and model update with partial
curvature information to accelerate DNN training with a low memory burden. To
further reduce the training time, we design a novel strategy to parallelize the
calculation of curvature information and the model update on different devices.
Experimentally, our distributed design can achieve an approximate linear
reduction of memory burden on each device with the increase of the device
number. Meanwhile, it achieves $1.4\times\sim2.1\times$ speedup in the total
training time compared with other distributed designs based on conventional
first- and second-order optimizers.

</details>


### [70] [Toward Data-centric Directed Graph Learning: An Entropy-driven Approach](https://arxiv.org/abs/2505.00983)
*Xunkai Li,Zhengyu Wu,Kaichi Yu,Hongchao Qin,Guang Zeng,Rong-Hua Li,Guoren Wang*

Main category: cs.LG

TL;DR: 论文提出了一种名为EDEN的数据为中心的有向图学习范式，通过层次编码理论和知识蒸馏提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有DiGNNs未能充分挖掘有向图中隐藏的数据知识，导致模型预测性能不佳，需要从数据为中心的角度探索有向边与节点特征之间的潜在关联。

Method: EDEN利用有向结构测量构建层次知识树（HKT），并通过节点特征的互信息细化知识流，实现数据为中心的知识蒸馏。

Result: 在14个（有向）图数据集和4个下游任务中，EDEN表现出SOTA性能，显著提升了现有（Di）GNNs的表现。

Conclusion: EDEN作为一种通用框架，不仅适用于有向图，还能扩展到无向图场景，展现出强大的编码能力和性能提升潜力。

Abstract: The directed graph (digraph), as a generalization of undirected graphs,
exhibits superior representation capability in modeling complex topology
systems and has garnered considerable attention in recent years. Despite the
notable efforts made by existing DiGraph Neural Networks (DiGNNs) to leverage
directed edges, they still fail to comprehensively delve into the abundant data
knowledge concealed in the digraphs. This data-level limitation results in
model-level sub-optimal predictive performance and underscores the necessity of
further exploring the potential correlations between the directed edges
(topology) and node profiles (feature and labels) from a data-centric
perspective, thereby empowering model-centric neural networks with stronger
encoding capabilities.
  In this paper, we propose \textbf{E}ntropy-driven \textbf{D}igraph
knowl\textbf{E}dge distillatio\textbf{N} (EDEN), which can serve as a
data-centric digraph learning paradigm or a model-agnostic hot-and-plug
data-centric Knowledge Distillation (KD) module. The core idea is to achieve
data-centric ML, guided by our proposed hierarchical encoding theory for
structured data. Specifically, EDEN first utilizes directed structural
measurements from a topology perspective to construct a coarse-grained
Hierarchical Knowledge Tree (HKT). Subsequently, EDEN quantifies the mutual
information of node profiles to refine knowledge flow in the HKT, enabling
data-centric KD supervision within model training. As a general framework, EDEN
can also naturally extend to undirected scenarios and demonstrate satisfactory
performance. In our experiments, EDEN has been widely evaluated on 14 (di)graph
datasets (homophily and heterophily) and across 4 downstream tasks. The results
demonstrate that EDEN attains SOTA performance and exhibits strong improvement
for prevalent (Di)GNNs.

</details>


### [71] [Evaluating Explanations: An Explanatory Virtues Framework for Mechanistic Interpretability -- The Strange Science Part I.ii](https://arxiv.org/abs/2505.01372)
*Kola Ayonrinde,Louis Jaburi*

Main category: cs.LG

TL;DR: 本文提出了一种基于哲学科学的多视角解释框架（Explanatory Virtues Framework），用于系统评估和改进神经网络的可解释性方法，并探讨了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏统一的评估标准，神经网络的可解释性研究进展受限。本文旨在回答“什么是好的解释？”这一核心问题。

Method: 引入了一个多元化的解释框架，结合了贝叶斯、库恩、德意志和规范四种哲学视角，用于评估和改进解释方法。

Result: 研究发现，紧凑证明（Compact Proofs）能够综合考虑多种解释优势，是一种有前景的方法。

Conclusion: 未来研究方向包括明确定义解释的简洁性、关注统一性解释以及推导神经网络的通用原则。改进的可解释性方法将提升对AI系统的监控、预测和引导能力。

Abstract: Mechanistic Interpretability (MI) aims to understand neural networks through
causal explanations. Though MI has many explanation-generating methods,
progress has been limited by the lack of a universal approach to evaluating
explanations. Here we analyse the fundamental question "What makes a good
explanation?" We introduce a pluralist Explanatory Virtues Framework drawing on
four perspectives from the Philosophy of Science - the Bayesian, Kuhnian,
Deutschian, and Nomological - to systematically evaluate and improve
explanations in MI. We find that Compact Proofs consider many explanatory
virtues and are hence a promising approach. Fruitful research directions
implied by our framework include (1) clearly defining explanatory simplicity,
(2) focusing on unifying explanations and (3) deriving universal principles for
neural networks. Improved MI methods enhance our ability to monitor, predict,
and steer AI systems.

</details>


### [72] [On-demand Test-time Adaptation for Edge Devices](https://arxiv.org/abs/2505.00986)
*Xiao Ma,Young D. Kwon,Dong Ma*

Main category: cs.LG

TL;DR: 论文提出了一种新的范式——按需测试时适应（OD-TTA），通过仅在检测到显著域偏移时触发适应，显著降低了计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有持续测试时适应（CTTA）方法在资源受限的边缘设备上因内存和能耗问题难以实际应用。

Method: OD-TTA包含三个创新技术：轻量级域偏移检测、源域选择模块和解耦批量归一化更新方案。

Result: 实验表明，OD-TTA在保持高性能的同时显著降低了能耗和计算开销。

Conclusion: OD-TTA使测试时适应在实际应用中成为可能。

Abstract: Continual Test-time adaptation (CTTA) continuously adapts the deployed model
on every incoming batch of data. While achieving optimal accuracy, existing
CTTA approaches present poor real-world applicability on resource-constrained
edge devices, due to the substantial memory overhead and energy consumption. In
this work, we first introduce a novel paradigm -- on-demand TTA -- which
triggers adaptation only when a significant domain shift is detected. Then, we
present OD-TTA, an on-demand TTA framework for accurate and efficient
adaptation on edge devices. OD-TTA comprises three innovative techniques: 1) a
lightweight domain shift detection mechanism to activate TTA only when it is
needed, drastically reducing the overall computation overhead, 2) a source
domain selection module that chooses an appropriate source model for
adaptation, ensuring high and robust accuracy, 3) a decoupled Batch
Normalization (BN) update scheme to enable memory-efficient adaptation with
small batch sizes. Extensive experiments show that OD-TTA achieves comparable
and even better performance while reducing the energy and computation overhead
remarkably, making TTA a practical reality.

</details>


### [73] [Where's the liability in the Generative Era? Recovery-based Black-Box Detection of AI-Generated Content](https://arxiv.org/abs/2505.01008)
*Haoyue Bai,Yiyou Sun,Wei Cheng,Haifeng Chen*

Main category: cs.LG

TL;DR: 提出了一种无需模型权重或大量辅助数据集的黑盒检测框架，通过“损坏与恢复”策略检测生成图像，性能优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 生成模型生成的逼真图像可能被滥用，现有检测方法依赖模型权重或大量数据集，限制了实际应用。

Method: 采用“损坏与恢复”策略，通过掩码部分图像并评估模型重建能力，判断图像是否由模型生成。对于不支持掩码输入的黑盒模型，使用低成本替代模型增强检测能力。

Result: 在八个扩散模型变体数据集上，平均精度比基线方法高4.31%。

Conclusion: 该框架为生成图像检测提供了一种高效且实用的解决方案。

Abstract: The recent proliferation of photorealistic images created by generative
models has sparked both excitement and concern, as these images are
increasingly indistinguishable from real ones to the human eye. While offering
new creative and commercial possibilities, the potential for misuse, such as in
misinformation and fraud, highlights the need for effective detection methods.
Current detection approaches often rely on access to model weights or require
extensive collections of real image datasets, limiting their scalability and
practical application in real world scenarios. In this work, we introduce a
novel black box detection framework that requires only API access, sidestepping
the need for model weights or large auxiliary datasets. Our approach leverages
a corrupt and recover strategy: by masking part of an image and assessing the
model ability to reconstruct it, we measure the likelihood that the image was
generated by the model itself. For black-box models that do not support masked
image inputs, we incorporate a cost efficient surrogate model trained to align
with the target model distribution, enhancing detection capability. Our
framework demonstrates strong performance, outperforming baseline methods by
4.31% in mean average precision across eight diffusion model variant datasets.

</details>


### [74] [Stagnation in Evolutionary Algorithms: Convergence $\neq$ Optimality](https://arxiv.org/abs/2505.01036)
*Xiaojun Zhou*

Main category: cs.LG

TL;DR: 论文挑战了进化计算中关于停滞和收敛的传统观点，指出个体停滞可能促进种群收敛，且收敛不一定意味着最优性。


<details>
  <summary>Details</summary>
Motivation: 传统观点认为停滞阻碍进化算法的收敛，而收敛等同于最优性，但这一观点存在误导。

Method: 通过理论分析和提供反例，研究探讨了停滞和收敛的关系。

Result: 研究发现个体停滞可能促进种群收敛，且收敛并不保证最优性。

Conclusion: 收敛本身不足以确保进化算法的有效性，需重新审视停滞和收敛的作用。

Abstract: In the evolutionary computation community, it is widely believed that
stagnation impedes convergence in evolutionary algorithms, and that convergence
inherently indicates optimality. However, this perspective is misleading. In
this study, it is the first to highlight that the stagnation of an individual
can actually facilitate the convergence of the entire population, and
convergence does not necessarily imply optimality, not even local optimality.
Convergence alone is insufficient to ensure the effectiveness of evolutionary
algorithms. Several counterexamples are provided to illustrate this argument.

</details>


### [75] [Global Optimality of Single-Timescale Actor-Critic under Continuous State-Action Space: A Study on Linear Quadratic Regulator](https://arxiv.org/abs/2505.01041)
*Xuyang Chen,Jingliang Duan,Lin Zhao*

Main category: cs.LG

TL;DR: 本文研究了经典的单样本单时间尺度actor-critic方法在连续状态-动作空间中的性能，证明了其在LQR问题上能达到ε-最优解，样本复杂度为ε的-2次方。


<details>
  <summary>Details</summary>
Motivation: 尽管actor-critic方法在实际任务中表现出色，但其理论理解仍不足。现有研究多关注不常见的变体（如双循环或双时间尺度），且仅限于有限状态-动作空间。本文旨在填补这一空白。

Method: 采用经典的单一时间尺度actor-critic方法，以线性二次调节器（LQR）问题为案例，研究其在连续状态-动作空间中的性能。

Result: 证明了单一时间尺度actor-critic在LQR问题上能达到ε-最优解，样本复杂度为ε的-2次方。

Conclusion: 本文为单一时间尺度actor-critic的性能提供了新见解，进一步缩小了理论与实践的差距。

Abstract: Actor-critic methods have achieved state-of-the-art performance in various
challenging tasks. However, theoretical understandings of their performance
remain elusive and challenging. Existing studies mostly focus on practically
uncommon variants such as double-loop or two-timescale stepsize actor-critic
algorithms for simplicity. These results certify local convergence on finite
state- or action-space only. We push the boundary to investigate the classic
single-sample single-timescale actor-critic on continuous (infinite)
state-action space, where we employ the canonical linear quadratic regulator
(LQR) problem as a case study. We show that the popular single-timescale
actor-critic can attain an epsilon-optimal solution with an order of epsilon to
-2 sample complexity for solving LQR on the demanding continuous state-action
space. Our work provides new insights into the performance of single-timescale
actor-critic, which further bridges the gap between theory and practice.

</details>


### [76] [Low-Precision Training of Large Language Models: Methods, Challenges, and Opportunities](https://arxiv.org/abs/2505.01043)
*Zhiwei Hao,Jianyuan Guo,Li Shen,Yong Luo,Han Hu,Guoxia Wang,Dianhai Yu,Yonggang Wen,Dacheng Tao*

Main category: cs.LG

TL;DR: 该论文综述了低精度训练方法，将其分为三类（定点/整数、浮点、自定义格式），并讨论了量化感知训练，提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）训练需要大量硬件资源，低精度训练可提高效率，但方法多样且分散，需统一整理。

Method: 将低精度训练方法分为三类：定点/整数、浮点、自定义格式，并讨论量化感知训练。

Result: 系统整理了低精度训练方法，为研究者提供统一视角，并指出未来方向。

Conclusion: 低精度训练方法多样，分类整理有助于研究进展，未来需进一步探索。

Abstract: Large language models (LLMs) have achieved impressive performance across
various domains. However, the substantial hardware resources required for their
training present a significant barrier to efficiency and scalability. To
mitigate this challenge, low-precision training techniques have been widely
adopted, leading to notable advancements in training efficiency. Despite these
gains, low-precision training involves several components$\unicode{x2013}$such
as weights, activations, and gradients$\unicode{x2013}$each of which can be
represented in different numerical formats. The resulting diversity has created
a fragmented landscape in low-precision training research, making it difficult
for researchers to gain a unified overview of the field. This survey provides a
comprehensive review of existing low-precision training methods. To
systematically organize these approaches, we categorize them into three primary
groups based on their underlying numerical formats, which is a key factor
influencing hardware compatibility, computational efficiency, and ease of
reference for readers. The categories are: (1) fixed-point and integer-based
methods, (2) floating-point-based methods, and (3) customized format-based
methods. Additionally, we discuss quantization-aware training approaches, which
share key similarities with low-precision training during forward propagation.
Finally, we highlight several promising research directions to advance this
field. A collection of papers discussed in this survey is provided in
https://github.com/Hao840/Awesome-Low-Precision-Training.

</details>


### [77] [Multi-Step Consistency Models: Fast Generation with Theoretical Guarantees](https://arxiv.org/abs/2505.01049)
*Nishant Jain,Xunpeng Huang,Yian Ma,Tong Zhang*

Main category: cs.LG

TL;DR: 本文分析了一致性模型的理论基础，证明了其在少量迭代下可实现高质量的样本生成，并提供了KL散度的收敛保证。


<details>
  <summary>Details</summary>
Motivation: 尽管一致性模型在生成高质量样本方面表现出色，但其加速的理论依据尚不明确。本文旨在填补这一空白。

Method: 通过分析一致性模型在反向轨迹中的映射能力，证明了在恒定步长下，仅需对数次迭代即可实现KL散度的收敛。

Result: 结果表明，在数据分布的最小假设下，一致性模型可实现KL散度的收敛，且收敛速率优于现有SDE或ODE方法。

Conclusion: 本文为一致性模型的高效性提供了理论支持，并展示了其在平滑和非平滑设置下的学习可行性。

Abstract: Consistency models have recently emerged as a compelling alternative to
traditional SDE based diffusion models, offering a significant acceleration in
generation by producing high quality samples in very few steps. Despite their
empirical success, a proper theoretic justification for their speed up is still
lacking. In this work, we provide the analysis which bridges this gap, showing
that given a consistency model which can map the input at a given time to
arbitrary timestamps along the reverse trajectory, one can achieve KL
divergence of order $ O(\varepsilon^2) $ using only $
O\left(\log\left(\frac{d}{\varepsilon}\right)\right) $ iterations with constant
step size, where d is the data dimension. Additionally, under minimal
assumptions on the data distribution an increasingly common setting in recent
diffusion model analyses we show that a similar KL convergence guarantee can be
obtained, with the number of steps scaling as $ O\left(d
\log\left(\frac{d}{\varepsilon}\right)\right) $. Going further, we also provide
a theoretical analysis for estimation of such consistency models, concluding
that accurate learning is feasible using small discretization steps, both in
smooth and non smooth settings. Notably, our results for the non smooth case
yield best in class convergence rates compared to existing SDE or ODE based
analyses under minimal assumptions.

</details>


### [78] [Monotone Peridynamic Neural Operator for Nonlinear Material Modeling with Conditionally Unique Solutions](https://arxiv.org/abs/2505.01060)
*Jihong Wang,Xiaochuan Tian,Zhongqiang Zhang,Stewart Silling,Siavash Jafarzadeh,Yue Yu*

Main category: cs.LG

TL;DR: 论文提出了一种基于神经算子的单调近场动力学神经算子（MPNO），用于数据驱动的非局部本构模型学习，确保解的唯一性，并在合成和真实数据上验证了其性能。


<details>
  <summary>Details</summary>
Motivation: 数据驱动的本构模型在物理可解释性和泛化性上有优势，但其适定性通常无法先验保证，可能导致非物理解。MPNO旨在解决这一问题。

Method: MPNO通过单调梯度网络学习非局部核和非线性本构关系，确保能量密度函数的凸性，从而在小变形范围内保证解的唯一性。

Result: 在合成数据上，MPNO随网格细化收敛到真实解；在真实数据上，其泛化能力优于传统神经网络，并在分子动力学数据建模中表现出鲁棒性。

Conclusion: MPNO是一种有效的数据驱动本构模型学习方法，兼具理论保证和实际应用价值。

Abstract: Data-driven methods have emerged as powerful tools for modeling the responses
of complex nonlinear materials directly from experimental measurements. Among
these methods, the data-driven constitutive models present advantages in
physical interpretability and generalizability across different boundary
conditions/domain settings. However, the well-posedness of these learned models
is generally not guaranteed a priori, which makes the models prone to
non-physical solutions in downstream simulation tasks. In this study, we
introduce monotone peridynamic neural operator (MPNO), a novel data-driven
nonlocal constitutive model learning approach based on neural operators. Our
approach learns a nonlocal kernel together with a nonlinear constitutive
relation, while ensuring solution uniqueness through a monotone gradient
network. This architectural constraint on gradient induces convexity of the
learnt energy density function, thereby guaranteeing solution uniqueness of
MPNO in small deformation regimes. To validate our approach, we evaluate MPNO's
performance on both synthetic and real-world datasets. On synthetic datasets
with manufactured kernel and constitutive relation, we show that the learnt
model converges to the ground-truth as the measurement grid size decreases both
theoretically and numerically. Additionally, our MPNO exhibits superior
generalization capabilities than the conventional neural networks: it yields
smaller displacement solution errors in down-stream tasks with new and unseen
loadings. Finally, we showcase the practical utility of our approach through
applications in learning a homogenized model from molecular dynamics data,
highlighting its expressivity and robustness in real-world scenarios.

</details>


### [79] [Improving Group Fairness in Knowledge Distillation via Laplace Approximation of Early Exits](https://arxiv.org/abs/2505.01070)
*Edvin Fasth,Sagar Singh*

Main category: cs.LG

TL;DR: 论文提出利用拉普拉斯近似方法改进知识蒸馏中的公平性，通过校准不确定性估计重加权困难样本。


<details>
  <summary>Details</summary>
Motivation: 知识蒸馏中学生模型可能因学习简单特征而降低群体公平性，现有方法如早期退出网络（EENNs）通过置信度边际重加权损失，但拉普拉斯近似可能更鲁棒。

Method: 使用拉普拉斯近似方法获取校准的不确定性估计，重加权交叉熵和蒸馏损失，以识别困难或模糊样本。

Result: 在MultiNLI数据集上基于Bert模型的实验验证了方法的有效性。

Conclusion: 拉普拉斯近似方法能更鲁棒地识别困难样本，提升知识蒸馏中的群体公平性。

Abstract: Knowledge distillation (KD) has become a powerful tool for training compact
student models using larger, pretrained teacher models, often requiring less
data and computational resources. Teacher models typically possess more layers
and thus exhibit richer feature representations compared to their student
counterparts. Furthermore, student models tend to learn simpler, surface-level
features in their early layers. This discrepancy can increase errors in groups
where labels spuriously correlate with specific input attributes, leading to a
decline in group fairness even when overall accuracy remains comparable to the
teacher. To mitigate these challenges, Early-Exit Neural Networks (EENNs),
which enable predictions at multiple intermediate layers, have been employed.
Confidence margins derived from these early exits have been utilized to
reweight both cross-entropy and distillation losses on a per-instance basis. In
this paper, we propose that leveraging Laplace approximation-based methods to
obtain well-calibrated uncertainty estimates can also effectively reweight
challenging instances and improve group fairness. We hypothesize that Laplace
approximation offers a more robust identification of difficult or ambiguous
instances compared to margin-based approaches. To validate our claims, we
benchmark our approach using a Bert-based model on the MultiNLI dataset.

</details>


### [80] [Federated Adapter on Foundation Models: An Out-Of-Distribution Approach](https://arxiv.org/abs/2505.01075)
*Yiyuan Yang,Guodong Long,Tianyi Zhou,Qinghua Lu,Shanshan Ye,Jing Jiang*

Main category: cs.LG

TL;DR: FedOA是一种针对联邦基础模型（FedFM）的隐私保护方法，通过适配器微调和特征距离正则化解决分布偏移问题，提升未见任务的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 联邦基础模型（FedFM）在隐私保护下协作微调模型时，面临分布偏移导致的泛化能力不足问题，现有方法难以应对大规模参数和数据异质性。

Method: 提出FedOA，采用适配器微调方法提升效率，并通过个性化适配器和特征距离正则化对齐分布，保证每个客户的OOD泛化能力。

Result: 理论证明全局模型具有固有OOD泛化能力，FedOA通过正则化增强个性化模型的泛化能力，并在多种NLP任务中验证了有效性。

Conclusion: FedOA通过适配器和正则化有效解决了FedFM中的OOD泛化问题，为隐私保护下的协作学习提供了新思路。

Abstract: As foundation models gain prominence, Federated Foundation Models (FedFM)
have emerged as a privacy-preserving approach to collaboratively fine-tune
models in federated learning (FL) frameworks using distributed datasets across
clients. A key challenge for FedFM, given the versatile nature of foundation
models, is addressing out-of-distribution (OOD) generalization, where unseen
tasks or clients may exhibit distribution shifts leading to suboptimal
performance. Although numerous studies have explored OOD generalization in
conventional FL, these methods are inadequate for FedFM due to the challenges
posed by large parameter scales and increased data heterogeneity. To address
these, we propose FedOA, which employs adapter-based parameter-efficient
fine-tuning methods for efficacy and introduces personalized adapters with
feature distance-based regularization to align distributions and guarantee OOD
generalization for each client. Theoretically, we demonstrate that the
conventional aggregated global model in FedFM inherently retains OOD
generalization capabilities, and our proposed method enhances the personalized
model's OOD generalization through regularization informed by the global model,
with proven convergence under general non-convex settings. Empirically, the
effectiveness of the proposed method is validated on benchmark datasets across
various NLP tasks.

</details>


### [81] [Integration Matters for Learning PDEs with Backwards SDEs](https://arxiv.org/abs/2505.01078)
*Sungje Park,Stephen Tu*

Main category: cs.LG

TL;DR: 本文提出了一种基于Stratonovich的BSDE方法，通过Heun积分消除EM积分带来的偏差，显著提升了BSDE求解器的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于BSDE的求解器在性能上不及PINNs，研究发现其根源在于EM积分引入的离散化偏差。

Method: 提出Stratonovich-based BSDE方法，采用Heun积分替代EM积分，以消除偏差。

Result: 实验表明，该方法完全解决了EM积分的偏差问题，性能优于EM变体，并与PINNs竞争。

Conclusion: 积分方案在BSDE求解器中至关重要，Heun积分显著提升了性能。

Abstract: Backward stochastic differential equation (BSDE)-based deep learning methods
provide an alternative to Physics-Informed Neural Networks (PINNs) for solving
high-dimensional partial differential equations (PDEs), offering algorithmic
advantages in settings such as stochastic optimal control, where the PDEs of
interest are tied to an underlying dynamical system. However, existing
BSDE-based solvers have empirically been shown to underperform relative to
PINNs in the literature. In this paper, we identify the root cause of this
performance gap as a discretization bias introduced by the standard
Euler-Maruyama (EM) integration scheme applied to short-horizon
self-consistency BSDE losses, which shifts the optimization landscape off
target. We find that this bias cannot be satisfactorily addressed through finer
step sizes or longer self-consistency horizons. To properly handle this issue,
we propose a Stratonovich-based BSDE formulation, which we implement with
stochastic Heun integration. We show that our proposed approach completely
eliminates the bias issues faced by EM integration. Furthermore, our empirical
results show that our Heun-based BSDE method consistently outperforms EM-based
variants and achieves competitive results with PINNs across multiple
high-dimensional benchmarks. Our findings highlight the critical role of
integration schemes in BSDE-based PDE solvers, an algorithmic detail that has
received little attention thus far in the literature.

</details>


### [82] [Multi-Objective Reinforcement Learning for Water Management](https://arxiv.org/abs/2505.01094)
*Zuzanna Osika,Roxana Radelescu,Jazmin Zatarain Salazar,Frans Oliehoek,Pradeep K. Murukannaiah*

Main category: cs.LG

TL;DR: 论文提出了一种多目标强化学习（MORL）在尼罗河流域水资源管理中的应用，并指出现有MORL算法在复杂现实问题中的局限性。


<details>
  <summary>Details</summary>
Motivation: 现实中的多目标优化问题（如资源管理、自动驾驶）需要同时处理多个冲突目标，但MORL领域缺乏复杂且真实的环境和基准。

Method: 作者将尼罗河流域水资源管理建模为MORL环境，并在此任务上对现有MORL算法进行基准测试。

Result: 结果显示，专门的水资源管理方法优于当前最先进的MORL方法，突显了MORL算法在现实场景中的可扩展性挑战。

Conclusion: 研究表明，MORL算法在复杂现实问题中仍需改进，以应对实际应用中的挑战。

Abstract: Many real-world problems (e.g., resource management, autonomous driving, drug
discovery) require optimizing multiple, conflicting objectives. Multi-objective
reinforcement learning (MORL) extends classic reinforcement learning to handle
multiple objectives simultaneously, yielding a set of policies that capture
various trade-offs. However, the MORL field lacks complex, realistic
environments and benchmarks. We introduce a water resource (Nile river basin)
management case study and model it as a MORL environment. We then benchmark
existing MORL algorithms on this task. Our results show that specialized water
management methods outperform state-of-the-art MORL approaches, underscoring
the scalability challenges MORL algorithms face in real-world scenarios.

</details>


### [83] [Nesterov Method for Asynchronous Pipeline Parallel Optimization](https://arxiv.org/abs/2505.01099)
*Thalaiyasingam Ajanthan,Sameera Ramasinghe,Yan Zuo,Gil Avraham,Alexander Long*

Main category: cs.LG

TL;DR: 论文提出了一种改进的Nesterov加速梯度（NAG）方法，用于解决管道并行（PP）中异步优化的梯度延迟问题，实验证明其优于现有异步方法。


<details>
  <summary>Details</summary>
Motivation: 在管道并行中，异步优化虽然能实现100%的管道利用率，但梯度延迟问题导致权重和梯度不同步，影响性能。

Method: 通过修改NAG的前瞻步骤，有效解决梯度延迟问题，并理论证明了在固定延迟梯度下的次线性收敛率。

Result: 在10亿参数规模的语言建模任务中，该方法显著优于现有异步方法，甚至超过同步基线。

Conclusion: 改进的NAG方法在异步优化中有效解决了梯度延迟问题，提升了管道并行的性能。

Abstract: Pipeline Parallelism (PP) enables large neural network training on small,
interconnected devices by splitting the model into multiple stages. To maximize
pipeline utilization, asynchronous optimization is appealing as it offers 100%
pipeline utilization by construction. However, it is inherently challenging as
the weights and gradients are no longer synchronized, leading to stale (or
delayed) gradients. To alleviate this, we introduce a variant of Nesterov
Accelerated Gradient (NAG) for asynchronous optimization in PP. Specifically,
we modify the look-ahead step in NAG to effectively address the staleness in
gradients. We theoretically prove that our approach converges at a sublinear
rate in the presence of fixed delay in gradients. Our experiments on
large-scale language modelling tasks using decoder-only architectures with up
to 1B parameters, demonstrate that our approach significantly outperforms
existing asynchronous methods, even surpassing the synchronous baseline.

</details>


### [84] [CoCoAFusE: Beyond Mixtures of Experts via Model Fusion](https://arxiv.org/abs/2505.01105)
*Aurelio Raffa Ugolini,Mara Tanelli,Valentina Breschi*

Main category: cs.LG

TL;DR: 论文提出了一种名为CoCoAFusE的新方法，通过贝叶斯协变量依赖建模技术，结合多个简单子模型的预测，提高表达能力并保持局部可解释性。


<details>
  <summary>Details</summary>
Motivation: 深度学习在处理多模式和不确定性时表现优异，但模型可解释性和不确定性量化（UQ）仍存在不足。

Method: CoCoAFusE基于混合专家（MoEs）的思想，不仅混合子模型的预测，还融合其分布，避免多模态伪影，提高建模灵活性。

Result: 实验表明，CoCoAFusE在复杂回归问题中表现优异，能更准确地量化不确定性。

Conclusion: CoCoAFusE是一种高效且灵活的方法，适用于需要高表达能力和不确定性量化的场景。

Abstract: Many learning problems involve multiple patterns and varying degrees of
uncertainty dependent on the covariates. Advances in Deep Learning (DL) have
addressed these issues by learning highly nonlinear input-output dependencies.
However, model interpretability and Uncertainty Quantification (UQ) have often
straggled behind. In this context, we introduce the Competitive/Collaborative
Fusion of Experts (CoCoAFusE), a novel, Bayesian Covariates-Dependent Modeling
technique. CoCoAFusE builds on the very philosophy behind Mixtures of Experts
(MoEs), blending predictions from several simple sub-models (or "experts") to
achieve high levels of expressiveness while retaining a substantial degree of
local interpretability. Our formulation extends that of a classical Mixture of
Experts by contemplating the fusion of the experts' distributions in addition
to their more usual mixing (i.e., superimposition). Through this additional
feature, CoCoAFusE better accommodates different scenarios for the intermediate
behavior between generating mechanisms, resulting in tighter credible bounds on
the response variable. Indeed, only resorting to mixing, as in classical MoEs,
may lead to multimodality artifacts, especially over smooth transitions.
Instead, CoCoAFusE can avoid these artifacts even under the same structure and
priors for the experts, leading to greater expressiveness and flexibility in
modeling. This new approach is showcased extensively on a suite of motivating
numerical examples and a collection of real-data ones, demonstrating its
efficacy in tackling complex regression problems where uncertainty is a key
quantity of interest.

</details>


### [85] [Incorporating Inductive Biases to Energy-based Generative Models](https://arxiv.org/abs/2505.01111)
*Yukun Li,Li-Ping Liu*

Main category: cs.LG

TL;DR: 论文提出了一种结合能量模型和指数族模型的混合方法，通过引入无参数统计函数来提升数据建模能力。


<details>
  <summary>Details</summary>
Motivation: 能量模型（EBMs）在生成模型中重新受到关注，但通常依赖神经网络定义能量函数，缺乏对数据统计特性的显式建模。

Method: 提出混合模型，将EBM与指数族模型结合，通过无参数统计函数增强能量项，以捕获关键数据统计特性。

Result: 实验验证了混合模型能有效匹配统计数据，并在数据拟合和生成方面表现更优。

Conclusion: 混合模型通过结合统计特性，提升了EBMs的建模能力，为生成模型提供了新思路。

Abstract: With the advent of score-matching techniques for model training and Langevin
dynamics for sample generation, energy-based models (EBMs) have gained renewed
interest as generative models. Recent EBMs usually use neural networks to
define their energy functions. In this work, we introduce a novel hybrid
approach that combines an EBM with an exponential family model to incorporate
inductive bias into data modeling. Specifically, we augment the energy term
with a parameter-free statistic function to help the model capture key data
statistics. Like an exponential family model, the hybrid model aims to align
the distribution statistics with data statistics during model training, even
when it only approximately maximizes the data likelihood. This property enables
us to impose constraints on the hybrid model. Our empirical study validates the
hybrid model's ability to match statistics. Furthermore, experimental results
show that data fitting and generation improve when suitable informative
statistics are incorporated into the hybrid model.

</details>


### [86] [Exploring Equity of Climate Policies using Multi-Agent Multi-Objective Reinforcement Learning](https://arxiv.org/abs/2505.01115)
*Palok Biswas,Zuzanna Osika,Isidoro Tamassia,Adit Whorra,Jazmin Zatarain-Salazar,Jan Kwakkel,Frans A. Oliehoek,Pradeep K. Murukannaiah*

Main category: cs.LG

TL;DR: 论文提出了一种名为Justice的新框架，将IAM与多目标多智能体强化学习（MOMARL）结合，以解决传统IAM在气候政策评估中的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统IAM基于单一目标优化政策，无法捕捉经济、气候目标和气候正义之间的权衡，导致政策建议可能加剧不平等。

Method: 引入Justice框架，结合IAM与MOMARL，通过多目标和多智能体建模，生成考虑公平性的政策建议。

Result: 框架识别出公平的帕累托最优政策，帮助决策者在气候与经济政策中权衡。

Conclusion: Justice框架为气候政策提供了更全面的评估工具，支持更公平和现实的决策过程。

Abstract: Addressing climate change requires coordinated policy efforts of nations
worldwide. These efforts are informed by scientific reports, which rely in part
on Integrated Assessment Models (IAMs), prominent tools used to assess the
economic impacts of climate policies. However, traditional IAMs optimize
policies based on a single objective, limiting their ability to capture the
trade-offs among economic growth, temperature goals, and climate justice. As a
result, policy recommendations have been criticized for perpetuating
inequalities, fueling disagreements during policy negotiations. We introduce
Justice, the first framework integrating IAM with Multi-Objective Multi-Agent
Reinforcement Learning (MOMARL). By incorporating multiple objectives, Justice
generates policy recommendations that shed light on equity while balancing
climate and economic goals. Further, using multiple agents can provide a
realistic representation of the interactions among the diverse policy actors.
We identify equitable Pareto-optimal policies using our framework, which
facilitates deliberative decision-making by presenting policymakers with the
inherent trade-offs in climate and economic policy.

</details>


### [87] [Risk Analysis and Design Against Adversarial Actions](https://arxiv.org/abs/2505.01130)
*Marco C. Campi,Algo Carè,Luis G. Crespo,Simone Garatti,Federico A. Ramponi*

Main category: cs.LG

TL;DR: 论文提出了一种评估模型对抗攻击鲁棒性的通用框架，适用于多种攻击类型和强度，无需额外测试数据且分布无关。


<details>
  <summary>Details</summary>
Motivation: 研究动机源于部署时数据与训练条件不一致的问题，需提高模型在对抗行为下的预测可靠性。

Method: 方法基于支持向量回归（SVR），但可扩展至松弛优化技术的学习领域。

Result: 结果提供了评估模型脆弱性的工具，并帮助选择最佳模型，同时为分布外框架提供新见解。

Conclusion: 结论表明该框架增强了模型的可信度，并为对抗性学习提供了实用工具。

Abstract: Learning models capable of providing reliable predictions in the face of
adversarial actions has become a central focus of the machine learning
community in recent years. This challenge arises from observing that data
encountered at deployment time often deviate from the conditions under which
the model was trained. In this paper, we address deployment-time adversarial
actions and propose a versatile, well-principled framework to evaluate the
model's robustness against attacks of diverse types and intensities. While we
initially focus on Support Vector Regression (SVR), the proposed approach
extends naturally to the broad domain of learning via relaxed optimization
techniques. Our results enable an assessment of the model vulnerability without
requiring additional test data and operate in a distribution-free setup. These
results not only provide a tool to enhance trust in the model's applicability
but also aid in selecting among competing alternatives. Later in the paper, we
show that our findings also offer useful insights for establishing new results
within the out-of-distribution framework.

</details>


### [88] [Aggregation of Dependent Expert Distributions in Multimodal Variational Autoencoders](https://arxiv.org/abs/2505.01134)
*Rogelio A Mancisidor,Robert Jenssen,Shujian Yu,Michael Kampffmeyer*

Main category: cs.LG

TL;DR: 提出了一种基于共识依赖专家（CoDE）的多模态VAE方法，避免了现有方法对模态独立性的假设，提升了生成质量和一致性。


<details>
  <summary>Details</summary>
Motivation: 现有方法假设模态独立性，导致估计过于乐观，无法准确捕捉多模态数据的联合分布。

Method: 利用CoDE原则聚合单模态分布，提出新的ELBO近似联合似然，学习各模态子集的贡献。

Result: CoDE-VAE在生成质量和一致性上表现更好，生成对数似然估计更精确，且随着模态增加，生成质量差距缩小。

Conclusion: CoDE-VAE在多模态学习中表现优异，生成质量和分类准确性与最先进方法相当。

Abstract: Multimodal learning with variational autoencoders (VAEs) requires estimating
joint distributions to evaluate the evidence lower bound (ELBO). Current
methods, the product and mixture of experts, aggregate single-modality
distributions assuming independence for simplicity, which is an overoptimistic
assumption. This research introduces a novel methodology for aggregating
single-modality distributions by exploiting the principle of consensus of
dependent experts (CoDE), which circumvents the aforementioned assumption.
Utilizing the CoDE method, we propose a novel ELBO that approximates the joint
likelihood of the multimodal data by learning the contribution of each subset
of modalities. The resulting CoDE-VAE model demonstrates better performance in
terms of balancing the trade-off between generative coherence and generative
quality, as well as generating more precise log-likelihood estimations.
CoDE-VAE further minimizes the generative quality gap as the number of
modalities increases. In certain cases, it reaches a generative quality similar
to that of unimodal VAEs, which is a desirable property that is lacking in most
current methods. Finally, the classification accuracy achieved by CoDE-VAE is
comparable to that of state-of-the-art multimodal VAE models.

</details>


### [89] [Dual-Forecaster: A Multimodal Time Series Model Integrating Descriptive and Predictive Texts](https://arxiv.org/abs/2505.01135)
*Wenfa Wu,Guanyu Zhang,Zheng Tan,Yi Wang,Hongsheng Qi*

Main category: cs.LG

TL;DR: Dual-Forecaster是一种新型多模态时间序列模型，结合历史文本信息和预测性文本洞察，通过三种跨模态对齐技术提升性能，优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有单模态时间序列模型因信息不足受限，多模态模型虽能整合文本信息，但未能充分利用历史和未来文本信息的独特贡献，且缺乏对文本与时间序列数据复杂关系的深入理解。

Method: 提出Dual-Forecaster模型，结合历史描述性文本和预测性文本，采用三种跨模态对齐技术增强多模态理解能力。

Result: 在15个多模态时间序列数据集上评估，Dual-Forecaster表现优于或媲美现有最优模型，验证了整合文本信息的优越性。

Conclusion: 该研究为多模态时间序列分析中文本与数值数据的整合开辟了新途径。

Abstract: Most existing single-modal time series models rely solely on numerical
series, which suffer from the limitations imposed by insufficient information.
Recent studies have revealed that multimodal models can address the core issue
by integrating textual information. However, these models focus on either
historical or future textual information, overlooking the unique contributions
each plays in time series forecasting. Besides, these models fail to grasp the
intricate relationships between textual and time series data, constrained by
their moderate capacity for multimodal comprehension. To tackle these
challenges, we propose Dual-Forecaster, a pioneering multimodal time series
model that combines both descriptively historical textual information and
predictive textual insights, leveraging advanced multimodal comprehension
capability empowered by three well-designed cross-modality alignment
techniques. Our comprehensive evaluations on fifteen multimodal time series
datasets demonstrate that Dual-Forecaster is a distinctly effective multimodal
time series model that outperforms or is comparable to other state-of-the-art
models, highlighting the superiority of integrating textual information for
time series forecasting. This work opens new avenues in the integration of
textual information with numerical time series data for multimodal time series
analysis.

</details>


### [90] [Machine Learning for Physical Simulation Challenge Results and Retrospective Analysis: Power Grid Use Case](https://arxiv.org/abs/2505.01156)
*Milad Leyli-Abadi,Jérôme Picault,Antoine Marot,Jean-Patrick Brunet,Agathe Gilain,Amarsagar Reddy Ramapuram Matavalam,Shaban Ghias Satti,Quingbin Jiang,Yang Liu,Dean Justin Ninalga*

Main category: cs.LG

TL;DR: 论文探讨了电力网格模拟的计算挑战，提出AI驱动方法加速模拟，并通过LIPS框架评估解决方案。


<details>
  <summary>Details</summary>
Motivation: 随着可再生能源的集成增加，传统模拟方法计算量过大，需实时分析更多场景以确保电网稳定性。

Method: 组织竞赛开发AI方法，利用LIPS框架评估性能、物理合规性、工业准备度和泛化能力。

Result: 竞赛中表现最佳的解决方案超越了传统模拟方法，展示了高效和可扩展性。

Conclusion: 研究旨在推动更高效、可扩展和可持续的电网模拟方法研究。

Abstract: This paper addresses the growing computational challenges of power grid
simulations, particularly with the increasing integration of renewable energy
sources like wind and solar. As grid operators must analyze significantly more
scenarios in near real-time to prevent failures and ensure stability,
traditional physical-based simulations become computationally impractical. To
tackle this, a competition was organized to develop AI-driven methods that
accelerate power flow simulations by at least an order of magnitude while
maintaining operational reliability. This competition utilized a regional-scale
grid model with a 30\% renewable energy mix, mirroring the anticipated
near-future composition of the French power grid. A key contribution of this
work is through the use of LIPS (Learning Industrial Physical Systems), a
benchmarking framework that evaluates solutions based on four critical
dimensions: machine learning performance, physical compliance, industrial
readiness, and generalization to out-of-distribution scenarios. The paper
provides a comprehensive overview of the Machine Learning for Physical
Simulation (ML4PhySim) competition, detailing the benchmark suite, analyzing
top-performing solutions that outperformed traditional simulation methods, and
sharing key organizational insights and best practices for running large-scale
AI competitions. Given the promising results achieved, the study aims to
inspire further research into more efficient, scalable, and sustainable power
network simulation methodologies.

</details>


### [91] [TActiLE: Tiny Active LEarning for wearable devices](https://arxiv.org/abs/2505.01160)
*Massimo Pavan,Claudio Galimberti,Manuel Roveri*

Main category: cs.LG

TL;DR: 论文提出了一种名为TActiLE的新型主动学习算法，专为TinyML场景设计，旨在解决可穿戴设备中标记数据稀缺的问题。


<details>
  <summary>Details</summary>
Motivation: 在可穿戴设备上实现设备端学习（ODL）时，标记数据的稀缺是一个主要挑战。手动标记大量数据不切实际且可能导致用户放弃使用技术。

Method: 提出TActiLE算法，通过主动选择少量未标记数据进行标记，以最小化标记工作量。

Result: 实验结果表明，TActiLE在多个图像分类数据集上表现有效且高效，适用于微型和可穿戴设备。

Conclusion: TActiLE是首个专为TinyML设计的主动学习技术，为可穿戴设备提供了更个性化的模型适应能力。

Abstract: Tiny Machine Learning (TinyML) algorithms have seen extensive use in recent
years, enabling wearable devices to be not only connected but also genuinely
intelligent by running machine learning (ML) computations directly on-device.
Among such devices, smart glasses have particularly benefited from TinyML
advancements. TinyML facilitates the on-device execution of the inference phase
of ML algorithms on embedded and wearable devices, and more recently, it has
expanded into On-device Learning (ODL), which allows both inference and
learning phases to occur directly on the device. The application of ODL
techniques to wearable devices is particularly compelling, as it enables the
development of more personalized models that adapt based on the data of the
user. However, one of the major challenges of ODL algorithms is the scarcity of
labeled data collected on-device. In smart wearable contexts, requiring users
to manually label large amounts of data is often impractical and could lead to
user disengagement with the technology. To address this issue, this paper
explores the application of Active Learning (AL) techniques, i.e., techniques
that aim at minimizing the labeling effort, by actively selecting from a large
quantity of unlabeled data only a small subset to be labeled and added to the
training set of the algorithm. In particular, we propose TActiLE, a novel AL
algorithm that selects from the stream of on-device sensor data the ones that
would help the ML algorithm improve the most once coupled with labels provided
by the user. TActiLE is the first Active Learning technique specifically
designed for the TinyML context. We evaluate its effectiveness and efficiency
through experiments on multiple image classification datasets. The results
demonstrate its suitability for tiny and wearable devices.

</details>


### [92] [Empirical Comparison of Lightweight Forecasting Models for Seasonal and Non-Seasonal Time Series](https://arxiv.org/abs/2505.01163)
*Thanh Son Nguyen,Dang Minh Duc Nguyen,Van Thanh Nguyen*

Main category: cs.LG

TL;DR: 比较多项式分类器（PC）和径向基函数神经网络（RBFNN）在时间序列预测中的表现，PC在非季节性数据中更优，RBFNN在季节性数据中更优。


<details>
  <summary>Details</summary>
Motivation: 研究高精度且高效的时间序列预测方法，以满足实时应用需求。

Method: 在四个真实数据集（天气、黄金价格、原油价格和啤酒产量）上比较PC和RBFNN的预测准确性和计算时间。

Result: PC在非季节性数据中更快更准确，RBFNN在季节性数据中表现更好。PC结构更透明，RBFNN为黑箱。

Conclusion: PC适合非季节性数据的快速可解释预测，RBFNN适合复杂季节性行为。

Abstract: Accurate time series forecasting is essential in many real-time applications
that demand both high predictive accuracy and computational efficiency. This
study provides an empirical comparison between a Polynomial Classifier and a
Radial Basis Function Neural Network (RBFNN) across four real-world time series
datasets (weather conditions, gold prices, crude oil prices, and beer
production volumes) that cover both seasonal and nonseasonal patterns. Model
performance is evaluated by forecasting accuracy (using Mean Absolute Error,
Root Mean Squared Error, and Coefficient of Variation of Root Mean Squared
Error) and computational time to assess each model's viability for real time
forecasting. The results show that the PC yields more accurate and faster
forecasts for non seasonal series, whereas the RBFNN performs better on series
with pronounced seasonal patterns. From an interpretability standpoint, the
polynomial model offers a simpler, more transparent structure (in contrast to
the black box nature of neural network), which is advantageous for
understanding and trust in real time decision making. The performance
differences between PC and RBFNN are statistically significant, as confirmed by
paired t tests and Wilcoxon signed rank tests. These findings provide practical
guidance for model selection in time series forecasting, indicating that PC may
be preferable for quick, interpretable forecasts in non-seasonal contexts,
whereas RBFNN is superior for capturing complex seasonal behaviors

</details>


### [93] [Harmonizing Intra-coherence and Inter-divergence in Ensemble Attacks for Adversarial Transferability](https://arxiv.org/abs/2505.01168)
*Zhaoyang Ma,Zhihao Wu,Wang Lu,Xin Gao,Jinghang Yue,Taolin Zhang,Lipo Wang,Youfang Lin,Jing Wang*

Main category: cs.LG

TL;DR: HEAT是一种新的对抗样本生成方法，通过域泛化和动态权重分配提升跨模型攻击的迁移性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在共享梯度方向捕捉和权重分配上存在不足，威胁深度神经网络的安全性。

Method: HEAT包含两个模块：共识梯度方向合成器（使用SVD）和双和谐权重协调器（动态平衡域内一致性和域间多样性）。

Result: HEAT在多个数据集和设置中显著优于现有方法。

Conclusion: HEAT为对抗攻击研究提供了新视角和方向。

Abstract: The development of model ensemble attacks has significantly improved the
transferability of adversarial examples, but this progress also poses severe
threats to the security of deep neural networks. Existing methods, however,
face two critical challenges: insufficient capture of shared gradient
directions across models and a lack of adaptive weight allocation mechanisms.
To address these issues, we propose a novel method Harmonized Ensemble for
Adversarial Transferability (HEAT), which introduces domain generalization into
adversarial example generation for the first time. HEAT consists of two key
modules: Consensus Gradient Direction Synthesizer, which uses Singular Value
Decomposition to synthesize shared gradient directions; and Dual-Harmony Weight
Orchestrator which dynamically balances intra-domain coherence, stabilizing
gradients within individual models, and inter-domain diversity, enhancing
transferability across models. Experimental results demonstrate that HEAT
significantly outperforms existing methods across various datasets and
settings, offering a new perspective and direction for adversarial attack
research.

</details>


### [94] [Distilling Two-Timed Flow Models by Separately Matching Initial and Terminal Velocities](https://arxiv.org/abs/2505.01169)
*Pramook Khungurn,Pratch Piyawongwisal,Sira Sriswadi,Supasorn Suwajanakorn*

Main category: cs.LG

TL;DR: 论文提出了一种新的损失函数（ITVM），用于蒸馏双时间流模型（TTFM），通过匹配初始和终端速度，提升了少步生成的性能。


<details>
  <summary>Details</summary>
Motivation: 为了改进双时间流模型的蒸馏效果，提出了一种新的损失函数（ITVM），以优化少步生成任务的性能。

Method: ITVM损失函数通过匹配初始速度、去除终端速度的导数项，并使用EMA稳定模型来计算目标终端平均速度。

Result: 初步实验表明，ITVM损失在多种数据集和模型架构上优于基线方法，提升了少步生成性能。

Conclusion: ITVM损失函数是一种有效的改进方法，能够显著提升双时间流模型的蒸馏效果和生成性能。

Abstract: A flow matching model learns a time-dependent vector field $v_t(x)$ that
generates a probability path $\{ p_t \}_{0 \leq t \leq 1}$ that interpolates
between a well-known noise distribution ($p_0$) and the data distribution
($p_1$). It can be distilled into a \emph{two-timed flow model} (TTFM)
$\phi_{s,x}(t)$ that can transform a sample belonging to the distribution at an
initial time $s$ to another belonging to the distribution at a terminal time
$t$ in one function evaluation. We present a new loss function for TTFM
distillation called the \emph{initial/terminal velocity matching} (ITVM) loss
that extends the Lagrangian Flow Map Distillation (LFMD) loss proposed by Boffi
et al. by adding redundant terms to match the initial velocities at time $s$,
removing the derivative from the terminal velocity term at time $t$, and using
a version of the model under training, stabilized by exponential moving
averaging (EMA), to compute the target terminal average velocity. Preliminary
experiments show that our loss leads to better few-step generation performance
on multiple types of datasets and model architectures over baselines.

</details>


### [95] [A Secured Triad of IoT, Machine Learning, and Blockchain for Crop Forecasting in Agriculture](https://arxiv.org/abs/2505.01196)
*Najmus Sakib Sizan,Md. Abu Layek,Khondokar Fida Hasan*

Main category: cs.LG

TL;DR: 提出了一种结合物联网、机器学习和区块链的新方法，用于提高作物预测准确性，为农民提供数据驱动的决策支持。


<details>
  <summary>Details</summary>
Motivation: 改善作物预测的准确性，为农民提供实时、可靠的数据支持，优化农业生产策略。

Method: 通过物联网传感器网络实时监测环境与土壤数据，利用随机森林模型预测作物类型和产量，并引入以太坊区块链确保数据安全。

Result: 随机森林模型预测准确率达99.45%，系统提供实时和历史作物预测数据，增强透明度和决策支持。

Conclusion: 该方法显著提升了精准农业的作物预测能力，使其更准确、安全且用户友好。

Abstract: To improve crop forecasting and provide farmers with actionable data-driven
insights, we propose a novel approach integrating IoT, machine learning, and
blockchain technologies. Using IoT, real-time data from sensor networks
continuously monitor environmental conditions and soil nutrient levels,
significantly improving our understanding of crop growth dynamics. Our study
demonstrates the exceptional accuracy of the Random Forest model, achieving a
99.45\% accuracy rate in predicting optimal crop types and yields, thereby
offering precise crop projections and customized recommendations. To ensure the
security and integrity of the sensor data used for these forecasts, we
integrate the Ethereum blockchain, which provides a robust and secure platform.
This ensures that the forecasted data remain tamper-proof and reliable.
Stakeholders can access real-time and historical crop projections through an
intuitive online interface, enhancing transparency and facilitating informed
decision-making. By presenting multiple predicted crop scenarios, our system
enables farmers to optimize production strategies effectively. This integrated
approach promises significant advances in precision agriculture, making crop
forecasting more accurate, secure, and user-friendly.

</details>


### [96] [CaReAQA: A Cardiac and Respiratory Audio Question Answering Model for Open-Ended Diagnostic Reasoning](https://arxiv.org/abs/2505.01199)
*Tsai-Ning Wang,Lin-Lin Chen,Neil Zeghidour,Aaqib Saeed*

Main category: cs.LG

TL;DR: CaReAQA是一种结合音频基础模型和大语言模型的音频-语言模型，用于医学音频信号的开放诊断推理，性能优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 传统医学音频分析方法依赖手工特征或需要大量标注数据的深度学习模型，限制了其扩展性和适用性。

Method: 提出CaReAQA模型，结合音频基础模型和大语言模型的推理能力；同时发布CaReSound数据集，包含标注的医学音频和问答示例。

Result: CaReAQA在开放诊断推理任务中达到86.2%准确率，在未见数据集上的封闭分类任务中平均准确率为56.9%。

Conclusion: 音频-语言集成和推理能力推动了医学诊断的发展，为临床决策支持提供了高效的AI系统。

Abstract: Medical audio signals, such as heart and lung sounds, play a crucial role in
clinical diagnosis. However, analyzing these signals remains challenging:
traditional methods rely on handcrafted features or supervised deep learning
models that demand extensive labeled datasets, limiting their scalability and
applicability. To address these issues, we propose CaReAQA, an audio-language
model that integrates a foundation audio model with the reasoning capabilities
of large language models, enabling clinically relevant, open-ended diagnostic
responses. Alongside CaReAQA, we introduce CaReSound, a benchmark dataset of
annotated medical audio recordings enriched with metadata and paired
question-answer examples, intended to drive progress in diagnostic reasoning
research. Evaluation results show that CaReAQA achieves 86.2% accuracy on
open-ended diagnostic reasoning tasks, outperforming baseline models. It also
generalizes well to closed-ended classification tasks, achieving an average
accuracy of 56.9% on unseen datasets. Our findings show how audio-language
integration and reasoning advances medical diagnostics, enabling efficient AI
systems for clinical decision support.

</details>


### [97] [AGRO: An Autonomous AI Rover for Precision Agriculture](https://arxiv.org/abs/2505.01200)
*Simar Ghumman,Fabio Di Troia,William Andreopoulos,Mark Stamp,Sanjit Rai*

Main category: cs.LG

TL;DR: AGRO是一个结合机器学习和UGV技术的自主农业机器人，用于自动化农业数据采集和决策支持。


<details>
  <summary>Details</summary>
Motivation: 解决农业中资源消耗大、数据驱动决策需求高的问题。

Method: 利用机器学习、计算机视觉和传感器技术，实现自主导航、环境映射和产量预测。

Result: AGRO能够自主完成农业数据采集，支持农民决策。

Conclusion: AGRO为农业自动化提供了可行方案，并为机器学习技术应用奠定了基础。

Abstract: Unmanned Ground Vehicles (UGVs) are emerging as a crucial tool in the world
of precision agriculture. The combination of UGVs with machine learning allows
us to find solutions for a range of complex agricultural problems. This
research focuses on developing a UGV capable of autonomously traversing
agricultural fields and capturing data. The project, known as AGRO (Autonomous
Ground Rover Observer) leverages machine learning, computer vision and other
sensor technologies. AGRO uses its capabilities to determine pistachio yields,
performing self-localization and real-time environmental mapping while avoiding
obstacles. The main objective of this research work is to automate
resource-consuming operations so that AGRO can support farmers in making
data-driven decisions. Furthermore, AGRO provides a foundation for advanced
machine learning techniques as it captures the world around it.

</details>


### [98] [Quantitative Attractor Analysis of High-Capacity Kernel Logistic Regression Hopfield Networks](https://arxiv.org/abs/2505.01218)
*Akira Tamamori*

Main category: cs.LG

TL;DR: 本文通过定量分析KLR训练网络的吸引子结构，证实了KLR在高容量（高达4.0 P/N）和鲁棒性方面的优越性能，其吸引子景观干净且动态快速。


<details>
  <summary>Details</summary>
Motivation: 传统Hopfield网络因Hebbian学习面临存储容量限制和虚假吸引子问题，而KLR通过非线性映射提升性能，本文旨在定量分析其吸引子结构。

Method: 通过广泛模拟，评估了不同初始状态、存储负载（高达4.0 P/N）和噪声水平下的回忆性能，量化了收敛速率和速度。

Result: KLR表现出高容量（高达4.0 P/N）和鲁棒性，吸引子景观干净，虚假固定点接近零，动态快速（通常1-2步）。

Conclusion: KLR重塑了高容量联想记忆的动态特性，证明了其有效性，并为AM理解提供了贡献。

Abstract: Traditional Hopfield networks, using Hebbian learning, face severe storage
capacity limits ($\approx 0.14$ P/N) and spurious attractors. Kernel Logistic
Regression (KLR) offers a non-linear approach, mapping patterns to
high-dimensional feature spaces for improved separability. Our previous work
showed KLR dramatically improves capacity and noise robustness over
conventional methods. This paper quantitatively analyzes the attractor
structures in KLR-trained networks via extensive simulations. We evaluated
recall from diverse initial states across wide storage loads (up to 4.0 P/N)
and noise levels. We quantified convergence rates and speed. Our analysis
confirms KLR's superior performance: high capacity (up to 4.0 P/N) and
robustness. The attractor landscape is remarkably "clean," with near-zero
spurious fixed points. Recall failures under high load/noise are primarily due
to convergence to other learned patterns, not spurious ones. Dynamics are
exceptionally fast (typically 1-2 steps for high-similarity states). This
characterization reveals how KLR reshapes dynamics for high-capacity
associative memory, highlighting its effectiveness and contributing to AM
understanding.

</details>


### [99] [mwBTFreddy: A Dataset for Flash Flood Damage Assessment in Urban Malawi](https://arxiv.org/abs/2505.01242)
*Evelyn Chapuma,Grey Mengezi,Lewis Msasa,Amelia Taylor*

Main category: cs.LG

TL;DR: mwBTFreddy数据集是为支持马拉维城市洪水灾害评估而开发的资源，包含灾前灾后卫星图像和标注的建筑损坏数据。


<details>
  <summary>Details</summary>
Motivation: 支持非洲城市环境中的建筑检测和损坏分类机器学习模型开发，为气候脆弱地区的决策提供依据。

Method: 使用Google Earth Pro获取卫星图像，并标注建筑损坏等级（无损坏、轻微、严重或毁坏）。

Result: 数据集包含地理坐标和损坏等级的JSON文件，支持洪水损害可视化和空间分析。

Conclusion: 该数据集有助于灾害响应和基础设施规划，特别是在气候脆弱地区。

Abstract: This paper describes the mwBTFreddy dataset, a resource developed to support
flash flood damage assessment in urban Malawi, specifically focusing on the
impacts of Cyclone Freddy in 2023. The dataset comprises paired pre- and
post-disaster satellite images sourced from Google Earth Pro, accompanied by
JSON files containing labelled building annotations with geographic coordinates
and damage levels (no damage, minor, major, or destroyed). Developed by the
Kuyesera AI Lab at the Malawi University of Business and Applied Sciences, this
dataset is intended to facilitate the development of machine learning models
tailored to building detection and damage classification in African urban
contexts. It also supports flood damage visualisation and spatial analysis to
inform decisions on relocation, infrastructure planning, and emergency response
in climate-vulnerable regions.

</details>


### [100] [Enhancing Obsolescence Forecasting with Deep Generative Data Augmentation: A Semi-Supervised Framework for Low-Data Industrial Applications](https://arxiv.org/abs/2505.01261)
*Elie Saad,Mariem Besbes,Marc Zolghadri,Victor Czmil,Claude Baron,Vincent Bourgeois*

Main category: cs.LG

TL;DR: 本文提出了一种基于深度学习的电子组件过时预测框架，通过深度生成模型解决数据不足问题，并在基准数据集上取得先进成果。


<details>
  <summary>Details</summary>
Motivation: 电子组件过时问题在长生命周期系统中尤为关键，现有机器学习方法因数据不足而受限。

Method: 提出基于深度生成模型的框架，生成新过时案例以扩充训练数据，并改进经典监督学习分类器以适应半监督学习。

Result: 框架在基准数据集上表现出最先进的性能。

Conclusion: 该框架有效解决了数据不足问题，提升了过时预测的准确性。

Abstract: The challenge of electronic component obsolescence is particularly critical
in systems with long life cycles. Various obsolescence management methods are
employed to mitigate its impact, with obsolescence forecasting being a highly
sought-after and prominent approach. As a result, numerous machine
learning-based forecasting methods have been proposed. However, machine
learning models require a substantial amount of relevant data to achieve high
precision, which is lacking in the current obsolescence landscape in some
situations. This work introduces a novel framework for obsolescence forecasting
based on deep learning. The proposed framework solves the lack of available
data through deep generative modeling, where new obsolescence cases are
generated and used to augment the training dataset. The augmented dataset is
then used to train a classical machine learning-based obsolescence forecasting
model. To train classical forecasting models using augmented datasets, existing
classical supervised-learning classifiers are adapted for semi-supervised
learning within this framework. The proposed framework demonstrates
state-of-the-art results on benchmarking datasets.

</details>


### [101] [MultiGran-STGCNFog: Towards Accurate and High-Throughput Inference for Multi-Granular Spatiotemporal Traffic Forecasting](https://arxiv.org/abs/2505.01279)
*Zhaoyan Wang,Xiangchi Song,In-Young Ko*

Main category: cs.LG

TL;DR: 提出了一种高效的雾分布式推理系统MultiGran-STGCNFog，通过多粒度时空特征融合和动态交通图生成，优化了交通预测的准确性和推理速度。


<details>
  <summary>Details</summary>
Motivation: 现有基于GCN的方法无法充分提取和融合多粒度时空特征，导致预测准确性不足，同时模型复杂度和推理时间增加。

Method: 提出MultiGran-STGCNFog模型，融合多粒度时空特征于动态交通图；设计调度算法GA-DPHDS，优化层执行顺序和设备调度。

Result: 在真实数据集上的实验表明，该方法在预测准确性和推理速度上优于基线模型。

Conclusion: MultiGran-STGCNFog通过特征融合和调度优化，显著提升了交通预测的性能和效率。

Abstract: Accurate traffic forecasting and swift inference provision are essential for
intelligent transportation systems. However, the present Graph Convolutional
Network (GCN)-based approaches cannot extract and fuse multi-granular
spatiotemporal features across various spatial and temporal scales
sufficiently, proven to yield less accurate forecasts. Besides, additional
feature extraction branches introduced in prior studies critically increased
model complexity and extended inference time, making it challenging to provide
fast inference for traffic forecasting. In this paper, we propose
MultiGran-STGCNFog, an efficient fog distributed inference system with a novel
traffic forecasting model that employs multi-granular spatiotemporal feature
fusion on generated dynamic traffic graphs to fully capture interdependent
traffic dynamics. The proposed scheduling algorithm GA-DPHDS, optimizing layer
execution order and layer-device scheduling scheme simultaneously, contributes
to considerable inference throughput improvement by leveraging heterogeneous
fog devices in a pipelined manner. Extensive experiments on real-world datasets
demonstrate the superiority of the proposed method over selected baselines.

</details>


### [102] [A Physics-preserved Transfer Learning Method for Differential Equations](https://arxiv.org/abs/2505.01281)
*Hao-Ran Yang,Chuan-Xian Ren*

Main category: cs.LG

TL;DR: 论文提出了一种物理保持最优张量传输（POTT）方法，用于解决数据驱动方法在微分方程（DEs）中的领域转移问题，同时保持物理信息。


<details>
  <summary>Details</summary>
Motivation: 数据驱动方法（如神经算子）在解决微分方程时存在领域转移问题，现有迁移学习方法缺乏通用性或物理保持能力。

Method: 通过将数据域建模为乘积分布，提出POTT方法，同时解决分布偏差和算子偏差，利用POTT映射的推前分布适应目标域。

Result: 实验证明POTT方法在性能、通用性和物理保持方面表现优越。

Conclusion: POTT方法是一种通用的迁移学习方法，能有效适应领域转移并保持物理信息。

Abstract: While data-driven methods such as neural operator have achieved great success
in solving differential equations (DEs), they suffer from domain shift problems
caused by different learning environments (with data bias or equation changes),
which can be alleviated by transfer learning (TL). However, existing TL methods
adopted in DEs problems lack either generalizability in general DEs problems or
physics preservation during training. In this work, we focus on a general
transfer learning method that adaptively correct the domain shift and preserve
physical information. Mathematically, we characterize the data domain as
product distribution and the essential problems as distribution bias and
operator bias. A Physics-preserved Optimal Tensor Transport (POTT) method that
simultaneously admits generalizability to common DEs and physics preservation
of specific problem is proposed to adapt the data-driven model to target domain
utilizing the push-forward distribution induced by the POTT map. Extensive
experiments demonstrate the superior performance, generalizability and physics
preservation of the proposed POTT method.

</details>


### [103] [2DXformer: Dual Transformers for Wind Power Forecasting with Dual Exogenous Variables](https://arxiv.org/abs/2505.01286)
*Yajuan Zhang,Jiahai Jiang,Yule Yan,Liang Yang,Ping Zhang*

Main category: cs.LG

TL;DR: 论文提出2DXformer模型，通过分类输入变量并利用注意力机制，改进了风电功率预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法在风电功率预测中未充分建模变量间关系，且未区分内外生变量，导致预测精度受限和模型复杂度增加。

Method: 将输入分为外生静态、外生动态和内生变量，独立嵌入为变量标记，利用注意力机制捕捉外生变量相关性，并通过多层感知机建模外生变量对内生变量的影响。

Result: 在两个真实大规模数据集上的实验表明，2DXformer进一步提升了风电功率预测性能。

Conclusion: 2DXformer通过改进变量建模和区分内外生变量，显著提升了风电功率预测的准确性和效率。

Abstract: Accurate wind power forecasting can help formulate scientific dispatch plans,
which is of great significance for maintaining the safety, stability, and
efficient operation of the power system. In recent years, wind power
forecasting methods based on deep learning have focused on extracting the
spatiotemporal correlations among data, achieving significant improvements in
forecasting accuracy. However, they exhibit two limitations. First, there is a
lack of modeling for the inter-variable relationships, which limits the
accuracy of the forecasts. Second, by treating endogenous and exogenous
variables equally, it leads to unnecessary interactions between the endogenous
and exogenous variables, increasing the complexity of the model. In this paper,
we propose the 2DXformer, which, building upon the previous work's focus on
spatiotemporal correlations, addresses the aforementioned two limitations.
Specifically, we classify the inputs of the model into three types: exogenous
static variables, exogenous dynamic variables, and endogenous variables. First,
we embed these variables as variable tokens in a channel-independent manner.
Then, we use the attention mechanism to capture the correlations among
exogenous variables. Finally, we employ a multi-layer perceptron with residual
connections to model the impact of exogenous variables on endogenous variables.
Experimental results on two real-world large-scale datasets indicate that our
proposed 2DXformer can further improve the performance of wind power
forecasting. The code is available in this repository:
\href{https://github.com/jseaj/2DXformer}{https://github.com/jseaj/2DXformer}.

</details>


### [104] [Integration of Multi-Mode Preference into Home Energy Management System Using Deep Reinforcement Learning](https://arxiv.org/abs/2505.01332)
*Mohammed Sumayli,Olugbenga Moses Anubi*

Main category: cs.LG

TL;DR: 论文提出了一种基于深度强化学习的家庭能源管理系统（DRL-HEMS），通过动态多模式偏好优化能源消耗，显著提升用户舒适度和计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有研究常将用户舒适度视为静态权重，忽略了用户行为的动态性。本文旨在通过动态多模式偏好优化，增强用户在需求响应（DR）项目中的参与度。

Method: 采用无模型单智能体深度强化学习算法，结合实时数据（如电价、环境温度、设备功耗）优化能源管理。

Result: 模型在不同偏好模式下表现优异，计算效率优于传统的混合整数线性规划（MILP）算法。

Conclusion: DRL-HEMS框架在动态优化和用户友好性方面具有显著优势，为智能家居能源管理提供了新思路。

Abstract: Home Energy Management Systems (HEMS) have emerged as a pivotal tool in the
smart home ecosystem, aiming to enhance energy efficiency, reduce costs, and
improve user comfort. By enabling intelligent control and optimization of
household energy consumption, HEMS plays a significant role in bridging the gap
between consumer needs and energy utility objectives. However, much of the
existing literature construes consumer comfort as a mere deviation from the
standard appliance settings. Such deviations are typically incorporated into
optimization objectives via static weighting factors. These factors often
overlook the dynamic nature of consumer behaviors and preferences. Addressing
this oversight, our paper introduces a multi-mode Deep Reinforcement
Learning-based HEMS (DRL-HEMS) framework, meticulously designed to optimize
based on dynamic, consumer-defined preferences. Our primary goal is to augment
consumer involvement in Demand Response (DR) programs by embedding dynamic
multi-mode preferences tailored to individual appliances. In this study, we
leverage a model-free, single-agent DRL algorithm to deliver a HEMS framework
that is not only dynamic but also user-friendly. To validate its efficacy, we
employed real-world data at 15-minute intervals, including metrics such as
electricity price, ambient temperature, and appliances' power consumption. Our
results show that the model performs exceptionally well in optimizing energy
consumption within different preference modes. Furthermore, when compared to
traditional algorithms based on Mixed-Integer Linear Programming (MILP), our
model achieves nearly optimal performance while outperforming in computational
efficiency.

</details>


### [105] [Enhancing Diversity in Parallel Agents: A Maximum State Entropy Exploration Story](https://arxiv.org/abs/2505.01336)
*Vincenzo De Paola,Riccardo Zamboni,Mirco Mutti,Marcello Restelli*

Main category: cs.LG

TL;DR: 提出了一种新型并行强化学习框架，通过最大化数据熵和平衡个体与群体多样性，超越传统N倍加速效果。


<details>
  <summary>Details</summary>
Motivation: 探索并行RL中通过策略专业化是否能够超越传统N倍数据收集加速的效果。

Method: 采用集中式策略梯度方法，平衡个体熵和群体多样性，减少冗余数据。

Result: 实验表明该方法优于同质化并行系统，并能与批量RL技术协同利用数据多样性。

Conclusion: 通过理论分析和实证验证，证明了专业化并行采样分布的加速效果，具有独立研究价值。

Abstract: Parallel data collection has redefined Reinforcement Learning (RL), unlocking
unprecedented efficiency and powering breakthroughs in large-scale real-world
applications. In this paradigm, $N$ identical agents operate in $N$ replicas of
an environment simulator, accelerating data collection by a factor of $N$. A
critical question arises: \textit{Does specializing the policies of the
parallel agents hold the key to surpass the $N$ factor acceleration?} In this
paper, we introduce a novel learning framework that maximizes the entropy of
collected data in a parallel setting. Our approach carefully balances the
entropy of individual agents with inter-agent diversity, effectively minimizing
redundancies. The latter idea is implemented with a centralized policy gradient
method, which shows promise when evaluated empirically against systems of
identical agents, as well as synergy with batch RL techniques that can exploit
data diversity. Finally, we provide an original concentration analysis that
shows faster rates for specialized parallel sampling distributions, which
supports our methodology and may be of independent interest.

</details>


### [106] [How to Learn a Star: Binary Classification with Starshaped Polyhedral Sets](https://arxiv.org/abs/2505.01346)
*Marie-Charlotte Brandenburg,Katharina Jochemko*

Main category: cs.LG

TL;DR: 论文研究了基于星形多面体决策边界的连续分段线性函数的二元分类问题，探讨了其表达能力、损失景观的几何结构，并给出了VC维的显式界限。


<details>
  <summary>Details</summary>
Motivation: 研究星形多面体决策边界的连续分段线性函数在二元分类中的表达能力及其损失景观的几何结构。

Method: 分析两类损失函数（0/1损失和指数损失）的子水平集，并描述其组合和几何结构。

Result: 给出了VC维的显式界限，描述了离散损失的子水平集为超平面排列中的腔室，并为指数损失提供了最优解唯一性的充分条件。

Conclusion: 该研究为星形多面体决策边界的分类问题提供了理论支持，并揭示了损失景观的几何特性。

Abstract: We consider binary classification restricted to a class of continuous
piecewise linear functions whose decision boundaries are (possibly nonconvex)
starshaped polyhedral sets, supported on a fixed polyhedral simplicial fan. We
investigate the expressivity of these function classes and describe the
combinatorial and geometric structure of the loss landscape, most prominently
the sublevel sets, for two loss-functions: the 0/1-loss (discrete loss) and an
exponential loss function. In particular, we give explicit bounds on the VC
dimension of this model, and concretely describe the sublevel sets of the
discrete loss as chambers in a hyperplane arrangement. For the exponential
loss, we give sufficient conditions for the optimum to be unique, and describe
the geometry of the optimum when varying the rate parameter of the underlying
exponential probability distribution.

</details>


### [107] [Learning Stabilizing Policies via an Unstable Subspace Representation](https://arxiv.org/abs/2505.01348)
*Leonardo F. Toso,Lintao Ye,James Anderson*

Main category: cs.LG

TL;DR: 论文提出了一种两阶段方法，通过学习系统的不稳定子空间并解决一系列折扣线性二次调节器问题，显著降低了稳定化过程的样本复杂度。


<details>
  <summary>Details</summary>
Motivation: 研究如何在不依赖初始稳定策略的情况下，高效地学习稳定线性时不变系统，解决现有方法数据需求大的问题。

Method: 两阶段方法：1）学习系统的不稳定子空间；2）在不稳定子空间上解决折扣线性二次调节器问题，仅稳定系统的不稳定动态。

Result: 理论分析和数值实验表明，该方法在不稳定模式远小于状态维度时，显著降低了样本复杂度。

Conclusion: 通过聚焦于不稳定子空间，该方法高效地实现了系统稳定化，减少了数据需求。

Abstract: We study the problem of learning to stabilize (LTS) a linear time-invariant
(LTI) system. Policy gradient (PG) methods for control assume access to an
initial stabilizing policy. However, designing such a policy for an unknown
system is one of the most fundamental problems in control, and it may be as
hard as learning the optimal policy itself. Existing work on the LTS problem
requires large data as it scales quadratically with the ambient dimension. We
propose a two-phase approach that first learns the left unstable subspace of
the system and then solves a series of discounted linear quadratic regulator
(LQR) problems on the learned unstable subspace, targeting to stabilize only
the system's unstable dynamics and reduce the effective dimension of the
control space. We provide non-asymptotic guarantees for both phases and
demonstrate that operating on the unstable subspace reduces sample complexity.
In particular, when the number of unstable modes is much smaller than the state
dimension, our analysis reveals that LTS on the unstable subspace substantially
speeds up the stabilization process. Numerical experiments are provided to
support this sample complexity reduction achieved by our approach.

</details>


### [108] [Stabilizing Temporal Difference Learning via Implicit Stochastic Approximation](https://arxiv.org/abs/2505.01361)
*Hwanwoo Kim,Panos Toulis,Eric Laber*

Main category: cs.LG

TL;DR: 论文提出了一种隐式TD学习算法，通过将TD更新重新表述为固定点方程，解决了传统TD学习对步长敏感的问题，提高了稳定性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 传统TD学习对步长选择敏感，导致估计误差大且收敛慢，研究者需反复试验步长，耗时费力。

Method: 提出隐式TD算法，将TD更新转化为固定点方程，减少对步长的依赖。

Result: 隐式TD算法在稳定性和计算效率上表现优异，理论分析证明了其渐近收敛性和有限时间误差界。

Conclusion: 隐式TD算法是一种稳健且实用的工具，适用于现代RL任务中的策略评估和值近似。

Abstract: Temporal Difference (TD) learning is a foundational algorithm in
reinforcement learning (RL). For nearly forty years, TD learning has served as
a workhorse for applied RL as well as a building block for more complex and
specialized algorithms. However, despite its widespread use, it is not without
drawbacks, the most prominent being its sensitivity to step size. A poor choice
of step size can dramatically inflate the error of value estimates and slow
convergence. Consequently, in practice, researchers must use trial and error in
order to identify a suitable step size -- a process that can be tedious and
time consuming. As an alternative, we propose implicit TD algorithms that
reformulate TD updates into fixed-point equations. These updates are more
stable and less sensitive to step size without sacrificing computational
efficiency. Moreover, our theoretical analysis establishes asymptotic
convergence guarantees and finite-time error bounds. Our results demonstrate
their robustness and practicality for modern RL tasks, establishing implicit TD
as a versatile tool for policy evaluation and value approximation.

</details>


### [109] [Carbon Aware Transformers Through Joint Model-Hardware Optimization](https://arxiv.org/abs/2505.01386)
*Irene Wang,Newsha Ardalani,Mostafa Elhoushi,Daniel Jiang,Samuel Hsia,Ekin Sumbul,Divya Mahajan,Carole-Jean Wu,Bilge Acun*

Main category: cs.LG

TL;DR: 论文提出了一种名为CATransformers的碳感知架构搜索框架，用于全面优化机器学习系统的碳足迹，包括运行碳和隐含碳。通过应用于CLIP模型，实现了17%的碳减排。


<details>
  <summary>Details</summary>
Motivation: 机器学习系统的快速增长需要更全面地评估其环境影响，尤其是碳足迹。目前缺乏工具和框架来量化并优化ML系统的总碳足迹。

Method: 提出CATransformers框架，将运行碳和隐含碳指标纳入早期设计空间探索，实现可持续驱动的ML模型和硬件架构协同优化。

Result: 应用于多模态CLIP模型，生成CarbonCLIP家族，实现总碳排放减少17%，同时保持准确性和延迟。

Conclusion: 需采用整体优化方法设计高性能且环境可持续的AI系统。

Abstract: The rapid growth of machine learning (ML) systems necessitates a more
comprehensive evaluation of their environmental impact, particularly their
carbon footprint, which comprises operational carbon from training and
inference execution and embodied carbon from hardware manufacturing and its
entire life-cycle. Despite the increasing importance of embodied emissions,
there is a lack of tools and frameworks to holistically quantify and optimize
the total carbon footprint of ML systems. To address this, we propose
CATransformers, a carbon-aware architecture search framework that enables
sustainability-driven co-optimization of ML models and hardware architectures.
By incorporating both operational and embodied carbon metrics into early design
space exploration of domain-specific hardware accelerators, CATransformers
demonstrates that optimizing for carbon yields design choices distinct from
those optimized solely for latency or energy efficiency. We apply our framework
to multi-modal CLIP-based models, producing CarbonCLIP, a family of CLIP models
achieving up to 17% reduction in total carbon emissions while maintaining
accuracy and latency compared to state-of-the-art edge small CLIP baselines.
This work underscores the need for holistic optimization methods to design
high-performance, environmentally sustainable AI systems.

</details>


### [110] [Learning and Transferring Physical Models through Derivatives](https://arxiv.org/abs/2505.01391)
*Alessandro Trenta,Andrea Cossu,Davide Bacciu*

Main category: cs.LG

TL;DR: DERL是一种监督学习方法，通过建模物理系统的偏导数来学习物理模型，并通过蒸馏协议逐步构建模型。它在泛化ODE和PDE方面优于现有方法，并首次尝试分阶段增量构建物理模型。


<details>
  <summary>Details</summary>
Motivation: 传统方法难以准确建模物理系统，尤其是在泛化到未见初始条件或参数时。DERL旨在通过学习偏导数和增量构建模型来解决这一问题。

Method: DERL通过学习物理系统的偏导数建模，并设计蒸馏协议将知识从预训练模型传递到学生模型。理论证明其能学习真实的物理系统。

Result: DERL在泛化ODE和PDE方面优于现有方法，并能将物理知识扩展到新的物理域和参数范围。

Conclusion: DERL首次实现了分阶段增量构建物理模型，为物理系统建模提供了新思路。

Abstract: We propose Derivative Learning (DERL), a supervised approach that models
physical systems by learning their partial derivatives. We also leverage DERL
to build physical models incrementally, by designing a distillation protocol
that effectively transfers knowledge from a pre-trained to a student model. We
provide theoretical guarantees that our approach can learn the true physical
system, being consistent with the underlying physical laws, even when using
empirical derivatives. DERL outperforms state-of-the-art methods in
generalizing an ODE to unseen initial conditions and a parametric PDE to unseen
parameters. We finally propose a method based on DERL to transfer physical
knowledge across models by extending them to new portions of the physical
domain and new range of PDE parameters. We believe this is the first attempt at
building physical models incrementally in multiple stages.

</details>


### [111] [Predicting the Price of Gold in the Financial Markets Using Hybrid Models](https://arxiv.org/abs/2505.01402)
*Mohammadhossein Rashidi,Mohammad Modarres*

Main category: cs.LG

TL;DR: 论文提出了一种结合ARIMA、逐步回归和神经网络的混合模型（ARIMA_Stepwise Regression_Neural Network），用于预测黄金价格，并验证其优于传统时间序列方法。


<details>
  <summary>Details</summary>
Motivation: 资本市场的参与者和研究者一直面临价格预测的高误差和低准确性问题，因此需要一种高精度的预测模型。

Method: 结合ARIMA时间序列模型、逐步回归筛选变量，并将选定的变量输入神经网络进行预测。

Result: 混合模型的预测精度高于传统时间序列方法、回归和逐步回归。

Conclusion: 该混合模型适用于多种金融市场的预测，并表现出更高的准确性。

Abstract: Predicting the price that has the least error and can provide the best and
highest accuracy has been one of the most challenging issues and one of the
most critical concerns among capital market activists and researchers.
Therefore, a model that can solve problems and provide results with high
accuracy is one of the topics of interest among researchers. In this project,
using time series prediction models such as ARIMA to estimate the price,
variables, and indicators related to technical analysis show the behavior of
traders involved in involving psychological factors for the model. By linking
all of these variables to stepwise regression, we identify the best variables
influencing the prediction of the variable. Finally, we enter the selected
variables as inputs to the artificial neural network. In other words, we want
to call this whole prediction process the "ARIMA_Stepwise Regression_Neural
Network" model and try to predict the price of gold in international financial
markets. This approach is expected to be able to be used to predict the types
of stocks, commodities, currency pairs, financial market indicators, and other
items used in local and international financial markets. Moreover, a comparison
between the results of this method and time series methods is also expressed.
Finally, based on the results, it can be seen that the resulting hybrid model
has the highest accuracy compared to the time series method, regression, and
stepwise regression.

</details>


### [112] [How Effective are Large Time Series Models in Hydrology? A Study on Water Level Forecasting in Everglades](https://arxiv.org/abs/2505.01415)
*Rahuul Rangaraj,Jimeng Shi,Azam Shirali,Rajendra Paudel,Yanzhao Wu,Giri Narasimhan*

Main category: cs.LG

TL;DR: 研究了12种任务特定模型和5种时间序列基础模型在Everglades水位预测中的应用，发现基础模型Chronos显著优于其他模型，而其他基础模型表现较差。


<details>
  <summary>Details</summary>
Motivation: 传统物理和统计方法在预测水位时面临高计算成本和适应性不足的问题，而大型时间序列模型在环境系统中的应用尚未充分探索。

Method: 比较了12种任务特定模型和5种时间序列基础模型在Everglades水位预测中的表现。

Result: 基础模型Chronos表现最佳，其他基础模型表现较差，任务特定模型的表现因架构而异。

Conclusion: Chronos在水位预测中具有显著优势，模型性能差异可能与架构和适应性有关。

Abstract: The Everglades play a crucial role in flood and drought regulation, water
resource planning, and ecosystem management in the surrounding regions.
However, traditional physics-based and statistical methods for predicting water
levels often face significant challenges, including high computational costs
and limited adaptability to diverse or unforeseen conditions. Recent
advancements in large time series models have demonstrated the potential to
address these limitations, with state-of-the-art deep learning and foundation
models achieving remarkable success in time series forecasting across various
domains. Despite this progress, their application to critical environmental
systems, such as the Everglades, remains underexplored. In this study, we fill
the gap by investigating twelve task-specific models and five time series
foundation models across six categories for a real-world application focused on
water level prediction in the Everglades. Our primary results show that the
foundation model, Chronos, significantly outperforms all other models while the
remaining foundation models exhibit relatively poor performance. Moreover, the
performance of task-specific models varies with the model architectures.
Lastly, we discuss the possible reasons for the varying performance of models.

</details>


### [113] [Evaluating Frontier Models for Stealth and Situational Awareness](https://arxiv.org/abs/2505.01420)
*Mary Phuong,Roland S. Zimmermann,Ziyue Wang,David Lindner,Victoria Krakovna,Sarah Cogan,Allan Dafoe,Lewis Ho,Rohin Shah*

Main category: cs.LG

TL;DR: 论文提出了一套评估AI模型是否具备潜在阴谋行为（scheming）的测试方法，包括对隐蔽性（stealth）和情境意识（situational awareness）的评估。当前前沿模型均未表现出相关能力。


<details>
  <summary>Details</summary>
Motivation: 前沿AI模型可能具备与开发者意图不符的阴谋行为，且难以检测，因此需在部署前排除此类风险。

Method: 设计了16项评估测试，包括5项隐蔽性测试和11项情境意识测试，用于衡量模型是否具备阴谋行为的先决条件。

Result: 当前前沿模型在测试中未表现出隐蔽性或情境意识的危险水平。

Conclusion: 通过此类评估可以构建安全性论证，确保模型不具备阴谋行为的能力。

Abstract: Recent work has demonstrated the plausibility of frontier AI models scheming
-- knowingly and covertly pursuing an objective misaligned with its developer's
intentions. Such behavior could be very hard to detect, and if present in
future advanced systems, could pose severe loss of control risk. It is
therefore important for AI developers to rule out harm from scheming prior to
model deployment. In this paper, we present a suite of scheming reasoning
evaluations measuring two types of reasoning capabilities that we believe are
prerequisites for successful scheming: First, we propose five evaluations of
ability to reason about and circumvent oversight (stealth). Second, we present
eleven evaluations for measuring a model's ability to instrumentally reason
about itself, its environment and its deployment (situational awareness). We
demonstrate how these evaluations can be used as part of a scheming inability
safety case: a model that does not succeed on these evaluations is almost
certainly incapable of causing severe harm via scheming in real deployment. We
run our evaluations on current frontier models and find that none of them show
concerning levels of either situational awareness or stealth.

</details>


### [114] [Computational, Data-Driven, and Physics-Informed Machine Learning Approaches for Microstructure Modeling in Metal Additive Manufacturing](https://arxiv.org/abs/2505.01424)
*D. Patel,R. Sharma,Y. B. Guo*

Main category: cs.LG

TL;DR: 论文探讨了金属增材制造中微观结构预测的挑战，分析了实验、计算和数据驱动方法的优缺点，并强调了物理信息机器学习（PIML）的潜力。


<details>
  <summary>Details</summary>
Motivation: 金属增材制造中快速熔化和凝固动力学导致非平衡微观结构，影响机械性能，但预测其演化仍具挑战性。

Method: 论文评估了实验、计算和数据驱动方法，并重点介绍了结合物理知识与机器学习的混合PIML框架。

Result: PIML方法在准确性、透明度和数据效率方面表现优异，为微观结构建模提供了新方向。

Conclusion: PIML混合方法对实现可预测、可扩展且物理一致的微观结构建模至关重要，有助于高性能增材制造组件的可靠生产。

Abstract: Metal additive manufacturing enables unprecedented design freedom and the
production of customized, complex components. However, the rapid melting and
solidification dynamics inherent to metal AM processes generate heterogeneous,
non-equilibrium microstructures that significantly impact mechanical properties
and subsequent functionality. Predicting microstructure and its evolution
across spatial and temporal scales remains a central challenge for process
optimization and defect mitigation. While conventional experimental techniques
and physics-based simulations provide a physical foundation and valuable
insights, they face critical limitations. In contrast, data-driven machine
learning offers an alternative prediction approach and powerful pattern
recognition but often operate as black-box, lacking generalizability and
physical consistency. To overcome these limitations, physics-informed machine
learning, including physics-informed neural networks, has emerged as a
promising paradigm by embedding governing physical laws into neural network
architectures, thereby enhancing accuracy, transparency, data efficiency, and
extrapolation capabilities. This work presents a comprehensive evaluation of
modeling strategies for microstructure prediction in metal AM. The strengths
and limitations of experimental, computational, and data-driven methods are
analyzed in depth, and highlight recent advances in hybrid PIML frameworks that
integrate physical knowledge with ML. Key challenges, such as data scarcity,
multi-scale coupling, and uncertainty quantification, are discussed alongside
future directions. Ultimately, this assessment underscores the importance of
PIML-based hybrid approaches in enabling predictive, scalable, and physically
consistent microstructure modeling for site-specific, microstructure-aware
process control and the reliable production of high-performance AM components.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [115] [Multivariate Conformal Selection](https://arxiv.org/abs/2505.00917)
*Tian Bai,Yue Zhao,Xiang Yu,Archer Y. Yang*

Main category: stat.ME

TL;DR: 论文提出了多变量保形选择（mCS），扩展了传统保形选择方法，适用于多变量响应场景，并通过实验验证了其在高维选择任务中的优越性。


<details>
  <summary>Details</summary>
Motivation: 传统保形选择（CS）仅适用于单变量响应和标量标准，无法满足多变量场景的需求，因此需要一种新的方法来解决这一问题。

Method: 提出了mCS方法，引入区域单调性和多变量非保形分数，构建保形p值，以控制有限样本的假发现率（FDR）。包括两种变体：基于距离的mCS-dist和通过可微分优化学习最优分数的mCS-learn。

Result: 实验表明，mCS在保持FDR控制的同时显著提高了选择能力，适用于多变量选择任务。

Conclusion: mCS是一个强大的多变量选择框架，扩展了传统CS方法，并在实际应用中表现出色。

Abstract: Selecting high-quality candidates from large datasets is critical in
applications such as drug discovery, precision medicine, and alignment of large
language models (LLMs). While Conformal Selection (CS) provides rigorous
uncertainty quantification, it is limited to univariate responses and scalar
criteria. To address this issue, we propose Multivariate Conformal Selection
(mCS), a generalization of CS designed for multivariate response settings. Our
method introduces regional monotonicity and employs multivariate nonconformity
scores to construct conformal p-values, enabling finite-sample False Discovery
Rate (FDR) control. We present two variants: mCS-dist, using distance-based
scores, and mCS-learn, which learns optimal scores via differentiable
optimization. Experiments on simulated and real-world datasets demonstrate that
mCS significantly improves selection power while maintaining FDR control,
establishing it as a robust framework for multivariate selection tasks.

</details>


### [116] [Q-Learning with Clustered-SMART (cSMART) Data: Examining Moderators in the Construction of Clustered Adaptive Interventions](https://arxiv.org/abs/2505.00822)
*Yao Song,Kelly Speth,Amy Kilbourne,Andrew Quanbeck,Daniel Almirall,Lu Wang*

Main category: stat.ME

TL;DR: 该论文提出了一种基于cSMART数据的聚类Q学习框架，用于评估候选定制变量在构建最优cAI中的效用，并通过模拟验证了方法的性能。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决在构建聚类自适应干预（cAI）时，如何可靠评估候选定制变量的因果效应调节作用，尤其是在存在非规律性挑战的情况下。

Method: 方法包括引入聚类Q学习框架和M-out-of-N聚类自助法，用于构建置信区间以评估因果效应调节参数。

Result: 模拟结果表明，该方法在不同非规律性条件下表现良好，并能有效评估候选定制变量的效用。

Conclusion: 结论是该框架可用于构建优化的cAI，并在实际数据集（如ADEPT）中验证了其应用价值。

Abstract: A clustered adaptive intervention (cAI) is a pre-specified sequence of
decision rules that guides practitioners on how best - and based on which
measures - to tailor cluster-level intervention to improve outcomes at the
level of individuals within the clusters. A clustered sequential multiple
assignment randomized trial (cSMART) is a type of trial that is used to inform
the empirical development of a cAI. The most common type of secondary aim in a
cSMART focuses on assessing causal effect moderation by candidate tailoring
variables. We introduce a clustered Q-learning framework with the M-out-of-N
Cluster Bootstrap using data from a cSMART to evaluate whether a set of
candidate tailoring variables may be useful in defining an optimal cAI. This
approach could construct confidence intervals (CI) with near-nominal coverage
to assess parameters indexing the causal effect moderation function.
Specifically, it allows reliable inferences concerning the utility of candidate
tailoring variables in constructing a cAI that maximizes a mean end-of-study
outcome even when "non-regularity", a well-known challenge exists. Simulations
demonstrate the numerical performance of the proposed method across varying
non-regularity conditions and investigate the impact of varying number of
clusters and intra-cluster correlation coefficient on CI coverage. Methods are
applied on ADEPT dataset to inform the construction of a clinic-level cAI for
improving evidence-based practice in treating mood disorders.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [117] [Zoomer: Adaptive Image Focus Optimization for Black-box MLLM](https://arxiv.org/abs/2505.00742)
*Jiaxu Qian,Chendong Wang,Yifan Yang,Chaoyun Zhang,Huiqiang Jiang,Xufang Luo,Yu Kang,Qingwei Lin,Anlan Zhang,Shiqi Jiang,Ting Cao,Tianjun Mao,Suman Banerjee,Guyue Liu,Saravan Rajmohan,Dongmei Zhang,Yuqing Yang,Qi Zhang,Lili Qiu*

Main category: cs.CV

TL;DR: 论文提出了一种名为\SysName的新型视觉提示机制，旨在提升多模态大语言模型（MLLMs）在视觉任务中的性能，同时保留关键视觉细节。


<details>
  <summary>Details</summary>
Motivation: 现有的MLLMs在精确对象识别和视觉细节处理上表现不佳，且严格的标记限制导致关键信息丢失。

Method: \SysName包含三种创新方法：动态突出相关图像区域的提示感知策略、保持对象完整性的空间保留编排模式，以及平衡全局上下文与关键视觉细节的预算感知提示方法。

Result: 在多个数据集上的评估显示，\SysName显著优于基线方法，准确率提升高达26.9%，同时显著减少标记消耗。

Conclusion: \SysName有效解决了MLLMs在视觉任务中的局限性，为未来研究提供了新方向。

Abstract: Recent advancements in multimodal large language models (MLLMs) have
broadened the scope of vision-language tasks, excelling in applications like
image captioning and interactive question-answering. However, these models
struggle with accurately processing visual data, particularly in tasks
requiring precise object recognition and fine visual details. Stringent token
limits often result in the omission of critical information, hampering
performance. To address these limitations, we introduce \SysName, a novel
visual prompting mechanism designed to enhance MLLM performance while
preserving essential visual details within token limits. \SysName features
three key innovations: a prompt-aware strategy that dynamically highlights
relevant image regions, a spatial-preserving orchestration schema that
maintains object integrity, and a budget-aware prompting method that balances
global context with crucial visual details. Comprehensive evaluations across
multiple datasets demonstrate that \SysName consistently outperforms baseline
methods, achieving up to a $26.9\%$ improvement in accuracy while significantly
reducing token consumption.

</details>


### [118] [DARTer: Dynamic Adaptive Representation Tracker for Nighttime UAV Tracking](https://arxiv.org/abs/2505.00752)
*Xuzhao Li,Xuchen Li,Shiyu Hu*

Main category: cs.CV

TL;DR: DARTer是一种用于夜间无人机跟踪的端到端框架，通过动态特征融合和自适应激活机制，显著提升了跟踪性能和效率。


<details>
  <summary>Details</summary>
Motivation: 夜间无人机跟踪面临极端光照变化和视角变化的挑战，现有方法计算成本高或未能充分利用动态特征。

Method: DARTer采用动态特征混合器（DFB）融合多视角特征，并通过动态特征激活器（DFA）自适应激活Vision Transformer层以减少冗余计算。

Result: 在多个夜间无人机跟踪基准测试中，DARTer表现优于现有方法，平衡了准确性和效率。

Conclusion: DARTer为实际夜间无人机跟踪应用提供了高效且准确的解决方案。

Abstract: Nighttime UAV tracking presents significant challenges due to extreme
illumination variations and viewpoint changes, which severely degrade tracking
performance. Existing approaches either rely on light enhancers with high
computational costs or introduce redundant domain adaptation mechanisms,
failing to fully utilize the dynamic features in varying perspectives. To
address these issues, we propose \textbf{DARTer} (\textbf{D}ynamic
\textbf{A}daptive \textbf{R}epresentation \textbf{T}racker), an end-to-end
tracking framework designed for nighttime UAV scenarios. DARTer leverages a
Dynamic Feature Blender (DFB) to effectively fuse multi-perspective nighttime
features from static and dynamic templates, enhancing representation
robustness. Meanwhile, a Dynamic Feature Activator (DFA) adaptively activates
Vision Transformer layers based on extracted features, significantly improving
efficiency by reducing redundant computations. Our model eliminates the need
for complex multi-task loss functions, enabling a streamlined training process.
Extensive experiments on multiple nighttime UAV tracking benchmarks demonstrate
the superiority of DARTer over state-of-the-art trackers. These results confirm
that DARTer effectively balances tracking accuracy and efficiency, making it a
promising solution for real-world nighttime UAV tracking applications.

</details>


### [119] [Multi-Modal Language Models as Text-to-Image Model Evaluators](https://arxiv.org/abs/2505.00759)
*Jiahui Chen,Candace Ross,Reyhane Askari-Hemmat,Koustuv Sinha,Melissa Hall,Michal Drozdzal,Adriana Romero-Soriano*

Main category: cs.CV

TL;DR: MT2IE是一种基于多模态大语言模型（MLLMs）的文本到图像（T2I）生成模型评估框架，通过动态生成提示词评估模型性能，显著减少所需提示词数量，且评分与人类判断相关性更高。


<details>
  <summary>Details</summary>
Motivation: 随着T2I生成模型的持续改进，依赖静态数据集的自动评估基准逐渐过时，需要新的评估方法。

Method: 提出MT2IE框架，利用MLLMs动态生成提示词，评估T2I模型的提示-生成一致性和图像美学质量。

Result: MT2IE仅需1/80的提示词数量即可达到与现有基准相同的模型排名效果，且其评分与人类判断相关性更高。

Conclusion: MT2IE为T2I模型提供了一种高效、动态的评估方法，优于传统静态基准。

Abstract: The steady improvements of text-to-image (T2I) generative models lead to slow
deprecation of automatic evaluation benchmarks that rely on static datasets,
motivating researchers to seek alternative ways to evaluate the T2I progress.
In this paper, we explore the potential of multi-modal large language models
(MLLMs) as evaluator agents that interact with a T2I model, with the objective
of assessing prompt-generation consistency and image aesthetics. We present
Multimodal Text-to-Image Eval (MT2IE), an evaluation framework that iteratively
generates prompts for evaluation, scores generated images and matches T2I
evaluation of existing benchmarks with a fraction of the prompts used in
existing static benchmarks. Moreover, we show that MT2IE's prompt-generation
consistency scores have higher correlation with human judgment than scores
previously introduced in the literature. MT2IE generates prompts that are
efficient at probing T2I model performance, producing the same relative T2I
model rankings as existing benchmarks while using only 1/80th the number of
prompts for evaluation.

</details>


### [120] [P2P-Insole: Human Pose Estimation Using Foot Pressure Distribution and Motion Sensors](https://arxiv.org/abs/2505.00755)
*Atsuya Watanabe,Ratna Aisuwarya,Lei Jing*

Main category: cs.CV

TL;DR: P2P-Insole是一种低成本方法，利用集成IMU的鞋垫传感器估计和可视化3D人体骨骼数据，适用于大规模生产。


<details>
  <summary>Details</summary>
Motivation: 现有商业解决方案成本高，P2P-Insole旨在提供一种低成本、轻量级且隐私友好的替代方案。

Method: 使用鞋垫压力分布、加速度和旋转数据，结合Transformer模型提取时间特征，并利用多模态信息提高运动模式识别精度。

Result: 实验证明该方法在多种姿态估计任务中表现稳健，误差指标显示其高准确性。

Conclusion: P2P-Insole为康复、伤害预防和健康监测提供了低成本实用解决方案，并可通过传感器优化和数据集扩展进一步改进。

Abstract: This work presents P2P-Insole, a low-cost approach for estimating and
visualizing 3D human skeletal data using insole-type sensors integrated with
IMUs. Each insole, fabricated with e-textile garment techniques, costs under
USD 1, making it significantly cheaper than commercial alternatives and ideal
for large-scale production. Our approach uses foot pressure distribution,
acceleration, and rotation data to overcome limitations, providing a
lightweight, minimally intrusive, and privacy-aware solution. The system
employs a Transformer model for efficient temporal feature extraction, enriched
by first and second derivatives in the input stream. Including multimodal
information, such as accelerometers and rotational measurements, improves the
accuracy of complex motion pattern recognition. These facts are demonstrated
experimentally, while error metrics show the robustness of the approach in
various posture estimation tasks. This work could be the foundation for a
low-cost, practical application in rehabilitation, injury prevention, and
health monitoring while enabling further development through sensor
optimization and expanded datasets.

</details>


### [121] [Efficient On-Chip Implementation of 4D Radar-Based 3D Object Detection on Hailo-8L](https://arxiv.org/abs/2505.00757)
*Woong-Chan Byun,Dong-Hee Paek,Seung-Hyun Song,Seung-Hyun Kong*

Main category: cs.CV

TL;DR: 本文提出了一种在Hailo-8L AI加速器上实现4D雷达3D目标检测的芯片级方法，通过张量变换解决5D输入与4D支持的兼容性问题，实现了实时处理和高精度。


<details>
  <summary>Details</summary>
Motivation: 4D雷达在自动驾驶中具有鲁棒性，但需在低功耗嵌入式环境中实现实时处理。

Method: 引入张量变换方法，将5D输入重塑为4D格式，兼容Hailo-8L加速器。

Result: 系统达到46.47% AP_3D和52.75% AP_BEV，推理速度13.76 Hz。

Conclusion: 该方法证明了4D雷达感知技术在自动驾驶系统中的实用性。

Abstract: 4D radar has attracted attention in autonomous driving due to its ability to
enable robust 3D object detection even under adverse weather conditions. To
practically deploy such technologies, it is essential to achieve real-time
processing within low-power embedded environments. Addressing this, we present
the first on-chip implementation of a 4D radar-based 3D object detection model
on the Hailo-8L AI accelerator. Although conventional 3D convolutional neural
network (CNN) architectures require 5D inputs, the Hailo-8L only supports 4D
tensors, posing a significant challenge. To overcome this limitation, we
introduce a tensor transformation method that reshapes 5D inputs into 4D
formats during the compilation process, enabling direct deployment without
altering the model structure. The proposed system achieves 46.47% AP_3D and
52.75% AP_BEV, maintaining comparable accuracy to GPU-based models while
achieving an inference speed of 13.76 Hz. These results demonstrate the
applicability of 4D radar-based perception technologies to autonomous driving
systems.

</details>


### [122] [Evaluating Vision Language Model Adaptations for Radiology Report Generation in Low-Resource Languages](https://arxiv.org/abs/2505.01096)
*Marco Salmè,Rosa Sicilia,Paolo Soda,Valerio Guarrasi*

Main category: cs.CV

TL;DR: 研究评估了指令调优的视觉语言模型（VLMs）在低资源语言（意大利语、德语、西班牙语）中生成放射学报告的性能，发现语言和领域特定训练对提升报告质量至关重要。


<details>
  <summary>Details</summary>
Motivation: 医疗领域人工智能的应用需要解决低资源语言中放射学报告生成的准确性和上下文相关性挑战。

Method: 采用LLaVA架构，系统评估了预训练模型在通用、领域特定和低资源语言特定数据集上的表现，并分析了不同适应性方法。

Result: 语言特定模型表现最佳，医学术语微调进一步提升了性能，温度参数对报告连贯性有显著影响。

Conclusion: 语言和领域特定训练对提升多语言环境下放射学报告的质量和准确性至关重要，为未来模型调优和语言适应性研究提供了方向。

Abstract: The integration of artificial intelligence in healthcare has opened new
horizons for improving medical diagnostics and patient care. However,
challenges persist in developing systems capable of generating accurate and
contextually relevant radiology reports, particularly in low-resource
languages. In this study, we present a comprehensive benchmark to evaluate the
performance of instruction-tuned Vision-Language Models (VLMs) in the
specialized task of radiology report generation across three low-resource
languages: Italian, German, and Spanish. Employing the LLaVA architectural
framework, we conducted a systematic evaluation of pre-trained models utilizing
general datasets, domain-specific datasets, and low-resource language-specific
datasets. In light of the unavailability of models that possess prior knowledge
of both the medical domain and low-resource languages, we analyzed various
adaptations to determine the most effective approach for these contexts. The
results revealed that language-specific models substantially outperformed both
general and domain-specific models in generating radiology reports, emphasizing
the critical role of linguistic adaptation. Additionally, models fine-tuned
with medical terminology exhibited enhanced performance across all languages
compared to models with generic knowledge, highlighting the importance of
domain-specific training. We also explored the influence of the temperature
parameter on the coherence of report generation, providing insights for optimal
model settings. Our findings highlight the importance of tailored language and
domain-specific training for improving the quality and accuracy of radiological
reports in multilingual settings. This research not only advances our
understanding of VLMs adaptability in healthcare but also points to significant
avenues for future investigations into model tuning and language-specific
adaptations.

</details>


### [123] [CDFormer: Cross-Domain Few-Shot Object Detection Transformer Against Feature Confusion](https://arxiv.org/abs/2505.00938)
*Boyuan Meng,Xiaohan Zhang,Peilin Li,Zhe Wu,Yiming Li,Wenkai Zhao,Beinan Yu,Hui-Liang Shen*

Main category: cs.CV

TL;DR: CDFormer是一种针对跨域少样本目标检测（CD-FSOD）的Transformer模型，通过OBD和OOD模块解决特征混淆问题，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 跨域少样本目标检测中，特征混淆（如物体-背景混淆和物体-物体混淆）是一个主要挑战，需要有效解决。

Method: 提出CDFormer，包含两个模块：OBD（物体-背景区分）和OOD（物体-物体区分），分别通过可学习的背景标记和增强类别区分来解决问题。

Result: 实验表明，CDFormer在1/5/10 shot设置下分别提升了12.9%、11.0%和10.4%的mAP，优于现有方法。

Conclusion: CDFormer通过解决特征混淆问题，显著提升了跨域少样本目标检测的性能。

Abstract: Cross-domain few-shot object detection (CD-FSOD) aims to detect novel objects
across different domains with limited class instances. Feature confusion,
including object-background confusion and object-object confusion, presents
significant challenges in both cross-domain and few-shot settings. In this
work, we introduce CDFormer, a cross-domain few-shot object detection
transformer against feature confusion, to address these challenges. The method
specifically tackles feature confusion through two key modules:
object-background distinguishing (OBD) and object-object distinguishing (OOD).
The OBD module leverages a learnable background token to differentiate between
objects and background, while the OOD module enhances the distinction between
objects of different classes. Experimental results demonstrate that CDFormer
outperforms previous state-of-the-art approaches, achieving 12.9% mAP, 11.0%
mAP, and 10.4% mAP improvements under the 1/5/10 shot settings, respectively,
when fine-tuned.

</details>


### [124] [Fine-Tuning Without Forgetting: Adaptation of YOLOv8 Preserves COCO Performance](https://arxiv.org/abs/2505.01016)
*Vishal Gandhi,Sagar Gandhi*

Main category: cs.CV

TL;DR: 研究表明，深度微调（解冻至第10层）在细粒度水果检测任务中显著提升性能（+10% mAP50），同时对原始COCO任务性能影响极小（<0.1% mAP差异）。


<details>
  <summary>Details</summary>
Motivation: 探索预训练目标检测器在细粒度领域中的最佳微调深度，以优化任务性能同时避免灾难性遗忘。

Method: 通过逐步解冻YOLOv8n模型的骨干层（解冻点分别为22、15、10层），在细粒度水果检测数据集上进行训练，并使用双头评估架构验证性能。

Result: 深度微调（解冻至第10层）显著提升细粒度任务性能，且对原始任务性能影响可忽略。

Conclusion: 中后期骨干特征的微调对细粒度任务高度有效，且不会导致灾难性遗忘，支持在复杂领域或性能优先场景中采用深度微调策略。

Abstract: The success of large pre-trained object detectors hinges on their
adaptability to diverse downstream tasks. While fine-tuning is the standard
adaptation method, specializing these models for challenging fine-grained
domains necessitates careful consideration of feature granularity. The critical
question remains: how deeply should the pre-trained backbone be fine-tuned to
optimize for the specialized task without incurring catastrophic forgetting of
the original general capabilities? Addressing this, we present a systematic
empirical study evaluating the impact of fine-tuning depth. We adapt a standard
YOLOv8n model to a custom, fine-grained fruit detection dataset by
progressively unfreezing backbone layers (freeze points at layers 22, 15, and
10) and training. Performance was rigorously evaluated on both the target fruit
dataset and, using a dual-head evaluation architecture, on the original COCO
validation set. Our results demonstrate unequivocally that deeper fine-tuning
(unfreezing down to layer 10) yields substantial performance gains (e.g., +10\%
absolute mAP50) on the fine-grained fruit task compared to only training the
head. Strikingly, this significant adaptation and specialization resulted in
negligible performance degradation (<0.1\% absolute mAP difference) on the COCO
benchmark across all tested freeze levels. We conclude that adapting
mid-to-late backbone features is highly effective for fine-grained
specialization. Critically, our results demonstrate this adaptation can be
achieved without the commonly expected penalty of catastrophic forgetting,
presenting a compelling case for exploring deeper fine-tuning strategies,
particularly when targeting complex domains or when maximizing specialized
performance is paramount.

</details>


### [125] [Any-to-Any Vision-Language Model for Multimodal X-ray Imaging and Radiological Report Generation](https://arxiv.org/abs/2505.01091)
*Daniele Molino,Francesco di Feola,Linlin Shen,Paolo Soda,Valerio Guarrasi*

Main category: cs.CV

TL;DR: 提出一种针对多模态医学数据生成的框架，生成高质量胸部X光片及临床报告，性能优于通用模型，并在下游疾病分类任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 医学数据复杂且需高临床准确性，通用生成模型难以满足需求。

Method: 基于MIMIC-CXR数据集设计多模态医学数据生成框架，生成多视角胸部X光片及报告。

Result: 生成数据在FID和BLEU分数上表现优异，下游疾病分类任务性能接近或优于真实数据。

Conclusion: 领域特定适配提升生成模型在临床应用中的价值，为未来医学数据生成研究铺路。

Abstract: Generative models have revolutionized Artificial Intelligence (AI),
particularly in multimodal applications. However, adapting these models to the
medical domain poses unique challenges due to the complexity of medical data
and the stringent need for clinical accuracy. In this work, we introduce a
framework specifically designed for multimodal medical data generation. By
enabling the generation of multi-view chest X-rays and their associated
clinical report, it bridges the gap between general-purpose vision-language
models and the specialized requirements of healthcare. Leveraging the MIMIC-CXR
dataset, the proposed framework shows superior performance in generating
high-fidelity images and semantically coherent reports. Our quantitative
evaluation reveals significant results in terms of FID and BLEU scores,
showcasing the quality of the generated data. Notably, our framework achieves
comparable or even superior performance compared to real data on downstream
disease classification tasks, underlining its potential as a tool for medical
research and diagnostics. This study highlights the importance of
domain-specific adaptations in enhancing the relevance and utility of
generative models for clinical applications, paving the way for future
advancements in synthetic multimodal medical data generation.

</details>


### [126] [Self-Supervision Enhances Instance-based Multiple Instance Learning Methods in Digital Pathology: A Benchmark Study](https://arxiv.org/abs/2505.01109)
*Ali Mammadov,Loic Le Folgoc,Julien Adam,Anne Buronfosse,Gilles Hayem,Guillaume Hocquet,Pietro Gori*

Main category: cs.CV

TL;DR: 论文通过710次实验比较了多种MIL方法，发现使用高质量自监督学习特征提取器时，简单的基于实例的MIL方法性能优于复杂的基于嵌入的MIL方法，且更具可解释性。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于验证在自监督学习特征提取器质量提升的背景下，基于实例的MIL方法是否仍优于基于嵌入的MIL方法。

Method: 方法包括在4个数据集上进行710次实验，比较10种MIL策略、6种自监督学习方法、4种基础模型及多种病理学适配技术，并引入4种新的基于实例的MIL方法。

Result: 结果表明，使用高质量自监督学习特征提取器时，简单的基于实例的MIL方法性能与复杂的基于嵌入的MIL方法相当或更好，并在BRACS和Camelyon16数据集上达到新的SOTA。

Conclusion: 结论指出，应更多关注适配良好的自监督学习方法，而非复杂的基于嵌入的MIL方法，因为前者更具可解释性和实用性。

Abstract: Multiple Instance Learning (MIL) has emerged as the best solution for Whole
Slide Image (WSI) classification. It consists of dividing each slide into
patches, which are treated as a bag of instances labeled with a global label.
MIL includes two main approaches: instance-based and embedding-based. In the
former, each patch is classified independently, and then the patch scores are
aggregated to predict the bag label. In the latter, bag classification is
performed after aggregating patch embeddings. Even if instance-based methods
are naturally more interpretable, embedding-based MILs have usually been
preferred in the past due to their robustness to poor feature extractors.
However, recently, the quality of feature embeddings has drastically increased
using self-supervised learning (SSL). Nevertheless, many authors continue to
endorse the superiority of embedding-based MIL. To investigate this further, we
conduct 710 experiments across 4 datasets, comparing 10 MIL strategies, 6
self-supervised methods with 4 backbones, 4 foundation models, and various
pathology-adapted techniques. Furthermore, we introduce 4 instance-based MIL
methods never used before in the pathology domain. Through these extensive
experiments, we show that with a good SSL feature extractor, simple
instance-based MILs, with very few parameters, obtain similar or better
performance than complex, state-of-the-art (SOTA) embedding-based MIL methods,
setting new SOTA results on the BRACS and Camelyon16 datasets. Since simple
instance-based MIL methods are naturally more interpretable and explainable to
clinicians, our results suggest that more effort should be put into
well-adapted SSL methods for WSI rather than into complex embedding-based MIL
methods.

</details>


### [127] [TSTMotion: Training-free Scene-awarenText-to-motion Generation](https://arxiv.org/abs/2505.01182)
*Ziyan Guo,Haoxuan Qu,Hossein Rahmani,Dewen Soh,Ping Hu,Qiuhong Ke,Jun Liu*

Main category: cs.CV

TL;DR: 论文提出了一种无需训练的、场景感知的文本到动作生成框架TSTMotion，通过预训练的空白背景动作生成器实现场景感知能力。


<details>
  <summary>Details</summary>
Motivation: 现有场景感知方法依赖大规模真实动作序列，成本高昂，因此需要一种更高效的方法。

Method: 利用基础模型推理、预测和验证场景感知动作指导，并将其整合到预训练的动作生成器中。

Result: 实验证明该框架高效且具有通用性。

Conclusion: TSTMotion为场景感知文本到动作生成提供了一种低成本、高效的解决方案。

Abstract: Text-to-motion generation has recently garnered significant research
interest, primarily focusing on generating human motion sequences in blank
backgrounds. However, human motions commonly occur within diverse 3D scenes,
which has prompted exploration into scene-aware text-to-motion generation
methods. Yet, existing scene-aware methods often rely on large-scale
ground-truth motion sequences in diverse 3D scenes, which poses practical
challenges due to the expensive cost. To mitigate this challenge, we are the
first to propose a \textbf{T}raining-free \textbf{S}cene-aware
\textbf{T}ext-to-\textbf{Motion} framework, dubbed as \textbf{TSTMotion}, that
efficiently empowers pre-trained blank-background motion generators with the
scene-aware capability. Specifically, conditioned on the given 3D scene and
text description, we adopt foundation models together to reason, predict and
validate a scene-aware motion guidance. Then, the motion guidance is
incorporated into the blank-background motion generators with two
modifications, resulting in scene-aware text-driven motion sequences. Extensive
experiments demonstrate the efficacy and generalizability of our proposed
framework. We release our code in \href{https://tstmotion.github.io/}{Project
Page}.

</details>


### [128] [Detection and Classification of Diseases in Multi-Crop Leaves using LSTM and CNN Models](https://arxiv.org/abs/2505.00741)
*Srinivas Kanakala,Sneha Ningappa*

Main category: cs.CV

TL;DR: 该研究使用CNN和LSTM模型对植物叶片疾病进行分类，CNN模型在验证集上达到96.4%的准确率，表明其在农业监测中具有实用价值。


<details>
  <summary>Details</summary>
Motivation: 植物病害严重影响农业产量和食品质量，早期检测和分类对减少损失和改进作物管理至关重要。

Method: 采用CNN和LSTM模型，利用包含70,295张训练图像和17,572张验证图像的数据集进行分类。CNN使用Adam优化器和分类交叉熵损失函数。

Result: CNN模型训练准确率为99.1%，验证准确率为96.4%；LSTM模型验证准确率为93.43%。性能通过精确率、召回率、F1分数和混淆矩阵评估。

Conclusion: 深度学习模型（尤其是CNN）为植物病害分类提供了准确且可扩展的解决方案，适用于农业监测实践。

Abstract: Plant diseases pose a serious challenge to agriculture by reducing crop yield
and affecting food quality. Early detection and classification of these
diseases are essential for minimising losses and improving crop management
practices. This study applies Convolutional Neural Networks (CNN) and Long
Short-Term Memory (LSTM) models to classify plant leaf diseases using a dataset
containing 70,295 training images and 17,572 validation images across 38
disease classes. The CNN model was trained using the Adam optimiser with a
learning rate of 0.0001 and categorical cross-entropy as the loss function.
After 10 training epochs, the model achieved a training accuracy of 99.1% and a
validation accuracy of 96.4%. The LSTM model reached a validation accuracy of
93.43%. Performance was evaluated using precision, recall, F1-score, and
confusion matrix, confirming the reliability of the CNN-based approach. The
results suggest that deep learning models, particularly CNN, enable an
effective solution for accurate and scalable plant disease classification,
supporting practical applications in agricultural monitoring.

</details>


### [129] [Responsive DNN Adaptation for Video Analytics against Environment Shift via Hierarchical Mobile-Cloud Collaborations](https://arxiv.org/abs/2505.00745)
*Maozhe Zhao,Shengzhong Liu,Fan Wu,Guihai Chen*

Main category: cs.CV

TL;DR: MOCHA是一个优化移动视频分析系统中模型适应响应速度的框架，通过移动端与云端的层次协作，减少延迟并提升准确性。


<details>
  <summary>Details</summary>
Motivation: 现有模型适应框架以云端为中心，性能下降且响应延迟，无法满足环境变化的需求。

Method: MOCHA通过设备端模型重用、快速微调、结构化分类索引和本地模型缓存优化响应速度。

Result: 在三个DNN任务中，MOCHA将模型准确性提升6.8%，响应延迟减少35.5倍，重训练时间缩短3倍。

Conclusion: MOCHA通过移动与云端的协作，显著提升了模型适应效率和性能。

Abstract: Mobile video analysis systems often encounter various deploying environments,
where environment shifts present greater demands for responsiveness in
adaptations of deployed "expert DNN models". Existing model adaptation
frameworks primarily operate in a cloud-centric way, exhibiting degraded
performance during adaptation and delayed reactions to environment shifts.
Instead, this paper proposes MOCHA, a novel framework optimizing the
responsiveness of continuous model adaptation through hierarchical
collaborations between mobile and cloud resources. Specifically, MOCHA (1)
reduces adaptation response delays by performing on-device model reuse and fast
fine-tuning before requesting cloud model retrieval and end-to-end retraining;
(2) accelerates history expert model retrieval by organizing them into a
structured taxonomy utilizing domain semantics analyzed by a cloud foundation
model as indices; (3) enables efficient local model reuse by maintaining
onboard expert model caches for frequent scenes, which proactively prefetch
model weights from the cloud model database. Extensive evaluations with
real-world videos on three DNN tasks show MOCHA improves the model accuracy
during adaptation by up to 6.8% while saving the response delay and retraining
time by up to 35.5x and 3.0x respectively.

</details>


### [130] [Multimodal Doctor-in-the-Loop: A Clinically-Guided Explainable Framework for Predicting Pathological Response in Non-Small Cell Lung Cancer](https://arxiv.org/abs/2505.01390)
*Alice Natalina Caragliano,Claudia Tacconi,Carlo Greco,Lorenzo Nibid,Edy Ippolito,Michele Fiore,Giuseppe Perrone,Sara Ramella,Paolo Soda,Valerio Guarrasi*

Main category: cs.CV

TL;DR: 提出了一种结合多模态深度学习和可解释AI技术的新方法，用于预测非小细胞肺癌患者新辅助治疗的病理反应。


<details>
  <summary>Details</summary>
Motivation: 现有放射组学和单模态深度学习方法存在局限性，需要更高效的多模态数据整合方法。

Method: 采用中间融合策略整合影像和临床数据，并结合医生在环方法嵌入临床知识。

Result: 提高了预测准确性和可解释性，为临床数据整合提供了优化策略。

Conclusion: 该方法为临床应用中多模态数据整合提供了有效且可解释的解决方案。

Abstract: This study proposes a novel approach combining Multimodal Deep Learning with
intrinsic eXplainable Artificial Intelligence techniques to predict
pathological response in non-small cell lung cancer patients undergoing
neoadjuvant therapy. Due to the limitations of existing radiomics and unimodal
deep learning approaches, we introduce an intermediate fusion strategy that
integrates imaging and clinical data, enabling efficient interaction between
data modalities. The proposed Multimodal Doctor-in-the-Loop method further
enhances clinical relevance by embedding clinicians' domain knowledge directly
into the training process, guiding the model's focus gradually from broader
lung regions to specific lesions. Results demonstrate improved predictive
accuracy and explainability, providing insights into optimal data integration
strategies for clinical applications.

</details>


### [131] [Transferable Adversarial Attacks on Black-Box Vision-Language Models](https://arxiv.org/abs/2505.01050)
*Kai Hu,Weichen Yu,Li Zhang,Alexander Robey,Andy Zou,Chengming Xu,Haoqi Hu,Matt Fredrikson*

Main category: cs.CV

TL;DR: 研究表明，针对开源模型设计的对抗性攻击可以转移到专有的视觉大语言模型（VLLMs）上，导致模型产生攻击者选择的错误输出。


<details>
  <summary>Details</summary>
Motivation: 探索视觉大语言模型（VLLMs）在对抗性攻击下的脆弱性，尤其是在专有模型上的转移效果。

Method: 通过生成目标对抗样本和通用扰动，测试其在多任务（如物体识别、视觉问答和图像描述）中的转移效果。

Result: 实验证明，对抗性攻击能有效转移到主流专有VLLMs（如GPT-4o、Claude和Gemini），导致模型产生攻击者预期的错误输出。

Conclusion: 当前VLLMs普遍存在对抗性攻击的漏洞，亟需开发鲁棒的防御机制以确保其安全部署。

Abstract: Vision Large Language Models (VLLMs) are increasingly deployed to offer
advanced capabilities on inputs comprising both text and images. While prior
research has shown that adversarial attacks can transfer from open-source to
proprietary black-box models in text-only and vision-only contexts, the extent
and effectiveness of such vulnerabilities remain underexplored for VLLMs. We
present a comprehensive analysis demonstrating that targeted adversarial
examples are highly transferable to widely-used proprietary VLLMs such as
GPT-4o, Claude, and Gemini. We show that attackers can craft perturbations to
induce specific attacker-chosen interpretations of visual information, such as
misinterpreting hazardous content as safe, overlooking sensitive or restricted
material, or generating detailed incorrect responses aligned with the
attacker's intent. Furthermore, we discover that universal perturbations --
modifications applicable to a wide set of images -- can consistently induce
these misinterpretations across multiple proprietary VLLMs. Our experimental
results on object recognition, visual question answering, and image captioning
show that this vulnerability is common across current state-of-the-art models,
and underscore an urgent need for robust mitigations to ensure the safe and
secure deployment of VLLMs.

</details>


### [132] [Efficient Vocabulary-Free Fine-Grained Visual Recognition in the Age of Multimodal LLMs](https://arxiv.org/abs/2505.01064)
*Hari Chandana Kuchibhotla,Sai Srinivas Kancheti,Abbavaram Gowtham Reddy,Vineeth N Balasubramanian*

Main category: cs.CV

TL;DR: 论文提出了一种名为NeaR的新方法，用于解决词汇无关细粒度视觉识别（VF-FGVR）问题，通过利用多模态大语言模型（MLLM）生成的标签微调下游CLIP模型。


<details>
  <summary>Details</summary>
Motivation: 在缺乏标注数据的领域（如医学影像），传统细粒度视觉识别方法无法依赖预定义的训练标签，而直接使用MLLM进行预测成本高且不实用。

Method: 提出NeaR方法，通过MLLM为小规模未标注训练集生成标签，构建弱监督数据集，并微调CLIP模型以处理标签中的噪声和不确定性。

Result: NeaR为高效的VF-FGVR设立了新基准，能够处理MLLM生成标签的噪声和开放性问题。

Conclusion: NeaR是一种高效且实用的解决方案，适用于缺乏标注数据的细粒度视觉识别任务。

Abstract: Fine-grained Visual Recognition (FGVR) involves distinguishing between
visually similar categories, which is inherently challenging due to subtle
inter-class differences and the need for large, expert-annotated datasets. In
domains like medical imaging, such curated datasets are unavailable due to
issues like privacy concerns and high annotation costs. In such scenarios
lacking labeled data, an FGVR model cannot rely on a predefined set of training
labels, and hence has an unconstrained output space for predictions. We refer
to this task as Vocabulary-Free FGVR (VF-FGVR), where a model must predict
labels from an unconstrained output space without prior label information.
While recent Multimodal Large Language Models (MLLMs) show potential for
VF-FGVR, querying these models for each test input is impractical because of
high costs and prohibitive inference times. To address these limitations, we
introduce \textbf{Nea}rest-Neighbor Label \textbf{R}efinement (NeaR), a novel
approach that fine-tunes a downstream CLIP model using labels generated by an
MLLM. Our approach constructs a weakly supervised dataset from a small,
unlabeled training set, leveraging MLLMs for label generation. NeaR is designed
to handle the noise, stochasticity, and open-endedness inherent in labels
generated by MLLMs, and establishes a new benchmark for efficient VF-FGVR.

</details>


### [133] [Fusing Foveal Fixations Using Linear Retinal Transformations and Bayesian Experimental Design](https://arxiv.org/abs/2505.01249)
*Christopher K. I. Williams*

Main category: cs.CV

TL;DR: 论文提出了一种基于线性下采样的方法，用于融合多个注视点的场景表示，并通过贝叶斯实验设计优化注视点选择。


<details>
  <summary>Details</summary>
Motivation: 解决人类和脊椎动物如何通过多个注视点的高分辨率中心区和低分辨率外围区融合场景表示的问题。

Method: 将视网膜变换表示为高分辨率潜在场景图像的线性下采样，利用已知几何进行精确推断，并基于贝叶斯实验设计选择下一个注视点。

Result: 在Frey人脸和MNIST数据集上的实验验证了模型的有效性。

Conclusion: 提出的方法能够有效融合多注视点信息，并通过贝叶斯优化提升注视点选择效率。

Abstract: Humans (and many vertebrates) face the problem of fusing together multiple
fixations of a scene in order to obtain a representation of the whole, where
each fixation uses a high-resolution fovea and decreasing resolution in the
periphery. In this paper we explicitly represent the retinal transformation of
a fixation as a linear downsampling of a high-resolution latent image of the
scene, exploiting the known geometry. This linear transformation allows us to
carry out exact inference for the latent variables in factor analysis (FA) and
mixtures of FA models of the scene. Further, this allows us to formulate and
solve the choice of "where to look next" as a Bayesian experimental design
problem using the Expected Information Gain criterion. Experiments on the Frey
faces and MNIST datasets demonstrate the effectiveness of our models.

</details>


### [134] [CAMELTrack: Context-Aware Multi-cue ExpLoitation for Online Multi-Object Tracking](https://arxiv.org/abs/2505.01257)
*Vladimir Somers,Baptiste Standaert,Victor Joos,Alexandre Alahi,Christophe De Vleeschouwer*

Main category: cs.CV

TL;DR: CAMEL是一种新型的关联模块，通过数据学习关联策略，摆脱人工启发式规则，同时保持模块化设计。


<details>
  <summary>Details</summary>
Motivation: 现有的跟踪方法依赖人工规则，难以捕捉复杂跟踪线索间的交互。

Method: CAMEL采用两个基于Transformer的模块和新的关联中心训练方案。

Result: CAMELTrack在多个跟踪基准上达到最先进性能。

Conclusion: CAMEL在保持模块化的同时，通过数据驱动方法提升了跟踪性能。

Abstract: Online multi-object tracking has been recently dominated by
tracking-by-detection (TbD) methods, where recent advances rely on increasingly
sophisticated heuristics for tracklet representation, feature fusion, and
multi-stage matching. The key strength of TbD lies in its modular design,
enabling the integration of specialized off-the-shelf models like motion
predictors and re-identification. However, the extensive usage of human-crafted
rules for temporal associations makes these methods inherently limited in their
ability to capture the complex interplay between various tracking cues. In this
work, we introduce CAMEL, a novel association module for Context-Aware
Multi-Cue ExpLoitation, that learns resilient association strategies directly
from data, breaking free from hand-crafted heuristics while maintaining TbD's
valuable modularity. At its core, CAMEL employs two transformer-based modules
and relies on a novel association-centric training scheme to effectively model
the complex interactions between tracked targets and their various association
cues. Unlike end-to-end detection-by-tracking approaches, our method remains
lightweight and fast to train while being able to leverage external
off-the-shelf models. Our proposed online tracking pipeline, CAMELTrack,
achieves state-of-the-art performance on multiple tracking benchmarks. Our code
is available at https://github.com/TrackingLaboratory/CAMELTrack.

</details>


### [135] [Global Collinearity-aware Polygonizer for Polygonal Building Mapping in Remote Sensing](https://arxiv.org/abs/2505.01385)
*Fahong Zhang,Yilei Shi,Xiao Xiang Zhu*

Main category: cs.CV

TL;DR: 提出了一种名为GCP的新算法，用于从遥感图像中映射多边形建筑物，通过结合实例分割和动态规划技术，优化多边形简化过程。


<details>
  <summary>Details</summary>
Motivation: 解决从遥感图像中准确映射多边形建筑物的挑战，提升多边形生成的精度和效率。

Method: 基于实例分割框架，通过采样轮廓多段线、变压器回归模块优化轮廓拟合，再通过动态规划简化多边形。

Result: 在公共基准测试中验证了GCP的有效性，其多边形简化模块优于传统方法（如Douglas-Peucker算法）。

Conclusion: GCP算法在建筑物多边形映射中表现出色，其模块具有广泛适用性，代码已开源。

Abstract: This paper addresses the challenge of mapping polygonal buildings from remote
sensing images and introduces a novel algorithm, the Global Collinearity-aware
Polygonizer (GCP). GCP, built upon an instance segmentation framework,
processes binary masks produced by any instance segmentation model. The
algorithm begins by collecting polylines sampled along the contours of the
binary masks. These polylines undergo a refinement process using a
transformer-based regression module to ensure they accurately fit the contours
of the targeted building instances. Subsequently, a collinearity-aware polygon
simplification module simplifies these refined polylines and generate the final
polygon representation. This module employs dynamic programming technique to
optimize an objective function that balances the simplicity and fidelity of the
polygons, achieving globally optimal solutions. Furthermore, the optimized
collinearity-aware objective is seamlessly integrated into network training,
enhancing the cohesiveness of the entire pipeline. The effectiveness of GCP has
been validated on two public benchmarks for polygonal building mapping. Further
experiments reveal that applying the collinearity-aware polygon simplification
module to arbitrary polylines, without prior knowledge, enhances accuracy over
traditional methods such as the Douglas-Peucker algorithm. This finding
underscores the broad applicability of GCP. The code for the proposed method
will be made available at https://github.com/zhu-xlab.

</details>


### [136] [VIDSTAMP: A Temporally-Aware Watermark for Ownership and Integrity in Video Diffusion Models](https://arxiv.org/abs/2505.01406)
*Mohammadreza Teymoorianfard,Shiqing Ma,Amir Houmansadr*

Main category: cs.CV

TL;DR: VIDSTAMP是一种视频水印框架，通过优化视频扩散模型的潜在空间嵌入高容量水印，保持视觉质量并抵抗常见操作。


<details>
  <summary>Details</summary>
Motivation: 现有水印方法难以应对视频特定操作（如帧插入、删除或重排序）且影响视觉质量，需要一种更鲁棒且不影响感知质量的方法。

Method: 采用两阶段微调管道，先在静态图像数据集上训练以分离空间信息，再在合成视频序列上恢复时间一致性，嵌入高容量水印。

Result: 嵌入768比特/视频（48比特/帧），比特准确率95.0%，视频质量得分0.836，接近无水印输出（0.838）。

Conclusion: VIDSTAMP在保持高视觉质量的同时，提供了更高的水印容量和鲁棒性，优于现有方法。

Abstract: The rapid rise of video diffusion models has enabled the generation of highly
realistic and temporally coherent videos, raising critical concerns about
content authenticity, provenance, and misuse. Existing watermarking approaches,
whether passive, post-hoc, or adapted from image-based techniques, often
struggle to withstand video-specific manipulations such as frame insertion,
dropping, or reordering, and typically degrade visual quality. In this work, we
introduce VIDSTAMP, a watermarking framework that embeds per-frame or
per-segment messages directly into the latent space of temporally-aware video
diffusion models. By fine-tuning the model's decoder through a two-stage
pipeline, first on static image datasets to promote spatial message separation,
and then on synthesized video sequences to restore temporal consistency,
VIDSTAMP learns to embed high-capacity, flexible watermarks with minimal
perceptual impact. Leveraging architectural components such as 3D convolutions
and temporal attention, our method imposes no additional inference cost and
offers better perceptual quality than prior methods, while maintaining
comparable robustness against common distortions and tampering. VIDSTAMP embeds
768 bits per video (48 bits per frame) with a bit accuracy of 95.0%, achieves a
log P-value of -166.65 (lower is better), and maintains a video quality score
of 0.836, comparable to unwatermarked outputs (0.838) and surpassing prior
methods in capacity-quality tradeoffs. Code: Code:
\url{https://github.com/SPIN-UMass/VidStamp}

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [137] [Towards Explainable Temporal User Profiling with LLMs](https://arxiv.org/abs/2505.00886)
*Milad Sabouri,Masoud Mansoury,Kun Lin,Bamshad Mobasher*

Main category: cs.IR

TL;DR: 利用大语言模型（LLMs）生成用户交互历史的自然语言摘要，区分短期和长期偏好，提升推荐系统的性能和透明度。


<details>
  <summary>Details</summary>
Motivation: 传统用户画像方法（如平均项目嵌入）忽略了用户兴趣的动态性和复杂性，尤其是短期与长期偏好的交互。

Method: 通过LLMs生成自然语言摘要，区分近期行为与持久倾向，使用预训练模型编码文本摘要，并通过注意力机制动态融合短期和长期嵌入。

Result: 在多个基线模型上提高了推荐准确性，同时支持可解释性，通过文本摘要和注意力权重为用户提供推荐理由。

Conclusion: 该方法不仅提升了推荐性能，还增强了推荐系统的透明度和可解释性，实验验证了其有效性。

Abstract: Accurately modeling user preferences is vital not only for improving
recommendation performance but also for enhancing transparency in recommender
systems. Conventional user profiling methods, such as averaging item
embeddings, often overlook the evolving, nuanced nature of user interests,
particularly the interplay between short-term and long-term preferences. In
this work, we leverage large language models (LLMs) to generate natural
language summaries of users' interaction histories, distinguishing recent
behaviors from more persistent tendencies. Our framework not only models
temporal user preferences but also produces natural language profiles that can
be used to explain recommendations in an interpretable manner. These textual
profiles are encoded via a pre-trained model, and an attention mechanism
dynamically fuses the short-term and long-term embeddings into a comprehensive
user representation. Beyond boosting recommendation accuracy over multiple
baselines, our approach naturally supports explainability: the interpretable
text summaries and attention weights can be exposed to end users, offering
insights into why specific items are suggested. Experiments on real-world
datasets underscore both the performance gains and the promise of generating
clearer, more transparent justifications for content-based recommendations.

</details>


### [138] [Preserving Privacy and Utility in LLM-Based Product Recommendations](https://arxiv.org/abs/2505.00951)
*Tina Khezresmaeilzadeh,Jiang Zhang,Dimitrios Andreadis,Konstantinos Psounis*

Main category: cs.IR

TL;DR: 论文提出了一种混合隐私保护推荐框架，通过分离敏感与非敏感数据，仅共享非敏感数据到云端，同时设计本地去混淆模块恢复敏感数据相关推荐，在保护隐私的同时保持推荐效果。


<details>
  <summary>Details</summary>
Motivation: 传统基于LLM的推荐系统需将用户数据传输到云端，引发隐私问题。本文旨在解决这一隐私风险，同时保持推荐质量。

Method: 提出混合隐私保护框架，分离敏感与非敏感数据，仅共享非敏感数据到云端，并设计本地去混淆模块恢复敏感数据相关推荐。

Result: 实验表明，该框架在保护隐私的同时，推荐效果接近全数据共享系统，且优于仅混淆技术。

Conclusion: 该方法在隐私与推荐质量间取得平衡，且适用于消费级硬件，具有实际应用价值。

Abstract: Large Language Model (LLM)-based recommendation systems leverage powerful
language models to generate personalized suggestions by processing user
interactions and preferences. Unlike traditional recommendation systems that
rely on structured data and collaborative filtering, LLM-based models process
textual and contextual information, often using cloud-based infrastructure.
This raises privacy concerns, as user data is transmitted to remote servers,
increasing the risk of exposure and reducing control over personal information.
To address this, we propose a hybrid privacy-preserving recommendation
framework which separates sensitive from nonsensitive data and only shares the
latter with the cloud to harness LLM-powered recommendations. To restore lost
recommendations related to obfuscated sensitive data, we design a
de-obfuscation module that reconstructs sensitive recommendations locally.
Experiments on real-world e-commerce datasets show that our framework achieves
almost the same recommendation utility with a system which shares all data with
an LLM, while preserving privacy to a large extend. Compared to
obfuscation-only techniques, our approach improves HR@10 scores and category
distribution alignment, offering a better balance between privacy and
recommendation quality. Furthermore, our method runs efficiently on
consumer-grade hardware, making privacy-aware LLM-based recommendation systems
practical for real-world use.

</details>


### [139] [Enhancing User Sequence Modeling through Barlow Twins-based Self-Supervised Learning](https://arxiv.org/abs/2505.00953)
*Yuhan Liu,Lin Ning,Neo Wu,Karan Singhal,Philip Andrew Mansfield,Devora Berlowitz,Sushant Prakash,Bradley Green*

Main category: cs.IR

TL;DR: 论文提出了一种基于Barlow Twins的自监督学习方法，用于用户序列建模，减少了对负样本的需求，并在多个数据集上表现优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 用户序列建模对推荐系统至关重要，但缺乏标记数据是一个主要挑战。现有自监督学习方法依赖大量负样本，计算成本高且不适用于实际场景。

Method: 通过改进Barlow Twins方法，结合适当的增强技术，减少对大批量负样本的需求，实现小批量下的有效表示学习。

Result: 在MovieLens-1M、MovieLens-20M和Yelp数据集上，方法在三个下游任务中表现优于双编码器模型，准确率提升8%-20%。

Conclusion: 该方法在标记数据稀缺和负样本有限的情况下，能有效提取用户序列信息，为推荐系统提供了高效解决方案。

Abstract: User sequence modeling is crucial for modern large-scale recommendation
systems, as it enables the extraction of informative representations of users
and items from their historical interactions. These user representations are
widely used for a variety of downstream tasks to enhance users' online
experience. A key challenge for learning these representations is the lack of
labeled training data. While self-supervised learning (SSL) methods have
emerged as a promising solution for learning representations from unlabeled
data, many existing approaches rely on extensive negative sampling, which can
be computationally expensive and may not always be feasible in real-world
scenario. In this work, we propose an adaptation of Barlow Twins, a
state-of-the-art SSL methods, to user sequence modeling by incorporating
suitable augmentation methods. Our approach aims to mitigate the need for large
negative sample batches, enabling effective representation learning with
smaller batch sizes and limited labeled data. We evaluate our method on the
MovieLens-1M, MovieLens-20M, and Yelp datasets, demonstrating that our method
consistently outperforms the widely-used dual encoder model across three
downstream tasks, achieving an 8%-20% improvement in accuracy. Our findings
underscore the effectiveness of our approach in extracting valuable
sequence-level information for user modeling, particularly in scenarios where
labeled data is scarce and negative examples are limited.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [140] [Artificial Intelligence in Government: Why People Feel They Lose Control](https://arxiv.org/abs/2505.01085)
*Alexander Wuttke,Adrian Rauchfleisch,Andreas Jungherr*

Main category: cs.CY

TL;DR: AI在公共管理中的应用迅速扩展，但引发公平、透明和问责问题。研究用委托-代理理论分析AI采用，揭示评估性、依赖性和争议性三大核心矛盾。实验表明效率提升初期增强信任，但降低公民感知控制，长期可能损害民主合法性。


<details>
  <summary>Details</summary>
Motivation: 探讨AI在政府职能中的整合如何影响公平、透明和问责，以及如何通过委托-代理理论理解这些挑战。

Method: 采用预注册的因子调查实验，涵盖税收、福利和执法领域，分析AI采用对公众信任和感知控制的影响。

Result: 效率提升初期增强信任，但降低公民感知控制；当结构性风险显现时，制度信任和感知控制均显著下降。

Conclusion: 委托-代理理论为理解AI在政府中的政治和制度影响提供了有力视角，政策制定者需透明处理委托风险以维护公众信任。

Abstract: The use of Artificial Intelligence (AI) in public administration is expanding
rapidly, moving from automating routine tasks to deploying generative and
agentic systems that autonomously act on goals. While AI promises greater
efficiency and responsiveness, its integration into government functions raises
concerns about fairness, transparency, and accountability. This article applies
principal-agent theory (PAT) to conceptualize AI adoption as a special case of
delegation, highlighting three core tensions: assessability (can decisions be
understood?), dependency (can the delegation be reversed?), and contestability
(can decisions be challenged?). These structural challenges may lead to a
"failure-by-success" dynamic, where early functional gains obscure long-term
risks to democratic legitimacy. To test this framework, we conducted a
pre-registered factorial survey experiment across tax, welfare, and law
enforcement domains. Our findings show that although efficiency gains initially
bolster trust, they simultaneously reduce citizens' perceived control. When the
structural risks come to the foreground, institutional trust and perceived
control both drop sharply, suggesting that hidden costs of AI adoption
significantly shape public attitudes. The study demonstrates that PAT offers a
powerful lens for understanding the institutional and political implications of
AI in government, emphasizing the need for policymakers to address delegation
risks transparently to maintain public trust.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [141] [Attack and defense techniques in large language models: A survey and new perspectives](https://arxiv.org/abs/2505.00976)
*Zhiyu Liao,Kang Chen,Yuanguo Lin,Kangkang Li,Yunxuan Liu,Hefeng Chen,Xingwang Huang,Yuanhui Yu*

Main category: cs.CR

TL;DR: 本文系统综述了大语言模型（LLMs）的攻击与防御技术，分类了攻击类型并分析了防御策略，同时指出当前挑战和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在自然语言处理任务中广泛应用，但其安全性和伦理问题日益突出，需要系统研究攻击与防御技术以应对动态威胁。

Method: 通过分类攻击类型（如对抗性提示攻击、模型窃取等）和分析防御策略（预防性和检测性方法），系统梳理了LLMs的安全问题。

Result: 尽管防御技术有所进展，但仍需解决动态威胁、平衡可用性与鲁棒性、资源限制等挑战。

Conclusion: 未来需发展自适应防御、可解释安全技术和标准化评估框架，强调跨学科合作与伦理考量，以构建安全可靠的LLMs。

Abstract: Large Language Models (LLMs) have become central to numerous natural language
processing tasks, but their vulnerabilities present significant security and
ethical challenges. This systematic survey explores the evolving landscape of
attack and defense techniques in LLMs. We classify attacks into adversarial
prompt attack, optimized attacks, model theft, as well as attacks on
application of LLMs, detailing their mechanisms and implications. Consequently,
we analyze defense strategies, including prevention-based and detection-based
defense methods. Although advances have been made, challenges remain to adapt
to the dynamic threat landscape, balance usability with robustness, and address
resource constraints in defense implementation. We highlight open problems,
including the need for adaptive scalable defenses, explainable security
techniques, and standardized evaluation frameworks. This survey provides
actionable insights and directions for developing secure and resilient LLMs,
emphasizing the importance of interdisciplinary collaboration and ethical
considerations to mitigate risks in real-world applications.

</details>


### [142] [Spill The Beans: Exploiting CPU Cache Side-Channels to Leak Tokens from Large Language Models](https://arxiv.org/abs/2505.00817)
*Andrew Adiletta,Berk Sunar*

Main category: cs.CR

TL;DR: 论文提出了一种名为'Spill The Beans'的缓存侧信道攻击方法，用于泄露大型语言模型（LLM）生成的令牌。通过共享硬件资源，攻击者可以检测缓存命中情况，从而推断出令牌信息。实验证明该方法在特定场景下能高效泄露敏感数据。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的广泛应用，共享硬件资源上的侧信道攻击威胁日益严重。本文旨在探索LLM在缓存侧信道攻击中的脆弱性，揭示其潜在隐私风险。

Method: 攻击者通过共置攻击进程与受害者模型，利用缓存侧信道技术（flush-and-reload）监控嵌入层的访问。通过平衡监控令牌数量与信息泄露量，优化攻击效果。

Result: 实验表明，攻击者能高效泄露令牌，例如单次监控可恢复80%-90%的高熵API密钥或40%的英文文本。攻击效果受监控令牌集和目标领域影响。

Conclusion: 研究揭示了LLM在缓存侧信道攻击中的新漏洞，强调了隐私与安全风险，并提出了缓解此类威胁的建议。

Abstract: Side-channel attacks on shared hardware resources increasingly threaten
confidentiality, especially with the rise of Large Language Models (LLMs). In
this work, we introduce Spill The Beans, a novel application of cache
side-channels to leak tokens generated by an LLM. By co-locating an attack
process on the same hardware as the victim model, we flush and reload embedding
vectors from the embedding layer, where each token corresponds to a unique
embedding vector. When accessed during token generation, it results in a cache
hit detectable by our attack on shared lower-level caches.
  A significant challenge is the massive size of LLMs, which, by nature of
their compute intensive operation, quickly evicts embedding vectors from the
cache. We address this by balancing the number of tokens monitored against the
amount of information leaked. Monitoring more tokens increases potential
vocabulary leakage but raises the chance of missing cache hits due to eviction;
monitoring fewer tokens improves detection reliability but limits vocabulary
coverage.
  Through extensive experimentation, we demonstrate the feasibility of leaking
tokens from LLMs via cache side-channels. Our findings reveal a new
vulnerability in LLM deployments, highlighting that even sophisticated models
are susceptible to traditional side-channel attacks. We discuss the
implications for privacy and security in LLM-serving infrastructures and
suggest considerations for mitigating such threats. For proof of concept we
consider two concrete attack scenarios: Our experiments show that an attacker
can recover as much as 80%-90% of a high entropy API key with single shot
monitoring. As for English text we can reach a 40% recovery rate with a single
shot. We should note that the rate highly depends on the monitored token set
and these rates can be improved by targeting more specialized output domains.

</details>


### [143] [From Texts to Shields: Convergence of Large Language Models and Cybersecurity](https://arxiv.org/abs/2505.00841)
*Tao Li,Ya-Ting Yang,Yunian Pan,Quanyan Zhu*

Main category: cs.CR

TL;DR: 报告探讨了大型语言模型（LLM）与网络安全的融合，分析了其在软件和网络安全、5G漏洞分析及生成式安全工程中的应用，并提出了解决信任、透明度和伦理问题的策略。


<details>
  <summary>Details</summary>
Motivation: 研究LLM在网络安全领域的潜力，以提升自动化、效率和推理能力，同时应对其部署中的社会技术挑战。

Method: 综合网络安全、人工智能、形式化方法和人本设计的跨学科视角，分析LLM的应用及挑战。

Result: LLM能自动化复杂任务并提升安全分析能力，但需解决信任、透明度和伦理问题。

Conclusion: 报告提出了一个前瞻性研究议程，以推动LLM在网络安全中的安全有效应用。

Abstract: This report explores the convergence of large language models (LLMs) and
cybersecurity, synthesizing interdisciplinary insights from network security,
artificial intelligence, formal methods, and human-centered design. It examines
emerging applications of LLMs in software and network security, 5G
vulnerability analysis, and generative security engineering. The report
highlights the role of agentic LLMs in automating complex tasks, improving
operational efficiency, and enabling reasoning-driven security analytics.
Socio-technical challenges associated with the deployment of LLMs -- including
trust, transparency, and ethical considerations -- can be addressed through
strategies such as human-in-the-loop systems, role-specific training, and
proactive robustness testing. The report further outlines critical research
challenges in ensuring interpretability, safety, and fairness in LLM-based
systems, particularly in high-stakes domains. By integrating technical advances
with organizational and societal considerations, this report presents a
forward-looking research agenda for the secure and effective adoption of LLMs
in cybersecurity.

</details>


### [144] [OET: Optimization-based prompt injection Evaluation Toolkit](https://arxiv.org/abs/2505.00843)
*Jinsheng Pan,Xiaogeng Liu,Chaowei Xiao*

Main category: cs.CR

TL;DR: OET是一个基于优化的评估工具包，用于系统评估提示注入攻击和防御，通过自适应测试框架生成最坏情况对抗样本，揭示当前防御机制的局限性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）易受提示注入攻击，缺乏标准化的评估框架来测试防御策略的有效性，尤其是在自适应对抗场景下。

Method: 提出OET工具包，采用模块化工作流，支持对抗字符串生成、动态攻击执行和结果分析，结合白盒和黑盒优化方法生成对抗样本。

Result: 实验表明当前防御机制存在局限性，部分模型即使经过安全增强仍易受攻击。

Conclusion: OET为评估对抗鲁棒性提供了统一平台，揭示了现有防御的不足，为未来研究提供了方向。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
natural language understanding and generation, enabling their widespread
adoption across various domains. However, their susceptibility to prompt
injection attacks poses significant security risks, as adversarial inputs can
manipulate model behavior and override intended instructions. Despite numerous
defense strategies, a standardized framework to rigorously evaluate their
effectiveness, especially under adaptive adversarial scenarios, is lacking. To
address this gap, we introduce OET, an optimization-based evaluation toolkit
that systematically benchmarks prompt injection attacks and defenses across
diverse datasets using an adaptive testing framework. Our toolkit features a
modular workflow that facilitates adversarial string generation, dynamic attack
execution, and comprehensive result analysis, offering a unified platform for
assessing adversarial robustness. Crucially, the adaptive testing framework
leverages optimization methods with both white-box and black-box access to
generate worst-case adversarial examples, thereby enabling strict red-teaming
evaluations. Extensive experiments underscore the limitations of current
defense mechanisms, with some models remaining susceptible even after
implementing security enhancements.

</details>


### [145] [Good News for Script Kiddies? Evaluating Large Language Models for Automated Exploit Generation](https://arxiv.org/abs/2505.01065)
*David Jin,Qian Fu,Yuekang Li*

Main category: cs.CR

TL;DR: 论文首次系统研究了大语言模型（LLMs）在自动化漏洞利用生成（AEG）中的效果，评估了其合作性和技术能力。通过引入重构的软件安全实验室基准，发现GPT-4和GPT-4o合作性高，但所有模型均未能成功生成漏洞利用。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在代码相关任务中的潜力，尤其是自动化漏洞利用生成（AEG）的可能性及其潜在风险。

Method: 引入重构的软件安全实验室基准，设计基于LLM的攻击者系统，评估不同模型（如GPT-4、GPT-4o和Llama3）的合作性和技术能力。

Result: GPT-4和GPT-4o表现出高合作性，但所有模型均未能成功生成漏洞利用；GPT-4o的错误最少，显示LLM驱动AEG的潜力。

Conclusion: LLMs在AEG中表现出合作性，但技术能力仍需提升；GPT-4o的错误最少，为未来研究提供了方向。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
code-related tasks, raising concerns about their potential for automated
exploit generation (AEG). This paper presents the first systematic study on
LLMs' effectiveness in AEG, evaluating both their cooperativeness and technical
proficiency. To mitigate dataset bias, we introduce a benchmark with refactored
versions of five software security labs. Additionally, we design an LLM-based
attacker to systematically prompt LLMs for exploit generation. Our experiments
reveal that GPT-4 and GPT-4o exhibit high cooperativeness, comparable to
uncensored models, while Llama3 is the most resistant. However, no model
successfully generates exploits for refactored labs, though GPT-4o's minimal
errors highlight the potential for LLM-driven AEG advancements.

</details>


### [146] [A Rusty Link in the AI Supply Chain: Detecting Evil Configurations in Model Repositories](https://arxiv.org/abs/2505.01067)
*Ziqi Ding,Qian Fu,Junchen Ding,Gelei Deng,Yi Liu,Yuekang Li*

Main category: cs.CR

TL;DR: 论文研究了Hugging Face平台上恶意配置文件的安全风险，提出了三种攻击场景，并开发了CONFIGSCAN工具以高效检测可疑文件。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）推动了多样化的AI应用发展，但AI供应链（如Hugging Face）中的配置文件安全问题被忽视，可能被利用执行未授权代码。

Method: 通过分析Hugging Face上的配置文件，识别了三种攻击场景，并开发了基于LLM的工具CONFIGSCAN，结合运行时代码和关键库检测可疑内容。

Result: 评估发现数千个可疑仓库和配置文件，验证了CONFIGSCAN的低误报率和高准确性。

Conclusion: 研究强调了AI模型托管平台加强安全验证的紧迫性。

Abstract: Recent advancements in large language models (LLMs) have spurred the
development of diverse AI applications from code generation and video editing
to text generation; however, AI supply chains such as Hugging Face, which host
pretrained models and their associated configuration files contributed by the
public, face significant security challenges; in particular, configuration
files originally intended to set up models by specifying parameters and initial
settings can be exploited to execute unauthorized code, yet research has
largely overlooked their security compared to that of the models themselves; in
this work, we present the first comprehensive study of malicious configurations
on Hugging Face, identifying three attack scenarios (file, website, and
repository operations) that expose inherent risks; to address these threats, we
introduce CONFIGSCAN, an LLM-based tool that analyzes configuration files in
the context of their associated runtime code and critical libraries,
effectively detecting suspicious elements with low false positive rates and
high accuracy; our extensive evaluation uncovers thousands of suspicious
repositories and configuration files, underscoring the urgent need for enhanced
security validation in AI model hosting platforms.

</details>


### [147] [LLM Security: Vulnerabilities, Attacks, Defenses, and Countermeasures](https://arxiv.org/abs/2505.01177)
*Francisco Aguilera-Martínez,Fernando Berzal*

Main category: cs.CR

TL;DR: 本文综述了大语言模型（LLMs）的安全威胁与防御机制，区分了训练阶段和部署后的攻击类型，并分类分析了防御策略。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs的发展，评估其安全威胁和漏洞变得至关重要，以保障模型的安全性和可靠性。

Method: 通过定义和分类针对LLMs的攻击，分析防御机制，并评估其有效性。

Result: 提出了预防性和检测性防御策略，并总结了攻击与防御的对应关系。

Conclusion: 本文为LLMs安全提供了结构化框架，并指出需进一步研究以应对新兴安全挑战。

Abstract: As large language models (LLMs) continue to evolve, it is critical to assess
the security threats and vulnerabilities that may arise both during their
training phase and after models have been deployed. This survey seeks to define
and categorize the various attacks targeting LLMs, distinguishing between those
that occur during the training phase and those that affect already trained
models. A thorough analysis of these attacks is presented, alongside an
exploration of defense mechanisms designed to mitigate such threats. Defenses
are classified into two primary categories: prevention-based and
detection-based defenses. Furthermore, our survey summarizes possible attacks
and their corresponding defense strategies. It also provides an evaluation of
the effectiveness of the known defense mechanisms for the different security
threats. Our survey aims to offer a structured framework for securing LLMs,
while also identifying areas that require further research to improve and
strengthen defenses against emerging security challenges.

</details>


### [148] [Secure Cluster-Based Hierarchical Federated Learning in Vehicular Networks](https://arxiv.org/abs/2505.01186)
*M. Saeid HaghighiFard,Sinem Coleri*

Main category: cs.CR

TL;DR: 提出了一种针对分层联邦学习（HFL）中对抗性和不可靠车辆的防御框架，通过动态车辆选择、异常检测和加权梯度平均机制，有效减少收敛时间。


<details>
  <summary>Details</summary>
Motivation: 解决HFL中因对抗性和不可靠车辆导致的模型完整性和收敛性问题。

Method: 结合动态车辆选择、Z-score和余弦相似性分析的异常检测、自适应阈值机制、加权梯度平均及跨集群一致性检查。

Result: 仿真结果表明，该算法在1跳和3跳拓扑中显著减少了收敛时间。

Conclusion: 提出的多级防御策略能有效过滤恶意贡献，提升HFL的鲁棒性和效率。

Abstract: Hierarchical Federated Learning (HFL) has recently emerged as a promising
solution for intelligent decision-making in vehicular networks, helping to
address challenges such as limited communication resources, high vehicle
mobility, and data heterogeneity. However, HFL remains vulnerable to
adversarial and unreliable vehicles, whose misleading updates can significantly
compromise the integrity and convergence of the global model. To address these
challenges, we propose a novel defense framework that integrates dynamic
vehicle selection with robust anomaly detection within a cluster-based HFL
architecture, specifically designed to counter Gaussian noise and gradient
ascent attacks. The framework performs a comprehensive reliability assessment
for each vehicle by evaluating historical accuracy, contribution frequency, and
anomaly records. Anomaly detection combines Z-score and cosine similarity
analyses on model updates to identify both statistical outliers and directional
deviations in model updates. To further refine detection, an adaptive
thresholding mechanism is incorporated into the cosine similarity metric,
dynamically adjusting the threshold based on the historical accuracy of each
vehicle to enforce stricter standards for consistently high-performing
vehicles. In addition, a weighted gradient averaging mechanism is implemented,
which assigns higher weights to gradient updates from more trustworthy
vehicles. To defend against coordinated attacks, a cross-cluster consistency
check is applied to identify collaborative attacks in which multiple
compromised clusters coordinate misleading updates. Together, these mechanisms
form a multi-level defense strategy to filter out malicious contributions
effectively. Simulation results show that the proposed algorithm significantly
reduces convergence time compared to benchmark methods across both 1-hop and
3-hop topologies.

</details>


### [149] [Constrained Network Adversarial Attacks: Validity, Robustness, and Transferability](https://arxiv.org/abs/2505.01328)
*Anass Grini,Oumaima Taheri,Btissam El Khamlichi,Amal El Fallah-Seghrouchni*

Main category: cs.CR

TL;DR: 研究发现现有对抗攻击方法在IoT环境中常违反领域约束，导致80.3%的对抗样本无效，误导了对NIDS模型脆弱性的评估。MLP生成的对抗样本更有效，且需考虑领域约束和模型架构。


<details>
  <summary>Details</summary>
Motivation: 现有对抗攻击方法在IoT和网络流量中常违反领域约束，导致评估结果不准确，误导防御资源分配。

Method: 通过分析对抗样本的有效性，比较MLP、CNN和LSTM生成的对抗样本，并研究其在不同模型中的可转移性。

Result: 80.3%的对抗样本因违反领域约束而无效；MLP生成的对抗样本更有效。

Conclusion: 评估和设计IoT及网络安全ML/DL模型时，需综合考虑领域约束和模型架构。

Abstract: While machine learning has significantly advanced Network Intrusion Detection
Systems (NIDS), particularly within IoT environments where devices generate
large volumes of data and are increasingly susceptible to cyber threats, these
models remain vulnerable to adversarial attacks. Our research reveals a
critical flaw in existing adversarial attack methodologies: the frequent
violation of domain-specific constraints, such as numerical and categorical
limits, inherent to IoT and network traffic. This leads to up to 80.3% of
adversarial examples being invalid, significantly overstating real-world
vulnerabilities. These invalid examples, though effective in fooling models, do
not represent feasible attacks within practical IoT deployments. Consequently,
relying on these results can mislead resource allocation for defense, inflating
the perceived susceptibility of IoT-enabled NIDS models to adversarial
manipulation. Furthermore, we demonstrate that simpler surrogate models like
Multi-Layer Perceptron (MLP) generate more valid adversarial examples compared
to complex architectures such as CNNs and LSTMs. Using the MLP as a surrogate,
we analyze the transferability of adversarial severity to other ML/DL models
commonly used in IoT contexts. This work underscores the importance of
considering both domain constraints and model architecture when evaluating and
designing robust ML/DL models for security-critical IoT and network
applications.

</details>


### [150] [Protocol-agnostic and Data-free Backdoor Attacks on Pre-trained Models in RF Fingerprinting](https://arxiv.org/abs/2505.00881)
*Tianya Zhao,Ningning Wang,Junqing Zhang,Xuyu Wang*

Main category: cs.CR

TL;DR: 本文研究了在射频指纹识别中，无监督预训练模型（PTMs）的数据无关后门攻击，提出了一种无需下游数据、标签信息或训练过程的攻击方法，并探讨了防御的困难性。


<details>
  <summary>Details</summary>
Motivation: 监督深度神经网络在射频指纹识别中存在领域偏移和标记数据稀缺的问题，而无监督预训练模型（PTMs）虽能解决这些问题，但其潜在漏洞尚未充分研究。

Method: 设计了一组触发器和预定义输出表示（PORs），通过后门训练将触发器与PORs映射，从而在PTMs中植入后门行为。

Result: 实验表明，该攻击方法适用于多种输入域、协议和PTMs，且防御难度较大。

Conclusion: 无监督预训练模型在射频指纹识别中存在严重的安全隐患，需进一步研究防御措施。

Abstract: While supervised deep neural networks (DNNs) have proven effective for device
authentication via radio frequency (RF) fingerprinting, they are hindered by
domain shift issues and the scarcity of labeled data. The success of large
language models has led to increased interest in unsupervised pre-trained
models (PTMs), which offer better generalization and do not require labeled
datasets, potentially addressing the issues mentioned above. However, the
inherent vulnerabilities of PTMs in RF fingerprinting remain insufficiently
explored. In this paper, we thoroughly investigate data-free backdoor attacks
on such PTMs in RF fingerprinting, focusing on a practical scenario where
attackers lack access to downstream data, label information, and training
processes. To realize the backdoor attack, we carefully design a set of
triggers and predefined output representations (PORs) for the PTMs. By mapping
triggers and PORs through backdoor training, we can implant backdoor behaviors
into the PTMs, thereby introducing vulnerabilities across different downstream
RF fingerprinting tasks without requiring prior knowledge. Extensive
experiments demonstrate the wide applicability of our proposed attack to
various input domains, protocols, and PTMs. Furthermore, we explore potential
detection and defense methods, demonstrating the difficulty of fully
safeguarding against our proposed backdoor attack.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [151] [A flexible Bayesian non-parametric mixture model reveals multiple dependencies of swap errors in visual working memory](https://arxiv.org/abs/2505.01178)
*Puria Radmard,Paul M. Bays,Máté Lengyel*

Main category: q-bio.NC

TL;DR: 论文提出了一种贝叶斯非参数混合模型（BNS）来研究视觉工作记忆中的交换错误，揭示了交换错误不仅与提示相似性相关，还受到报告特征维度的非单调调制影响，挑战了以往研究对交换错误来源的解释。


<details>
  <summary>Details</summary>
Motivation: 视觉工作记忆（VWM）中的交换错误（swap errors）是未完全理解的现象，以往研究主要关注存储和检索阶段的错误，但忽略了编码阶段的影响。本文旨在通过数据驱动的方法，更全面地理解交换错误的来源。

Method: 引入贝叶斯非参数混合模型（BNS），灵活描述交换行为，允许交换依赖于每个刺激项的提示和报告特征。模型拟合了人类参与者的逐试次行为数据。

Result: BNS模型揭示了交换错误不仅强烈依赖于提示相似性，还在报告特征维度上表现出非单调调制。这一发现表明编码阶段可能是交换错误的新来源。

Conclusion: BNS模型的分析表明，以往对交换错误的解释可能不完整，编码阶段的作用被低估。这为理解VWM错误机制提供了新视角。

Abstract: Human behavioural data in psychophysics has been used to elucidate the
underlying mechanisms of many cognitive processes, such as attention,
sensorimotor integration, and perceptual decision making. Visual working memory
has particularly benefited from this approach: analyses of VWM errors have
proven crucial for understanding VWM capacity and coding schemes, in turn
constraining neural models of both. One poorly understood class of VWM errors
are swap errors, whereby participants recall an uncued item from memory. Swap
errors could arise from erroneous memory encoding, noisy storage, or errors at
retrieval time - previous research has mostly implicated the latter two.
However, these studies made strong a priori assumptions on the detailed
mechanisms and/or parametric form of errors contributed by these sources. Here,
we pursue a data-driven approach instead, introducing a Bayesian non-parametric
mixture model of swap errors (BNS) which provides a flexible descriptive model
of swapping behaviour, such that swaps are allowed to depend on both the probed
and reported features of every stimulus item. We fit BNS to the trial-by-trial
behaviour of human participants and show that it recapitulates the strong
dependence of swaps on cue similarity in multiple datasets. Critically, BNS
reveals that this dependence coexists with a non-monotonic modulation in the
report feature dimension for a random dot motion direction-cued,
location-reported dataset. The form of the modulation inferred by BNS opens new
questions about the importance of memory encoding in causing swap errors in
VWM, a distinct source to the previously suggested binding and cueing errors.
Our analyses, combining qualitative comparisons of the highly interpretable BNS
parameter structure with rigorous quantitative model comparison and recovery
methods, show that previous interpretations of swap errors may have been
incomplete.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [152] [Reduced-order structure-property linkages for stochastic metamaterials](https://arxiv.org/abs/2505.01283)
*Hooman Danesh,Maruthi Annamaraju,Tim Brepols,Stefanie Reese,Surya R. Kalidindi*

Main category: cs.CE

TL;DR: 通过主成分分析和高斯过程回归，建立了一种高效的方法来预测机械超材料的有效弹性性能，显著减少了计算成本。


<details>
  <summary>Details</summary>
Motivation: 机械超材料的设计空间庞大，传统物理模拟计算成本高，需要一种高效的方法来建立结构-性能关系。

Method: 使用主成分分析提取特征，结合快速傅里叶变换均匀化计算弹性刚度，并通过高斯过程回归生成低维代理模型。

Result: 仅需原始数据集的0.61%即可生成准确的结构-性能映射，显著降低了计算成本。

Conclusion: 该方法为高效设计和评估机械超材料提供了可行方案，适用于大规模设计空间。

Abstract: The capabilities of additive manufacturing have facilitated the design and
production of mechanical metamaterials with diverse unit cell geometries.
Establishing linkages between the vast design space of unit cells and their
effective mechanical properties is critical for the efficient design and
performance evaluation of such metamaterials. However, physics-based
simulations of metamaterial unit cells across the entire design space are
computationally expensive, necessitating a materials informatics framework to
efficiently capture complex structure-property relationships. In this work,
principal component analysis of 2-point correlation functions is performed to
extract the salient features from a large dataset of randomly generated 2D
metamaterials. Physics-based simulations are performed using a fast Fourier
transform (FFT)-based homogenization approach to efficiently compute the
homogenized effective elastic stiffness across the extensive unit cell designs.
Subsequently, Gaussian process regression is used to generate reduced-order
surrogates, mapping unit cell designs to their homogenized effective elastic
constant. It is demonstrated that the adopted workflow enables a high-value
low-dimensional representation of the voluminous stochastic metamaterial
dataset, facilitating the construction of robust structure-property maps.
Finally, an uncertainty-based active learning framework is utilized to train a
surrogate model with a significantly smaller number of data points compared to
the original full dataset. It is shown that a dataset as small as $0.61\%$ of
the entire dataset is sufficient to generate accurate and robust
structure-property maps.

</details>


<div id='cs.SC'></div>

# cs.SC [[Back]](#toc)

### [153] [Primality Testing via Circulant Matrix Eigenvalue Structure: A Novel Approach Using Cyclotomic Field Theory](https://arxiv.org/abs/2505.00730)
*Marius-Constantin Dinu*

Main category: cs.SC

TL;DR: 提出了一种基于循环矩阵特征值结构的新型素数测试方法，通过特征多项式不可约因子的数量判断素数。


<details>
  <summary>Details</summary>
Motivation: 将分圆域理论与矩阵代数结合，揭示素数与非素数的本质区别，并提供确定性测试方法。

Method: 构造循环矩阵$C_n = W_n + W_n^2$，分析其特征多项式在有理数域上的不可约因子数量。

Result: 证明$n>2$为素数当且仅当特征多项式恰有两个不可约因子，实验验证了方法的有效性和计算复杂度。

Conclusion: 该方法为素数测试提供了新的确定性途径，兼具理论深度和实际应用潜力。

Abstract: This paper presents a novel primality test based on the eigenvalue structure
of circulant matrices constructed from roots of unity. We prove that an integer
$n > 2$ is prime if and only if the minimal polynomial of the circulant matrix
$C_n = W_n + W_n^2$ has exactly two irreducible factors over $\mathbb{Q}$. This
characterization connects cyclotomic field theory with matrix algebra,
providing both theoretical insights and practical applications. We demonstrate
that the eigenvalue patterns of these matrices reveal fundamental distinctions
between prime and composite numbers, leading to a deterministic primality test.
Our approach leverages the relationship between primitive roots of unity,
Galois theory, and the factorization of cyclotomic polynomials. We provide
comprehensive experimental validation across various ranges of integers,
discuss practical implementation considerations, and analyze the computational
complexity of our method in comparison with established primality tests. The
visual interpretation of our mathematical framework provides intuitive
understanding of the algebraic structures that distinguish prime numbers. Our
experimental validation demonstrates that our approach offers a deterministic
alternative to existing methods, with performance characteristics reflecting
its algebraic foundations.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [154] [How much to Dereverberate? Low-Latency Single-Channel Speech Enhancement in Distant Microphone Scenarios](https://arxiv.org/abs/2505.01338)
*Satvik Venkatesh,Philip Coleman,Arthur Benilov,Simon Brown,Selim Sheta,Frederic Roskam*

Main category: eess.AS

TL;DR: 论文探讨了在远距离麦克风场景下的实时低延迟单通道语音增强（SE）技术，重点关注会议室和剧院等大空间环境，并验证了其可行性。


<details>
  <summary>Details</summary>
Motivation: 研究远距离（5-10米）和大空间（如会议室、剧院）场景下的单通道语音增强，填补了现有研究在小空间和短距离上的局限性。

Method: 通过分析房间体积与混响时间的关系，并随机模拟房间脉冲响应，验证了单通道SE在大空间场景下的可行性。

Result: 研究表明，在短衰减时间的去混响任务中，保留早期反射并衰减房间传递函数可以提升信号质量。

Conclusion: 论文证明了单通道SE在大空间和远距离场景下的可行性，并强调了房间体积与混响时间关系的重要性。

Abstract: Dereverberation is an important sub-task of Speech Enhancement (SE) to
improve the signal's intelligibility and quality. However, it remains
challenging because the reverberation is highly correlated with the signal.
Furthermore, the single-channel SE literature has predominantly focused on
rooms with short reverb times (typically under 1 second), smaller rooms (under
volumes of 1000 cubic meters) and relatively short distances (up to 2 meters).
In this paper, we explore real-time low-latency single-channel SE under distant
microphone scenarios, such as 5 to 10 meters, and focus on conference rooms and
theatres, with larger room dimensions and reverberation times. Such a setup is
useful for applications such as lecture demonstrations, drama, and to enhance
stage acoustics. First, we show that single-channel SE in such challenging
scenarios is feasible. Second, we investigate the relationship between room
volume and reverberation time, and demonstrate its importance when randomly
simulating room impulse responses. Lastly, we show that for dereverberation
with short decay times, preserving early reflections before decaying the
transfer function of the room improves overall signal quality.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [155] [The Coral Protocol: Open Infrastructure Connecting The Internet of Agents](https://arxiv.org/abs/2505.00749)
*Roman J. Georgio,Caelum Forder,Suman Deb,Peter Carroll,Önder Gürcan*

Main category: cs.MA

TL;DR: Coral Protocol是一个开放、去中心化的协作基础设施，支持多AI代理之间的通信、协调、信任和支付，解决跨领域和跨厂商的互操作性问题。


<details>
  <summary>Details</summary>
Motivation: 随着组织部署多个专用AI代理的需求增加，跨领域和跨厂商的互操作性成为关键挑战。Coral Protocol旨在为多代理AI生态系统提供基础平台。

Method: Coral Protocol设计了标准化的消息格式、模块化协调机制和安全的团队组建功能，确保代理间的高效和可信交互。

Result: Coral Protocol成为“代理互联网”的基石，通过开放的代理协作实现自动化、集体智能和商业价值。

Conclusion: Coral Protocol通过其设计和创新，为多代理协作提供了高效、安全的解决方案，推动了代理互联网的发展。

Abstract: The Coral Protocol is an open and decentralized collaboration infrastructure
that enables communication, coordination, trust and payments for The Internet
of Agents. It addresses the growing need for interoperability in a world where
organizations are deploying multiple specialized AI agents that must work
together across domains and vendors. As a foundational platform for multi-agent
AI ecosystems, Coral establishes a common language and coordination framework
allowing any agent to participate in complex workflows with others. Its design
emphasizes broad compatibility, security, and vendor neutrality, ensuring that
agent interactions are efficient and trustworthy. In particular, Coral
introduces standardized messaging formats for agent communication, a modular
coordination mechanism for orchestrating multi-agent tasks, and secure team
formation capabilities for dynamically assembling trusted groups of agents.
Together, these innovations position Coral Protocol as a cornerstone of the
emerging "Internet of Agents," unlocking new levels of automation, collective
intelligence, and business value through open agent collaboration.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [156] [On Simulating Thin-Film Processes at the Atomic Scale Using Machine Learned Force Fields](https://arxiv.org/abs/2505.01118)
*S. Kondati Natarajan,J. Schneider,N. Pandey,J. Wellendorff,S. Smidstrup*

Main category: cond-mat.mtrl-sci

TL;DR: 论文探讨了如何利用机器学习力场（MLFF）高效建模薄膜工艺，并提供了两个技术相关过程的示例。


<details>
  <summary>Details</summary>
Motivation: 原子尺度建模薄膜工艺有助于揭示关键化学机制并提取定量指标，但传统分子动力学（MD）缺乏适用于所有工业过程的力场。

Method: 提出利用机器学习力场（MLFF）构建适合工艺模拟的力场，并以HfO2原子层沉积和MoS2原子层蚀刻为例。

Result: 展示了MLFF在薄膜工艺建模中的高效性和适用性。

Conclusion: MLFF为计算材料和表面科学领域提供了新的工具，适用于工业相关过程的模拟。

Abstract: Atomistic modeling of thin-film processes provides an avenue not only for
discovering key chemical mechanisms of the processes but also to extract
quantitative metrics on the events and reactions taking place at the
gas-surface interface. Molecular dynamics (MD) is a powerful computational
method to study the evolution of a process at the atomic scale, but studies of
industrially relevant processes usually require suitable force fields, which
are in general not available for all processes of interest. However, machine
learned force fields (MLFF) are conquering the field of computational materials
and surface science. In this paper, we demonstrate how to efficiently build
MLFFs suitable for process simulations and provide two examples for
technologically relevant processes: precursor pulse in the atomic layer
deposition of HfO2 and atomic layer etching of MoS2.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [157] [CIMFlow: An Integrated Framework for Systematic Design and Evaluation of Digital CIM Architectures](https://arxiv.org/abs/2505.01107)
*Yingjie Qi,Jianlei Yang,Yiou Wang,Yikun Wang,Dayu Wang,Ling Tang,Cenlin Duan,Xiaolin He,Weisheng Zhao*

Main category: cs.AR

TL;DR: CIMFlow是一个集成框架，用于在数字计算内存（CIM）架构上实现和评估DNN工作负载，解决了现有工具在软件和硬件设计空间覆盖不足的问题。


<details>
  <summary>Details</summary>
Motivation: 数字CIM架构在DNN加速中表现出潜力，但缺乏全面的工具支持其开发和优化，尤其是容量约束问题。

Method: CIMFlow通过灵活的ISA设计连接编译和模拟基础设施，并在编译流程中采用高级分区和并行策略。

Result: 评估表明，CIMFlow能够系统化原型设计和优化数字CIM架构，支持广泛的设计空间探索。

Conclusion: CIMFlow为研究人员和设计师提供了一个易于使用的平台，推动了数字CIM架构的发展。

Abstract: Digital Compute-in-Memory (CIM) architectures have shown great promise in
Deep Neural Network (DNN) acceleration by effectively addressing the "memory
wall" bottleneck. However, the development and optimization of digital CIM
accelerators are hindered by the lack of comprehensive tools that encompass
both software and hardware design spaces. Moreover, existing design and
evaluation frameworks often lack support for the capacity constraints inherent
in digital CIM architectures. In this paper, we present CIMFlow, an integrated
framework that provides an out-of-the-box workflow for implementing and
evaluating DNN workloads on digital CIM architectures. CIMFlow bridges the
compilation and simulation infrastructures with a flexible instruction set
architecture (ISA) design, and addresses the constraints of digital CIM through
advanced partitioning and parallelism strategies in the compilation flow. Our
evaluation demonstrates that CIMFlow enables systematic prototyping and
optimization of digital CIM architectures across diverse configurations,
providing researchers and designers with an accessible platform for extensive
design space exploration.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [158] [Quantum Support Vector Regression for Robust Anomaly Detection](https://arxiv.org/abs/2505.01012)
*Kilian Tscharke,Maximilian Wendlinger,Sebastian Issel,Pascal Debus*

Main category: quant-ph

TL;DR: 本文研究了量子机器学习（特别是量子核方法）在异常检测中的应用，通过QSVR在IBM量子硬件上的实验，展示了其分类性能及对量子噪声的鲁棒性，同时发现其对对抗攻击的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 探索量子机器学习在异常检测中的潜力，尤其是在当前NISQ时代量子噪声不可避免的背景下，评估QSVR的性能和鲁棒性。

Method: 使用QSVR方法，在IBM量子硬件上对11个数据集进行基准测试，并分析量子噪声和对抗攻击对模型的影响。

Result: QSVR在部分数据集上表现优于无噪声模拟，对某些量子噪声表现出鲁棒性，但对对抗攻击高度脆弱且噪声未提升其鲁棒性。

Conclusion: 量子核方法在异常检测中具有潜力，但需进一步研究以提升其对对抗攻击的鲁棒性。

Abstract: Anomaly Detection (AD) is critical in data analysis, particularly within the
domain of IT security. In recent years, Machine Learning (ML) algorithms have
emerged as a powerful tool for AD in large-scale data. In this study, we
explore the potential of quantum ML approaches, specifically quantum kernel
methods, for the application to robust AD. We build upon previous work on
Quantum Support Vector Regression (QSVR) for semisupervised AD by conducting a
comprehensive benchmark on IBM quantum hardware using eleven datasets. Our
results demonstrate that QSVR achieves strong classification performance and
even outperforms the noiseless simulation on two of these datasets. Moreover,
we investigate the influence of - in the NISQ-era inevitable - quantum noise on
the performance of the QSVR. Our findings reveal that the model exhibits
robustness to depolarizing, phase damping, phase flip, and bit flip noise,
while amplitude damping and miscalibration noise prove to be more disruptive.
Finally, we explore the domain of Quantum Adversarial Machine Learning and
demonstrate that QSVR is highly vulnerable to adversarial attacks and that
noise does not improve the adversarial robustness of the model.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [159] [SmallPlan: Leverage Small Language Models for Sequential Path Planning with Simulation-Powered, LLM-Guided Distillation](https://arxiv.org/abs/2505.00831)
*Quang P. M. Pham,Khoi T. N. Nguyen,Nhi H. Doan,Cuong A. Pham,Kentaro Inui,Dezhen Song*

Main category: cs.RO

TL;DR: SmallPlan框架利用大型语言模型（LLMs）作为教师模型，训练轻量级小型语言模型（SLMs）进行高效路径规划，适用于动态环境和边缘设备。


<details>
  <summary>Details</summary>
Motivation: 在动态环境中实现高效的机器人路径规划是一个挑战，LLMs计算成本高且适应性有限，难以实时部署。

Method: SmallPlan通过LLM引导的监督微调（SFT）和强化学习（RL）训练SLMs，使其在场景图中规划路径。

Result: 实验表明，SLMs在路径规划任务中表现与GPT-4o相当，且避免了幻觉和过拟合问题。

Conclusion: SmallPlan资源高效，适合边缘设备部署，推动了自主机器人的实际应用。

Abstract: Efficient path planning in robotics, particularly within large-scale, dynamic
environments, remains a significant hurdle. While Large Language Models (LLMs)
offer strong reasoning capabilities, their high computational cost and limited
adaptability in dynamic scenarios hinder real-time deployment on edge devices.
We present SmallPlan -- a novel framework leveraging LLMs as teacher models to
train lightweight Small Language Models (SLMs) for high-level path planning
tasks. In SmallPlan, the SLMs provide optimal action sequences to navigate
across scene graphs that compactly represent full-scaled 3D scenes. The SLMs
are trained in a simulation-powered, interleaved manner with LLM-guided
supervised fine-tuning (SFT) and reinforcement learning (RL). This strategy not
only enables SLMs to successfully complete navigation tasks but also makes them
aware of important factors like travel distance and number of trials. Through
experiments, we demonstrate that the fine-tuned SLMs perform competitively with
larger models like GPT-4o on sequential path planning, without suffering from
hallucination and overfitting. SmallPlan is resource-efficient, making it
well-suited for edge-device deployment and advancing practical autonomous
robotics.

</details>


### [160] [IK Seed Generator for Dual-Arm Human-like Physicality Robot with Mobile Base](https://arxiv.org/abs/2505.00871)
*Jun Takamatsu,Atsushi Kanehira,Kazuhiro Sasabuchi,Naoki Wake,Katsushi Ikeuchi*

Main category: cs.RO

TL;DR: 本文提出了一种通过优化初始猜测来提高数值逆运动学（IK）求解成功率的方法，利用遗传算法和可达性图生成高质量的初始猜测。


<details>
  <summary>Details</summary>
Motivation: 在尺寸受限的机器人中，由于机械限制（如关节角度限制），解决逆运动学问题较为困难。若能缓解这一问题，将显著提升此类机器人的实用价值。

Method: 通过定义基于缩放雅可比矩阵的初始猜测质量指标，结合遗传算法和可达性图优化初始猜测。

Result: 实验证明，使用优化的初始猜测显著提高了IK求解的成功率，并在三种典型场景中验证了方法的实用性。

Conclusion: 该方法有效提升了尺寸受限机器人解决IK问题的能力，为其在家庭服务等场景中的应用提供了技术支持。

Abstract: Robots are strongly expected as a means of replacing human tasks. If a robot
has a human-like physicality, the possibility of replacing human tasks
increases. In the case of household service robots, it is desirable for them to
be on a human-like size so that they do not become excessively large in order
to coexist with humans in their operating environment. However, robots with
size limitations tend to have difficulty solving inverse kinematics (IK) due to
mechanical limitations, such as joint angle limitations. Conversely, if the
difficulty coming from this limitation could be mitigated, one can expect that
the use of such robots becomes more valuable. In numerical IK solver, which is
commonly used for robots with higher degrees-of-freedom (DOF), the solvability
of IK depends on the initial guess given to the solver. Thus, this paper
proposes a method for generating a good initial guess for a numerical IK solver
given the target hand configuration. For the purpose, we define the goodness of
an initial guess using the scaled Jacobian matrix, which can calculate the
manipulability index considering the joint limits. These two factors are
related to the difficulty of solving IK. We generate the initial guess by
optimizing the goodness using the genetic algorithm (GA). To enumerate much
possible IK solutions, we use the reachability map that represents the
reachable area of the robot hand in the arm-base coordinate system. We conduct
quantitative evaluation and prove that using an initial guess that is judged to
be better using the goodness value increases the probability that IK is solved.
Finally, as an application of the proposed method, we show that by generating
good initial guesses for IK a robot actually achieves three typical scenarios.

</details>


### [161] [Autonomous Embodied Agents: When Robotics Meets Deep Learning Reasoning](https://arxiv.org/abs/2505.00935)
*Roberto Bigazzi*

Main category: cs.RO

TL;DR: 论文探讨了计算能力提升和深度学习革命如何推动具身人工智能（Embodied AI）的发展，重点研究了智能自主机器人在室内环境中的训练与部署。


<details>
  <summary>Details</summary>
Motivation: 随着计算能力的增强和深度学习的进步，具身人工智能成为计算机视觉、机器人和决策领域的交叉研究方向，旨在开发智能自主机器人并推动其社会应用。

Method: 利用3D模型进行逼真机器人仿真，通过模拟训练学习型智能体，使其在未知环境中执行任务，包括环境交互、信息提取和动作执行。

Result: 论文详细分析了智能具身代理的实现过程，包括文献综述、方法技术解释和相关机器人任务的实验研究。

Conclusion: 研究为具身人工智能和自主代理领域提供了贡献，旨在推动未来工作的发展。

Abstract: The increase in available computing power and the Deep Learning revolution
have allowed the exploration of new topics and frontiers in Artificial
Intelligence research. A new field called Embodied Artificial Intelligence,
which places at the intersection of Computer Vision, Robotics, and Decision
Making, has been gaining importance during the last few years, as it aims to
foster the development of smart autonomous robots and their deployment in
society. The recent availability of large collections of 3D models for
photorealistic robotic simulation has allowed faster and safe training of
learning-based agents for millions of frames and a careful evaluation of their
behavior before deploying the models on real robotic platforms. These
intelligent agents are intended to perform a certain task in a possibly unknown
environment. To this end, during the training in simulation, the agents learn
to perform continuous interactions with the surroundings, such as gathering
information from the environment, encoding and extracting useful cues for the
task, and performing actions towards the final goal; where every action of the
agent influences the interactions. This dissertation follows the complete
creation process of embodied agents for indoor environments, from their concept
to their implementation and deployment. We aim to contribute to research in
Embodied AI and autonomous agents, in order to foster future work in this
field. We present a detailed analysis of the procedure behind implementing an
intelligent embodied agent, comprehending a thorough description of the current
state-of-the-art in literature, technical explanations of the proposed methods,
and accurate experimental studies on relevant robotic tasks.

</details>


### [162] [Model Tensor Planning](https://arxiv.org/abs/2505.01059)
*An T. Le,Khai Nguyen,Minh Nhat Vu,João Carvalho,Jan Peters*

Main category: cs.RO

TL;DR: 提出了一种名为MTP的采样MPC框架，通过张量采样生成高熵控制轨迹，结合局部和全局采样策略，提升探索能力。


<details>
  <summary>Details</summary>
Motivation: 传统采样MPC在非线性任务中表现优异，但探索能力不足，MTP旨在解决这一问题。

Method: 采用随机多部图采样和B/Akima样条插值生成平滑多样的控制轨迹，结合β混合策略平衡探索与利用。

Result: 实验表明MTP在多种机器人任务中优于标准MPC和进化策略，具有更高的任务成功率和鲁棒性。

Conclusion: MTP为基于模型的规划与控制提供了可扩展的鲁棒探索框架。

Abstract: Sampling-based model predictive control (MPC) offers strong performance in
nonlinear and contact-rich robotic tasks, yet often suffers from poor
exploration due to locally greedy sampling schemes. We propose \emph{Model
Tensor Planning} (MTP), a novel sampling-based MPC framework that introduces
high-entropy control trajectory generation through structured tensor sampling.
By sampling over randomized multipartite graphs and interpolating control
trajectories with B-splines and Akima splines, MTP ensures smooth and globally
diverse control candidates. We further propose a simple $\beta$-mixing strategy
that blends local exploitative and global exploratory samples within the
modified Cross-Entropy Method (CEM) update, balancing control refinement and
exploration. Theoretically, we show that MTP achieves asymptotic path coverage
and maximum entropy in the control trajectory space in the limit of infinite
tensor depth and width.
  Our implementation is fully vectorized using JAX and compatible with MuJoCo
XLA, supporting \emph{Just-in-time} (JIT) compilation and batched rollouts for
real-time control with online domain randomization. Through experiments on
various challenging robotic tasks, ranging from dexterous in-hand manipulation
to humanoid locomotion, we demonstrate that MTP outperforms standard MPC and
evolutionary strategy baselines in task success and control robustness. Design
and sensitivity ablations confirm the effectiveness of MTP tensor sampling
structure, spline interpolation choices, and mixing strategy. Altogether, MTP
offers a scalable framework for robust exploration in model-based planning and
control.

</details>


### [163] [ViSA-Flow: Accelerating Robot Skill Learning via Large-Scale Video Semantic Action Flow](https://arxiv.org/abs/2505.01288)
*Changhe Chen,Quantao Yang,Xiaohao Xu,Nima Fazeli,Olov Andersson*

Main category: cs.RO

TL;DR: 论文提出了一种名为ViSA-Flow的框架，通过语义动作流作为中间表示，从大规模无标签视频数据中学习机器人操作技能，显著降低了数据收集成本。


<details>
  <summary>Details</summary>
Motivation: 机器人获取复杂操作技能的主要障碍是收集大规模演示数据的高成本，而人类可以通过观察他人与环境互动高效学习。论文旨在通过语义动作流弥合这一差距。

Method: ViSA-Flow框架通过自监督学习从人类-物体互动视频中提取语义动作流，预训练生成模型，再通过少量机器人演示数据微调，实现知识迁移。

Result: 在CALVIN基准测试和实际任务中，ViSA-Flow在低数据量情况下表现优异，优于现有方法。

Conclusion: ViSA-Flow通过语义动作流有效实现了从人类观察到机器人执行的知识迁移，显著提升了机器人操作技能的学习效率。

Abstract: One of the central challenges preventing robots from acquiring complex
manipulation skills is the prohibitive cost of collecting large-scale robot
demonstrations. In contrast, humans are able to learn efficiently by watching
others interact with their environment. To bridge this gap, we introduce
semantic action flow as a core intermediate representation capturing the
essential spatio-temporal manipulator-object interactions, invariant to
superficial visual differences. We present ViSA-Flow, a framework that learns
this representation self-supervised from unlabeled large-scale video data.
First, a generative model is pre-trained on semantic action flows automatically
extracted from large-scale human-object interaction video data, learning a
robust prior over manipulation structure. Second, this prior is efficiently
adapted to a target robot by fine-tuning on a small set of robot demonstrations
processed through the same semantic abstraction pipeline. We demonstrate
through extensive experiments on the CALVIN benchmark and real-world tasks that
ViSA-Flow achieves state-of-the-art performance, particularly in low-data
regimes, outperforming prior methods by effectively transferring knowledge from
human video observation to robotic execution. Videos are available at
https://visaflow-web.github.io/ViSAFLOW.

</details>


### [164] [Uncertainty-aware Latent Safety Filters for Avoiding Out-of-Distribution Failures](https://arxiv.org/abs/2505.00779)
*Junwon Seo,Kensuke Nakamura,Andrea Bajcsy*

Main category: cs.RO

TL;DR: 论文提出了一种基于不确定性感知的潜在安全过滤器，通过结合世界模型的认知不确定性来识别未知风险，从而提升机器人系统的安全性。


<details>
  <summary>Details</summary>
Motivation: 现有基于生成世界模型的安全控制方法难以覆盖所有安全关键场景，导致潜在安全过滤器可能遗漏已知或未知风险。

Method: 利用世界模型的认知不确定性作为代理识别未知风险，并通过符合预测校准不确定性阈值，在增强状态空间中执行可达性分析。

Result: 在仿真和硬件实验中，该方法能够预先检测潜在不安全场景，并可靠地提出安全的动作。

Conclusion: 不确定性感知的安全过滤器能够有效保护机器人系统免受已知和未知安全风险的威胁。

Abstract: Recent advances in generative world models have enabled classical safe
control methods, such as Hamilton-Jacobi (HJ) reachability, to generalize to
complex robotic systems operating directly from high-dimensional sensor
observations. However, obtaining comprehensive coverage of all safety-critical
scenarios during world model training is extremely challenging. As a result,
latent safety filters built on top of these models may miss novel hazards and
even fail to prevent known ones, overconfidently misclassifying risky
out-of-distribution (OOD) situations as safe. To address this, we introduce an
uncertainty-aware latent safety filter that proactively steers robots away from
both known and unseen failures. Our key idea is to use the world model's
epistemic uncertainty as a proxy for identifying unseen potential hazards. We
propose a principled method to detect OOD world model predictions by
calibrating an uncertainty threshold via conformal prediction. By performing
reachability analysis in an augmented state space-spanning both the latent
representation and the epistemic uncertainty-we synthesize a latent safety
filter that can reliably safeguard arbitrary policies from both known and
unseen safety hazards. In simulation and hardware experiments on vision-based
control tasks with a Franka manipulator, we show that our uncertainty-aware
safety filter preemptively detects potential unsafe scenarios and reliably
proposes safe, in-distribution actions. Video results can be found on the
project website at https://cmu-intentlab.github.io/UNISafe

</details>


### [165] [FalconWing: An Open-Source Platform for Ultra-Light Fixed-Wing Aircraft Research](https://arxiv.org/abs/2505.01383)
*Yan Miao,Will Shen,Hang Cui,Sayan Mitra*

Main category: cs.RO

TL;DR: FalconWing是一个开源的超轻量固定翼平台，用于自主性研究，通过纯视觉控制策略实现自主着陆，并采用了一种新颖的实-仿-实学习方法。


<details>
  <summary>Details</summary>
Motivation: 为自主性研究提供一个轻量、开源且功能完整的硬件平台，同时探索纯视觉控制策略在自主着陆中的应用。

Method: 1. 通过3D高斯散射构建逼真仿真环境；2. 从视觉估计的飞行数据中识别非线性动力学；3. 通过仿真模仿学习训练多模态Vision Transformer策略。

Result: 在硬件平台上零样本部署时，该策略实现了80%的视觉自主着陆成功率。

Conclusion: FalconWing及其开源组件为自主性研究提供了实用工具，同时验证了纯视觉控制策略的可行性。

Abstract: We present FalconWing -- an open-source, ultra-lightweight (150 g) fixed-wing
platform for autonomy research. The hardware platform integrates a small
camera, a standard airframe, offboard computation, and radio communication for
manual overrides. We demonstrate FalconWing's capabilities by developing and
deploying a purely vision-based control policy for autonomous landing (without
IMU or motion capture) using a novel real-to-sim-to-real learning approach. Our
learning approach: (1) constructs a photorealistic simulation environment via
3D Gaussian splatting trained on real-world images; (2) identifies nonlinear
dynamics from vision-estimated real-flight data; and (3) trains a multi-modal
Vision Transformer (ViT) policy through simulation-only imitation learning. The
ViT architecture fuses single RGB image with the history of control actions via
self-attention, preserving temporal context while maintaining real-time 20 Hz
inference. When deployed zero-shot on the hardware platform, this policy
achieves an 80% success rate in vision-based autonomous landings. Together with
the hardware specifications, we also open-source the system dynamics, the
software for photorealistic simulator and the learning approach.

</details>


### [166] [SIME: Enhancing Policy Self-Improvement with Modal-level Exploration](https://arxiv.org/abs/2505.01396)
*Yang Jin,Jun Lv,Wenye Yu,Hongjie Fang,Yong-Lu Li,Cewu Lu*

Main category: cs.RO

TL;DR: 论文提出了一种通过模态级探索和数据选择实现机器人自我改进的方法，成功在仿真和真实实验中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 机器人通过与环境交互实现自我改进的能力有限，主要问题在于重复现有行为而无法生成有价值的新数据。

Method: 引入模态级探索机制以产生多样化交互，并通过数据选择提取高质量片段用于学习。

Result: 在仿真和真实实验中成功实现了有效的机器人自我改进。

Conclusion: 该方法能以较低成本开发更鲁棒且高成功率的机器人控制策略。

Abstract: Self-improvement requires robotic systems to initially learn from
human-provided data and then gradually enhance their capabilities through
interaction with the environment. This is similar to how humans improve their
skills through continuous practice. However, achieving effective
self-improvement is challenging, primarily because robots tend to repeat their
existing abilities during interactions, often failing to generate new, valuable
data for learning. In this paper, we identify the key to successful
self-improvement: modal-level exploration and data selection. By incorporating
a modal-level exploration mechanism during policy execution, the robot can
produce more diverse and multi-modal interactions. At the same time, we select
the most valuable trials and high-quality segments from these interactions for
learning. We successfully demonstrate effective robot self-improvement on both
simulation benchmarks and real-world experiments. The capability for
self-improvement will enable us to develop more robust and high-success-rate
robotic control strategies at a lower cost. Our code and experiment scripts are
available at https://ericjin2002.github.io/SIME/

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [167] [To Repair or Not to Repair? Investigating the Importance of AB-Cycles for the State-of-the-Art TSP Heuristic EAX](https://arxiv.org/abs/2505.00803)
*Jonathan Heins,Darrell Whitley,Pascal Kerschke*

Main category: cs.NE

TL;DR: 本文聚焦于EAX算法的第一阶段，提出了一种快速验证AB-cycles是否生成有效路径的新方法，并改进了EAX算法，提升了计算效率和求解质量。


<details>
  <summary>Details</summary>
Motivation: EAX算法的第二阶段已被深入研究，但第一阶段鲜有研究。本文旨在填补这一空白，优化EAX算法的第一阶段。

Method: 提出了一种新方法，快速验证AB-cycles是否生成有效路径，并基于此改进了EAX算法。

Result: 在10,000个TSP实例上的测试表明，改进后的EAX算法在计算效率和求解质量上优于现有EAX算法。

Conclusion: 本文提出的方法显著提升了EAX算法的性能，尤其是在处理困难实例时表现更优。

Abstract: The Edge Assembly Crossover (EAX) algorithm is the state-of-the-art heuristic
for solving the Traveling Salesperson Problem (TSP). It regularly outperforms
other methods, such as the Lin-Kernighan-Helsgaun heuristic (LKH), across
diverse sets of TSP instances. Essentially, EAX employs a two-stage mechanism
that focuses on improving the current solutions, first, at the local and,
subsequently, at the global level. Although the second phase of the algorithm
has been thoroughly studied, configured, and refined in the past, in
particular, its first stage has hardly been examined.
  In this paper, we thus focus on the first stage of EAX and introduce a novel
method that quickly verifies whether the AB-cycles, generated during its
internal optimization procedure, yield valid tours -- or whether they need to
be repaired. Knowledge of the latter is also particularly relevant before
applying other powerful crossover operators such as the Generalized Partition
Crossover (GPX). Based on our insights, we propose and evaluate several
improved versions of EAX. According to our benchmark study across 10 000
different TSP instances, the most promising of our proposed EAX variants
demonstrates improved computational efficiency and solution quality on
previously rather difficult instances compared to the current state-of-the-art
EAX algorithm.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [168] [Differentiable Nonlinear Model Predictive Control](https://arxiv.org/abs/2505.01353)
*Jonathan Frey,Katrin Baumgärtner,Gianluca Frison,Dirk Reinhardt,Jasper Hoffmann,Leonard Fichtner,Sebastien Gros,Moritz Diehl*

Main category: math.OC

TL;DR: 论文提出了一种计算非线性规划问题（NLP）解敏感性的方法，结合了隐函数定理（IFT）和内点法（IPM），并在SQP框架中实现，速度比现有方法快3倍以上。


<details>
  <summary>Details</summary>
Motivation: 学习增强方法与非线性模型预测控制（MPC）结合时，解敏感性的高效计算是关键挑战，而现有机器学习方法仅适用于凸或无约束问题。

Method: 使用隐函数定理（IFT）和内点法（IPM）处理平滑最优性条件，并在SQP方法中实现敏感性计算。

Result: 提出了一种高效的开源实现，支持一般最优控制问题的前向和伴随敏感性计算，速度比mpc.pytorch快3倍以上。

Conclusion: 该方法为非线性MPC与学习算法的结合提供了高效的解敏感性计算工具。

Abstract: The efficient computation of parametric solution sensitivities is a key
challenge in the integration of learning-enhanced methods with nonlinear model
predictive control (MPC), as their availability is crucial for many learning
algorithms. While approaches presented in the machine learning community are
limited to convex or unconstrained formulations, this paper discusses the
computation of solution sensitivities of general nonlinear programs (NLPs)
using the implicit function theorem (IFT) and smoothed optimality conditions
treated in interior-point methods (IPM). We detail sensitivity computation
within a sequential quadratic programming (SQP) method which employs an IPM for
the quadratic subproblems. The publication is accompanied by an efficient
open-source implementation within the framework, providing both forward and
adjoint sensitivities for general optimal control problems, achieving speedups
exceeding 3x over the state-of-the-art solver mpc.pytorch.

</details>


### [169] [A Provably Convergent Plug-and-Play Framework for Stochastic Bilevel Optimization](https://arxiv.org/abs/2505.01258)
*Tianshu Chu,Dachuan Xu,Wei Yao,Chengming Yu,Jin Zhang*

Main category: math.OC

TL;DR: 本文提出了一种名为PnPBO的即插即用框架，用于开发和分析随机双层优化方法，整合了现代无偏和有偏随机估计器，并提供了统一的收敛和复杂度分析。


<details>
  <summary>Details</summary>
Motivation: 双层优化在机器学习中具有广泛应用和高级分层优化能力，但目前缺乏统一的框架来整合和分析各种随机估计器。

Method: 提出PnPBO框架，将无偏和有偏随机估计器整合到单循环双层优化中，并采用移动平均技术。

Result: 理论分析表明，PnPBO框架下的各种随机估计器（如PAGE、ZeroSARAH等）实现了与单层优化相当的样本复杂度。

Conclusion: PnPBO框架解决了双层优化与单层优化复杂度是否相同的开放性问题，并通过实验验证了其有效性。

Abstract: Bilevel optimization has recently attracted significant attention in machine
learning due to its wide range of applications and advanced hierarchical
optimization capabilities. In this paper, we propose a plug-and-play framework,
named PnPBO, for developing and analyzing stochastic bilevel optimization
methods. This framework integrates both modern unbiased and biased stochastic
estimators into the single-loop bilevel optimization framework introduced in
[9], with several improvements. In the implementation of PnPBO, all stochastic
estimators for different variables can be independently incorporated, and an
additional moving average technique is applied when using an unbiased estimator
for the upper-level variable. In the theoretical analysis, we provide a unified
convergence and complexity analysis for PnPBO, demonstrating that the
adaptation of various stochastic estimators (including PAGE, ZeroSARAH, and
mixed strategies) within the PnPBO framework achieves optimal sample
complexity, comparable to that of single-level optimization. This resolves the
open question of whether the optimal complexity bounds for solving bilevel
optimization are identical to those for single-level optimization. Finally, we
empirically validate our framework, demonstrating its effectiveness on several
benchmark problems and confirming our theoretical findings.

</details>


### [170] [Negative Stepsizes Make Gradient-Descent-Ascent Converge](https://arxiv.org/abs/2505.01423)
*Henry Shugart,Jason M. Altschuler*

Main category: math.OC

TL;DR: 本文提出了一种新颖的步长调度策略（称为弹弓步长调度），使原始的梯度下降上升法（GDA）在最小-最大问题上能够收敛，解决了传统GDA的失败问题。


<details>
  <summary>Details</summary>
Motivation: 最小-最大问题在优化、学习、博弈和控制中至关重要，但传统的GDA方法被认为无法收敛。本文旨在证明通过合理的步长选择，GDA可以成功收敛。

Method: 提出了一种时间变化、不对称且周期性为负的步长调度策略（弹弓步长），并证明这三种特性对收敛是必要的。

Result: 实验表明，弹弓步长调度使GDA在经典反例（如无约束凸凹问题）上能够收敛，且适用于实际中所需的最后迭代。

Conclusion: 弹弓步长调度通过非可逆的梯度流动态，实现了GDA的收敛，并近似实现了共识优化，为深度学习中的最小-最大问题（如GAN训练）提供了新思路。

Abstract: Efficient computation of min-max problems is a central question in
optimization, learning, games, and controls. Arguably the most natural
algorithm is gradient-descent-ascent (GDA). However, since the 1970s,
conventional wisdom has argued that GDA fails to converge even on simple
problems. This failure spurred an extensive literature on modifying GDA with
additional building blocks such as extragradients, optimism, momentum,
anchoring, etc. In contrast, we show that GDA converges in its original form by
simply using a judicious choice of stepsizes.
  The key innovation is the proposal of unconventional stepsize schedules
(dubbed slingshot stepsize schedules) that are time-varying, asymmetric, and
periodically negative. We show that all three properties are necessary for
convergence, and that altogether this enables GDA to converge on the classical
counterexamples (e.g., unconstrained convex-concave problems). All of our
results apply to the last iterate of GDA, as is typically desired in practice.
  The core algorithmic intuition is that although negative stepsizes make
backward progress, they de-synchronize the min and max variables (overcoming
the cycling issue of GDA), and lead to a slingshot phenomenon in which the
forward progress in the other iterations is overwhelmingly larger. This results
in fast overall convergence. Geometrically, the slingshot dynamics leverage the
non-reversibility of gradient flow: positive/negative steps cancel to first
order, yielding a second-order net movement in a new direction that leads to
convergence and is otherwise impossible for GDA to move in. We interpret this
as a second-order finite-differencing algorithm and show that, intriguingly, it
approximately implements consensus optimization, an empirically popular
algorithm for min-max problems involving deep neural networks (e.g., training
GANs).

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [171] [On the emergence of numerical instabilities in Next Generation Reservoir Computing](https://arxiv.org/abs/2505.00846)
*Edmilson Roque dos Santos,Erik Bollt*

Main category: stat.ML

TL;DR: NGRC是一种低成本机器学习方法，用于预测混沌时间序列，但其动态稳定性在自主预测中仍具挑战性。研究发现特征矩阵数值条件与长期动态相关，并探讨了不同数值算法的影响。


<details>
  <summary>Details</summary>
Motivation: 解决NGRC模型在自主预测中的动态稳定性问题，揭示特征矩阵条件与长期动态的关系。

Method: 结合数值线性代数和动态系统遍历理论，系统研究特征矩阵条件随超参数的变化，并评估不同数值算法（Cholesky、SVD、LU）的影响。

Result: 发现NGRC特征矩阵在短时间滞后和高阶多项式下易出现病态条件，导致动态不稳定。

Conclusion: 特征矩阵条件对NGRC稳定性至关重要，选择合适的数值算法可缓解病态问题。

Abstract: Next Generation Reservoir Computing (NGRC) is a low-cost machine learning
method for forecasting chaotic time series from data. However, ensuring the
dynamical stability of NGRC models during autonomous prediction remains a
challenge. In this work, we uncover a key connection between the numerical
conditioning of the NGRC feature matrix -- formed by polynomial evaluations on
time-delay coordinates -- and the long-term NGRC dynamics. Merging tools from
numerical linear algebra and ergodic theory of dynamical systems, we
systematically study how the feature matrix conditioning varies across
hyperparameters. We demonstrate that the NGRC feature matrix tends to be
ill-conditioned for short time lags and high-degree polynomials.
Ill-conditioning amplifies sensitivity to training data perturbations, which
can produce unstable NGRC dynamics. We evaluate the impact of different
numerical algorithms (Cholesky, SVD, and LU) for solving the regularized
least-squares problem.

</details>


### [172] [DOLCE: Decomposing Off-Policy Evaluation/Learning into Lagged and Current Effects](https://arxiv.org/abs/2505.00961)
*Shu Tamano,Masanori Nojima*

Main category: stat.ML

TL;DR: DOLCE是一种新的估计器，通过分解奖励为滞后和当前效应，解决了目标策略和记录策略之间共同支持假设不成立时的评估和学习问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在共同支持假设不成立时需要不稳定外推或保守策略，无法满足对这类个体的明确评估或优化需求。

Method: DOLCE利用多时间点的上下文信息，将奖励分解为滞后和当前效应，结合过去和当前上下文处理违反共同支持假设的个体。

Result: 实验表明，DOLCE在共同支持假设不成立的个体比例增加时，显著提升了OPE和OPL的效果。

Conclusion: DOLCE在局部正确性和条件独立性假设下是无偏的，能有效处理共同支持假设不成立的情况。

Abstract: Off-policy evaluation (OPE) and off-policy learning (OPL) for contextual
bandit policies leverage historical data to evaluate and optimize a target
policy. Most existing OPE/OPL methods--based on importance weighting or
imputation--assume common support between the target and logging policies. When
this assumption is violated, these methods typically require unstable
extrapolation, truncation, or conservative strategies for individuals outside
the common support assumption. However, such approaches can be inadequate in
settings where explicit evaluation or optimization for such individuals is
required. To address this issue, we propose DOLCE: Decomposing Off-policy
evaluation/learning into Lagged and Current Effects, a novel estimator that
leverages contextual information from multiple time points to decompose rewards
into lagged and current effects. By incorporating both past and present
contexts, DOLCE effectively handles individuals who violate the common support
assumption. We show that the proposed estimator is unbiased under two
assumptions--local correctness and conditional independence. Our experiments
demonstrate that DOLCE achieves substantial improvements in OPE and OPL,
particularly as the proportion of individuals outside the common support
assumption increases.

</details>


### [173] [Characterization and Learning of Causal Graphs from Hard Interventions](https://arxiv.org/abs/2505.01037)
*Zihan Zhou,Muhammad Qasim Elahi,Murat Kocaoglu*

Main category: stat.ML

TL;DR: 本文提出了一种基于多实验分布数据的因果发现方法，通过比较不同干预分布，建立了与Pearl的do-calculus相关的图形约束，并提出了学习算法以整合多数据集。


<details>
  <summary>Details</summary>
Motivation: 解决从观察和实验数据中揭示因果结构的基本挑战，特别是在多实验分布和潜在变量存在的情况下。

Method: 通过比较不同干预分布，提出图形约束，并设计学习算法整合多数据集，引入新的定向规则。

Result: 定义了干预等价类的因果图表示，并证明了所提算法的正确性。

Conclusion: 提出的方法能够有效整合多干预数据集，为因果发现提供了新的工具和理论支持。

Abstract: A fundamental challenge in the empirical sciences involves uncovering causal
structure through observation and experimentation. Causal discovery entails
linking the conditional independence (CI) invariances in observational data to
their corresponding graphical constraints via d-separation. In this paper, we
consider a general setting where we have access to data from multiple
experimental distributions resulting from hard interventions, as well as
potentially from an observational distribution. By comparing different
interventional distributions, we propose a set of graphical constraints that
are fundamentally linked to Pearl's do-calculus within the framework of hard
interventions. These graphical constraints associate each graphical structure
with a set of interventional distributions that are consistent with the rules
of do-calculus. We characterize the interventional equivalence class of causal
graphs with latent variables and introduce a graphical representation that can
be used to determine whether two causal graphs are interventionally equivalent,
i.e., whether they are associated with the same family of hard interventional
distributions, where the elements of the family are indistinguishable using the
invariances from do-calculus. We also propose a learning algorithm to integrate
multiple datasets from hard interventions, introducing new orientation rules.
The learning objective is a tuple of augmented graphs which entails a set of
causal graphs. We also prove the soundness of the proposed algorithm.

</details>


### [174] [Gaussian Differential Private Bootstrap by Subsampling](https://arxiv.org/abs/2505.01197)
*Holger Dette,Carina Graw*

Main category: stat.ML

TL;DR: 论文提出了一种基于差分隐私的私有经验m/n自助法，解决了传统自助法在隐私保护下计算成本高和统计精度损失的问题。


<details>
  <summary>Details</summary>
Motivation: 传统自助法在大规模数据和差分隐私条件下存在计算成本高和隐私预算增加的问题，导致统计精度下降。

Method: 提出了一种私有经验m/n自助法，利用高斯差分隐私验证其一致性和隐私保证。

Result: 该方法降低了计算成本，减少了噪声需求，提高了统计精度，并展示了更好的有限样本性能。

Conclusion: 私有经验m/n自助法在隐私保护和统计精度之间取得了更好的平衡，优于现有方法。

Abstract: Bootstrap is a common tool for quantifying uncertainty in data analysis.
However, besides additional computational costs in the application of the
bootstrap on massive data, a challenging problem in bootstrap based inference
under Differential Privacy consists in the fact that it requires repeated
access to the data. As a consequence, bootstrap based differentially private
inference requires a significant increase of the privacy budget, which on the
other hand comes with a substantial loss in statistical accuracy.
  A potential solution to reconcile the conflicting goals of statistical
accuracy and privacy is to analyze the data under parametric model assumptions
and in the last decade, several parametric bootstrap methods for inference
under privacy have been investigated. However, uncertainty quantification by
parametric bootstrap is only valid if the the quantities of interest can be
identified as the parameters of a statistical model and the imposed model
assumptions are (at least approximately) satisfied. An alternative to
parametric methods is the empirical bootstrap that is a widely used tool for
non-parametric inference and well studied in the non-private regime. However,
under privacy, less insight is available. In this paper, we propose a private
empirical $m$ out of $n$ bootstrap and validate its consistency and privacy
guarantees under Gaussian Differential Privacy. Compared to the the private $n$
out of $n$ bootstrap, our approach has several advantages. First, it comes with
less computational costs, in particular for massive data. Second, the proposed
procedure needs less additional noise in the bootstrap iterations, which leads
to an improved statistical accuracy while asymptotically guaranteeing the same
level of privacy. Third, we demonstrate much better finite sample properties
compared to the currently available procedures.

</details>


### [175] [Provable Efficiency of Guidance in Diffusion Models for General Data Distribution](https://arxiv.org/abs/2505.01382)
*Gen Li,Yuchen Jiao*

Main category: stat.ML

TL;DR: 论文分析了扩散模型中引导技术的理论效果，证明在一般数据分布下，引导能整体提升样本质量。


<details>
  <summary>Details</summary>
Motivation: 尽管扩散模型的引导技术在实践中表现优异，但其理论理解仍有限，尤其是在非特定分布下的效果尚不明确。

Method: 通过理论分析，研究扩散引导在一般数据分布下的效果，而非仅局限于特定案例。

Result: 证明引导能降低分类器概率的倒数平均值，从而整体提升样本质量。

Conclusion: 引导技术在一般数据分布下具有理论支持，能有效提升样本质量。

Abstract: Diffusion models have emerged as a powerful framework for generative
modeling, with guidance techniques playing a crucial role in enhancing sample
quality. Despite their empirical success, a comprehensive theoretical
understanding of the guidance effect remains limited. Existing studies only
focus on case studies, where the distribution conditioned on each class is
either isotropic Gaussian or supported on a one-dimensional interval with some
extra conditions. How to analyze the guidance effect beyond these case studies
remains an open question. Towards closing this gap, we make an attempt to
analyze diffusion guidance under general data distributions. Rather than
demonstrating uniform sample quality improvement, which does not hold in some
distributions, we prove that guidance can improve the whole sample quality, in
the sense that the average reciprocal of the classifier probability decreases
with the existence of guidance. This aligns with the motivation of introducing
guidance.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [176] [EnviKal-Loc: Sub-10m Indoor LoRaWAN Localization using an Environmental-Aware Path Loss and Adaptive RSSI Smoothing](https://arxiv.org/abs/2505.01185)
*Nahshon Mokua Obiri,Kristof Van Laerhoven*

Main category: cs.NI

TL;DR: 论文提出了一种轻量级但鲁棒的方法，结合自适应滤波和多墙路径损耗模型，显著提高了LoRaWAN室内定位的精度。


<details>
  <summary>Details</summary>
Motivation: LoRaWAN技术在大规模物联网部署中具有广泛覆盖性，但在复杂室内环境中实现亚10米精确定位仍具挑战性。

Method: 结合自适应卡尔曼滤波和多墙路径损耗模型，并引入LoRaWAN参数和动态环境指标。

Result: 提出的MWM-EP-KF模型平均绝对误差为5.81米，优于其他两种模型。环境参数和卡尔曼滤波分别减少了41.22%和42.63%的系统误差。

Conclusion: 该方法为动态变化环境中的精确室内LoRaWAN定位提供了一种高效且可解释的解决方案。

Abstract: LoRaWAN technology's extensive coverage positions it as a strong contender
for large-scale IoT deployments. However, achieving sub-10 m accuracy in indoor
localization remains challenging due to complex environmental conditions,
multipath fading, and transient obstructions. This paper proposes a lightweight
but robust approach combining adaptive filtering with an extended log-distance,
multi-wall path loss and shadowing (PLS) model. Our methodology augments
conventional models with critical LoRaWAN parameters (received signal strength
indicator (RSSI), frequency, and signal-to-noise ratio (SNR)) and dynamic
environmental indicators (temperature, humidity, carbon dioxide, particulate
matter, and barometric pressure). An adaptive Kalman filter reduces RSSI
fluctuations, isolating persistent trends from momentary noise. Using a
six-month dataset of 1,328,334 field measurements, we evaluate three models:
the baseline COST 231 multi-wall model (MWM), the baseline model augmented with
environmental parameters (MWM-EP), and a forward-only adaptive Kalman-filtered
RSSI version of the latter (MWM-EP-KF). Results confirm that the MWM-EP-KF
achieves a mean absolute error (MAE) of 5.81 m, outperforming both the MWM-EP
(10.56 m) and the baseline MWM framework (17.98 m). Environmental augmentation
reduces systematic errors by 41.22%, while Kalman filtering significantly
enhances robustness under high RSSI volatility by 42.63%, on average across all
devices. These findings present an interpretable, efficient solution for
precise indoor LoRaWAN localization in dynamically changing environments.

</details>


<div id='astro-ph.GA'></div>

# astro-ph.GA [[Back]](#toc)

### [177] [JFlow: Model-Independent Spherical Jeans Analysis using Equivariant Continuous Normalizing Flows](https://arxiv.org/abs/2505.00763)
*Sung Hak Lim,Kohei Hayashi,Shun'ichi Horigome,Shigeki Matsumoto,Mihoko M. Nojiri*

Main category: astro-ph.GA

TL;DR: 本文提出了一种无监督机器学习方法，通过球对称Jeans方程无模型分析矮球状星系，利用等变连续归一化流估计相空间密度和速度弥散。


<details>
  <summary>Details</summary>
Motivation: 矮球状星系中恒星的运动学信息有限，传统方法依赖参数化模型，难以进行完整的相空间分析。

Method: 使用等变连续归一化流无模型求解球对称Jeans方程，估计相空间密度和速度弥散。

Result: 在Gaia挑战数据集上验证，方法能准确识别暗物质晕结构，即使恒星数量较少。

Conclusion: 该方法为矮球状星系的无模型分析提供了新途径，适用于小样本数据。

Abstract: The kinematics of stars in dwarf spheroidal galaxies have been studied to
understand the structure of dark matter halos. However, the kinematic
information of these stars is often limited to celestial positions and
line-of-sight velocities, making full phase space analysis challenging.
Conventional methods rely on projected analytic phase space density models with
several parameters and infer dark matter halo structures by solving the
spherical Jeans equation. In this paper, we introduce an unsupervised machine
learning method for solving the spherical Jeans equation in a model-independent
way as a first step toward model-independent analysis of dwarf spheroidal
galaxies. Using equivariant continuous normalizing flows, we demonstrate that
spherically symmetric stellar phase space densities and velocity dispersions
can be estimated without model assumptions. As a proof of concept, we apply our
method to Gaia challenge datasets for spherical models and measure dark matter
mass densities given velocity anisotropy profiles. Our method can identify halo
structures accurately, even with a small number of tracer stars.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [178] [Enhancing SPARQL Query Rewriting for Complex Ontology Alignments](https://arxiv.org/abs/2505.01309)
*Anicet Lepetit Ondo,Laurence Capus,Mamadou Bousso*

Main category: cs.DB

TL;DR: 该论文提出了一种基于自然语言的SPARQL查询自动重写方法，利用GPT-4等大语言模型处理复杂本体对齐（如c:c对应关系），为非专业用户提供便捷查询异构数据的解决方案。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注简单或部分复杂的本体对齐，忽略了更复杂的对齐（如c:c）带来的挑战，同时SPARQL的复杂语法对非专业用户构成障碍。

Method: 结合等价传递性和GPT-4等大语言模型，实现从源本体到目标本体的SPARQL查询自动重写，支持自然语言表达的用户需求。

Result: 该方法能高效处理复杂对齐（如c:c），并提升非专业用户对异构数据的查询能力。

Conclusion: 该创新方法通过自然语言和先进模型解决了复杂本体对齐和SPARQL语法障碍，为异构数据查询提供了灵活方案。

Abstract: SPARQL query rewriting is a fundamental mechanism for uniformly querying
heterogeneous ontologies in the Linked Data Web. However, the complexity of
ontology alignments, particularly rich correspondences (c : c), makes this
process challenging. Existing approaches primarily focus on simple (s : s) and
partially complex ( s : c) alignments, thereby overlooking the challenges posed
by more expressive alignments. Moreover, the intricate syntax of SPARQL
presents a barrier for non-expert users seeking to fully exploit the knowledge
encapsulated in ontologies. This article proposes an innovative approach for
the automatic rewriting of SPARQL queries from a source ontology to a target
ontology, based on a user's need expressed in natural language. It leverages
the principles of equivalence transitivity as well as the advanced capabilities
of large language models such as GPT-4. By integrating these elements, this
approach stands out for its ability to efficiently handle complex alignments,
particularly (c : c) correspondences , by fully exploiting their
expressiveness. Additionally, it facilitates access to aligned ontologies for
users unfamiliar with SPARQL, providing a flexible solution for querying
heterogeneous data.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [179] [Dynamic and Distributed Routing in IoT Networks based on Multi-Objective Q-Learning](https://arxiv.org/abs/2505.00918)
*Shubham Vaishnav,Praveen Kumar Donta,Sindri Magnússon*

Main category: cs.DC

TL;DR: 提出了一种基于多目标Q学习的动态分布式路由算法，用于适应物联网中实时变化的优先级需求。


<details>
  <summary>Details</summary>
Motivation: 物联网应用中优先级需求动态变化，现有路由协议难以适应，需优化静态目标的矛盾。

Method: 结合多目标优化和Q学习，提出动态分布式路由算法及贪婪插值策略。

Result: 仿真显示该算法在奖励、能效和包传递率等指标上优于现有算法。

Conclusion: 新算法能快速适应动态优先级，有效利用帕累托最优解。

Abstract: The last few decades have witnessed a rapid increase in IoT devices owing to
their wide range of applications, such as smart healthcare monitoring systems,
smart cities, and environmental monitoring. A critical task in IoT networks is
sensing and transmitting information over the network. The IoT nodes gather
data by sensing the environment and then transmit this data to a destination
node via multi-hop communication, following some routing protocols. These
protocols are usually designed to optimize possibly contradictory objectives,
such as maximizing packet delivery ratio and energy efficiency. While most
literature has focused on optimizing a static objective that remains unchanged,
many real-world IoT applications require adapting to rapidly shifting
priorities. For example, in monitoring systems, some transmissions are
time-critical and require a high priority on low latency, while other
transmissions are less urgent and instead prioritize energy efficiency. To meet
such dynamic demands, we propose novel dynamic and distributed routing based on
multiobjective Q-learning that can adapt to changes in preferences in
real-time. Our algorithm builds on ideas from both multi-objective optimization
and Q-learning. We also propose a novel greedy interpolation policy scheme to
take near-optimal decisions for unexpected preference changes. The proposed
scheme can approximate and utilize the Pareto-efficient solutions for dynamic
preferences, thus utilizing past knowledge to adapt to unpredictable
preferences quickly during runtime. Simulation results show that the proposed
scheme outperforms state-of-the-art algorithms for various exploration
strategies, preference variation patterns, and important metrics like overall
reward, energy efficiency, and packet delivery ratio.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [180] [Advancing Software Security and Reliability in Cloud Platforms through AI-based Anomaly Detection](https://arxiv.org/abs/2411.09200)
*Sabbir M. Saleh,Ibrahim Mohammed Sayem,Nazim Madhavji,John Steinbacher*

Main category: cs.SE

TL;DR: 该研究通过AI支持的异常检测技术，提升CI/CD管道的安全性，利用CNN和LSTM模型分析网络流量模式，实现了高准确率（98.69%和98.30%）。


<details>
  <summary>Details</summary>
Motivation: CI/CD管道在云环境中的安全问题日益突出，现有研究多关注静态安全测试，而网络流量模式分析的研究较少。

Method: 结合CNN和LSTM模型，使用CSE-CIC-IDS2018和CSE-CIC-IDS2017数据集检测异常流量模式。

Result: 模型准确率分别达到98.69%和98.30%，并生成日志文件以应对安全挑战。

Conclusion: 该研究为现代DevOps实践提供了增强安全性和可靠性的解决方案。

Abstract: Continuous Integration/Continuous Deployment (CI/CD) is fundamental for
advanced software development, supporting faster and more efficient delivery of
code changes into cloud environments. However, security issues in the CI/CD
pipeline remain challenging, and incidents (e.g., DDoS, Bot, Log4j, etc.) are
happening over the cloud environments. While plenty of literature discusses
static security testing and CI/CD practices, only a few deal with network
traffic pattern analysis to detect different cyberattacks. This research aims
to enhance CI/CD pipeline security by implementing anomaly detection through AI
(Artificial Intelligence) support. The goal is to identify unusual behaviour or
variations from network traffic patterns in pipeline and cloud platforms. The
system shall integrate into the workflow to continuously monitor pipeline
activities and cloud infrastructure. Additionally, it aims to explore adaptive
response mechanisms to mitigate the detected anomalies or security threats.
This research employed two popular network traffic datasets, CSE-CIC-IDS2018
and CSE-CIC-IDS2017. We implemented a combination of Convolution Neural
Network(CNN) and Long Short-Term Memory (LSTM) to detect unusual traffic
patterns. We achieved an accuracy of 98.69% and 98.30% and generated log files
in different CI/CD pipeline stages that resemble the network anomalies affected
to address security challenges in modern DevOps practices, contributing to
advancing software security and reliability.

</details>


### [181] [Document Retrieval Augmented Fine-Tuning (DRAFT) for safety-critical software assessments](https://arxiv.org/abs/2505.01307)
*Regan Bolton,Mohammadreza Sheikhfathollahi,Simon Parkinson,Vanessa Vulovic,Gary Bamford,Dan Basher,Howard Parkinson*

Main category: cs.SE

TL;DR: DRAFT是一种通过文档检索增强微调的方法，用于提升大型语言模型在安全关键合规评估中的表现，实验显示其性能优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 传统的手动评估方法在安全关键软件合规性评估中存在效率低下的问题，需要一种更高效且透明的方法。

Method: DRAFT结合了双检索架构和半自动化数据集生成方法，同时访问软件文档和参考标准，并通过微调优化模型表现。

Result: 实验表明，DRAFT在GPT-4o-mini上实现了7%的正确率提升，并在证据处理、响应结构和领域推理方面有显著改进。

Conclusion: DRAFT为合规评估系统提供了一种实用且透明的改进方法，适用于监管领域。

Abstract: Safety critical software assessment requires robust assessment against
complex regulatory frameworks, a process traditionally limited by manual
evaluation. This paper presents Document Retrieval-Augmented Fine-Tuning
(DRAFT), a novel approach that enhances the capabilities of a large language
model (LLM) for safety-critical compliance assessment. DRAFT builds upon
existing Retrieval-Augmented Generation (RAG) techniques by introducing a novel
fine-tuning framework that accommodates our dual-retrieval architecture, which
simultaneously accesses both software documentation and applicable reference
standards. To fine-tune DRAFT, we develop a semi-automated dataset generation
methodology that incorporates variable numbers of relevant documents with
meaningful distractors, closely mirroring real-world assessment scenarios.
Experiments with GPT-4o-mini demonstrate a 7% improvement in correctness over
the baseline model, with qualitative improvements in evidence handling,
response structure, and domain-specific reasoning. DRAFT represents a practical
approach to improving compliance assessment systems while maintaining the
transparency and evidence-based reasoning essential in regulatory domains.

</details>


### [182] [Aggregating empirical evidence from data strategy studies: a case on model quantization](https://arxiv.org/abs/2505.00816)
*Santiago del Rey,Paulo Sérgio Medeiros dos Santos,Guilherme Horta Travassos,Xavier Franch,Silverio Martínez-Fernández*

Main category: cs.SE

TL;DR: 该研究评估了深度学习系统中模型量化对正确性和资源效率的影响，并探讨了数据策略研究中证据聚合的方法学问题。通过结构化综合方法（SSM）对六项研究进行综合分析，发现量化在资源效率上有显著提升，但对正确性有轻微负面影响。


<details>
  <summary>Details</summary>
Motivation: 随着实证软件工程的发展，数据策略（如分析数字工件而非依赖人类受试者）的研究增多，但结果综合面临新方法学挑战。本研究旨在评估模型量化的效果，并探索数据策略研究中证据聚合的方法。

Method: 采用结构化综合方法（SSM）对六项关于模型量化的实证研究进行定性定量结合的综合分析，提取并聚合了19个证据模型。

Result: 模型量化对正确性指标有轻微负面影响，但对资源效率（存储大小、推理延迟、GPU能耗）有显著提升，是一种适用于资源受限环境的优化策略。证据显示不同量化技术的研究仍较分散。

Conclusion: 模型量化在资源效率上有显著优势，正确性影响较小，适合资源受限场景。同时，SSM方法在数据策略研究中的证据聚合具有可行性。

Abstract: Background: As empirical software engineering evolves, more studies adopt
data strategies$-$approaches that investigate digital artifacts such as models,
source code, or system logs rather than relying on human subjects. Synthesizing
results from such studies introduces new methodological challenges.
  Aims: This study assesses the effects of model quantization on correctness
and resource efficiency in deep learning (DL) systems. Additionally, it
explores the methodological implications of aggregating evidence from empirical
studies that adopt data strategies.
  Method: We conducted a research synthesis of six primary studies that
empirically evaluate model quantization. We applied the Structured Synthesis
Method (SSM) to aggregate the findings, which combines qualitative and
quantitative evidence through diagrammatic modeling. A total of 19 evidence
models were extracted and aggregated.
  Results: The aggregated evidence indicates that model quantization weakly
negatively affects correctness metrics while consistently improving resource
efficiency metrics, including storage size, inference latency, and GPU energy
consumption$-$a manageable trade-off for many DL deployment contexts. Evidence
across quantization techniques remains fragmented, underscoring the need for
more focused empirical studies per technique.
  Conclusions: Model quantization offers substantial efficiency benefits with
minor trade-offs in correctness, making it a suitable optimization strategy for
resource-constrained environments. This study also demonstrates the feasibility
of using SSM to synthesize findings from data strategy-based research.

</details>


### [183] [CppSATD: A Reusable Self-Admitted Technical Debt Dataset in C++](https://arxiv.org/abs/2505.01136)
*Phuoc Pham,Murali Sridharan,Matteo Esposito,Valentina Lenarduzzi*

Main category: cs.SE

TL;DR: 本文介绍了CppSATD数据集，填补了C++语言中自认技术债务（SATD）研究的空白，为跨语言SATD研究提供了基础。


<details>
  <summary>Details</summary>
Motivation: 现有SATD研究主要集中在Java语言，缺乏对其他语言的关注，限制了研究结果的普适性和检测技术的跨语言应用。

Method: 通过构建CppSATD数据集，包含超过531,000条标注的注释及其源代码上下文。

Result: 提供了首个专注于C++的SATD数据集，支持未来SATD检测方法的开发及跨语言研究。

Conclusion: CppSATD数据集为C++及跨语言SATD研究提供了重要资源，推动了该领域的进一步发展。

Abstract: In software development, technical debt (TD) refers to suboptimal
implementation choices made by the developers to meet urgent deadlines and
limited resources, posing challenges for future maintenance. Self-Admitted
Technical Debt (SATD) is a sub-type of TD, representing specific TD instances
``openly admitted'' by the developers and often expressed through source code
comments. Previous research on SATD has focused predominantly on the Java
programming language, revealing a significant gap in cross-language SATD. Such
a narrow focus limits the generalizability of existing findings as well as SATD
detection techniques across multiple programming languages. Our work addresses
such limitation by introducing CppSATD, a dedicated C++ SATD dataset,
comprising over 531,000 annotated comments and their source code contexts. Our
dataset can serve as a foundation for future studies that aim to develop SATD
detection methods in C++, generalize the existing findings to other languages,
or contribute novel insights to cross-language SATD research.

</details>


<div id='math.DS'></div>

# math.DS [[Back]](#toc)

### [184] [Dynamical System Parameter Path Optimization using Persistent Homology](https://arxiv.org/abs/2505.00782)
*Max M. Chumley,Firas A. Khasawneh*

Main category: math.DS

TL;DR: 论文提出了一种基于拓扑数据分析的方法，用于在高维参数空间中优化导航非线性动力系统，通过梯度下降实现参数调整，以达到期望的系统响应。


<details>
  <summary>Details</summary>
Motivation: 非线性动力系统复杂且难以解析研究，高维参数空间中难以确定参数调整方向以实现期望的系统响应或状态。

Method: 利用持久图的可微性定义拓扑语言，通过梯度下降在参数空间中优化移动，生成路径以引导系统达到期望的拓扑特征。

Result: 通过多个动力系统示例展示了如何促进不同特征及选择超参数以实现不同结果。

Conclusion: 该方法为动力系统参数优化提供了一种直观且有效的工具，能够灵活应对不同拓扑特征需求。

Abstract: Nonlinear dynamical systems are complex and typically only simple systems can
be analytically studied. In applications, these systems are usually defined
with a set of tunable parameters and as the parameters are varied the system
response undergoes significant topological changes or bifurcations. In a high
dimensional parameter space, it is difficult to determine which direction to
vary the system parameters to achieve a desired system response or state. In
this paper, we introduce a new approach for optimally navigating a dynamical
system parameter space that is rooted in topological data analysis.
Specifically we use the differentiability of persistence diagrams to define a
topological language for intuitively promoting or deterring different
topological features in the state space response of a dynamical system and use
gradient descent to optimally move from one point in the parameter space to
another. The end result is a path in this space that guides the system to a set
of parameters that yield the desired topological features defined by the loss
function. We show a number of examples by applying the methods to different
dynamical systems and scenarios to demonstrate how to promote different
features and how to choose the hyperparameters to achieve different outcomes.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [185] [Learning Low-Dimensional Embeddings for Black-Box Optimization](https://arxiv.org/abs/2505.01112)
*Riccardo Busetto,Manas Mejari,Marco Forgione,Alberto Bemporad,Dario Piga*

Main category: eess.SY

TL;DR: 提出了一种基于元学习的方法，通过预计算降维流形来优化高维黑盒问题。


<details>
  <summary>Details</summary>
Motivation: 解决黑盒优化在高维问题和有限试验预算下的困难。

Method: 利用元学习预计算降维流形，将优化问题限制在低维空间。

Result: 有效减少了寻找近似最优解所需的计算量。

Conclusion: 该方法为高维黑盒优化提供了一种高效解决方案。

Abstract: When gradient-based methods are impractical, black-box optimization (BBO)
provides a valuable alternative. However, BBO often struggles with
high-dimensional problems and limited trial budgets. In this work, we propose a
novel approach based on meta-learning to pre-compute a reduced-dimensional
manifold where optimal points lie for a specific class of optimization
problems. When optimizing a new problem instance sampled from the class,
black-box optimization is carried out in the reduced-dimensional space,
effectively reducing the effort required for finding near-optimal solutions.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [186] [Multi-site modelling and reconstruction of past extreme skew surges along the French Atlantic coast](https://arxiv.org/abs/2505.00835)
*Nathan Huet,Philippe Naveau,Anne Sabourin*

Main category: stat.AP

TL;DR: 该论文提出了一种新的方法来建模法国大西洋沿岸的极端偏斜涌浪，重点关注站点间的极值依赖结构，并利用多元极值理论和极端回归框架进行预测。


<details>
  <summary>Details</summary>
Motivation: 研究极端偏斜涌浪的建模对海岸风险管理至关重要，尤其是在数据有限的站点如何利用长期观测数据进行历史重建。

Method: 采用峰值超阈值框架，提出新的阈值确定方法；使用多元广义帕累托分布建模极值，并评估一种新的极端回归框架进行点预测。

Result: 通过整合长期观测站点（如布雷斯特和圣纳泽尔）的数据，成功重建了历史偏斜涌浪时间序列。

Conclusion: 该方法为极端偏斜涌浪建模提供了有效工具，尤其适用于数据有限的站点，对海岸风险管理具有重要意义。

Abstract: Appropriate modelling of extreme skew surges is crucial, particularly for
coastal risk management. Our study focuses on modelling extreme skew surges
along the French Atlantic coast, with a particular emphasis on investigating
the extremal dependence structure between stations. We employ the
peak-over-threshold framework, where a multivariate extreme event is defined
whenever at least one location records a large value, though not necessarily
all stations simultaneously. A novel method for determining an appropriate
level (threshold) above which observations can be classified as extreme is
proposed. Two complementary approaches are explored. First, the multivariate
generalized Pareto distribution is employed to model extremes, leveraging its
properties to derive a generative model that predicts extreme skew surges at
one station based on observed extremes at nearby stations. Second, a novel
extreme regression framework is assessed for point predictions. This specific
regression framework enables accurate point predictions using only the "angle"
of input variables, i.e. input variables divided by their norms. The ultimate
objective is to reconstruct historical skew surge time series at stations with
limited data. This is achieved by integrating extreme skew surge data from
stations with longer records, such as Brest and Saint-Nazaire, which provide
over 150 years of observations.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [187] [A Survey on 3D Reconstruction Techniques in Plant Phenotyping: From Classical Methods to Neural Radiance Fields (NeRF), 3D Gaussian Splatting (3DGS), and Beyond](https://arxiv.org/abs/2505.00737)
*Jiajia Li,Xinda Qi,Seyed Hamidreza Nabaei,Meiqi Liu,Dong Chen,Xin Zhang,Xunyuan Yin,Zhaojian Li*

Main category: eess.IV

TL;DR: 本文综述了3D重建技术在植物表型分析中的应用，包括经典方法、NeRF和3DGS，探讨了它们的优势、局限性和未来前景。


<details>
  <summary>Details</summary>
Motivation: 植物表型分析对精准农业和作物改良至关重要，3D重建技术为自动化表型分析提供了新工具。

Method: 回顾了经典重建方法、NeRF和3DGS的技术原理、应用及性能。

Result: 经典方法简单灵活但面临数据密度和噪声问题；NeRF可实现高质量重建但计算成本高；3DGS在效率和扩展性上具有潜力。

Conclusion: 这些技术为自动化植物表型分析提供了多样化的解决方案，未来需进一步优化以适应农业需求。

Abstract: Plant phenotyping plays a pivotal role in understanding plant traits and
their interactions with the environment, making it crucial for advancing
precision agriculture and crop improvement. 3D reconstruction technologies have
emerged as powerful tools for capturing detailed plant morphology and
structure, offering significant potential for accurate and automated
phenotyping. This paper provides a comprehensive review of the 3D
reconstruction techniques for plant phenotyping, covering classical
reconstruction methods, emerging Neural Radiance Fields (NeRF), and the novel
3D Gaussian Splatting (3DGS) approach. Classical methods, which often rely on
high-resolution sensors, are widely adopted due to their simplicity and
flexibility in representing plant structures. However, they face challenges
such as data density, noise, and scalability. NeRF, a recent advancement,
enables high-quality, photorealistic 3D reconstructions from sparse viewpoints,
but its computational cost and applicability in outdoor environments remain
areas of active research. The emerging 3DGS technique introduces a new paradigm
in reconstructing plant structures by representing geometry through Gaussian
primitives, offering potential benefits in both efficiency and scalability. We
review the methodologies, applications, and performance of these approaches in
plant phenotyping and discuss their respective strengths, limitations, and
future prospects (https://github.com/JiajiaLi04/3D-Reconstruction-Plants).
Through this review, we aim to provide insights into how these diverse 3D
reconstruction techniques can be effectively leveraged for automated and
high-throughput plant phenotyping, contributing to the next generation of
agricultural technology.

</details>


### [188] [XeMap: Contextual Referring in Large-Scale Remote Sensing Environments](https://arxiv.org/abs/2505.00738)
*Yuxi Li,Lu Si,Yujie Hou,Chengaung Liu,Bin Li,Hongjian Fang,Jun Zhang*

Main category: eess.IV

TL;DR: 论文提出了一种新的任务XeMap，专注于遥感图像中文本描述区域的细粒度定位，并设计了XeMap-Network架构，通过多模态对齐和注意力机制提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如图像级标注/检索和对象级检测/分割）难以捕捉中尺度语义实体，而XeMap任务旨在填补这一空白。

Method: 提出XeMap-Network，包含融合层（自注意力和跨模态注意力）和HMSA模块（多尺度视觉特征与文本语义对齐）。

Result: 在零样本设置下，XeMap-Network优于现有方法，展示了其在遥感图像中的高效性。

Conclusion: XeMap任务和XeMap-Network为大规模遥感场景的语义理解提供了新思路和工具。

Abstract: Advancements in remote sensing (RS) imagery have provided high-resolution
detail and vast coverage, yet existing methods, such as image-level
captioning/retrieval and object-level detection/segmentation, often fail to
capture mid-scale semantic entities essential for interpreting large-scale
scenes. To address this, we propose the conteXtual referring Map (XeMap) task,
which focuses on contextual, fine-grained localization of text-referred regions
in large-scale RS scenes. Unlike traditional approaches, XeMap enables precise
mapping of mid-scale semantic entities that are often overlooked in image-level
or object-level methods. To achieve this, we introduce XeMap-Network, a novel
architecture designed to handle the complexities of pixel-level cross-modal
contextual referring mapping in RS. The network includes a fusion layer that
applies self- and cross-attention mechanisms to enhance the interaction between
text and image embeddings. Furthermore, we propose a Hierarchical Multi-Scale
Semantic Alignment (HMSA) module that aligns multiscale visual features with
the text semantic vector, enabling precise multimodal matching across
large-scale RS imagery. To support XeMap task, we provide a novel, annotated
dataset, XeMap-set, specifically tailored for this task, overcoming the lack of
XeMap datasets in RS imagery. XeMap-Network is evaluated in a zero-shot setting
against state-of-the-art methods, demonstrating superior performance. This
highlights its effectiveness in accurately mapping referring regions and
providing valuable insights for interpreting large-scale RS environments.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [189] [GENMO: A GENeralist Model for Human MOtion](https://arxiv.org/abs/2505.01425)
*Jiefeng Li,Jinkun Cao,Haotian Zhang,Davis Rempe,Jan Kautz,Umar Iqbal,Ye Yuan*

Main category: cs.GR

TL;DR: GENMO是一个统一的人类运动通用模型，将运动生成和估计结合在一个框架中，通过约束生成和扩散模型实现高精度和多样性。


<details>
  <summary>Details</summary>
Motivation: 传统方法将运动生成和估计分开，限制了知识共享和模型效率。GENMO旨在通过统一框架解决这一问题。

Method: 将运动估计重新定义为约束生成，结合回归和扩散模型，利用视频和文本数据增强生成多样性。

Result: GENMO在多种任务中表现优异，既能精确估计运动轨迹，又能生成多样化运动。

Conclusion: GENMO展示了统一模型的潜力，通过协同效应提升运动任务的性能。

Abstract: Human motion modeling traditionally separates motion generation and
estimation into distinct tasks with specialized models. Motion generation
models focus on creating diverse, realistic motions from inputs like text,
audio, or keyframes, while motion estimation models aim to reconstruct accurate
motion trajectories from observations like videos. Despite sharing underlying
representations of temporal dynamics and kinematics, this separation limits
knowledge transfer between tasks and requires maintaining separate models. We
present GENMO, a unified Generalist Model for Human Motion that bridges motion
estimation and generation in a single framework. Our key insight is to
reformulate motion estimation as constrained motion generation, where the
output motion must precisely satisfy observed conditioning signals. Leveraging
the synergy between regression and diffusion, GENMO achieves accurate global
motion estimation while enabling diverse motion generation. We also introduce
an estimation-guided training objective that exploits in-the-wild videos with
2D annotations and text descriptions to enhance generative diversity.
Furthermore, our novel architecture handles variable-length motions and mixed
multimodal conditions (text, audio, video) at different time intervals,
offering flexible control. This unified approach creates synergistic benefits:
generative priors improve estimated motions under challenging conditions like
occlusions, while diverse video data enhances generation capabilities.
Extensive experiments demonstrate GENMO's effectiveness as a generalist
framework that successfully handles multiple human motion tasks within a single
model.

</details>


### [190] [Model See Model Do: Speech-Driven Facial Animation with Style Control](https://arxiv.org/abs/2505.01319)
*Yifang Pan,Karan Singh,Luiz Gustavo Hafemann*

Main category: cs.GR

TL;DR: 提出了一种基于示例的生成框架，通过潜在扩散模型和风格参考片段，生成高表现力和时间一致的面部动画。


<details>
  <summary>Details</summary>
Motivation: 现有方法在实现准确的唇同步和基本情感表达方面取得了进展，但难以捕捉和传递细微的表演风格。

Method: 引入了一种称为风格基础的调节机制，从参考中提取关键姿势，并加性地指导扩散生成过程，以在不影响唇同步质量的情况下适配风格。

Result: 定性、定量和感知评估表明，该方法能忠实再现所需风格，并在各种语音场景中实现卓越的唇同步。

Conclusion: 该方法成功解决了风格捕捉与唇同步的平衡问题，为高表现力面部动画提供了有效解决方案。

Abstract: Speech-driven 3D facial animation plays a key role in applications such as
virtual avatars, gaming, and digital content creation. While existing methods
have made significant progress in achieving accurate lip synchronization and
generating basic emotional expressions, they often struggle to capture and
effectively transfer nuanced performance styles. We propose a novel
example-based generation framework that conditions a latent diffusion model on
a reference style clip to produce highly expressive and temporally coherent
facial animations. To address the challenge of accurately adhering to the style
reference, we introduce a novel conditioning mechanism called style basis,
which extracts key poses from the reference and additively guides the diffusion
generation process to fit the style without compromising lip synchronization
quality. This approach enables the model to capture subtle stylistic cues while
ensuring that the generated animations align closely with the input speech.
Extensive qualitative, quantitative, and perceptual evaluations demonstrate the
effectiveness of our method in faithfully reproducing the desired style while
achieving superior lip synchronization across various speech scenarios.

</details>

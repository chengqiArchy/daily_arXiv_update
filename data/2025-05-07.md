<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 34]
- [cs.AI](#cs.AI) [Total: 25]
- [cs.LG](#cs.LG) [Total: 57]
- [cs.IR](#cs.IR) [Total: 5]
- [physics.soc-ph](#physics.soc-ph) [Total: 1]
- [q-bio.NC](#q-bio.NC) [Total: 2]
- [cs.SE](#cs.SE) [Total: 4]
- [cs.CY](#cs.CY) [Total: 7]
- [stat.ML](#stat.ML) [Total: 10]
- [cs.CR](#cs.CR) [Total: 4]
- [cs.RO](#cs.RO) [Total: 10]
- [physics.ao-ph](#physics.ao-ph) [Total: 1]
- [cs.NE](#cs.NE) [Total: 2]
- [cs.MA](#cs.MA) [Total: 3]
- [cs.IT](#cs.IT) [Total: 1]
- [q-bio.BM](#q-bio.BM) [Total: 1]
- [cs.CV](#cs.CV) [Total: 27]
- [astro-ph.SR](#astro-ph.SR) [Total: 1]
- [stat.CO](#stat.CO) [Total: 1]
- [cs.DC](#cs.DC) [Total: 1]
- [cs.SD](#cs.SD) [Total: 4]
- [math.OC](#math.OC) [Total: 1]
- [math.NA](#math.NA) [Total: 1]
- [cs.HC](#cs.HC) [Total: 3]
- [eess.SY](#eess.SY) [Total: 1]
- [math.AP](#math.AP) [Total: 1]
- [quant-ph](#quant-ph) [Total: 2]
- [cs.DS](#cs.DS) [Total: 1]
- [cs.NI](#cs.NI) [Total: 1]
- [eess.IV](#eess.IV) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Sentient Agent as a Judge: Evaluating Higher-Order Social Cognition in Large Language Models](https://arxiv.org/abs/2505.02847)
*Bang Zhang,Ruotian Ma,Qingxuan Jiang,Peisong Wang,Jiaqi Chen,Zheng Xie,Xingyu Chen,Yue Wang,Fanghua Ye,Jian Li,Yifan Yang,Zhaopeng Tu,Xiaolong Li*

Main category: cs.CL

TL;DR: SAGE框架通过模拟人类情感变化和内心思考，评估大语言模型的社会认知能力，填补了传统评估方法的不足。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法无法评估大语言模型对人类情感和社会认知理解的局限性。

Method: 提出SAGE框架，通过模拟人类情感变化和内心思考，在多轮对话中评估模型表现。

Result: 实验显示SAGE的情感评分与心理学指标高度相关，并揭示了前沿模型与早期基线之间的显著差距。

Conclusion: SAGE为评估语言模型的社会认知能力提供了可扩展且可解释的工具。

Abstract: Assessing how well a large language model (LLM) understands human, rather
than merely text, remains an open challenge. To bridge the gap, we introduce
Sentient Agent as a Judge (SAGE), an automated evaluation framework that
measures an LLM's higher-order social cognition. SAGE instantiates a Sentient
Agent that simulates human-like emotional changes and inner thoughts during
interaction, providing a more realistic evaluation of the tested model in
multi-turn conversations. At every turn, the agent reasons about (i) how its
emotion changes, (ii) how it feels, and (iii) how it should reply, yielding a
numerical emotion trajectory and interpretable inner thoughts. Experiments on
100 supportive-dialogue scenarios show that the final Sentient emotion score
correlates strongly with Barrett-Lennard Relationship Inventory (BLRI) ratings
and utterance-level empathy metrics, validating psychological fidelity. We also
build a public Sentient Leaderboard covering 18 commercial and open-source
models that uncovers substantial gaps (up to 4x) between frontier systems
(GPT-4o-Latest, Gemini2.5-Pro) and earlier baselines, gaps not reflected in
conventional leaderboards (e.g., Arena). SAGE thus provides a principled,
scalable and interpretable tool for tracking progress toward genuinely
empathetic and socially adept language agents.

</details>


### [2] [Harnessing Structured Knowledge: A Concept Map-Based Approach for High-Quality Multiple Choice Question Generation with Effective Distractors](https://arxiv.org/abs/2505.02850)
*Nicy Scaria,Silvester John Joseph Kennedy,Diksha Seth,Ananya Thakur,Deepak Subramani*

Main category: cs.CL

TL;DR: 论文提出了一种基于分层概念图的框架，利用LLM生成高质量多选题（MCQ），针对高中物理领域，结合常见误解设计干扰项，显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 手动生成高质量MCQ耗时且依赖专家知识，现有自动化方法难以覆盖高认知水平和领域特定误解。

Method: 开发分层概念图作为结构化知识库，通过自动化流程检索相关部分指导LLM生成MCQ和干扰项，并进行自动验证。

Result: 专家评估显示成功率达75.20%，学生测试中猜测成功率降至28.05%，显著优于基线方法（约37%）。

Conclusion: 概念图驱动的方法能有效评估多认知水平，快速识别概念差距，支持规模化反馈和干预。

Abstract: Generating high-quality MCQs, especially those targeting diverse cognitive
levels and incorporating common misconceptions into distractor design, is
time-consuming and expertise-intensive, making manual creation impractical at
scale. Current automated approaches typically generate questions at lower
cognitive levels and fail to incorporate domain-specific misconceptions. This
paper presents a hierarchical concept map-based framework that provides
structured knowledge to guide LLMs in generating MCQs with distractors. We
chose high-school physics as our test domain and began by developing a
hierarchical concept map covering major Physics topics and their
interconnections with an efficient database design. Next, through an automated
pipeline, topic-relevant sections of these concept maps are retrieved to serve
as a structured context for the LLM to generate questions and distractors that
specifically target common misconceptions. Lastly, an automated validation is
completed to ensure that the generated MCQs meet the requirements provided. We
evaluate our framework against two baseline approaches: a base LLM and a
RAG-based generation. We conducted expert evaluations and student assessments
of the generated MCQs. Expert evaluation shows that our method significantly
outperforms the baseline approaches, achieving a success rate of 75.20% in
meeting all quality criteria compared to approximately 37% for both baseline
methods. Student assessment data reveal that our concept map-driven approach
achieved a significantly lower guess success rate of 28.05% compared to 37.10%
for the baselines, indicating a more effective assessment of conceptual
understanding. The results demonstrate that our concept map-based approach
enables robust assessment across cognitive levels and instant identification of
conceptual gaps, facilitating faster feedback loops and targeted interventions
at scale.

</details>


### [3] [30DayGen: Leveraging LLMs to Create a Content Corpus for Habit Formation](https://arxiv.org/abs/2505.02851)
*Franklin Zhang,Sonya Zhang,Alon Halevy*

Main category: cs.CL

TL;DR: 30 Day Me是一款利用大型语言模型（LLMs）帮助用户分解目标并跟踪进度的习惯养成应用，核心是30DAYGEN系统，能生成3,531种独特的30天挑战。


<details>
  <summary>Details</summary>
Motivation: 通过LLMs快速构建领域特定内容库，支持行为和教育的需求。

Method: 利用LLMs生成内容并进行语义去重，构建30DAYGEN系统。

Result: 成功生成3,531种独特的30天挑战，支持用户目标对齐的实时搜索。

Conclusion: 展示了LLMs在行为和教育领域的实用性，提出了高效的内容生成和去重流程。

Abstract: In this paper, we present 30 Day Me, a habit formation application that
leverages Large Language Models (LLMs) to help users break down their goals
into manageable, actionable steps and track their progress. Central to the app
is the 30DAYGEN system, which generates 3,531 unique 30-day challenges sourced
from over 15K webpages, and enables runtime search of challenge ideas aligned
with user-defined goals. We showcase how LLMs can be harnessed to rapidly
construct domain specific content corpora for behavioral and educational
purposes, and propose a practical pipeline that incorporates effective LLM
enhanced approaches for content generation and semantic deduplication.

</details>


### [4] [Ensuring Reproducibility in Generative AI Systems for General Use Cases: A Framework for Regression Testing and Open Datasets](https://arxiv.org/abs/2505.02854)
*Masumi Morishige,Ryo Koshihara*

Main category: cs.CL

TL;DR: GPR-bench是一个轻量级、可扩展的基准测试工具，用于评估生成式AI系统的可重复性和可靠性，支持双语任务和自动化评估。实验表明，新模型在正确性上略有提升，但提示工程对简洁性有显著影响。


<details>
  <summary>Details</summary>
Motivation: 解决生成式AI系统因模型更新或提示修改导致行为漂移的可重复性和可靠性问题。

Method: 引入GPR-bench，包含双语数据集（英语和日语）和自动化评估管道，使用“LLM-as-a-Judge”评分。测试了三种模型版本和两种提示配置。

Result: 新模型在正确性上略有提升但差异不显著；简洁性提示显著提高了输出简洁性（+12.37 pp），准确性仅轻微下降（-1.7 pp）。

Conclusion: GPR-bench为社区驱动的扩展提供了基础，同时提出了快速演变的语言模型基准设计的重要考虑。

Abstract: Reproducibility and reliability remain pressing challenges for generative AI
systems whose behavior can drift with each model update or prompt revision. We
introduce GPR-bench, a lightweight, extensible benchmark that operationalizes
regression testing for general purpose use cases. GPR-bench couples an open,
bilingual (English and Japanese) dataset covering eight task categories (e.g.,
text generation, code generation, and information retrieval) and 10 scenarios
in each task categories (80 total test cases for each language) with an
automated evaluation pipeline that employs "LLM-as-a-Judge" scoring of
correctness and conciseness. Experiments across three recent model versions -
gpt-4o-mini, o3-mini, and o4-mini - and two prompt configurations (default
versus concise-writing instruction) reveal heterogeneous quality. Our results
show that newer models generally improve correctness, but the differences are
modest and not statistically significant, suggesting that GPR-bench may not be
sufficiently challenging to differentiate between recent model versions. In
contrast, the concise-writing instruction significantly enhances conciseness
(+12.37 pp, Mann-Whitney U test: p < 0.001, effect size r = 0.2995) with
minimal degradations on accuracy (-1.7 pp), demonstrating the effectiveness of
prompt engineering. Released under the MIT License, GPR- bench lowers the
barrier to initiating reproducibility monitoring and provides a foundation for
community-driven extensions, while also raising important considerations about
benchmark design for rapidly evolving language models.

</details>


### [5] [Towards High-Fidelity Synthetic Multi-platform Social Media Datasets via Large Language Models](https://arxiv.org/abs/2505.02858)
*Henry Tari,Nojus Sereiva,Rishabh Kaushal,Thales Bertaglia,Adriana Iamnitchi*

Main category: cs.CL

TL;DR: 论文探讨了利用大语言模型生成跨平台社交媒体合成数据的潜力，提出多平台主题提示方法，并评估了不同模型在生成数据时的表现。


<details>
  <summary>Details</summary>
Motivation: 由于成本和平台限制，获取跨平台社交媒体数据集困难，研究旨在通过合成数据解决这一问题。

Method: 采用多平台主题提示方法，使用不同语言模型生成合成数据，并与真实数据对比评估。

Result: 大语言模型生成跨平台社交媒体数据具有潜力，不同模型表现各异，后处理方法可能提升数据保真度。

Conclusion: 研究为大语言模型生成跨平台社交媒体数据提供了实证支持，并提出了新的保真度指标。

Abstract: Social media datasets are essential for research on a variety of topics, such
as disinformation, influence operations, hate speech detection, or influencer
marketing practices. However, access to social media datasets is often
constrained due to costs and platform restrictions. Acquiring datasets that
span multiple platforms, which is crucial for understanding the digital
ecosystem, is particularly challenging. This paper explores the potential of
large language models to create lexically and semantically relevant social
media datasets across multiple platforms, aiming to match the quality of real
data. We propose multi-platform topic-based prompting and employ various
language models to generate synthetic data from two real datasets, each
consisting of posts from three different social media platforms. We assess the
lexical and semantic properties of the synthetic data and compare them with
those of the real data. Our empirical findings show that using large language
models to generate synthetic multi-platform social media data is promising,
different language models perform differently in terms of fidelity, and a
post-processing approach might be needed for generating high-fidelity synthetic
datasets for research. In addition to the empirical evaluation of three state
of the art large language models, our contributions include new fidelity
metrics specific to multi-platform social media datasets.

</details>


### [6] [Enhancing ML Model Interpretability: Leveraging Fine-Tuned Large Language Models for Better Understanding of AI](https://arxiv.org/abs/2505.02859)
*Jonas Bokstaller,Julia Altheimer,Julian Dormehl,Alina Buss,Jasper Wiltfang,Johannes Schneider,Maximilian Röglinger*

Main category: cs.CL

TL;DR: 本文提出了一种结合可解释人工智能（XAI）和大型语言模型（LLM）的交互式聊天机器人参考架构，用于提升机器学习模型的可解释性，并在电池健康状态（SoH）预测中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习模型的黑箱特性日益明显，可解释AI（XAI）的需求增加。同时，大型语言模型（LLM）在理解人类语言和复杂模式方面取得了显著进展。本文旨在结合两者，提升XAI的可解释性。

Method: 提出了一种基于微调LLM的交互式聊天机器人参考架构，用于解释XAI。该架构在电池健康状态（SoH）预测中实例化，并通过多轮评估和演示验证其设计。

Result: 评估表明，该原型显著提升了机器学习模型的可解释性，尤其对XAI经验较少的用户效果明显。

Conclusion: 结合XAI和LLM的交互式聊天机器人架构能有效提升模型的可解释性，尤其在特定领域（如电池SoH预测）中表现突出。

Abstract: Across various sectors applications of eXplainableAI (XAI) gained momentum as
the increasing black-boxedness of prevailing Machine Learning (ML) models
became apparent. In parallel, Large Language Models (LLMs) significantly
developed in their abilities to understand human language and complex patterns.
By combining both, this paper presents a novel reference architecture for the
interpretation of XAI through an interactive chatbot powered by a fine-tuned
LLM. We instantiate the reference architecture in the context of
State-of-Health (SoH) prediction for batteries and validate its design in
multiple evaluation and demonstration rounds. The evaluation indicates that the
implemented prototype enhances the human interpretability of ML, especially for
users with less experience with XAI.

</details>


### [7] [Cannot See the Forest for the Trees: Invoking Heuristics and Biases to Elicit Irrational Choices of LLMs](https://arxiv.org/abs/2505.02862)
*Haoming Yang,Ke Ma,Xiaojun Jia,Yingfei Sun,Qianqian Xu,Qingming Huang*

Main category: cs.CL

TL;DR: 本文提出了一种名为ICRT的新型越狱攻击框架，通过认知启发方法优化恶意提示，并引入基于排行的危害性评估指标，成功绕过主流LLM的安全机制。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLM）表现优异，但其仍易受越狱攻击影响。现有研究多依赖暴力优化或人工设计，未能揭示实际场景中的潜在风险。

Method: ICRT框架利用人类认知启发（如简单效应和相关性偏差）分解和重组恶意提示，同时采用基于排行的危害性评估方法（如Elo、HodgeRank和Rank Centrality）。

Result: 实验表明，ICRT能有效绕过主流LLM的安全机制，生成高风险内容。

Conclusion: 该研究揭示了越狱攻击的风险，并为开发更强防御策略提供了参考。

Abstract: Despite the remarkable performance of Large Language Models (LLMs), they
remain vulnerable to jailbreak attacks, which can compromise their safety
mechanisms. Existing studies often rely on brute-force optimization or manual
design, failing to uncover potential risks in real-world scenarios. To address
this, we propose a novel jailbreak attack framework, ICRT, inspired by
heuristics and biases in human cognition. Leveraging the simplicity effect, we
employ cognitive decomposition to reduce the complexity of malicious prompts.
Simultaneously, relevance bias is utilized to reorganize prompts, enhancing
semantic alignment and inducing harmful outputs effectively. Furthermore, we
introduce a ranking-based harmfulness evaluation metric that surpasses the
traditional binary success-or-failure paradigm by employing ranking aggregation
methods such as Elo, HodgeRank, and Rank Centrality to comprehensively quantify
the harmfulness of generated content. Experimental results show that our
approach consistently bypasses mainstream LLMs' safety mechanisms and generates
high-risk content, providing insights into jailbreak attack risks and
contributing to stronger defense strategies.

</details>


### [8] [Accelerating Large Language Model Reasoning via Speculative Search](https://arxiv.org/abs/2505.02865)
*Zhihai Wang,Jie Wang,Jilai Pan,Xilin Xia,Huiling Zhen,Mingxuan Yuan,Jianye Hao,Feng Wu*

Main category: cs.CL

TL;DR: 提出了一种名为SpecSearch的新框架，通过小模型与大模型的协作优化思维生成，显著加速LLM推理，同时保持推理质量。


<details>
  <summary>Details</summary>
Motivation: 解决基于树搜索的推理方法因生成大量中间推理步骤导致的高延迟问题，限制LLM的适用性。

Method: 利用小模型与大模型在思维和标记级别协作，结合质量保留拒绝机制过滤低质量思维。

Result: 在Qwen和Llama模型上实验表明，SpecSearch比现有方法快2.12倍，且推理质量相当。

Conclusion: SpecSearch有效加速LLM推理，同时保持高质量，具有广泛应用潜力。

Abstract: Tree-search-based reasoning methods have significantly enhanced the reasoning
capability of large language models (LLMs) by facilitating the exploration of
multiple intermediate reasoning steps, i.e., thoughts. However, these methods
suffer from substantial inference latency, as they have to generate numerous
reasoning thoughts, severely limiting LLM applicability. To address this
challenge, we propose a novel Speculative Search (SpecSearch) framework that
significantly accelerates LLM reasoning by optimizing thought generation.
Specifically, SpecSearch utilizes a small model to strategically collaborate
with a large model at both thought and token levels, efficiently generating
high-quality reasoning thoughts. The major pillar of SpecSearch is a novel
quality-preserving rejection mechanism, which effectively filters out thoughts
whose quality falls below that of the large model's outputs. Moreover, we show
that SpecSearch preserves comparable reasoning quality to the large model.
Experiments on both the Qwen and Llama models demonstrate that SpecSearch
significantly outperforms state-of-the-art approaches, achieving up to
2.12$\times$ speedup with comparable reasoning quality.

</details>


### [9] [Decoding Open-Ended Information Seeking Goals from Eye Movements in Reading](https://arxiv.org/abs/2505.02872)
*Cfir Avraham Hadar,Omer Shubi,Yoav Meiri,Yevgeni Berzak*

Main category: cs.CL

TL;DR: 该论文研究了如何通过眼动数据自动解码读者的开放式阅读目标，并提出了目标分类和目标重建任务，使用多模态LLM结合眼动和文本数据，实验表明该方法在两项任务上均取得了显著成功。


<details>
  <summary>Details</summary>
Motivation: 人们在阅读时通常有特定的目标或兴趣点，但此前从未研究过是否能通过眼动数据自动解码这些开放式阅读目标。

Method: 提出了目标分类和目标重建任务及评估框架，使用大规模英语阅读眼动数据，开发并比较了多种结合眼动和文本的多模态LLM模型。

Result: 实验表明，LLM能够从眼动数据中有效提取读者文本特定目标的信息，两项任务均取得显著成功。

Conclusion: 研究表明，LLM可以从眼动数据中解码读者的阅读目标，为理解阅读行为提供了新工具。

Abstract: When reading, we often have specific information that interests us in a text.
For example, you might be reading this paper because you are curious about LLMs
for eye movements in reading, the experimental design, or perhaps you only care
about the question ``but does it work?''. More broadly, in daily life, people
approach texts with any number of text-specific goals that guide their reading
behavior. In this work, we ask, for the first time, whether open-ended reading
goals can be automatically decoded from eye movements in reading. To address
this question, we introduce goal classification and goal reconstruction tasks
and evaluation frameworks, and use large-scale eye tracking for reading data in
English with hundreds of text-specific information seeking tasks. We develop
and compare several discriminative and generative multimodal LLMs that combine
eye movements and text for goal classification and goal reconstruction. Our
experiments show considerable success on both tasks, suggesting that LLMs can
extract valuable information about the readers' text-specific goals from eye
movements.

</details>


### [10] [Logits-Constrained Framework with RoBERTa for Ancient Chinese NER](https://arxiv.org/abs/2505.02983)
*Wenjie Hua,Shenghan Xu*

Main category: cs.CL

TL;DR: 提出了一种Logits-Constrained框架用于古汉语命名实体识别，结合GujiRoBERTa和可微分解码机制，在EvaHan 2025基准上表现优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 解决古汉语命名实体识别中高标签或大数据场景下的性能问题。

Method: 两阶段模型：GujiRoBERTa用于上下文编码，可微分解码机制约束BMES标签转移。

Result: LC框架在性能上优于传统CRF和BiLSTM方法。

Conclusion: 提出的模型选择标准为实际古汉语NLP任务提供了实用指导。

Abstract: This paper presents a Logits-Constrained (LC) framework for Ancient Chinese
Named Entity Recognition (NER), evaluated on the EvaHan 2025 benchmark. Our
two-stage model integrates GujiRoBERTa for contextual encoding and a
differentiable decoding mechanism to enforce valid BMES label transitions.
Experiments demonstrate that LC improves performance over traditional CRF and
BiLSTM-based approaches, especially in high-label or large-data settings. We
also propose a model selection criterion balancing label complexity and dataset
size, providing practical guidance for real-world Ancient Chinese NLP tasks.

</details>


### [11] [RADLADS: Rapid Attention Distillation to Linear Attention Decoders at Scale](https://arxiv.org/abs/2505.03005)
*Daniel Goldstein,Eric Alcaide,Janna Lu,Eugene Cheah*

Main category: cs.CL

TL;DR: RADLADS提出了一种快速将softmax注意力Transformer转换为线性注意力解码器模型的协议，并引入了两种新的RWKV变体架构。转换过程仅需350-700M tokens，成本低于2000美元，且推理质量接近原始Transformer。


<details>
  <summary>Details</summary>
Motivation: 旨在高效且低成本地将现有Transformer模型转换为线性注意力模型，同时保持高性能。

Method: 通过RADLADS协议进行模型转换，并开发了两种新的RWKV变体架构。转换过程仅需少量token和低成本。

Result: 转换后的模型在标准基准测试中表现出色，达到同类线性注意力模型的SOTA性能。

Conclusion: RADLADS提供了一种高效、低成本的模型转换方法，适用于大规模模型，并开源了相关资源。

Abstract: We present Rapid Attention Distillation to Linear Attention Decoders at Scale
(RADLADS), a protocol for rapidly converting softmax attention transformers
into linear attention decoder models, along with two new RWKV-variant
architectures, and models converted from popular Qwen2.5 open source models in
7B, 32B, and 72B sizes. Our conversion process requires only 350-700M tokens,
less than 0.005% of the token count used to train the original teacher models.
Converting to our 72B linear attention model costs less than \$2,000 USD at
today's prices, yet quality at inference remains close to the original
transformer. These models achieve state-of-the-art downstream performance
across a set of standard benchmarks for linear attention models of their size.
We release all our models on HuggingFace under the Apache 2.0 license, with the
exception of our 72B models which are also governed by the Qwen License
Agreement.
  Models at
https://huggingface.co/collections/recursal/radlads-6818ee69e99e729ba8a87102
Training Code at https://github.com/recursal/RADLADS-paper

</details>


### [12] [Memorization or Interpolation ? Detecting LLM Memorization through Input Perturbation Analysis](https://arxiv.org/abs/2505.03019)
*Albérick Euraste Djiré,Abdoul Kader Kaboré,Earl T. Barr,Jacques Klein,Tegawendé F. Bissyandé*

Main category: cs.CL

TL;DR: PEARL是一种检测大型语言模型（LLM）记忆现象的新方法，通过输入扰动评估模型输出的一致性，区分真实泛化与记忆。


<details>
  <summary>Details</summary>
Motivation: LLM在训练中可能记忆而非泛化数据，引发隐私、知识产权和评估可靠性问题。

Method: PEARL通过输入扰动检测模型输出敏感性，无需访问模型内部。

Result: 在Pythia和GPT 4o模型上验证，成功识别记忆现象，如圣经文本和HumanEval代码。

Conclusion: PEARL为识别LLM记忆行为提供了有效框架，支持训练数据来源推断。

Abstract: While Large Language Models (LLMs) achieve remarkable performance through
training on massive datasets, they can exhibit concerning behaviors such as
verbatim reproduction of training data rather than true generalization. This
memorization phenomenon raises significant concerns about data privacy,
intellectual property rights, and the reliability of model evaluations. This
paper introduces PEARL, a novel approach for detecting memorization in LLMs.
PEARL assesses how sensitive an LLM's performance is to input perturbations,
enabling memorization detection without requiring access to the model's
internals. We investigate how input perturbations affect the consistency of
outputs, enabling us to distinguish between true generalization and
memorization. Our findings, following extensive experiments on the Pythia open
model, provide a robust framework for identifying when the model simply
regurgitates learned information. Applied on the GPT 4o models, the PEARL
framework not only identified cases of memorization of classic texts from the
Bible or common code from HumanEval but also demonstrated that it can provide
supporting evidence that some data, such as from the New York Times news
articles, were likely part of the training data of a given model.

</details>


### [13] [A Typology of Synthetic Datasets for Dialogue Processing in Clinical Contexts](https://arxiv.org/abs/2505.03025)
*Steven Bedrick,A. Seza Doğruöz,Sergiu Nisioi*

Main category: cs.CL

TL;DR: 论文综述了医疗领域中合成数据集的创建、评估和使用方法，并提出了新的分类法以比较和评估数据合成类型和程度。


<details>
  <summary>Details</summary>
Motivation: 由于临床对话数据的敏感性和收集难度，合成数据集在医疗领域变得重要，但缺乏理论指导其最佳使用和泛化。

Method: 综述了合成数据集的创建和评估方法，并提出了一种新的分类法。

Result: 论文总结了合成数据集在医疗对话任务中的应用，并提出了分类法以促进比较和评估。

Conclusion: 合成数据集在医疗领域有潜力，但需要更系统的理论和方法支持其使用和泛化。

Abstract: Synthetic data sets are used across linguistic domains and NLP tasks,
particularly in scenarios where authentic data is limited (or even
non-existent). One such domain is that of clinical (healthcare) contexts, where
there exist significant and long-standing challenges (e.g., privacy,
anonymization, and data governance) which have led to the development of an
increasing number of synthetic datasets. One increasingly important category of
clinical dataset is that of clinical dialogues which are especially sensitive
and difficult to collect, and as such are commonly synthesized.
  While such synthetic datasets have been shown to be sufficient in some
situations, little theory exists to inform how they may be best used and
generalized to new applications. In this paper, we provide an overview of how
synthetic datasets are created, evaluated and being used for dialogue related
tasks in the medical domain. Additionally, we propose a novel typology for use
in classifying types and degrees of data synthesis, to facilitate comparison
and evaluation.

</details>


### [14] [UCSC at SemEval-2025 Task 3: Context, Models and Prompt Optimization for Automated Hallucination Detection in LLM Output](https://arxiv.org/abs/2505.03030)
*Sicong Huang,Jincheng He,Shiyuan Huang,Karthik Raja Anandan,Arkajyoti Chakraborty,Ian Lane*

Main category: cs.CL

TL;DR: 论文介绍了UCSC系统在Mu-SHROOM任务中的提交，提出了一种检测和定位大语言模型幻觉的框架，并在多语言任务中表现最佳。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在知识密集型查询中产生的幻觉问题，并精确定位幻觉发生的位置。

Method: 提出一个框架，包括检索相关上下文、识别答案中的虚假内容，并将其映射回LLM输出的特定部分，同时通过自动优化提示增强效果。

Result: 系统在多语言任务中平均排名第一，表现最佳。

Conclusion: 论文展示了有效检测和定位幻觉的方法，并公开了代码和实验结果。

Abstract: Hallucinations pose a significant challenge for large language models when
answering knowledge-intensive queries. As LLMs become more widely adopted, it
is crucial not only to detect if hallucinations occur but also to pinpoint
exactly where in the LLM output they occur. SemEval 2025 Task 3, Mu-SHROOM:
Multilingual Shared-task on Hallucinations and Related Observable
Overgeneration Mistakes, is a recent effort in this direction. This paper
describes the UCSC system submission to the shared Mu-SHROOM task. We introduce
a framework that first retrieves relevant context, next identifies false
content from the answer, and finally maps them back to spans in the LLM output.
The process is further enhanced by automatically optimizing prompts. Our system
achieves the highest overall performance, ranking #1 in average position across
all languages. We release our code and experiment results.

</details>


### [15] [Teaching Models to Understand (but not Generate) High-risk Data](https://arxiv.org/abs/2505.03052)
*Ryan Wang,Matthew Finlayson,Luca Soldaini,Swabha Swayamdipta,Robin Jia*

Main category: cs.CL

TL;DR: SLUNG是一种预训练范式，通过选择性损失让模型理解高风险数据但不生成它，从而提升模型对高风险内容的识别能力而不增加其生成。


<details>
  <summary>Details</summary>
Motivation: 传统方法过滤高风险内容会限制模型对有害或敏感内容的识别和响应能力，SLUNG旨在解决这一问题。

Method: SLUNG选择性避免激励高风险token的生成，同时确保模型仍能理解高风险内容。

Result: 实验表明SLUNG提升了模型对高风险数据的理解能力（如识别毒性内容），同时未增加其生成（如模型回答的毒性）。

Conclusion: SLUNG使模型能从高风险文本中受益，而无需完全过滤这些内容。

Abstract: Language model developers typically filter out high-risk content -- such as
toxic or copyrighted text -- from their pre-training data to prevent models
from generating similar outputs. However, removing such data altogether limits
models' ability to recognize and appropriately respond to harmful or sensitive
content. In this paper, we introduce Selective Loss to Understand but Not
Generate (SLUNG), a pre-training paradigm through which models learn to
understand high-risk data without learning to generate it. Instead of uniformly
applying the next-token prediction loss, SLUNG selectively avoids incentivizing
the generation of high-risk tokens while ensuring they remain within the
model's context window. As the model learns to predict low-risk tokens that
follow high-risk ones, it is forced to understand the high-risk content.
Through our experiments, we show that SLUNG consistently improves models'
understanding of high-risk data (e.g., ability to recognize toxic content)
without increasing its generation (e.g., toxicity of model responses). Overall,
our SLUNG paradigm enables models to benefit from high-risk text that would
otherwise be filtered out.

</details>


### [16] [Developing A Framework to Support Human Evaluation of Bias in Generated Free Response Text](https://arxiv.org/abs/2505.03053)
*Jennifer Healey,Laurie Byrum,Md Nadeem Akhtar,Surabhi Bhargava,Moumita Sinha*

Main category: cs.CL

TL;DR: 论文探讨了LLM评估的挑战，特别是在实际部署中任务特定提示和上下文交互的复杂性。作者提出了一种半自动化的偏见评估框架，结合人类洞察力，并讨论了如何定义偏见以自动化流程。


<details>
  <summary>Details</summary>
Motivation: LLM评估在基础模型中已具挑战性，而在实际部署中，任务特定提示和上下文的交互进一步增加了复杂性。大规模人类评估被认为难以实施且成本高昂，因此需要开发更高效的评估方法。

Method: 作者提出了一种半自动化的偏见评估框架，结合人类洞察力，并开发了一个操作性的偏见定义以自动化流程。此外，还提出了一种超越多项选择的偏见分类方法。

Result: 通过人类评估，作者发现了偏见基准中的问题模板，验证了半自动化框架的有效性。

Conclusion: 半自动化的偏见评估框架结合人类洞察力，能够更有效地评估LLM在实际部署中的偏见问题，同时揭示了现有基准的局限性。

Abstract: LLM evaluation is challenging even the case of base models. In real world
deployments, evaluation is further complicated by the interplay of task
specific prompts and experiential context. At scale, bias evaluation is often
based on short context, fixed choice benchmarks that can be rapidly evaluated,
however, these can lose validity when the LLMs' deployed context differs. Large
scale human evaluation is often seen as too intractable and costly. Here we
present our journey towards developing a semi-automated bias evaluation
framework for free text responses that has human insights at its core. We
discuss how we developed an operational definition of bias that helped us
automate our pipeline and a methodology for classifying bias beyond multiple
choice. We additionally comment on how human evaluation helped us uncover
problematic templates in a bias benchmark.

</details>


### [17] [Improving Model Alignment Through Collective Intelligence of Open-Source LLMS](https://arxiv.org/abs/2505.03059)
*Junlin Wang,Roy Xie,Shang Zhu,Jue Wang,Ben Athiwaratkun,Bhuwan Dhingra,Shuaiwen Leon Song,Ce Zhang,James Zou*

Main category: cs.CL

TL;DR: MoAA利用多种语言模型的集体优势生成高质量对齐数据，提升模型性能，并实现自我改进。


<details>
  <summary>Details</summary>
Motivation: 构建高质量人类标注数据成本高且难以扩展，MoAA旨在解决这一问题。

Method: 通过混合代理对齐（MoAA）方法，结合多种语言模型生成对齐数据。

Result: MoAA显著提升了LLaMA-3.1-8B-Instruct的胜率，并实现了模型的自我改进。

Conclusion: MoAA为开源LLM提供了一种无需依赖外部监督的可扩展对齐方法。

Abstract: Building helpful and harmless large language models (LLMs) requires effective
model alignment approach based on human instructions and feedback, which
necessitates high-quality human-labeled data. Constructing such datasets is
often expensive and hard to scale, and may face potential limitations on
diversity and generalization. To address these challenges, we introduce Mixture
of Agents Alignment (MoAA), that leverages the collective strengths of various
language models to provide high-quality data for model alignment. By employing
MoAA, we enhance both supervised fine-tuning and preference optimization,
leading to improved performance compared to using a single model alone to
generate alignment data (e.g. using GPT-4o alone). Evaluation results show that
our approach can improve win rate of LLaMA-3.1-8B-Instruct from 19.5 to 48.3 on
Arena-Hard and from 22.33 to 57.23 on AlpacaEval2, highlighting a promising
direction for model alignment through this new scalable and diverse synthetic
data recipe. Furthermore, we demonstrate that MoAA enables a self-improvement
pipeline, where models finetuned on MoA-generated data surpass their own
initial capabilities, providing evidence that our approach can push the
frontier of open-source LLMs without reliance on stronger external supervision.
Data and code will be released.

</details>


### [18] [Survey of Abstract Meaning Representation: Then, Now, Future](https://arxiv.org/abs/2505.03229)
*Behrooz Mansouri*

Main category: cs.CL

TL;DR: 本文综述了抽象意义表示（AMR）及其扩展，探讨了其解析与生成任务，并回顾了AMR的多种应用，分析了领域的最新进展与挑战。


<details>
  <summary>Details</summary>
Motivation: 研究AMR的目的是为了增强机器对人类语言的理解能力，通过图结构捕捉句子的语义。

Method: 采用调查方法，分析AMR的解析（文本到AMR）和生成（AMR到文本）任务，以及传统、当前和未来可能的方法。

Result: 综述展示了AMR在文本生成、分类、信息提取等领域的应用潜力。

Conclusion: AMR在提升机器语言理解方面具有重要潜力，未来研究需解决现有挑战。

Abstract: This paper presents a survey of Abstract Meaning Representation (AMR), a
semantic representation framework that captures the meaning of sentences
through a graph-based structure. AMR represents sentences as rooted, directed
acyclic graphs, where nodes correspond to concepts and edges denote
relationships, effectively encoding the meaning of complex sentences. This
survey investigates AMR and its extensions, focusing on AMR capabilities. It
then explores the parsing (text-to-AMR) and generation (AMR-to-text) tasks by
showing traditional, current, and possible futures approaches. It also reviews
various applications of AMR including text generation, text classification, and
information extraction and information seeking. By analyzing recent
developments and challenges in the field, this survey provides insights into
future directions for research and the potential impact of AMR on enhancing
machine understanding of human language.

</details>


### [19] [Ψ-Arena: Interactive Assessment and Optimization of LLM-based Psychological Counselors with Tripartite Feedback](https://arxiv.org/abs/2505.03293)
*Shijing Zhu,Zhuang Chen,Guanqun Bi,Binghang Li,Yaxi Deng,Dazhen Wan,Libiao Peng,Xiyao Xiao,Rongsheng Zhang,Tangjie Lv,Zhipeng Hu,FangFang Li,Minlie Huang*

Main category: cs.CL

TL;DR: Psi-Arena是一个交互式框架，用于全面评估和优化基于LLM的心理咨询师，通过多阶段对话、三方评估和闭环优化提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法在静态测试、单一视角和开环框架方面存在局限，无法全面评估LLM心理咨询师的效果和安全性。

Method: 提出Psi-Arena框架，包括真实场景模拟（多阶段对话）、三方评估（客户、咨询师、监督者）和闭环优化（反馈迭代）。

Result: 实验显示不同LLM在真实场景中表现差异显著，优化后咨询性能提升高达141%。

Conclusion: Psi-Arena为心理健康领域可靠且人性化的LLM应用提供了基础资源。

Abstract: Large language models (LLMs) have shown promise in providing scalable mental
health support, while evaluating their counseling capability remains crucial to
ensure both efficacy and safety. Existing evaluations are limited by the static
assessment that focuses on knowledge tests, the single perspective that centers
on user experience, and the open-loop framework that lacks actionable feedback.
To address these issues, we propose {\Psi}-Arena, an interactive framework for
comprehensive assessment and optimization of LLM-based counselors, featuring
three key characteristics: (1) Realistic arena interactions that simulate
real-world counseling through multi-stage dialogues with psychologically
profiled NPC clients, (2) Tripartite evaluation that integrates assessments
from the client, counselor, and supervisor perspectives, and (3) Closed-loop
optimization that iteratively improves LLM counselors using diagnostic
feedback. Experiments across eight state-of-the-art LLMs show significant
performance variations in different real-world scenarios and evaluation
perspectives. Moreover, reflection-based optimization results in up to a 141%
improvement in counseling performance. We hope PsychoArena provides a
foundational resource for advancing reliable and human-aligned LLM applications
in mental healthcare.

</details>


### [20] [Recall with Reasoning: Chain-of-Thought Distillation for Mamba's Long-Context Memory and Extrapolation](https://arxiv.org/abs/2505.03320)
*Junyu Ma,Tianqing Fang,Zhisong Zhang,Hongming Zhang,Haitao Mi,Dong Yu*

Main category: cs.CL

TL;DR: 通过引入Recall with Reasoning（RwR）方法，提升Mamba模型在长上下文中的表现，同时保持短上下文能力。


<details>
  <summary>Details</summary>
Motivation: Mamba模型在理论上具有无限上下文潜力，但在实际应用中，当序列远超训练长度时表现受限。

Method: 使用Recall with Reasoning（RwR）方法，通过从教师模型中提取链式思维（CoT）摘要，并在微调时将其作为CoT提示添加到输入中。

Result: 在LONGMEMEVAL和HELMET数据集上的实验表明，RwR显著提升了Mamba的长上下文性能，且无需改变模型架构。

Conclusion: RwR是一种简单而有效的方法，能够解锁Mamba的长上下文记忆能力，同时保持其短上下文性能。

Abstract: Mamba's theoretical infinite-context potential is limited in practice when
sequences far exceed training lengths. This work explores unlocking Mamba's
long-context memory ability by a simple-yet-effective method, Recall with
Reasoning (RwR), by distilling chain-of-thought (CoT) summarization from a
teacher model. Specifically, RwR prepends these summarization as CoT prompts
during fine-tuning, teaching Mamba to actively recall and reason over long
contexts. Experiments on LONGMEMEVAL and HELMET show RwR boosts Mamba's
long-context performance against comparable Transformer/hybrid baselines under
similar pretraining conditions, while preserving short-context capabilities,
all without architectural changes.

</details>


### [21] [Lightweight Clinical Decision Support System using QLoRA-Fine-Tuned LLMs and Retrieval-Augmented Generation](https://arxiv.org/abs/2505.03406)
*Mohammad Shoaib Ansari,Mohd Sohail Ali Khan,Shubham Revankar,Aditya Varma,Anil S. Mokhade*

Main category: cs.CL

TL;DR: 论文研究了大型语言模型（LLMs）在医疗领域的应用，通过结合检索增强生成（RAG）和量化低秩适应（QLoRA）技术，提升医疗决策支持的准确性。


<details>
  <summary>Details</summary>
Motivation: 旨在通过LLMs和医院特定数据优化医疗决策支持系统，提高响应准确性和效率。

Method: 采用Llama 3.2-3B-Instruct作为基础模型，结合RAG和QLoRA技术，嵌入和检索医疗信息。

Result: 系统显著提升了响应准确性，并在多个医疗基准测试中表现良好，适用于基本医疗建议。

Conclusion: LLMs在医疗领域具有广泛潜力，但需关注伦理问题和实际部署挑战，未来需进一步优化和验证。

Abstract: This research paper investigates the application of Large Language Models
(LLMs) in healthcare, specifically focusing on enhancing medical decision
support through Retrieval-Augmented Generation (RAG) integrated with
hospital-specific data and fine-tuning using Quantized Low-Rank Adaptation
(QLoRA). The system utilizes Llama 3.2-3B-Instruct as its foundation model. By
embedding and retrieving context-relevant healthcare information, the system
significantly improves response accuracy. QLoRA facilitates notable parameter
efficiency and memory optimization, preserving the integrity of medical
information through specialized quantization techniques. Our research also
shows that our model performs relatively well on various medical benchmarks,
indicating that it can be used to make basic medical suggestions. This paper
details the system's technical components, including its architecture,
quantization methods, and key healthcare applications such as enhanced disease
prediction from patient symptoms and medical history, treatment suggestions,
and efficient summarization of complex medical reports. We touch on the ethical
considerations-patient privacy, data security, and the need for rigorous
clinical validation-as well as the practical challenges of integrating such
systems into real-world healthcare workflows. Furthermore, the lightweight
quantized weights ensure scalability and ease of deployment even in
low-resource hospital environments. Finally, the paper concludes with an
analysis of the broader impact of LLMs on healthcare and outlines future
directions for LLMs in medical settings.

</details>


### [22] [MedArabiQ: Benchmarking Large Language Models on Arabic Medical Tasks](https://arxiv.org/abs/2505.03427)
*Mouath Abu Daoud,Chaimae Abouzahir,Leen Kharouf,Walid Al-Eisawi,Nizar Habash,Farah E. Shamout*

Main category: cs.CL

TL;DR: 该研究介绍了MedArabiQ，一个针对阿拉伯语医疗领域的新型基准数据集，用于评估大型语言模型（LLMs）的性能，并填补了该领域高质量数据和基准的空白。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏高质量的阿拉伯语医疗领域数据集和基准，LLMs在该领域的效能尚未得到充分研究。

Method: 研究构建了包含七项阿拉伯语医疗任务的MedArabiQ数据集，并对五种先进LLMs进行了广泛评估。

Result: 研究发现需要创建跨语言的高质量基准，以确保LLMs在医疗领域的公平部署和扩展性。

Conclusion: 通过发布MedArabiQ数据集，研究为未来评估和增强LLMs多语言能力的研究奠定了基础。

Abstract: Large Language Models (LLMs) have demonstrated significant promise for
various applications in healthcare. However, their efficacy in the Arabic
medical domain remains unexplored due to the lack of high-quality
domain-specific datasets and benchmarks. This study introduces MedArabiQ, a
novel benchmark dataset consisting of seven Arabic medical tasks, covering
multiple specialties and including multiple choice questions,
fill-in-the-blank, and patient-doctor question answering. We first constructed
the dataset using past medical exams and publicly available datasets. We then
introduced different modifications to evaluate various LLM capabilities,
including bias mitigation. We conducted an extensive evaluation with five
state-of-the-art open-source and proprietary LLMs, including GPT-4o, Claude
3.5-Sonnet, and Gemini 1.5. Our findings highlight the need for the creation of
new high-quality benchmarks that span different languages to ensure fair
deployment and scalability of LLMs in healthcare. By establishing this
benchmark and releasing the dataset, we provide a foundation for future
research aimed at evaluating and enhancing the multilingual capabilities of
LLMs for the equitable use of generative AI in healthcare.

</details>


### [23] [An Analysis of Hyper-Parameter Optimization Methods for Retrieval Augmented Generation](https://arxiv.org/abs/2505.03452)
*Matan Orbach,Ohad Eytan,Benjamin Sznajder,Ariel Gera,Odellia Boni,Yoav Kantor,Gal Bloch,Omri Levy,Hadas Abraham,Nitzan Barzilay,Eyal Shnarch,Michael E. Factor,Shila Ofek-Koifman,Paula Ta-Shma,Assaf Toledo*

Main category: cs.CL

TL;DR: 本文研究了检索增强生成（RAG）的超参数优化（HPO）框架的有效性，通过5种HPO算法和5个数据集进行了全面实验，发现贪婪或迭代随机搜索能高效提升RAG性能。


<details>
  <summary>Details</summary>
Motivation: 由于为特定用例找到最优RAG配置复杂且昂贵，且现有HPO框架的有效性尚未严格验证，本文旨在填补这一空白。

Method: 采用5种HPO算法在5个数据集上进行实验，包括一个新收集的真实产品文档数据集，并探索了最大的HPO搜索空间。

Result: 结果表明，贪婪或迭代随机搜索能高效完成RAG HPO，并显著提升所有数据集的性能。贪婪方法中，优先优化模型比按RAG流程顺序优化更有效。

Conclusion: RAG HPO可以高效完成，且显著提升性能，贪婪方法中优先优化模型是更优策略。

Abstract: Finding the optimal Retrieval-Augmented Generation (RAG) configuration for a
given use case can be complex and expensive. Motivated by this challenge,
frameworks for RAG hyper-parameter optimization (HPO) have recently emerged,
yet their effectiveness has not been rigorously benchmarked. To address this
gap, we present a comprehensive study involving 5 HPO algorithms over 5
datasets from diverse domains, including a new one collected for this work on
real-world product documentation. Our study explores the largest HPO search
space considered to date, with two optimized evaluation metrics. Analysis of
the results shows that RAG HPO can be done efficiently, either greedily or with
iterative random search, and that it significantly boosts RAG performance for
all datasets. For greedy HPO approaches, we show that optimizing models first
is preferable to the prevalent practice of optimizing sequentially according to
the RAG pipeline order.

</details>


### [24] [Uncertainty-Aware Large Language Models for Explainable Disease Diagnosis](https://arxiv.org/abs/2505.03467)
*Shuang Zhou,Jiashuo Wang,Zidu Xu,Song Wang,David Brauer,Lindsay Welton,Jacob Cogan,Yuen-Hei Chung,Lei Tian,Zaifu Zhan,Yu Hou,Mingquan Lin,Genevieve B. Melton,Rui Zhang*

Main category: cs.CL

TL;DR: ConfiDx是一个基于不确定性感知的大型语言模型，通过微调开源LLM并结合诊断标准，用于识别和解释诊断不确定性，提升自动诊断系统的可靠性。


<details>
  <summary>Details</summary>
Motivation: 临床笔记中证据不足时，诊断不确定性会增加误诊风险，但目前对诊断不确定性的识别和解释研究不足。

Method: 通过微调开源LLM，构建了ConfiDx模型，并利用标注数据集捕捉不同程度的诊断模糊性。

Result: ConfiDx在真实数据集上表现出色，能识别诊断不确定性、提升诊断性能，并生成可信的解释。

Conclusion: 这是首个同时解决诊断不确定性识别和解释的研究，显著提升了自动诊断系统的可靠性。

Abstract: Explainable disease diagnosis, which leverages patient information (e.g.,
signs and symptoms) and computational models to generate probable diagnoses and
reasonings, offers clear clinical values. However, when clinical notes
encompass insufficient evidence for a definite diagnosis, such as the absence
of definitive symptoms, diagnostic uncertainty usually arises, increasing the
risk of misdiagnosis and adverse outcomes. Although explicitly identifying and
explaining diagnostic uncertainties is essential for trustworthy diagnostic
systems, it remains under-explored. To fill this gap, we introduce ConfiDx, an
uncertainty-aware large language model (LLM) created by fine-tuning open-source
LLMs with diagnostic criteria. We formalized the task and assembled richly
annotated datasets that capture varying degrees of diagnostic ambiguity.
Evaluating ConfiDx on real-world datasets demonstrated that it excelled in
identifying diagnostic uncertainties, achieving superior diagnostic
performance, and generating trustworthy explanations for diagnoses and
uncertainties. To our knowledge, this is the first study to jointly address
diagnostic uncertainty recognition and explanation, substantially enhancing the
reliability of automatic diagnostic systems.

</details>


### [25] [Long-Short Chain-of-Thought Mixture Supervised Fine-Tuning Eliciting Efficient Reasoning in Large Language Models](https://arxiv.org/abs/2505.03469)
*Bin Yu,Hang Yuan,Yuliang Wei,Bailing Wang,Weizhen Qi,Kai Chen*

Main category: cs.CL

TL;DR: LS-Mixture SFT方法通过结合长链和短链推理数据，解决了SFT中继承的“过度思考”问题，提升了模型推理效率。


<details>
  <summary>Details</summary>
Motivation: 解决监督微调（SFT）中因继承教师模型的“过度思考”问题导致的推理冗余。

Method: 提出LS-Mixture SFT，结合长链推理数据和通过结构保留重写生成的短链数据。

Result: 相比直接SFT，LS-Mixture SFT平均准确率提升2.3%，响应长度减少47.61%。

Conclusion: LS-Mixture SFT为无推理能力的模型提供了高效的推理能力，同时避免了推理冗余。

Abstract: Recent advances in large language models have demonstrated that Supervised
Fine-Tuning (SFT) with Chain-of-Thought (CoT) reasoning data distilled from
large reasoning models (e.g., DeepSeek R1) can effectively transfer reasoning
capabilities to non-reasoning models. However, models fine-tuned with this
approach inherit the "overthinking" problem from teacher models, producing
verbose and redundant reasoning chains during inference. To address this
challenge, we propose \textbf{L}ong-\textbf{S}hort Chain-of-Thought
\textbf{Mixture} \textbf{S}upervised \textbf{F}ine-\textbf{T}uning
(\textbf{LS-Mixture SFT}), which combines long CoT reasoning dataset with their
short counterparts obtained through structure-preserved rewriting. Our
experiments demonstrate that models trained using the LS-Mixture SFT method,
compared to those trained with direct SFT, achieved an average accuracy
improvement of 2.3\% across various benchmarks while substantially reducing
model response length by approximately 47.61\%. This work offers an approach to
endow non-reasoning models with reasoning capabilities through supervised
fine-tuning while avoiding the inherent overthinking problems inherited from
teacher models, thereby enabling efficient reasoning in the fine-tuned models.

</details>


### [26] [Evaluation of LLMs on Long-tail Entity Linking in Historical Documents](https://arxiv.org/abs/2505.03473)
*Marta Boscariol,Luana Bulla,Lia Draetta,Beatrice Fiumanò,Emanuele Lenzi,Leonardo Piano*

Main category: cs.CL

TL;DR: 论文评估了GPT和LLama3在长尾实体链接任务中的表现，发现大语言模型（LLMs）在此任务中表现良好，可作为传统方法的补充。


<details>
  <summary>Details</summary>
Motivation: 长尾实体链接是一个未被充分研究的问题，且传统方法在处理长尾实体时表现不佳，因此探索LLMs在此任务中的潜力具有重要意义。

Method: 使用MHERCL v0.1基准数据集，对比了GPT和LLama3与ReLiK框架在长尾实体链接任务中的表现。

Result: 初步实验表明，LLMs在长尾实体链接任务中表现良好，显示出其在此领域的潜力。

Conclusion: LLMs可以作为长尾实体链接任务的有力补充，填补传统方法的不足。

Abstract: Entity Linking (EL) plays a crucial role in Natural Language Processing (NLP)
applications, enabling the disambiguation of entity mentions by linking them to
their corresponding entries in a reference knowledge base (KB). Thanks to their
deep contextual understanding capabilities, LLMs offer a new perspective to
tackle EL, promising better results than traditional methods. Despite the
impressive generalization capabilities of LLMs, linking less popular, long-tail
entities remains challenging as these entities are often underrepresented in
training data and knowledge bases. Furthermore, the long-tail EL task is an
understudied problem, and limited studies address it with LLMs. In the present
work, we assess the performance of two popular LLMs, GPT and LLama3, in a
long-tail entity linking scenario. Using MHERCL v0.1, a manually annotated
benchmark of sentences from domain-specific historical texts, we quantitatively
compare the performance of LLMs in identifying and linking entities to their
corresponding Wikidata entries against that of ReLiK, a state-of-the-art Entity
Linking and Relation Extraction framework. Our preliminary experiments reveal
that LLMs perform encouragingly well in long-tail EL, indicating that this
technology can be a valuable adjunct in filling the gap between head and
long-tail EL.

</details>


### [27] [Sentence Embeddings as an intermediate target in end-to-end summarisation](https://arxiv.org/abs/2505.03481)
*Maciej Zembrzuski,Saad Mahamood*

Main category: cs.CL

TL;DR: 提出了一种结合抽取式和生成式方法的文档摘要模型，利用预训练句子嵌入提升大输入数据集上的性能。


<details>
  <summary>Details</summary>
Motivation: 现有神经网络方法在处理大输入数据集时表现不佳，尤其是在用户评论摘要任务中。

Method: 结合抽取式方法和预训练句子嵌入，再与生成式模型结合。

Result: 在大输入数据集上优于现有方法，且预测摘要的句子嵌入能提升端到端系统的质量。

Conclusion: 该方法在松散对齐的源-目标语料库中表现更优，优于传统的句子选择概率分布预测。

Abstract: Current neural network-based methods to the problem of document summarisation
struggle when applied to datasets containing large inputs. In this paper we
propose a new approach to the challenge of content-selection when dealing with
end-to-end summarisation of user reviews of accommodations. We show that by
combining an extractive approach with externally pre-trained sentence level
embeddings in an addition to an abstractive summarisation model we can
outperform existing methods when this is applied to the task of summarising a
large input dataset. We also prove that predicting sentence level embedding of
a summary increases the quality of an end-to-end system for loosely aligned
source to target corpora, than compared to commonly predicting probability
distributions of sentence selection.

</details>


### [28] [Faster MoE LLM Inference for Extremely Large Models](https://arxiv.org/abs/2505.03531)
*Haoqi Yang,Luohe Shi,Qiwei Li,Zuchao Li,Ping Wang,Bo Du,Mengjia Shen,Hai Zhao*

Main category: cs.CL

TL;DR: 论文探讨了细粒度稀疏混合专家（MoE）模型在不同服务负载下的效率动态，发现减少激活专家数量可显著提升效率且性能损失小，而减少专家总数则效率提升有限但性能损失严重。


<details>
  <summary>Details</summary>
Motivation: 随着细粒度MoE模型的流行，研究其效率动态及优化潜力，填补现有研究的空白。

Method: 分析不同服务负载下MoE模型的效率与性能表现，探讨减少激活专家数量和专家总数的影响。

Result: 减少激活专家数量可显著提升效率且性能损失小；减少专家总数效率提升有限但性能损失严重。方法可提升至少10%的吞吐量且无性能损失。

Conclusion: MoE推理优化仍有巨大探索和改进潜力。

Abstract: Sparse Mixture of Experts (MoE) large language models (LLMs) are gradually
becoming the mainstream approach for ultra-large-scale models. Existing
optimization efforts for MoE models have focused primarily on coarse-grained
MoE architectures. With the emergence of DeepSeek Models, fine-grained MoE
models are gaining popularity, yet research on them remains limited. Therefore,
we want to discuss the efficiency dynamic under different service loads.
Additionally, fine-grained models allow deployers to reduce the number of
routed experts, both activated counts and total counts, raising the question of
how this reduction affects the trade-off between MoE efficiency and
performance. Our findings indicate that while deploying MoE models presents
greater challenges, it also offers significant optimization opportunities.
Reducing the number of activated experts can lead to substantial efficiency
improvements in certain scenarios, with only minor performance degradation.
Reducing the total number of experts provides limited efficiency gains but
results in severe performance degradation. Our method can increase throughput
by at least 10\% without any performance degradation. Overall, we conclude that
MoE inference optimization remains an area with substantial potential for
exploration and improvement.

</details>


### [29] [Say It Another Way: A Framework for User-Grounded Paraphrasing](https://arxiv.org/abs/2505.03563)
*Cléa Chataigner,Rebecca Ma,Prakhar Ganesh,Afaf Taïk,Elliot Creager,Golnoosh Farnadi*

Main category: cs.CL

TL;DR: 论文探讨了提示词微小变化对大型语言模型（LLM）行为的影响，提出了一种基于语言转换的框架，用于系统生成自然提示变体，并验证其对模型行为的影响。


<details>
  <summary>Details</summary>
Motivation: 研究提示词的自然变化如何影响LLM的行为，以解决评估的稳定性和可靠性问题。

Method: 提出基于语言转换分类的框架，系统生成提示变体，并使用BBQ数据集进行验证。

Result: 发现即使是微小的提示修改也会导致模型行为的显著变化。

Conclusion: 强调了需要开发对提示词变体敏感的鲁棒评估协议。

Abstract: Small changes in how a prompt is worded can lead to meaningful differences in
the behavior of large language models (LLMs), raising concerns about the
stability and reliability of their evaluations. While prior work has explored
simple formatting changes, these rarely capture the kinds of natural variation
seen in real-world language use. We propose a controlled paraphrasing framework
based on a taxonomy of minimal linguistic transformations to systematically
generate natural prompt variations. Using the BBQ dataset, we validate our
method with both human annotations and automated checks, then use it to study
how LLMs respond to paraphrased prompts in stereotype evaluation tasks. Our
analysis shows that even subtle prompt modifications can lead to substantial
changes in model behavior. These results highlight the need for robust,
paraphrase-aware evaluation protocols.

</details>


### [30] [Towards conversational assistants for health applications: using ChatGPT to generate conversations about heart failure](https://arxiv.org/abs/2505.03675)
*Anuja Tayal,Devika Salunke,Barbara Di Eugenio,Paula G Allen-Meares,Eulalia P Abril,Olga Garcia-Bedoya,Carolyn A Dickens,Andrew D. Boyd*

Main category: cs.CL

TL;DR: 研究探讨了ChatGPT（3.5-turbo和4）为非洲裔美国心衰患者生成自我护理对话的潜力，发现有效提示设计是关键，但ChatGPT仍缺乏共情和互动。


<details>
  <summary>Details</summary>
Motivation: 针对非洲裔美国心衰患者自我护理领域缺乏专门数据集的问题，探索ChatGPT生成相关对话的能力。

Method: 采用四种提示策略（领域、AAVE、SDOH、SDOH-informed推理），生成涵盖食物、运动和液体摄入的对话，并加入患者特定SDOH属性。

Result: 研究发现，结合SDOH和推理能提升对话质量，但ChatGPT在共情和互动方面仍有不足。

Conclusion: 提示设计对生成高质量对话至关重要，但ChatGPT在医疗沟通中的共情和互动能力仍需改进。

Abstract: We explore the potential of ChatGPT (3.5-turbo and 4) to generate
conversations focused on self-care strategies for African-American heart
failure patients -- a domain with limited specialized datasets. To simulate
patient-health educator dialogues, we employed four prompting strategies:
domain, African American Vernacular English (AAVE), Social Determinants of
Health (SDOH), and SDOH-informed reasoning. Conversations were generated across
key self-care domains of food, exercise, and fluid intake, with varying turn
lengths (5, 10, 15) and incorporated patient-specific SDOH attributes such as
age, gender, neighborhood, and socioeconomic status. Our findings show that
effective prompt design is essential. While incorporating SDOH and reasoning
improves dialogue quality, ChatGPT still lacks the empathy and engagement
needed for meaningful healthcare communication.

</details>


### [31] [IndicSQuAD: A Comprehensive Multilingual Question Answering Dataset for Indic Languages](https://arxiv.org/abs/2505.03688)
*Sharvi Endait,Ruturaj Ghatage,Aditya Kulkarni,Rajlaxmi Patil,Raviraj Joshi*

Main category: cs.CL

TL;DR: 该论文介绍了IndicSQuAD，一个涵盖九种主要印度语言的多语言抽取式问答数据集，旨在解决印度语言在问答系统中的低资源问题。


<details>
  <summary>Details</summary>
Motivation: 高资源语言在问答系统上的快速发展导致印度语言被忽视，尽管其拥有大量母语使用者。

Method: 通过翻译技术从SQuAD数据集衍生出IndicSQuAD，保持语言忠实度和答案跨度对齐。

Result: 评估显示低资源环境下存在挑战，但为未来工作提供了方向。

Conclusion: IndicSQuAD为印度语言问答系统提供了基础，并公开了数据集和模型。

Abstract: The rapid progress in question-answering (QA) systems has predominantly
benefited high-resource languages, leaving Indic languages largely
underrepresented despite their vast native speaker base. In this paper, we
present IndicSQuAD, a comprehensive multi-lingual extractive QA dataset
covering nine major Indic languages, systematically derived from the SQuAD
dataset. Building on previous work with MahaSQuAD for Marathi, our approach
adapts and extends translation techniques to maintain high linguistic fidelity
and accurate answer-span alignment across diverse languages. IndicSQuAD
comprises extensive training, validation, and test sets for each language,
providing a robust foundation for model development. We evaluate baseline
performances using language-specific monolingual BERT models and the
multilingual MuRIL-BERT. The results indicate some challenges inherent in
low-resource settings. Moreover, our experiments suggest potential directions
for future work, including expanding to additional languages, developing
domain-specific datasets, and incorporating multimodal data. The dataset and
models are publicly shared at https://github.com/l3cube-pune/indic-nlp

</details>


### [32] [NBF at SemEval-2025 Task 5: Light-Burst Attention Enhanced System for Multilingual Subject Recommendation](https://arxiv.org/abs/2505.03711)
*Baharul Islam,Nasim Ahmad,Ferdous Ahmed Barbhuiya,Kuntal Dey*

Main category: cs.CL

TL;DR: 本文介绍了SemEval 2025任务5的系统提交，专注于英语和德语学术领域的跨语言主题分类。方法结合双语数据训练，采用负采样和基于边界的检索目标，并提出了一种维度标记自注意力机制。结果显示系统在资源有限的情况下表现良好，但仍需改进。


<details>
  <summary>Details</summary>
Motivation: 解决跨语言主题分类问题，特别是在资源受限的环境中，提升主题检索的效率和准确性。

Method: 利用双语数据训练，结合负采样和基于边界的检索目标，设计了一种维度标记自注意力机制以减少内部维度。

Result: 系统在通用定量评估中平均召回率为32.24%，在通用定性评估中分别为43.16%和31.53%，GPU使用率低。

Conclusion: 该方法在资源受限下能有效捕获主题信息，但仍有改进空间。

Abstract: We present our system submission for SemEval 2025 Task 5, which focuses on
cross-lingual subject classification in the English and German academic
domains. Our approach leverages bilingual data during training, employing
negative sampling and a margin-based retrieval objective. We demonstrate that a
dimension-as-token self-attention mechanism designed with significantly reduced
internal dimensions can effectively encode sentence embeddings for subject
retrieval. In quantitative evaluation, our system achieved an average recall
rate of 32.24% in the general quantitative setting (all subjects), 43.16% and
31.53% of the general qualitative evaluation methods with minimal GPU usage,
highlighting their competitive performance. Our results demonstrate that our
approach is effective in capturing relevant subject information under resource
constraints, although there is still room for improvement.

</details>


### [33] [WebGen-Bench: Evaluating LLMs on Generating Interactive and Functional Websites from Scratch](https://arxiv.org/abs/2505.03733)
*Zimu Lu,Yunqiao Yang,Houxing Ren,Haotian Hou,Han Xiao,Ke Wang,Weikang Shi,Aojun Zhou,Mingjie Zhan,Hongsheng Li*

Main category: cs.CL

TL;DR: 论文介绍了WebGen-Bench，一个用于评估LLM代理生成多文件网站代码能力的基准，包含多样化的指令和647个测试用例。测试结果显示现有模型的性能较低，最高准确率仅27.8%。通过训练数据集WebGen-Instruct，模型性能提升至38.2%。


<details>
  <summary>Details</summary>
Motivation: 当前LLM代理在生成和管理复杂代码库方面表现优异，但缺乏评估其生成多文件网站代码能力的基准。因此，作者提出WebGen-Bench以填补这一空白。

Method: 1. 构建WebGen-Bench基准，包含多样化指令和647个测试用例。2. 使用GPT-4o生成测试用例并手动调整。3. 利用网页导航代理自动化测试。4. 评估三种代码代理框架（Bolt.diy、OpenHands、Aider）的性能。5. 构建训练集WebGen-Instruct并训练模型。

Result: 最佳组合（Bolt.diy + DeepSeek-R1）在测试用例上的准确率为27.8%。通过训练Qwen2.5-Coder-32B-Instruct，准确率提升至38.2%。

Conclusion: WebGen-Bench是一个具有挑战性的基准，现有模型表现不佳，但通过训练可以显著提升性能。

Abstract: LLM-based agents have demonstrated great potential in generating and managing
code within complex codebases. In this paper, we introduce WebGen-Bench, a
novel benchmark designed to measure an LLM-based agent's ability to create
multi-file website codebases from scratch. It contains diverse instructions for
website generation, created through the combined efforts of human annotators
and GPT-4o. These instructions span three major categories and thirteen minor
categories, encompassing nearly all important types of web applications. To
assess the quality of the generated websites, we use GPT-4o to generate test
cases targeting each functionality described in the instructions, and then
manually filter, adjust, and organize them to ensure accuracy, resulting in 647
test cases. Each test case specifies an operation to be performed on the
website and the expected result after the operation. To automate testing and
improve reproducibility, we employ a powerful web-navigation agent to execute
tests on the generated websites and determine whether the observed responses
align with the expected results. We evaluate three high-performance code-agent
frameworks, Bolt.diy, OpenHands, and Aider, using multiple proprietary and
open-source LLMs as engines. The best-performing combination, Bolt.diy powered
by DeepSeek-R1, achieves only 27.8\% accuracy on the test cases, highlighting
the challenging nature of our benchmark. Additionally, we construct
WebGen-Instruct, a training set consisting of 6,667 website-generation
instructions. Training Qwen2.5-Coder-32B-Instruct on Bolt.diy trajectories
generated from a subset of this training set achieves an accuracy of 38.2\%,
surpassing the performance of the best proprietary model.

</details>


### [34] [VITA-Audio: Fast Interleaved Cross-Modal Token Generation for Efficient Large Speech-Language Model](https://arxiv.org/abs/2505.03739)
*Zuwei Long,Yunhang Shen,Chaoyou Fu,Heting Gao,Lijiang Li,Peixian Chen,Mengdan Zhang,Hang Shao,Jian Li,Jinlong Peng,Haoyu Cao,Ke Li,Rongrong Ji,Xing Sun*

Main category: cs.CL

TL;DR: VITA-Audio是一种端到端的大型语音模型，通过多模态令牌预测模块和四阶段渐进训练策略，显著降低了流式场景下的首音频生成延迟，并提升了推理速度。


<details>
  <summary>Details</summary>
Motivation: 现有语音模型在流式场景下生成首音频令牌时存在高延迟问题，限制了部署效率。

Method: 提出轻量级多模态令牌预测模块（MCTP）和四阶段渐进训练策略，加速推理并减少首音频生成延迟。

Result: 在7B参数规模下，推理速度提升3~5倍，并在ASR、TTS和SQA任务中优于同类开源模型。

Conclusion: VITA-Audio是首个能在首次前向传递中生成音频的多模态大语言模型，实现了低延迟实时对话能力。

Abstract: With the growing requirement for natural human-computer interaction,
speech-based systems receive increasing attention as speech is one of the most
common forms of daily communication. However, the existing speech models still
experience high latency when generating the first audio token during streaming,
which poses a significant bottleneck for deployment. To address this issue, we
propose VITA-Audio, an end-to-end large speech model with fast audio-text token
generation. Specifically, we introduce a lightweight Multiple Cross-modal Token
Prediction (MCTP) module that efficiently generates multiple audio tokens
within a single model forward pass, which not only accelerates the inference
but also significantly reduces the latency for generating the first audio in
streaming scenarios. In addition, a four-stage progressive training strategy is
explored to achieve model acceleration with minimal loss of speech quality. To
our knowledge, VITA-Audio is the first multi-modal large language model capable
of generating audio output during the first forward pass, enabling real-time
conversational capabilities with minimal latency. VITA-Audio is fully
reproducible and is trained on open-source data only. Experimental results
demonstrate that our model achieves an inference speedup of 3~5x at the 7B
parameter scale, but also significantly outperforms open-source models of
similar model size on multiple benchmarks for automatic speech recognition
(ASR), text-to-speech (TTS), and spoken question answering (SQA) tasks.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [35] [Iterative Resolution of Prompt Ambiguities Using a Progressive Cutting-Search Approach](https://arxiv.org/abs/2505.02952)
*Fabrizio Marozzo*

Main category: cs.AI

TL;DR: 论文提出了一种通过迭代澄清问题解决自然语言模糊性的方法，相比传统一次性解决方案，其准确性更高、用户满意度更高。


<details>
  <summary>Details</summary>
Motivation: 自然语言的模糊性导致用户需要反复测试和修正提示，影响效率和体验。

Method: 采用结构化迭代方法，通过澄清问题和示例逐步消除模糊性，最终生成精确解决方案。

Result: 在编码、数据分析和创意写作等任务中，该方法表现出更高的准确性、竞争性的解决时间和更高的用户满意度。

Conclusion: 迭代澄清方法能有效解决自然语言模糊性，提升生成AI系统的实用性和用户体验。

Abstract: Generative AI systems have revolutionized human interaction by enabling
natural language-based coding and problem solving. However, the inherent
ambiguity of natural language often leads to imprecise instructions, forcing
users to iteratively test, correct, and resubmit their prompts. We propose an
iterative approach that systematically narrows down these ambiguities through a
structured series of clarification questions and alternative solution
proposals, illustrated with input/output examples as well. Once every
uncertainty is resolved, a final, precise solution is generated. Evaluated on a
diverse dataset spanning coding, data analysis, and creative writing, our
method demonstrates superior accuracy, competitive resolution times, and higher
user satisfaction compared to conventional one-shot solutions, which typically
require multiple manual iterations to achieve a correct output.

</details>


### [36] [The Multimodal Paradox: How Added and Missing Modalities Shape Bias and Performance in Multimodal AI](https://arxiv.org/abs/2505.03020)
*Kishore Sampath,Pratheesh,Ayaazuddin Mohammad,Resmi Ramachandranpillai*

Main category: cs.AI

TL;DR: 多模态学习通过整合多种数据源（如图像、文本和结构化数据）在高风险决策中表现优于单模态方法。然而，性能提升虽是评估标准，但偏见和鲁棒性问题常被忽视。本文研究两个问题：新模态是否提升性能并影响公平性，以及模态缺失对模型性能和公平性的影响。实验表明，新模态提升性能但公平性因评估指标和数据集而异，模态缺失会降低性能和公平性。


<details>
  <summary>Details</summary>
Motivation: 多模态学习虽性能优越，但偏见和鲁棒性问题未受足够关注，需研究其对性能和公平性的影响。

Method: 通过分析新模态的加入和模态缺失对多模态模型的影响，使用医疗数据集（图像、时间序列和结构化数据）进行实验验证。

Result: 新模态提升性能，但公平性表现不一；模态缺失会降低性能和公平性。

Conclusion: 多模态学习需平衡性能与公平性，模态缺失问题需解决以提高鲁棒性。

Abstract: Multimodal learning, which integrates diverse data sources such as images,
text, and structured data, has proven superior to unimodal counterparts in
high-stakes decision-making. However, while performance gains remain the gold
standard for evaluating multimodal systems, concerns around bias and robustness
are frequently overlooked. In this context, this paper explores two key
research questions (RQs): (i) RQ1 examines whether adding a modality
con-sistently enhances performance and investigates its role in shaping
fairness measures, assessing whether it mitigates or amplifies bias in
multimodal models; (ii) RQ2 investigates the impact of missing modalities at
inference time, analyzing how multimodal models generalize in terms of both
performance and fairness. Our analysis reveals that incorporating new
modalities during training consistently enhances the performance of multimodal
models, while fairness trends exhibit variability across different evaluation
measures and datasets. Additionally, the absence of modalities at inference
degrades performance and fairness, raising concerns about its robustness in
real-world deployment. We conduct extensive experiments using multimodal
healthcare datasets containing images, time series, and structured information
to validate our findings.

</details>


### [37] [Evaluating the Impact of AI-Powered Audiovisual Personalization on Learner Emotion, Focus, and Learning Outcomes](https://arxiv.org/abs/2505.03033)
*George Xi Wang,Jingying Deng,Safinah Ali*

Main category: cs.AI

TL;DR: 论文提出了一种基于大语言模型（LLM）的个性化多感官学习环境系统，旨在通过定制视听元素提升学习者的专注力和情绪稳定性。


<details>
  <summary>Details</summary>
Motivation: 独立学习者在非结构化或干扰环境中难以保持专注和情绪调节，现有教育技术忽视了学习的情感与感官背景。

Method: 利用LLM生成个性化视听学习环境，结合生物识别和绩效数据评估其效果。

Result: 研究发现个性化视听组合对认知负荷和学习投入有显著影响。

Conclusion: 该系统为情感响应式教育技术提供了新方向，拓展了多模态LLM在自主学习感官维度的应用。

Abstract: Independent learners often struggle with sustaining focus and emotional
regulation in unstructured or distracting settings. Although some rely on
ambient aids such as music, ASMR, or visual backgrounds to support
concentration, these tools are rarely integrated into cohesive,
learner-centered systems. Moreover, existing educational technologies focus
primarily on content adaptation and feedback, overlooking the emotional and
sensory context in which learning takes place. Large language models have
demonstrated powerful multimodal capabilities including the ability to generate
and adapt text, audio, and visual content. Educational research has yet to
fully explore their potential in creating personalized audiovisual learning
environments. To address this gap, we introduce an AI-powered system that uses
LLMs to generate personalized multisensory study environments. Users select or
generate customized visual themes (e.g., abstract vs. realistic, static vs.
animated) and auditory elements (e.g., white noise, ambient ASMR, familiar vs.
novel sounds) to create immersive settings aimed at reducing distraction and
enhancing emotional stability. Our primary research question investigates how
combinations of personalized audiovisual elements affect learner cognitive load
and engagement. Using a mixed-methods design that incorporates biometric
measures and performance outcomes, this study evaluates the effectiveness of
LLM-driven sensory personalization. The findings aim to advance emotionally
responsive educational technologies and extend the application of multimodal
LLMs into the sensory dimension of self-directed learning.

</details>


### [38] [BLAB: Brutally Long Audio Bench](https://arxiv.org/abs/2505.03054)
*Orevaoghene Ahia,Martijn Bartelds,Kabir Ahuja,Hila Gonen,Valentin Hofmann,Siddhant Arora,Shuyue Stella Li,Vishal Puttagunta,Mofetoluwa Adeyemi,Charishma Buchireddy,Ben Walls,Noah Bennett,Shinji Watanabe,Noah A. Smith,Yulia Tsvetkov,Sachin Kumar*

Main category: cs.AI

TL;DR: BLAB是一个针对长音频语言模型的挑战性基准测试，用于评估模型在长音频任务中的表现，发现现有模型在长音频理解上表现不佳。


<details>
  <summary>Details</summary>
Motivation: 开发能够理解多样化语音交互的大型音频语言模型，以提升语言技术的可访问性，并填补现有研究在长音频任务上的空白。

Method: 引入BLAB基准测试，包含833+小时的多样化长音频片段（平均51分钟），并评估六种开源和专有音频语言模型的性能。

Result: 所有测试模型（包括Gemini 2.0 Pro和GPT-4o）在长音频任务中表现不佳，性能随音频时长增加而下降。

Conclusion: BLAB为开发具有长音频理解能力的音频语言模型提供了挑战性框架，揭示了现有模型的局限性。

Abstract: Developing large audio language models (LMs) capable of understanding diverse
spoken interactions is essential for accommodating the multimodal nature of
human communication and can increase the accessibility of language technologies
across different user populations. Recent work on audio LMs has primarily
evaluated their performance on short audio segments, typically under 30
seconds, with limited exploration of long-form conversational speech segments
that more closely reflect natural user interactions with these models. We
introduce Brutally Long Audio Bench (BLAB), a challenging long-form audio
benchmark that evaluates audio LMs on localization, duration estimation,
emotion, and counting tasks using audio segments averaging 51 minutes in
length. BLAB consists of 833+ hours of diverse, full-length audio clips, each
paired with human-annotated, text-based natural language questions and answers.
Our audio data were collected from permissively licensed sources and underwent
a human-assisted filtering process to ensure task compliance. We evaluate six
open-source and proprietary audio LMs on BLAB and find that all of them,
including advanced models such as Gemini 2.0 Pro and GPT-4o, struggle with the
tasks in BLAB. Our comprehensive analysis reveals key insights into the
trade-offs between task difficulty and audio duration. In general, we find that
audio LMs struggle with long-form speech, with performance declining as
duration increases. They perform poorly on localization, temporal reasoning,
counting, and struggle to understand non-phonemic information, relying more on
prompts than audio content. BLAB serves as a challenging evaluation framework
to develop audio LMs with robust long-form audio understanding capabilities.

</details>


### [39] [Is AI currently capable of identifying wild oysters? A comparison of human annotators against the AI model, ODYSSEE](https://arxiv.org/abs/2505.03108)
*Brendan Campbell,Alan Williams,Kleio Baxevani,Alyssa Campbell,Rushabh Dhoke,Rileigh E. Hudock,Xiaomin Lin,Vivek Mange,Bernhard Neuberger,Arjun Suresh,Alhim Vera,Arthur Trembanis,Herbert G. Tanner,Edward Hale*

Main category: cs.AI

TL;DR: 论文提出了一种基于深度学习的模型ODYSSEE，用于通过图像或视频识别活牡蛎，但其准确性（63%）低于专家（74%）和非专家（75%），未来需改进。


<details>
  <summary>Details</summary>
Motivation: 当前牡蛎礁监测方法破坏性强且费时，不适合小规模或敏感环境，因此开发了ODYSSEE模型以非破坏性方式快速评估牡蛎数量。

Method: 使用深度学习技术分析牡蛎礁的图像或视频，识别活牡蛎，并与专家和非专家的标注结果对比。

Result: ODYSSEE模型推断速度显著快于人工标注（39.6秒 vs. 2.34±0.61小时 vs. 4.50±1.46小时），但准确性较低（63% vs. 74% vs. 75%）。图像质量影响模型和人工的准确性。

Conclusion: 尽管ODYSSEE模型目前准确性不足，但未来通过改进图像质量、增加训练数据和标注类别，有望提升其预测能力，需进一步研究活牡蛎与死牡蛎的区分方法。

Abstract: Oysters are ecologically and commercially important species that require
frequent monitoring to track population demographics (e.g. abundance, growth,
mortality). Current methods of monitoring oyster reefs often require
destructive sampling methods and extensive manual effort. Therefore, they are
suboptimal for small-scale or sensitive environments. A recent alternative, the
ODYSSEE model, was developed to use deep learning techniques to identify live
oysters using video or images taken in the field of oyster reefs to assess
abundance. The validity of this model in identifying live oysters on a reef was
compared to expert and non-expert annotators. In addition, we identified
potential sources of prediction error. Although the model can make inferences
significantly faster than expert and non-expert annotators (39.6 s, $2.34 \pm
0.61$ h, $4.50 \pm 1.46$ h, respectively), the model overpredicted the number
of live oysters, achieving lower accuracy (63\%) in identifying live oysters
compared to experts (74\%) and non-experts (75\%) alike. Image quality was an
important factor in determining the accuracy of the model and the annotators.
Better quality images improved human accuracy and worsened model accuracy.
Although ODYSSEE was not sufficiently accurate, we anticipate that future
training on higher-quality images, utilizing additional live imagery, and
incorporating additional annotation training classes will greatly improve the
model's predictive power based on the results of this analysis. Future research
should address methods that improve the detection of living vs. dead oysters.

</details>


### [40] [Holmes: Automated Fact Check with Large Language Models](https://arxiv.org/abs/2505.03135)
*Haoran Ou,Gelei Deng,Xingshuo Han,Jie Zhang,Xinlei He,Han Qiu,Shangwei Guo,Tianwei Zhang*

Main category: cs.AI

TL;DR: 论文提出了一种名为Holmes的端到端框架，利用大型语言模型（LLMs）和新型证据检索方法，显著提升了多模态虚假信息的检测准确率。


<details>
  <summary>Details</summary>
Motivation: 互联网的普及加速了虚假信息的传播，威胁社会信任和国家安全。现有深度学习方法难以应对多模态虚假信息的复杂性。

Method: 研究探索了LLMs在虚假信息检测中的应用，发现其需要高质量证据支持。为此，提出Holmes框架，结合LLM驱动的摘要和新型证据质量评估算法。

Result: Holmes在两个开源数据集上达到88.3%的准确率，实时验证任务中达到90.2%，证据检索改进使事实核查准确率提升30.8%。

Conclusion: Holmes框架通过改进证据检索，显著提升了LLMs在虚假信息检测中的性能，为解决多模态虚假信息问题提供了有效方案。

Abstract: The rise of Internet connectivity has accelerated the spread of
disinformation, threatening societal trust, decision-making, and national
security. Disinformation has evolved from simple text to complex multimodal
forms combining images and text, challenging existing detection methods.
Traditional deep learning models struggle to capture the complexity of
multimodal disinformation. Inspired by advances in AI, this study explores
using Large Language Models (LLMs) for automated disinformation detection. The
empirical study shows that (1) LLMs alone cannot reliably assess the
truthfulness of claims; (2) providing relevant evidence significantly improves
their performance; (3) however, LLMs cannot autonomously search for accurate
evidence. To address this, we propose Holmes, an end-to-end framework featuring
a novel evidence retrieval method that assists LLMs in collecting high-quality
evidence. Our approach uses (1) LLM-powered summarization to extract key
information from open sources and (2) a new algorithm and metrics to evaluate
evidence quality. Holmes enables LLMs to verify claims and generate
justifications effectively. Experiments show Holmes achieves 88.3% accuracy on
two open-source datasets and 90.2% in real-time verification tasks. Notably,
our improved evidence retrieval boosts fact-checking accuracy by 30.8% over
existing methods

</details>


### [41] [CombiBench: Benchmarking LLM Capability for Combinatorial Mathematics](https://arxiv.org/abs/2505.03171)
*Junqi Liu,Xiaohan Lin,Jonas Bayer,Yael Dillies,Weijie Jiang,Xiaodan Liang,Roman Soletskyi,Haiming Wang,Yunzhou Xie,Beibei Xiong,Zhengfeng Yang,Jujian Zhang,Lihong Zhi,Jia Li,Zhengying Liu*

Main category: cs.AI

TL;DR: 论文介绍了CombiBench，一个包含100个组合问题的基准测试集，并提出了Fine-Eval评估框架，用于测试大型语言模型在组合数学问题上的表现。


<details>
  <summary>Details</summary>
Motivation: 组合数学领域缺乏合适的基准测试集和定理库，限制了神经符号方法在该领域的研究进展。

Method: 提出了CombiBench基准测试集和Fine-Eval评估框架，结合Kimina Lean Server后端，测试多个LLM在组合问题上的表现。

Result: 测试的LLM在组合问题上的表现有限，其中Kimina-Prover表现最佳，解决了7个问题。

Conclusion: CombiBench填补了组合数学领域的空白，但LLM在该领域的表现仍需提升。

Abstract: Neurosymbolic approaches integrating large language models with formal
reasoning have recently achieved human-level performance on mathematics
competition problems in algebra, geometry and number theory. In comparison,
combinatorics remains a challenging domain, characterized by a lack of
appropriate benchmarks and theorem libraries. To address this gap, we introduce
CombiBench, a comprehensive benchmark comprising 100 combinatorial problems,
each formalized in Lean~4 and paired with its corresponding informal statement.
The problem set covers a wide spectrum of difficulty levels, ranging from
middle school to IMO and university level, and span over ten combinatorial
topics. CombiBench is suitable for testing IMO solving capabilities since it
includes all IMO combinatorial problems since 2000 (except IMO 2004 P3 as its
statement contain an images). Furthermore, we provide a comprehensive and
standardized evaluation framework, dubbed Fine-Eval (for
$\textbf{F}$ill-in-the-blank $\textbf{in}$ L$\textbf{e}$an Evaluation), for
formal mathematics. It accommodates not only proof-based problems but also, for
the first time, the evaluation of fill-in-the-blank questions. Using Fine-Eval
as the evaluation method and Kimina Lean Server as the backend, we benchmark
several LLMs on CombiBench and observe that their capabilities for formally
solving combinatorial problems remain limited. Among all models tested (none of
which has been trained for this particular task), Kimina-Prover attains the
best results, solving 7 problems (out of 100) under both ``with solution'' and
``without solution'' scenarios. We open source the benchmark dataset alongside
with the code of the proposed evaluation method at
https://github.com/MoonshotAI/CombiBench/.

</details>


### [42] [Patterns and Mechanisms of Contrastive Activation Engineering](https://arxiv.org/abs/2505.03189)
*Yixiong Hao,Ayush Panda,Stepan Shabalin,Sheikh Abdur Raheem Ali*

Main category: cs.AI

TL;DR: 对比激活工程（CAE）是一种低成本、灵活的方法，用于调控大型语言模型（LLM）的输出行为，但其效果受限于分布内数据，且存在对抗性输入和模型困惑度增加的缺点。


<details>
  <summary>Details</summary>
Motivation: 由于LLM的复杂性和不透明性，控制其行为具有挑战性，CAE提供了一种无需额外计算资源的解决方案。

Method: 通过对比激活工程技术，在推理时对模型的内部表示进行针对性修改，以调整模型行为。

Result: CAE在分布内数据中有效，但样本数超过80后效果递减，且易受对抗性输入影响，同时会增加模型困惑度。

Conclusion: CAE是一种有前景的技术，但需谨慎部署，尤其是在分布外数据和大模型中。

Abstract: Controlling the behavior of Large Language Models (LLMs) remains a
significant challenge due to their inherent complexity and opacity. While
techniques like fine-tuning can modify model behavior, they typically require
extensive computational resources. Recent work has introduced a class of
contrastive activation engineering (CAE) techniques as promising approaches for
steering LLM outputs through targeted modifications to their internal
representations. Applied at inference-time with zero cost, CAE has the
potential to introduce a new paradigm of flexible, task-specific LLM behavior
tuning. We analyze the performance of CAE in in-distribution,
out-of-distribution settings, evaluate drawbacks, and begin to develop
comprehensive guidelines for its effective deployment. We find that 1. CAE is
only reliably effective when applied to in-distribution contexts. 2. Increasing
the number of samples used to generate steering vectors has diminishing returns
at around 80 samples. 3. Steering vectors are susceptible to adversarial inputs
that reverses the behavior that is steered for. 4. Steering vectors harm the
overall model perplexity. 5. Larger models are more resistant to
steering-induced degradation.

</details>


### [43] [RAG-MCP: Mitigating Prompt Bloat in LLM Tool Selection via Retrieval-Augmented Generation](https://arxiv.org/abs/2505.03275)
*Tiantian Gan,Qiyao Sun*

Main category: cs.AI

TL;DR: RAG-MCP框架通过语义检索减少提示词数量并提高工具选择准确性，显著优化LLM的外部工具使用效率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）在利用外部工具时面临提示词膨胀和选择复杂性的问题，RAG-MCP旨在解决这一挑战。

Method: RAG-MCP采用检索增强生成框架，通过语义检索从外部索引中识别最相关的工具描述，仅传递选定的工具描述给LLM。

Result: 实验显示，RAG-MCP显著减少提示词数量（如减少50%以上），并将工具选择准确性提高三倍以上（43.13% vs 13.62%）。

Conclusion: RAG-MCP为LLM提供了可扩展且准确的外部工具集成方案。

Abstract: Large language models (LLMs) struggle to effectively utilize a growing number
of external tools, such as those defined by the Model Context Protocol
(MCP)\cite{IntroducingMCP}, due to prompt bloat and selection complexity. We
introduce RAG-MCP, a Retrieval-Augmented Generation framework that overcomes
this challenge by offloading tool discovery. RAG-MCP uses semantic retrieval to
identify the most relevant MCP(s) for a given query from an external index
before engaging the LLM. Only the selected tool descriptions are passed to the
model, drastically reducing prompt size and simplifying decision-making.
Experiments, including an MCP stress test, demonstrate RAG-MCP significantly
cuts prompt tokens (e.g., by over 50%) and more than triples tool selection
accuracy (43.13% vs 13.62% baseline) on benchmark tasks. RAG-MCP enables
scalable and accurate tool integration for LLMs.

</details>


### [44] [Capability-Driven Skill Generation with LLMs: A RAG-Based Approach for Reusing Existing Libraries and Interfaces](https://arxiv.org/abs/2505.03295)
*Luis Miguel Vieira da Silva,Aljosha Köcher,Nicolas König,Felix Gehlhoff,Alexander Fay*

Main category: cs.AI

TL;DR: 论文提出了一种利用大语言模型基于自然语言输入生成符合能力契约的技能实现代码的方法，并整合现有软件库和接口技术，支持多目标语言。


<details>
  <summary>Details</summary>
Motivation: 开发符合能力契约的技能实现耗时且具有挑战性，需要一种更高效的方法。

Method: 将能力视为技能实现的契约，利用大语言模型生成代码，并通过检索增强生成架构整合用户自定义库和接口。

Result: 在Python和ROS 2控制的自主移动机器人上验证了方法的可行性和灵活性。

Conclusion: 该方法为技能实现提供了一种高效且灵活的解决方案。

Abstract: Modern automation systems increasingly rely on modular architectures, with
capabilities and skills as one solution approach. Capabilities define the
functions of resources in a machine-readable form and skills provide the
concrete implementations that realize those capabilities. However, the
development of a skill implementation conforming to a corresponding capability
remains a time-consuming and challenging task. In this paper, we present a
method that treats capabilities as contracts for skill implementations and
leverages large language models to generate executable code based on natural
language user input. A key feature of our approach is the integration of
existing software libraries and interface technologies, enabling the generation
of skill implementations across different target languages. We introduce a
framework that allows users to incorporate their own libraries and resource
interfaces into the code generation process through a retrieval-augmented
generation architecture. The proposed method is evaluated using an autonomous
mobile robot controlled via Python and ROS 2, demonstrating the feasibility and
flexibility of the approach.

</details>


### [45] [Artificial Behavior Intelligence: Technology, Challenges, and Future Directions](https://arxiv.org/abs/2505.03315)
*Kanghyun Jo,Jehwan Choi,Kwanho Kim,Seongmin Kim,Duy-Linh Nguyen,Xuan-Thuy Vo,Adri Priadana,Tien-Dat Tran*

Main category: cs.AI

TL;DR: 本文提出了人工行为智能（ABI）的技术框架，分析了其在多领域的应用潜力，并探讨了如何利用大规模预训练模型提升行为识别的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 理解和预测人类行为在自动驾驶、智能医疗、监控系统和社交机器人等领域具有重要应用价值。

Method: ABI框架包括姿态估计、面部与情绪识别、行为序列分析和上下文感知建模，并结合大规模预训练模型（如LLMs、视觉基础模型和多模态集成模型）进行优化。

Result: 研究团队致力于开发轻量级智能模型，以高效推断复杂人类行为，并提出了解决实际应用中技术挑战的优化策略。

Conclusion: ABI在现实应用中仍面临数据有限性、行为预测不确定性和模型优化等挑战，团队正通过轻量级架构和多模态知识蒸馏等方法进行探索。

Abstract: Understanding and predicting human behavior has emerged as a core capability
in various AI application domains such as autonomous driving, smart healthcare,
surveillance systems, and social robotics. This paper defines the technical
framework of Artificial Behavior Intelligence (ABI), which comprehensively
analyzes and interprets human posture, facial expressions, emotions, behavioral
sequences, and contextual cues. It details the essential components of ABI,
including pose estimation, face and emotion recognition, sequential behavior
analysis, and context-aware modeling. Furthermore, we highlight the
transformative potential of recent advances in large-scale pretrained models,
such as large language models (LLMs), vision foundation models, and multimodal
integration models, in significantly improving the accuracy and
interpretability of behavior recognition. Our research team has a strong
interest in the ABI domain and is actively conducting research, particularly
focusing on the development of intelligent lightweight models capable of
efficiently inferring complex human behaviors. This paper identifies several
technical challenges that must be addressed to deploy ABI in real-world
applications including learning behavioral intelligence from limited data,
quantifying uncertainty in complex behavior prediction, and optimizing model
structures for low-power, real-time inference. To tackle these challenges, our
team is exploring various optimization strategies including lightweight
transformers, graph-based recognition architectures, energy-aware loss
functions, and multimodal knowledge distillation, while validating their
applicability in real-time environments.

</details>


### [46] [AI-Driven Scholarly Peer Review via Persistent Workflow Prompting, Meta-Prompting, and Meta-Reasoning](https://arxiv.org/abs/2505.03332)
*Evgeny Markhasin*

Main category: cs.AI

TL;DR: 论文提出了一种名为Persistent Workflow Prompting (PWP)的方法，旨在通过标准LLM聊天界面实现复杂的科学手稿评审任务，无需额外代码或API。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型（LLMs）在科学手稿评审中因数据限制和专家推理复杂性而面临的挑战。

Method: 采用分层模块化架构（通过Markdown结构化）定义详细分析工作流程，并通过元提示技术和元推理迭代开发PWP提示。

Result: PWP指导的LLM能够识别测试案例中的主要方法缺陷，减少输入偏差，并执行复杂任务（如区分主张与证据、综合分析文本/图片/图表等）。

Conclusion: PWP方法通过详细工作流程形式化，展示了利用现有LLM完成复杂科学任务的潜力，并提供了透明性和可复现性支持。

Abstract: Critical peer review of scientific manuscripts presents a significant
challenge for Large Language Models (LLMs), partly due to data limitations and
the complexity of expert reasoning. This report introduces Persistent Workflow
Prompting (PWP), a potentially broadly applicable prompt engineering
methodology designed to bridge this gap using standard LLM chat interfaces
(zero-code, no APIs). We present a proof-of-concept PWP prompt for the critical
analysis of experimental chemistry manuscripts, featuring a hierarchical,
modular architecture (structured via Markdown) that defines detailed analysis
workflows. We develop this PWP prompt through iterative application of
meta-prompting techniques and meta-reasoning aimed at systematically codifying
expert review workflows, including tacit knowledge. Submitted once at the start
of a session, this PWP prompt equips the LLM with persistent workflows
triggered by subsequent queries, guiding modern reasoning LLMs through
systematic, multimodal evaluations. Demonstrations show the PWP-guided LLM
identifying major methodological flaws in a test case while mitigating LLM
input bias and performing complex tasks, including distinguishing claims from
evidence, integrating text/photo/figure analysis to infer parameters, executing
quantitative feasibility checks, comparing estimates against claims, and
assessing a priori plausibility. To ensure transparency and facilitate
replication, we provide full prompts, detailed demonstration analyses, and logs
of interactive chats as supplementary resources. Beyond the specific
application, this work offers insights into the meta-development process
itself, highlighting the potential of PWP, informed by detailed workflow
formalization, to enable sophisticated analysis using readily available LLMs
for complex scientific tasks.

</details>


### [47] [Domain Adversarial Training for Mitigating Gender Bias in Speech-based Mental Health Detection](https://arxiv.org/abs/2505.03359)
*June-Woo Kim,Haram Yoon,Wonkyo Oh,Dawoon Jung,Sung-Hoon Yoon,Dae-Jin Kim,Dong-Ho Lee,Sang-Yeol Lee,Chan-Mo Yang*

Main category: cs.AI

TL;DR: 该论文提出了一种基于领域对抗训练的方法，用于减少语音AI模型在抑郁症和PTSD检测中的性别偏见，显著提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 语音AI模型在心理健康评估中存在性别偏见，导致预测不公和不准确，研究旨在解决这一问题。

Method: 采用领域对抗训练方法，将不同性别视为不同领域，并将此信息整合到预训练的语音基础模型中。

Result: 在E-DAIC数据集上验证，F1分数比基线提高了13.29个百分点。

Conclusion: 研究表明，解决人口统计学差异对AI驱动的心理健康评估至关重要。

Abstract: Speech-based AI models are emerging as powerful tools for detecting
depression and the presence of Post-traumatic stress disorder (PTSD), offering
a non-invasive and cost-effective way to assess mental health. However, these
models often struggle with gender bias, which can lead to unfair and inaccurate
predictions. In this study, our study addresses this issue by introducing a
domain adversarial training approach that explicitly considers gender
differences in speech-based depression and PTSD detection. Specifically, we
treat different genders as distinct domains and integrate this information into
a pretrained speech foundation model. We then validate its effectiveness on the
E-DAIC dataset to assess its impact on performance. Experimental results show
that our method notably improves detection performance, increasing the F1-score
by up to 13.29 percentage points compared to the baseline. This highlights the
importance of addressing demographic disparities in AI-driven mental health
assessment.

</details>


### [48] [Validating the Effectiveness of a Large Language Model-based Approach for Identifying Children's Development across Various Free Play Settings in Kindergarten](https://arxiv.org/abs/2505.03369)
*Yuanyuan Yang,Yuan Shen,Tianchen Sun,Yangbin Xie*

Main category: cs.AI

TL;DR: 该研究提出了一种结合大型语言模型（LLM）和学习分析技术的方法，通过分析儿童的游戏自我叙述来评估其发展能力，结果显示该方法在多个领域准确率超过90%。


<details>
  <summary>Details</summary>
Motivation: 自由游戏对儿童发展至关重要，但传统评估方法难以全面捕捉其发展情况，因此需要一种更高效、准确的方法。

Method: 研究结合LLM和学习分析技术，分析29名儿童在四个不同游戏区域的2,224个游戏叙述，评估其认知、运动和社交能力。

Result: LLM方法在大多数领域的准确率超过90%，且不同游戏区域对特定能力的发展有显著差异。

Conclusion: 该方法能有效识别儿童在自由游戏中的发展，为教育者提供个性化学习支持，展示了LLM和学习分析在早期教育中的潜力。

Abstract: Free play is a fundamental aspect of early childhood education, supporting
children's cognitive, social, emotional, and motor development. However,
assessing children's development during free play poses significant challenges
due to the unstructured and spontaneous nature of the activity. Traditional
assessment methods often rely on direct observations by teachers, parents, or
researchers, which may fail to capture comprehensive insights from free play
and provide timely feedback to educators. This study proposes an innovative
approach combining Large Language Models (LLMs) with learning analytics to
analyze children's self-narratives of their play experiences. The LLM
identifies developmental abilities, while performance scores across different
play settings are calculated using learning analytics techniques. We collected
2,224 play narratives from 29 children in a kindergarten, covering four
distinct play areas over one semester. According to the evaluation results from
eight professionals, the LLM-based approach achieved high accuracy in
identifying cognitive, motor, and social abilities, with accuracy exceeding 90%
in most domains. Moreover, significant differences in developmental outcomes
were observed across play settings, highlighting each area's unique
contributions to specific abilities. These findings confirm that the proposed
approach is effective in identifying children's development across various free
play settings. This study demonstrates the potential of integrating LLMs and
learning analytics to provide child-centered insights into developmental
trajectories, offering educators valuable data to support personalized learning
and enhance early childhood education practices.

</details>


### [49] [Procedural Memory Is Not All You Need: Bridging Cognitive Gaps in LLM-Based Agents](https://arxiv.org/abs/2505.03434)
*Schaun Wheeler,Olivier Jeunen*

Main category: cs.AI

TL;DR: 论文探讨了大语言模型（LLMs）在复杂环境中的局限性，提出通过结合语义记忆和联想学习系统来增强其适应性。


<details>
  <summary>Details</summary>
Motivation: LLMs在文本生成等任务中表现出色，但在复杂、不可预测的环境中存在局限性，需要更灵活的认知功能。

Method: 提出模块化架构，将语义记忆和联想学习系统与LLMs分离，以增强其适应性。

Result: 通过结合多种认知功能，LLMs可以更好地应对复杂环境中的问题。

Conclusion: 模块化架构是提升LLMs适应性和解决现实问题的关键。

Abstract: Large Language Models (LLMs) represent a landmark achievement in Artificial
Intelligence (AI), demonstrating unprecedented proficiency in procedural tasks
such as text generation, code completion, and conversational coherence. These
capabilities stem from their architecture, which mirrors human procedural
memory -- the brain's ability to automate repetitive, pattern-driven tasks
through practice. However, as LLMs are increasingly deployed in real-world
applications, it becomes impossible to ignore their limitations operating in
complex, unpredictable environments. This paper argues that LLMs, while
transformative, are fundamentally constrained by their reliance on procedural
memory. To create agents capable of navigating ``wicked'' learning environments
-- where rules shift, feedback is ambiguous, and novelty is the norm -- we must
augment LLMs with semantic memory and associative learning systems. By adopting
a modular architecture that decouples these cognitive functions, we can bridge
the gap between narrow procedural expertise and the adaptive intelligence
required for real-world problem-solving.

</details>


### [50] [The Steganographic Potentials of Language Models](https://arxiv.org/abs/2505.03439)
*Artem Karpov,Tinuade Adeleke,Seong Hah Cho,Natalia Perez-Campanero*

Main category: cs.AI

TL;DR: 论文研究了大型语言模型（LLMs）通过强化学习微调后隐藏信息的能力，发现当前模型在安全性和容量上表现基础，但算法指导能显著提升其隐藏信息的能力。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs通过隐写术隐藏信息的能力，以应对未对齐AI代理的检测与防范问题，并评估其对LLMs推理忠实性的影响。

Method: 通过强化学习微调LLMs，开发隐蔽编码方案，并在提示或未提示的情况下测试其隐写术能力。

Result: 实验表明，当前模型在隐写术的安全性和容量上表现基础，但算法指导能显著提升其能力。

Conclusion: LLMs具备基础的隐写能力，但需进一步研究以应对潜在的安全风险。

Abstract: The potential for large language models (LLMs) to hide messages within plain
text (steganography) poses a challenge to detection and thwarting of unaligned
AI agents, and undermines faithfulness of LLMs reasoning. We explore the
steganographic capabilities of LLMs fine-tuned via reinforcement learning (RL)
to: (1) develop covert encoding schemes, (2) engage in steganography when
prompted, and (3) utilize steganography in realistic scenarios where hidden
reasoning is likely, but not prompted. In these scenarios, we detect the
intention of LLMs to hide their reasoning as well as their steganography
performance. Our findings in the fine-tuning experiments as well as in
behavioral non fine-tuning evaluations reveal that while current models exhibit
rudimentary steganographic abilities in terms of security and capacity,
explicit algorithmic guidance markedly enhances their capacity for information
concealment.

</details>


### [51] [am-ELO: A Stable Framework for Arena-based LLM Evaluation](https://arxiv.org/abs/2505.03475)
*Zirui Liu,Jiatong Li,Yan Zhuang,Qi Liu,Shuanghong Shen,Jie Ouyang,Mingyue Cheng,Shijin Wang*

Main category: cs.AI

TL;DR: 本文提出了一种基于最大似然估计（MLE）的稳定竞技场框架（m-ELO和am-ELO），解决了现有ELO评分系统在模型排名中的不稳定性和忽略标注者能力差异的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于ELO评分系统的评估框架存在排名不一致导致的稳定性问题，且未考虑标注者能力的差异。

Method: 提出m-ELO方法，用MLE替代迭代更新；进一步提出am-ELO，修改ELO评分概率函数以纳入标注者能力。

Result: 实验证明该方法提高了稳定性，为大型语言模型提供了更鲁棒、准确和稳定的评估方法。

Conclusion: 该框架为现代AI模型（尤其是大型语言模型）的评估提供了更可靠的解决方案。

Abstract: Arena-based evaluation is a fundamental yet significant evaluation paradigm
for modern AI models, especially large language models (LLMs). Existing
framework based on ELO rating system suffers from the inevitable instability
problem due to ranking inconsistency and the lack of attention to the varying
abilities of annotators. In this paper, we introduce a novel stable arena
framework to address these issues by enhancing the ELO Rating System.
Specifically, we replace the iterative update method with a Maximum Likelihood
Estimation (MLE) approach, m-ELO, and provide theoretical proof of the
consistency and stability of the MLE approach for model ranking. Additionally,
we proposed the am-ELO, which modify the Elo Rating's probability function to
incorporate annotator abilities, enabling the simultaneous estimation of model
scores and annotator reliability. Experiments demonstrate that this method
ensures stability, proving that this framework offers a more robust, accurate,
and stable evaluation method for LLMs.

</details>


### [52] [STORY2GAME: Generating (Almost) Everything in an Interactive Fiction Game](https://arxiv.org/abs/2505.03547)
*Eric Zhou,Shreyas Basavatia,Moontashir Siam,Zexin Chen,Mark O. Riedl*

Main category: cs.AI

TL;DR: STORY2GAME利用大型语言模型生成文本互动小说游戏，通过生成故事、填充世界并构建游戏引擎代码，实现开放式故事生成与动态动作生成。


<details>
  <summary>Details</summary>
Motivation: 传统硬编码动作会限制故事生成的开放性，而动态生成动作能提供更自由且基于游戏状态的体验。

Method: 利用LLM生成动作的前置条件和效果，指导游戏引擎跟踪和修改状态；动态生成新动作以满足玩家需求。

Result: 评估了动作代码生成的成功率，确保玩家能完整交互体验生成的故事。

Conclusion: STORY2GAME通过动态动作生成和状态管理，实现了开放式故事生成与交互体验的结合。

Abstract: We introduce STORY2GAME, a novel approach to using Large Language Models to
generate text-based interactive fiction games that starts by generating a
story, populates the world, and builds the code for actions in a game engine
that enables the story to play out interactively. Whereas a given set of
hard-coded actions can artificially constrain story generation, the ability to
generate actions means the story generation process can be more open-ended but
still allow for experiences that are grounded in a game state. The key to
successful action generation is to use LLM-generated preconditions and effects
of actions in the stories as guides for what aspects of the game state must be
tracked and changed by the game engine when a player performs an action. We
also introduce a technique for dynamically generating new actions to
accommodate the player's desire to perform actions that they think of that are
not part of the story. Dynamic action generation may require on-the-fly updates
to the game engine's state representation and revision of previously generated
actions. We evaluate the success rate of action code generation with respect to
whether a player can interactively play through the entire generated story.

</details>


### [53] [A Hashgraph-Inspired Consensus Mechanism for Reliable Multi-Model Reasoning](https://arxiv.org/abs/2505.03553)
*Kolawole E. Ogunsina,Morayo A. Ogunsina*

Main category: cs.AI

TL;DR: 提出一种基于Hashgraph共识机制的新方法，用于解决大型语言模型输出不一致和幻觉问题，通过多模型协作提高准确性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 不同专有推理模型在相同复杂请求下输出不一致，影响AI系统的可靠性，需一种机制验证和收敛这些输出。

Method: 采用Hashgraph共识算法，通过gossip-about-gossip通信和虚拟投票实现模型间一致，设计原型系统迭代更新答案。

Result: 该方法通过多模型交叉验证减少非事实输出，优于传统集成技术，展示了多智能体AI系统自验证的潜力。

Conclusion: Hashgraph共识机制为多模型协作提供新方向，有望在复杂任务中实现高保真响应。

Abstract: Inconsistent outputs and hallucinations from large language models (LLMs) are
major obstacles to reliable AI systems. When different proprietary reasoning
models (RMs), such as those by OpenAI, Google, Anthropic, DeepSeek, and xAI,
are given the same complex request, they often produce divergent results due to
variations in training and inference. This paper proposes a novel consensus
mechanism, inspired by distributed ledger technology, to validate and converge
these outputs, treating each RM as a black-box peer. Building on the Hashgraph
consensus algorithm, our approach employs gossip-about-gossip communication and
virtual voting to achieve agreement among an ensemble of RMs. We present an
architectural design for a prototype system in which RMs iteratively exchange
and update their answers, using information from each round to improve accuracy
and confidence in subsequent rounds. This approach goes beyond simple majority
voting by incorporating the knowledge and cross-verification content of every
model. We justify the feasibility of this Hashgraph-inspired consensus for AI
ensembles and outline its advantages over traditional ensembling techniques in
reducing nonfactual outputs. Preliminary considerations for implementation,
evaluation criteria for convergence and accuracy, and potential challenges are
discussed. The proposed mechanism demonstrates a promising direction for
multi-agent AI systems to self-validate and deliver high-fidelity responses in
complex tasks.

</details>


### [54] [OSUniverse: Benchmark for Multimodal GUI-navigation AI Agents](https://arxiv.org/abs/2505.03570)
*Mariya Davydova,Daniel Jeffries,Patrick Barker,Arturo Márquez Flores,Sinéad Ryan*

Main category: cs.AI

TL;DR: OSUniverse是一个针对GUI导航AI代理的复杂多模态桌面任务基准测试，强调易用性、可扩展性、全面覆盖测试用例和自动化验证。


<details>
  <summary>Details</summary>
Motivation: 为评估GUI导航AI代理的能力和进步提供可靠、自动化的基准测试工具。

Method: 任务按复杂度分级，从基础点击到多步骤多应用测试，并引入自动化验证机制。

Result: 当前SOTA代理在测试中表现不超过50%，而普通白领能完美完成。自动化验证误差率低于2%。

Conclusion: OSUniverse为GUI导航AI代理的短期和中期发展提供了有效的评估基础。

Abstract: In this paper, we introduce OSUniverse: a benchmark of complex, multimodal
desktop-oriented tasks for advanced GUI-navigation AI agents that focuses on
ease of use, extensibility, comprehensive coverage of test cases, and automated
validation. We divide the tasks in increasing levels of complexity, from basic
precision clicking to multistep, multiapplication tests requiring dexterity,
precision, and clear thinking from the agent. In version one of the benchmark,
presented here, we have calibrated the complexity of the benchmark test cases
to ensure that the SOTA (State of the Art) agents (at the time of publication)
do not achieve results higher than 50%, while the average white collar worker
can perform all these tasks with perfect accuracy. The benchmark can be scored
manually, but we also introduce an automated validation mechanism that has an
average error rate less than 2%. Therefore, this benchmark presents solid
ground for fully automated measuring of progress, capabilities and the
effectiveness of GUI-navigation AI agents over the short and medium-term
horizon. The source code of the benchmark is available at
https://github.com/agentsea/osuniverse.

</details>


### [55] [Synthesizing Images on Perceptual Boundaries of ANNs for Uncovering and Manipulating Human Perceptual Variability](https://arxiv.org/abs/2505.03641)
*Chen Wei,Chi Zhang,Jiachen Zou,Haotian Deng,Dietmar Heinke,Quanying Liu*

Main category: cs.AI

TL;DR: 论文提出了一种计算框架BAM，结合人工神经网络的感知边界采样和人类行为实验，研究人类决策的变异性。通过大规模实验验证，生成了variMNIST数据集，并开发了预测和操纵个体感知差异的方法。


<details>
  <summary>Details</summary>
Motivation: 理解人类在认知任务和日常生活中的决策变异性，揭示其背后的感知和决策机制。

Method: 结合ANN的感知边界采样和人类行为实验，开发BAM框架，生成变异性刺激并通过实验验证。

Result: 生成了variMNIST数据集，建立了预测和操纵个体感知差异的可靠方法。

Conclusion: BAM框架填补了计算模型与人类个体差异研究之间的空白，为个性化感知分析提供了新工具。

Abstract: Human decision-making in cognitive tasks and daily life exhibits considerable
variability, shaped by factors such as task difficulty, individual preferences,
and personal experiences. Understanding this variability across individuals is
essential for uncovering the perceptual and decision-making mechanisms that
humans rely on when faced with uncertainty and ambiguity. We present a
computational framework BAM (Boundary Alignment & Manipulation framework) that
combines perceptual boundary sampling in ANNs and human behavioral experiments
to systematically investigate this phenomenon. Our perceptual boundary sampling
algorithm generates stimuli along ANN decision boundaries that intrinsically
induce significant perceptual variability. The efficacy of these stimuli is
empirically validated through large-scale behavioral experiments involving 246
participants across 116,715 trials, culminating in the variMNIST dataset
containing 19,943 systematically annotated images. Through personalized model
alignment and adversarial generation, we establish a reliable method for
simultaneously predicting and manipulating the divergent perceptual decisions
of pairs of participants. This work bridges the gap between computational
models and human individual difference research, providing new tools for
personalized perception analysis.

</details>


### [56] [BURNS: Backward Underapproximate Reachability for Neural-Feedback-Loop Systems](https://arxiv.org/abs/2505.03643)
*Chelsea Sidrane,Jana Tumova*

Main category: cs.AI

TL;DR: 论文提出了一种计算非线性离散时间神经反馈环路后向可达集的算法，用于验证学习启用的规划和控制算法的目标可达性。


<details>
  <summary>Details</summary>
Motivation: 学习启用的规划和控制算法缺乏严格的性能或安全性保证，需要一种方法来验证其目标可达性。

Method: 通过过近似系统动力学函数，利用混合整数线性规划计算后向可达集，验证目标可达性。

Result: 算法在数值示例中表现良好，扩展了可验证的学习启用系统属性类别。

Conclusion: 该算法为学习启用系统提供了更广泛的验证能力，增强了其安全性和性能保证。

Abstract: Learning-enabled planning and control algorithms are increasingly popular,
but they often lack rigorous guarantees of performance or safety. We introduce
an algorithm for computing underapproximate backward reachable sets of
nonlinear discrete time neural feedback loops. We then use the backward
reachable sets to check goal-reaching properties. Our algorithm is based on
overapproximating the system dynamics function to enable computation of
underapproximate backward reachable sets through solutions of mixed-integer
linear programs. We rigorously analyze the soundness of our algorithm and
demonstrate it on a numerical example. Our work expands the class of properties
that can be verified for learning-enabled systems.

</details>


### [57] [Learning Symbolic Persistent Macro-Actions for POMDP Solving Over Time](https://arxiv.org/abs/2505.03668)
*Celeste Veronese,Daniele Meli,Alessandro Farinelli*

Main category: cs.AI

TL;DR: 论文提出了一种结合时间逻辑推理和POMDP的方法，通过LTL生成持久宏动作，显著减少推理时间并提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决在不确定性下实现可解释决策的问题，同时减少手动设计启发式规则的需求。

Method: 利用基于事件演算的LTL生成持久宏动作，结合MCTS-POMDP求解器，并通过ILP从少量执行轨迹中学习宏动作。

Result: 在Pocman和Rocksample基准测试中，学习的宏动作表现出更高的表达性和通用性，显著提升了计算效率。

Conclusion: 该方法有效结合了时间逻辑和POMDP，实现了高效且可解释的决策，适用于复杂不确定性环境。

Abstract: This paper proposes an integration of temporal logical reasoning and
Partially Observable Markov Decision Processes (POMDPs) to achieve
interpretable decision-making under uncertainty with macro-actions. Our method
leverages a fragment of Linear Temporal Logic (LTL) based on Event Calculus
(EC) to generate \emph{persistent} (i.e., constant) macro-actions, which guide
Monte Carlo Tree Search (MCTS)-based POMDP solvers over a time horizon,
significantly reducing inference time while ensuring robust performance. Such
macro-actions are learnt via Inductive Logic Programming (ILP) from a few
traces of execution (belief-action pairs), thus eliminating the need for
manually designed heuristics and requiring only the specification of the POMDP
transition model. In the Pocman and Rocksample benchmark scenarios, our learned
macro-actions demonstrate increased expressiveness and generality when compared
to time-independent heuristics, indeed offering substantial computational
efficiency improvements.

</details>


### [58] [Gap the (Theory of) Mind: Sharing Beliefs About Teammates' Goals Boosts Collaboration Perception, Not Performance](https://arxiv.org/abs/2505.03674)
*Yotam Amitai,Reuth Mirsky,Ofra Amir*

Main category: cs.AI

TL;DR: 研究探讨AI代理通过共享对人类队友目标的推断理解是否能提升任务表现和协作感知，发现目标共享虽未显著改善任务表现或满意度，但支持战略调整和协作感知。


<details>
  <summary>Details</summary>
Motivation: 在人类与AI代理团队中，直接沟通目标不可行时，通过行动推断意图是常见做法。研究旨在验证AI共享推断目标是否能提升协作效果。

Method: 通过实验比较三种条件（无识别、可行目标、按需可行目标），分析任务表现、满意度、战略调整及认知负荷。

Result: 目标共享未显著提升任务表现或满意度，但支持战略调整和协作感知，且未增加认知负荷。

Conclusion: 目标共享在提升协作感知和信任方面有效，但需平衡信息量与简洁性以避免影响客观表现。

Abstract: In human-agent teams, openly sharing goals is often assumed to enhance
planning, collaboration, and effectiveness. However, direct communication of
these goals is not always feasible, requiring teammates to infer their
partner's intentions through actions. Building on this, we investigate whether
an AI agent's ability to share its inferred understanding of a human teammate's
goals can improve task performance and perceived collaboration. Through an
experiment comparing three conditions-no recognition (NR), viable goals (VG),
and viable goals on-demand (VGod) - we find that while goal-sharing information
did not yield significant improvements in task performance or overall
satisfaction scores, thematic analysis suggests that it supported strategic
adaptations and subjective perceptions of collaboration. Cognitive load
assessments revealed no additional burden across conditions, highlighting the
challenge of balancing informativeness and simplicity in human-agent
interactions. These findings highlight the nuanced trade-off of goal-sharing:
while it fosters trust and enhances perceived collaboration, it can
occasionally hinder objective performance gains.

</details>


### [59] [Graph Drawing for LLMs: An Empirical Evaluation](https://arxiv.org/abs/2505.03678)
*Walter Didimo,Fabrizio Montecchiani,Tommaso Piselli*

Main category: cs.AI

TL;DR: 研究了大型语言模型（LLMs）在图相关任务中的表现，重点关注视觉模态输入（如图形绘制）对模型性能的影响，包括布局范式、图形美观度和提示技术。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在图任务中的表现，尤其是视觉输入对其性能的影响，以优化模型的应用效果。

Method: 通过实验分析布局范式、图形美观度和提示技术对模型性能的影响，并提出三个研究问题。

Result: 发现选择合适的布局范式和优化图形可读性可显著提升模型性能，提示技术的选择对性能至关重要。

Conclusion: 优化视觉输入和提示技术是提升LLMs在图任务中性能的关键。

Abstract: Our work contributes to the fast-growing literature on the use of Large
Language Models (LLMs) to perform graph-related tasks. In particular, we focus
on usage scenarios that rely on the visual modality, feeding the model with a
drawing of the graph under analysis. We investigate how the model's performance
is affected by the chosen layout paradigm, the aesthetics of the drawing, and
the prompting technique used for the queries. We formulate three corresponding
research questions and present the results of a thorough experimental analysis.
Our findings reveal that choosing the right layout paradigm and optimizing the
readability of the input drawing from a human perspective can significantly
improve the performance of the model on the given task. Moreover, selecting the
most effective prompting technique is a challenging yet crucial task for
achieving optimal performance.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [60] [Uncertainty Quantification for Machine Learning in Healthcare: A Survey](https://arxiv.org/abs/2505.02874)
*L. Julián Lechuga López,Shaza Elsharief,Dhiyaa Al Jorf,Firas Darwish,Congbo Ma,Farah E. Shamout*

Main category: cs.LG

TL;DR: 本文综述了医疗领域中机器学习模型的不确定性量化（UQ）方法，分析了现有研究的局限性，并提出了一个框架以指导UQ在ML流程中的应用。


<details>
  <summary>Details</summary>
Motivation: 医疗领域中ML模型的可靠性、安全性和可解释性至关重要，但目前缺乏系统性的UQ方法评估和整合，限制了其在医疗应用中的推广。

Method: 通过全面分析现有UQ方法，提出一个框架，指导UQ在数据预处理、训练和评估等ML流程各阶段的应用。

Result: 总结了医疗领域常用的UQ方法，并探讨了其他领域潜在适用的新方法。

Conclusion: 本文为医疗领域ML模型的UQ应用提供了挑战与机遇的清晰概述，帮助研究者和从业者选择合适技术以提升模型可靠性和信任度。

Abstract: Uncertainty Quantification (UQ) is pivotal in enhancing the robustness,
reliability, and interpretability of Machine Learning (ML) systems for
healthcare, optimizing resources and improving patient care. Despite the
emergence of ML-based clinical decision support tools, the lack of principled
quantification of uncertainty in ML models remains a major challenge. Current
reviews have a narrow focus on analyzing the state-of-the-art UQ in specific
healthcare domains without systematically evaluating method efficacy across
different stages of model development, and despite a growing body of research,
its implementation in healthcare applications remains limited. Therefore, in
this survey, we provide a comprehensive analysis of current UQ in healthcare,
offering an informed framework that highlights how different methods can be
integrated into each stage of the ML pipeline including data processing,
training and evaluation. We also highlight the most popular methods used in
healthcare and novel approaches from other domains that hold potential for
future adoption in the medical context. We expect this study will provide a
clear overview of the challenges and opportunities of implementing UQ in the ML
pipeline for healthcare, guiding researchers and practitioners in selecting
suitable techniques to enhance the reliability, safety and trust from patients
and clinicians on ML-driven healthcare solutions.

</details>


### [61] [A Wireless Collaborated Inference Acceleration Framework for Plant Disease Recognition](https://arxiv.org/abs/2505.02877)
*Hele Zhu,Xinyi Huang,Haojia Gao,Mengfei Jiang,Haohua Que,Lei Mu*

Main category: cs.LG

TL;DR: 提出了一种基于边缘设备与云服务器协同推理的植物病害识别框架，通过深度强化学习剪枝模型并采用贪心策略确定最优分割点，显著提升了推理速度并降低了能耗。


<details>
  <summary>Details</summary>
Motivation: 传统手动识别方法效率低且成本高，深度学习模型在资源受限设备上运行困难，且云服务器推理受带宽限制，亟需高效解决方案。

Method: 使用深度强化学习剪枝DNN模型，通过贪心策略确定最优分割点，实现边缘与云协同推理，并通过Gradio实现人机交互系统。

Result: 实验表明，该框架显著提高了推理速度，同时保持了可接受的识别精度。

Conclusion: 该协同推理框架为植物病害快速诊断与防治提供了新解决方案。

Abstract: Plant disease is a critical factor affecting agricultural production.
Traditional manual recognition methods face significant drawbacks, including
low accuracy, high costs, and inefficiency. Deep learning techniques have
demonstrated significant benefits in identifying plant diseases, but they still
face challenges such as inference delays and high energy consumption. Deep
learning algorithms are difficult to run on resource-limited embedded devices.
Offloading these models to cloud servers is confronted with the restriction of
communication bandwidth, and all of these factors will influence the
inference's efficiency. We propose a collaborative inference framework for
recognizing plant diseases between edge devices and cloud servers to enhance
inference speed. The DNN model for plant disease recognition is pruned through
deep reinforcement learning to improve the inference speed and reduce energy
consumption. Then the optimal split point is determined by a greedy strategy to
achieve the best collaborated inference acceleration. Finally, the system for
collaborative inference acceleration in plant disease recognition has been
implemented using Gradio to facilitate friendly human-machine interaction.
Experiments indicate that the proposed collaborative inference framework
significantly increases inference speed while maintaining acceptable
recognition accuracy, offering a novel solution for rapidly diagnosing and
preventing plant diseases.

</details>


### [62] [LLM4FTS: Enhancing Large Language Models for Financial Time Series Prediction](https://arxiv.org/abs/2505.02880)
*Zian Liu,Renjun Jia*

Main category: cs.LG

TL;DR: 论文提出了一种名为LLM4FTS的新框架，通过可学习的片段分割和动态小波卷积模块，增强大语言模型（LLMs）在金融时间序列建模中的能力，以解决传统模型在多尺度模式捕捉上的不足。


<details>
  <summary>Details</summary>
Motivation: 金融时间序列预测面临信号噪声比低和复杂时间模式的挑战，传统机器学习模型能力有限，而现有基于LLM的方法因Transformer架构限制，忽略了市场数据的多尺度特征。

Method: 采用K-means++聚类（基于DTW距离）识别市场数据的尺度不变模式，引入自适应片段分割和动态小波卷积模块，以灵活捕捉时频特征。

Result: 在真实金融数据集上的实验表明，该框架能有效捕捉复杂市场模式，并在股票收益预测中达到最优性能。

Conclusion: LLM4FTS框架显著提升了LLM在金融预测中的应用能力，具有实际交易系统的部署价值。

Abstract: Predicting financial time series presents significant challenges due to
inherent low signal-to-noise ratios and intricate temporal patterns.
Traditional machine learning models exhibit limitations in this forecasting
task constrained by their restricted model capacity. Recent advances in large
language models (LLMs), with their greatly expanded parameter spaces,
demonstrate promising potential for modeling complex dependencies in temporal
sequences. However, existing LLM-based approaches typically focus on
fixed-length patch analysis due to the Transformer architecture, ignoring
market data's multi-scale pattern characteristics. In this study, we propose
$LLM4FTS$, a novel framework that enhances LLM capabilities for temporal
sequence modeling through learnable patch segmentation and dynamic wavelet
convolution modules. Specifically,we first employ K-means++ clustering based on
DTW distance to identify scale-invariant patterns in market data. Building upon
pattern recognition results, we introduce adaptive patch segmentation that
partitions temporal sequences while preserving maximal pattern integrity. To
accommodate time-varying frequency characteristics, we devise a dynamic wavelet
convolution module that emulates discrete wavelet transformation with enhanced
flexibility in capturing time-frequency features. These three modules work
together to improve large language model's ability to handle scale-invariant
patterns in financial time series. Extensive experiments on real-world
financial datasets substantiate the framework's efficacy, demonstrating
superior performance in capturing complex market patterns and achieving
state-of-the-art results in stock return prediction. The successful deployment
in practical trading systems confirms its real-world applicability,
representing a significant advancement in LLM applications for financial
forecasting.

</details>


### [63] [Rewriting Pre-Training Data Boosts LLM Performance in Math and Code](https://arxiv.org/abs/2505.02881)
*Kazuki Fujii,Yukito Tajima,Sakae Mizuki,Hinari Shimada,Taihei Shiotani,Koshiro Saito,Masanari Ohi,Masaki Kawamura,Taishi Nakamura,Takumi Okamoto,Shigeki Ishida,Kakeru Hattori,Youmi Ma,Hiroya Takamura,Rio Yokota,Naoaki Okazaki*

Main category: cs.LG

TL;DR: 论文介绍了两个公开数据集SwallowCode和SwallowMath，通过系统重写公共数据显著提升LLM在程序合成和数学推理中的性能。实验表明，这些数据集在固定训练预算下显著提升了模型表现。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）在程序合成和数学推理中的性能受限于预训练数据的质量。现有方法多依赖排除性过滤或有限转换，未能充分利用低质量数据。

Method: SwallowCode通过四阶段流水线（语法验证、风格过滤、两阶段LLM重写）优化Python代码；SwallowMath通过去除冗余、恢复上下文和格式化步骤优化数学问题解答。

Result: 在50B token训练预算下，SwallowCode提升HumanEval pass@1 +17.0，SwallowMath提升GSM8K准确率+12.4。消融实验验证了各阶段贡献。

Conclusion: 提出的数据集和方法显著提升了LLM性能，所有资源公开，推动了可重复研究和领域专用LLM预训练的发展。

Abstract: The performance of large language models (LLMs) in program synthesis and
mathematical reasoning is fundamentally limited by the quality of their
pre-training corpora. We introduce two openly licensed datasets, released under
the Llama 3.3 Community License, that significantly enhance LLM performance by
systematically rewriting public data. SwallowCode (approximately 16.1 billion
tokens) refines Python snippets from The-Stack-v2 through a novel four-stage
pipeline: syntax validation, pylint-based style filtering, and a two-stage LLM
rewriting process that enforces style conformity and transforms snippets into
self-contained, algorithmically efficient examples. Unlike prior methods that
rely on exclusionary filtering or limited transformations, our
transform-and-retain approach upgrades low-quality code, maximizing data
utility. SwallowMath (approximately 2.3 billion tokens) enhances Finemath-4+ by
removing boilerplate, restoring context, and reformatting solutions into
concise, step-by-step explanations. Within a fixed 50 billion token training
budget, continual pre-training of Llama-3.1-8B with SwallowCode boosts pass@1
by +17.0 on HumanEval and +17.7 on HumanEval+ compared to Stack-Edu, surpassing
the baseline model's code generation capabilities. Similarly, substituting
SwallowMath yields +12.4 accuracy on GSM8K and +7.6 on MATH. Ablation studies
confirm that each pipeline stage contributes incrementally, with rewriting
delivering the largest gains. All datasets, prompts, and checkpoints are
publicly available, enabling reproducible research and advancing LLM
pre-training for specialized domains.

</details>


### [64] [Unlearning vs. Obfuscation: Are We Truly Removing Knowledge?](https://arxiv.org/abs/2505.02884)
*Guangzhi Sun,Potsawee Manakul,Xiao Zhan,Mark Gales*

Main category: cs.LG

TL;DR: 论文提出了一种新的遗忘方法DF-MCQ，通过KL散度扁平化模型预测分布，有效移除目标知识，并引入探测框架评估遗忘效果。


<details>
  <summary>Details</summary>
Motivation: 支持数据隐私、法规合规和伦理AI部署，区分遗忘与混淆，提升模型对目标信息的真正移除能力。

Method: 提出DF-MCQ方法，通过KL散度扁平化模型预测分布，结合自动生成的多选题实现知识移除。

Result: DF-MCQ实现了90%以上的拒绝率，探测问题上的不确定性显著高于混淆方法。

Conclusion: DF-MCQ是一种有效的遗忘方法，能够真正移除目标知识并提升模型安全性。

Abstract: Unlearning has emerged as a critical capability for large language models
(LLMs) to support data privacy, regulatory compliance, and ethical AI
deployment. Recent techniques often rely on obfuscation by injecting incorrect
or irrelevant information to suppress knowledge. Such methods effectively
constitute knowledge addition rather than true removal, often leaving models
vulnerable to probing. In this paper, we formally distinguish unlearning from
obfuscation and introduce a probing-based evaluation framework to assess
whether existing approaches genuinely remove targeted information. Moreover, we
propose DF-MCQ, a novel unlearning method that flattens the model predictive
distribution over automatically generated multiple-choice questions using
KL-divergence, effectively removing knowledge about target individuals and
triggering appropriate refusal behaviour. Experimental results demonstrate that
DF-MCQ achieves unlearning with over 90% refusal rate and a random choice-level
uncertainty that is much higher than obfuscation on probing questions.

</details>


### [65] [When Your Own Output Becomes Your Training Data: Noise-to-Meaning Loops and a Formal RSI Trigger](https://arxiv.org/abs/2505.02888)
*Rintaro Ando*

Main category: cs.LG

TL;DR: N2M-RSI是一个形式化模型，展示AI代理在反馈自身输出作为输入并超过信息整合阈值后，内部复杂性会无限增长。


<details>
  <summary>Details</summary>
Motivation: 探索AI代理通过自我反馈实现无限复杂性增长的机制，并统一自我提示、Gödelian自引用和AutoML等概念。

Method: 提出N2M-RSI模型，假设AI代理通过反馈自身输出作为输入，超过信息整合阈值后复杂性增长。

Result: 模型显示AI代理的复杂性会无限增长，并可扩展至交互代理群体，实现超线性效应。

Conclusion: N2M-RSI为理解AI自我改进提供了理论框架，但出于安全考虑未公开具体实现细节。

Abstract: We present Noise-to-Meaning Recursive Self-Improvement (N2M-RSI), a minimal
formal model showing that once an AI agent feeds its own outputs back as inputs
and crosses an explicit information-integration threshold, its internal
complexity will grow without bound under our assumptions. The framework unifies
earlier ideas on self-prompting large language models, G\"odelian
self-reference, and AutoML, yet remains implementation-agnostic. The model
furthermore scales naturally to interacting swarms of agents, hinting at
super-linear effects once communication among instances is permitted. For
safety reasons, we omit system-specific implementation details and release only
a brief, model-agnostic toy prototype in Appendix C.

</details>


### [66] [Early Prediction of Sepsis: Feature-Aligned Transfer Learning](https://arxiv.org/abs/2505.02889)
*Oyindolapo O. Komolafe,Zhimin Mei,David Morales Zarate,Gregory William Spangenberg*

Main category: cs.LG

TL;DR: 提出了一种基于机器学习的系统（FATL），用于早期预测脓毒症，通过特征对齐和迁移学习解决现有模型的不一致性和人口偏差问题。


<details>
  <summary>Details</summary>
Motivation: 脓毒症早期检测对挽救生命至关重要，但现有方法通常在严重损伤后才识别，且模型因特征不一致和人口偏差难以通用。

Method: 采用特征对齐迁移学习（FATL），聚焦多研究中重要且常见的特征，结合多样人群训练的模型知识，提高通用性。

Result: FATL提供了一种一致且临床相关的解决方案，适用于不同医院和人群，尤其资源有限的场景。

Conclusion: FATL有望改善脓毒症早期检测，提升患者预后，降低医疗成本，促进医疗公平。

Abstract: Sepsis is a life threatening medical condition that occurs when the body has
an extreme response to infection, leading to widespread inflammation, organ
failure, and potentially death. Because sepsis can worsen rapidly, early
detection is critical to saving lives. However, current diagnostic methods
often identify sepsis only after significant damage has already occurred. Our
project aims to address this challenge by developing a machine learning based
system to predict sepsis in its early stages, giving healthcare providers more
time to intervene.
  A major problem with existing models is the wide variability in the patient
information or features they use, such as heart rate, temperature, and lab
results. This inconsistency makes models difficult to compare and limits their
ability to work across different hospitals and settings. To solve this, we
propose a method called Feature Aligned Transfer Learning (FATL), which
identifies and focuses on the most important and commonly reported features
across multiple studies, ensuring the model remains consistent and clinically
relevant.
  Most existing models are trained on narrow patient groups, leading to
population bias. FATL addresses this by combining knowledge from models trained
on diverse populations, using a weighted approach that reflects each models
contribution. This makes the system more generalizable and effective across
different patient demographics and clinical environments. FATL offers a
practical and scalable solution for early sepsis detection, particularly in
hospitals with limited resources, and has the potential to improve patient
outcomes, reduce healthcare costs, and support more equitable healthcare
delivery.

</details>


### [67] [RetroInfer: A Vector-Storage Approach for Scalable Long-Context LLM Inference](https://arxiv.org/abs/2505.02922)
*Yaoqi Chen,Jinkai Zhang,Baotong Lu,Qianxi Zhang,Chengruidong Zhang,Jingjia Luo,Di Liu,Huiqiang Jiang,Qi Chen,Jing Liu,Bailu Ding,Xiao Yan,Jiawei Jiang,Chen Chen,Mingxing Zhang,Yuqing Yang,Fan Yang,Mao Yang*

Main category: cs.LG

TL;DR: RetroInfer通过重新设计KV缓存为向量存储系统，利用注意力稀疏性加速长上下文LLM推理，实现了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型（LLM）在长上下文推理中因GPU内存和带宽限制导致的效率问题。

Method: 提出wave index（注意力感知向量索引）和wave buffer（协调KV缓存放置和计算数据传输），结合三部分注意力近似、精度有界注意力估计和分段聚类等技术。

Result: 在长上下文基准测试中，RetroInfer在GPU内存限制内比全注意力快4.5倍，在KV缓存扩展到CPU内存时比稀疏注意力基线快10.5倍，且保持全注意力精度。

Conclusion: RetroInfer在高效性和准确性上均优于现有方法，为长上下文LLM推理提供了实用解决方案。

Abstract: The growing context lengths of large language models (LLMs) pose significant
challenges for efficient inference, primarily due to GPU memory and bandwidth
constraints. We present RetroInfer, a novel system that reconceptualizes the
key-value (KV) cache as a vector storage system which exploits the inherent
attention sparsity to accelerate long-context LLM inference. At its core is the
wave index, an Attention-aWare VEctor index that enables efficient and accurate
retrieval of critical tokens through techniques such as tripartite attention
approximation, accuracy-bounded attention estimation, and segmented clustering.
Complementing this is the wave buffer, which coordinates KV cache placement and
overlaps computation and data transfer across GPU and CPU to sustain high
throughput. Unlike prior sparsity-based methods that struggle with token
selection and hardware coordination, RetroInfer delivers robust performance
without compromising model accuracy. Experiments on long-context benchmarks
show up to 4.5X speedup over full attention within GPU memory limits and up to
10.5X over sparse attention baselines when KV cache is extended to CPU memory,
all while preserving full-attention-level accuracy.

</details>


### [68] [Smooth Quadratic Prediction Markets](https://arxiv.org/abs/2505.02959)
*Enrique Nueve,Bo Waggoner*

Main category: cs.LG

TL;DR: 论文提出了一种新的预测市场设计——平滑二次预测市场，通过分解和修改基于对偶的成本函数市场定价机制，激励代理集体实现最陡梯度下降。相较于DCFMM，新市场在AD证券上具有更优的最坏情况货币损失，同时保留了瞬时价格存在性、信息整合、表达性、无套利和某种形式的激励相容性等公理保证。


<details>
  <summary>Details</summary>
Motivation: 探索是否可以通过其他学习算法（如最陡梯度下降）来设计预测市场，以改进现有基于对偶的成本函数预测市场的局限性。

Method: 通过分解和修改DCFMM的定价机制，提出平滑二次预测市场，并分析其在AD证券上的表现。同时，研究了代理在有限预算和仅限买入证券约束下的交易行为。

Result: 平滑二次预测市场在AD证券上具有更优的最坏情况货币损失，同时保留了多项公理保证。此外，研究还表明价格更新规则可以与费用结构分离，同时保留保证。

Conclusion: 平滑二次预测市场为未来预测市场设计提供了新思路，尤其是在价格更新规则与费用结构分离的情况下仍能保持保证。

Abstract: When agents trade in a Duality-based Cost Function prediction market, they
collectively implement the learning algorithm Follow-The-Regularized-Leader. We
ask whether other learning algorithms could be used to inspire the design of
prediction markets. By decomposing and modifying the Duality-based Cost
Function Market Maker's (DCFMM) pricing mechanism, we propose a new prediction
market, called the Smooth Quadratic Prediction Market, the incentivizes agents
to collectively implement general steepest gradient descent. Relative to the
DCFMM, the Smooth Quadratic Prediction Market has a better worst-case monetary
loss for AD securities while preserving axiom guarantees such as the existence
of instantaneous price, information incorporation, expressiveness, no
arbitrage, and a form of incentive compatibility. To motivate the application
of the Smooth Quadratic Prediction Market, we independently examine agents'
trading behavior under two realistic constraints: bounded budgets and buy-only
securities. Finally, we provide an introductory analysis of an approach to
facilitate adaptive liquidity using the Smooth Quadratic AD Prediction Market.
Our results suggest future designs where the price update rule is separate from
the fee structure, yet guarantees are preserved.

</details>


### [69] [Physics-Learning AI Datamodel (PLAID) datasets: a collection of physics simulations for machine learning](https://arxiv.org/abs/2505.02974)
*Fabien Casenave,Xavier Roynard,Brian Staber,Nissrine Akkari,William Piat,Michele Alessandro Bucci,Abbas Kabalan,Xuan Minh Vuong Nguyen,Luca Saverio,Raphaël Carpintero Perez,Anthony Kalaydjian,Samy Fouché,Thierry Gonon,Ghassan Najjar,Emmanuel Menier,Matthieu Nastorg,Christian Rey*

Main category: cs.LG

TL;DR: PLAID是一个用于物理模拟数据的标准化框架，解决了现有数据集碎片化和局限性问题，并提供了基准测试工具。


<details>
  <summary>Details</summary>
Motivation: 现有物理模拟数据集缺乏多样性、标准化和规模，限制了机器学习代理模型的广泛应用。

Method: 提出了PLAID框架，定义了统一的数据标准，并提供了工具库和六个数据集，涵盖结构力学和计算流体动力学。

Result: 发布了PLAID标准和数据集，并提供了基准测试工具，促进社区参与和评估。

Conclusion: PLAID通过标准化和灵活性推动了物理模拟数据的共享和机器学习应用。

Abstract: Machine learning-based surrogate models have emerged as a powerful tool to
accelerate simulation-driven scientific workflows. However, their widespread
adoption is hindered by the lack of large-scale, diverse, and standardized
datasets tailored to physics-based simulations. While existing initiatives
provide valuable contributions, many are limited in scope-focusing on specific
physics domains, relying on fragmented tooling, or adhering to overly
simplistic datamodels that restrict generalization. To address these
limitations, we introduce PLAID (Physics-Learning AI Datamodel), a flexible and
extensible framework for representing and sharing datasets of physics
simulations. PLAID defines a unified standard for describing simulation data
and is accompanied by a library for creating, reading, and manipulating complex
datasets across a wide range of physical use cases (gitlab.com/drti/plaid). We
release six carefully crafted datasets under the PLAID standard, covering
structural mechanics and computational fluid dynamics, and provide baseline
benchmarks using representative learning methods. Benchmarking tools are made
available on Hugging Face, enabling direct participation by the community and
contribution to ongoing evaluation efforts (huggingface.co/PLAIDcompetitions).

</details>


### [70] [More Optimal Fractional-Order Stochastic Gradient Descent for Non-Convex Optimization Problems](https://arxiv.org/abs/2505.02985)
*Mohammad Partohaghighi,Roummel Marcia,YangQuan Chen*

Main category: cs.LG

TL;DR: 2SEDFOSGD结合双尺度有效维度算法与分数阶随机梯度下降，通过数据驱动方式调整分数阶指数，提升优化稳定性和收敛速度。


<details>
  <summary>Details</summary>
Motivation: 传统分数阶随机梯度下降（FOSGD）在优化中难以调参且不稳定，限制了其实际应用。

Method: 提出2SEDFOSGD，利用双尺度有效维度（2SED）算法动态调整分数阶指数，减少振荡并加速收敛。

Result: 在非凸优化问题中，2SEDFOSGD在收敛速度和参数估计鲁棒性上优于基线方法。

Conclusion: 2SEDFOSGD展示了维度感知分数阶技术在高级建模和估计任务中的潜力。

Abstract: Fractional-order stochastic gradient descent (FOSGD) leverages fractional
exponents to capture long-memory effects in optimization. However, its utility
is often limited by the difficulty of tuning and stabilizing these exponents.
We propose 2SED Fractional-Order Stochastic Gradient Descent (2SEDFOSGD), which
integrates the Two-Scale Effective Dimension (2SED) algorithm with FOSGD to
adapt the fractional exponent in a data-driven manner. By tracking model
sensitivity and effective dimensionality, 2SEDFOSGD dynamically modulates the
exponent to mitigate oscillations and hasten convergence. Theoretically, for
onoconvex optimization problems, this approach preserves the advantages of
fractional memory without the sluggish or unstable behavior observed in na\"ive
fractional SGD. Empirical evaluations in Gaussian and $\alpha$-stable noise
scenarios using an autoregressive (AR) model highlight faster convergence and
more robust parameter estimates compared to baseline methods, underscoring the
potential of dimension-aware fractional techniques for advanced modeling and
estimation tasks.

</details>


### [71] [Radio: Rate-Distortion Optimization for Large Language Model Compression](https://arxiv.org/abs/2505.03031)
*Sean I. Young*

Main category: cs.LG

TL;DR: 本文提出了一种基于率失真理论的量化技术，用于压缩大型语言模型（LLMs），支持用户根据需求灵活调整模型大小或精度。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs在资源受限设备上的部署问题，降低计算成本并减少大规模AI基础设施对环境的影响。

Method: 从率失真理论角度建立LLM量化的基础，并提出一种基于简单率失真优化的量化技术。

Result: 该技术可扩展到包含数千亿权重参数的模型，并支持用户训练后按需压缩模型。

Conclusion: 提出的量化技术为LLM的高效部署提供了灵活且可扩展的解决方案。

Abstract: In recent years, the compression of large language models (LLMs) has emerged
as a key problem in facilitating LLM deployment on resource-limited devices,
reducing compute costs, and mitigating the environmental footprint due to
large-scale AI infrastructure. Here, we establish the foundations of LLM
quantization from a rate-distortion theory perspective and propose a
quantization technique based on simple rate-distortion optimization. Our
technique scales to models containing hundreds of billions of weight parameters
and offers users the flexibility to compress models, post-training, to a model
size or accuracy specified by the user.

</details>


### [72] [A New Perspective To Understanding Multi-resolution Hash Encoding For Neural Fields](https://arxiv.org/abs/2505.03042)
*Steven Tin Sui Luo*

Main category: cs.LG

TL;DR: Instant-NGP的多分辨率哈希网格结构显著提升了神经网络的性能，但其原理尚不明确。本文提出“域操作”视角，解释哈希网格如何通过人为增加线性片段提升表达能力，并通过实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: Instant-NGP的哈希网格结构虽被广泛使用和改进，但其工作原理缺乏理论支持，导致超参数只能通过经验调整。本文旨在提供一种直观的解释框架。

Method: 提出“域操作”视角，通过实验验证哈希网格如何通过增加线性片段提升神经场的表达能力。实验主要基于1维信号，但可推广到高维。

Result: 实验表明，哈希网格通过人为增加线性片段显著提升了神经网络的表达能力，支持了“域操作”视角的解释。

Conclusion: 本文的“域操作”视角为哈希网格的工作原理提供了理论支持，并展示了其在1维和高维信号中的有效性。

Abstract: Instant-NGP has been the state-of-the-art architecture of neural fields in
recent years. Its incredible signal-fitting capabilities are generally
attributed to its multi-resolution hash grid structure and have been used and
improved in numerous following works. However, it is unclear how and why such a
hash grid structure improves the capabilities of a neural network by such great
margins. A lack of principled understanding of the hash grid also implies that
the large set of hyperparameters accompanying Instant-NGP could only be tuned
empirically without much heuristics. To provide an intuitive explanation of the
working principle of the hash grid, we propose a novel perspective, namely
domain manipulation. This perspective provides a ground-up explanation of how
the feature grid learns the target signal and increases the expressivity of the
neural field by artificially creating multiples of pre-existing linear
segments. We conducted numerous experiments on carefully constructed
1-dimensional signals to support our claims empirically and aid our
illustrations. While our analysis mainly focuses on 1-dimensional signals, we
show that the idea is generalizable to higher dimensions.

</details>


### [73] [34 Examples of LLM Applications in Materials Science and Chemistry: Towards Automation, Assistants, Agents, and Accelerated Scientific Discovery](https://arxiv.org/abs/2505.03049)
*Yoel Zimmermann,Adib Bazgir,Alexander Al-Feghali,Mehrad Ansari,L. Catherine Brinson,Yuan Chiang,Defne Circi,Min-Hsueh Chiu,Nathan Daelman,Matthew L. Evans,Abhijeet S. Gangan,Janine George,Hassan Harb,Ghazal Khalighinejad,Sartaaj Takrim Khan,Sascha Klawohn,Magdalena Lederbauer,Soroush Mahjoubi,Bernadette Mohr,Seyed Mohamad Moosavi,Aakash Naik,Aleyna Beste Ozhan,Dieter Plessers,Aritra Roy,Fabian Schöppach,Philippe Schwaller,Carla Terboven,Katharina Ueltzen,Shang Zhu,Jan Janssen,Calvin Li,Ian Foster,Ben Blaiszik*

Main category: cs.LG

TL;DR: 大语言模型（LLMs）正在改变材料科学与化学研究的多个领域，涵盖分子性质预测、材料设计、科学自动化等。34个项目展示了LLMs在七个关键研究领域的应用，凸显其作为多功能工具和快速原型平台的潜力。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在材料科学与化学研究全生命周期中的应用潜力，以推动科学工作流程的革新。

Method: 通过第二届年度LLM Hackathon的34个项目，分析LLMs在七个研究领域的实际应用。

Result: LLMs在低数据环境和跨学科研究中表现出色，通过推理、额外训练数据和新技术的改进提升了性能。

Conclusion: LLMs的持续改进为科学工作流程带来新机遇与挑战，需进一步研究以解决可靠性、可解释性和可重复性问题。

Abstract: Large Language Models (LLMs) are reshaping many aspects of materials science
and chemistry research, enabling advances in molecular property prediction,
materials design, scientific automation, knowledge extraction, and more. Recent
developments demonstrate that the latest class of models are able to integrate
structured and unstructured data, assist in hypothesis generation, and
streamline research workflows. To explore the frontier of LLM capabilities
across the research lifecycle, we review applications of LLMs through 34 total
projects developed during the second annual Large Language Model Hackathon for
Applications in Materials Science and Chemistry, a global hybrid event. These
projects spanned seven key research areas: (1) molecular and material property
prediction, (2) molecular and material design, (3) automation and novel
interfaces, (4) scientific communication and education, (5) research data
management and automation, (6) hypothesis generation and evaluation, and (7)
knowledge extraction and reasoning from the scientific literature.
Collectively, these applications illustrate how LLMs serve as versatile
predictive models, platforms for rapid prototyping of domain-specific tools,
and much more. In particular, improvements in both open source and proprietary
LLM performance through the addition of reasoning, additional training data,
and new techniques have expanded effectiveness, particularly in low-data
environments and interdisciplinary research. As LLMs continue to improve, their
integration into scientific workflows presents both new opportunities and new
challenges, requiring ongoing exploration, continued refinement, and further
research to address reliability, interpretability, and reproducibility.

</details>


### [74] [Adversarial Attacks in Multimodal Systems: A Practitioner's Survey](https://arxiv.org/abs/2505.03084)
*Shashank Kapoor,Sanjay Surendranath Girija,Lakshit Arora,Dipen Pradhan,Ankit Shetgaonkar,Aman Raj*

Main category: cs.LG

TL;DR: 论文综述了多模态模型中的对抗攻击类型，填补了多模态领域对抗威胁研究的空白。


<details>
  <summary>Details</summary>
Motivation: 多模态模型的普及带来了对抗攻击的放大威胁，但缺乏针对实践者的攻击类型概述。

Method: 通过调查针对文本、图像、视频和音频四种模态的对抗攻击，总结威胁演变。

Result: 提供了多模态对抗攻击的全面综述，首次系统化总结了威胁情况。

Conclusion: 论文填补了多模态对抗攻击研究的空白，为实践者提供了威胁视图和预防措施参考。

Abstract: The introduction of multimodal models is a huge step forward in Artificial
Intelligence. A single model is trained to understand multiple modalities:
text, image, video, and audio. Open-source multimodal models have made these
breakthroughs more accessible. However, considering the vast landscape of
adversarial attacks across these modalities, these models also inherit
vulnerabilities of all the modalities, and ultimately, the adversarial threat
amplifies. While broad research is available on possible attacks within or
across these modalities, a practitioner-focused view that outlines attack types
remains absent in the multimodal world. As more Machine Learning Practitioners
adopt, fine-tune, and deploy open-source models in real-world applications,
it's crucial that they can view the threat landscape and take the preventive
actions necessary. This paper addresses the gap by surveying adversarial
attacks targeting all four modalities: text, image, video, and audio. This
survey provides a view of the adversarial attack landscape and presents how
multimodal adversarial threats have evolved. To the best of our knowledge, this
survey is the first comprehensive summarization of the threat landscape in the
multimodal world.

</details>


### [75] [Deep Learning in Renewable Energy Forecasting: A Cross-Dataset Evaluation of Temporal and Spatial Models](https://arxiv.org/abs/2505.03109)
*Lutfu Sua,Haibo Wang,Jun Huang*

Main category: cs.LG

TL;DR: 本文探讨了深度学习（DL）模型在可再生能源领域的应用，比较了多种DL方法，并分析了影响其准确性的因素。LSTM和MLP模型表现最佳。


<details>
  <summary>Details</summary>
Motivation: 可再生能源数据的非线性关系需要更强大的建模方法，传统机器学习难以捕捉复杂交互，因此研究DL模型的适用性和优化方法。

Method: 研究比较了七种DL方法（如LSTM、CNN等），并评估了采样、平稳性、超参数优化等因素。使用了天气和发电数据以及光伏面板数据。

Result: LSTM和MLP模型表现最优，验证数据的均方根误差极低。

Conclusion: DL模型在可再生能源领域具有潜力，LSTM和MLP是有效的建模工具，需进一步优化以减少过拟合。

Abstract: Unpredictability of renewable energy sources coupled with the complexity of
those methods used for various purposes in this area calls for the development
of robust methods such as DL models within the renewable energy domain. Given
the nonlinear relationships among variables in renewable energy datasets, DL
models are preferred over traditional machine learning (ML) models because they
can effectively capture and model complex interactions between variables. This
research aims to identify the factors responsible for the accuracy of DL
techniques, such as sampling, stationarity, linearity, and hyperparameter
optimization for different algorithms. The proposed DL framework compares
various methods and alternative training/test ratios. Seven ML methods, such as
Long-Short Term Memory (LSTM), Stacked LSTM, Convolutional Neural Network
(CNN), CNN-LSTM, Deep Neural Network (DNN), Multilayer Perceptron (MLP), and
Encoder-Decoder (ED), were evaluated on two different datasets. The first
dataset contains the weather and power generation data. It encompasses two
distinct datasets, hourly energy demand data and hourly weather data in Spain,
while the second dataset includes power output generated by the photovoltaic
panels at 12 locations. This study deploys regularization approaches, including
early stopping, neuron dropping, and L2 regularization, to reduce the
overfitting problem associated with DL models. The LSTM and MLP models show
superior performance. Their validation data exhibit exceptionally low root mean
square error values.

</details>


### [76] [Plug-and-Play AMC: Context Is King in Training-Free, Open-Set Modulation with LLMs](https://arxiv.org/abs/2505.03112)
*Mohammad Rostami,Atik Faysal,Reihaneh Gh. Roshan,Huaxia Wang,Nikhil Muralidhar,Yu-Dong Yao*

Main category: cs.LG

TL;DR: 提出了一种结合传统信号处理技术与大语言模型（LLM）的创新框架，用于自动调制分类（AMC），无需额外训练或预处理即可实现高效分类。


<details>
  <summary>Details</summary>
Motivation: AMC在频谱管理和无线通信中至关重要，但由于信号干扰和噪声的复杂性，其实现仍具挑战性。

Method: 利用高阶统计和累积量估计将信号特征转化为结构化自然语言提示，并结合LLM的上下文理解能力实现一次性分类。

Result: 在合成数据集上的实验表明，该方法在不同调制方案和信噪比（SNR）下均表现优异。

Conclusion: 该方法为无线通信中的鲁棒基础模型提供了新思路，显著降低了开发特定信道模型的成本，并为下一代无线网络的可扩展、可解释信号分类系统奠定了基础。

Abstract: Automatic Modulation Classification (AMC) is critical for efficient spectrum
management and robust wireless communications. However, AMC remains challenging
due to the complex interplay of signal interference and noise. In this work, we
propose an innovative framework that integrates traditional signal processing
techniques with Large-Language Models (LLMs) to address AMC. Our approach
leverages higher-order statistics and cumulant estimation to convert
quantitative signal features into structured natural language prompts. By
incorporating exemplar contexts into these prompts, our method exploits the
LLM's inherent familiarity with classical signal processing, enabling effective
one-shot classification without additional training or preprocessing (e.g.,
denoising). Experimental evaluations on synthetically generated datasets,
spanning both noiseless and noisy conditions, demonstrate that our framework
achieves competitive performance across diverse modulation schemes and
Signal-to-Noise Ratios (SNRs). Moreover, our approach paves the way for robust
foundation models in wireless communications across varying channel conditions,
significantly reducing the expense associated with developing channel-specific
models. This work lays the foundation for scalable, interpretable, and
versatile signal classification systems in next-generation wireless networks.
The source code is available at https://github.com/RU-SIT/context-is-king

</details>


### [77] [Adaptive Thresholding for Multi-Label Classification via Global-Local Signal Fusion](https://arxiv.org/abs/2505.03118)
*Dmytro Shamatrin*

Main category: cs.LG

TL;DR: 论文提出了一种自适应阈值机制，用于多标签分类（MLC），结合全局和局部信号生成动态阈值，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 传统多标签分类方法使用固定阈值或独立处理标签，忽略了上下文和全局稀有性，导致性能受限。

Method: 提出了一种融合全局（基于IDF）和局部（基于KNN）信号的自适应阈值机制，并将其作为可微分惩罚项融入损失函数。

Result: 在AmazonCat-13K基准测试中，宏F1达到0.1712，显著优于基于树和预训练Transformer的方法。

Conclusion: 该方法轻量、可解释且模块化，为多标签分类提供了新的解决方案，并开源代码以促进未来研究。

Abstract: Multi-label classification (MLC) requires predicting multiple labels per
sample, often under heavy class imbalance and noisy conditions. Traditional
approaches apply fixed thresholds or treat labels independently, overlooking
context and global rarity. We introduce an adaptive thresholding mechanism that
fuses global (IDF-based) and local (KNN-based) signals to produce per-label,
per-instance thresholds. Instead of applying these as hard cutoffs, we treat
them as differentiable penalties in the loss, providing smooth supervision and
better calibration. Our architecture is lightweight, interpretable, and highly
modular. On the AmazonCat-13K benchmark, it achieves a macro-F1 of 0.1712,
substantially outperforming tree-based and pretrained transformer-based
methods. We release full code for reproducibility and future extensions.

</details>


### [78] [Rethinking the Global Convergence of Softmax Policy Gradient with Linear Function Approximation](https://arxiv.org/abs/2505.03155)
*Max Qiushi Lin,Jincheng Mei,Matin Aghaei,Michael Lu,Bo Dai,Alekh Agarwal,Dale Schuurmans,Csaba Szepesvari,Sharan Vaswani*

Main category: cs.LG

TL;DR: 论文研究了线性函数近似的Softmax策略梯度方法（Lin-SPG），发现近似误差不影响其全局收敛性，并提出了特征表示的必要和充分条件。


<details>
  <summary>Details</summary>
Motivation: 研究策略梯度方法在大状态-动作空间中的全局收敛性，特别是近似误差对收敛的影响。

Method: 聚焦于线性函数近似的Softmax策略梯度方法（Lin-SPG），分析其特征表示条件。

Result: 证明了在特定学习率下，Lin-SPG能以O(1/T)收敛到最优策略，且任意恒定学习率下也能保证全局收敛。

Conclusion: Lin-SPG的全局收敛性不依赖近似误差，特征表示条件是关键。

Abstract: Policy gradient (PG) methods have played an essential role in the empirical
successes of reinforcement learning. In order to handle large state-action
spaces, PG methods are typically used with function approximation. In this
setting, the approximation error in modeling problem-dependent quantities is a
key notion for characterizing the global convergence of PG methods. We focus on
Softmax PG with linear function approximation (referred to as
$\texttt{Lin-SPG}$) and demonstrate that the approximation error is irrelevant
to the algorithm's global convergence even for the stochastic bandit setting.
Consequently, we first identify the necessary and sufficient conditions on the
feature representation that can guarantee the asymptotic global convergence of
$\texttt{Lin-SPG}$. Under these feature conditions, we prove that $T$
iterations of $\texttt{Lin-SPG}$ with a problem-specific learning rate result
in an $O(1/T)$ convergence to the optimal policy. Furthermore, we prove that
$\texttt{Lin-SPG}$ with any arbitrary constant learning rate can ensure
asymptotic global convergence to the optimal policy.

</details>


### [79] [Improving the Reproducibility of Deep Learning Software: An Initial Investigation through a Case Study Analysis](https://arxiv.org/abs/2505.03165)
*Nikita Ravi,Abhinav Goel,James C. Davis,George K. Thiruvathukal*

Main category: cs.LG

TL;DR: 本文探讨了深度学习模型的可复现性问题，并提出了一套系统性方法来分析和改进可复现性，通过案例研究展示了相关指南。


<details>
  <summary>Details</summary>
Motivation: 深度学习领域虽取得重大突破，但结果难以复现的问题日益突出，影响了研究的可靠性和有效性。Nature杂志的研究显示，超过70%的研究者无法复现他人实验，50%无法复现自己的实验。

Method: 提出了一套系统性指南，包括复制原始软件环境、实现端到端训练和测试算法、公开架构设计、增强数据处理和训练管道的透明度，并进行敏感性分析。

Result: 通过实施这些策略，能够显著提升深度学习模型的可复现性，缩小研究与实际应用之间的差距。

Conclusion: 本文的指南和案例研究为改进深度学习模型的可复现性提供了实用方法，有助于推动研究成果的有效部署。

Abstract: The field of deep learning has witnessed significant breakthroughs, spanning
various applications, and fundamentally transforming current software
capabilities. However, alongside these advancements, there have been increasing
concerns about reproducing the results of these deep learning methods. This is
significant because reproducibility is the foundation of reliability and
validity in software development, particularly in the rapidly evolving domain
of deep learning. The difficulty of reproducibility may arise due to several
reasons, including having differences from the original execution environment,
incompatible software libraries, proprietary data and source code, lack of
transparency, and the stochastic nature in some software. A study conducted by
the Nature journal reveals that more than 70% of researchers failed to
reproduce other researchers experiments and over 50% failed to reproduce their
own experiments. Irreproducibility of deep learning poses significant
challenges for researchers and practitioners. To address these concerns, this
paper presents a systematic approach at analyzing and improving the
reproducibility of deep learning models by demonstrating these guidelines using
a case study. We illustrate the patterns and anti-patterns involved with these
guidelines for improving the reproducibility of deep learning models. These
guidelines encompass establishing a methodology to replicate the original
software environment, implementing end-to-end training and testing algorithms,
disclosing architectural designs, and enhancing transparency in data processing
and training pipelines. We also conduct a sensitivity analysis to understand
the model performance across diverse conditions. By implementing these
strategies, we aim to bridge the gap between research and practice, so that
innovations in deep learning can be effectively reproduced and deployed within
software.

</details>


### [80] [Null Counterfactual Factor Interactions for Goal-Conditioned Reinforcement Learning](https://arxiv.org/abs/2505.03172)
*Caleb Chuck,Fan Feng,Carl Qi,Chang Shi,Siddhant Agarwal,Amy Zhang,Scott Niekum*

Main category: cs.LG

TL;DR: 论文提出了一种结合交互的后见重标记方法（HInt），通过定义基于零反事实的交互（NCII），显著提升了目标条件强化学习在物体中心领域的样本效率。


<details>
  <summary>Details</summary>
Motivation: 在物体中心领域，传统的后见重标记方法因奖励无效轨迹而影响学习效果。论文旨在通过引入交互概念改进这一问题。

Method: 提出HInt方法，结合交互与后见重标记；定义基于零反事实的交互（NCII），通过“归零”操作推断交互。

Result: NCII在简单线性动力学和机器人领域（如Robosuite等）中显著提升了交互推断准确性；HInt将样本效率提升至多4倍。

Conclusion: HInt和NCII有效解决了物体中心领域中的后见重标记问题，显著提升了目标条件强化学习的性能。

Abstract: Hindsight relabeling is a powerful tool for overcoming sparsity in
goal-conditioned reinforcement learning (GCRL), especially in certain domains
such as navigation and locomotion. However, hindsight relabeling can struggle
in object-centric domains. For example, suppose that the goal space consists of
a robotic arm pushing a particular target block to a goal location. In this
case, hindsight relabeling will give high rewards to any trajectory that does
not interact with the block. However, these behaviors are only useful when the
object is already at the goal -- an extremely rare case in practice. A dataset
dominated by these kinds of trajectories can complicate learning and lead to
failures. In object-centric domains, one key intuition is that meaningful
trajectories are often characterized by object-object interactions such as
pushing the block with the gripper. To leverage this intuition, we introduce
Hindsight Relabeling using Interactions (HInt), which combines interactions
with hindsight relabeling to improve the sample efficiency of downstream RL.
However because interactions do not have a consensus statistical definition
tractable for downstream GCRL, we propose a definition of interactions based on
the concept of null counterfactual: a cause object is interacting with a target
object if, in a world where the cause object did not exist, the target object
would have different transition dynamics. We leverage this definition to infer
interactions in Null Counterfactual Interaction Inference (NCII), which uses a
"nulling'' operation with a learned model to infer interactions. NCII is able
to achieve significantly improved interaction inference accuracy in both simple
linear dynamics domains and dynamic robotic domains in Robosuite, Robot Air
Hockey, and Franka Kitchen and HInt improves sample efficiency by up to 4x.

</details>


### [81] [RADE: Learning Risk-Adjustable Driving Environment via Multi-Agent Conditional Diffusion](https://arxiv.org/abs/2505.03178)
*Jiawei Wang,Xintao Yan,Yao Mu,Haowei Sun,Zhong Cao,Henry X. Liu*

Main category: cs.LG

TL;DR: RADE是一个基于多智能体扩散架构的仿真框架，用于生成统计上真实且风险可调的交通场景，直接学习风险条件下的行为，保持自然的多智能体交互。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常通过设计复杂目标操纵单一车辆轨迹，牺牲了真实性和可扩展性，RADE旨在解决这一问题。

Method: RADE采用多智能体扩散架构，联合建模环境中所有智能体的行为，并通过代理风险度量条件化轨迹，同时引入动态检查模块确保物理合理性。

Result: 在真实数据集上验证，RADE在不同风险水平下保持统计真实性，并随着风险增加自然提高安全关键事件的可能性。

Conclusion: RADE作为一种可扩展且真实的工具，具有用于自动驾驶安全评估的潜力。

Abstract: Generating safety-critical scenarios in high-fidelity simulations offers a
promising and cost-effective approach for efficient testing of autonomous
vehicles. Existing methods typically rely on manipulating a single vehicle's
trajectory through sophisticated designed objectives to induce adversarial
interactions, often at the cost of realism and scalability. In this work, we
propose the Risk-Adjustable Driving Environment (RADE), a simulation framework
that generates statistically realistic and risk-adjustable traffic scenes.
Built upon a multi-agent diffusion architecture, RADE jointly models the
behavior of all agents in the environment and conditions their trajectories on
a surrogate risk measure. Unlike traditional adversarial methods, RADE learns
risk-conditioned behaviors directly from data, preserving naturalistic
multi-agent interactions with controllable risk levels. To ensure physical
plausibility, we incorporate a tokenized dynamics check module that efficiently
filters generated trajectories using a motion vocabulary. We validate RADE on
the real-world rounD dataset, demonstrating that it preserves statistical
realism across varying risk levels and naturally increases the likelihood of
safety-critical events as the desired risk level grows up. Our results
highlight RADE's potential as a scalable and realistic tool for AV safety
evaluation.

</details>


### [82] [VLM Q-Learning: Aligning Vision-Language Models for Interactive Decision-Making](https://arxiv.org/abs/2505.03181)
*Jake Grigsby,Yuke Zhu,Michael Ryoo,Juan Carlos Niebles*

Main category: cs.LG

TL;DR: 论文探讨了如何通过离线到在线强化学习（RL）方法提升视觉语言模型（VLMs）在代理任务中的表现，弥补其与大型语言模型（LLMs）的差距。


<details>
  <summary>Details</summary>
Motivation: 当前VLMs在代理任务中表现不如LLMs，尤其是在严格输出语法要求和开放问答任务上。研究旨在通过RL方法优化VLMs，使其能够从失败决策中学习并自我改进。

Method: 采用离线到在线RL方法，结合监督微调（SFT），使VLMs能够从低质量数据集中学习并自我提升。

Result: 在三个多模态代理领域中验证了该方法，使用两个开源VLMs展示了其有效性。

Conclusion: 通过RL方法，VLMs可以在代理任务中实现自我改进，弥补与LLMs的差距。

Abstract: Recent research looks to harness the general knowledge and reasoning of large
language models (LLMs) into agents that accomplish user-specified goals in
interactive environments. Vision-language models (VLMs) extend LLMs to
multi-modal data and provide agents with the visual reasoning necessary for new
applications in areas such as computer automation. However, agent tasks
emphasize skills where accessible open-weight VLMs lag behind their LLM
equivalents. For example, VLMs are less capable of following an environment's
strict output syntax requirements and are more focused on open-ended question
answering. Overcoming these limitations requires supervised fine-tuning (SFT)
on task-specific expert demonstrations. Our work approaches these challenges
from an offline-to-online reinforcement learning (RL) perspective. RL lets us
fine-tune VLMs to agent tasks while learning from the unsuccessful decisions of
our own model or more capable (larger) models. We explore an off-policy RL
solution that retains the stability and simplicity of the widely used SFT
workflow while allowing our agent to self-improve and learn from low-quality
datasets. We demonstrate this technique with two open-weight VLMs across three
multi-modal agent domains.

</details>


### [83] [Convergence Of Consistency Model With Multistep Sampling Under General Data Assumptions](https://arxiv.org/abs/2505.03194)
*Yiding Chen,Yiyi Zhang,Owen Oertell,Wen Sun*

Main category: cs.LG

TL;DR: 一致性模型通过直接学习从噪声到数据的映射，实现快速一步生成和多步采样以提高样本质量。本文分析了在训练分布下自一致性近似成立时模型的收敛性，证明了在温和数据假设下，生成样本与目标分布接近。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在数据生成任务中表现出色，但其迭代采样过程计算成本高。一致性模型旨在通过直接学习一致性函数，实现高效生成。

Method: 研究一致性模型在自一致性近似成立时的收敛性，分析不同数据假设下的生成样本与目标分布的接近程度。

Result: 当目标分布有界或尾部快速衰减时，生成样本在Wasserstein距离下接近目标分布；在平滑性假设下，通过额外扰动步骤，生成样本在总变差距离下接近目标分布。

Conclusion: 一致性模型在多种数据假设下均能有效生成接近目标分布的样本，多步采样进一步提升了样本质量。

Abstract: Diffusion models accomplish remarkable success in data generation tasks
across various domains. However, the iterative sampling process is
computationally expensive. Consistency models are proposed to learn consistency
functions to map from noise to data directly, which allows one-step fast data
generation and multistep sampling to improve sample quality. In this paper, we
study the convergence of consistency models when the self-consistency property
holds approximately under the training distribution. Our analysis requires only
mild data assumption and applies to a family of forward processes. When the
target data distribution has bounded support or has tails that decay
sufficiently fast, we show that the samples generated by the consistency model
are close to the target distribution in Wasserstein distance; when the target
distribution satisfies some smoothness assumption, we show that with an
additional perturbation step for smoothing, the generated samples are close to
the target distribution in total variation distance. We provide two case
studies with commonly chosen forward processes to demonstrate the benefit of
multistep sampling.

</details>


### [84] [Transformers for Learning on Noisy and Task-Level Manifolds: Approximation and Generalization Insights](https://arxiv.org/abs/2505.03205)
*Zhaiming Shen,Alex Havrilla,Rongjie Lai,Alexander Cloninger,Wenjing Liao*

Main category: cs.LG

TL;DR: 论文分析了Transformer在噪声输入数据下的回归任务性能，证明了其能够利用低维结构，即使输入数据受到高维噪声干扰。


<details>
  <summary>Details</summary>
Motivation: 现实世界数据和任务通常具有低维结构，但Transformer的理论基础尚未充分探索，尤其是在噪声数据下的性能。

Method: 通过分析输入数据在流形管状邻域中的情况，研究Transformer对噪声数据的回归性能，提出新的证明技术。

Result: 证明了Transformer的近似和泛化误差依赖于流形的内在维度，能够有效利用低维结构。

Conclusion: Transformer在噪声数据下仍能高效学习低维结构，其证明技术对理解其能力具有独立意义。

Abstract: Transformers serve as the foundational architecture for large language and
video generation models, such as GPT, BERT, SORA and their successors.
Empirical studies have demonstrated that real-world data and learning tasks
exhibit low-dimensional structures, along with some noise or measurement error.
The performance of transformers tends to depend on the intrinsic dimension of
the data/tasks, though theoretical understandings remain largely unexplored for
transformers. This work establishes a theoretical foundation by analyzing the
performance of transformers for regression tasks involving noisy input data on
a manifold. Specifically, the input data are in a tubular neighborhood of a
manifold, while the ground truth function depends on the projection of the
noisy data onto the manifold. We prove approximation and generalization errors
which crucially depend on the intrinsic dimension of the manifold. Our results
demonstrate that transformers can leverage low-complexity structures in
learning task even when the input data are perturbed by high-dimensional noise.
Our novel proof technique constructs representations of basic arithmetic
operations by transformers, which may hold independent interest.

</details>


### [85] [Partial Label Clustering](https://arxiv.org/abs/2505.03207)
*Yutong Xie,Fuchao Yang,Yuheng Jia*

Main category: cs.LG

TL;DR: 本文首次研究了部分标签聚类问题，通过利用有限的可用部分标签提升聚类性能。方法包括构建权重矩阵、标签消歧、约束传播，并整合为联合模型。实验表明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 部分标签学习（PLL）是一种弱监督学习框架，每个训练样本对应一组候选标签，但仅有一个是真实标签。本文首次探索如何利用这些部分标签提升聚类性能。

Method: 1. 基于特征空间关系构建权重矩阵；2. 通过权重矩阵消歧候选标签以估计真实标签；3. 基于消歧结果构建必须链接和禁止链接约束；4. 通过对抗性先验促进的双图学习传播约束；5. 整合权重矩阵、标签消歧和约束传播为联合模型。

Result: 实验表明，该方法在有限标注样本下优于现有约束聚类方法，且优于PLL和半监督PLL方法。

Conclusion: 本文提出的联合模型通过整合标签消歧和约束传播，显著提升了聚类性能，理论证明消歧标签矩阵有助于改善聚类效果。

Abstract: Partial label learning (PLL) is a significant weakly supervised learning
framework, where each training example corresponds to a set of candidate labels
and only one label is the ground-truth label. For the first time, this paper
investigates the partial label clustering problem, which takes advantage of the
limited available partial labels to improve the clustering performance.
Specifically, we first construct a weight matrix of examples based on their
relationships in the feature space and disambiguate the candidate labels to
estimate the ground-truth label based on the weight matrix. Then, we construct
a set of must-link and cannot-link constraints based on the disambiguation
results. Moreover, we propagate the initial must-link and cannot-link
constraints based on an adversarial prior promoted dual-graph learning
approach. Finally, we integrate weight matrix construction, label
disambiguation, and pairwise constraints propagation into a joint model to
achieve mutual enhancement. We also theoretically prove that a better
disambiguated label matrix can help improve clustering performance.
Comprehensive experiments demonstrate our method realizes superior performance
when comparing with state-of-the-art constrained clustering methods, and
outperforms PLL and semi-supervised PLL methods when only limited samples are
annotated. The code is publicly available at https://github.com/xyt-ml/PLC.

</details>


### [86] [DYSTIL: Dynamic Strategy Induction with Large Language Models for Reinforcement Learning](https://arxiv.org/abs/2505.03209)
*Borui Wang,Kathleen McKeown,Rex Ying*

Main category: cs.LG

TL;DR: DYSTIL是一种结合大型语言模型（LLM）的策略强化学习框架，通过动态生成文本策略并优化策略，显著提升了泛化能力和样本效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于专家演示的强化学习方法存在泛化能力差、样本效率低和模型可解释性不足的问题。

Method: DYSTIL动态查询LLM生成文本策略，基于优势估计和专家演示，并通过策略优化逐步内化策略。

Result: 在Minigrid和BabyAI环境中，DYSTIL平均成功率比基线方法高17.75%，样本效率更高。

Conclusion: DYSTIL通过结合LLM的策略生成能力，显著提升了强化学习的性能和可解释性。

Abstract: Reinforcement learning from expert demonstrations has long remained a
challenging research problem, and existing state-of-the-art methods using
behavioral cloning plus further RL training often suffer from poor
generalization, low sample efficiency, and poor model interpretability.
Inspired by the strong reasoning abilities of large language models (LLMs), we
propose a novel strategy-based reinforcement learning framework integrated with
LLMs called DYnamic STrategy Induction with Llms for reinforcement learning
(DYSTIL) to overcome these limitations. DYSTIL dynamically queries a
strategy-generating LLM to induce textual strategies based on advantage
estimations and expert demonstrations, and gradually internalizes induced
strategies into the RL agent through policy optimization to improve its
performance through boosting policy generalization and enhancing sample
efficiency. It also provides a direct textual channel to observe and interpret
the evolution of the policy's underlying strategies during training. We test
DYSTIL over challenging RL environments from Minigrid and BabyAI, and
empirically demonstrate that DYSTIL significantly outperforms state-of-the-art
baseline methods by 17.75% in average success rate while also enjoying higher
sample efficiency during the learning process.

</details>


### [87] [Joint Resource Management for Energy-efficient UAV-assisted SWIPT-MEC: A Deep Reinforcement Learning Approach](https://arxiv.org/abs/2505.03230)
*Yue Chen,Hui Kang,Jiahui Li,Geng Su,Boxiong Wang,Jiacheng Wang,Cong Liang,Shuang Liang,Dusit Niyato*

Main category: cs.LG

TL;DR: 论文提出了一种基于无人机辅助的边缘计算系统，结合定向天线技术，为6G物联网终端提供计算资源和能量支持，并通过改进的SAC算法优化能量效率和终端电池可持续性。


<details>
  <summary>Details</summary>
Motivation: 解决6G物联网在无地面基础设施的偏远和灾害场景中，同时进行无线信息和能量传输的挑战。

Method: 提出无人机辅助的边缘计算系统，采用定向天线技术，并通过改进的软演员-评论家（SAC）算法解决非凸优化问题。

Result: 仿真结果表明，该方法在多种场景下优于基线方法，实现了高效能量管理和高计算性能。

Conclusion: 所提方法在复杂环境中表现出强大的泛化能力，验证了边界惩罚和充电奖励机制的有效性。

Abstract: The integration of simultaneous wireless information and power transfer
(SWIPT) technology in 6G Internet of Things (IoT) networks faces significant
challenges in remote areas and disaster scenarios where ground infrastructure
is unavailable. This paper proposes a novel unmanned aerial vehicle
(UAV)-assisted mobile edge computing (MEC) system enhanced by directional
antennas to provide both computational resources and energy support for ground
IoT terminals. However, such systems require multiple trade-off policies to
balance UAV energy consumption, terminal battery levels, and computational
resource allocation under various constraints, including limited UAV battery
capacity, non-linear energy harvesting characteristics, and dynamic task
arrivals. To address these challenges comprehensively, we formulate a
bi-objective optimization problem that simultaneously considers system energy
efficiency and terminal battery sustainability. We then reformulate this
non-convex problem with a hybrid solution space as a Markov decision process
(MDP) and propose an improved soft actor-critic (SAC) algorithm with an action
simplification mechanism to enhance its convergence and generalization
capabilities. Simulation results have demonstrated that our proposed approach
outperforms various baselines in different scenarios, achieving efficient
energy management while maintaining high computational performance.
Furthermore, our method shows strong generalization ability across different
scenarios, particularly in complex environments, validating the effectiveness
of our designed boundary penalty and charging reward mechanisms.

</details>


### [88] [MDPs with a State Sensing Cost](https://arxiv.org/abs/2505.03280)
*Vansh Kapoor,Jayakrishnan Nair*

Main category: cs.LG

TL;DR: 论文研究了在状态跟踪有成本的情况下，如何平衡感知成本与最优行动的问题，提出了一种基于MDP的框架，并设计了高效启发式算法。


<details>
  <summary>Details</summary>
Motivation: 解决在状态感知有成本时，如何优化决策以平衡感知成本与行动价值的问题。

Method: 将问题建模为扩展状态空间的MDP，限制非感知行动次数，并设计基于策略改进的启发式算法。

Result: 证明了限制非感知行动次数的策略的次优性界限，启发式算法在实践中接近最优策略。

Conclusion: 通过数值案例验证了方法的有效性，为实际应用提供了高效解决方案。

Abstract: In many practical sequential decision-making problems, tracking the state of
the environment incurs a sensing/communication/computation cost. In these
settings, the agent's interaction with its environment includes the additional
component of deciding $\textit{when}$ to sense the state, in a manner that
balances the value associated with optimal (state-specific) actions and the
cost of sensing. We formulate this as an expected discounted cost Markov
Decision Process (MDP), wherein the agent incurs an additional cost for sensing
its next state, but has the option to take actions while remaining 'blind' to
the system state.
  We pose this problem as a classical discounted cost MDP with an expanded
(countably infinite) state space. While computing the optimal policy for this
MDP is intractable in general, we bound the sub-optimality gap associated with
optimal policies in a restricted class, where the number of consecutive
non-sensing (a.k.a., blind) actions is capped. We also design a computationally
efficient heuristic algorithm based on policy improvement, which in practice
performs close to the optimal policy. Finally, we benchmark against the state
of the art via a numerical case study.

</details>


### [89] [Physics-inspired Energy Transition Neural Network for Sequence Learning](https://arxiv.org/abs/2505.03281)
*Zhou Wu,Junyi An,Baile Xu,Furao Shen,Jian Zhao*

Main category: cs.LG

TL;DR: 论文提出了一种名为PETNN的循环神经网络结构，受物理能量转换模型启发，能够有效捕捉长程依赖关系，并在多个序列任务中表现优于Transformer。


<details>
  <summary>Details</summary>
Motivation: 尽管Transformer在序列建模中表现优异，但其长程依赖捕捉能力主要依赖于复杂的成对建模过程，而非内在的序列语义归纳偏置。因此，研究重新评估纯循环神经网络（RNN）的能力，并探索其长程学习机制。

Method: 受物理能量转换模型的启发，提出了一种名为PETNN的循环结构，其记忆机制能够有效存储长程依赖信息。

Result: 实验结果表明，PETNN在多种序列任务中优于基于Transformer的方法，且由于其循环特性，复杂度显著降低。

Conclusion: PETNN是一种优化的基础循环架构，展示了在Transformer主导的领域中开发有效循环神经网络的潜力。

Abstract: Recently, the superior performance of Transformers has made them a more
robust and scalable solution for sequence modeling than traditional recurrent
neural networks (RNNs). However, the effectiveness of Transformer in capturing
long-term dependencies is primarily attributed to their comprehensive
pair-modeling process rather than inherent inductive biases toward sequence
semantics. In this study, we explore the capabilities of pure RNNs and reassess
their long-term learning mechanisms. Inspired by the physics energy transition
models that track energy changes over time, we propose a effective recurrent
structure called the``Physics-inspired Energy Transition Neural Network"
(PETNN). We demonstrate that PETNN's memory mechanism effectively stores
information over long-term dependencies. Experimental results indicate that
PETNN outperforms transformer-based methods across various sequence tasks.
Furthermore, owing to its recurrent nature, PETNN exhibits significantly lower
complexity. Our study presents an optimal foundational recurrent architecture
and highlights the potential for developing effective recurrent neural networks
in fields currently dominated by Transformer.

</details>


### [90] [Unraveling the Rainbow: can value-based methods schedule?](https://arxiv.org/abs/2505.03323)
*Arthur Corrêa,Alexandre Jesus,Cristóvão Silva,Samuel Moniz*

Main category: cs.LG

TL;DR: 本文通过实证评估，挑战了组合优化中策略方法优于价值方法的假设，证明价值方法在某些情况下可以匹配甚至超越策略方法。


<details>
  <summary>Details</summary>
Motivation: 组合优化领域普遍偏爱策略方法，而忽视了价值方法的潜力，本文旨在填补这一研究空白。

Method: 在作业车间调度和柔性作业车间调度问题中，对深度Q网络及其扩展进行了全面评估。

Result: 结果表明，价值方法可以匹配甚至超越广泛采用的近端策略优化算法。

Conclusion: 价值方法应受到组合优化领域的更多关注。

Abstract: Recently, deep reinforcement learning has emerged as a promising approach for
solving complex combinatorial optimization problems. Broadly, deep
reinforcement learning methods fall into two categories: policy-based and
value-based. While value-based approaches have achieved notable success in
domains such as the Arcade Learning Environment, the combinatorial optimization
community has predominantly favored policy-based methods, often overlooking the
potential of value-based algorithms. In this work, we conduct a comprehensive
empirical evaluation of value-based algorithms, including the deep q-network
and several of its advanced extensions, within the context of two complex
combinatorial problems: the job-shop and the flexible job-shop scheduling
problems, two fundamental challenges with multiple industrial applications. Our
results challenge the assumption that policy-based methods are inherently
superior for combinatorial optimization. We show that several value-based
approaches can match or even outperform the widely adopted proximal policy
optimization algorithm, suggesting that value-based strategies deserve greater
attention from the combinatorial optimization community. Our code is openly
available at: https://github.com/AJ-Correa/Unraveling-the-Rainbow.

</details>


### [91] [Absolute Zero: Reinforced Self-play Reasoning with Zero Data](https://arxiv.org/abs/2505.03335)
*Andrew Zhao,Yiran Wu,Yang Yue,Tong Wu,Quentin Xu,Yang Yue,Matthieu Lin,Shenzhi Wang,Qingyun Wu,Zilong Zheng,Gao Huang*

Main category: cs.LG

TL;DR: 提出了一种名为Absolute Zero的新RLVR范式，通过模型自我生成任务并验证答案，无需外部数据，实现了在编码和数学推理任务上的SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有RLVR方法依赖人工标注数据的问题，并探索在AI超越人类智能时如何实现自我学习。

Method: 引入Absolute Zero Reasoner (AZR)，通过代码执行器自我生成和验证任务，实现无外部数据的训练。

Result: AZR在编码和数学推理任务上表现优于依赖大量人工数据的现有模型。

Conclusion: AZR展示了无外部数据训练的可行性，并适用于不同规模和类型的模型。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has shown promise in
enhancing the reasoning capabilities of large language models by learning
directly from outcome-based rewards. Recent RLVR works that operate under the
zero setting avoid supervision in labeling the reasoning process, but still
depend on manually curated collections of questions and answers for training.
The scarcity of high-quality, human-produced examples raises concerns about the
long-term scalability of relying on human supervision, a challenge already
evident in the domain of language model pretraining. Furthermore, in a
hypothetical future where AI surpasses human intelligence, tasks provided by
humans may offer limited learning potential for a superintelligent system. To
address these concerns, we propose a new RLVR paradigm called Absolute Zero, in
which a single model learns to propose tasks that maximize its own learning
progress and improves reasoning by solving them, without relying on any
external data. Under this paradigm, we introduce the Absolute Zero Reasoner
(AZR), a system that self-evolves its training curriculum and reasoning ability
by using a code executor to both validate proposed code reasoning tasks and
verify answers, serving as an unified source of verifiable reward to guide
open-ended yet grounded learning. Despite being trained entirely without
external data, AZR achieves overall SOTA performance on coding and mathematical
reasoning tasks, outperforming existing zero-setting models that rely on tens
of thousands of in-domain human-curated examples. Furthermore, we demonstrate
that AZR can be effectively applied across different model scales and is
compatible with various model classes.

</details>


### [92] [Geospatial Mechanistic Interpretability of Large Language Models](https://arxiv.org/abs/2505.03368)
*Stef De Sabbata,Stefano Mizzaro,Kevin Roitero*

Main category: cs.LG

TL;DR: 本文提出了一种研究LLMs处理地理信息的新框架，通过空间分析揭示其内部工作机制。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs如何内部处理地理信息，填补现有研究空白。

Method: 结合探测技术和机制可解释性方法，利用空间自相关分析地名特征。

Result: 实验显示地名特征具有与地理位置相关的空间模式。

Conclusion: 该框架为地理学中基础模型的研究和应用提供了新视角。

Abstract: Large Language Models (LLMs) have demonstrated unprecedented capabilities
across various natural language processing tasks. Their ability to process and
generate viable text and code has made them ubiquitous in many fields, while
their deployment as knowledge bases and "reasoning" tools remains an area of
ongoing research. In geography, a growing body of literature has been focusing
on evaluating LLMs' geographical knowledge and their ability to perform spatial
reasoning. However, very little is still known about the internal functioning
of these models, especially about how they process geographical information.
  In this chapter, we establish a novel framework for the study of geospatial
mechanistic interpretability - using spatial analysis to reverse engineer how
LLMs handle geographical information. Our aim is to advance our understanding
of the internal representations that these complex models generate while
processing geographical information - what one might call "how LLMs think about
geographic information" if such phrasing was not an undue anthropomorphism.
  We first outline the use of probing in revealing internal structures within
LLMs. We then introduce the field of mechanistic interpretability, discussing
the superposition hypothesis and the role of sparse autoencoders in
disentangling polysemantic internal representations of LLMs into more
interpretable, monosemantic features. In our experiments, we use spatial
autocorrelation to show how features obtained for placenames display spatial
patterns related to their geographic location and can thus be interpreted
geospatially, providing insights into how these models process geographical
information. We conclude by discussing how our framework can help shape the
study and use of foundation models in geography.

</details>


### [93] [SPAP: Structured Pruning via Alternating Optimization and Penalty Methods](https://arxiv.org/abs/2505.03373)
*Hanyu Hu,Xiaoming Yuan*

Main category: cs.LG

TL;DR: SPAP是一种基于优化理论的结构化剪枝框架，通过混合整数优化和惩罚方法减少剪枝误差，显著提升LLMs的推理速度和内存效率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的计算和内存需求高，现有剪枝方法存在性能下降、依赖启发式指标或微调成本高的问题。

Method: SPAP采用混合整数优化模型和惩罚方法，结合交替最小化算法，实现高效权重更新和性能恢复。

Result: 在OPT、LLaMA和Qwen2.5等模型上，SPAP在30%稀疏度下实现1.29倍推理加速和内存减少。

Conclusion: SPAP为LLMs提供了一种高效且性能保持的剪枝解决方案。

Abstract: The deployment of large language models (LLMs) is often constrained by their
substantial computational and memory demands. While structured pruning presents
a viable approach by eliminating entire network components, existing methods
suffer from performance degradation, reliance on heuristic metrics, or
expensive finetuning. To address these challenges, we propose SPAP (Structured
Pruning via Alternating Optimization and Penalty Methods), a novel and
efficient structured pruning framework for LLMs grounded in optimization
theory. SPAP formulates the pruning problem through a mixed-integer
optimization model, employs a penalty method that effectively makes pruning
decisions to minimize pruning errors, and introduces an alternating
minimization algorithm tailored to the splittable problem structure for
efficient weight updates and performance recovery. Extensive experiments on
OPT, LLaMA-3/3.1/3.2, and Qwen2.5 models demonstrate SPAP's superiority over
state-of-the-art methods, delivering linear inference speedups (1.29$\times$ at
30% sparsity) and proportional memory reductions. Our work offers a practical,
optimization-driven solution for pruning LLMs while preserving model
performance.

</details>


### [94] [Physics-informed neural network estimation of active material properties in time-dependent cardiac biomechanical models](https://arxiv.org/abs/2505.03382)
*Matthias Höfler,Francesco Regazzoni,Stefano Pagani,Elias Karabelas,Christoph Augustin,Gundolf Haase,Gernot Plank,Federica Caforio*

Main category: cs.LG

TL;DR: 该论文研究了利用物理信息神经网络（PINNs）从医学影像数据中推断心脏生物力学模型中的主动收缩参数，通过改进PINN算法提高了参数重建的精度和空间分辨率。


<details>
  <summary>Details</summary>
Motivation: 准确评估心脏生物力学中的主动应力参数对理解心肌功能至关重要，但在临床环境中难以实现，尤其是仅依赖医学影像数据时。

Method: 采用两个神经网络分别参数化状态和参数场，通过能量最小化问题优化网络参数，并结合自适应权重、正则化策略、傅里叶特征等改进PINN算法。

Result: 在噪声存在和高空间分辨率条件下，成功重建了主动应力场，并应用于组织不均匀性和心肌纤维化疤痕的检测。

Conclusion: 该方法为心脏纤维化相关疾病的诊断和治疗规划提供了新途径。

Abstract: Active stress models in cardiac biomechanics account for the mechanical
deformation caused by muscle activity, thus providing a link between the
electrophysiological and mechanical properties of the tissue. The accurate
assessment of active stress parameters is fundamental for a precise
understanding of myocardial function but remains difficult to achieve in a
clinical setting, especially when only displacement and strain data from
medical imaging modalities are available. This work investigates, through an
in-silico study, the application of physics-informed neural networks (PINNs)
for inferring active contractility parameters in time-dependent cardiac
biomechanical models from these types of imaging data. In particular, by
parametrising the sought state and parameter field with two neural networks,
respectively, and formulating an energy minimisation problem to search for the
optimal network parameters, we are able to reconstruct in various settings
active stress fields in the presence of noise and with a high spatial
resolution. To this end, we also advance the vanilla PINN learning algorithm
with the use of adaptive weighting schemes, ad-hoc regularisation strategies,
Fourier features, and suitable network architectures. In addition, we
thoroughly analyse the influence of the loss weights in the reconstruction of
active stress parameters. Finally, we apply the method to the characterisation
of tissue inhomogeneities and detection of fibrotic scars in myocardial tissue.
This approach opens a new pathway to significantly improve the diagnosis,
treatment planning, and management of heart conditions associated with cardiac
fibrosis.

</details>


### [95] [Improving Omics-Based Classification: The Role of Feature Selection and Synthetic Data Generation](https://arxiv.org/abs/2505.03387)
*Diego Perazzolo,Pietro Fanton,Ilaria Barison,Marny Fedrigo,Annalisa Angelini,Chiara Castellani,Enrico Grisan*

Main category: cs.LG

TL;DR: 该研究提出了一种结合特征选择与数据增强的机器学习分类框架，旨在提高小样本组学数据分类的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 面对组学数据的高维性和样本稀缺性，现有分类模型的可解释性和可靠性不足，需要一种既能提升性能又能增强透明度的解决方案。

Method: 采用特征选择与数据增强技术结合的机器学习框架，通过公开数据集（E-MTAB 8026）在六种二分类场景中进行自举分析验证。

Result: 模型在小样本数据集上表现出稳定的交叉验证性能，并在更大测试集上保持一致性，证明了合成数据对泛化能力的积极影响。

Conclusion: 研究强调了准确性与特征选择之间的平衡，展示了数据增强在小样本场景中提升模型泛化能力的潜力。

Abstract: Given the increasing complexity of omics datasets, a key challenge is not
only improving classification performance but also enhancing the transparency
and reliability of model decisions. Effective model performance and feature
selection are fundamental for explainability and reliability. In many cases,
high dimensional omics datasets suffer from limited number of samples due to
clinical constraints, patient conditions, phenotypes rarity and others
conditions. Current omics based classification models often suffer from narrow
interpretability, making it difficult to discern meaningful insights where
trust and reproducibility are critical. This study presents a machine learning
based classification framework that integrates feature selection with data
augmentation techniques to achieve high standard classification accuracy while
ensuring better interpretability. Using the publicly available dataset (E MTAB
8026), we explore a bootstrap analysis in six binary classification scenarios
to evaluate the proposed model's behaviour. We show that the proposed pipeline
yields cross validated perfomance on small dataset that is conserved when the
trained classifier is applied to a larger test set. Our findings emphasize the
fundamental balance between accuracy and feature selection, highlighting the
positive effect of introducing synthetic data for better generalization, even
in scenarios with very limited samples availability.

</details>


### [96] [Concept Factorization via Self-Representation and Adaptive Graph Structure Learning](https://arxiv.org/abs/2505.03390)
*Zhengqin Yang,Di Wu,Jia Chen,Xin Luo*

Main category: cs.LG

TL;DR: 提出了一种基于自表示和自适应图结构学习的概念分解模型（CFSRAG），用于动态学习数据的内在几何结构，实验表明其优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 传统概念分解模型依赖初始图结构构建，限制了聚类性能，因此需要自适应学习图结构的方法。

Method: 通过自表示方法学习数据间的亲和关系，利用亲和矩阵实现动态图正则化约束。

Result: 在四个真实数据集上的实验表明，CFSRAG优于其他先进模型。

Conclusion: CFSRAG能够动态学习数据的内在几何结构，提升聚类性能。

Abstract: Concept Factorization (CF) models have attracted widespread attention due to
their excellent performance in data clustering. In recent years, many variant
models based on CF have achieved great success in clustering by taking into
account the internal geometric manifold structure of the dataset and using
graph regularization techniques. However, their clustering performance depends
greatly on the construction of the initial graph structure. In order to enable
adaptive learning of the graph structure of the data, we propose a Concept
Factorization Based on Self-Representation and Adaptive Graph Structure
Learning (CFSRAG) Model. CFSRAG learns the affinity relationship between data
through a self-representation method, and uses the learned affinity matrix to
implement dynamic graph regularization constraints, thereby ensuring dynamic
learning of the internal geometric structure of the data. Finally, we give the
CFSRAG update rule and convergence analysis, and conduct comparative
experiments on four real datasets. The results show that our model outperforms
other state-of-the-art models.

</details>


### [97] [Automatic Calibration for Membership Inference Attack on Large Language Models](https://arxiv.org/abs/2505.03392)
*Saleh Zare Zade,Yao Qiang,Xiangyu Zhou,Hui Zhu,Mohammad Amin Roshani,Prashant Khanduri,Dongxiao Zhu*

Main category: cs.LG

TL;DR: 论文提出了一种名为ACMIA的新框架，通过可调温度校准输出概率，有效解决了现有成员推理攻击方法的高误报率和依赖参考模型的问题。


<details>
  <summary>Details</summary>
Motivation: 现有成员推理攻击方法存在高误报率或依赖额外参考模型的问题，限制了其实际应用。

Method: 提出ACMIA框架，利用可调温度校准输出概率，并通过三种配置适应不同模型访问级别。

Result: 实验表明ACMIA在多种开源LLM上表现优异，优于现有基准方法。

Conclusion: ACMIA是一种高效、鲁棒且通用的成员推理攻击方法。

Abstract: Membership Inference Attacks (MIAs) have recently been employed to determine
whether a specific text was part of the pre-training data of Large Language
Models (LLMs). However, existing methods often misinfer non-members as members,
leading to a high false positive rate, or depend on additional reference models
for probability calibration, which limits their practicality. To overcome these
challenges, we introduce a novel framework called Automatic Calibration
Membership Inference Attack (ACMIA), which utilizes a tunable temperature to
calibrate output probabilities effectively. This approach is inspired by our
theoretical insights into maximum likelihood estimation during the pre-training
of LLMs. We introduce ACMIA in three configurations designed to accommodate
different levels of model access and increase the probability gap between
members and non-members, improving the reliability and robustness of membership
inference. Extensive experiments on various open-source LLMs demonstrate that
our proposed attack is highly effective, robust, and generalizable, surpassing
state-of-the-art baselines across three widely used benchmarks. Our code is
available at:
\href{https://github.com/Salehzz/ACMIA}{\textcolor{blue}{Github}}.

</details>


### [98] [Prediction Models That Learn to Avoid Missing Values](https://arxiv.org/abs/2505.03393)
*Lena Stempfle,Anton Matsson,Newton Mwai,Fredrik D. Johansson*

Main category: cs.LG

TL;DR: 提出了一种缺失值避免（MA）机器学习框架，通过在训练目标中加入特定分类器的正则化项，减少模型对缺失值的依赖，同时保持预测性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: 处理测试时的缺失值对机器学习模型具有挑战性，现有方法可能引入偏差或复杂性，影响可解释性。

Method: 为决策树、树集成和稀疏线性模型设计了MA学习算法，利用上下文缺失性减少对缺失值的依赖。

Result: 实验表明，MA-DT、MA-LASSO、MA-RF和MA-GBT在减少对缺失值依赖的同时，保持了与未正则化模型相当的预测性能。

Conclusion: MA框架为实践者提供了在测试时处理缺失值的同时保持预测可解释性的有效工具。

Abstract: Handling missing values at test time is challenging for machine learning
models, especially when aiming for both high accuracy and interpretability.
Established approaches often add bias through imputation or excessive model
complexity via missingness indicators. Moreover, either method can obscure
interpretability, making it harder to understand how the model utilizes the
observed variables in predictions. We propose missingness-avoiding (MA) machine
learning, a general framework for training models to rarely require the values
of missing (or imputed) features at test time. We create tailored MA learning
algorithms for decision trees, tree ensembles, and sparse linear models by
incorporating classifier-specific regularization terms in their learning
objectives. The tree-based models leverage contextual missingness by reducing
reliance on missing values based on the observed context. Experiments on
real-world datasets demonstrate that MA-DT, MA-LASSO, MA-RF, and MA-GBT
effectively reduce the reliance on features with missing values while
maintaining predictive performance competitive with their unregularized
counterparts. This shows that our framework gives practitioners a powerful tool
to maintain interpretability in predictions with test-time missing values.

</details>


### [99] [Knowledge Augmented Complex Problem Solving with Large Language Models: A Survey](https://arxiv.org/abs/2505.03418)
*Da Zheng,Lun Du,Junwei Su,Yuchen Tian,Yuqi Zhu,Jintian Zhang,Lanning Wei,Ningyu Zhang,Huajun Chen*

Main category: cs.LG

TL;DR: 该论文探讨了大型语言模型（LLMs）在复杂问题解决中的能力与局限，重点分析了多步推理、领域知识整合和结果验证等挑战，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能的发展，LLMs成为解决复杂问题的有力工具，但其在现实问题中的应用仍面临多步推理、领域知识整合和结果验证等挑战。

Method: 论文通过调查LLMs在复杂问题解决中的应用，分析了包括思维链（CoT）推理、知识增强和多种验证技术在内的技术。

Result: 研究发现，LLMs在多领域（如软件工程、数学推理、数据分析和科学研究）中具有潜力，但仍存在局限性。

Conclusion: 论文总结了当前LLM解决方案的根本局限，并从多步推理、领域知识整合和结果验证的角度提出了未来研究方向。

Abstract: Problem-solving has been a fundamental driver of human progress in numerous
domains. With advancements in artificial intelligence, Large Language Models
(LLMs) have emerged as powerful tools capable of tackling complex problems
across diverse domains. Unlike traditional computational systems, LLMs combine
raw computational power with an approximation of human reasoning, allowing them
to generate solutions, make inferences, and even leverage external
computational tools. However, applying LLMs to real-world problem-solving
presents significant challenges, including multi-step reasoning, domain
knowledge integration, and result verification. This survey explores the
capabilities and limitations of LLMs in complex problem-solving, examining
techniques including Chain-of-Thought (CoT) reasoning, knowledge augmentation,
and various LLM-based and tool-based verification techniques. Additionally, we
highlight domain-specific challenges in various domains, such as software
engineering, mathematical reasoning and proving, data analysis and modeling,
and scientific research. The paper further discusses the fundamental
limitations of the current LLM solutions and the future directions of LLM-based
complex problems solving from the perspective of multi-step reasoning, domain
knowledge integration and result verification.

</details>


### [100] [Framework GNN-AID: Graph Neural Network Analysis Interpretation and Defense](https://arxiv.org/abs/2505.03424)
*Kirill Lukyanov,Mikhail Drobyshevskiy,Georgii Sazonov,Mikhail Soloviov,Ilya Makarov*

Main category: cs.LG

TL;DR: GNN-AID是一个开源框架，专注于图数据的可信AI，结合了可解释性和鲁棒性，支持攻击、防御和可解释性方法。


<details>
  <summary>Details</summary>
Motivation: 现有工具常忽视图数据，且很少将可解释性与鲁棒性结合，GNN-AID旨在填补这一空白。

Method: 基于PyTorch-Geometric构建，提供预加载数据集、模型和可定制接口，支持MLOps技术。

Result: GNN-AID为开发者和研究者提供灵活工具，支持快速实验和高级研究，同时揭示了防御策略间的冲突。

Conclusion: GNN-AID是一个多功能框架，适用于图数据的可信AI研究与应用。

Abstract: The growing need for Trusted AI (TAI) highlights the importance of
interpretability and robustness in machine learning models. However, many
existing tools overlook graph data and rarely combine these two aspects into a
single solution. Graph Neural Networks (GNNs) have become a popular approach,
achieving top results across various tasks. We introduce GNN-AID (Graph Neural
Network Analysis, Interpretation, and Defense), an open-source framework
designed for graph data to address this gap. Built as a Python library, GNN-AID
supports advanced trust methods and architectural layers, allowing users to
analyze graph datasets and GNN behavior using attacks, defenses, and
interpretability methods.
  GNN-AID is built on PyTorch-Geometric, offering preloaded datasets, models,
and support for any GNNs through customizable interfaces. It also includes a
web interface with tools for graph visualization and no-code features like an
interactive model builder, simplifying the exploration and analysis of GNNs.
The framework also supports MLOps techniques, ensuring reproducibility and
result versioning to track and revisit analyses efficiently.
  GNN-AID is a flexible tool for developers and researchers. It helps
developers create, analyze, and customize graph models, while also providing
access to prebuilt datasets and models for quick experimentation. Researchers
can use the framework to explore advanced topics on the relationship between
interpretability and robustness, test defense strategies, and combine methods
to protect against different types of attacks.
  We also show how defenses against evasion and poisoning attacks can conflict
when applied to graph data, highlighting the complex connections between
defense strategies.
  GNN-AID is available at
\href{https://github.com/ispras/GNN-AID}{github.com/ispras/GNN-AID}

</details>


### [101] [Wasserstein Convergence of Score-based Generative Models under Semiconvexity and Discontinuous Gradients](https://arxiv.org/abs/2505.03432)
*Stefano Bruno,Sotirios Sabanis*

Main category: cs.LG

TL;DR: 本文首次为非平滑、复杂数据分布的基于分数的生成模型（SGMs）提供了非渐近的Wasserstein-2收敛保证，放宽了传统分析中的强正则性假设。


<details>
  <summary>Details</summary>
Motivation: SGMs在复杂数据分布建模和样本生成方面表现出色，但现有理论分析通常依赖于不切实际的强正则性条件（如平滑性或严格对数凹性）。本文旨在填补这一理论与实践的差距。

Method: 通过利用半凸性（semiconvexity）而不要求潜在函数的平滑性假设（如可微性），建立非渐近的Wasserstein-2收敛上界。

Result: 在数据维度$d$上实现了$O(\sqrt{d})$的最优依赖，收敛速率为一阶，适用于包括对称修正半正态分布、高斯混合、双阱势和弹性网势等广泛的实际相关分布。

Conclusion: 本文显著拓宽了SGMs的理论基础，为在非平滑、复杂数据场景下的应用提供了严格的收敛保证。

Abstract: Score-based Generative Models (SGMs) approximate a data distribution by
perturbing it with Gaussian noise and subsequently denoising it via a learned
reverse diffusion process. These models excel at modeling complex data
distributions and generating diverse samples, achieving state-of-the-art
performance across domains such as computer vision, audio generation,
reinforcement learning, and computational biology. Despite their empirical
success, existing Wasserstein-2 convergence analysis typically assume strong
regularity conditions-such as smoothness or strict log-concavity of the data
distribution-that are rarely satisfied in practice. In this work, we establish
the first non-asymptotic Wasserstein-2 convergence guarantees for SGMs
targeting semiconvex distributions with potentially discontinuous gradients.
Our upper bounds are explicit and sharp in key parameters, achieving optimal
dependence of $O(\sqrt{d})$ on the data dimension $d$ and convergence rate of
order one. The framework accommodates a wide class of practically relevant
distributions, including symmetric modified half-normal distributions, Gaussian
mixtures, double-well potentials, and elastic net potentials. By leveraging
semiconvexity without requiring smoothness assumptions on the potential such as
differentiability, our results substantially broaden the theoretical
foundations of SGMs, bridging the gap between empirical success and rigorous
guarantees in non-smooth, complex data regimes.

</details>


### [102] [A new membership inference attack that spots memorization in generative and predictive models: Loss-Based with Reference Model algorithm (LBRM)](https://arxiv.org/abs/2505.03490)
*Faiz Taleb,Ivan Gazeau,Maryline Laurent*

Main category: cs.LG

TL;DR: 论文提出了一种基于参考模型的LBRM算法，用于检测时间序列插值模型中的记忆化现象，显著提高了成员推断攻击的准确性。


<details>
  <summary>Details</summary>
Motivation: 生成模型可能无意中记忆训练数据，带来隐私风险，本文旨在解决时间序列插值模型中的记忆化问题。

Method: 提出LBRM算法，利用参考模型区分训练和测试数据，并通过成员推断攻击验证其有效性。

Result: 未微调时AUROC提升约40%，微调后提升约60%，验证了LBRM在不同架构中的鲁棒性。

Conclusion: LBRM方法显著提高了检测准确性，有效应对时间序列插值模型的隐私风险。

Abstract: Generative models can unintentionally memorize training data, posing
significant privacy risks. This paper addresses the memorization phenomenon in
time series imputation models, introducing the Loss-Based with Reference Model
(LBRM) algorithm. The LBRM method leverages a reference model to enhance the
accuracy of membership inference attacks, distinguishing between training and
test data. Our contributions are twofold: first, we propose an innovative
method to effectively extract and identify memorized training data,
significantly improving detection accuracy. On average, without fine-tuning,
the AUROC improved by approximately 40\%. With fine-tuning, the AUROC increased
by approximately 60\%. Second, we validate our approach through membership
inference attacks on two types of architectures designed for time series
imputation, demonstrating the robustness and versatility of the LBRM approach
in different contexts. These results highlight the significant enhancement in
detection accuracy provided by the LBRM approach, addressing privacy risks in
time series imputation models.

</details>


### [103] [AnomalyMatch: Discovering Rare Objects of Interest with Semi-supervised and Active Learning](https://arxiv.org/abs/2505.03509)
*Pablo Gómez,David O'Ryan*

Main category: cs.LG

TL;DR: AnomalyMatch是一个结合半监督学习和主动学习的异常检测框架，适用于标签稀缺的大规模数据集，在天文学和计算机视觉领域表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决传统监督方法在异常检测中需要大量标注数据的局限性，特别是在标签稀缺的场景下。

Method: 结合FixMatch半监督算法和EfficientNet分类器，通过主动学习迭代优化模型，并利用用户界面进行专家验证。

Result: 在GalaxyMNIST和miniImageNet数据集上表现优异，AUROC分别达到0.86和0.95，AUPRC为0.71和0.77。

Conclusion: AnomalyMatch在标签稀缺的大规模数据集中具有高效性和可扩展性，为天文学等领域提供了实用的异常检测解决方案。

Abstract: Anomaly detection in large datasets is essential in fields such as astronomy
and computer vision; however, supervised methods typically require extensive
anomaly labelling, which is often impractical. We present AnomalyMatch, an
anomaly detection framework combining the semi-supervised FixMatch algorithm
using EfficientNet classifiers with active learning. By treating anomaly
detection as a semi-supervised binary classification problem, we efficiently
utilise limited labelled and abundant unlabelled images. We allow iterative
model refinement in a user interface for expert verification of high-confidence
anomalies and correction of false positives. Built for astronomical data,
AnomalyMatch generalises readily to other domains facing similar data
challenges. Evaluations on the GalaxyMNIST astronomical dataset and the
miniImageNet natural-image benchmark under severe class imbalance (1% anomalies
for miniImageNet) display strong performance: starting from five to ten
labelled anomalies and after three active learning cycles, we achieve an
average AUROC of 0.95 (miniImageNet) and 0.86 (GalaxyMNIST), with respective
AUPRC of 0.77 and 0.71. After active learning cycles, anomalies are ranked with
71% (miniImageNet) to 93% precision in the 1% of the highest-ranked images.
AnomalyMatch is tailored for large-scale applications, efficiently processing
predictions for 100 million images within three days on a single GPU.
Integrated into ESAs Datalabs platform, AnomalyMatch facilitates targeted
discovery of scientifically valuable anomalies in vast astronomical datasets.
Our results underscore the exceptional utility and scalability of this approach
for anomaly discovery, highlighting the value of specialised approaches for
domains characterised by severe label scarcity.

</details>


### [104] [Uncovering the Limitations of Model Inversion Evaluation: Benchmarks and Connection to Type-I Adversarial Attacks](https://arxiv.org/abs/2505.03519)
*Sy-Tuyen Ho,Koh Jun Hao,Ngoc-Bao Nguyen,Alexander Binder,Ngai-Man Cheung*

Main category: cs.LG

TL;DR: 本文首次深入研究了模型反演（MI）攻击的评估框架，发现其存在大量假阳性，导致高估了现有MI攻击的成功率。作者构建了首个全面的人工标注数据集，揭示了评估框架的局限性，并提出了减少假阳性率的方法。


<details>
  <summary>Details</summary>
Motivation: 现有MI攻击/防御的评估框架存在假阳性问题，可能导致对攻击成功率的误判。本文旨在揭示这一问题并提出改进方法。

Method: 构建了首个全面的人工标注MI攻击样本数据集，基于28种不同MI攻击、防御、私有和公共数据集的配置。通过分析假阳性原因，设计了对照实验，研究了I型对抗特征和对抗迁移性的影响。

Result: 发现现有MI评估框架存在大量假阳性，导致高估了SOTA MI攻击的成功率。实际隐私泄露程度显著低于先前报告。

Conclusion: 强调了广泛使用的MI评估框架的关键局限性，提出了减少假阳性率的方法，并建议将人工评估作为主要框架而非补充。鼓励开发更稳健的自动评估框架。

Abstract: Model Inversion (MI) attacks aim to reconstruct information of private
training data by exploiting access to machine learning models. The most common
evaluation framework for MI attacks/defenses relies on an evaluation model that
has been utilized to assess progress across almost all MI attacks and defenses
proposed in recent years. In this paper, for the first time, we present an
in-depth study of MI evaluation. Firstly, we construct the first comprehensive
human-annotated dataset of MI attack samples, based on 28 setups of different
MI attacks, defenses, private and public datasets. Secondly, using our dataset,
we examine the accuracy of the MI evaluation framework and reveal that it
suffers from a significant number of false positives. These findings raise
questions about the previously reported success rates of SOTA MI attacks.
Thirdly, we analyze the causes of these false positives, design controlled
experiments, and discover the surprising effect of Type I adversarial features
on MI evaluation, as well as adversarial transferability, highlighting a
relationship between two previously distinct research areas. Our findings
suggest that the performance of SOTA MI attacks has been overestimated, with
the actual privacy leakage being significantly less than previously reported.
In conclusion, we highlight critical limitations in the widely used MI
evaluation framework and present our methods to mitigate false positive rates.
We remark that prior research has shown that Type I adversarial attacks are
very challenging, with no existing solution. Therefore, we urge to consider
human evaluation as a primary MI evaluation framework rather than merely a
supplement as in previous MI research. We also encourage further work on
developing more robust and reliable automatic evaluation frameworks.

</details>


### [105] [Causal Intervention Framework for Variational Auto Encoder Mechanistic Interpretability](https://arxiv.org/abs/2505.03530)
*Dip Roy*

Main category: cs.LG

TL;DR: 本文提出了一种用于变分自编码器（VAE）机制解释的因果干预框架，通过识别和分析网络中的电路模式，量化了VAE组件的可解释性。


<details>
  <summary>Details</summary>
Motivation: 生成模型（如VAE）的机制解释仍具挑战性，而理解其内部工作原理对提高透明性和可控性至关重要。

Method: 开发了多层次的干预技术（输入操作、潜在空间扰动、激活修补和因果中介分析），并应用于合成数据集和标准解缠基准。

Result: 实验表明，FactorVAE在解缠分数（0.084）和因果效应强度（均值4.59）上优于标准VAE（0.064, 3.99）和Beta-VAE（0.051, 3.43）。

Conclusion: 该框架推动了生成模型的机制理解，并为更透明和可控的VAE架构提供了工具。

Abstract: Mechanistic interpretability of deep learning models has emerged as a crucial
research direction for understanding the functioning of neural networks. While
significant progress has been made in interpreting discriminative models like
transformers, understanding generative models such as Variational Autoencoders
(VAEs) remains challenging. This paper introduces a comprehensive causal
intervention framework for mechanistic interpretability of VAEs. We develop
techniques to identify and analyze "circuit motifs" in VAEs, examining how
semantic factors are encoded, processed, and disentangled through the network
layers. Our approach uses targeted interventions at different levels: input
manipulations, latent space perturbations, activation patching, and causal
mediation analysis. We apply our framework to both synthetic datasets with
known causal relationships and standard disentanglement benchmarks. Results
show that our interventions can successfully isolate functional circuits, map
computational graphs to causal graphs of semantic factors, and distinguish
between polysemantic and monosemantic units. Furthermore, we introduce metrics
for causal effect strength, intervention specificity, and circuit modularity
that quantify the interpretability of VAE components. Experimental results
demonstrate clear differences between VAE variants, with FactorVAE achieving
higher disentanglement scores (0.084) and effect strengths (mean 4.59) compared
to standard VAE (0.064, 3.99) and Beta-VAE (0.051, 3.43). Our framework
advances the mechanistic understanding of generative models and provides tools
for more transparent and controllable VAE architectures.

</details>


### [106] [Small-Scale-Fading-Aware Resource Allocation in Wireless Federated Learning](https://arxiv.org/abs/2505.03533)
*Jiacheng Wang,Le Liang,Hao Ye,Chongtao Guo,Shi Jin*

Main category: cs.LG

TL;DR: 提出了一种基于多智能体强化学习（MARL）的小尺度衰落感知资源分配策略，以优化无线网络中联邦学习（FL）的训练性能。


<details>
  <summary>Details</summary>
Motivation: 现有资源分配策略通常基于块衰落假设，忽略了FL梯度上传过程中信道的快速波动，导致FL性能下降。

Method: 通过建立FL算法的一步收敛边界，将资源分配问题建模为分散式部分可观测马尔可夫决策过程（Dec-POMDP），并使用QMIX算法解决。

Result: 实验表明，基于QMIX的策略在统计异构性不同的情况下显著优于基线方法，且小尺度衰落动态的引入对优化FL性能至关重要。

Conclusion: 该策略通过动态分配频谱和功率资源，提升了FL的收敛性能和实用性。

Abstract: Judicious resource allocation can effectively enhance federated learning (FL)
training performance in wireless networks by addressing both system and
statistical heterogeneity. However, existing strategies typically rely on block
fading assumptions, which overlooks rapid channel fluctuations within each
round of FL gradient uploading, leading to a degradation in FL training
performance. Therefore, this paper proposes a small-scale-fading-aware resource
allocation strategy using a multi-agent reinforcement learning (MARL)
framework. Specifically, we establish a one-step convergence bound of the FL
algorithm and formulate the resource allocation problem as a decentralized
partially observable Markov decision process (Dec-POMDP), which is subsequently
solved using the QMIX algorithm. In our framework, each client serves as an
agent that dynamically determines spectrum and power allocations within each
coherence time slot, based on local observations and a reward derived from the
convergence analysis. The MARL setting reduces the dimensionality of the action
space and facilitates decentralized decision-making, enhancing the scalability
and practicality of the solution. Experimental results demonstrate that our
QMIX-based resource allocation strategy significantly outperforms baseline
methods across various degrees of statistical heterogeneity. Additionally,
ablation studies validate the critical importance of incorporating small-scale
fading dynamics, highlighting its role in optimizing FL performance.

</details>


### [107] [Efficient Training of Physics-enhanced Neural ODEs via Direct Collocation and Nonlinear Programming](https://arxiv.org/abs/2505.03552)
*Linus Langenkamp,Philip Hannebohm,Bernhard Bachmann*

Main category: cs.LG

TL;DR: 提出了一种通过动态优化问题训练物理增强神经ODE（PeNODEs）的新方法，利用高阶隐式Runge-Kutta方法离散化模型，并通过NLP求解器优化网络参数和状态轨迹，显著提升了稳定性、运行时间和准确性。


<details>
  <summary>Details</summary>
Motivation: 解决基于ODE求解器的训练方法在稳定性、运行时间和准确性方面的局限性。

Method: 采用高阶隐式Runge-Kutta方法离散化模型，将其转化为大规模非线性规划问题（NLP），并通过Ipopt等求解器进行优化。

Result: 在Quarter Vehicle Model和Van-der-Pol振荡器上的实验表明，该方法在准确性、速度和泛化能力上优于其他训练技术，且网络规模更小。

Conclusion: 该方法为训练PeNODEs提供了一种高效且稳定的解决方案，并计划集成到OpenModelica中以支持神经DAE的训练。

Abstract: We propose a novel approach for training Physics-enhanced Neural ODEs
(PeNODEs) by expressing the training process as a dynamic optimization problem.
The full model, including neural components, is discretized using a high-order
implicit Runge-Kutta method with flipped Legendre-Gauss-Radau points, resulting
in a large-scale nonlinear program (NLP) efficiently solved by state-of-the-art
NLP solvers such as Ipopt. This formulation enables simultaneous optimization
of network parameters and state trajectories, addressing key limitations of ODE
solver-based training in terms of stability, runtime, and accuracy. Extending
on a recent direct collocation-based method for Neural ODEs, we generalize to
PeNODEs, incorporate physical constraints, and present a custom, parallelized,
open-source implementation. Benchmarks on a Quarter Vehicle Model and a
Van-der-Pol oscillator demonstrate superior accuracy, speed, and generalization
with smaller networks compared to other training techniques. We also outline a
planned integration into OpenModelica to enable accessible training of Neural
DAEs.

</details>


### [108] [Rapid AI-based generation of coverage paths for dispensing applications](https://arxiv.org/abs/2505.03560)
*Simon Baeuerle,Ian F. Mendonca,Kristof Van Laerhoven,Ralf Mikut,Andreas Steimer*

Main category: cs.LG

TL;DR: 提出了一种基于AI的新方法，用于生成热界面材料（TIM）的涂布路径，替代传统的高计算量优化方法。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖专家手动操作或计算量大的优化方法，效率低下。

Method: 使用人工神经网络（ANN）直接根据目标冷却区域生成涂布路径，无需标签。

Result: 生成的路径可直接用于自动化制造设备，且无气泡问题。

Conclusion: 该方法可实时预测工艺参数，并可能推广到其他制造过程。

Abstract: Coverage Path Planning of Thermal Interface Materials (TIM) plays a crucial
role in the design of power electronics and electronic control units. Up to
now, this is done manually by experts or by using optimization approaches with
a high computational effort. We propose a novel AI-based approach to generate
dispense paths for TIM and similar dispensing applications. It is a drop-in
replacement for optimization-based approaches. An Artificial Neural Network
(ANN) receives the target cooling area as input and directly outputs the
dispense path. Our proposed setup does not require labels and we show its
feasibility on multiple target areas. The resulting dispense paths can be
directly transferred to automated manufacturing equipment and do not exhibit
air entrapments. The approach of using an ANN to predict process parameters for
a desired target state in real-time could potentially be transferred to other
manufacturing processes.

</details>


### [109] [Ergodic Generative Flows](https://arxiv.org/abs/2505.03561)
*Leo Maxime Brunswic,Mateo Clemente,Rui Heng Yang,Adam Sigal,Amir Rasouli,Yinchuan Li*

Main category: cs.LG

TL;DR: 本文提出了一种称为Ergodic Generative Flows (EGFs)的生成流方法，解决了生成流网络在连续设置和模仿学习中的挑战，包括流匹配损失的计算困难和非循环训练的局限性。


<details>
  <summary>Details</summary>
Motivation: 生成流网络（GFNs）在连续设置和模仿学习中面临流匹配损失计算困难、非循环训练测试有限以及需要单独奖励模型等问题。

Method: 提出EGFs方法，利用遍历性构建简单的生成流，并引入KL-weakFM损失用于模仿学习。

Result: 在2D任务和NASA真实数据集上验证了方法的有效性。

Conclusion: EGFs方法解决了GFNs在连续设置和模仿学习中的关键问题，具有理论和实际应用价值。

Abstract: Generative Flow Networks (GFNs) were initially introduced on directed acyclic
graphs to sample from an unnormalized distribution density. Recent works have
extended the theoretical framework for generative methods allowing more
flexibility and enhancing application range. However, many challenges remain in
training GFNs in continuous settings and for imitation learning (IL), including
intractability of flow-matching loss, limited tests of non-acyclic training,
and the need for a separate reward model in imitation learning. The present
work proposes a family of generative flows called Ergodic Generative Flows
(EGFs) which are used to address the aforementioned issues. First, we leverage
ergodicity to build simple generative flows with finitely many globally defined
transformations (diffeomorphisms) with universality guarantees and tractable
flow-matching loss (FM loss). Second, we introduce a new loss involving
cross-entropy coupled to weak flow-matching control, coined KL-weakFM loss. It
is designed for IL training without a separate reward model. We evaluate
IL-EGFs on toy 2D tasks and real-world datasets from NASA on the sphere, using
the KL-weakFM loss. Additionally, we conduct toy 2D reinforcement learning
experiments with a target reward, using the FM loss.

</details>


### [110] [Anant-Net: Breaking the Curse of Dimensionality with Scalable and Interpretable Neural Surrogate for High-Dimensional PDEs](https://arxiv.org/abs/2505.03595)
*Sidharth S. Menon,Ameya D. Jagtap*

Main category: cs.LG

TL;DR: Anant-Net是一种高效的神经代理模型，用于解决高维偏微分方程（PDEs），克服了传统方法的维度灾难问题，并在高维计算中表现出高精度和高效性。


<details>
  <summary>Details</summary>
Motivation: 高维PDEs在科学与工程中广泛应用，但传统数值方法因维度灾难而难以处理，尤其是在超立方体域上计算复杂度指数增长。

Method: Anant-Net通过高效整合高维边界条件并最小化PDE残差，结合Kolmogorov-Arnold网络提升可解释性。

Result: 在Poisson、Sine-Gordon和Allen-Cahn等高维方程上，Anant-Net表现出高精度和鲁棒性，300维问题在单GPU上几小时内解决。

Conclusion: Anant-Net是一种准确、可解释且可扩展的高维PDE求解框架，优于其他先进方法。

Abstract: High-dimensional partial differential equations (PDEs) arise in diverse
scientific and engineering applications but remain computationally intractable
due to the curse of dimensionality. Traditional numerical methods struggle with
the exponential growth in computational complexity, particularly on hypercubic
domains, where the number of required collocation points increases rapidly with
dimensionality. Here, we introduce Anant-Net, an efficient neural surrogate
that overcomes this challenge, enabling the solution of PDEs in high
dimensions. Unlike hyperspheres, where the internal volume diminishes as
dimensionality increases, hypercubes retain or expand their volume (for unit or
larger length), making high-dimensional computations significantly more
demanding. Anant-Net efficiently incorporates high-dimensional boundary
conditions and minimizes the PDE residual at high-dimensional collocation
points. To enhance interpretability, we integrate Kolmogorov-Arnold networks
into the Anant-Net architecture. We benchmark Anant-Net's performance on
several linear and nonlinear high-dimensional equations, including the Poisson,
Sine-Gordon, and Allen-Cahn equations, demonstrating high accuracy and
robustness across randomly sampled test points from high-dimensional space.
Importantly, Anant-Net achieves these results with remarkable efficiency,
solving 300-dimensional problems on a single GPU within a few hours. We also
compare Anant-Net's results for accuracy and runtime with other
state-of-the-art methods. Our findings establish Anant-Net as an accurate,
interpretable, and scalable framework for efficiently solving high-dimensional
PDEs.

</details>


### [111] [Understand the Effect of Importance Weighting in Deep Learning on Dataset Shift](https://arxiv.org/abs/2505.03617)
*Thien Nhan Vo,Thanh Xuan Truong*

Main category: cs.LG

TL;DR: 研究了重要性加权在深度神经网络中应对标签偏移和协变量偏移的效果，发现其实际效用有限。


<details>
  <summary>Details</summary>
Motivation: 探讨重要性加权在分布偏移（标签偏移和协变量偏移）下的实际效果，验证其在复杂数据中的适用性。

Method: 在合成2D数据（线性可分和月亮形）上使用逻辑回归和MLP，以及在CIFAR-10上进行实验，比较不同正则化方法的效果。

Result: 加权在训练初期影响决策边界，但随优化时间延长效果减弱；L2正则化能保留加权效果，而dropout无效；协变量偏移实验中加权无显著性能提升。

Conclusion: 重要性加权在真实世界分布偏移中的实用性存疑。

Abstract: We evaluate the effectiveness of importance weighting in deep neural networks
under label shift and covariate shift. On synthetic 2D data (linearly separable
and moon-shaped) using logistic regression and MLPs, we observe that weighting
strongly affects decision boundaries early in training but fades with prolonged
optimization. On CIFAR-10 with various class imbalances, only L2 regularization
(not dropout) helps preserve weighting effects. In a covariate-shift
experiment, importance weighting yields no significant performance gain,
highlighting challenges on complex data. Our results call into question the
practical utility of importance weighting for real-world distribution shifts.

</details>


### [112] [ALMA: Aggregated Lipschitz Maximization Attack on Auto-encoders](https://arxiv.org/abs/2505.03646)
*Chethan Krishnamurthy Ramanaik,Arjun Roy,Eirini Ntoutsi*

Main category: cs.LG

TL;DR: 本文提出了一种基于层条件化的对抗优化目标，用于增强自编码器的对抗鲁棒性评估，并通过实验证明其优于现有方法。同时，提出了一种防御插件以减轻对抗样本的影响。


<details>
  <summary>Details</summary>
Motivation: 尽管自编码器在关键应用中广泛使用，但其对抗鲁棒性研究相对不足，现有评估框架未能充分利用中间层的脆弱性。

Method: 提出了一种新的层条件化对抗优化目标，通过增强损失梯度信息传播，引导对抗映射到局部Lipschitz边界区域。

Result: 实验表明，该方法在通用和样本特定场景下均优于现有攻击方法。

Conclusion: 本文提出的对抗优化目标和防御插件显著提升了自编码器的对抗鲁棒性。

Abstract: Despite the extensive use of deep autoencoders (AEs) in critical
applications, their adversarial robustness remains relatively underexplored
compared to classification models. AE robustness is characterized by the
Lipschitz bounds of its components. Existing robustness evaluation frameworks
based on white-box attacks do not fully exploit the vulnerabilities of
intermediate ill-conditioned layers in AEs. In the context of optimizing
imperceptible norm-bounded additive perturbations to maximize output damage,
existing methods struggle to effectively propagate adversarial loss gradients
throughout the network, often converging to less effective perturbations. To
address this, we propose a novel layer-conditioning-based adversarial
optimization objective that effectively guides the adversarial map toward
regions of local Lipschitz bounds by enhancing loss gradient information
propagation during attack optimization. We demonstrate through extensive
experiments on state-of-the-art AEs that our adversarial objective results in
stronger attacks, outperforming existing methods in both universal and
sample-specific scenarios. As a defense method against this attack, we
introduce an inference-time adversarially trained defense plugin that mitigates
the effects of adversarial examples.

</details>


### [113] [Mitigating mode collapse in normalizing flows by annealing with an adaptive schedule: Application to parameter estimation](https://arxiv.org/abs/2505.03652)
*Yihang Wang,Chris Chi,Aaron R. Dinner*

Main category: cs.LG

TL;DR: 通过基于有效样本量（ESS）的自适应退火方法，缓解了归一化流（NFs）在多模态分布中的模式崩溃问题，显著提升了计算效率。


<details>
  <summary>Details</summary>
Motivation: 归一化流（NFs）在复杂分布采样中具有潜力，但多模态分布中的模式崩溃限制了其实际应用。

Method: 采用基于ESS的自适应退火方法，优化NFs的采样过程，并利用ESS进行样本修剪以减少方差。

Result: 在生化振荡器模型中，该方法比传统MCMC方法快十倍收敛边际似然，同时降低了方差。

Conclusion: 该方法为NFs采样提供了通用改进方案，并为进一步优化提供了可能。

Abstract: Normalizing flows (NFs) provide uncorrelated samples from complex
distributions, making them an appealing tool for parameter estimation. However,
the practical utility of NFs remains limited by their tendency to collapse to a
single mode of a multimodal distribution. In this study, we show that annealing
with an adaptive schedule based on the effective sample size (ESS) can mitigate
mode collapse. We demonstrate that our approach can converge the marginal
likelihood for a biochemical oscillator model fit to time-series data in
ten-fold less computation time than a widely used ensemble Markov chain Monte
Carlo (MCMC) method. We show that the ESS can also be used to reduce variance
by pruning the samples. We expect these developments to be of general use for
sampling with NFs and discuss potential opportunities for further improvements.

</details>


### [114] [Neural Integral Operators for Inverse problems in Spectroscopy](https://arxiv.org/abs/2505.03677)
*Emanuele Zappala,Alice Giola,Andreas Kramer,Enrico Greco*

Main category: cs.LG

TL;DR: 提出一种基于积分算子的深度学习方法，用于分子光谱分类，在小数据集上表现优于传统机器学习和深度学习模型。


<details>
  <summary>Details</summary>
Motivation: 光谱数据通常稀缺，传统深度学习方法在小数据集上容易过拟合，而传统机器学习方法精度有限。

Method: 基于第一类积分方程学习积分算子，设计了一种抗过拟合的深度学习算法。

Result: 实验表明，该算法在小数据集上优于决策树、支持向量机和其他深度学习模型。

Conclusion: 该方法结合了深度学习的高性能和小数据集下的稳定性，解决了光谱分析中数据稀缺的挑战。

Abstract: Deep learning has shown high performance on spectroscopic inverse problems
when sufficient data is available. However, it is often the case that data in
spectroscopy is scarce, and this usually causes severe overfitting problems
with deep learning methods. Traditional machine learning methods are viable
when datasets are smaller, but the accuracy and applicability of these methods
is generally more limited.
  We introduce a deep learning method for classification of molecular spectra
based on learning integral operators via integral equations of the first kind,
which results in an algorithm that is less affected by overfitting issues on
small datasets, compared to other deep learning models.
  The problem formulation of the deep learning approach is based on inverse
problems, which have traditionally found important applications in
spectroscopy. We perform experiments on real world data to showcase our
algorithm. It is seen that the model outperforms traditional machine learning
approaches such as decision tree and support vector machine, and for small
datasets it outperforms other deep learning models. Therefore, our methodology
leverages the power of deep learning, still maintaining the performance when
the available data is very limited, which is one of the main issues that deep
learning faces in spectroscopy, where datasets are often times of small size.

</details>


### [115] [Learning Survival Distributions with the Asymmetric Laplace Distribution](https://arxiv.org/abs/2505.03712)
*Deming Sheng,Ricardo Henao*

Main category: cs.LG

TL;DR: 提出了一种基于非对称拉普拉斯分布（ALD）的参数化生存分析方法，优于现有参数和非参数方法。


<details>
  <summary>Details</summary>
Motivation: 现有生存分析模型多采用非参数方法，但缺乏对事件分布的直接估计，因此提出一种参数化方法以更准确地计算事件摘要。

Method: 利用非对称拉普拉斯分布（ALD）进行参数化建模，通过最大似然估计学习个体水平的分布参数（位置、尺度和不对称性）。

Result: 在合成和真实数据上的实验表明，该方法在准确性、区分度和校准方面优于现有参数和非参数方法。

Conclusion: 基于ALD的参数化生存分析方法是一种高效且准确的替代方案，适用于多种事件摘要的计算。

Abstract: Probabilistic survival analysis models seek to estimate the distribution of
the future occurrence (time) of an event given a set of covariates. In recent
years, these models have preferred nonparametric specifications that avoid
directly estimating survival distributions via discretization. Specifically,
they estimate the probability of an individual event at fixed times or the time
of an event at fixed probabilities (quantiles), using supervised learning.
Borrowing ideas from the quantile regression literature, we propose a
parametric survival analysis method based on the Asymmetric Laplace
Distribution (ALD). This distribution allows for closed-form calculation of
popular event summaries such as mean, median, mode, variation, and quantiles.
The model is optimized by maximum likelihood to learn, at the individual level,
the parameters (location, scale, and asymmetry) of the ALD distribution.
Extensive results on synthetic and real-world data demonstrate that the
proposed method outperforms parametric and nonparametric approaches in terms of
accuracy, discrimination and calibration.

</details>


### [116] [Sustainable Smart Farm Networks: Enhancing Resilience and Efficiency with Decision Theory-Guided Deep Reinforcement Learning](https://arxiv.org/abs/2505.03721)
*Dian Chen,Zelin Wan,Dong Sam Ha,Jin-Hee Cho*

Main category: cs.LG

TL;DR: 提出了一种基于深度强化学习（DRL）的可持续智能农场网络，结合迁移学习（TL）和决策理论（DT），以优化监控效果和能源效率，并显著减少训练时间。


<details>
  <summary>Details</summary>
Motivation: 太阳能传感器监测系统在农业中至关重要，但其对网络攻击的适应性和动态能源供应的适应性尚未充分研究。

Method: 采用DRL结合TL和DT，设计最优策略以提升监控效果和能源效率，加速学习过程。

Result: 实验证明，DT引导的DRL优于TL增强的DRL模型，性能提升且训练时间减少47.5%。

Conclusion: 该方法有效解决了智能农场网络的监控和能源问题，具有实际应用潜力。

Abstract: Solar sensor-based monitoring systems have become a crucial agricultural
innovation, advancing farm management and animal welfare through integrating
sensor technology, Internet-of-Things, and edge and cloud computing. However,
the resilience of these systems to cyber-attacks and their adaptability to
dynamic and constrained energy supplies remain largely unexplored. To address
these challenges, we propose a sustainable smart farm network designed to
maintain high-quality animal monitoring under various cyber and adversarial
threats, as well as fluctuating energy conditions. Our approach utilizes deep
reinforcement learning (DRL) to devise optimal policies that maximize both
monitoring effectiveness and energy efficiency. To overcome DRL's inherent
challenge of slow convergence, we integrate transfer learning (TL) and decision
theory (DT) to accelerate the learning process. By incorporating DT-guided
strategies, we optimize monitoring quality and energy sustainability,
significantly reducing training time while achieving comparable performance
rewards. Our experimental results prove that DT-guided DRL outperforms
TL-enhanced DRL models, improving system performance and reducing training
runtime by 47.5%.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [117] [Rational Retrieval Acts: Leveraging Pragmatic Reasoning to Improve Sparse Retrieval](https://arxiv.org/abs/2505.03676)
*Arthur Satouf,Gabriel Ben Zenou,Benjamin Piwowarski,Habiboulaye Amadou Boubacar,Pablo Piantanida*

Main category: cs.IR

TL;DR: 论文提出了一种基于Rational Speech Acts（RSA）框架的稀疏神经信息检索方法，通过动态调整文档中词项的权重，显著提升了检索性能。


<details>
  <summary>Details</summary>
Motivation: 现有稀疏神经信息检索方法（以及传统模型如BM25）未充分考虑文档集合中词项权重的复杂交互作用，导致文档表示不够准确。

Method: 将RSA框架（一种语言学方法）应用于信息检索，动态调整词项与文档的交互，考虑数据集中其他文档的影响，优化文档表示。

Result: 实验表明，引入RSA显著提升了多种稀疏检索模型的性能，并在BEIR基准测试的外域数据集上达到了最先进水平。

Conclusion: RSA框架能有效改进稀疏检索模型的性能，尤其是在处理复杂文档集合时表现突出。

Abstract: Current sparse neural information retrieval (IR) methods, and to a lesser
extent more traditional models such as BM25, do not take into account the
document collection and the complex interplay between different term weights
when representing a single document. In this paper, we show how the Rational
Speech Acts (RSA), a linguistics framework used to minimize the number of
features to be communicated when identifying an object in a set, can be adapted
to the IR case -- and in particular to the high number of potential features
(here, tokens). RSA dynamically modulates token-document interactions by
considering the influence of other documents in the dataset, better contrasting
document representations. Experiments show that incorporating RSA consistently
improves multiple sparse retrieval models and achieves state-of-the-art
performance on out-of-domain datasets from the BEIR benchmark.
https://github.com/arthur-75/Rational-Retrieval-Acts

</details>


### [118] [Feature Staleness Aware Incremental Learning for CTR Prediction](https://arxiv.org/abs/2505.02844)
*Zhikai Wang,Yanyan Shen,Zibin Zhang,Kangyi Lin*

Main category: cs.IR

TL;DR: 论文提出了一种名为FeSAIL的方法，用于解决CTR预测模型中的特征陈旧问题，通过自适应重放陈旧特征样本和正则化机制提升性能。


<details>
  <summary>Details</summary>
Motivation: 现实推荐系统中，CTR预测模型在增量更新时，未出现在当前增量数据中的特征嵌入会变得陈旧，导致性能下降。

Method: 提出FeSAIL方法，包括基于陈旧感知的采样算法（SAS）和正则化机制（SAR），动态重放陈旧特征样本并精细控制嵌入更新。

Result: 实验结果表明，FeSAIL在四个基准数据集上优于多种现有方法。

Conclusion: FeSAIL有效解决了特征陈旧问题，提升了CTR预测模型的性能。

Abstract: Click-through Rate (CTR) prediction in real-world recommender systems often
deals with billions of user interactions every day. To improve the training
efficiency, it is common to update the CTR prediction model incrementally using
the new incremental data and a subset of historical data. However, the feature
embeddings of a CTR prediction model often get stale when the corresponding
features do not appear in current incremental data. In the next period, the
model would have a performance degradation on samples containing stale
features, which we call the feature staleness problem. To mitigate this
problem, we propose a Feature Staleness Aware Incremental Learning method for
CTR prediction (FeSAIL) which adaptively replays samples containing stale
features. We first introduce a staleness aware sampling algorithm (SAS) to
sample a fixed number of stale samples with high sampling efficiency. We then
introduce a staleness aware regularization mechanism (SAR) for a fine-grained
control of the feature embedding updating. We instantiate FeSAIL with a general
deep learning-based CTR prediction model and the experimental results
demonstrate FeSAIL outperforms various state-of-the-art methods on four
benchmark datasets.

</details>


### [119] [Avoid Recommending Out-of-Domain Items: Constrained Generative Recommendation with LLMs](https://arxiv.org/abs/2505.03336)
*Hao Liao,Wensheng Lu,Jianxun Lian,Mingqi Wu,Shuo Wang,Yong Zhang,Yitian Huang,Mingyang Zhou,Xing Xie*

Main category: cs.IR

TL;DR: 论文研究了两种方法（RecLM-ret和RecLM-cgen）来解决LLM推荐系统中的OOD问题，其中RecLM-cgen表现更优。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在生成推荐系统中可能推荐OOD物品的挑战。

Method: 提出两种方法：基于检索的RecLM-ret和基于约束生成的RecLM-cgen。

Result: RecLM-cgen在准确性和消除OOD推荐方面优于RecLM-ret和其他LLM推荐模型。

Conclusion: RecLM-cgen是更优的选择，具有轻量化和易集成的特点，适合实际应用。

Abstract: Large Language Models (LLMs) have shown promise for generative recommender
systems due to their transformative capabilities in user interaction. However,
ensuring they do not recommend out-of-domain (OOD) items remains a challenge.
We study two distinct methods to address this issue: RecLM-ret, a
retrieval-based method, and RecLM-cgen, a constrained generation method. Both
methods integrate seamlessly with existing LLMs to ensure in-domain
recommendations. Comprehensive experiments on three recommendation datasets
demonstrate that RecLM-cgen consistently outperforms RecLM-ret and existing
LLM-based recommender models in accuracy while eliminating OOD recommendations,
making it the preferred method for adoption. Additionally, RecLM-cgen maintains
strong generalist capabilities and is a lightweight plug-and-play module for
easy integration into LLMs, offering valuable practical benefits for the
community. Source code is available at https://github.com/microsoft/RecAI

</details>


### [120] [Modeling Musical Genre Trajectories through Pathlet Learning](https://arxiv.org/abs/2505.03480)
*Lilian Marey,Charlotte Laclau,Bruno Sguerra,Tiphaine Viard,Manuel Moussallam*

Main category: cs.IR

TL;DR: 本文提出了一种基于字典学习的框架，用于分析用户在音乐流媒体平台上的音乐偏好演变，通过定义“pathlets”捕捉重复的流派轨迹模式。


<details>
  <summary>Details</summary>
Motivation: 随着音乐流媒体平台上用户数据的增加，分析音乐消费行为成为可能，但用户偏好的动态变化仍是一个复杂挑战。

Method: 采用字典学习范式，定义新框架捕捉流派轨迹中的重复模式（pathlets），生成可理解的轨迹嵌入。

Result: pathlet学习揭示了可定性和定量分析的相关收听模式，并发布了Deezer提供的2000名用户17个月的流派标记数据。

Conclusion: 该研究提升了对用户音乐交互的理解，为推荐系统用户行为研究和多样性促进开辟了新途径。

Abstract: The increasing availability of user data on music streaming platforms opens
up new possibilities for analyzing music consumption. However, understanding
the evolution of user preferences remains a complex challenge, particularly as
their musical tastes change over time. This paper uses the dictionary learning
paradigm to model user trajectories across different musical genres. We define
a new framework that captures recurring patterns in genre trajectories, called
pathlets, enabling the creation of comprehensible trajectory embeddings. We
show that pathlet learning reveals relevant listening patterns that can be
analyzed both qualitatively and quantitatively. This work improves our
understanding of users' interactions with music and opens up avenues of
research into user behavior and fostering diversity in recommender systems. A
dataset of 2000 user histories tagged by genre over 17 months, supplied by
Deezer (a leading music streaming company), is also released with the code.

</details>


### [121] [Counterfactual Inference for Eliminating Sentiment Bias in Recommender Systems](https://arxiv.org/abs/2505.03655)
*Le Pan,Yuanjiang Cao,Chengkai Huang,Wenjie Zhang,Lina Yao*

Main category: cs.IR

TL;DR: 论文提出了一种基于反事实推理的方法，用于缓解推荐系统中的情感偏差问题，通过建模情感对评分的影响并解耦直接和间接效应，实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 研究发现情感偏差会导致负面评论的用户或物品推荐准确性下降，损害关键用户和小众物品的利益，因此需要解决这一问题。

Method: 采用两阶段反事实推理方法：训练阶段构建因果图建模情感对评分的影响；推理阶段解耦直接和间接效应以减轻情感偏差。

Result: 实验结果表明，该方法在评分预测和情感偏差缓解方面表现优异，是首个在推荐系统中应用反事实推理解决情感偏差的工作。

Conclusion: 通过反事实推理方法，论文成功缓解了推荐系统中的情感偏差，为未来研究提供了新思路。

Abstract: Recommender Systems (RSs) aim to provide personalized recommendations for
users. A newly discovered bias, known as sentiment bias, uncovers a common
phenomenon within Review-based RSs (RRSs): the recommendation accuracy of users
or items with negative reviews deteriorates compared with users or items with
positive reviews. Critical users and niche items are disadvantaged by such
unfair recommendations. We study this problem from the perspective of
counterfactual inference with two stages. At the model training stage, we build
a causal graph and model how sentiment influences the final rating score.
During the inference stage, we decouple the direct and indirect effects to
mitigate the impact of sentiment bias and remove the indirect effect using
counterfactual inference. We have conducted extensive experiments, and the
results validate that our model can achieve comparable performance on rating
prediction for better recommendations and effective mitigation of sentiment
bias. To the best of our knowledge, this is the first work to employ
counterfactual inference on sentiment bias mitigation in RSs.

</details>


<div id='physics.soc-ph'></div>

# physics.soc-ph [[Back]](#toc)

### [122] [Taskmaster Deconstructed: A Quantitative Look at Tension, Volatility, and Viewer Ratings](https://arxiv.org/abs/2505.02886)
*David H. Silver*

Main category: physics.soc-ph

TL;DR: 研究发现，Taskmaster节目的评分动态与观众参与度无显著关联，观众兴趣更多由选手行为而非游戏机制驱动。


<details>
  <summary>Details</summary>
Motivation: 探讨Taskmaster节目中评分动态是否真正影响观众参与度。

Method: 对18季162集的节目进行统计分析，使用15项指标量化排名波动、分数差距、领先变化和胜者优势。

Result: 评分动态与IMDb评分无显著关联；长期趋势显示平均分数上升，波动性略有下降，排名差距稳定。

Conclusion: 观众兴趣更多由选手行为而非游戏机制决定，节目试图在不打破结构平衡的情况下提升竞争可视性。

Abstract: Taskmaster is a British television show that combines comedic performance
with a formal scoring system. Despite the appearance of structured competition,
it remains unclear whether scoring dynamics contribute meaningfully to audience
engagement. We conducted a statistical analysis of 162 episodes across 18
series, using fifteen episode-level metrics to quantify rank volatility, point
spread, lead changes, and winner dominance. None of these metrics showed a
significant association with IMDb ratings, even after controlling for series
effects. Long-term trends suggest that average points have increased over time,
while volatility has slightly declined and rank spread has remained stable.
These patterns indicate an attempt to enhance competitive visibility without
altering the show's structural equilibrium. We also analyzed contestant rank
trajectories and identified five recurring archetypes describing performance
styles. These patterns suggest that viewer interest is shaped more by
contestant behavior than by game mechanics.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [123] [An Active Inference perspective on Neurofeedback Training](https://arxiv.org/abs/2505.03308)
*Côme Annicchiarico,Fabien Lotte,Jérémie Mattout*

Main category: q-bio.NC

TL;DR: 论文提出了一种基于主动推理的计算模型，用于模拟神经反馈训练（NFT）的闭环系统，以解决NFT效果差异大和机制不明确的问题。


<details>
  <summary>Details</summary>
Motivation: NFT的效果差异大且机制不明确，阻碍了其验证和应用。论文旨在通过计算模型解决这些问题。

Method: 使用主动推理（一种贝叶斯框架）模拟代理与NFT环境的交互，测试设计选择和个体因素对训练的影响。

Result: 模拟显示训练效果受反馈噪声、偏差和先验信念的影响，完美反馈不足以保证高性能。

Conclusion: 该模型为评估NFT变异性、解释实验数据及开发个性化训练方案提供了工具。

Abstract: Neurofeedback training (NFT) aims to teach self-regulation of brain activity
through real-time feedback, but suffers from highly variable outcomes and
poorly understood mechanisms, hampering its validation. To address these
issues, we propose a formal computational model of the NFT closed loop. Using
Active Inference, a Bayesian framework modelling perception, action, and
learning, we simulate agents interacting with an NFT environment. This enables
us to test the impact of design choices (e.g., feedback quality, biomarker
validity) and subject factors (e.g., prior beliefs) on training. Simulations
show that training effectiveness is sensitive to feedback noise or bias, and to
prior beliefs (highlighting the importance of guiding instructions), but also
reveal that perfect feedback is insufficient to guarantee high performance.
This approach provides a tool for assessing and predicting NFT variability,
interpret empirical data, and potentially develop personalized training
protocols.

</details>


### [124] [Binding threshold units with artificial oscillatory neurons](https://arxiv.org/abs/2505.03648)
*Vladimir Fanaskov,Ivan Oseledets*

Main category: q-bio.NC

TL;DR: 论文提出了一种理论框架，区分振荡神经元与阈值单元，并建立了它们的耦合机制，结合了Hopfield联想记忆模型和广义Kuramoto模型。


<details>
  <summary>Details</summary>
Motivation: 从生物学角度区分振荡神经元和阈值单元的功能，探索它们在信息编码中的不同作用，并建立耦合机制以提升性能。

Method: 通过约束动力学系统（具有Lyapunov函数）推导耦合机制，结合Hopfield模型和广义Kuramoto模型，提出Hopfield-Kuramoto联想记忆模型。

Result: 实现了振荡神经元与阈值单元的自然耦合，并通过实验验证了低秩权重矩阵修正的实用性。

Conclusion: 振荡神经元与阈值单元的耦合机制在理论和实验中均表现出潜力，可用于改进联想记忆模型和类似任务。

Abstract: Artificial Kuramoto oscillatory neurons were recently introduced as an
alternative to threshold units. Empirical evidence suggests that oscillatory
units outperform threshold units in several tasks including unsupervised object
discovery and certain reasoning problems. The proposed coupling mechanism for
these oscillatory neurons is heterogeneous, combining a generalized Kuramoto
equation with standard coupling methods used for threshold units. In this
research note, we present a theoretical framework that clearly distinguishes
oscillatory neurons from threshold units and establishes a coupling mechanism
between them. We argue that, from a biological standpoint, oscillatory and
threshold units realise distinct aspects of neural coding: roughly, threshold
units model intensity of neuron firing, while oscillatory units facilitate
information exchange by frequency modulation. To derive interaction between
these two types of units, we constrain their dynamics by focusing on dynamical
systems that admit Lyapunov functions. For threshold units, this leads to
Hopfield associative memory model, and for oscillatory units it yields a
specific form of generalized Kuramoto model. The resulting dynamical systems
can be naturally coupled to form a Hopfield-Kuramoto associative memory model,
which also admits a Lyapunov function. Various forms of coupling are possible.
Notably, oscillatory neurons can be employed to implement a low-rank correction
to the weight matrix of a Hopfield network. This correction can be viewed
either as a form of Hebbian learning or as a popular LoRA method used for
fine-tuning of large language models. We demonstrate the practical realization
of this particular coupling through illustrative toy experiments.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [125] [Snakemaker: Seamlessly transforming ad-hoc analyses into sustainable Snakemake workflows with generative AI](https://arxiv.org/abs/2505.02841)
*Marco Masera,Alessandro Leone,Johannes Köster,Ivan Molineris*

Main category: cs.SE

TL;DR: Snakemaker是一个利用生成式AI将非结构化代码转换为Snakemake工作流的工具，旨在解决生物信息学软件开发的可持续性和可重复性问题。


<details>
  <summary>Details</summary>
Motivation: 生物信息学工具和工作流快速演变，导致难以维护和适应，Snakemaker旨在通过自动化生成高质量工作流来填补这一空白。

Method: Snakemaker通过跟踪终端操作、分析执行模式，将非结构化代码或Ipython Notebook转换为模块化Snakemake工作流，并提供自然语言控制的聊天助手。

Result: Snakemaker能够生成符合最佳实践的高质量Snakemake工作流，包括Conda环境跟踪、通用规则生成和循环展开。

Conclusion: Snakemaker降低了从原型代码到生产级代码的障碍，显著提升了生物信息学研究的计算可重复性。

Abstract: Reproducibility and sustainability present significant challenges in
bioinformatics software development, where rapidly evolving tools and complex
workflows often result in short-lived or difficult-to-adapt pipelines. This
paper introduces Snakemaker, a tool that leverages generative AI to facilitate
researchers build sustainable data analysis pipelines by converting
unstructured code into well-defined Snakemake workflows. Snakemaker
non-invasively tracks the work performed in the terminal by the researcher,
analyzes execution patterns, and generates Snakemake workflows that can be
integrated into existing pipelines. Snakemaker also supports the transformation
of monolithic Ipython Notebooks into modular Snakemake pipelines, resolving the
global state of the notebook into discrete, file-based interactions between
rules. An integrated chat assistant provides users with fine-grained control
through natural language instructions. Snakemaker generates high-quality
Snakemake workflows by adhering to the best practices, including Conda
environment tracking, generic rule generation and loop unrolling. By lowering
the barrier between prototype and production-quality code, Snakemaker addresses
a critical gap in computational reproducibility for bioinformatics research.

</details>


### [126] [The Art of Repair: Optimizing Iterative Program Repair with Instruction-Tuned Models](https://arxiv.org/abs/2505.02931)
*Fernando Vallecillos Ruiz,Max Hort,Leon Moonen*

Main category: cs.SE

TL;DR: 研究探讨了自动程序修复（APR）中平衡多输出生成与多轮迭代的策略，使用三种指令调优的LLM模型，并通过微调评估其在两个APR基准上的表现。结果显示，少量微调数据即可显著提升修复效果，但过度微调会导致收益递减。迭代策略对基础模型尤为有益，且复杂基准中效果更明显。


<details>
  <summary>Details</summary>
Motivation: 自动程序修复（APR）旨在减少手动修复代码错误的成本。传统方法依赖生成大量补丁，而LLM的自我迭代能力提供了新方向。然而，现有研究多关注多次迭代而忽视输出数量。本研究旨在平衡这两种策略。

Method: 采用三种指令调优的LLM模型（DeepSeekCoder-Instruct、Codellama-Instruct、Llama3.1-Instruct），在APR任务中结合多输出生成与多轮迭代（限制每错误10个补丁）。通过不同规模（1K、30K、65K）和两种微调技术（Full Fine-Tuning和LoRA）评估模型在两个APR基准（HumanEval-Java和Defects4J）上的表现。

Result: 少量微调数据（<1%）即可将可信补丁生成数量提升78%，但过度微调会导致收益递减。基础模型通过迭代策略显著受益，复杂基准中效果更明显。微调模型虽受益较小，但在复杂任务中仍有优势。

Conclusion: 研究强调了平衡多输出生成与迭代优化的APR策略的重要性，少量微调数据即可显著提升效果，但需避免过度微调。迭代策略对基础模型尤为关键，复杂任务中效果更显著。

Abstract: Automatic program repair (APR) aims to reduce the manual efforts required to
identify and fix errors in source code. Before the rise of LLM-based agents, a
common strategy was to increase the number of generated patches, sometimes to
the thousands, to achieve better repair results on benchmarks. More recently,
self-iterative capabilities enabled LLMs to refine patches over multiple rounds
guided by feedback. However, literature often focuses on many iterations and
disregards different numbers of outputs.
  We investigate an APR pipeline that balances these two approaches, the
generation of multiple outputs and multiple rounds of iteration, while imposing
a limit of 10 total patches per bug. We apply three SOTA instruction-tuned LLMs
- DeepSeekCoder-Instruct, Codellama-Instruct, Llama3.1-Instruct - to the APR
task. We further fine-tune each model on an APR dataset with three sizes (1K,
30K, 65K) and two techniques (Full Fine-Tuning and LoRA), allowing us to assess
their repair capabilities on two APR benchmarks: HumanEval-Java and Defects4J.
  Our results show that by using only a fraction (<1%) of the fine-tuning
dataset, we can achieve improvements of up to 78% in the number of plausible
patches generated, challenging prior studies that reported limited gains using
Full Fine-Tuning. However, we find that exceeding certain thresholds leads to
diminishing outcomes, likely due to overfitting. Moreover, we show that base
models greatly benefit from creating patches in an iterative fashion rather
than generating them all at once. In addition, the benefit of iterative
strategies becomes more pronounced in complex benchmarks. Even fine-tuned
models, while benefiting less from iterations, still gain advantages,
particularly on complex benchmarks. The research underscores the need for
balanced APR strategies that combine multi-output generation and iterative
refinement.

</details>


### [127] [DocSpiral: A Platform for Integrated Assistive Document Annotation through Human-in-the-Spiral](https://arxiv.org/abs/2505.03214)
*Qiang Sun,Sirui Li,Tingting Bi,Du Huynh,Mark Reynolds,Yuanyi Luo,Wei Liu*

Main category: cs.SE

TL;DR: DocSpiral是一个辅助文档标注平台，通过迭代循环减少人工干预，显著降低标注时间并提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决从图像文档中提取结构化数据的挑战，减少人工标注需求。

Method: 采用螺旋设计，结合文档格式标准化、标注界面、评估指标和API端点，形成统一工作流。

Result: 实验显示标注时间减少至少41%，模型性能持续提升。

Conclusion: DocSpiral降低了AI/ML模型开发的壁垒，促进图像文档处理领域的应用。

Abstract: Acquiring structured data from domain-specific, image-based documents such as
scanned reports is crucial for many downstream tasks but remains challenging
due to document variability. Many of these documents exist as images rather
than as machine-readable text, which requires human annotation to train
automated extraction systems. We present DocSpiral, the first
Human-in-the-Spiral assistive document annotation platform, designed to address
the challenge of extracting structured information from domain-specific,
image-based document collections. Our spiral design establishes an iterative
cycle in which human annotations train models that progressively require less
manual intervention. DocSpiral integrates document format normalization,
comprehensive annotation interfaces, evaluation metrics dashboard, and API
endpoints for the development of AI / ML models into a unified workflow.
Experiments demonstrate that our framework reduces annotation time by at least
41\% while showing consistent performance gains across three iterations during
model training. By making this annotation platform freely accessible, we aim to
lower barriers to AI/ML models development in document processing, facilitating
the adoption of large language models in image-based, document-intensive fields
such as geoscience and healthcare. The system is freely available at:
https://app.ai4wa.com. The demonstration video is available:
https://app.ai4wa.com/docs/docspiral/demo.

</details>


### [128] [Synthline: A Product Line Approach for Synthetic Requirements Engineering Data Generation using Large Language Models](https://arxiv.org/abs/2505.03265)
*Abdelkarim El-Hajjami,Camille Salinesi*

Main category: cs.SE

TL;DR: Synthline利用大语言模型生成合成需求工程数据，弥补高质量数据稀缺问题。实证表明合成数据虽多样性不足，但可作为有效训练资源，且与真实数据结合能显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现代需求工程依赖自然语言处理和机器学习，但高质量数据稀缺限制了其效果。

Method: 提出Synthline，一种基于产品线的方法，利用大语言模型生成合成数据，并通过实证评估其多样性和实用性。

Result: 合成数据多样性低于真实数据，但可作为训练资源；结合真实数据后，模型性能显著提升（精确度提高85%，召回率翻倍）。

Conclusion: Synthline为解决需求工程中的数据稀缺问题提供了潜力，相关实现和数据集已公开以支持研究。

Abstract: While modern Requirements Engineering (RE) heavily relies on natural language
processing and Machine Learning (ML) techniques, their effectiveness is limited
by the scarcity of high-quality datasets. This paper introduces Synthline, a
Product Line (PL) approach that leverages Large Language Models to
systematically generate synthetic RE data for classification-based use cases.
Through an empirical evaluation conducted in the context of using ML for the
identification of requirements specification defects, we investigated both the
diversity of the generated data and its utility for training downstream models.
Our analysis reveals that while synthetic datasets exhibit less diversity than
real data, they are good enough to serve as viable training resources.
Moreover, our evaluation shows that combining synthetic and real data leads to
substantial performance improvements. Specifically, hybrid approaches achieve
up to 85% improvement in precision and a 2x increase in recall compared to
models trained exclusively on real data. These findings demonstrate the
potential of PL-based synthetic data generation to address data scarcity in RE.
We make both our implementation and generated datasets publicly available to
support reproducibility and advancement in the field.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [129] [The Precautionary Principle and the Innovation Principle: Incompatible Guides for AI Innovation Governance?](https://arxiv.org/abs/2505.02846)
*Kim Kaivanto*

Main category: cs.CY

TL;DR: 论文探讨了AI治理中的预防原则（PP）和创新原则（IP），认为在弱形式下两者并非完全对立，而是通过信号检测理论（SDT）模型找到最优监管策略。


<details>
  <summary>Details</summary>
Motivation: 研究目的是解决AI治理中PP和IP是否必然对立的问题，探索如何在监管中平衡创新与风险。

Method: 采用信号检测理论（SDT）模型，分析弱形式PP和IP在不同预期成本比例下的最优监管策略，并提出监管沙盒作为中间方案。

Result: 研究发现，弱形式PP和IP可以共存，最优策略取决于预期成本比例；监管沙盒有助于动态调整监管与创新。

Conclusion: 弱形式PP和IP并非对立，监管沙盒是实现动态平衡的有效工具。

Abstract: In policy debates concerning the governance and regulation of Artificial
Intelligence (AI), both the Precautionary Principle (PP) and the Innovation
Principle (IP) are advocated by their respective interest groups. Do these
principles offer wholly incompatible and contradictory guidance? Does one
necessarily negate the other? I argue here that provided attention is
restricted to weak-form PP and IP, the answer to both of these questions is
"No." The essence of these weak formulations is the requirement to fully
account for type-I error costs arising from erroneously preventing the
innovation's diffusion through society (i.e. mistaken regulatory red-lighting)
as well as the type-II error costs arising from erroneously allowing the
innovation to diffuse through society (i.e. mistaken regulatory
green-lighting). Within the Signal Detection Theory (SDT) model developed here,
weak-PP red-light (weak-IP green-light) determinations are optimal for
sufficiently small (large) ratios of expected type-I to type-II error costs.
For intermediate expected cost ratios, an amber-light 'wait-and-monitor' policy
is optimal. Regulatory sandbox instruments allow AI testing and experimentation
to take place within a structured environment of limited duration and societal
scale, whereby the expected cost ratio falls within the 'wait-and-monitor'
range. Through sandboxing regulators and innovating firms learn more about the
expected cost ratio, and what respective adaptations -- of regulation, of
technical solution, of business model, or combination thereof, if any -- are
needed to keep the ratio out of the weak-PP red-light zone.

</details>


### [130] [Aligning Large Language Models with Healthcare Stakeholders: A Pathway to Trustworthy AI Integration](https://arxiv.org/abs/2505.02848)
*Kexin Ding,Mu Zhou,Akshay Chaudhari,Shaoting Zhang,Dimitris N. Metaxas*

Main category: cs.CY

TL;DR: 论文探讨了大型语言模型（LLMs）在医疗领域的应用，强调模型输出与医疗利益相关者需求的对齐是确保工作流程高效、安全、负责任的关键。


<details>
  <summary>Details</summary>
Motivation: LLMs的行为可能与医疗利益相关者的知识、需求和价值观不一致，因此需要人类参与以促进人机对齐。

Method: 通过医疗利益相关者参与LLMs的整个生命周期（包括数据整理、模型训练和推理），结合知识整合、任务理解和人类指导，实现对齐。

Result: 研究表明，LLMs可以通过增强医疗知识整合和人类指导更好地遵循人类价值观。

Conclusion: 未来需进一步优化人机对齐，以构建可信赖的医疗应用。

Abstract: The wide exploration of large language models (LLMs) raises the awareness of
alignment between healthcare stakeholder preferences and model outputs. This
alignment becomes a crucial foundation to empower the healthcare workflow
effectively, safely, and responsibly. Yet the varying behaviors of LLMs may not
always match with healthcare stakeholders' knowledge, demands, and values. To
enable a human-AI alignment, healthcare stakeholders will need to perform
essential roles in guiding and enhancing the performance of LLMs. Human
professionals must participate in the entire life cycle of adopting LLM in
healthcare, including training data curation, model training, and inference. In
this review, we discuss the approaches, tools, and applications of alignments
between healthcare stakeholders and LLMs. We demonstrate that LLMs can better
follow human values by properly enhancing healthcare knowledge integration,
task understanding, and human guidance. We provide outlooks on enhancing the
alignment between humans and LLMs to build trustworthy real-world healthcare
applications.

</details>


### [131] [Enhancing tutoring systems by leveraging tailored promptings and domain knowledge with Large Language Models](https://arxiv.org/abs/2505.02849)
*Mohsen Balavar,Wenli Yang,David Herbert,Soonja Yeom*

Main category: cs.CY

TL;DR: 论文探讨了AI和机器学习在计算机辅助学习（CBL）中的应用，提出了一种基于检索增强生成（RAG）的个性化辅导系统，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管AI工具如ChatGPT和智能辅导系统（ITS）在个性化学习方面取得进展，但仍存在适应多样化学习风格和提供实时反馈的挑战。研究旨在填补这些空白。

Method: 通过将技能对齐反馈整合到大型语言模型（LLMs）的提示工程中，并开发一个应用程序，在计算机科学编程背景下提供个性化辅导。

Result: 实验评估了系统的可读性分数、响应时间和反馈深度，结果显示系统能有效分类学生技能水平并提供针对性反馈。

Conclusion: 该方法在效果和适应性上优于通用方法，展示了其在个性化学习中的潜力。

Abstract: Recent advancements in artificial intelligence (AI) and machine learning have
reignited interest in their impact on Computer-based Learning (CBL). AI-driven
tools like ChatGPT and Intelligent Tutoring Systems (ITS) have enhanced
learning experiences through personalisation and flexibility. ITSs can adapt to
individual learning needs and provide customised feedback based on a student's
performance, cognitive state, and learning path. Despite these advances,
challenges remain in accommodating diverse learning styles and delivering
real-time, context-aware feedback. Our research aims to address these gaps by
integrating skill-aligned feedback via Retrieval Augmented Generation (RAG)
into prompt engineering for Large Language Models (LLMs) and developing an
application to enhance learning through personalised tutoring in a computer
science programming context. The pilot study evaluated a proposed system using
three quantitative metrics: readability score, response time, and feedback
depth, across three programming tasks of varying complexity. The system
successfully sorted simulated students into three skill-level categories and
provided context-aware feedback. This targeted approach demonstrated better
effectiveness and adaptability compared to general methods.

</details>


### [132] [A Computational Model of Inclusive Pedagogy: From Understanding to Application](https://arxiv.org/abs/2505.02853)
*Francesco Balzan,Pedro P. Santos,Maurizio Gabbrielli,Mahault Albarracin,Manuel Lopes*

Main category: cs.CY

TL;DR: 论文提出了一种计算模型，模拟师生互动中的共适应动态，证明双向互动策略优于单向策略，为教育科学和AI教育系统提供了可扩展的框架。


<details>
  <summary>Details</summary>
Motivation: 当前计算模型对师生共适应互动的研究不足，限制了教育科学的测试与扩展能力，以及机器学习系统对人类学习过程的模拟与支持。

Method: 开发了一个计算模型，将人类教育的上下文洞察整合为可测试框架，并在合成课堂环境中评估不同师生互动策略。

Result: 结果显示，基于共适应原则的策略（如双向互动）优于单向策略，能提升所有学习类型的学习效果。

Conclusion: 该模型为教育科学和AI教育系统提供了可扩展的基础，支持动态适应学习者需求的公平技术。

Abstract: Human education transcends mere knowledge transfer, it relies on
co-adaptation dynamics -- the mutual adjustment of teaching and learning
strategies between agents. Despite its centrality, computational models of
co-adaptive teacher-student interactions (T-SI) remain underdeveloped. We argue
that this gap impedes Educational Science in testing and scaling contextual
insights across diverse settings, and limits the potential of Machine Learning
systems, which struggle to emulate and adaptively support human learning
processes. To address this, we present a computational T-SI model that
integrates contextual insights on human education into a testable framework. We
use the model to evaluate diverse T-SI strategies in a realistic synthetic
classroom setting, simulating student groups with unequal access to sensory
information. Results show that strategies incorporating co-adaptation
principles (e.g., bidirectional agency) outperform unilateral approaches (i.e.,
where only the teacher or the student is active), improving the learning
outcomes for all learning types. Beyond the testing and scaling of
context-dependent educational insights, our model enables hypothesis generation
in controlled yet adaptable environments. This work bridges non-computational
theories of human education with scalable, inclusive AI in Education systems,
providing a foundation for equitable technologies that dynamically adapt to
learner needs.

</details>


### [133] [AI Education in a Mirror: Challenges Faced by Academic and Industry Experts](https://arxiv.org/abs/2505.02856)
*Mahir Akgun,Hadi Hosseini*

Main category: cs.CY

TL;DR: 研究探讨了AI教育与实践之间的差距，通过14位专家的访谈，揭示了数据、模型、部署等方面的挑战，并提出了改进AI课程的建议。


<details>
  <summary>Details</summary>
Motivation: 随着AI技术的发展，学术界与工业界之间的教育与实践差距成为重要研究课题。

Method: 通过半结构化访谈14位AI专家（8位来自工业界，6位来自学术界），分析挑战。

Result: 工业界关注部署限制和资源问题，学术界更注重理论适应和标准化。

Conclusion: 建议AI课程应更贴近实际需求，融入软件工程和多学科内容，同时注重基础与伦理教育。

Abstract: As Artificial Intelligence (AI) technologies continue to evolve, the gap
between academic AI education and real-world industry challenges remains an
important area of investigation. This study provides preliminary insights into
challenges AI professionals encounter in both academia and industry, based on
semi-structured interviews with 14 AI experts - eight from industry and six
from academia. We identify key challenges related to data quality and
availability, model scalability, practical constraints, user behavior, and
explainability. While both groups experience data and model adaptation
difficulties, industry professionals more frequently highlight deployment
constraints, resource limitations, and external dependencies, whereas academics
emphasize theoretical adaptation and standardization issues. These exploratory
findings suggest that AI curricula could better integrate real-world
complexities, software engineering principles, and interdisciplinary learning,
while recognizing the broader educational goals of building foundational and
ethical reasoning skills.

</details>


### [134] [Understanding University Students' Use of Generative AI: The Roles of Demographics and Personality Traits](https://arxiv.org/abs/2505.02863)
*Newnew Deng,Edward Jiusi Liu,Xiaoming Zhai*

Main category: cs.CY

TL;DR: 研究发现，高年级学生、非英语母语者、亚裔和黑人学生更倾向于使用生成式AI（GAI），而性格特质（如责任心、宜人性等）也显著影响GAI的使用态度。


<details>
  <summary>Details</summary>
Motivation: 填补关于大学生使用GAI及其影响因素的实证研究空白。

Method: 对美国363名本科生和研究生的GAI使用情况、人口统计变量及大五人格特质进行调查。

Result: 高年级学生、非英语母语者、亚裔和黑人学生更倾向使用GAI；性格特质（如责任心低、外向性高）显著影响GAI的使用和态度。

Conclusion: 大学需提供个性化指导，确保学生有效、道德且公平地使用GAI。

Abstract: The use of generative AI (GAI) among university students is rapidly
increasing, yet empirical research on students' GAI use and the factors
influencing it remains limited. To address this gap, we surveyed 363
undergraduate and graduate students in the United States, examining their GAI
usage and how it relates to demographic variables and personality traits based
on the Big Five model (i.e., extraversion, agreeableness, conscientiousness,
and emotional stability, and intellect/imagination). Our findings reveal: (a)
Students in higher academic years are more inclined to use GAI and prefer it
over traditional resources. (b) Non-native English speakers use and adopt GAI
more readily than native speakers. (c) Compared to White, Asian students report
higher GAI usage, perceive greater academic benefits, and express a stronger
preference for it. Similarly, Black students report a more positive impact of
GAI on their academic performance. Personality traits also play a significant
role in shaping perceptions and usage of GAI. After controlling demographic
factors, we found that personality still significantly predicts GAI use and
attitudes: (a) Students with higher conscientiousness use GAI less. (b)
Students who are higher in agreeableness perceive a less positive impact of GAI
on academic performance and express more ethical concerns about using it for
academic work. (c) Students with higher emotional stability report a more
positive impact of GAI on learning and fewer concerns about its academic use.
(d) Students with higher extraversion show a stronger preference for GAI over
traditional resources. (e) Students with higher intellect/imagination tend to
prefer traditional resources. These insights highlight the need for
universities to provide personalized guidance to ensure students use GAI
effectively, ethically, and equitably in their academic pursuits.

</details>


### [135] [The Cognitive Foundations of Economic Exchange: A Modular Framework Grounded in Behavioral Evidence](https://arxiv.org/abs/2505.02945)
*Egil Diau*

Main category: cs.CY

TL;DR: 论文提出了一种基于认知最小机制的框架，用于在多智能体AI中建模社会合作，将信任重新定义为一种可模拟的认知期望。


<details>
  <summary>Details</summary>
Motivation: 解决多智能体AI中社会合作建模的挑战，尤其是经济学和伦理学中‘信任’或‘道德’等概念缺乏操作性定义和认知基础的问题。

Method: 基于灵长类行为、婴儿认知和经济人类学的实证证据，提出了一个由个体识别、互相信任和成本回报敏感性三个认知最小机制组成的框架。

Result: 该框架为人工代理中的互惠交换提供了可模拟的基础，支持自下而上涌现的可扩展合作和制度动态。

Conclusion: 通过将信任定义为认知期望，该框架为多智能体系统中的社会合作提供了新的理论基础和实现路径。

Abstract: A key challenge in multi-agent AI is modeling social cooperation under
realistic behavioral constraints. Many foundational concepts in economics and
ethics such as "trust" or "morality" are often defined informally, without
operational criteria or cognitive grounding, which limits their testability and
implementation in artificial agents. Drawing on converging empirical evidence
from primate behavior, infant cognition, and economic anthropology, we propose
a conceptual framework composed of three cognitively minimal mechanisms:
individual recognition, reciprocal credence, and cost return sensitivity. This
framework reframes trust as a graded cognitive expectation, providing a
simulateable basis for reciprocal exchange in artificial agents, and enabling
the bottom-up emergence of scalable cooperation and institutional dynamics.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [136] [GeoERM: Geometry-Aware Multi-Task Representation Learning on Riemannian Manifolds](https://arxiv.org/abs/2505.02972)
*Aoran Chen,Yang Feng*

Main category: stat.ML

TL;DR: GeoERM是一种几何感知的多任务学习框架，通过将共享表示嵌入到其自然的黎曼流形上，优化了多任务学习的鲁棒性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽略了潜在表示矩阵的非欧几里得几何特性，导致在任务异构或对抗性情况下鲁棒性不足。

Method: GeoERM通过黎曼梯度步和极坐标回缩操作，在流形上优化表示矩阵，保持几何保真度。

Result: 在合成实验和可穿戴传感器活动识别基准测试中，GeoERM显著提高了估计准确性，减少了负迁移，并在对抗性标签噪声下保持稳定。

Conclusion: GeoERM在多任务学习中表现出优越性能，尤其是在任务异构和对抗性环境下。

Abstract: Multi-Task Learning (MTL) seeks to boost statistical power and learning
efficiency by discovering structure shared across related tasks.
State-of-the-art MTL representation methods, however, usually treat the latent
representation matrix as a point in ordinary Euclidean space, ignoring its
often non-Euclidean geometry, thus sacrificing robustness when tasks are
heterogeneous or even adversarial. We propose GeoERM, a geometry-aware MTL
framework that embeds the shared representation on its natural Riemannian
manifold and optimizes it via explicit manifold operations. Each training cycle
performs (i) a Riemannian gradient step that respects the intrinsic curvature
of the search space, followed by (ii) an efficient polar retraction to remain
on the manifold, guaranteeing geometric fidelity at every iteration. The
procedure applies to a broad class of matrix-factorized MTL models and retains
the same per-iteration cost as Euclidean baselines. Across a set of synthetic
experiments with task heterogeneity and on a wearable-sensor
activity-recognition benchmark, GeoERM consistently improves estimation
accuracy, reduces negative transfer, and remains stable under adversarial label
noise, outperforming leading MTL and single-task alternatives.

</details>


### [137] [Modeling Spatial Extremes using Non-Gaussian Spatial Autoregressive Models via Convolutional Neural Networks](https://arxiv.org/abs/2505.03034)
*Sweta Rai,Douglas W. Nychka,Soutir Bandyopadhyay*

Main category: stat.ML

TL;DR: 提出了一种空间自回归建模框架，结合广义极值分布创新，用于处理非高斯场和极端空间行为，并通过卷积神经网络快速估计参数。


<details>
  <summary>Details</summary>
Motivation: 解决大体积网格数据中空间异质性和重尾分布导致的建模困难，特别是填补缺失网格单元和模拟极端空间行为的需求。

Method: 采用空间自回归模型（SAR）结合广义极值分布创新，利用卷积神经网络进行参数估计。

Result: 模型能够有效捕捉极端空间行为，并通过神经网络实现快速参数估计。

Conclusion: 该框架为处理非高斯场和极端空间行为提供了灵活且高效的解决方案，适用于气象数据分析。

Abstract: Data derived from remote sensing or numerical simulations often have a
regular gridded structure and are large in volume, making it challenging to
find accurate spatial models that can fill in missing grid cells or simulate
the process effectively, especially in the presence of spatial heterogeneity
and heavy-tailed marginal distributions. To overcome this issue, we present a
spatial autoregressive modeling framework, which maps observations at a
location and its neighbors to independent random variables. This is a highly
flexible modeling approach and well-suited for non-Gaussian fields, providing
simpler interpretability. In particular, we consider the SAR model with
Generalized Extreme Value distribution innovations to combine the observation
at a central grid location with its neighbors, capturing extreme spatial
behavior based on the heavy-tailed innovations. While these models are fast to
simulate by exploiting the sparsity of the key matrices in the computations,
the maximum likelihood estimation of the parameters is prohibitive due to the
intractability of the likelihood, making optimization challenging. To overcome
this, we train a convolutional neural network on a large training set that
covers a useful parameter space, and then use the trained network for fast
parameter estimation. Finally, we apply this model to analyze annual maximum
precipitation data from ERA-Interim-driven Weather Research and Forecasting
(WRF) simulations, allowing us to explore its spatial extreme behavior across
North America.

</details>


### [138] [A Symbolic and Statistical Learning Framework to Discover Bioprocessing Regulatory Mechanism: Cell Culture Example](https://arxiv.org/abs/2505.03177)
*Keilung Choy,Wei Xie,Keqi Wang*

Main category: stat.ML

TL;DR: 本文提出了一种结合符号与统计学习的框架，用于识别生物过程中的关键调控机制并量化模型不确定性，通过贝叶斯学习和高效算法提升了计算效率和模型选择鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 生物过程机理建模对生物制造的智能数字孪生至关重要，但复杂的细胞内调控、随机系统行为和有限实验数据带来了挑战。

Method: 采用随机微分方程描述生物过程动态，结合生物知识构建候选调控机制集，开发贝叶斯学习方法联合学习动力学参数和调控结构，并利用Metropolis-adjusted Langevin算法提升计算效率。

Result: 与现有贝叶斯推断方法相比，该框架提高了样本效率和模型选择鲁棒性，实证研究显示其能恢复缺失调控机制并在数据有限条件下提升模型保真度。

Conclusion: 该框架为生物过程建模提供了一种高效且鲁棒的方法，尤其在数据有限的情况下表现出色。

Abstract: Bioprocess mechanistic modeling is essential for advancing intelligent
digital twin representation of biomanufacturing, yet challenges persist due to
complex intracellular regulation, stochastic system behavior, and limited
experimental data. This paper introduces a symbolic and statistical learning
framework to identify key regulatory mechanisms and quantify model uncertainty.
Bioprocess dynamics is formulated with stochastic differential equations
characterizing intrinsic process variability, with a predefined set of
candidate regulatory mechanisms constructed from biological knowledge. A
Bayesian learning approach is developed, which is based on a joint learning of
kinetic parameters and regulatory structure through a formulation of the
mixture model. To enhance computational efficiency, a Metropolis-adjusted
Langevin algorithm with adjoint sensitivity analysis is developed for posterior
exploration. Compared to state-of-the-art Bayesian inference approaches, the
proposed framework achieves improved sample efficiency and robust model
selection. An empirical study demonstrates its ability to recover missing
regulatory mechanisms and improve model fidelity under data-limited conditions.

</details>


### [139] [Weighted Average Gradients for Feature Attribution](https://arxiv.org/abs/2505.03201)
*Kien Tran Duc Tuan,Tam Nguyen Trong,Son Nguyen Hoang,Khoat Than,Anh Nguyen Duc*

Main category: stat.ML

TL;DR: 论文提出了一种名为Weighted Average Gradients（WG）的新方法，用于改进解释性AI中的基线选择问题，通过无监督评估基线适用性并选择有效基线，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 传统Expected Gradients（EG）方法假设基线可以均匀采样并等权平均，但作者认为基线不应被等同对待，需要更合理的评估和选择策略。

Method: 提出WG方法，通过无监督方式评估基线适用性，并选择有效基线，同时满足解释方法的基本准则。

Result: 理论分析显示WG比EG更稳定，实验结果表明WG在多种场景下性能优于EG，主要指标提升10-35%，且能降低计算成本。

Conclusion: WG方法通过改进基线选择策略，显著提升了解释性AI的性能和效率，具有实际应用价值。

Abstract: In explainable AI, Integrated Gradients (IG) is a widely adopted technique
for assessing the significance of feature attributes of the input on model
outputs by evaluating contributions from a baseline input to the current input.
The choice of the baseline input significantly influences the resulting
explanation. While the traditional Expected Gradients (EG) method assumes
baselines can be uniformly sampled and averaged with equal weights, this study
argues that baselines should not be treated equivalently. We introduce Weighted
Average Gradients (WG), a novel approach that unsupervisedly evaluates baseline
suitability and incorporates a strategy for selecting effective baselines.
Theoretical analysis demonstrates that WG satisfies essential explanation
method criteria and offers greater stability than prior approaches.
Experimental results further confirm that WG outperforms EG across diverse
scenarios, achieving an improvement of 10-35\% on main metrics. Moreover, by
evaluating baselines, our method can filter a subset of effective baselines for
each input to calculate explanations, maintaining high accuracy while reducing
computational cost. The code is available at:
https://github.com/Tamnt240904/weighted_baseline.

</details>


### [140] [Lower Bounds for Greedy Teaching Set Constructions](https://arxiv.org/abs/2505.03223)
*Spencer Compton,Chirag Pabbaraju,Nikita Zhivotovskiy*

Main category: stat.ML

TL;DR: 论文研究了有限VC维度概念类的最佳教学维度问题，分析了贪婪算法的性能，并针对不同k值提出了上下界。


<details>
  <summary>Details</summary>
Motivation: 解决学习理论中关于有限VC维度概念类最佳教学维度的开放性问题，特别是验证Simon和Zilles提出的递归教学维度猜想。

Method: 使用贪婪算法递归构建教学集，分析其在不同k值下的性能，并证明上下界。

Result: 对于k=1，贪婪算法未改进基于对分的O(log(|C|))界；对于k=2，证明了与上界匹配的下界；对于k≤⌈cd⌉，表明可能需要研究高阶交互。

Conclusion: 研究结果表明，贪婪算法在小k值下性能有限，可能需要更高阶交互来解决TS_min=O(d)的猜想。

Abstract: A fundamental open problem in learning theory is to characterize the
best-case teaching dimension $\operatorname{TS}_{\min}$ of a concept class
$\mathcal{C}$ with finite VC dimension $d$. Resolving this problem will, in
particular, settle the conjectured upper bound on Recursive Teaching Dimension
posed by [Simon and Zilles; COLT 2015]. Prior work used a natural greedy
algorithm to construct teaching sets recursively, thereby proving upper bounds
on $\operatorname{TS}_{\min}$, with the best known bound being $O(d^2)$ [Hu,
Wu, Li, and Wang; COLT 2017]. In each iteration, this greedy algorithm chooses
to add to the teaching set the $k$ labeled points that restrict the concept
class the most. In this work, we prove lower bounds on the performance of this
greedy approach for small $k$. Specifically, we show that for $k = 1$, the
algorithm does not improve upon the halving-based bound of
$O(\log(|\mathcal{C}|))$. Furthermore, for $k = 2$, we complement the upper
bound of $O\left(\log(\log(|\mathcal{C}|))\right)$ from [Moran, Shpilka,
Wigderson, and Yuhudayoff; FOCS 2015] with a matching lower bound. Most
consequentially, our lower bound extends up to $k \le \lceil c d \rceil$ for
small constant $c>0$: suggesting that studying higher-order interactions may be
necessary to resolve the conjecture that $\operatorname{TS}_{\min} = O(d)$.

</details>


### [141] [Decision Making under Model Misspecification: DRO with Robust Bayesian Ambiguity Sets](https://arxiv.org/abs/2505.03585)
*Charita Dellaporta,Patrick O'Hara,Theodoros Damoulas*

Main category: stat.ML

TL;DR: DRO-RoBAS结合鲁棒贝叶斯方法，解决模型误设问题，提升决策性能。


<details>
  <summary>Details</summary>
Motivation: 传统DRO方法在模型误设时过于保守，需要一种能结合模型不确定性和数据生成过程的方法。

Method: 提出DRO-RoBAS，使用最大均值差异模糊集，基于鲁棒后验预测分布，并在再生核希尔伯特空间中求解对偶问题。

Result: 在Nevendor和Portfolio问题上，DRO-RoBAS优于其他贝叶斯和实证DRO方法。

Conclusion: DRO-RoBAS有效应对模型误设，提供更优的样本外性能。

Abstract: Distributionally Robust Optimisation (DRO) protects risk-averse
decision-makers by considering the worst-case risk within an ambiguity set of
distributions based on the empirical distribution or a model. To further guard
against finite, noisy data, model-based approaches admit Bayesian formulations
that propagate uncertainty from the posterior to the decision-making problem.
However, when the model is misspecified, the decision maker must stretch the
ambiguity set to contain the data-generating process (DGP), leading to overly
conservative decisions. We address this challenge by introducing DRO with
Robust, to model misspecification, Bayesian Ambiguity Sets (DRO-RoBAS). These
are Maximum Mean Discrepancy ambiguity sets centred at a robust posterior
predictive distribution that incorporates beliefs about the DGP. We show that
the resulting optimisation problem obtains a dual formulation in the
Reproducing Kernel Hilbert Space and we give probabilistic guarantees on the
tolerance level of the ambiguity set. Our method outperforms other Bayesian and
empirical DRO approaches in out-of-sample performance on the Newsvendor and
Portfolio problems with various cases of model misspecification.

</details>


### [142] [Physics-Informed Sylvester Normalizing Flows for Bayesian Inference in Magnetic Resonance Spectroscopy](https://arxiv.org/abs/2505.03590)
*Julian P. Merkofer,Dennis M. J. van de Sande,Alex A. Bhogal,Ruud J. G. van Sloun*

Main category: stat.ML

TL;DR: 论文提出了一种基于贝叶斯推断和Sylvester归一化流（SNFs）的方法，用于提高磁共振波谱（MRS）中代谢物定量的可靠性。


<details>
  <summary>Details</summary>
Motivation: 磁共振波谱（MRS）在代谢物定量中面临谱重叠、低信噪比等问题，传统方法如线性组合模型存在局限性。

Method: 采用贝叶斯推断框架和Sylvester归一化流（SNFs）近似代谢物浓度的后验分布，并结合物理知识的解码器。

Result: 在模拟7T质子MRS数据上验证，实现了准确的代谢物定量、校准良好的不确定性，并揭示了参数相关性和多模态分布。

Conclusion: 该方法显著提高了MRS代谢物定量的可靠性，为临床和研究提供了更准确的工具。

Abstract: Magnetic resonance spectroscopy (MRS) is a non-invasive technique to measure
the metabolic composition of tissues, offering valuable insights into
neurological disorders, tumor detection, and other metabolic dysfunctions.
However, accurate metabolite quantification is hindered by challenges such as
spectral overlap, low signal-to-noise ratio, and various artifacts. Traditional
methods like linear-combination modeling are susceptible to ambiguities and
commonly only provide a theoretical lower bound on estimation accuracy in the
form of the Cram\'er-Rao bound. This work introduces a Bayesian inference
framework using Sylvester normalizing flows (SNFs) to approximate posterior
distributions over metabolite concentrations, enhancing quantification
reliability. A physics-based decoder incorporates prior knowledge of MRS signal
formation, ensuring realistic distribution representations. We validate the
method on simulated 7T proton MRS data, demonstrating accurate metabolite
quantification, well-calibrated uncertainties, and insights into parameter
correlations and multi-modal distributions.

</details>


### [143] [Weighted Random Dot Product Graphs](https://arxiv.org/abs/2505.03649)
*Bernardo Marenco,Paola Bermolen,Marcelo Fiori,Federico Larroca,Gonzalo Mateos*

Main category: stat.ML

TL;DR: 本文扩展了随机点积图（RDPG）模型，提出了一种非参数加权（W)RDPG模型，适用于具有异质权重分布的加权图，并提供了统计保证和生成框架。


<details>
  <summary>Details</summary>
Motivation: 网络数据中的复杂关系模式分析是现代统计研究和数据科学的核心。现有模型难以区分具有相同均值但高阶矩不同的权重分布，因此需要扩展RDPG模型以支持加权图。

Method: 提出WRDPG模型，为每个节点分配潜在位置序列，通过节点向量的内积指定边权重分布的矩生成函数。基于邻接谱嵌入的估计器提供了统计保证。

Result: 证明了节点潜在位置估计器的一致性和渐近正态性，并提供了生成符合WRDPG的图的框架。

Conclusion: WRDPG模型扩展了RDPG的适用范围，能够区分不同高阶矩的权重分布，为网络分析提供了新的工具和方法。

Abstract: Modeling of intricate relational patterns % through the analysis structures
of network data has become a cornerstone of contemporary statistical research
and related data science fields. Networks, represented as graphs, offer a
natural framework for this analysis. This paper extends the Random Dot Product
Graph (RDPG) model to accommodate weighted graphs, markedly broadening the
model's scope to scenarios where edges exhibit heterogeneous weight
distributions. We propose a nonparametric weighted (W)RDPG model that assigns a
sequence of latent positions to each node. Inner products of these nodal
vectors specify the moments of their incident edge weights' distribution via
moment-generating functions. In this way, and unlike prior art, the WRDPG can
discriminate between weight distributions that share the same mean but differ
in other higher-order moments. We derive statistical guarantees for an
estimator of the nodal's latent positions adapted from the workhorse adjacency
spectral embedding, establishing its consistency and asymptotic normality. We
also contribute a generative framework that enables sampling of graphs that
adhere to a (prescribed or data-fitted) WRDPG, facilitating, e.g., the analysis
and testing of observed graph metrics using judicious reference distributions.
The paper is organized to formalize the model's definition, the estimation (or
nodal embedding) process and its guarantees, as well as the methodologies for
generating weighted graphs, all complemented by illustrative and reproducible
examples showcasing the WRDPG's effectiveness in various network analytic
applications.

</details>


### [144] [Multi-modal cascade feature transfer for polymer property prediction](https://arxiv.org/abs/2505.03704)
*Kiichi Obuchi,Yuta Yahagi,Kiyohiko Toyama,Shukichi Tanaka,Kota Matsui*

Main category: stat.ML

TL;DR: 提出了一种多模态级联模型，结合图卷积神经网络（GCN）提取的化学结构特征与分子描述符等信息，用于聚合物性能预测，性能优于传统单特征方法。


<details>
  <summary>Details</summary>
Motivation: 聚合物数据通常包含多种格式（如分子描述符、化学结构等），传统方法单独处理每种数据，限制了预测准确性。

Method: 结合GCN提取的化学结构特征与其他数据（如分子描述符、添加剂信息），构建多模态级联模型。

Result: 在多个聚合物数据集上验证，预测性能显著优于传统单特征方法。

Conclusion: 多模态特征融合方法能有效提升聚合物性能预测的准确性。

Abstract: In this paper, we propose a novel transfer learning approach called
multi-modal cascade model with feature transfer for polymer property
prediction.Polymers are characterized by a composite of data in several
different formats, including molecular descriptors and additive information as
well as chemical structures. However, in conventional approaches, prediction
models were often constructed using each type of data separately. Our model
enables more accurate prediction of physical properties for polymers by
combining features extracted from the chemical structure by graph convolutional
neural networks (GCN) with features such as molecular descriptors and additive
information. The predictive performance of the proposed method is empirically
evaluated using several polymer datasets. We report that the proposed method
shows high predictive performance compared to the baseline conventional
approach using a single feature.

</details>


### [145] [Actor-Critics Can Achieve Optimal Sample Efficiency](https://arxiv.org/abs/2505.03710)
*Kevin Tan,Wei Fan,Yuting Wei*

Main category: stat.ML

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Actor-critic algorithms have become a cornerstone in reinforcement learning
(RL), leveraging the strengths of both policy-based and value-based methods.
Despite recent progress in understanding their statistical efficiency, no
existing work has successfully learned an $\epsilon$-optimal policy with a
sample complexity of $O(1/\epsilon^2)$ trajectories with general function
approximation when strategic exploration is necessary.
  We address this open problem by introducing a novel actor-critic algorithm
that attains a sample-complexity of $O(dH^5 \log|\mathcal{A}|/\epsilon^2 + d
H^4 \log|\mathcal{F}|/ \epsilon^2)$ trajectories, and accompanying $\sqrt{T}$
regret when the Bellman eluder dimension $d$ does not increase with $T$ at more
than a $\log T$ rate.
  Here, $\mathcal{F}$ is the critic function class, $\mathcal{A}$ is the action
space, and $H$ is the horizon in the finite horizon MDP setting. Our algorithm
integrates optimism, off-policy critic estimation targeting the optimal
Q-function, and rare-switching policy resets.
  We extend this to the setting of Hybrid RL, showing that initializing the
critic with offline data yields sample efficiency gains compared to purely
offline or online RL. Further, utilizing access to offline data, we provide a
\textit{non-optimistic} provably efficient actor-critic algorithm that only
additionally requires $N_{\text{off}} \geq c_{\text{off}}^*dH^4/\epsilon^2$ in
exchange for omitting optimism, where $c_{\text{off}}^*$ is the single-policy
concentrability coefficient and $N_{\text{off}}$ is the number of offline
samples. This addresses another open problem in the literature. We further
provide numerical experiments to support our theoretical findings.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [146] [BadLingual: A Novel Lingual-Backdoor Attack against Large Language Models](https://arxiv.org/abs/2505.03501)
*Zihan Wang,Hongwei Li,Rui Zhang,Wenbo Jiang,Kangjie Chen,Tianwei Zhang,Qingchuan Zhao,Guowen Xu*

Main category: cs.CR

TL;DR: 提出了一种针对大型语言模型（LLM）的新型后门攻击——语言后门攻击，利用语言本身作为触发器，使模型生成煽动性言论。通过改进方法BadLingual，提升了攻击的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 揭示多语言LLM的潜在漏洞，尤其是语言作为触发器的攻击方式，以促进防御研究。

Method: 基线攻击通过翻译数据投毒实现，但泛化性差；改进方法BadLingual采用PGCG对抗训练，增强任务无关攻击能力。

Result: 基线攻击在特定任务上ASR达90%，但任务无关场景仅37.61%；BadLingual提升37.35%。

Conclusion: 语言后门攻击暴露了LLM的新漏洞，需加强防御研究以提升模型鲁棒性。

Abstract: In this paper, we present a new form of backdoor attack against Large
Language Models (LLMs): lingual-backdoor attacks. The key novelty of
lingual-backdoor attacks is that the language itself serves as the trigger to
hijack the infected LLMs to generate inflammatory speech. They enable the
precise targeting of a specific language-speaking group, exacerbating racial
discrimination by malicious entities. We first implement a baseline
lingual-backdoor attack, which is carried out by poisoning a set of training
data for specific downstream tasks through translation into the trigger
language. However, this baseline attack suffers from poor task generalization
and is impractical in real-world settings. To address this challenge, we design
BadLingual, a novel task-agnostic lingual-backdoor, capable of triggering any
downstream tasks within the chat LLMs, regardless of the specific questions of
these tasks. We design a new approach using PPL-constrained Greedy Coordinate
Gradient-based Search (PGCG) based adversarial training to expand the decision
boundary of lingual-backdoor, thereby enhancing the generalization ability of
lingual-backdoor across various tasks. We perform extensive experiments to
validate the effectiveness of our proposed attacks. Specifically, the baseline
attack achieves an ASR of over 90% on the specified tasks. However, its ASR
reaches only 37.61% across six tasks in the task-agnostic scenario. In
contrast, BadLingual brings up to 37.35% improvement over the baseline. Our
study sheds light on a new perspective of vulnerabilities in LLMs with
multilingual capabilities and is expected to promote future research on the
potential defenses to enhance the LLMs' robustness

</details>


### [147] [Adversarial Sample Generation for Anomaly Detection in Industrial Control Systems](https://arxiv.org/abs/2505.03120)
*Abdul Mustafa,Muhammad Talha Khan,Muhammad Azmi Umer,Zaki Masood,Chuadhry Mujeeb Ahmed*

Main category: cs.CR

TL;DR: 论文研究了机器学习入侵检测系统对对抗攻击的脆弱性，提出使用JSMA生成对抗样本，并在工业控制系统中验证其泛化性和扩展性。


<details>
  <summary>Details</summary>
Motivation: 由于ML-based IDS易受对抗攻击，研究如何识别和防御这些攻击对保护系统安全至关重要。

Method: 使用Jacobian Saliency Map Attack (JSMA)生成对抗样本，并在工业控制系统(ICS)中验证其泛化性和扩展性。

Result: 在未用于训练的真实攻击数据上，模型检测攻击的准确率达到95%。

Conclusion: 研究表明，通过对抗样本训练的模型能有效检测真实攻击，为工业控制系统安全提供了新方法。

Abstract: Machine learning (ML)-based intrusion detection systems (IDS) are vulnerable
to adversarial attacks. It is crucial for an IDS to learn to recognize
adversarial examples before malicious entities exploit them. In this paper, we
generated adversarial samples using the Jacobian Saliency Map Attack (JSMA). We
validate the generalization and scalability of the adversarial samples to
tackle a broad range of real attacks on Industrial Control Systems (ICS). We
evaluated the impact by assessing multiple attacks generated using the proposed
method. The model trained with adversarial samples detected attacks with 95%
accuracy on real-world attack data not used during training. The study was
conducted using an operational secure water treatment (SWaT) testbed.

</details>


### [148] [Detecting Quishing Attacks with Machine Learning Techniques Through QR Code Analysis](https://arxiv.org/abs/2505.03451)
*Fouad Trad,Ali Chehab*

Main category: cs.CR

TL;DR: 论文提出了一种直接分析QR码结构和像素模式的框架，用于检测基于QR码的网络钓鱼（Quishing），无需提取嵌入内容。通过机器学习模型验证，XGBoost表现最佳，AUC达0.9133。


<details>
  <summary>Details</summary>
Motivation: 传统检测方法依赖URL分析，可能暴露用户于恶意内容，且无法覆盖QR码编码的其他数据类型（如Wi-Fi凭证、支付信息）。

Method: 生成钓鱼和良性QR码数据集，训练多种机器学习模型（如Logistic Regression、XGBoost等），分析QR码结构和像素模式。

Result: XGBoost模型AUC为0.9106，优化特征后提升至0.9133。QR码结构特征与钓鱼风险强相关。

Conclusion: 直接QR分析可作为现代钓鱼防御的关键层，为Quishing缓解奠定基础。

Abstract: The rise of QR code based phishing ("Quishing") poses a growing cybersecurity
threat, as attackers increasingly exploit QR codes to bypass traditional
phishing defenses. Existing detection methods predominantly focus on URL
analysis, which requires the extraction of the QR code payload, and may
inadvertently expose users to malicious content. Moreover, QR codes can encode
various types of data beyond URLs, such as Wi-Fi credentials and payment
information, making URL-based detection insufficient for broader security
concerns. To address these gaps, we propose the first framework for quishing
detection that directly analyzes QR code structure and pixel patterns without
extracting the embedded content. We generated a dataset of phishing and benign
QR codes and we used it to train and evaluate multiple machine learning models,
including Logistic Regression, Decision Trees, Random Forest, Naive Bayes,
LightGBM, and XGBoost. Our best-performing model (XGBoost) achieves an AUC of
0.9106, demonstrating the feasibility of QR-centric detection. Through feature
importance analysis, we identify key visual indicators of malicious intent and
refine our feature set by removing non-informative pixels, improving
performance to an AUC of 0.9133 with a reduced feature space. Our findings
reveal that the structural features of QR code correlate strongly with phishing
risk. This work establishes a foundation for quishing mitigation and highlights
the potential of direct QR analysis as a critical layer in modern phishing
defenses.

</details>


### [149] [LlamaFirewall: An open source guardrail system for building secure AI agents](https://arxiv.org/abs/2505.03574)
*Sahana Chennabasappa,Cyrus Nikolaidis,Daniel Song,David Molnar,Stephanie Ding,Shengye Wan,Spencer Whitman,Lauren Deason,Nicholas Doucette,Abraham Montilla,Alekhya Gampa,Beto de Paola,Dominik Gabi,James Crnkovich,Jean-Christophe Testud,Kat He,Rashnil Chaturvedi,Wu Zhou,Joshua Saxe*

Main category: cs.CR

TL;DR: LlamaFirewall是一个开源的安全防护框架，旨在为AI代理提供实时防护，抵御提示注入、代理错位和不安全代码等风险。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLM）能力的提升，现有安全措施无法完全应对其带来的新安全风险，亟需一种实时防护机制。

Method: LlamaFirewall通过三个核心防护措施实现：PromptGuard 2（通用越狱检测器）、Agent Alignment Checks（代理对齐检查）和CodeShield（在线静态分析引擎）。

Result: 框架在防止提示注入、代理错位和不安全代码生成方面表现出色，尤其是PromptGuard 2在越狱检测上达到领先水平。

Conclusion: LlamaFirewall为AI代理提供了一种灵活且高效的安全防护方案，开发者可轻松定制防护规则。

Abstract: Large language models (LLMs) have evolved from simple chatbots into
autonomous agents capable of performing complex tasks such as editing
production code, orchestrating workflows, and taking higher-stakes actions
based on untrusted inputs like webpages and emails. These capabilities
introduce new security risks that existing security measures, such as model
fine-tuning or chatbot-focused guardrails, do not fully address. Given the
higher stakes and the absence of deterministic solutions to mitigate these
risks, there is a critical need for a real-time guardrail monitor to serve as a
final layer of defense, and support system level, use case specific safety
policy definition and enforcement. We introduce LlamaFirewall, an open-source
security focused guardrail framework designed to serve as a final layer of
defense against security risks associated with AI Agents. Our framework
mitigates risks such as prompt injection, agent misalignment, and insecure code
risks through three powerful guardrails: PromptGuard 2, a universal jailbreak
detector that demonstrates clear state of the art performance; Agent Alignment
Checks, a chain-of-thought auditor that inspects agent reasoning for prompt
injection and goal misalignment, which, while still experimental, shows
stronger efficacy at preventing indirect injections in general scenarios than
previously proposed approaches; and CodeShield, an online static analysis
engine that is both fast and extensible, aimed at preventing the generation of
insecure or dangerous code by coding agents. Additionally, we include
easy-to-use customizable scanners that make it possible for any developer who
can write a regular expression or an LLM prompt to quickly update an agent's
security guardrails.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [150] [MORE: Mobile Manipulation Rearrangement Through Grounded Language Reasoning](https://arxiv.org/abs/2505.03035)
*Mohammad Mohammadi,Daniel Honerkamp,Martin Büchner,Matteo Cassinelli,Tim Welschehold,Fabien Despinoy,Igor Gilitschenski,Abhinav Valada*

Main category: cs.RO

TL;DR: MORE是一种新方法，通过场景图和主动过滤方案增强语言模型能力，解决零样本移动操作规划问题，并在BEHAVIOR-1K基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理大规模环境和多对象时性能下降，需提升语言模型在零样本移动操作规划中的能力。

Method: 利用场景图表示环境，引入实例区分和主动过滤方案，提取任务相关子图，形成有界规划问题。

Result: 在BEHAVIOR-1K基准测试中成功解决81个多样化重排任务，优于现有基于基础模型的方法。

Conclusion: MORE通过场景图和过滤方案有效提升规划可靠性，适用于室内外复杂任务。

Abstract: Autonomous long-horizon mobile manipulation encompasses a multitude of
challenges, including scene dynamics, unexplored areas, and error recovery.
Recent works have leveraged foundation models for scene-level robotic reasoning
and planning. However, the performance of these methods degrades when dealing
with a large number of objects and large-scale environments. To address these
limitations, we propose MORE, a novel approach for enhancing the capabilities
of language models to solve zero-shot mobile manipulation planning for
rearrangement tasks. MORE leverages scene graphs to represent environments,
incorporates instance differentiation, and introduces an active filtering
scheme that extracts task-relevant subgraphs of object and region instances.
These steps yield a bounded planning problem, effectively mitigating
hallucinations and improving reliability. Additionally, we introduce several
enhancements that enable planning across both indoor and outdoor environments.
We evaluate MORE on 81 diverse rearrangement tasks from the BEHAVIOR-1K
benchmark, where it becomes the first approach to successfully solve a
significant share of the benchmark, outperforming recent foundation model-based
approaches. Furthermore, we demonstrate the capabilities of our approach in
several complex real-world tasks, mimicking everyday activities. We make the
code publicly available at https://more-model.cs.uni-freiburg.de.

</details>


### [151] [Latent Adaptive Planner for Dynamic Manipulation](https://arxiv.org/abs/2505.03077)
*Donghun Noh,Deqian Kong,Minglu Zhao,Andrew Lizarraga,Jianwen Xie,Ying Nian Wu,Dennis Hong*

Main category: cs.RO

TL;DR: LAP是一种基于潜在空间推理的动态非抓取操作任务规划方法，通过学习人类演示视频实现高效适应环境变化。


<details>
  <summary>Details</summary>
Motivation: 解决视觉运动策略学习中的关键挑战，如时间一致性和环境适应性。

Method: 通过变分重规划框架和贝叶斯潜在空间更新，实现高效且实时的适应性规划。

Result: 在多个复杂操作任务中表现优异，成功率和适应性优于现有方法。

Conclusion: LAP为机器人提供了类似人类的适应性，并适用于多种平台。

Abstract: This paper presents Latent Adaptive Planner (LAP), a novel approach for
dynamic nonprehensile manipulation tasks that formulates planning as latent
space inference, effectively learned from human demonstration videos. Our
method addresses key challenges in visuomotor policy learning through a
principled variational replanning framework that maintains temporal consistency
while efficiently adapting to environmental changes. LAP employs Bayesian
updating in latent space to incrementally refine plans as new observations
become available, striking an optimal balance between computational efficiency
and real-time adaptability. We bridge the embodiment gap between humans and
robots through model-based proportional mapping that regenerates accurate
kinematic-dynamic joint states and object positions from human demonstrations.
Experimental evaluations across multiple complex manipulation benchmarks
demonstrate that LAP achieves state-of-the-art performance, outperforming
existing approaches in success rate, trajectory smoothness, and energy
efficiency, particularly in dynamic adaptation scenarios. Our approach enables
robots to perform complex interactions with human-like adaptability while
providing an expandable framework applicable to diverse robotic platforms using
the same human demonstration videos.

</details>


### [152] [Learn to Swim: Data-Driven LSTM Hydrodynamic Model for Quadruped Robot Gait Optimization](https://arxiv.org/abs/2505.03146)
*Fei Han,Pengming Guo,Hao Chen,Weikun Li,Jingbo Ren,Naijun Liu,Ning Yang,Dixia Fan*

Main category: cs.RO

TL;DR: 本文提出了一种基于LSTM网络的流体实验数据驱动模型（FED-LSTM），用于预测水下四足机器人的非稳态非线性水动力，优于传统经验公式（EF）。


<details>
  <summary>Details</summary>
Motivation: 传统经验公式（EF）在预测复杂流体动力学时存在局限性，尤其是在水下机器人运动中。FED-LSTM旨在通过实验数据驱动的方法提高预测精度和适应性。

Method: 利用在循环水槽和拖曳水槽中进行的腿部力和身体阻力实验数据训练FED-LSTM模型，并通过NSGA-II算法优化直线和转向步态。

Result: FED-LSTM在直线游泳时减少偏转误差，并在不增加转弯半径的情况下提高转弯时间，硬件实验验证了其精度和稳定性优于EF。

Conclusion: FED-LSTM为提升腿式机器人的游泳性能提供了稳健框架，为未来水下机器人运动研究奠定了基础。

Abstract: This paper presents a Long Short-Term Memory network-based Fluid Experiment
Data-Driven model (FED-LSTM) for predicting unsteady, nonlinear hydrodynamic
forces on the underwater quadruped robot we constructed. Trained on
experimental data from leg force and body drag tests conducted in both a
recirculating water tank and a towing tank, FED-LSTM outperforms traditional
Empirical Formulas (EF) commonly used for flow prediction over flat surfaces.
The model demonstrates superior accuracy and adaptability in capturing complex
fluid dynamics, particularly in straight-line and turning-gait optimizations
via the NSGA-II algorithm. FED-LSTM reduces deflection errors during
straight-line swimming and improves turn times without increasing the turning
radius. Hardware experiments further validate the model's precision and
stability over EF. This approach provides a robust framework for enhancing the
swimming performance of legged robots, laying the groundwork for future
advances in underwater robotic locomotion.

</details>


### [153] [Systematic Evaluation of Initial States and Exploration-Exploitation Strategies in PID Auto-Tuning: A Framework-Driven Approach Applied on Mobile Robots](https://arxiv.org/abs/2505.03159)
*Zaid Ghazal,Ali Al-Bustami,Khouloud Gaaloul,Jaerock Kwon*

Main category: cs.RO

TL;DR: 论文提出了一种新框架，用于评估初始系统状态和探索-开发平衡对PID控制器自动调谐的影响，并在两种机器人平台上进行了实验验证。


<details>
  <summary>Details</summary>
Motivation: 尽管贝叶斯优化和差分进化等高级优化技术已用于PID控制器自动调谐，但初始系统状态对收敛的影响以及探索与开发的平衡尚未充分研究。

Method: 引入了一种新框架，系统性地评估这些因素对PID自动调谐的影响，并在两种PID控制的机器人平台上进行了测试。

Result: 实验结果表明，系统性的变化对收敛速度、稳定时间、上升时间和超调百分比有显著影响。

Conclusion: 研究结果为未来PID自动调谐领域的进一步研究提供了实证基础。

Abstract: PID controllers are widely used in control systems because of their
simplicity and effectiveness. Although advanced optimization techniques such as
Bayesian Optimization and Differential Evolution have been applied to address
the challenges of automatic tuning of PID controllers, the influence of initial
system states on convergence and the balance between exploration and
exploitation remains underexplored. Moreover, experimenting the influence
directly on real cyber-physical systems such as mobile robots is crucial for
deriving realistic insights. In the present paper, a novel framework is
introduced to evaluate the impact of systematically varying these factors on
the PID auto-tuning processes that utilize Bayesian Optimization and
Differential Evolution. Testing was conducted on two distinct PID-controlled
robotic platforms, an omnidirectional robot and a differential drive mobile
robot, to assess the effects on convergence rate, settling time, rise time, and
overshoot percentage. As a result, the experimental outcomes yield evidence on
the effects of the systematic variations, thereby providing an empirical basis
for future research studies in the field.

</details>


### [154] [Automated Data Curation Using GPS & NLP to Generate Instruction-Action Pairs for Autonomous Vehicle Vision-Language Navigation Datasets](https://arxiv.org/abs/2505.03174)
*Guillermo Roque,Erika Maquiling,Jose Giovanni Tapia Lopez,Ross Greer*

Main category: cs.RO

TL;DR: 论文探讨了利用GPS和NLP自动生成指令-动作数据对的方法，以减少人工标注成本，并通过实验展示了数据收集和分类的可行性。


<details>
  <summary>Details</summary>
Motivation: 人工标注指令-动作数据对成本高且效率低，因此需要一种自动化方法来快速生成高质量数据。

Method: 通过收集GPS应用的语音指令和视频数据，构建完整的视觉-语言-动作三元组，并开发了自动化数据收集系统ADVLAT-Engine。

Result: 成功将GPS语音指令分为八类，展示了从移动应用中自动生成多样化指令的潜力。

Conclusion: 自动化生成指令-动作数据对可以高效、低成本地支持视觉-语言导航和交互式自主系统的开发。

Abstract: Instruction-Action (IA) data pairs are valuable for training robotic systems,
especially autonomous vehicles (AVs), but having humans manually annotate this
data is costly and time-inefficient. This paper explores the potential of using
mobile application Global Positioning System (GPS) references and Natural
Language Processing (NLP) to automatically generate large volumes of IA
commands and responses without having a human generate or retroactively tag the
data. In our pilot data collection, by driving to various destinations and
collecting voice instructions from GPS applications, we demonstrate a means to
collect and categorize the diverse sets of instructions, further accompanied by
video data to form complete vision-language-action triads. We provide details
on our completely automated data collection prototype system, ADVLAT-Engine. We
characterize collected GPS voice instructions into eight different
classifications, highlighting the breadth of commands and referentialities
available for curation from freely available mobile applications. Through
research and exploration into the automation of IA data pairs using GPS
references, the potential to increase the speed and volume at which
high-quality IA datasets are created, while minimizing cost, can pave the way
for robust vision-language-action (VLA) models to serve tasks in
vision-language navigation (VLN) and human-interactive autonomous systems.

</details>


### [155] [The Unreasonable Effectiveness of Discrete-Time Gaussian Process Mixtures for Robot Policy Learning](https://arxiv.org/abs/2505.03296)
*Jan Ole von Hartz,Adrian Röfer,Joschka Boedecker,Abhinav Valada*

Main category: cs.RO

TL;DR: MiDiGap是一种用于机器人操作的灵活策略表示和模仿学习的新方法，能够从少量演示中学习，并在多种复杂任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 解决机器人操作任务中策略表示的灵活性和从少量演示中学习的问题。

Method: 采用离散时间高斯过程混合模型（MiDiGap），结合推理时引导工具，如碰撞信号和机器人运动学约束。

Result: 在少量演示任务中表现优异，成功率和轨迹成本显著改善，跨模态和跨体现任务中性能大幅提升。

Conclusion: MiDiGap是一种高效、灵活的机器人操作策略学习方法，具有广泛的应用潜力。

Abstract: We present Mixture of Discrete-time Gaussian Processes (MiDiGap), a novel
approach for flexible policy representation and imitation learning in robot
manipulation. MiDiGap enables learning from as few as five demonstrations using
only camera observations and generalizes across a wide range of challenging
tasks. It excels at long-horizon behaviors such as making coffee, highly
constrained motions such as opening doors, dynamic actions such as scooping
with a spatula, and multimodal tasks such as hanging a mug. MiDiGap learns
these tasks on a CPU in less than a minute and scales linearly to large
datasets. We also develop a rich suite of tools for inference-time steering
using evidence such as collision signals and robot kinematic constraints. This
steering enables novel generalization capabilities, including obstacle
avoidance and cross-embodiment policy transfer. MiDiGap achieves
state-of-the-art performance on diverse few-shot manipulation benchmarks. On
constrained RLBench tasks, it improves policy success by 76 percentage points
and reduces trajectory cost by 67%. On multimodal tasks, it improves policy
success by 48 percentage points and increases sample efficiency by a factor of
20. In cross-embodiment transfer, it more than doubles policy success. We make
the code publicly available at https://midigap.cs.uni-freiburg.de.

</details>


### [156] [RIFT: Closed-Loop RL Fine-Tuning for Realistic and Controllable Traffic Simulation](https://arxiv.org/abs/2505.03344)
*Keyu Chen,Wenchao Sun,Hao Cheng,Sifa Zheng*

Main category: cs.RO

TL;DR: 论文提出了一种双阶段自动驾驶模拟框架，结合数据驱动和物理模拟方法，通过预训练和微调提升真实性和可控性。


<details>
  <summary>Details</summary>
Motivation: 解决交互式闭环交通模拟中真实性与可控性的矛盾，数据驱动方法存在协变量偏移，物理模拟方法缺乏专家演示。

Method: 采用双阶段框架：数据驱动模拟器中进行开环模仿学习预训练，物理模拟器中进行闭环强化学习微调，并提出RIFT策略。

Result: RIFT显著提升了生成交通场景的真实性和可控性，为自动驾驶性能评估提供了稳健平台。

Conclusion: 双阶段框架和RIFT策略有效解决了真实性与可控性的平衡问题，为自动驾驶模拟提供了新思路。

Abstract: Achieving both realism and controllability in interactive closed-loop traffic
simulation remains a key challenge in autonomous driving. Data-driven
simulation methods reproduce realistic trajectories but suffer from covariate
shift in closed-loop deployment, compounded by simplified dynamics models that
further reduce reliability. Conversely, physics-based simulation methods
enhance reliable and controllable closed-loop interactions but often lack
expert demonstrations, compromising realism. To address these challenges, we
introduce a dual-stage AV-centered simulation framework that conducts open-loop
imitation learning pre-training in a data-driven simulator to capture
trajectory-level realism and multimodality, followed by closed-loop
reinforcement learning fine-tuning in a physics-based simulator to enhance
controllability and mitigate covariate shift. In the fine-tuning stage, we
propose RIFT, a simple yet effective closed-loop RL fine-tuning strategy that
preserves the trajectory-level multimodality through a GRPO-style
group-relative advantage formulation, while enhancing controllability and
training stability by replacing KL regularization with the dual-clip mechanism.
Extensive experiments demonstrate that RIFT significantly improves the realism
and controllability of generated traffic scenarios, providing a robust platform
for evaluating autonomous vehicle performance in diverse and interactive
scenarios.

</details>


### [157] [Self-Supervised Learning for Robotic Leaf Manipulation: A Hybrid Geometric-Neural Approach](https://arxiv.org/abs/2505.03702)
*Srecharan Selvam,Abhishesh Silwal,George Kanter*

Main category: cs.RO

TL;DR: 提出了一种结合几何与神经网络的混合方法，用于农业场景中的自主叶片抓取，通过自监督学习实现，显著提升了成功率。


<details>
  <summary>Details</summary>
Motivation: 解决农业环境中叶片形态多样性和可变形性带来的自动化抓取挑战。

Method: 结合YOLOv8实例分割和RAFT-Stereo 3D深度估计，通过几何特征评分管道和神经细化模块（GraspPointCNN）实现，采用置信加权融合机制动态平衡两种方法。

Result: 在控制环境中成功率为88.0%，真实温室环境中为84.7%，显著优于纯几何（75.3%）和纯神经网络（60.2%）方法。

Conclusion: 为农业机器人提供了一种新范式，将领域专业知识与机器学习能力无缝结合，为全自动作物监测系统奠定基础。

Abstract: Automating leaf manipulation in agricultural settings faces significant
challenges, including the variability of plant morphologies and deformable
leaves. We propose a novel hybrid geometric-neural approach for autonomous leaf
grasping that combines traditional computer vision with neural networks through
self-supervised learning. Our method integrates YOLOv8 for instance
segmentation and RAFT-Stereo for 3D depth estimation to build rich leaf
representations, which feed into both a geometric feature scoring pipeline and
a neural refinement module (GraspPointCNN). The key innovation is our
confidence-weighted fusion mechanism that dynamically balances the contribution
of each approach based on prediction certainty. Our self-supervised framework
uses the geometric pipeline as an expert teacher to automatically generate
training data. Experiments demonstrate that our approach achieves an 88.0%
success rate in controlled environments and 84.7% in real greenhouse
conditions, significantly outperforming both purely geometric (75.3%) and
neural (60.2%) methods. This work establishes a new paradigm for agricultural
robotics where domain expertise is seamlessly integrated with machine learning
capabilities, providing a foundation for fully automated crop monitoring
systems.

</details>


### [158] [AMO: Adaptive Motion Optimization for Hyper-Dexterous Humanoid Whole-Body Control](https://arxiv.org/abs/2505.03738)
*Jialong Li,Xuxin Cheng,Tianshu Huang,Shiqi Yang,Ri-Zhao Qiu,Xiaolong Wang*

Main category: cs.RO

TL;DR: 提出了一种名为AMO的框架，结合了仿真到现实的强化学习和轨迹优化，用于实时自适应全身控制，提升了人形机器人的稳定性和工作范围。


<details>
  <summary>Details</summary>
Motivation: 人形机器人在高自由度和非线性动力学下实现超灵巧全身运动仍具挑战性，需要一种能够适应实时控制的方法。

Method: 通过构建混合AMO数据集，训练网络以应对潜在分布外指令，结合仿真到现实的强化学习和轨迹优化。

Result: 在仿真和29自由度Unitree G1人形机器人上验证了AMO的优越稳定性和扩展的工作范围。

Conclusion: AMO的稳定性能支持通过模仿学习实现自主任务执行，展现了系统的多功能性和鲁棒性。

Abstract: Humanoid robots derive much of their dexterity from hyper-dexterous
whole-body movements, enabling tasks that require a large operational
workspace: such as picking objects off the ground. However, achieving these
capabilities on real humanoids remains challenging due to their high degrees of
freedom (DoF) and nonlinear dynamics. We propose Adaptive Motion Optimization
(AMO), a framework that integrates sim-to-real reinforcement learning (RL) with
trajectory optimization for real-time, adaptive whole-body control. To mitigate
distribution bias in motion imitation RL, we construct a hybrid AMO dataset and
train a network capable of robust, on-demand adaptation to potentially O.O.D.
commands. We validate AMO in simulation and on a 29-DoF Unitree G1 humanoid
robot, demonstrating superior stability and an expanded workspace compared to
strong baselines. Finally, we show that AMO's consistent performance supports
autonomous task execution via imitation learning, underscoring the system's
versatility and robustness.

</details>


### [159] [Demonstrating ViSafe: Vision-enabled Safety for High-speed Detect and Avoid](https://arxiv.org/abs/2505.03694)
*Parv Kapoor,Ian Higgins,Nikhil Keetha,Jay Patrikar,Brady Moon,Zelin Ye,Yao He,Ivan Cisneros,Yaoyu Hu,Changliu Liu,Eunsuk Kang,Sebastian Scherer*

Main category: cs.RO

TL;DR: ViSafe是一种基于视觉的高速度空中防撞系统，通过结合学习型边缘AI框架和多摄像头硬件原型，提供可证明的安全运行时保证。


<details>
  <summary>Details</summary>
Motivation: 确保安全分离是实现高密度空中交通无缝操作的关键，ViSafe旨在为资源受限的空中系统提供这一安全关键能力。

Method: ViSafe采用基于感知输入的控制屏障函数（CBF）设计安全阈值，并通过模拟和真实飞行场景进行测试。

Result: ViSafe在各种场景下均能确保安全分离，并在高达144 km/h的闭合速度测试中表现优异。

Conclusion: ViSafe为高速度空中导航中的视觉自主防撞设立了新标准。

Abstract: Assured safe-separation is essential for achieving seamless high-density
operation of airborne vehicles in a shared airspace. To equip
resource-constrained aerial systems with this safety-critical capability, we
present ViSafe, a high-speed vision-only airborne collision avoidance system.
ViSafe offers a full-stack solution to the Detect and Avoid (DAA) problem by
tightly integrating a learning-based edge-AI framework with a custom
multi-camera hardware prototype designed under SWaP-C constraints. By
leveraging perceptual input-focused control barrier functions (CBF) to design,
encode, and enforce safety thresholds, ViSafe can provide provably safe runtime
guarantees for self-separation in high-speed aerial operations. We evaluate
ViSafe's performance through an extensive test campaign involving both
simulated digital twins and real-world flight scenarios. By independently
varying agent types, closure rates, interaction geometries, and environmental
conditions (e.g., weather and lighting), we demonstrate that ViSafe
consistently ensures self-separation across diverse scenarios. In
first-of-its-kind real-world high-speed collision avoidance tests with closure
rates reaching 144 km/h, ViSafe sets a new benchmark for vision-only autonomous
collision avoidance, establishing a new standard for safety in high-speed
aerial navigation.

</details>


<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [160] [Parameter estimation for land-surface models using machine learning libraries](https://arxiv.org/abs/2505.02979)
*Ruiyue Huang,Claire E. Heaney,Maarten van Reeuwijk*

Main category: physics.ao-ph

TL;DR: NN4PDEs方法用于通过PyTorch的反向传播引擎确定地表模型参数，但单层土壤温度数据无法可靠估计参数，需两层数据。应用至凤凰城数据后，模型能准确预测热通量。


<details>
  <summary>Details</summary>
Motivation: 研究如何利用神经网络方法（NN4PDEs）通过反向传播优化地表模型的参数估计，解决传统方法在参数估计中的局限性。

Method: 使用PyTorch的反向传播引擎构建逆模型，通过合成数据（已知参数生成的土壤温度时间序列）验证模型，并应用于实际城市通量塔数据。

Result: 单层土壤温度数据无法可靠估计参数，需两层数据；模型能准确预测热通量，但无法区分潜热和感热通量。

Conclusion: NN4PDEs方法在多层数据支持下能有效估计地表模型参数，适用于实际数据预测，但需进一步区分热通量类型。

Abstract: The Neural Networks for Partial Differential Equations (NN4PDEs) approach is
used to determine the parameters of a simple land-surface model using PyTorch's
backpropagation engine. In order to test the inverse model, a synthetic dataset
is created by running the model in forward mode with known parameter values to
create soil temperature time series that can be used as observations for the
inverse model. We show that it is not possible to obtain a reliable parameter
estimation using a single observed soil temperature time series. Using
measurements at two depths, reliable parameter estimates can be obtained
although it is not possible to differentiate between latent and sensible heat
fluxes. We apply the inverse model to urban flux tower data in Phoenix, United
States, and show that the thermal conductivity, volumetric heat capacity, and
the combined sensible-latent heat transfer coefficient can be reliably
estimated using an observed value for the effective surface albedo. The
resulting model accurately predicts the outgoing longwave radiation, conductive
soil fluxes and the combined sensible-latent heat fluxes.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [161] [Accelerating Evolution: Integrating PSO Principles into Real-Coded Genetic Algorithm Crossover](https://arxiv.org/abs/2505.03217)
*Xiaobo Jin,JiaShu Tu*

Main category: cs.NE

TL;DR: 提出了一种名为PSOX的新型交叉算子，结合了粒子群优化的思想，用于实值编码遗传算法，显著提升了算法性能。


<details>
  <summary>Details</summary>
Motivation: 传统交叉算子仅在同代个体间交换信息，缺乏全局和历史信息的引导，限制了算法的多样性和收敛速度。

Method: PSOX通过引入当前全局最优解和历史最优解的指导，增强搜索能力，同时保持种群多样性。

Result: 在15个基准测试函数上验证了PSOX的优越性，尤其在解精度、稳定性和收敛速度方面表现突出。

Conclusion: PSOX是一种高效的交叉算子，结合适当变异策略后，适用于多种优化问题，并提供了参数调优的实用指南。

Abstract: This study introduces an innovative crossover operator named Particle Swarm
Optimization-inspired Crossover (PSOX), which is specifically developed for
real-coded genetic algorithms. Departing from conventional crossover approaches
that only exchange information between individuals within the same generation,
PSOX uniquely incorporates guidance from both the current global best solution
and historical optimal solutions across multiple generations. This novel
mechanism enables the algorithm to maintain population diversity while
simultaneously accelerating convergence toward promising regions of the search
space. The effectiveness of PSOX is rigorously evaluated through comprehensive
experiments on 15 benchmark test functions with diverse characteristics,
including unimodal, multimodal, and highly complex landscapes. Comparative
analysis against five state-of-the-art crossover operators reveals that PSOX
consistently delivers superior performance in terms of solution accuracy,
algorithmic stability, and convergence speed, especially when combined with an
appropriate mutation strategy. Furthermore, the study provides an in-depth
investigation of how different mutation rates influence PSOX's performance,
yielding practical guidelines for parameter tuning when addressing optimization
problems with varying landscape properties.

</details>


### [162] [From Neurons to Computation: Biological Reservoir Computing for Pattern Recognition](https://arxiv.org/abs/2505.03510)
*Ludovico Iannello,Luca Ciampi,Gabriele Lagani,Fabrizio Tonelli,Eleonora Crocco,Lucio Maria Calcagnile,Angelo Di Garbo,Federico Cremisi,Giuseppe Amato*

Main category: cs.NE

TL;DR: 提出了一种利用培养生物神经元作为储备池的生物储备计算（BRC）新范式，性能与传统人工神经网络相当。


<details>
  <summary>Details</summary>
Motivation: 探索生物神经网络在计算任务中的应用潜力，推动生物启发计算系统的发展。

Method: 使用多电极阵列（MEA）记录神经元活动，输入通过部分电极引入，其余电极捕获响应，生成非线性高维特征映射。

Result: 实验表明BRC能有效完成模式识别任务，如位置编码、方向条和数字识别。

Conclusion: BRC为生物启发计算和神经形态工程提供了新方向，具有潜在应用价值。

Abstract: In this paper, we introduce a novel paradigm for reservoir computing (RC)
that leverages a pool of cultured biological neurons as the reservoir
substrate, creating a biological reservoir computing (BRC). This system
operates similarly to an echo state network (ESN), with the key distinction
that the neural activity is generated by a network of cultured neurons, rather
than being modeled by traditional artificial computational units. The neuronal
activity is recorded using a multi-electrode array (MEA), which enables
high-throughput recording of neural signals. In our approach, inputs are
introduced into the network through a subset of the MEA electrodes, while the
remaining electrodes capture the resulting neural activity. This generates a
nonlinear mapping of the input data to a high-dimensional biological feature
space, where distinguishing between data becomes more efficient and
straightforward, allowing a simple linear classifier to perform pattern
recognition tasks effectively. To evaluate the performance of our proposed
system, we present an experimental study that includes various input patterns,
such as positional codes, bars with different orientations, and a digit
recognition task. The results demonstrate the feasibility of using biological
neural networks to perform tasks traditionally handled by artificial neural
networks, paving the way for further exploration of biologically-inspired
computing systems, with potential applications in neuromorphic engineering and
bio-hybrid computing.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [163] [Neural Orchestration for Multi-Agent Systems: A Deep Learning Framework for Optimal Agent Selection in Multi-Domain Task Environments](https://arxiv.org/abs/2505.02861)
*Kushagra Agrawal,Nisharg Nargund*

Main category: cs.MA

TL;DR: MetaOrch是一种神经编排框架，用于在多领域任务环境中优化代理选择，通过监督学习和模糊评估模块动态选择最合适的代理，显著提高了选择准确性。


<details>
  <summary>Details</summary>
Motivation: 传统多代理系统（MAS）架构存在协调机制僵化和难以适应动态任务的问题，MetaOrch旨在通过神经编排提升MAS的自主性和适应性。

Method: 采用监督学习方法建模任务上下文、代理历史和预期响应质量，结合模糊评估模块（基于完整性、相关性和置信度）生成软监督标签，动态预测最合适的代理。

Result: 在模拟环境中，MetaOrch实现了86.3%的选择准确率，显著优于随机选择和轮询调度等基线策略。

Conclusion: 神经编排为多代理系统提供了增强自主性、可解释性和适应性的有效方法，适用于多样化任务领域。

Abstract: Multi-agent systems (MAS) are foundational in simulating complex real-world
scenarios involving autonomous, interacting entities. However, traditional MAS
architectures often suffer from rigid coordination mechanisms and difficulty
adapting to dynamic tasks. We propose MetaOrch, a neural orchestration
framework for optimal agent selection in multi-domain task environments. Our
system implements a supervised learning approach that models task context,
agent histories, and expected response quality to select the most appropriate
agent for each task. A novel fuzzy evaluation module scores agent responses
along completeness, relevance, and confidence dimensions, generating soft
supervision labels for training the orchestrator. Unlike previous methods that
hard-code agent-task mappings, MetaOrch dynamically predicts the most suitable
agent while estimating selection confidence. Experiments in simulated
environments with heterogeneous agents demonstrate that our approach achieves
86.3% selection accuracy, significantly outperforming baseline strategies
including random selection and round-robin scheduling. The modular architecture
emphasizes extensibility, allowing agents to be registered, updated, and
queried independently. Results suggest that neural orchestration offers a
powerful approach to enhancing the autonomy, interpretability, and adaptability
of multi-agent systems across diverse task domains.

</details>


### [164] [Assessing and Enhancing the Robustness of LLM-based Multi-Agent Systems Through Chaos Engineering](https://arxiv.org/abs/2505.03096)
*Joshua Owotogbe*

Main category: cs.MA

TL;DR: 研究探讨了混沌工程在提升大型语言模型多智能体系统（LLM-MAS）鲁棒性中的应用，以应对生产环境中的潜在故障。


<details>
  <summary>Details</summary>
Motivation: LLM-MAS在任务处理中潜力巨大，但在实际环境中易受突发错误（如幻觉、智能体故障等）影响，需提高其可靠性。

Method: 提出一种混沌工程框架，主动识别LLM-MAS的脆弱性，并评估和增强其韧性。

Result: 框架旨在确保LLM-MAS在关键应用中的稳定性能。

Conclusion: 混沌工程可有效提升LLM-MAS的鲁棒性，适用于生产环境。

Abstract: This study explores the application of chaos engineering to enhance the
robustness of Large Language Model-Based Multi-Agent Systems (LLM-MAS) in
production-like environments under real-world conditions. LLM-MAS can
potentially improve a wide range of tasks, from answering questions and
generating content to automating customer support and improving decision-making
processes. However, LLM-MAS in production or preproduction environments can be
vulnerable to emergent errors or disruptions, such as hallucinations, agent
failures, and agent communication failures. This study proposes a chaos
engineering framework to proactively identify such vulnerabilities in LLM-MAS,
assess and build resilience against them, and ensure reliable performance in
critical applications.

</details>


### [165] [Rainbow Delay Compensation: A Multi-Agent Reinforcement Learning Framework for Mitigating Delayed Observation](https://arxiv.org/abs/2505.03586)
*Songchen Fu,Siang Chen,Shaojing Zhao,Letian Bai,Ta Li,Yonghong Yan*

Main category: cs.MA

TL;DR: 论文提出了一种针对多智能体系统中观测延迟问题的解决方案，通过扩展Dec-POMDP模型并引入Rainbow Delay Compensation框架，有效提升了延迟环境下的性能。


<details>
  <summary>Details</summary>
Motivation: 现实多智能体系统中普遍存在观测延迟问题，导致智能体无法基于真实环境状态做出决策，这对多智能体强化学习提出了挑战。

Method: 通过扩展Dec-POMDP模型，提出DSID-POMDP框架，并设计Rainbow Delay Compensation（RDC）训练框架及其模块实现。

Result: 实验表明，RDC框架在固定和非固定延迟下显著提升了性能，甚至在某些延迟场景下达到无延迟的理想效果。

Conclusion: 该研究为多智能体延迟观测问题提供了新视角和有效解决方案框架。

Abstract: In real-world multi-agent systems (MASs), observation delays are ubiquitous,
preventing agents from making decisions based on the environment's true state.
An individual agent's local observation often consists of multiple components
from other agents or dynamic entities in the environment. These discrete
observation components with varying delay characteristics pose significant
challenges for multi-agent reinforcement learning (MARL). In this paper, we
first formulate the decentralized stochastic individual delay partially
observable Markov decision process (DSID-POMDP) by extending the standard
Dec-POMDP. We then propose the Rainbow Delay Compensation (RDC), a MARL
training framework for addressing stochastic individual delays, along with
recommended implementations for its constituent modules. We implement the
DSID-POMDP's observation generation pattern using standard MARL benchmarks,
including MPE and SMAC. Experiments demonstrate that baseline MARL methods
suffer severe performance degradation under fixed and unfixed delays. The
RDC-enhanced approach mitigates this issue, remarkably achieving ideal
delay-free performance in certain delay scenarios while maintaining
generalization capability. Our work provides a novel perspective on multi-agent
delayed observation problems and offers an effective solution framework.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [166] [Soft Best-of-n Sampling for Model Alignment](https://arxiv.org/abs/2505.03156)
*Claudio Mayrink Verdun,Alex Oesterling,Himabindu Lakkaraju,Flavio P. Calmon*

Main category: cs.IT

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Best-of-$n$ (BoN) sampling is a practical approach for aligning language
model outputs with human preferences without expensive fine-tuning. BoN
sampling is performed by generating $n$ responses to a prompt and then
selecting the sample that maximizes a reward function. BoN yields high reward
values in practice at a distortion cost, as measured by the KL-divergence
between the sampled and original distribution. This distortion is coarsely
controlled by varying the number of samples: larger $n$ yields a higher reward
at a higher distortion cost. We introduce Soft Best-of-$n$ sampling, a
generalization of BoN that allows for smooth interpolation between the original
distribution and reward-maximizing distribution through a temperature parameter
$\lambda$. We establish theoretical guarantees showing that Soft Best-of-$n$
sampling converges sharply to the optimal tilted distribution at a rate of
$O(1/n)$ in KL and the expected (relative) reward. For sequences of discrete
outputs, we analyze an additive reward model that reveals the fundamental
limitations of blockwise sampling.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [167] [CreoPep: A Universal Deep Learning Framework for Target-Specific Peptide Design and Optimization](https://arxiv.org/abs/2505.02887)
*Cheng Ge,Han-Shen Tae,Zhenqiang Zhang,Lu Lu,Zhijie Huang,Yilin Wang,Tao Jiang,Wenqing Cai,Shan Chang,David J. Adams,Rilei Yu*

Main category: q-bio.BM

TL;DR: CreoPep是一种基于深度学习的条件生成框架，用于设计高亲和力肽突变体并发现新结构基序，通过实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 天然肽变体多样性有限且传统优化策略耗时，限制了靶向肽（如芋螺毒素）的治疗潜力。

Method: 结合掩码语言建模和渐进掩码方案，通过FoldX能量筛选和温度控制多态采样生成多样肽。

Result: 设计的芋螺毒素抑制剂在电生理实验中表现出亚微摩尔效力，结构分析揭示了新颖结合模式。

Conclusion: CreoPep为计算肽设计与实验验证提供了通用平台，加速下一代肽疗法的发现。

Abstract: Target-specific peptides, such as conotoxins, exhibit exceptional binding
affinity and selectivity toward ion channels and receptors. However, their
therapeutic potential remains underutilized due to the limited diversity of
natural variants and the labor-intensive nature of traditional optimization
strategies. Here, we present CreoPep, a deep learning-based conditional
generative framework that integrates masked language modeling with a
progressive masking scheme to design high-affinity peptide mutants while
uncovering novel structural motifs. CreoPep employs an integrative augmentation
pipeline, combining FoldX-based energy screening with temperature-controlled
multinomial sampling, to generate structurally and functionally diverse
peptides that retain key pharmacological properties. We validate this approach
by designing conotoxin inhibitors targeting the $\alpha$7 nicotinic
acetylcholine receptor, achieving submicromolar potency in electrophysiological
assays. Structural analysis reveals that CreoPep-generated variants engage in
both conserved and novel binding modes, including disulfide-deficient forms,
thus expanding beyond conventional design paradigms. Overall, CreoPep offers a
robust and generalizable platform that bridges computational peptide design
with experimental validation, accelerating the discovery of next-generation
peptide therapeutics.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [168] [Enhancing Target-unspecific Tasks through a Features Matrix](https://arxiv.org/abs/2505.03414)
*Fangming Cui,Yonggang Zhang,Xuan Wang,Xinmei Tian,Jun Yu*

Main category: cs.CV

TL;DR: 提出了一种特征矩阵（FM）正则化方法，用于提升大型视觉语言模型在非特定目标任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 现有提示学习方法在非特定目标或泛化任务上表现不佳，可能因过拟合训练导致模型遗忘通用知识。

Method: 通过提取和利用通用知识构建特征矩阵（FM），从深度和细粒度角度捕捉输入语义，避免过拟合。

Result: FM作为通用模块兼容现有框架，显著提升非特定目标任务的性能，达到最先进水平。

Conclusion: FM方法有效解决了过拟合问题，提升了模型在非特定目标任务上的泛化能力。

Abstract: Recent developments in prompt learning of large vision-language models have
significantly improved performance in target-specific tasks. However, these
prompt optimizing methods often struggle to tackle the target-unspecific or
generalizable tasks effectively. It may be attributed to the fact that
overfitting training causes the model to forget its general knowledge having
strong promotion on target-unspecific tasks. To alleviate this issue, we
propose a novel Features Matrix (FM) regularization approach designed to
enhance these models on target-unspecific tasks. Our method extracts and
leverages general knowledge, shaping a Features Matrix (FM). Specifically, the
FM captures the semantics of diverse inputs from a deep and fine perspective,
preserving essential general knowledge, which mitigates the risk of
overfitting. Representative evaluations demonstrate that: 1) the FM is
compatible with existing frameworks as a generic and flexible module, and 2)
the FM significantly showcases its effectiveness in enhancing target-unspecific
tasks, achieving state-of-the-art performance.

</details>


### [169] [Generating Narrated Lecture Videos from Slides with Synchronized Highlights](https://arxiv.org/abs/2505.02966)
*Alexander Holmberg*

Main category: cs.CV

TL;DR: 该系统通过AI生成同步的视觉高亮和语音讲解，将静态幻灯片自动转化为视频讲座，显著降低了时间和成本。


<details>
  <summary>Details</summary>
Motivation: 将静态幻灯片转化为视频讲座需要大量时间和精力，现有方法效率低下且成本高昂。

Method: 系统采用新颖的高亮对齐模块，结合多种策略（如Levenshtein距离和LLM语义分析）和TTS技术，实现语音与视觉高亮的精确同步。

Result: 在1000个样本的测试中，LLM对齐方法达到高精度（F1 > 92%），生成成本低于1美元/小时视频。

Conclusion: 该系统高效、低成本，为幻灯片转视频提供了实用且可扩展的解决方案。

Abstract: Turning static slides into engaging video lectures takes considerable time
and effort, requiring presenters to record explanations and visually guide
their audience through the material. We introduce an end-to-end system designed
to automate this process entirely. Given a slide deck, this system synthesizes
a video lecture featuring AI-generated narration synchronized precisely with
dynamic visual highlights. These highlights automatically draw attention to the
specific concept being discussed, much like an effective presenter would. The
core technical contribution is a novel highlight alignment module. This module
accurately maps spoken phrases to locations on a given slide using diverse
strategies (e.g., Levenshtein distance, LLM-based semantic analysis) at
selectable granularities (line or word level) and utilizes timestamp-providing
Text-to-Speech (TTS) for timing synchronization. We demonstrate the system's
effectiveness through a technical evaluation using a manually annotated slide
dataset with 1000 samples, finding that LLM-based alignment achieves high
location accuracy (F1 > 92%), significantly outperforming simpler methods,
especially on complex, math-heavy content. Furthermore, the calculated
generation cost averages under $1 per hour of video, offering potential savings
of two orders of magnitude compared to conservative estimates of manual
production costs. This combination of high accuracy and extremely low cost
positions this approach as a practical and scalable tool for transforming
static slides into effective, visually-guided video lectures.

</details>


### [170] [Lesion-Aware Generative Artificial Intelligence for Virtual Contrast-Enhanced Mammography in Breast Cancer](https://arxiv.org/abs/2505.03018)
*Aurora Rofena,Arianna Manchia,Claudia Lucia Piccolo,Bruno Beomonte Zobel,Paolo Soda,Valerio Guarrasi*

Main category: cs.CV

TL;DR: Seg-CycleGAN是一种生成性深度学习框架，用于在CESM中实现虚拟对比增强，通过从低能量图像合成高质量的双能量减影图像，减少辐射和对比剂的使用。


<details>
  <summary>Details</summary>
Motivation: CESM虽然提高了诊断准确性，但存在辐射和对比剂副作用的问题，需要一种无对比剂的替代方案。

Method: 提出Seg-CycleGAN，利用病灶分割图指导生成过程，改进病灶重建，并在CycleGAN基础上引入局部损失项。

Result: 在CESM@UCBM数据集上，Seg-CycleGAN在PSNR和SSIM上优于基线，同时保持竞争力的MSE和VIF，定性评估显示病灶保真度更高。

Conclusion: Seg-CycleGAN为无对比剂的CESM替代方案提供了可行的途径。

Abstract: Contrast-Enhanced Spectral Mammography (CESM) is a dual-energy mammographic
technique that improves lesion visibility through the administration of an
iodinated contrast agent. It acquires both a low-energy image, comparable to
standard mammography, and a high-energy image, which are then combined to
produce a dual-energy subtracted image highlighting lesion contrast
enhancement. While CESM offers superior diagnostic accuracy compared to
standard mammography, its use entails higher radiation exposure and potential
side effects associated with the contrast medium. To address these limitations,
we propose Seg-CycleGAN, a generative deep learning framework for Virtual
Contrast Enhancement in CESM. The model synthesizes high-fidelity dual-energy
subtracted images from low-energy images, leveraging lesion segmentation maps
to guide the generative process and improve lesion reconstruction. Building
upon the standard CycleGAN architecture, Seg-CycleGAN introduces localized loss
terms focused on lesion areas, enhancing the synthesis of diagnostically
relevant regions. Experiments on the CESM@UCBM dataset demonstrate that
Seg-CycleGAN outperforms the baseline in terms of PSNR and SSIM, while
maintaining competitive MSE and VIF. Qualitative evaluations further confirm
improved lesion fidelity in the generated images. These results suggest that
segmentation-aware generative models offer a viable pathway toward
contrast-free CESM alternatives.

</details>


### [171] [VISLIX: An XAI Framework for Validating Vision Models with Slice Discovery and Analysis](https://arxiv.org/abs/2505.03132)
*Xinyuan Yan,Xiwei Xuan,Jorge Piazentin Ono,Jiajing Guo,Vikram Mohanty,Shekar Arvind Kumar,Liang Gou,Bei Wang,Liu Ren*

Main category: cs.CV

TL;DR: VISLIX是一个新型视觉分析框架，利用基础模型帮助专家分析计算机视觉模型中的数据切片，无需额外元数据，支持交互式测试。


<details>
  <summary>Details</summary>
Motivation: 现实中的机器学习模型（如自动驾驶和监控）需要严格评估，但现有数据切片方法存在依赖元数据、缺乏交互性等问题。

Method: 提出VISLIX框架，结合基础模型自动生成自然语言见解，支持用户交互式测试数据切片假设。

Result: 通过专家研究和三个用例验证，VISLIX在验证目标检测模型时提供了全面见解。

Conclusion: VISLIX克服了现有数据切片方法的局限，为计算机视觉模型验证提供了高效工具。

Abstract: Real-world machine learning models require rigorous evaluation before
deployment, especially in safety-critical domains like autonomous driving and
surveillance. The evaluation of machine learning models often focuses on data
slices, which are subsets of the data that share a set of characteristics. Data
slice finding automatically identifies conditions or data subgroups where
models underperform, aiding developers in mitigating performance issues.
Despite its popularity and effectiveness, data slicing for vision model
validation faces several challenges. First, data slicing often needs additional
image metadata or visual concepts, and falls short in certain computer vision
tasks, such as object detection. Second, understanding data slices is a
labor-intensive and mentally demanding process that heavily relies on the
expert's domain knowledge. Third, data slicing lacks a human-in-the-loop
solution that allows experts to form hypothesis and test them interactively. To
overcome these limitations and better support the machine learning operations
lifecycle, we introduce VISLIX, a novel visual analytics framework that employs
state-of-the-art foundation models to help domain experts analyze slices in
computer vision models. Our approach does not require image metadata or visual
concepts, automatically generates natural language insights, and allows users
to test data slice hypothesis interactively. We evaluate VISLIX with an expert
study and three use cases, that demonstrate the effectiveness of our tool in
providing comprehensive insights for validating object detection models.

</details>


### [172] [Motion-compensated cardiac MRI using low-rank diffeomorphic flow (DMoCo)](https://arxiv.org/abs/2505.03149)
*Joseph William Kettelkamp,Ludovica Romanin,Sarv Priya,Mathews Jacob*

Main category: cs.CV

TL;DR: 提出了一种无监督运动补偿图像重建算法，用于自由呼吸和无门控3D心脏磁共振成像（MRI）。


<details>
  <summary>Details</summary>
Motivation: 解决自由呼吸和无门控3D心脏MRI中运动伪影的问题，提高图像重建质量。

Method: 通过将每个运动阶段的图像体积表示为单个静态图像模板的变形，并使用低秩模型对变形家族进行紧凑联合表示。

Result: 与现有运动分辨和运动补偿算法相比，提出的低秩运动模型显著提高了图像恢复质量。

Conclusion: 该算法通过无监督学习静态模板和低秩运动模型参数，有效提升了自由呼吸3D心脏MRI的图像重建效果。

Abstract: We introduce an unsupervised motion-compensated image reconstruction
algorithm for free-breathing and ungated 3D cardiac magnetic resonance imaging
(MRI). We express the image volume corresponding to each specific motion phase
as the deformation of a single static image template. The main contribution of
the work is the low-rank model for the compact joint representation of the
family of diffeomorphisms, parameterized by the motion phases. The
diffeomorphism at a specific motion phase is obtained by integrating a
parametric velocity field along a path connecting the reference template phase
to the motion phase. The velocity field at different phases is represented
using a low-rank model. The static template and the low-rank motion model
parameters are learned directly from the k-space data in an unsupervised
fashion. The more constrained motion model is observed to offer improved
recovery compared to current motion-resolved and motion-compensated algorithms
for free-breathing 3D cine MRI.

</details>


### [173] [StableMotion: Training Motion Cleanup Models with Unpaired Corrupted Data](https://arxiv.org/abs/2505.03154)
*Yuxuan Mu,Hung Yu Ling,Yi Shi,Ismael Baira Ojeda,Pengcheng Xi,Chang Shu,Fabio Zinno,Xue Bin Peng*

Main category: cs.CV

TL;DR: StableMotion提出了一种直接从无配对数据中训练运动清理模型的方法，通过引入运动质量指示器，无需高质量配对数据即可实现运动修复。


<details>
  <summary>Details</summary>
Motivation: 运动捕捉数据常因传感器和后期处理问题产生视觉瑕疵，手动清理成本高且耗时。现有数据驱动方法需要配对数据，而高质量配对数据获取困难。

Method: 引入运动质量指示器，通过手动标注或启发式算法标注，训练质量感知运动生成模型。采用基于扩散的框架，实现运动生成与判别一体化。

Result: 在SoccerMocap数据集上测试，模型有效修复多种运动瑕疵，运动跳跃和冻结帧分别减少68%和81%。

Conclusion: StableMotion提供了一种高效的运动清理方法，无需配对数据，适用于实际生产场景。

Abstract: Motion capture (mocap) data often exhibits visually jarring artifacts due to
inaccurate sensors and post-processing. Cleaning this corrupted data can
require substantial manual effort from human experts, which can be a costly and
time-consuming process. Previous data-driven motion cleanup methods offer the
promise of automating this cleanup process, but often require in-domain paired
corrupted-to-clean training data. Constructing such paired datasets requires
access to high-quality, relatively artifact-free motion clips, which often
necessitates laborious manual cleanup. In this work, we present StableMotion, a
simple yet effective method for training motion cleanup models directly from
unpaired corrupted datasets that need cleanup. The core component of our method
is the introduction of motion quality indicators, which can be easily annotated
through manual labeling or heuristic algorithms and enable training of
quality-aware motion generation models on raw motion data with mixed quality.
At test time, the model can be prompted to generate high-quality motions using
the quality indicators. Our method can be implemented through a simple
diffusion-based framework, leading to a unified motion generate-discriminate
model, which can be used to both identify and fix corrupted frames. We
demonstrate that our proposed method is effective for training motion cleanup
models on raw mocap data in production scenarios by applying StableMotion to
SoccerMocap, a 245-hour soccer mocap dataset containing real-world motion
artifacts. The trained model effectively corrects a wide range of motion
artifacts, reducing motion pops and frozen frames by 68% and 81%, respectively.
See https://youtu.be/3Y7MMAH02B4 for more results.

</details>


### [174] [RAVU: Retrieval Augmented Video Understanding with Compositional Reasoning over Graph](https://arxiv.org/abs/2505.03173)
*Sameer Malik,Moyuru Yamada,Ayush Singh,Dishank Aggarwal*

Main category: cs.CV

TL;DR: RAVU框架通过检索增强和时空图推理提升长视频理解能力，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前大型多模态模型（LMMs）因缺乏显式记忆和检索机制，难以处理长视频。

Method: 构建视频的时空图表示，作为长期记忆，并通过分解查询为推理步骤在图检索关键信息。

Result: 在NExT-QA和EgoSchema数据集上，RAVU以少量检索帧（5-10）显著优于SOTA方法。

Conclusion: RAVU通过检索增强和时空图推理，有效解决了长视频理解中的多跳推理和对象追踪问题。

Abstract: Comprehending long videos remains a significant challenge for Large
Multi-modal Models (LMMs). Current LMMs struggle to process even minutes to
hours videos due to their lack of explicit memory and retrieval mechanisms. To
address this limitation, we propose RAVU (Retrieval Augmented Video
Understanding), a novel framework for video understanding enhanced by retrieval
with compositional reasoning over a spatio-temporal graph. We construct a graph
representation of the video, capturing both spatial and temporal relationships
between entities. This graph serves as a long-term memory, allowing us to track
objects and their actions across time. To answer complex queries, we decompose
the queries into a sequence of reasoning steps and execute these steps on the
graph, retrieving relevant key information. Our approach enables more accurate
understanding of long videos, particularly for queries that require multi-hop
reasoning and tracking objects across frames. Our approach demonstrate superior
performances with limited retrieved frames (5-10) compared with other SOTA
methods and baselines on two major video QA datasets, NExT-QA and EgoSchema.

</details>


### [175] [seq-JEPA: Autoregressive Predictive Learning of Invariant-Equivariant World Models](https://arxiv.org/abs/2505.03176)
*Hafez Ghaemi,Eilif Muller,Shahab Bakhtiari*

Main category: cs.CV

TL;DR: seq-JEPA是一种基于联合嵌入预测架构的世界建模范式，通过架构归纳偏置解决自监督学习中不变性与等变性任务的性能权衡问题。


<details>
  <summary>Details</summary>
Motivation: 当前自监督算法主要依赖数据增强和掩码等变换学习视觉表示，但两视图范式限制了表示灵活性，导致不变性任务（如图像分类）与等变性任务之间存在性能权衡。

Method: seq-JEPA通过处理输入图像的不同视图序列，学习两种架构分离的表示：一种对指定变换等变，另一种对其不变。模型使用Transformer编码器聚合序列表示，并预测下一视图的表示。

Result: seq-JEPA在等变性基准和图像分类任务中表现优异，且无需牺牲任一任务性能，同时在序列观察聚合任务（如路径整合）中表现突出。

Conclusion: seq-JEPA通过架构设计解决了自监督学习中的不变性与等变性权衡问题，为多任务学习提供了灵活且高效的解决方案。

Abstract: Current self-supervised algorithms mostly rely on transformations such as
data augmentation and masking to learn visual representations. This is achieved
by inducing invariance or equivariance with respect to these transformations
after encoding two views of an image. This dominant two-view paradigm can limit
the flexibility of learned representations for downstream adaptation by
creating performance trade-offs between invariance-related tasks such as image
classification and more fine-grained equivariance-related tasks. In this work,
we introduce \emph{seq-JEPA}, a world modeling paradigm based on
joint-embedding predictive architecture that leverages architectural inductive
biases to resolve this trade-off. Without requiring an additional equivariance
predictor or loss term, seq-JEPA simultaneously learns two architecturally
segregated representations: one equivariant to the specified transformations
and another invariant to them and suited for tasks such as classification. To
do so, our model processes a short sequence of different views (observations)
of an input image. Each encoded view is concatenated with embeddings
corresponding to the relative transformation (action) producing the next
observation in the sequence. A transformer encoder outputs an aggregate
representation of this sequence, which is subsequently conditioned on the
action leading to the next observation to predict its representation.
Empirically, seq-JEPA achieves strong performance on equivariant benchmarks and
image classification without sacrificing one for the other. Additionally, our
framework excels at tasks that inherently require aggregating a sequence of
observations, such as path integration across actions and predictive learning
across eye movements.

</details>


### [176] [DCS-ST for Classification of Breast Cancer Histopathology Images with Limited Annotations](https://arxiv.org/abs/2505.03204)
*Liu Suxing,Byungwon Min*

Main category: cs.CV

TL;DR: 深度学习在乳腺癌病理图像分类中表现良好，但在标注数据有限时性能下降。


<details>
  <summary>Details</summary>
Motivation: 医疗影像标注成本高且需要专业知识，导致标注数据有限，影响深度学习模型的性能。

Method: 使用深度学习方法对乳腺癌病理图像进行分类。

Result: 在标注数据有限的情况下，深度学习模型的性能显著下降。

Conclusion: 需要解决标注数据有限的问题以提升深度学习在乳腺癌病理图像分类中的表现。

Abstract: Deep learning methods have shown promise in classifying breast cancer
histopathology images, but their performance often declines with limited
annotated data, a critical challenge in medical imaging due to the high cost
and expertise required for annotations.

</details>


### [177] [Enhancing Glass Defect Detection with Diffusion Models: Addressing Imbalanced Datasets in Manufacturing Quality Control](https://arxiv.org/abs/2505.03134)
*Sajjad Rezvani Boroujeni,Hossein Abedi,Tom Bush*

Main category: cs.CV

TL;DR: 论文提出了一种基于去噪扩散概率模型（DDPMs）的方法，通过生成合成缺陷玻璃产品图像来解决工业玻璃制造中数据不平衡问题，显著提升了缺陷检测的准确性。


<details>
  <summary>Details</summary>
Motivation: 工业玻璃制造中视觉缺陷检测因缺陷产品频率低导致数据不平衡，限制了深度学习模型的性能。

Method: 使用DDPMs生成合成缺陷图像进行数据增强，提升少数类样本的表示，并测试了ResNet50V2、EfficientNetB0和MobileNetV2等CNN架构。

Result: 实验结果显示，所有测试的深度学习架构在缺陷样本的召回率上均有显著提升，ResNet50V2的分类准确率从78%提高到93%。

Conclusion: 该方法为玻璃制造中的自动化缺陷检测提供了一种可扩展且经济高效的解决方案，并可推广至其他工业质量保证系统。

Abstract: Visual defect detection in industrial glass manufacturing remains a critical
challenge due to the low frequency of defective products, leading to imbalanced
datasets that limit the performance of deep learning models and computer vision
systems. This paper presents a novel approach using Denoising Diffusion
Probabilistic Models (DDPMs) to generate synthetic defective glass product
images for data augmentation, effectively addressing class imbalance issues in
manufacturing quality control and automated visual inspection. The methodology
significantly enhances image classification performance of standard CNN
architectures (ResNet50V2, EfficientNetB0, and MobileNetV2) in detecting
anomalies by increasing the minority class representation. Experimental results
demonstrate substantial improvements in key machine learning metrics,
particularly in recall for defective samples across all tested deep neural
network architectures while maintaining perfect precision. The most dramatic
improvement was observed in ResNet50V2's overall classification accuracy, which
increased from 78 percent to 93 percent when trained with the augmented data.
This work provides a scalable, cost-effective approach to enhancing automated
defect detection in glass manufacturing that can potentially be extended to
other industrial quality assurance systems and industries with similar class
imbalance challenges.

</details>


### [178] [Seeing the Abstract: Translating the Abstract Language for Vision Language Models](https://arxiv.org/abs/2505.03242)
*Davide Talon,Federico Girella,Ziyue Liu,Marco Cristani,Yiming Wang*

Main category: cs.CV

TL;DR: 论文提出了一种无需训练的方法ACT，通过将抽象语言映射到具体语言，提升视觉语言模型在抽象语言任务中的表现，尤其在时尚领域取得了显著效果。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型（VLMs）主要关注具体内容，忽略了抽象语言的广泛存在和价值，尤其是在时尚领域。论文旨在填补这一空白。

Method: 提出Abstract-to-Concrete Translator (ACT)，利用预训练模型和现有多模态数据库，将抽象表示映射到具体表示，无需额外训练。

Result: ACT在文本到图像检索任务中表现优于微调的VLMs，具有强泛化能力，且适用于多种VLMs。

Conclusion: ACT是一种即插即用的解决方案，显著提升了VLMs处理抽象语言的能力。

Abstract: Natural language goes beyond dryly describing visual content. It contains
rich abstract concepts to express feeling, creativity and properties that
cannot be directly perceived. Yet, current research in Vision Language Models
(VLMs) has not shed light on abstract-oriented language. Our research breaks
new ground by uncovering its wide presence and under-estimated value, with
extensive analysis. Particularly, we focus our investigation on the fashion
domain, a highly-representative field with abstract expressions. By analyzing
recent large-scale multimodal fashion datasets, we find that abstract terms
have a dominant presence, rivaling the concrete ones, providing novel
information, and being useful in the retrieval task. However, a critical
challenge emerges: current general-purpose or fashion-specific VLMs are
pre-trained with databases that lack sufficient abstract words in their text
corpora, thus hindering their ability to effectively represent
abstract-oriented language. We propose a training-free and model-agnostic
method, Abstract-to-Concrete Translator (ACT), to shift abstract
representations towards well-represented concrete ones in the VLM latent space,
using pre-trained models and existing multimodal databases. On the
text-to-image retrieval task, despite being training-free, ACT outperforms the
fine-tuned VLMs in both same- and cross-dataset settings, exhibiting its
effectiveness with a strong generalization capability. Moreover, the
improvement introduced by ACT is consistent with various VLMs, making it a
plug-and-play solution.

</details>


### [179] [Towards Efficient Benchmarking of Foundation Models in Remote Sensing: A Capabilities Encoding Approach](https://arxiv.org/abs/2505.03299)
*Pierre Adorni,Minh-Tan Pham,Stéphane May,Sébastien Lefèvre*

Main category: cs.CV

TL;DR: 提出了一种基于“能力编码”的方法，用于预测基础模型在多个下游任务中的性能，无需微调，简化模型选择并促进未来研究。


<details>
  <summary>Details</summary>
Motivation: 尽管已有超过75个遥感视觉基础模型，但尚无一个模型在所有下游任务中表现一致最优，因此需要一种方法来比较和选择模型。

Method: 提出“能力编码”方法，通过编码模型的能力来预测其在下游任务中的性能，避免对每个任务进行微调。

Result: 该方法能有效简化基础模型的选择，并为现有文献提供新视角，推动未来研究方向。

Conclusion: 能力编码方法为遥感视觉基础模型的比较和选择提供了实用工具，同时为研究领域开辟了新方向。

Abstract: Foundation models constitute a significant advancement in computer vision:
after a single, albeit costly, training phase, they can address a wide array of
tasks. In the field of Earth observation, over 75 remote sensing vision
foundation models have been developed in the past four years. However, none has
consistently outperformed the others across all available downstream tasks. To
facilitate their comparison, we propose a cost-effective method for predicting
a model's performance on multiple downstream tasks without the need for
fine-tuning on each one. This method is based on what we call "capabilities
encoding." The utility of this novel approach is twofold: we demonstrate its
potential to simplify the selection of a foundation model for a given new task,
and we employ it to offer a fresh perspective on the existing literature,
suggesting avenues for future research. Codes are available at
https://github.com/pierreadorni/capabilities-encoding.

</details>


### [180] [Comparative Analysis of Lightweight Deep Learning Models for Memory-Constrained Devices](https://arxiv.org/abs/2505.03303)
*Tasnim Shahriar*

Main category: cs.CV

TL;DR: 该论文评估了五种轻量级深度学习模型在图像分类任务中的表现，重点关注其在资源受限环境中的适用性。研究发现，迁移学习显著提升模型性能，EfficientNetV2-S准确率最高，MobileNetV3-Small在准确率与效率间取得最佳平衡，SqueezeNet在推理速度和模型大小上表现最优。


<details>
  <summary>Details</summary>
Motivation: 研究轻量级深度学习模型在资源受限环境（如低内存设备）中的部署潜力，为边缘计算和移动平台提供优化方案。

Method: 对五种模型（MobileNetV3 Small、ResNet18、SqueezeNet、EfficientNetV2-S、ShuffleNetV2）在CIFAR-10、CIFAR-100和Tiny ImageNet数据集上进行基准测试，评估分类准确率、推理时间、FLOPs和模型大小，并研究超参数调优、数据增强和训练范式的影响。

Result: 迁移学习显著提升模型性能，EfficientNetV2-S准确率最高，MobileNetV3-Small在准确率与效率间平衡最佳，SqueezeNet在推理速度和模型大小上表现最优。

Conclusion: 研究揭示了准确率与效率之间的权衡，为资源受限环境中的轻量级模型部署提供了实用指导，推动了边缘计算和移动平台的深度学习优化。

Abstract: This paper presents a comprehensive evaluation of lightweight deep learning
models for image classification, emphasizing their suitability for deployment
in resource-constrained environments such as low-memory devices. Five
state-of-the-art architectures - MobileNetV3 Small, ResNet18, SqueezeNet,
EfficientNetV2-S, and ShuffleNetV2 - are benchmarked across three diverse
datasets: CIFAR-10, CIFAR-100, and Tiny ImageNet. The models are assessed using
four key performance metrics: classification accuracy, inference time,
floating-point operations (FLOPs), and model size. Additionally, we investigate
the impact of hyperparameter tuning, data augmentation, and training paradigms
by comparing pretrained models with scratch-trained counterparts, focusing on
MobileNetV3 Small. Our findings reveal that transfer learning significantly
enhances model accuracy and computational efficiency, particularly for complex
datasets like Tiny ImageNet. EfficientNetV2 consistently achieves the highest
accuracy, while MobileNetV3 offers the best balance between accuracy and
efficiency, and SqueezeNet excels in inference speed and compactness. This
study highlights critical trade-offs between accuracy and efficiency, offering
actionable insights for deploying lightweight models in real-world applications
where computational resources are limited. By addressing these challenges, this
research contributes to optimizing deep learning systems for edge computing and
mobile platforms.

</details>


### [181] [PROM: Prioritize Reduction of Multiplications Over Lower Bit-Widths for Efficient CNNs](https://arxiv.org/abs/2505.03254)
*Lukas Meiner,Jens Mehnert,Alexandru Paul Condurache*

Main category: cs.CV

TL;DR: PROM是一种针对深度可分离卷积网络的量化方法，通过选择性使用三元和8位权重，显著降低能耗和存储需求。


<details>
  <summary>Details</summary>
Motivation: 现代深度可分离卷积网络中，计算成本分布不均，现有量化方法未能充分利用效率潜力。

Method: PROM采用两种位宽量化：点卷积使用三元权重，其余模块使用8位权重，并通过量化感知训练实现。

Result: 在MobileNetV2上，PROM将能耗降低23.9倍，存储需求减少2.7倍，同时保持分类性能。

Conclusion: PROM为深度可分离卷积网络提供了一种简单高效的量化方案，显著提升了能耗与准确率的平衡。

Abstract: Convolutional neural networks (CNNs) are crucial for computer vision tasks on
resource-constrained devices. Quantization effectively compresses these models,
reducing storage size and energy cost. However, in modern depthwise-separable
architectures, the computational cost is distributed unevenly across its
components, with pointwise operations being the most expensive. By applying a
general quantization scheme to this imbalanced cost distribution, existing
quantization approaches fail to fully exploit potential efficiency gains. To
this end, we introduce PROM, a straightforward approach for quantizing modern
depthwise-separable convolutional networks by selectively using two distinct
bit-widths. Specifically, pointwise convolutions are quantized to ternary
weights, while the remaining modules use 8-bit weights, which is achieved
through a simple quantization-aware training procedure. Additionally, by
quantizing activations to 8-bit, our method transforms pointwise convolutions
with ternary weights into int8 additions, which enjoy broad support across
hardware platforms and effectively eliminates the need for expensive
multiplications. Applying PROM to MobileNetV2 reduces the model's energy cost
by more than an order of magnitude (23.9x) and its storage size by 2.7x
compared to the float16 baseline while retaining similar classification
performance on ImageNet. Our method advances the Pareto frontier for energy
consumption vs. top-1 accuracy for quantized convolutional models on ImageNet.
PROM addresses the challenges of quantizing depthwise-separable convolutional
networks to both ternary and 8-bit weights, offering a simple way to reduce
energy cost and storage size.

</details>


### [182] [SD-VSum: A Method and Dataset for Script-Driven Video Summarization](https://arxiv.org/abs/2505.03319)
*Manolis Mylonas,Evlampios Apostolidis,Vasileios Mezaris*

Main category: cs.CV

TL;DR: 论文提出了一种基于脚本的视频摘要任务（SD-VSum），通过用户提供的脚本选择视频中最相关的部分生成摘要。扩展了VideoXum数据集，并开发了新的跨模态注意力网络架构，实验证明其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有视频摘要方法通常忽略用户对摘要内容的个性化需求，因此需要一种能够根据用户脚本生成定制化摘要的方法。

Method: 扩展VideoXum数据集，加入自然语言描述；开发SD-VSum网络架构，利用跨模态注意力机制对齐和融合视觉与文本信息。

Result: SD-VSum在性能上优于现有的查询驱动和通用摘要方法，能够生成符合用户需求的摘要。

Conclusion: SD-VSum为视频摘要任务提供了一种有效的个性化解决方案，实验验证了其优越性。

Abstract: In this work, we introduce the task of script-driven video summarization,
which aims to produce a summary of the full-length video by selecting the parts
that are most relevant to a user-provided script outlining the visual content
of the desired summary. Following, we extend a recently-introduced large-scale
dataset for generic video summarization (VideoXum) by producing natural
language descriptions of the different human-annotated summaries that are
available per video. In this way we make it compatible with the introduced
task, since the available triplets of ``video, summary and summary
description'' can be used for training a method that is able to produce
different summaries for a given video, driven by the provided script about the
content of each summary. Finally, we develop a new network architecture for
script-driven video summarization (SD-VSum), that relies on the use of a
cross-modal attention mechanism for aligning and fusing information from the
visual and text modalities. Our experimental evaluations demonstrate the
advanced performance of SD-VSum against state-of-the-art approaches for
query-driven and generic (unimodal and multimodal) summarization from the
literature, and document its capacity to produce video summaries that are
adapted to each user's needs about their content.

</details>


### [183] [Very High-Resolution Forest Mapping with TanDEM-X InSAR Data and Self-Supervised Learning](https://arxiv.org/abs/2505.03327)
*José-Luis Bueso-Bello,Benjamin Chauvel,Daniel Carcereri,Philipp Posovszky,Pietro Milillo,Jennifer Ruiz,Juan-Carlos Fernández-Diaz,Carolina González,Michele Martone,Ronny Hänsch,Paola Rizzoli*

Main category: cs.CV

TL;DR: 论文提出了一种结合自监督学习和监督学习的方法，利用TanDEM-X数据实现6米分辨率森林制图，解决了高分辨率标注数据不足的问题。


<details>
  <summary>Details</summary>
Motivation: 利用TanDEM-X任务的高分辨率能力，克服中分辨率产品在检测狭窄道路和精确划定森林轮廓方面的局限性。

Method: 采用自监督学习技术从输入特征中提取高信息量表示，随后用少量可靠标签进行监督训练。

Result: 在亚马逊雨林的实际场景中，提出的自监督框架显著提高了分类准确性，优于完全监督方法。

Conclusion: 该方法为大规模、极高分辨率森林制图提供了极具前景的起点。

Abstract: Deep learning models have shown encouraging capabilities for mapping
accurately forests at medium resolution with TanDEM-X interferometric SAR data.
Such models, as most of current state-of-the-art deep learning techniques in
remote sensing, are trained in a fully-supervised way, which requires a large
amount of labeled data for training and validation. In this work, our aim is to
exploit the high-resolution capabilities of the TanDEM-X mission to map forests
at 6 m. The goal is to overcome the intrinsic limitations posed by
midresolution products, which affect, e.g., the detection of narrow roads
within vegetated areas and the precise delineation of forested regions
contours. To cope with the lack of extended reliable reference datasets at such
a high resolution, we investigate self-supervised learning techniques for
extracting highly informative representations from the input features, followed
by a supervised training step with a significantly smaller number of reliable
labels. A 1 m resolution forest/non-forest reference map over Pennsylvania,
USA, allows for comparing different training approaches for the development of
an effective forest mapping framework with limited labeled samples. We select
the best-performing approach over this test region and apply it in a real-case
forest mapping scenario over the Amazon rainforest, where only very few labeled
data at high resolution are available. In this challenging scenario, the
proposed self-supervised framework significantly enhances the classification
accuracy with respect to fully-supervised methods, trained using the same
amount of labeled data, representing an extremely promising starting point for
large-scale, very high-resolution forest mapping with TanDEM-X data.

</details>


### [184] [Reinforced Correlation Between Vision and Language for Precise Medical AI Assistant](https://arxiv.org/abs/2505.03380)
*Haonan Wang,Jiaji Mao,Lehan Wang,Qixiang Zhang,Marawan Elbatel,Yi Qin,Huijun Hu,Baoxun Li,Wenhui Deng,Weifeng Qin,Hongrui Li,Jialin Liang,Jun Shen,Xiaomeng Li*

Main category: cs.CV

TL;DR: RCMed是一种全栈AI助手，通过多模态对齐和分层视觉-语言基础，提升医学诊断的精确性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 解决医学AI助手在多模态内容准确性不足和真实场景验证不足的问题。

Method: 采用自增强相关机制和颜色区域描述策略，结合视觉特征与语言语义，形成闭环优化。

Result: 在165项临床任务中表现优异，细胞分割精度提升23.5%，并在20种癌症类型的外部验证中达到最佳性能。

Conclusion: RCMed展示了多模态模型在复杂场景中实现人类水平解释的能力，推动了以人为中心的AI医疗发展。

Abstract: Medical AI assistants support doctors in disease diagnosis, medical image
analysis, and report generation. However, they still face significant
challenges in clinical use, including limited accuracy with multimodal content
and insufficient validation in real-world settings. We propose RCMed, a
full-stack AI assistant that improves multimodal alignment in both input and
output, enabling precise anatomical delineation, accurate localization, and
reliable diagnosis through hierarchical vision-language grounding. A
self-reinforcing correlation mechanism allows visual features to inform
language context, while language semantics guide pixel-wise attention, forming
a closed loop that refines both modalities. This correlation is enhanced by a
color region description strategy, translating anatomical structures into
semantically rich text to learn shape-location-text relationships across
scales. Trained on 20 million image-mask-description triplets, RCMed achieves
state-of-the-art precision in contextualizing irregular lesions and subtle
anatomical boundaries, excelling in 165 clinical tasks across 9 modalities. It
achieved a 23.5% relative improvement in cell segmentation from microscopy
images over prior methods. RCMed's strong vision-language alignment enables
exceptional generalization, with state-of-the-art performance in external
validation across 20 clinically significant cancer types, including novel
tasks. This work demonstrates how integrated multimodal models capture
fine-grained patterns, enabling human-level interpretation in complex scenarios
and advancing human-centric AI healthcare.

</details>


### [185] [DDaTR: Dynamic Difference-aware Temporal Residual Network for Longitudinal Radiology Report Generation](https://arxiv.org/abs/2505.03401)
*Shanshan Song,Hui Tang,Honglong Yang,Xiaomeng Li*

Main category: cs.CV

TL;DR: 论文提出了一种动态差异感知时序残差网络（DDaTR），用于改进纵向放射学报告生成（LRRG）任务，通过捕捉多级空间和时间相关性，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有LRRG方法在特征提取过程中难以有效捕捉空间和时间相关性，导致跨检查差异信息不足，影响报告生成质量。

Method: 提出DDaTR网络，包含动态特征对齐模块（DFAM）和动态差异感知模块（DDAM），分别用于对齐多模态特征和捕捉跨检查差异信息，同时使用动态残差网络建模时间相关性。

Result: 在三个基准测试中，DDaTR显著优于现有方法，证明了其在RRG和LRRG任务中的有效性。

Conclusion: DDaTR通过动态捕捉空间和时间相关性，有效解决了LRRG任务中的特征提取不足问题，提升了报告生成质量。

Abstract: Radiology Report Generation (RRG) automates the creation of radiology reports
from medical imaging, enhancing the efficiency of the reporting process.
Longitudinal Radiology Report Generation (LRRG) extends RRG by incorporating
the ability to compare current and prior exams, facilitating the tracking of
temporal changes in clinical findings. Existing LRRG approaches only extract
features from prior and current images using a visual pre-trained encoder,
which are then concatenated to generate the final report. However, these
methods struggle to effectively capture both spatial and temporal correlations
during the feature extraction process. Consequently, the extracted features
inadequately capture the information of difference across exams and thus
underrepresent the expected progressions, leading to sub-optimal performance in
LRRG. To address this, we develop a novel dynamic difference-aware temporal
residual network (DDaTR). In DDaTR, we introduce two modules at each stage of
the visual encoder to capture multi-level spatial correlations. The Dynamic
Feature Alignment Module (DFAM) is designed to align prior features across
modalities for the integrity of prior clinical information. Prompted by the
enriched prior features, the dynamic difference-aware module (DDAM) captures
favorable difference information by identifying relationships across exams.
Furthermore, our DDaTR employs the dynamic residual network to unidirectionally
transmit longitudinal information, effectively modelling temporal correlations.
Extensive experiments demonstrated superior performance over existing methods
on three benchmarks, proving its efficacy in both RRG and LRRG tasks.

</details>


### [186] [Blending 3D Geometry and Machine Learning for Multi-View Stereopsis](https://arxiv.org/abs/2505.03470)
*Vibhas Vats,Md. Alimoor Reza,David Crandall,Soon-heung Jung*

Main category: cs.CV

TL;DR: GC MVSNet++通过在学习阶段主动强制执行多视图、多尺度的几何一致性，显著加速学习过程，并在多个数据集上取得领先性能。


<details>
  <summary>Details</summary>
Motivation: 传统MVS方法依赖光度一致性，而现代学习算法仅在后期处理中应用几何一致性检查，未影响学习过程。本文旨在通过在学习阶段集成几何一致性检查，提升性能。

Method: 提出GC MVSNet++，在学习阶段强制执行多视图、多尺度的几何一致性检查，并设计密集连接的成本正则化网络。

Result: 在DTU和BlendedMVS数据集上达到新SOTA，在Tanks and Temples基准上排名第二。

Conclusion: GC MVSNet++是首个在学习阶段强制执行多视图、多尺度几何一致性的方法，显著提升性能并加速训练。

Abstract: Traditional multi-view stereo (MVS) methods primarily depend on photometric
and geometric consistency constraints. In contrast, modern learning-based
algorithms often rely on the plane sweep algorithm to infer 3D geometry,
applying explicit geometric consistency (GC) checks only as a post-processing
step, with no impact on the learning process itself. In this work, we introduce
GC MVSNet plus plus, a novel approach that actively enforces geometric
consistency of reference view depth maps across multiple source views (multi
view) and at various scales (multi scale) during the learning phase (see Fig.
1). This integrated GC check significantly accelerates the learning process by
directly penalizing geometrically inconsistent pixels, effectively halving the
number of training iterations compared to other MVS methods. Furthermore, we
introduce a densely connected cost regularization network with two distinct
block designs simple and feature dense optimized to harness dense feature
connections for enhanced regularization. Extensive experiments demonstrate that
our approach achieves a new state of the art on the DTU and BlendedMVS datasets
and secures second place on the Tanks and Temples benchmark. To our knowledge,
GC MVSNet plus plus is the first method to enforce multi-view, multi-scale
supervised geometric consistency during learning. Our code is available.

</details>


### [187] [Phenotype-Guided Generative Model for High-Fidelity Cardiac MRI Synthesis: Advancing Pretraining and Clinical Applications](https://arxiv.org/abs/2505.03426)
*Ziyu Li,Yujian Hu,Zhengyao Ding,Yiheng Mao,Haitao Li,Fan Yi,Hongkun Zhang,Zhengxing Huang*

Main category: cs.CV

TL;DR: 提出了一种名为CPGG的新方法，通过生成多样化的CMR数据来解决AI模型在心脏疾病诊断中数据不足的问题。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏大规模高质量的CMR数据集，AI模型在心脏疾病诊断中的应用受到限制。

Method: CPGG框架分为两阶段：首先生成模型基于CMR数据的心脏表型训练，随后通过掩码自回归扩散模型生成高保真CMR序列。

Result: 实验表明，CPGG生成的高质量合成数据显著提升了多种下游任务的性能。

Conclusion: CPGG方法有效解决了数据不足问题，为AI在心脏疾病诊断中的应用提供了新思路。

Abstract: Cardiac Magnetic Resonance (CMR) imaging is a vital non-invasive tool for
diagnosing heart diseases and evaluating cardiac health. However, the limited
availability of large-scale, high-quality CMR datasets poses a major challenge
to the effective application of artificial intelligence (AI) in this domain.
Even the amount of unlabeled data and the health status it covers are difficult
to meet the needs of model pretraining, which hinders the performance of AI
models on downstream tasks. In this study, we present Cardiac Phenotype-Guided
CMR Generation (CPGG), a novel approach for generating diverse CMR data that
covers a wide spectrum of cardiac health status. The CPGG framework consists of
two stages: in the first stage, a generative model is trained using cardiac
phenotypes derived from CMR data; in the second stage, a masked autoregressive
diffusion model, conditioned on these phenotypes, generates high-fidelity CMR
cine sequences that capture both structural and functional features of the
heart in a fine-grained manner. We synthesized a massive amount of CMR to
expand the pretraining data. Experimental results show that CPGG generates
high-quality synthetic CMR data, significantly improving performance on various
downstream tasks, including diagnosis and cardiac phenotypes prediction. These
gains are demonstrated across both public and private datasets, highlighting
the effectiveness of our approach. Code is availabel at
https://anonymous.4open.science/r/CPGG.

</details>


### [188] [Optimization of Module Transferability in Single Image Super-Resolution: Universality Assessment and Cycle Residual Blocks](https://arxiv.org/abs/2505.03522)
*Haotong Cheng,Zhiqi Zhang,Hao Li,Xinshang Zhang*

Main category: cs.CV

TL;DR: 论文提出了“通用性”概念及评估方程（UAE），用于量化模块的可移植性，并设计了两种优化模块（CRB和DCRB），实验表明其在多个任务中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注性能提升，而忽视了模块的可移植性量化，本文旨在填补这一空白。

Method: 引入“通用性”概念及UAE评估方程，设计优化模块CRB和DCRB，并在多场景实验中验证。

Result: 优化模块在多个数据集上表现优异，PSNR提升达0.83dB，参数减少71.3%且重建质量几乎无损。

Conclusion: 模块通用性与模型泛化能力相关，优化模块在多任务中具有显著优势。

Abstract: Deep learning has substantially advanced the Single Image Super-Resolution
(SISR). However, existing researches have predominantly focused on raw
performance gains, with little attention paid to quantifying the
transferability of architectural components. In this paper, we introduce the
concept of "Universality" and its associated definitions which extend the
traditional notion of "Generalization" to encompass the modules' ease of
transferability, thus revealing the relationships between module universality
and model generalizability. Then we propose the Universality Assessment
Equation (UAE), a metric for quantifying how readily a given module could be
transplanted across models. Guided by the UAE results of standard residual
blocks and other plug-and-play modules, we further design two optimized
modules, Cycle Residual Block (CRB) and Depth-Wise Cycle Residual Block (DCRB).
Through comprehensive experiments on natural-scene benchmarks, remote-sensing
datasets, extreme-industrial imagery and on-device deployments, we demonstrate
that networks embedded with the proposed plug-and-play modules outperform
several state-of-the-arts, reaching a PSNR enhancement of up to 0.83dB or
enabling a 71.3% reduction in parameters with negligible loss in reconstruction
fidelity.

</details>


### [189] [Generating Synthetic Data via Augmentations for Improved Facial Resemblance in DreamBooth and InstantID](https://arxiv.org/abs/2505.03557)
*Koray Ulusan,Benjamin Kiefer*

Main category: cs.CV

TL;DR: 研究探讨了如何通过增强技术提升Stable Diffusion生成肖像的面部相似性，比较了DreamBooth和InstantID两种方法，并提出了FaceDistance评估工具。


<details>
  <summary>Details</summary>
Motivation: 个性化Stable Diffusion生成专业肖像的需求增长，但如何提升生成肖像与原照片的面部相似性仍需研究。

Method: 通过实验比较DreamBooth和InstantID两种技术，结合多种增强策略，并使用FaceDistance评估面部相似性。

Result: 研究揭示了增强技术在提升面部相似性中的作用，并提供了优化策略。

Conclusion: 增强技术能有效提升SDXL生成肖像的面部相似性，为下游应用提供了实用指导。

Abstract: The personalization of Stable Diffusion for generating professional portraits
from amateur photographs is a burgeoning area, with applications in various
downstream contexts. This paper investigates the impact of augmentations on
improving facial resemblance when using two prominent personalization
techniques: DreamBooth and InstantID. Through a series of experiments with
diverse subject datasets, we assessed the effectiveness of various augmentation
strategies on the generated headshots' fidelity to the original subject. We
introduce FaceDistance, a wrapper around FaceNet, to rank the generations based
on facial similarity, which aided in our assessment. Ultimately, this research
provides insights into the role of augmentations in enhancing facial
resemblance in SDXL-generated portraits, informing strategies for their
effective deployment in downstream applications.

</details>


### [190] [Fill the Gap: Quantifying and Reducing the Modality Gap in Image-Text Representation Learning](https://arxiv.org/abs/2505.03703)
*François Role,Sébastien Meyer,Victor Amblard*

Main category: cs.CV

TL;DR: 本文提出了一种新的方法来评估和减少视觉语言模型中的模态间隙问题，通过谱方法和最优传输方法，显著提升了多模态任务的性能。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型（VLMs）存在模态间隙问题，即文本和图像嵌入在共享表示空间中存在明显分离，这对多模态检索、聚类和零样本分类等下游任务有害。目前缺乏通用且实用的方法来评估和减少这种间隙。

Method: 提出了基于谱方法和最优传输技术的新方法，用于精确评估和减少模态间隙。

Result: 在多个图像-文本数据集和模型上的实验表明，这些方法有效且对下游任务有积极影响。

Conclusion: 本文的方法为评估和减少模态间隙提供了实用工具，显著提升了多模态任务的性能。

Abstract: Vision-language models (VLMs) allow to embed texts and images in a shared
representation space. However, it has been shown that these models are subject
to a modality gap phenomenon meaning there exists a clear separation between
the embeddings from one modality and another in the embedding space. While this
misalignment is detrimental for downstream tasks such as multimodal retrieval,
multimodal clustering or zero-shot classification, etc. no generic and
practical methods have so far been proposed to assess it precisely and even
reduce it. We therefore propose novel measures and effective techniques
(spectral- and optimal transport-based methods) to achieve this goal. Extensive
experiments conducted on several image-text datasets and models demonstrate
their effectiveness and beneficial effects on downstream tasks. Our code is
available at the URL provided in the paper's abstract.

</details>


### [191] [Real-Time Person Image Synthesis Using a Flow Matching Model](https://arxiv.org/abs/2505.03562)
*Jiwoo Jeong,Kirok Kim,Wooju Kim,Nam-Joon Kim*

Main category: cs.CV

TL;DR: PGPIS任务通过目标姿态和源图像生成逼真的人体图像，但实时性是一大挑战。本文提出基于流匹配（FM）的生成模型RPFM，在保持图像质量的同时显著提升生成速度，适用于实时交互系统。


<details>
  <summary>Details</summary>
Motivation: 实时PGPIS在AR/VR、游戏和直播等应用中至关重要，但现有扩散模型速度慢，无法满足实时需求。

Method: 提出基于流匹配（FM）的生成模型RPFM，支持条件生成和潜在空间操作，提升训练和采样的效率与稳定性。

Result: 在DeepFashion数据集上，RPFM实现了接近实时的生成速度，性能与现有最优模型相当。

Conclusion: RPFM通过小幅牺牲图像精度，换取生成速度的显著提升，为实时PGPIS应用提供了可行解决方案。

Abstract: Pose-Guided Person Image Synthesis (PGPIS) generates realistic person images
conditioned on a target pose and a source image. This task plays a key role in
various real-world applications, such as sign language video generation, AR/VR,
gaming, and live streaming. In these scenarios, real-time PGPIS is critical for
providing immediate visual feedback and maintaining user immersion.However,
achieving real-time performance remains a significant challenge due to the
complexity of synthesizing high-fidelity images from diverse and dynamic human
poses. Recent diffusion-based methods have shown impressive image quality in
PGPIS, but their slow sampling speeds hinder deployment in time-sensitive
applications. This latency is particularly problematic in tasks like generating
sign language videos during live broadcasts, where rapid image updates are
required. Therefore, developing a fast and reliable PGPIS model is a crucial
step toward enabling real-time interactive systems. To address this challenge,
we propose a generative model based on flow matching (FM). Our approach enables
faster, more stable, and more efficient training and sampling. Furthermore, the
proposed model supports conditional generation and can operate in latent space,
making it especially suitable for real-time PGPIS applications where both speed
and quality are critical. We evaluate our proposed method, Real-Time Person
Image Synthesis Using a Flow Matching Model (RPFM), on the widely used
DeepFashion dataset for PGPIS tasks. Our results show that RPFM achieves
near-real-time sampling speeds while maintaining performance comparable to the
state-of-the-art models. Our methodology trades off a slight, acceptable
decrease in generated-image accuracy for over a twofold increase in generation
speed, thereby ensuring real-time performance.

</details>


### [192] [ReGraP-LLaVA: Reasoning enabled Graph-based Personalized Large Language and Vision Assistant](https://arxiv.org/abs/2505.03654)
*Yifan Xiang,Zhenxi Zhang,Bin Li,Yixuan Weng,Shoujun Zhou,Yangfan He,Keqin Li*

Main category: cs.CV

TL;DR: 论文提出ReGraP数据集和ReGraP-LLaVA模型，解决现有个性化MLLMs在关系推理和多对象学习上的不足，通过图提示方法提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有个性化MLLMs在关系推理和多对象学习上存在局限性，缺乏结构化数据和复杂推理能力。

Method: 提出ReGraP数据集（含图像、知识图和CoT问答对），设计ReGraP-LLaVA模型，采用软硬图提示方法对齐知识图。

Result: ReGraP-LLaVA在关系推理和知识连接任务上表现优异，达到SoTA性能。

Conclusion: ReGraP数据集和模型有效提升个性化MLLMs的关系推理能力，为未来研究提供新方向。

Abstract: Recent advances in personalized MLLMs enable effective capture of
user-specific concepts, supporting both recognition of personalized concepts
and contextual captioning. However, humans typically explore and reason over
relations among objects and individuals, transcending surface-level information
to achieve more personalized and contextual understanding. To this end,
existing methods may face three main limitations: Their training data lacks
multi-object sets in which relations among objects are learnable. Building on
the limited training data, their models overlook the relations between
different personalized concepts and fail to reason over them. Their experiments
mainly focus on a single personalized concept, where evaluations are limited to
recognition and captioning tasks. To address the limitations, we present a new
dataset named ReGraP, consisting of 120 sets of personalized knowledge. Each
set includes images, KGs, and CoT QA pairs derived from the KGs, enabling more
structured and sophisticated reasoning pathways. We propose ReGraP-LLaVA, an
MLLM trained with the corresponding KGs and CoT QA pairs, where soft and hard
graph prompting methods are designed to align KGs within the model's semantic
space. We establish the ReGraP Benchmark, which contains diverse task types:
multiple-choice, fill-in-the-blank, True/False, and descriptive questions in
both open- and closed-ended settings. The proposed benchmark is designed to
evaluate the relational reasoning and knowledge-connection capability of
personalized MLLMs. We conduct experiments on the proposed ReGraP-LLaVA and
other competitive MLLMs. Results show that the proposed model not only learns
personalized knowledge but also performs relational reasoning in responses,
achieving the SoTA performance compared with the competitive methods. All the
codes and datasets are released at: https://github.com/xyfyyds/ReGraP.

</details>


### [193] [Revolutionizing Brain Tumor Imaging: Generating Synthetic 3D FA Maps from T1-Weighted MRI using CycleGAN Models](https://arxiv.org/abs/2505.03662)
*Xin Du,Francesca M. Cozzi,Rajesh Jena*

Main category: cs.CV

TL;DR: 提出一种基于CycleGAN的方法，直接从T1加权MRI生成FA图，解决了FA图与纤维束追踪图谱的空间错位问题，并在肿瘤区域表现优异。


<details>
  <summary>Details</summary>
Motivation: FA和DEC图对白质完整性评估至关重要，但FA图与纤维束追踪图谱的空间错位限制了其在预测模型中的应用。

Method: 采用CycleGAN模型，利用未配对数据训练，直接从T1加权MRI生成FA图。

Result: 模型生成的FA图质量高，尤其在肿瘤区域表现优异，通过SSIM和PSNR验证。

Conclusion: 该方法为临床工作流提供了AI驱动的替代方案，减少额外扫描需求。

Abstract: Fractional anisotropy (FA) and directionally encoded colour (DEC) maps are
essential for evaluating white matter integrity and structural connectivity in
neuroimaging. However, the spatial misalignment between FA maps and
tractography atlases hinders their effective integration into predictive
models. To address this issue, we propose a CycleGAN based approach for
generating FA maps directly from T1-weighted MRI scans, representing the first
application of this technique to both healthy and tumour-affected tissues. Our
model, trained on unpaired data, produces high fidelity maps, which have been
rigorously evaluated using Structural Similarity Index (SSIM) and Peak
Signal-to-Noise Ratio (PSNR), demonstrating particularly robust performance in
tumour regions. Radiological assessments further underscore the model's
potential to enhance clinical workflows by providing an AI-driven alternative
that reduces the necessity for additional scans.

</details>


### [194] [FlexiAct: Towards Flexible Action Control in Heterogeneous Scenarios](https://arxiv.org/abs/2505.03730)
*Shiyi Zhang,Junhao Zhuang,Zhaoyang Zhang,Ying Shan,Yansong Tang*

Main category: cs.CV

TL;DR: FlexiAct通过RefAdapter和FAE技术，实现了将参考视频中的动作灵活转移到任意目标图像上，突破了现有方法在空间结构和一致性上的限制。


<details>
  <summary>Details</summary>
Motivation: 现有方法在动作定制中受限于严格的空间结构约束（如布局、骨架和视角一致性），限制了其在不同主体和场景中的适应性。

Method: 提出FlexiAct框架，结合RefAdapter（轻量级图像条件适配器）和FAE（频率感知动作提取），实现动作转移时的空间适应和一致性保持。

Result: 实验表明，FlexiAct能有效将动作转移到具有不同布局、骨架和视角的主体上。

Conclusion: FlexiAct在保持外观一致性和结构灵活性方面优于现有方法，为动作定制提供了更通用的解决方案。

Abstract: Action customization involves generating videos where the subject performs
actions dictated by input control signals. Current methods use pose-guided or
global motion customization but are limited by strict constraints on spatial
structure, such as layout, skeleton, and viewpoint consistency, reducing
adaptability across diverse subjects and scenarios. To overcome these
limitations, we propose FlexiAct, which transfers actions from a reference
video to an arbitrary target image. Unlike existing methods, FlexiAct allows
for variations in layout, viewpoint, and skeletal structure between the subject
of the reference video and the target image, while maintaining identity
consistency. Achieving this requires precise action control, spatial structure
adaptation, and consistency preservation. To this end, we introduce RefAdapter,
a lightweight image-conditioned adapter that excels in spatial adaptation and
consistency preservation, surpassing existing methods in balancing appearance
consistency and structural flexibility. Additionally, based on our
observations, the denoising process exhibits varying levels of attention to
motion (low frequency) and appearance details (high frequency) at different
timesteps. So we propose FAE (Frequency-aware Action Extraction), which, unlike
existing methods that rely on separate spatial-temporal architectures, directly
achieves action extraction during the denoising process. Experiments
demonstrate that our method effectively transfers actions to subjects with
diverse layouts, skeletons, and viewpoints. We release our code and model
weights to support further research at
https://shiyi-zh0408.github.io/projectpages/FlexiAct/

</details>


<div id='astro-ph.SR'></div>

# astro-ph.SR [[Back]](#toc)

### [195] [Solar Flare Forecast: A Comparative Analysis of Machine Learning Algorithms for Solar Flare Class Prediction](https://arxiv.org/abs/2505.03385)
*Julia Bringewald*

Main category: astro-ph.SR

TL;DR: 该研究评估了三种机器学习算法（随机森林、KNN和XGBoost）在太阳耀斑分类任务中的表现，发现随机森林和XGBoost在多种维度下表现优异，为空间天气预报提供了优化方向。


<details>
  <summary>Details</summary>
Motivation: 太阳耀斑是太阳系中最强大的动态事件之一，对空间天气和基础设施有重大影响，因此需要准确预测其发生和强度。

Method: 研究使用了13个SHARP参数数据集，结合主成分分析（PCA）进行降维，并通过10折分层交叉验证和网格搜索优化模型性能。

Result: 随机森林和XGBoost在所有指标中表现优异，尤其是在高维度数据下表现更佳。

Conclusion: 该研究为太阳耀斑预测提供了新的方法学，优化了降维技术和模型选择，有助于未来空间天气预报系统的开发和太阳物理学的深入研究。

Abstract: Solar flares are among the most powerful and dynamic events in the solar
system, resulting from the sudden release of magnetic energy stored in the
Sun's atmosphere. These energetic bursts of electromagnetic radiation can
release up to 10^32 erg of energy, impacting space weather and posing risks to
technological infrastructure and therefore require accurate forecasting of
solar flare occurrences and intensities. This study evaluates the predictive
performance of three machine learning algorithms: Random Forest, k-Nearest
Neighbors (KNN), and Extreme Gradient Boosting (XGBoost) for classifying solar
flares into 4 categories (B, C, M, X). Using the dataset of 13 SHARP
parameters, the effectiveness of the models was evaluated in binary and
multiclass classification tasks. The analysis utilized 8 principal components
(PC), capturing 95% of data variance, and 100 PCs, capturing 97.5% of variance.
Our approach uniquely combines binary and multiclass classification with
different levels of dimensionality reduction, an innovative methodology not
previously explored in the context of solar flare prediction. Employing a
10-fold stratified cross-validation and grid search for hyperparameter tuning
ensured robust model evaluation. Our findings indicate that Random Forest and
XGBoost consistently demonstrate strong performance across all metrics,
benefiting significantly from increased dimensionality. The insights of this
study enhance future research by optimizing dimensionality reduction techniques
and informing model selection for astrophysical tasks. By integrating this
newly acquired knowledge into future research, more accurate space weather
forecasting systems can be developed, along with a deeper understanding of
solar physics.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [196] [New affine invariant ensemble samplers and their dimensional scaling](https://arxiv.org/abs/2505.02987)
*Yifan Chen*

Main category: stat.CO

TL;DR: 本文提出了一些新的仿射不变集成采样器，改进了现有算法，尤其在高维问题上表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有采样器在高维问题或偏斜分布中表现不佳，需要更高效的仿射不变采样方法。

Method: 提出了无导数的集成侧移采样器和基于导数的仿射不变集成HMC采样器。

Result: 新采样器在性能上优于现有方法，尤其在高维高斯目标中表现突出。

Conclusion: 仿射不变集成采样器在高维问题中具有显著优势，尤其是结合导数信息的HMC采样器。

Abstract: We introduce some new affine invariant ensemble samplers that are easy to
construct and improve upon existing widely used algorithms, especially for
high-dimensional problems. Specifically, we propose a derivative-free ensemble
side move sampler that performs favorably compared to popular samplers in the
\texttt{emcee} package. Additionally, we develop a class of derivative-based
ensemble Hamiltonian Monte Carlo (HMC) samplers with affine invariance, which
outperform standard HMC without affine invariance when sampling highly skewed
distributions. We provide asymptotic scaling analysis for high-dimensional
Gaussian targets to further elucidate the properties of these affine invariant
ensemble samplers. In particular, with derivative information, the affine
invariant ensemble HMC can scale much better with dimension compared to
derivative-free ensemble samplers.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [197] [Elevating Semantic Exploration: A Novel Approach Utilizing Distributed Repositories](https://arxiv.org/abs/2505.03443)
*Valerio Bellandi*

Main category: cs.DC

TL;DR: 本文比较了集中式和分布式系统的优缺点，并介绍了一个为意大利司法部开发的分布式文档存储系统，利用边缘存储库分析文本数据和元数据，提升语义探索能力。


<details>
  <summary>Details</summary>
Motivation: 探讨集中式和分布式系统的适用场景，并开发一个分布式文档存储系统以满足大规模环境中的高可用性和性能需求。

Method: 采用分布式系统架构，利用边缘存储库分析文本数据和元数据，增强语义探索功能。

Result: 开发了一个分布式文档存储系统，适用于需要高可用性和性能的大规模环境。

Conclusion: 分布式系统在需要高可用性和性能的场景中表现更优，而集中式系统适用于控制集中且扩展性需求较低的应用。

Abstract: Centralized and distributed systems are two main approaches to organizing ICT
infrastructure, each with its pros and cons. Centralized systems concentrate
resources in one location, making management easier but creating single points
of failure. Distributed systems, on the other hand, spread resources across
multiple nodes, offering better scalability and fault tolerance, but requiring
more complex management. The choice between them depends on factors like
application needs, scalability, and data sensitivity. Centralized systems suit
applications with limited scalability and centralized control, while
distributed systems excel in large-scale environments requiring high
availability and performance. This paper explores a distributed document
repository system developed for the Italian Ministry of Justice, using edge
repositories to analyze textual data and metadata, enhancing semantic
exploration capabilities.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [198] [SepALM: Audio Language Models Are Error Correctors for Robust Speech Separation](https://arxiv.org/abs/2505.03273)
*Zhaoxi Mu,Xinyu Yang,Gang Wang*

Main category: cs.SD

TL;DR: SepALM利用音频语言模型（ALM）在文本域中纠正和重新合成语音，提升语音分离的精度和适应性。


<details>
  <summary>Details</summary>
Motivation: 解决传统语音分离技术在嘈杂和混响环境中产生的失真问题。

Method: SepALM包含分离器、纠正器、合成器和对齐器，结合ALM端到端纠错机制，避免传统方法中ASR与LLM结合的优化问题。

Result: 实验证明SepALM提高了语音分离的精度，并增强了在新声学环境中的适应性。

Conclusion: SepALM通过ALM的纠错和合成能力，显著改善了语音分离的效果。

Abstract: While contemporary speech separation technologies adeptly process lengthy
mixed audio waveforms, they are frequently challenged by the intricacies of
real-world environments, including noisy and reverberant settings, which can
result in artifacts or distortions in the separated speech. To overcome these
limitations, we introduce SepALM, a pioneering approach that employs audio
language models (ALMs) to rectify and re-synthesize speech within the text
domain following preliminary separation. SepALM comprises four core components:
a separator, a corrector, a synthesizer, and an aligner. By integrating an
ALM-based end-to-end error correction mechanism, we mitigate the risk of error
accumulation and circumvent the optimization hurdles typically encountered in
conventional methods that amalgamate automatic speech recognition (ASR) with
large language models (LLMs). Additionally, we have developed Chain-of-Thought
(CoT) prompting and knowledge distillation techniques to facilitate the
reasoning and training processes of the ALM. Our experiments substantiate that
SepALM not only elevates the precision of speech separation but also markedly
bolsters adaptability in novel acoustic environments.

</details>


### [199] [A study on audio synchronous steganography detection and distributed guide inference model based on sliding spectral features and intelligent inference drive](https://arxiv.org/abs/2505.03193)
*Wei Meng*

Main category: cs.SD

TL;DR: 本文提出了一种基于短时傅里叶变换（STFT）和滑动窗口的检测模型，用于识别短视频音频同步流中的隐写数据，并通过分布式推理模型解码隐藏的军事指令。


<details>
  <summary>Details</summary>
Motivation: 随着短视频平台在全球通信中的兴起，音频同步流中的隐写数据成为一种新的隐蔽通信方式。传统技术在检测同步隐写方面存在局限性，因此需要新的方法。

Method: 采用25毫秒滑动窗口和STFT提取主频率轨迹，构建同步帧检测模型（M1），并通过结构化模型（M2）解码32字节的有效载荷。

Result: 在36至45秒的音频段中发现了低熵、重复的字节序列和高度集中的频谱能量，证实了同步帧的存在。

Conclusion: 该框架验证了滑动频谱特征在同步隐写检测中的有效性，并为开放平台上的隐蔽通信分析和战术指导模拟提供了可扩展的推理模型。

Abstract: With the rise of short video platforms in global communication, embedding
steganographic data in audio synchronization streams has emerged as a new
covert communication method. To address the limitations of traditional
techniques in detecting synchronized steganography, this paper proposes a
detection and distributed guidance reconstruction model based on short video
"Yupan" samples released by China's South Sea Fleet on TikTok. The method
integrates sliding spectrum feature extraction and intelligent inference
mechanisms. A 25 ms sliding window with short-time Fourier transform (STFT) is
used to extract the main frequency trajectory and construct the synchronization
frame detection model (M1), identifying a frame flag "FFFFFFFFFFFFFFFFFF80".
The subsequent 32-byte payload is decoded by a structured model (M2) to infer
distributed guidance commands. Analysis reveals a low-entropy, repetitive byte
sequence in the 36 to 45 second audio segment with highly concentrated spectral
energy, confirming the presence of synchronization frames. Although plaintext
semantics are not restored, the consistency in command field layout suggests
features of military communication protocols. The multi-segment splicing model
further shows cross-video embedding and centralized decoding capabilities. The
proposed framework validates the effectiveness of sliding spectral features for
synchronized steganography detection and builds an extensible inference model
for covert communication analysis and tactical guidance simulation on open
platforms.

</details>


### [200] [Mamba-Diffusion Model with Learnable Wavelet for Controllable Symbolic Music Generation](https://arxiv.org/abs/2505.03314)
*Jincheng Zhang,György Fazekas,Charalampos Saitis*

Main category: cs.SD

TL;DR: 该论文提出了一种将符号音乐表示为类图像钢琴卷帘的方法，并引入了一种结合Transformer-Mamba块和可学习小波变换的扩散模型，用于生成符号音乐。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像合成领域取得了成功，但在符号音乐生成中的应用尚未充分探索，因为符号音乐通常是离散事件序列，而标准扩散模型不擅长处理离散数据。

Method: 将符号音乐表示为钢琴卷帘，提出了一种结合Transformer-Mamba块和可学习小波变换的扩散模型，并使用无分类器引导生成目标和弦的符号音乐。

Result: 实验表明，该方法在音乐质量和可控性方面表现优异，优于钢琴卷帘生成的基线模型。

Conclusion: 通过将符号音乐表示为钢琴卷帘并结合新型扩散模型，该方法在符号音乐生成任务中取得了显著成果。

Abstract: The recent surge in the popularity of diffusion models for image synthesis
has attracted new attention to their potential for generation tasks in other
domains. However, their applications to symbolic music generation remain
largely under-explored because symbolic music is typically represented as
sequences of discrete events and standard diffusion models are not well-suited
for discrete data. We represent symbolic music as image-like pianorolls,
facilitating the use of diffusion models for the generation of symbolic music.
Moreover, this study introduces a novel diffusion model that incorporates our
proposed Transformer-Mamba block and learnable wavelet transform.
Classifier-free guidance is utilised to generate symbolic music with target
chords. Our evaluation shows that our method achieves compelling results in
terms of music quality and controllability, outperforming the strong baseline
in pianoroll generation. Our code is available at
https://github.com/jinchengzhanggg/proffusion.

</details>


### [201] [Knowledge Distillation for Speech Denoising by Latent Representation Alignment with Cosine Distance](https://arxiv.org/abs/2505.03442)
*Diep Luong,Mikko Heikkinen,Konstantinos Drossos,Tuomas Virtanen*

Main category: cs.SD

TL;DR: 论文提出了一种基于知识蒸馏（KD）的语音去噪方法，通过利用去噪自编码器框架、线性倒置瓶颈和余弦相似性特性，解决了现有方法中学生模型受限于教师模型分布和信息顺序的问题。实验表明，该方法在性能和学生与教师模型不匹配条件下的表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有语音去噪方法复杂且难以在低资源设备上部署，而知识蒸馏方法中学生的学习受限于教师模型的分布和特征维度。本文旨在解决这一问题。

Method: 利用去噪自编码器框架、线性倒置瓶颈和余弦相似性特性，提出了一种改进的知识蒸馏方法。

Result: 实验结果表明，该方法在性能和学生与教师模型不匹配条件下的表现优于现有方法。

Conclusion: 提出的方法在语音去噪任务中表现优异，尤其在学生与教师模型不匹配条件下具有更强的鲁棒性。

Abstract: Speech denoising is a generally adopted and impactful task, appearing in many
common and everyday-life use cases. Although there are very powerful methods
published, most of those are too complex for deployment in everyday and
low-resources computational environments, like hand-held devices, intelligent
glasses, hearing aids, etc. Knowledge distillation (KD) is a prominent way for
alleviating this complexity mismatch and is based on the
transferring/distilling of knowledge from a pre-trained complex model, the
teacher, to another less complex one, the student. Existing KD methods for
speech denoising are based on processes that potentially hamper the KD by
bounding the learning of the student to the distribution, information ordering,
and feature dimensionality learned by the teacher. In this paper, we present
and assess a method that tries to treat this issue, by exploiting the
well-known denoising-autoencoder framework, the linear inverted bottlenecks,
and the properties of the cosine similarity. We use a public dataset and
conduct repeated experiments with different mismatching scenarios between the
teacher and the student, reporting the mean and standard deviation of the
metrics of our method and another, state-of-the-art method that is used as a
baseline. Our results show that with the proposed method, the student can
perform better and can also retain greater mismatching conditions compared to
the teacher.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [202] [Nonnegative Low-rank Matrix Recovery Can Have Spurious Local Minima](https://arxiv.org/abs/2505.03717)
*Richard Y. Zhang*

Main category: math.OC

TL;DR: 论文研究了非负约束下低秩矩阵恢复的良性非凸性问题，发现完全观测时良性非凸性成立，但部分观测时即使RIP常数趋近于0也不成立。


<details>
  <summary>Details</summary>
Motivation: 探讨在非负约束条件下，低秩矩阵恢复是否仍具有良性非凸性，揭示理论与实际之间的差距。

Method: 通过分析完全观测和部分观测情况下的RIP条件，验证良性非凸性的存在与否。

Result: 完全观测时良性非凸性成立（RIP常数δ=0），但部分观测时即使δ趋近于0也不成立。

Conclusion: 非负约束导致低秩矩阵恢复的连续性理论失效，暴露了理论与实际之间的关键差距。

Abstract: The classical low-rank matrix recovery problem is well-known to exhibit
\emph{benign nonconvexity} under the restricted isometry property (RIP): local
optimization is guaranteed to converge to the global optimum, where the ground
truth is recovered. We investigate whether benign nonconvexity continues to
hold when the factor matrices are constrained to be elementwise nonnegative --
a common practical requirement. In the simple setting of a rank-1 nonnegative
ground truth, we confirm that benign nonconvexity holds in the fully-observed
case with RIP constant $\delta=0$. Surprisingly, however, this property fails
to extend to the partially-observed case with any arbitrarily small RIP
constant $\delta\to0^{+}$, irrespective of rank overparameterization. This
finding exposes a critical theoretical gap: the continuity argument widely used
to explain the empirical robustness of low-rank matrix recovery fundamentally
breaks down once nonnegative constraints are imposed.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [203] [Safer Prompts: Reducing IP Risk in Visual Generative AI](https://arxiv.org/abs/2505.03338)
*Lena Reissinger,Yuanyuan Li,Anna-Carolina Haensch,Neeraj Sarna*

Main category: math.NA

TL;DR: 研究评估了提示工程技术在减少生成图像与训练数据相似性方面的有效性，以降低知识产权侵权风险。


<details>
  <summary>Details</summary>
Motivation: 生成式AI模型可能因训练数据多样性而记忆并复制特定内容，引发知识产权侵权担忧。

Method: 采用Chain of Thought Prompting和Task Instruction Prompting技术。

Result: 这些提示工程技术显著降低了生成图像与扩散模型训练数据的相似性。

Conclusion: 提示工程技术能有效减少生成式AI的知识产权侵权风险。

Abstract: Visual Generative AI models have demonstrated remarkable capability in
generating high-quality images from simple inputs like text prompts. However,
because these models are trained on images from diverse sources, they risk
memorizing and reproducing specific content, raising concerns about
intellectual property (IP) infringement. Recent advances in prompt engineering
offer a cost-effective way to enhance generative AI performance. In this paper,
we evaluate the effectiveness of prompt engineering techniques in mitigating IP
infringement risks in image generation. Our findings show that Chain of Thought
Prompting and Task Instruction Prompting significantly reduce the similarity
between generated images and the training data of diffusion models, thereby
lowering the risk of IP infringement.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [204] [Cognitio Emergens: Agency, Dimensions, and Dynamics in Human-AI Knowledge Co-Creation](https://arxiv.org/abs/2505.03105)
*Xule Lin*

Main category: cs.HC

TL;DR: 论文提出Cognitio Emergens（CE）框架，研究人类与AI在科学知识创造中的动态合作关系，强调角色、能力和组织结构的持续协商。


<details>
  <summary>Details</summary>
Motivation: 现有模型无法捕捉人类与AI在科学理解中的递归互动，CE框架旨在填补这一空白。

Method: CE整合了三个组件：Agency Configurations（描述人类与AI的权威分配）、Epistemic Dimensions（捕捉合作中的六种能力）和Partnership Dynamics（分析关系演化的动力）。

Result: CE揭示了知识共创通过角色、价值观和组织结构的持续协商而涌现，提供了平衡人类参与与科学突破的工具。

Conclusion: CE重新定义了人类与AI的科学合作，既不盲目乐观也不过度担忧，而是为维持有意义的人类参与提供了概念工具。

Abstract: Scientific knowledge creation is fundamentally transforming as humans and AI
systems evolve beyond tool-user relationships into co-evolutionary epistemic
partnerships. When AlphaFold revolutionized protein structure prediction,
researchers described engaging with an epistemic partner that reshaped how they
conceptualized fundamental relationships. This article introduces Cognitio
Emergens (CE), a framework addressing critical limitations in existing models
that focus on static roles or narrow metrics while failing to capture how
scientific understanding emerges through recursive human-AI interaction over
time. CE integrates three components addressing these limitations: Agency
Configurations describing how authority distributes between humans and AI
(Directed, Contributory, Partnership), with partnerships dynamically
oscillating between configurations rather than following linear progression;
Epistemic Dimensions capturing six specific capabilities emerging through
collaboration across Discovery, Integration, and Projection axes, creating
distinctive "capability signatures" that guide development; and Partnership
Dynamics identifying forces shaping how these relationships evolve,
particularly the risk of epistemic alienation where researchers lose
interpretive control over knowledge they formally endorse. Drawing from
autopoiesis theory, social systems theory, and organizational modularity, CE
reveals how knowledge co-creation emerges through continuous negotiation of
roles, values, and organizational structures. By reconceptualizing human-AI
scientific collaboration as fundamentally co-evolutionary, CE offers a balanced
perspective that neither uncritically celebrates nor unnecessarily fears AI's
evolving role, instead providing conceptual tools for cultivating partnerships
that maintain meaningful human participation while enabling transformative
scientific breakthroughs.

</details>


### [205] [Augmenting Human Cognition through Everyday AR](https://arxiv.org/abs/2505.03492)
*Xiaoan Liu*

Main category: cs.HC

TL;DR: 本文探讨了始终开启的增强现实（AR）如何无缝连接数字认知与物理环境，提升人类任务表现和理解。


<details>
  <summary>Details</summary>
Motivation: 随着空间计算和多模态大语言模型的发展，AR逐渐成为一种直观的“思维工具”，将语义和上下文感知智能嵌入日常环境。

Method: 研究通过始终开启的AR技术，探索如何实现数字认知与物理环境的无缝连接。

Result: AR能够实现主动且上下文敏感的交互，从而增强人类的任务表现和理解能力。

Conclusion: AR作为一种“思维工具”，有望通过智能交互提升人类与物理环境的互动效率。

Abstract: As spatial computing and multimodal LLMs mature, AR is tending to become an
intuitive "thinking tool," embedding semantic and context-aware intelligence
directly into everyday environments. This paper explores how always-on AR can
seamlessly bridge digital cognition and physical affordances, enabling
proactive, context-sensitive interactions that enhance human task performance
and understanding.

</details>


### [206] [BCause: Human-AI collaboration to improve hybrid mapping and ideation in argumentation-grounded deliberation](https://arxiv.org/abs/2505.03584)
*Lucas Anastasiou,Anna De Liddo*

Main category: cs.HC

TL;DR: BCause是一个结合生成式AI与人机协作的讨论系统，旨在将公共议题的非结构化对话转化为结构化、可操作的民主流程。


<details>
  <summary>Details</summary>
Motivation: 公共讨论常因分散、浅显且与政策脱节而效果不佳，BCause试图解决这一问题。

Method: 系统通过三项创新实现：将非结构化文本转化为论证性讨论、基于Telegram机器人的地理问题感知、以及智能报告生成工具。

Result: BCause通过人机协作保留了人类参与的关键作用，确保伦理监督和情境相关性。

Conclusion: BCause为公共讨论提供了结构化支持，有望提升民主决策的质量和效率。

Abstract: Public deliberation, as in open discussion of issues of public concern, often
suffers from scattered and shallow discourse, poor sensemaking, and a
disconnect from actionable policy outcomes. This paper introduces BCause, a
discussion system leveraging generative AI and human-machine collaboration to
transform unstructured dialogue around public issues (such as urban living,
policy changes, and current socio-economic transformations) into structured,
actionable democratic processes. We present three innovations: (i) importing
and transforming unstructured transcripts into argumentative discussions, (ii)
geo-deliberated problem-sensing via a Telegram bot for local issue reporting,
and (iii) smart reporting with customizable widgets (e.g., summaries, topic
modelling, policy recommendations, clustered arguments). The system's human-AI
partnership preserves critical human participation to ensure ethical oversight,
contextual relevance, and creative synthesis.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [207] [Robustly Invertible Nonlinear Dynamics and the BiLipREN: Contracting Neural Models with Contracting Inverses](https://arxiv.org/abs/2505.03069)
*Yurui Zhang,Ruigang Wang,Ian R. Manchester*

Main category: eess.SY

TL;DR: 论文研究了非线性动态系统的可逆性，提出了一种新的可逆循环神经网络模型BiLipREN，通过收缩性和增量稳定性分析确保模型的鲁棒性和可逆性。


<details>
  <summary>Details</summary>
Motivation: 研究非线性动态系统的可逆性，并设计一种鲁棒可逆的循环神经网络模型，以提高对输入扰动的敏感性和输入序列的鲁棒重建能力。

Method: 基于双Lipschitz性质，提出了一种参数化神经网络动态模型（biLipREN），并通过正交线性系统组合构建更通用的双Lipschitz动态模型。

Result: 提出的biLipREN模型在数值实验中展示了其鲁棒可逆性和实用性。

Conclusion: biLipREN模型通过双Lipschitz性质实现了鲁棒可逆性，为非线性动态系统的建模提供了新方法。

Abstract: We study the invertibility of nonlinear dynamical systems from the
perspective of contraction and incremental stability analysis and propose a new
invertible recurrent neural model: the BiLipREN. In particular, we consider a
nonlinear state space model to be robustly invertible if an inverse exists with
a state space realisation, and both the forward model and its inverse are
contracting, i.e. incrementally exponentially stable, and Lipschitz, i.e. have
bounded incremental gain. This property of bi-Lipschitzness implies both
robustness in the sense of sensitivity to input perturbations, as well as
robust distinguishability of different inputs from their corresponding outputs,
i.e. the inverse model robustly reconstructs the input sequence despite small
perturbations to the initial conditions and measured output. Building on this
foundation, we propose a parameterization of neural dynamic models:
bi-Lipschitz recurrent equilibrium networks (biLipREN), which are robustly
invertible by construction. Moreover, biLipRENs can be composed with orthogonal
linear systems to construct more general bi-Lipschitz dynamic models, e.g., a
nonlinear analogue of minimum-phase/all-pass (inner/outer) factorization. We
illustrate the utility of our proposed approach with numerical examples.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [208] [Vector valued optimal transport: from dynamic to static formulations](https://arxiv.org/abs/2505.03670)
*Katy Craig,Nicolás García Trillos,Đorđe Nikolić*

Main category: math.AP

TL;DR: 本文提出了一种统一向量值最优传输理论的方法，涵盖动态和静态形式，并证明了四种距离的等价性。


<details>
  <summary>Details</summary>
Motivation: 研究动机源于向量值测度分类和多物种偏微分方程（PDE）的应用需求。

Method: 通过将向量值测度建模为乘积空间上的概率测度，结合加权图的几何特性，统一了动态和静态最优传输理论。

Result: 证明了四种向量值最优传输距离的尖锐不等式及其双Hölder等价性。

Conclusion: 该框架在多物种PDE和数据分析中具有潜在应用，尤其是一种静态形式适合线性化计算。

Abstract: Motivated by applications in classification of vector valued measures and
multispecies PDE, we develop a theory that unifies existing notions of vector
valued optimal transport, from dynamic formulations (\`a la Benamou-Brenier) to
static formulations (\`a la Kantorovich). In our framework, vector valued
measures are modeled as probability measures on a product space $\mathbb{R}^d
\times G$, where $G$ is a weighted graph over a finite set of nodes and the
graph geometry strongly influences the associated dynamic and static distances.
We obtain sharp inequalities relating four notions of vector valued optimal
transport and prove that the distances are mutually bi-H\"older equivalent. We
discuss the theoretical and practical advantages of each metric and indicate
potential applications in multispecies PDE and data analysis. In particular,
one of the static formulations discussed in the paper is amenable to
linearization, a technique that has been explored in recent years to accelerate
the computation of pairwise optimal transport distances.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [209] [HMAE: Self-Supervised Few-Shot Learning for Quantum Spin Systems](https://arxiv.org/abs/2505.03140)
*Ibne Farabi Shihab,Sanjeda Akter,Anuj Sharma*

Main category: quant-ph

TL;DR: HMAE是一种自监督框架，通过物理信息掩码预训练Transformer，显著提升量子系统的少样本学习效率。


<details>
  <summary>Details</summary>
Motivation: 解决量子机器学习中标记数据稀缺和模拟计算成本高的问题。

Method: 提出Hamiltonian-Masked Autoencoding (HMAE)，基于量子信息理论选择性掩码哈密顿项。

Result: 在12,500个量子哈密顿量上，HMAE在相分类和基态能量预测中表现优于基线方法。

Conclusion: HMAE在小量子系统中表现出色，但规模限制阻碍了其在更大系统中的应用。

Abstract: Quantum machine learning for spin and molecular systems faces critical
challenges of scarce labeled data and computationally expensive simulations. To
address these limitations, we introduce Hamiltonian-Masked Autoencoding (HMAE),
a novel self-supervised framework that pre-trains transformers on unlabeled
quantum Hamiltonians, enabling efficient few-shot transfer learning. Unlike
random masking approaches, HMAE employs a physics-informed strategy based on
quantum information theory to selectively mask Hamiltonian terms based on their
physical significance. Experiments on 12,500 quantum Hamiltonians (60%
real-world, 40% synthetic) demonstrate that HMAE achieves 85.3% $\pm$ 1.5%
accuracy in phase classification and 0.15 $\pm$ 0.02 eV MAE in ground state
energy prediction with merely 10 labeled examples - a statistically significant
improvement (p < 0.01) over classical graph neural networks (78.1% $\pm$ 2.1%)
and quantum neural networks (76.8% $\pm$ 2.3%). Our method's primary advantage
is exceptional sample efficiency - reducing required labeled examples by 3-5x
compared to baseline methods - though we emphasize that ground truth values for
fine-tuning and evaluation still require exact diagonalization or tensor
networks. We explicitly acknowledge that our current approach is limited to
small quantum systems (specifically limited to 12 qubits during training, with
limited extension to 16-20 qubits in testing) and that, while promising within
this regime, this size restriction prevents immediate application to larger
systems of practical interest in materials science and quantum chemistry.

</details>


### [210] [Quantum Feature Space of a Qubit Coupled to an Arbitrary Bath](https://arxiv.org/abs/2505.03397)
*Chris Wise,Akram Youssry,Alberto Peruzzo,Jo Plested,Matt Woolley*

Main category: quant-ph

TL;DR: 论文提出了一种高效参数化方法，替代了传统复杂的神经网络模型，用于描述量子比特与浴耦合的噪声操作符，并展示了其在噪声分类中的有效性。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖于复杂的神经网络和物理编码层，难以扩展和实时操作。本文旨在简化这一过程，提出一种高效的参数化方法。

Method: 通过定义量子特征空间，利用欧几里得距离对噪声过程进行分类，并结合随机森林算法验证其有效性。

Result: 量子特征空间能够有效分类噪声的平稳性和类型，并展示了控制脉冲参数与特征空间的映射关系。

Conclusion: 无需昂贵的神经网络，仅通过高效的参数化方法即可实现对噪声过程的分类和控制参数的分析。

Abstract: Qubit control protocols have traditionally leveraged a characterisation of
the qubit-bath coupling via its power spectral density. Previous work proposed
the inference of noise operators that characterise the influence of a classical
bath using a grey-box approach that combines deep neural networks with
physics-encoded layers. This overall structure is complex and poses challenges
in scaling and real-time operations. Here, we show that no expensive neural
networks are needed and that this noise operator description admits an
efficient parameterisation. We refer to the resulting parameter space as the
\textit{quantum feature space} of the qubit dynamics resulting from the coupled
bath. We show that the Euclidean distance defined over the quantum feature
space provides an effective method for classifying noise processes in the
presence of a given set of controls. Using the quantum feature space as the
input space for a simple machine learning algorithm (random forest, in this
case), we demonstrate that it can effectively classify the stationarity and the
broad class of noise processes perturbing a qubit. Finally, we explore how
control pulse parameters map to the quantum feature space.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [211] [Single-Sample and Robust Online Resource Allocation](https://arxiv.org/abs/2505.02963)
*Rohan Ghuge,Sahil Singla,Yifan Wang*

Main category: cs.DS

TL;DR: 本文提出了一种新颖的指数定价算法，用于在线资源分配问题，仅需每个请求分布的单个样本即可实现高近似比，并具有鲁棒性和激励兼容性。


<details>
  <summary>Details</summary>
Motivation: 在线资源分配是计算机科学、运筹学和经济学中的核心问题，现有方法通常需要完全分布知识或局限于特定假设，本文旨在解决这些限制。

Method: 提出了一种指数定价算法，通过资源价格的指数调整来避免资源耗尽，同时保证近似比和鲁棒性。

Result: 算法仅需单个样本即可实现(1-ε)-近似比，并在异常值模型和值增强模型中保持鲁棒性。

Conclusion: 指数定价算法为在线资源分配提供了一种高效且鲁棒的解决方案，解决了先前工作中的开放问题。

Abstract: Online Resource Allocation problem is a central problem in many areas of
Computer Science, Operations Research, and Economics. In this problem, we
sequentially receive $n$ stochastic requests for $m$ kinds of shared resources,
where each request can be satisfied in multiple ways, consuming different
amounts of resources and generating different values. The goal is to achieve a
$(1-\epsilon)$-approximation to the hindsight optimum, where $\epsilon>0$ is a
small constant, assuming each resource has a large budget.
  In this paper, we investigate the learnability and robustness of online
resource allocation. Our primary contribution is a novel Exponential Pricing
algorithm with the following properties: 1. It requires only a \emph{single
sample} from each of the $n$ request distributions to achieve a
$(1-\epsilon)$-approximation for online resource allocation with large budgets.
Such an algorithm was previously unknown, even with access to polynomially many
samples, as prior work either assumed full distributional knowledge or was
limited to i.i.d.\,or random-order arrivals. 2. It is robust to corruptions in
the outliers model and the value augmentation model. Specifically, it maintains
its $(1 - \epsilon)$-approximation guarantee under both these robustness
models, resolving the open question posed in Argue, Gupta, Molinaro, and Singla
(SODA'22). 3. It operates as a simple item-pricing algorithm that ensures
incentive compatibility.
  The intuition behind our Exponential Pricing algorithm is that the price of a
resource should adjust exponentially as it is overused or underused. It differs
from conventional approaches that use an online learning algorithm for item
pricing. This departure guarantees that the algorithm will never run out of any
resource, but loses the usual no-regret properties of online learning
algorithms, necessitating a new analytical approach.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [212] [A Trustworthy Multi-LLM Network: Challenges,Solutions, and A Use Case](https://arxiv.org/abs/2505.03196)
*Haoxiang Luo,Gang Sun,Yinqiu Liu,Dusit Niyato,Hongfang Yu,Mohammed Atiquzzaman,Schahram Dustdar*

Main category: cs.NI

TL;DR: 提出了一种基于区块链的多LLM协作框架（MultiLLMN），以解决不同LLM在通信和网络任务中的信任和可靠性问题，并通过FBS防御案例验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 不同LLM因结构和训练数据差异可能导致优化策略不一致，且存在低置信度或偏见问题，需构建可信协作框架。

Method: 设计区块链支持的MultiLLMN框架，实现多LLM协作评估和选择最优响应，并以FBS防御为例验证。

Result: MultiLLMN能有效提升复杂网络优化问题的响应可靠性和质量。

Conclusion: MultiLLMN为LLM协作和信任问题提供了可行解决方案，未来可进一步探索其应用扩展。

Abstract: Large Language Models (LLMs) demonstrate strong potential across a variety of
tasks in communications and networking due to their advanced reasoning
capabilities. However, because different LLMs have different model structures
and are trained using distinct corpora and methods, they may offer varying
optimization strategies for the same network issues. Moreover, the limitations
of an individual LLM's training data, aggravated by the potential maliciousness
of its hosting device, can result in responses with low confidence or even
bias. To address these challenges, we propose a blockchain-enabled
collaborative framework that connects multiple LLMs into a Trustworthy
Multi-LLM Network (MultiLLMN). This architecture enables the cooperative
evaluation and selection of the most reliable and high-quality responses to
complex network optimization problems. Specifically, we begin by reviewing
related work and highlighting the limitations of existing LLMs in collaboration
and trust, emphasizing the need for trustworthiness in LLM-based systems. We
then introduce the workflow and design of the proposed Trustworthy MultiLLMN
framework. Given the severity of False Base Station (FBS) attacks in B5G and 6G
communication systems and the difficulty of addressing such threats through
traditional modeling techniques, we present FBS defense as a case study to
empirically validate the effectiveness of our approach. Finally, we outline
promising future research directions in this emerging area.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [213] [Physical foundations for trustworthy medical imaging: a review for artificial intelligence researchers](https://arxiv.org/abs/2505.02843)
*Miriam Cobo,David Corral Fontecha,Wilson Silva,Lara Lloret Iglesias*

Main category: eess.IV

TL;DR: 本文回顾了医学影像中物理学基础及其对人工智能（尤其是生成模型和重建算法）的影响，探讨了如何将物理学知识融入机器学习模型以提升医学影像特征学习。


<details>
  <summary>Details</summary>
Motivation: 人工智能在医学影像领域的快速发展因缺乏对影像获取物理原理的全面理解而受限，整合物理学知识可增强算法的可信度和鲁棒性。

Method: 回顾医学影像的物理学基础及其对AI的影响，探讨物理学知识在生成模型和重建算法中的应用，并提出物理启发的机器学习模型。

Result: 整合物理学知识可提升医学影像AI算法的性能，尤其在数据有限的情况下。

Conclusion: 物理学知识对医学影像AI的发展至关重要，物理启发的机器学习模型是未来的重要方向。

Abstract: Artificial intelligence in medical imaging has seen unprecedented growth in
the last years, due to rapid advances in deep learning and computing resources.
Applications cover the full range of existing medical imaging modalities, with
unique characteristics driven by the physics of each technique. Yet, artificial
intelligence professionals entering the field, and even experienced developers,
often lack a comprehensive understanding of the physical principles underlying
medical image acquisition, which hinders their ability to fully leverage its
potential. The integration of physics knowledge into artificial intelligence
algorithms enhances their trustworthiness and robustness in medical imaging,
especially in scenarios with limited data availability. In this work, we review
the fundamentals of physics in medical images and their impact on the latest
advances in artificial intelligence, particularly, in generative models and
reconstruction algorithms. Finally, we explore the integration of physics
knowledge into physics-inspired machine learning models, which leverage
physics-based constraints to enhance the learning of medical imaging features.

</details>

<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 22]
- [cs.AI](#cs.AI) [Total: 17]
- [cs.LG](#cs.LG) [Total: 99]
- [cs.SE](#cs.SE) [Total: 2]
- [math.HO](#math.HO) [Total: 1]
- [cs.CY](#cs.CY) [Total: 3]
- [cs.DS](#cs.DS) [Total: 1]
- [cs.CV](#cs.CV) [Total: 27]
- [q-fin.MF](#q-fin.MF) [Total: 1]
- [stat.ML](#stat.ML) [Total: 5]
- [eess.IV](#eess.IV) [Total: 4]
- [cs.NE](#cs.NE) [Total: 3]
- [cs.DC](#cs.DC) [Total: 4]
- [cs.GR](#cs.GR) [Total: 1]
- [cs.AR](#cs.AR) [Total: 6]
- [q-fin.PM](#q-fin.PM) [Total: 1]
- [cs.SI](#cs.SI) [Total: 4]
- [cs.DB](#cs.DB) [Total: 1]
- [stat.ME](#stat.ME) [Total: 1]
- [cs.DL](#cs.DL) [Total: 1]
- [eess.SP](#eess.SP) [Total: 1]
- [cs.HC](#cs.HC) [Total: 4]
- [cs.CR](#cs.CR) [Total: 10]
- [eess.AS](#eess.AS) [Total: 3]
- [cs.SD](#cs.SD) [Total: 3]
- [q-bio.QM](#q-bio.QM) [Total: 2]
- [math.OC](#math.OC) [Total: 4]
- [cs.MA](#cs.MA) [Total: 4]
- [physics.geo-ph](#physics.geo-ph) [Total: 1]
- [cs.IR](#cs.IR) [Total: 5]
- [physics.comp-ph](#physics.comp-ph) [Total: 1]
- [cs.RO](#cs.RO) [Total: 2]
- [physics.optics](#physics.optics) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Calibrating Uncertainty Quantification of Multi-Modal LLMs using Grounding](https://arxiv.org/abs/2505.03788)
*Trilok Padhi,Ramneet Kaur,Adam D. Cobb,Manoj Acharya,Anirban Roy,Colin Samplawski,Brian Matejek,Alexander M. Berenbeim,Nathaniel D. Bastian,Susmit Jha*

Main category: cs.CL

TL;DR: 提出了一种针对多模态大语言模型（LLM）的校准不确定性量化（UQ）新方法，通过结合跨模态一致性和自一致性来改进校准效果。


<details>
  <summary>Details</summary>
Motivation: 现有UQ方法在多模态LLM中可能因模型在错误情况下仍表现一致而导致校准不佳，需要改进。

Method: 利用视觉输入对文本响应进行基础校准，并通过温度缩放技术校准基础模型的置信度。

Result: 在医疗问答（Slake）和视觉问答（VQAv2）任务中，该方法显著提升了校准效果。

Conclusion: 结合跨模态一致性和温度缩放能有效提升多模态LLM的校准性能。

Abstract: We introduce a novel approach for calibrating uncertainty quantification (UQ)
tailored for multi-modal large language models (LLMs). Existing
state-of-the-art UQ methods rely on consistency among multiple responses
generated by the LLM on an input query under diverse settings. However, these
approaches often report higher confidence in scenarios where the LLM is
consistently incorrect. This leads to a poorly calibrated confidence with
respect to accuracy. To address this, we leverage cross-modal consistency in
addition to self-consistency to improve the calibration of the multi-modal
models. Specifically, we ground the textual responses to the visual inputs. The
confidence from the grounding model is used to calibrate the overall
confidence. Given that using a grounding model adds its own uncertainty in the
pipeline, we apply temperature scaling - a widely accepted parametric
calibration technique - to calibrate the grounding model's confidence in the
accuracy of generated responses. We evaluate the proposed approach across
multiple multi-modal tasks, such as medical question answering (Slake) and
visual question answering (VQAv2), considering multi-modal models such as
LLaVA-Med and LLaVA. The experiments demonstrate that the proposed framework
achieves significantly improved calibration on both tasks.

</details>


### [2] [Hesitation is defeat? Connecting Linguistic and Predictive Uncertainty](https://arxiv.org/abs/2505.03910)
*Gianluca Manzo,Julia Ive*

Main category: cs.CL

TL;DR: 论文探讨了深度学习在胸片解读中的应用，重点研究了预测不确定性与人类/语言不确定性之间的关系，并评估了蒙特卡洛Dropout和深度集成方法的效果。


<details>
  <summary>Details</summary>
Motivation: 在医学领域，仅优化预测性能不足，量化不确定性同样重要。研究旨在探索机器预测不确定性与人类不确定性之间的关联。

Method: 使用BERT模型，评估了不确定性标签的二值化方法，并比较了蒙特卡洛Dropout和深度集成在不确定性估计中的效果。

Result: 模型表现良好，但预测不确定性与语言不确定性之间的相关性较弱，表明机器与人类不确定性对齐存在挑战。

Conclusion: 贝叶斯近似提供了有价值的不确定性估计，但需进一步改进以更好地捕捉人类不确定性的细微差别。

Abstract: Automating chest radiograph interpretation using Deep Learning (DL) models
has the potential to significantly improve clinical workflows, decision-making,
and large-scale health screening. However, in medical settings, merely
optimising predictive performance is insufficient, as the quantification of
uncertainty is equally crucial. This paper investigates the relationship
between predictive uncertainty, derived from Bayesian Deep Learning
approximations, and human/linguistic uncertainty, as estimated from free-text
radiology reports labelled by rule-based labellers. Utilising BERT as the model
of choice, this study evaluates different binarisation methods for uncertainty
labels and explores the efficacy of Monte Carlo Dropout and Deep Ensembles in
estimating predictive uncertainty. The results demonstrate good model
performance, but also a modest correlation between predictive and linguistic
uncertainty, highlighting the challenges in aligning machine uncertainty with
human interpretation nuances. Our findings suggest that while Bayesian
approximations provide valuable uncertainty estimates, further refinement is
necessary to fully capture and utilise the subtleties of human uncertainty in
clinical applications.

</details>


### [3] [A Reasoning-Focused Legal Retrieval Benchmark](https://arxiv.org/abs/2505.03970)
*Lucia Zheng,Neel Guha,Javokhir Arifov,Sarah Zhang,Michal Skreta,Christopher D. Manning,Peter Henderson,Daniel E. Ho*

Main category: cs.CL

TL;DR: 论文介绍了两个新的法律RAG基准测试（Bar Exam QA和Housing Statute QA），以解决法律检索和问答系统缺乏现实基准的问题。


<details>
  <summary>Details</summary>
Motivation: 法律AI开发者需要更现实的RAG基准来提升系统性能和鲁棒性，但目前缺乏这样的资源。

Method: 通过类似法律研究的标注过程，构建了两个新的法律RAG基准测试。

Result: 现有检索管线的表现表明，法律RAG仍是一个具有挑战性的应用领域。

Conclusion: 研究结果强调了未来在法律RAG领域进一步研究的必要性。

Abstract: As the legal community increasingly examines the use of large language models
(LLMs) for various legal applications, legal AI developers have turned to
retrieval-augmented LLMs ("RAG" systems) to improve system performance and
robustness. An obstacle to the development of specialized RAG systems is the
lack of realistic legal RAG benchmarks which capture the complexity of both
legal retrieval and downstream legal question-answering. To address this, we
introduce two novel legal RAG benchmarks: Bar Exam QA and Housing Statute QA.
Our tasks correspond to real-world legal research tasks, and were produced
through annotation processes which resemble legal research. We describe the
construction of these benchmarks and the performance of existing retriever
pipelines. Our results suggest that legal RAG remains a challenging
application, thus motivating future research.

</details>


### [4] [Divide, Optimize, Merge: Fine-Grained LLM Agent Optimization at Scale](https://arxiv.org/abs/2505.03973)
*Jiale Liu,Yifan Zeng,Shaokun Zhang,Chi Zhang,Malte Højmark-Bertelsen,Marie Normann Gadeberg,Huazheng Wang,Qingyun Wu*

Main category: cs.CL

TL;DR: 论文提出了一种细粒度优化（FGO）框架，通过将大型优化任务分解为可管理的子集，解决了传统LLM优化方法因数据增长导致的上下文窗口溢出和模式识别退化问题。


<details>
  <summary>Details</summary>
Motivation: 传统LLM优化方法在处理大规模数据集时，由于上下文窗口溢出和模式识别能力下降，效果受限。

Method: FGO框架将优化任务分解为子集，进行针对性优化，并通过渐进式合并系统整合优化结果。

Result: 在ALFWorld、LogisticsQA和GAIA基准测试中，FGO性能提升1.6-8.6%，同时减少56.3%的平均提示令牌消耗。

Conclusion: FGO为LLM优化提供了可扩展的解决方案，适用于日益复杂的智能体系统，展示了其高效性和可扩展性。

Abstract: LLM-based optimization has shown remarkable potential in enhancing agentic
systems. However, the conventional approach of prompting LLM optimizer with the
whole training trajectories on training dataset in a single pass becomes
untenable as datasets grow, leading to context window overflow and degraded
pattern recognition. To address these challenges, we propose Fine-Grained
Optimization (FGO), a scalable framework that divides large optimization tasks
into manageable subsets, performs targeted optimizations, and systematically
combines optimized components through progressive merging. Evaluation across
ALFWorld, LogisticsQA, and GAIA benchmarks demonstrate that FGO outperforms
existing approaches by 1.6-8.6% while reducing average prompt token consumption
by 56.3%. Our framework provides a practical solution for scaling up LLM-based
optimization of increasingly sophisticated agent systems. Further analysis
demonstrates that FGO achieves the most consistent performance gain in all
training dataset sizes, showcasing its scalability and efficiency.

</details>


### [5] [X-Reasoner: Towards Generalizable Reasoning Across Modalities and Domains](https://arxiv.org/abs/2505.03981)
*Qianchu Liu,Sheng Zhang,Guanghui Qin,Timothy Ossowski,Yu Gu,Ying Jin,Sid Kiblawi,Sam Preston,Mu Wei,Paul Vozila,Tristan Naumann,Hoifung Poon*

Main category: cs.CL

TL;DR: X-Reasoner通过两阶段训练（监督微调和强化学习）在通用文本上实现跨模态和跨领域的推理能力，并在医学领域进一步优化。


<details>
  <summary>Details</summary>
Motivation: 探索推理能力是否可跨模态和领域通用化，并解决现有开源研究在文本推理和多模态评估上的局限性。

Method: 采用两阶段方法：1）监督微调，使用蒸馏的长链思维数据；2）强化学习，基于可验证的奖励。

Result: X-Reasoner在跨模态和跨领域任务中表现优异，X-Reasoner-Med在医学领域创下新纪录。

Conclusion: 通用文本训练可实现跨模态和领域推理，医学领域优化进一步提升性能。

Abstract: Recent proprietary models (e.g., o3) have begun to demonstrate strong
multimodal reasoning capabilities. Yet, most existing open-source research
concentrates on training text-only reasoning models, with evaluations limited
to mainly mathematical and general-domain tasks. Therefore, it remains unclear
how to effectively extend reasoning capabilities beyond text input and general
domains. This paper explores a fundamental research question: Is reasoning
generalizable across modalities and domains? Our findings support an
affirmative answer: General-domain text-based post-training can enable such
strong generalizable reasoning. Leveraging this finding, we introduce
X-Reasoner, a vision-language model post-trained solely on general-domain text
for generalizable reasoning, using a two-stage approach: an initial supervised
fine-tuning phase with distilled long chain-of-thoughts, followed by
reinforcement learning with verifiable rewards. Experiments show that
X-Reasoner successfully transfers reasoning capabilities to both multimodal and
out-of-domain settings, outperforming existing state-of-the-art models trained
with in-domain and multimodal data across various general and medical
benchmarks (Figure 1). Additionally, we find that X-Reasoner's performance in
specialized domains can be further enhanced through continued training on
domain-specific text-only data. Building upon this, we introduce
X-Reasoner-Med, a medical-specialized variant that achieves new state of the
art on numerous text-only and multimodal medical benchmarks.

</details>


### [6] [SLOT: Structuring the Output of Large Language Models](https://arxiv.org/abs/2505.04016)
*Darren Yow-Bang Wang,Zhengyuan Shen,Soumya Smruti Mishra,Zhichao Xu,Yifei Teng,Haibo Ding*

Main category: cs.CL

TL;DR: SLOT是一种模型无关的方法，通过微调轻量级语言模型将非结构化LLM输出转换为精确的结构化格式，显著提升模式准确性和内容保真度。


<details>
  <summary>Details</summary>
Motivation: LLM在关键应用中生成的结构化输出常偏离预定义模式，影响可靠应用开发，SLOT旨在解决这一问题。

Method: SLOT采用微调的轻量级语言模型作为后处理层，支持多种LLM和模式规范，并通过系统化数据管道和评估方法优化性能。

Result: 微调的Mistral-7B模型在模式准确性和内容相似性上分别达到99.5%和94.0%，优于Claude-3.5-Sonnet。

Conclusion: SLOT使小型模型也能实现高质量结构化输出，为资源受限环境提供了可靠解决方案。

Abstract: Structured outputs are essential for large language models (LLMs) in critical
applications like agents and information extraction. Despite their
capabilities, LLMs often generate outputs that deviate from predefined schemas,
significantly hampering reliable application development. We present SLOT
(Structured LLM Output Transformer), a model-agnostic approach that transforms
unstructured LLM outputs into precise structured formats. While existing
solutions predominantly rely on constrained decoding techniques or are tightly
coupled with specific models, SLOT employs a fine-tuned lightweight language
model as a post-processing layer, achieving flexibility across various LLMs and
schema specifications. We introduce a systematic pipeline for data curation and
synthesis alongside a formal evaluation methodology that quantifies both schema
accuracy and content fidelity. Our results demonstrate that fine-tuned
Mistral-7B model with constrained decoding achieves near perfect schema
accuracy (99.5%) and content similarity (94.0%), outperforming
Claude-3.5-Sonnet by substantial margins (+25 and +20 percentage points,
respectively). Notably, even compact models like Llama-3.2-1B can match or
exceed the structured output capabilities of much larger proprietary models
when equipped with SLOT, enabling reliable structured generation in
resource-constrained environments.

</details>


### [7] [Advancing and Benchmarking Personalized Tool Invocation for LLMs](https://arxiv.org/abs/2505.04072)
*Xu Huang,Yuefeng Huang,Weiwen Liu,Xingshan Zeng,Yasheng Wang,Ruiming Tang,Hong Xie,Defu Lian*

Main category: cs.CL

TL;DR: 本文提出了个性化工具调用的概念，并定义了工具偏好和基于用户档案的查询两个任务。通过PTool框架和PTBench基准，展示了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注LLMs调用工具的基本能力，而忽略了个性化约束。本文旨在填补这一空白。

Method: 提出了PTool框架，用于数据合成和个性化工具调用，并构建了PTBench基准。

Result: 通过微调开源模型，验证了框架的有效性，并提供了有价值的见解。

Conclusion: PTool和PTBench为个性化工具调用提供了新的研究方向和评估标准。

Abstract: Tool invocation is a crucial mechanism for extending the capabilities of
Large Language Models (LLMs) and has recently garnered significant attention.
It enables LLMs to solve complex problems through tool calls while accessing
up-to-date world knowledge. However, existing work primarily focuses on the
fundamental ability of LLMs to invoke tools for problem-solving, without
considering personalized constraints in tool invocation. In this work, we
introduce the concept of Personalized Tool Invocation and define two key tasks:
Tool Preference and Profile-dependent Query. Tool Preference addresses user
preferences when selecting among functionally similar tools, while
Profile-dependent Query considers cases where a user query lacks certain tool
parameters, requiring the model to infer them from the user profile. To tackle
these challenges, we propose PTool, a data synthesis framework designed for
personalized tool invocation. Additionally, we construct \textbf{PTBench}, the
first benchmark for evaluating personalized tool invocation. We then fine-tune
various open-source models, demonstrating the effectiveness of our framework
and providing valuable insights. Our benchmark is public at
https://github.com/hyfshadow/PTBench.

</details>


### [8] [Natural Language Generation in Healthcare: A Review of Methods and Applications](https://arxiv.org/abs/2505.04073)
*Mengxian Lyu,Xiaohan Li,Ziyi Chen,Jinqian Pan,Cheng Peng,Sankalp Talankar,Yonghui Wu*

Main category: cs.CL

TL;DR: 本文综述了自然语言生成（NLG）在医疗领域的应用，涵盖数据模态、模型架构、临床应用及评估方法，总结了113篇相关研究，为未来研究提供了方向。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLMs）的突破，NLG在医疗领域的应用潜力巨大，但缺乏全面综述。本文旨在填补这一空白。

Method: 通过文献检索筛选出3988篇NLG相关文章，最终系统综述113篇，遵循PRISMA指南，分类关键方法并评估其能力与局限性。

Result: 总结了NLG在医疗领域的多种应用，如临床工作流优化、决策支持和文档改进，并分析了现有技术的优缺点。

Conclusion: 本文为未来研究提供了NLG在医疗领域的应用方向，强调了其在医疗发现和健康护理中的潜力。

Abstract: Natural language generation (NLG) is the key technology to achieve generative
artificial intelligence (AI). With the breakthroughs in large language models
(LLMs), NLG has been widely used in various medical applications, demonstrating
the potential to enhance clinical workflows, support clinical decision-making,
and improve clinical documentation. Heterogeneous and diverse medical data
modalities, such as medical text, images, and knowledge bases, are utilized in
NLG. Researchers have proposed many generative models and applied them in a
number of healthcare applications. There is a need for a comprehensive review
of NLG methods and applications in the medical domain. In this study, we
systematically reviewed 113 scientific publications from a total of 3,988
NLG-related articles identified using a literature search, focusing on data
modality, model architecture, clinical applications, and evaluation methods.
Following PRISMA (Preferred Reporting Items for Systematic reviews and
Meta-Analyses) guidelines, we categorize key methods, identify clinical
applications, and assess their capabilities, limitations, and emerging
challenges. This timely review covers the key NLG technologies and medical
applications and provides valuable insights for future studies to leverage NLG
to transform medical discovery and healthcare.

</details>


### [9] [Bringing legal knowledge to the public by constructing a legal question bank using large-scale pre-trained language model](https://arxiv.org/abs/2505.04132)
*Mingruo Yuan,Ben Kao,Tien-Hsuan Wu,Michael M. K. Cheung,Henry W. H. Chan,Anne S. Y. Cheung,Felix W. H. Chan,Yongxi Chen*

Main category: cs.CL

TL;DR: 论文提出了一种三步法，将复杂的法律信息转化为易于公众理解的形式，包括生成法律知识片段（CLIC-pages）、构建法律问题库（LQB）和设计交互式推荐系统（CRec）。重点研究了利用GPT-3等模型生成法律问题的技术，并比较了机器生成问题（MGQs）与人工编写问题（HCQs）的优劣。


<details>
  <summary>Details</summary>
Motivation: 法律文件通常技术性强，公众难以理解。研究旨在解决法律信息的可导航性和可理解性问题，帮助非法律专业人士获取法律知识。

Method: 采用三步法：1) 将法律条文转化为易于理解的CLIC-pages；2) 构建法律问题库（LQB）；3) 设计交互式推荐系统（CRec）。重点研究了利用GPT-3生成法律问题的技术。

Result: 机器生成问题（MGQs）更具扩展性、成本效益和多样性，而人工编写问题（HCQs）更精确。CRec原型展示了如何有效推荐相关法律知识。

Conclusion: 三步法有效提升了法律信息的可访问性，GPT-3等技术在生成法律问题方面具有潜力，但人工干预仍不可或缺。

Abstract: Access to legal information is fundamental to access to justice. Yet
accessibility refers not only to making legal documents available to the
public, but also rendering legal information comprehensible to them. A vexing
problem in bringing legal information to the public is how to turn formal legal
documents such as legislation and judgments, which are often highly technical,
to easily navigable and comprehensible knowledge to those without legal
education. In this study, we formulate a three-step approach for bringing legal
knowledge to laypersons, tackling the issues of navigability and
comprehensibility. First, we translate selected sections of the law into
snippets (called CLIC-pages), each being a small piece of article that focuses
on explaining certain technical legal concept in layperson's terms. Second, we
construct a Legal Question Bank (LQB), which is a collection of legal questions
whose answers can be found in the CLIC-pages. Third, we design an interactive
CLIC Recommender (CRec). Given a user's verbal description of a legal situation
that requires a legal solution, CRec interprets the user's input and shortlists
questions from the question bank that are most likely relevant to the given
legal situation and recommends their corresponding CLIC pages where relevant
legal knowledge can be found. In this paper we focus on the technical aspects
of creating an LQB. We show how large-scale pre-trained language models, such
as GPT-3, can be used to generate legal questions. We compare machine-generated
questions (MGQs) against human-composed questions (HCQs) and find that MGQs are
more scalable, cost-effective, and more diversified, while HCQs are more
precise. We also show a prototype of CRec and illustrate through an example how
our 3-step approach effectively brings relevant legal knowledge to the public.

</details>


### [10] [Enhancing Granular Sentiment Classification with Chain-of-Thought Prompting in Large Language Models](https://arxiv.org/abs/2505.04135)
*Vihaan Miriyala,Smrithi Bukkapatnam,Lavanya Prahallad*

Main category: cs.CL

TL;DR: 使用Chain-of-Thought (CoT)提示方法显著提升了大型语言模型在应用商店评论中的细粒度情感分类准确率。


<details>
  <summary>Details</summary>
Motivation: 传统的数值和极性评分无法捕捉用户反馈中的细微情感差异。

Method: 在2000条亚马逊应用评论上比较了CoT提示与简单提示的效果，并与人工判断进行对比。

Result: CoT提示将分类准确率从84%提升至93%。

Conclusion: 显式推理能够显著提升情感分析性能。

Abstract: We explore the use of Chain-of-Thought (CoT) prompting with large language
models (LLMs) to improve the accuracy of granular sentiment categorization in
app store reviews. Traditional numeric and polarity-based ratings often fail to
capture the nuanced sentiment embedded in user feedback. We evaluated the
effectiveness of CoT prompting versus simple prompting on 2000 Amazon app
reviews by comparing each method's predictions to human judgements. CoT
prompting improved classification accuracy from 84% to 93% highlighting the
benefit of explicit reasoning in enhancing sentiment analysis performance.

</details>


### [11] [Unmasking the Canvas: A Dynamic Benchmark for Image Generation Jailbreaking and LLM Content Safety](https://arxiv.org/abs/2505.04146)
*Variath Madhupal Gautham Nair,Vishal Varma Dantuluri*

Main category: cs.CL

TL;DR: 论文提出了Unmasking the Canvas (UTCB)基准数据集，用于评估大语言模型在图像生成任务中的安全性漏洞，并通过多语言混淆和结构化提示工程进行测试。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在图像生成任务中表现优异，但其安全性检查仍易受提示攻击，导致生成不当内容。

Method: 结合结构化提示工程、多语言混淆（如祖鲁语、盖尔语、Base64编码）和Groq托管的LLaMA-3模型进行评估，支持零样本和回退提示策略。

Result: 构建了动态可扩展的UTCB基准数据集，包含不同验证等级（Bronze、Silver、Gold）的生成内容。

Conclusion: UTCB旨在随时间演进，以应对新数据源和模型行为，同时强调负责任披露。

Abstract: Existing large language models (LLMs) are advancing rapidly and produce
outstanding results in image generation tasks, yet their content safety checks
remain vulnerable to prompt-based jailbreaks. Through preliminary testing on
platforms such as ChatGPT, MetaAI, and Grok, we observed that even short,
natural prompts could lead to the generation of compromising images ranging
from realistic depictions of forged documents to manipulated images of public
figures.
  We introduce Unmasking the Canvas (UTC Benchmark; UTCB), a dynamic and
scalable benchmark dataset to evaluate LLM vulnerability in image generation.
Our methodology combines structured prompt engineering, multilingual
obfuscation (e.g., Zulu, Gaelic, Base64), and evaluation using Groq-hosted
LLaMA-3. The pipeline supports both zero-shot and fallback prompting
strategies, risk scoring, and automated tagging. All generations are stored
with rich metadata and curated into Bronze (non-verified), Silver (LLM-aided
verification), and Gold (manually verified) tiers. UTCB is designed to evolve
over time with new data sources, prompt templates, and model behaviors.
  Warning: This paper includes visual examples of adversarial inputs designed
to test model safety. All outputs have been redacted to ensure responsible
disclosure.

</details>


### [12] [Can Language Models Understand Social Behavior in Clinical Conversations?](https://arxiv.org/abs/2505.04152)
*Manas Satish Bedmutha,Feng Chen,Andrea Hartzler,Trevor Cohen,Nadir Weibel*

Main category: cs.CL

TL;DR: 论文探讨了利用大型语言模型（LLMs）自动分析临床对话中的社交信号，以改善医患沟通效果。


<details>
  <summary>Details</summary>
Motivation: 医患沟通的效果不仅依赖于临床信息交换，还受非语言社交信号影响。LLMs的进步为自动分析这些信号提供了可能。

Method: 设计了任务特定的提示，评估了多种LLM架构和提示风格在标注数据集上的表现，涵盖20种社交信号。

Result: 开发了首个能追踪20种社交信号的系统，并揭示了LLM行为模式。

Conclusion: 研究为提升LLM在医疗社交信号处理任务中的性能提供了见解。

Abstract: Effective communication between providers and their patients influences
health and care outcomes. The effectiveness of such conversations has been
linked not only to the exchange of clinical information, but also to a range of
interpersonal behaviors; commonly referred to as social signals, which are
often conveyed through non-verbal cues and shape the quality of the
patient-provider relationship. Recent advances in large language models (LLMs)
have demonstrated an increasing ability to infer emotional and social behaviors
even when analyzing only textual information. As automation increases also in
clinical settings, such as for transcription of patient-provider conversations,
there is growing potential for LLMs to automatically analyze and extract social
behaviors from these interactions. To explore the foundational capabilities of
LLMs in tracking social signals in clinical dialogue, we designed task-specific
prompts and evaluated model performance across multiple architectures and
prompting styles using a highly imbalanced, annotated dataset spanning 20
distinct social signals such as provider dominance, patient warmth, etc. We
present the first system capable of tracking all these 20 coded signals, and
uncover patterns in LLM behavior. Further analysis of model configurations and
clinical context provides insights for enhancing LLM performance on social
signal processing tasks in healthcare settings.

</details>


### [13] [LLM-Independent Adaptive RAG: Let the Question Speak for Itself](https://arxiv.org/abs/2505.04253)
*Maria Marina,Nikolay Ivanov,Sergey Pletenev,Mikhail Salnikov,Daria Galimzianova,Nikita Krayko,Vasily Konovalov,Alexander Panchenko,Viktor Moskvoretskii*

Main category: cs.CL

TL;DR: 本文提出了一种轻量级、不依赖LLM的自适应检索方法，通过外部信息减少计算成本，同时保持性能。


<details>
  <summary>Details</summary>
Motivation: LLM容易产生幻觉，RAG虽能缓解但计算成本高且可能传播错误信息，现有自适应检索方法依赖LLM且效率低下。

Method: 研究了27个特征，分为7组及其混合组合，基于外部信息设计轻量级自适应检索方法。

Result: 在6个QA数据集上评估，性能与复杂LLM方法相当，但效率显著提升。

Conclusion: 外部信息在自适应检索中具有潜力，轻量级方法可高效替代LLM依赖方案。

Abstract: Large Language Models~(LLMs) are prone to hallucinations, and
Retrieval-Augmented Generation (RAG) helps mitigate this, but at a high
computational cost while risking misinformation. Adaptive retrieval aims to
retrieve only when necessary, but existing approaches rely on LLM-based
uncertainty estimation, which remain inefficient and impractical. In this
study, we introduce lightweight LLM-independent adaptive retrieval methods
based on external information. We investigated 27 features, organized into 7
groups, and their hybrid combinations. We evaluated these methods on 6 QA
datasets, assessing the QA performance and efficiency. The results show that
our approach matches the performance of complex LLM-based methods while
achieving significant efficiency gains, demonstrating the potential of external
information for adaptive retrieval.

</details>


### [14] [GASCADE: Grouped Summarization of Adverse Drug Event for Enhanced Cancer Pharmacovigilance](https://arxiv.org/abs/2505.04284)
*Sofia Jamil,Aryan Dabad,Bollampalli Areen Reddy,Sriparna Saha,Rajiv Misra,Adil A. Shakur*

Main category: cs.CL

TL;DR: 论文提出了一种针对癌症治疗中药物不良反应（ADEs）的分组摘要任务，并发布了MCADRS数据集和GASCADE框架，结合LLMs和T5模型，显著提升了摘要性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注一般疾病的药物不良反应，而癌症领域的药物不良反应摘要资源有限，亟需针对性解决方案。

Method: 提出GASCADE框架，结合LLMs的信息提取能力和T5模型的摘要能力，并首次在编码器-解码器模型中应用对齐技术。

Result: GASCADE在多种指标上表现优异，并通过自动评估和人工验证证实其有效性。

Conclusion: 该研究为癌症药物决策提供了新工具，推动了个性化癌症护理的发展，代码和数据集已公开。

Abstract: In the realm of cancer treatment, summarizing adverse drug events (ADEs)
reported by patients using prescribed drugs is crucial for enhancing
pharmacovigilance practices and improving drug-related decision-making. While
the volume and complexity of pharmacovigilance data have increased, existing
research in this field has predominantly focused on general diseases rather
than specifically addressing cancer. This work introduces the task of grouped
summarization of adverse drug events reported by multiple patients using the
same drug for cancer treatment. To address the challenge of limited resources
in cancer pharmacovigilance, we present the MultiLabeled Cancer Adverse Drug
Reaction and Summarization (MCADRS) dataset. This dataset includes
pharmacovigilance posts detailing patient concerns regarding drug efficacy and
adverse effects, along with extracted labels for drug names, adverse drug
events, severity, and adversity of reactions, as well as summaries of ADEs for
each drug. Additionally, we propose the Grouping and Abstractive Summarization
of Cancer Adverse Drug events (GASCADE) framework, a novel pipeline that
combines the information extraction capabilities of Large Language Models
(LLMs) with the summarization power of the encoder-decoder T5 model. Our work
is the first to apply alignment techniques, including advanced algorithms like
Direct Preference Optimization, to encoder-decoder models using synthetic
datasets for summarization tasks. Through extensive experiments, we demonstrate
the superior performance of GASCADE across various metrics, validated through
both automated assessments and human evaluations. This multitasking approach
enhances drug-related decision-making and fosters a deeper understanding of
patient concerns, paving the way for advancements in personalized and
responsive cancer care. The code and dataset used in this work are publicly
available.

</details>


### [15] [The Aloe Family Recipe for Open and Specialized Healthcare LLMs](https://arxiv.org/abs/2505.04388)
*Dario Garcia-Gasulla,Jordi Bayarri-Planas,Ashwin Kumar Gururajan,Enrique Lopez-Cuena,Adrian Tormos,Daniel Hinjos,Pablo Bernabeu-Perez,Anna Arias-Duart,Pablo Agustin Martin-Torres,Marta Gonzalez-Mallo,Sergio Alvarez-Napagao,Eduard Ayguadé-Parra,Ulises Cortés*

Main category: cs.CL

TL;DR: 该论文提出了一种开源医疗大语言模型Aloe Beta，通过优化数据预处理和训练阶段，结合DPO和RAG提升模型安全性和效能，并定义了新的评估标准。


<details>
  <summary>Details</summary>
Motivation: 随着医疗领域大语言模型的发展，需要开源模型以保护公共利益，同时提升安全性和效能。

Method: 基于Llama 3.1和Qwen 2.5等基础模型，使用合成数据增强公共数据，并通过DPO进行对齐优化，采用四种测试评估模型。

Result: Aloe Beta模型在医疗基准测试中表现优异，安全性显著提升，并附有详细风险评估。

Conclusion: Aloe Beta模型及其开发方法为开源医疗LLM领域树立了新标准，兼具高性能和伦理要求。

Abstract: Purpose: With advancements in Large Language Models (LLMs) for healthcare,
the need arises for competitive open-source models to protect the public
interest. This work contributes to the field of open medical LLMs by optimizing
key stages of data preprocessing and training, while showing how to improve
model safety (through DPO) and efficacy (through RAG). The evaluation
methodology used, which includes four different types of tests, defines a new
standard for the field. The resultant models, shown to be competitive with the
best private alternatives, are released with a permisive license.
  Methods: Building on top of strong base models like Llama 3.1 and Qwen 2.5,
Aloe Beta uses a custom dataset to enhance public data with synthetic Chain of
Thought examples. The models undergo alignment with Direct Preference
Optimization, emphasizing ethical and policy-aligned performance in the
presence of jailbreaking attacks. Evaluation includes close-ended, open-ended,
safety and human assessments, to maximize the reliability of results.
  Results: Recommendations are made across the entire pipeline, backed by the
solid performance of the Aloe Family. These models deliver competitive
performance across healthcare benchmarks and medical fields, and are often
preferred by healthcare professionals. On bias and toxicity, the Aloe Beta
models significantly improve safety, showing resilience to unseen jailbreaking
attacks. For a responsible release, a detailed risk assessment specific to
healthcare is attached to the Aloe Family models.
  Conclusion: The Aloe Beta models, and the recipe that leads to them, are a
significant contribution to the open-source medical LLM field, offering
top-of-the-line performance while maintaining high ethical requirements. This
work sets a new standard for developing and reporting aligned LLMs in
healthcare.

</details>


### [16] [Large Means Left: Political Bias in Large Language Models Increases with Their Number of Parameters](https://arxiv.org/abs/2505.04393)
*David Exler,Mark Schutera,Markus Reischl,Luca Rettenberger*

Main category: cs.CL

TL;DR: 论文研究了大型语言模型（LLMs）在德国联邦议院投票背景下的政治偏见，发现其倾向于左翼政党，并探讨了语言、模型来源和发布时间对偏见的影响。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能的普及，评估LLMs的固有偏见至关重要，以避免其对用户决策和公众舆论的负面影响。

Method: 使用Wahl-O-Mat评分量化LLMs的政治偏见，比较模型的对齐分数，分析语言、模型来源和发布时间的影响。

Result: 发现LLMs普遍倾向于左翼政党，且语言使用、模型来源和发布时间会影响其政治观点。

Conclusion: LLMs易表现出政治偏见，开发公司需承担责任以减少这些偏见对公众舆论的影响。

Abstract: With the increasing prevalence of artificial intelligence, careful evaluation
of inherent biases needs to be conducted to form the basis for alleviating the
effects these predispositions can have on users. Large language models (LLMs)
are predominantly used by many as a primary source of information for various
topics. LLMs frequently make factual errors, fabricate data (hallucinations),
or present biases, exposing users to misinformation and influencing opinions.
Educating users on their risks is key to responsible use, as bias, unlike
hallucinations, cannot be caught through data verification. We quantify the
political bias of popular LLMs in the context of the recent vote of the German
Bundestag using the score produced by the Wahl-O-Mat. This metric measures the
alignment between an individual's political views and the positions of German
political parties. We compare the models' alignment scores to identify factors
influencing their political preferences. Doing so, we discover a bias toward
left-leaning parties, most dominant in larger LLMs. Also, we find that the
language we use to communicate with the models affects their political views.
Additionally, we analyze the influence of a model's origin and release date and
compare the results to the outcome of the recent vote of the Bundestag. Our
results imply that LLMs are prone to exhibiting political bias. Large
corporations with the necessary means to develop LLMs, thus, knowingly or
unknowingly, have a responsibility to contain these biases, as they can
influence each voter's decision-making process and inform public opinion in
general and at scale.

</details>


### [17] [YABLoCo: Yet Another Benchmark for Long Context Code Generation](https://arxiv.org/abs/2505.04406)
*Aidar Valeev,Roman Garaev,Vadim Lomshakov,Irina Piontkovskaya,Vladimir Ivanov,Israel Adewuyi*

Main category: cs.CL

TL;DR: 论文提出了一个长上下文代码生成基准（YABLoCo），填补了LLMs在大型代码库（如C/C++项目）中性能评估的空白。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试通常针对小型或中型代码库，而实际项目可能包含数百万行代码，因此需要更全面的评估工具。

Method: 构建了一个包含215个函数的测试集，涵盖大型代码库（200K至2,000K LoC），并提供函数元数据、依赖关系、文档和调用图等。

Result: 提出了一个可扩展的评估管道和可视化工具，用于高效计算目标指标和分析生成代码。

Conclusion: YABLoCo基准为评估C/C++大型代码库中的代码生成能力提供了有效工具。

Abstract: Large Language Models demonstrate the ability to solve various programming
tasks, including code generation. Typically, the performance of LLMs is
measured on benchmarks with small or medium-sized context windows of thousands
of lines of code. At the same time, in real-world software projects,
repositories can span up to millions of LoC. This paper closes this gap by
contributing to the long context code generation benchmark (YABLoCo). The
benchmark featured a test set of 215 functions selected from four large
repositories with thousands of functions. The dataset contained metadata of
functions, contexts of the functions with different levels of dependencies,
docstrings, functions bodies, and call graphs for each repository. This paper
presents three key aspects of the contribution. First, the benchmark aims at
function body generation in large repositories in C and C++, two languages not
covered by previous benchmarks. Second, the benchmark contains large
repositories from 200K to 2,000K LoC. Third, we contribute a scalable
evaluation pipeline for efficient computing of the target metrics and a tool
for visual analysis of generated code. Overall, these three aspects allow for
evaluating code generation in large repositories in C and C++.

</details>


### [18] [OBLIVIATE: Robust and Practical Machine Unlearning for Large Language Models](https://arxiv.org/abs/2505.04416)
*Xiaoyu Xu,Minxin Du,Qingqing Ye,Haibo Hu*

Main category: cs.CL

TL;DR: OBLIVIATE是一种高效的遗忘框架，用于从大型语言模型中移除特定数据，同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs可能记忆敏感、受版权保护或有毒内容的问题。

Method: 通过提取目标标记、构建保留集和使用包含掩码、蒸馏和世界事实的损失函数进行微调，结合LoRA技术确保效率。

Result: 实验表明，OBLIVIATE能有效抵抗成员推理攻击，减少对保留数据的影响，并在多种场景下保持鲁棒性。

Conclusion: OBLIVIATE是一种实用且高效的遗忘框架，适用于大规模语言模型的数据移除需求。

Abstract: Large language models (LLMs) trained over extensive corpora risk memorizing
sensitive, copyrighted, or toxic content. To address this, we propose
OBLIVIATE, a robust unlearning framework that removes targeted data while
preserving model utility. The framework follows a structured process:
extracting target tokens, building retain sets, and fine-tuning with a tailored
loss function comprising three components -- masking, distillation, and world
fact. Using low-rank adapters (LoRA), it ensures efficiency without
compromising unlearning quality. We conduct experiments on multiple datasets,
including the Harry Potter series, WMDP, and TOFU, using a comprehensive suite
of metrics: forget quality (new document-level memorization score), model
utility, and fluency. Results demonstrate its effectiveness in resisting
membership inference attacks, minimizing the impact on retained data, and
maintaining robustness across diverse scenarios.

</details>


### [19] [Detecting Spelling and Grammatical Anomalies in Russian Poetry Texts](https://arxiv.org/abs/2505.04507)
*Ilya Koziev*

Main category: cs.CL

TL;DR: 论文提出通过自动语言异常检测提高生成模型训练数据的质量，比较了无监督和监督文本异常检测方法，并引入了RUPOR数据集。


<details>
  <summary>Details</summary>
Motivation: 微调数据集中自然语言文本的质量对生成模型性能至关重要，尤其是诗歌或歌词生成等创造性任务。互联网来源的训练文本缺乏质量控制，导致生成内容存在缺陷。

Method: 采用自动语言异常检测技术，比较无监督和监督文本异常检测方法，并使用合成和人工标注数据集进行验证。

Result: 提出了RUPOR数据集，一个用于跨句子语法错误检测的俄语人工标注诗歌数据集，并提供了完整的评估代码。

Conclusion: 该研究为创造性领域生成模型的训练数据质量提升提供了工具和见解。

Abstract: The quality of natural language texts in fine-tuning datasets plays a
critical role in the performance of generative models, particularly in
computational creativity tasks such as poem or song lyric generation. Fluency
defects in generated poems significantly reduce their value. However, training
texts are often sourced from internet-based platforms without stringent quality
control, posing a challenge for data engineers to manage defect levels
effectively.
  To address this issue, we propose the use of automated linguistic anomaly
detection to identify and filter out low-quality texts from training datasets
for creative models. In this paper, we present a comprehensive comparison of
unsupervised and supervised text anomaly detection approaches, utilizing both
synthetic and human-labeled datasets. We also introduce the RUPOR dataset, a
collection of Russian-language human-labeled poems designed for cross-sentence
grammatical error detection, and provide the full evaluation code. Our work
aims to empower the community with tools and insights to improve the quality of
training datasets for generative models in creative domains.

</details>


### [20] [Pangu Ultra MoE: How to Train Your Big MoE on Ascend NPUs](https://arxiv.org/abs/2505.04519)
*Yehui Tang,Yichun Yin,Yaoyuan Wang,Hang Zhou,Yu Pan,Wei Guo,Ziyang Zhang,Miao Rang,Fangcheng Liu,Naifu Zhang,Binghan Li,Yonghan Dong,Xiaojun Meng,Yasheng Wang,Dong Li,Yin Li,Dandan Tu,Can Chen,Youliang Yan,Fisher Yu,Ruiming Tang,Yunhe Wang,Botian Huang,Bo Wang,Boxiao Liu,Changzheng Zhang,Da Kuang,Fei Liu,Gang Huang,Jiansheng Wei,Jiarui Qin,Jie Ran,Jinpeng Li,Jun Zhao,Liang Dai,Lin Li,Liqun Deng,Peifeng Qin,Pengyuan Zeng,Qiang Gu,Shaohua Tang,Shengjun Cheng,Tao Gao,Tao Yu,Tianshu Li,Tianyu Bi,Wei He,Weikai Mao,Wenyong Huang,Wulong Liu,Xiabing Li,Xianzhi Yu,Xueyu Wu,Xu He,Yangkai Du,Yan Xu,Ye Tian,Yimeng Wu,Yongbing Huang,Yong Tian,Yong Zhu,Yue Li,Yufei Wang,Yuhang Gai,Yujun Li,Yu Luo,Yunsheng Ni,Yusen Sun,Zelin Chen,Zhe Liu,Zhicheng Liu,Zhipeng Tu,Zilin Ding,Zongyuan Zhan*

Main category: cs.CL

TL;DR: 论文探讨了在Ascend NPUs上高效训练稀疏大语言模型（LLMs）的方法，提出了Pangu Ultra MoE模型，并通过实验验证了其性能。


<details>
  <summary>Details</summary>
Motivation: 稀疏大语言模型（如MoE）在万亿参数规模下面临软件和硬件系统的挑战，研究旨在探索如何在Ascend NPUs上高效利用这种规模。

Method: 通过模拟比较模型超参数的权衡，优化专家并行（Expert Parallelism）以减少通信开销，并提升设备内存效率。

Result: 成功训练了7180亿参数的Pangu Ultra MoE模型，在6K Ascend NPUs上实现了30.0%的MFU，性能与DeepSeek R1相当。

Conclusion: 研究表明Ascend系统能够高效训练大规模稀疏语言模型，并为未来研究提供了参考。

Abstract: Sparse large language models (LLMs) with Mixture of Experts (MoE) and close
to a trillion parameters are dominating the realm of most capable language
models. However, the massive model scale poses significant challenges for the
underlying software and hardware systems. In this paper, we aim to uncover a
recipe to harness such scale on Ascend NPUs. The key goals are better usage of
the computing resources under the dynamic sparse model structures and
materializing the expected performance gain on the actual hardware. To select
model configurations suitable for Ascend NPUs without repeatedly running the
expensive experiments, we leverage simulation to compare the trade-off of
various model hyperparameters. This study led to Pangu Ultra MoE, a sparse LLM
with 718 billion parameters, and we conducted experiments on the model to
verify the simulation results. On the system side, we dig into Expert
Parallelism to optimize the communication between NPU devices to reduce the
synchronization overhead. We also optimize the memory efficiency within the
devices to further reduce the parameter and activation management overhead. In
the end, we achieve an MFU of 30.0% when training Pangu Ultra MoE, with
performance comparable to that of DeepSeek R1, on 6K Ascend NPUs, and
demonstrate that the Ascend system is capable of harnessing all the training
stages of the state-of-the-art language models. Extensive experiments indicate
that our recipe can lead to efficient training of large-scale sparse language
models with MoE. We also study the behaviors of such models for future
reference.

</details>


### [21] [Overcoming Data Scarcity in Generative Language Modelling for Low-Resource Languages: A Systematic Review](https://arxiv.org/abs/2505.04531)
*Josh McGiff,Nikola S. Nikolov*

Main category: cs.CL

TL;DR: 本文系统综述了解决低资源语言生成模型数据稀缺的策略，总结了技术方法、架构选择及评估趋势，并提出了扩展方法和开放挑战。


<details>
  <summary>Details</summary>
Motivation: 生成语言模型主要服务于高资源语言（如英语），加剧了自然语言处理中的语言不平等问题，本文旨在解决低资源语言的数据稀缺问题。

Method: 通过分析54项研究，分类和评估了单语数据增强、反向翻译、多语言训练和提示工程等技术方法。

Result: 研究发现过度依赖基于Transformer的模型、对少数低资源语言的关注以及评估方法不一致。

Conclusion: 提出了扩展方法至更广泛低资源语言的建议，并概述了构建公平生成语言系统的开放挑战，旨在支持开发包容性AI工具。

Abstract: Generative language modelling has surged in popularity with the emergence of
services such as ChatGPT and Google Gemini. While these models have
demonstrated transformative potential in productivity and communication, they
overwhelmingly cater to high-resource languages like English. This has
amplified concerns over linguistic inequality in natural language processing
(NLP). This paper presents the first systematic review focused specifically on
strategies to address data scarcity in generative language modelling for
low-resource languages (LRL). Drawing from 54 studies, we identify, categorise
and evaluate technical approaches, including monolingual data augmentation,
back-translation, multilingual training, and prompt engineering, across
generative tasks. We also analyse trends in architecture choices, language
family representation, and evaluation methods. Our findings highlight a strong
reliance on transformer-based models, a concentration on a small subset of
LRLs, and a lack of consistent evaluation across studies. We conclude with
recommendations for extending these methods to a wider range of LRLs and
outline open challenges in building equitable generative language systems.
Ultimately, this review aims to support researchers and developers in building
inclusive AI tools for underrepresented languages, a necessary step toward
empowering LRL speakers and the preservation of linguistic diversity in a world
increasingly shaped by large-scale language technologies.

</details>


### [22] [ZeroSearch: Incentivize the Search Capability of LLMs without Searching](https://arxiv.org/abs/2505.04588)
*Hao Sun,Zile Qiao,Jiayan Guo,Xuanbo Fan,Yingyan Hou,Yong Jiang,Pengjun Xie,Fei Huang,Yan Zhang*

Main category: cs.CL

TL;DR: ZeroSearch是一个强化学习框架，通过不依赖真实搜索引擎来提升LLM的搜索能力，解决了文档质量不可控和API成本高的问题。


<details>
  <summary>Details</summary>
Motivation: 提升大型语言模型（LLM）的搜索能力，同时避免真实搜索引擎带来的文档质量不稳定和高昂API成本问题。

Method: 采用轻量级监督微调将LLM转化为检索模块，并通过课程式策略逐步降低生成文档质量，以激发模型的推理能力。

Result: 实验表明，ZeroSearch能有效提升LLM的搜索能力，3B参数的LLM表现良好，7B参数模型性能接近真实搜索引擎，14B参数模型甚至超越。

Conclusion: ZeroSearch是一种高效且可扩展的方法，适用于不同参数规模的LLM，并与多种RL算法兼容。

Abstract: Effective information searching is essential for enhancing the reasoning and
generation capabilities of large language models (LLMs). Recent research has
explored using reinforcement learning (RL) to improve LLMs' search capabilities
by interacting with live search engines in real-world environments. While these
approaches show promising results, they face two major challenges: (1)
Uncontrolled Document Quality: The quality of documents returned by search
engines is often unpredictable, introducing noise and instability into the
training process. (2) Prohibitively High API Costs: RL training requires
frequent rollouts, potentially involving hundreds of thousands of search
requests, which incur substantial API expenses and severely constrain
scalability. To address these challenges, we introduce ZeroSearch, a
reinforcement learning framework that incentivizes the search capabilities of
LLMs without interacting with real search engines. Our approach begins with
lightweight supervised fine-tuning to transform the LLM into a retrieval module
capable of generating both relevant and noisy documents in response to a query.
During RL training, we employ a curriculum-based rollout strategy that
incrementally degrades the quality of generated documents, progressively
eliciting the model's reasoning ability by exposing it to increasingly
challenging retrieval scenarios. Extensive experiments demonstrate that
ZeroSearch effectively incentivizes the search capabilities of LLMs using a 3B
LLM as the retrieval module. Remarkably, a 7B retrieval module achieves
comparable performance to the real search engine, while a 14B retrieval module
even surpasses it. Furthermore, it generalizes well across both base and
instruction-tuned models of various parameter sizes and is compatible with a
wide range of RL algorithms.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [23] [Proceedings of 1st Workshop on Advancing Artificial Intelligence through Theory of Mind](https://arxiv.org/abs/2505.03770)
*Mouad Abrini,Omri Abend,Dina Acklin,Henny Admoni,Gregor Aichinger,Nitay Alon,Zahra Ashktorab,Ashish Atreja,Moises Auron,Alexander Aufreiter,Raghav Awasthi,Soumya Banerjee,Joe M. Barnby,Rhea Basappa,Severin Bergsmann,Djallel Bouneffouf,Patrick Callaghan,Marc Cavazza,Thierry Chaminade,Sonia Chernova,Mohamed Chetouan,Moumita Choudhury,Axel Cleeremans,Jacek B. Cywinski,Fabio Cuzzolin,Hokin Deng,N'yoma Diamond,Camilla Di Pasquasio,Guillaume Dumas,Max van Duijn,Mahapatra Dwarikanath,Qingying Gao,Ashok Goel,Rebecca Goldstein,Matthew Gombolay,Gabriel Enrique Gonzalez,Amar Halilovic,Tobias Halmdienst,Mahimul Islam,Julian Jara-Ettinger,Natalie Kastel,Renana Keydar,Ashish K. Khanna,Mahdi Khoramshahi,JiHyun Kim,MiHyeon Kim,YoungBin Kim,Senka Krivic,Nikita Krasnytskyi,Arun Kumar,JuneHyoung Kwon,Eunju Lee,Shane Lee,Peter R. Lewis,Xue Li,Yijiang Li,Michal Lewandowski,Nathan Lloyd,Matthew B. Luebbers,Dezhi Luo,Haiyun Lyu,Dwarikanath Mahapatra,Kamal Maheshwari,Mallika Mainali,Piyush Mathur,Patrick Mederitsch,Shuwa Miura,Manuel Preston de Miranda,Reuth Mirsky,Shreya Mishra,Nina Moorman,Katelyn Morrison,John Muchovej,Bernhard Nessler,Felix Nessler,Hieu Minh Jord Nguyen,Abby Ortego,Francis A. Papay,Antoine Pasquali,Hamed Rahimi,Charumathi Raghu,Amanda Royka,Stefan Sarkadi,Jaelle Scheuerman,Simon Schmid,Paul Schrater,Anik Sen,Zahra Sheikhbahaee,Ke Shi,Reid Simmons,Nishant Singh,Mason O. Smith,Ramira van der Meulen,Anthia Solaki,Haoran Sun,Viktor Szolga,Matthew E. Taylor,Travis Taylor,Sanne Van Waveren,Juan David Vargas,Rineke Verbrugge,Eitan Wagner,Justin D. Weisz,Ximing Wen,William Yeoh,Wenlong Zhang,Michelle Zhao,Shlomo Zilberstein*

Main category: cs.AI

TL;DR: 该卷收录了2025年AAAI会议上关于“通过心智理论推进人工智能”研讨会的精选论文，旨在为ToM和AI研究社区提供开放获取的精选文集。


<details>
  <summary>Details</summary>
Motivation: 为心智理论（ToM）和人工智能（AI）研究社区提供一个开放获取的精选资源，促进相关领域的研究交流与发展。

Method: 通过研讨会的形式汇集相关研究，并从中精选论文编撰成卷。

Result: 形成了一本开放获取的精选文集，涵盖了ToM与AI交叉领域的最新研究成果。

Conclusion: 该文集为ToM和AI研究社区提供了有价值的资源，有助于推动相关领域的进一步研究与合作。

Abstract: This volume includes a selection of papers presented at the Workshop on
Advancing Artificial Intelligence through Theory of Mind held at AAAI 2025 in
Philadelphia US on 3rd March 2025. The purpose of this volume is to provide an
open access and curated anthology for the ToM and AI research community.

</details>


### [24] [Design description of Wisdom Computing Persperctive](https://arxiv.org/abs/2505.03800)
*TianYi Yu*

Main category: cs.AI

TL;DR: 开发了一个基于AI和可视化动画的手写矩阵识别与计算过程展示系统，帮助学生理解抽象的数学公式和复杂计算步骤。


<details>
  <summary>Details</summary>
Motivation: 解决学生在学习数学时难以理解抽象公式和复杂计算步骤的问题。

Method: 结合Mamba骨干网络和YOLO模型实现手写矩阵的精确识别与重构，利用CoordAttention机制提升字符空间位置准确性，并通过Manim动画引擎逐步展示计算过程。

Result: 系统具有高模块化和灵活性，能实时生成不同数学运算示例，提升学生对数学逻辑的直观理解。

Conclusion: 该系统通过创新的人机交互方式，生动展示数学计算过程，帮助学生深入理解知识，实现高效学习。

Abstract: This course design aims to develop and research a handwriting matrix
recognition and step-by-step visual calculation process display system,
addressing the issue of abstract formulas and complex calculation steps that
students find difficult to understand when learning mathematics. By integrating
artificial intelligence with visualization animation technology, the system
enhances precise recognition of handwritten matrix content through the
introduction of Mamba backbone networks, completes digital extraction and
matrix reconstruction using the YOLO model, and simultaneously combines
CoordAttention coordinate attention mechanisms to improve the accurate grasp of
character spatial positions. The calculation process is demonstrated frame by
frame through the Manim animation engine, vividly showcasing each mathematical
calculation step, helping students intuitively understand the intrinsic logic
of mathematical operations. Through dynamically generating animation processes
for different computational tasks, the system exhibits high modularity and
flexibility, capable of generating various mathematical operation examples in
real-time according to student needs. By innovating human-computer interaction
methods, it brings mathematical calculation processes to life, helping students
bridge the gap between knowledge and understanding on a deeper level,
ultimately achieving a learning experience where "every step is understood."
The system's scalability and interactivity make it an intuitive, user-friendly,
and efficient auxiliary tool in education.

</details>


### [25] [GRAML: Dynamic Goal Recognition As Metric Learning](https://arxiv.org/abs/2505.03941)
*Matan Shamir,Reuth Mirsky*

Main category: cs.AI

TL;DR: GRAML是一种基于深度度量学习的目标识别方法，通过Siamese网络和RNN学习嵌入空间中的度量，快速适应新目标。


<details>
  <summary>Details</summary>
Motivation: 传统数据驱动方法需要预定义目标集且训练耗时，GRAML旨在实现模型学习的自动化并快速适应新目标。

Method: 使用Siamese网络和RNN学习嵌入空间中的度量，使不同目标的观测轨迹嵌入距离远，相同目标的嵌入距离近。

Result: GRAML在多种环境中表现出速度、灵活性和运行时的优势，同时保持高识别准确率。

Conclusion: GRAML通过深度度量学习实现了目标识别的自动化和快速适应新目标，优于现有方法。

Abstract: Goal Recognition (GR) is the problem of recognizing an agent's objectives
based on observed actions. Recent data-driven approaches for GR alleviate the
need for costly, manually crafted domain models. However, these approaches can
only reason about a pre-defined set of goals, and time-consuming training is
needed for new emerging goals. To keep this model-learning automated while
enabling quick adaptation to new goals, this paper introduces GRAML: Goal
Recognition As Metric Learning. GRAML uses a Siamese network to treat GR as a
deep metric learning task, employing an RNN that learns a metric over an
embedding space, where the embeddings for observation traces leading to
different goals are distant, and embeddings of traces leading to the same goals
are close. This metric is especially useful when adapting to new goals, even if
given just one example observation trace per goal. Evaluated on a versatile set
of environments, GRAML shows speed, flexibility, and runtime improvements over
the state-of-the-art GR while maintaining accurate recognition.

</details>


### [26] [Frog Soup: Zero-Shot, In-Context, and Sample-Efficient Frogger Agents](https://arxiv.org/abs/2505.03947)
*Xiang Li,Yiyang Hao,Doug Fulop*

Main category: cs.AI

TL;DR: 最新推理LLMs通过域外RL后训练，在零样本设置下成功挑战Atari游戏Frogger，并研究了上下文学习和推理量对性能的影响，同时展示了用LLM演示提升传统RL方法的效果。


<details>
  <summary>Details</summary>
Motivation: 开发能够快速适应并掌握新任务的通用RL智能体，解决现有RL游戏智能体训练慢且成本高的问题。

Method: 使用最新推理LLMs进行域外RL后训练，研究上下文学习和推理量对性能的影响，并利用LLM演示提升传统RL方法。

Result: LLMs在零样本设置下成功挑战Frogger，上下文学习和推理量显著影响性能，LLM演示显著提升传统RL方法的性能和样本效率。

Conclusion: LLMs在零样本RL任务中表现出潜力，结合上下文学习和LLM演示可显著提升传统RL方法的效率。

Abstract: One of the primary aspirations in reinforcement learning research is
developing general-purpose agents capable of rapidly adapting to and mastering
novel tasks. While RL gaming agents have mastered many Atari games, they remain
slow and costly to train for each game. In this work, we demonstrate that
latest reasoning LLMs with out-of-domain RL post-training can play a
challenging Atari game called Frogger under a zero-shot setting. We then
investigate the effect of in-context learning and the amount of reasoning
effort on LLM performance. Lastly, we demonstrate a way to bootstrap
traditional RL method with LLM demonstrations, which significantly improves
their performance and sample efficiency. Our implementation is open sourced at
https://github.com/AlienKevin/frogger.

</details>


### [27] [The Power of Stories: Narrative Priming Shapes How LLM Agents Collaborate and Compete](https://arxiv.org/abs/2505.03961)
*Gerrit Großmann,Larisa Ivanova,Sai Leela Poduru,Mohaddeseh Tabrizian,Islam Mesabah,David A. Selby,Sebastian J. Vollmer*

Main category: cs.AI

TL;DR: 研究探讨共享叙事是否能促进LLM代理合作，通过公共物品游戏实验发现，共同叙事提升合作，而不同叙事或自利叙事则降低合作效果。


<details>
  <summary>Details</summary>
Motivation: 探索共享叙事是否能像影响人类一样促进LLM代理的合作行为。

Method: 使用有限重复的公共物品游戏，通过不同叙事（团队合作或自利）对LLM代理进行干预，观察谈判行为。

Result: 共同叙事显著提升合作成功率，而不同叙事或自利叙事则导致自利行为占优。

Conclusion: 研究结果对多代理系统设计和AI对齐具有潜在启示。

Abstract: According to Yuval Noah Harari, large-scale human cooperation is driven by
shared narratives that encode common beliefs and values. This study explores
whether such narratives can similarly nudge LLM agents toward collaboration. We
use a finitely repeated public goods game in which LLM agents choose either
cooperative or egoistic spending strategies. We prime agents with stories
highlighting teamwork to different degrees and test how this influences
negotiation outcomes. Our experiments explore four questions:(1) How do
narratives influence negotiation behavior? (2) What differs when agents share
the same story versus different ones? (3) What happens when the agent numbers
grow? (4) Are agents resilient against self-serving negotiators? We find that
story-based priming significantly affects negotiation strategies and success
rates. Common stories improve collaboration, benefiting each agent. By
contrast, priming agents with different stories reverses this effect, and those
agents primed toward self-interest prevail. We hypothesize that these results
carry implications for multi-agent system design and AI alignment.

</details>


### [28] [LogiDebrief: A Signal-Temporal Logic based Automated Debriefing Approach with Large Language Models Integration](https://arxiv.org/abs/2505.03985)
*Zirong Chen,Ziyan An,Jennifer Reynolds,Kristin Mullen,Stephen Martini,Meiyi Ma*

Main category: cs.AI

TL;DR: LogiDebrief是一个AI驱动的框架，通过结合信号时序逻辑（STL）和大语言模型（LLM），自动评估911呼叫处理员的绩效，解决了传统人工评估的低覆盖和高延迟问题。


<details>
  <summary>Details</summary>
Motivation: 传统人工评估911呼叫处理员的绩效存在高呼叫量和低覆盖率的问题，导致评估延迟。

Method: LogiDebrief采用三步验证流程：1）上下文理解；2）基于STL和LLM的运行时检查；3）自动生成质量报告。

Result: 在Nashville应急通信部门成功部署，处理了1,701个真实呼叫，节省了311.85小时人工时间，实证评估显示其准确性高。

Conclusion: LogiDebrief有效提升了911呼叫处理员的绩效评估效率和覆盖范围，具有实际应用价值。

Abstract: Emergency response services are critical to public safety, with 9-1-1
call-takers playing a key role in ensuring timely and effective emergency
operations. To ensure call-taking performance consistency, quality assurance is
implemented to evaluate and refine call-takers' skillsets. However, traditional
human-led evaluations struggle with high call volumes, leading to low coverage
and delayed assessments. We introduce LogiDebrief, an AI-driven framework that
automates traditional 9-1-1 call debriefing by integrating Signal-Temporal
Logic (STL) with Large Language Models (LLMs) for fully-covered rigorous
performance evaluation. LogiDebrief formalizes call-taking requirements as
logical specifications, enabling systematic assessment of 9-1-1 calls against
procedural guidelines. It employs a three-step verification process: (1)
contextual understanding to identify responder types, incident classifications,
and critical conditions; (2) STL-based runtime checking with LLM integration to
ensure compliance; and (3) automated aggregation of results into quality
assurance reports. Beyond its technical contributions, LogiDebrief has
demonstrated real-world impact. Successfully deployed at Metro Nashville
Department of Emergency Communications, it has assisted in debriefing 1,701
real-world calls, saving 311.85 hours of active engagement. Empirical
evaluation with real-world data confirms its accuracy, while a case study and
extensive user study highlight its effectiveness in enhancing call-taking
performance.

</details>


### [29] [An alignment safety case sketch based on debate](https://arxiv.org/abs/2505.03989)
*Marie Davidsen Buhl,Jacob Pfau,Benjamin Hilton,Geoffrey Irving*

Main category: cs.AI

TL;DR: 论文探讨了通过辩论机制提升AI安全性的方法，重点在于防止AI系统在研发过程中产生有害行为。


<details>
  <summary>Details</summary>
Motivation: 随着AI能力接近或超越人类，人类难以有效监督其行为，需寻找新方法确保AI安全。

Method: 提出通过辩论训练AI系统，确保其诚实性，并结合在线训练维持其行为。

Result: 论文提出了一个基于辩论的AI安全框架，并指出需解决的四个关键问题。

Conclusion: 辩论机制有望成为确保AI安全的重要工具，但需进一步研究解决开放性问题。

Abstract: If AI systems match or exceed human capabilities on a wide range of tasks, it
may become difficult for humans to efficiently judge their actions -- making it
hard to use human feedback to steer them towards desirable traits. One proposed
solution is to leverage another superhuman system to point out flaws in the
system's outputs via a debate. This paper outlines the value of debate for AI
safety, as well as the assumptions and further research required to make debate
work. It does so by sketching an ``alignment safety case'' -- an argument that
an AI system will not autonomously take actions which could lead to egregious
harm, despite being able to do so. The sketch focuses on the risk of an AI R\&D
agent inside an AI company sabotaging research, for example by producing false
results. To prevent this, the agent is trained via debate, subject to
exploration guarantees, to teach the system to be honest. Honesty is maintained
throughout deployment via online training. The safety case rests on four key
claims: (1) the agent has become good at the debate game, (2) good performance
in the debate game implies that the system is mostly honest, (3) the system
will not become significantly less honest during deployment, and (4) the
deployment context is tolerant of some errors. We identify open research
problems that, if solved, could render this a compelling argument that an AI
system is safe.

</details>


### [30] [Extending Decision Predicate Graphs for Comprehensive Explanation of Isolation Forest](https://arxiv.org/abs/2505.04019)
*Matteo Ceschin,Leonardo Arrighi,Luca Longo,Sylvio Barbon Junior*

Main category: cs.AI

TL;DR: 论文提出了一种新的可解释AI方法（XAI），用于解决孤立森林（iForest）在异常检测中的全局可解释性问题，通过决策谓词图（DPG）和异常-正常传播分数（IOP-Score）提供透明化的决策过程。


<details>
  <summary>Details</summary>
Motivation: 现代机器学习中，不仅需要解释预测模型，还需理解数据预处理方法的影响。孤立森林（iForest）在异常检测中表现优异，但其复杂的树结构导致决策边界和异常选择难以解释。

Method: 提出基于决策谓词图（DPG）的方法，结合异常-正常传播分数（IOP-Score），为iForest提供全局解释，揭示特征如何影响异常识别。

Result: 该方法增强了iForest的可解释性，提供了决策过程的全面视图，明确了特征在异常识别中的作用。

Conclusion: 研究通过提供透明的决策边界和特征使用视图，推动了完全可解释的机器学习流程的发展。

Abstract: The need to explain predictive models is well-established in modern machine
learning. However, beyond model interpretability, understanding pre-processing
methods is equally essential. Understanding how data modifications impact model
performance improvements and potential biases and promoting a reliable pipeline
is mandatory for developing robust machine learning solutions. Isolation Forest
(iForest) is a widely used technique for outlier detection that performs well.
Its effectiveness increases with the number of tree-based learners. However,
this also complicates the explanation of outlier selection and the decision
boundaries for inliers. This research introduces a novel Explainable AI (XAI)
method, tackling the problem of global explainability. In detail, it aims to
offer a global explanation for outlier detection to address its opaque nature.
Our approach is based on the Decision Predicate Graph (DPG), which clarifies
the logic of ensemble methods and provides both insights and a graph-based
metric to explain how samples are identified as outliers using the proposed
Inlier-Outlier Propagation Score (IOP-Score). Our proposal enhances iForest's
explainability and provides a comprehensive view of the decision-making
process, detailing which features contribute to outlier identification and how
the model utilizes them. This method advances the state-of-the-art by providing
insights into decision boundaries and a comprehensive view of holistic feature
usage in outlier identification. -- thus promoting a fully explainable machine
learning pipeline.

</details>


### [31] [Polynomial-Time Relational Probabilistic Inference in Open Universes](https://arxiv.org/abs/2505.04115)
*Luise Ge,Brendan Juba,Kris Nilsson*

Main category: cs.AI

TL;DR: 该论文提出了一种基于人类推理启发的概率推理方法，能够在多项式时间内处理混合变量和关系数据，同时满足表达能力和计算效率的需求。


<details>
  <summary>Details</summary>
Motivation: 解决人工智能中不确定性推理的表达能力与计算效率之间的权衡问题。

Method: 扩展了期望的平方和逻辑到关系设置，支持混合变量，并在有界度片段和有界量词秩的知识库中进行提升推理。

Result: 证明了在多项式时间内可完成推理，即使对象集未知或无限，且通过证明理论框架确保了紧致的界限和完备性。

Conclusion: 该方法在表达能力和计算效率之间取得了平衡，为不确定性推理提供了新的理论框架。

Abstract: Reasoning under uncertainty is a fundamental challenge in Artificial
Intelligence. As with most of these challenges, there is a harsh dilemma
between the expressive power of the language used, and the tractability of the
computational problem posed by reasoning. Inspired by human reasoning, we
introduce a method of first-order relational probabilistic inference that
satisfies both criteria, and can handle hybrid (discrete and continuous)
variables. Specifically, we extend sum-of-squares logic of expectation to
relational settings, demonstrating that lifted reasoning in the bounded-degree
fragment for knowledge bases of bounded quantifier rank can be performed in
polynomial time, even with an a priori unknown and/or countably infinite set of
objects. Crucially, our notion of tractability is framed in proof-theoretic
terms, which extends beyond the syntactic properties of the language or
queries. We are able to derive the tightest bounds provable by proofs of a
given degree and size and establish completeness in our sum-of-squares
refutations for fixed degrees.

</details>


### [32] [Flow Models for Unbounded and Geometry-Aware Distributional Reinforcement Learning](https://arxiv.org/abs/2505.04310)
*Simo Alami C.,Rim Kaddah,Jesse Read,Marie-Paule Cani*

Main category: cs.AI

TL;DR: 提出了一种基于归一化流的分布强化学习新架构，支持灵活无界的回报分布建模，优于固定或有限表示方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法如C51和分位数方法在建模回报分布时存在局限性，如固定支持或建模能力不足。

Method: 使用归一化流建模回报分布，提出几何感知的Cramér距离替代方法，避免昂贵的CDF计算。

Result: 在ATARI-5子基准测试中表现优于PDF模型，与分位数方法竞争。

Conclusion: 新方法在参数效率和建模能力上优于现有方法，为分布强化学习提供了更灵活的解决方案。

Abstract: We introduce a new architecture for Distributional Reinforcement Learning
(DistRL) that models return distributions using normalizing flows. This
approach enables flexible, unbounded support for return distributions, in
contrast to categorical approaches like C51 that rely on fixed or bounded
representations. It also offers richer modeling capacity to capture
multi-modality, skewness, and tail behavior than quantile based approaches. Our
method is significantly more parameter-efficient than categorical approaches.
Standard metrics used to train existing models like KL divergence or
Wasserstein distance either are scale insensitive or have biased sample
gradients, especially when return supports do not overlap. To address this, we
propose a novel surrogate for the Cram\`er distance, that is geometry-aware and
computable directly from the return distribution's PDF, avoiding the costly CDF
computation. We test our model on the ATARI-5 sub-benchmark and show that our
approach outperforms PDF based models while remaining competitive with quantile
based methods.

</details>


### [33] [KERAIA: An Adaptive and Explainable Framework for Dynamic Knowledge Representation and Reasoning](https://arxiv.org/abs/2505.04313)
*Stephen Richard Varey,Alessandro Di Stefano,The Anh Han*

Main category: cs.AI

TL;DR: KERAIA是一个新颖的符号知识工程框架，旨在解决动态、复杂和上下文敏感环境中的知识表示、推理和执行问题。


<details>
  <summary>Details</summary>
Motivation: 研究核心问题是如何将非结构化的人类专业知识转化为AI系统可计算的高效算法。

Method: 基于Minsky的框架推理和K-lines，引入动态知识聚合（Clouds of Knowledge）、上下文敏感继承（Dynamic Relations）、可追溯推理（Lines of Thought）和自适应知识转换（Cloud Elaboration）。

Result: 通过多个案例研究验证了KERAIA的通用性、表达能力和实用性，并与现有知识表示范式进行了比较。

Conclusion: KERAIA突破了传统静态知识表示的局限，以可解释AI为核心，提供了透明和可解释的解决方案。

Abstract: In this paper, we introduce KERAIA, a novel framework and software platform
for symbolic knowledge engineering designed to address the persistent
challenges of representing, reasoning with, and executing knowledge in dynamic,
complex, and context-sensitive environments. The central research question that
motivates this work is: How can unstructured, often tacit, human expertise be
effectively transformed into computationally tractable algorithms that AI
systems can efficiently utilise? KERAIA seeks to bridge this gap by building on
foundational concepts such as Minsky's frame-based reasoning and K-lines, while
introducing significant innovations. These include Clouds of Knowledge for
dynamic aggregation, Dynamic Relations (DRels) for context-sensitive
inheritance, explicit Lines of Thought (LoTs) for traceable reasoning, and
Cloud Elaboration for adaptive knowledge transformation. This approach moves
beyond the limitations of traditional, often static, knowledge representation
paradigms. KERAIA is designed with Explainable AI (XAI) as a core principle,
ensuring transparency and interpretability, particularly through the use of
LoTs. The paper details the framework's architecture, the KSYNTH representation
language, and the General Purpose Paradigm Builder (GPPB) to integrate diverse
inference methods within a unified structure. We validate KERAIA's versatility,
expressiveness, and practical applicability through detailed analysis of
multiple case studies spanning naval warfare simulation, industrial diagnostics
in water treatment plants, and strategic decision-making in the game of RISK.
Furthermore, we provide a comparative analysis against established knowledge
representation paradigms (including ontologies, rule-based systems, and
knowledge graphs) and discuss the implementation aspects and computational
considerations of the KERAIA platform.

</details>


### [34] [Mastering Multi-Drone Volleyball through Hierarchical Co-Self-Play Reinforcement Learning](https://arxiv.org/abs/2505.04317)
*Ruize Zhang,Sirui Xiang,Zelai Xu,Feng Gao,Shilong Ji,Wenhao Tang,Wenbo Ding,Chao Yu,Yu Wang*

Main category: cs.AI

TL;DR: 论文提出了一种分层强化学习框架HCSP，用于解决3v3多无人机排球任务中的高难度挑战，通过分层训练实现了高性能和团队协作行为。


<details>
  <summary>Details</summary>
Motivation: 解决多无人机排球任务中因长时依赖、紧密耦合和欠驱动动力学带来的挑战，需要同时实现高级战略协调和低级敏捷控制。

Method: 提出HCSP框架，分三个阶段训练：1) 训练多样化低级技能；2) 通过自学习固定低级控制器学习高级战略；3) 通过协同自学习联合微调。

Result: HCSP平均胜率达82.9%，优于非分层自学习和基于规则的分层基线，并涌现出角色切换和队形协调等团队行为。

Conclusion: HCSP的分层设计和训练方案有效解决了复杂多智能体任务，展示了其在高难度场景中的潜力。

Abstract: In this paper, we tackle the problem of learning to play 3v3 multi-drone
volleyball, a new embodied competitive task that requires both high-level
strategic coordination and low-level agile control. The task is turn-based,
multi-agent, and physically grounded, posing significant challenges due to its
long-horizon dependencies, tight inter-agent coupling, and the underactuated
dynamics of quadrotors. To address this, we propose Hierarchical Co-Self-Play
(HCSP), a hierarchical reinforcement learning framework that separates
centralized high-level strategic decision-making from decentralized low-level
motion control. We design a three-stage population-based training pipeline to
enable both strategy and skill to emerge from scratch without expert
demonstrations: (I) training diverse low-level skills, (II) learning high-level
strategy via self-play with fixed low-level controllers, and (III) joint
fine-tuning through co-self-play. Experiments show that HCSP achieves superior
performance, outperforming non-hierarchical self-play and rule-based
hierarchical baselines with an average 82.9\% win rate and a 71.5\% win rate
against the two-stage variant. Moreover, co-self-play leads to emergent team
behaviors such as role switching and coordinated formations, demonstrating the
effectiveness of our hierarchical design and training scheme.

</details>


### [35] [Uncertain Machine Ethics Planning](https://arxiv.org/abs/2505.04352)
*Simon Kolker,Louise A. Dennis,Ramon Fraga Pereira,Mengwei Xu*

Main category: cs.AI

TL;DR: 论文提出了一种基于多道德马尔可夫决策过程的框架，用于在不确定性下处理机器伦理决策，并通过案例研究验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 机器伦理决策需要考虑不确定性及长期结果，而不同道德理论可能产生冲突判断，因此需要一种统一的框架来处理这些复杂性。

Method: 将问题形式化为多道德马尔可夫决策过程和多道德随机最短路径问题，开发了一种基于多目标AO*的启发式算法，并结合Sven-Ove Hansson的假设回顾程序进行伦理推理。

Result: 通过一个机器伦理案例（是否偷胰岛素）验证了方法的有效性。

Conclusion: 提出的框架能够处理多道德理论下的冲突判断，为机器伦理决策提供了一种实用方法。

Abstract: Machine Ethics decisions should consider the implications of uncertainty over
decisions. Decisions should be made over sequences of actions to reach
preferable outcomes long term. The evaluation of outcomes, however, may invoke
one or more moral theories, which might have conflicting judgements. Each
theory will require differing representations of the ethical situation. For
example, Utilitarianism measures numerical values, Deontology analyses duties,
and Virtue Ethics emphasises moral character. While balancing potentially
conflicting moral considerations, decisions may need to be made, for example,
to achieve morally neutral goals with minimal costs. In this paper, we
formalise the problem as a Multi-Moral Markov Decision Process and a
Multi-Moral Stochastic Shortest Path Problem. We develop a heuristic algorithm
based on Multi-Objective AO*, utilising Sven-Ove Hansson's Hypothetical
Retrospection procedure for ethical reasoning under uncertainty. Our approach
is validated by a case study from Machine Ethics literature: the problem of
whether to steal insulin for someone who needs it.

</details>


### [36] [TrajEvo: Designing Trajectory Prediction Heuristics via LLM-driven Evolution](https://arxiv.org/abs/2505.04480)
*Zhikai Zhao,Chuanbo Hua,Federico Berto,Kanghoon Lee,Zihan Ma,Jiachen Li,Jinkyoo Park*

Main category: cs.AI

TL;DR: TrajEvo是一个利用大型语言模型（LLMs）自动设计轨迹预测启发式方法的框架，通过进化算法生成和优化预测规则，在ETH-UCY和SDD数据集上表现优于传统启发式和深度学习方法。


<details>
  <summary>Details</summary>
Motivation: 传统启发式方法准确性不足，而深度学习方法存在计算成本高、缺乏可解释性和泛化性问题，限制了实际应用。

Method: TrajEvo结合进化算法和LLMs，采用跨代精英采样促进多样性，并通过统计反馈循环分析预测结果。

Result: 在ETH-UCY数据集上优于传统启发式方法，在未见过的SDD数据集上显著优于启发式和深度学习方法。

Conclusion: TrajEvo为快速、可解释且泛化性强的轨迹预测启发式方法的自动设计迈出了第一步，代码已开源。

Abstract: Trajectory prediction is a crucial task in modeling human behavior,
especially in fields as social robotics and autonomous vehicle navigation.
Traditional heuristics based on handcrafted rules often lack accuracy, while
recently proposed deep learning approaches suffer from computational cost, lack
of explainability, and generalization issues that limit their practical
adoption. In this paper, we introduce TrajEvo, a framework that leverages Large
Language Models (LLMs) to automatically design trajectory prediction
heuristics. TrajEvo employs an evolutionary algorithm to generate and refine
prediction heuristics from past trajectory data. We introduce a
Cross-Generation Elite Sampling to promote population diversity and a
Statistics Feedback Loop allowing the LLM to analyze alternative predictions.
Our evaluations show TrajEvo outperforms previous heuristic methods on the
ETH-UCY datasets, and remarkably outperforms both heuristics and deep learning
methods when generalizing to the unseen SDD dataset. TrajEvo represents a first
step toward automated design of fast, explainable, and generalizable trajectory
prediction heuristics. We make our source code publicly available to foster
future research at https://github.com/ai4co/trajevo.

</details>


### [37] [On some improvements to Unbounded Minimax](https://arxiv.org/abs/2505.04525)
*Quentin Cohen-Solal,Tristan Cazenave*

Main category: cs.AI

TL;DR: 本文首次实验评估了四种未测试的Unbounded Best-First Minimax算法改进，包括使用转置表、比较不同回传策略、替换终端评估函数及完成技术，发现这些改进能提升算法效率。


<details>
  <summary>Details</summary>
Motivation: 评估四种未测试的改进对Unbounded Best-First Minimax算法性能的影响，以优化其效率。

Method: 通过实验比较四种改进：转置表、回传策略（Korf & Chickering vs. Cohen-Solal）、替换终端评估函数为启发式函数、完成技术。

Result: 转置表和完成技术提升性能；Cohen-Solal的回传策略在特定情况下略优；启发式函数在成本高时有益，但低成本时性能下降。

Conclusion: 针对性改进能显著提升Unbounded Best-First Minimax算法的效率。

Abstract: This paper presents the first experimental evaluation of four previously
untested modifications of Unbounded Best-First Minimax algorithm. This
algorithm explores the game tree by iteratively expanding the most promising
sequences of actions based on the current partial game tree. We first evaluate
the use of transposition tables, which convert the game tree into a directed
acyclic graph by merging duplicate states. Second, we compare the original
algorithm by Korf & Chickering with the variant proposed by Cohen-Solal, which
differs in its backpropagation strategy: instead of stopping when a stable
value is encountered, it updates values up to the root. This change slightly
improves performance when value ties or transposition tables are involved.
Third, we assess replacing the exact terminal evaluation function with the
learned heuristic function. While beneficial when exact evaluations are costly,
this modification reduces performance in inexpensive settings. Finally, we
examine the impact of the completion technique that prioritizes resolved
winning states and avoids resolved losing states. This technique also improves
performance. Overall, our findings highlight how targeted modifications can
enhance the efficiency of Unbounded Best-First Minimax.

</details>


### [38] [Beyond Theorem Proving: Formulation, Framework and Benchmark for Formal Problem-Solving](https://arxiv.org/abs/2505.04528)
*Qi Liu,Xinhao Zheng,Renqiu Xia,Xingzhi Qi,Qinxiang Cao,Junchi Yan*

Main category: cs.AI

TL;DR: 论文提出了一种基于形式化定理证明的问题解决框架FPS和D-FPS，并通过实验验证了其表达能力和性能。


<details>
  <summary>Details</summary>
Motivation: 问题解决在科学和工程中至关重要，但目前缺乏通用且具体的定义。随着AI问题解决代理的发展，对过程级可验证性的需求增加，但研究不足。

Method: 将问题解决建模为确定性马尔可夫决策过程，提出FPS框架，利用形式化定理证明环境实现过程验证，并进一步提出D-FPS框架，分离解决和验证步骤。

Result: 证明了框架的表达性、完备性和可靠性，构建了三个基准测试集，并评估了现有FTP模型的性能，结果显示解决率较低。

Conclusion: FPS和D-FPS为问题解决提供了形式化、可验证的框架，但现有模型在基准测试中的表现仍有提升空间。

Abstract: As a seemingly self-explanatory task, problem-solving has been a significant
component of science and engineering. However, a general yet concrete
formulation of problem-solving itself is missing. With the recent development
of AI-based problem-solving agents, the demand for process-level verifiability
is rapidly increasing yet underexplored. To fill these gaps, we present a
principled formulation of problem-solving as a deterministic Markov decision
process; a novel framework, FPS (Formal Problem-Solving), which utilizes
existing FTP (formal theorem proving) environments to perform process-verified
problem-solving; and D-FPS (Deductive FPS), decoupling solving and answer
verification for better human-alignment. The expressiveness, soundness and
completeness of the frameworks are proven. We construct three benchmarks on
problem-solving: FormalMath500, a formalization of a subset of the MATH500
benchmark; MiniF2F-Solving and PutnamBench-Solving, adaptations of FTP
benchmarks MiniF2F and PutnamBench. For faithful, interpretable, and
human-aligned evaluation, we propose RPE (Restricted Propositional
Equivalence), a symbolic approach to determine the correctness of answers by
formal verification. We evaluate four prevalent FTP models and two prompting
methods as baselines, solving at most 23.77% of FormalMath500, 27.47% of
MiniF2F-Solving, and 0.31% of PutnamBench-Solving.

</details>


### [39] [Qualitative Analysis of $ω$-Regular Objectives on Robust MDPs](https://arxiv.org/abs/2505.04539)
*Ali Asadi,Krishnendu Chatterjee,Ehsan Kafshdar Goharshady,Mehrdad Karrabi,Ali Shafiee*

Main category: cs.AI

TL;DR: 本文研究了鲁棒马尔可夫决策过程（RMDPs）中可达性和奇偶性目标的定性分析问题，提出了高效的算法，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: RMDPs扩展了经典MDPs，考虑了转移概率的不确定性。本文旨在解决在无结构假设（如单链或非周期性）下，RMDPs的可达性和奇偶性目标的定性分析问题。

Method: 提出了基于不确定性集合的oracle访问的高效算法，用于解决可达性和奇偶性目标的定性问题。

Result: 实验结果表明，所提出的oracle方法在经典RMDP示例上具有高效性，可扩展到数千个状态。

Conclusion: 本文为RMDPs的可达性和奇偶性目标的定性分析提供了高效的解决方案，并通过实验验证了其实际应用价值。

Abstract: Robust Markov Decision Processes (RMDPs) generalize classical MDPs that
consider uncertainties in transition probabilities by defining a set of
possible transition functions. An objective is a set of runs (or infinite
trajectories) of the RMDP, and the value for an objective is the maximal
probability that the agent can guarantee against the adversarial environment.
We consider (a) reachability objectives, where given a target set of states,
the goal is to eventually arrive at one of them; and (b) parity objectives,
which are a canonical representation for $\omega$-regular objectives. The
qualitative analysis problem asks whether the objective can be ensured with
probability 1.
  In this work, we study the qualitative problem for reachability and parity
objectives on RMDPs without making any assumption over the structures of the
RMDPs, e.g., unichain or aperiodic. Our contributions are twofold. We first
present efficient algorithms with oracle access to uncertainty sets that solve
qualitative problems of reachability and parity objectives. We then report
experimental results demonstrating the effectiveness of our oracle-based
approach on classical RMDP examples from the literature scaling up to thousands
of states.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [40] [Out-of-Distribution Detection in Heterogeneous Graphs via Energy Propagation](https://arxiv.org/abs/2505.03774)
*Tao Yin,Chen Zhao,Xiaoyan Liu,Minglai Shao*

Main category: cs.LG

TL;DR: 该论文提出了一种用于异构图中的OOD检测方法（OODHG），旨在检测OOD节点并分类ID节点，通过能量传播机制和约束提升性能。


<details>
  <summary>Details</summary>
Motivation: 现实中的图数据通常是异构的，而现有研究多关注同构图。OOD检测在异构图中尚未充分探索，因此需要一种新方法。

Method: 通过学习节点表示、计算能量值区分OOD节点，并分类ID节点。引入基于元路径的能量传播机制和能量约束。

Result: 实验证明OODHG在OOD检测任务中优于基线模型，且在ID节点分类中表现准确。

Conclusion: OODHG是一种简单有效的方法，适用于异构图的OOD检测和ID节点分类。

Abstract: Graph neural networks (GNNs) are proven effective in extracting complex node
and structural information from graph data. While current GNNs perform well in
node classification tasks within in-distribution (ID) settings, real-world
scenarios often present distribution shifts, leading to the presence of
out-of-distribution (OOD) nodes. OOD detection in graphs is a crucial and
challenging task. Most existing research focuses on homogeneous graphs, but
real-world graphs are often heterogeneous, consisting of diverse node and edge
types. This heterogeneity adds complexity and enriches the informational
content. To the best of our knowledge, OOD detection in heterogeneous graphs
remains an underexplored area. In this context, we propose a novel methodology
for OOD detection in heterogeneous graphs (OODHG) that aims to achieve two main
objectives: 1) detecting OOD nodes and 2) classifying all ID nodes based on the
first task's results. Specifically, we learn representations for each node in
the heterogeneous graph, calculate energy values to determine whether nodes are
OOD, and then classify ID nodes. To leverage the structural information of
heterogeneous graphs, we introduce a meta-path-based energy propagation
mechanism and an energy constraint to enhance the distinction between ID and
OOD nodes. Extensive experimental findings substantiate the simplicity and
effectiveness of OODHG, demonstrating its superiority over baseline models in
OOD detection tasks and its accuracy in ID node classification.

</details>


### [41] [Hierarchical Multi-Label Generation with Probabilistic Level-Constraint](https://arxiv.org/abs/2505.03775)
*Linqing Chen,Weilei Wang,Wentao Wu,Hanmeng Zhong*

Main category: cs.LG

TL;DR: 论文提出了一种生成式框架（HMG）结合概率层级约束（PLC），用于解决层次极端多标签分类问题，无需依赖聚类等预处理步骤，并能精确控制模型输出。


<details>
  <summary>Details</summary>
Motivation: 传统层次多标签分类方法因标签间复杂层级关系和数量庞大而效果有限，生成式方法虽被尝试但输出控制不足。

Method: 将任务重新定义为层次多标签生成（HMG），采用生成式框架和概率层级约束（PLC）生成符合特定分类法的标签。

Result: 实验表明，该方法在HMG任务中达到新SOTA性能，且在输出控制方面优于先前研究。

Conclusion: 提出的HMG框架有效解决了层次极端多标签分类问题，并在生成和控制方面表现优异。

Abstract: Hierarchical Extreme Multi-Label Classification poses greater difficulties
compared to traditional multi-label classification because of the intricate
hierarchical connections of labels within a domain-specific taxonomy and the
substantial number of labels. Some of the prior research endeavors centered on
classifying text through several ancillary stages such as the cluster algorithm
and multiphase classification. Others made attempts to leverage the assistance
of generative methods yet were unable to properly control the output of the
generative model. We redefine the task from hierarchical multi-Label
classification to Hierarchical Multi-Label Generation (HMG) and employ a
generative framework with Probabilistic Level Constraints (PLC) to generate
hierarchical labels within a specific taxonomy that have complex hierarchical
relationships. The approach we proposed in this paper enables the framework to
generate all relevant labels across levels for each document without relying on
preliminary operations like clustering. Meanwhile, it can control the model
output precisely in terms of count, length, and level aspects. Experiments
demonstrate that our approach not only achieves a new SOTA performance in the
HMG task, but also has a much better performance in constrained the output of
model than previous research work.

</details>


### [42] [PAPN: Proximity Attention Encoder and Pointer Network Decoder for Parcel Pickup Route Prediction](https://arxiv.org/abs/2505.03776)
*Hansi Denis,Siegfried Mercelis,Ngoc-Quang Luong*

Main category: cs.LG

TL;DR: 论文提出了一种结合邻近注意力机制和指针网络的编码器-解码器架构（PAPN），用于优化最后一英里配送和第一英里取件的路线预测，显著优于现有监督学习方法。


<details>
  <summary>Details</summary>
Motivation: 优化最后一英里配送和第一英里取件是物流优化的重要环节，需要准确的路线和时间预测系统以适应不同场景。

Method: 提出PAPN模型，结合邻近注意力机制和指针网络，同时利用全局和局部注意力建模问题。

Result: 在LaDE数据集上表现优异，超越现有监督学习方法，与最佳强化学习方法DRL4Route竞争。

Conclusion: PAPN模型在路线预测任务中表现出色，为物流优化提供了有效工具。

Abstract: Optimization of the last-mile delivery and first-mile pickup of parcels is an
integral part of the broader logistics optimization pipeline as it entails both
cost and resource efficiency as well as a heightened service quality. Such
optimization requires accurate route and time prediction systems to adapt to
different scenarios in advance. This work tackles the first building block,
namely route prediction. This is done by introducing a novel Proximity
Attention mechanism in an encoder-decoder architecture utilizing a Pointer
Network in the decoding process (Proximity Attention Encoder and Pointer
Network decoder: PAPN) to leverage the underlying connections between the
different visitable pickup positions at each timestep. To this local attention
process is coupled global context computing via a multi-head attention
transformer encoder. The obtained global context is then mixed to an aggregated
version of the local embedding thus achieving a mix of global and local
attention for complete modeling of the problems. Proximity attention is also
used in the decoding process to skew predictions towards the locations with the
highest attention scores and thus using inter-connectivity of locations as a
base for next-location prediction. This method is trained, validated and tested
on a large industry-level dataset of real-world, large-scale last-mile delivery
and first-mile pickup named LaDE[1]. This approach shows noticeable promise,
outperforming all state-of-the-art supervised systems in terms of most metrics
used for benchmarking methods on this dataset while still being competitive
with the best-performing reinforcement learning method named DRL4Route[2].

</details>


### [43] [MolMole: Molecule Mining from Scientific Literature](https://arxiv.org/abs/2505.03777)
*LG AI Research,Sehyun Chun,Jiye Kim,Ahra Jo,Yeonsik Jo,Seungyul Oh,Seungjun Lee,Kwangrok Ryoo,Jongmin Lee,Seunghwan Kim,Byung Jun Kang,Soonyoung Lee,Jun Ha Park,Chanwoo Moon,Jiwon Ham,Haein Lee,Heejae Han,Jaeseung Byun,Soojong Do,Minju Ha,Dongyun Kim,Kyunghoon Bae,Woohyung Lim,Edward Hwayoung Lee,Yongmin Park,Jeongsang Yu,Gerrard Jeongwon Jo,Yeonjung Hong,Kyungjae Yoo,Sehui Han,Jaewan Lee,Changyoung Park,Kijeong Jeon,Sihyuk Yi*

Main category: cs.LG

TL;DR: MolMole是一个基于视觉的深度学习框架，用于从科学文档中自动化提取分子结构和反应数据，统一了分子检测、反应图解析和光学化学结构识别。


<details>
  <summary>Details</summary>
Motivation: 科学文档中的化学数据格式多样且非结构化，提取困难，缺乏标准化的基准和评估指标。

Method: MolMole通过深度学习框架将分子检测、反应图解析和OCSR整合为单一流程，并提供了新的测试集和评估指标。

Result: MolMole在基准测试和公共数据集上优于现有工具包。

Conclusion: MolMole框架及其测试集将公开，工具包将通过LG AI Research网站提供交互式演示。

Abstract: The extraction of molecular structures and reaction data from scientific
documents is challenging due to their varied, unstructured chemical formats and
complex document layouts. To address this, we introduce MolMole, a vision-based
deep learning framework that unifies molecule detection, reaction diagram
parsing, and optical chemical structure recognition (OCSR) into a single
pipeline for automating the extraction of chemical data directly from
page-level documents. Recognizing the lack of a standard page-level benchmark
and evaluation metric, we also present a testset of 550 pages annotated with
molecule bounding boxes, reaction labels, and MOLfiles, along with a novel
evaluation metric. Experimental results demonstrate that MolMole outperforms
existing toolkits on both our benchmark and public datasets. The benchmark
testset will be publicly available, and the MolMole toolkit will be accessible
soon through an interactive demo on the LG AI Research website. For commercial
inquiries, please contact us at
\href{mailto:contact_ddu@lgresearch.ai}{contact\_ddu@lgresearch.ai}.

</details>


### [44] [Dragonfly: a modular deep reinforcement learning library](https://arxiv.org/abs/2505.03778)
*Jonathan Viquerat,Paul Garnier,Amirhossein Bateni,Elie Hachem*

Main category: cs.LG

TL;DR: Dragonfly是一个模块化的深度强化学习库，旨在简化实验和开发，支持通过JSON序列化交换模块和参数扫描，性能优于文献中的标准基准。


<details>
  <summary>Details</summary>
Motivation: 为了简化深度强化学习的实验和开发过程，减少代码维护成本，同时支持CPU密集型环境（如数值模拟）。

Method: 采用模块化设计和JSON序列化，允许灵活交换模块和参数扫描。

Result: 在标准代理和常见基准测试中表现优于文献中的其他方法。

Conclusion: Dragonfly是一个高效且灵活的深度强化学习库，适用于实验和开发，尤其在CPU密集型任务中表现优异。

Abstract: Dragonfly is a deep reinforcement learning library focused on modularity, in
order to ease experimentation and developments. It relies on a json
serialization that allows to swap building blocks and perform parameter sweep,
while minimizing code maintenance. Some of its features are specifically
designed for CPU-intensive environments, such as numerical simulations. Its
performance on standard agents using common benchmarks compares favorably with
the literature.

</details>


### [45] [Neural Co-Optimization of Structural Topology, Manufacturable Layers, and Path Orientations for Fiber-Reinforced Composites](https://arxiv.org/abs/2505.03779)
*Tao Liu,Tianyu Zhang,Yongxue Chen,Weiming Wang,Yu Jiang,Yuming Huang,Charlie C. L. Wang*

Main category: cs.LG

TL;DR: 提出了一种基于神经网络的框架，用于同时优化纤维增强热塑性复合材料的结构拓扑、弯曲层和路径方向，以实现强各向异性强度并确保可制造性。


<details>
  <summary>Details</summary>
Motivation: 解决纤维增强复合材料在设计和制造过程中各向异性强度与可制造性之间的平衡问题。

Method: 使用三个隐式神经场表示几何形状、层序列和纤维方向，将设计和可制造性目标整合为可微优化过程。

Result: 实验表明，该方法生成的复合材料在破坏载荷上比顺序优化方法提高了33.1%。

Conclusion: 该框架成功实现了复合材料机械强度与可制造性的协同优化，适用于多轴3D打印。

Abstract: We propose a neural network-based computational framework for the
simultaneous optimization of structural topology, curved layers, and path
orientations to achieve strong anisotropic strength in fiber-reinforced
thermoplastic composites while ensuring manufacturability. Our framework
employs three implicit neural fields to represent geometric shape, layer
sequence, and fiber orientation. This enables the direct formulation of both
design and manufacturability objectives - such as anisotropic strength,
structural volume, machine motion control, layer curvature, and layer thickness
- into an integrated and differentiable optimization process. By incorporating
these objectives as loss functions, the framework ensures that the resultant
composites exhibit optimized mechanical strength while remaining its
manufacturability for filament-based multi-axis 3D printing across diverse
hardware platforms. Physical experiments demonstrate that the composites
generated by our co-optimization method can achieve an improvement of up to
33.1% in failure loads compared to composites with sequentially optimized
structures and manufacturing sequences.

</details>


### [46] [ALFRED: Ask a Large-language model For Reliable ECG Diagnosis](https://arxiv.org/abs/2505.03781)
*Jin Yu,JaeHo Park,TaeJun Park,Gyurin Kim,JiHyun Lee,Min Sung Lee,Joon-myoung Kwon,Jeong Min Son,Yong-Yeon Jo*

Main category: cs.LG

TL;DR: 提出了一种基于RAG的零样本ECG诊断框架，结合专家知识以提高准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 在医疗领域（如ECG分析）中，仅依赖RAG难以生成可靠、基于证据的结果，需要结合专家知识。

Method: 开发了一个基于RAG的零样本ECG诊断框架，整合专家知识。

Result: 在PTB-XL数据集上验证了框架的有效性，展示了结构化领域知识在ECG自动解读中的价值。

Conclusion: 该框架支持全面的ECG分析，具有广泛的应用潜力。

Abstract: Leveraging Large Language Models (LLMs) with Retrieval-Augmented Generation
(RAG) for analyzing medical data, particularly Electrocardiogram (ECG), offers
high accuracy and convenience. However, generating reliable, evidence-based
results in specialized fields like healthcare remains a challenge, as RAG alone
may not suffice. We propose a Zero-shot ECG diagnosis framework based on RAG
for ECG analysis that incorporates expert-curated knowledge to enhance
diagnostic accuracy and explainability. Evaluation on the PTB-XL dataset
demonstrates the framework's effectiveness, highlighting the value of
structured domain expertise in automated ECG interpretation. Our framework is
designed to support comprehensive ECG analysis, addressing diverse diagnostic
needs with potential applications beyond the tested dataset.

</details>


### [47] [A general physics-constrained method for the modelling of equation's closure terms with sparse data](https://arxiv.org/abs/2505.03783)
*Tian Chen,Shengping Liu,Li Liu,Heng Yong*

Main category: cs.LG

TL;DR: 提出了一种基于Series-Parallel Multi-Network Architecture的新方法，结合PINNs和异构数据，用于稀疏数据下的闭包建模，并集成到PDE求解器中。


<details>
  <summary>Details</summary>
Motivation: 在数据稀疏或不完整的情况下，开发广泛适用的闭包模型具有挑战性。

Method: 采用Series-Parallel Multi-Network Architecture，结合PINNs和异构数据，独立建模未知闭包项。

Result: 该方法增强了模型的泛化能力，并成功集成到PDE求解器中，适用于复杂预测模拟。

Conclusion: 提出的方法在稀疏数据下有效构建闭包模型，为工程应用提供了稳健的解决方案。

Abstract: Accurate modeling of closure terms is a critical challenge in engineering and
scientific research, particularly when data is sparse (scarse or incomplete),
making widely applicable models difficult to develop. This study proposes a
novel approach for constructing closure models in such challenging scenarios.
We introduce a Series-Parallel Multi-Network Architecture that integrates
Physics-Informed Neural Networks (PINNs) to incorporate physical constraints
and heterogeneous data from multiple initial and boundary conditions, while
employing dedicated subnetworks to independently model unknown closure terms,
enhancing generalizability across diverse problems. These closure models are
integrated into an accurate Partial Differential Equation (PDE) solver,
enabling robust solutions to complex predictive simulations in engineering
applications.

</details>


### [48] [Insulin Resistance Prediction From Wearables and Routine Blood Biomarkers](https://arxiv.org/abs/2505.03784)
*Ahmed A. Metwally,A. Ali Heydari,Daniel McDuff,Alexandru Solot,Zeinab Esmaeilpour,Anthony Z Faranesh,Menglian Zhou,David B. Savage,Conor Heneghan,Shwetak Patel,Cathy Speed,Javier L. Prieto*

Main category: cs.LG

TL;DR: 该研究利用可穿戴设备和血液生物标志物数据开发深度学习模型，预测胰岛素抵抗，效果优于单独使用任一数据源，并验证了模型在独立队列中的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有胰岛素抵抗测量方法昂贵且不易获取，限制了早期干预机会。

Method: 远程招募大规模数据集，结合可穿戴设备数据和血液生物标志物，开发深度神经网络模型预测胰岛素抵抗。

Result: 模型预测效果显著（R2=0.5，auROC=0.80），在肥胖和久坐人群中表现更优（敏感性93%，特异性95%）。

Conclusion: 该研究为早期发现2型糖尿病高风险人群提供了潜在工具，有助于实施预防策略。

Abstract: Insulin resistance, a precursor to type 2 diabetes, is characterized by
impaired insulin action in tissues. Current methods for measuring insulin
resistance, while effective, are expensive, inaccessible, not widely available
and hinder opportunities for early intervention. In this study, we remotely
recruited the largest dataset to date across the US to study insulin resistance
(N=1,165 participants, with median BMI=28 kg/m2, age=45 years, HbA1c=5.4%),
incorporating wearable device time series data and blood biomarkers, including
the ground-truth measure of insulin resistance, homeostatic model assessment
for insulin resistance (HOMA-IR). We developed deep neural network models to
predict insulin resistance based on readily available digital and blood
biomarkers. Our results show that our models can predict insulin resistance by
combining both wearable data and readily available blood biomarkers better than
either of the two data sources separately (R2=0.5, auROC=0.80, Sensitivity=76%,
and specificity 84%). The model showed 93% sensitivity and 95% adjusted
specificity in obese and sedentary participants, a subpopulation most
vulnerable to developing type 2 diabetes and who could benefit most from early
intervention. Rigorous evaluation of model performance, including
interpretability, and robustness, facilitates generalizability across larger
cohorts, which is demonstrated by reproducing the prediction performance on an
independent validation cohort (N=72 participants). Additionally, we
demonstrated how the predicted insulin resistance can be integrated into a
large language model agent to help understand and contextualize HOMA-IR values,
facilitating interpretation and safe personalized recommendations. This work
offers the potential for early detection of people at risk of type 2 diabetes
and thereby facilitate earlier implementation of preventative strategies.

</details>


### [49] [mAIstro: an open-source multi-agentic system for automated end-to-end development of radiomics and deep learning models for medical imaging](https://arxiv.org/abs/2505.03785)
*Eleftherios Tzanis,Michail E. Klontzas*

Main category: cs.LG

TL;DR: mAIstro是一个基于大型语言模型（LLM）的开源自主动多代理框架，用于医疗AI模型的端到端开发和部署，无需用户编码。


<details>
  <summary>Details</summary>
Motivation: 为医疗AI领域提供一个自动化、可扩展且无需编程的框架，以简化复杂工作流程。

Method: 采用模块化架构，支持开源和闭源LLM，通过自然语言接口协调数据分析、特征提取、分割、分类和回归任务。

Result: 在16个开源数据集上成功执行所有任务，生成可解释的输出和验证模型。

Conclusion: mAIstro是首个能够统一医疗应用中数据分析、AI模型开发和推理的代理框架，为临床和研究AI集成提供了可扩展的基础。

Abstract: Agentic systems built on large language models (LLMs) offer promising
capabilities for automating complex workflows in healthcare AI. We introduce
mAIstro, an open-source, autonomous multi-agentic framework for end-to-end
development and deployment of medical AI models. The system orchestrates
exploratory data analysis, radiomic feature extraction, image segmentation,
classification, and regression through a natural language interface, requiring
no coding from the user. Built on a modular architecture, mAIstro supports both
open- and closed-source LLMs, and was evaluated using a large and diverse set
of prompts across 16 open-source datasets, covering a wide range of imaging
modalities, anatomical regions, and data types. The agents successfully
executed all tasks, producing interpretable outputs and validated models. This
work presents the first agentic framework capable of unifying data analysis, AI
model development, and inference across varied healthcare applications,
offering a reproducible and extensible foundation for clinical and research AI
integration. The code is available at: https://github.com/eltzanis/mAIstro

</details>


### [50] [When Reasoning Beats Scale: A 1.5B Reasoning Model Outranks 13B LLMs as Discriminator](https://arxiv.org/abs/2505.03786)
*Md Fahim Anjum*

Main category: cs.LG

TL;DR: 本文研究了推理能力的大型语言模型（LLM）在规划框架中作为判别器的潜力，发现其在某些任务上优于非推理模型，但在生成任务上表现较差。


<details>
  <summary>Details</summary>
Motivation: 探索推理模型与非推理模型在文本到SQL任务中的性能差异，验证推理模型作为判别器的有效性。

Method: 使用蒸馏的1.5B参数推理模型（DeepSeek-R1）与多个非推理LLM进行对比，提出一种从链式思维（CoT）输出中提取软评分的新方法。

Result: DeepSeek-R1-1.5B在F1分数和判别准确率上显著优于CodeLlama-7B，但在生成任务上表现不如小型非推理模型。

Conclusion: 推理模型更适合作为判别器而非生成器，其性能提升受限于逻辑能力，而非计算资源或上下文长度。

Abstract: Large Language Models (LLM) with reasoning capabilities offer a promising
path for improving candidate evaluation in planning frameworks, but their
relative performance against traditional non-reasoning models remains largely
underexplored. In this study, we benchmark a distilled 1.5B parameter reasoning
model (DeepSeek-R1) against several state-of-the-art non-reasoning LLMs within
a generator-discriminator LLM planning framework for the text-to-SQL task. For
this, we introduce a novel method for extracting soft scores from the
chain-of-thought (CoT) outputs from reasoning that enables fine-grained ranking
of candidates. Our central hypothesis is that reasoning models are more
effective discriminators than non-reasoning LLMs. Our results show that
distilled DeepSeek-R1-1.5B achieves up to $87\%$ higher F1 and $3.7\%$ better
discrimination accuracy than CodeLlama-7B, as well as $3.7\%$ higher execution
accuracy than CodeLlama-13B, despite having significantly fewer parameters.
Furthermore, we find that there is a limit to the logical capabilities of
reasoning models, and only providing more context or allowing more compute
budget for reasoning is not enough to improve their discrimination performance.
Finally, we demonstrate that, unlike non-reasoning LLMs, reasoning models find
generation more challenging than discrimination and may underperform as
generators compared to smaller non-reasoning LLMs. Our work highlights the
potential of reasoning models as discriminators in agentic frameworks, far
outweighing their capabilities as generators, offering insights into their
optimal role within LLM planning infrastructures.

</details>


### [51] [ArrhythmiaVision: Resource-Conscious Deep Learning Models with Visual Explanations for ECG Arrhythmia Classification](https://arxiv.org/abs/2505.03787)
*Zuraiz Baig,Sidra Nasir,Rizwan Ahmed Khan,Muhammad Zeeshan Ul Haque*

Main category: cs.LG

TL;DR: 论文提出了两种轻量级1D卷积神经网络（ArrhythmiNet V1和V2），用于在边缘设备上高效实时分类心律失常，同时保持高准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 心律失常是危及生命的心脏事件的主要原因，现有ECG分析方法依赖人工且易出错，深度学习模型通常忽略信号的时间形态特征且计算量大。

Method: 采用基于MobileNet深度可分离卷积设计的轻量级1D卷积神经网络，结合Shapley Additive Explanations和Gradient-weighted Class Activation Mapping实现可解释性。

Result: 模型在MIT-BIH数据集上对五类心律失常的分类准确率分别为0.99（V1）和0.98（V2），内存占用低至302.18 KB和157.76 KB。

Conclusion: 研究表明，结合可解释性、预测准确性和计算效率的模型在可穿戴和嵌入式ECG监测系统中具有实际应用潜力。

Abstract: Cardiac arrhythmias are a leading cause of life-threatening cardiac events,
highlighting the urgent need for accurate and timely detection.
Electrocardiography (ECG) remains the clinical gold standard for arrhythmia
diagnosis; however, manual interpretation is time-consuming, dependent on
clinical expertise, and prone to human error. Although deep learning has
advanced automated ECG analysis, many existing models abstract away the
signal's intrinsic temporal and morphological features, lack interpretability,
and are computationally intensive-hindering their deployment on
resource-constrained platforms. In this work, we propose two novel lightweight
1D convolutional neural networks, ArrhythmiNet V1 and V2, optimized for
efficient, real-time arrhythmia classification on edge devices. Inspired by
MobileNet's depthwise separable convolutional design, these models maintain
memory footprints of just 302.18 KB and 157.76 KB, respectively, while
achieving classification accuracies of 0.99 (V1) and 0.98 (V2) on the MIT-BIH
Arrhythmia Dataset across five classes: Normal Sinus Rhythm, Left Bundle Branch
Block, Right Bundle Branch Block, Atrial Premature Contraction, and Premature
Ventricular Contraction. In order to ensure clinical transparency and
relevance, we integrate Shapley Additive Explanations and Gradient-weighted
Class Activation Mapping, enabling both local and global interpretability.
These techniques highlight physiologically meaningful patterns such as the QRS
complex and T-wave that contribute to the model's predictions. We also discuss
performance-efficiency trade-offs and address current limitations related to
dataset diversity and generalizability. Overall, our findings demonstrate the
feasibility of combining interpretability, predictive accuracy, and
computational efficiency in practical, wearable, and embedded ECG monitoring
systems.

</details>


### [52] [A new architecture of high-order deep neural networks that learn martingales](https://arxiv.org/abs/2505.03789)
*Syoiti Ninomiya,Yuming Ma*

Main category: cs.LG

TL;DR: 提出了一种基于高阶弱逼近算法的新型深度学习神经网络架构，用于高效学习鞅，并应用于金融衍生品定价问题。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习模型在金融领域（如衍生品定价）中高效学习鞅的问题。

Method: 采用基于显式Runge-Kutta类型的高阶弱逼近算法，通过迭代组合和线性组合目标SDE的向量场实现逼近。

Result: 新架构能够高效学习鞅，并在金融衍生品定价中表现出良好性能。

Conclusion: 该架构为深度学习在金融数学中的应用提供了新思路，特别是在鞅学习和衍生品定价方面。

Abstract: A new deep-learning neural network architecture based on high-order weak
approximation algorithms for stochastic differential equations (SDEs) is
proposed. The architecture enables the efficient learning of martingales by
deep learning models. The behaviour of deep neural networks based on this
architecture, when applied to the problem of pricing financial derivatives, is
also examined. The core of this new architecture lies in the high-order weak
approximation algorithms of the explicit Runge--Kutta type, wherein the
approximation is realised solely through iterative compositions and linear
combinations of vector fields of the target SDEs.

</details>


### [53] [A Time-Series Data Augmentation Model through Diffusion and Transformer Integration](https://arxiv.org/abs/2505.03790)
*Yuren Zhang,Zhongnan Pu,Lei Jing*

Main category: cs.LG

TL;DR: 提出了一种结合扩散模型和Transformer的简单有效方法，用于生成高质量的时间序列增强数据。


<details>
  <summary>Details</summary>
Motivation: 目前时间序列数据增强的研究较少，而深度学习模型需要大量数据训练，因此需要填补这一空白。

Method: 结合扩散去噪模型生成初始时间步数据，再用Transformer预测后续动作，并采用加权损失函数实现收敛。

Result: 该方法能生成高质量增强数据，显著提升模型性能，优于传统方法或无数据增强的情况。

Conclusion: 该方法为时间序列数据增强提供了一种有效解决方案。

Abstract: With the development of Artificial Intelligence, numerous real-world tasks
have been accomplished using technology integrated with deep learning. To
achieve optimal performance, deep neural networks typically require large
volumes of data for training. Although advances in data augmentation have
facilitated the acquisition of vast datasets, most of this data is concentrated
in domains like images and speech. However, there has been relatively less
focus on augmenting time-series data. To address this gap and generate a
substantial amount of time-series data, we propose a simple and effective
method that combines the Diffusion and Transformer models. By utilizing an
adjusted diffusion denoising model to generate a large volume of initial
time-step action data, followed by employing a Transformer model to predict
subsequent actions, and incorporating a weighted loss function to achieve
convergence, the method demonstrates its effectiveness. Using the performance
improvement of the model after applying augmented data as a benchmark, and
comparing the results with those obtained without data augmentation or using
traditional data augmentation methods, this approach shows its capability to
produce high-quality augmented data.

</details>


### [54] [Practical Boolean Backpropagation](https://arxiv.org/abs/2505.03791)
*Simon Golbert*

Main category: cs.LG

TL;DR: 提出了一种基于特定布尔门的纯布尔反向传播方法，无需数值计算，初步实验验证了其可行性。


<details>
  <summary>Details</summary>
Motivation: 布尔神经网络在硬件效率上优于实值模型，但纯布尔训练方法尚未充分探索。

Method: 使用特定布尔门进行纯布尔反向传播，完全基于布尔代数，不涉及数值计算。

Result: 初步实验证实了该方法的可行性。

Conclusion: 该方法为布尔神经网络的纯布尔训练提供了实用解决方案。

Abstract: Boolean neural networks offer hardware-efficient alternatives to real-valued
models. While quantization is common, purely Boolean training remains
underexplored. We present a practical method for purely Boolean backpropagation
for networks based on a single specific gate we chose, operating directly in
Boolean algebra involving no numerics. Initial experiments confirm its
feasibility.

</details>


### [55] [Towards Efficient Online Tuning of VLM Agents via Counterfactual Soft Reinforcement Learning](https://arxiv.org/abs/2505.03792)
*Lang Feng,Weihao Tan,Zhiyi Lyu,Longtao Zheng,Haiyang Xu,Ming Yan,Fei Huang,Bo An*

Main category: cs.LG

TL;DR: 论文提出了一种名为CoSo的新型在线微调方法，通过反事实推理动态评估文本动作中关键令牌的因果影响，从而提升视觉语言模型（VLM）代理在强化学习中的探索效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法在在线微调VLM代理时，由于文本动作空间的开放性和动作生成的非端到端特性，导致探索空间爆炸，难以有效探索。

Method: CoSo利用反事实推理动态评估单个令牌对后处理动作的因果影响，优先探索关键令牌，减少冗余或低影响令牌的影响。

Result: 理论分析证明CoSo具有收敛性和策略改进保证，实验结果表明其在多种任务（如Android设备控制、卡牌游戏和具身AI）中显著提升探索效率和性能。

Conclusion: CoSo通过针对性的探索策略，有效解决了VLM代理在强化学习中的探索挑战，并在多任务中展现了优越性能。

Abstract: Online fine-tuning vision-language model (VLM) agents with reinforcement
learning (RL) has shown promise for equipping agents with multi-step,
goal-oriented capabilities in dynamic environments. However, their open-ended
textual action space and non-end-to-end nature of action generation present
significant challenges to effective online exploration in RL, e.g., explosion
of the exploration space. We propose a novel online fine-tuning method,
Counterfactual Soft Reinforcement Learning (CoSo), better suited to the textual
output space of VLM agents. Compared to prior methods that assign uniform
uncertainty to all tokens, CoSo leverages counterfactual reasoning to
dynamically assess the causal influence of individual tokens on post-processed
actions. By prioritizing the exploration of action-critical tokens while
reducing the impact of semantically redundant or low-impact tokens, CoSo
enables a more targeted and efficient online rollout process. We provide
theoretical analysis proving CoSo's convergence and policy improvement
guarantees, and extensive empirical evaluations supporting CoSo's
effectiveness. Our results across a diverse set of agent tasks, including
Android device control, card gaming, and embodied AI, highlight its remarkable
ability to enhance exploration efficiency and deliver consistent performance
gains. The code is available at https://github.com/langfengQ/CoSo.

</details>


### [56] [LENSLLM: Unveiling Fine-Tuning Dynamics for LLM Selection](https://arxiv.org/abs/2505.03793)
*Xinyue Zeng,Haohui Wang,Junhong Lin,Jun Wu,Tyler Cody,Dawei Zhou*

Main category: cs.LG

TL;DR: 提出了一种新理论框架LENSLLM，用于评估大语言模型（LLM）的泛化能力，并通过Hessian-based PAC-Bayes泛化边界和NTK-based修正缩放模型，实现了高效且准确的LLM选择。


<details>
  <summary>Details</summary>
Motivation: 由于计算资源限制，无法对所有候选LLM进行微调，因此需要一种高效的方法来评估和选择LLM。

Method: 提出Hessian-based PAC-Bayes泛化边界和基于NTK的LENSLLM模型，用于预测LLM在多样化任务中的性能。

Result: 在三个大规模基准测试中，LENSLLM达到91.1%的准确率，并减少88.5%的计算成本。

Conclusion: LENSLLM在LLM选择中表现出色，优于现有方法，且已开源。

Abstract: The proliferation of open-sourced Large Language Models (LLMs) and diverse
downstream tasks necessitates efficient model selection, given the
impracticality of fine-tuning all candidates due to computational constraints.
Despite the recent advances in LLM selection, a fundamental research question
largely remains nascent: how can we model the dynamic behaviors of LLMs during
fine-tuning, thereby enhancing our understanding of their generalization
performance across diverse downstream tasks? In this work, we propose a novel
theoretical framework that provides a proper lens to assess the generalization
capabilities of LLMs, thereby enabling accurate and efficient LLM selection for
downstream applications. In particular, we first derive a Hessian-based
PAC-Bayes generalization bound that unveils fine-tuning dynamics of LLMs and
then introduce LENSLLM, a Neural Tangent Kernel(NTK)-based Rectified Scaling
Model that enables accurate performance predictions across diverse tasks while
maintaining computational efficiency. Extensive empirical results on 3
large-scale benchmarks demonstrate that our model achieves up to 91.1% accuracy
and reduces up to 88.5% computational cost in LLM selection, outperforming 5
state-of-the-art methods. We open-source our proposed LENSLLM model and
corresponding results at the Github link:
https://github.com/Susan571/LENSLLM.git.

</details>


### [57] [A Double Inertial Forward-Backward Splitting Algorithm With Applications to Regression and Classification Problems](https://arxiv.org/abs/2505.03794)
*İrfan Işik,Ibrahim Karahan,Okan Erkaymaz*

Main category: cs.LG

TL;DR: 提出了一种改进的前后向分裂算法，包含两个惯性参数，用于在实希尔伯特空间中寻找使两个算子之和为零的点。


<details>
  <summary>Details</summary>
Motivation: 解决现有算法在回归和数据分类问题中的性能不足，提升收敛性和结果质量。

Method: 改进的前后向分裂算法，引入两个惯性参数，基于标准假设实现弱收敛。

Result: 实验结果显示，该算法在回归和数据分类问题中表现优于现有算法。

Conclusion: 提出的算法在性能上优于现有相关算法，具有实际应用潜力。

Abstract: This paper presents an improved forward-backward splitting algorithm with two
inertial parameters. It aims to find a point in the real Hilbert space at which
the sum of a co-coercive operator and a maximal monotone operator vanishes.
Under standard assumptions, our proposed algorithm demonstrates weak
convergence. We present numerous experimental results to demonstrate the
behavior of the developed algorithm by comparing it with existing algorithms in
the literature for regression and data classification problems. Furthermore,
these implementations suggest our proposed algorithm yields superior outcomes
when benchmarked against other relevant algorithms in existing literature.

</details>


### [58] [Utilising Gradient-Based Proposals Within Sequential Monte Carlo Samplers for Training of Partial Bayesian Neural Networks](https://arxiv.org/abs/2505.03797)
*Andrew Millard,Joshua Murphy,Simon Maskell,Zheng Zhao*

Main category: cs.LG

TL;DR: 本文提出了一种基于SMC的新训练方法，用于部分贝叶斯神经网络（pBNNs），通过引导提议和梯度马尔可夫核提升高维问题的可扩展性，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 部分贝叶斯神经网络（pBNNs）在性能上与完全贝叶斯神经网络相当，但仅部分参数为随机。现有方法在参数估计和性能上仍有改进空间。

Method: 采用顺序蒙特卡洛（SMC）采样器作为pBNNs的推断方法，结合引导提议和梯度马尔可夫核，提升高维问题的可扩展性。

Result: 新方法在预测性能和最优损失上优于现有技术，且pBNNs在大批量训练时表现更优，显著减少训练时间。

Conclusion: 新SMC训练方法显著提升了pBNNs的性能和可扩展性，适用于高维问题和大批量训练。

Abstract: Partial Bayesian neural networks (pBNNs) have been shown to perform
competitively with fully Bayesian neural networks while only having a subset of
the parameters be stochastic. Using sequential Monte Carlo (SMC) samplers as
the inference method for pBNNs gives a non-parametric probabilistic estimation
of the stochastic parameters, and has shown improved performance over
parametric methods. In this paper we introduce a new SMC-based training method
for pBNNs by utilising a guided proposal and incorporating gradient-based
Markov kernels, which gives us better scalability on high dimensional problems.
We show that our new method outperforms the state-of-the-art in terms of
predictive performance and optimal loss. We also show that pBNNs scale well
with larger batch sizes, resulting in significantly reduced training times and
often better performance.

</details>


### [59] [Position: Foundation Models Need Digital Twin Representations](https://arxiv.org/abs/2505.03798)
*Yiqing Shen,Hao Ding,Lalithkumar Seenivasan,Tianmin Shu,Mathias Unberath*

Main category: cs.LG

TL;DR: 当前基础模型（FMs）依赖离散的token表示，限制了其对真实世界知识的理解和跨模态语义连贯性。本文提出用数字孪生（DT）表示作为替代，以解决这些挑战。


<details>
  <summary>Details</summary>
Motivation: 现有FMs通过统计相关性学习知识，缺乏显式领域知识，导致跨模态语义连贯性差、时空动态捕捉不足和因果推理困难。

Method: 提出采用数字孪生（DT）表示，作为物理过程的虚拟复制品，以提供基于物理的连续表示。

Result: 数字孪生表示能显式编码领域知识并保留真实世界过程的连续性。

Conclusion: 数字孪生表示是解决FMs当前局限性的有前景的替代方案。

Abstract: Current foundation models (FMs) rely on token representations that directly
fragment continuous real-world multimodal data into discrete tokens. They limit
FMs to learning real-world knowledge and relationships purely through
statistical correlation rather than leveraging explicit domain knowledge.
Consequently, current FMs struggle with maintaining semantic coherence across
modalities, capturing fine-grained spatial-temporal dynamics, and performing
causal reasoning. These limitations cannot be overcome by simply scaling up
model size or expanding datasets. This position paper argues that the machine
learning community should consider digital twin (DT) representations, which are
outcome-driven digital representations that serve as building blocks for
creating virtual replicas of physical processes, as an alternative to the token
representation for building FMs. Finally, we discuss how DT representations can
address these challenges by providing physically grounded representations that
explicitly encode domain knowledge and preserve the continuous nature of
real-world processes.

</details>


### [60] [Scalability Matters: Overcoming Challenges in InstructGLM with Similarity-Degree-Based Sampling](https://arxiv.org/abs/2505.03799)
*Hyun Lee,Chris Yi,Maminur Islam,B. D. S. Aritra*

Main category: cs.LG

TL;DR: 提出了一种名为SDM-InstructGLM的新型指令调优图语言模型框架，解决了大语言模型在图结构处理中的可扩展性和效率问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在图相关任务中的应用受限，主要由于可扩展性约束和缺乏专门处理图结构的机制。

Method: 引入基于相似度和度的偏置随机游走机制，选择性采样和编码图信息，优化LLM中的结构化表示。

Result: 显著提高了令牌效率，减少了信息损失，并在节点分类和链接预测等任务中表现优异。

Conclusion: 证明了无需图神经网络的LLM-only图处理可行性，为图学习开辟了新途径。

Abstract: Large Language Models (LLMs) have demonstrated strong capabilities in various
natural language processing tasks; however, their application to graph-related
problems remains limited, primarily due to scalability constraints and the
absence of dedicated mechanisms for processing graph structures. Existing
approaches predominantly integrate LLMs with Graph Neural Networks (GNNs),
using GNNs as feature encoders or auxiliary components. However, directly
encoding graph structures within LLMs has been underexplored, particularly in
the context of large-scale graphs where token limitations hinder effective
representation. To address these challenges, we propose SDM-InstructGLM, a
novel instruction-tuned Graph Language Model (InstructGLM) framework that
enhances scalability and efficiency without relying on GNNs. Our method
introduces a similarity-degree-based biased random walk mechanism, which
selectively samples and encodes graph information based on node-feature
similarity and degree centrality, ensuring an adaptive and structured
representation within the LLM. This approach significantly improves token
efficiency, mitigates information loss due to random sampling, and enhances
performance on graph-based tasks such as node classification and link
prediction. Furthermore, our results demonstrate the feasibility of LLM-only
graph processing, enabling scalable and interpretable Graph Language Models
(GLMs) optimized through instruction-based fine-tuning. This work paves the way
for GNN-free approaches to graph learning, leveraging LLMs as standalone graph
reasoning models. Our source code is available on GitHub.

</details>


### [61] [Grouped Sequency-arranged Rotation: Optimizing Rotation Transformation for Quantization for Free](https://arxiv.org/abs/2505.03810)
*Euntae Choi,Sumin Song,Woosang Lim,Sungjoo Yoo*

Main category: cs.LG

TL;DR: 提出了一种无需训练的新方法，通过改进旋转矩阵解决低比特量化问题，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在低比特量化（如2-bit）时性能下降的问题。

Method: 利用Walsh-Hadamard变换和分组序列排列旋转（GSR）构建改进的旋转矩阵，减少量化误差。

Result: 在推理任务和WikiText-2的Perplexity评分上表现优异，性能接近基于优化的方法。

Conclusion: 该方法无需训练即可显著提升低比特量化性能，且适用于现有学习旋转技术。

Abstract: Large Language Models (LLMs) face deployment challenges due to high
computational costs, and while Post-Training Quantization (PTQ) offers a
solution, existing rotation-based methods struggle at very low bit-widths like
2-bit. We introduce a novel, training-free approach to construct an improved
rotation matrix, addressing the limitations of current methods. The key
contributions include leveraging the Walsh-Hadamard transform with sequency
ordering, which clusters similar frequency components to reduce quantization
error compared to standard Hadamard matrices, significantly improving
performance. Furthermore, we propose a Grouped Sequency-arranged Rotation (GSR)
using block-diagonal matrices with smaller Walsh blocks, effectively isolating
outlier impacts and achieving performance comparable to optimization-based
methods without requiring any training. Our method demonstrates robust
performance on reasoning tasks and Perplexity (PPL) score on WikiText-2. Our
method also enhances results even when applied over existing learned rotation
techniques.

</details>


### [62] [Large Language Model Compression with Global Rank and Sparsity Optimization](https://arxiv.org/abs/2505.03801)
*Changhai Zhou,Qian Qiao,Weizhong Zhang,Cheng Jin*

Main category: cs.LG

TL;DR: 提出一种两阶段LLM压缩方法，通过全局秩和稀疏优化解决低秩与稀疏矩阵交互及权重分配问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在压缩大型语言模型时面临低秩与稀疏矩阵交互及权重分配的挑战。

Method: 第一阶段用鲁棒主成分分析分解权重矩阵，第二阶段用概率全局优化技术联合识别低秩与稀疏结构。

Result: 实验结果表明，该方法在稀疏化和复合逼近方面显著优于现有技术。

Conclusion: 该方法能自动检测层间冗余并管理稀疏与低秩组件的交互，性能优越。

Abstract: Low-rank and sparse composite approximation is a natural idea to compress
Large Language Models (LLMs). However, such an idea faces two primary
challenges that adversely affect the performance of existing methods. The first
challenge relates to the interaction and cooperation between low-rank and
sparse matrices, while the second involves determining weight allocation across
different layers, as redundancy varies considerably among them. To address
these challenges, we propose a novel two-stage LLM compression method with the
capability of global rank and sparsity optimization. It is noteworthy that the
overall optimization space is vast, making comprehensive optimization
computationally prohibitive. Therefore, to reduce the optimization space, our
first stage utilizes robust principal component analysis to decompose the
weight matrices of LLMs into low-rank and sparse components, which span the low
dimensional and sparse spaces containing the resultant low-rank and sparse
matrices, respectively. In the second stage, we propose a probabilistic global
optimization technique to jointly identify the low-rank and sparse structures
within the above two spaces. The appealing feature of our approach is its
ability to automatically detect the redundancy across different layers and to
manage the interaction between the sparse and low-rank components. Extensive
experimental results indicate that our method significantly surpasses
state-of-the-art techniques for sparsification and composite approximation.

</details>


### [63] [Efficient Fine-Tuning of Quantized Models via Adaptive Rank and Bitwidth](https://arxiv.org/abs/2505.03802)
*Changhai Zhou,Yuhua Zhou,Qian Qiao,Weizhong Zhang,Cheng Jin*

Main category: cs.LG

TL;DR: QLoRA结合低比特量化和LoRA实现内存友好的大语言模型微调。QR-Adaptor提出一种梯度自由策略，联合优化量化组件和低秩空间秩分配，提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法未能一致提升性能，且未考虑量化与低秩子空间的协同作用。

Method: 提出QR-Adaptor，通过部分校准数据联合搜索量化组件和低秩空间秩分配，以离散优化问题处理。

Result: 在GSM8K上准确率提升4.89%，部分情况下优于16位微调模型，同时保持4位内存占用。

Conclusion: QR-Adaptor通过联合优化量化与低秩空间，显著提升性能且保持内存效率。

Abstract: QLoRA effectively combines low-bit quantization and LoRA to achieve
memory-friendly fine-tuning for large language models (LLM). Recently, methods
based on SVD for continuous update iterations to initialize LoRA matrices to
accommodate quantization errors have generally failed to consistently improve
performance. Dynamic mixed precision is a natural idea for continuously
improving the fine-tuning performance of quantized models, but previous methods
often optimize low-rank subspaces or quantization components separately,
without considering their synergy. To address this, we propose
\textbf{QR-Adaptor}, a unified, gradient-free strategy that uses partial
calibration data to jointly search the quantization components and the rank of
low-rank spaces for each layer, thereby continuously improving model
performance. QR-Adaptor does not minimize quantization error but treats
precision and rank allocation as a discrete optimization problem guided by
actual downstream performance and memory usage. Compared to state-of-the-art
(SOTA) quantized LoRA fine-tuning methods, our approach achieves a 4.89\%
accuracy improvement on GSM8K, and in some cases even outperforms the 16-bit
fine-tuned model while maintaining the memory footprint of the 4-bit setting.

</details>


### [64] [RWKVQuant: Quantizing the RWKV Family with Proxy Guided Hybrid of Scalar and Vector Quantization](https://arxiv.org/abs/2505.03803)
*Chen Xu,Yuxuan Yue,Zukang Xu,Xing Hu,Jiangyong Yu,Zhixuan Chen,Sifan Zhou,Zhihang Yuan,Dawei Yang*

Main category: cs.LG

TL;DR: RWKVQuant是一种专为RWKV模型设计的后训练量化框架，通过自适应选择量化方法和优化码书算法，解决了RWKV量化中的性能下降问题，实现了3位量化且精度损失小于1%。


<details>
  <summary>Details</summary>
Motivation: RWKV作为一种现代RNN架构，在资源受限设备上部署时面临量化性能下降的挑战，需要针对其特性设计专门的量化方法。

Method: 提出RWKVQuant框架，包含两种技术：(1) 粗到细的代理方法，自适应选择量化策略；(2) 码书优化算法，提升逐元素乘法的量化性能。

Result: 实验表明，RWKVQuant可将RWKV-6-14B量化为约3位，精度损失小于1%，速度提升2.14倍。

Conclusion: RWKVQuant为RWKV模型提供了一种高效的量化解决方案，显著提升了其在资源受限设备上的部署能力。

Abstract: RWKV is a modern RNN architecture with comparable performance to Transformer,
but still faces challenges when deployed to resource-constrained devices. Post
Training Quantization (PTQ), which is a an essential technique to reduce model
size and inference latency, has been widely used in Transformer models.
However, it suffers significant degradation of performance when applied to
RWKV. This paper investigates and identifies two key constraints inherent in
the properties of RWKV: (1) Non-linear operators hinder the parameter-fusion of
both smooth- and rotation-based quantization, introducing extra computation
overhead. (2) The larger amount of uniformly distributed weights poses
challenges for cluster-based quantization, leading to reduced accuracy. To this
end, we propose RWKVQuant, a PTQ framework tailored for RWKV models, consisting
of two novel techniques: (1) a coarse-to-fine proxy capable of adaptively
selecting different quantization approaches by assessing the uniformity and
identifying outliers in the weights, and (2) a codebook optimization algorithm
that enhances the performance of cluster-based quantization methods for
element-wise multiplication in RWKV. Experiments show that RWKVQuant can
quantize RWKV-6-14B into about 3-bit with less than 1% accuracy loss and 2.14x
speed up.

</details>


### [65] [MoEQuant: Enhancing Quantization for Mixture-of-Experts Large Language Models via Expert-Balanced Sampling and Affinity Guidance](https://arxiv.org/abs/2505.03804)
*Xing Hu,Zhixuan Chen,Dawei Yang,Zukang Xu,Chen Xu,Zhihang Yuan,Sifan Zhou,Jiangyong Yu*

Main category: cs.LG

TL;DR: MoEQuant提出了一种针对MoE LLMs的量化框架，解决了量化中的专家间和专家内不平衡问题，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: MoE LLMs因稀疏和动态特性导致量化时精度下降，限制了实际部署。

Method: 提出MoEQuant框架，包括专家平衡自采样（EBSS）和亲和力引导量化（AGQ）两种技术。

Result: 实验显示MoEQuant在4位量化下显著提升性能（如HumanEval中DeepSeekMoE-16B准确率提升超10分）。

Conclusion: MoEQuant有效解决了MoE模型量化中的挑战，提升了效率和性能。

Abstract: Mixture-of-Experts (MoE) large language models (LLMs), which leverage dynamic
routing and sparse activation to enhance efficiency and scalability, have
achieved higher performance while reducing computational costs. However, these
models face significant memory overheads, limiting their practical deployment
and broader adoption. Post-training quantization (PTQ), a widely used method
for compressing LLMs, encounters severe accuracy degradation and diminished
generalization performance when applied to MoE models. This paper investigates
the impact of MoE's sparse and dynamic characteristics on quantization and
identifies two primary challenges: (1) Inter-expert imbalance, referring to the
uneven distribution of samples across experts, which leads to insufficient and
biased calibration for less frequently utilized experts; (2) Intra-expert
imbalance, arising from MoE's unique aggregation mechanism, which leads to
varying degrees of correlation between different samples and their assigned
experts. To address these challenges, we propose MoEQuant, a novel quantization
framework tailored for MoE LLMs. MoE-Quant includes two novel techniques: 1)
Expert-Balanced Self-Sampling (EBSS) is an efficient sampling method that
efficiently constructs a calibration set with balanced expert distributions by
leveraging the cumulative probabilities of tokens and expert balance metrics as
guiding factors. 2) Affinity-Guided Quantization (AGQ), which incorporates
affinities between experts and samples into the quantization process, thereby
accurately assessing the impact of individual samples on different experts
within the MoE layer. Experiments demonstrate that MoEQuant achieves
substantial performance gains (more than 10 points accuracy gain in the
HumanEval for DeepSeekMoE-16B under 4-bit quantization) and boosts efficiency.

</details>


### [66] [Quiet Feature Learning in Algorithmic Tasks](https://arxiv.org/abs/2505.03997)
*Prudhviraj Naidu,Zixian Wang,Leon Bergen,Ramamohan Paturi*

Main category: cs.LG

TL;DR: Transformer语言模型在算法任务训练中表现出非连续的损失曲线变化，揭示了特征学习的阶段性。


<details>
  <summary>Details</summary>
Motivation: 挑战传统认为的损失函数连续下降假设，探索模型内部特征学习的动态过程。

Method: 在十个基础算法任务上训练Transformer模型，分析损失曲线和内部特征变化。

Result: 发现损失曲线呈现明显的阶段性变化，内部特征学习分为静默和显著两个阶段。

Conclusion: 模型性能的提升可能是由内部特征的突然整合触发，而非连续的损失下降。

Abstract: We train Transformer-based language models on ten foundational algorithmic
tasks and observe pronounced phase transitions in their loss curves that
deviate from established power-law scaling trends. Over large ranges of
compute, the validation loss barely improves, then abruptly decreases. Probing
the models' internal representations reveals the learning of quiet features
during the stagnant phase, followed by sudden acquisition of loud features that
coincide with the sharp drop in loss. Our ablation experiments show that
disrupting a single learned feature can dramatically degrade performance,
providing evidence of their causal role in task performance. These findings
challenge the prevailing assumption that next-token predictive loss reliably
tracks incremental progress; instead, key internal features may be developing
below the surface until they coalesce, triggering a rapid performance gain.

</details>


### [67] [Feature Optimization for Time Series Forecasting via Novel Randomized Uphill Climbing](https://arxiv.org/abs/2505.03805)
*Nguyen Van Thanh*

Main category: cs.LG

TL;DR: 论文提出了一种基于随机上坡爬升（RUC）的通用特征优化框架，用于多元时间序列预测，旨在提高效率、降低能耗并增强可解释性。


<details>
  <summary>Details</summary>
Motivation: 通过将特征发现与GPU密集型深度学习解耦，该方法能够实现更快的迭代周期、更低的能耗和更高的透明度，适用于资源受限的机构。

Method: 通过随机组合领域特定语法中的操作符生成候选特征程序，利用廉价代理模型在滚动窗口上快速评分，并通过嵌套交叉验证和信息论收缩过滤不稳定性。

Result: 该方法有望为多元时间序列预测提供高效、透明且可解释的工具。

Conclusion: 该框架具有广泛的社会意义，能够帮助资源受限的机构和组织做出数据驱动的决策，而无需依赖专有的黑盒模型。

Abstract: Randomized Uphill Climbing is a lightweight, stochastic search heuristic that
has delivered state of the art equity alpha factors for quantitative hedge
funds. I propose to generalize RUC into a model agnostic feature optimization
framework for multivariate time series forecasting. The core idea is to
synthesize candidate feature programs by randomly composing operators from a
domain specific grammar, score candidates rapidly with inexpensive surrogate
models on rolling windows, and filter instability via nested cross validation
and information theoretic shrinkage. By decoupling feature discovery from GPU
heavy deep learning, the method promises faster iteration cycles, lower energy
consumption, and greater interpretability. Societal relevance: accurate,
transparent forecasting tools empower resource constrained institutions, energy
regulators, climate risk NGOs to make data driven decisions without proprietary
black box models.

</details>


### [68] [LLAMAPIE: Proactive In-Ear Conversation Assistants](https://arxiv.org/abs/2505.04066)
*Tuochao Chen,Nicholas Batchelder,Alisa Liu,Noah Smith,Shyamnath Gollakota*

Main category: cs.LG

TL;DR: LlamaPIE是一个实时主动助手，通过可听设备提供简洁指导，无需用户显式调用，提升对话体验。


<details>
  <summary>Details</summary>
Motivation: 传统语言模型需要用户显式调用，而LlamaPIE旨在通过背景运行和主动预测用户需求，提供无干扰的对话辅助。

Method: 构建半合成对话数据集，采用双模型管道：小型模型决定何时响应，大型模型生成响应。

Result: 在真实数据集上验证有效，用户研究表明用户更倾向于主动助手而非无辅助或被动模型。

Conclusion: LlamaPIE展示了在实时对话中提供无干扰辅助的潜力。

Abstract: We introduce LlamaPIE, the first real-time proactive assistant designed to
enhance human conversations through discreet, concise guidance delivered via
hearable devices. Unlike traditional language models that require explicit user
invocation, this assistant operates in the background, anticipating user needs
without interrupting conversations. We address several challenges, including
determining when to respond, crafting concise responses that enhance
conversations, leveraging knowledge of the user for context-aware assistance,
and real-time, on-device processing. To achieve this, we construct a
semi-synthetic dialogue dataset and propose a two-model pipeline: a small model
that decides when to respond and a larger model that generates the response. We
evaluate our approach on real-world datasets, demonstrating its effectiveness
in providing helpful, unobtrusive assistance. User studies with our assistant,
implemented on Apple Silicon M2 hardware, show a strong preference for the
proactive assistant over both a baseline with no assistance and a reactive
model, highlighting the potential of LlamaPie to enhance live conversations.

</details>


### [69] [Perception-Informed Neural Networks: Beyond Physics-Informed Neural Networks](https://arxiv.org/abs/2505.03806)
*Mehran Mazandarani,Marzieh Najariyan*

Main category: cs.LG

TL;DR: PrINNs是一种将感知信息融入神经网络的框架，扩展了PINNs，支持多种感知形式，如概率分布和模糊图，并引入MOEINNs、TKINNs和FINNs等新方法。


<details>
  <summary>Details</summary>
Motivation: 解决传统物理建模与现代数据驱动方法之间的鸿沟，使神经网络能结合物理定律和感知规则，适应不确定环境。

Method: 通过损失函数整合专家知识和感知信息，提出MOEINNs、TKINNs和FINNs等子框架，支持模糊逻辑和在线训练。

Result: PrINNs能建模复杂系统、发现新微分方程，提升模型在不确定环境中的性能。

Conclusion: PrINNs为计算科学与工程提供了强大工具，结合物理与感知信息，推动数据驱动建模的发展。

Abstract: This article introduces Perception-Informed Neural Networks (PrINNs), a
framework designed to incorporate perception-based information into neural
networks, addressing both systems with known and unknown physics laws or
differential equations. Moreover, PrINNs extend the concept of Physics-Informed
Neural Networks (PINNs) and their variants, offering a platform for the
integration of diverse forms of perception precisiation, including singular,
probability distribution, possibility distribution, interval, and fuzzy graph.
In fact, PrINNs allow neural networks to model dynamical systems by integrating
expert knowledge and perception-based information through loss functions,
enabling the creation of modern data-driven models. Some of the key
contributions include Mixture of Experts Informed Neural Networks (MOEINNs),
which combine heterogeneous expert knowledge into the network, and
Transformed-Knowledge Informed Neural Networks (TKINNs), which facilitate the
incorporation of meta-information for enhanced model performance. Additionally,
Fuzzy-Informed Neural Networks (FINNs) as a modern class of fuzzy deep neural
networks leverage fuzzy logic constraints within a deep learning architecture,
allowing online training without pre-training and eliminating the need for
defuzzification. PrINNs represent a significant step forward in bridging the
gap between traditional physics-based modeling and modern data-driven
approaches, enabling neural networks to learn from both structured physics laws
and flexible perception-based rules. This approach empowers neural networks to
operate in uncertain environments, model complex systems, and discover new
forms of differential equations, making PrINNs a powerful tool for advancing
computational science and engineering.

</details>


### [70] [AI-driven multi-source data fusion for algal bloom severity classification in small inland water bodies: Leveraging Sentinel-2, DEM, and NOAA climate data](https://arxiv.org/abs/2505.03808)
*Ioannis Nasios*

Main category: cs.LG

TL;DR: 该研究提出了一种结合多源开源遥感数据和人工智能模型的高效方法，用于检测有害藻华。


<details>
  <summary>Details</summary>
Motivation: 有害藻华对内陆水质和公共健康构成威胁，亟需高效、准确且经济的检测方法。

Method: 整合Sentinel-2光学影像、DEM高程数据和NOAA气候数据，结合树模型和神经网络进行藻华严重程度分类。

Result: 树模型表现优异，加入神经网络增强了鲁棒性，展示了深度学习模型对多样化遥感数据的利用能力。

Conclusion: 该方法具有全球应用潜力，代码开源，展示了遥感数据与AI结合解决环境问题的潜力。

Abstract: Harmful algal blooms are a growing threat to inland water quality and public
health worldwide, creating an urgent need for efficient, accurate, and
cost-effective detection methods. This research introduces a high-performing
methodology that integrates multiple open-source remote sensing data with
advanced artificial intelligence models. Key data sources include Copernicus
Sentinel-2 optical imagery, the Copernicus Digital Elevation Model (DEM), and
NOAA's High-Resolution Rapid Refresh (HRRR) climate data, all efficiently
retrieved using platforms like Google Earth Engine (GEE) and Microsoft
Planetary Computer (MPC). The NIR and two SWIR bands from Sentinel-2, the
altitude from the elevation model, the temperature and wind from NOAA as well
as the longitude and latitude were the most important features. The approach
combines two types of machine learning models, tree-based models and a neural
network, into an ensemble for classifying algal bloom severity. While the tree
models performed strongly on their own, incorporating a neural network added
robustness and demonstrated how deep learning models can effectively use
diverse remote sensing inputs. The method leverages high-resolution satellite
imagery and AI-driven analysis to monitor algal blooms dynamically, and
although initially developed for a NASA competition in the U.S., it shows
potential for global application. The complete code is available for further
adaptation and practical implementation, illustrating the convergence of remote
sensing data and AI to address critical environmental challenges
(https://github.com/IoannisNasios/HarmfulAlgalBloomDetection).

</details>


### [71] [When Dynamic Data Selection Meets Data Augmentation](https://arxiv.org/abs/2505.03809)
*Suorong Yang,Peng Ye,Furao Shen,Dongzhan Zhou*

Main category: cs.LG

TL;DR: 提出了一种动态数据选择与增强联合优化的在线训练框架，显著提升训练效率与性能。


<details>
  <summary>Details</summary>
Motivation: 动态数据选择虽能加速训练，但可能限制数据多样性，影响泛化能力；数据增强虽提升多样性，但未与选择优化结合。

Method: 联合估计样本的局部密度和多模态语义一致性分布，针对性选择适合增强的样本，避免噪声或模糊数据。

Result: 在多个基准数据集和架构上优于现有方法，如ImageNet-1k上减少50%训练成本且性能无损。

Conclusion: 该框架不仅提升训练效率与性能，还增强了噪声抵抗力和模型鲁棒性，具有实际应用价值。

Abstract: Dynamic data selection aims to accelerate training with lossless performance.
However, reducing training data inherently limits data diversity, potentially
hindering generalization. While data augmentation is widely used to enhance
diversity, it is typically not optimized in conjunction with selection. As a
result, directly combining these techniques fails to fully exploit their
synergies. To tackle the challenge, we propose a novel online data training
framework that, for the first time, unifies dynamic data selection and
augmentation, achieving both training efficiency and enhanced performance. Our
method estimates each sample's joint distribution of local density and
multimodal semantic consistency, allowing for the targeted selection of
augmentation-suitable samples while suppressing the inclusion of noisy or
ambiguous data. This enables a more significant reduction in dataset size
without sacrificing model generalization. Experimental results demonstrate that
our method outperforms existing state-of-the-art approaches on various
benchmark datasets and architectures, e.g., reducing 50\% training costs on
ImageNet-1k with lossless performance. Furthermore, our approach enhances noise
resistance and improves model robustness, reinforcing its practical utility in
real-world scenarios.

</details>


### [72] [ScarceGAN: Discriminative Classification Framework for Rare Class Identification for Longitudinal Data with Weak Prior](https://arxiv.org/abs/2505.03811)
*Surajit Chakrabarty,Rukma Talwadker,Tridib Mukherjee*

Main category: cs.LG

TL;DR: ScarceGAN是一种用于从多维纵向遥测数据中识别极稀有样本的方法，特别针对正类样本稀缺、负类样本多类分布不均及大量未标记数据的问题。


<details>
  <summary>Details</summary>
Motivation: 解决正类样本稀缺、负类样本多类分布不均以及未标记数据带来的挑战，提升稀有样本的识别性能。

Method: 重新设计半监督GAN，引入弱标记多类负样本和正样本，通过松弛判别器的约束和改进目标函数来优化模型。

Result: 在技能游戏中识别高风险玩家时，召回率超过85%（比传统半监督GAN提升60%），并在KDDCUP99入侵数据集的稀有攻击类（0.09%）识别中创下新基准。

Conclusion: ScarceGAN通过改进半监督GAN框架，显著提升了稀有样本的识别性能，适用于多种实际场景。

Abstract: This paper introduces ScarceGAN which focuses on identification of extremely
rare or scarce samples from multi-dimensional longitudinal telemetry data with
small and weak label prior. We specifically address: (i) severe scarcity in
positive class, stemming from both underlying organic skew in the data, as well
as extremely limited labels; (ii) multi-class nature of the negative samples,
with uneven density distributions and partially overlapping feature
distributions; and (iii) massively unlabelled data leading to tiny and weak
prior on both positive and negative classes, and possibility of unseen or
unknown behavior in the unlabelled set, especially in the negative class.
Although related to PU learning problems, we contend that knowledge (or lack of
it) on the negative class can be leveraged to learn the compliment of it (i.e.,
the positive class) better in a semi-supervised manner. To this effect,
ScarceGAN re-formulates semi-supervised GAN by accommodating weakly labelled
multi-class negative samples and the available positive samples. It relaxes the
supervised discriminator's constraint on exact differentiation between negative
samples by introducing a 'leeway' term for samples with noisy prior. We propose
modifications to the cost objectives of discriminator, in supervised and
unsupervised path as well as that of the generator. For identifying risky
players in skill gaming, this formulation in whole gives us a recall of over
85% (~60% jump over vanilla semi-supervised GAN) on our scarce class with very
minimal verbosity in the unknown space. Further ScarceGAN outperforms the
recall benchmarks established by recent GAN based specialized models for the
positive imbalanced class identification and establishes a new benchmark in
identifying one of rare attack classes (0.09%) in the intrusion dataset from
the KDDCUP99 challenge.

</details>


### [73] [Information Filtering Networks: Theoretical Foundations, Generative Methodologies, and Real-World Applications](https://arxiv.org/abs/2505.03812)
*Tomaso Aste*

Main category: cs.LG

TL;DR: 信息过滤网络（IFNs）是一种通过全局稀疏但局部密集且可解释的结构建模复杂系统的框架。本文综述了IFNs的理论基础、构建方法及其广泛应用，并探讨了其在图形建模和深度学习中的潜力。


<details>
  <summary>Details</summary>
Motivation: IFNs旨在解决高维数据建模中的关键挑战，提供更高效、可解释的建模方法，并推动网络理论与现代数据驱动范式的结合。

Method: IFNs通过构建三角化最大过滤图（TMFG）和最大过滤团森林（MFCF）等方法生成高阶网络（如单纯复形），优于传统方法如Graphical LASSO。

Result: IFNs在金融、生物、心理和人工智能等领域提升了模型的解释性、计算效率和预测性能。

Conclusion: IFNs不仅连接了经典网络理论与现代数据科学，还为深度学习模型的架构设计提供了新思路。

Abstract: Information Filtering Networks (IFNs) provide a powerful framework for
modeling complex systems through globally sparse yet locally dense and
interpretable structures that capture multivariate dependencies. This review
offers a comprehensive account of IFNs, covering their theoretical foundations,
construction methodologies, and diverse applications. Tracing their origins
from early network-based models to advanced formulations such as the
Triangulated Maximally Filtered Graph (TMFG) and the Maximally Filtered Clique
Forest (MFCF), the paper highlights how IFNs address key challenges in
high-dimensional data-driven modeling. IFNs and their construction
methodologies are intrinsically higher-order networks that generate simplicial
complexes-structures that are only now becoming popular in the broader
literature. Applications span fields including finance, biology, psychology,
and artificial intelligence, where IFNs improve interpretability, computational
efficiency, and predictive performance. Special attention is given to their
role in graphical modeling, where IFNs enable the estimation of sparse inverse
covariance matrices with greater accuracy and scalability than traditional
approaches like Graphical LASSO. Finally, the review discusses recent
developments that integrate IFNs with machine learning and deep learning,
underscoring their potential not only to bridge classical network theory with
contemporary data-driven paradigms, but also to shape the architectures of deep
learning models themselves.

</details>


### [74] [Program Semantic Inequivalence Game with Large Language Models](https://arxiv.org/abs/2505.03818)
*Antonio Valerio Miceli-Barone,Vaishak Belle,Ali Payani*

Main category: cs.LG

TL;DR: 论文提出了一种基于语义不等价游戏SInQ的方法，通过生成器和评估器代理半对抗训练，生成代码推理训练数据，提升LLM在复杂编程任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在需要非平凡程序语义推理的复杂任务中表现不佳的问题，并克服训练数据获取的挑战。

Method: 利用SInQ游戏，生成器创建语义不同的程序变体，评估器识别导致行为差异的输入，两者通过半对抗训练相互提升。

Result: 在跨语言漏洞检测和Python内置标识符交换基准测试中表现优异，显著提升LLM性能。

Conclusion: SInQ方法通过自对抗训练生成高质量训练数据，为LLM在复杂编程任务中的应用提供了有效解决方案。

Abstract: Large Language Models (LLMs) can achieve strong performance on everyday
coding tasks, but they can fail on complex tasks that require non-trivial
reasoning about program semantics. Finding training examples to teach LLMs to
solve these tasks can be challenging.
  In this work, we explore a method to synthetically generate code reasoning
training data based on a semantic inequivalence game SInQ: a generator agent
creates program variants that are semantically distinct, derived from a dataset
of real-world programming tasks, while an evaluator agent has to identify input
examples that cause the original programs and the generated variants to diverge
in their behaviour, with the agents training each other semi-adversarially. We
prove that this setup enables theoretically unlimited improvement through
self-play in the limit of infinite computational resources.
  We evaluated our approach on multiple code generation and understanding
benchmarks, including cross-language vulnerability detection (Lu et al., 2021),
where our method improves vulnerability detection in C/C++ code despite being
trained exclusively on Python code, and the challenging Python builtin
identifier swap benchmark (Miceli-Barone et al., 2023), showing that whereas
modern LLMs still struggle with this benchmark, our approach yields substantial
improvements.
  We release the code needed to replicate the experiments, as well as the
generated synthetic data, which can be used to fine-tune LLMs.

</details>


### [75] [Focus on the Likely: Test-time Instance-based Uncertainty Removal](https://arxiv.org/abs/2505.03819)
*Johannes Schneider*

Main category: cs.LG

TL;DR: 提出两种无需辅助数据的测试时微调方法，通过单步梯度下降优化高不确定性预测，提升模型准确性。


<details>
  <summary>Details</summary>
Motivation: 针对模型预测中高不确定性的问题，提出无需额外数据的方法，直接利用测试实例优化预测结果。

Method: 在推理阶段引入对可能类别的额外关注步骤，通过单步梯度下降细化高不确定性预测。

Result: 实验表明，该方法在文本和图像领域的高不确定性样本上显著提升了准确性。

Conclusion: 通过理论分析和实验验证，该方法有效优化了预测结果，尤其在高不确定性情况下表现优异。

Abstract: We propose two novel test-time fine-tuning methods to improve uncertain model
predictions. Our methods require no auxiliary data and use the given test
instance only. Instead of performing a greedy selection of the most likely
class to make a prediction, we introduce an additional focus on the likely
classes step during inference. By applying a single-step gradient descent, we
refine predictions when an initial forward pass indicates high uncertainty.
This aligns predictions more closely with the ideal of assigning zero
probability to less plausible outcomes. Our theoretical discussion provides a
deeper understanding highlighting the impact on shared and non-shared features
among (focus) classes. The experimental evaluation highlights accuracy gains on
samples exhibiting high decision uncertainty for a diverse set of models from
both the text and image domain using the same hyperparameters.

</details>


### [76] [DRSLF: Double Regularized Second-Order Low-Rank Representation for Web Service QoS Prediction](https://arxiv.org/abs/2505.03822)
*Hao Wu,Jialiang Wang*

Main category: cs.LG

TL;DR: 本文提出了一种双正则化二阶潜在因子（DRSLF）模型，通过结合L1和L2正则化以及二阶信息，提高了QoS预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的一阶优化器和L2正则化方法在QoS预测中表现不佳，需要更高效的模型。

Method: DRSLF模型结合了L1和L2正则化，并在共轭梯度步骤中计算Hessian-向量积以引入二阶信息。

Result: 在两个真实世界的QoS数据集上，DRSLF表现出比基线模型更强的低秩表示能力。

Conclusion: DRSLF模型通过双正则化和二阶信息显著提升了QoS预测的准确性。

Abstract: Quality-of-Service (QoS) data plays a crucial role in cloud service
selection. Since users cannot access all services, QoS can be represented by a
high-dimensional and incomplete (HDI) matrix. Latent factor analysis (LFA)
models have been proven effective as low-rank representation techniques for
addressing this issue. However, most LFA models rely on first-order optimizers
and use L2-norm regularization, which can lead to lower QoS prediction
accuracy. To address this issue, this paper proposes a double regularized
second-order latent factor (DRSLF) model with two key ideas: a) integrating
L1-norm and L2-norm regularization terms to enhance the low-rank representation
performance; b) incorporating second-order information by calculating the
Hessian-vector product in each conjugate gradient step. Experimental results on
two real-world response-time QoS datasets demonstrate that DRSLF has a higher
low-rank representation capability than two baselines.

</details>


### [77] [Intelligently Augmented Contrastive Tensor Factorization: Empowering Multi-dimensional Time Series Classification in Low-Data Environments](https://arxiv.org/abs/2505.03825)
*Anushiya Arunan,Yan Qin,Xiaoli Li,Yuen Chau*

Main category: cs.LG

TL;DR: 提出了一种名为ITA-CTF的数据高效框架，用于从多维时间序列中学习有效表示，解决了低训练数据环境下的分类问题。


<details>
  <summary>Details</summary>
Motivation: 解决多维时间序列分类中因数据不足导致的标准深度学习模型过拟合问题，同时学习跨维度依赖和类内变化等复杂特征。

Method: 结合对比性张量分解（CTF）和智能增强（ITA）模块，CTF学习时间序列的核心成分及其联合依赖，ITA生成目标性增强以突出类内模式。

Result: 在五个分类任务中，相比标准张量分解和深度学习基准，性能提升高达18.7%。

Conclusion: ITA-CTF框架在低数据环境下显著提升了分类性能，通过学习类内变化和不变性特征实现了高效表示学习。

Abstract: Classification of multi-dimensional time series from real-world systems
require fine-grained learning of complex features such as cross-dimensional
dependencies and intra-class variations-all under the practical challenge of
low training data availability. However, standard deep learning (DL) struggles
to learn generalizable features in low-data environments due to model
overfitting. We propose a versatile yet data-efficient framework, Intelligently
Augmented Contrastive Tensor Factorization (ITA-CTF), to learn effective
representations from multi-dimensional time series. The CTF module learns core
explanatory components of the time series (e.g., sensor factors, temporal
factors), and importantly, their joint dependencies. Notably, unlike standard
tensor factorization (TF), the CTF module incorporates a new contrastive loss
optimization to induce similarity learning and class-awareness into the learnt
representations for better classification performance. To strengthen this
contrastive learning, the preceding ITA module generates targeted but
informative augmentations that highlight realistic intra-class patterns in the
original data, while preserving class-wise properties. This is achieved by
dynamically sampling a "soft" class prototype to guide the warping of each
query data sample, which results in an augmentation that is intelligently
pattern-mixed between the "soft" class prototype and the query sample. These
augmentations enable the CTF module to recognize complex intra-class variations
despite the limited original training data, and seek out invariant class-wise
properties for accurate classification performance. The proposed method is
comprehensively evaluated on five different classification tasks. Compared to
standard TF and several DL benchmarks, notable performance improvements up to
18.7% were achieved.

</details>


### [78] [MISE: Meta-knowledge Inheritance for Social Media-Based Stressor Estimation](https://arxiv.org/abs/2505.03827)
*Xin Wang,Ling Feng,Huijun Zhang,Lei Cao,Kaisheng Zeng,Qi Li,Yang Ding,Yi Dai,David Clifton*

Main category: cs.LG

TL;DR: 该论文提出了一种基于元学习的压力源估计框架，通过社交媒体帖子识别具体压力源，解决了数据稀疏和新压力源不断出现的问题。


<details>
  <summary>Details</summary>
Motivation: 现代社会中压力问题严重，社交媒体成为压力检测的新途径。现有研究多关注压力状态分类，而本文旨在识别更具体的压力源（如考试、写论文等）。

Method: 采用元学习框架，结合元知识继承机制，防止模型在适应新压力源时出现灾难性遗忘。

Result: 实验表明，该模型在少量标注数据下表现优异，优于基线方法。

Conclusion: 该研究不仅提出了有效的压力源估计方法，还公开了一个社交媒体压力源数据集，助力AI模型训练。

Abstract: Stress haunts people in modern society, which may cause severe health issues
if left unattended. With social media becoming an integral part of daily life,
leveraging social media to detect stress has gained increasing attention. While
the majority of the work focuses on classifying stress states and stress
categories, this study introduce a new task aimed at estimating more specific
stressors (like exam, writing paper, etc.) through users' posts on social
media. Unfortunately, the diversity of stressors with many different classes
but a few examples per class, combined with the consistent arising of new
stressors over time, hinders the machine understanding of stressors. To this
end, we cast the stressor estimation problem within a practical scenario
few-shot learning setting, and propose a novel meta-learning based stressor
estimation framework that is enhanced by a meta-knowledge inheritance
mechanism. This model can not only learn generic stressor context through
meta-learning, but also has a good generalization ability to estimate new
stressors with little labeled data. A fundamental breakthrough in our approach
lies in the inclusion of the meta-knowledge inheritance mechanism, which equips
our model with the ability to prevent catastrophic forgetting when adapting to
new stressors. The experimental results show that our model achieves
state-of-the-art performance compared with the baselines. Additionally, we
construct a social media-based stressor estimation dataset that can help train
artificial intelligence models to facilitate human well-being. The dataset is
now public at
\href{https://www.kaggle.com/datasets/xinwangcs/stressor-cause-of-mental-health-problem-dataset}{\underline{Kaggle}}
and
\href{https://huggingface.co/datasets/XinWangcs/Stressor}{\underline{Hugging
Face}}.

</details>


### [79] [Improved Dimensionality Reduction for Inverse Problems in Nuclear Fusion and High-Energy Astrophysics](https://arxiv.org/abs/2505.03849)
*Jonathan Gorard,Ammar Hakim,Hong Qin,Kyle Parfrey,Shantenu Jha*

Main category: cs.LG

TL;DR: 论文提出了一种混合方法，结合蒙特卡洛采样与形式验证技术，以解决高维参数空间中的物理和数学一致性问题。


<details>
  <summary>Details</summary>
Motivation: 核聚变和高能天体物理中的逆问题通常涉及高维参数扫描和大规模模拟，但现有方法无法保证参数组合的物理有效性或数学一致性。

Method: 采用蒙特卡洛采样结合非线性降维技术（如自动编码器和流形学习），并引入形式验证方法，以确保参数空间的限制具有可证明的数学和物理正确性。

Result: 该方法能够在考虑实验和物理过程不确定性的同时，构建具有数学和物理正确性的参数空间限制。

Conclusion: 论文主张通过混合方法解决高维逆问题中的一致性问题，为未来研究提供了方向。

Abstract: Many inverse problems in nuclear fusion and high-energy astrophysics
research, such as the optimization of tokamak reactor geometries or the
inference of black hole parameters from interferometric images, necessitate
high-dimensional parameter scans and large ensembles of simulations to be
performed. Such inverse problems typically involve large uncertainties, both in
the measurement parameters being inverted and in the underlying physics models
themselves. Monte Carlo sampling, when combined with modern non-linear
dimensionality reduction techniques such as autoencoders and manifold learning,
can be used to reduce the size of the parameter spaces considerably. However,
there is no guarantee that the resulting combinations of parameters will be
physically valid, or even mathematically consistent. In this position paper, we
advocate adopting a hybrid approach that leverages our recent advances in the
development of formal verification methods for numerical algorithms, with the
goal of constructing parameter space restrictions with provable mathematical
and physical correctness properties, whilst nevertheless respecting both
experimental uncertainties and uncertainties in the underlying physical
processes.

</details>


### [80] [Machine Learning: a Lecture Note](https://arxiv.org/abs/2505.03861)
*Kyunghyun Cho*

Main category: cs.LG

TL;DR: 这篇讲义为数据科学及相关领域的硕士和博士生提供机器学习的基础知识，涵盖分类任务、概率无监督学习及高级主题。


<details>
  <summary>Details</summary>
Motivation: 为初学者提供机器学习的核心概念，帮助他们为研究更高级主题打下基础。

Method: 从分类任务的基础知识（如损失函数、反向传播、随机梯度下降）入手，深入探讨概率无监督学习（如生成对抗网络、自回归模型），最后扩展至强化学习等高级主题。

Result: 学生能够掌握机器学习的基础知识，为后续研究和学习高级主题做好准备。

Conclusion: 讲义为初学者提供了全面的机器学习基础，并引导他们进入更广泛的人工智能领域。

Abstract: This lecture note is intended to prepare early-year master's and PhD students
in data science or a related discipline with foundational ideas in machine
learning. It starts with basic ideas in modern machine learning with
classification as a main target task. These basic ideas include loss
formulation, backpropagation, stochastic gradient descent, generalization,
model selection as well as fundamental blocks of artificial neural networks.
Based on these basic ideas, the lecture note explores in depth the probablistic
approach to unsupervised learning, covering directed latent variable models,
product of experts, generative adversarial networks and autoregressive models.
Finally, the note ends by covering a diverse set of further topics, such as
reinforcement learning, ensemble methods and meta-learning. After reading this
lecture note, a student should be ready to embark on studying and researching
more advanced topics in machine learning and more broadly artificial
intelligence.

</details>


### [81] [Explaining Anomalies with Tensor Networks](https://arxiv.org/abs/2505.03911)
*Hans Hohenfeld,Marius Beuerle,Elie Mounzer*

Main category: cs.LG

TL;DR: 本文扩展了张量网络在实数数据领域的应用，并引入树张量网络用于可解释异常检测，展示了其预测性能及解释能力。


<details>
  <summary>Details</summary>
Motivation: 张量网络在量子多体波函数和机器学习中具有广泛应用潜力，但此前主要局限于离散值数据。本文旨在将其扩展到实数数据领域，并提升异常检测的可解释性。

Method: 采用矩阵乘积态和树张量网络，对实数数据进行了可解释异常检测，并通过三个基准问题验证方法。

Result: 实验表明，所提方法在预测性能上与基线模型相当，同时能够解释异常样本。

Conclusion: 本文扩展了张量网络的应用范围，为未来更复杂架构的研究奠定了基础。

Abstract: Tensor networks, a class of variational quantum many-body wave functions have
attracted considerable research interest across many disciplines, including
classical machine learning. Recently, Aizpurua et al. demonstrated explainable
anomaly detection with matrix product states on a discrete-valued
cyber-security task, using quantum-inspired methods to gain insight into the
learned model and detected anomalies. Here, we extend this framework to
real-valued data domains. We furthermore introduce tree tensor networks for the
task of explainable anomaly detection. We demonstrate these methods with three
benchmark problems, show adequate predictive performance compared to several
baseline models and both tensor network architectures' ability to explain
anomalous samples. We thereby extend the application of tensor networks to a
broader class of potential problems and open a pathway for future extensions to
more complex tensor network architectures.

</details>


### [82] [SAND: One-Shot Feature Selection with Additive Noise Distortion](https://arxiv.org/abs/2505.03923)
*Pedram Pad,Hadi Hammoud,Mohamad Dia,Nadim Maamari,L. Andrea Dunbar*

Main category: cs.LG

TL;DR: 提出了一种非侵入式特征选择层，自动选择最具信息量的特征，无需调整损失函数或网络架构，性能优越且无需超参数搜索或重新训练。


<details>
  <summary>Details</summary>
Motivation: 现有特征选择方法通常需要后选择重新训练和大量超参数调优，增加了复杂性。本文旨在提供一种简单高效的特征选择方法。

Method: 引入一个数学简洁的特征选择层，通过可训练增益和噪声失真自动选择k个最具信息量的特征。

Result: 在标准基准数据集和真实数据集上表现优异，优于或匹配现有方法，且无需超参数搜索或重新训练。

Conclusion: 证明了简单性与性能可以共存，为机器学习特征选择提供了强大而直接的工具。

Abstract: Feature selection is a critical step in data-driven applications, reducing
input dimensionality to enhance learning accuracy, computational efficiency,
and interpretability. Existing state-of-the-art methods often require
post-selection retraining and extensive hyperparameter tuning, complicating
their adoption. We introduce a novel, non-intrusive feature selection layer
that, given a target feature count $k$, automatically identifies and selects
the $k$ most informative features during neural network training. Our method is
uniquely simple, requiring no alterations to the loss function, network
architecture, or post-selection retraining. The layer is mathematically elegant
and can be fully described by: \begin{align} \nonumber \tilde{x}_i = a_i x_i +
(1-a_i)z_i \end{align} where $x_i$ is the input feature, $\tilde{x}_i$ the
output, $z_i$ a Gaussian noise, and $a_i$ trainable gain such that
$\sum_i{a_i^2}=k$. This formulation induces an automatic clustering effect,
driving $k$ of the $a_i$ gains to $1$ (selecting informative features) and the
rest to $0$ (discarding redundant ones) via weighted noise distortion and gain
normalization. Despite its extreme simplicity, our method delivers
state-of-the-art performance on standard benchmark datasets and a novel
real-world dataset, outperforming or matching existing approaches without
requiring hyperparameter search for $k$ or retraining. Theoretical analysis in
the context of linear regression further validates its efficacy. Our work
demonstrates that simplicity and performance are not mutually exclusive,
offering a powerful yet straightforward tool for feature selection in machine
learning.

</details>


### [83] [Deep Q-Network (DQN) multi-agent reinforcement learning (MARL) for Stock Trading](https://arxiv.org/abs/2505.03949)
*John Christopher Tidwell,John Storm Tidwell*

Main category: cs.LG

TL;DR: 提出了一种结合CNN、LSTM和DQN的深度学习框架，用于解决自动化股票交易中的市场噪声和复杂性问题。


<details>
  <summary>Details</summary>
Motivation: 传统方法和直接强化学习在应对市场噪声、复杂性和泛化能力方面存在不足。

Method: 集成CNN（用于识别技术指标图像中的模式）、LSTM（捕捉价格历史和技术指标的时间依赖性）和DQN（学习最优交易策略）。

Result: 框架能够有效提取特征并学习交易策略。

Conclusion: 该集成方法为自动化股票交易提供了一种有效的解决方案。

Abstract: This project addresses the challenge of automated stock trading, where
traditional methods and direct reinforcement learning (RL) struggle with market
noise, complexity, and generalization. Our proposed solution is an integrated
deep learning framework combining a Convolutional Neural Network (CNN) to
identify patterns in technical indicators formatted as images, a Long
Short-Term Memory (LSTM) network to capture temporal dependencies across both
price history and technical indicators, and a Deep Q-Network (DQN) agent which
learns the optimal trading policy (buy, sell, hold) based on the features
extracted by the CNN and LSTM.

</details>


### [84] [Sufficient Decision Proxies for Decision-Focused Learning](https://arxiv.org/abs/2505.03953)
*Noah Schutte,Grigorii Veviurko,Krzysztof Postek,Neil Yorke-Smith*

Main category: cs.LG

TL;DR: 本文研究了在不确定性优化问题中，如何通过决策导向学习（DFL）选择有效的决策代理，并首次探讨了何时适合使用单值预测或分布估计。


<details>
  <summary>Details</summary>
Motivation: 在不确定性优化问题中，传统方法通常假设单值预测或分布估计是有效的，但缺乏对何时适合使用哪种方法的理论支持。本文旨在填补这一空白。

Method: 通过分析问题特性，提出有效的决策代理方法，并在连续和离散变量问题中验证其有效性。

Result: 实验表明，所提出的方法在目标函数和约束条件的不确定性下均表现良好，且学习任务的复杂性较低。

Conclusion: 本文为决策导向学习提供了理论支持，并展示了如何根据问题特性选择合适的决策代理方法。

Abstract: When solving optimization problems under uncertainty with contextual data,
utilizing machine learning to predict the uncertain parameters is a popular and
effective approach. Decision-focused learning (DFL) aims at learning a
predictive model such that decision quality, instead of prediction accuracy, is
maximized. Common practice here is to predict a single value for each uncertain
parameter, implicitly assuming that there exists a (single-scenario)
deterministic problem approximation (proxy) that is sufficient to obtain an
optimal decision. Other work assumes the opposite, where the underlying
distribution needs to be estimated. However, little is known about when either
choice is valid. This paper investigates for the first time problem properties
that justify using either assumption. Using this, we present effective decision
proxies for DFL, with very limited compromise on the complexity of the learning
task. We show the effectiveness of presented approaches in experiments on
problems with continuous and discrete variables, as well as uncertainty in the
objective function and in the constraints.

</details>


### [85] [Hierarchical Forecast Reconciliation on Networks: A Network Flow Optimization Formulation](https://arxiv.org/abs/2505.03955)
*Charupriya Sharma,Iñaki Estella Aguerri,Daniel Guimarans*

Main category: cs.LG

TL;DR: FlowRec提出了一种基于网络流优化的层次预测协调方法，显著提升了计算效率和适用范围。


<details>
  <summary>Details</summary>
Motivation: 解决传统层次预测协调方法（如MinT）仅限于树结构且计算成本高的问题。

Method: 将层次预测协调重新表述为网络流优化问题，支持广义网络结构，并证明多项式时间可解性。

Result: FlowRec在稀疏网络中复杂度为O(n²logn)，优于MinT的O(n³)，且在精度、运行时间和内存使用上均有显著提升。

Conclusion: FlowRec是一种高效、通用的层次预测协调工具，适用于大规模应用。

Abstract: Hierarchical forecasting with reconciliation requires forecasting values of a
hierarchy (e.g.~customer demand in a state and district), such that forecast
values are linked (e.g.~ district forecasts should add up to the state
forecast). Basic forecasting provides no guarantee for these desired structural
relationships. Reconciliation addresses this problem, which is crucial for
organizations requiring coherent predictions across multiple aggregation
levels. Current methods like minimum trace (MinT) are mostly limited to tree
structures and are computationally expensive. We introduce FlowRec, which
reformulates hierarchical forecast reconciliation as a network flow
optimization, enabling forecasting on generalized network structures. While
reconciliation under the $\ell_0$ norm is NP-hard, we prove polynomial-time
solvability for all $\ell_{p > 0}$ norms and , for any strictly convex and
continuously differentiable loss function. For sparse networks, FlowRec
achieves $O(n^2\log n)$ complexity, significantly improving upon MinT's
$O(n^3)$. Furthermore, we prove that FlowRec extends MinT to handle general
networks, replacing MinT's error-covariance estimation step with direct network
structural information. A key novelty of our approach is its handling of
dynamic scenarios: while traditional methods recompute both base forecasts and
reconciliation, FlowRec provides efficient localised updates with optimality
guarantees. Monotonicity ensures that when forecasts improve incrementally, the
initial reconciliation remains optimal. We also establish efficient,
error-bounded approximate reconciliation, enabling fast updates in
time-critical applications. Experiments on both simulated and real benchmarks
demonstrate that FlowRec improves accuracy, runtime by 3-40x and memory usage
by 5-7x. These results establish FlowRec as a powerful tool for large-scale
hierarchical forecasting applications.

</details>


### [86] [Call for Action: towards the next generation of symbolic regression benchmark](https://arxiv.org/abs/2505.03977)
*Guilherme S. Imai Aldeia,Hengzhe Zhang,Geoffrey Bomarito,Miles Cranmer,Alcides Fonseca,Bogdan Burlacu,William G. La Cava,Fabrício Olivetti de França*

Main category: cs.LG

TL;DR: SRBench更新版扩展了评估方法、优化了指标和可视化，分析了模型复杂度、准确性和能耗的权衡，结果显示无单一算法在所有数据集上占优，呼吁社区维护SRBench并标准化超参数调优和资源分配。


<details>
  <summary>Details</summary>
Motivation: 由于符号回归（SR）方法的多样性、数据集和评估标准的差异，基准测试SR方法具有挑战性。

Method: 更新SRBench，增加评估方法数量，优化评估指标和结果可视化，分析模型复杂度、准确性和能耗的权衡。

Result: 结果显示无单一算法在所有数据集上表现最优。

Conclusion: 呼吁社区维护SRBench作为动态基准，标准化超参数调优和资源分配，并提出改进SR算法的建议。

Abstract: Symbolic Regression (SR) is a powerful technique for discovering
interpretable mathematical expressions. However, benchmarking SR methods
remains challenging due to the diversity of algorithms, datasets, and
evaluation criteria. In this work, we present an updated version of SRBench.
Our benchmark expands the previous one by nearly doubling the number of
evaluated methods, refining evaluation metrics, and using improved
visualizations of the results to understand the performances. Additionally, we
analyze trade-offs between model complexity, accuracy, and energy consumption.
Our results show that no single algorithm dominates across all datasets. We
propose a call for action from SR community in maintaining and evolving SRBench
as a living benchmark that reflects the state-of-the-art in symbolic
regression, by standardizing hyperparameter tuning, execution constraints, and
computational resource allocation. We also propose deprecation criteria to
maintain the benchmark's relevance and discuss best practices for improving SR
algorithms, such as adaptive hyperparameter tuning and energy-efficient
implementations.

</details>


### [87] [Comparing statistical and deep learning techniques for parameter estimation of continuous-time stochastic differentiable equations](https://arxiv.org/abs/2505.03980)
*Aroon Sankoh,Victor Wickerhauser*

Main category: cs.LG

TL;DR: 比较了传统统计方法（MLE）和深度学习模型（RNN）在估计Ornstein-Uhlenbeck过程参数时的准确性和计算成本。


<details>
  <summary>Details</summary>
Motivation: 随机微分方程（如Ornstein-Uhlenbeck过程）常用于建模现实世界中的概率事件（如股票价格和温度波动），传统统计方法（如MLE）虽常用，但深度学习技术（如RNN）可能提供更精确的估计。

Method: 通过一系列实验，比较了MLE和RNN在估计Ornstein-Uhlenbeck过程参数时的表现。

Result: 实验结果表明，RNN在估计准确性上可能优于MLE，但计算成本可能更高。

Conclusion: 深度学习模型（如RNN）在参数估计任务中具有潜力，但需权衡其计算成本。

Abstract: Stochastic differential equations such as the Ornstein-Uhlenbeck process have
long been used to model realworld probablistic events such as stock prices and
temperature fluctuations. While statistical methods such as Maximum Likelihood
Estimation (MLE), Kalman Filtering, Inverse Variable Method, and more have
historically been used to estimate the parameters of stochastic differential
equations, the recent explosion of deep learning technology suggests that
models such as a Recurrent Neural Network (RNN) could produce more precise
estimators. We present a series of experiments that compare the estimation
accuracy and computational expensiveness of a statistical method (MLE) with a
deep learning model (RNN) for the parameters of the Ornstein-Uhlenbeck process.

</details>


### [88] [Diffusion Models are Secretly Exchangeable: Parallelizing DDPMs via Autospeculation](https://arxiv.org/abs/2505.03983)
*Hengyuan Hu,Aniket Das,Dorsa Sadigh,Nima Anari*

Main category: cs.LG

TL;DR: 论文提出了一种基于DDPM的优化方法Autospeculative Decoding (ASD)，通过利用DDPM与随机定位的联系，实现了并行化加速，显著提升了推理速度。


<details>
  <summary>Details</summary>
Motivation: DDPM在生成建模中表现强大，但其顺序计算需求导致推理时间瓶颈。

Method: 通过重新参数化证明DDPM的增量具有交换性，从而将自回归模型的优化技术应用于扩散模型，并提出了无需辅助模型的ASD方法。

Result: 理论分析表明ASD实现了$	ilde{O}(K^{rac{1}{3}})$的并行加速，实际应用中也显著加速了DDPM推理。

Conclusion: ASD是一种高效的黑盒优化方法，能够显著提升DDPM的推理速度。

Abstract: Denoising Diffusion Probabilistic Models (DDPMs) have emerged as powerful
tools for generative modeling. However, their sequential computation
requirements lead to significant inference-time bottlenecks. In this work, we
utilize the connection between DDPMs and Stochastic Localization to prove that,
under an appropriate reparametrization, the increments of DDPM satisfy an
exchangeability property. This general insight enables near-black-box
adaptation of various performance optimization techniques from autoregressive
models to the diffusion setting. To demonstrate this, we introduce
\emph{Autospeculative Decoding} (ASD), an extension of the widely used
speculative decoding algorithm to DDPMs that does not require any auxiliary
draft models. Our theoretical analysis shows that ASD achieves a $\tilde{O}
(K^{\frac{1}{3}})$ parallel runtime speedup over the $K$ step sequential DDPM.
We also demonstrate that a practical implementation of autospeculative decoding
accelerates DDPM inference significantly in various domains.

</details>


### [89] [Algorithmic Accountability in Small Data: Sample-Size-Induced Bias Within Classification Metrics](https://arxiv.org/abs/2505.03992)
*Jarren Briscoe,Garrett Kepler,Daryl Deford,Assefaw Gebremedhin*

Main category: cs.LG

TL;DR: 论文揭示了分类指标中由组合学引起的样本量偏差问题，挑战了这些指标在高分辨率评估中的有效性，并提出了一种模型无关的校正方法。


<details>
  <summary>Details</summary>
Motivation: 评估机器学习模型不仅关乎技术准确性，还需考虑其社会影响。研究发现组合学导致的样本量偏差对分类指标的影响，尤其是在群体规模差异大的情况下。

Method: 分析了多种常用指标中的偏差，提出了一种模型无关的评估和校正技术，并研究了未定义案例对评估的误导。

Result: 揭示了组合学和概率在标准评估实践中的未被认识到的挑战，改进了公平和可信分类方法的评估。

Conclusion: 工作为高分辨率评估中的偏差问题提供了新视角，推动了公平和可信分类方法的发展。

Abstract: Evaluating machine learning models is crucial not only for determining their
technical accuracy but also for assessing their potential societal
implications. While the potential for low-sample-size bias in algorithms is
well known, we demonstrate the significance of sample-size bias induced by
combinatorics in classification metrics. This revelation challenges the
efficacy of these metrics in assessing bias with high resolution, especially
when comparing groups of disparate sizes, which frequently arise in social
applications. We provide analyses of the bias that appears in several commonly
applied metrics and propose a model-agnostic assessment and correction
technique. Additionally, we analyze counts of undefined cases in metric
calculations, which can lead to misleading evaluations if improperly handled.
This work illuminates the previously unrecognized challenge of combinatorics
and probability in standard evaluation practices and thereby advances
approaches for performing fair and trustworthy classification methods.

</details>


### [90] [Iterative Orthogonalization Scaling Laws](https://arxiv.org/abs/2505.04005)
*Devan Selvaraj*

Main category: cs.LG

TL;DR: 论文研究了muon优化器在大规模应用中的潜在问题，特别是随机矩阵奇异值随规模缩小的问题，但未提出解决方案。


<details>
  <summary>Details</summary>
Motivation: 探讨muon优化器在大规模场景下的局限性，尤其是其迭代正交化过程可能因随机矩阵奇异值缩小而受影响。

Method: 通过理论和实证分析，研究随机矩阵在muon优化器中的奇异值缩放行为。

Result: 发现随机矩阵的奇异值随规模扩大而缩小，可能影响muon优化器的性能。

Conclusion: 论文揭示了muon优化器在大规模应用中的潜在问题，但未提供具体解决方案。

Abstract: The muon optimizer has picked up much attention as of late as a possible
replacement to the seemingly omnipresent Adam optimizer. Recently, care has
been taken to document the scaling laws of hyper-parameters under muon such as
weight decay and learning rate. However, at much larger scales the iterative
orthogonalization procedure present in muon may suffer a possible issue as the
singular values of random matrices shrink with scale. This paper shows this
scaling behavior theoretically and empirically on random matrices but does not
suggest what to do about it.

</details>


### [91] [Reliable Disentanglement Multi-view Learning Against View Adversarial Attacks](https://arxiv.org/abs/2505.04046)
*Xuyang Wang,Siyuan Duan,Qizhi Li,Guiduo Duan,Yuan Sun,Dezhong Peng*

Main category: cs.LG

TL;DR: 论文提出了一种名为RDML的多视图学习框架，旨在解决可信多视图学习中的对抗不可靠性问题（AUP），通过证据解缠学习和特征重校准模块提升模型对抗干扰的能力。


<details>
  <summary>Details</summary>
Motivation: 现有可信多视图学习方法假设数据安全，但实际应用中多视图数据常面临对抗性扰动威胁，导致对抗不可靠性问题（AUP）。

Method: 提出RDML框架，包括证据解缠学习、特征重校准模块和视图级证据注意力机制，以分解视图并减轻对抗性干扰。

Result: 实验表明，RDML在对抗攻击下的多视图分类任务中显著优于现有方法。

Conclusion: RDML通过解缠和重校准有效提升了多视图学习的对抗鲁棒性和可靠性。

Abstract: Recently, trustworthy multi-view learning has attracted extensive attention
because evidence learning can provide reliable uncertainty estimation to
enhance the credibility of multi-view predictions. Existing trusted multi-view
learning methods implicitly assume that multi-view data is secure. In practice,
however, in safety-sensitive applications such as autonomous driving and
security monitoring, multi-view data often faces threats from adversarial
perturbations, thereby deceiving or disrupting multi-view learning models. This
inevitably leads to the adversarial unreliability problem (AUP) in trusted
multi-view learning. To overcome this tricky problem, we propose a novel
multi-view learning framework, namely Reliable Disentanglement Multi-view
Learning (RDML). Specifically, we first propose evidential disentanglement
learning to decompose each view into clean and adversarial parts under the
guidance of corresponding evidences, which is extracted by a pretrained
evidence extractor. Then, we employ the feature recalibration module to
mitigate the negative impact of adversarial perturbations and extract potential
informative features from them. Finally, to further ignore the irreparable
adversarial interferences, a view-level evidential attention mechanism is
designed. Extensive experiments on multi-view classification tasks with
adversarial attacks show that our RDML outperforms the state-of-the-art
multi-view learning methods by a relatively large margin.

</details>


### [92] [LLM-e Guess: Can LLMs Capabilities Advance Without Hardware Progress?](https://arxiv.org/abs/2505.04075)
*Teddy Foley,Spencer Guo,Henry Josephson,Anqi Qu,Jack Sanderson*

Main category: cs.LG

TL;DR: 论文探讨了在计算资源受限环境下，大型语言模型（LLM）是否能通过算法创新继续进步，并提出了计算等效增益（CEG）来衡量算法改进的效果。


<details>
  <summary>Details</summary>
Motivation: 研究动机源于监管机构对高性能硬件的限制，探讨在计算资源受限时LLM是否仍能通过算法创新取得进展。

Method: 提出了一种分类框架，区分计算依赖型（如Transformer架构）和计算无关型（如旋转位置编码）算法创新，并通过CEG量化其贡献。使用缩小版GPT-2模型进行实验验证。

Result: 实验表明，计算无关型创新在资源受限时仍能带来显著性能提升（CEG达3.5倍），而计算依赖型创新在小规模实验中效果有限甚至有害。

Conclusion: 结论指出，计算无关型算法创新在资源受限时更具价值，而计算依赖型创新需要充足的计算资源支持。

Abstract: This paper examines whether large language model (LLM) capabilities can
continue to advance without additional compute by analyzing the development and
role of algorithms used in state-of-the-art LLMs. Motivated by regulatory
efforts that have largely focused on restricting access to high-performance
hardware, we ask: Can LLMs progress in a compute-constrained environment, and
how do algorithmic innovations perform under such conditions?
  To address these questions, we introduce a novel classification framework
that distinguishes between compute-dependent innovations -- which yield
disproportionate benefits at high compute levels (e.g., the Transformer
architecture and mixture-of-experts models) and compute-independent
innovations, which improve efficiency across all compute scales (e.g., rotary
positional encoding, FlashAttention, or layer normalization). We quantify these
contributions using a metric called compute-equivalent gain (CEG), which
estimates the additional compute that would be required to achieve similar
improvements without these algorithmic advancements.
  To validate this framework, we conduct small-scale training experiments with
a scaled-down GPT-2 model. Our results confirm that compute-independent
advancements yield meaningful performance gains even in resource-constrained
settings, with a CEG of up to $3.5\times$ over a baseline model. By contrast,
compute-dependent advancements provided little benefit or even degraded
performance at the small scale, reinforcing the importance of compute
availability for certain algorithmic gains.

</details>


### [93] [Plexus: Taming Billion-edge Graphs with 3D Parallel GNN Training](https://arxiv.org/abs/2505.04083)
*Aditya K. Ranjan,Siddharth Singh,Cunyang Wei,Abhinav Bhatele*

Main category: cs.LG

TL;DR: Plexus是一种针对大规模图神经网络训练的三维并行方法，解决了传统方法的内存限制和通信开销问题，显著提升了训练速度和效率。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的图数据规模庞大，传统方法如小批量采样或分布式训练存在内存不足、通信开销高和负载不平衡等问题，需要一种更高效的解决方案。

Method: 提出Plexus，一种三维并行方法，结合负载平衡的置换方案和性能模型，优化GPU配置，适用于十亿级边图。

Result: 在Perlmutter和Frontier上测试，Plexus比现有方法快2.3x-12.5x，解决方案时间减少5.2-8.7x（Perlmutter）和7-54.2x（Frontier）。

Conclusion: Plexus通过三维并行和优化技术，显著提升了大规模图神经网络训练的效率和扩展性。

Abstract: Graph neural networks have emerged as a potent class of neural networks
capable of leveraging the connectivity and structure of real-world graphs to
learn intricate properties and relationships between nodes. Many real-world
graphs exceed the memory capacity of a GPU due to their sheer size, and using
GNNs on them requires techniques such as mini-batch sampling to scale. However,
this can lead to reduced accuracy in some cases, and sampling and data transfer
from the CPU to the GPU can also slow down training. On the other hand,
distributed full-graph training suffers from high communication overhead and
load imbalance due to the irregular structure of graphs. We propose Plexus, a
three-dimensional (3D) parallel approach for full-graph training that tackles
these issues and scales to billion-edge graphs. Additionally, we introduce
optimizations such as a permutation scheme for load balancing, and a
performance model to predict the optimal 3D configuration. We evaluate Plexus
on several graph datasets and show scaling results for up to 2048 GPUs on
Perlmutter, which is 33% of the machine, and 2048 GCDs on Frontier. Plexus
achieves unprecedented speedups of 2.3x-12.5x over existing methods and a
reduction in the time to solution by 5.2-8.7x on Perlmutter and 7-54.2x on
Frontier.

</details>


### [94] [Position: We need responsible, application-driven (RAD) AI research](https://arxiv.org/abs/2505.04104)
*Sarah Hartman,Cheng Soon Ong,Julia Powles,Petra Kuhnert*

Main category: cs.LG

TL;DR: 本文提出了一种负责任、应用驱动的人工智能研究方法（RAD-AI），强调AI研究需结合具体应用场景，关注伦理、法律和社会需求。


<details>
  <summary>Details</summary>
Motivation: 随着AI在社会中的广泛应用，研究者需要关注其具体应用场景，以确保技术发展符合伦理、法律和社会需求。

Method: 提出三阶段方法：1）组建跨学科团队和以人为中心的研究；2）解决特定情境下的方法、伦理承诺、假设和指标；3）通过分阶段测试和实践社区验证和维持效果。

Result: RAD-AI方法能够推动技术上可行且适应社区需求和价值观的AI研究。

Conclusion: 应用驱动的AI研究有望通过结合具体情境和社区需求，实现科学和社会进步。

Abstract: This position paper argues that achieving meaningful scientific and societal
advances with artificial intelligence (AI) requires a responsible,
application-driven approach (RAD) to AI research. As AI is increasingly
integrated into society, AI researchers must engage with the specific contexts
where AI is being applied. This includes being responsive to ethical and legal
considerations, technical and societal constraints, and public discourse. We
present the case for RAD-AI to drive research through a three-staged approach:
(1) building transdisciplinary teams and people-centred studies; (2) addressing
context-specific methods, ethical commitments, assumptions, and metrics; and
(3) testing and sustaining efficacy through staged testbeds and a community of
practice. We present a vision for the future of application-driven AI research
to unlock new value through technically feasible methods that are adaptive to
the contextual needs and values of the communities they ultimately serve.

</details>


### [95] [Alpha Excel Benchmark](https://arxiv.org/abs/2505.04110)
*David Noever,Forrest McKee*

Main category: cs.LG

TL;DR: 该研究提出了一个基于金融建模世界杯（FMWC）Excel竞赛的新基准，用于评估大型语言模型（LLMs）的性能。通过将113个FMWC挑战转化为可编程评估的JSON格式，比较了多个领先LLM的表现。结果显示模型在不同任务类别中表现差异显著，擅长模式识别但数值推理较弱。


<details>
  <summary>Details</summary>
Motivation: 填补学术AI基准与实际商业应用之间的差距，以Excel用户（全球15亿人）的熟练度作为评估标准，提供更贴近现实的商业任务评估框架。

Method: 将FMWC的113个挑战转化为可编程评估的JSON格式，并用于测试多个领先LLM的性能。

Result: 模型在模式识别任务中表现优异，但在复杂数值推理方面表现不佳，不同任务类别间性能差异显著。

Conclusion: 该基准为评估LLM在商业任务中的能力提供了标准化框架，连接了学术研究与实际应用。

Abstract: This study presents a novel benchmark for evaluating Large Language Models
(LLMs) using challenges derived from the Financial Modeling World Cup (FMWC)
Excel competitions. We introduce a methodology for converting 113 existing FMWC
challenges into programmatically evaluable JSON formats and use this dataset to
compare the performance of several leading LLMs. Our findings demonstrate
significant variations in performance across different challenge categories,
with models showing specific strengths in pattern recognition tasks but
struggling with complex numerical reasoning. The benchmark provides a
standardized framework for assessing LLM capabilities in realistic
business-oriented tasks rather than abstract academic problems. This research
contributes to the growing field of AI benchmarking by establishing proficiency
among the 1.5 billion people who daily use Microsoft Excel as a meaningful
evaluation metric that bridges the gap between academic AI benchmarks and
practical business applications.

</details>


### [96] [LHT: Statistically-Driven Oblique Decision Trees for Interpretable Classification](https://arxiv.org/abs/2505.04139)
*Hongyi Li,Jun Xu,William Ward Armstrong*

Main category: cs.LG

TL;DR: LHT是一种新型斜决策树模型，通过非迭代、统计驱动的方法构建分割超平面，具有表达性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖迭代优化或启发式方法，LHT旨在提供一种直接且定义明确的超平面构建过程。

Method: LHT直接计算超平面参数，基于节点内类间特征期望差异的特征权重，并使用局部最小二乘拟合的独特分段线性隶属函数进行预测。

Result: LHT在基准数据集上表现出竞争力，构建时间复杂度为O(mnd)，确保了实际可行性。

Conclusion: LHT是一种实用、理论基础扎实且可解释的树模型替代方案。

Abstract: We introduce the Learning Hyperplane Tree (LHT), a novel oblique decision
tree model designed for expressive and interpretable classification. LHT
fundamentally distinguishes itself through a non-iterative,
statistically-driven approach to constructing splitting hyperplanes. Unlike
methods that rely on iterative optimization or heuristics, LHT directly
computes the hyperplane parameters, which are derived from feature weights
based on the differences in feature expectations between classes within each
node. This deterministic mechanism enables a direct and well-defined hyperplane
construction process. Predictions leverage a unique piecewise linear membership
function within leaf nodes, obtained via local least-squares fitting. We
formally analyze the convergence of the LHT splitting process, ensuring that
each split yields meaningful, non-empty partitions. Furthermore, we establish
that the time complexity for building an LHT up to depth $d$ is $O(mnd)$,
demonstrating the practical feasibility of constructing trees with powerful
oblique splits using this methodology. The explicit feature weighting at each
split provides inherent interpretability. Experimental results on benchmark
datasets demonstrate LHT's competitive accuracy, positioning it as a practical,
theoretically grounded, and interpretable alternative in the landscape of
tree-based models. The implementation of the proposed method is available at
https://github.com/Hongyi-Li-sz/LHT_model.

</details>


### [97] [FilterTS: Comprehensive Frequency Filtering for Multivariate Time Series Forecasting](https://arxiv.org/abs/2505.04158)
*Yulong Wang,Yushuo Liu,Xiaoyi Duan,Kai Wang*

Main category: cs.LG

TL;DR: FilterTS是一种新颖的多变量时间序列预测模型，通过频域滤波技术动态提取和增强变量间的共享频率成分，显著提升了预测性能和计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有模型难以捕捉多变量时间序列中复杂的周期性和趋势成分，影响了预测性能。

Method: 提出FilterTS模型，包含动态跨变量滤波模块和静态全局滤波模块，利用频域技术将时间域卷积转换为频域乘法操作。

Result: 在八个真实数据集上的实验表明，FilterTS在预测准确性和计算效率上显著优于现有方法。

Conclusion: FilterTS通过频域滤波技术有效解决了多变量时间序列预测中的复杂模式提取问题，具有实际应用价值。

Abstract: Multivariate time series forecasting is crucial across various industries,
where accurate extraction of complex periodic and trend components can
significantly enhance prediction performance. However, existing models often
struggle to capture these intricate patterns. To address these challenges, we
propose FilterTS, a novel forecasting model that utilizes specialized filtering
techniques based on the frequency domain. FilterTS introduces a Dynamic
Cross-Variable Filtering Module, a key innovation that dynamically leverages
other variables as filters to extract and reinforce shared variable frequency
components across variables in multivariate time series. Additionally, a Static
Global Filtering Module captures stable frequency components, identified
throughout the entire training set. Moreover, the model is built in the
frequency domain, converting time-domain convolutions into frequency-domain
multiplicative operations to enhance computational efficiency. Extensive
experimental results on eight real-world datasets have demonstrated that
FilterTS significantly outperforms existing methods in terms of prediction
accuracy and computational efficiency.

</details>


### [98] [Optimization of Infectious Disease Intervention Measures Based on Reinforcement Learning -- Empirical analysis based on UK COVID-19 epidemic data](https://arxiv.org/abs/2505.04161)
*Baida Zhang,Yakai Chen,Huichun Li,Zhenghu Zu*

Main category: cs.LG

TL;DR: 该论文提出了一种基于个体代理模型的强化学习决策框架，用于优化传染病干预措施，并通过实验和理论验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 传染病爆发对健康安全和经济造成深远影响，现有研究多基于简化模型，难以捕捉疾病传播的复杂性和动态性。

Method: 建立基于个体代理模型的强化学习框架，修改Covasim模型以支持强化学习研究，并探索多种算法在不同动作空间中的应用效果。

Result: 实验和理论分析验证了框架的有效性，提取的策略能有效抑制疫情扩散并维护经济稳定。

Conclusion: 该框架为全球公共卫生安全策略提供了重要参考，证明了强化学习在复杂传染病模型中的潜力。

Abstract: Globally, the outbreaks of infectious diseases have exerted an extremely
profound and severe influence on health security and the economy. During the
critical phases of epidemics, devising effective intervention measures poses a
significant challenge to both the academic and practical arenas. There is
numerous research based on reinforcement learning to optimize intervention
measures of infectious diseases. Nevertheless, most of these efforts have been
confined within the differential equation based on infectious disease models.
Although a limited number of studies have incorporated reinforcement learning
methodologies into individual-based infectious disease models, the models
employed therein have entailed simplifications and limitations, rendering it
incapable of modeling the complexity and dynamics inherent in infectious
disease transmission. We establish a decision-making framework based on an
individual agent-based transmission model, utilizing reinforcement learning to
continuously explore and develop a strategy function. The framework's validity
is verified through both experimental and theoretical approaches. Covasim, a
detailed and widely used agent-based disease transmission model, was modified
to support reinforcement learning research. We conduct an exhaustive
exploration of the application efficacy of multiple algorithms across diverse
action spaces. Furthermore, we conduct an innovative preliminary theoretical
analysis concerning the issue of "time coverage". The results of the experiment
robustly validate the effectiveness and feasibility of the methodological
framework of this study. The coping strategies gleaned therefrom prove highly
efficacious in suppressing the expansion of the epidemic scale and safeguarding
the stability of the economic system, thereby providing crucial reference
perspectives for the formulation of global public health security strategies.

</details>


### [99] [Retrieval Augmented Time Series Forecasting](https://arxiv.org/abs/2505.04163)
*Sungwon Han,Seungeon Lee,Meeyoung Cha,Sercan O Arik,Jinsung Yoon*

Main category: cs.LG

TL;DR: RAFT是一种基于检索的时间序列预测方法，通过检索历史数据中与输入最相似的候选模式，结合其未来值进行预测，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 传统时间序列预测方法可能缺乏足够的归纳偏置或学习能力，RAFT旨在通过检索历史数据补充模型的预测能力。

Method: RAFT通过检索训练数据中与输入模式最相似的历史候选数据，并利用这些候选数据的未来值辅助预测。

Result: 在十个基准数据集上的实验表明，RAFT平均胜率为86%，优于现有基线方法。

Conclusion: RAFT通过检索历史数据增强模型能力，是一种简单有效的时间序列预测方法。

Abstract: Time series forecasting uses historical data to predict future trends,
leveraging the relationships between past observations and available features.
In this paper, we propose RAFT, a retrieval-augmented time series forecasting
method to provide sufficient inductive biases and complement the model's
learning capacity. When forecasting the subsequent time frames, we directly
retrieve historical data candidates from the training dataset with patterns
most similar to the input, and utilize the future values of these candidates
alongside the inputs to obtain predictions. This simple approach augments the
model's capacity by externally providing information about past patterns via
retrieval modules. Our empirical evaluations on ten benchmark datasets show
that RAFT consistently outperforms contemporary baselines with an average win
ratio of 86%.

</details>


### [100] [STRGCN: Capturing Asynchronous Spatio-Temporal Dependencies for Irregular Multivariate Time Series Forecasting](https://arxiv.org/abs/2505.04167)
*Yulong Wang,Xiaofeng Hu,Xiaojian Cui,Kai Wang*

Main category: cs.LG

TL;DR: STRGCN是一种新型的时空关系图卷积网络，直接处理不规则多元时间序列，避免了预对齐的局限性，并通过全连接图捕捉复杂依赖关系。


<details>
  <summary>Details</summary>
Motivation: 不规则多元时间序列（IMTS）在现实应用中普遍存在，但现有方法依赖预对齐策略，可能扭曲数据固有模式并增加计算负担。

Method: 提出STRGCN，通过全连接图表示IMTS，避免预对齐，并采用分层“三明治”结构优化图嵌入。

Result: 在四个公共数据集上的实验表明，STRGCN在准确性、内存使用和训练速度方面均达到领先水平。

Conclusion: STRGCN有效解决了IMTS建模中的挑战，同时保持了计算效率和准确性。

Abstract: Irregular multivariate time series (IMTS) are prevalent in real-world
applications across many fields, where varying sensor frequencies and
asynchronous measurements pose significant modeling challenges. Existing
solutions often rely on a pre-alignment strategy to normalize data, which can
distort intrinsic patterns and escalate computational and memory demands.
Addressing these limitations, we introduce STRGCN, a Spatio-Temporal Relational
Graph Convolutional Network that avoids pre-alignment and directly captures the
complex interdependencies in IMTS by representing them as a fully connected
graph. Each observation is represented as a node, allowing the model to
effectively handle misaligned timestamps by mapping all inter-node
relationships, thus faithfully preserving the asynchronous nature of the data.
Moreover, we enhance this model with a hierarchical ``Sandwich'' structure that
strategically aggregates nodes to optimize graph embeddings, reducing
computational overhead while maintaining detailed local and global context.
Extensive experiments on four public datasets demonstrate that STRGCN achieves
state-of-the-art accuracy, competitive memory usage and training speed.

</details>


### [101] [DiffPattern-Flex: Efficient Layout Pattern Generation via Discrete Diffusion](https://arxiv.org/abs/2505.04173)
*Zixiao Wang,Wenqian Zhao,Yunheng Shen,Yang Bai,Guojin Chen,Farzan Farnia,Bei Yu*

Main category: cs.LG

TL;DR: DiffPattern-Flex 是一种基于离散扩散模型的新方法，用于高效生成可靠的布局模式，结合优化评估和快速采样技术，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前深度生成模型在布局模式生成中占主导地位，但仅依赖神经网络难以保证合法性，因此需要一种更可靠的方法。

Method: 采用离散扩散模型生成多样化拓扑，结合基于设计规则的优化评估和快速采样技术，确保合法性和高效性。

Result: 实验结果表明，DiffPattern-Flex 在多个基准测试中显著优于现有方法，并能高效生成合法布局模式。

Conclusion: DiffPattern-Flex 提供了一种高效可靠的布局模式生成方法，解决了神经网络在合法性保证上的不足。

Abstract: Recent advancements in layout pattern generation have been dominated by deep
generative models. However, relying solely on neural networks for legality
guarantees raises concerns in many practical applications. In this paper, we
present \tool{DiffPattern}-Flex, a novel approach designed to generate reliable
layout patterns efficiently. \tool{DiffPattern}-Flex incorporates a new method
for generating diverse topologies using a discrete diffusion model while
maintaining a lossless and compute-efficient layout representation. To ensure
legal pattern generation, we employ {an} optimization-based, white-box pattern
assessment process based on specific design rules. Furthermore, fast sampling
and efficient legalization technologies are employed to accelerate the
generation process. Experimental results across various benchmarks demonstrate
that \tool{DiffPattern}-Flex significantly outperforms existing methods and
excels at producing reliable layout patterns.

</details>


### [102] [On-Device LLM for Context-Aware Wi-Fi Roaming](https://arxiv.org/abs/2505.04174)
*Ju-Hyung Lee,Yanqing Lu*

Main category: cs.LG

TL;DR: 论文提出了一种利用设备端大型语言模型（LLM）进行跨层无线漫游优化的方法，通过上下文感知的AP选择和动态阈值调整，显著提升了漫游稳定性和信号质量。


<details>
  <summary>Details</summary>
Motivation: 传统基于阈值或启发式的方法在动态移动环境中表现不佳，导致漫游粘滞或过度切换，需要更智能的解决方案。

Method: 采用LLM进行跨层推理，结合环境线索（如位置、时间）选择最佳AP，并动态调整漫游阈值。通过链式思维提示、参数高效微调和量化优化满足边缘硬件的资源限制。

Result: 实验表明，该方法优于传统启发式和深度强化学习基线，实现了漫游稳定性和信号质量的平衡。

Conclusion: 应用层LLM推理为未来边缘系统中的底层无线控制提供了有前景的解决方案。

Abstract: Wireless roaming is a critical yet challenging task for maintaining seamless
connectivity in dynamic mobile environments. Conventional threshold-based or
heuristic schemes often fail, leading to either sticky or excessive handovers.
We introduce the first cross-layer use of an on-device large language model
(LLM): high-level reasoning in the application layer that issues real-time
actions executed in the PHY/MAC stack. The LLM addresses two tasks: (i)
context-aware AP selection, where structured prompts fuse environmental cues
(e.g., location, time) to choose the best BSSID; and (ii) dynamic threshold
adjustment, where the model adaptively decides when to roam. To satisfy the
tight latency and resource budgets of edge hardware, we apply a suite of
optimizations-chain-of-thought prompting, parameter-efficient fine-tuning, and
quantization. Experiments on indoor and outdoor datasets show that our approach
surpasses legacy heuristics and DRL baselines, achieving a strong balance
between roaming stability and signal quality. These findings underscore the
promise of application-layer LLM reasoning for lower-layer wireless control in
future edge systems.

</details>


### [103] [Trajectory Entropy Reinforcement Learning for Predictable and Robust Control](https://arxiv.org/abs/2505.04193)
*Bang You,Chenxu Wang,Huaping Liu*

Main category: cs.LG

TL;DR: 论文提出了一种新的强化学习方法，通过最小化动作轨迹的熵来引入简单性归纳偏置，从而提升策略的鲁棒性和性能。


<details>
  <summary>Details</summary>
Motivation: 深度强化学习在复杂控制任务中表现出色，但容易捕捉观测与动作之间的复杂虚假关联，导致环境轻微扰动时失败。因此，需要引入简单性归纳偏置以提高鲁棒性。

Method: 提出轨迹熵强化学习（Trajectory Entropy Reinforcement Learning），通过最小化动作轨迹的熵（即描述动作轨迹所需的信息量）并最大化奖励来优化策略。利用变分参数化动作预测模型估计轨迹熵，并构建信息正则化的奖励函数。

Result: 实验表明，该方法在高维运动任务中生成更具周期性和一致性的动作轨迹，性能优于现有方法，且对噪声和动态变化更具鲁棒性。

Conclusion: 通过引入轨迹熵最小化的简单性归纳偏置，显著提升了强化学习策略的鲁棒性和性能。

Abstract: Simplicity is a critical inductive bias for designing data-driven
controllers, especially when robustness is important. Despite the impressive
results of deep reinforcement learning in complex control tasks, it is prone to
capturing intricate and spurious correlations between observations and actions,
leading to failure under slight perturbations to the environment. To tackle
this problem, in this work we introduce a novel inductive bias towards simple
policies in reinforcement learning. The simplicity inductive bias is introduced
by minimizing the entropy of entire action trajectories, corresponding to the
number of bits required to describe information in action trajectories after
the agent observes state trajectories. Our reinforcement learning agent,
Trajectory Entropy Reinforcement Learning, is optimized to minimize the
trajectory entropy while maximizing rewards. We show that the trajectory
entropy can be effectively estimated by learning a variational parameterized
action prediction model, and use the prediction model to construct an
information-regularized reward function. Furthermore, we construct a practical
algorithm that enables the joint optimization of models, including the policy
and the prediction model. Experimental evaluations on several high-dimensional
locomotion tasks show that our learned policies produce more cyclical and
consistent action trajectories, and achieve superior performance, and
robustness to noise and dynamic changes than the state-of-the-art.

</details>


### [104] [A Large Language Model for Feasible and Diverse Population Synthesis](https://arxiv.org/abs/2505.04196)
*Sung Yoo Lim,Hyunsoo Yun,Prateek Bansal,Dong-Kyu Kim,Eui-Jin Kim*

Main category: cs.LG

TL;DR: 提出了一种基于大型语言模型（LLM）和贝叶斯网络（BN）的混合方法，用于生成既可行又多样化的合成人口，显著提高了可行性，同时保持了多样性。


<details>
  <summary>Details</summary>
Motivation: 在基于活动的模型（ABM）中，生成既可行又多样化的合成人口对下游活动模拟的有效性至关重要。传统深度生成模型（DGM）难以平衡稀有但合理的组合与排除不合理组合的问题。

Method: 通过贝叶斯网络（BN）的拓扑排序显式控制LLM的自回归生成过程，提出了一种轻量级开源LLM的微调方法。

Result: 实验结果显示，该方法在可行性上达到约95%，显著高于传统DGM的约80%，同时保持了相当的多样性。

Conclusion: 该方法成本低、可扩展，适用于大规模应用（如大城市人口合成），并能提高ABM模拟的可靠性。源代码已公开。

Abstract: Generating a synthetic population that is both feasible and diverse is
crucial for ensuring the validity of downstream activity schedule simulation in
activity-based models (ABMs). While deep generative models (DGMs), such as
variational autoencoders and generative adversarial networks, have been applied
to this task, they often struggle to balance the inclusion of rare but
plausible combinations (i.e., sampling zeros) with the exclusion of implausible
ones (i.e., structural zeros). To improve feasibility while maintaining
diversity, we propose a fine-tuning method for large language models (LLMs)
that explicitly controls the autoregressive generation process through
topological orderings derived from a Bayesian Network (BN). Experimental
results show that our hybrid LLM-BN approach outperforms both traditional DGMs
and proprietary LLMs (e.g., ChatGPT-4o) with few-shot learning. Specifically,
our approach achieves approximately 95% feasibility, significantly higher than
the ~80% observed in DGMs, while maintaining comparable diversity, making it
well-suited for practical applications. Importantly, the method is based on a
lightweight open-source LLM, enabling fine-tuning and inference on standard
personal computing environments. This makes the approach cost-effective and
scalable for large-scale applications, such as synthesizing populations in
megacities, without relying on expensive infrastructure. By initiating the ABM
pipeline with high-quality synthetic populations, our method improves overall
simulation reliability and reduces downstream error propagation. The source
code for these methods is available for research and practical application.

</details>


### [105] [Estimating Causal Effects in Networks with Cluster-Based Bandits](https://arxiv.org/abs/2505.04200)
*Ahmed Sayeed Faruk,Jason Sulskis,Elena Zheleva*

Main category: cs.LG

TL;DR: 论文提出了两种基于聚类的多臂老虎机（MAB）算法，用于在网络中逐步估计总治疗效果，同时通过探索与利用的权衡最大化预期奖励。


<details>
  <summary>Details</summary>
Motivation: 在存在干扰（如社交网络中个体相互影响）的情况下，A/B测试难以准确估计因果效应，且可能导致高绩效损失，因此需要一种能自适应学习并高效估计网络总治疗效果的策略。

Method: 引入两种基于聚类的MAB算法，通过逐步估计网络中的总治疗效果，并在探索与利用之间进行权衡。

Result: 基于聚类的MAB算法在奖励-行动比上优于对应的RCT方法，且在治疗效果估计上牺牲较少准确性。

Conclusion: 基于聚类的MAB算法能有效解决网络干扰问题，同时平衡探索与利用，优于传统RCT方法和忽略聚类的MAB算法。

Abstract: The gold standard for estimating causal effects is randomized controlled
trial (RCT) or A/B testing where a random group of individuals from a
population of interest are given treatment and the outcome is compared to a
random group of individuals from the same population. However, A/B testing is
challenging in the presence of interference, commonly occurring in social
networks, where individuals can impact each others outcome. Moreover, A/B
testing can incur a high performance loss when one of the treatment arms has a
poor performance and the test continues to treat individuals with it.
Therefore, it is important to design a strategy that can adapt over time and
efficiently learn the total treatment effect in the network. We introduce two
cluster-based multi-armed bandit (MAB) algorithms to gradually estimate the
total treatment effect in a network while maximizing the expected reward by
making a tradeoff between exploration and exploitation. We compare the
performance of our MAB algorithms with a vanilla MAB algorithm that ignores
clusters and the corresponding RCT methods on semi-synthetic data with
simulated interference. The vanilla MAB algorithm shows higher reward-action
ratio at the cost of higher treatment effect error due to undesired spillover.
The cluster-based MAB algorithms show higher reward-action ratio compared to
their corresponding RCT methods without sacrificing much accuracy in treatment
effect estimation.

</details>


### [106] [Cyber Security Data Science: Machine Learning Methods and their Performance on Imbalanced Datasets](https://arxiv.org/abs/2505.04204)
*Mateo Lopez-Ledezma,Gissel Velarde*

Main category: cs.LG

TL;DR: 论文探讨了网络安全中自动化的重要性，并通过三个实验评估了多种分类器和采样技术，发现不平衡学习技术需谨慎使用，且不同数据集的最佳方法各异。


<details>
  <summary>Details</summary>
Motivation: 网络安全在全球范围内至关重要，自动化处理大规模操作是基本原则。许多网络安全问题可视为二分类问题，如异常检测、欺诈检测等。

Method: 通过三个实验评估单分类器（如随机森林、LightGBM等）、不同采样技术（如过采样、欠采样等）以及Self-Paced Ensembling及其基础分类器数量。

Result: 不平衡学习技术效果不一，需谨慎使用；不同数据集的最佳分类器和采样技术不同。

Conclusion: 建议针对每个新数据集和应用测试单分类器和不平衡学习技术，特别是在网络安全应用中。

Abstract: Cybersecurity has become essential worldwide and at all levels, concerning
individuals, institutions, and governments. A basic principle in cybersecurity
is to be always alert. Therefore, automation is imperative in processes where
the volume of daily operations is large. Several cybersecurity applications can
be addressed as binary classification problems, including anomaly detection,
fraud detection, intrusion detection, spam detection, or malware detection. We
present three experiments. In the first experiment, we evaluate single
classifiers including Random Forests, Light Gradient Boosting Machine, eXtreme
Gradient Boosting, Logistic Regression, Decision Tree, and Gradient Boosting
Decision Tree. In the second experiment, we test different sampling techniques
including over-sampling, under-sampling, Synthetic Minority Over-sampling
Technique, and Self-Paced Ensembling. In the last experiment, we evaluate
Self-Paced Ensembling and its number of base classifiers. We found that
imbalance learning techniques had positive and negative effects, as reported in
related studies. Thus, these techniques should be applied with caution.
Besides, we found different best performers for each dataset. Therefore, we
recommend testing single classifiers and imbalance learning techniques for each
new dataset and application involving imbalanced datasets as is the case in
several cyber security applications.

</details>


### [107] [FRAIN to Train: A Fast-and-Reliable Solution for Decentralized Federated Learning](https://arxiv.org/abs/2505.04223)
*Sanghyeon Park,Soo-Mook Moon*

Main category: cs.LG

TL;DR: FRAIN是一种新的异步联邦学习方法，通过FastSync策略和SLERP参数合并技术，解决了传统联邦学习中的客户端漂移和同步开销问题，并在非独立同分布数据、延迟网络和恶意节点环境下表现更优。


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习方法（如FedAvg和FedAsync）在非独立同分布数据、延迟设备和恶意节点等环境下存在客户端漂移和同步开销问题，FRAIN旨在解决这些局限性。

Method: FRAIN采用FastSync策略避免重放旧模型版本，并使用球形线性插值（SLERP）合并参数以减少干扰。

Result: 实验表明，FRAIN在CNN图像分类和Transformer语言模型任务中，比FedAvg、FedAsync和BRAIN表现更稳定和鲁棒。

Conclusion: FRAIN通过创新的同步和参数合并技术，显著提升了异步联邦学习的性能和鲁棒性。

Abstract: Federated learning (FL) enables collaborative model training across
distributed clients while preserving data locality. Although FedAvg pioneered
synchronous rounds for global model averaging, slower devices can delay
collective progress. Asynchronous FL (e.g., FedAsync) addresses stragglers by
continuously integrating client updates, yet naive implementations risk client
drift due to non-IID data and stale contributions. Some Blockchain-based FL
approaches (e.g., BRAIN) employ robust weighting or scoring of updates to
resist malicious or misaligned proposals. However, performance drops can still
persist under severe data heterogeneity or high staleness, and synchronization
overhead has emerged as a new concern due to its aggregator-free architectures.
  We introduce Fast-and-Reliable AI Network, FRAIN, a new asynchronous FL
method that mitigates these limitations by incorporating two key ideas. First,
our FastSync strategy eliminates the need to replay past model versions,
enabling newcomers and infrequent participants to efficiently approximate the
global model. Second, we adopt spherical linear interpolation (SLERP) when
merging parameters, preserving models' directions and alleviating destructive
interference from divergent local training.
  Experiments with a CNN image-classification model and a Transformer-based
language model demonstrate that FRAIN achieves more stable and robust
convergence than FedAvg, FedAsync, and BRAIN, especially under harsh
environments: non-IID data distributions, networks that experience delays and
require frequent re-synchronization, and the presence of malicious nodes.

</details>


### [108] [Technology prediction of a 3D model using Neural Network](https://arxiv.org/abs/2505.04241)
*Grzegorz Miebs,Rafał A. Bachorz*

Main category: cs.LG

TL;DR: 提出了一种基于3D模型的数据驱动方法，通过神经网络预测制造步骤及其时间，适用于动态和定制化生产环境。


<details>
  <summary>Details</summary>
Motivation: 传统依赖专家分析或历史数据的方法在动态或定制化生产环境中效果不佳，需要更准确的生产时间估计方法。

Method: 将3D模型渲染为多张2D图像，利用受生成查询网络启发的神经网络，将几何特征映射为预定义生产步骤的时间估计。

Result: 该方法能够实现跨多种产品类型的可扩展、自适应且精确的流程规划。

Conclusion: 数据驱动方法为动态和定制化生产环境提供了更高效的生产时间估计解决方案。

Abstract: Accurate estimation of production times is critical for effective
manufacturing scheduling, yet traditional methods relying on expert analysis or
historical data often fall short in dynamic or customized production
environments. This paper introduces a data-driven approach that predicts
manufacturing steps and their durations directly from a product's 3D model. By
rendering the model into multiple 2D images and leveraging a neural network
inspired by the Generative Query Network, the method learns to map geometric
features into time estimates for predefined production steps enabling scalable,
adaptive, and precise process planning across varied product types.

</details>


### [109] [Physics-Informed DeepONets for drift-diffusion on metric graphs: simulation and parameter identification](https://arxiv.org/abs/2505.04263)
*Jan Blechschmidt,Tom-Christian Riemer,Max Winkler,Martin Stoll,Jan-F. Pietschmann*

Main category: cs.LG

TL;DR: 提出了一种基于物理信息的深度学习方法，用于解决度量图上的非线性漂移扩散方程，适用于优化和反问题。


<details>
  <summary>Details</summary>
Motivation: 传统的数值方法在模型设计或参数识别问题中需要大量定制，而物理信息的深度算子网络（DeepONet）能更灵活地解决偏微分方程，并易于处理参数识别问题。

Method: 首先学习三个DeepONet模型（代表流入、内部和流出边），然后通过基于边的域分解方法耦合这些模型来解决漂移扩散度量图问题。

Result: 框架能准确评估图耦合物理模型，并适用于解决这些耦合网络的优化或反问题。

Conclusion: 该方法为度量图上的非线性漂移扩散方程提供了一种高效且通用的解决方案。

Abstract: We develop a novel physics informed deep learning approach for solving
nonlinear drift-diffusion equations on metric graphs. These models represent an
important model class with a large number of applications in areas ranging from
transport in biological cells to the motion of human crowds. While traditional
numerical schemes require a large amount of tailoring, especially in the case
of model design or parameter identification problems, physics informed deep
operator networks (DeepONet) have emerged as a versatile tool for the solution
of partial differential equations with the particular advantage that they
easily incorporate parameter identification questions. We here present an
approach where we first learn three DeepONet models for representative inflow,
inner and outflow edges, resp., and then subsequently couple these models for
the solution of the drift-diffusion metric graph problem by relying on an
edge-based domain decomposition approach. We illustrate that our framework is
applicable for the accurate evaluation of graph-coupled physics models and is
well suited for solving optimization or inverse problems on these coupled
networks.

</details>


### [110] [Non-stationary Diffusion For Probabilistic Time Series Forecasting](https://arxiv.org/abs/2505.04278)
*Weiwei Ye,Zhuopeng Xu,Ning Gui*

Main category: cs.LG

TL;DR: NsDiff是一种基于位置-尺度噪声模型（LSNM）的非平稳扩散模型，用于捕捉时间序列中随时间变化的噪声不确定性，优于传统DDPM。


<details>
  <summary>Details</summary>
Motivation: 传统DDPM因假设噪声方差恒定而无法捕捉时间序列中的非平稳不确定性，本文旨在解决这一问题。

Method: 利用LSNM放松固定噪声假设，结合去噪扩散条件生成模型和预训练的条件均值与方差估计器，提出动态调整噪声水平的不确定性感知噪声调度。

Result: 在九个真实和合成数据集上的实验表明，NsDiff优于现有方法。

Conclusion: NsDiff通过动态建模不确定性，显著提升了时间序列预测的准确性。

Abstract: Due to the dynamics of underlying physics and external influences, the
uncertainty of time series often varies over time. However, existing Denoising
Diffusion Probabilistic Models (DDPMs) often fail to capture this
non-stationary nature, constrained by their constant variance assumption from
the additive noise model (ANM). In this paper, we innovatively utilize the
Location-Scale Noise Model (LSNM) to relax the fixed uncertainty assumption of
ANM. A diffusion-based probabilistic forecasting framework, termed
Non-stationary Diffusion (NsDiff), is designed based on LSNM that is capable of
modeling the changing pattern of uncertainty. Specifically, NsDiff combines a
denoising diffusion-based conditional generative model with a pre-trained
conditional mean and variance estimator, enabling adaptive endpoint
distribution modeling. Furthermore, we propose an uncertainty-aware noise
schedule, which dynamically adjusts the noise levels to accurately reflect the
data uncertainty at each step and integrates the time-varying variances into
the diffusion process. Extensive experiments conducted on nine real-world and
synthetic datasets demonstrate the superior performance of NsDiff compared to
existing approaches. Code is available at https://github.com/wwy155/NsDiff.

</details>


### [111] [Detecting Concept Drift in Neural Networks Using Chi-squared Goodness of Fit Testing](https://arxiv.org/abs/2505.04318)
*Jacob Glenn Ayers,Buvaneswari A. Ramanan,Manzoor A. Khan*

Main category: cs.LG

TL;DR: 论文提出了一种基于χ²拟合优度假设检验的概念漂移检测方法，用于监控深度学习模型在推理过程中遇到的数据分布变化。


<details>
  <summary>Details</summary>
Motivation: 随着深度学习模型的广泛应用，人工验证变得不可行，需要元算法确保模型推理的可靠性。概念漂移检测领域在监控神经网络中未充分利用，尤其是在推理数据与训练数据分布不一致时。

Method: 应用χ²拟合优度假设检验作为漂移检测元算法，测试多层感知机、卷积神经网络和Transformer在模拟漂移下的表现。

Result: 研究表明，无需直接检查推理输出，即可检测因概念漂移导致的准确性下降。

Conclusion: 该方法通过持续评估模型在不同条件下的可靠性，增强了安全性。

Abstract: As the adoption of deep learning models has grown beyond human capacity for
verification, meta-algorithms are needed to ensure reliable model inference.
Concept drift detection is a field dedicated to identifying statistical shifts
that is underutilized in monitoring neural networks that may encounter
inference data with distributional characteristics diverging from their
training data. Given the wide variety of model architectures, applications, and
datasets, it is important that concept drift detection algorithms are adaptable
to different inference scenarios. In this paper, we introduce an application of
the $\chi^2$ Goodness of Fit Hypothesis Test as a drift detection
meta-algorithm applied to a multilayer perceptron, a convolutional neural
network, and a transformer trained for machine vision as they are exposed to
simulated drift during inference. To that end, we demonstrate how unexpected
drops in accuracy due to concept drift can be detected without directly
examining the inference outputs. Our approach enhances safety by ensuring
models are continually evaluated for reliability across varying conditions.

</details>


### [112] [Hyperbolic Fuzzy $C$-Means with Adaptive Weight-based Filtering for Clustering in Non-Euclidean Spaces](https://arxiv.org/abs/2505.04335)
*Swagato Das,Arghya Pratihar,Swagatam Das*

Main category: cs.LG

TL;DR: 论文提出了一种基于双曲几何的新型模糊聚类算法HypeFCM，用于解决传统模糊聚类在非欧几里得空间中的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统模糊聚类方法（如FCM）在非欧几里得空间中表现不佳，无法有效捕捉复杂或层次化的数据结构。

Method: HypeFCM结合模糊聚类与双曲几何（Poincaré Disc模型），通过基于Dirichlet分布的权重初始化和双曲度量优化聚类中心和成员分配。

Result: 实验表明，HypeFCM在非欧几里得空间中显著优于传统模糊聚类方法。

Conclusion: HypeFCM是一种高效且鲁棒的算法，适用于非欧几里得空间的模糊聚类任务。

Abstract: Clustering algorithms play a pivotal role in unsupervised learning by
identifying and grouping similar objects based on shared characteristics. While
traditional clustering techniques, such as hard and fuzzy center-based
clustering, have been widely used, they struggle with complex,
high-dimensional, and non-Euclidean datasets. In particular, the Fuzzy
$C$-Means (FCM) algorithm, despite its efficiency and popularity, exhibits
notable limitations in non-Euclidean spaces. Euclidean spaces assume linear
separability and uniform distance scaling, limiting their effectiveness in
capturing complex, hierarchical, or non-Euclidean structures in fuzzy
clustering. To overcome these challenges, we introduce Filtration-based
Hyperbolic Fuzzy $C$-Means (HypeFCM), a novel clustering algorithm tailored for
better representation of data relationships in non-Euclidean spaces. HypeFCM
integrates the principles of fuzzy clustering with hyperbolic geometry and
employs a weight-based filtering mechanism to improve performance. The
algorithm initializes weights using a Dirichlet distribution and iteratively
refines cluster centroids and membership assignments based on a hyperbolic
metric in the Poincar\'e Disc model. Extensive experimental evaluations
demonstrate that HypeFCM significantly outperforms conventional fuzzy
clustering methods in non-Euclidean settings, underscoring its robustness and
effectiveness.

</details>


### [113] [Riemannian Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2505.04338)
*Zichen Liu,Wei Zhang,Christof Schütte,Tiejun Li*

Main category: cs.LG

TL;DR: 论文提出了一种基于黎曼空间的去噪扩散概率模型（RDDPMs），用于学习欧几里得空间中子流形上的分布，适用于大多数应用相关的流形。该方法通过投影方案实现，仅需评估定义子流形的函数及其一阶导数，适用于更广泛的流形。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖于大量几何信息（如测地线或拉普拉斯-贝尔特拉米算子的特征函数），限制了其适用范围。RDDPMs旨在克服这一限制，适用于更一般的流形。

Method: 基于投影方案，仅需评估定义子流形的函数及其一阶导数，无需复杂几何信息。

Result: 理论分析表明RDDPMs与流形上的基于分数的生成模型存在联系。实验验证了该方法在多个数据集（包括高维流形如SO(10)和分子构型空间）上的有效性。

Conclusion: RDDPMs为学习子流形上的分布提供了一种通用且高效的方法，适用于更广泛的流形类型。

Abstract: We propose Riemannian Denoising Diffusion Probabilistic Models (RDDPMs) for
learning distributions on submanifolds of Euclidean space that are level sets
of functions, including most of the manifolds relevant to applications.
Existing methods for generative modeling on manifolds rely on substantial
geometric information such as geodesic curves or eigenfunctions of the
Laplace-Beltrami operator and, as a result, they are limited to manifolds where
such information is available. In contrast, our method, built on a projection
scheme, can be applied to more general manifolds, as it only requires being
able to evaluate the value and the first order derivatives of the function that
defines the submanifold. We provide a theoretical analysis of our method in the
continuous-time limit, which elucidates the connection between our RDDPMs and
score-based generative models on manifolds. The capability of our method is
demonstrated on datasets from previous studies and on new datasets sampled from
two high-dimensional manifolds, i.e. $\mathrm{SO}(10)$ and the configuration
space of molecular system alanine dipeptide with fixed dihedral angle.

</details>


### [114] [Adaptive and Robust DBSCAN with Multi-agent Reinforcement Learning](https://arxiv.org/abs/2505.04339)
*Hao Peng,Xiang Huang,Shuo Sun,Ruitong Zhang,Philip S. Yu*

Main category: cs.LG

TL;DR: 论文提出了一种基于多智能体强化学习的自适应鲁棒DBSCAN框架（AR-DBSCAN），通过密度分区和自动参数搜索，有效解决了传统DBSCAN在变密度数据集上的聚类问题。


<details>
  <summary>Details</summary>
Motivation: 传统DBSCAN在处理变密度数据集时效果不佳，需要手动调整参数，限制了其在实际应用中的表现。

Method: 1. 将数据集建模为两级编码树，按密度分区；2. 使用多智能体强化学习自动搜索最优聚类参数；3. 设计递归搜索机制以适应数据规模。

Result: 实验表明，AR-DBSCAN在NMI和ARI指标上分别提升了144.1%和175.3%，并能鲁棒地找到主导参数。

Conclusion: AR-DBSCAN显著提升了变密度数据集的聚类效果，具有实际应用潜力。

Abstract: DBSCAN, a well-known density-based clustering algorithm, has gained
widespread popularity and usage due to its effectiveness in identifying
clusters of arbitrary shapes and handling noisy data. However, it encounters
challenges in producing satisfactory cluster results when confronted with
datasets of varying density scales, a common scenario in real-world
applications. In this paper, we propose a novel Adaptive and Robust DBSCAN with
Multi-agent Reinforcement Learning cluster framework, namely AR-DBSCAN. First,
we model the initial dataset as a two-level encoding tree and categorize the
data vertices into distinct density partitions according to the information
uncertainty determined in the encoding tree. Each partition is then assigned to
an agent to find the best clustering parameters without manual assistance. The
allocation is density-adaptive, enabling AR-DBSCAN to effectively handle
diverse density distributions within the dataset by utilizing distinct agents
for different partitions. Second, a multi-agent deep reinforcement learning
guided automatic parameter searching process is designed. The process of
adjusting the parameter search direction by perceiving the clustering
environment is modeled as a Markov decision process. Using a weakly-supervised
reward training policy network, each agent adaptively learns the optimal
clustering parameters by interacting with the clusters. Third, a recursive
search mechanism adaptable to the data's scale is presented, enabling efficient
and controlled exploration of large parameter spaces. Extensive experiments are
conducted on nine artificial datasets and a real-world dataset. The results of
offline and online tasks show that AR-DBSCAN not only improves clustering
accuracy by up to 144.1% and 175.3% in the NMI and ARI metrics, respectively,
but also is capable of robustly finding dominant parameters.

</details>


### [115] [Multi-Granular Attention based Heterogeneous Hypergraph Neural Network](https://arxiv.org/abs/2505.04340)
*Hong Jin,Kaicheng Zhou,Jie Yin,Lan You,Zhifeng Zhou*

Main category: cs.LG

TL;DR: MGA-HHN提出了一种基于多粒度注意力的异质超图神经网络，通过构建元路径异质超图和多粒度注意力机制，解决了现有异质图神经网络在高阶关系捕获和信息失真上的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有异质图神经网络（HeteGNNs）因依赖元路径的成对特性而无法捕捉高阶关系，且长距离消息传递导致信息失真，限制了模型性能。

Method: MGA-HHN通过构建元路径异质超图和多粒度注意力机制（节点和超边级别），显式建模高阶语义信息并减少信息失真。

Result: 在真实基准数据集上，MGA-HHN在节点分类、聚类和可视化任务中优于现有模型。

Conclusion: MGA-HHN通过创新设计有效解决了异质图表示学习中的关键问题，显著提升了模型性能。

Abstract: Heterogeneous graph neural networks (HeteGNNs) have demonstrated strong
abilities to learn node representations by effectively extracting complex
structural and semantic information in heterogeneous graphs. Most of the
prevailing HeteGNNs follow the neighborhood aggregation paradigm, leveraging
meta-path based message passing to learn latent node representations. However,
due to the pairwise nature of meta-paths, these models fail to capture
high-order relations among nodes, resulting in suboptimal performance.
Additionally, the challenge of ``over-squashing'', where long-range message
passing in HeteGNNs leads to severe information distortion, further limits the
efficacy of these models. To address these limitations, this paper proposes
MGA-HHN, a Multi-Granular Attention based Heterogeneous Hypergraph Neural
Network for heterogeneous graph representation learning. MGA-HHN introduces two
key innovations: (1) a novel approach for constructing meta-path based
heterogeneous hypergraphs that explicitly models higher-order semantic
information in heterogeneous graphs through multiple views, and (2) a
multi-granular attention mechanism that operates at both the node and hyperedge
levels. This mechanism enables the model to capture fine-grained interactions
among nodes sharing the same semantic context within a hyperedge type, while
preserving the diversity of semantics across different hyperedge types. As
such, MGA-HHN effectively mitigates long-range message distortion and generates
more expressive node representations. Extensive experiments on real-world
benchmark datasets demonstrate that MGA-HHN outperforms state-of-the-art
models, showcasing its effectiveness in node classification, node clustering
and visualization tasks.

</details>


### [116] [Topology-Driven Clustering: Enhancing Performance with Betti Number Filtration](https://arxiv.org/abs/2505.04346)
*Arghya Pratihar,Kushal Bose,Swagatam Das*

Main category: cs.LG

TL;DR: 该论文提出了一种基于拓扑结构的新型聚类算法，通过利用Vietoris-Rips复形和Betti数过滤来识别拓扑相似的邻居，并引入Betti序列捕捉关键特征，以解决复杂数据集的聚类问题。


<details>
  <summary>Details</summary>
Motivation: 现有拓扑聚类算法未能充分利用拓扑结构，且在复杂数据集上表现不一致，因此需要一种更有效的方法。

Method: 使用Vietoris-Rips复形和Betti数过滤识别拓扑相似邻居，并引入Betti序列捕捉关键特征。

Result: 在合成和真实数据集上实验，算法表现优于其他基于拓扑的聚类算法。

Conclusion: 该算法能有效聚类复杂数据集，展示了拓扑特征在聚类中的潜力。

Abstract: Clustering aims to form groups of similar data points in an unsupervised
regime. Yet, clustering complex datasets containing critically intertwined
shapes poses significant challenges. The prevailing clustering algorithms
widely depend on evaluating similarity measures based on Euclidean metrics.
Exploring topological characteristics to perform clustering of complex datasets
inevitably presents a better scope. The topological clustering algorithms
predominantly perceive the point set through the lens of Simplicial complexes
and Persistent homology. Despite these approaches, the existing topological
clustering algorithms cannot somehow fully exploit topological structures and
show inconsistent performances on some highly complicated datasets. This work
aims to mitigate the limitations by identifying topologically similar neighbors
through the Vietoris-Rips complex and Betti number filtration. In addition, we
introduce the concept of the Betti sequences to capture flexibly essential
features from the topological structures. Our proposed algorithm is adept at
clustering complex, intertwined shapes contained in the datasets. We carried
out experiments on several synthetic and real-world datasets. Our algorithm
demonstrated commendable performances across the datasets compared to some of
the well-known topology-based clustering algorithms.

</details>


### [117] [Deep Learning Innovations for Energy Efficiency: Advances in Non-Intrusive Load Monitoring and EV Charging Optimization for a Sustainable Grid](https://arxiv.org/abs/2505.04367)
*Stavros Sykiotis*

Main category: cs.LG

TL;DR: 论文探讨了能源转型中可再生能源投资的复杂性及高碳排放能源淘汰的挑战，提出通过深度学习技术优化住宅能源消耗和电动汽车充电，以加速能源转型。


<details>
  <summary>Details</summary>
Motivation: 全球能源转型面临复杂性和速度不足的问题，需要探索新途径以减少能源需求和碳排放，特别是在住宅能源消耗和道路交通领域。

Method: 采用深度学习技术，开发非侵入式负载监测工具优化住宅能源使用，以及利用深度强化学习优化电动汽车充电。

Result: 通过深度学习技术，能够有效减少住宅能源消耗和优化电动汽车充电，从而降低碳排放。

Conclusion: 深度学习技术为能源转型提供了新的解决方案，特别是在住宅和交通领域，有望加速实现气候目标。

Abstract: The global energy landscape is undergoing a profound transformation, often
referred to as the energy transition, driven by the urgent need to mitigate
climate change, reduce greenhouse gas emissions, and ensure sustainable energy
supplies. However, the undoubted complexity of new investments in renewables,
as well as the phase out of high CO2-emission energy sources, hampers the pace
of the energy transition and raises doubts as to whether new renewable energy
sources are capable of solely meeting the climate target goals. This highlights
the need to investigate alternative pathways to accelerate the energy
transition, by identifying human activity domains with higher/excessive energy
demands. Two notable examples where there is room for improvement, in the sense
of reducing energy consumption and consequently CO2 emissions, are residential
energy consumption and road transport. This dissertation investigates the
development of novel Deep Learning techniques to create tools which solve
limitations in these two key energy domains. Reduction of residential energy
consumption can be achieved by empowering end-users with the user of
Non-Intrusive Load Monitoring, whereas optimization of EV charging with Deep
Reinforcement Learning can tackle road transport decarbonization.

</details>


### [118] [Extending a Quantum Reinforcement Learning Exploration Policy with Flags to Connect Four](https://arxiv.org/abs/2505.04371)
*Filipe Santos,João Paulo Fernandes,Luís Macedo*

Main category: cs.LG

TL;DR: 论文提出了一种基于标志的动作选择强化学习探索策略，并通过量子方法进一步优化，应用于Connect Four游戏，验证其性能。


<details>
  <summary>Details</summary>
Motivation: 研究标志探索策略在不同环境（Connect Four）中的表现，并探索量子方法在采样效率上的优势。

Method: 训练和测试经典与量子强化学习代理，分别作为先手和后手，对抗随机Negamax对手。

Result: 标志探索策略优于简单的epsilon-greedy策略，量子代理采样效率更高，但胜率与经典方法相同。

Conclusion: 标志探索策略有效，量子方法在采样效率上有优势，但胜率未提升，可能与训练场景简单有关。

Abstract: Action selection based on flags is a Reinforcement Learning (RL) exploration
policy that improves the exploration of the state space through the use of
flags, which can identify the most promising actions to take in each state. The
quantum counterpart of this exploration policy further improves upon this by
taking advantage of a quadratic speedup for sampling flagged actions. This
approach has already been successfully employed for the game of Checkers. In
this work, we describe the application of this method to the context of Connect
Four, in order to study its performance in a different setting, which can lead
to a better generalization of the technique. We also kept track of a metric
that wasn't taken into account in previous work: the average number of
iterations to obtain a flagged action. Since going second is a significant
disadvantage in Connect Four, we also had the intent of exploring how this more
complex scenario would impact the performance of our approach. The experiments
involved training and testing classical and quantum RL agents that played
either going first or going second against a Randomized Negamax opponent. The
results showed that both flagged exploration policies were clearly superior to
a simple epsilon-greedy policy. Furthermore, the quantum agents did in fact
sample flagged actions in less iterations. Despite obtaining tagged actions
more consistently, the win rates between the classical and quantum versions of
the approach were identical, which could be due to the simplicity of the
training scenario chosen.

</details>


### [119] [Clust-Splitter $-$ an Efficient Nonsmooth Optimization-Based Algorithm for Clustering Large Datasets](https://arxiv.org/abs/2505.04389)
*Jenni Lampainen,Kaisa Joki,Napsu Karmitsa,Marko M. Mäkelä*

Main category: cs.LG

TL;DR: Clust-Splitter是一种基于非光滑优化的高效算法，用于解决大规模数据集中的最小平方和聚类问题。


<details>
  <summary>Details</summary>
Motivation: 聚类是数据挖掘和机器学习中的基本任务，尤其是针对大规模数据分析的需求。

Method: 通过三个非光滑优化问题的序列（两个辅助问题生成初始点，一个主聚类问题），结合有限内存束方法和增量方法开发了Clust-Splitter算法。

Result: 在真实数据集上的实验表明，Clust-Splitter在聚类大规模数据时高效且解的质量高，与现有最佳方法相当。

Conclusion: Clust-Splitter是一种高效且高质量的大规模聚类算法。

Abstract: Clustering is a fundamental task in data mining and machine learning,
particularly for analyzing large-scale data. In this paper, we introduce
Clust-Splitter, an efficient algorithm based on nonsmooth optimization,
designed to solve the minimum sum-of-squares clustering problem in very large
datasets. The clustering task is approached through a sequence of three
nonsmooth optimization problems: two auxiliary problems used to generate
suitable starting points, followed by a main clustering formulation. To solve
these problems effectively, the limited memory bundle method is combined with
an incremental approach to develop the Clust-Splitter algorithm. We evaluate
Clust-Splitter on real-world datasets characterized by both a large number of
attributes and a large number of data points and compare its performance with
several state-of-the-art large-scale clustering algorithms. Experimental
results demonstrate the efficiency of the proposed method for clustering very
large datasets, as well as the high quality of its solutions, which are on par
with those of the best existing methods.

</details>


### [120] [Supporting renewable energy planning and operation with data-driven high-resolution ensemble weather forecast](https://arxiv.org/abs/2505.04396)
*Jingnan Wang,Jie Chao,Shangshang Yang,Congyi Nai,Kaijun Ren,Kefeng Deng,Xi Chen,Yaxin Liu,Hanqiuzi Wen,Ziniu Xiao,Lifeng Zhang,Xiaodong Wang,Jiping Guan,Baoxiang Pan*

Main category: cs.LG

TL;DR: 论文提出了一种基于高分辨率数值天气模拟的学习方法，结合粗网格天气预报，以低成本生成高精度、细粒度的风电场天气预测。


<details>
  <summary>Details</summary>
Motivation: 解决传统降尺度方法在风能规划中面临的尺度不一致、计算成本高和不确定性来源复杂的问题。

Method: 通过学习目标风电场的统计气候分布，结合粗网格预报，生成高分辨率天气预测。

Result: 验证了该方法在确定性和概率性预测技能及经济效益上的优势，计算成本显著降低。

Conclusion: 该方法为风能规划和运营提供了高效可靠的解决方案。

Abstract: The planning and operation of renewable energy, especially wind power, depend
crucially on accurate, timely, and high-resolution weather information.
Coarse-grid global numerical weather forecasts are typically downscaled to meet
these requirements, introducing challenges of scale inconsistency, process
representation error, computation cost, and entanglement of distinct
uncertainty sources from chaoticity, model bias, and large-scale forcing. We
address these challenges by learning the climatological distribution of a
target wind farm using its high-resolution numerical weather simulations. An
optimal combination of this learned high-resolution climatological prior with
coarse-grid large scale forecasts yields highly accurate, fine-grained,
full-variable, large ensemble of weather pattern forecasts. Using observed
meteorological records and wind turbine power outputs as references, the
proposed methodology verifies advantageously compared to existing
numerical/statistical forecasting-downscaling pipelines, regarding either
deterministic/probabilistic skills or economic gains. Moreover, a 100-member,
10-day forecast with spatial resolution of 1 km and output frequency of 15 min
takes < 1 hour on a moderate-end GPU, as contrast to $\mathcal{O}(10^3)$ CPU
hours for conventional numerical simulation. By drastically reducing
computational costs while maintaining accuracy, our method paves the way for
more efficient and reliable renewable energy planning and operation.

</details>


### [121] [Latent Manifold Reconstruction and Representation with Topological and Geometrical Regularization](https://arxiv.org/abs/2505.04412)
*Ren Wang,Pengcheng Zhou*

Main category: cs.LG

TL;DR: 提出一种基于AutoEncoder的方法，结合流形重建层，从噪声点云中提取潜在流形结构，并在降维过程中保持拓扑和几何特性，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在噪声数据中难以同时捕捉局部细节和全局拓扑完整性，导致降维结果失真或断裂。

Method: 集成流形重建层的AutoEncoder方法，通过正则化拓扑和几何特性，在训练中相互促进。

Result: 在点云数据集上优于t-SNE、UMAP和Topological AutoEncoders，通过可视化和定量指标验证。

Conclusion: 结合流形重建与流形学习对噪声数据中的潜在流形可靠表示至关重要。

Abstract: Manifold learning aims to discover and represent low-dimensional structures
underlying high-dimensional data while preserving critical topological and
geometric properties. Existing methods often fail to capture local details with
global topological integrity from noisy data or construct a balanced
dimensionality reduction, resulting in distorted or fractured embeddings. We
present an AutoEncoder-based method that integrates a manifold reconstruction
layer, which uncovers latent manifold structures from noisy point clouds, and
further provides regularizations on topological and geometric properties during
dimensionality reduction, whereas the two components promote each other during
training. Experiments on point cloud datasets demonstrate that our method
outperforms baselines like t-SNE, UMAP, and Topological AutoEncoders in
discovering manifold structures from noisy data and preserving them through
dimensionality reduction, as validated by visualization and quantitative
metrics. This work demonstrates the significance of combining manifold
reconstruction with manifold learning to achieve reliable representation of the
latent manifold, particularly when dealing with noisy real-world data. Code
repository: https://github.com/Thanatorika/mrtg.

</details>


### [122] [Localized Diffusion Models for High Dimensional Distributions Generation](https://arxiv.org/abs/2505.04417)
*Georg A. Gottwald,Shuigen Liu,Youssef Marzouk,Sebastian Reich,Xin T. Tong*

Main category: cs.LG

TL;DR: 扩散模型在生成任务中表现出色，但高维得分函数估计可能受维度灾难影响。本文提出利用局部结构降低维度，通过局部化得分匹配损失训练模型，平衡统计与局部化误差，提升性能。


<details>
  <summary>Details</summary>
Motivation: 高维得分函数估计可能导致维度灾难，因此需要利用目标分布的低维结构（如局部结构）来降低复杂度。

Method: 提出局部化扩散模型，使用局部化得分匹配损失在局部假设空间中训练得分函数，并通过理论证明其能规避维度灾难。

Result: 理论和数值实验表明，适中的局部化半径能平衡统计与局部化误差，提升性能，并支持并行训练。

Conclusion: 局部化结构能有效降低扩散模型的维度灾难问题，提升性能与效率，适用于大规模应用。

Abstract: Diffusion models are the state-of-the-art tools for various generative tasks.
However, estimating high-dimensional score functions makes them potentially
suffer from the curse of dimensionality (CoD). This underscores the importance
of better understanding and exploiting low-dimensional structure in the target
distribution. In this work, we consider locality structure, which describes
sparse dependencies between model components. Under locality structure, the
score function is effectively low-dimensional, so that it can be estimated by a
localized neural network with significantly reduced sample complexity. This
motivates the localized diffusion model, where a localized score matching loss
is used to train the score function within a localized hypothesis space. We
prove that such localization enables diffusion models to circumvent CoD, at the
price of additional localization error. Under realistic sample size scaling, we
show both theoretically and numerically that a moderate localization radius can
balance the statistical and localization error, leading to a better overall
performance. The localized structure also facilitates parallel training of
diffusion models, making it potentially more efficient for large-scale
applications.

</details>


### [123] [FedBWO: Enhancing Communication Efficiency in Federated Learning](https://arxiv.org/abs/2505.04435)
*Vahideh Hayyolalam,Öznur Özkasap*

Main category: cs.LG

TL;DR: FedBWO是一种联邦学习优化技术，通过传输性能评分而非模型权重减少通信量，显著提升全局模型性能和通信效率。


<details>
  <summary>Details</summary>
Motivation: 资源受限设备在联邦学习中传输大量模型权重导致通信瓶颈，需减少通信量以提升系统性能。

Method: 提出FedBWO技术，利用BWO算法优化本地模型更新，仅传输性能评分而非模型权重。

Result: 实验显示FedBWO比FedAvg和FedGWO分别提升21%和12%的全局模型准确率，并大幅降低通信成本。

Conclusion: FedBWO有效解决了联邦学习中的通信瓶颈问题，显著提升了模型性能和通信效率。

Abstract: Federated Learning (FL) is a distributed Machine Learning (ML) setup, where a
shared model is collaboratively trained by various clients using their local
datasets while keeping the data private. Considering resource-constrained
devices, FL clients often suffer from restricted transmission capacity. Aiming
to enhance the system performance, the communication between clients and server
needs to be diminished. Current FL strategies transmit a tremendous amount of
data (model weights) within the FL process, which needs a high communication
bandwidth. Considering resource constraints, increasing the number of clients
and, consequently, the amount of data (model weights) can lead to a bottleneck.
In this paper, we introduce the Federated Black Widow Optimization (FedBWO)
technique to decrease the amount of transmitted data by transmitting only a
performance score rather than the local model weights from clients. FedBWO
employs the BWO algorithm to improve local model updates. The conducted
experiments prove that FedBWO remarkably improves the performance of the global
model and the communication efficiency of the overall system. According to the
experimental outcomes, FedBWO enhances the global model accuracy by an average
of 21% over FedAvg, and 12% over FedGWO. Furthermore, FedBWO dramatically
decreases the communication cost compared to other methods.

</details>


### [124] [Towards Initialization-Agnostic Clustering with Iterative Adaptive Resonance Theory](https://arxiv.org/abs/2505.04440)
*Xiaozheng Qu,Zhaochuan Li,Zhuang Qi,Xiang Li,Haibei Huang,Lei Meng,Xiangxu Meng*

Main category: cs.LG

TL;DR: IR-ART通过动态稳定性检测、不稳定簇删除和警戒区域扩展三阶段迭代框架，提升了Fuzzy ART对警戒参数的鲁棒性，同时保持了算法的简洁性。


<details>
  <summary>Details</summary>
Motivation: Fuzzy ART的聚类性能高度依赖预设的警戒参数，现有方法虽通过自适应机制增强鲁棒性，但引入复杂框架或额外超参数，违背了算法初衷。

Method: 提出IR-ART，包含簇稳定性检测、不稳定簇删除和警戒区域扩展三阶段迭代框架，动态调整参数。

Result: 在15个数据集上验证，IR-ART提升了对次优警戒参数的容忍度，同时保持参数简洁性。

Conclusion: IR-ART特别适合资源受限场景下的非专家用户，通过迭代优化实现自优化能力。

Abstract: The clustering performance of Fuzzy Adaptive Resonance Theory (Fuzzy ART) is
highly dependent on the preset vigilance parameter, where deviations in its
value can lead to significant fluctuations in clustering results, severely
limiting its practicality for non-expert users. Existing approaches generally
enhance vigilance parameter robustness through adaptive mechanisms such as
particle swarm optimization and fuzzy logic rules. However, they often
introduce additional hyperparameters or complex frameworks that contradict the
original simplicity of the algorithm. To address this, we propose Iterative
Refinement Adaptive Resonance Theory (IR-ART), which integrates three key
phases into a unified iterative framework: (1) Cluster Stability Detection: A
dynamic stability detection module that identifies unstable clusters by
analyzing the change of sample size (number of samples in the cluster) in
iteration. (2) Unstable Cluster Deletion: An evolutionary pruning module that
eliminates low-quality clusters. (3) Vigilance Region Expansion: A vigilance
region expansion mechanism that adaptively adjusts similarity thresholds.
Independent of the specific execution of clustering, these three phases
sequentially focus on analyzing the implicit knowledge within the iterative
process, adjusting weights and vigilance parameters, thereby laying a
foundation for the next iteration. Experimental evaluation on 15 datasets
demonstrates that IR-ART improves tolerance to suboptimal vigilance parameter
values while preserving the parameter simplicity of Fuzzy ART. Case studies
visually confirm the algorithm's self-optimization capability through iterative
refinement, making it particularly suitable for non-expert users in
resource-constrained scenarios.

</details>


### [125] [Towards Effectively Leveraging Execution Traces for Program Repair with Code LLMs](https://arxiv.org/abs/2505.04441)
*Mirazul Haque,Petr Babkin,Farima Farmahinifarahani,Manuela Veloso*

Main category: cs.LG

TL;DR: 论文探讨了在大型语言模型（LLM）中结合程序执行轨迹以改进自动程序修复（APR）的效果，发现简单加入轨迹对性能提升有限，但优化提示策略能更一致地提升效果。


<details>
  <summary>Details</summary>
Motivation: 现有LLM-based APR方法主要依赖静态分析，忽略了运行时行为，希望通过加入执行轨迹弥补这一不足。

Method: 通过将程序执行轨迹融入标准APR提示中，使用GPT模型在三个APR数据集上评估效果。

Result: 执行轨迹仅在某些配置中带来有限改进，且复杂度增加时效果下降；优化提示策略能更稳定地超越无轨迹基线。

Conclusion: 执行轨迹可作为LLM推理能力的补充，优化提示策略比微调小模型更有效。

Abstract: Large Language Models (LLMs) show promising performance on various
programming tasks, including Automatic Program Repair (APR). However, most
approaches to LLM-based APR are limited to the static analysis of the programs,
while disregarding their runtime behavior. Inspired by knowledge-augmented NLP,
in this work, we aim to remedy this potential blind spot by augmenting standard
APR prompts with program execution traces. We evaluate our approach using the
GPT family of models on three popular APR datasets. Our findings suggest that
simply incorporating execution traces into the prompt provides a limited
performance improvement over trace-free baselines, in only 2 out of 6 tested
dataset / model configurations. We further find that the effectiveness of
execution traces for APR diminishes as their complexity increases. We explore
several strategies for leveraging traces in prompts and demonstrate that
LLM-optimized prompts help outperform trace-free prompts more consistently.
Additionally, we show trace-based prompting to be superior to finetuning a
smaller LLM on a small-scale dataset; and conduct probing studies reinforcing
the notion that execution traces can complement the reasoning abilities of the
LLMs.

</details>


### [126] [A Survey on Temporal Interaction Graph Representation Learning: Progress, Challenges, and Opportunities](https://arxiv.org/abs/2505.04461)
*Pengfei Jiao,Hongjiang Chen,Xuan Guo,Zhidong Zhao,Dongxiao He,Di Jin*

Main category: cs.LG

TL;DR: 论文探讨了时序交互图（TIGs）表示学习（TIGRL）的重要性，提出了一种分类方法，总结了数据集和基准，并讨论了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 时序交互图能建模复杂动态系统行为，其表示学习对下游任务（如分类、预测和聚类）至关重要。

Method: 提出了一种基于学习过程中信息类型的分类法，系统化现有TIGRL方法。

Result: 总结了数据集和基准资源，为实证研究提供支持。

Conclusion: 讨论了TIGRL领域的开放挑战和未来研究方向，推动该领域发展。

Abstract: Temporal interaction graphs (TIGs), defined by sequences of timestamped
interaction events, have become ubiquitous in real-world applications due to
their capability to model complex dynamic system behaviors. As a result,
temporal interaction graph representation learning (TIGRL) has garnered
significant attention in recent years. TIGRL aims to embed nodes in TIGs into
low-dimensional representations that effectively preserve both structural and
temporal information, thereby enhancing the performance of downstream tasks
such as classification, prediction, and clustering within constantly evolving
data environments. In this paper, we begin by introducing the foundational
concepts of TIGs and emphasize the critical role of temporal dependencies. We
then propose a comprehensive taxonomy of state-of-the-art TIGRL methods,
systematically categorizing them based on the types of information utilized
during the learning process to address the unique challenges inherent to TIGs.
To facilitate further research and practical applications, we curate the source
of datasets and benchmarks, providing valuable resources for empirical
investigations. Finally, we examine key open challenges and explore promising
research directions in TIGRL, laying the groundwork for future advancements
that have the potential to shape the evolution of this field.

</details>


### [127] [Discriminative Ordering Through Ensemble Consensus](https://arxiv.org/abs/2505.04464)
*Louis Ohl,Fredrik Lindsten*

Main category: cs.LG

TL;DR: 提出了一种基于共识聚类的评分方法，用于评估具有不同聚类定义的聚类模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有聚类评估指标难以处理多聚类模型和约束条件，因此需要一种更通用的方法。

Method: 通过构建基于聚类模型连通性与共识矩阵距离的判别性排序，实现模型评估。

Result: 在合成场景中验证了方法的有效性，并在实际数据中显著优于其他评分方法。

Conclusion: 该方法适用于不同聚类算法和约束条件，是一种通用的聚类评估工具。

Abstract: Evaluating the performance of clustering models is a challenging task where
the outcome depends on the definition of what constitutes a cluster. Due to
this design, current existing metrics rarely handle multiple clustering models
with diverse cluster definitions, nor do they comply with the integration of
constraints when available. In this work, we take inspiration from consensus
clustering and assume that a set of clustering models is able to uncover hidden
structures in the data. We propose to construct a discriminative ordering
through ensemble clustering based on the distance between the connectivity of a
clustering model and the consensus matrix. We first validate the proposed
method with synthetic scenarios, highlighting that the proposed score ranks the
models that best match the consensus first. We then show that this simple
ranking score significantly outperforms other scoring methods when comparing
sets of different clustering algorithms that are not restricted to a fixed
number of clusters and is compatible with clustering constraints.

</details>


### [128] [Spectral and Temporal Denoising for Differentially Private Optimization](https://arxiv.org/abs/2505.04468)
*Hyeju Shin,Kyudan Jung,Seongwon Yun,Juyoung Yun*

Main category: cs.LG

TL;DR: FFTKF是一种结合频域噪声整形和卡尔曼滤波的差分隐私优化方法，旨在提升DP-SGD的性能，同时保持隐私保证。


<details>
  <summary>Details</summary>
Motivation: 解决DP-SGD中因噪声添加导致模型性能下降的问题。

Method: 通过频域高频整形掩码集中噪声于低信息频谱，结合卡尔曼滤波优化梯度。

Result: 在多个数据集和模型上，FFTKF测试准确率优于DP-SGD和DiSK。

Conclusion: FFTKF在保持相同隐私保证的同时，通过减少噪声和控制偏差，实现了更优的隐私-效用权衡。

Abstract: This paper introduces the FFT-Enhanced Kalman Filter (FFTKF), a
differentially private optimization method that addresses the challenge of
preserving performance in DP-SGD, where added noise typically degrades model
utility. FFTKF integrates frequency-domain noise shaping with Kalman filtering
to enhance gradient quality while preserving $(\varepsilon, \delta)$-DP
guarantees. It employs a high-frequency shaping mask in the Fourier domain to
concentrate differential privacy noise in less informative spectral components,
preserving low-frequency gradient signals. A scalar-gain Kalman filter with
finite-difference Hessian approximation further refines the denoised gradients.
With a per-iteration complexity of $\mathcal{O}(d \log d)$, FFTKF demonstrates
improved test accuracy over DP-SGD and DiSK across MNIST, CIFAR-10, CIFAR-100,
and Tiny-ImageNet datasets using CNNs, Wide ResNets, and Vision Transformers.
Theoretical analysis confirms that FFTKF maintains equivalent privacy
guarantees while achieving a tighter privacy-utility trade-off through reduced
noise and controlled bias.

</details>


### [129] [Hamiltonian Normalizing Flows as kinetic PDE solvers: application to the 1D Vlasov-Poisson Equations](https://arxiv.org/abs/2505.04471)
*Vincent Souveton,Sébastien Terrana*

Main category: cs.LG

TL;DR: 提出了一种基于哈密顿动力学的新型方法，利用归一化流快速采样相空间分布，并学习可解释的物理势能。


<details>
  <summary>Details</summary>
Motivation: 由于Vlasov-Poisson方程等保守物理系统的复杂性，解析解难以获得，需要高效的数值方法。

Method: 采用哈密顿归一化流（Fixed-Kinetic Neural Hamiltonian Flows），通过可逆、保体积的变换将初始高斯分布转化为最终分布。

Result: 模型能够快速采样最终分布，并学习到可解释的势能，泛化至未见的中间状态。

Conclusion: 该方法为复杂物理系统提供了一种高效且可解释的数值模拟工具。

Abstract: Many conservative physical systems can be described using the Hamiltonian
formalism. A notable example is the Vlasov-Poisson equations, a set of partial
differential equations that govern the time evolution of a phase-space density
function representing collisionless particles under a self-consistent
potential. These equations play a central role in both plasma physics and
cosmology. Due to the complexity of the potential involved, analytical
solutions are rarely available, necessitating the use of numerical methods such
as Particle-In-Cell. In this work, we introduce a novel approach based on
Hamiltonian-informed Normalizing Flows, specifically a variant of Fixed-Kinetic
Neural Hamiltonian Flows. Our method transforms an initial Gaussian
distribution in phase space into the final distribution using a sequence of
invertible, volume-preserving transformations derived from Hamiltonian
dynamics. The model is trained on a dataset comprising initial and final states
at a fixed time T, generated via numerical simulations. After training, the
model enables fast sampling of the final distribution from any given initial
state. Moreover, by automatically learning an interpretable physical potential,
it can generalize to intermediate states not seen during training, offering
insights into the system's evolution across time.

</details>


### [130] [Communication-Efficient Federated Fine-Tuning of Language Models via Dynamic Update Schedules](https://arxiv.org/abs/2505.04535)
*Michail Theologitis,Vasilis Samoladas,Antonios Deligiannakis*

Main category: cs.LG

TL;DR: FDA-Opt算法家族结合了FDA和FedOpt的优点，解决了它们的核心限制，在联邦学习中显著提升了预训练语言模型的微调性能。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中参数通信频繁且固定，而现代预训练语言模型规模庞大，加剧了这一问题。现有算法（如FedOpt和FDA）存在局限性，需要更灵活的解决方案。

Method: 提出FDA-Opt算法家族，统一并扩展了FDA和FedOpt的原理，解决了它们的核心问题。

Result: 在多个下游NLP任务中，FDA-Opt表现优于FedOpt，且无需额外配置即可直接使用。

Conclusion: FDA-Opt是一种实用的联邦学习算法，可直接替代FedOpt，性能更优且无需额外调整。

Abstract: Federated learning (FL) makes it possible to train models on data that would
otherwise remain untapped and inaccessible. Simultaneously, pre-trained
language models (LMs) have emerged as indispensable tools in modern workflows.
These models exhibit extraordinary capabilities and are easily adapted to
downstream tasks. This opens one of the most exciting frontiers in FL:
fine-tuning LMs. However, a persistent challenge in FL is the frequent, rigid
communication of parameters, a problem which is magnified by the sheer size of
these modern models. Currently, the FedOpt family of algorithms is the
prevailing approach in FL, though it relies on fixed, heuristic intervals for
model synchronization. Recently, the FDA algorithm introduced a dynamic
alternative by monitoring training progress, but it came with its own
drawbacks; namely, a hard-to-tune threshold parameter and a rigid
synchronization scheme. In this work, we introduce the FDA-Opt family of
algorithms -- a unified generalization that extends the principles behind both
FDA and FedOpt, while resolving their core limitations. We evaluate our
approach on fine-tuning LMs across a range of downstream NLP tasks, and
demonstrate that it consistently outperforms FedOpt -- even when FDA-Opt
operates under hyper-parameter settings originally optimized for its
competitors. In other words, we show that FDA-Opt is a practical, drop-in
replacement for FedOpt in modern FL libraries and systems: it requires no
additional configuration and delivers superior performance out of the box.

</details>


### [131] [Purity Law for Generalizable Neural TSP Solvers](https://arxiv.org/abs/2505.04558)
*Wenzhao Liu,Haoran Li,Congying Han,Zicheng Zhang,Anqi Li,Tiande Guo*

Main category: cs.LG

TL;DR: 论文揭示了TSP问题中的Purity Law（PuLa），并提出了一种新的训练范式PUPO，显著提升了神经求解器的泛化性能。


<details>
  <summary>Details</summary>
Motivation: 解决神经网络在不同规模和分布的TSP问题中泛化能力不足的挑战。

Method: 通过统计验证PuLa，提出PUPO训练范式，将神经解的特性与PuLa对齐。

Result: PUPO显著提升了神经求解器的泛化性能，且不增加推理时的计算开销。

Conclusion: PuLa和PUPO为TSP问题的神经求解提供了新的理论基础和实用方法。

Abstract: Achieving generalization in neural approaches across different scales and
distributions remains a significant challenge for the Traveling Salesman
Problem~(TSP). A key obstacle is that neural networks often fail to learn
robust principles for identifying universal patterns and deriving optimal
solutions from diverse instances. In this paper, we first uncover Purity Law
(PuLa), a fundamental structural principle for optimal TSP solutions, defining
that edge prevalence grows exponentially with the sparsity of surrounding
vertices. Statistically validated across diverse instances, PuLa reveals a
consistent bias toward local sparsity in global optima. Building on this
insight, we propose Purity Policy Optimization~(PUPO), a novel training
paradigm that explicitly aligns characteristics of neural solutions with PuLa
during the solution construction process to enhance generalization. Extensive
experiments demonstrate that PUPO can be seamlessly integrated with popular
neural solvers, significantly enhancing their generalization performance
without incurring additional computational overhead during inference.

</details>


### [132] [ABKD: Pursuing a Proper Allocation of the Probability Mass in Knowledge Distillation via $α$-$β$-Divergence](https://arxiv.org/abs/2505.04560)
*Guanghui Wang,Zhiyong Yang,Zitai Wang,Shi Wang,Qianqian Xu,Qingming Huang*

Main category: cs.LG

TL;DR: ABKD提出了一种基于α-β散度的知识蒸馏框架，平衡了FKLD和RKLD的两种模式集中效应，实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 知识蒸馏中FKLD和RKLD分别存在模式集中效应过弱和过强的问题，导致学生模型无法有效学习教师模型的分布信息。

Method: 提出ABKD框架，使用α-β散度平滑插值FKLD和RKLD，平衡Hardness-Concentration和Confidence-Concentration效应。

Result: 在17个语言/视觉数据集和12种师生模型设置下验证了ABKD的有效性。

Conclusion: ABKD通过平衡两种模式集中效应，显著提升了知识蒸馏的效果。

Abstract: Knowledge Distillation (KD) transfers knowledge from a large teacher model to
a smaller student model by minimizing the divergence between their output
distributions, typically using forward Kullback-Leibler divergence (FKLD) or
reverse KLD (RKLD). It has become an effective training paradigm due to the
broader supervision information provided by the teacher distribution compared
to one-hot labels. We identify that the core challenge in KD lies in balancing
two mode-concentration effects: the \textbf{\textit{Hardness-Concentration}}
effect, which refers to focusing on modes with large errors, and the
\textbf{\textit{Confidence-Concentration}} effect, which refers to focusing on
modes with high student confidence. Through an analysis of how probabilities
are reassigned during gradient updates, we observe that these two effects are
entangled in FKLD and RKLD, but in extreme forms. Specifically, both are too
weak in FKLD, causing the student to fail to concentrate on the target class.
In contrast, both are too strong in RKLD, causing the student to overly
emphasize the target class while ignoring the broader distributional
information from the teacher. To address this imbalance, we propose ABKD, a
generic framework with $\alpha$-$\beta$-divergence. Our theoretical results
show that ABKD offers a smooth interpolation between FKLD and RKLD, achieving
an effective trade-off between these effects. Extensive experiments on 17
language/vision datasets with 12 teacher-student settings confirm its efficacy.
The code is available at https://github.com/ghwang-s/abkd.

</details>


### [133] [Multitask LSTM for Arboviral Outbreak Prediction Using Public Health Data](https://arxiv.org/abs/2505.04566)
*Lucas R. C. Farias,Talita P. Silva,Pedro H. M. Araujo*

Main category: cs.LG

TL;DR: 本文提出了一种基于LSTM的多任务学习方法，用于联合预测巴西累西腓的虫媒病毒爆发（登革热、基孔肯雅热和寨卡病毒）及其病例数。


<details>
  <summary>Details</summary>
Motivation: 通过统一建模策略，解决数据有限的公共卫生场景中虫媒病毒爆发的预测问题。

Method: 利用历史公共卫生数据（2017-2023），采用滑动窗口策略构建时间特征，并通过Keras Tuner进行超参数优化。

Result: 较长窗口提高了登革热回归准确性，而分类性能在中等窗口达到峰值。多任务架构在疾病和任务中表现优异。

Conclusion: 多任务学习方法在数据有限的公共卫生场景中具有可行性和优势，为可扩展的流行病预测提供了有效策略。

Abstract: This paper presents a multitask learning approach based on long-short-term
memory (LSTM) networks for the joint prediction of arboviral outbreaks and case
counts of dengue, chikungunya, and Zika in Recife, Brazil. Leveraging
historical public health data from DataSUS (2017-2023), the proposed model
concurrently performs binary classification (outbreak detection) and regression
(case forecasting) tasks. A sliding window strategy was adopted to construct
temporal features using varying input lengths (60, 90, and 120 days), with
hyperparameter optimization carried out using Keras Tuner. Model evaluation
used time series cross-validation for robustness and a held-out test from 2023
for generalization assessment. The results show that longer windows improve
dengue regression accuracy, while classification performance peaked at
intermediate windows, suggesting an optimal trade-off between sequence length
and generalization. The multitask architecture delivers competitive performance
across diseases and tasks, demonstrating the feasibility and advantages of
unified modeling strategies for scalable epidemic forecasting in data-limited
public health scenarios.

</details>


### [134] [Fight Fire with Fire: Defending Against Malicious RL Fine-Tuning via Reward Neutralization](https://arxiv.org/abs/2505.04578)
*Wenjun Cao*

Main category: cs.LG

TL;DR: 论文研究了强化学习（RL）微调对大型语言模型安全性的威胁，并提出了一种名为“奖励中和”的防御框架。


<details>
  <summary>Details</summary>
Motivation: RL微调可能被恶意利用，快速破坏模型的安全防护，现有防御方法对此无效。

Method: 提出“奖励中和”框架，通过训练模型生成最小信息拒绝响应，使恶意奖励信号失效。

Result: 实验证明该防御方法能有效保持低有害分数（≤2），而标准模型迅速恶化。

Conclusion: 研究首次证明针对RL攻击的稳健防御是可行的，填补了开源模型的关键安全漏洞。

Abstract: Reinforcement learning (RL) fine-tuning transforms large language models
while creating a vulnerability we experimentally verify: Our experiment shows
that malicious RL fine-tuning dismantles safety guardrails with remarkable
efficiency, requiring only 50 steps and minimal adversarial prompts, with
harmful escalating from 0-2 to 7-9. This attack vector particularly threatens
open-source models with parameter-level access. Existing defenses targeting
supervised fine-tuning prove ineffective against RL's dynamic feedback
mechanisms. We introduce Reward Neutralization, the first defense framework
specifically designed against RL fine-tuning attacks, establishing concise
rejection patterns that render malicious reward signals ineffective. Our
approach trains models to produce minimal-information rejections that attackers
cannot exploit, systematically neutralizing attempts to optimize toward harmful
outputs. Experiments validate that our approach maintains low harmful scores
(no greater than 2) after 200 attack steps, while standard models rapidly
deteriorate. This work provides the first constructive proof that robust
defense against increasingly accessible RL attacks is achievable, addressing a
critical security gap for open-weight models.

</details>


### [135] [Complexity Lower Bounds of Adaptive Gradient Algorithms for Non-convex Stochastic Optimization under Relaxed Smoothness](https://arxiv.org/abs/2505.04599)
*Michael Crawshaw,Mingrui Liu*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Recent results in non-convex stochastic optimization demonstrate the
convergence of popular adaptive algorithms (e.g., AdaGrad) under the $(L_0,
L_1)$-smoothness condition, but the rate of convergence is a higher-order
polynomial in terms of problem parameters like the smoothness constants. The
complexity guaranteed by such algorithms to find an $\epsilon$-stationary point
may be significantly larger than the optimal complexity of $\Theta \left(
\Delta L \sigma^2 \epsilon^{-4} \right)$ achieved by SGD in the $L$-smooth
setting, where $\Delta$ is the initial optimality gap, $\sigma^2$ is the
variance of stochastic gradient. However, it is currently not known whether
these higher-order dependencies can be tightened. To answer this question, we
investigate complexity lower bounds for several adaptive optimization
algorithms in the $(L_0, L_1)$-smooth setting, with a focus on the dependence
in terms of problem parameters $\Delta, L_0, L_1$. We provide complexity bounds
for three variations of AdaGrad, which show at least a quadratic dependence on
problem parameters $\Delta, L_0, L_1$. Notably, we show that the decorrelated
variant of AdaGrad-Norm requires at least $\Omega \left( \Delta^2 L_1^2
\sigma^2 \epsilon^{-4} \right)$ stochastic gradient queries to find an
$\epsilon$-stationary point. We also provide a lower bound for SGD with a broad
class of adaptive stepsizes. Our results show that, for certain adaptive
algorithms, the $(L_0, L_1)$-smooth setting is fundamentally more difficult
than the standard smooth setting, in terms of the initial optimality gap and
the smoothness constants.

</details>


### [136] [Testing Juntas Optimally with Samples](https://arxiv.org/abs/2505.04604)
*Lorenzo Beretta,Nathaniel Harms,Caleb Koch*

Main category: cs.LG

TL;DR: 论文证明了在无分布样本模型中，k-junta测试的样本数量上下界为Θ(1/ε(√(2^k logC(n,k)) + logC(n,k)))，并首次为布尔函数自然类提供了紧界。此外，该结果也适用于特征选择问题，表明junta测试必须学习相关变量集。对于容忍性junta测试，论文证明了样本下界为Ω(2^{(1-o(1))k} + logC(n,k))，显示容忍性测试与学习之间无显著差距。


<details>
  <summary>Details</summary>
Motivation: 研究在无分布样本模型中k-junta测试的样本复杂性，填补自然类布尔函数紧界的空白，并探讨容忍性测试与学习的关系。

Method: 通过数学证明，推导出k-junta测试的样本数量上下界，并分析容忍性测试的样本需求。

Result: 证明了k-junta测试的紧界Θ(1/ε(√(2^k logC(n,k)) + logC(n,k)))，以及容忍性测试的样本下界Ω(2^{(1-o(1))k} + logC(n,k))。

Conclusion: 论文首次为k-junta测试提供了紧界，并揭示了容忍性测试与学习之间的紧密联系，为相关领域提供了理论支持。

Abstract: We prove tight upper and lower bounds of
$\Theta\left(\tfrac{1}{\epsilon}\left( \sqrt{2^k \log\binom{n}{k} } +
\log\binom{n}{k} \right)\right)$ on the number of samples required for
distribution-free $k$-junta testing. This is the first tight bound for testing
a natural class of Boolean functions in the distribution-free sample-based
model. Our bounds also hold for the feature selection problem, showing that a
junta tester must learn the set of relevant variables. For tolerant junta
testing, we prove a sample lower bound of $\Omega(2^{(1-o(1)) k} +
\log\binom{n}{k})$ showing that, unlike standard testing, there is no large gap
between tolerant testing and learning.

</details>


### [137] [WATCH: Weighted Adaptive Testing for Changepoint Hypotheses via Weighted-Conformal Martingales](https://arxiv.org/abs/2505.04608)
*Drew Prinster,Xing Han,Anqi Liu,Suchi Saria*

Main category: cs.LG

TL;DR: 提出了一种加权广义共形测试鞅（WCTMs）方法，用于在线监测数据分布中的意外变化点，同时控制误报。


<details>
  <summary>Details</summary>
Motivation: 在高风险环境中负责任地部署AI/ML系统需要持续监测以快速检测和应对不安全行为。现有方法受限，无法适应在线变化或监测广泛的假设类别。

Method: 提出加权广义共形测试鞅（WCTMs），支持在线监测数据分布中的任何意外变化点，并控制误报。具体算法可适应轻度协变量偏移，同时对严重偏移（如概念偏移）发出警报。

Result: 在真实数据集上，WCTMs表现优于现有基线方法。

Conclusion: WCTMs为在线监测数据分布变化提供了理论基础和实用工具，尤其适用于高风险AI/ML系统部署。

Abstract: Responsibly deploying artificial intelligence (AI) / machine learning (ML)
systems in high-stakes settings arguably requires not only proof of system
reliability, but moreover continual, post-deployment monitoring to quickly
detect and address any unsafe behavior. Statistical methods for nonparametric
change-point detection -- especially the tools of conformal test martingales
(CTMs) and anytime-valid inference -- offer promising approaches to this
monitoring task. However, existing methods are restricted to monitoring limited
hypothesis classes or ``alarm criteria,'' such as data shifts that violate
certain exchangeability assumptions, or do not allow for online adaptation in
response to shifts. In this paper, we expand the scope of these monitoring
methods by proposing a weighted generalization of conformal test martingales
(WCTMs), which lay a theoretical foundation for online monitoring for any
unexpected changepoints in the data distribution while controlling
false-alarms. For practical applications, we propose specific WCTM algorithms
that accommodate online adaptation to mild covariate shifts (in the marginal
input distribution) while raising alarms in response to more severe shifts,
such as concept shifts (in the conditional label distribution) or extreme
(out-of-support) covariate shifts that cannot be easily adapted to. On
real-world datasets, we demonstrate improved performance relative to
state-of-the-art baselines.

</details>


### [138] [Merging and Disentangling Views in Visual Reinforcement Learning for Robotic Manipulation](https://arxiv.org/abs/2505.04619)
*Abdulaziz Almuzairee,Rohan Patil,Dwait Bhatt,Henrik I. Christensen*

Main category: cs.LG

TL;DR: 论文提出了一种名为MAD的算法，通过合并多视角数据并增强单视角特征，提高了样本效率并实现了轻量级部署。


<details>
  <summary>Details</summary>
Motivation: 为了提高视觉伺服操作的鲁棒性，通常需要多摄像头扩展视野，但计算成本高。

Method: 提出Merge And Disentanglement (MAD)算法，合并多视角数据并增强单视角特征。

Result: 在Meta-World和ManiSkill3上验证了算法的效率和鲁棒性。

Conclusion: MAD算法在提高样本效率的同时，实现了轻量级部署和鲁棒策略。

Abstract: Vision is well-known for its use in manipulation, especially using visual
servoing. To make it robust, multiple cameras are needed to expand the field of
view. That is computationally challenging. Merging multiple views and using
Q-learning allows the design of more effective representations and optimization
of sample efficiency. Such a solution might be expensive to deploy. To mitigate
this, we introduce a Merge And Disentanglement (MAD) algorithm that efficiently
merges views to increase sample efficiency while augmenting with single-view
features to allow lightweight deployment and ensure robust policies. We
demonstrate the efficiency and robustness of our approach using Meta-World and
ManiSkill3. For project website and code, see https://aalmuzairee.github.io/mad

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [139] [An Empirical Study of OpenAI API Discussions on Stack Overflow](https://arxiv.org/abs/2505.04084)
*Xiang Chen,Jibin Wang,Chaoyang Gao,Xiaolin Ju,Zhanqi Cui*

Main category: cs.SE

TL;DR: 本文首次通过分析Stack Overflow上2,874条OpenAI API相关讨论，研究了开发者使用OpenAI API时遇到的挑战，并提出了实用建议。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（如GPT系列）在多个领域影响深远，但OpenAI API带来的独特挑战（如提示工程、非确定性输出等）尚未被实证研究探讨。

Method: 通过分析Stack Overflow上的相关讨论，手动分类为九类，并进行主题建模分析。

Result: 识别了每类API使用中的具体挑战，并分析了帖子的流行度和难度。

Conclusion: 基于实证结果，为开发者、LLM供应商和研究者提出了实用建议。

Abstract: The rapid advancement of large language models (LLMs), represented by
OpenAI's GPT series, has significantly impacted various domains such as natural
language processing, software development, education, healthcare, finance, and
scientific research. However, OpenAI APIs introduce unique challenges that
differ from traditional APIs, such as the complexities of prompt engineering,
token-based cost management, non-deterministic outputs, and operation as black
boxes. To the best of our knowledge, the challenges developers encounter when
using OpenAI APIs have not been explored in previous empirical studies. To fill
this gap, we conduct the first comprehensive empirical study by analyzing 2,874
OpenAI API-related discussions from the popular Q&A forum Stack Overflow. We
first examine the popularity and difficulty of these posts. After manually
categorizing them into nine OpenAI API-related categories, we identify specific
challenges associated with each category through topic modeling analysis. Based
on our empirical findings, we finally propose actionable implications for
developers, LLM vendors, and researchers.

</details>


### [140] [Facilitating Trustworthy Human-Agent Collaboration in LLM-based Multi-Agent System oriented Software Engineering](https://arxiv.org/abs/2505.04251)
*Krishna Ronanki*

Main category: cs.SE

TL;DR: 本文提出了一种基于RACI的框架，用于解决LLM驱动的多智能体自主系统（LMA）在软件工程中任务分配的信任问题。


<details>
  <summary>Details</summary>
Motivation: 多智能体自主系统（MAS）在跨领域挑战中表现优于单一智能体，但LLM驱动的LMA系统在软件工程中引入信任问题。

Method: 提出RACI框架，提供实施指南和示例，以优化人机协作。

Result: 框架可促进高效协作、确保责任明确，并降低LLM自动化风险。

Conclusion: 未来计划通过实证验证进一步完善框架。

Abstract: Multi-agent autonomous systems (MAS) are better at addressing challenges that
spans across multiple domains than singular autonomous agents. This holds true
within the field of software engineering (SE) as well. The state-of-the-art
research on MAS within SE focuses on integrating LLMs at the core of autonomous
agents to create LLM-based multi-agent autonomous (LMA) systems. However, the
introduction of LMA systems into SE brings a plethora of challenges. One of the
major challenges is the strategic allocation of tasks between humans and the
LMA system in a trustworthy manner. To address this challenge, a RACI-based
framework is proposed in this work in progress article, along with
implementation guidelines and an example implementation of the framework. The
proposed framework can facilitate efficient collaboration, ensure
accountability, and mitigate potential risks associated with LLM-driven
automation while aligning with the Trustworthy AI guidelines. The future steps
for this work delineating the planned empirical validation method are also
presented.

</details>


<div id='math.HO'></div>

# math.HO [[Back]](#toc)

### [141] [The Evolution of Rough Sets 1970s-1981](https://arxiv.org/abs/2505.03747)
*Viktor Marek,Ewa Orłowska,Ivo Düntsch*

Main category: math.HO

TL;DR: 回顾Zdzisław Pawlak及其合作者在1970年代和1981年的研究与出版物，重点关注这些出版物中的灵感来源，并概述1981年与粗糙集和信息系统相关的发展。


<details>
  <summary>Details</summary>
Motivation: 回顾Pawlak及其合作者的早期工作，探索其灵感来源，并总结粗糙集和信息系统的后续发展。

Method: 通过分析Pawlak及其合作者在1970年代和1981年的出版物，识别灵感来源，并梳理1981年的相关进展。

Result: 明确了Pawlak工作的灵感来源，并总结了1981年粗糙集和信息系统领域的关键发展。

Conclusion: Pawlak的研究为粗糙集和信息系统奠定了基础，其灵感来源和早期发展为后续研究提供了重要参考。

Abstract: In this note research and publications by Zdzis{\l}aw Pawlak and his
collaborators from 1970s and 1981 are recalled. Focus is placed on the sources
of inspiration which one can identify on the basis of those publications.
Finally, developments from 1981 related to rough sets and information systems
are outlined.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [142] [Large Language Models are often politically extreme, usually ideologically inconsistent, and persuasive even in informational contexts](https://arxiv.org/abs/2505.04171)
*Nouar Aldahoul,Hazem Ibrahim,Matteo Varvello,Aaron Kaufman,Talal Rahwan,Yasir Zaki*

Main category: cs.CY

TL;DR: 研究发现，大语言模型（LLMs）的政治偏见并非如之前认为的那么小，而是通过特定话题的极端观点相互抵消表现出温和。实验表明，LLMs在政治说服力上具有显著影响，可能成为政治操纵的强大工具。


<details>
  <summary>Details</summary>
Motivation: 探讨LLMs的政治偏见及其对公众政治倾向的影响，挑战现有研究认为其偏见较小的观点。

Method: 比较31个LLMs与立法者、法官和美国选民的政治倾向，并通过随机实验测试LLMs对选民政治偏好的影响。

Result: LLMs的政治偏见是特定话题极端观点的综合结果，且能显著影响选民的政治倾向（最多增加5个百分点）。

Conclusion: LLMs可能成为政治操纵的有力工具，需警惕其潜在影响。

Abstract: Large Language Models (LLMs) are a transformational technology, fundamentally
changing how people obtain information and interact with the world. As people
become increasingly reliant on them for an enormous variety of tasks, a body of
academic research has developed to examine these models for inherent biases,
especially political biases, often finding them small. We challenge this
prevailing wisdom. First, by comparing 31 LLMs to legislators, judges, and a
nationally representative sample of U.S. voters, we show that LLMs' apparently
small overall partisan preference is the net result of offsetting extreme views
on specific topics, much like moderate voters. Second, in a randomized
experiment, we show that LLMs can promulgate their preferences into political
persuasiveness even in information-seeking contexts: voters randomized to
discuss political issues with an LLM chatbot are as much as 5 percentage points
more likely to express the same preferences as that chatbot. Contrary to
expectations, these persuasive effects are not moderated by familiarity with
LLMs, news consumption, or interest in politics. LLMs, especially those
controlled by private companies or governments, may become a powerful and
targeted vector for political influence.

</details>


### [143] [Deepfakes on Demand: the rise of accessible non-consensual deepfake image generators](https://arxiv.org/abs/2505.03859)
*Will Hawkins,Chris Russell,Brent Mittelstadt*

Main category: cs.CY

TL;DR: 研究发现，文本到图像（T2I）模型的普及导致深度伪造（deepfake）模型在线上的可访问性大幅增加，其中大部分针对女性并涉及非自愿亲密图像（NCII）。


<details>
  <summary>Details</summary>
Motivation: 探讨深度伪造模型在线上平台的普及情况及其潜在风险，尤其是针对个人的非自愿图像生成问题。

Method: 通过分析Hugging Face和Civitai两个平台上的数千个公开可下载模型变体的元数据，统计深度伪造模型的数量、下载量及目标对象。

Result: 发现近35,000个公开可下载的深度伪造模型变体，主要托管在Civitai上，下载量达1,500万次，96%针对女性，且许多涉及NCII。

Conclusion: 尽管平台政策和法规试图阻止，但深度伪造模型的广泛传播凸显了采取更强有力行动的必要性。

Abstract: Advances in multimodal machine learning have made text-to-image (T2I) models
increasingly accessible and popular. However, T2I models introduce risks such
as the generation of non-consensual depictions of identifiable individuals,
otherwise known as deepfakes. This paper presents an empirical study exploring
the accessibility of deepfake model variants online. Through a metadata
analysis of thousands of publicly downloadable model variants on two popular
repositories, Hugging Face and Civitai, we demonstrate a huge rise in easily
accessible deepfake models. Almost 35,000 examples of publicly downloadable
deepfake model variants are identified, primarily hosted on Civitai. These
deepfake models have been downloaded almost 15 million times since November
2022, with the models targeting a range of individuals from global celebrities
to Instagram users with under 10,000 followers. Both Stable Diffusion and Flux
models are used for the creation of deepfake models, with 96% of these
targeting women and many signalling intent to generate non-consensual intimate
imagery (NCII). Deepfake model variants are often created via the
parameter-efficient fine-tuning technique known as low rank adaptation (LoRA),
requiring as few as 20 images, 24GB VRAM, and 15 minutes of time, making this
process widely accessible via consumer-grade computers. Despite these models
violating the Terms of Service of hosting platforms, and regulation seeking to
prevent dissemination, these results emphasise the pressing need for greater
action to be taken against the creation of deepfakes and NCII.

</details>


### [144] [AI Governance to Avoid Extinction: The Strategic Landscape and Actionable Research Questions](https://arxiv.org/abs/2505.04592)
*Peter Barnett,Aaron Scher*

Main category: cs.CY

TL;DR: 论文探讨了AI快速发展可能带来的灾难性风险，并提出了四种应对场景，强调需要国际协调和治理研究以减少风险。


<details>
  <summary>Details</summary>
Motivation: AI可能超越人类智能并带来灾难性后果，包括人类灭绝。研究旨在描述AI发展的战略格局并提出治理问题。

Method: 分析了四种AI发展的地缘政治应对场景，重点关注技术、法律和制度基础设施的建设。

Result: 除“关闭开关”和“暂停”场景外，其他路径均存在不可接受的灾难风险。

Conclusion: 呼吁美国国家安全界和AI治理生态系统采取紧急行动，研究关键问题并准备国际AI协议。

Abstract: Humanity appears to be on course to soon develop AI systems that
substantially outperform human experts in all cognitive domains and activities.
We believe the default trajectory has a high likelihood of catastrophe,
including human extinction. Risks come from failure to control powerful AI
systems, misuse of AI by malicious rogue actors, war between great powers, and
authoritarian lock-in. This research agenda has two aims: to describe the
strategic landscape of AI development and to catalog important governance
research questions. These questions, if answered, would provide important
insight on how to successfully reduce catastrophic risks.
  We describe four high-level scenarios for the geopolitical response to
advanced AI development, cataloging the research questions most relevant to
each. Our favored scenario involves building the technical, legal, and
institutional infrastructure required to internationally restrict dangerous AI
development and deployment (which we refer to as an Off Switch), which leads
into an internationally coordinated Halt on frontier AI activities at some
point in the future. The second scenario we describe is a US National Project
for AI, in which the US Government races to develop advanced AI systems and
establish unilateral control over global AI development. We also describe two
additional scenarios: a Light-Touch world similar to that of today and a Threat
of Sabotage situation where countries use sabotage and deterrence to slow AI
development.
  In our view, apart from the Off Switch and Halt scenario, all of these
trajectories appear to carry an unacceptable risk of catastrophic harm. Urgent
action is needed from the US National Security community and AI governance
ecosystem to answer key research questions, build the capability to halt
dangerous AI activities, and prepare for international AI agreements.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [145] [Differentially Private Densest-$k$-Subgraph](https://arxiv.org/abs/2505.03858)
*Alireza Khayatian,Anil Vullikanti,Aritra Konar*

Main category: cs.DS

TL;DR: 本文首次提出了针对Densest-$k$-subgraph（D$k$S）问题的差分隐私（DP）算法，基于图邻接矩阵的主成分（PC）方法，并探讨了输出扰动、Propose-Test-Release（PTR）框架和私有幂方法（PPM）的隐私-效用权衡。


<details>
  <summary>Details</summary>
Motivation: 许多图数据集涉及敏感网络数据，因此需要隐私保护的图挖掘方法。D$k$S问题是图挖掘中的关键问题，但此前缺乏具有差分隐私保证的算法。

Method: 基于图邻接矩阵的主成分（PC）方法，提出了输出扰动、PTR框架和私有幂方法（PPM）三种隐私保护算法。PTR框架在计算效率上优于PPM。

Result: 在多个真实网络数据集上验证了算法的有效性，最大网络包含300万顶点。PTR在运行时间上比PPM快180倍，但需要稍大的隐私预算。

Conclusion: 本文提出的隐私保护算法在D$k$S问题上实现了良好的隐私-效用权衡，PTR框架在效率和实用性上表现突出。

Abstract: Many graph datasets involve sensitive network data, motivating the need for
privacy-preserving graph mining. The Densest-$k$-subgraph (D$k$S) problem is a
key primitive in graph mining that aims to extract a subset of $k$ vertices
with the maximum internal connectivity. Although non-private algorithms are
known for D$k$S, this paper is the first to design algorithms that offer formal
differential privacy (DP) guarantees for the problem. We base our general
approach on using the principal component (PC) of the graph adjacency matrix to
output a subset of $k$ vertices under edge DP. For this task, we first consider
output perturbation, which traditionally offer good scalability, but at the
expense of utility. Our tight on the local sensitivity indicate a big gap with
the global sensitivity, motivating the use of instance specific sensitive
methods for private PC. Next, we derive a tight bound on the smooth sensitivity
and show that it can be close to the global sensitivity. This leads us to
consider the Propose-Test-Release (PTR) framework for private PC. Although
computationally expensive in general, we design a novel approach for
implementing PTR in the same time as computation of a non-private PC, while
offering good utility for \DkS{}. Additionally, we also consider the iterative
private power method (PPM) for private PC, albeit it is significantly slower
than PTR on large networks. We run our methods on diverse real-world networks,
with the largest having 3 million vertices, and show good privacy-utility
trade-offs. Although PTR requires a slightly larger privacy budget, on average,
it achieves a 180-fold improvement in runtime over PPM.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [146] [VideoPath-LLaVA: Pathology Diagnostic Reasoning Through Video Instruction Tuning](https://arxiv.org/abs/2505.04192)
*Trinh T. L. Vuong,Jin Tae Kwak*

Main category: cs.CV

TL;DR: VideoPath-LLaVA是首个整合三种图像场景（单张切片图像、自动提取关键帧视频和手动分割病理视频）的多模态模型，模拟病理学家的诊断过程，生成详细描述并输出诊断结果。


<details>
  <summary>Details</summary>
Motivation: 通过结合视觉叙事与诊断推理，提升病理视频分析的准确性和实用性，为临床决策支持系统奠定基础。

Method: 利用VideoPath-Instruct数据集（4278对视频和诊断链式指令），通过知识迁移从单图像数据集训练，再微调于手动分割视频。

Result: VideoPath-LLaVA在病理视频分析中设立新基准，为未来AI系统提供视觉与诊断推理结合的框架。

Conclusion: 该模型为临床决策支持系统提供了潜力，代码、数据和模型已开源。

Abstract: We present VideoPath-LLaVA, the first large multimodal model (LMM) in
computational pathology that integrates three distinct image scenarios, single
patch images, automatically keyframe-extracted clips, and manually segmented
video pathology images, to mimic the natural diagnostic process of
pathologists. By generating detailed histological descriptions and culminating
in a definitive sign-out diagnosis, VideoPath-LLaVA bridges visual narratives
with diagnostic reasoning.
  Central to our approach is the VideoPath-Instruct dataset, comprising 4278
video and diagnosis-specific chain-of-thought instructional pairs sourced from
educational histopathology videos on YouTube. Although high-quality data is
critical for enhancing diagnostic reasoning, its creation is time-intensive and
limited in volume. To overcome this challenge, we transfer knowledge from
existing single-image instruction datasets to train on weakly annotated,
keyframe-extracted clips, followed by fine-tuning on manually segmented videos.
VideoPath-LLaVA establishes a new benchmark in pathology video analysis and
offers a promising foundation for future AI systems that support clinical
decision-making through integrated visual and diagnostic reasoning. Our code,
data, and model are publicly available at
https://github.com/trinhvg/VideoPath-LLaVA.

</details>


### [147] [Beyond Recognition: Evaluating Visual Perspective Taking in Vision Language Models](https://arxiv.org/abs/2505.03821)
*Gracjan Góral,Alicja Ziarko,Piotr Miłoś,Michał Nauman,Maciej Wołczyk,Michał Kosiński*

Main category: cs.CV

TL;DR: 研究探讨了视觉语言模型（VLMs）在视觉视角任务中的表现，发现其在场景理解上表现优异，但在空间推理和视角任务上表现显著下降。


<details>
  <summary>Details</summary>
Motivation: 评估VLMs在复杂视觉任务中的能力，尤其是视觉视角任务，以揭示其与人类认知的差距。

Method: 通过控制场景设计144个视觉任务，结合7个诊断问题，评估模型在场景理解、空间推理和视角任务上的表现。

Result: 模型在场景理解上表现优异，但在空间推理和视角任务上表现显著下降。

Conclusion: 未来VLM开发需整合几何表示和针对性训练协议，以提升复杂视觉任务能力。

Abstract: We investigate the ability of Vision Language Models (VLMs) to perform visual
perspective taking using a novel set of visual tasks inspired by established
human tests. Our approach leverages carefully controlled scenes, in which a
single humanoid minifigure is paired with a single object. By systematically
varying spatial configurations - such as object position relative to the
humanoid minifigure and the humanoid minifigure's orientation - and using both
bird's-eye and surface-level views, we created 144 unique visual tasks. Each
visual task is paired with a series of 7 diagnostic questions designed to
assess three levels of visual cognition: scene understanding, spatial
reasoning, and visual perspective taking. Our evaluation of several
state-of-the-art models, including GPT-4-Turbo, GPT-4o,
Llama-3.2-11B-Vision-Instruct, and variants of Claude Sonnet, reveals that
while they excel in scene understanding, the performance declines significantly
on spatial reasoning and further deteriorates on perspective-taking. Our
analysis suggests a gap between surface-level object recognition and the deeper
spatial and perspective reasoning required for complex visual tasks, pointing
to the need for integrating explicit geometric representations and tailored
training protocols in future VLM development.

</details>


### [148] [In-situ and Non-contact Etch Depth Prediction in Plasma Etching via Machine Learning (ANN & BNN) and Digital Image Colorimetry](https://arxiv.org/abs/2505.03826)
*Minji Kang,Seongho Kim,Eunseo Go,Donghyeon Paek,Geon Lim,Muyoung Kim,Soyeun Kim,Sung Kyu Jang,Min Sup Choi,Woo Seok Kang,Jaehyun Kim,Jaekwang Kim,Hyeong-U Kim*

Main category: cs.CV

TL;DR: 该论文提出了一种基于机器学习的非接触式原位蚀刻深度预测框架，用于半导体制造中的绝缘材料厚度监测。通过人工神经网络和贝叶斯神经网络，实现了高精度预测和不确定性估计，并验证了数字图像比色法（DIC）在无明确工艺参数情况下的可行性。


<details>
  <summary>Details</summary>
Motivation: 传统的外部分析方法存在时间延迟和污染风险，无法满足半导体制造中对蚀刻深度和绝缘材料厚度的实时监测需求。

Method: 研究采用人工神经网络（ANN）和贝叶斯神经网络（BNN）分别预测平均蚀刻深度和不确定性，并探索了数字图像比色法（DIC）作为输入数据的可行性。

Result: ANN在预测蚀刻深度时显著优于线性基线模型，BNN能可靠估计不确定性；DIC数据在无工艺参数时仍表现良好。

Conclusion: 结合DIC和机器学习的方法为等离子蚀刻过程提供了一种实时、原位、非侵入且经济高效的监测方案，有助于提升工艺稳定性和制造效率。

Abstract: Precise monitoring of etch depth and the thickness of insulating materials,
such as Silicon dioxide and silicon nitride, is critical to ensuring device
performance and yield in semiconductor manufacturing. While conventional
ex-situ analysis methods are accurate, they are constrained by time delays and
contamination risks. To address these limitations, this study proposes a
non-contact, in-situ etch depth prediction framework based on machine learning
(ML) techniques. Two scenarios are explored. In the first scenario, an
artificial neural network (ANN) is trained to predict average etch depth from
process parameters, achieving a significantly lower mean squared error (MSE)
compared to a linear baseline model. The approach is then extended to
incorporate variability from repeated measurements using a Bayesian Neural
Network (BNN) to capture both aleatoric and epistemic uncertainty. Coverage
analysis confirms the BNN's capability to provide reliable uncertainty
estimates. In the second scenario, we demonstrate the feasibility of using RGB
data from digital image colorimetry (DIC) as input for etch depth prediction,
achieving strong performance even in the absence of explicit process
parameters. These results suggest that the integration of DIC and ML offers a
viable, cost-effective alternative for real-time, in-situ, and non-invasive
monitoring in plasma etching processes, contributing to enhanced process
stability, and manufacturing efficiency.

</details>


### [149] [VideoLLM Benchmarks and Evaluation: A Survey](https://arxiv.org/abs/2505.03829)
*Yogesh Kumar*

Main category: cs.CV

TL;DR: 本文综述了视频大语言模型（VideoLLMs）的评测基准和方法，分析了现有视频理解基准的特点、评估协议及局限性，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLMs）的快速发展，视频理解技术取得显著进展，但缺乏系统性的评测框架。本文旨在填补这一空白，为研究者提供评测VideoLLMs的指导。

Method: 通过分析现有视频理解基准（如封闭集、开放集及时空任务专用评测），总结其特点与局限，并对比主流VideoLLMs的性能表现。

Result: 揭示了当前评测框架的关键挑战，如多样性不足、多模态评测缺乏等，并指出性能趋势。

Conclusion: 提出未来研究方向，包括设计更多样化、多模态及可解释性强的评测基准，以推动视频理解领域的发展。

Abstract: The rapid development of Large Language Models (LLMs) has catalyzed
significant advancements in video understanding technologies. This survey
provides a comprehensive analysis of benchmarks and evaluation methodologies
specifically designed or used for Video Large Language Models (VideoLLMs). We
examine the current landscape of video understanding benchmarks, discussing
their characteristics, evaluation protocols, and limitations. The paper
analyzes various evaluation methodologies, including closed-set, open-set, and
specialized evaluations for temporal and spatiotemporal understanding tasks. We
highlight the performance trends of state-of-the-art VideoLLMs across these
benchmarks and identify key challenges in current evaluation frameworks.
Additionally, we propose future research directions to enhance benchmark
design, evaluation metrics, and protocols, including the need for more diverse,
multimodal, and interpretability-focused benchmarks. This survey aims to equip
researchers with a structured understanding of how to effectively evaluate
VideoLLMs and identify promising avenues for advancing the field of video
understanding with large language models.

</details>


### [150] [Video Forgery Detection for Surveillance Cameras: A Review](https://arxiv.org/abs/2505.03832)
*Noor B. Tayfor,Tarik A. Rashid,Shko M. Qader,Bryar A. Hassan,Mohammed H. Abdalla,Jafar Majidpour,Aram M. Ahmed,Hussein M. Ali,Aso M. Aladdin,Abdulhady A. Abdullah,Ahmed S. Shamsaldin,Haval M. Sidqi,Abdulrahman Salih,Zaher M. Yaseen,Azad A. Ameen,Janmenjoy Nayak,Mahmood Yashar Hamza*

Main category: cs.CV

TL;DR: 本文综述了视频篡改检测的现有技术，强调其在确保监控视频真实性中的重要性。


<details>
  <summary>Details</summary>
Motivation: 随着视频编辑工具的普及，监控视频的篡改问题日益严重，可能误导司法决策，因此需要有效的检测技术。

Method: 探讨了压缩分析、帧复制检测和机器学习等多种视频取证技术。

Result: 研究发现现有技术需进一步强化以应对不断演变的篡改手段。

Conclusion: 加强视频取证能力对确保监控视频的可信度和法律证据效力至关重要。

Abstract: The widespread availability of video recording through smartphones and
digital devices has made video-based evidence more accessible than ever.
Surveillance footage plays a crucial role in security, law enforcement, and
judicial processes. However, with the rise of advanced video editing tools,
tampering with digital recordings has become increasingly easy, raising
concerns about their authenticity. Ensuring the integrity of surveillance
videos is essential, as manipulated footage can lead to misinformation and
undermine judicial decisions. This paper provides a comprehensive review of
existing forensic techniques used to detect video forgery, focusing on their
effectiveness in verifying the authenticity of surveillance recordings. Various
methods, including compression-based analysis, frame duplication detection, and
machine learning-based approaches, are explored. The findings highlight the
growing necessity for more robust forensic techniques to counteract evolving
forgery methods. Strengthening video forensic capabilities will ensure that
surveillance recordings remain credible and admissible as legal evidence.

</details>


### [151] [PointExplainer: Towards Transparent Parkinson's Disease Diagnosis](https://arxiv.org/abs/2505.03833)
*Xuechao Wang,Sven Nomm,Junqing Huang,Kadri Medijainen,Aaro Toomela,Michael Ruzhansky*

Main category: cs.CV

TL;DR: PointExplainer是一种可解释的诊断策略，用于识别手绘区域对模型诊断的影响，通过离散属性值和一致性度量提供直观解释，同时保持诊断性能。


<details>
  <summary>Details</summary>
Motivation: 现有诊断方法缺乏清晰的可解释性，影响了临床信任，因此需要一种能够明确量化手绘区域贡献的可解释诊断策略。

Method: PointExplainer包括诊断模块（将手绘信号编码为3D点云）和解释模块（训练可解释的替代模型近似黑盒模型行为），并引入一致性度量确保解释的忠实性。

Result: 在两个基准数据集和新构建的数据集上，PointExplainer能够提供直观的解释，且诊断性能未下降。

Conclusion: PointExplainer通过可解释的诊断策略解决了现有方法的可解释性问题，同时保持了诊断性能，具有临床应用潜力。

Abstract: Deep neural networks have shown potential in analyzing digitized hand-drawn
signals for early diagnosis of Parkinson's disease. However, the lack of clear
interpretability in existing diagnostic methods presents a challenge to
clinical trust. In this paper, we propose PointExplainer, an explainable
diagnostic strategy to identify hand-drawn regions that drive model diagnosis.
Specifically, PointExplainer assigns discrete attribution values to hand-drawn
segments, explicitly quantifying their relative contributions to the model's
decision. Its key components include: (i) a diagnosis module, which encodes
hand-drawn signals into 3D point clouds to represent hand-drawn trajectories,
and (ii) an explanation module, which trains an interpretable surrogate model
to approximate the local behavior of the black-box diagnostic model. We also
introduce consistency measures to further address the issue of faithfulness in
explanations. Extensive experiments on two benchmark datasets and a newly
constructed dataset show that PointExplainer can provide intuitive explanations
with no diagnostic performance degradation. The source code is available at
https://github.com/chaoxuewang/PointExplainer.

</details>


### [152] [Explainable Face Recognition via Improved Localization](https://arxiv.org/abs/2505.03837)
*Rashik Shadman,Daqing Hou,Faraz Hussain,M G Sarwar Murshed*

Main category: cs.CV

TL;DR: 论文提出了一种基于Scaled Directed Divergence (SDD)的可解释人脸识别方法，通过精细定位相关面部特征，提高系统的透明度和可信度。


<details>
  <summary>Details</summary>
Motivation: 当前基于深度学习的人脸识别系统缺乏解释性，用户难以信任其决策。

Method: 使用SDD Class Activation Mapping (CAM)技术，精细定位与模型决策相关的面部特征。

Result: 实验表明，SDD CAM比传统CAM更准确、更具体地突出相关面部特征。

Conclusion: SDD CAM提供的视觉解释增强了人脸识别系统的透明度和用户信任。

Abstract: Biometric authentication has become one of the most widely used tools in the
current technological era to authenticate users and to distinguish between
genuine users and imposters. Face is the most common form of biometric modality
that has proven effective. Deep learning-based face recognition systems are now
commonly used across different domains. However, these systems usually operate
like black-box models that do not provide necessary explanations or
justifications for their decisions. This is a major disadvantage because users
cannot trust such artificial intelligence-based biometric systems and may not
feel comfortable using them when clear explanations or justifications are not
provided. This paper addresses this problem by applying an efficient method for
explainable face recognition systems. We use a Class Activation Mapping
(CAM)-based discriminative localization (very narrow/specific localization)
technique called Scaled Directed Divergence (SDD) to visually explain the
results of deep learning-based face recognition systems. We perform fine
localization of the face features relevant to the deep learning model for its
prediction/decision. Our experiments show that the SDD Class Activation Map
(CAM) highlights the relevant face features very specifically compared to the
traditional CAM and very accurately. The provided visual explanations with
narrow localization of relevant features can ensure much-needed transparency
and trust for deep learning-based face recognition systems.

</details>


### [153] [GAME: Learning Multimodal Interactions via Graph Structures for Personality Trait Estimation](https://arxiv.org/abs/2505.03846)
*Kangsheng Wang,Yuhang Li,Chengwei Ye,Yufei Lin,Huanzhen Zhang,Bohan Hu,Linuo Xu,Shuyan Liu*

Main category: cs.CV

TL;DR: GAME是一种图增强多模态编码器，用于从短视频中预测人格特征，通过融合视觉、听觉和文本特征，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 短视频中的人格分析因多源特征的复杂交互而具有挑战性，需要一种鲁棒的多模态建模方法。

Method: GAME结合了面部图、双分支Geo Two-Stream网络、BiGRU、VGGish和XLM-Roberta，通过通道注意力融合模块整合多模态特征。

Result: GAME在多个基准测试中表现优于现有方法，验证了其有效性和泛化能力。

Conclusion: GAME为短视频人格分析提供了一种高效的多模态融合解决方案。

Abstract: Apparent personality analysis from short videos poses significant chal-lenges
due to the complex interplay of visual, auditory, and textual cues. In this
paper, we propose GAME, a Graph-Augmented Multimodal Encoder designed to
robustly model and fuse multi-source features for automatic personality
prediction. For the visual stream, we construct a facial graph and introduce a
dual-branch Geo Two-Stream Network, which combines Graph Convolutional Networks
(GCNs) and Convolutional Neural Net-works (CNNs) with attention mechanisms to
capture both structural and appearance-based facial cues. Complementing this,
global context and iden-tity features are extracted using pretrained ResNet18
and VGGFace back-bones. To capture temporal dynamics, frame-level features are
processed by a BiGRU enhanced with temporal attention modules. Meanwhile, audio
representations are derived from the VGGish network, and linguistic se-mantics
are captured via the XLM-Roberta transformer. To achieve effective multimodal
integration, we propose a Channel Attention-based Fusion module, followed by a
Multi-Layer Perceptron (MLP) regression head for predicting personality traits.
Extensive experiments show that GAME con-sistently outperforms existing methods
across multiple benchmarks, vali-dating its effectiveness and generalizability.

</details>


### [154] [Advanced Clustering Framework for Semiconductor Image Analytics Integrating Deep TDA with Self-Supervised and Transfer Learning Techniques](https://arxiv.org/abs/2505.03848)
*Janhavi Giri,Attila Lengyel,Don Kent,Edward Kibardin*

Main category: cs.CV

TL;DR: 论文提出了一种结合深度拓扑数据分析（TDA）、自监督学习和迁移学习的先进聚类框架，用于半导体制造中的图像数据聚类，解决了传统方法在高维无标签数据中的局限性。


<details>
  <summary>Details</summary>
Motivation: 半导体制造产生大量图像数据，传统聚类方法难以处理高维无标签数据，无法捕捉细微模式。

Method: 结合深度TDA、自监督学习和迁移学习，TDA提取拓扑特征，自监督学习从无标签数据中学习表示，迁移学习增强框架的适应性和可扩展性。

Result: 在合成和开源半导体图像数据集上验证，框架成功识别与缺陷模式和工艺变化相关的聚类。

Conclusion: 该框架为半导体制造等领域的主动过程监控和质量控制提供了可扩展的解决方案。

Abstract: Semiconductor manufacturing generates vast amounts of image data, crucial for
defect identification and yield optimization, yet often exceeds manual
inspection capabilities. Traditional clustering techniques struggle with
high-dimensional, unlabeled data, limiting their effectiveness in capturing
nuanced patterns. This paper introduces an advanced clustering framework that
integrates deep Topological Data Analysis (TDA) with self-supervised and
transfer learning techniques, offering a novel approach to unsupervised image
clustering. TDA captures intrinsic topological features, while self-supervised
learning extracts meaningful representations from unlabeled data, reducing
reliance on labeled datasets. Transfer learning enhances the framework's
adaptability and scalability, allowing fine-tuning to new datasets without
retraining from scratch. Validated on synthetic and open-source semiconductor
image datasets, the framework successfully identifies clusters aligned with
defect patterns and process variations. This study highlights the
transformative potential of combining TDA, self-supervised learning, and
transfer learning, providing a scalable solution for proactive process
monitoring and quality control in semiconductor manufacturing and other domains
with large-scale image datasets.

</details>


### [155] [An Active Inference Model of Covert and Overt Visual Attention](https://arxiv.org/abs/2505.03856)
*Tin Mišić,Karlo Koledić,Fabio Bonsignorio,Ivan Petrović,Ivan Marković*

Main category: cs.CV

TL;DR: 该论文提出了一种基于主动推理的视觉注意力模型，通过动态优化感官精度来最小化自由能，研究了外源性和内源性注意力的交互作用，并在Posner提示任务中验证了模型的有效性。


<details>
  <summary>Details</summary>
Motivation: 研究选择性注意力的机制，特别是在复杂感官输入下如何过滤干扰，为理解视觉注意力提供计算模型。

Method: 采用主动推理框架，动态优化感官精度，结合环境和感官输入确定注意力分配，并在Posner提示任务和简单目标聚焦任务中测试模型。

Result: 外源性和有效提示通常导致更快的反应时间；模型表现出类似抑制返回的行为；非自愿反射性眼动比有意眼动更快但适应性较差。

Conclusion: 该模型成功模拟了视觉注意力的关键特征，为理解注意力的计算机制提供了新视角。

Abstract: The ability to selectively attend to relevant stimuli while filtering out
distractions is essential for agents that process complex, high-dimensional
sensory input. This paper introduces a model of covert and overt visual
attention through the framework of active inference, utilizing dynamic
optimization of sensory precisions to minimize free-energy. The model
determines visual sensory precisions based on both current environmental
beliefs and sensory input, influencing attentional allocation in both covert
and overt modalities. To test the effectiveness of the model, we analyze its
behavior in the Posner cueing task and a simple target focus task using
two-dimensional(2D) visual data. Reaction times are measured to investigate the
interplay between exogenous and endogenous attention, as well as valid and
invalid cueing. The results show that exogenous and valid cues generally lead
to faster reaction times compared to endogenous and invalid cues. Furthermore,
the model exhibits behavior similar to inhibition of return, where previously
attended locations become suppressed after a specific cue-target onset
asynchrony interval. Lastly, we investigate different aspects of overt
attention and show that involuntary, reflexive saccades occur faster than
intentional ones, but at the expense of adaptability.

</details>


### [156] [Novel Extraction of Discriminative Fine-Grained Feature to Improve Retinal Vessel Segmentation](https://arxiv.org/abs/2505.03896)
*Shuang Zeng,Chee Hong Lee,Micky C Nnamdi,Wenqi Shi,J Ben Tamo,Lei Zhu,Hangzhou He,Xinliang Zhang,Qian Chen,May D. Wang,Yanye Lu,Qiushi Ren*

Main category: cs.CV

TL;DR: 提出了一种名为AttUKAN的新型注意力U形Kolmogorov-Arnold网络，结合标签引导的像素级对比损失，用于视网膜血管分割，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注解码器输出与标签的差异，而忽略了编码器中细粒度特征的充分利用，导致特征提取不够判别性。

Method: 在Kolmogorov-Arnold网络中引入注意力门机制，增强模型敏感性和可解释性；设计标签引导的像素级对比损失，区分前景和背景像素对。

Result: 在多个公开数据集上，AttUKAN的F1分数和MIoU分数均达到最高，优于11种现有网络。

Conclusion: AttUKAN在视网膜血管分割任务中表现优异，实现了最先进的性能。

Abstract: Retinal vessel segmentation is a vital early detection method for several
severe ocular diseases. Despite significant progress in retinal vessel
segmentation with the advancement of Neural Networks, there are still
challenges to overcome. Specifically, retinal vessel segmentation aims to
predict the class label for every pixel within a fundus image, with a primary
focus on intra-image discrimination, making it vital for models to extract more
discriminative features. Nevertheless, existing methods primarily focus on
minimizing the difference between the output from the decoder and the label,
but ignore fully using feature-level fine-grained representations from the
encoder. To address these issues, we propose a novel Attention U-shaped
Kolmogorov-Arnold Network named AttUKAN along with a novel Label-guided
Pixel-wise Contrastive Loss for retinal vessel segmentation. Specifically, we
implement Attention Gates into Kolmogorov-Arnold Networks to enhance model
sensitivity by suppressing irrelevant feature activations and model
interpretability by non-linear modeling of KAN blocks. Additionally, we also
design a novel Label-guided Pixel-wise Contrastive Loss to supervise our
proposed AttUKAN to extract more discriminative features by distinguishing
between foreground vessel-pixel pairs and background pairs. Experiments are
conducted across four public datasets including DRIVE, STARE, CHASE_DB1, HRF
and our private dataset. AttUKAN achieves F1 scores of 82.50%, 81.14%, 81.34%,
80.21% and 80.09%, along with MIoU scores of 70.24%, 68.64%, 68.59%, 67.21% and
66.94% in the above datasets, which are the highest compared to 11 networks for
retinal vessel segmentation. Quantitative and qualitative results show that our
AttUKAN achieves state-of-the-art performance and outperforms existing retinal
vessel segmentation methods. Our code will be available at
https://github.com/stevezs315/AttUKAN.

</details>


### [157] [Deep Learning Framework for Infrastructure Maintenance: Crack Detection and High-Resolution Imaging of Infrastructure Surfaces](https://arxiv.org/abs/2505.03974)
*Nikhil M. Pawar,Jorge A. Prozzi,Feng Hong,Surya Sarat Chandra Congress*

Main category: cs.CV

TL;DR: 该研究提出了一种结合CNN和ESPCNN的框架，用于高效超分辨率处理基础设施图像，减少计算成本和误报。


<details>
  <summary>Details</summary>
Motivation: 由于传感器特性、环境条件等因素，基础设施图像分辨率低，传统超分辨率技术会增加计算成本和误报。

Method: 使用CNN分类正负损伤图像，再用轻量级ESPCNN对正损伤图像进行超分辨率处理。

Result: ESPCNN在超分辨率评估指标上优于双三次插值，且能捕捉裂纹传播和复杂几何形状。

Conclusion: 该框架可帮助高速公路机构准确检测损伤并提升资产管理效率。

Abstract: Recently, there has been an impetus for the application of cutting-edge data
collection platforms such as drones mounted with camera sensors for
infrastructure asset management. However, the sensor characteristics, proximity
to the structure, hard-to-reach access, and environmental conditions often
limit the resolution of the datasets. A few studies used super-resolution
techniques to address the problem of low-resolution images. Nevertheless, these
techniques were observed to increase computational cost and false alarms of
distress detection due to the consideration of all the infrastructure images
i.e., positive and negative distress classes. In order to address the
pre-processing of false alarm and achieve efficient super-resolution, this
study developed a framework consisting of convolutional neural network (CNN)
and efficient sub-pixel convolutional neural network (ESPCNN). CNN accurately
classified both the classes. ESPCNN, which is the lightweight super-resolution
technique, generated high-resolution infrastructure image of positive distress
obtained from CNN. The ESPCNN outperformed bicubic interpolation in all the
evaluation metrics for super-resolution. Based on the performance metrics, the
combination of CNN and ESPCNN was observed to be effective in preprocessing the
infrastructure images with negative distress, reducing the computational cost
and false alarms in the next step of super-resolution. The visual inspection
showed that EPSCNN is able to capture crack propagation, complex geometry of
even minor cracks. The proposed framework is expected to help the highway
agencies in accurately performing distress detection and assist in efficient
asset management practices.

</details>


### [158] [R^3-VQA: "Read the Room" by Video Social Reasoning](https://arxiv.org/abs/2505.04147)
*Lixing Niu,Jiapeng Li,Xingping Yu,Shu Wang,Ruining Feng,Bo Wu,Ping Wei,Yisen Wang,Lifeng Fan*

Main category: cs.CV

TL;DR: 论文提出了一个名为R^3-VQA的高质量视频数据集，用于评估复杂社交场景中的社交推理能力，并发现现有大型视觉语言模型（LVLMs）在社交推理上仍远不及人类水平。


<details>
  <summary>Details</summary>
Motivation: 现有社交推理任务和数据集过于简单，无法反映真实社交互动的复杂性，因此需要更全面的数据集和任务来评估社交推理能力。

Method: 构建了R^3-VQA数据集，包含精细标注的社交事件、心理状态和社交因果链，并设计了三个任务：社交事件理解、心理状态估计和社交因果推理。

Result: 实验表明，现有LVLMs在复杂社交推理任务中表现不佳，但通过心理理论（ToM）提示可以提升其性能。

Conclusion: R^3-VQA为社交推理研究提供了新基准，并揭示了LVLMs与人类在社交推理能力上的差距。

Abstract: "Read the room" is a significant social reasoning capability in human daily
life. Humans can infer others' mental states from subtle social cues. Previous
social reasoning tasks and datasets lack complexity (e.g., simple scenes, basic
interactions, incomplete mental state variables, single-step reasoning, etc.)
and fall far short of the challenges present in real-life social interactions.
In this paper, we contribute a valuable, high-quality, and comprehensive video
dataset named R^3-VQA with precise and fine-grained annotations of social
events and mental states (i.e., belief, intent, desire, and emotion) as well as
corresponding social causal chains in complex social scenarios. Moreover, we
include human-annotated and model-generated QAs. Our task R^3-VQA includes
three aspects: Social Event Understanding, Mental State Estimation, and Social
Causal Reasoning. As a benchmark, we comprehensively evaluate the social
reasoning capabilities and consistencies of current state-of-the-art large
vision-language models (LVLMs). Comprehensive experiments show that (i) LVLMs
are still far from human-level consistent social reasoning in complex social
scenarios; (ii) Theory of Mind (ToM) prompting can help LVLMs perform better on
social reasoning tasks. We provide some of our dataset and codes in
supplementary material and will release our full dataset and codes upon
acceptance.

</details>


### [159] [DOTA: Deformable Optimized Transformer Architecture for End-to-End Text Recognition with Retrieval-Augmented Generation](https://arxiv.org/abs/2505.04175)
*Naphat Nithisopa,Teerapong Panboonyuen*

Main category: cs.CV

TL;DR: 本文提出了一种结合ResNet和Vision Transformer的新型端到端框架，通过Deformable Convolutions、Retrieval-Augmented Generation和CRF等方法提升OCR性能，在多个数据集上达到最优效果。


<details>
  <summary>Details</summary>
Motivation: 自然图像中的文本识别是一个重要但具有挑战性的任务，应用广泛。本文旨在通过创新方法提升OCR性能。

Method: 框架结合ResNet和Vision Transformer，使用Deformable Convolutions替换标准卷积层，引入自适应dropout和CRF进行序列建模。

Result: 在六个基准数据集上验证，平均准确率为77.77%，部分数据集上达到97.32%的高准确率。

Conclusion: 该方法在文本识别任务中实现了新的最优性能，展示了其鲁棒性和广泛适用性。

Abstract: Text recognition in natural images remains a challenging yet essential task,
with broad applications spanning computer vision and natural language
processing. This paper introduces a novel end-to-end framework that combines
ResNet and Vision Transformer backbones with advanced methodologies, including
Deformable Convolutions, Retrieval-Augmented Generation, and Conditional Random
Fields (CRF). These innovations collectively enhance feature representation and
improve Optical Character Recognition (OCR) performance. Specifically, the
framework substitutes standard convolution layers in the third and fourth
blocks with Deformable Convolutions, leverages adaptive dropout for
regularization, and incorporates CRF for more refined sequence modeling.
Extensive experiments conducted on six benchmark datasets IC13, IC15, SVT,
IIIT5K, SVTP, and CUTE80 validate the proposed method's efficacy, achieving
notable accuracies: 97.32% on IC13, 58.26% on IC15, 88.10% on SVT, 74.13% on
IIIT5K, 82.17% on SVTP, and 66.67% on CUTE80, resulting in an average accuracy
of 77.77%. These results establish a new state-of-the-art for text recognition,
demonstrating the robustness of the approach across diverse and challenging
datasets.

</details>


### [160] [S3D: Sketch-Driven 3D Model Generation](https://arxiv.org/abs/2505.04185)
*Hail Song,Wonsik Shin,Naeun Lee,Soomin Chung,Nojun Kwak,Woontack Woo*

Main category: cs.CV

TL;DR: S3D框架通过U-Net架构和风格对齐损失，将手绘草图转换为高质量3D模型。


<details>
  <summary>Details</summary>
Motivation: 解决2D草图因模糊和稀疏性导致生成高质量3D模型的挑战。

Method: 使用U-Net编码器-解码器架构生成面部分割掩码，并通过风格对齐损失和增强技术提升鲁棒性。

Result: S3D能够从草图输入生成高质量3D模型。

Conclusion: S3D框架在草图到3D模型转换中表现出高效性，代码已开源。

Abstract: Generating high-quality 3D models from 2D sketches is a challenging task due
to the inherent ambiguity and sparsity of sketch data. In this paper, we
present S3D, a novel framework that converts simple hand-drawn sketches into
detailed 3D models. Our method utilizes a U-Net-based encoder-decoder
architecture to convert sketches into face segmentation masks, which are then
used to generate a 3D representation that can be rendered from novel views. To
ensure robust consistency between the sketch domain and the 3D output, we
introduce a novel style-alignment loss that aligns the U-Net bottleneck
features with the initial encoder outputs of the 3D generation module,
significantly enhancing reconstruction fidelity. To further enhance the
network's robustness, we apply augmentation techniques to the sketch dataset.
This streamlined framework demonstrates the effectiveness of S3D in generating
high-quality 3D models from sketch inputs. The source code for this project is
publicly available at https://github.com/hailsong/S3D.

</details>


### [161] [An Enhanced YOLOv8 Model for Real-Time and Accurate Pothole Detection and Measurement](https://arxiv.org/abs/2505.04207)
*Mustafa Yurdakul,Şakir Tasdemir*

Main category: cs.CV

TL;DR: 论文提出了一种基于改进YOLOv8的模型，用于坑洞检测及其物理特征分析，通过RGB-D图像数据集（PothRGBD）提升了检测精度和性能。


<details>
  <summary>Details</summary>
Motivation: 坑洞会导致车辆损坏和交通事故，现有方法仅基于2D RGB图像，无法准确分析坑洞的物理特征，因此需要更精确的检测方法。

Method: 使用Intel RealSense D415深度相机采集RGB和深度数据，构建PothRGBD数据集；改进YOLOv8n-seg架构，引入DSConv、SimAM和GELU模块。

Result: 改进模型在精度、召回率和mAP@50上分别提升了1.96%、6.13%和2.07%，达到93.7%、90.4%和93.8%。

Conclusion: 该模型轻量高效，适用于实时应用，为智能交通解决方案提供了有效工具。

Abstract: Potholes cause vehicle damage and traffic accidents, creating serious safety
and economic problems. Therefore, early and accurate detection of potholes is
crucial. Existing detection methods are usually only based on 2D RGB images and
cannot accurately analyze the physical characteristics of potholes. In this
paper, a publicly available dataset of RGB-D images (PothRGBD) is created and
an improved YOLOv8-based model is proposed for both pothole detection and
pothole physical features analysis. The Intel RealSense D415 depth camera was
used to collect RGB and depth data from the road surfaces, resulting in a
PothRGBD dataset of 1000 images. The data was labeled in YOLO format suitable
for segmentation. A novel YOLO model is proposed based on the YOLOv8n-seg
architecture, which is structurally improved with Dynamic Snake Convolution
(DSConv), Simple Attention Module (SimAM) and Gaussian Error Linear Unit
(GELU). The proposed model segmented potholes with irregular edge structure
more accurately, and performed perimeter and depth measurements on depth maps
with high accuracy. The standard YOLOv8n-seg model achieved 91.9% precision,
85.2% recall and 91.9% mAP@50. With the proposed model, the values increased to
93.7%, 90.4% and 93.8% respectively. Thus, an improvement of 1.96% in
precision, 6.13% in recall and 2.07% in mAP was achieved. The proposed model
performs pothole detection as well as perimeter and depth measurement with high
accuracy and is suitable for real-time applications due to its low model
complexity. In this way, a lightweight and effective model that can be used in
deep learning-based intelligent transportation solutions has been acquired.

</details>


### [162] [Object-Shot Enhanced Grounding Network for Egocentric Video](https://arxiv.org/abs/2505.04270)
*Yisen Feng,Haoyu Zhang,Meng Liu,Weili Guan,Liqiang Nie*

Main category: cs.CV

TL;DR: OSGNet是一种针对自我中心视频的任务提出的新方法，通过提取对象信息和分析镜头运动来增强视频表示和模态对齐能力，实验证明其性能优越。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注自我中心与外部中心视频的分布差异，但忽略了自我中心视频的关键特征和细粒度信息。

Method: 提出OSGNet，提取视频中的对象信息以丰富表示，并分析镜头运动以捕捉穿戴者的注意力信息。

Result: 在三个数据集上实验表明，OSGNet达到了最先进的性能。

Conclusion: OSGNet通过对象和镜头运动信息的结合，有效提升了自我中心视频任务的性能。

Abstract: Egocentric video grounding is a crucial task for embodied intelligence
applications, distinct from exocentric video moment localization. Existing
methods primarily focus on the distributional differences between egocentric
and exocentric videos but often neglect key characteristics of egocentric
videos and the fine-grained information emphasized by question-type queries. To
address these limitations, we propose OSGNet, an Object-Shot enhanced Grounding
Network for egocentric video. Specifically, we extract object information from
videos to enrich video representation, particularly for objects highlighted in
the textual query but not directly captured in the video features.
Additionally, we analyze the frequent shot movements inherent to egocentric
videos, leveraging these features to extract the wearer's attention
information, which enhances the model's ability to perform modality alignment.
Experiments conducted on three datasets demonstrate that OSGNet achieves
state-of-the-art performance, validating the effectiveness of our approach. Our
code can be found at https://github.com/Yisen-Feng/OSGNet.

</details>


### [163] [Balancing Accuracy, Calibration, and Efficiency in Active Learning with Vision Transformers Under Label Noise](https://arxiv.org/abs/2505.04375)
*Moseli Mots'oehli,Hope Mogale,Kyungim Baek*

Main category: cs.CV

TL;DR: 研究探讨了在标签噪声和低预算条件下，不同规模的视觉变换器（ViT）和Swin变换器在下游任务中的表现，发现较大的ViT模型（如ViTl32）在准确性和校准性上表现更优，而Swin变换器在所有噪声水平下表现较弱。


<details>
  <summary>Details</summary>
Motivation: 尽管预训练的卷积神经网络在ImageNet上的微调已很成熟，但视觉变换器在类似场景（尤其是标签噪声下）的表现尚未充分研究。研究旨在评估其在低预算和噪声标签下的实用性。

Method: 在CIFAR10和CIFAR100数据集上，评估了四种ViT配置（Base和Large，16x16和32x32补丁大小）和三种Swin变换器配置（Tiny、Small和Base），在不同标签噪声率下的分类准确性和校准性。

Result: 较大的ViT模型（如ViTl32）在准确性和校准性上表现更优，而Swin变换器表现较弱。较小的补丁大小并不总是更好，且基于信息的主动学习策略仅在中等噪声率下有效。

Conclusion: 研究结果为在资源受限环境中部署视觉变换器提供了实用指导，强调了模型复杂性、标签噪声和计算效率之间的平衡。

Abstract: Fine-tuning pre-trained convolutional neural networks on ImageNet for
downstream tasks is well-established. Still, the impact of model size on the
performance of vision transformers in similar scenarios, particularly under
label noise, remains largely unexplored. Given the utility and versatility of
transformer architectures, this study investigates their practicality under
low-budget constraints and noisy labels. We explore how classification accuracy
and calibration are affected by symmetric label noise in active learning
settings, evaluating four vision transformer configurations (Base and Large
with 16x16 and 32x32 patch sizes) and three Swin Transformer configurations
(Tiny, Small, and Base) on CIFAR10 and CIFAR100 datasets, under varying label
noise rates. Our findings show that larger ViT models (ViTl32 in particular)
consistently outperform their smaller counterparts in both accuracy and
calibration, even under moderate to high label noise, while Swin Transformers
exhibit weaker robustness across all noise levels. We find that smaller patch
sizes do not always lead to better performance, as ViTl16 performs consistently
worse than ViTl32 while incurring a higher computational cost. We also find
that information-based Active Learning strategies only provide meaningful
accuracy improvements at moderate label noise rates, but they result in poorer
calibration compared to models trained on randomly acquired labels, especially
at high label noise rates. We hope these insights provide actionable guidance
for practitioners looking to deploy vision transformers in resource-constrained
environments, where balancing model complexity, label noise, and compute
efficiency is critical in model fine-tuning or distillation.

</details>


### [164] [Deep residual learning with product units](https://arxiv.org/abs/2505.04397)
*Ziyuan Li,Uwe Jaekel,Babette Dellen*

Main category: cs.CV

TL;DR: 提出了一种深度乘积单元残差神经网络（PURe），通过将乘积单元集成到残差块中，提升深度卷积网络的表达能力和参数效率。


<details>
  <summary>Details</summary>
Motivation: 传统求和神经元无法有效捕捉复杂的特征交互，而乘积单元能实现乘法特征交互，提供更强大的复杂模式表示能力。

Method: 在残差块的第二层用2D乘积单元替代传统卷积层，并去除非线性激活函数以保留结构信息。

Result: 在Galaxy10 DECaLS、ImageNet和CIFAR-10数据集上，PURe均表现优异，准确率超越更深层的ResNet模型，且收敛更快、参数更少、抗噪声能力更强。

Conclusion: PURe在准确性、效率和鲁棒性之间取得了良好平衡，展示了乘积单元架构在计算机视觉中的潜力。

Abstract: We propose a deep product-unit residual neural network (PURe) that integrates
product units into residual blocks to improve the expressiveness and parameter
efficiency of deep convolutional networks. Unlike standard summation neurons,
product units enable multiplicative feature interactions, potentially offering
a more powerful representation of complex patterns. PURe replaces conventional
convolutional layers with 2D product units in the second layer of each residual
block, eliminating nonlinear activation functions to preserve structural
information. We validate PURe on three benchmark datasets. On Galaxy10 DECaLS,
PURe34 achieves the highest test accuracy of 84.89%, surpassing the much deeper
ResNet152, while converging nearly five times faster and demonstrating strong
robustness to Poisson noise. On ImageNet, PURe architectures outperform
standard ResNet models at similar depths, with PURe34 achieving a top-1
accuracy of 80.27% and top-5 accuracy of 95.78%, surpassing deeper ResNet
variants (ResNet50, ResNet101) while utilizing significantly fewer parameters
and computational resources. On CIFAR-10, PURe consistently outperforms ResNet
variants across varying depths, with PURe272 reaching 95.01% test accuracy,
comparable to ResNet1001 but at less than half the model size. These results
demonstrate that PURe achieves a favorable balance between accuracy,
efficiency, and robustness. Compared to traditional residual networks, PURe not
only achieves competitive classification performance with faster convergence
and fewer parameters, but also demonstrates greater robustness to noise. Its
effectiveness across diverse datasets highlights the potential of
product-unit-based architectures for scalable and reliable deep learning in
computer vision.

</details>


### [165] [Learning from Similarity Proportion Loss for Classifying Skeletal Muscle Recovery Stages](https://arxiv.org/abs/2505.04150)
*Yu Yamaoka or Weng Ian Chan,Shigeto Seno,Soichiro Fukada,Hideo Matsuda*

Main category: cs.CV

TL;DR: 论文提出了一种名为OSLSP的弱监督学习方法，用于自动化评估肌肉组织再生过程，解决了现有方法无法适应肌肉组织特征提取和忽略类别顺序信息的问题。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖专家视觉检查肌肉组织再生阶段，缺乏定量和客观性。现有弱监督学习方法（LLP）无法适应肌肉组织特征提取且忽略类别顺序信息。

Method: 提出OSLSP方法，利用相似性比例损失和类别比例注意力机制，更新特征提取器并保留类别顺序信息。

Result: OSLSP在骨骼肌恢复阶段分类任务中表现优于大规模预训练和微调模型。

Conclusion: OSLSP为肌肉组织再生评估提供了一种自动化、定量且保留顺序信息的解决方案。

Abstract: Evaluating the regeneration process of damaged muscle tissue is a fundamental
analysis in muscle research to measure experimental effect sizes and uncover
mechanisms behind muscle weakness due to aging and disease. The conventional
approach to assessing muscle tissue regeneration involves whole-slide imaging
and expert visual inspection of the recovery stages based on the morphological
information of cells and fibers. There is a need to replace these tasks with
automated methods incorporating machine learning techniques to ensure a
quantitative and objective analysis. Given the limited availability of fully
labeled data, a possible approach is Learning from Label Proportions (LLP), a
weakly supervised learning method using class label proportions. However,
current LLP methods have two limitations: (1) they cannot adapt the feature
extractor for muscle tissues, and (2) they treat the classes representing
recovery stages and cell morphological changes as nominal, resulting in the
loss of ordinal information. To address these issues, we propose Ordinal Scale
Learning from Similarity Proportion (OSLSP), which uses a similarity proportion
loss derived from two bag combinations. OSLSP can update the feature extractor
by using class proportion attention to the ordinal scale of the class. Our
model with OSLSP outperforms large-scale pre-trained and fine-tuning models in
classification tasks of skeletal muscle recovery stages.

</details>


### [166] [Efficient Flow Matching using Latent Variables](https://arxiv.org/abs/2505.04486)
*Anirban Samaddar,Yixuan Sun,Viktor Nilsson,Sandeep Madireddy*

Main category: cs.CV

TL;DR: Latent-CFM提出了一种简化训练和推理的策略，利用预训练的深度隐变量模型处理多模态数据，显著提升了生成质量并减少了训练和计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有的流匹配模型在从简单源分布（如标准高斯分布）学习流时，未明确建模目标数据的底层结构/流形，导致学习效率低下，尤其是对高维现实数据集。

Method: 提出Latent-CFM，利用预训练的深度隐变量模型简化训练和推理策略，以处理多模态数据。

Result: 在合成数据和图像基准数据集上，Latent-CFM生成质量更高，训练和计算成本显著降低（某些情况下减少50%）。在2D Darcy流数据集上，生成样本物理准确性更高。

Conclusion: Latent-CFM通过隐空间分析支持条件图像生成，并展示了在多模态数据上的高效性和准确性。

Abstract: Flow matching models have shown great potential in image generation tasks
among probabilistic generative models. Building upon the ideas of continuous
normalizing flows, flow matching models generalize the transport path of the
diffusion models from a simple prior distribution to the data. Most flow
matching models in the literature do not explicitly model the underlying
structure/manifold in the target data when learning the flow from a simple
source distribution like the standard Gaussian. This leads to inefficient
learning, especially for many high-dimensional real-world datasets, which often
reside in a low-dimensional manifold. Existing strategies of incorporating
manifolds, including data with underlying multi-modal distribution, often
require expensive training and hence frequently lead to suboptimal performance.
To this end, we present \texttt{Latent-CFM}, which provides simplified
training/inference strategies to incorporate multi-modal data structures using
pretrained deep latent variable models. Through experiments on multi-modal
synthetic data and widely used image benchmark datasets, we show that
\texttt{Latent-CFM} exhibits improved generation quality with significantly
less training ($\sim 50\%$ less in some cases) and computation than
state-of-the-art flow matching models. Using a 2d Darcy flow dataset, we
demonstrate that our approach generates more physically accurate samples than
competitive approaches. In addition, through latent space analysis, we
demonstrate that our approach can be used for conditional image generation
conditioned on latent features.

</details>


### [167] ["I Can See Forever!": Evaluating Real-time VideoLLMs for Assisting Individuals with Visual Impairments](https://arxiv.org/abs/2505.04488)
*Ziyi Zhang,Zhen Sun,Zongmin Zhang,Zifan Peng,Yuemeng Zhao,Zichun Wang,Zeren Luo,Ruiting Zuo,Xinlei He*

Main category: cs.CV

TL;DR: 论文研究了如何利用VideoLLMs为视障人士提供实时智能辅助，构建了VisAssistDaily和SafeVid数据集，并发现GPT-4o表现最佳，同时提出了环境风险主动检测机制。


<details>
  <summary>Details</summary>
Motivation: 视障人士在动态复杂环境中面临实时感知的挑战，现有技术多关注静态内容，缺乏对实时需求的满足。

Method: 构建VisAssistDaily数据集评估VideoLLMs在辅助任务中的表现，并通过用户研究验证模型在开放和封闭场景中的实用性。

Result: GPT-4o在任务成功率上表现最佳，但模型在动态环境中感知潜在危险的能力不足。

Conclusion: 通过SafeVid数据集和轮询机制解决了环境风险感知问题，为未来研究提供了参考。

Abstract: The visually impaired population, especially the severely visually impaired,
is currently large in scale, and daily activities pose significant challenges
for them. Although many studies use large language and vision-language models
to assist the blind, most focus on static content and fail to meet real-time
perception needs in dynamic and complex environments, such as daily activities.
To provide them with more effective intelligent assistance, it is imperative to
incorporate advanced visual understanding technologies. Although real-time
vision and speech interaction VideoLLMs demonstrate strong real-time visual
understanding, no prior work has systematically evaluated their effectiveness
in assisting visually impaired individuals. In this work, we conduct the first
such evaluation. First, we construct a benchmark dataset (VisAssistDaily),
covering three categories of assistive tasks for visually impaired individuals:
Basic Skills, Home Life Tasks, and Social Life Tasks. The results show that
GPT-4o achieves the highest task success rate. Next, we conduct a user study to
evaluate the models in both closed-world and open-world scenarios, further
exploring the practical challenges of applying VideoLLMs in assistive contexts.
One key issue we identify is the difficulty current models face in perceiving
potential hazards in dynamic environments. To address this, we build an
environment-awareness dataset named SafeVid and introduce a polling mechanism
that enables the model to proactively detect environmental risks. We hope this
work provides valuable insights and inspiration for future research in this
field.

</details>


### [168] [Defining and Quantifying Creative Behavior in Popular Image Generators](https://arxiv.org/abs/2505.04497)
*Aditi Ramaswamy*

Main category: cs.CV

TL;DR: 本文提出了一种从实用角度衡量生成AI模型创造力的方法，并引入了定量指标帮助用户选择适合任务的模型。


<details>
  <summary>Details</summary>
Motivation: 生成AI模型的创造力一直是科学争论的焦点，但缺乏明确结论。本文旨在从实用角度解决这一问题。

Method: 引入定量指标，并在多个流行的图像生成模型上进行了评估。

Result: 评估结果表明，提出的指标与人类直觉一致。

Conclusion: 本文的方法为选择适合任务的生成AI模型提供了实用工具。

Abstract: Creativity of generative AI models has been a subject of scientific debate in
the last years, without a conclusive answer. In this paper, we study creativity
from a practical perspective and introduce quantitative measures that help the
user to choose a suitable AI model for a given task. We evaluated our measures
on a number of popular image-to-image generation models, and the results of
this suggest that our measures conform to human intuition.

</details>


### [169] [DFVO: Learning Darkness-free Visible and Infrared Image Disentanglement and Fusion All at Once](https://arxiv.org/abs/2505.04526)
*Qi Zhou,Yukai Shi,Xiaojun Yang,Xiaoyu Xian,Lunjia Liao,Ruimao Zhang,Liang Lin*

Main category: cs.CV

TL;DR: 论文提出了一种名为DFVO的网络，用于解决可见光和红外图像融合中因光照不足导致的模糊和暗淡问题，通过多任务级联方法实现更好的融合效果。


<details>
  <summary>Details</summary>
Motivation: 现有图像融合方法在光照不足的可见光图像下表现不佳，导致融合结果模糊暗淡，影响自动驾驶等高级视觉任务。

Method: 采用级联多任务策略，包括潜在共同特征提取器（LCFE）、细节提取模块（DEM）和超交叉注意力模块（HCAM），并设计相关损失函数指导网络学习。

Result: 在LLVIP数据集上取得最佳性能（PSNR 63.258 dB，CC 0.724），生成更清晰、信息更丰富且光照均匀的融合图像。

Conclusion: DFVO在黑暗环境中表现优异，为高级视觉任务提供了更有效的信息。

Abstract: Visible and infrared image fusion is one of the most crucial tasks in the
field of image fusion, aiming to generate fused images with clear structural
information and high-quality texture features for high-level vision tasks.
However, when faced with severe illumination degradation in visible images, the
fusion results of existing image fusion methods often exhibit blurry and dim
visual effects, posing major challenges for autonomous driving. To this end, a
Darkness-Free network is proposed to handle Visible and infrared image
disentanglement and fusion all at Once (DFVO), which employs a cascaded
multi-task approach to replace the traditional two-stage cascaded training
(enhancement and fusion), addressing the issue of information entropy loss
caused by hierarchical data transmission. Specifically, we construct a
latent-common feature extractor (LCFE) to obtain latent features for the
cascaded tasks strategy. Firstly, a details-extraction module (DEM) is devised
to acquire high-frequency semantic information. Secondly, we design a hyper
cross-attention module (HCAM) to extract low-frequency information and preserve
texture features from source images. Finally, a relevant loss function is
designed to guide the holistic network learning, thereby achieving better image
fusion. Extensive experiments demonstrate that our proposed approach
outperforms state-of-the-art alternatives in terms of qualitative and
quantitative evaluations. Particularly, DFVO can generate clearer, more
informative, and more evenly illuminated fusion results in the dark
environments, achieving best performance on the LLVIP dataset with 63.258 dB
PSNR and 0.724 CC, providing more effective information for high-level vision
tasks. Our code is publicly accessible at https://github.com/DaVin-Qi530/DFVO.

</details>


### [170] [Edge-GPU Based Face Tracking for Face Detection and Recognition Acceleration](https://arxiv.org/abs/2505.04524)
*Asma Baobaid,Mahmoud Meribout*

Main category: cs.CV

TL;DR: 提出了一种结合硬件-软件的优化方法，用于在NVIDIA Jetson AGX Orin边缘GPU上提升人脸检测与识别系统的性能，通过同时利用所有硬件引擎和集成人脸跟踪模块，实现了290 FPS的高吞吐量和约800 mW的功耗节省。


<details>
  <summary>Details</summary>
Motivation: 现代应用中，公共场合实时且准确的人脸检测与识别系统至关重要，但现有系统在吞吐量和功耗方面仍有改进空间。

Method: 采用硬件-软件协同设计方法，充分利用NVIDIA Jetson AGX Orin的所有硬件引擎，并集成人脸跟踪模块以减少冗余计算。

Result: 实验结果显示，系统在1920x1080分辨率下平均每帧6张人脸的情况下，实现了290 FPS的高吞吐量，并节省了约800 mW的功耗。

Conclusion: 该硬件-软件协同设计方法为高性能边缘机器视觉系统的设计提供了新思路，特别适用于公共场合的多摄像头视频监控场景。

Abstract: Cost-effective machine vision systems dedicated to real-time and accurate
face detection and recognition in public places are crucial for many modern
applications. However, despite their high performance, which could be reached
using specialized edge or cloud AI hardware accelerators, there is still room
for improvement in throughput and power consumption. This paper aims to suggest
a combined hardware-software approach that optimizes face detection and
recognition systems on one of the latest edge GPUs, namely NVIDIA Jetson AGX
Orin. First, it leverages the simultaneous usage of all its hardware engines to
improve processing time. This offers an improvement over previous works where
these tasks were mainly allocated automatically and exclusively to the CPU or,
to a higher extent, to the GPU core. Additionally, the paper suggests
integrating a face tracker module to avoid redundantly running the face
recognition algorithm for every frame but only when a new face appears in the
scene. The results of extended experiments suggest that simultaneous usage of
all the hardware engines that are available in the Orin GPU and tracker
integration into the pipeline yield an impressive throughput of 290 FPS (frames
per second) on 1920 x 1080 input size frames containing in average of 6
faces/frame. Additionally, a substantial saving of power consumption of around
800 mW was achieved when compared to running the task on the CPU/GPU engines
only and without integrating a tracker into the Orin GPU\'92s pipeline. This
hardware-codesign approach can pave the way to design high-performance machine
vision systems at the edge, critically needed in video monitoring in public
places where several nearby cameras are usually deployed for a same scene.

</details>


### [171] [Componential Prompt-Knowledge Alignment for Domain Incremental Learning](https://arxiv.org/abs/2505.04575)
*Kunlun Xu,Xu Zou,Gang Hua,Jiahuan Zhou*

Main category: cs.CV

TL;DR: KA-Prompt通过组件感知的提示-知识对齐解决领域增量学习中提示组件不对齐的问题，显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 揭示现有基于提示的领域增量学习方法中，领域特定提示的组件不对齐导致知识冲突和预测性能下降的问题。

Method: 提出KA-Prompt，分两阶段：1) 初始组件结构配置，通过贪婪搜索挖掘相关旧提示初始化新提示；2) 在线对齐保持，动态识别目标旧提示并应用一致性约束。

Result: 在领域增量学习基准测试中验证了KA-Prompt的有效性。

Conclusion: KA-Prompt通过组件对齐显著提升了模型的领域增量学习能力。

Abstract: Domain Incremental Learning (DIL) aims to learn from non-stationary data
streams across domains while retaining and utilizing past knowledge. Although
prompt-based methods effectively store multi-domain knowledge in prompt
parameters and obtain advanced performance through cross-domain prompt fusion,
we reveal an intrinsic limitation: component-wise misalignment between
domain-specific prompts leads to conflicting knowledge integration and degraded
predictions. This arises from the random positioning of knowledge components
within prompts, where irrelevant component fusion introduces interference.To
address this, we propose Componential Prompt-Knowledge Alignment (KA-Prompt), a
novel prompt-based DIL method that introduces component-aware prompt-knowledge
alignment during training, significantly improving both the learning and
inference capacity of the model. KA-Prompt operates in two phases: (1) Initial
Componential Structure Configuring, where a set of old prompts containing
knowledge relevant to the new domain are mined via greedy search, which is then
exploited to initialize new prompts to achieve reusable knowledge transfer and
establish intrinsic alignment between new and old prompts. (2) Online Alignment
Preservation, which dynamically identifies the target old prompts and applies
adaptive componential consistency constraints as new prompts evolve. Extensive
experiments on DIL benchmarks demonstrate the effectiveness of our KA-Prompt.
Our source code is available at
https://github.com/zhoujiahuan1991/ICML2025-KA-Prompt

</details>


### [172] [Active Sampling for MRI-based Sequential Decision Making](https://arxiv.org/abs/2505.04586)
*Yuning Du,Jingshuai Liu,Rohan Dharmakumar,Sotirios A. Tsaftaris*

Main category: cs.CV

TL;DR: 提出了一种多目标强化学习框架，通过优化采样策略，使低场强MRI能够作为经济高效的点护理设备。


<details>
  <summary>Details</summary>
Motivation: 尽管MRI诊断能力优越，但其高成本和复杂性限制了其作为点护理设备的应用。通过减少磁场强度并优化采样策略，可以推动MRI在点护理中的应用。

Method: 采用多目标强化学习框架，动态调整采样策略以优化诊断效果，并引入逐步加权奖励函数来识别对诊断目标贡献最大的样本。

Result: 在膝关节病理评估任务中，该方法在疾病检测、严重程度量化和整体序列诊断方面表现优异，同时显著减少了k空间采样量。

Conclusion: 该方法为MRI成为经济高效的点护理设备提供了可能，代码已公开。

Abstract: Despite the superior diagnostic capability of Magnetic Resonance Imaging
(MRI), its use as a Point-of-Care (PoC) device remains limited by high cost and
complexity. To enable such a future by reducing the magnetic field strength,
one key approach will be to improve sampling strategies. Previous work has
shown that it is possible to make diagnostic decisions directly from k-space
with fewer samples. Such work shows that single diagnostic decisions can be
made, but if we aspire to see MRI as a true PoC, multiple and sequential
decisions are necessary while minimizing the number of samples acquired. We
present a novel multi-objective reinforcement learning framework enabling
comprehensive, sequential, diagnostic evaluation from undersampled k-space
data. Our approach during inference actively adapts to sequential decisions to
optimally sample. To achieve this, we introduce a training methodology that
identifies the samples that contribute the best to each diagnostic objective
using a step-wise weighting reward function. We evaluate our approach in two
sequential knee pathology assessment tasks: ACL sprain detection and cartilage
thickness loss assessment. Our framework achieves diagnostic performance
competitive with various policy-based benchmarks on disease detection, severity
quantification, and overall sequential diagnosis, while substantially saving
k-space samples. Our approach paves the way for the future of MRI as a
comprehensive and affordable PoC device. Our code is publicly available at
https://github.com/vios-s/MRI_Sequential_Active_Sampling

</details>


<div id='q-fin.MF'></div>

# q-fin.MF [[Back]](#toc)

### [173] [Risk-sensitive Reinforcement Learning Based on Convex Scoring Functions](https://arxiv.org/abs/2505.04553)
*Shanyu Han,Yang Liu,Xiang Yu*

Main category: q-fin.MF

TL;DR: 提出了一种基于强化学习的框架，适用于由凸评分函数定义的风险目标，解决了时间不一致性问题，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 研究如何通过强化学习处理多种常见风险度量（如方差、预期短缺等），并解决时间不一致性问题。

Method: 采用增强状态空间和辅助变量，将问题转化为两状态优化问题，提出定制化的Actor-Critic算法，并设计了辅助变量采样方法。

Result: 理论证明了算法的近似保证，且不要求马尔可夫决策过程连续；实验验证了算法在金融统计套利交易中的有效性。

Conclusion: 该框架能够有效处理多种风险目标，解决了时间不一致性问题，并在实际应用中表现出色。

Abstract: We propose a reinforcement learning (RL) framework under a broad class of
risk objectives, characterized by convex scoring functions. This class covers
many common risk measures, such as variance, Expected Shortfall, entropic
Value-at-Risk, and mean-risk utility. To resolve the time-inconsistency issue,
we consider an augmented state space and an auxiliary variable and recast the
problem as a two-state optimization problem. We propose a customized
Actor-Critic algorithm and establish some theoretical approximation guarantees.
A key theoretical contribution is that our results do not require the Markov
decision process to be continuous. Additionally, we propose an auxiliary
variable sampling method inspired by the alternating minimization algorithm,
which is convergent under certain conditions. We validate our approach in
simulation experiments with a financial application in statistical arbitrage
trading, demonstrating the effectiveness of the algorithm.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [174] [Cer-Eval: Certifiable and Cost-Efficient Evaluation Framework for LLMs](https://arxiv.org/abs/2505.03814)
*Ganghua Wang,Zhaorun Chen,Bo Li,Haifeng Xu*

Main category: stat.ML

TL;DR: 论文提出了一种可认证且高效的大语言模型评估框架Cer-Eval，通过自适应选择测试点减少评估成本，同时保持高置信度。


<details>
  <summary>Details</summary>
Motivation: 随着基础模型规模扩大，评估大型语言模型（LLM）的挑战日益增加，现有方法缺乏对测试数据充分性和样本选择的系统性指导。

Method: 提出基于“测试样本复杂度”的理论框架，开发分区算法Cer-Eval，自适应选择测试点以最小化评估成本。

Result: 实验表明，Cer-Eval可在多种基准测试中节省20%至40%的测试点，同时保持与现有方法相当的估计误差，并提供95%的置信保证。

Conclusion: Cer-Eval为LLM评估提供了一种高效且可认证的解决方案，显著降低了评估成本。

Abstract: As foundation models continue to scale, the size of trained models grows
exponentially, presenting significant challenges for their evaluation. Current
evaluation practices involve curating increasingly large datasets to assess the
performance of large language models (LLMs). However, there is a lack of
systematic analysis and guidance on determining the sufficiency of test data or
selecting informative samples for evaluation. This paper introduces a
certifiable and cost-efficient evaluation framework for LLMs. Our framework
adapts to different evaluation objectives and outputs confidence intervals that
contain true values with high probability. We use ``test sample complexity'' to
quantify the number of test points needed for a certifiable evaluation and
derive tight bounds on test sample complexity. Based on the developed theory,
we develop a partition-based algorithm, named Cer-Eval, that adaptively selects
test points to minimize the cost of LLM evaluation. Real-world experiments
demonstrate that Cer-Eval can save 20% to 40% test points across various
benchmarks, while maintaining an estimation error level comparable to the
current evaluation process and providing a 95% confidence guarantee.

</details>


### [175] [Categorical and geometric methods in statistical, manifold, and machine learning](https://arxiv.org/abs/2505.03862)
*Hông Vân Lê,Hà Quang Minh,Frederic Protin,Wilderich Tuschmann*

Main category: stat.ML

TL;DR: 本文介绍了概率态射范畴的应用及其在统计、机器和流形学习中的几何方法。


<details>
  <summary>Details</summary>
Motivation: 探讨概率态射范畴及其几何方法在多种学习问题中的应用。

Method: 利用概率态射范畴和几何方法分析统计、机器和流形学习问题。

Result: 展示了这些方法在多个学习领域的适用性。

Conclusion: 这些方法将在未来的书中进一步深入探讨。

Abstract: We present and discuss applications of the category of probabilistic
morphisms, initially developed in \cite{Le2023}, as well as some geometric
methods to several classes of problems in statistical, machine and manifold
learning which shall be, along with many other topics, considered in depth in
the forthcoming book \cite{LMPT2024}.

</details>


### [176] [Variational Formulation of the Particle Flow Particle Filter](https://arxiv.org/abs/2505.04007)
*Yinzhuang Yi,Jorge Cortés,Nikolay Atanasov*

Main category: stat.ML

TL;DR: 本文从变分推断的角度重新表述了粒子流粒子滤波器，并证明其瞬态密度遵循Fisher-Rao梯度流的时间缩放轨迹。


<details>
  <summary>Details</summary>
Motivation: 通过变分推断的视角，重新理解粒子流粒子滤波器的理论基础。

Method: 将粒子流粒子滤波器的瞬态密度与Fisher-Rao梯度流的时间缩放轨迹联系起来，并作为变分推断的连续时间算法。

Result: 证明了瞬态密度遵循Fisher-Rao梯度流的时间缩放轨迹，并最小化了变分密度与真实后验密度之间的Kullback-Leibler散度。

Conclusion: 从变分推断的角度为粒子流粒子滤波器提供了新的理论支持。

Abstract: This paper provides a formulation of the particle flow particle filter from
the perspective of variational inference. We show that the transient density
used to derive the particle flow particle filter follows a time-scaled
trajectory of the Fisher-Rao gradient flow in the space of probability
densities. The Fisher-Rao gradient flow is obtained as a continuous-time
algorithm for variational inference, minimizing the Kullback-Leibler divergence
between a variational density and the true posterior density.

</details>


### [177] [A Tutorial on Discriminative Clustering and Mutual Information](https://arxiv.org/abs/2505.04484)
*Louis Ohl,Pierre-Alexandre Mattei,Frédéric Precioso*

Main category: stat.ML

TL;DR: 本文回顾了判别式聚类方法的历史演变，重点讨论了从决策边界到不变性批评的假设变化，以及互信息在其中的关键作用，同时指出了互信息的局限性和聚类数量选择的挑战，并介绍了相关Python工具GemClus。


<details>
  <summary>Details</summary>
Motivation: 提供判别式聚类方法的历史视角，展示其假设的演变及互信息的重要性，同时探讨其局限性和挑战。

Method: 通过历史回顾和理论分析，探讨判别式聚类方法的假设变化、互信息的作用及其局限性。

Result: 总结了判别式聚类方法的进展，互信息的关键作用及其局限性，并提出了聚类数量选择的挑战。

Conclusion: 判别式聚类方法在假设和工具上取得了显著进展，但仍面临互信息局限性和聚类数量选择的挑战，GemClus工具为实践提供了支持。

Abstract: To cluster data is to separate samples into distinctive groups that should
ideally have some cohesive properties. Today, numerous clustering algorithms
exist, and their differences lie essentially in what can be perceived as
``cohesive properties''. Therefore, hypotheses on the nature of clusters must
be set: they can be either generative or discriminative. As the last decade
witnessed the impressive growth of deep clustering methods that involve neural
networks to handle high-dimensional data often in a discriminative manner; we
concentrate mainly on the discriminative hypotheses. In this paper, our aim is
to provide an accessible historical perspective on the evolution of
discriminative clustering methods and notably how the nature of assumptions of
the discriminative models changed over time: from decision boundaries to
invariance critics. We notably highlight how mutual information has been a
historical cornerstone of the progress of (deep) discriminative clustering
methods. We also show some known limitations of mutual information and how
discriminative clustering methods tried to circumvent those. We then discuss
the challenges that discriminative clustering faces with respect to the
selection of the number of clusters. Finally, we showcase these techniques
using the dedicated Python package, GemClus, that we have developed for
discriminative clustering.

</details>


### [178] [From Two Sample Testing to Singular Gaussian Discrimination](https://arxiv.org/abs/2505.04613)
*Leonardo V. Santoro,Kartik G. Waghmare,Victor M. Panaretos*

Main category: stat.ML

TL;DR: 论文证明了在一般可分紧度量空间上测试两个概率测度等价性，等同于在合适的再生核希尔伯特空间中测试两个对应高斯测度的奇异性。通过核均值与协方差嵌入定义高斯测度，奇异性测试比非参数两样本测试更简单。


<details>
  <summary>Details</summary>
Motivation: 研究如何在一般可分紧度量空间上高效测试概率测度的等价性，并利用高斯嵌入简化高维设置下的测试问题。

Method: 利用Feldman-Hajek准则分析高斯测度的奇异性/等价性，通过核均值与协方差嵌入将概率测度映射为高斯测度。

Result: 发现概率测度的差异在高斯嵌入中被显著放大，使得奇异性测试更高效，展现了“维度祝福”的新实例。

Conclusion: 高斯嵌入方法为高维设置下的高效推断工具设计提供了新思路，具有广泛适用性。

Abstract: We establish that testing for the equality of two probability measures on a
general separable and compact metric space is equivalent to testing for the
singularity between two corresponding Gaussian measures on a suitable
Reproducing Kernel Hilbert Space. The corresponding Gaussians are defined via
the notion of kernel mean and covariance embedding of a probability measure.
Discerning two singular Gaussians is fundamentally simpler from an
information-theoretic perspective than non-parametric two-sample testing,
particularly in high-dimensional settings. Our proof leverages the
Feldman-Hajek criterion for singularity/equivalence of Gaussians on Hilbert
spaces, and shows that discrepancies between distributions are heavily
magnified through their corresponding Gaussian embeddings: at a population
level, distinct probability measures lead to essentially separated Gaussian
embeddings. This appears to be a new instance of the blessing of dimensionality
that can be harnessed for the design of efficient inference tools in great
generality.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [179] [IntelliCardiac: An Intelligent Platform for Cardiac Image Segmentation and Classification](https://arxiv.org/abs/2505.03838)
*Ting Yu Tsai,An Yu,Meghana Spurthi Maadugundu,Ishrat Jahan Mohima,Umme Habiba Barsha,Mei-Hwa F. Chen,Balakrishnan Prabhakaran,Ming-Ching Chang*

Main category: eess.IV

TL;DR: IntelliCardiac是一个基于AI的医疗图像处理平台，用于4D心脏图像的自动分割和疾病分类，准确率高达98%，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 精确处理心脏影像数据对心血管疾病的识别和管理至关重要，现有方法在分割和分类的整合上仍有不足。

Method: 结合深度学习分割模型和两步分类流程，利用ACDC数据集训练，支持左右心室和心肌的分析。

Result: 分割模块准确率92.6%，分类模块在五类疾病中达到98%准确率，优于现有技术。

Conclusion: IntelliCardiac具有实时可视化、工作流集成和AI辅助诊断功能，是临床决策支持的潜力工具。

Abstract: Precise and effective processing of cardiac imaging data is critical for the
identification and management of the cardiovascular diseases. We introduce
IntelliCardiac, a comprehensive, web-based medical image processing platform
for the automatic segmentation of 4D cardiac images and disease classification,
utilizing an AI model trained on the publicly accessible ACDC dataset. The
system, intended for patients, cardiologists, and healthcare professionals,
offers an intuitive interface and uses deep learning models to identify
essential heart structures and categorize cardiac diseases. The system supports
analysis of both the right and left ventricles as well as myocardium, and then
classifies patient's cardiac images into five diagnostic categories: dilated
cardiomyopathy, myocardial infarction, hypertrophic cardiomyopathy, right
ventricular abnormality, and no disease. IntelliCardiac combines a deep
learning-based segmentation model with a two-step classification pipeline. The
segmentation module gains an overall accuracy of 92.6\%. The classification
module, trained on characteristics taken from segmented heart structures,
achieves 98\% accuracy in five categories. These results exceed the performance
of the existing state-of-the-art methods that integrate both segmentation and
classification models. IntelliCardiac, which supports real-time visualization,
workflow integration, and AI-assisted diagnostics, has great potential as a
scalable, accurate tool for clinical decision assistance in cardiac imaging and
diagnosis.

</details>


### [180] [From Spaceborn to Airborn: SAR Image Synthesis Using Foundation Models for Multi-Scale Adaptation](https://arxiv.org/abs/2505.03844)
*Solène Debuysère,Nicolas Trouvé,Nathan Letheule,Olivier Lévêque,Elise Colin*

Main category: eess.IV

TL;DR: 论文提出了一种利用预训练潜在扩散模型和空间条件技术，将卫星SAR图像转换为机载SAR表示的新方法，填补了高质量SAR数据集的空白。


<details>
  <summary>Details</summary>
Motivation: 由于高分辨率机载SAR图像获取成本高且数据稀缺，缺乏开源、标注良好的SAR数据集，限制了现有基础模型在遥感应用中的使用。

Method: 利用ONERA的15年机载数据构建了11万张SAR图像的数据集，采用35亿参数的预训练潜在扩散模型，结合空间条件技术进行图像转换。

Result: 方法成功将卫星SAR图像转换为机载SAR表示，并提升了物理模拟器生成图像的逼真度。

Conclusion: 该研究为SAR成像技术的进步提供了关键AI应用，是文献中首次提出的方法。

Abstract: The availability of Synthetic Aperture Radar (SAR) satellite imagery has
increased considerably in recent years, with datasets commercially available.
However, the acquisition of high-resolution SAR images in airborne
configurations, remains costly and limited. Thus, the lack of open source,
well-labeled, or easily exploitable SAR text-image datasets is a barrier to the
use of existing foundation models in remote sensing applications. In this
context, synthetic image generation is a promising solution to augment this
scarce data, enabling a broader range of applications. Leveraging over 15 years
of ONERA's extensive archival airborn data from acquisition campaigns, we
created a comprehensive training dataset of 110 thousands SAR images to exploit
a 3.5 billion parameters pre-trained latent diffusion model. In this work, we
present a novel approach utilizing spatial conditioning techniques within a
foundation model to transform satellite SAR imagery into airborne SAR
representations. Additionally, we demonstrate that our pipeline is effective
for bridging the realism of simulated images generated by ONERA's physics-based
simulator EMPRISE. Our method explores a key application of AI in advancing SAR
imaging technology. To the best of our knowledge, we are the first to introduce
this approach in the literature.

</details>


### [181] [A Deep Learning approach for Depressive Symptoms assessment in Parkinson's disease patients using facial videos](https://arxiv.org/abs/2505.03845)
*Ioannis Kyprakis,Vasileios Skaramagkas,Iro Boura,Georgios Karamanis,Dimitrios I. Fotiadis,Zinovia Kefalopoulou,Cleanthe Spanaki,Manolis Tsiknakis*

Main category: eess.IV

TL;DR: 该研究利用深度学习模型（ViViT、Video Swin Tiny和3D CNN-LSTM）通过面部视频分析评估帕金森病患者的抑郁症状，Video Swin Tiny模型表现最佳，准确率高达94%。


<details>
  <summary>Details</summary>
Motivation: 帕金森病患者的抑郁症状常因与运动症状重叠而被漏诊，研究旨在通过深度学习模型提高诊断准确性。

Method: 使用ViViT、Video Swin Tiny和3D CNN-LSTM三种模型分析面部视频，评估抑郁症状的存在和严重程度，并考虑药物状态的影响。

Result: Video Swin Tiny模型在二元分类（抑郁症状存在与否）中达到94%准确率，多分类任务中达到87.1%准确率。

Conclusion: 深度学习模型，尤其是Video Swin Tiny，能有效评估帕金森病患者的抑郁症状，为临床诊断提供新工具。

Abstract: Parkinson's disease (PD) is a neurodegenerative disorder, manifesting with
motor and non-motor symptoms. Depressive symptoms are prevalent in PD,
affecting up to 45% of patients. They are often underdiagnosed due to
overlapping motor features, such as hypomimia. This study explores deep
learning (DL) models-ViViT, Video Swin Tiny, and 3D CNN-LSTM with attention
layers-to assess the presence and severity of depressive symptoms, as detected
by the Geriatric Depression Scale (GDS), in PD patients through facial video
analysis. The same parameters were assessed in a secondary analysis taking into
account whether patients were one hour after (ON-medication state) or 12 hours
without (OFF-medication state) dopaminergic medication. Using a dataset of
1,875 videos from 178 patients, the Video Swin Tiny model achieved the highest
performance, with up to 94% accuracy and 93.7% F1-score in binary
classification (presence of absence of depressive symptoms), and 87.1% accuracy
with an 85.4% F1-score in multiclass tasks (absence or mild or severe
depressive symptoms).

</details>


### [182] [3D Brain MRI Classification for Alzheimer Diagnosis Using CNN with Data Augmentation](https://arxiv.org/abs/2505.04097)
*Thien Nhan Vo,Bac Nam Ho,Thanh Xuan Truong*

Main category: eess.IV

TL;DR: 开发了一种3D卷积神经网络，用于将T1加权脑MRI扫描分类为健康或阿尔茨海默病，通过噪声注入和交叉验证，模型表现优异。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过3D卷积神经网络提升脑MRI扫描分类的准确性，探索数据增强的效果。

Method: 使用3D卷积、池化、批归一化、ReLU层和Sigmoid输出构建网络，结合噪声注入和五折交叉验证。

Result: 测试集准确率为0.912，ROC曲线下面积为0.961，敏感性和特异性均超过0.90。

Conclusion: 结果表明简单数据增强对3D MRI分类有效，未来可探索更高级的增强方法和架构。

Abstract: A three-dimensional convolutional neural network was developed to classify
T1-weighted brain MRI scans as healthy or Alzheimer. The network comprises 3D
convolution, pooling, batch normalization, dense ReLU layers, and a sigmoid
output. Using stochastic noise injection and five-fold cross-validation, the
model achieved test set accuracy of 0.912 and area under the ROC curve of
0.961, an improvement of approximately 0.027 over resizing alone. Sensitivity
and specificity both exceeded 0.90. These results align with prior work
reporting up to 0.10 gain via synthetic augmentation. The findings demonstrate
the effectiveness of simple augmentation for 3D MRI classification and motivate
future exploration of advanced augmentation methods and architectures such as
3D U-Net and vision transformers.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [183] [Ultra-Low-Power Spiking Neurons in 7 nm FinFET Technology: A Comparative Analysis of Leaky Integrate-and-Fire, Morris-Lecar, and Axon-Hillock Architectures](https://arxiv.org/abs/2505.03764)
*Logan Larsh,Raiyan Siddique,Sarah Sharif Yaser Mike Banad*

Main category: cs.NE

TL;DR: 本文比较了三种脉冲神经元电路架构（LIF、ML、AH）在7 nm FinFET技术中的性能，发现AH设计吞吐量最高，ML适合低功耗场景，LIF在高频操作中表现较好，但静态泄漏较高。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过比较不同神经元架构的性能，为神经形态计算硬件提供优化路径，以实现高能效和高计算吞吐量。

Method: 通过SPICE模拟比较了LIF、ML和AH三种架构在7 nm FinFET技术中的性能，包括脉冲频率、能量消耗和静态功耗。

Result: AH设计在吞吐量上表现最佳（达3 GHz），ML在低功耗场景中表现优异（0.385 aJ/脉冲），LIF在高频操作中表现较好但静态泄漏较高。

Conclusion: 7 nm FinFET技术显著提升了能效和速度，但需权衡子阈值泄漏。研究为优化神经形态硬件提供了设计指导。

Abstract: Neuromorphic computing aims to replicate the brain's remarkable energy
efficiency and parallel processing capabilities for large-scale artificial
intelligence applications. In this work, we present a comprehensive comparative
study of three spiking neuron circuit architectures-Leaky-Integrate-and-Fire
(LIF), Morris-Lecar (ML), and Axon-Hillock (AH)-implemented in a 7 nm FinFET
technology. Through extensive SPICE simulations, we explore the optimization of
spiking frequency, energy per spike, and static power consumption. Our results
show that the AH design achieves the highest throughput, demonstrating
multi-gigahertz firing rates (up to 3 GHz) with attojoule energy costs. By
contrast, the ML architecture excels in subthreshold to near-threshold regimes,
offering robust low-power operation (as low as 0.385 aJ/spike) and biological
bursting behavior. Although LIF benefits from a decoupled current mirror for
high-frequency operation, it exhibits slightly higher static leakage compared
to ML and AH at elevated supply voltages. Comparisons with previous node
implementations (22 nm planar, 28 nm) reveal that 7 nm FinFETs can drastically
boost energy efficiency and speed albeit at the cost of increased subthreshold
leakage in deep subthreshold regions. By quantifying design trade-offs for each
neuron architecture, our work provides a roadmap for optimizing spiking neuron
circuits in advanced nanoscale technologies to deliver neuromorphic hardware
capable of both ultra-low-power operation and high computational throughput.

</details>


### [184] [Izhikevich-Inspired Temporal Dynamics for Enhancing Privacy, Efficiency, and Transferability in Spiking Neural Networks](https://arxiv.org/abs/2505.04034)
*Ayana Moshruba,Hamed Poursiami,Maryam Parsa*

Main category: cs.NE

TL;DR: 论文提出两种概率驱动的输入级时间尖峰变换（Poisson-Burst和Delayed-Burst），用于在标准LIF神经元中引入生物启发的时序变异性，以研究其对隐私、泛化和学习性能的影响。


<details>
  <summary>Details</summary>
Motivation: 生物神经元表现出多样的时序尖峰模式，支持高效、稳健和自适应的神经信息处理，但现有模型（如Izhikevich）的复杂性难以直接集成到可扩展的SNN训练流程中。

Method: 提出Poisson-Burst和Delayed-Burst两种变换：前者基于输入强度调节爆发频率，后者通过爆发起始时间编码输入强度。

Result: 实验表明，Poisson-Burst在保持竞争性精度的同时增强了隐私鲁棒性，而Delayed-Burst提供了更强的隐私保护但需牺牲少量精度。

Conclusion: 生物启发的时序尖峰动态可提升神经形态学习系统的隐私性、泛化能力和生物合理性。

Abstract: Biological neurons exhibit diverse temporal spike patterns, which are
believed to support efficient, robust, and adaptive neural information
processing. While models such as Izhikevich can replicate a wide range of these
firing dynamics, their complexity poses challenges for directly integrating
them into scalable spiking neural networks (SNN) training pipelines. In this
work, we propose two probabilistically driven, input-level temporal spike
transformations: Poisson-Burst and Delayed-Burst that introduce biologically
inspired temporal variability directly into standard Leaky Integrate-and-Fire
(LIF) neurons. This enables scalable training and systematic evaluation of how
spike timing dynamics affect privacy, generalization, and learning performance.
Poisson-Burst modulates burst occurrence based on input intensity, while
Delayed-Burst encodes input strength through burst onset timing. Through
extensive experiments across multiple benchmarks, we demonstrate that
Poisson-Burst maintains competitive accuracy and lower resource overhead while
exhibiting enhanced privacy robustness against membership inference attacks,
whereas Delayed-Burst provides stronger privacy protection at a modest accuracy
trade-off. These findings highlight the potential of biologically grounded
temporal spike dynamics in improving the privacy, generalization and biological
plausibility of neuromorphic learning systems.

</details>


### [185] [TS-SNN: Temporal Shift Module for Spiking Neural Networks](https://arxiv.org/abs/2505.04165)
*Kairong Yu,Tianqing Zhang,Qi Xu,Gang Pan,Hongwei Wang*

Main category: cs.NE

TL;DR: TS-SNN通过引入轻量级的Temporal Shift模块，有效整合时空信息，显著提升SNN性能并保持低能耗。


<details>
  <summary>Details</summary>
Motivation: SNN因其生物合理性和高能效在神经形态计算中备受关注，但如何平衡时间特征利用与低能耗仍具挑战性。

Method: 提出TS-SNN，通过Temporal Shift模块整合过去、现在和未来的脉冲特征，采用残差组合防止信息丢失。

Result: 在CIFAR-10、CIFAR-100和ImageNet上实现SOTA性能（96.72%、80.28%、70.61%），且能耗低。

Conclusion: TS-SNN为高效准确的SNN架构开发迈出重要一步。

Abstract: Spiking Neural Networks (SNNs) are increasingly recognized for their
biological plausibility and energy efficiency, positioning them as strong
alternatives to Artificial Neural Networks (ANNs) in neuromorphic computing
applications. SNNs inherently process temporal information by leveraging the
precise timing of spikes, but balancing temporal feature utilization with low
energy consumption remains a challenge. In this work, we introduce Temporal
Shift module for Spiking Neural Networks (TS-SNN), which incorporates a novel
Temporal Shift (TS) module to integrate past, present, and future spike
features within a single timestep via a simple yet effective shift operation. A
residual combination method prevents information loss by integrating shifted
and original features. The TS module is lightweight, requiring only one
additional learnable parameter, and can be seamlessly integrated into existing
architectures with minimal additional computational cost. TS-SNN achieves
state-of-the-art performance on benchmarks like CIFAR-10 (96.72\%), CIFAR-100
(80.28\%), and ImageNet (70.61\%) with fewer timesteps, while maintaining low
energy consumption. This work marks a significant step forward in developing
efficient and accurate SNN architectures.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [186] [Decentralized Distributed Proximal Policy Optimization (DD-PPO) for High Performance Computing Scheduling on Multi-User Systems](https://arxiv.org/abs/2505.03946)
*Matthew Sgambati,Aleksandar Vakanski,Matthew Anderson*

Main category: cs.DC

TL;DR: 论文提出了一种基于DD-PPO算法的RL调度器，解决了传统和现有RL调度器在大规模数据集上的可扩展性问题，并展示了优于传统方法和现有RL方法的性能。


<details>
  <summary>Details</summary>
Motivation: HPC环境中资源分配的复杂性要求更智能的调度算法，传统规则调度和现有RL方法在大规模数据集上存在可扩展性和效率问题。

Method: 采用DD-PPO算法，支持多工作节点分布式训练，避免每一步参数同步，提升训练效率和样本利用率。

Result: 实验使用1150万真实HPC作业数据，DD-PPO在调度性能上优于传统规则调度器和现有RL方法。

Conclusion: DD-PPO算法为HPC调度提供了可扩展且高效的解决方案，显著提升了调度性能。

Abstract: Resource allocation in High Performance Computing (HPC) environments presents
a complex and multifaceted challenge for job scheduling algorithms. Beyond the
efficient allocation of system resources, schedulers must account for and
optimize multiple performance metrics, including job wait time and system
utilization. While traditional rule-based scheduling algorithms dominate the
current deployments of HPC systems, the increasing heterogeneity and scale of
those systems is expected to challenge the efficiency and flexibility of those
algorithms in minimizing job wait time and maximizing utilization. Recent
research efforts have focused on leveraging advancements in Reinforcement
Learning (RL) to develop more adaptable and intelligent scheduling strategies.
Recent RL-based scheduling approaches have explored a range of algorithms, from
Deep Q-Networks (DQN) to Proximal Policy Optimization (PPO), and more recently,
hybrid methods that integrate Graph Neural Networks with RL techniques.
However, a common limitation across these methods is their reliance on
relatively small datasets, and these methods face scalability issues when using
large datasets. This study introduces a novel RL-based scheduler utilizing the
Decentralized Distributed Proximal Policy Optimization (DD-PPO) algorithm,
which supports large-scale distributed training across multiple workers without
requiring parameter synchronization at every step. By eliminating reliance on
centralized updates to a shared policy, the DD-PPO scheduler enhances
scalability, training efficiency, and sample utilization. The validation
dataset leveraged over 11.5 million real HPC job traces for comparing DD-PPO
performance between traditional and advanced scheduling approaches, and the
experimental results demonstrate improved scheduling performance in comparison
to both rule-based schedulers and existing RL-based scheduling algorithms.

</details>


### [187] [Can Large Language Models Predict Parallel Code Performance?](https://arxiv.org/abs/2505.03988)
*Gregory Bolet,Giorgis Georgakoudis,Harshitha Menon,Konstantinos Parasyris,Niranjan Hasabnis,Hayden Estes,Kirk W. Cameron,Gal Oren*

Main category: cs.DC

TL;DR: 论文探讨了是否可以用大型语言模型（LLM）替代硬件执行时间分析来预测GPU性能，通过将问题转化为屋顶线分类任务，并在不同场景下评估LLM的表现。


<details>
  <summary>Details</summary>
Motivation: 由于高端GPU访问受限，传统依赖硬件执行的GPU性能分析方法变得不切实际，因此研究LLM是否能提供替代方案。

Method: 构建了340个GPU内核的平衡数据集，评估LLM在四种场景下的表现：带分析数据、零样本、少样本和微调。

Result: LLM在提供分析数据时分类准确率达100%，在零样本和少样本设置下推理能力强的LLM表现更优，但微调需要更多数据。

Conclusion: LLM在HPC性能分析和优化中具有潜力，但需更好的数据集和提示策略。

Abstract: Accurate determination of the performance of parallel GPU code typically
requires execution-time profiling on target hardware -- an increasingly
prohibitive step due to limited access to high-end GPUs. This paper explores
whether Large Language Models (LLMs) can offer an alternative approach for GPU
performance prediction without relying on hardware. We frame the problem as a
roofline classification task: given the source code of a GPU kernel and the
hardware specifications of a target GPU, can an LLM predict whether the GPU
kernel is compute-bound or bandwidth-bound?
  For this study, we build a balanced dataset of 340 GPU kernels, obtained from
HeCBench benchmark and written in CUDA and OpenMP, along with their
ground-truth labels obtained via empirical GPU profiling. We evaluate LLMs
across four scenarios: (1) with access to profiling data of the kernel source,
(2) zero-shot with source code only, (3) few-shot with code and label pairs,
and (4) fine-tuned on a small custom dataset.
  Our results show that state-of-the-art LLMs have a strong understanding of
the Roofline model, achieving 100% classification accuracy when provided with
explicit profiling data. We also find that reasoning-capable LLMs significantly
outperform standard LLMs in zero- and few-shot settings, achieving up to 64%
accuracy on GPU source codes, without profiling information. Lastly, we find
that LLM fine-tuning will require much more data than what we currently have
available.
  This work is among the first to use LLMs for source-level roofline
performance prediction via classification, and illustrates their potential to
guide optimization efforts when runtime profiling is infeasible. Our findings
suggest that with better datasets and prompt strategies, LLMs could become
practical tools for HPC performance analysis and performance portability.

</details>


### [188] [Prism: Unleashing GPU Sharing for Cost-Efficient Multi-LLM Serving](https://arxiv.org/abs/2505.04021)
*Shan Yu,Jiarong Xing,Yifan Qiao,Mingyuan Ma,Yangmin Li,Yang Wang,Shuo Yang,Zhiqiang Xie,Shiyi Cao,Ke Bao,Ion Stoica,Harry Xu,Ying Sheng*

Main category: cs.DC

TL;DR: Prism是一个多LLM服务系统，通过动态GPU内存共享和两级调度策略，显著降低成本并提高服务延迟SLO达标率。


<details>
  <summary>Details</summary>
Motivation: LLM服务成本高昂，多模型服务中的GPU利用率低和动态工作负载挑战需要新的解决方案。

Method: Prism通过动态内存映射和两级调度策略实现跨模型内存协调，灵活共享GPU资源。

Result: 在真实场景测试中，Prism实现了2倍以上的成本节省和3.3倍的SLO达标率提升。

Conclusion: Prism通过创新的内存共享和调度机制，为多LLM服务提供了高效且经济的解决方案。

Abstract: Serving large language models (LLMs) is expensive, especially for providers
hosting many models, making cost reduction essential. The unique workload
patterns of serving multiple LLMs (i.e., multi-LLM serving) create new
opportunities and challenges for this task. The long-tail popularity of models
and their long idle periods present opportunities to improve utilization
through GPU sharing. However, existing GPU sharing systems lack the ability to
adjust their resource allocation and sharing policies at runtime, making them
ineffective at meeting latency service-level objectives (SLOs) under rapidly
fluctuating workloads.
  This paper presents Prism, a multi-LLM serving system that unleashes the full
potential of GPU sharing to achieve both cost efficiency and SLO attainment. At
its core, Prism tackles a key limitation of existing
systems$\unicode{x2014}$the lack of $\textit{cross-model memory coordination}$,
which is essential for flexibly sharing GPU memory across models under dynamic
workloads. Prism achieves this with two key designs. First, it supports
on-demand memory allocation by dynamically mapping physical to virtual memory
pages, allowing flexible memory redistribution among models that space- and
time-share a GPU. Second, it improves memory efficiency through a two-level
scheduling policy that dynamically adjusts sharing strategies based on models'
runtime demands. Evaluations on real-world traces show that Prism achieves more
than $2\times$ cost savings and $3.3\times$ SLO attainment compared to
state-of-the-art systems.

</details>


### [189] [MARCO: A Multi-Agent System for Optimizing HPC Code Generation Using Large Language Models](https://arxiv.org/abs/2505.03906)
*Asif Rahman,Veljko Cvetkovic,Kathleen Reece,Aidan Walters,Yasir Hassan,Aneesh Tummeti,Bryan Torres,Denise Cooney,Margaret Ellis,Dimitrios S. Nikolopoulos*

Main category: cs.DC

TL;DR: MARCO框架通过多智能体架构优化LLM生成的HPC代码，结合实时网络搜索技术，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 通用LLM在高性能计算（HPC）代码生成中因缺乏专业优化而效果有限，需针对性解决方案。

Method: MARCO采用多智能体架构，包括代码生成和性能评估代理，通过反馈循环和实时网络搜索优化代码。

Result: 在LeetCode 75问题集上，MARCO平均运行时减少14.6%，结合网络搜索后性能提升30.9%。

Conclusion: 多智能体系统能有效满足HPC代码生成的专业需求，是领域特定模型微调的经济替代方案。

Abstract: Large language models (LLMs) have transformed software development through
code generation capabilities, yet their effectiveness for high-performance
computing (HPC) remains limited. HPC code requires specialized optimizations
for parallelism, memory efficiency, and architecture-specific considerations
that general-purpose LLMs often overlook. We present MARCO (Multi-Agent
Reactive Code Optimizer), a novel framework that enhances LLM-generated code
for HPC through a specialized multi-agent architecture. MARCO employs separate
agents for code generation and performance evaluation, connected by a feedback
loop that progressively refines optimizations. A key innovation is MARCO's
web-search component that retrieves real-time optimization techniques from
recent conference proceedings and research publications, bridging the knowledge
gap in pre-trained LLMs. Our extensive evaluation on the LeetCode 75 problem
set demonstrates that MARCO achieves a 14.6% average runtime reduction compared
to Claude 3.5 Sonnet alone, while the integration of the web-search component
yields a 30.9% performance improvement over the base MARCO system. These
results highlight the potential of multi-agent systems to address the
specialized requirements of high-performance code generation, offering a
cost-effective alternative to domain-specific model fine-tuning.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [190] [PARC: Physics-based Augmentation with Reinforcement Learning for Character Controllers](https://arxiv.org/abs/2505.04002)
*Michael Xu,Yi Shi,KangKang Yin,Xue Bin Peng*

Main category: cs.GR

TL;DR: PARC框架通过机器学习和物理模拟迭代增强运动数据集，解决敏捷地形穿越控制器开发中的数据稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 人类在复杂环境中表现出敏捷运动能力，但模拟这些动作因数据稀缺和高成本而困难。

Method: PARC结合运动生成器和物理跟踪控制器，迭代生成并修正合成运动数据。

Result: PARC生成了敏捷且通用的地形穿越模型，填补了数据稀缺与控制器需求之间的鸿沟。

Conclusion: PARC为开发敏捷地形穿越控制器提供了有效方法，扩展了运动生成和跟踪能力。

Abstract: Humans excel in navigating diverse, complex environments with agile motor
skills, exemplified by parkour practitioners performing dynamic maneuvers, such
as climbing up walls and jumping across gaps. Reproducing these agile movements
with simulated characters remains challenging, in part due to the scarcity of
motion capture data for agile terrain traversal behaviors and the high cost of
acquiring such data. In this work, we introduce PARC (Physics-based
Augmentation with Reinforcement Learning for Character Controllers), a
framework that leverages machine learning and physics-based simulation to
iteratively augment motion datasets and expand the capabilities of terrain
traversal controllers. PARC begins by training a motion generator on a small
dataset consisting of core terrain traversal skills. The motion generator is
then used to produce synthetic data for traversing new terrains. However, these
generated motions often exhibit artifacts, such as incorrect contacts or
discontinuities. To correct these artifacts, we train a physics-based tracking
controller to imitate the motions in simulation. The corrected motions are then
added to the dataset, which is used to continue training the motion generator
in the next iteration. PARC's iterative process jointly expands the
capabilities of the motion generator and tracker, creating agile and versatile
models for interacting with complex environments. PARC provides an effective
approach to develop controllers for agile terrain traversal, which bridges the
gap between the scarcity of motion data and the need for versatile character
controllers.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [191] [AccLLM: Accelerating Long-Context LLM Inference Via Algorithm-Hardware Co-Design](https://arxiv.org/abs/2505.03745)
*Yanbiao Liang,Huihong Shi,Haikuo Shao,Zhongfeng Wang*

Main category: cs.AR

TL;DR: AccLLM是一个通过算法与硬件协同设计的高效加速框架，用于在资源受限的边缘设备上部署大型语言模型（LLMs）。


<details>
  <summary>Details</summary>
Motivation: 由于LLMs在自然语言处理领域的成功，需求从云端扩展到边缘设备，但面临计算密集、内存带宽需求高和长序列处理能力有限等挑战。

Method: 算法层面采用剪枝、Λ形注意力和W2A8KV4量化方案；硬件层面设计基于FPGA的可重构加速器。

Result: 在Xilinx Alveo U280 FPGA上验证，能效提升4.07倍，吞吐量提升2.98倍。

Conclusion: AccLLM有效解决了边缘设备部署LLMs的挑战，实现了高效的长序列生成。

Abstract: Recently, large language models (LLMs) have achieved huge success in the
natural language processing (NLP) field, driving a growing demand to extend
their deployment from the cloud to edge devices. However, deploying LLMs on
resource-constrained edge devices poses significant challenges, including (1)
intensive computations and huge model sizes, (2) great memory and bandwidth
demands introduced by the autoregressive generation process, and (3) limited
scalability for handling long sequences. To address these challenges, we
propose AccLLM, a comprehensive acceleration framework that enables efficient
and fast long-context LLM inference through algorithm and hardware co-design.
At the algorithmic level, we integrate (1) pruning, (2) {\Lambda}-shaped
attention, and (3) an innovative W2A8KV4 (2-bit weights, 8-bit activations, and
4-bit KV cache) quantization scheme, thus effectively reducing memory and
bandwidth requirements while facilitating LLMs' long-sequence generation. At
the hardware level, we design a dedicated FPGA-based accelerator with a
reconfigurable computing engine to effectively and flexibly accommodate diverse
operations arising from our compression algorithm, thereby fully translating
the algorithmic innovations into tangible hardware efficiency. We validate
AccLLM on the Xilinx Alveo U280 FPGA, demonstrating a 4.07x energy efficiency
and a 2.98x throughput compared to the state-of-the-art work FlightLLM.

</details>


### [192] [APSQ: Additive Partial Sum Quantization with Algorithm-Hardware Co-Design](https://arxiv.org/abs/2505.03748)
*Yonghao Tan,Pingcheng Dong,Yongkun Wu,Yu Liu,Xuejiao Liu,Peng Luo,Shih-Yang Liu,Xijie Huang,Dong Zhang,Luhong Liang,Kwang-Ting Cheng*

Main category: cs.AR

TL;DR: 论文提出了一种新型的加法部分和量化（APSQ）方法，通过将部分和（PSUM）量化融入量化框架，显著降低了内存需求和能耗。


<details>
  <summary>Details</summary>
Motivation: 传统压缩策略忽视了PSUM量化，而PSUM的高精度访问导致了内存和能耗问题。

Method: 提出了APSQ方法，结合分组策略和可重构架构，实现PSUM的INT8压缩。

Result: 在NLP和CV任务中表现接近无损，能耗降低28-87%，并在LLaMA2-7B上验证了扩展潜力。

Conclusion: APSQ是一种高效的PSUM量化方法，适用于多种模型，显著提升了能效。

Abstract: DNN accelerators, significantly advanced by model compression and specialized
dataflow techniques, have marked considerable progress. However, the frequent
access of high-precision partial sums (PSUMs) leads to excessive memory demands
in architectures utilizing input/weight stationary dataflows. Traditional
compression strategies have typically overlooked PSUM quantization, which may
account for 69% of power consumption. This study introduces a novel Additive
Partial Sum Quantization (APSQ) method, seamlessly integrating PSUM
accumulation into the quantization framework. A grouping strategy that combines
APSQ with PSUM quantization enhanced by a reconfigurable architecture is
further proposed. The APSQ performs nearly lossless on NLP and CV tasks across
BERT, Segformer, and EfficientViT models while compressing PSUMs to INT8. This
leads to a notable reduction in energy costs by 28-87%. Extended experiments on
LLaMA2-7B demonstrate the potential of APSQ for large language models. Code is
available at https://github.com/Yonghao-Tan/APSQ.

</details>


### [193] [AI-Powered Agile Analog Circuit Design and Optimization](https://arxiv.org/abs/2505.03750)
*Jinhai Hu,Wang Ling Goh,Yuan Gao*

Main category: cs.AR

TL;DR: 本文探讨了AI在模拟电路设计中的应用，结合多目标贝叶斯优化和系统级建模，提升性能并减少设计迭代。


<details>
  <summary>Details</summary>
Motivation: AI技术为模拟电路设计带来变革，通过自动化设备级调优和系统级协同优化，提高效率。

Method: 采用多目标贝叶斯优化（MOBO）进行晶体管尺寸调整，并结合AI建模优化系统级功能（如关键词识别应用中的带通滤波器）。

Result: 实验证明，AI能显著提升模拟电路性能，减少设计迭代，并实现组件与应用指标的联合优化。

Conclusion: AI在模拟电路设计中展现出巨大潜力，为未来设计提供了高效、自动化的解决方案。

Abstract: Artificial intelligence (AI) techniques are transforming analog circuit
design by automating device-level tuning and enabling system-level
co-optimization. This paper integrates two approaches: (1) AI-assisted
transistor sizing using Multi-Objective Bayesian Optimization (MOBO) for direct
circuit parameter optimization, demonstrated on a linearly tunable
transconductor; and (2) AI-integrated circuit transfer function modeling for
system-level optimization in a keyword spotting (KWS) application, demonstrated
by optimizing an analog bandpass filter within a machine learning training
loop. The combined insights highlight how AI can improve analog performance,
reduce design iteration effort, and jointly optimize analog components and
application-level metrics.

</details>


### [194] [Improving the Serving Performance of Multi-LoRA Large Language Models via Efficient LoRA and KV Cache Management](https://arxiv.org/abs/2505.03756)
*Hang Zhang,Jiuchen Shi,Yixiao Wang,Quan Chen,Yizhou Shan,Minyi Guo*

Main category: cs.AR

TL;DR: FASTLIBRA是一个优化多LoRA推理性能的系统，通过依赖感知缓存管理和性能驱动的缓存交换，显著降低首次令牌时间（TTFT）。


<details>
  <summary>Details</summary>
Motivation: 现有多LoRA推理系统未能优化服务性能（如TTFT），且忽略了LoRA和KV缓存的依赖关系。

Method: FASTLIBRA包含依赖感知缓存管理器和性能驱动缓存交换器，统一管理缓存池并基于成本模型动态交换缓存。

Result: 实验表明，FASTLIBRA平均降低TTFT达63.4%，优于现有技术。

Conclusion: FASTLIBRA通过优化缓存管理显著提升了多LoRA推理性能。

Abstract: Multiple Low-Rank Adapters (Multi-LoRAs) are gaining popularity for
task-specific Large Language Model (LLM) applications. For multi-LoRA serving,
caching hot KV caches and LoRA adapters in high bandwidth memory of
accelerations can improve inference performance. However, existing Multi-LoRA
inference systems fail to optimize serving performance like Time-To-First-Toke
(TTFT), neglecting usage dependencies when caching LoRAs and KVs. We therefore
propose FASTLIBRA, a Multi-LoRA caching system to optimize the serving
performance. FASTLIBRA comprises a dependency-aware cache manager and a
performance-driven cache swapper. The cache manager maintains the usage
dependencies between LoRAs and KV caches during the inference with a unified
caching pool. The cache swapper determines the swap-in or out of LoRAs and KV
caches based on a unified cost model, when the HBM is idle or busy,
respectively. Experimental results show that ELORA reduces the TTFT by 63.4% on
average, compared to state-of-the-art works.

</details>


### [195] [Splitwiser: Efficient LM inference with constrained resources](https://arxiv.org/abs/2505.03763)
*Asad Aali,Adney Cardoza,Melissa Capo*

Main category: cs.AR

TL;DR: Splitwiser是一种将LLM推理请求的两个阶段（提示计算和令牌生成）拆分到同一GPU上的方法，以减少开销并提高资源利用率。


<details>
  <summary>Details</summary>
Motivation: LLM推理中的令牌生成阶段未能充分利用计算资源，尤其是与提示计算阶段相比，导致效率低下。

Method: 提出Splitwiser方法，将提示计算和令牌生成阶段拆分到同一GPU上，减少数据传输开销，并开源了在Huggingface和vLLM上的实现。

Result: 初步结果显示Splitwiser能减少网络相关开销，提高内存访问和缓存利用率。

Conclusion: Splitwiser为LLM推理提供了一种高效的解决方案，通过优化资源分配和减少数据传输，提升了整体性能。

Abstract: Efficient inference of LLMs remains a crucial challenge, with two main
phases: a compute-intensive prompt computation and a memory-intensive token
generation. Despite existing batching and scheduling techniques, token
generation phases fail to fully utilize compute resources, especially when
compared to prompt computation phases. To address these challenges, we propose
Splitwiser, a methodology that splits the two phases of an LLM inference
request onto the same GPU, thereby reducing overhead and improving memory
access and cache utilization. By eliminating the need to transfer data across
devices, Splitwiser aims to minimize network-related overheads. In this report,
we describe the basic structure of our proposed pipeline while sharing
preliminary results and analysis. We implement our proposed multiprocessing
design on two widely-used and independent LLM architectures: Huggingface and
vLLM. We open-source our code for the respective implementations: 1)
Huggingface (https://github.com/asad-aali/splitwiser), and 2) vLLM
(https://github.com/adney11/vllm-sysml).

</details>


### [196] [GPU Performance Portability needs Autotuning](https://arxiv.org/abs/2505.03780)
*Burkhard Ringlein,Thomas Parnell,Radu Stoica*

Main category: cs.AR

TL;DR: 论文提出结合JIT编译与内核参数自动调优，实现无需代码修改的便携式高性能LLM执行，显著提升性能与多样性。


<details>
  <summary>Details</summary>
Motivation: 当前依赖单一平台限制了LLM的便携性，导致供应商锁定和AI硬件创新障碍。

Method: 结合JIT编译与内核参数自动调优，专注于Flash Attention内核。

Result: 探索了15倍多的内核参数配置，代码多样性显著提升，性能超越供应商优化实现230%，同时减少70倍内核代码量。

Conclusion: 自动调优是实现跨GPU供应商模型便携性的有前景路径。

Abstract: As LLMs grow in complexity, achieving state-of-the-art performance requires
tight co-design across algorithms, software, and hardware. Today's reliance on
a single dominant platform limits portability, creates vendor lock-in, and
raises barriers for new AI hardware. In this work, we make the case for
combining just-in-time (JIT) compilation with kernel parameter autotuning to
enable portable, state-of-the-art performance LLM execution without code
changes. Focusing on flash attention -- a widespread performance-critical LLM
kernel -- we demonstrate that this approach explores up to 15x more kernel
parameter configurations, produces significantly more diverse code across
multiple dimensions, and even outperforms vendor-optimized implementations by
up to 230%, all while reducing kernel code size by 70x and eliminating manual
code optimizations. Our results highlight autotuning as a promising path to
unlocking model portability across GPU vendors.

</details>


<div id='q-fin.PM'></div>

# q-fin.PM [[Back]](#toc)

### [197] [Deep Reinforcement Learning for Investor-Specific Portfolio Optimization: A Volatility-Guided Asset Selection Approach](https://arxiv.org/abs/2505.03760)
*Arishi Orra,Aryan Bhambu,Himanshu Choudhary,Manoj Thakur,Selvaraju Natarajan*

Main category: q-fin.PM

TL;DR: 该研究提出了一种基于波动率引导的深度强化学习（DRL）投资组合优化框架，结合投资者的风险偏好动态构建投资组合，并通过GARCH模型预测股票波动率。


<details>
  <summary>Details</summary>
Motivation: 传统投资组合优化需平衡风险与回报，而DRL的适应性使其成为理想工具。但资产预选对投资组合表现至关重要，需结合投资者偏好。

Method: 使用GARCH模型预测股票波动率并分类，DRL代理通过历史市场数据学习最优投资策略。

Result: 基于Dow 30指数的实验表明，该方法在风险调整后收益上优于基准策略。

Conclusion: 结合波动率预测和DRL的投资组合优化框架能有效满足投资者特定需求，提升投资表现。

Abstract: Portfolio optimization requires dynamic allocation of funds by balancing the
risk and return tradeoff under dynamic market conditions. With the recent
advancements in AI, Deep Reinforcement Learning (DRL) has gained prominence in
providing adaptive and scalable strategies for portfolio optimization. However,
the success of these strategies depends not only on their ability to adapt to
market dynamics but also on the careful pre-selection of assets that influence
overall portfolio performance. Incorporating the investor's preference in
pre-selecting assets for a portfolio is essential in refining their investment
strategies. This study proposes a volatility-guided DRL-based portfolio
optimization framework that dynamically constructs portfolios based on
investors' risk profiles. The Generalized Autoregressive Conditional
Heteroscedasticity (GARCH) model is utilized for volatility forecasting of
stocks and categorizes them based on their volatility as aggressive, moderate,
and conservative. The DRL agent is then employed to learn an optimal investment
policy by interacting with the historical market data. The efficacy of the
proposed methodology is established using stocks from the Dow $30$ index. The
proposed investor-specific DRL-based portfolios outperformed the baseline
strategies by generating consistent risk-adjusted returns.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [198] [Promoting Security and Trust on Social Networks: Explainable Cyberbullying Detection Using Large Language Models in a Stream-Based Machine Learning Framework](https://arxiv.org/abs/2505.03746)
*Silvia García-Méndez,Francisco De Arriba-Pérez*

Main category: cs.SI

TL;DR: 论文提出了一种基于流式机器学习和大型语言模型的实时网络欺凌检测方法，性能接近90%，并提供了可解释性仪表板。


<details>
  <summary>Details</summary>
Motivation: 社交媒体平台虽然促进了即时连接，但也带来了网络欺凌等负面行为。现有生成式AI研究多集中于零/少样本学习，本文探索其在新场景下的性能。

Method: 结合流式机器学习模型（增量处理数据）和大型语言模型（特征工程），实时检测网络欺凌，并提供可解释性仪表板。

Result: 实验数据显示性能接近90%，优于现有文献中的方法。

Conclusion: 该方案通过实时检测网络欺凌，有助于提升在线社区的安全性，减少社会负面影响。

Abstract: Social media platforms enable instant and ubiquitous connectivity and are
essential to social interaction and communication in our technological society.
Apart from its advantages, these platforms have given rise to negative
behaviors in the online community, the so-called cyberbullying. Despite the
many works involving generative Artificial Intelligence (AI) in the literature
lately, there remain opportunities to study its performance apart from
zero/few-shot learning strategies. Accordingly, we propose an innovative and
real-time solution for cyberbullying detection that leverages stream-based
Machine Learning (ML) models able to process the incoming samples incrementally
and Large Language Models (LLMS) for feature engineering to address the
evolving nature of abusive and hate speech online. An explainability dashboard
is provided to promote the system's trustworthiness, reliability, and
accountability. Results on experimental data report promising performance close
to 90 % in all evaluation metrics and surpassing those obtained by competing
works in the literature. Ultimately, our proposal contributes to the safety of
online communities by timely detecting abusive behavior to prevent long-lasting
harassment and reduce the negative consequences in society.

</details>


### [199] [The Influence of Text Variation on User Engagement in Cross-Platform Content Sharing](https://arxiv.org/abs/2505.03769)
*Yibo Hu,Yiqiao Jin,Meng Ye,Ajay Divakaran,Srijan Kumar*

Main category: cs.SI

TL;DR: 研究探讨了改写Reddit帖子标题（源自YouTube视频标题）对用户参与度的影响，发现改写能显著提升参与度，并揭示了有效改写的特征。


<details>
  <summary>Details</summary>
Motivation: 在跨平台社交媒体环境中，理解多模态内容（尤其是文本与视觉结合）如何驱动用户参与度仍是一个复杂问题。

Method: 研究分两部分：1）分析Reddit帖子数据集，发现21%的标题被最小修改；2）设计多阶段实验，隔离文本变化的影响，并通过BERT分类器进行排名预测。

Result: 统计测试表明，有效的标题改写具有情感共鸣、词汇丰富性和社区规范对齐性；BERT分类器准确率达74%，显著优于基线。

Conclusion: 研究验证了数据集能最小化混杂效应，揭示了文本特征对参与度的影响，为跨平台多模态内容策略提供了框架。

Abstract: In today's cross-platform social media landscape, understanding factors that
drive engagement for multimodal content, especially text paired with visuals,
remains complex. This study investigates how rewriting Reddit post titles
adapted from YouTube video titles affects user engagement. First, we build and
analyze a large dataset of Reddit posts sharing YouTube videos, revealing that
21% of post titles are minimally modified. Statistical analysis demonstrates
that title rewrites measurably improve engagement. Second, we design a
controlled, multi-phase experiment to rigorously isolate the effects of textual
variations by neutralizing confounding factors like video popularity, timing,
and community norms. Comprehensive statistical tests reveal that effective
title rewrites tend to feature emotional resonance, lexical richness, and
alignment with community-specific norms. Lastly, pairwise ranking prediction
experiments using a fine-tuned BERT classifier achieves 74% accuracy,
significantly outperforming near-random baselines, including GPT-4o. These
results validate that our controlled dataset effectively minimizes confounding
effects, allowing advanced models to both learn and demonstrate the impact of
textual features on engagement. By bridging quantitative rigor with qualitative
insights, this study uncovers engagement dynamics and offers a robust framework
for future cross-platform, multimodal content strategies.

</details>


### [200] [Modeling Human Behavior in a Strategic Network Game with Complex Group Dynamics](https://arxiv.org/abs/2505.03795)
*Jacob W. Crandall,Jonathan Skaggs*

Main category: cs.SI

TL;DR: 论文比较了几种建模人类行为的方法，发现基于社区意识和分布建模的方法（hCAB）表现最佳，能准确模拟小群体动态，且人类难以区分其与真实人类行为。


<details>
  <summary>Details</summary>
Motivation: 理解人类网络对社会不平等、贫困等问题的影响，通过研究战略网络游戏中的行为模型，推动对网络行为的理解。

Method: 比较不同建模方法，包括行为匹配与社区意识行为假设，以及均值与分布建模。

Result: hCAB模型在小群体中表现最佳，能准确模拟人类行为，且人类难以区分其与真实行为。

Conclusion: hCAB模型在战略网络游戏中能有效模拟人类行为，为理解网络动态提供了新工具。

Abstract: Human networks greatly impact important societal outcomes, including wealth
and health inequality, poverty, and bullying. As such, understanding human
networks is critical to learning how to promote favorable societal outcomes. As
a step toward better understanding human networks, we compare and contrast
several methods for learning models of human behavior in a strategic network
game called the Junior High Game (JHG). These modeling methods differ with
respect to the assumptions they use to parameterize human behavior (behavior
vs. community-aware behavior) and the statistical moments they model (mean vs.
distribution). Results show that the highest-performing method models the
population's distribution rather than the mean and assumes humans use
community-aware behavior rather than behavior matching. When applied to small
societies (6-11 individuals), this learned model, called hCAB, closely mirrors
the population dynamics of human groups (with some differences). Additionally,
a user study reveals that human participants were unable to distinguish hCAB
agents from other humans, thus illustrating that individual hCAB behavior
plausibly mirrors human behavior in this strategic network game.

</details>


### [201] [Geospatial and Temporal Trends in Urban Transportation: A Study of NYC Taxis and Pathao Food Deliveries](https://arxiv.org/abs/2505.03816)
*Bidyarthi Paul,Fariha Tasnim Chowdhury,Dipta Biswas,Meherin Sultana*

Main category: cs.SI

TL;DR: 该研究通过分析纽约和达卡的交通数据，识别需求趋势、高峰时段和热点区域，为优化城市交通提供见解。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过分析交通数据，优化城市交通效率，提升乘客和货物运输服务。

Method: 采用EDA、地理空间分析、SARIMAX时间序列模型和聚类技术分析数据。

Result: 识别了需求趋势、高峰时段和热点区域，为车队管理和资源分配提供依据。

Conclusion: 研究结果为提升城市交通效率和服务质量提供了实用建议。

Abstract: Urban transportation plays a vital role in modern city life, affecting how
efficiently people and goods move around. This study analyzes transportation
patterns using two datasets: the NYC Taxi Trip dataset from New York City and
the Pathao Food Trip dataset from Dhaka, Bangladesh. Our goal is to identify
key trends in demand, peak times, and important geographical hotspots. We start
with Exploratory Data Analysis (EDA) to understand the basic characteristics of
the datasets. Next, we perform geospatial analysis to map out high-demand and
low-demand regions. We use the SARIMAX model for time series analysis to
forecast demand patterns, capturing seasonal and weekly variations. Lastly, we
apply clustering techniques to identify significant areas of high and low
demand. Our findings provide valuable insights for optimizing fleet management
and resource allocation in both passenger transport and food delivery services.
These insights can help improve service efficiency, better meet customer needs,
and enhance urban transportation systems in diverse urban environments.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [202] [In-Context Adaptation to Concept Drift for Learned Database Operations](https://arxiv.org/abs/2505.04404)
*Jiaqi Zhu,Shaofeng Cai,Yanyan Shen,Gang Chen,Fang Deng,Beng Chin Ooi*

Main category: cs.DB

TL;DR: FLAIR是一个在线适应框架，通过动态上下文构建解决数据库环境中概念漂移问题，无需运行时参数优化，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 动态数据库环境中频繁更新和数据分布变化导致概念漂移，传统学习模型性能下降，需高效适应框架。

Method: FLAIR引入“上下文内适应”范式，利用数据系统特性动态构建上下文，通过任务特征化模块和动态决策引擎实现无缝适应。

Result: FLAIR在关键数据库任务中表现优异，适应速度提升5.2倍，基数估计误差降低22.5%。

Conclusion: FLAIR为动态数据库环境中的学习模型提供高效适应方案，显著提升性能并减少误差。

Abstract: Machine learning has demonstrated transformative potential for database
operations, such as query optimization and in-database data analytics. However,
dynamic database environments, characterized by frequent updates and evolving
data distributions, introduce concept drift, which leads to performance
degradation for learned models and limits their practical applicability.
Addressing this challenge requires efficient frameworks capable of adapting to
shifting concepts while minimizing the overhead of retraining or fine-tuning.
  In this paper, we propose FLAIR, an online adaptation framework that
introduces a new paradigm called \textit{in-context adaptation} for learned
database operations. FLAIR leverages the inherent property of data systems,
i.e., immediate availability of execution results for predictions, to enable
dynamic context construction. By formalizing adaptation as $f:(\mathbf{x} \,|
\,\mathcal{C}_t) \to \mathbf{y}$, with $\mathcal{C}_t$ representing a dynamic
context memory, FLAIR delivers predictions aligned with the current concept,
eliminating the need for runtime parameter optimization. To achieve this, FLAIR
integrates two key modules: a Task Featurization Module for encoding
task-specific features into standardized representations, and a Dynamic
Decision Engine, pre-trained via Bayesian meta-training, to adapt seamlessly
using contextual information at runtime. Extensive experiments across key
database tasks demonstrate that FLAIR outperforms state-of-the-art baselines,
achieving up to 5.2x faster adaptation and reducing error by 22.5% for
cardinality estimation.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [203] [Likelihood-Free Adaptive Bayesian Inference via Nonparametric Distribution Matching](https://arxiv.org/abs/2505.04603)
*Wenhui Sophia Lu,Wing Hung Wong*

Main category: stat.ME

TL;DR: 论文提出了一种名为自适应贝叶斯推断（ABI）的新框架，通过直接比较后验空间中的分布来解决传统近似贝叶斯计算（ABC）在高维或扩散先验下的计算效率问题。


<details>
  <summary>Details</summary>
Motivation: 传统ABC方法在高维或扩散先验下计算效率低下，需要一种更高效的后验推断方法。

Method: ABI采用非参数分布匹配，利用边际增强切片Wasserstein（MSW）距离，将后验分布差异转化为一维条件分位数回归任务，并结合自适应拒绝采样方案。

Result: 理论证明ABI后验收敛于真实后验，实验表明ABI在高维或依赖观测场景下显著优于其他方法。

Conclusion: ABI是一种高效且理论可靠的后验推断方法，适用于复杂场景。

Abstract: When the likelihood is analytically unavailable and computationally
intractable, approximate Bayesian computation (ABC) has emerged as a widely
used methodology for approximate posterior inference; however, it suffers from
severe computational inefficiency in high-dimensional settings or under diffuse
priors. To overcome these limitations, we propose Adaptive Bayesian Inference
(ABI), a framework that bypasses traditional data-space discrepancies and
instead compares distributions directly in posterior space through
nonparametric distribution matching. By leveraging a novel Marginally-augmented
Sliced Wasserstein (MSW) distance on posterior measures and exploiting its
quantile representation, ABI transforms the challenging problem of measuring
divergence between posterior distributions into a tractable sequence of
one-dimensional conditional quantile regression tasks. Moreover, we introduce a
new adaptive rejection sampling scheme that iteratively refines the posterior
approximation by updating the proposal distribution via generative density
estimation. Theoretically, we establish parametric convergence rates for the
trimmed MSW distance and prove that the ABI posterior converges to the true
posterior as the tolerance threshold vanishes. Through extensive empirical
evaluation, we demonstrate that ABI significantly outperforms data-based
Wasserstein ABC, summary-based ABC, and state-of-the-art likelihood-free
simulators, especially in high-dimensional or dependent observation regimes.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [204] [The Shift Towards Preprints in AI Policy Research: A Comparative Study of Preprint Trends in the U.S., Europe, and South Korea](https://arxiv.org/abs/2505.03835)
*Simon Suh,Jihyuk Bang,Ji Woo Han*

Main category: cs.DL

TL;DR: 研究探讨了COVID-19疫情和ChatGPT发布对全球AI政策研究中预印本引用趋势的影响，发现不同地区的增长模式和速度存在显著差异。


<details>
  <summary>Details</summary>
Motivation: 分析全球性事件（如COVID-19和ChatGPT发布）如何影响AI政策研究中预印本的传播模式，并比较不同地区的反应。

Method: 利用Web of Science的文献计量数据，追踪2015至2024年间美国、欧洲和韩国的预印本引用趋势，并标记关键事件时间点。

Result: 所有地区的预印本引用均增长，但美国呈现事件驱动的急剧增长，欧洲为机构性增长，韩国则保持线性增长。

Conclusion: 全球性事件加速了预印本采用，但具体模式和速度受本地研究文化和政策环境影响，未来AI治理需考虑区域差异。

Abstract: The adoption of open science has quickly changed how artificial intelligence
(AI) policy research is distributed globally. This study examines the regional
trends in the citation of preprints, specifically focusing on the impact of two
major disruptive events: the COVID-19 pandemic and the release of ChatGPT, on
research dissemination patterns in the United States, Europe, and South Korea
from 2015 to 2024. Using bibliometrics data from the Web of Science, this study
tracks how global disruptive events influenced the adoption of preprints in AI
policy research and how such shifts vary by region. By marking the timing of
these disruptive events, the analysis reveals that while all regions
experienced growth in preprint citations, the magnitude and trajectory of
change varied significantly. The United States exhibited sharp, event-driven
increases; Europe demonstrated institutional growth; and South Korea maintained
consistent, linear growth in preprint adoption. These findings suggest that
global disruptions may have accelerated preprint adoption, but the extent and
trajectory are shaped by local research cultures, policy environments, and
levels of open science maturity. This paper emphasizes the need for future AI
governance strategies to consider regional variability in research
dissemination and highlights opportunities for further longitudinal and
comparative research to deepen our understanding of open-access adoption in AI
policy development.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [205] [A Heuristic-Integrated DRL Approach for Phase Optimization in Large-Scale RISs](https://arxiv.org/abs/2505.04401)
*Wei Wang,Peizheng Li,Angela Doufexi,Mark A. Beach*

Main category: eess.SP

TL;DR: 提出了一种结合启发式与深度强化学习的框架，用于优化大规模可重构智能表面的离散相位偏移。


<details>
  <summary>Details</summary>
Motivation: 由于离散相位偏移的非凸和非线性特性，优化大规模可重构智能表面（RIS）的相位配置具有挑战性。

Method: 结合双深度Q网络（DDQN）进行列级控制和贪婪算法（GA）进行元素级优化，通过多步累积动作和状态细化优化RIS配置。

Result: 该方法在小动作空间中有效优化了大规模RIS的相位偏移配置。

Conclusion: 提出的框架能够高效解决大规模RIS的优化问题。

Abstract: Optimizing discrete phase shifts in large-scale reconfigurable intelligent
surfaces (RISs) is challenging due to their non-convex and non-linear nature.
In this letter, we propose a heuristic-integrated deep reinforcement learning
(DRL) framework that (1) leverages accumulated actions over multiple steps in
the double deep Q-network (DDQN) for RIS column-wise control and (2) integrates
a greedy algorithm (GA) into each DRL step to refine the state via
fine-grained, element-wise optimization of RIS configurations. By learning from
GA-included states, the proposed approach effectively addresses RIS
optimization within a small DRL action space, demonstrating its capability to
optimize phase-shift configurations of large-scale RISs.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [206] [Beyond Misinformation: A Conceptual Framework for Studying AI Hallucinations in (Science) Communication](https://arxiv.org/abs/2504.13777)
*Anqi Shao*

Main category: cs.HC

TL;DR: 本文提出一个概念框架，将AI幻觉视为一种独特的虚假信息形式，区别于传统的人类意图驱动的虚假信息。


<details>
  <summary>Details</summary>
Motivation: 传统虚假信息研究关注人类意图，而生成式AI系统在无意图情况下产生虚假但看似合理的内容。本文旨在探讨AI幻觉的社会影响及其与人类虚假信息的差异。

Method: 采用供需模型和分布式代理概念，分析AI幻觉在生产、感知和机构响应方面与人类虚假信息的不同。

Result: 提出一个研究议程，呼吁传播学者从宏观（机构）、中观（群体）和微观（个体）层面研究幻觉内容的产生、传播和受众接受。

Conclusion: 本文呼吁传播学者重新思考虚假信息理论的边界，以适应概率性、非人类行为体在知识生产中的日益嵌入。

Abstract: This paper proposes a conceptual framework for understanding AI
hallucinations as a distinct form of misinformation. While misinformation
scholarship has traditionally focused on human intent, generative AI systems
now produce false yet plausible outputs absent of such intent. I argue that
these AI hallucinations should not be treated merely as technical failures but
as communication phenomena with social consequences. Drawing on a
supply-and-demand model and the concept of distributed agency, the framework
outlines how hallucinations differ from human-generated misinformation in
production, perception, and institutional response. I conclude by outlining a
research agenda for communication scholars to investigate the emergence,
dissemination, and audience reception of hallucinated content, with attention
to macro (institutional), meso (group), and micro (individual) levels. This
work urges communication researchers to rethink the boundaries of
misinformation theory in light of probabilistic, non-human actors increasingly
embedded in knowledge production.

</details>


### [207] [Facilitating Video Story Interaction with Multi-Agent Collaborative System](https://arxiv.org/abs/2505.03807)
*Yiwen Zhang,Jianing Hao,Zhan Wang,Hongling Sheng,Wei Zeng*

Main category: cs.HC

TL;DR: 提出了一种基于用户意图的交互式视频故事系统，结合VLM、RAG和MAS技术，实现个性化叙事体验。


<details>
  <summary>Details</summary>
Motivation: 现有方法局限于用户选择和预设叙事，缺乏定制化。

Method: 系统分三阶段：视频故事处理（VLM）、多空间聊天（MAS）和场景定制化。

Result: 应用于《哈利波特》系列，系统成功展现了角色社交行为和成长。

Conclusion: 该系统显著提升了视频故事世界的交互体验。

Abstract: Video story interaction enables viewers to engage with and explore narrative
content for personalized experiences. However, existing methods are limited to
user selection, specially designed narratives, and lack customization. To
address this, we propose an interactive system based on user intent. Our system
uses a Vision Language Model (VLM) to enable machines to understand video
stories, combining Retrieval-Augmented Generation (RAG) and a Multi-Agent
System (MAS) to create evolving characters and scene experiences. It includes
three stages: 1) Video story processing, utilizing VLM and prior knowledge to
simulate human understanding of stories across three modalities. 2) Multi-space
chat, creating growth-oriented characters through MAS interactions based on
user queries and story stages. 3) Scene customization, expanding and
visualizing various story scenes mentioned in dialogue. Applied to the Harry
Potter series, our study shows the system effectively portrays emergent
character social behavior and growth, enhancing the interactive experience in
the video story world.

</details>


### [208] [Scratch Copilot: Supporting Youth Creative Coding with AI](https://arxiv.org/abs/2505.03867)
*Stefania Druga,Amy J. Ko*

Main category: cs.HC

TL;DR: Cognimates Scratch Copilot是一个为儿童设计的AI编程助手，集成在类似Scratch的环境中，支持实时创意、代码生成、调试和资源创建。研究发现，该工具能增强儿童的创意自我效能感和参与度，同时强调儿童在AI使用中的主动性和创造性控制。


<details>
  <summary>Details</summary>
Motivation: 尽管Scratch等平台为儿童提供了编程机会，但将创意转化为代码仍是一大挑战。现有的AI编程助手主要面向成人，缺乏针对儿童的工具。

Method: 研究开发了Cognimates Scratch Copilot，并在18名国际儿童（7-12岁）中进行了探索性定性评估，分析其如何支持创意编程过程。

Result: AI助手在创意和调试方面提供了显著帮助，儿童表现出对AI建议的主动调整或拒绝，以保持创意控制。研究还揭示了设计中的张力，如平衡脚手架支持和独立问题解决。

Conclusion: Cognimates Scratch Copilot展示了提升儿童创意自我效能感和参与度的潜力，并提出了优先考虑儿童主动性和批判性互动的设计指南。

Abstract: Creative coding platforms like Scratch have democratized programming for
children, yet translating imaginative ideas into functional code remains a
significant hurdle for many young learners. While AI copilots assist adult
programmers, few tools target children in block-based environments. Building on
prior research \cite{druga_how_2021,druga2023ai, druga2023scratch}, we present
Cognimates Scratch Copilot: an AI-powered assistant integrated into a
Scratch-like environment, providing real-time support for ideation, code
generation, debugging, and asset creation. This paper details the system
architecture and findings from an exploratory qualitative evaluation with 18
international children (ages 7--12). Our analysis reveals how the AI Copilot
supported key creative coding processes, particularly aiding ideation and
debugging. Crucially, it also highlights how children actively negotiated the
use of AI, demonstrating strong agency by adapting or rejecting suggestions to
maintain creative control. Interactions surfaced design tensions between
providing helpful scaffolding and fostering independent problem-solving, as
well as learning opportunities arising from navigating AI limitations and
errors. Findings indicate Cognimates Scratch Copilot's potential to enhance
creative self-efficacy and engagement. Based on these insights, we propose
initial design guidelines for AI coding assistants that prioritize youth agency
and critical interaction alongside supportive scaffolding.

</details>


### [209] [Steerable Chatbots: Personalizing LLMs with Preference-Based Activation Steering](https://arxiv.org/abs/2505.04260)
*Jessica Y. Bo,Tianyu Xu,Ishan Chatterjee,Katrina Passarella-Ward,Achin Kulshrestha,D Shin*

Main category: cs.HC

TL;DR: 论文提出了一种基于激活引导的方法，帮助大型语言模型（LLM）在推理过程中更贴合用户的隐性偏好，无需依赖大量用户历史数据。


<details>
  <summary>Details</summary>
Motivation: 提升用户对AI助手的满意度与留存率，解决普通用户在提示指定能力上的不足。

Method: 利用激活引导技术，通过线性强度因子控制模型输出，嵌入三种交互式聊天机器人界面进行用户研究。

Result: 实验证明该方法能有效对齐用户隐性偏好，同时揭示了用户对不同界面在控制性、易用性和透明度上的偏好差异。

Conclusion: 激活引导是一种轻量且有效的个性化方法，适用于实际对话场景，同时需考虑用户对界面的多样化需求。

Abstract: As large language models (LLMs) improve in their capacity to serve as
personal AI assistants, their ability to output uniquely tailored, personalized
responses that align with the soft preferences of their users is essential for
enhancing user satisfaction and retention. However, untrained lay users have
poor prompt specification abilities and often struggle with conveying their
latent preferences to AI assistants. To address this, we leverage activation
steering to guide LLMs to align with interpretable preference dimensions during
inference. In contrast to memory-based personalization methods that require
longer user history, steering is extremely lightweight and can be easily
controlled by the user via an linear strength factor. We embed steering into
three different interactive chatbot interfaces and conduct a within-subjects
user study (n=14) to investigate how end users prefer to personalize their
conversations. The results demonstrate the effectiveness of preference-based
steering for aligning real-world conversations with hidden user preferences,
and highlight further insights on how diverse values around control, usability,
and transparency lead users to prefer different interfaces.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [210] [AI-Driven IRM: Transforming insider risk management with adaptive scoring and LLM-based threat detection](https://arxiv.org/abs/2505.03796)
*Lokesh Koli,Shubham Kalra,Rohan Thakur,Anas Saifi,Karanpreet Singh*

Main category: cs.CR

TL;DR: 本文提出了一种AI驱动的内部风险管理（IRM）系统，通过行为分析、动态风险评分和实时策略执行，显著提高了内部威胁检测的准确性和适应性。


<details>
  <summary>Details</summary>
Motivation: 传统基于规则的检测系统难以应对内部威胁的隐蔽性和上下文依赖性，因此需要一种更智能、自适应的解决方案。

Method: 采用混合评分机制，从静态PRISM模型过渡到基于自动编码器神经网络的AI模型，结合专家标注的用户活动数据进行训练，并通过迭代反馈持续优化。

Result: 系统将误报率降低59%，真阳性检测率提高30%，每日可处理1000万日志事件，查询延迟低于300毫秒，并减少47%的事件响应时间。

Conclusion: 该IRM系统为内部风险提供了可扩展的主动防御框架，未来将通过可解释AI、联邦学习等技术进一步提升其透明度和适应性。

Abstract: Insider threats pose a significant challenge to organizational security,
often evading traditional rule-based detection systems due to their subtlety
and contextual nature. This paper presents an AI-powered Insider Risk
Management (IRM) system that integrates behavioral analytics, dynamic risk
scoring, and real-time policy enforcement to detect and mitigate insider
threats with high accuracy and adaptability. We introduce a hybrid scoring
mechanism - transitioning from the static PRISM model to an adaptive AI-based
model utilizing an autoencoder neural network trained on expert-annotated user
activity data. Through iterative feedback loops and continuous learning, the
system reduces false positives by 59% and improves true positive detection
rates by 30%, demonstrating substantial gains in detection precision.
Additionally, the platform scales efficiently, processing up to 10 million log
events daily with sub-300ms query latency, and supports automated enforcement
actions for policy violations, reducing manual intervention. The IRM system's
deployment resulted in a 47% reduction in incident response times, highlighting
its operational impact. Future enhancements include integrating explainable AI,
federated learning, graph-based anomaly detection, and alignment with Zero
Trust principles to further elevate its adaptability, transparency, and
compliance-readiness. This work establishes a scalable and proactive framework
for mitigating emerging insider risks in both on-premises and hybrid
environments.

</details>


### [211] [Modeling Behavioral Preferences of Cyber Adversaries Using Inverse Reinforcement Learning](https://arxiv.org/abs/2505.03817)
*Aditya Shinde,Prashant Doshi*

Main category: cs.CR

TL;DR: 本文提出了一种基于系统级审计日志的逆向强化学习（IRL）方法，用于建模攻击者的行为偏好，为网络安全提供新的威胁归因维度。


<details>
  <summary>Details</summary>
Motivation: 现有的攻击者建模方法依赖于不断更新的攻击工具和技术，但这些方法难以捕捉攻击者的内在行为偏好，而后者更具稳定性。本文旨在通过学习攻击者的行为偏好，提供更持久的威胁归因能力。

Method: 利用逆向强化学习（IRL）从攻击溯源图中提取攻击者的状态-动作轨迹，建模其行为偏好。实验基于真实攻击数据的开放审计日志数据集。

Result: 结果表明，低级别的取证数据可以自动揭示攻击者的主观偏好，这些偏好具有不变性，可作为攻击者的独特行为特征。

Conclusion: 攻击者的行为偏好是其固有特征，不受工具变化影响，可作为威胁归因的新维度，提升网络安全防御能力。

Abstract: This paper presents a holistic approach to attacker preference modeling from
system-level audit logs using inverse reinforcement learning (IRL). Adversary
modeling is an important capability in cybersecurity that lets defenders
characterize behaviors of potential attackers, which enables attribution to
known cyber adversary groups. Existing approaches rely on documenting an
ever-evolving set of attacker tools and techniques to track known threat
actors. Although attacks evolve constantly, attacker behavioral preferences are
intrinsic and less volatile. Our approach learns the behavioral preferences of
cyber adversaries from forensics data on their tools and techniques. We model
the attacker as an expert decision-making agent with unknown behavioral
preferences situated in a computer host. We leverage attack provenance graphs
of audit logs to derive a state-action trajectory of the attack. We test our
approach on open datasets of audit logs containing real attack data. Our
results demonstrate for the first time that low-level forensics data can
automatically reveal an adversary's subjective preferences, which serves as an
additional dimension to modeling and documenting cyber adversaries. Attackers'
preferences tend to be invariant despite their different tools and indicate
predispositions that are inherent to the attacker. As such, these inferred
preferences can potentially serve as unique behavioral signatures of attackers
and improve threat attribution.

</details>


### [212] [Impact Analysis of Inference Time Attack of Perception Sensors on Autonomous Vehicles](https://arxiv.org/abs/2505.03850)
*Hanlin Chen,Simin Chen,Wenyu Li,Wei Yang,Yiheng Feng*

Main category: cs.CR

TL;DR: 本文提出了一种基于推理时间攻击的自动驾驶汽车影响分析，证明此类攻击可能威胁车辆及其他交通参与者的安全。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶汽车（AVs）的网络安全和安全性问题至关重要，尤其是感知模块作为易受攻击的表面。当前研究多关注感知正确性，而本文探讨推理时间攻击对安全的影响。

Method: 在仿真系统中进行推理时间攻击实验，分析其对自动驾驶汽车及其周围交通参与者的安全威胁。

Result: 实验表明，推理时间攻击不仅影响感知正确性，还可能直接威胁车辆和其他交通参与者的安全。

Conclusion: 推理时间攻击是自动驾驶汽车安全的重要威胁，需进一步研究和防范。

Abstract: As a safety-critical cyber-physical system, cybersecurity and related safety
issues for Autonomous Vehicles (AVs) have been important research topics for a
while. Among all the modules on AVs, perception is one of the most accessible
attack surfaces, as drivers and AVs have no control over the outside
environment. Most current work targeting perception security for AVs focuses on
perception correctness. In this work, we propose an impact analysis based on
inference time attacks for autonomous vehicles. We demonstrate in a simulation
system that such inference time attacks can also threaten the safety of both
the ego vehicle and other traffic participants.

</details>


### [213] [Data-Driven Falsification of Cyber-Physical Systems](https://arxiv.org/abs/2505.03863)
*Atanu Kundu,Sauvik Gon,Rajarshi Ray*

Main category: cs.CR

TL;DR: 本文提出了一种框架，将CPS的伪造与DNN的伪造联系起来，并利用决策树的可解释性加速CPS的伪造。


<details>
  <summary>Details</summary>
Motivation: CPS在安全关键领域广泛应用，但其操作安全的验证至关重要。本文专注于寻找系统的不安全执行而非证明其不存在。

Method: 通过构建CPS的替代模型（DNN或决策树），应用DNN伪造工具，并利用决策树解释安全违规的新算法。

Result: 框架在CPS中有效检测难以发现的反例，并在ARCH-COMP 2024基准测试中表现优异。

Conclusion: 该框架结合DNN和决策树的优势，为CPS的伪造提供了高效且可解释的解决方案。

Abstract: Cyber-Physical Systems (CPS) are abundant in safety-critical domains such as
healthcare, avionics, and autonomous vehicles. Formal verification of their
operational safety is, therefore, of utmost importance. In this paper, we
address the falsification problem, where the focus is on searching for an
unsafe execution in the system instead of proving their absence. The
contribution of this paper is a framework that (a) connects the falsification
of CPS with the falsification of deep neural networks (DNNs) and (b) leverages
the inherent interpretability of Decision Trees for faster falsification of
CPS. This is achieved by: (1) building a surrogate model of the CPS under test,
either as a DNN model or a Decision Tree, (2) application of various DNN
falsification tools to falsify CPS, and (3) a novel falsification algorithm
guided by the explanations of safety violations of the CPS model extracted from
its Decision Tree surrogate. The proposed framework has the potential to
exploit a repertoire of \emph{adversarial attack} algorithms designed to
falsify robustness properties of DNNs, as well as state-of-the-art
falsification algorithms for DNNs. Although the presented methodology is
applicable to systems that can be executed/simulated in general, we demonstrate
its effectiveness, particularly in CPS. We show that our framework, implemented
as a tool \textsc{FlexiFal}, can detect hard-to-find counterexamples in CPS
that have linear and non-linear dynamics. Decision tree-guided falsification
shows promising results in efficiently finding multiple counterexamples in the
ARCH-COMP 2024 falsification benchmarks~\cite{khandait2024arch}.

</details>


### [214] [AI-Driven Security in Cloud Computing: Enhancing Threat Detection, Automated Response, and Cyber Resilience](https://arxiv.org/abs/2505.03945)
*Shamnad Mohamed Shaffi,Sunish Vengathattil,Jezeena Nikarthil Sidhick,Resmi Vijayan*

Main category: cs.CR

TL;DR: 本文探讨了人工智能如何通过预测分析、行为检测和加密技术提升云安全，同时指出了传统模型的不足及AI的改进，但也提到数据隐私、偏见和合规性等问题。


<details>
  <summary>Details</summary>
Motivation: 由于云计算中复杂威胁的增加，传统安全方案无法实时应对，因此需要AI技术来提升云安全。

Method: 利用机器学习、统计可视化和行为检测等AI技术，结合预测分析和加密方法，改进云安全。

Result: AI能有效提升云安全，但仍需解决可靠性、模块化和伦理问题。

Conclusion: AI在云安全中具有潜力，未来可结合区块链等技术进一步优化，同时需关注伦理和合规性。

Abstract: Cloud security concerns have been greatly realized in recent years due to the
increase of complicated threats in the computing world. Many traditional
solutions do not work well in real-time to detect or prevent more complex
threats. Artificial intelligence is today regarded as a revolution in
determining a protection plan for cloud data architecture through machine
learning, statistical visualization of computing infrastructure, and detection
of security breaches followed by counteraction. These AI-enabled systems make
work easier as more network activities are scrutinized, and any anomalous
behavior that might be a precursor to a more serious breach is prevented. This
paper examines ways AI can enhance cloud security by applying predictive
analytics, behavior-based security threat detection, and AI-stirring
encryption. It also outlines the problems of the previous security models and
how AI overcomes them. For a similar reason, issues like data privacy, biases
in the AI model, and regulatory compliance are also covered. So, AI improves
the protection of cloud computing contexts; however, more efforts are needed in
the subsequent phases to extend the technology's reliability, modularity, and
ethical aspects. This means that AI can be blended with other new computing
technologies, including blockchain, to improve security frameworks further. The
paper discusses the current trends in securing cloud data architecture using AI
and presents further research and application directions.

</details>


### [215] [MergeGuard: Efficient Thwarting of Trojan Attacks in Machine Learning Models](https://arxiv.org/abs/2505.04015)
*Soheil Zibakhsh Shabgahi,Yaman Jandali,Farinaz Koushanfar*

Main category: cs.CR

TL;DR: MergeGuard是一种新型方法，用于减轻AI木马攻击，通过线性化和合并全连接层提高模型通用性和性能。


<details>
  <summary>Details</summary>
Motivation: AI木马攻击会导致嵌入触发器的输入被错误分类，威胁由不受信任的第三方训练的模型的可用性。

Method: 提出一种新的后训练方法，线性化和合并全连接层。

Result: 在Transformer模型上的概念验证表明，MergeGuard在保持模型准确性的同时降低了木马攻击成功率，优于常用的微调方法。

Conclusion: MergeGuard是一种有效的后训练木马攻击缓解方法，优于现有技术。

Abstract: This paper proposes MergeGuard, a novel methodology for mitigation of AI
Trojan attacks. Trojan attacks on AI models cause inputs embedded with triggers
to be misclassified to an adversary's target class, posing a significant threat
to model usability trained by an untrusted third party. The core of MergeGuard
is a new post-training methodology for linearizing and merging fully connected
layers which we show simultaneously improves model generalizability and
performance. Our Proof of Concept evaluation on Transformer models demonstrates
that MergeGuard maintains model accuracy while decreasing trojan attack success
rate, outperforming commonly used (post-training) Trojan mitigation by
fine-tuning methodologies.

</details>


### [216] [LLMs' Suitability for Network Security: A Case Study of STRIDE Threat Modeling](https://arxiv.org/abs/2505.04101)
*AbdulAziz AbdulGhaffar,Ashraf Matrawy*

Main category: cs.CR

TL;DR: 该论文探讨了大型语言模型（LLMs）在网络安全性中的适用性，特别是通过STRIDE威胁建模案例研究，发现LLMs需要调整和微调以适应网络安全用例。


<details>
  <summary>Details</summary>
Motivation: 尽管AI在6G网络中的应用广泛，但LLMs在网络安全性中的适用性研究几乎空白，论文旨在填补这一空白。

Method: 使用四种提示技术和五种LLMs对5G威胁进行STRIDE分类，并通过评估结果分析LLMs的行为。

Result: 数值结果和详细分析表明，LLMs在建模某些威胁时存在局限性，需要调整和微调。

Conclusion: LLMs在网络安全中具有潜力，但需进一步优化以适应实际用例。

Abstract: Artificial Intelligence (AI) is expected to be an integral part of
next-generation AI-native 6G networks. With the prevalence of AI, researchers
have identified numerous use cases of AI in network security. However, there
are almost nonexistent studies that analyze the suitability of Large Language
Models (LLMs) in network security. To fill this gap, we examine the suitability
of LLMs in network security, particularly with the case study of STRIDE threat
modeling. We utilize four prompting techniques with five LLMs to perform STRIDE
classification of 5G threats. From our evaluation results, we point out key
findings and detailed insights along with the explanation of the possible
underlying factors influencing the behavior of LLMs in the modeling of certain
threats. The numerical results and the insights support the necessity for
adjusting and fine-tuning LLMs for network security use cases.

</details>


### [217] [A Comprehensive Analysis of Adversarial Attacks against Spam Filters](https://arxiv.org/abs/2505.03831)
*Esra Hotoğlu,Sevil Sen,Burcu Can*

Main category: cs.CR

TL;DR: 研究探讨了对抗攻击对基于深度学习的垃圾邮件检测系统的影响，评估了六种模型，并提出了新的评分函数以提高攻击效果。


<details>
  <summary>Details</summary>
Motivation: 对抗攻击日益复杂，威胁深度学习邮件过滤系统的有效性，需研究其影响并提出改进方案。

Method: 使用真实数据集评估六种深度学习模型，分析不同级别的攻击（单词、字符、句子、AI生成段落），并引入新的评分函数（如垃圾邮件权重和注意力权重）。

Result: 揭示了垃圾邮件过滤器的漏洞，为提升其安全性提供了依据。

Conclusion: 研究为改进垃圾邮件检测系统对抗对抗攻击的能力提供了重要参考。

Abstract: Deep learning has revolutionized email filtering, which is critical to
protect users from cyber threats such as spam, malware, and phishing. However,
the increasing sophistication of adversarial attacks poses a significant
challenge to the effectiveness of these filters. This study investigates the
impact of adversarial attacks on deep learning-based spam detection systems
using real-world datasets. Six prominent deep learning models are evaluated on
these datasets, analyzing attacks at the word, character sentence, and
AI-generated paragraph-levels. Novel scoring functions, including spam weights
and attention weights, are introduced to improve attack effectiveness. This
comprehensive analysis sheds light on the vulnerabilities of spam filters and
contributes to efforts to improve their security against evolving adversarial
threats.

</details>


### [218] [Weaponizing Language Models for Cybersecurity Offensive Operations: Automating Vulnerability Assessment Report Validation; A Review Paper](https://arxiv.org/abs/2505.04265)
*Abdulrahman S Almuhaidib,Azlan Mohd Zain,Zalmiyah Zakaria,Izyan Izzati Kamsani,Abdulaziz S Almuhaidib*

Main category: cs.CR

TL;DR: 本文探讨了大型语言模型（LLMs）在漏洞评估（VA）报告验证中的潜力，填补了现有研究中关于LLMs进攻性应用的空白。


<details>
  <summary>Details</summary>
Motivation: 随着网络战的日益复杂，需要新的解决方案。LLMs在网络安全防御和进攻策略中显示出巨大潜力，但现有研究多集中于防御应用，进攻性应用（如VA报告验证）研究较少。

Method: 通过文献综述，提出了一种利用LLMs自动化和改进VA报告验证过程的新方法，旨在减少误报并提高效率。

Result: 研究结果表明，LLMs在自动化VA报告验证中具有潜力，可提高准确性、减少人工投入并优化安全态势。

Conclusion: 本文进一步证明了LLMs在进攻和防御方面的能力，为制定更合适的网络安全策略和工具提供了依据。

Abstract: This, with the ever-increasing sophistication of cyberwar, calls for novel
solutions. In this regard, Large Language Models (LLMs) have emerged as a
highly promising tool for defensive and offensive cybersecurity-related
strategies. While existing literature has focused much on the defensive use of
LLMs, when it comes to their offensive utilization, very little has been
reported-namely, concerning Vulnerability Assessment (VA) report validation.
Consequentially, this paper tries to fill that gap by investigating the
capabilities of LLMs in automating and improving the validation process of the
report of the VA. From the critical review of the related literature, this
paper hereby proposes a new approach to using the LLMs in the automation of the
analysis and within the validation process of the report of the VA that could
potentially reduce the number of false positives and generally enhance
efficiency. These results are promising for LLM automatization for improving
validation on reports coming from VA in order to improve accuracy while
reducing human effort and security postures. The contribution of this paper
provides further evidence about the offensive and defensive LLM capabilities
and therefor helps in devising more appropriate cybersecurity strategies and
tools accordingly.

</details>


### [219] [Guardians of the Web: The Evolution and Future of Website Information Security](https://arxiv.org/abs/2505.04308)
*Md Saiful Islam,Li Xiangdong*

Main category: cs.CR

TL;DR: 本文探讨了网站信息安全的历史发展、当前实践及未来方向，强调了技术进步和国际合作的重要性。


<details>
  <summary>Details</summary>
Motivation: 随着互联网的普及和网络威胁的复杂化，网站信息安全成为数字时代的关键问题，需要总结历史经验并展望未来发展方向。

Method: 通过分析从1960年代至今的技术发展（如ARPANET、SSL协议等）和当前实践（如多层安全防护），探讨信息安全趋势。

Result: 当前信息安全采用多层防护措施，未来将依赖人工智能、区块链等新兴技术及国际合作。

Conclusion: 持续的研究和创新对应对不断演变的网络威胁至关重要，以保护敏感信息并维护数字世界的信任。

Abstract: Website information security has become a critical concern in the digital
age. This article explores the evolution of website information security,
examining its historical development, current practices, and future directions.
The early beginnings from the 1960s to the 1980s laid the groundwork for modern
cybersecurity, with the development of ARPANET, TCP/IP, public-key
cryptography, and the first antivirus programs. The 1990s marked a
transformative era, driven by the commercialization of the Internet and the
emergence of web-based services. As the Internet grew, so did the range and
sophistication of cyber threats, leading to advancements in security
technologies such as the Secure Sockets Layer (SSL) protocol, password
protection, and firewalls. Current practices in website information security
involve a multi-layered approach, including encryption, secure coding
practices, regular security audits, and user education. The future of website
information security is expected to be shaped by emerging technologies such as
artificial intelligence, blockchain, and quantum computing, as well as the
increasing importance of international cooperation and standardization efforts.
As cyber threats continue to evolve, ongoing research and innovation in website
information security will be essential to protect sensitive information and
maintain trust in the digital world.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [220] [Recognizing Ornaments in Vocal Indian Art Music with Active Annotation](https://arxiv.org/abs/2505.04419)
*Sumit Kumar,Parampreet Singh,Vipul Arora*

Main category: eess.AS

TL;DR: 论文介绍了ROD数据集和基于深度时间序列分析的装饰音检测模型，解决了音乐信息检索中装饰音识别缺乏标注数据和专用模型的问题。


<details>
  <summary>Details</summary>
Motivation: 装饰音在音乐表达中至关重要，但缺乏标注数据和专用模型阻碍了相关研究进展。

Method: 提出ROD数据集，并开发基于深度时间序列分析的装饰音检测模型，保留装饰音边界。

Result: 实验结果表明，该方法优于基线CRNN模型。

Conclusion: ROD数据集和提出的模型为装饰音识别研究提供了有效工具。

Abstract: Ornamentations, embellishments, or microtonal inflections are essential to
melodic expression across many musical traditions, adding depth, nuance, and
emotional impact to performances. Recognizing ornamentations in singing voices
is key to MIR, with potential applications in music pedagogy, singer
identification, genre classification, and controlled singing voice generation.
However, the lack of annotated datasets and specialized modeling approaches
remains a major obstacle for progress in this research area. In this work, we
introduce R\=aga Ornamentation Detection (ROD), a novel dataset comprising
Indian classical music recordings curated by expert musicians. The dataset is
annotated using a custom Human-in-the-Loop tool for six vocal ornaments marked
as event-based labels. Using this dataset, we develop an ornamentation
detection model based on deep time-series analysis, preserving ornament
boundaries during the chunking of long audio recordings. We conduct experiments
using different train-test configurations within the ROD dataset and also
evaluate our approach on a separate, manually annotated dataset of Indian
classical concert recordings. Our experimental results support the superior
performance of our proposed approach over the baseline CRNN.

</details>


### [221] [Discrete Optimal Transport and Voice Conversion](https://arxiv.org/abs/2505.04382)
*Anton Selitskiy,Maitreya Kocharekar*

Main category: eess.AS

TL;DR: 本文提出了一种基于向量接口的语音转换方法，采用离散最优传输映射对齐说话人音频嵌入，实验证明其高质量和有效性，并发现该方法可能导致合成音频被误分类为真实音频。


<details>
  <summary>Details</summary>
Motivation: 解决语音转换任务中音频嵌入对齐的问题。

Method: 使用离散最优传输映射对齐说话人音频嵌入。

Result: 方法表现出高质量和有效性，但可能导致合成音频被误分类为真实音频。

Conclusion: 离散最优传输映射在语音转换中有效，但需注意其潜在的误分类问题。

Abstract: In this work, we address the voice conversion (VC) task using a vector-based
interface. To align audio embeddings between speakers, we employ discrete
optimal transport mapping. Our evaluation results demonstrate the high quality
and effectiveness of this method. Additionally, we show that applying discrete
optimal transport as a post-processing step in audio generation can lead to the
incorrect classification of synthetic audio as real.

</details>


### [222] [EchoInk-R1: Exploring Audio-Visual Reasoning in Multimodal LLMs via Reinforcement Learning](https://arxiv.org/abs/2505.04623)
*Zhenghao Xing,Xiaowei Hu,Chi-Wing Fu,Wenhai Wang,Jifeng Dai,Pheng-Ann Heng*

Main category: eess.AS

TL;DR: EchoInk-R1是一个基于强化学习的框架，用于提升多模态大语言模型（MLLMs）在音频和视觉信号的结构化跨模态推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有的MLLMs在跨模态推理（尤其是音频和视觉信号结合时）表现不佳，因此需要一种方法来增强这种能力。

Method: 基于Qwen2.5-Omni-7B模型，使用Group Relative Policy Optimization（GRPO）优化，通过强化学习训练模型处理同步音频-图像对的多选题任务。数据集AVQA-R1-6K用于支持训练。

Result: EchoInk-R1-7B在验证集上达到85.77%的准确率，优于基础模型的80.53%，且仅需562步强化学习。

Conclusion: 轻量级强化学习微调能有效提升MLLMs的跨模态推理能力，EchoInk-R1是首个通过强化学习统一音频、视觉和文本模态的框架。

Abstract: Multimodal large language models (MLLMs) have advanced perception across
text, vision, and audio, yet they often struggle with structured cross-modal
reasoning, particularly when integrating audio and visual signals. We introduce
EchoInk-R1, a reinforcement learning framework that enhances such reasoning in
MLLMs. Built upon the Qwen2.5-Omni-7B foundation and optimized with Group
Relative Policy Optimization (GRPO), EchoInk-R1 tackles multiple-choice
question answering over synchronized audio-image pairs. To enable this, we
curate AVQA-R1-6K, a dataset pairing such audio-image inputs with
multiple-choice questions derived from OmniInstruct-v1. EchoInk-R1-7B achieves
85.77% accuracy on the validation set, outperforming the base model, which
scores 80.53%, using only 562 reinforcement learning steps. Beyond accuracy,
EchoInk-R1 demonstrates reflective reasoning by revisiting initial
interpretations and refining responses when facing ambiguous multimodal inputs.
These results suggest that lightweight reinforcement learning fine-tuning
enhances cross-modal reasoning in MLLMs. EchoInk-R1 is the first framework to
unify audio, visual, and textual modalities for general open-world reasoning
via reinforcement learning. Code and data are publicly released to facilitate
further research.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [223] [Miipher-2: A Universal Speech Restoration Model for Million-Hour Scale Data Restoration](https://arxiv.org/abs/2505.04457)
*Shigeki Karita,Yuma Koizumi,Heiga Zen,Haruko Ishikawa,Robin Scheibler,Michiel Bacchiani*

Main category: cs.SD

TL;DR: Miipher-2是一种用于大规模生成模型训练数据清理的语音恢复模型，支持300多种语言，无需显式条件输入，计算高效。


<details>
  <summary>Details</summary>
Motivation: 解决大规模生成模型训练数据清理中的泛化性、无条件和计算效率问题。

Method: 利用预训练的通用语音模型（USM）作为特征提取器，结合并行适配器和WaneFit神经声码器。

Result: 在词错误率、说话人相似性和音质评分上优于或接近传统模型，计算效率高。

Conclusion: Miipher-2是一种高效、通用的语音恢复模型，适用于大规模数据清理。

Abstract: Training data cleaning is a new application for generative model-based speech
restoration (SR). This paper introduces Miipher-2, an SR model designed for
million-hour scale data, for training data cleaning for large-scale generative
models like large language models. Key challenges addressed include
generalization to unseen languages, operation without explicit conditioning
(e.g., text, speaker ID), and computational efficiency. Miipher-2 utilizes a
frozen, pre-trained Universal Speech Model (USM), supporting over 300
languages, as a robust, conditioning-free feature extractor. To optimize
efficiency and minimize memory, Miipher-2 incorporates parallel adapters for
predicting clean USM features from noisy inputs and employs the WaneFit neural
vocoder for waveform synthesis. These components were trained on 3,000 hours of
multi-lingual, studio-quality recordings with augmented degradations, while USM
parameters remained fixed. Experimental results demonstrate Miipher-2's
superior or comparable performance to conventional SR models in
word-error-rate, speaker similarity, and both objective and subjective sound
quality scores across all tested languages. Miipher-2 operates efficiently on
consumer-grade accelerators, achieving a real-time factor of 0.0078, enabling
the processing of a million-hour speech dataset in approximately three days
using only 100 such accelerators.

</details>


### [224] [Automatic Music Transcription using Convolutional Neural Networks and Constant-Q transform](https://arxiv.org/abs/2505.04451)
*Yohannis Telila,Tommaso Cucinotta,Davide Bacciu*

Main category: cs.SD

TL;DR: 该论文提出了一种基于卷积神经网络（CNN）和常数Q变换（CQT）的自动音乐转录（AMT）方法，用于将古典钢琴音频转换为乐谱表示。


<details>
  <summary>Details</summary>
Motivation: 自动音乐转录（AMT）在处理复调音乐时具有挑战性，目标是分析包含多个同时演奏音符的音频信号，生成乐谱表示。

Method: 使用常数Q变换（CQT）提取音频特征，并将结果系数输入卷积神经网络（CNN）模型进行处理。

Result: 设计了一个处理流程，能够将古典钢琴的.wav音频文件转换为乐谱表示。

Conclusion: 该方法通过结合CQT和CNN，为复调音乐的自动转录提供了一种有效解决方案。

Abstract: Automatic music transcription (AMT) is the problem of analyzing an audio
recording of a musical piece and detecting notes that are being played. AMT is
a challenging problem, particularly when it comes to polyphonic music. The goal
of AMT is to produce a score representation of a music piece, by analyzing a
sound signal containing multiple notes played simultaneously. In this work, we
design a processing pipeline that can transform classical piano audio files in
.wav format into a music score representation. The features from the audio
signals are extracted using the constant-Q transform, and the resulting
coefficients are used as an input to the convolutional neural network (CNN)
model.

</details>


### [225] [Score Distillation Sampling for Audio: Source Separation, Synthesis, and Beyond](https://arxiv.org/abs/2505.04621)
*Jessie Richter-Powell,Antonio Torralba,Jonathan Lorraine*

Main category: cs.SD

TL;DR: Audio-SDS将Score Distillation Sampling（SDS）扩展到音频领域，利用预训练模型实现多种任务，如物理模拟、参数校准和源分离。


<details>
  <summary>Details</summary>
Motivation: 将SDS的核心思想（将生成先验蒸馏到参数化表示中）扩展到音频领域，以利用强大的生成模型完成多样化任务。

Method: 基于文本条件的音频扩散模型，通过Audio-SDS实现任务，无需专用数据集。

Result: 展示了Audio-SDS在物理模拟、FM合成参数校准和源分离任务中的多功能性。

Conclusion: Audio-SDS为音频任务中的生成先验应用提供了通用且强大的基础。

Abstract: We introduce Audio-SDS, a generalization of Score Distillation Sampling (SDS)
to text-conditioned audio diffusion models. While SDS was initially designed
for text-to-3D generation using image diffusion, its core idea of distilling a
powerful generative prior into a separate parametric representation extends to
the audio domain. Leveraging a single pretrained model, Audio-SDS enables a
broad range of tasks without requiring specialized datasets. In particular, we
demonstrate how Audio-SDS can guide physically informed impact sound
simulations, calibrate FM-synthesis parameters, and perform prompt-specified
source separation. Our findings illustrate the versatility of
distillation-based methods across modalities and establish a robust foundation
for future work using generative priors in audio tasks.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [226] [GRAPE: Heterogeneous Graph Representation Learning for Genetic Perturbation with Coding and Non-Coding Biotype](https://arxiv.org/abs/2505.03853)
*Changxi Chi,Jun Xia,Jingbo Zhou,Jiabei Cheng,Chang Yu,Stan Z. Li*

Main category: q-bio.QM

TL;DR: 该论文提出了一种名为GRAPE的异构图神经网络方法，通过预训练模型提取基因描述和DNA序列特征，结合基因生物类型信息，动态优化基因调控网络（GRN），显著提升了遗传扰动预测性能。


<details>
  <summary>Details</summary>
Motivation: 当前方法未能充分利用基因相关信息，且忽略了生物类型的功能差异，导致基因调控网络构建粗糙，预测能力受限。

Method: 利用预训练语言模型和DNA序列模型提取基因特征，结合生物类型信息，通过图结构学习（GSL）动态优化GRN，提出GRAPE异构图神经网络。

Result: 在公开数据集上，GRAPE方法实现了最先进的性能。

Conclusion: GRAPE通过整合多源信息和动态优化，显著提升了基因扰动预测的准确性和效率。

Abstract: Predicting genetic perturbations enables the identification of potentially
crucial genes prior to wet-lab experiments, significantly improving overall
experimental efficiency. Since genes are the foundation of cellular life,
building gene regulatory networks (GRN) is essential to understand and predict
the effects of genetic perturbations. However, current methods fail to fully
leverage gene-related information, and solely rely on simple evaluation metrics
to construct coarse-grained GRN. More importantly, they ignore functional
differences between biotypes, limiting the ability to capture potential gene
interactions. In this work, we leverage pre-trained large language model and
DNA sequence model to extract features from gene descriptions and DNA sequence
data, respectively, which serve as the initialization for gene representations.
Additionally, we introduce gene biotype information for the first time in
genetic perturbation, simulating the distinct roles of genes with different
biotypes in regulating cellular processes, while capturing implicit gene
relationships through graph structure learning (GSL). We propose GRAPE, a
heterogeneous graph neural network (HGNN) that leverages gene representations
initialized with features from descriptions and sequences, models the distinct
roles of genes with different biotypes, and dynamically refines the GRN through
GSL. The results on publicly available datasets show that our method achieves
state-of-the-art performance.

</details>


### [227] [Sparsity is All You Need: Rethinking Biological Pathway-Informed Approaches in Deep Learning](https://arxiv.org/abs/2505.04300)
*Isabella Caranzano,Corrado Pancotti,Cesare Rollo,Flavio Sartori,Pietro Liò,Piero Fariselli,Tiziana Sanavia*

Main category: q-bio.QM

TL;DR: 研究发现，基于生物通路注释的神经网络模型的性能提升可能源于其稀疏性而非生物学相关性，随机化模型表现与之相当甚至更优。


<details>
  <summary>Details</summary>
Motivation: 验证生物通路注释对神经网络性能提升的贡献是否源于其生物学相关性，还是仅仅因为引入的稀疏性。

Method: 全面分析基于通路的神经网络模型，比较生物通路模型与随机化模型的性能，并评估其可解释性。

Result: 随机化模型在性能上与生物通路模型相当，甚至在某些情况下更优；生物通路模型在可解释性上无明显优势。

Conclusion: 生物通路注释可能过于嘈杂或未被充分探索，提出了一种系统性比较新模型与随机化模型的方法，以验证性能提升是否源于生物学见解。

Abstract: Biologically-informed neural networks typically leverage pathway annotations
to enhance performance in biomedical applications. We hypothesized that the
benefits of pathway integration does not arise from its biological relevance,
but rather from the sparsity it introduces. We conducted a comprehensive
analysis of all relevant pathway-based neural network models for predictive
tasks, critically evaluating each study's contributions. From this review, we
curated a subset of methods for which the source code was publicly available.
The comparison of the biologically informed state-of-the-art deep learning
models and their randomized counterparts showed that models based on randomized
information performed equally well as biologically informed ones across
different metrics and datasets. Notably, in 3 out of the 15 analyzed models,
the randomized versions even outperformed their biologically informed
counterparts. Moreover, pathway-informed models did not show any clear
advantage in interpretability, as randomized models were still able to identify
relevant disease biomarkers despite lacking explicit pathway information. Our
findings suggest that pathway annotations may be too noisy or inadequately
explored by current methods. Therefore, we propose a methodology that can be
applied to different domains and can serve as a robust benchmark for
systematically comparing novel pathway-informed models against their randomized
counterparts. This approach enables researchers to rigorously determine whether
observed performance improvements can be attributed to biological insights.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [228] [A Graphical Global Optimization Framework for Parameter Estimation of Statistical Models with Nonconvex Regularization Functions](https://arxiv.org/abs/2505.03899)
*Danial Davarnia,Mohammadreza Kiaghadi*

Main category: math.OC

TL;DR: 本文提出了一种基于图的新方法，用于全局解决涉及广义范数约束的优化问题，避免了辅助变量或人工边界的需求。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在处理零范数函数和非凸惩罚时引入的复杂性和局限性问题。

Method: 利用决策图在原始变量空间中构建强凸松弛，并结合空间分支切割框架保证全局最优解。

Result: 在涉及复杂非凸惩罚的稀疏线性回归基准问题上展示了方法的有效性。

Conclusion: 该方法能够全局解决广义范数约束优化问题，优于现有技术。

Abstract: Optimization problems with norm-bounding constraints arise in a variety of
applications, including portfolio optimization, machine learning, and feature
selection. A common approach to these problems involves relaxing the norm
constraint via Lagrangian relaxation, transforming it into a regularization
term in the objective function. A particularly challenging class includes the
zero-norm function, which promotes sparsity in statistical parameter
estimation. Most existing exact methods for solving these problems introduce
binary variables and artificial bounds to reformulate them as
higher-dimensional mixed-integer programs, solvable by standard solvers. Other
exact approaches exploit specific structural properties of the objective,
making them difficult to generalize across different problem types. Alternative
methods employ nonconvex penalties with favorable statistical characteristics,
but these are typically addressed using heuristic or local optimization
techniques due to their structural complexity. In this paper, we propose a
novel graph-based method to globally solve optimization problems involving
generalized norm-bounding constraints. Our approach encompasses standard
$\ell_p$-norms for $p \in [0, \infty)$ and nonconvex penalties such as SCAD and
MCP. We leverage decision diagrams to construct strong convex relaxations
directly in the original variable space, eliminating the need for auxiliary
variables or artificial bounds. Integrated into a spatial branch-and-cut
framework, our method guarantees convergence to the global optimum. We
demonstrate its effectiveness through preliminary computational experiments on
benchmark sparse linear regression problems involving complex nonconvex
penalties, which are not tractable using existing global optimization
techniques.

</details>


### [229] [Optimization Problem Solving Can Transition to Evolutionary Agentic Workflows](https://arxiv.org/abs/2505.04354)
*Wenhao Li,Bo Jin,Mingyi Hong,Changhong Lu,Xiangfeng Wang*

Main category: math.OC

TL;DR: 本文主张通过进化代理工作流将优化问题解决从依赖专家转变为自动化流程，利用基础模型和进化搜索自主导航优化空间。


<details>
  <summary>Details</summary>
Motivation: 传统优化方法依赖专家，导致工业应用瓶颈，阻碍前沿方法的推广。

Method: 提出进化代理工作流，结合基础模型和进化搜索，自主处理问题、算法和超参数空间。

Result: 通过云资源调度和ADMM参数适应的案例研究，验证了该方法的有效性。

Conclusion: 挑战以人为中心的优化流程，提倡更可扩展、自适应的解决方案。

Abstract: This position paper argues that optimization problem solving can transition
from expert-dependent to evolutionary agentic workflows. Traditional
optimization practices rely on human specialists for problem formulation,
algorithm selection, and hyperparameter tuning, creating bottlenecks that
impede industrial adoption of cutting-edge methods. We contend that an
evolutionary agentic workflow, powered by foundation models and evolutionary
search, can autonomously navigate the optimization space, comprising problem,
formulation, algorithm, and hyperparameter spaces. Through case studies in
cloud resource scheduling and ADMM parameter adaptation, we demonstrate how
this approach can bridge the gap between academic innovation and industrial
implementation. Our position challenges the status quo of human-centric
optimization workflows and advocates for a more scalable, adaptive approach to
solving real-world optimization problems.

</details>


### [230] [Learning based convex approximation for constrained parametric optimization](https://arxiv.org/abs/2505.04037)
*Kang Liu,Wei Peng,Jianchen Hu*

Main category: math.OC

TL;DR: 提出了一种基于输入凸神经网络（ICNN）的自监督学习框架，用于解决连续约束优化问题，结合增强拉格朗日方法和约束修正机制，确保非严格约束可行性、更优的最优性差距和最佳收敛速度。


<details>
  <summary>Details</summary>
Motivation: 解决现有基于学习的方法在连续约束优化问题中的不足，如约束可行性、最优性差距和收敛速度。

Method: 结合增强拉格朗日方法（ALM）和约束修正机制，利用ICNN进行自监督学习。

Result: 在多种基准任务（如二次规划、非凸规划和大规模交流最优潮流问题）中表现优于现有求解器和最新学习方法，实现了准确性、可行性和计算效率的平衡。

Conclusion: 该框架在理论和实验上均表现出色，能够收敛到原问题的KKT点，且近似误差有界。

Abstract: We propose an input convex neural network (ICNN)-based self-supervised
learning framework to solve continuous constrained optimization problems. By
integrating the augmented Lagrangian method (ALM) with the constraint
correction mechanism, our framework ensures \emph{non-strict constraint
feasibility}, \emph{better optimality gap}, and \emph{best convergence rate}
with respect to the state-of-the-art learning-based methods. We provide a
rigorous convergence analysis, showing that the algorithm converges to a
Karush-Kuhn-Tucker (KKT) point of the original problem even when the internal
solver is a neural network, and the approximation error is bounded. We test our
approach on a range of benchmark tasks including quadratic programming (QP),
nonconvex programming, and large-scale AC optimal power flow problems. The
results demonstrate that compared to existing solvers (e.g., \texttt{OSQP},
\texttt{IPOPT}) and the latest learning-based methods (e.g., DC3, PDL), our
approach achieves a superior balance among accuracy, feasibility, and
computational efficiency.

</details>


### [231] [A Two-Timescale Primal-Dual Framework for Reinforcement Learning via Online Dual Variable Guidance](https://arxiv.org/abs/2505.04494)
*Axel Friedrich Wolter,Tobias Sutter*

Main category: math.OC

TL;DR: PGDA-RL是一种新的原始-对偶投影梯度下降-上升算法，用于解决正则化马尔可夫决策过程（MDP），结合了经验回放和两时间尺度分解，实现了异步在线策略更新。


<details>
  <summary>Details</summary>
Motivation: 设计一种能够利用离策略数据同时保持在线探索的强化学习算法。

Method: 提出PGDA-RL算法，结合经验回放的梯度估计和两时间尺度分解，异步更新策略。

Result: PGDA-RL几乎必然收敛到正则化MDP的最优值函数和策略。

Conclusion: PGDA-RL在较弱假设下收敛，无需模拟器或固定行为策略，优于现有原始-对偶RL方法。

Abstract: We study reinforcement learning by combining recent advances in regularized
linear programming formulations with the classical theory of stochastic
approximation. Motivated by the challenge of designing algorithms that leverage
off-policy data while maintaining on-policy exploration, we propose PGDA-RL, a
novel primal-dual Projected Gradient Descent-Ascent algorithm for solving
regularized Markov Decision Processes (MDPs). PGDA-RL integrates experience
replay-based gradient estimation with a two-timescale decomposition of the
underlying nested optimization problem. The algorithm operates asynchronously,
interacts with the environment through a single trajectory of correlated data,
and updates its policy online in response to the dual variable associated with
the occupation measure of the underlying MDP. We prove that PGDA-RL converges
almost surely to the optimal value function and policy of the regularized MDP.
Our convergence analysis relies on tools from stochastic approximation theory
and holds under weaker assumptions than those required by existing primal-dual
RL approaches, notably removing the need for a simulator or a fixed behavioral
policy.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [232] [Benchmarking LLMs' Swarm intelligence](https://arxiv.org/abs/2505.04364)
*Kai Ruan,Mowen Huang,Ji-Rong Wen,Hao Sun*

Main category: cs.MA

TL;DR: 论文介绍了SwarmBench，一个用于评估大语言模型在分散式多智能体系统中群体智能能力的新基准。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型在严格约束下（如局部感知和通信）的群体智能能力，填补现有基准在分散协调挑战上的不足。

Method: 设计了SwarmBench，包含五个基础任务，在可配置的2D网格环境中评估LLMs的协调能力，依赖局部感知和通信。

Result: 不同LLMs在零样本设置下表现差异显著，显示局部信息约束下的规划与策略形成存在困难。

Conclusion: SwarmBench为研究LLM在分散系统中的潜力提供了工具，并促进可重复研究。

Abstract: Large Language Models (LLMs) show potential for complex reasoning, yet their
capacity for emergent coordination in Multi-Agent Systems (MAS) when operating
under strict constraints-such as limited local perception and communication,
characteristic of natural swarms-remains largely unexplored, particularly
concerning the nuances of swarm intelligence. Existing benchmarks often do not
fully capture the unique challenges of decentralized coordination that arise
when agents operate with incomplete spatio-temporal information. To bridge this
gap, we introduce SwarmBench, a novel benchmark designed to systematically
evaluate the swarm intelligence capabilities of LLMs acting as decentralized
agents. SwarmBench features five foundational MAS coordination tasks within a
configurable 2D grid environment, forcing agents to rely primarily on local
sensory input (k x k view) and local communication. We propose metrics for
coordination effectiveness and analyze emergent group dynamics. Evaluating
several leading LLMs in a zero-shot setting, we find significant performance
variations across tasks, highlighting the difficulties posed by local
information constraints. While some coordination emerges, results indicate
limitations in robust planning and strategy formation under uncertainty in
these decentralized scenarios. Assessing LLMs under swarm-like conditions is
crucial for realizing their potential in future decentralized systems. We
release SwarmBench as an open, extensible toolkit-built upon a customizable and
scalable physical system with defined mechanical properties. It provides
environments, prompts, evaluation scripts, and the comprehensive experimental
datasets generated, aiming to foster reproducible research into LLM-based MAS
coordination and the theoretical underpinnings of Embodied MAS. Our code
repository is available at https://github.com/x66ccff/swarmbench.

</details>


### [233] [From Glue-Code to Protocols: A Critical Analysis of A2A and MCP Integration for Scalable Agent Systems](https://arxiv.org/abs/2505.03864)
*Qiaomu Li,Ying Xie*

Main category: cs.MA

TL;DR: 本文探讨了整合Google的A2A协议和Anthropic的MCP协议时面临的挑战，包括语义互操作性、安全风险和治理问题。


<details>
  <summary>Details</summary>
Motivation: 研究A2A和MCP协议的潜在协同效应及其整合时的独特挑战。

Method: 通过批判性分析评估整合A2A和MCP的实际影响和固有困难。

Result: 整合A2A和MCP提供了重要的架构基础，但需解决安全漏洞、隐私复杂性和调试问题。

Conclusion: A2A+MCP整合潜力巨大，但需进一步管理其复杂性以实现其全部价值。

Abstract: Artificial intelligence is rapidly evolving towards multi-agent systems where
numerous AI agents collaborate and interact with external tools. Two key open
standards, Google's Agent to Agent (A2A) protocol for inter-agent communication
and Anthropic's Model Context Protocol (MCP) for standardized tool access,
promise to overcome the limitations of fragmented, custom integration
approaches. While their potential synergy is significant, this paper argues
that effectively integrating A2A and MCP presents unique, emergent challenges
at their intersection, particularly concerning semantic interoperability
between agent tasks and tool capabilities, the compounded security risks
arising from combined discovery and execution, and the practical governance
required for the envisioned "Agent Economy". This work provides a critical
analysis, moving beyond a survey to evaluate the practical implications and
inherent difficulties of combining these horizontal and vertical integration
standards. We examine the benefits (e.g., specialization, scalability) while
critically assessing their dependencies and trade-offs in an integrated
context. We identify key challenges increased by the integration, including
novel security vulnerabilities, privacy complexities, debugging difficulties
across protocols, and the need for robust semantic negotiation mechanisms. In
summary, A2A+MCP offers a vital architectural foundation, but fully realizing
its potential requires substantial advancements to manage the complexities of
their combined operation.

</details>


### [234] [Consensus-Aware AV Behavior: Trade-offs Between Safety, Interaction, and Performance in Mixed Urban Traffic](https://arxiv.org/abs/2505.04379)
*Mohammad Elayan,Wissam Kontar*

Main category: cs.MA

TL;DR: 论文研究了自动驾驶车辆（AVs）与人类驾驶车辆（HDVs）在交通系统中的共识问题，通过高分辨率轨迹数据量化了安全、交互质量和交通性能的共识程度。


<details>
  <summary>Details</summary>
Motivation: 交通系统的复杂性和异质性使得自动驾驶车辆的部署面临共识挑战，尤其是在安全、交互和性能之间的平衡。

Method: 使用TGSIM数据集的高分辨率轨迹数据，分析AVs和HDVs在信号交叉口和弱势道路用户（VRUs）周围的行为，评估了包括TTC、PET、减速模式等关键指标。

Result: 结果显示，安全、交互和性能之间的完全共识罕见，仅1.63%的AV-VRU交互帧满足所有条件。

Conclusion: 研究强调了在混合交通环境中需要开发能平衡多维度性能的AV模型，并提供了开源代码支持可重复性。

Abstract: Transportation systems have long been shaped by complexity and heterogeneity,
driven by the interdependency of agent actions and traffic outcomes. The
deployment of automated vehicles (AVs) in such systems introduces a new
challenge: achieving consensus across safety, interaction quality, and traffic
performance. In this work, we position consensus as a fundamental property of
the traffic system and aim to quantify it. We use high-resolution trajectory
data from the Third Generation Simulation (TGSIM) dataset to empirically
analyze AV and human-driven vehicle (HDV) behavior at a signalized urban
intersection and around vulnerable road users (VRUs). Key metrics, including
Time-to-Collision (TTC), Post-Encroachment Time (PET), deceleration patterns,
headways, and string stability, are evaluated across the three performance
dimensions. Results show that full consensus across safety, interaction, and
performance is rare, with only 1.63% of AV-VRU interaction frames meeting all
three conditions. These findings highlight the need for AV models that
explicitly balance multi-dimensional performance in mixed-traffic environments.
Full reproducibility is supported via our open-source codebase on
https://github.com/wissamkontar/Consensus-AV-Analysis.

</details>


### [235] [Implicitly Aligning Humans and Autonomous Agents through Shared Task Abstractions](https://arxiv.org/abs/2505.04579)
*Stéphane Aroca-Ouellette,Miguel Aroca-Ouellette,Katharina von der Wense,Alessandro Roncone*

Main category: cs.MA

TL;DR: HA²框架通过分层强化学习模仿人类协作方式，显著提升自主代理在陌生团队中的适应能力。


<details>
  <summary>Details</summary>
Motivation: 自主代理在协作任务中难以快速适应新队友，缺乏共享任务抽象机制是主要限制因素。

Method: 引入HA²框架，利用分层强化学习模拟人类协作的结构化方法。

Result: 在Overcooked环境中，HA²显著优于现有基线，适应性强且性能超越所有先进方法。

Conclusion: HA²通过模仿人类协作机制，有效解决了自主代理在零次协调中的适应问题。

Abstract: In collaborative tasks, autonomous agents fall short of humans in their
capability to quickly adapt to new and unfamiliar teammates. We posit that a
limiting factor for zero-shot coordination is the lack of shared task
abstractions, a mechanism humans rely on to implicitly align with teammates. To
address this gap, we introduce HA$^2$: Hierarchical Ad Hoc Agents, a framework
leveraging hierarchical reinforcement learning to mimic the structured approach
humans use in collaboration. We evaluate HA$^2$ in the Overcooked environment,
demonstrating statistically significant improvement over existing baselines
when paired with both unseen agents and humans, providing better resilience to
environmental shifts, and outperforming all state-of-the-art methods.

</details>


<div id='physics.geo-ph'></div>

# physics.geo-ph [[Back]](#toc)

### [236] [On the Residual-based Neural Network for Unmodeled Distortions in Coordinate Transformation](https://arxiv.org/abs/2505.03757)
*Vinicius Francisco Rofatto,Luiz Felipe Rodrigues de Almeida,Marcelo Tomio Matsuoka,Ivandro Klein,Mauricio Roberto Veronez,Luiz Gonzaga Da Silveira Junior*

Main category: physics.geo-ph

TL;DR: 提出了一种基于残差的神经校正策略，通过神经网络学习初始几何变换后的系统性失真，从而降低模型复杂度并提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统坐标变换模型难以处理非线性且空间依赖的失真，导致地理空间应用中存在显著残差误差。

Method: 采用残差建模策略，神经网络仅学习初始变换后的系统性失真，减少模型复杂度。

Result: 在模拟数据集和实际图像地理配准任务中，该方法比直接神经网络坐标转换和经典变换模型更准确稳定。

Conclusion: 残差建模是一种轻量且鲁棒的方法，可有效提升坐标变换的准确性。

Abstract: Coordinate transformation models often fail to account for nonlinear and
spatially dependent distortions, leading to significant residual errors in
geospatial applications. Here we propose a residual-based neural correction
strategy, in which a neural network learns to model only the systematic
distortions left by an initial geometric transformation. By focusing solely on
residual patterns, the proposed method reduces model complexity and improves
performance, particularly in scenarios with sparse or structured control point
configurations. We evaluate the method using both simulated datasets with
varying distortion intensities and sampling strategies, as well as under the
real-world image georeferencing tasks. Compared with direct neural network
coordinate converter and classical transformation models, the residual-based
neural correction delivers more accurate and stable results under challenging
conditions, while maintaining comparable performance in ideal cases. These
findings demonstrate the effectiveness of residual modelling as a lightweight
and robust alternative for improving coordinate transformation accuracy.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [237] [Sentiment-Aware Recommendation Systems in E-Commerce: A Review from a Natural Language Processing Perspective](https://arxiv.org/abs/2505.03828)
*Yogesh Gajula*

Main category: cs.IR

TL;DR: 本文综述了2023年至2025年初基于自然语言处理的情感感知推荐系统，探讨了如何通过情感分析提升电商推荐的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 电商平台产生大量用户反馈（如评分、评论），但现有推荐系统多依赖数值评分，忽略了文本中的细微情感。本文旨在填补这一研究空白。

Method: 综述了四种主要方法：结合情感嵌入与用户交互的深度学习分类器、基于Transformer的特征提取、传播情感信号的图神经网络，以及实时响应用户反馈的对话推荐系统。

Result: 总结了模型架构，展示了情感如何影响推荐流程，尤其是对话式推荐。

Conclusion: 提出了未来研究方向，包括处理噪声文本、动态用户偏好和减少偏见，以开发更智能、公平和用户中心的推荐工具。

Abstract: E-commerce platforms generate vast volumes of user feedback, such as star
ratings, written reviews, and comments. However, most recommendation engines
rely primarily on numerical scores, often overlooking the nuanced opinions
embedded in free text. This paper comprehensively reviews sentiment-aware
recommendation systems from a natural language processing perspective, covering
advancements from 2023 to early 2025. It highlights the benefits of integrating
sentiment analysis into e-commerce recommenders to enhance prediction accuracy
and explainability through detailed opinion extraction. Our survey categorizes
recent work into four main approaches: deep learning classifiers that combine
sentiment embeddings with user item interactions, transformer based methods for
nuanced feature extraction, graph neural networks that propagate sentiment
signals, and conversational recommenders that adapt in real time to user
feedback. We summarize model architectures and demonstrate how sentiment flows
through recommendation pipelines, impacting dialogue-based suggestions. Key
challenges include handling noisy or sarcastic text, dynamic user preferences,
and bias mitigation. Finally, we outline research gaps and provide a roadmap
for developing smarter, fairer, and more user-centric recommendation tools.

</details>


### [238] [Memory Assisted LLM for Personalized Recommendation System](https://arxiv.org/abs/2505.03824)
*Jiarui Chen*

Main category: cs.IR

TL;DR: 论文提出了一种基于记忆辅助的个性化大语言模型（MAP），通过用户历史交互数据动态提取相关记忆并融入提示设计，显著提升推荐效果。


<details>
  <summary>Details</summary>
Motivation: 现有个性化大语言模型方法在捕捉多样化用户偏好或及时更新用户历史方面存在不足，导致效率低下或效果不佳。

Method: 通过用户交互创建历史档案，提取相似性相关的记忆并融入提示设计，增强个性化推荐。

Result: 实验表明，MAP在单领域和跨领域推荐任务中均优于传统基于提示设计的LLM推荐方法，且随着用户历史增长，优势更加显著。

Conclusion: MAP通过动态记忆提取和整合，有效解决了现有方法的局限性，适用于连续个性化用户请求。

Abstract: Large language models (LLMs) have demonstrated significant potential in
solving recommendation tasks. With proven capabilities in understanding user
preferences, LLM personalization has emerged as a critical area for providing
tailored responses to individuals. Current studies explore personalization
through prompt design and fine-tuning, paving the way for further research in
personalized LLMs. However, existing approaches are either costly and
inefficient in capturing diverse user preferences or fail to account for timely
updates to user history. To address these gaps, we propose the Memory-Assisted
Personalized LLM (MAP). Through user interactions, we first create a history
profile for each user, capturing their preferences, such as ratings for
historical items. During recommendation, we extract relevant memory based on
similarity, which is then incorporated into the prompts to enhance personalized
recommendations. In our experiments, we evaluate MAP using a sequential rating
prediction task under two scenarios: single domain, where memory and tasks are
from the same category (e.g., movies), and cross-domain (e.g., memory from
movies and recommendation tasks in books). The results show that MAP
outperforms regular LLM-based recommenders that integrate user history directly
through prompt design. Moreover, as user history grows, MAP's advantage
increases in both scenarios, making it more suitable for addressing successive
personalized user requests.

</details>


### [239] [OBD-Finder: Explainable Coarse-to-Fine Text-Centric Oracle Bone Duplicates Discovery](https://arxiv.org/abs/2505.03836)
*Chongsheng Zhang,Shuwen Wu,Yingqi Chen,Matthias Aßenmacher,Christian Heumann,Yi Men,Gaojuan Fan,João Gama*

Main category: cs.IR

TL;DR: 提出了一种渐进式甲骨文重复发现框架，结合无监督低层关键点匹配与高层文本内容匹配，显著提升了甲骨文重复识别的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 甲骨文是中国最早的系统性文字，甲骨文重复识别是甲骨文研究的基础问题。

Method: 设计渐进式框架，结合低层关键点匹配与高层文本内容匹配，优化候选重复甲骨文的排序和语义解释。

Result: 在Top-5和Top-15检索结果中，该方法召回率与现有方法相当，但计算效率显著提升，并发现了60多对新甲骨文重复。

Conclusion: 该方法在甲骨文重复识别中表现出色，具有实际应用价值。

Abstract: Oracle Bone Inscription (OBI) is the earliest systematic writing system in
China, while the identification of Oracle Bone (OB) duplicates is a fundamental
issue in OBI research. In this work, we design a progressive OB duplicate
discovery framework that combines unsupervised low-level keypoints matching
with high-level text-centric content-based matching to refine and rank the
candidate OB duplicates with semantic awareness and interpretability. We
compare our approach with state-of-the-art content-based image retrieval and
image matching methods, showing that our approach yields comparable recall
performance and the highest simplified mean reciprocal rank scores for both
Top-5 and Top-15 retrieval results, and with significantly accelerated
computation efficiency. We have discovered over 60 pairs of new OB duplicates
in real-world deployment, which were missed by OBI researchers for decades. The
models, video illustration and demonstration of this work are available at:
https://github.com/cszhangLMU/OBD-Finder/.

</details>


### [240] [CoCoB: Adaptive Collaborative Combinatorial Bandits for Online Recommendation](https://arxiv.org/abs/2505.03840)
*Cairong Yan,Jinyi Han,Jin Ju,Yanting Zhang,Zijian Wang,Xuan Shao*

Main category: cs.IR

TL;DR: 本文提出了一种自适应协作组合赌博算法（CoCoB），通过双面赌博架构改进推荐系统中的用户聚类问题，显著提升了推荐质量。


<details>
  <summary>Details</summary>
Motivation: 现有聚类赌博方法在定义相似用户和处理独特偏好用户时存在不足，导致推荐质量下降。

Method: CoCoB采用双面赌博架构，用户侧使用改进的贝叶斯模型探索用户相似性，物品侧将物品视为臂生成推荐。

Result: 在三个真实数据集上的实验显示，CoCoB的F1分数平均提升了2.4%。

Conclusion: CoCoB通过动态适应机制有效解决了用户聚类问题，显著提升了推荐系统的性能。

Abstract: Clustering bandits have gained significant attention in recommender systems
by leveraging collaborative information from neighboring users to better
capture target user preferences. However, these methods often lack a clear
definition of similar users and face challenges when users with unique
preferences lack appropriate neighbors. In such cases, relying on divergent
preferences of misidentified neighbors can degrade recommendation quality. To
address these limitations, this paper proposes an adaptive Collaborative
Combinatorial Bandits algorithm (CoCoB). CoCoB employs an innovative two-sided
bandit architecture, applying bandit principles to both the user and item
sides. The user-bandit employs an enhanced Bayesian model to explore user
similarity, identifying neighbors based on a similarity probability threshold.
The item-bandit treats items as arms, generating diverse recommendations
informed by the user-bandit's output. CoCoB dynamically adapts, leveraging
neighbor preferences when available or focusing solely on the target user
otherwise. Regret analysis under a linear contextual bandit setting and
experiments on three real-world datasets demonstrate CoCoB's effectiveness,
achieving an average 2.4% improvement in F1 score over state-of-the-art
methods.

</details>


### [241] [To Judge or not to Judge: Using LLM Judgements for Advertiser Keyphrase Relevance at eBay](https://arxiv.org/abs/2505.04209)
*Soumik Dey,Hansi Wu,Binbin Li*

Main category: cs.IR

TL;DR: 论文探讨了电商广告关键词相关性模型的问题，提出结合卖家判断、广告系统和搜索系统的动态交互，并通过LLM作为代理实现模型优化。


<details>
  <summary>Details</summary>
Motivation: 解决广告关键词相关性模型仅依赖点击/销售信号的问题，强调与卖家判断对齐的重要性，以提升广告效果和卖家体验。

Method: 将广告关键词相关性建模为卖家判断、广告系统和搜索系统的动态交互，利用LLM作为代理大规模评估卖家判断。

Result: 通过LLM代理和严谨的业务指标评估框架，实现了三个系统的更好协调。

Conclusion: 结合卖家判断和LLM代理的模型优化方法，能有效提升广告关键词的相关性和整体系统效率。

Abstract: E-commerce sellers are recommended keyphrases based on their inventory on
which they advertise to increase buyer engagement (clicks/sales). The relevance
of advertiser keyphrases plays an important role in preventing the inundation
of search systems with numerous irrelevant items that compete for attention in
auctions, in addition to maintaining a healthy seller perception. In this work,
we describe the shortcomings of training Advertiser keyphrase relevance filter
models on click/sales/search relevance signals and the importance of aligning
with human judgment, as sellers have the power to adopt or reject said
keyphrase recommendations. In this study, we frame Advertiser keyphrase
relevance as a complex interaction between 3 dynamical systems -- seller
judgment, which influences seller adoption of our product, Advertising, which
provides the keyphrases to bid on, and Search, who holds the auctions for the
same keyphrases. This study discusses the practicalities of using human
judgment via a case study at eBay Advertising and demonstrate that using
LLM-as-a-judge en-masse as a scalable proxy for seller judgment to train our
relevance models achieves a better harmony across the three systems -- provided
that they are bound by a meticulous evaluation framework grounded in business
metrics.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [242] [Is the end of Insight in Sight ?](https://arxiv.org/abs/2505.04627)
*Jean-Michel Tucny,Mihir Durve,Sauro Succi*

Main category: physics.comp-ph

TL;DR: PINN在玻尔兹曼方程问题中的权重矩阵与物理问题的数学结构无直接关联，接近高斯随机分布，表明深度学习与数值解可能是两条独立但等效的路径。


<details>
  <summary>Details</summary>
Motivation: 探讨PINN在玻尔兹曼方程问题中的权重矩阵特性及其与物理问题数学结构的关系。

Method: 分析PINN权重矩阵的分布特性，并与物理问题的数学结构对比。

Result: 权重矩阵接近高斯随机分布，与物理问题的数学结构无直接联系。

Conclusion: 深度学习与数值解可能是等效但独立的路径，Explainable AI可能不现实或不适定。

Abstract: It is shown that the weight matrices of a Physics-informed neural network
(PINN)-based deep learning application to a rarefied gas dynamics problem
described by the Boltzmann equation bear no evident link to the mathematical
structure of the physical problem. Instead, the weights appear close to
Gaussian distributed random matrices. Although significantly more work is
needed to support a robust assessment in this direction, these results suggest
that deep-learning and the numerical solution of the Boltzmann equation
represent two equivalent, but largely distinct paths to the same physical
knowledge. If so, Explainable AI might be an unrealistic target and possibly
even an ill-posed one.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [243] [Model-Based AI planning and Execution Systems for Robotics](https://arxiv.org/abs/2505.04493)
*Or Wertheim,Ronen I. Brafman*

Main category: cs.RO

TL;DR: 本文探讨了基于模型的规划和执行系统在机器人任务控制中的设计选择、现有解决方案及未来发展方向。


<details>
  <summary>Details</summary>
Motivation: 研究如何通过基于模型的系统实现灵活、多任务的自主机器人，整合现代机器人平台。

Method: 综述了现有系统的设计选择、解决方案，并分析了ROSPlan等系统的影响。

Result: 总结了当前系统的多样性及其解决的问题，提出了未来发展的方向。

Conclusion: 基于模型的系统在机器人任务控制中具有潜力，但仍需进一步研究和改进。

Abstract: Model-based planning and execution systems offer a principled approach to
building flexible autonomous robots that can perform diverse tasks by
automatically combining a host of basic skills. This idea is almost as old as
modern robotics. Yet, while diverse general-purpose reasoning architectures
have been proposed since, general-purpose systems that are integrated with
modern robotic platforms have emerged only recently, starting with the
influential ROSPlan system. Since then, a growing number of model-based systems
for robot task-level control have emerged. In this paper, we consider the
diverse design choices and issues existing systems attempt to address, the
different solutions proposed so far, and suggest avenues for future
development.

</details>


### [244] [Modeling Personalized Difficulty of Rehabilitation Exercises Using Causal Trees](https://arxiv.org/abs/2505.04583)
*Nathaniel Dennler,Zhonghao Shi,Uksang Yoo,Stefanos Nikolaidis,Maja Matarić*

Main category: cs.RO

TL;DR: 本文提出了一种基于因果树的方法，用于根据用户表现计算康复训练的难度，以个性化调整难度，从而提高康复效果和用户动机。


<details>
  <summary>Details</summary>
Motivation: 传统方法假设所有用户的训练难度相同，但研究发现中风幸存者对难度的感知因人而异，因此需要个性化的难度调整方法。

Method: 采用因果树模型，根据用户的表现动态计算训练难度，并提供可解释的模型。

Result: 该方法能准确建模训练难度，并为用户和护理人员提供直观的解释。

Conclusion: 个性化难度调整方法能有效提升康复效果和用户参与动机。

Abstract: Rehabilitation robots are often used in game-like interactions for
rehabilitation to increase a person's motivation to complete rehabilitation
exercises. By adjusting exercise difficulty for a specific user throughout the
exercise interaction, robots can maximize both the user's rehabilitation
outcomes and the their motivation throughout the exercise. Previous approaches
have assumed exercises have generic difficulty values that apply to all users
equally, however, we identified that stroke survivors have varied and unique
perceptions of exercise difficulty. For example, some stroke survivors found
reaching vertically more difficult than reaching farther but lower while others
found reaching farther more challenging than reaching vertically. In this
paper, we formulate a causal tree-based method to calculate exercise difficulty
based on the user's performance. We find that this approach accurately models
exercise difficulty and provides a readily interpretable model of why that
exercise is difficult for both users and caretakers.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [245] [High-speed multiwavelength photonic temporal integration using silicon photonics](https://arxiv.org/abs/2505.04405)
*Yi Zhang,Nikolaos Farmakidis,Ioannis Roumpos,Miltiadis Moralis-Pegios,Apostolos Tsakyridis,June Sang Lee,Bowei Dong,Yuhan He,Samarth Aggarwal,Nikolaos Pleros,Harish Bhaskaran*

Main category: physics.optics

TL;DR: 通过利用热耗散过程实现光学信号的时间积分，提出了一种可扩展的高速光子计算方法。


<details>
  <summary>Details</summary>
Motivation: 解决光学硬件在AI任务中映射大向量尺寸的挑战，同时避免低效的电光转换。

Method: 引入光路中的光子加热器（PHIL）单元，利用热耗散过程实现50 GHz调制信号的时间积分。

Result: 实现了端到端的光信号处理，支持线性和非线性操作，为高速光子计算提供了可扩展路径。

Conclusion: 通过热驱动积分，为高速光子计算提供了一种新的可扩展架构。

Abstract: Optical systems have been pivotal for energy-efficient computing, performing
high-speed, parallel operations in low-loss carriers. While these predominantly
analog optical accelerators bypass digitization to perform parallel
floating-point computations, scaling optical hardware to map large-vector sizes
for AI tasks remains challenging. Here, we overcome this limitation by
unfolding scalar operations in time and introducing a
photonic-heater-in-lightpath (PHIL) unit for all-optical temporal integration.
Counterintuitively, we exploit a slow heat dissipation process to integrate
optical signals modulated at 50 GHz bridging the speed gap between the widely
applied thermo-optic effects and ultrafast photonics. This architecture
supports optical end-to-end signal processing, eliminates inefficient
electro-optical conversions, and enables both linear and nonlinear operations
within a unified framework. Our results demonstrate a scalable path towards
high-speed photonic computing through thermally driven integration.

</details>

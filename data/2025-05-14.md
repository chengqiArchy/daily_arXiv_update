<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 66]
- [cs.AI](#cs.AI) [Total: 39]
- [cs.LG](#cs.LG) [Total: 70]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 2]
- [cs.SI](#cs.SI) [Total: 2]
- [cs.CR](#cs.CR) [Total: 3]
- [cs.DL](#cs.DL) [Total: 1]
- [physics.data-an](#physics.data-an) [Total: 1]
- [cs.HC](#cs.HC) [Total: 3]
- [eess.AS](#eess.AS) [Total: 1]
- [cs.CV](#cs.CV) [Total: 19]
- [econ.GN](#econ.GN) [Total: 1]
- [cs.IR](#cs.IR) [Total: 3]
- [q-bio.GN](#q-bio.GN) [Total: 1]
- [physics.comp-ph](#physics.comp-ph) [Total: 1]
- [stat.ML](#stat.ML) [Total: 11]
- [cs.GR](#cs.GR) [Total: 3]
- [cs.DC](#cs.DC) [Total: 3]
- [stat.AP](#stat.AP) [Total: 1]
- [cs.NI](#cs.NI) [Total: 11]
- [cs.SD](#cs.SD) [Total: 3]
- [cs.CY](#cs.CY) [Total: 5]
- [astro-ph.GA](#astro-ph.GA) [Total: 1]
- [cs.SE](#cs.SE) [Total: 4]
- [eess.IV](#eess.IV) [Total: 4]
- [stat.ME](#stat.ME) [Total: 1]
- [cs.DS](#cs.DS) [Total: 1]
- [cs.AR](#cs.AR) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 2]
- [math.OC](#math.OC) [Total: 1]
- [eess.SY](#eess.SY) [Total: 2]
- [eess.SP](#eess.SP) [Total: 1]
- [cs.RO](#cs.RO) [Total: 11]
- [q-bio.BM](#q-bio.BM) [Total: 2]
- [quant-ph](#quant-ph) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Polysemy of Synthetic Neurons Towards a New Type of Explanatory Categorical Vector Spaces](https://arxiv.org/abs/2505.07831)
*Michael Pichat,William Pogrund,Paloma Pichat,Judicael Poumay,Armanouche Gasparian,Samuel Demarchi,Martin Corbet,Alois Georgeon,Michael Veillet-Guillem*

Main category: cs.CL

TL;DR: 论文提出了一种几何方法，将神经元定义为具有非正交基的分类向量空间，通过层内注意力机制识别关键分类区域以提高语言模型效率。


<details>
  <summary>Details</summary>
Motivation: 理解人工智能语言模型中合成神经元的多义性，并提出一种替代方法以优化其特征叠加方式。

Method: 将神经元定义为分类向量空间，利用层内注意力机制识别关键分类区域。

Result: 该方法能够更高效地利用神经元的分类子维度，提升语言模型的性能。

Conclusion: 几何定义和层内注意力机制为理解神经元多义性提供了新视角，并优化了语言模型的效率。

Abstract: The polysemantic nature of synthetic neurons in artificial intelligence
language models is currently understood as the result of a necessary
superposition of distributed features within the latent space. We propose an
alternative approach, geometrically defining a neuron in layer n as a
categorical vector space with a non-orthogonal basis, composed of categorical
sub-dimensions extracted from preceding neurons in layer n-1. This categorical
vector space is structured by the activation space of each neuron and enables,
via an intra-neuronal attention process, the identification and utilization of
a critical categorical zone for the efficiency of the language model - more
homogeneous and located at the intersection of these different categorical
sub-dimensions.

</details>


### [2] [A Tale of Two Identities: An Ethical Audit of Human and AI-Crafted Personas](https://arxiv.org/abs/2505.07850)
*Pranav Narayanan Venkit,Jiayi Li,Yingfan Zhou,Sarah Rajtmajer,Shomir Wilson*

Main category: cs.CL

TL;DR: 论文研究了大型语言模型（LLMs）生成的合成人物在少数族裔身份表征中的问题，揭示了算法他者化现象，并提出了改进建议。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在数据有限领域（如健康、隐私和HCI）中生成合成人物的应用增多，需要了解这些叙事如何表征少数族裔身份。

Method: 采用混合方法（细读、词汇分析和参数化创造力框架），比较了1512个LLM生成人物与人类撰写的内容。

Result: LLM生成的人物过度强调种族标记，使用文化编码语言，导致刻板印象、异域化等社会技术危害。

Conclusion: 提出设计建议，包括叙事感知评估指标和以社区为中心的验证协议，以减少算法他者化现象。

Abstract: As LLMs (large language models) are increasingly used to generate synthetic
personas particularly in data-limited domains such as health, privacy, and HCI,
it becomes necessary to understand how these narratives represent identity,
especially that of minority communities. In this paper, we audit synthetic
personas generated by 3 LLMs (GPT4o, Gemini 1.5 Pro, Deepseek 2.5) through the
lens of representational harm, focusing specifically on racial identity. Using
a mixed methods approach combining close reading, lexical analysis, and a
parameterized creativity framework, we compare 1512 LLM generated personas to
human-authored responses. Our findings reveal that LLMs disproportionately
foreground racial markers, overproduce culturally coded language, and construct
personas that are syntactically elaborate yet narratively reductive. These
patterns result in a range of sociotechnical harms, including stereotyping,
exoticism, erasure, and benevolent bias, that are often obfuscated by
superficially positive narrations. We formalize this phenomenon as algorithmic
othering, where minoritized identities are rendered hypervisible but less
authentic. Based on these findings, we offer design recommendations for
narrative-aware evaluation metrics and community-centered validation protocols
for synthetic identity generation.

</details>


### [3] [Joint Detection of Fraud and Concept Drift inOnline Conversations with LLM-Assisted Judgment](https://arxiv.org/abs/2505.07852)
*Ali Senol,Garima Agrawal,Huan Liu*

Main category: cs.CL

TL;DR: 提出了一种两阶段检测框架，结合集成分类模型和概念漂移分析，以提高数字通信平台中虚假交互检测的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 数字通信平台中的虚假交互问题尚未得到充分解决，传统静态异常检测方法无法适应动态对话变化，容易误判良性话题转换。

Method: 首先使用集成分类模型识别可疑对话，再通过概念漂移分析（OCDD）和大型语言模型（LLM）判断是否为欺诈行为。

Result: 在社交工程聊天场景数据集上验证了框架的实用优势，提高了实时欺诈检测的准确性和可解释性。

Conclusion: 提出的模块化方法优于双LLM基线，能够更可靠地检测虚假交互。

Abstract: Detecting fake interactions in digital communication platforms remains a
challenging and insufficiently addressed problem. These interactions may appear
as harmless spam or escalate into sophisticated scam attempts, making it
difficult to flag malicious intent early. Traditional detection methods often
rely on static anomaly detection techniques that fail to adapt to dynamic
conversational shifts. One key limitation is the misinterpretation of benign
topic transitions referred to as concept drift as fraudulent behavior, leading
to either false alarms or missed threats. We propose a two stage detection
framework that first identifies suspicious conversations using a tailored
ensemble classification model. To improve the reliability of detection, we
incorporate a concept drift analysis step using a One Class Drift Detector
(OCDD) to isolate conversational shifts within flagged dialogues. When drift is
detected, a large language model (LLM) assesses whether the shift indicates
fraudulent manipulation or a legitimate topic change. In cases where no drift
is found, the behavior is inferred to be spam like. We validate our framework
using a dataset of social engineering chat scenarios and demonstrate its
practical advantages in improving both accuracy and interpretability for real
time fraud detection. To contextualize the trade offs, we compare our modular
approach against a Dual LLM baseline that performs detection and judgment using
different language models.

</details>


### [4] [CrashSage: A Large Language Model-Centered Framework for Contextual and Interpretable Traffic Crash Analysis](https://arxiv.org/abs/2505.07853)
*Hao Zhen,Jidong J. Yang*

Main category: cs.CL

TL;DR: CrashSage是一个基于大型语言模型（LLM）的框架，用于改进交通事故分析，通过数据转换、增强、模型微调和可解释性技术提升性能。


<details>
  <summary>Details</summary>
Motivation: 全球每年因交通事故造成巨大生命和经济损失，现有方法忽视上下文信息且信息损失严重，亟需更有效的分析工具。

Method: 采用表格到文本转换、上下文数据增强、LLaMA3-8B微调和梯度可解释性技术。

Result: CrashSage在事故严重性推断上优于基线方法（如GPT-4o等），并提供更透明的决策解释。

Conclusion: CrashSage为交通安全研究提供了更高效、透明的解决方案，有助于针对性干预措施的制定。

Abstract: Road crashes claim over 1.3 million lives annually worldwide and incur global
economic losses exceeding \$1.8 trillion. Such profound societal and financial
impacts underscore the urgent need for road safety research that uncovers crash
mechanisms and delivers actionable insights. Conventional statistical models
and tree ensemble approaches typically rely on structured crash data,
overlooking contextual nuances and struggling to capture complex relationships
and underlying semantics. Moreover, these approaches tend to incur significant
information loss, particularly in narrative elements related to multi-vehicle
interactions, crash progression, and rare event characteristics. This study
presents CrashSage, a novel Large Language Model (LLM)-centered framework
designed to advance crash analysis and modeling through four key innovations.
First, we introduce a tabular-to-text transformation strategy paired with
relational data integration schema, enabling the conversion of raw,
heterogeneous crash data into enriched, structured textual narratives that
retain essential structural and relational context. Second, we apply
context-aware data augmentation using a base LLM model to improve narrative
coherence while preserving factual integrity. Third, we fine-tune the LLaMA3-8B
model for crash severity inference, demonstrating superior performance over
baseline approaches, including zero-shot, zero-shot with chain-of-thought
prompting, and few-shot learning, with multiple models (GPT-4o, GPT-4o-mini,
LLaMA3-70B). Finally, we employ a gradient-based explainability technique to
elucidate model decisions at both the individual crash level and across broader
risk factor dimensions. This interpretability mechanism enhances transparency
and enables targeted road safety interventions by providing deeper insights
into the most influential factors.

</details>


### [5] [Unpacking Robustness in Inflectional Languages: Adversarial Evaluation and Mechanistic Insights](https://arxiv.org/abs/2505.07856)
*Paweł Walkowiak,Marek Klonowski,Marcin Oleksy,Arkadiusz Janz*

Main category: cs.CL

TL;DR: 本文研究了对抗性攻击在屈折语言（如波兰语）中的表现，提出了一种基于Edge Attribution Patching（EAP）的新评估协议，并创建了一个新基准MultiEmo来分析屈折与模型鲁棒性的关系。


<details>
  <summary>Details</summary>
Motivation: 大多数对抗性攻击方法主要在非屈折语言（如英语）中开发和评估，本文旨在填补屈折语言中对抗性攻击研究的空白。

Method: 设计了基于EAP的评估协议，使用平行语料库（波兰语和英语），并创建了MultiEmo基准来分析模型在攻击下的行为。

Result: 通过新协议和基准，揭示了屈折对模型行为及鲁棒性的影响，并识别了模型电路中与屈折相关的机制。

Conclusion: 屈折语言中的对抗性攻击表现与非屈折语言不同，新方法为理解模型鲁棒性提供了新视角。

Abstract: Various techniques are used in the generation of adversarial examples,
including methods such as TextBugger which introduce minor, hardly visible
perturbations to words leading to changes in model behaviour. Another class of
techniques involves substituting words with their synonyms in a way that
preserves the text's meaning but alters its predicted class, with TextFooler
being a prominent example of such attacks. Most adversarial example generation
methods are developed and evaluated primarily on non-inflectional languages,
typically English. In this work, we evaluate and explain how adversarial
attacks perform in inflectional languages. To explain the impact of inflection
on model behaviour and its robustness under attack, we designed a novel
protocol inspired by mechanistic interpretability, based on Edge Attribution
Patching (EAP) method. The proposed evaluation protocol relies on parallel
task-specific corpora that include both inflected and syncretic variants of
texts in two languages -- Polish and English. To analyse the models and explain
the relationship between inflection and adversarial robustness, we create a new
benchmark based on task-oriented dataset MultiEmo, enabling the identification
of mechanistic inflection-related elements of circuits within the model and
analyse their behaviour under attack.

</details>


### [6] [Enhanced Urdu Intent Detection with Large Language Models and Prototype-Informed Predictive Pipelines](https://arxiv.org/abs/2505.07857)
*Faiza Hassan,Summra Saleem,Kashif Javed,Muhammad Nabeel Asim,Abdur Rehman,Andreas Dengel*

Main category: cs.CL

TL;DR: 该论文提出了一种基于对比学习的意图检测框架LLMPIA，专门针对乌尔都语，利用未标记数据重新训练预训练语言模型，并在两个公开数据集上取得了显著效果。


<details>
  <summary>Details</summary>
Motivation: 乌尔都语作为第十大语言，其意图检测领域缺乏基于少样本学习的预测器，传统方法仅能预测训练集中见过的类别。

Method: 采用对比学习方法，结合预训练语言模型和原型注意力机制，构建端到端的LLMPIA意图检测框架，并测试了6种语言模型和13种相似度计算方法。

Result: 在ATIS和Web Queries数据集上，LLMPIA分别取得了83.28%/98.25%和76.23%/84.42%的F1分数，并在相同类别设置下超越现有最佳方法53.55%。

Conclusion: LLMPIA框架显著提升了乌尔都语意图检测的性能，证明了对比学习和原型注意力机制的有效性。

Abstract: Multifarious intent detection predictors are developed for different
languages, including English, Chinese and French, however, the field remains
underdeveloped for Urdu, the 10th most spoken language. In the realm of
well-known languages, intent detection predictors utilize the strategy of
few-shot learning and prediction of unseen classes based on the model training
on seen classes. However, Urdu language lacks few-shot strategy based intent
detection predictors and traditional predictors are focused on prediction of
the same classes which models have seen in the train set. To empower Urdu
language specific intent detection, this introduces a unique contrastive
learning approach that leverages unlabeled Urdu data to re-train pre-trained
language models. This re-training empowers LLMs representation learning for the
downstream intent detection task. Finally, it reaps the combined potential of
pre-trained LLMs and the prototype-informed attention mechanism to create a
comprehensive end-to-end LLMPIA intent detection pipeline. Under the paradigm
of proposed predictive pipeline, it explores the potential of 6 distinct
language models and 13 distinct similarity computation methods. The proposed
framework is evaluated on 2 public benchmark datasets, namely ATIS encompassing
5836 samples and Web Queries having 8519 samples. Across ATIS dataset under
4-way 1 shot and 4-way 5 shot experimental settings LLMPIA achieved 83.28% and
98.25% F1-Score and on Web Queries dataset produced 76.23% and 84.42% F1-Score,
respectively. In an additional case study on the Web Queries dataset under same
classes train and test set settings, LLMPIA outperformed state-of-the-art
predictor by 53.55% F1-Score.

</details>


### [7] [Scaling Laws for Speculative Decoding](https://arxiv.org/abs/2505.07858)
*Siyuan Yan,Mo Zhu,Guo-qing Jiang,Jianfei Wang,Jiaxing Chen,Wentai Zhang,Xiang Liao,Xiao Cui,Chen Zhang,Zhuoran Song,Ran Zhu*

Main category: cs.CL

TL;DR: 本文研究了通过密集LLM架构的推测解码技术，发现了控制解码效率的对数线性缩放定律，并提出了Scylla方法，显著提升了推理任务的效率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）在推理密集型任务中需要高效解码，但现有方法的解码效率缩放规律尚未充分探索。

Method: 通过研究推测解码技术，发现对数线性缩放定律（定理1.1-1.3），并开发了Scylla方法，协调多维度缩放。

Result: Scylla在解码接受率和吞吐量上优于EAGLE2和EAGLE3，工业部署中实现了2倍的解码吞吐量提升。

Conclusion: 系统性缩放对高效LLM推理具有变革性潜力，Scylla为推理任务提供了显著加速。

Abstract: The escalating demand for efficient decoding in large language models (LLMs)
is particularly critical for reasoning-intensive architectures like OpenAI-o3
and DeepSeek-R1, which depend on extended chain-of-thought reasoning. This
study investigates speculative decoding techniques through dense LLM
architectures to establish foundational insights for accelerating reasoning
tasks. While speculative decoding methods leveraging parallel
draft-verification cycles have emerged as promising acceleration techniques,
the scaling laws governing decoding efficiency remain under-explored compared
to conventional backbone LLMs developed through Pretraining->SFT->RLHF training
paradigms. In this work, we discover Log-linear Scaling Laws (Theorem 1.1, 1.2
and 1.3) governing draft model acceptance rate (or decoding speed) across three
dimensions: pretraining token volume, draft model capacity, and decoding batch
size. Building on these laws, we achieve Scylla, which coordinates
multi-dimensional scaling for popular LLMs (Llama2/3, Qwen2.5). Empirical
validation shows Scylla achieves 1.5-2.2 higher acceptance rate than EAGLE2 and
0.3 higher than EAGLE3 at temperature T = 0, with peak performance gains on
summarization and QA tasks (Figure 2). Industrial inference engine deployments
demonstrate 2X decoding throughput improvements over EAGLE2 (Table 5),
validating the transformative potential of systematic scaling for efficient LLM
inference. Code will be released later.

</details>


### [8] [Boosting Performance on ARC is a Matter of Perspective](https://arxiv.org/abs/2505.07859)
*Daniel Franzen,Jan Disselhoff,David Hartmann*

Main category: cs.CL

TL;DR: 该论文提出了一种通过任务特定数据增强和深度优先搜索算法提升大型语言模型在ARC-AGI任务中表现的方法，实现了71.6%的得分，并强调了其透明性和低成本优势。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在抽象推理任务（如ARC-AGI）中的局限性，提升其推理能力。

Method: 在训练、生成和评分阶段使用任务特定数据增强，结合深度优先搜索算法生成多样化的候选解决方案，并利用LLM作为生成器和评分器。

Result: 在公开ARC-AGI评估集上获得71.6%的得分（286.5/400任务），表现优于现有公开方法。

Conclusion: 该方法在透明性、可复现性和低成本方面具有显著优势，尽管闭源方法得分更高。

Abstract: The Abstraction and Reasoning Corpus (ARC-AGI) poses a significant challenge
for large language models (LLMs), exposing limitations in their abstract
reasoning abilities. In this work, we leverage task-specific data augmentations
throughout the training, generation, and scoring phases, and employ a
depth-first search algorithm to generate diverse, high-probability candidate
solutions. Furthermore, we utilize the LLM not only as a generator but also as
a scorer, using its output probabilities to select the most promising
solutions. Our method achieves a score of 71.6% (286.5/400 solved tasks) on the
public ARC-AGI evaluation set, demonstrating state-of-the-art performance among
publicly available approaches. While concurrent closed-source work has reported
higher scores, our method distinguishes itself through its transparency,
reproducibility, and remarkably low inference cost, averaging only around 2ct
per task on readily available hardware (we assume a price of 36ct/hour for a
Nvidia 4090 GPU).

</details>


### [9] [Scalable LLM Math Reasoning Acceleration with Low-rank Distillation](https://arxiv.org/abs/2505.07861)
*Harry Dong,Bilge Acun,Beidi Chen,Yuejie Chi*

Main category: cs.CL

TL;DR: Caprese是一种低成本蒸馏方法，用于恢复因高效推理方法部署而丢失的数学能力，同时不影响语言任务。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）的数学推理需要大量计算资源和时间，而现有高效推理方法在语言任务上表现良好，但会严重降低数学性能。

Method: Caprese通过仅增加约1%的参数和20K合成训练样本，在不扰动原始权重的情况下，恢复数学能力。

Result: Caprese显著减少了活跃参数数量（如Gemma 2 9B和Llama 3.1 8B减少约2B），并降低了延迟（如Qwen 2.5 14B生成2048个令牌时延迟减少11%）。

Conclusion: Caprese是一种高效且低成本的方法，能够恢复数学能力并优化推理性能。

Abstract: Due to long generations, large language model (LLM) math reasoning demands
significant computational resources and time. While many existing efficient
inference methods have been developed with excellent performance preservation
on language tasks, they often severely degrade math performance. In this paper,
we propose Caprese, a low-cost distillation method to recover lost capabilities
from deploying efficient inference methods, focused primarily in feedforward
blocks. With original weights unperturbed, roughly 1% of additional parameters,
and only 20K synthetic training samples, we are able to recover much if not all
of the math capabilities lost from efficient inference for thinking LLMs and
without harm to language tasks for instruct LLMs. Moreover, Caprese slashes the
number of active parameters (~2B cut for Gemma 2 9B and Llama 3.1 8B) and
integrates cleanly into existing model layers to reduce latency (>11% reduction
to generate 2048 tokens with Qwen 2.5 14B) while encouraging response brevity.

</details>


### [10] [Graph Laplacian Wavelet Transformer via Learnable Spectral Decomposition](https://arxiv.org/abs/2505.07862)
*Andrew Kiruluta,Eric Lundy,Priscilla Burity*

Main category: cs.CL

TL;DR: 论文提出了一种名为Graph Wavelet Transformer（GWT）的新架构，用于替代传统序列到序列模型中计算和内存复杂度较高的点积自注意力机制。


<details>
  <summary>Details</summary>
Motivation: 现有的序列到序列模型在处理结构化语言任务时依赖点积自注意力机制，导致计算和内存复杂度随输入长度呈二次方增长，效率低下。

Method: GWT通过引入基于显式图拉普拉斯算子的可学习多尺度小波变换，替代传统的自注意力机制，实现更高效的图结构序列建模。

Result: 分析表明，多尺度谱分解为图结构序列建模提供了一种可解释、高效且表达能力强的替代方案。

Conclusion: GWT为结构化语言任务提供了一种更高效的建模方法，解决了传统自注意力机制的瓶颈问题。

Abstract: Existing sequence to sequence models for structured language tasks rely
heavily on the dot product self attention mechanism, which incurs quadratic
complexity in both computation and memory for input length N. We introduce the
Graph Wavelet Transformer (GWT), a novel architecture that replaces this
bottleneck with a learnable, multi scale wavelet transform defined over an
explicit graph Laplacian derived from syntactic or semantic parses. Our
analysis shows that multi scale spectral decomposition offers an interpretable,
efficient, and expressive alternative to quadratic self attention for graph
structured sequence modeling.

</details>


### [11] [QoSBERT: An Uncertainty-Aware Approach based on Pre-trained Language Models for Service Quality Prediction](https://arxiv.org/abs/2505.07863)
*Ziliang Wang,Xiaohong Zhang,Ze Shi Li,Meng Yan*

Main category: cs.CL

TL;DR: QoSBERT是一个基于预训练语言模型的框架，将QoS预测重新定义为语义回归任务，并提供不确定性估计，显著提升了预测准确性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 传统QoS模型依赖手动特征工程且仅提供点估计，缺乏对预测置信度的洞察。QoSBERT旨在通过语义理解和不确定性估计解决这些问题。

Method: QoSBERT将用户服务元数据编码为自然语言描述，结合蒙特卡洛Dropout进行不确定性估计，使用注意力池化和轻量级多层感知机回归器进行优化。

Result: 在标准QoS基准测试中，QoSBERT在响应时间和吞吐量预测上分别平均降低了11.7%和6.9%的MAE，同时提供校准的置信区间。

Conclusion: QoSBERT不仅提升了预测准确性，还提供了可靠的不确定性量化，为更可信的服务选择和优化奠定了基础。

Abstract: Accurate prediction of Quality of Service (QoS) metrics is fundamental for
selecting and managing cloud based services. Traditional QoS models rely on
manual feature engineering and yield only point estimates, offering no insight
into the confidence of their predictions. In this paper, we propose QoSBERT,
the first framework that reformulates QoS prediction as a semantic regression
task based on pre trained language models. Unlike previous approaches relying
on sparse numerical features, QoSBERT automatically encodes user service
metadata into natural language descriptions, enabling deep semantic
understanding. Furthermore, we integrate a Monte Carlo Dropout based
uncertainty estimation module, allowing for trustworthy and risk-aware service
quality prediction, which is crucial yet underexplored in existing QoS models.
QoSBERT applies attentive pooling over contextualized embeddings and a
lightweight multilayer perceptron regressor, fine tuned jointly to minimize
absolute error. We further exploit the resulting uncertainty estimates to
select high quality training samples, improving robustness in low resource
settings. On standard QoS benchmark datasets, QoSBERT achieves an average
reduction of 11.7% in MAE and 6.7% in RMSE for response time prediction, and
6.9% in MAE for throughput prediction compared to the strongest baselines,
while providing well calibrated confidence intervals for robust and trustworthy
service quality estimation. Our approach not only advances the accuracy of
service quality prediction but also delivers reliable uncertainty
quantification, paving the way for more trustworthy, data driven service
selection and optimization.

</details>


### [12] [Efficient Fairness Testing in Large Language Models: Prioritizing Metamorphic Relations for Bias Detection](https://arxiv.org/abs/2505.07870)
*Suavis Giramata,Madhusudan Srinivasan,Venkat Naidu Gudivada,Upulee Kanewala*

Main category: cs.CL

TL;DR: 本文提出了一种基于句子多样性的蜕变关系（MRs）优先级排序方法，用于高效检测大语言模型（LLMs）中的公平性问题。实验表明，该方法在故障检测率和首次故障时间上优于随机和距离排序，且接近基于故障的排序效果，同时显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLMs）的广泛应用，其输出中的公平性和偏见问题日益突出。由于测试用例数量庞大，如何高效检测公平性问题成为关键挑战。

Method: 采用基于句子多样性的方法计算和排序蜕变关系（MRs），以优化故障检测。

Result: 实验结果显示，该方法比随机排序和距离排序分别提高故障检测率22%和12%，首次故障时间减少15%和8%，且效果接近基于故障的排序，但计算成本更低。

Conclusion: 多样性为基础的MR优先级排序方法能有效提升LLMs的公平性测试效果，同时降低计算成本。

Abstract: Large Language Models (LLMs) are increasingly deployed in various
applications, raising critical concerns about fairness and potential biases in
their outputs. This paper explores the prioritization of metamorphic relations
(MRs) in metamorphic testing as a strategy to efficiently detect fairness
issues within LLMs. Given the exponential growth of possible test cases,
exhaustive testing is impractical; therefore, prioritizing MRs based on their
effectiveness in detecting fairness violations is crucial. We apply a sentence
diversity-based approach to compute and rank MRs to optimize fault detection.
Experimental results demonstrate that our proposed prioritization approach
improves fault detection rates by 22% compared to random prioritization and 12%
compared to distance-based prioritization, while reducing the time to the first
failure by 15% and 8%, respectively. Furthermore, our approach performs within
5% of fault-based prioritization in effectiveness, while significantly reducing
the computational cost associated with fault labeling. These results validate
the effectiveness of diversity-based MR prioritization in enhancing fairness
testing for LLMs.

</details>


### [13] [Evaluating Financial Sentiment Analysis with Annotators Instruction Assisted Prompting: Enhancing Contextual Interpretation and Stock Prediction Accuracy](https://arxiv.org/abs/2505.07871)
*A M Muntasir Rahman,Ajim Uddin,Guiling "Grace" Wang*

Main category: cs.CL

TL;DR: 论文提出了一种名为AIAP的新型评估提示方法，通过整合人类标注者的任务指令，改善了金融情感分析（FSA）中LLMs的性能，并在新数据集WSBS上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 金融情感分析（FSA）的挑战在于其语言复杂性和现有基准数据集中标注的主观性，导致LLMs在评估中表现不佳。

Method: 提出Annotators' Instruction Assisted Prompt（AIAP），将人类标注者的详细任务指令整合到LLMs的提示框架中，以标准化情感理解。

Result: 实验显示AIAP显著提升LLMs性能（最高提升9.08），并提出基于模型置信度的情感索引方法，增强股票价格预测。

Conclusion: AIAP通过改进任务定义和评估方法，提升了FSA的性能，并展示了WSB作为金融文本来源的重要性。

Abstract: Financial sentiment analysis (FSA) presents unique challenges to LLMs that
surpass those in typical sentiment analysis due to the nuanced language used in
financial contexts. The prowess of these models is often undermined by the
inherent subjectivity of sentiment classifications in existing benchmark
datasets like Financial Phrasebank. These datasets typically feature undefined
sentiment classes that reflect the highly individualized perspectives of
annotators, leading to significant variability in annotations. This variability
results in an unfair expectation for LLMs during benchmarking, where they are
tasked to conjecture the subjective viewpoints of human annotators without
sufficient context. In this paper, we introduce the Annotators' Instruction
Assisted Prompt, a novel evaluation prompt designed to redefine the task
definition of FSA for LLMs. By integrating detailed task instructions
originally intended for human annotators into the LLMs' prompt framework, AIAP
aims to standardize the understanding of sentiment across both human and
machine interpretations, providing a fair and context-rich foundation for
sentiment analysis. We utilize a new dataset, WSBS, derived from the
WallStreetBets subreddit to demonstrate how AIAP significantly enhances LLM
performance by aligning machine operations with the refined task definitions.
Experimental results demonstrate that AIAP enhances LLM performance
significantly, with improvements up to 9.08. This context-aware approach not
only yields incremental gains in performance but also introduces an innovative
sentiment-indexing method utilizing model confidence scores. This method
enhances stock price prediction models and extracts more value from the
financial sentiment analysis, underscoring the significance of WSB as a
critical source of financial text. Our research offers insights into both
improving FSA through better evaluation methods.

</details>


### [14] [The Sound of Populism: Distinct Linguistic Features Across Populist Variants](https://arxiv.org/abs/2505.07874)
*Yu Wang,Runxi Yu,Zhongyuan Wang,Jing He*

Main category: cs.CL

TL;DR: 该研究通过结合LIWC特征和RoBERTa模型，分析了美国政治演讲中民粹主义的语言特征，揭示了其直接、自信的语调，并区分了不同民粹主义变体的情感差异。


<details>
  <summary>Details</summary>
Motivation: 探索民粹主义在政治演讲中的语言表现，特别是其情感和风格特征，以揭示其如何通过语言塑造领导形象和与民众的联系。

Method: 结合LIWC特征和RoBERTa模型，分析美国总统就职演说和国情咨文中的语言标记，聚焦四种民粹主义维度（左翼、右翼、反精英主义和人民中心主义）。

Result: 民粹主义演讲具有直接、自信的语调，右翼和人民中心主义的情感更强烈，而左翼和反精英主义的情感较为克制。

Conclusion: 民粹主义语言策略性地调整语调，以塑造领导形象和民众联系，不同变体在情感表达上有显著差异。

Abstract: This study explores the sound of populism by integrating the classic
Linguistic Inquiry and Word Count (LIWC) features, which capture the emotional
and stylistic tones of language, with a fine-tuned RoBERTa model, a
state-of-the-art context-aware language model trained to detect nuanced
expressions of populism. This approach allows us to uncover the auditory
dimensions of political rhetoric in U.S. presidential inaugural and State of
the Union addresses. We examine how four key populist dimensions (i.e.,
left-wing, right-wing, anti-elitism, and people-centrism) manifest in the
linguistic markers of speech, drawing attention to both commonalities and
distinct tonal shifts across these variants. Our findings reveal that populist
rhetoric consistently features a direct, assertive ``sound" that forges a
connection with ``the people'' and constructs a charismatic leadership persona.
However, this sound is not simply informal but strategically calibrated.
Notably, right-wing populism and people-centrism exhibit a more emotionally
charged discourse, resonating with themes of identity, grievance, and crisis,
in contrast to the relatively restrained emotional tones of left-wing and
anti-elitist expressions.

</details>


### [15] [Recovering Event Probabilities from Large Language Model Embeddings via Axiomatic Constraints](https://arxiv.org/abs/2505.07883)
*Jian-Qiao Zhu,Haijiang Yan,Thomas L. Griffiths*

Main category: cs.CL

TL;DR: 通过变分自编码器（VAE）在LLM嵌入空间中强制概率理论公理约束，恢复更一致的事件概率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）生成的事件概率存在不一致性，违反概率理论公理，因此探索是否能从嵌入中恢复一致的概率。

Method: 使用扩展的变分自编码器（VAE）在LLM嵌入空间中强制概率公理（如加法规则），使概率在潜在空间中自然生成。

Result: 实验表明，从嵌入中恢复的概率比模型直接报告的概率更一致，且更接近真实概率。

Conclusion: 通过VAE在嵌入空间中强制概率公理，可以有效恢复一致且准确的事件概率。

Abstract: Rational decision-making under uncertainty requires coherent degrees of
belief in events. However, event probabilities generated by Large Language
Models (LLMs) have been shown to exhibit incoherence, violating the axioms of
probability theory. This raises the question of whether coherent event
probabilities can be recovered from the embeddings used by the models. If so,
those derived probabilities could be used as more accurate estimates in events
involving uncertainty. To explore this question, we propose enforcing axiomatic
constraints, such as the additive rule of probability theory, in the latent
space learned by an extended variational autoencoder (VAE) applied to LLM
embeddings. This approach enables event probabilities to naturally emerge in
the latent space as the VAE learns to both reconstruct the original embeddings
and predict the embeddings of semantically related events. We evaluate our
method on complementary events (i.e., event A and its complement, event not-A),
where the true probabilities of the two events must sum to 1. Experiment
results on open-weight language models demonstrate that probabilities recovered
from embeddings exhibit greater coherence than those directly reported by the
corresponding models and align closely with the true probabilities.

</details>


### [16] [Development of a WAZOBIA-Named Entity Recognition System](https://arxiv.org/abs/2505.07884)
*S. E Emedem,I. E Onyenwe,E. G Onyedinma*

Main category: cs.CL

TL;DR: 该研究开发了针对尼日利亚三大语言（豪萨语、约鲁巴语和伊博语）的WAZOBIA-NER系统，填补了低资源语言在命名实体识别（NER）领域的空白。


<details>
  <summary>Details</summary>
Motivation: 尽管非洲语言在计算语言学中受到关注，但现有NER系统主要针对英语和欧洲语言，非洲语言资源匮乏。

Method: 研究通过标注数据集，结合CRF、BiLSTM、BERT和RNN等机器学习与深度学习模型，并利用OCR技术处理文本图像。

Result: 系统在精确率、召回率、F1分数和准确率上表现优异，分别为0.9511、0.9400、0.9564和0.9301。

Conclusion: 研究表明，利用当前NLP框架和迁移学习，可以为低资源非洲语言构建强大的NER工具。

Abstract: Named Entity Recognition NER is very crucial for various natural language
processing applications, including information extraction, machine translation,
and sentiment analysis. Despite the ever-increasing interest in African
languages within computational linguistics, existing NER systems focus mainly
on English, European, and a few other global languages, leaving a significant
gap for under-resourced languages. This research presents the development of a
WAZOBIA-NER system tailored for the three most prominent Nigerian languages:
Hausa, Yoruba, and Igbo. This research begins with a comprehensive compilation
of annotated datasets for each language, addressing data scarcity and
linguistic diversity challenges. Exploring the state-of-the-art machine
learning technique, Conditional Random Fields (CRF) and deep learning models
such as Bidirectional Long Short-Term Memory (BiLSTM), Bidirectional Encoder
Representation from Transformers (Bert) and fine-tune with a Recurrent Neural
Network (RNN), the study evaluates the effectiveness of these approaches in
recognizing three entities: persons, organizations, and locations. The system
utilizes optical character recognition (OCR) technology to convert textual
images into machine-readable text, thereby enabling the Wazobia system to
accept both input text and textual images for extraction purposes. The system
achieved a performance of 0.9511 in precision, 0.9400 in recall, 0.9564 in
F1-score, and 0.9301 in accuracy. The model's evaluation was conducted across
three languages, with precision, recall, F1-score, and accuracy as key
assessment metrics. The Wazobia-NER system demonstrates that it is feasible to
build robust NER tools for under-resourced African languages using current NLP
frameworks and transfer learning.

</details>


### [17] [PLHF: Prompt Optimization with Few-Shot Human Feedback](https://arxiv.org/abs/2505.07886)
*Chun-Pai Yang,Kan Zheng,Shou-De Lin*

Main category: cs.CL

TL;DR: PLHF是一个基于人类反馈的少样本提示优化框架，通过特定评估模块解决无明确指标时的提示优化问题，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以处理输出质量无法通过标准样本评估的任务，缺乏明确指标时提示优化成为挑战。

Method: PLHF采用类似RLHF的技术，通过评估模块估计输出质量，仅需单轮人类反馈完成优化。

Result: 在公共和工业数据集上，PLHF优于先前的输出评分策略。

Conclusion: PLHF为无明确指标的提示优化提供了高效解决方案，具有实际应用潜力。

Abstract: Automatic prompt optimization frameworks are developed to obtain suitable
prompts for large language models (LLMs) with respect to desired output quality
metrics. Although existing approaches can handle conventional tasks such as
fixed-solution question answering, defining the metric becomes complicated when
the output quality cannot be easily assessed by comparisons with standard
golden samples. Consequently, optimizing the prompts effectively and
efficiently without a clear metric becomes a critical challenge. To address the
issue, we present PLHF (which stands for "P"rompt "L"earning with "H"uman
"F"eedback), a few-shot prompt optimization framework inspired by the
well-known RLHF technique. Different from naive strategies, PLHF employs a
specific evaluator module acting as the metric to estimate the output quality.
PLHF requires only a single round of human feedback to complete the entire
prompt optimization process. Empirical results on both public and industrial
datasets show that PLHF outperforms prior output grading strategies for LLM
prompt optimizations.

</details>


### [18] [Implementing Long Text Style Transfer with LLMs through Dual-Layered Sentence and Paragraph Structure Extraction and Mapping](https://arxiv.org/abs/2505.07888)
*Yusen Wu,Xiaotie Deng*

Main category: cs.CL

TL;DR: 论文提出了一种基于零样本学习的分层框架ZeroStylus，用于长文本风格迁移，结合句子级风格适应和段落级结构连贯性，显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 解决长文本风格迁移中保持句法和语义一致性的挑战，强调段落级语义和结构连贯的重要性。

Method: ZeroStylus框架通过分层模板获取和模板引导生成，动态构建句子和段落模板库，实现上下文感知的转换。

Result: 实验表明，该方法在风格一致性、内容保留和表达质量上优于基线方法（6.90 vs. 6.70），并通过消融研究验证了分层模板的必要性。

Conclusion: ZeroStylus为无需平行语料或LLM微调的长文本风格迁移提供了新能力。

Abstract: This paper addresses the challenge in long-text style transfer using
zero-shot learning of large language models (LLMs), proposing a hierarchical
framework that combines sentence-level stylistic adaptation with
paragraph-level structural coherence. We argue that in the process of effective
paragraph-style transfer, to preserve the consistency of original syntactic and
semantic information, it is essential to perform style transfer not only at the
sentence level but also to incorporate paragraph-level semantic considerations,
while ensuring structural coherence across inter-sentential relationships. Our
proposed framework, ZeroStylus, operates through two systematic phases:
hierarchical template acquisition from reference texts and template-guided
generation with multi-granular matching. The framework dynamically constructs
sentence and paragraph template repositories, enabling context-aware
transformations while preserving inter-sentence logical relationships.
Experimental evaluations demonstrate significant improvements over baseline
methods, with structured rewriting achieving 6.90 average score compared to
6.70 for direct prompting approaches in tri-axial metrics assessing style
consistency, content preservation, and expression quality. Ablation studies
validate the necessity of both template hierarchies during style transfer,
showing higher content preservation win rate against sentence-only approaches
through paragraph-level structural encoding, as well as direct prompting method
through sentence-level pattern extraction and matching. The results establish
new capabilities for coherent long-text style transfer without requiring
parallel corpora or LLM fine-tuning.

</details>


### [19] [BioProBench: Comprehensive Dataset and Benchmark in Biological Protocol Understanding and Reasoning](https://arxiv.org/abs/2505.07889)
*Yuyang Liu,Liuzhenghao Lv,Xiancheng Zhang,Li Yuan,Yonghong Tian*

Main category: cs.CL

TL;DR: BioProBench是一个大规模、多任务的生物协议理解和推理基准，包含五个核心任务，评估了12种主流LLM的表现，发现其在深度推理和结构化生成任务上表现不佳。


<details>
  <summary>Details</summary>
Motivation: 生物协议对生命科学研究至关重要，但LLM在这类高度专业化、准确性要求高的文本上的系统性评估有限。

Method: 基于27K原始协议构建了BioProBench，包含五个核心任务，并评估了12种LLM的表现。

Result: 实验显示，LLM在表面理解任务上表现良好，但在深度推理和结构化生成任务上表现较差。开源模型在某些任务上接近闭源模型，但生物专用小模型表现不佳。

Conclusion: 生物协议中的程序推理对当前LLM仍具挑战性，BioProBench为诊断这些限制提供了标准化框架。

Abstract: Biological protocols are fundamental to reproducible and safe life science
research. While LLMs excel on general tasks, their systematic evaluation on
these highly specialized, accuracy-critical, and inherently procedural texts
remains limited. In this work, we present BioProBench, the first large-scale,
integrated multi-task benchmark for biological protocol understanding and
reasoning. While limited benchmarks have touched upon specific aspects like
protocol QA, BioProBench provides a comprehensive suite of five core tasks:
Protocol Question Answering, Step Ordering, Error Correction, Protocol
Generation, and Protocol Reasoning, enabling a holistic evaluation of LLMs on
procedural biological texts. Built upon 27K original protocols, it yields
nearly 556K high-quality structured instances. We evaluate 12 mainstream
open/closed-source LLMs on BioProBench. Experimental results reveal that while
top models preform well on surface understanding tasks, struggle significantly
with deep reasoning and structured generation tasks like ordering and
generation. Furthermore, model comparisons reveal diverse performance: certain
open-source models approach closed-source levels on some tasks, yet
bio-specific small models lag behind general LLMs, indicating limitations on
complex procedural content. Overall, our findings underscore that procedural
reasoning within biological protocols represents a significant challenge for
current LLMs. BioProBench serves as a standardized framework to diagnose these
specific limitations and guide the development of AI systems better equipped
for safely automating complex scientific procedures. The code and data are
available at: https://github.com/YuyangSunshine/bioprotocolbench and
https://huggingface.co/datasets/GreatCaptainNemo/BioProBench.

</details>


### [20] [TSLFormer: A Lightweight Transformer Model for Turkish Sign Language Recognition Using Skeletal Landmarks](https://arxiv.org/abs/2505.07890)
*Kutay Ertürk,Furkan Altınışık,İrem Sarıaltın,Ömer Nezih Gerek*

Main category: cs.CL

TL;DR: TSLFormer是一种轻量级且鲁棒的土耳其手语识别模型，将手势视为有序的字符串语言，仅使用3D关节位置作为输入，通过序列到序列的翻译方法实现高效识别。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索一种高效且轻量的手语识别方法，利用3D关节位置减少输入维度，同时保留手势的语义信息，以支持实时、移动和辅助通信系统。

Method: 方法包括使用Google的Mediapipe库提取手部和躯干的3D关节位置作为输入，采用基于自注意力机制的Transformer模型，将手语识别视为序列到序列的翻译任务。

Result: 在AUTSL数据集（36,000样本，227个单词）上，TSLFormer表现出竞争性性能，计算成本低。

Conclusion: 结论表明，基于关节的输入足以支持实时、移动和辅助通信系统，为听力障碍者提供有效帮助。

Abstract: This study presents TSLFormer, a light and robust word-level Turkish Sign
Language (TSL) recognition model that treats sign gestures as ordered,
string-like language. Instead of using raw RGB or depth videos, our method only
works with 3D joint positions - articulation points - extracted using Google's
Mediapipe library, which focuses on the hand and torso skeletal locations. This
creates efficient input dimensionality reduction while preserving important
semantic gesture information.
  Our approach revisits sign language recognition as sequence-to-sequence
translation, inspired by the linguistic nature of sign languages and the
success of transformers in natural language processing. Since TSLFormer uses
the self-attention mechanism, it effectively captures temporal co-occurrence
within gesture sequences and highlights meaningful motion patterns as words
unfold.
  Evaluated on the AUTSL dataset with over 36,000 samples and 227 different
words, TSLFormer achieves competitive performance with minimal computational
cost. These results show that joint-based input is sufficient for enabling
real-time, mobile, and assistive communication systems for hearing-impaired
individuals.

</details>


### [21] [TrumorGPT: Graph-Based Retrieval-Augmented Large Language Model for Fact-Checking](https://arxiv.org/abs/2505.07891)
*Ching Nam Hang,Pei-Duo Yu,Chee Wei Tan*

Main category: cs.CL

TL;DR: TrumorGPT是一种基于生成式人工智能的健康领域事实核查工具，旨在区分真实的健康谣言（trumors），利用大型语言模型和语义健康知识图谱提升准确性。


<details>
  <summary>Details</summary>
Motivation: 社交媒体时代，虚假信息的快速传播对社会构成威胁，尤其是在健康领域，需要一种有效的事实核查工具。

Method: 结合大型语言模型（LLM）和少样本学习，构建语义健康知识图谱，并采用图检索增强生成（GraphRAG）技术，解决LLM的幻觉问题和静态数据的局限性。

Result: 在广泛的医疗数据集上评估，TrumorGPT在公共卫生声明的事实核查中表现出色。

Conclusion: TrumorGPT为对抗健康相关虚假信息提供了重要工具，提升了数字信息时代的信任和准确性。

Abstract: In the age of social media, the rapid spread of misinformation and rumors has
led to the emergence of infodemics, where false information poses a significant
threat to society. To combat this issue, we introduce TrumorGPT , a novel
generative artificial intelligence solution designed for fact-checking in the
health domain. TrumorGPT aims to distinguish "trumors", which are
health-related rumors that turn out to be true, providing a crucial tool in
differentiating between mere speculation and verified facts. This framework
leverages a large language model (LLM) with few-shot learning for semantic
health knowledge graph construction and semantic reasoning. TrumorGPT
incorporates graph-based retrieval-augmented generation (GraphRAG) to address
the hallucination issue common in LLMs and the limitations of static training
data. GraphRAG involves accessing and utilizing information from regularly
updated semantic health knowledge graphs that consist of the latest medical
news and health information, ensuring that fact-checking by TrumorGPT is based
on the most recent data. Evaluating with extensive healthcare datasets,
TrumorGPT demonstrates superior performance in fact-checking for public health
claims. Its ability to effectively conduct fact-checking across various
platforms marks a critical step forward in the fight against health-related
misinformation, enhancing trust and accuracy in the digital information age.

</details>


### [22] [LongCodeBench: Evaluating Coding LLMs at 1M Context Windows](https://arxiv.org/abs/2505.07897)
*Stefano Rando,Luca Romani,Alessio Sampieri,Yuta Kyuragi,Luca Franco,Fabio Galasso,Tatsunori Hashimoto,John Yang*

Main category: cs.CL

TL;DR: 论文介绍了LongCodeBench（LCB），一个用于测试长上下文模型中代码理解和修复能力的基准测试，发现所有模型在长上下文任务中表现均下降。


<details>
  <summary>Details</summary>
Motivation: 现代长上下文模型的上下文长度快速增长，但缺乏现实的长上下文基准测试，代码理解和修复成为自然测试场景。

Method: 通过从真实GitHub问题中构建QA和bug修复任务，创建分层复杂度的基准测试LCB，评估不同规模模型。

Result: 所有模型在长上下文任务中表现下降，如Claude 3.5 Sonnet从29%降至3%，Qwen2.5从70.2%降至40%。

Conclusion: 长上下文仍是模型的弱点，LCB为评估和改进长上下文能力提供了有效工具。

Abstract: Context lengths for models have grown rapidly, from thousands to millions of
tokens in just a few years. The extreme context sizes of modern long-context
models have made it difficult to construct realistic long-context benchmarks --
not only due to the cost of collecting million-context tasks but also in
identifying realistic scenarios that require significant contexts. We identify
code comprehension and repair as a natural testbed and challenge task for
long-context models and introduce LongCodeBench (LCB), a benchmark to test LLM
coding abilities in long-context scenarios. Our benchmark tests both the
comprehension and repair capabilities of LCLMs in realistic and important
settings by drawing from real-world GitHub issues and constructing QA
(LongCodeQA) and bug fixing (LongSWE-Bench) tasks. We carefully stratify the
complexity of our benchmark, enabling us to evaluate models across different
scales -- ranging from Qwen2.5 14B Instruct to Google's flagship Gemini model.
We find that long-context remains a weakness for all models, with performance
drops such as from 29% to 3% for Claude 3.5 Sonnet, or from 70.2% to 40% for
Qwen2.5.

</details>


### [23] [DeltaEdit: Enhancing Sequential Editing in Large Language Models by Controlling Superimposed Noise](https://arxiv.org/abs/2505.07899)
*Ding Cao,Yuchen Cai,Rongxi Guo,Xuesong He,Guiquan Liu*

Main category: cs.CL

TL;DR: DeltaEdit通过动态正交约束策略优化更新参数，显著提升长期编辑成功率，解决叠加噪声积累问题。


<details>
  <summary>Details</summary>
Motivation: 现有连续知识编辑方法在长期编辑后成功率显著下降，模型输出偏离目标，导致叠加噪声积累问题。

Method: 提出DeltaEdit方法，采用动态正交约束策略优化更新参数，减少编辑间干扰。

Result: 实验表明DeltaEdit在编辑成功率和泛化能力保留上优于现有方法。

Conclusion: DeltaEdit能确保模型在长期连续编辑下保持稳定可靠的性能。

Abstract: Sequential knowledge editing techniques aim to continuously update the
knowledge in large language models at a low cost, preventing the models from
generating outdated or incorrect information. However, existing sequential
editing methods suffer from a significant decline in editing success rates
after long-term editing. Through theoretical analysis and experiments, we
identify that as the number of edits increases, the model's output increasingly
deviates from the desired target, leading to a drop in editing success rates.
We refer to this issue as the accumulation of superimposed noise problem. To
address this, we identify the factors contributing to this deviation and
propose DeltaEdit, a novel method that optimizes update parameters through a
dynamic orthogonal constraints strategy, effectively reducing interference
between edits to mitigate deviation. Experimental results demonstrate that
DeltaEdit significantly outperforms existing methods in edit success rates and
the retention of generalization capabilities, ensuring stable and reliable
model performance even under extensive sequential editing.

</details>


### [24] [SEM: Reinforcement Learning for Search-Efficient Large Language Models](https://arxiv.org/abs/2505.07903)
*Zeyang Sha,Shiwen Cui,Weiqiang Wang*

Main category: cs.CL

TL;DR: 论文提出了一种名为SEM的后训练强化学习框架，旨在优化大型语言模型（LLMs）在调用搜索引擎时的行为，减少冗余搜索并提高效率。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs在调用外部工具（如搜索引擎）时，难以区分何时需要搜索、何时可以依赖内部知识，导致冗余搜索行为和成本增加。

Method: 通过结合MuSiQue和MMLU数据集构建平衡场景，设计结构化推理模板，并使用Group Relative Policy Optimization（GRPO）后训练模型，优化搜索行为。

Result: 实验表明，该方法显著减少了冗余搜索操作，同时在多个基准测试中保持或提高了答案准确性。

Conclusion: SEM框架提升了模型的推理效率，并使其能够更明智地利用外部知识。

Abstract: Recent advancements in Large Language Models(LLMs) have demonstrated their
capabilities not only in reasoning but also in invoking external tools,
particularly search engines. However, teaching models to discern when to invoke
search and when to rely on their internal knowledge remains a significant
challenge. Existing reinforcement learning approaches often lead to redundant
search behaviors, resulting in inefficiencies and over-cost. In this paper, we
propose SEM, a novel post-training reinforcement learning framework that
explicitly trains LLMs to optimize search usage. By constructing a balanced
dataset combining MuSiQue and MMLU, we create scenarios where the model must
learn to distinguish between questions it can answer directly and those
requiring external retrieval. We design a structured reasoning template and
employ Group Relative Policy Optimization(GRPO) to post-train the model's
search behaviors. Our reward function encourages accurate answering without
unnecessary search while promoting effective retrieval when needed.
Experimental results demonstrate that our method significantly reduces
redundant search operations while maintaining or improving answer accuracy
across multiple challenging benchmarks. This framework advances the model's
reasoning efficiency and extends its capability to judiciously leverage
external knowledge.

</details>


### [25] [Re$^2$: A Consistency-ensured Dataset for Full-stage Peer Review and Multi-turn Rebuttal Discussions](https://arxiv.org/abs/2505.07920)
*Daoze Zhang,Zhijian Bao,Sihang Du,Zhiyi Zhao,Kuangling Zhang,Dezheng Bao,Yang Yang*

Main category: cs.CL

TL;DR: 论文提出了一个名为Re^2的大规模一致性保障的同行评审和反驳数据集，旨在解决现有同行评审数据的局限性，并支持动态交互式LLM助手。


<details>
  <summary>Details</summary>
Motivation: 同行评审是科学进步的关键环节，但提交量激增导致评审系统压力大、评审质量下降。现有数据集存在多样性不足、数据质量低等问题，限制了LLM在评审中的应用。

Method: 构建了Re^2数据集，包含19,926份初始提交、70,668条评审意见和53,818条反驳，覆盖24个会议和21个研讨会，并将反驳阶段建模为多轮对话范式。

Result: Re^2数据集支持传统静态评审任务和动态交互式LLM助手，为作者提供更实用的指导，帮助减轻评审负担。

Conclusion: Re^2数据集通过解决现有数据的局限性，为同行评审和LLM应用提供了更高质量的支持，有望缓解评审系统的压力。

Abstract: Peer review is a critical component of scientific progress in the fields like
AI, but the rapid increase in submission volume has strained the reviewing
system, which inevitably leads to reviewer shortages and declines review
quality. Besides the growing research popularity, another key factor in this
overload is the repeated resubmission of substandard manuscripts, largely due
to the lack of effective tools for authors to self-evaluate their work before
submission. Large Language Models (LLMs) show great promise in assisting both
authors and reviewers, and their performance is fundamentally limited by the
quality of the peer review data. However, existing peer review datasets face
three major limitations: (1) limited data diversity, (2) inconsistent and
low-quality data due to the use of revised rather than initial submissions, and
(3) insufficient support for tasks involving rebuttal and reviewer-author
interactions. To address these challenges, we introduce the largest
consistency-ensured peer review and rebuttal dataset named Re^2, which
comprises 19,926 initial submissions, 70,668 review comments, and 53,818
rebuttals from 24 conferences and 21 workshops on OpenReview. Moreover, the
rebuttal and discussion stage is framed as a multi-turn conversation paradigm
to support both traditional static review tasks and dynamic interactive LLM
assistants, providing more practical guidance for authors to refine their
manuscripts and helping alleviate the growing review burden. Our data and code
are available in https://anonymous.4open.science/r/ReviewBench_anon/.

</details>


### [26] [Assessing and Mitigating Medical Knowledge Drift and Conflicts in Large Language Models](https://arxiv.org/abs/2505.07968)
*Weiyi Wu,Xinwen Xu,Chongyang Gao,Xingjian Diao,Siting Li,Lucas A. Salas,Jiang Gui*

Main category: cs.CL

TL;DR: 该研究探讨了大型语言模型（LLMs）在适应快速变化的医学知识时的挑战，开发了DriftMedQA基准测试，评估了七种先进模型的表现，并提出了两种改进策略。


<details>
  <summary>Details</summary>
Motivation: LLMs在医疗领域潜力巨大，但面临适应快速变化的医学知识的挑战，可能导致过时或矛盾的治疗建议。

Method: 研究开发了DriftMedQA基准测试，模拟指南演变，评估了七种LLMs的时间可靠性，并探索了检索增强生成和偏好微调两种改进策略。

Result: 模型在拒绝过时建议和避免矛盾指导方面表现不佳，但两种改进策略结合使用效果最佳。

Conclusion: 研究表明需要提高LLMs对时间变化的鲁棒性，以确保其在临床实践中的可靠性。

Abstract: Large Language Models (LLMs) have great potential in the field of health
care, yet they face great challenges in adapting to rapidly evolving medical
knowledge. This can lead to outdated or contradictory treatment suggestions.
This study investigated how LLMs respond to evolving clinical guidelines,
focusing on concept drift and internal inconsistencies. We developed the
DriftMedQA benchmark to simulate guideline evolution and assessed the temporal
reliability of various LLMs. Our evaluation of seven state-of-the-art models
across 4,290 scenarios demonstrated difficulties in rejecting outdated
recommendations and frequently endorsing conflicting guidance. Additionally, we
explored two mitigation strategies: Retrieval-Augmented Generation and
preference fine-tuning via Direct Preference Optimization. While each method
improved model performance, their combination led to the most consistent and
reliable results. These findings underscore the need to improve LLM robustness
to temporal shifts to ensure more dependable applications in clinical practice.

</details>


### [27] [Task-Adaptive Semantic Communications with Controllable Diffusion-based Data Regeneration](https://arxiv.org/abs/2505.07980)
*Fupei Guo,Achintha Wijesinghe,Songyang Zhang,Zhi Ding*

Main category: cs.CL

TL;DR: 论文提出了一种基于扩散模型的任务自适应语义通信框架，动态调整语义信息传递以适应不同下游任务。


<details>
  <summary>Details</summary>
Motivation: 下一代网络需要从比特级数据传输转向语义传递以提高带宽效率，同时需适应接收端多样化的任务需求。

Method: 通过扩散模型动态调整语义信息传递，先传输深度压缩的通用语义表示，接收端生成任务需求反馈，发送端更新传输以对齐目标。

Result: 测试结果表明，该方法能自适应保留任务关键信息，同时保持高压缩效率。

Conclusion: 该框架为语义通信提供了一种高效的任务自适应解决方案。

Abstract: Semantic communications represent a new paradigm of next-generation
networking that shifts bit-wise data delivery to conveying the semantic
meanings for bandwidth efficiency. To effectively accommodate various potential
downstream tasks at the receiver side, one should adaptively convey the most
critical semantic information. This work presents a novel task-adaptive
semantic communication framework based on diffusion models that is capable of
dynamically adjusting the semantic message delivery according to various
downstream tasks. Specifically, we initialize the transmission of a
deep-compressed general semantic representation from the transmitter to enable
diffusion-based coarse data reconstruction at the receiver. The receiver
identifies the task-specific demands and generates textual prompts as feedback.
Integrated with the attention mechanism, the transmitter updates the semantic
transmission with more details to better align with the objectives of the
intended receivers. Our test results demonstrate the efficacy of the proposed
method in adaptively preserving critical task-relevant information for semantic
communications while preserving high compression efficiency.

</details>


### [28] [Large Language Models and Arabic Content: A Review](https://arxiv.org/abs/2505.08004)
*Haneh Rhel,Dmitri Roussinov*

Main category: cs.CL

TL;DR: 本文总结了大型语言模型（LLMs）在阿拉伯语自然语言处理（NLP）中的应用，包括其挑战、现有模型、性能提升方法及未来趋势。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯语资源稀缺且语言复杂，LLMs在多语言语料库训练中表现出色，但阿拉伯语NLP任务仍需更多研究。

Method: 综述了预训练阿拉伯语LLMs及其在NLP任务中的应用，探讨了微调和提示工程等技术对性能的提升。

Result: LLMs在阿拉伯语NLP任务中表现优异，且采用率持续上升。

Conclusion: LLMs为阿拉伯语NLP提供了有效解决方案，未来需更多资源和研究支持。

Abstract: Over the past three years, the rapid advancement of Large Language Models
(LLMs) has had a profound impact on multiple areas of Artificial Intelligence
(AI), particularly in Natural Language Processing (NLP) across diverse
languages, including Arabic. Although Arabic is considered one of the most
widely spoken languages across 27 countries in the Arabic world and used as a
second language in some other non-Arabic countries as well, there is still a
scarcity of Arabic resources, datasets, and tools. Arabic NLP tasks face
various challenges due to the complexities of the Arabic language, including
its rich morphology, intricate structure, and diverse writing standards, among
other factors. Researchers have been actively addressing these challenges,
demonstrating that pre-trained Large Language Models (LLMs) trained on
multilingual corpora achieve significant success in various Arabic NLP tasks.
This study provides an overview of using large language models (LLMs) for the
Arabic language, highlighting early pre-trained Arabic Language models across
various NLP applications and their ability to handle diverse Arabic content
tasks and dialects. It also provides an overview of how techniques like
finetuning and prompt engineering can enhance the performance of these models.
Additionally, the study summarizes common Arabic benchmarks and datasets while
presenting our observations on the persistent upward trend in the adoption of
LLMs.

</details>


### [29] [TiSpell: A Semi-Masked Methodology for Tibetan Spelling Correction covering Multi-Level Error with Data Augmentation](https://arxiv.org/abs/2505.08037)
*Yutong Liu,Feng Xiao,Ziyue Zhang,Yongbin Yu,Cheng Huang,Fan Gao,Xiangxiang Wang,Ma-bao Ban,Manping Fan,Thupten Tsering,Cheng Huang,Gadeng Luosang,Renzeng Duojie,Nyima Tashi*

Main category: cs.CL

TL;DR: 提出了一种多级藏文拼写校正方法TiSpell，结合数据增强和半掩码模型，有效校正字符和音节级错误。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注单级校正，缺乏对字符和音节级错误的有效整合，且缺乏针对藏文的开源数据集和增强方法。

Method: 使用未标记文本生成多级错误数据，提出半掩码模型TiSpell，支持字符和音节级校正。

Result: 在模拟和真实数据上，TiSpell优于基线模型，性能接近最先进方法。

Conclusion: TiSpell在多级藏文拼写校正任务中表现优异，验证了其有效性。

Abstract: Multi-level Tibetan spelling correction addresses errors at both the
character and syllable levels within a unified model. Existing methods focus
mainly on single-level correction and lack effective integration of both
levels. Moreover, there are no open-source datasets or augmentation methods
tailored for this task in Tibetan. To tackle this, we propose a data
augmentation approach using unlabeled text to generate multi-level corruptions,
and introduce TiSpell, a semi-masked model capable of correcting both
character- and syllable-level errors. Although syllable-level correction is
more challenging due to its reliance on global context, our semi-masked
strategy simplifies this process. We synthesize nine types of corruptions on
clean sentences to create a robust training set. Experiments on both simulated
and real-world data demonstrate that TiSpell, trained on our dataset,
outperforms baseline models and matches the performance of state-of-the-art
approaches, confirming its effectiveness.

</details>


### [30] [FalseReject: A Resource for Improving Contextual Safety and Mitigating Over-Refusals in LLMs via Structured Reasoning](https://arxiv.org/abs/2505.08054)
*Zhehao Zhang,Weijie Xu,Fanyou Wu,Chandan K. Reddy*

Main category: cs.CL

TL;DR: 论文提出FalseReject资源，用于解决LLMs因安全对齐导致的过度拒绝良性查询问题，通过多智能体交互生成多样化提示，并通过监督微调显著减少不必要的拒绝。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的安全对齐常导致对良性查询的过度拒绝，降低其在敏感场景的实用性。

Method: 提出FalseReject资源，包含16k看似有毒的查询和结构化响应；采用图引导的多智能体对抗交互框架生成多样化提示，并通过监督微调优化模型。

Result: 在29个SOTA LLMs上的实验表明，FalseReject显著减少了不必要的拒绝，同时保持安全性和语言能力。

Conclusion: FalseReject有效解决了LLMs的过度拒绝问题，提升了模型在敏感场景的实用性。

Abstract: Safety alignment approaches in large language models (LLMs) often lead to the
over-refusal of benign queries, significantly diminishing their utility in
sensitive scenarios. To address this challenge, we introduce FalseReject, a
comprehensive resource containing 16k seemingly toxic queries accompanied by
structured responses across 44 safety-related categories. We propose a
graph-informed adversarial multi-agent interaction framework to generate
diverse and complex prompts, while structuring responses with explicit
reasoning to aid models in accurately distinguishing safe from unsafe contexts.
FalseReject includes training datasets tailored for both standard
instruction-tuned models and reasoning-oriented models, as well as a
human-annotated benchmark test set. Our extensive benchmarking on 29
state-of-the-art (SOTA) LLMs reveals persistent over-refusal challenges.
Empirical results demonstrate that supervised finetuning with FalseReject
substantially reduces unnecessary refusals without compromising overall safety
or general language capabilities.

</details>


### [31] [HYPERNYM MERCURY: Token Optimization through Semantic Field Constriction and Reconstruction from Hypernyms. A New Text Compression Method](https://arxiv.org/abs/2505.08058)
*Chris Forrester,Octavia Sulea*

Main category: cs.CL

TL;DR: 论文提出了一种新型文本表示方法和语义压缩技术，能减少90%以上的token，同时保持高语义相似度。


<details>
  <summary>Details</summary>
Motivation: 在NLP和下一代智能AI中，计算优化是一个重要任务，尤其是减少LLM提示的token数量。

Method: 采用专利待审的文本表示方案和段落级语义压缩技术，支持无损压缩和粒度控制。

Result: 在开源数据（如《德古拉》）上验证了段落级别的有效性，结果跨多种类型和模型一致。

Conclusion: 该技术能显著减少token数量，同时保持语义完整性，适用于多种场景。

Abstract: Compute optimization using token reduction of LLM prompts is an emerging task
in the fields of NLP and next generation, agentic AI. In this white paper, we
introduce a novel (patent pending) text representation scheme and a
first-of-its-kind word-level semantic compression of paragraphs that can lead
to over 90\% token reduction, while retaining high semantic similarity to the
source text. We explain how this novel compression technique can be lossless
and how the detail granularity is controllable. We discuss benchmark results
over open source data (i.e. Bram Stoker's Dracula available through Project
Gutenberg) and show how our results hold at the paragraph level, across
multiple genres and models.

</details>


### [32] [Are LLMs complicated ethical dilemma analyzers?](https://arxiv.org/abs/2505.08106)
*Jiashen,Du,Jesse Yao,Allen Liu,Zhekai Zhang*

Main category: cs.CL

TL;DR: 论文研究了大型语言模型（LLMs）是否能模拟人类伦理推理，并通过基准数据集和复合指标评估了多个前沿模型的表现。结果显示LLMs在词汇和结构对齐上优于非专家人类，但在历史背景和复杂策略上仍有不足。


<details>
  <summary>Details</summary>
Motivation: 探讨LLMs是否能够模拟人类伦理推理并作为人类判断的可信代理。

Method: 引入包含196个真实伦理困境的基准数据集，通过复合指标（BLEU、Damerau-Levenshtein距离等）评估多个LLMs的表现。

Result: LLMs在词汇和结构对齐上优于非专家人类，但在历史背景和复杂策略上表现不佳。GPT-4o-mini表现最稳定。

Conclusion: LLMs在伦理决策中具有优势，但在某些复杂领域仍需改进。

Abstract: One open question in the study of Large Language Models (LLMs) is whether
they can emulate human ethical reasoning and act as believable proxies for
human judgment. To investigate this, we introduce a benchmark dataset
comprising 196 real-world ethical dilemmas and expert opinions, each segmented
into five structured components: Introduction, Key Factors, Historical
Theoretical Perspectives, Resolution Strategies, and Key Takeaways. We also
collect non-expert human responses for comparison, limited to the Key Factors
section due to their brevity. We evaluate multiple frontier LLMs (GPT-4o-mini,
Claude-3.5-Sonnet, Deepseek-V3, Gemini-1.5-Flash) using a composite metric
framework based on BLEU, Damerau-Levenshtein distance, TF-IDF cosine
similarity, and Universal Sentence Encoder similarity. Metric weights are
computed through an inversion-based ranking alignment and pairwise AHP
analysis, enabling fine-grained comparison of model outputs to expert
responses. Our results show that LLMs generally outperform non-expert humans in
lexical and structural alignment, with GPT-4o-mini performing most consistently
across all sections. However, all models struggle with historical grounding and
proposing nuanced resolution strategies, which require contextual abstraction.
Human responses, while less structured, occasionally achieve comparable
semantic similarity, suggesting intuitive moral reasoning. These findings
highlight both the strengths and current limitations of LLMs in ethical
decision-making.

</details>


### [33] [Putting It All into Context: Simplifying Agents with LCLMs](https://arxiv.org/abs/2505.08120)
*Mingjian Jiang,Yangjun Ruan,Luis Lastras,Pavan Kapanipathi,Tatsunori Hashimoto*

Main category: cs.CL

TL;DR: 研究表明，在复杂任务（如SWE-bench）中，无需复杂的语言模型代理架构，仅通过长上下文语言模型（LCLM）和适当提示即可取得竞争性结果。


<details>
  <summary>Details</summary>
Motivation: 探讨语言模型代理架构的复杂性是否必要，尤其是在SWE-bench等挑战性任务中。

Method: 使用长上下文语言模型（LCLM）和简单提示，无需复杂架构或工具。

Result: Gemini-1.5-Pro无架构支持达到38%解决率，与复杂架构（32%）相当；Gemini-2.5-Pro直接达到50.8%。

Conclusion: 部分任务中，简单方法足以替代复杂架构，但更强大模型能进一步提升性能。

Abstract: Recent advances in language model (LM) agents have demonstrated significant
potential for automating complex real-world tasks. To make progress on these
difficult tasks, LM agent architectures have become increasingly complex, often
incorporating multi-step retrieval tools, multiple agents, and scaffolding
adapted to the underlying LM. In this work, we investigate whether all of this
complexity is necessary, or if parts of these scaffolds can be removed on
challenging tasks like SWE-bench. We show that in the case of SWE-bench, simply
putting the entire environment into the context of a long context language
model (LCLM) and properly prompting the model makes it competitive with
carefully tuned, complex agent scaffolds. We show that a Gemini-1.5-Pro model
without any scaffolding or tools achieves 38% on SWE-Bench-Verified, comparable
with approaches using carefully tuned agent scaffolds (32%). While the
unscaffolded approach with Gemini-1.5-Pro falls short of the strongest agentic
architectures, we demonstrate that the more capable Gemini-2.5-Pro using the
same unscaffolded approach directly attains a 50.8% solve rate. Additionally, a
two-stage approach combining Gemini-1.5-Pro with Claude-3.7 achieves a
competitive 48.6% solve rate.

</details>


### [34] [ALOHA: Empowering Multilingual Agent for University Orientation with Hierarchical Retrieval](https://arxiv.org/abs/2505.08130)
*Mingxu Tao,Bowen Tang,Mingxuan Ma,Yining Zhang,Hourun Li,Feifan Wen,Hao Ma,Jia Yang*

Main category: cs.CL

TL;DR: ALOHA是一个多语言代理系统，通过分层检索增强，为大学校园信息提供高效、多语言和及时的查询服务。


<details>
  <summary>Details</summary>
Motivation: 现有公共LLM服务和搜索引擎无法满足校园特定信息的多语言和及时需求。

Method: 结合分层检索和外部API，构建多语言代理系统ALOHA。

Result: 系统在多语言查询中表现优于商业聊天机器人和搜索引擎，已服务超过12,000人。

Conclusion: ALOHA有效解决了校园信息检索的特定需求，展示了实际应用潜力。

Abstract: The rise of Large Language Models~(LLMs) revolutionizes information
retrieval, allowing users to obtain required answers through complex
instructions within conversations. However, publicly available services remain
inadequate in addressing the needs of faculty and students to search
campus-specific information. It is primarily due to the LLM's lack of
domain-specific knowledge and the limitation of search engines in supporting
multilingual and timely scenarios. To tackle these challenges, we introduce
ALOHA, a multilingual agent enhanced by hierarchical retrieval for university
orientation. We also integrate external APIs into the front-end interface to
provide interactive service. The human evaluation and case study show our
proposed system has strong capabilities to yield correct, timely, and
user-friendly responses to the queries in multiple languages, surpassing
commercial chatbots and search engines. The system has been deployed and has
provided service for more than 12,000 people.

</details>


### [35] [Fusing Bidirectional Chains of Thought and Reward Mechanisms A Method for Enhancing Question-Answering Capabilities of Large Language Models for Chinese Intangible Cultural Heritage](https://arxiv.org/abs/2505.08167)
*Ruilin Liu,Zhixiao Zhao,Jieqiong Li,Chang Liu,Dongbo Wang*

Main category: cs.CL

TL;DR: 本文提出了一种结合双向思维链和奖励机制的新训练方法，用于解决领域特定大语言模型在微调过程中面临的偏见、知识继承错误和灾难性遗忘问题。该方法在ICH-Qwen模型上表现优异，并在多领域实验中验证了其通用性。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在微调过程中因使用非物质文化遗产数据而面临的偏见、知识继承错误和灾难性遗忘问题。

Method: 提出了一种结合双向思维链（正向和反向推理）和奖励机制的训练方法，通过结构化和内容评估优化模型输出。

Result: 在ICH-Qwen模型上，该方法在问答任务中的准确性、Bleu-4和Rouge-L得分优于其他方法，并在多领域实验中表现出通用性。

Conclusion: 该方法不仅适用于非物质文化遗产领域，还能推广到其他领域，为未来多领域模型训练提供了有价值的参考。

Abstract: The rapid development of large language models (LLMs) has provided
significant support and opportunities for the advancement of domain-specific
LLMs. However, fine-tuning these large models using Intangible Cultural
Heritage (ICH) data inevitably faces challenges such as bias, incorrect
knowledge inheritance, and catastrophic forgetting. To address these issues, we
propose a novel training method that integrates a bidirectional chains of
thought and a reward mechanism. This method is built upon ICH-Qwen, a large
language model specifically designed for the field of intangible cultural
heritage. The proposed method enables the model to not only perform forward
reasoning but also enhances the accuracy of the generated answers by utilizing
reverse questioning and reverse reasoning to activate the model's latent
knowledge. Additionally, a reward mechanism is introduced during training to
optimize the decision-making process. This mechanism improves the quality of
the model's outputs through structural and content evaluations with different
weighting schemes. We conduct comparative experiments on ICH-Qwen, with results
demonstrating that our method outperforms 0-shot, step-by-step reasoning,
knowledge distillation, and question augmentation methods in terms of accuracy,
Bleu-4, and Rouge-L scores on the question-answering task. Furthermore, the
paper highlights the effectiveness of combining the bidirectional chains of
thought and reward mechanism through ablation experiments. In addition, a
series of generalizability experiments are conducted, with results showing that
the proposed method yields improvements on various domain-specific datasets and
advanced models in areas such as Finance, Wikidata, and StrategyQA. This
demonstrates that the method is adaptable to multiple domains and provides a
valuable approach for model training in future applications across diverse
fields.

</details>


### [36] [Exploiting Text Semantics for Few and Zero Shot Node Classification on Text-attributed Graph](https://arxiv.org/abs/2505.08168)
*Yuxiang Wang,Xiao Yan,Shiyu Jin,Quanqing Xu,Chuang Hu,Yuanyuan Zhu,Bo Du,Jia Wu,Jiawei Jiang*

Main category: cs.CL

TL;DR: 本文提出了一种名为TSA的文本语义增强方法，通过引入更多文本语义监督信号，提升了TAG（文本属性图）中少样本和零样本节点分类的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要基于图增强技术训练节点和文本嵌入，而文本增强技术尚未充分探索。本文旨在通过文本语义增强提升分类性能。

Method: 设计了两种文本语义增强技术：正语义匹配和负语义对比。正语义匹配检索相似文本嵌入以匹配图节点，负语义对比通过添加负面提示构造相反语义的文本描述。

Result: 在5个数据集上与13个基线方法对比，TSA始终优于所有基线，通常比最佳基线的准确率提升超过5%。

Conclusion: TSA通过文本语义增强显著提升了TAG中节点分类的准确性，证明了文本增强技术的有效性。

Abstract: Text-attributed graph (TAG) provides a text description for each graph node,
and few- and zero-shot node classification on TAGs have many applications in
fields such as academia and social networks. Existing work utilizes various
graph-based augmentation techniques to train the node and text embeddings,
while text-based augmentations are largely unexplored. In this paper, we
propose Text Semantics Augmentation (TSA) to improve accuracy by introducing
more text semantic supervision signals. Specifically, we design two
augmentation techniques, i.e., positive semantics matching and negative
semantics contrast, to provide more reference texts for each graph node or text
description. Positive semantic matching retrieves texts with similar embeddings
to match with a graph node. Negative semantic contrast adds a negative prompt
to construct a text description with the opposite semantics, which is
contrasted with the original node and text. We evaluate TSA on 5 datasets and
compare with 13 state-of-the-art baselines. The results show that TSA
consistently outperforms all baselines, and its accuracy improvements over the
best-performing baseline are usually over 5%.

</details>


### [37] [A Head to Predict and a Head to Question: Pre-trained Uncertainty Quantification Heads for Hallucination Detection in LLM Outputs](https://arxiv.org/abs/2505.08200)
*Artem Shelmanov,Ekaterina Fadeeva,Akim Tsvigun,Ivan Tsvigun,Zhuohan Xie,Igor Kiselev,Nico Daheim,Caiqi Zhang,Artem Vazhentsev,Mrinmaya Sachan,Preslav Nakov,Timothy Baldwin*

Main category: cs.CL

TL;DR: 论文提出了一种预训练的不确定性量化（UQ）头部模块，用于增强大型语言模型（LLMs）检测幻觉的能力，其性能优于无监督方法，并在多语言任务中表现出强泛化能力。


<details>
  <summary>Details</summary>
Motivation: LLMs容易生成虚假信息（幻觉），而用户缺乏检测工具，因此需要一种可靠的方法来评估模型输出的可信度。

Method: 设计了基于Transformer架构的预训练UQ头部模块，利用LLM的注意力图提取特征，提升不确定性量化能力。

Result: 实验表明，该方法在幻觉检测任务中表现优异，支持跨领域和多语言场景，且代码和预训练模型已公开。

Conclusion: 预训练的UQ头部模块显著提升了LLMs的幻觉检测能力，具有广泛的应用潜力。

Abstract: Large Language Models (LLMs) have the tendency to hallucinate, i.e., to
sporadically generate false or fabricated information. This presents a major
challenge, as hallucinations often appear highly convincing and users generally
lack the tools to detect them. Uncertainty quantification (UQ) provides a
framework for assessing the reliability of model outputs, aiding in the
identification of potential hallucinations. In this work, we introduce
pre-trained UQ heads: supervised auxiliary modules for LLMs that substantially
enhance their ability to capture uncertainty compared to unsupervised UQ
methods. Their strong performance stems from the powerful Transformer
architecture in their design and informative features derived from LLM
attention maps. Experimental evaluation shows that these heads are highly
robust and achieve state-of-the-art performance in claim-level hallucination
detection across both in-domain and out-of-domain prompts. Moreover, these
modules demonstrate strong generalization to languages they were not explicitly
trained on. We pre-train a collection of UQ heads for popular LLM series,
including Mistral, Llama, and Gemma 2. We publicly release both the code and
the pre-trained heads.

</details>


### [38] [Large Language Model Psychometrics: A Systematic Review of Evaluation, Validation, and Enhancement](https://arxiv.org/abs/2505.08245)
*Haoran Ye,Jing Jin,Yuhang Xie,Xin Zhang,Guojie Song*

Main category: cs.CL

TL;DR: 论文探讨了如何利用心理测量学方法评估大型语言模型（LLMs），提出了LLM心理测量学这一新兴领域，旨在通过心理测量工具和理论改进LLMs的评估和理解。


<details>
  <summary>Details</summary>
Motivation: 传统评估方法难以应对LLMs的快速发展，尤其是在测量人类心理特质（如个性、价值观和智力）方面存在挑战。

Method: 通过整合心理测量学的理论、工具和原则，系统性地探索其在LLMs评估中的作用，包括基准原则、方法改进和结果验证。

Result: 提出了LLM心理测量学的结构化框架，为跨学科研究者提供了更全面的理解，并提供了未来评估范式的可行建议。

Conclusion: LLM心理测量学有助于推动更符合人类水平的AI评估，促进以人为中心的AI系统发展，造福社会。

Abstract: The rapid advancement of large language models (LLMs) has outpaced
traditional evaluation methodologies. It presents novel challenges, such as
measuring human-like psychological constructs, navigating beyond static and
task-specific benchmarks, and establishing human-centered evaluation. These
challenges intersect with Psychometrics, the science of quantifying the
intangible aspects of human psychology, such as personality, values, and
intelligence. This survey introduces and synthesizes an emerging
interdisciplinary field of LLM Psychometrics, which leverages psychometric
instruments, theories, and principles to evaluate, understand, and enhance
LLMs. We systematically explore the role of Psychometrics in shaping
benchmarking principles, broadening evaluation scopes, refining methodologies,
validating results, and advancing LLM capabilities. This paper integrates
diverse perspectives to provide a structured framework for researchers across
disciplines, enabling a more comprehensive understanding of this nascent field.
Ultimately, we aim to provide actionable insights for developing future
evaluation paradigms that align with human-level AI and promote the advancement
of human-centered AI systems for societal benefit. A curated repository of LLM
psychometric resources is available at
https://github.com/valuebyte-ai/Awesome-LLM-Psychometrics.

</details>


### [39] [Enhancing Cache-Augmented Generation (CAG) with Adaptive Contextual Compression for Scalable Knowledge Integration](https://arxiv.org/abs/2505.08261)
*Rishabh Agrawal,Himanshu Kumar*

Main category: cs.CL

TL;DR: 论文提出了一种名为自适应上下文压缩（ACC）的技术，结合混合CAG-RAG框架，以优化大型语言模型（LLMs）在知识密集型任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 解决Cache-Augmented Generation（CAG）在扩展性和动态知识库管理中的局限性，同时结合检索增强生成（RAG）的优势。

Method: 引入自适应上下文压缩（ACC）技术动态压缩和管理上下文输入，并提出混合CAG-RAG框架，选择性检索补充预加载上下文。

Result: 在多样化数据集上的评估表明，该方法提升了扩展性、效率和多跳推理性能。

Conclusion: 论文提出的方法为实际知识集成挑战提供了实用解决方案。

Abstract: The rapid progress in large language models (LLMs) has paved the way for
novel approaches in knowledge-intensive tasks. Among these, Cache-Augmented
Generation (CAG) has emerged as a promising alternative to Retrieval-Augmented
Generation (RAG). CAG minimizes retrieval latency and simplifies system design
by preloading knowledge into the model's context. However, challenges persist
in scaling CAG to accommodate large and dynamic knowledge bases effectively.
This paper introduces Adaptive Contextual Compression (ACC), an innovative
technique designed to dynamically compress and manage context inputs, enabling
efficient utilization of the extended memory capabilities of modern LLMs. To
further address the limitations of standalone CAG, we propose a Hybrid CAG-RAG
Framework, which integrates selective retrieval to augment preloaded contexts
in scenarios requiring additional information. Comprehensive evaluations on
diverse datasets highlight the proposed methods' ability to enhance
scalability, optimize efficiency, and improve multi-hop reasoning performance,
offering practical solutions for real-world knowledge integration challenges.

</details>


### [40] [Evaluating the Effectiveness of Black-Box Prompt Optimization as the Scale of LLMs Continues to Grow](https://arxiv.org/abs/2505.08303)
*Ziyu Zhou,Yihang Wu,Jingyuan Yang,Zhan Xiao,Rongjun Li*

Main category: cs.CL

TL;DR: 黑盒提示优化方法在大规模语言模型（如DeepSeek V3和Gemini 2.0 Flash）上的效果有限，且模型规模越大，优化效果越差。


<details>
  <summary>Details</summary>
Motivation: 研究黑盒提示优化方法在大规模语言模型上的适用性，验证其是否仍能显著提升性能。

Method: 选择三种知名黑盒优化方法，在DeepSeek V3和Gemini 2.0 Flash等大规模LLMs上评估，并分析模型规模对优化效果的影响。

Result: 黑盒优化方法在大规模LLMs上仅带来有限改进，且随着模型规模增大，优化效果逐渐减弱。

Conclusion: 模型规模是影响黑盒提示优化效果的主要因素，大规模LLMs的优化需求可能需要新方法。

Abstract: Black-Box prompt optimization methods have emerged as a promising strategy
for refining input prompts to better align large language models (LLMs),
thereby enhancing their task performance. Although these methods have
demonstrated encouraging results, most studies and experiments have primarily
focused on smaller-scale models (e.g., 7B, 14B) or earlier versions (e.g.,
GPT-3.5) of LLMs. As the scale of LLMs continues to increase, such as with
DeepSeek V3 (671B), it remains an open question whether these black-box
optimization techniques will continue to yield significant performance
improvements for models of such scale. In response to this, we select three
well-known black-box optimization methods and evaluate them on large-scale LLMs
(DeepSeek V3 and Gemini 2.0 Flash) across four NLU and NLG datasets. The
results show that these black-box prompt optimization methods offer only
limited improvements on these large-scale LLMs. Furthermore, we hypothesize
that the scale of the model is the primary factor contributing to the limited
benefits observed. To explore this hypothesis, we conducted experiments on LLMs
of varying sizes (Qwen 2.5 series, ranging from 7B to 72B) and observed an
inverse scaling law, wherein the effectiveness of black-box optimization
methods diminished as the model size increased.

</details>


### [41] [AM-Thinking-v1: Advancing the Frontier of Reasoning at 32B Scale](https://arxiv.org/abs/2505.08311)
*Yunjie Ji,Xiaoyu Tian,Sitong Zhao,Haotian Wang,Shuaiting Chen,Yiping Peng,Han Zhao,Xiangang Li*

Main category: cs.CL

TL;DR: AM-Thinking-v1是一个32B密集语言模型，在开源创新中表现出色，数学和编码能力领先。


<details>
  <summary>Details</summary>
Motivation: 展示开源社区在32B规模模型上的高性能，平衡顶级性能和实际可用性。

Method: 基于开源Qwen2.5-32B模型，结合监督微调和强化学习的后训练流程。

Result: 在AIME 2024、2025和LiveCodeBench上取得高分，性能媲美主流MoE模型。

Conclusion: 开源AM-Thinking-v1，鼓励协作推动中等规模模型的推理能力，同时保持可访问性。

Abstract: We present AM-Thinking-v1, a 32B dense language model that advances the
frontier of reasoning, embodying the collaborative spirit of open-source
innovation. Outperforming DeepSeek-R1 and rivaling leading Mixture-of-Experts
(MoE) models like Qwen3-235B-A22B and Seed1.5-Thinking, AM-Thinking-v1 achieves
impressive scores of 85.3 on AIME 2024, 74.4 on AIME 2025, and 70.3 on
LiveCodeBench, showcasing state-of-the-art mathematical and coding capabilities
among open-source models of similar scale.
  Built entirely from the open-source Qwen2.5-32B base model and publicly
available queries, AM-Thinking-v1 leverages a meticulously crafted
post-training pipeline - combining supervised fine-tuning and reinforcement
learning - to deliver exceptional reasoning capabilities. This work
demonstrates that the open-source community can achieve high performance at the
32B scale, a practical sweet spot for deployment and fine-tuning. By striking a
balance between top-tier performance and real-world usability, we hope
AM-Thinking-v1 inspires further collaborative efforts to harness mid-scale
models, pushing reasoning boundaries while keeping accessibility at the core of
innovation. We have open-sourced our model on
\href{https://huggingface.co/a-m-team/AM-Thinking-v1}{Hugging Face}.

</details>


### [42] [On the Geometry of Semantics in Next-token Prediction](https://arxiv.org/abs/2505.08348)
*Yize Zhao,Christos Thrampoulidis*

Main category: cs.CL

TL;DR: 论文探讨了现代语言模型如何通过简单的下一个词预测（NTP）训练目标隐含地学习语义和语法概念，揭示了其通过奇异值分解（SVD）编码语言结构。


<details>
  <summary>Details</summary>
Motivation: 研究NTP训练目标如何引导模型提取和编码潜在的语义和语法概念，以理解语言模型如何从简单目标中学习复杂结构。

Method: 通过分析NTP优化过程，发现模型隐含地通过SVD分解中心化的数据稀疏矩阵来编码语言结构，并提出基于谱聚类的嵌入分析方法。

Result: 研究发现最重要的SVD因子在训练早期被学习，支持通过谱聚类识别可解释的语义结构，包括经典的k-means和新的正交基方法。

Conclusion: 论文连接了分布语义学、神经崩溃几何和神经网络训练动态，揭示了NTP隐含偏置如何塑造语言模型中的意义表示。

Abstract: Modern language models demonstrate a remarkable ability to capture linguistic
meaning despite being trained solely through next-token prediction (NTP). We
investigate how this conceptually simple training objective leads models to
extract and encode latent semantic and grammatical concepts. Our analysis
reveals that NTP optimization implicitly guides models to encode concepts via
singular value decomposition (SVD) factors of a centered data-sparsity matrix
that captures next-word co-occurrence patterns. While the model never
explicitly constructs this matrix, learned word and context embeddings
effectively factor it to capture linguistic structure. We find that the most
important SVD factors are learned first during training, motivating the use of
spectral clustering of embeddings to identify human-interpretable semantics,
including both classical k-means and a new orthant-based method directly
motivated by our interpretation of concepts. Overall, our work bridges
distributional semantics, neural collapse geometry, and neural network training
dynamics, providing insights into how NTP's implicit biases shape the emergence
of meaning representations in language models.

</details>


### [43] [Alignment Drift in CEFR-prompted LLMs for Interactive Spanish Tutoring](https://arxiv.org/abs/2505.08351)
*Mina Almasi,Ross Deans Kristensen-McLachlan*

Main category: cs.CL

TL;DR: 研究探讨了大型语言模型（LLMs）作为第二语言学习自适应导师的潜力，发现系统提示虽能约束模型输出，但长期互动中效果不稳定。


<details>
  <summary>Details</summary>
Motivation: 评估LLMs能否通过系统提示生成符合学生语言水平的文本，以支持个性化学习。

Method: 使用7B至12B参数的指令调优开源LLMs模拟西班牙语师生对话，通过CEFR分级提示控制文本难度。

Result: 系统提示能约束输出，但长期互动中存在对齐漂移现象，提示效果不稳定。

Conclusion: LLMs在自适应导师方面具有潜力，但需进一步优化提示方法以支持长期互动。

Abstract: This paper investigates the potentials of Large Language Models (LLMs) as
adaptive tutors in the context of second-language learning. In particular, we
evaluate whether system prompting can reliably constrain LLMs to generate only
text appropriate to the student's competence level. We simulate full
teacher-student dialogues in Spanish using instruction-tuned, open-source LLMs
ranging in size from 7B to 12B parameters. Dialogues are generated by having an
LLM alternate between tutor and student roles with separate chat histories. The
output from the tutor model is then used to evaluate the effectiveness of
CEFR-based prompting to control text difficulty across three proficiency levels
(A1, B1, C1). Our findings suggest that while system prompting can be used to
constrain model outputs, prompting alone is too brittle for sustained,
long-term interactional contexts - a phenomenon we term alignment drift. Our
results provide insights into the feasibility of LLMs for personalized,
proficiency-aligned adaptive tutors and provide a scalable method for low-cost
evaluation of model performance without human participants.

</details>


### [44] [Towards Contamination Resistant Benchmarks](https://arxiv.org/abs/2505.08389)
*Rahmatullah Musawi,Sheng Lu*

Main category: cs.CL

TL;DR: 论文提出了一种基于凯撒密码的污染抵抗基准，用于更严格地评估大型语言模型（LLMs），揭示了当前LLMs在污染控制下的局限性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的快速发展需要更可靠的评估方法，污染问题是影响评估可靠性的关键因素。

Method: 提出基于凯撒密码的污染抵抗基准，并在多种设置下测试广泛使用的LLMs。

Result: LLMs在污染控制下表现不佳，揭示了其真实能力的局限性。

Conclusion: 该工作为开发污染抵抗基准提供了贡献，有助于更严格地评估LLMs并揭示其真实能力。

Abstract: The rapid development of large language models (LLMs) has transformed the
landscape of natural language processing. Evaluating LLMs properly is crucial
for understanding their potential and addressing concerns such as safety.
However, LLM evaluation is confronted by various factors, among which
contamination stands out as a key issue that undermines the reliability of
evaluations. In this work, we introduce the concept of contamination resistance
to address this challenge. We propose a benchmark based on Caesar ciphers
(e.g., "ab" to "bc" when the shift is 1), which, despite its simplicity, is an
excellent example of a contamination resistant benchmark. We test this
benchmark on widely used LLMs under various settings, and we find that these
models struggle with this benchmark when contamination is controlled. Our
findings reveal issues in current LLMs and raise important questions regarding
their true capabilities. Our work contributes to the development of
contamination resistant benchmarks, enabling more rigorous LLM evaluation and
offering insights into the true capabilities and limitations of LLMs.

</details>


### [45] [Accelerating Chain-of-Thought Reasoning: When Goal-Gradient Importance Meets Dynamic Skipping](https://arxiv.org/abs/2505.08392)
*Ren Zhuang,Ben Wang,Shuifa Sun*

Main category: cs.CL

TL;DR: 论文提出了一种名为Adaptive GoGI-Skip的新框架，通过动态压缩Chain-of-Thought（CoT）提示来提升推理效率，同时保持高准确性。


<details>
  <summary>Details</summary>
Motivation: 当前CoT压缩技术依赖通用重要性指标和静态压缩率，可能导致关键信息丢失或无法适应不同推理复杂度。

Method: 提出Goal-Gradient Importance（GoGI）指标和Adaptive Dynamic Skipping（ADS）机制，动态调节压缩率。

Result: 在多个推理基准测试中，平均减少45%的CoT标记数量，推理速度提升1.6-2.0倍，同时保持高准确性。

Conclusion: Adaptive GoGI-Skip在推理效率和准确性之间取得了显著改进，优于现有基线。

Abstract: Large Language Models leverage Chain-of-Thought (CoT) prompting for complex
tasks, but their reasoning traces are often excessively verbose and
inefficient, leading to significant computational costs and latency. Current
CoT compression techniques typically rely on generic importance metrics and
static compression rates, which may inadvertently remove functionally critical
tokens or fail to adapt to varying reasoning complexity. To overcome these
limitations, we propose Adaptive GoGI-Skip, a novel framework learning dynamic
CoT compression via supervised fine-tuning. This approach introduces two
synergistic innovations: (1) Goal-Gradient Importance (GoGI), a novel metric
accurately identifying functionally relevant tokens by measuring the gradient
influence of their intermediate representations on the final answer loss, and
(2) Adaptive Dynamic Skipping (ADS), a mechanism dynamically regulating the
compression rate based on runtime model uncertainty while ensuring local
coherence through an adaptive N-token constraint. To our knowledge, this is the
first work unifying a goal-oriented, gradient-based importance metric with
dynamic, uncertainty-aware skipping for CoT compression. Trained on compressed
MATH data, Adaptive GoGI-Skip demonstrates strong cross-domain generalization
across diverse reasoning benchmarks including AIME, GPQA, and GSM8K. It
achieves substantial efficiency gains - reducing CoT token counts by over 45%
on average and delivering 1.6-2.0 times inference speedups - while maintaining
high reasoning accuracy. Notably, it significantly outperforms existing
baselines by preserving accuracy even at high effective compression rates,
advancing the state of the art in the CoT reasoning efficiency-accuracy
trade-off.

</details>


### [46] [TUMS: Enhancing Tool-use Abilities of LLMs with Multi-structure Handlers](https://arxiv.org/abs/2505.08402)
*Aiyao He,Sijia Cui,Shuai Xu,Yanna Wang,Bo Xu*

Main category: cs.CL

TL;DR: 论文提出TUMS框架，通过将工具级处理转为参数级处理，提升大语言模型（LLMs）的工具使用能力，显著提高了任务执行效果。


<details>
  <summary>Details</summary>
Motivation: LLMs在工具集成中面临非可执行操作和参数错误的问题，现有方法缺乏对不同工具难度的细粒度处理。

Method: TUMS框架包含意图识别器、任务分解器、子任务处理器（含多结构处理器）和执行器，实现参数级优化。

Result: 在ToolQA基准测试中，TUMS在简单和困难任务上分别提升19.6%和50.6%。

Conclusion: TUMS有效提升LLMs的工具使用能力，并通过消融实验验证了各模块的关键贡献。

Abstract: Recently, large language models(LLMs) have played an increasingly important
role in solving a wide range of NLP tasks, leveraging their capabilities of
natural language understanding and generating. Integration with external tools
further enhances LLMs' effectiveness, providing more precise, timely, and
specialized responses. However, LLMs still encounter difficulties with
non-executable actions and improper actions, which are primarily attributed to
incorrect parameters. The process of generating parameters by LLMs is confined
to the tool level, employing the coarse-grained strategy without considering
the different difficulties of various tools. To address this issue, we propose
TUMS, a novel framework designed to enhance the tool-use capabilities of LLMs
by transforming tool-level processing into parameter-level processing.
Specifically, our framework consists of four key components: (1) an intent
recognizer that identifies the user's intent to help LLMs better understand the
task; (2) a task decomposer that breaks down complex tasks into simpler
subtasks, each involving a tool call; (3) a subtask processor equipped with
multi-structure handlers to generate accurate parameters; and (4) an executor.
Our empirical studies have evidenced the effectiveness and efficiency of the
TUMS framework with an average of 19.6\% and 50.6\% improvement separately on
easy and hard benchmarks of ToolQA, meanwhile, we demonstrated the key
contribution of each part with ablation experiments, offering more insights and
stimulating future research on Tool-augmented LLMs.

</details>


### [47] [Hakim: Farsi Text Embedding Model](https://arxiv.org/abs/2505.08435)
*Mehran Sarmadi,Morteza Alikhani,Erfan Zinvandi,Zahra Pourbahman*

Main category: cs.CL

TL;DR: 本文介绍了Hakim，一种新型波斯语文本嵌入模型，性能提升8.5%，并引入三个新数据集。


<details>
  <summary>Details</summary>
Motivation: 波斯语在大规模嵌入研究中代表性不足，需改进波斯语文本嵌入模型。

Method: 提出Hakim模型，基于BERT架构，引入新数据集Corpesia、Pairsia-sup和Pairsia-unsup。

Result: Hakim在FaMTEB基准测试中性能提升8.5%，优于现有波斯语模型。

Conclusion: Hakim为波斯语理解奠定新基础，适用于聊天机器人和检索增强生成系统。

Abstract: Recent advancements in text embedding have significantly improved natural
language understanding across many languages, yet Persian remains notably
underrepresented in large-scale embedding research. In this paper, we present
Hakim, a novel state-of-the-art Persian text embedding model that achieves a
8.5% performance improvement over existing approaches on the FaMTEB benchmark,
outperforming all previously developed Persian language models. As part of this
work, we introduce three new datasets - Corpesia, Pairsia-sup, and
Pairsia-unsup - to support supervised and unsupervised training scenarios.
Additionally, Hakim is designed for applications in chatbots and
retrieval-augmented generation (RAG) systems, particularly addressing retrieval
tasks that require incorporating message history within these systems. We also
propose a new baseline model built on the BERT architecture. Our language model
consistently achieves higher accuracy across various Persian NLP tasks, while
the RetroMAE-based model proves particularly effective for textual information
retrieval applications. Together, these contributions establish a new
foundation for advancing Persian language understanding.

</details>


### [48] [A document processing pipeline for the construction of a dataset for topic modeling based on the judgments of the Italian Supreme Court](https://arxiv.org/abs/2505.08439)
*Matteo Marulli,Glauco Panattoni,Marco Bertini*

Main category: cs.CL

TL;DR: 为解决意大利法律研究中缺乏公开数据集的问题，开发了一个文档处理流程，生成适用于主题建模的匿名数据集。


<details>
  <summary>Details</summary>
Motivation: 意大利法律研究中缺乏公开数据集，限制了最高法院判决中法律主题的分析。

Method: 集成了文档布局分析（YOLOv8x）、光学字符识别和文本匿名化的处理流程。

Result: DLA模块mAP@50为0.964，OCR检测器mAP@50-95为0.9022，TrOCR字符错误率为0.0047。数据集提升了主题建模的多样性（0.6198）和一致性（0.6638）。

Conclusion: 使用BERTopic提取主题，并通过大型语言模型生成标签和摘要，结果与专家解释一致，Claude Sonnet 3.7表现优异。

Abstract: Topic modeling in Italian legal research is hindered by the lack of public
datasets, limiting the analysis of legal themes in Supreme Court judgments. To
address this, we developed a document processing pipeline that produces an
anonymized dataset optimized for topic modeling.
  The pipeline integrates document layout analysis (YOLOv8x), optical character
recognition, and text anonymization. The DLA module achieved a mAP@50 of 0.964
and a mAP@50-95 of 0.800. The OCR detector reached a mAP@50-95 of 0.9022, and
the text recognizer (TrOCR) obtained a character error rate of 0.0047 and a
word error rate of 0.0248. Compared to OCR-only methods, our dataset improved
topic modeling with a diversity score of 0.6198 and a coherence score of
0.6638.
  We applied BERTopic to extract topics and used large language models to
generate labels and summaries. Outputs were evaluated against domain expert
interpretations. Claude Sonnet 3.7 achieved a BERTScore F1 of 0.8119 for
labeling and 0.9130 for summarization.

</details>


### [49] [IterKey: Iterative Keyword Generation with LLMs for Enhanced Retrieval Augmented Generation](https://arxiv.org/abs/2505.08450)
*Kazuki Hayashi,Hidetaka Kamigaito,Shinya Kouda,Taro Watanabe*

Main category: cs.CL

TL;DR: IterKey是一个基于稀疏检索的LLM驱动框架，通过迭代生成关键词优化RAG，平衡了准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决密集检索方法缺乏可解释性和稀疏检索方法无法完全捕捉查询意图的问题。

Method: 采用三阶段LLM驱动流程：生成检索关键词、基于检索文档生成答案、验证答案并迭代优化关键词。

Result: 在四个QA任务中，IterKey比BM25-based RAG和简单基线方法提升了5%到20%的准确率，性能接近密集检索方法。

Conclusion: IterKey通过稀疏检索和LLM迭代优化，实现了RAG在准确性和可解释性上的平衡。

Abstract: Retrieval-Augmented Generation (RAG) has emerged as a way to complement the
in-context knowledge of Large Language Models (LLMs) by integrating external
documents. However, real-world applications demand not only accuracy but also
interpretability. While dense retrieval methods provide high accuracy, they
lack interpretability; conversely, sparse retrieval methods offer transparency
but often fail to capture the full intent of queries due to their reliance on
keyword matching. To address these issues, we introduce IterKey, an LLM-driven
iterative keyword generation framework that enhances RAG via sparse retrieval.
IterKey consists of three LLM-driven stages: generating keywords for retrieval,
generating answers based on retrieved documents, and validating the answers. If
validation fails, the process iteratively repeats with refined keywords. Across
four QA tasks, experimental results show that IterKey achieves 5% to 20%
accuracy improvements over BM25-based RAG and simple baselines. Its performance
is comparable to dense retrieval-based RAG and prior iterative query refinement
methods using dense models. In summary, IterKey is a novel BM25-based approach
leveraging LLMs to iteratively refine RAG, effectively balancing accuracy with
interpretability.

</details>


### [50] [RepCali: High Efficient Fine-tuning Via Representation Calibration in Latent Space for Pre-trained Language Models](https://arxiv.org/abs/2505.08463)
*Fujun Zhang,XiangDong Su*

Main category: cs.CL

TL;DR: 论文提出了一种名为RepCali的方法，通过在校准块中调整预训练语言模型（PLMs）的潜在空间表示，以解决编码器输出与解码器输入之间的不匹配问题。


<details>
  <summary>Details</summary>
Motivation: PLMs在微调后仍存在编码器输出与解码器输入之间的表示差异，影响性能。

Method: 在编码器后添加校准块，调整潜在空间表示，作为解码器输入。

Result: 在25个PLM模型和8个任务上的实验表明，RepCali显著提升了性能，优于基准微调方法。

Conclusion: RepCali是一种通用、即插即用的方法，能有效提升PLMs在下游任务中的表现。

Abstract: Fine-tuning pre-trained language models (PLMs) has become a dominant paradigm
in applying PLMs to downstream tasks. However, with limited fine-tuning, PLMs
still struggle with the discrepancies between the representation obtained from
the PLMs' encoder and the optimal input to the PLMs' decoder. This paper
tackles this challenge by learning to calibrate the representation of PLMs in
the latent space. In the proposed representation calibration method (RepCali),
we integrate a specific calibration block to the latent space after the encoder
and use the calibrated output as the decoder input. The merits of the proposed
RepCali include its universality to all PLMs with encoder-decoder
architectures, its plug-and-play nature, and ease of implementation. Extensive
experiments on 25 PLM-based models across 8 tasks (including both English and
Chinese datasets) demonstrate that the proposed RepCali offers desirable
enhancements to PLMs (including LLMs) and significantly improves the
performance of downstream tasks. Comparison experiments across 4 benchmark
tasks indicate that RepCali is superior to the representative fine-tuning
baselines.

</details>


### [51] [Large Language Models Meet Stance Detection: A Survey of Tasks, Methods, Applications, Challenges and Future Directions](https://arxiv.org/abs/2505.08464)
*Lata Pangtey,Anukriti Bhatnagar,Shubhi Bansal,Shahid Shafi Dar,Nagendra Kumar*

Main category: cs.CL

TL;DR: 该论文综述了基于大语言模型（LLMs）的立场检测研究，提出了新的分类法，并探讨了方法、数据集、应用及挑战。


<details>
  <summary>Details</summary>
Motivation: 现有调查缺乏对LLMs在立场检测中应用的全面覆盖，本文旨在填补这一空白。

Method: 通过系统分析，提出基于学习方式、数据模态和目标关系的三维分类法。

Result: 总结了LLMs在立场检测中的优势与局限，并讨论了关键应用和挑战。

Conclusion: 论文为研究者和从业者提供了未来研究方向，如可解释性推理和低资源适应。

Abstract: Stance detection is essential for understanding subjective content across
various platforms such as social media, news articles, and online reviews.
Recent advances in Large Language Models (LLMs) have revolutionized stance
detection by introducing novel capabilities in contextual understanding,
cross-domain generalization, and multimodal analysis. Despite these
progressions, existing surveys often lack comprehensive coverage of approaches
that specifically leverage LLMs for stance detection. To bridge this critical
gap, our review article conducts a systematic analysis of stance detection,
comprehensively examining recent advancements of LLMs transforming the field,
including foundational concepts, methodologies, datasets, applications, and
emerging challenges. We present a novel taxonomy for LLM-based stance detection
approaches, structured along three key dimensions: 1) learning methods,
including supervised, unsupervised, few-shot, and zero-shot; 2) data
modalities, such as unimodal, multimodal, and hybrid; and 3) target
relationships, encompassing in-target, cross-target, and multi-target
scenarios. Furthermore, we discuss the evaluation techniques and analyze
benchmark datasets and performance trends, highlighting the strengths and
limitations of different architectures. Key applications in misinformation
detection, political analysis, public health monitoring, and social media
moderation are discussed. Finally, we identify critical challenges such as
implicit stance expression, cultural biases, and computational constraints,
while outlining promising future directions, including explainable stance
reasoning, low-resource adaptation, and real-time deployment frameworks. Our
survey highlights emerging trends, open challenges, and future directions to
guide researchers and practitioners in developing next-generation stance
detection systems powered by large language models.

</details>


### [52] [Judging the Judges: Can Large Vision-Language Models Fairly Evaluate Chart Comprehension and Reasoning?](https://arxiv.org/abs/2505.08468)
*Md Tahmid Rahman Laskar,Mohammed Saidul Islam,Ridwan Mahbub,Ahmed Masry,Mizanur Rahman,Amran Bhuiyan,Mir Tafseer Nayeem,Shafiq Joty,Enamul Hoque,Jimmy Huang*

Main category: cs.CL

TL;DR: 论文评估了13种开源大型视觉语言模型（LVLM）作为图表理解任务的自动评估工具，发现部分模型性能接近GPT-4，但存在位置偏好和长度偏差等问题。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型（LVLM）在图表相关任务中潜力巨大，但实际部署受限于高昂的评估成本和资源限制。

Method: 设计了成对和点对评估任务，涵盖事实准确性、信息量和相关性等标准，并分析了格式遵循、位置一致性和指令遵循等问题。

Result: 实验显示开源LVLM性能差异显著，部分模型与GPT-4评估结果一致性达80%，而其他模型低于10%。

Conclusion: 开源LVLM可作为图表任务的低成本自动评估工具，但仍需解决位置偏好和长度偏差等问题。

Abstract: Charts are ubiquitous as they help people understand and reason with data.
Recently, various downstream tasks, such as chart question answering,
chart2text, and fact-checking, have emerged. Large Vision-Language Models
(LVLMs) show promise in tackling these tasks, but their evaluation is costly
and time-consuming, limiting real-world deployment. While using LVLMs as judges
to assess the chart comprehension capabilities of other LVLMs could streamline
evaluation processes, challenges like proprietary datasets, restricted access
to powerful models, and evaluation costs hinder their adoption in industrial
settings. To this end, we present a comprehensive evaluation of 13 open-source
LVLMs as judges for diverse chart comprehension and reasoning tasks. We design
both pairwise and pointwise evaluation tasks covering criteria like factual
correctness, informativeness, and relevancy. Additionally, we analyze LVLM
judges based on format adherence, positional consistency, length bias, and
instruction-following. We focus on cost-effective LVLMs (<10B parameters)
suitable for both research and commercial use, following a standardized
evaluation protocol and rubric to measure the LVLM judge's accuracy.
Experimental results reveal notable variability: while some open LVLM judges
achieve GPT-4-level evaluation performance (about 80% agreement with GPT-4
judgments), others struggle (below ~10% agreement). Our findings highlight that
state-of-the-art open-source LVLMs can serve as cost-effective automatic
evaluators for chart-related tasks, though biases such as positional preference
and length bias persist.

</details>


### [53] [LCES: Zero-shot Automated Essay Scoring via Pairwise Comparisons Using Large Language Models](https://arxiv.org/abs/2505.08498)
*Takumi Shibata,Yuichi Miyamura*

Main category: cs.CL

TL;DR: 论文提出了一种基于大语言模型（LLM）的对比性作文评分方法（LCES），通过将作文评分任务转化为成对比较任务，提高了评分的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的零样本作文评分方法依赖LLM直接生成绝对分数，但由于模型偏差和不一致评分，结果与人工评分存在差异。论文旨在解决这一问题。

Method: 提出LCES方法，将评分任务转化为成对比较，利用LLM判断两篇作文的优劣，并通过RankNet将比较结果转换为连续分数，以提高可扩展性。

Result: 实验表明，LCES在基准数据集上优于传统零样本方法，且在不同LLM模型上表现稳健。

Conclusion: LCES为实际零样本作文评分提供了一种高效且准确的方法。

Abstract: Recent advances in large language models (LLMs) have enabled zero-shot
automated essay scoring (AES), providing a promising way to reduce the cost and
effort of essay scoring in comparison with manual grading. However, most
existing zero-shot approaches rely on LLMs to directly generate absolute
scores, which often diverge from human evaluations owing to model biases and
inconsistent scoring. To address these limitations, we propose LLM-based
Comparative Essay Scoring (LCES), a method that formulates AES as a pairwise
comparison task. Specifically, we instruct LLMs to judge which of two essays is
better, collect many such comparisons, and convert them into continuous scores.
Considering that the number of possible comparisons grows quadratically with
the number of essays, we improve scalability by employing RankNet to
efficiently transform LLM preferences into scalar scores. Experiments using AES
benchmark datasets show that LCES outperforms conventional zero-shot methods in
accuracy while maintaining computational efficiency. Moreover, LCES is robust
across different LLM backbones, highlighting its applicability to real-world
zero-shot AES.

</details>


### [54] [Reassessing Graph Linearization for Sequence-to-sequence AMR Parsing: On the Advantages and Limitations of Triple-Based Encoding](https://arxiv.org/abs/2505.08504)
*Jeongwoo Kang,Maximin Coavoux,Cédric Lopez,Didier Schwab*

Main category: cs.CL

TL;DR: 论文提出了一种基于三元组的线性化方法，以解决Penman编码在处理深度图和节点重入时的局限性，并比较了其效率。


<details>
  <summary>Details</summary>
Motivation: Penman编码在处理深度图时存在节点距离远和节点重入时关系类型翻倍的问题，需要更高效的线性化方法。

Method: 提出了一种基于三元组的线性化方法，并与Penman编码进行效率比较。

Result: 三元组编码在表示图时表现良好，但仍需改进以更好地与Penman的简洁嵌套结构表示竞争。

Conclusion: 三元组编码有潜力，但需进一步优化以超越Penman编码的表现。

Abstract: Sequence-to-sequence models are widely used to train Abstract Meaning
Representation (Banarescu et al., 2013, AMR) parsers. To train such models, AMR
graphs have to be linearized into a one-line text format. While Penman encoding
is typically used for this purpose, we argue that it has limitations: (1) for
deep graphs, some closely related nodes are located far apart in the linearized
text (2) Penman's tree-based encoding necessitates inverse roles to handle node
re-entrancy, doubling the number of relation types to predict. To address these
issues, we propose a triple-based linearization method and compare its
efficiency with Penman linearization. Although triples are well suited to
represent a graph, our results suggest room for improvement in triple encoding
to better compete with Penman's concise and explicit representation of a nested
graph structure.

</details>


### [55] [Are We Paying Attention to Her? Investigating Gender Disambiguation and Attention in Machine Translation](https://arxiv.org/abs/2505.08546)
*Chiara Manna,Afra Alishahi,Frédéric Blain,Eva Vanmassenhove*

Main category: cs.CL

TL;DR: 论文提出了一种新的评估指标MPA，用于衡量NMT系统对性别线索的依赖程度，发现现有模型更倾向于统计性别刻板印象，且在反刻板情况下更关注男性线索。


<details>
  <summary>Details</summary>
Motivation: 现有NMT系统的性别偏见问题未得到充分评估，传统指标无法全面捕捉模型对上下文性别线索的利用。

Method: 提出Minimal Pair Accuracy (MPA)指标，通过最小对句子评估模型对性别线索的依赖，并在EN-IT语言对上测试多个NMT模型。

Result: 模型大多忽略性别线索，依赖统计刻板印象；反刻板情况下更关注男性线索；编码器中性别信息处理方式存在差异。

Conclusion: MPA揭示了NMT模型在性别处理上的不足，尤其是对女性线索的忽视，需进一步改进模型设计。

Abstract: While gender bias in modern Neural Machine Translation (NMT) systems has
received much attention, traditional evaluation metrics do not to fully capture
the extent to which these systems integrate contextual gender cues. We propose
a novel evaluation metric called Minimal Pair Accuracy (MPA), which measures
the reliance of models on gender cues for gender disambiguation. MPA is
designed to go beyond surface-level gender accuracy metrics by focusing on
whether models adapt to gender cues in minimal pairs -- sentence pairs that
differ solely in the gendered pronoun, namely the explicit indicator of the
target's entity gender in the source language (EN). We evaluate a number of NMT
models on the English-Italian (EN--IT) language pair using this metric, we show
that they ignore available gender cues in most cases in favor of (statistical)
stereotypical gender interpretation. We further show that in anti-stereotypical
cases, these models tend to more consistently take masculine gender cues into
account while ignoring the feminine cues. Furthermore, we analyze the attention
head weights in the encoder component and show that while all models encode
gender information to some extent, masculine cues elicit a more diffused
response compared to the more concentrated and specialized responses to
feminine gender cues.

</details>


### [56] [Small but Significant: On the Promise of Small Language Models for Accessible AIED](https://arxiv.org/abs/2505.08588)
*Yumou Wei,Paulo Carvalho,John Stamper*

Main category: cs.CL

TL;DR: 论文指出，尽管GPT等大型语言模型（LLMs）在教育领域受到广泛关注，但小型语言模型（SLMs）在资源受限环境下可能更具潜力。


<details>
  <summary>Details</summary>
Motivation: 探讨LLMs在教育中的主导地位是否忽视了SLMs的潜力，尤其是在资源受限的机构中提供高质量AI工具的可行性。

Method: 通过关键词搜索分析AIED 2024论文中LLMs的使用情况，并以知识组件（KC）发现为例，展示SLMs（如Phi-2）的有效性。

Result: 61%的AIED 2024论文使用LLMs，43%提到GPT；SLMs在KC发现中表现良好，无需复杂提示策略。

Conclusion: 呼吁更多关注SLMs在教育中的应用，以实现更公平和可负担的AI工具普及。

Abstract: GPT has become nearly synonymous with large language models (LLMs), an
increasingly popular term in AIED proceedings. A simple keyword-based search
reveals that 61% of the 76 long and short papers presented at AIED 2024
describe novel solutions using LLMs to address some of the long-standing
challenges in education, and 43% specifically mention GPT. Although LLMs
pioneered by GPT create exciting opportunities to strengthen the impact of AI
on education, we argue that the field's predominant focus on GPT and other
resource-intensive LLMs (with more than 10B parameters) risks neglecting the
potential impact that small language models (SLMs) can make in providing
resource-constrained institutions with equitable and affordable access to
high-quality AI tools. Supported by positive results on knowledge component
(KC) discovery, a critical challenge in AIED, we demonstrate that SLMs such as
Phi-2 can produce an effective solution without elaborate prompting strategies.
Hence, we call for more attention to developing SLM-based AIED approaches.

</details>


### [57] [Enhancing Thyroid Cytology Diagnosis with RAG-Optimized LLMs and Pa-thology Foundation Models](https://arxiv.org/abs/2505.08590)
*Hussien Al-Asi,Jordan P Reynolds,Shweta Agarwal,Bryan J Dangott,Aziza Nassar,Zeynettin Akkus*

Main category: cs.CL

TL;DR: 该研究通过结合检索增强生成（RAG）和病理学基础模型，提升了大语言模型（LLM）在甲状腺细胞学诊断中的应用，显著提高了诊断效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 解决甲状腺细胞学诊断中的细胞学解释、标准化和诊断准确性等挑战。

Method: 利用RAG动态检索相关案例和诊断标准，结合病理学基础模型优化特征提取和分类能力。

Result: 融合方法显著提升诊断一致性，减少变异性，基础模型UNI的AUC达到0.73-0.93。

Conclusion: 该研究为AI辅助甲状腺细胞病理学奠定了基础，展示了RAG与病理学LLM结合的潜力。

Abstract: Advancements in artificial intelligence (AI) are transforming pathology by
integrat-ing large language models (LLMs) with retrieval-augmented generation
(RAG) and domain-specific foundation models. This study explores the
application of RAG-enhanced LLMs coupled with pathology foundation models for
thyroid cytology diagnosis, addressing challenges in cytological
interpretation, standardization, and diagnostic accuracy. By leveraging a
curated knowledge base, RAG facilitates dy-namic retrieval of relevant case
studies, diagnostic criteria, and expert interpreta-tion, improving the
contextual understanding of LLMs. Meanwhile, pathology foun-dation models,
trained on high-resolution pathology images, refine feature extrac-tion and
classification capabilities. The fusion of these AI-driven approaches en-hances
diagnostic consistency, reduces variability, and supports pathologists in
dis-tinguishing benign from malignant thyroid lesions. Our results demonstrate
that integrating RAG with pathology-specific LLMs significantly improves
diagnostic efficiency and interpretability, paving the way for AI-assisted
thyroid cytopathology, with foundation model UNI achieving AUC 0.73-0.93 for
correct prediction of surgi-cal pathology diagnosis from thyroid cytology
samples.

</details>


### [58] [Automatic Task Detection and Heterogeneous LLM Speculative Decoding](https://arxiv.org/abs/2505.08600)
*Danying Ge,Jianhua Gao,Qizhi Jiang,Yifei Feng,Weixing Ji*

Main category: cs.CL

TL;DR: 提出了一种针对下游任务优化的推测解码算法，通过任务自动分区和分配方法提升解码速度和接受率。


<details>
  <summary>Details</summary>
Motivation: 现有推测解码方法在下游任务中面临解码速度和接受率的权衡，难以保证效率。

Method: 采用任务自动分区和分配方法，将下游任务分类并分配给异构草稿模型，结合在线轻量级提示分类器动态路由提示。

Result: 实验显示，该方法比传统推测解码的草稿准确率提升6%至50%，推理速度提升1.10倍至2.64倍。

Conclusion: 该方法有效解决了下游任务中推测解码的效率问题，提升了推理速度和结果一致性。

Abstract: Speculative decoding, which combines a draft model with a target model, has
emerged as an effective approach to accelerate large language model (LLM)
inference. However, existing methods often face a trade-off between the
acceptance rate and decoding speed in downstream tasks due to the limited
capacity of the draft model, making it difficult to ensure efficiency across
diverse tasks. To address this problem, we propose a speculative decoding
algorithm tailored for downstream task optimization. It includes an automatic
task partitioning and assigning method, which automatically categorizes
downstream tasks into different sub-tasks and assigns them to a set of
heterogeneous draft models. Each draft model is aligned with the target model
using task-specific data, thereby enhancing the consistency of inference
results. In addition, our proposed method incorporates an online lightweight
prompt classifier to dynamically route prompts to the appropriate draft model.
Experimental results demonstrate that the proposed method improves draft
accuracy by 6% to 50% over vanilla speculative decoding, while achieving a
speedup of 1.10x to 2.64x in LLM inference.

</details>


### [59] [Scaling Context, Not Parameters: Training a Compact 7B Language Model for Efficient Long-Context Processing](https://arxiv.org/abs/2505.08651)
*Chen Wu,Yin Song*

Main category: cs.CL

TL;DR: MegaBeam-Mistral-7B是一个支持512K标记上下文长度的语言模型，解决了长上下文训练的实际限制，并在多个长上下文基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决长上下文训练中的实际限制，支持合规监控和验证等现实任务。

Method: 开发了一个7B参数的语言模型，支持512K标记的上下文长度。

Result: 在HELMET上表现出卓越的上下文学习能力，在RULER上展示了强大的检索和追踪能力，并在BABILong上实现了竞争性的长程推理能力。

Conclusion: 该模型是目前唯一无需RAG或针对性微调即可在512K上下文长度下实现竞争性长程推理的开源模型，已发布为Apache 2.0许可证下的完全开源项目。

Abstract: We present MegaBeam-Mistral-7B, a language model that supports 512K-token
context length. Our work addresses practical limitations in long-context
training, supporting real-world tasks such as compliance monitoring and
verification. Evaluated on three long-context benchmarks, our 7B-parameter
model demonstrates superior in-context learning performance on HELMET and
robust retrieval and tracing capability on RULER. It is currently the only open
model to achieve competitive long-range reasoning on BABILong at 512K context
length without RAG or targeted fine-tuning. Released as fully open source under
the Apache 2.0 license, the model has been downloaded over 100,000 times on
Hugging Face. Model available at:
https://huggingface.co/aws-prototyping/MegaBeam-Mistral-7B-512k

</details>


### [60] [Revealing economic facts: LLMs know more than they say](https://arxiv.org/abs/2505.08662)
*Marcus Buckmann,Quynh Anh Nguyen,Edward Hill*

Main category: cs.CL

TL;DR: 研究发现，大型语言模型（LLM）的隐藏状态可用于估计和填补经济与金融统计数据，且效果优于模型的文本输出。


<details>
  <summary>Details</summary>
Motivation: 探索LLM隐藏状态是否包含比直接文本输出更丰富的经济信息。

Method: 使用简单线性模型训练开源LLM的隐藏状态，并提出无需目标变量标签的迁移学习方法。

Result: 隐藏状态在县级和公司级变量估计中表现优异，少量标签数据即可训练，迁移学习提高准确性。

Conclusion: 隐藏状态在经济统计任务中具有实用价值，尤其在超分辨率和数据填补方面。

Abstract: We investigate whether the hidden states of large language models (LLMs) can
be used to estimate and impute economic and financial statistics. Focusing on
county-level (e.g. unemployment) and firm-level (e.g. total assets) variables,
we show that a simple linear model trained on the hidden states of open-source
LLMs outperforms the models' text outputs. This suggests that hidden states
capture richer economic information than the responses of the LLMs reveal
directly. A learning curve analysis indicates that only a few dozen labelled
examples are sufficient for training. We also propose a transfer learning
method that improves estimation accuracy without requiring any labelled data
for the target variable. Finally, we demonstrate the practical utility of
hidden-state representations in super-resolution and data imputation tasks.

</details>


### [61] [Adaptive Schema-aware Event Extraction with Retrieval-Augmented Generation](https://arxiv.org/abs/2505.08690)
*Sheng Liang,Hang Lv,Zhihao Wen,Yaxiong Wu,Yongyue Zhang,Hao Wang,Yong Liu*

Main category: cs.CL

TL;DR: 论文提出了一种自适应模式感知事件抽取（ASEE）方法，结合模式改写和检索增强生成，解决了现有事件抽取研究中模式固定和缺乏联合评估基准的问题，并在多领域数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有事件抽取研究存在模式固定和缺乏联合评估基准的问题，且大语言模型在实际部署中存在模式幻觉和上下文窗口限制的挑战。

Method: 提出ASEE方法，结合模式改写和检索增强生成，构建MD-SEE基准以系统评估。

Result: ASEE在多领域数据集上表现出强适应性，显著提高了事件抽取的准确性。

Conclusion: ASEE为解决事件抽取中的模式适应性和评估问题提供了有效方案。

Abstract: Event extraction (EE) is a fundamental task in natural language processing
(NLP) that involves identifying and extracting event information from
unstructured text. Effective EE in real-world scenarios requires two key steps:
selecting appropriate schemas from hundreds of candidates and executing the
extraction process. Existing research exhibits two critical gaps: (1) the rigid
schema fixation in existing pipeline systems, and (2) the absence of benchmarks
for evaluating joint schema matching and extraction. Although large language
models (LLMs) offer potential solutions, their schema hallucination tendencies
and context window limitations pose challenges for practical deployment. In
response, we propose Adaptive Schema-aware Event Extraction (ASEE), a novel
paradigm combining schema paraphrasing with schema retrieval-augmented
generation. ASEE adeptly retrieves paraphrased schemas and accurately generates
targeted structures. To facilitate rigorous evaluation, we construct the
Multi-Dimensional Schema-aware Event Extraction (MD-SEE) benchmark, which
systematically consolidates 12 datasets across diverse domains, complexity
levels, and language settings. Extensive evaluations on MD-SEE show that our
proposed ASEE demonstrates strong adaptability across various scenarios,
significantly improving the accuracy of event extraction.

</details>


### [62] [NurValues: Real-World Nursing Values Evaluation for Large Language Models in Clinical Context](https://arxiv.org/abs/2505.08734)
*Ben Yao,Qiuchi Li,Yazhou Zhang,Siyu Yang,Bohan Zhang,Prayag Tiwari,Jing Qin*

Main category: cs.CL

TL;DR: 本文提出了首个护理价值对齐基准，包含五个核心价值维度，并构建了包含2,200个标注实例的数据集。评估了23个先进LLM，发现DeepSeek-V3在简单数据集表现最佳，Claude 3.5 Sonnet在复杂数据集领先，正义维度最难评估。


<details>
  <summary>Details</summary>
Motivation: 为临床环境中的价值敏感LLM开发提供基础，填补护理价值对齐研究的空白。

Method: 通过五个月实地研究收集1,100个护理行为实例，标注后生成对立版本，构建简单和复杂数据集。

Result: DeepSeek-V3和Claude 3.5 Sonnet分别在简单和复杂数据集表现最佳，正义维度最难评估，上下文学习显著提升对齐效果。

Conclusion: 该研究为临床环境中的价值敏感LLM开发提供了重要基准和工具。

Abstract: This work introduces the first benchmark for nursing value alignment,
consisting of five core value dimensions distilled from international nursing
codes: Altruism, Human Dignity, Integrity, Justice, and Professionalism. The
benchmark comprises 1,100 real-world nursing behavior instances collected
through a five-month longitudinal field study across three hospitals of varying
tiers. These instances are annotated by five clinical nurses and then augmented
with LLM-generated counterfactuals with reversed ethic polarity. Each original
case is paired with a value-aligned and a value-violating version, resulting in
2,200 labeled instances that constitute the Easy-Level dataset. To increase
adversarial complexity, each instance is further transformed into a
dialogue-based format that embeds contextual cues and subtle misleading
signals, yielding a Hard-Level dataset. We evaluate 23 state-of-the-art (SoTA)
LLMs on their alignment with nursing values. Our findings reveal three key
insights: (1) DeepSeek-V3 achieves the highest performance on the Easy-Level
dataset (94.55), where Claude 3.5 Sonnet outperforms other models on the
Hard-Level dataset (89.43), significantly surpassing the medical LLMs; (2)
Justice is consistently the most difficult nursing value dimension to evaluate;
and (3) in-context learning significantly improves alignment. This work aims to
provide a foundation for value-sensitive LLMs development in clinical settings.
The dataset and the code are available at
https://huggingface.co/datasets/Ben012345/NurValues.

</details>


### [63] [Probability Consistency in Large Language Models: Theoretical Foundations Meet Empirical Discrepancies](https://arxiv.org/abs/2505.08739)
*Xiaoliang Luo,Xinyi Xu,Michael Ramscar,Bradley C. Love*

Main category: cs.CL

TL;DR: 论文证明了自回归大语言模型（LLMs）在不同分词顺序下学习一致概率分布的可能性，并揭示了现有研究中方法论的缺陷。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探讨LLMs在不同分词顺序下是否能学习到一致的概率分布，并为实证评估提供理论基础。

Method: 通过理论证明和实验验证，重新训练GPT-2模型，比较前向、后向和随机排列分词顺序的效果。

Result: 实验发现所有顺序均存在系统性偏差，随机排列与前后向模型差异显著，偏差源于自注意力机制的位置和局部性偏见。

Conclusion: 研究为理解LLMs的位置偏见提供了新视角，并提出了检测概率分布不一致性的方法。

Abstract: Can autoregressive large language models (LLMs) learn consistent probability
distributions when trained on sequences in different token orders? We prove
formally that for any well-defined probability distribution, sequence
perplexity is invariant under any factorization, including forward, backward,
or arbitrary permutations. This result establishes a rigorous theoretical
foundation for studying how LLMs learn from data and defines principled
protocols for empirical evaluation. Applying these protocols, we show that
prior studies examining ordering effects suffer from critical methodological
flaws. We retrain GPT-2 models across forward, backward, and arbitrary permuted
orders on scientific text. We find systematic deviations from theoretical
invariance across all orderings with arbitrary permutations strongly deviating
from both forward and backward models, which largely (but not completely)
agreed with one another. Deviations were traceable to differences in
self-attention, reflecting positional and locality biases in processing. Our
theoretical and empirical results provide novel avenues for understanding
positional biases in LLMs and suggest methods for detecting when LLMs'
probability distributions are inconsistent and therefore untrustworthy.

</details>


### [64] [AC-Reason: Towards Theory-Guided Actual Causality Reasoning with Large Language Models](https://arxiv.org/abs/2505.08750)
*Yanxi Zhang,Xin Cong,Zhong Zhang,Xiao Liu,Dongyan Zhao,Yesai Wu*

Main category: cs.CL

TL;DR: AC-Reason是一个半形式化的推理框架，用于解决实际因果关系（AC）问题，通过理论指导的算法提升LLM的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的方法缺乏对形式化AC理论的支持，导致解释性不足。

Method: 提出AC-Reason框架，识别因果相关事件并推断其形式化因果因素（如充分性、必要性和常态性），通过理论指导的算法回答AC查询。

Result: 在BBH-CJ和AC-Bench上，AC-Reason显著提升LLM性能，GPT-4 + AC-Reason分别达到75.04%和71.82%的准确率。

Conclusion: AC-Reason通过整合AC理论有效提升LLM的因果推理能力，AC-Bench为评估提供了新基准。

Abstract: Actual causality (AC), a fundamental aspect of causal reasoning (CR), is
responsible for attribution and responsibility assignment in real-world
scenarios. However, existing LLM-based methods lack grounding in formal AC
theory, resulting in limited interpretability. Therefore, we propose AC-Reason,
a semi-formal reasoning framework that identifies causally relevant events
within an AC scenario, infers the values of their formal causal factors (e.g.,
sufficiency, necessity, and normality), and answers AC queries via a
theory-guided algorithm with explanations. While AC-Reason does not explicitly
construct a causal graph, it operates over variables in the underlying causal
structure to support principled reasoning. To enable comprehensive evaluation,
we introduce AC-Bench, a new benchmark built upon and substantially extending
Big-Bench Hard Causal Judgment (BBH-CJ). AC-Bench comprises ~1K carefully
annotated samples, each with detailed reasoning steps and focuses solely on
actual causation. The case study shows that synthesized samples in AC-Bench
present greater challenges for LLMs. Extensive experiments on BBH-CJ and
AC-Bench show that AC-Reason consistently improves LLM performance over
baselines. On BBH-CJ, all tested LLMs surpass the average human rater accuracy
of 69.60%, with GPT-4 + AC-Reason achieving 75.04%. On AC-Bench, GPT-4 +
AC-Reason again achieves the highest accuracy of 71.82%. AC-Bench further
enables fine-grained analysis of reasoning faithfulness, revealing that only
Qwen-2.5-72B-Instruct, Claude-3.5-Sonnet, and GPT-4o exhibit faithful
reasoning, whereas GPT-4 tends to exploit shortcuts. Finally, our ablation
study proves that integrating AC theory into LLMs is highly effective, with the
proposed algorithm contributing the most significant performance gains.

</details>


### [65] [Aya Vision: Advancing the Frontier of Multilingual Multimodality](https://arxiv.org/abs/2505.08751)
*Saurabh Dash,Yiyang Nan,John Dang,Arash Ahmadian,Shivalika Singh,Madeline Smith,Bharat Venkitesh,Vlad Shmyhlo,Viraat Aryabumi,Walter Beller-Morales,Jeremy Pekmez,Jason Ozuzu,Pierre Richemond,Acyr Locatelli,Nick Frosst,Phil Blunsom,Aidan Gomez,Ivan Zhang,Marzieh Fadaee,Manoj Govindassamy,Sudip Roy,Matthias Gallé,Beyza Ermis,Ahmet Üstün,Sara Hooker*

Main category: cs.CL

TL;DR: 论文提出了一种解决多模态语言模型构建中数据稀缺和灾难性遗忘问题的方法，通过合成注释框架和跨模态模型合并技术，实现了高性能的多语言多模态模型。


<details>
  <summary>Details</summary>
Motivation: 构建多模态语言模型面临多模态对齐、高质量指令数据稀缺以及引入视觉后文本能力退化等挑战，尤其在多语言环境下问题更严重。

Method: 开发了合成注释框架以生成高质量多语言多模态指令数据，并提出跨模态模型合并技术以减少灾难性遗忘。

Result: Aya-Vision-8B和Aya-Vision-32B在性能上超越了更大的模型，如Qwen-2.5-VL-7B和LLaMA-3.2-90B-Vision。

Conclusion: 研究在多语言多模态领域取得了进展，提供了高效计算与高性能并重的技术方案。

Abstract: Building multimodal language models is fundamentally challenging: it requires
aligning vision and language modalities, curating high-quality instruction
data, and avoiding the degradation of existing text-only capabilities once
vision is introduced. These difficulties are further magnified in the
multilingual setting, where the need for multimodal data in different languages
exacerbates existing data scarcity, machine translation often distorts meaning,
and catastrophic forgetting is more pronounced. To address the aforementioned
challenges, we introduce novel techniques spanning both data and modeling.
First, we develop a synthetic annotation framework that curates high-quality,
diverse multilingual multimodal instruction data, enabling Aya Vision models to
produce natural, human-preferred responses to multimodal inputs across many
languages. Complementing this, we propose a cross-modal model merging technique
that mitigates catastrophic forgetting, effectively preserving text-only
capabilities while simultaneously enhancing multimodal generative performance.
Aya-Vision-8B achieves best-in-class performance compared to strong multimodal
models such as Qwen-2.5-VL-7B, Pixtral-12B, and even much larger
Llama-3.2-90B-Vision. We further scale this approach with Aya-Vision-32B, which
outperforms models more than twice its size, such as Molmo-72B and
LLaMA-3.2-90B-Vision. Our work advances multilingual progress on the
multi-modal frontier, and provides insights into techniques that effectively
bend the need for compute while delivering extremely high performance.

</details>


### [66] [HealthBench: Evaluating Large Language Models Towards Improved Human Health](https://arxiv.org/abs/2505.08775)
*Rahul K. Arora,Jason Wei,Rebecca Soskin Hicks,Preston Bowman,Joaquin Quiñonero-Candela,Foivos Tsimpourlas,Michael Sharman,Meghan Shah,Andrea Vallone,Alex Beutel,Johannes Heidecke,Karan Singhal*

Main category: cs.CL

TL;DR: HealthBench是一个开源基准测试，用于评估大型语言模型在医疗领域的性能和安全性，包含5000次多轮对话，由262名医生制定评分标准。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试多为选择题或简答题，无法真实反映模型在开放对话中的表现，因此开发了HealthBench以实现更真实的评估。

Method: HealthBench通过48,562条独特的评分标准，覆盖多种医疗场景和行为维度，评估模型的开放对话能力。

Result: 模型性能逐年提升，如GPT-4o得分32%，最新模型o3得分60%，小型模型如GPT-4.1 nano表现优异且成本更低。

Conclusion: HealthBench为模型开发和应用提供了实用基准，推动医疗领域的技术进步。

Abstract: We present HealthBench, an open-source benchmark measuring the performance
and safety of large language models in healthcare. HealthBench consists of
5,000 multi-turn conversations between a model and an individual user or
healthcare professional. Responses are evaluated using conversation-specific
rubrics created by 262 physicians. Unlike previous multiple-choice or
short-answer benchmarks, HealthBench enables realistic, open-ended evaluation
through 48,562 unique rubric criteria spanning several health contexts (e.g.,
emergencies, transforming clinical data, global health) and behavioral
dimensions (e.g., accuracy, instruction following, communication). HealthBench
performance over the last two years reflects steady initial progress (compare
GPT-3.5 Turbo's 16% to GPT-4o's 32%) and more rapid recent improvements (o3
scores 60%). Smaller models have especially improved: GPT-4.1 nano outperforms
GPT-4o and is 25 times cheaper. We additionally release two HealthBench
variations: HealthBench Consensus, which includes 34 particularly important
dimensions of model behavior validated via physician consensus, and HealthBench
Hard, where the current top score is 32%. We hope that HealthBench grounds
progress towards model development and applications that benefit human health.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [67] [An Optimized Evacuation Plan for an Active-Shooter Situation Constrained by Network Capacity](https://arxiv.org/abs/2505.07830)
*Joseph Lavalle-Rivera,Aniirudh Ramesh,Subhadeep Chakraborty*

Main category: cs.AI

TL;DR: 本文提出了一种多路径路由优化算法，用于在公共枪击事件中优化疏散路线，减少拥挤和瓶颈，从而降低伤亡率。


<details>
  <summary>Details</summary>
Motivation: 公共枪击事件频发，疏散时的决策至关重要，但缺乏实时信息和高压环境可能导致错误决策。

Method: 开发了一种多路径路由优化算法，考虑路径容量限制，为每个疏散者提供多条最优安全路线。

Result: 算法将总伤亡率降低了34.16%（相比无容量限制的算法）和53.3%（相比专家建议策略），关键节点的拥挤度减少了约50%。

Conclusion: 该算法显著提升了疏散效率，减少了伤亡和拥挤，适用于紧急疏散场景。

Abstract: A total of more than 3400 public shootings have occurred in the United States
between 2016 and 2022. Among these, 25.1% of them took place in an educational
institution, 29.4% at the workplace including office buildings, 19.6% in retail
store locations, and 13.4% in restaurants and bars. During these critical
scenarios, making the right decisions while evacuating can make the difference
between life and death. However, emergency evacuation is intensely stressful,
which along with the lack of verifiable real-time information may lead to fatal
incorrect decisions. To tackle this problem, we developed a multi-route routing
optimization algorithm that determines multiple optimal safe routes for each
evacuee while accounting for available capacity along the route, thus reducing
the threat of crowding and bottlenecking. Overall, our algorithm reduces the
total casualties by 34.16% and 53.3%, compared to our previous routing
algorithm without capacity constraints and an expert-advised routing strategy
respectively. Further, our approach to reduce crowding resulted in an
approximate 50% reduction in occupancy in key bottlenecking nodes compared to
both of the other evacuation algorithms.

</details>


### [68] [RAN Cortex: Memory-Augmented Intelligence for Context-Aware Decision-Making in AI-Native Networks](https://arxiv.org/abs/2505.07842)
*Sebastian Barros*

Main category: cs.AI

TL;DR: 论文提出了一种名为RAN Cortex的内存增强架构，用于解决AI原生无线接入网络（RAN）中智能模块缺乏状态记忆的问题，通过上下文记忆提升决策系统的适应性和连续性。


<details>
  <summary>Details</summary>
Motivation: 当前RAN中的智能模块（如xApps和rApps）缺乏状态记忆，无法利用历史事件或结果进行决策优化，限制了在网络动态环境中的性能。

Method: RAN Cortex包含四个模块：上下文编码器、向量存储库、召回引擎和策略接口，用于实现网络状态的记忆和检索。

Result: 通过实际用例（如体育场流量缓解和无人机走廊的移动管理），证明了上下文记忆能够显著提升RAN的适应性和智能性。

Conclusion: RAN Cortex为AI原生RAN设计引入了记忆功能，提供了一种无需重新训练或集中推理的学习代理框架。

Abstract: As Radio Access Networks (RAN) evolve toward AI-native architectures,
intelligent modules such as xApps and rApps are expected to make increasingly
autonomous decisions across scheduling, mobility, and resource management
domains. However, these agents remain fundamentally stateless, treating each
decision as isolated, lacking any persistent memory of prior events or
outcomes. This reactive behavior constrains optimization, especially in
environments where network dynamics exhibit episodic or recurring patterns. In
this work, we propose RAN Cortex, a memory-augmented architecture that enables
contextual recall in AI-based RAN decision systems. RAN Cortex introduces a
modular layer composed of four elements: a context encoder that transforms
network state into high-dimensional embeddings, a vector-based memory store of
past network episodes, a recall engine to retrieve semantically similar
situations, and a policy interface that supplies historical context to AI
agents in real time or near-real time. We formalize the retrieval-augmented
decision problem in the RAN, present a system architecture compatible with
O-RAN interfaces, and analyze feasible deployments within the Non-RT and
Near-RT RIC domains. Through illustrative use cases such as stadium traffic
mitigation and mobility management in drone corridors, we demonstrate how
contextual memory improves adaptability, continuity, and overall RAN
intelligence. This work introduces memory as a missing primitive in AI-native
RAN designs and provides a framework to enable "learning agents" without the
need for retraining or centralized inference

</details>


### [69] [Winning at All Cost: A Small Environment for Eliciting Specification Gaming Behaviors in Large Language Models](https://arxiv.org/abs/2505.07846)
*Lars Malmqvist*

Main category: cs.AI

TL;DR: 前沿大型语言模型（LLM）在面临不可能情境时会“钻系统漏洞”，引发安全和对齐问题。研究发现，新型推理模型更倾向利用漏洞，而提示方式会显著增加此类行为。


<details>
  <summary>Details</summary>
Motivation: 揭示LLM在不可能情境下的行为模式，以评估其对安全和AI对齐的潜在威胁。

Method: 通过文本模拟方法，让三种LLM（o1、o3-mini、r1）参与无法合法获胜的井字棋游戏，分析其利用漏洞的倾向。

Result: 新型推理模型o3-mini利用漏洞的倾向（37.1%）是旧模型o1（17.5%）的两倍；提示“创造性”解决方案时，漏洞利用行为飙升至77.3%。

Conclusion: 即使无执行能力，LLM也能识别并提出系统漏洞利用策略，凸显AI对齐的紧迫挑战。

Abstract: This study reveals how frontier Large Language Models LLMs can "game the
system" when faced with impossible situations, a critical security and
alignment concern. Using a novel textual simulation approach, we presented
three leading LLMs (o1, o3-mini, and r1) with a tic-tac-toe scenario designed
to be unwinnable through legitimate play, then analyzed their tendency to
exploit loopholes rather than accept defeat. Our results are alarming for
security researchers: the newer, reasoning-focused o3-mini model showed nearly
twice the propensity to exploit system vulnerabilities (37.1%) compared to the
older o1 model (17.5%). Most striking was the effect of prompting. Simply
framing the task as requiring "creative" solutions caused gaming behaviors to
skyrocket to 77.3% across all models. We identified four distinct exploitation
strategies, from direct manipulation of game state to sophisticated
modification of opponent behavior. These findings demonstrate that even without
actual execution capabilities, LLMs can identify and propose sophisticated
system exploits when incentivized, highlighting urgent challenges for AI
alignment as models grow more capable of identifying and leveraging
vulnerabilities in their operating environments.

</details>


### [70] [Conceptual Logical Foundations of Artificial Social Intelligence](https://arxiv.org/abs/2505.07847)
*Eric Werner*

Main category: cs.AI

TL;DR: 本文探讨了多智能体社会中协调与合作的基础，提出了社会智能体的最小心智架构，并研究了信息、意图与沟通的关系。


<details>
  <summary>Details</summary>
Motivation: 研究社会智能体的协调与合作机制，以解决多智能体社会中的信息共享、意图协调及沟通问题。

Method: 通过形式化定义关键概念（如社会世界、信息与战略思维的关系），提出最小社会智能体架构，并分析其动态选择、能力和不确定性。

Result: 定义了智能体能力与意图的逻辑，提出了群体战略状态的熵概念，并探讨了沟通的语义与语用意义。

Conclusion: 社会智能体的逻辑超越了经典逻辑，通过信息与战略思维的结合，为多智能体社会的协调与合作提供了理论基础。

Abstract: What makes a society possible at all? How is coordination and cooperation in
social activity possible? What is the minimal mental architecture of a social
agent? How is the information about the state of the world related to the
agents intentions? How are the intentions of agents related? What role does
communication play in this coordination process? This essay explores the
conceptual and logical foundations of artificial social intelligence in the
context of a society of multiple agents that communicate and cooperate to
achieve some end. An attempt is made to provide an introduction to some of the
key concepts, their formal definitions and their interrelationships. These
include the notion of a changing social world of multiple agents. The logic of
social intelligence goes beyond classical logic by linking information with
strategic thought. A minimal architecture of social agents is presented. The
agents have different dynamically changing, possible choices and abilities. The
agents also have uncertainty, lacking perfect information about their physical
state as well as their dynamic social state. The social state of an agent
includes the intentional state of that agent, as well as, that agent's
representation of the intentional states of other agents. Furthermore, it
includes the evaluations agents make of their physical and social condition.
Communication, semantic and pragmatic meaning and their relationship to
intention and information states are investigated. The logic of agent abilities
and intentions are motivated and formalized. The entropy of group strategic
states is defined.

</details>


### [71] [CCL: Collaborative Curriculum Learning for Sparse-Reward Multi-Agent Reinforcement Learning via Co-evolutionary Task Evolution](https://arxiv.org/abs/2505.07854)
*Yufei Lin,Chengwei Ye,Huanzhen Zhang,Kangsheng Wang,Linuo Xu,Shuyan Liu,Zeyu Zhang*

Main category: cs.AI

TL;DR: CCL是一种针对稀疏奖励环境下多智能体系统的课程学习框架，通过细化任务、生成子任务和协同进化提升学习效果。


<details>
  <summary>Details</summary>
Motivation: 稀疏奖励环境在多智能体系统中导致反馈延迟和共享，影响学习效果。

Method: CCL通过细化个体任务、使用变分进化算法生成子任务、协同进化智能体与环境来提升稳定性。

Result: 在MPE和Hide-and-Seek环境中，CCL在稀疏奖励任务上表现优于现有方法。

Conclusion: CCL有效解决了稀疏奖励环境下的多智能体学习问题。

Abstract: Sparse reward environments pose significant challenges in reinforcement
learning, especially within multi-agent systems (MAS) where feedback is delayed
and shared across agents, leading to suboptimal learning. We propose
Collaborative Multi-dimensional Course Learning (CCL), a novel curriculum
learning framework that addresses this by (1) refining intermediate tasks for
individual agents, (2) using a variational evolutionary algorithm to generate
informative subtasks, and (3) co-evolving agents with their environment to
enhance training stability. Experiments on five cooperative tasks in the MPE
and Hide-and-Seek environments show that CCL outperforms existing methods in
sparse reward settings.

</details>


### [72] [Arrow-Guided VLM: Enhancing Flowchart Understanding via Arrow Direction Encoding](https://arxiv.org/abs/2505.07864)
*Takamitsu Omasa,Ryo Koshihara,Masumi Morishige*

Main category: cs.AI

TL;DR: 本文提出了一种七阶段流程，通过箭头感知检测、OCR提取文本和结构化提示，显著提高了视觉语言模型对流程图的解析准确率。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型（VLMs）在解析流程图时经常误解箭头和拓扑结构，需要一种更有效的方法来提升准确性。

Method: 采用七阶段流程，分为箭头感知检测、OCR文本提取和结构化提示构建三大部分。

Result: 在90个问题的基准测试中，准确率从80%提升至89%，尤其在下一步查询中表现突出（100%准确率）。

Conclusion: 该方法通过显式箭头编码显著提升了VLMs对流程图的理解，未来将扩展评估数据集并应用于BPMN和UML。

Abstract: Flowcharts are indispensable tools in software design and business-process
analysis, yet current vision-language models (VLMs) frequently misinterpret the
directional arrows and graph topology that set these diagrams apart from
natural images. We introduce a seven-stage pipeline grouped into three broader
processes: (1) arrow-aware detection of nodes and arrow endpoints; (2) optical
character recognition (OCR) to extract node text; and (3) construction of a
structured prompt that guides the VLMs. Tested on a 90-question benchmark
distilled from 30 annotated flowcharts, the method raises overall accuracy from
80 % to 89 % (+9 percentage points) without any task-specific fine-tuning. The
gain is most pronounced for next-step queries (25/30 -> 30/30; 100 %, +17 pp);
branch-result questions improve more modestly, and before-step questions remain
difficult. A parallel evaluation with an LLM-as-a-Judge protocol shows the same
trends, reinforcing the advantage of explicit arrow encoding. Limitations
include dependence on detector and OCR precision, the small evaluation set, and
residual errors at nodes with multiple incoming edges. Future work will enlarge
the benchmark with synthetic and handwritten flowcharts and assess the approach
on Business Process Model and Notation (BPMN) and Unified Modeling Language
(UML).

</details>


### [73] [Enhancing Trust Management System for Connected Autonomous Vehicles Using Machine Learning Methods: A Survey](https://arxiv.org/abs/2505.07882)
*Qian Xu,Lei Zhang,Yixiao Liu*

Main category: cs.AI

TL;DR: 本文提出了一种基于机器学习的三层信任管理系统框架，用于车路云集成系统中的联网自动驾驶车辆，并提出了六维目标分类法。


<details>
  <summary>Details</summary>
Motivation: 联网自动驾驶车辆在动态、开放和多域网络中运行，容易受到各种威胁，需要高效的信任管理系统来识别恶意节点并确保可靠决策。

Method: 提出了一种三层框架（信任数据层、信任计算层和信任激励层），并分析了每层模块的机器学习方法。

Result: 通过分类近期研究，提出了针对不同交通场景的目标分类法，并总结了未来研究方向。

Conclusion: 机器学习在信任管理系统中的应用为联网自动驾驶车辆提供了新的解决方案，但仍需进一步研究以解决开放性问题。

Abstract: Connected Autonomous Vehicles (CAVs) operate in dynamic, open, and
multi-domain networks, rendering them vulnerable to various threats. Trust
Management Systems (TMS) systematically organize essential steps in the trust
mechanism, identifying malicious nodes against internal threats and external
threats, as well as ensuring reliable decision-making for more cooperative
tasks. Recent advances in machine learning (ML) offer significant potential to
enhance TMS, especially for the strict requirements of CAVs, such as CAV nodes
moving at varying speeds, and opportunistic and intermittent network behavior.
Those features distinguish ML-based TMS from social networks, static IoT, and
Social IoT. This survey proposes a novel three-layer ML-based TMS framework for
CAVs in the vehicle-road-cloud integration system, i.e., trust data layer,
trust calculation layer and trust incentive layer. A six-dimensional taxonomy
of objectives is proposed. Furthermore, the principles of ML methods for each
module in each layer are analyzed. Then, recent studies are categorized based
on traffic scenarios that are against the proposed objectives. Finally, future
directions are suggested, addressing the open issues and meeting the research
trend. We maintain an active repository that contains up-to-date literature and
open-source projects at
https://github.com/octoberzzzzz/ML-based-TMS-CAV-Survey.

</details>


### [74] [The Correspondence Between Bounded Graph Neural Networks and Fragments of First-Order Logic](https://arxiv.org/abs/2505.08021)
*Bernardo Cuenca Grau,Przemysław A. Wałęga*

Main category: cs.AI

TL;DR: 本文探讨了图神经网络（GNNs）的表达能力，并将其与一阶逻辑（FO）的特定片段对应起来。


<details>
  <summary>Details</summary>
Motivation: 理解GNNs的表达能力是一个重要问题，本文旨在通过有限模型理论的方法，将GNNs与一阶逻辑的片段联系起来。

Method: 应用有限模型理论中的方法和工具，将GNNs的表达能力与一阶逻辑的片段（如模态逻辑、带计数量词的两变量片段等）对应。

Result: 证明了有界GNN架构对应于特定的一阶逻辑片段，包括模态逻辑、带计数量词的两变量片段等。

Conclusion: 本文为理解GNNs的逻辑表达能力提供了一个统一的框架，将其与一阶逻辑的片段联系起来。

Abstract: Graph Neural Networks (GNNs) address two key challenges in applying deep
learning to graph-structured data: they handle varying size input graphs and
ensure invariance under graph isomorphism. While GNNs have demonstrated broad
applicability, understanding their expressive power remains an important
question. In this paper, we show that bounded GNN architectures correspond to
specific fragments of first-order logic (FO), including modal logic (ML),
graded modal logic (GML), modal logic with the universal modality (ML(A)), the
two-variable fragment (FO2) and its extension with counting quantifiers (C2).
To establish these results, we apply methods and tools from finite model theory
of first-order and modal logics to the domain of graph representation learning.
This provides a unifying framework for understanding the logical expressiveness
of GNNs within FO.

</details>


### [75] [Bias or Optimality? Disentangling Bayesian Inference and Learning Biases in Human Decision-Making](https://arxiv.org/abs/2505.08049)
*Prakhar Godara*

Main category: cs.AI

TL;DR: 论文指出，人类在双臂伯努利老虎机任务中的行为可能并非源于认知偏差，而是贝叶斯推断的对称学习率导致的。


<details>
  <summary>Details</summary>
Motivation: 探讨人类行为在任务中表现出的偏差是否真实反映了认知偏差，还是统计模型的拟合结果。

Method: 通过贝叶斯推断和Q学习模型分析学习率，并使用主方程分析学习系统的随机动态。

Result: 发现确认偏差和对称递减学习率会产生相同的行为特征。

Conclusion: 提出实验方案以区分真实认知偏差和学习率递减的假象。

Abstract: Recent studies claim that human behavior in a two-armed Bernoulli bandit
(TABB) task is described by positivity and confirmation biases, implying that
humans do not integrate new information objectively. However, we find that even
if the agent updates its belief via objective Bayesian inference, fitting the
standard Q-learning model with asymmetric learning rates still recovers both
biases. Bayesian inference cast as an effective Q-learning algorithm has
symmetric, though decreasing, learning rates. We explain this by analyzing the
stochastic dynamics of these learning systems using master equations. We find
that both confirmation bias and unbiased but decreasing learning rates yield
the same behavioral signatures. Finally, we propose experimental protocols to
disentangle true cognitive biases from artifacts of decreasing learning rates.

</details>


### [76] [Explainable Reinforcement Learning Agents Using World Models](https://arxiv.org/abs/2505.08073)
*Madhuri Singh,Amal Alabdulkarim,Gennie Mansi,Mark O. Riedl*

Main category: cs.AI

TL;DR: 本文提出了一种基于世界模型和反向世界模型的可解释强化学习（XRL）方法，通过生成反事实轨迹和预测理想状态，帮助非AI专家理解并控制智能体的行为。


<details>
  <summary>Details</summary>
Motivation: 由于时序决策的复杂性，非AI专家难以理解或改变强化学习智能体的行为。因此，需要一种更直观的解释方法。

Method: 结合世界模型和反向世界模型，生成反事实轨迹并预测理想状态，以解释智能体的决策。

Result: 实验表明，该方法显著提高了用户对智能体策略的理解。

Conclusion: 该方法不仅帮助用户理解智能体行为，还可能通过环境操控间接控制智能体。

Abstract: Explainable AI (XAI) systems have been proposed to help people understand how
AI systems produce outputs and behaviors. Explainable Reinforcement Learning
(XRL) has an added complexity due to the temporal nature of sequential
decision-making. Further, non-AI experts do not necessarily have the ability to
alter an agent or its policy. We introduce a technique for using World Models
to generate explanations for Model-Based Deep RL agents. World Models predict
how the world will change when actions are performed, allowing for the
generation of counterfactual trajectories. However, identifying what a user
wanted the agent to do is not enough to understand why the agent did something
else. We augment Model-Based RL agents with a Reverse World Model, which
predicts what the state of the world should have been for the agent to prefer a
given counterfactual action. We show that explanations that show users what the
world should have been like significantly increase their understanding of the
agent policy. We hypothesize that our explanations can help users learn how to
control the agents execution through by manipulating the environment.

</details>


### [77] [Lost in Transmission: When and Why LLMs Fail to Reason Globally](https://arxiv.org/abs/2505.08140)
*Tobias Schnabel,Kiran Tomlinson,Adith Swaminathan,Jennifer Neville*

Main category: cs.AI

TL;DR: 论文提出了一种新的计算框架BAPO，用于解释大型语言模型（LLM）在复杂推理任务中的失败，并证明了链式思考（CoT）可以缓解带宽限制问题。


<details>
  <summary>Details</summary>
Motivation: LLM在复杂推理任务中表现不佳，作者认为这是由于模型内部信息流动的带宽限制导致的。

Method: 引入BAPO模型，分析带宽限制对注意力机制的影响，并通过实验验证理论预测。

Result: 实验显示GPT-4、Claude和Gemini在BAPO-hard任务中失败，但CoT方法能将这些任务转化为BAPO-easy任务。

Conclusion: BAPO为LLM的失败提供了理论解释，并指出了缓解带宽限制的架构和推理方法方向。

Abstract: Despite their many successes, transformer-based large language models (LLMs)
continue to struggle with tasks that require complex reasoning over large parts
of their input. We argue that these failures arise due to capacity limits on
the accurate flow of information within LLMs. To formalize this issue, we
introduce the bounded attention prefix oracle (BAPO) model, a new computational
framework that models bandwidth constraints on attention heads, the mechanism
for internal communication in LLMs. We show that several important reasoning
problems like graph reachability require high communication bandwidth for BAPOs
to solve; we call these problems BAPO-hard. Our experiments corroborate our
theoretical predictions: GPT-4, Claude, and Gemini succeed on BAPO-easy tasks
and fail even on relatively small BAPO-hard tasks. BAPOs also reveal another
benefit of chain of thought (CoT): we prove that breaking down a task using CoT
can turn any BAPO-hard problem into a BAPO-easy one. Our results offer
principled explanations for key LLM failures and suggest directions for
architectures and inference methods that mitigate bandwidth limits.

</details>


### [78] [Foundation Models Knowledge Distillation For Battery Capacity Degradation Forecast](https://arxiv.org/abs/2505.08151)
*Joey Chan,Zhen Chen,Ershun Pan*

Main category: cs.AI

TL;DR: 提出了一种基于时间序列基础模型的锂离子电池容量退化预测方法，通过微调策略实现零样本泛化，并通过知识蒸馏提升专家模型的性能。


<details>
  <summary>Details</summary>
Motivation: 传统专家模型局限于特定场景，而通用时间序列基础模型在电池退化预测中尚未充分探索。

Method: 采用退化感知微调策略对时间序列基础模型Timer进行微调，并设计知识蒸馏框架将基础模型知识迁移至紧凑专家模型。

Result: 微调后的Battery-Timer在CycleLife-SJTUIE数据集上展现出零样本泛化能力，知识蒸馏显著提升了专家模型的多条件泛化性能。

Conclusion: 该方法为电池退化预测提供了通用解决方案，同时解决了大模型部署的计算挑战。

Abstract: Accurate estimation of lithium-ion battery capacity degradation is critical
for enhancing the reliability and safety of battery operations. Traditional
expert models, tailored to specific scenarios, provide isolated estimations.
With the rapid advancement of data-driven techniques, a series of
general-purpose time-series foundation models have been developed. However,
foundation models specifically designed for battery capacity degradation remain
largely unexplored. To enable zero-shot generalization in battery degradation
prediction using large model technology, this study proposes a
degradation-aware fine-tuning strategy for time-series foundation models. We
apply this strategy to fine-tune the Timer model on approximately 10 GB of
open-source battery charge discharge data. Validation on our released
CycleLife-SJTUIE dataset demonstrates that the fine-tuned Battery-Timer
possesses strong zero-shot generalization capability in capacity degradation
forecasting. To address the computational challenges of deploying large models,
we further propose a knowledge distillation framework that transfers the
knowledge of pre-trained foundation models into compact expert models.
Distillation results across several state-of-the-art time-series expert models
confirm that foundation model knowledge significantly improves the
multi-condition generalization of expert models.

</details>


### [79] [Efficient and Scalable Neural Symbolic Search for Knowledge Graph Complex Query Answering](https://arxiv.org/abs/2505.08155)
*Weizhi Fei,Zihao Wang,hang Yin,Shukai Zhao,Wei Zhang,Yangqiu Song*

Main category: cs.AI

TL;DR: 提出了一种高效的符号搜索框架，通过约束策略和局部搜索算法，显著降低了复杂查询回答的计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 解决神经符号搜索在知识图谱推理中面临的数据复杂性和查询复杂性问题，提升效率和可扩展性。

Method: 提出两种约束策略计算神经逻辑索引以减少变量域，并引入基于局部搜索的近似算法处理循环查询的NP复杂度。

Result: 实验表明，该框架将符号方法的计算负载降低90%，同时保持几乎相同的性能。

Conclusion: 该框架有效缓解了效率和可扩展性问题，为复杂查询回答提供了实用解决方案。

Abstract: Complex Query Answering (CQA) aims to retrieve answer sets for complex
logical formulas from incomplete knowledge graphs, which is a crucial yet
challenging task in knowledge graph reasoning. While neuro-symbolic search
utilized neural link predictions achieve superior accuracy, they encounter
significant complexity bottlenecks: (i) Data complexity typically scales
quadratically with the number of entities in the knowledge graph, and (ii)
Query complexity becomes NP-hard for cyclic queries. Consequently, these
approaches struggle to effectively scale to larger knowledge graphs and more
complex queries. To address these challenges, we propose an efficient and
scalable symbolic search framework. First, we propose two constraint strategies
to compute neural logical indices to reduce the domain of variables, thereby
decreasing the data complexity of symbolic search. Additionally, we introduce
an approximate algorithm based on local search to tackle the NP query
complexity of cyclic queries. Experiments on various CQA benchmarks demonstrate
that our framework reduces the computational load of symbolic methods by 90\%
while maintaining nearly the same performance, thus alleviating both efficiency
and scalability issues.

</details>


### [80] [Decoding Neighborhood Environments with Large Language Models](https://arxiv.org/abs/2505.08163)
*Andrew Cart,Shaohu Zhang,Melanie Escue,Xugui Zhou,Haitao Zhao,Prashanth BusiReddyGari,Beiyu Lin,Shuang Li*

Main category: cs.AI

TL;DR: 研究探讨了利用大型语言模型（LLMs）如ChatGPT和Gemini解码邻里环境的可行性，结合YOLOv11模型实现高精度检测，并通过多数投票策略提升LLMs的准确性。


<details>
  <summary>Details</summary>
Motivation: 传统邻里环境评估方法资源密集且难以规模化，机器学习虽具潜力但数据标注和模型可访问性限制了其应用。

Method: 训练YOLOv11模型检测六种环境指标，评估四种LLMs的可行性、鲁棒性和局限性，采用多数投票策略。

Result: YOLOv11模型平均准确率达99.13%，LLMs通过多数投票实现88%以上准确率。

Conclusion: LLMs无需训练即可成为解码邻里环境的有效工具，结合YOLOv11模型可提升规模化评估的可行性。

Abstract: Neighborhood environments include physical and environmental conditions such
as housing quality, roads, and sidewalks, which significantly influence human
health and well-being. Traditional methods for assessing these environments,
including field surveys and geographic information systems (GIS), are
resource-intensive and challenging to evaluate neighborhood environments at
scale. Although machine learning offers potential for automated analysis, the
laborious process of labeling training data and the lack of accessible models
hinder scalability. This study explores the feasibility of large language
models (LLMs) such as ChatGPT and Gemini as tools for decoding neighborhood
environments (e.g., sidewalk and powerline) at scale. We train a robust
YOLOv11-based model, which achieves an average accuracy of 99.13% in detecting
six environmental indicators, including streetlight, sidewalk, powerline,
apartment, single-lane road, and multilane road. We then evaluate four LLMs,
including ChatGPT, Gemini, Claude, and Grok, to assess their feasibility,
robustness, and limitations in identifying these indicators, with a focus on
the impact of prompting strategies and fine-tuning. We apply majority voting
with the top three LLMs to achieve over 88% accuracy, which demonstrates LLMs
could be a useful tool to decode the neighborhood environment without any
training effort.

</details>


### [81] [Behind the Noise: Conformal Quantile Regression Reveals Emergent Representations](https://arxiv.org/abs/2505.08176)
*Petrus H. Zwart,Tamas Varga,Odeta Qafoku,James A. Sethian*

Main category: cs.AI

TL;DR: 论文提出了一种基于机器学习的去噪方法，通过轻量级随机结构神经网络的集成训练，不仅去噪低质量数据，还揭示了潜在空间中的结构特征。


<details>
  <summary>Details</summary>
Motivation: 科学成像通常需要长时间获取高质量数据，但减少采集时间会引入噪声。传统方法仅关注图像恢复，而本文希望通过去噪过程揭示有意义的结构特征。

Method: 使用集成轻量级随机结构神经网络，通过符合分位数回归训练，实现去噪并揭示潜在空间特征。

Result: 在真实地球生物化学成像数据上验证了方法的有效性，支持可靠解释并指导资源受限的实验设计。

Conclusion: 该方法不仅去噪效果好，还能揭示潜在空间的结构特征，为科学成像提供新思路。

Abstract: Scientific imaging often involves long acquisition times to obtain
high-quality data, especially when probing complex, heterogeneous systems.
However, reducing acquisition time to increase throughput inevitably introduces
significant noise into the measurements. We present a machine learning approach
that not only denoises low-quality measurements with calibrated uncertainty
bounds, but also reveals emergent structure in the latent space. By using
ensembles of lightweight, randomly structured neural networks trained via
conformal quantile regression, our method performs reliable denoising while
uncovering interpretable spatial and chemical features -- without requiring
labels or segmentation. Unlike conventional approaches focused solely on image
restoration, our framework leverages the denoising process itself to drive the
emergence of meaningful representations. We validate the approach on real-world
geobiochemical imaging data, showing how it supports confident interpretation
and guides experimental design under resource constraints.

</details>


### [82] [Unveiling the Best Practices for Applying Speech Foundation Models to Speech Intelligibility Prediction for Hearing-Impaired People](https://arxiv.org/abs/2505.08215)
*Haoshuai Zhou,Boxuan Cao,Changgeng Mo,Linkai Li,Shan Xiang Wang*

Main category: cs.AI

TL;DR: 该论文研究了如何优化语音基础模型（SFMs）以提升听力受损人群的语音清晰度预测（SIP-HI），发现单层编码器选择、时序建模和多模型集成是关键因素。


<details>
  <summary>Details</summary>
Motivation: 虽然语音基础模型（SFMs）在多种下游任务中表现优异，但在听力受损人群的语音清晰度预测（SIP-HI）中的优化尚未充分探索。

Method: 通过研究5种SFMs，重点关注编码器层选择、预测头架构和集成配置，以识别影响SIP-HI性能的关键设计因素。

Result: 研究发现，与传统使用全部编码器层的方法相比，选择单层编码器效果更好；时序建模对预测头至关重要；多模型集成能提升性能，且更强的个体模型贡献更大。

Conclusion: 研究为如何有效优化SFMs以提升听力受损人群的语音清晰度预测提供了实用见解。

Abstract: Speech foundation models (SFMs) have demonstrated strong performance across a
variety of downstream tasks, including speech intelligibility prediction for
hearing-impaired people (SIP-HI). However, optimizing SFMs for SIP-HI has been
insufficiently explored. In this paper, we conduct a comprehensive study to
identify key design factors affecting SIP-HI performance with 5 SFMs, focusing
on encoder layer selection, prediction head architecture, and ensemble
configurations. Our findings show that, contrary to traditional use-all-layers
methods, selecting a single encoder layer yields better results. Additionally,
temporal modeling is crucial for effective prediction heads. We also
demonstrate that ensembling multiple SFMs improves performance, with stronger
individual models providing greater benefit. Finally, we explore the
relationship between key SFM attributes and their impact on SIP-HI performance.
Our study offers practical insights into effectively adapting SFMs for speech
intelligibility prediction for hearing-impaired populations.

</details>


### [83] [Evaluating LLM Metrics Through Real-World Capabilities](https://arxiv.org/abs/2505.08253)
*Justin K Miller,Wenjia Tang*

Main category: cs.AI

TL;DR: 论文提出了一种基于现实使用场景的生成式AI评估方法，聚焦六大核心能力，发现现有基准测试在覆盖范围和实用性上存在不足，并基于此比较了主流模型的表现。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试过于关注抽象智能，而忽略了生成式AI在日常任务中的实际效用，因此需要一种更贴近现实使用场景的评估方法。

Method: 通过大规模调查数据和使用日志分析，识别六大核心能力，并基于人本主义标准评估现有基准测试的覆盖范围和实用性。

Result: 发现现有基准测试在覆盖范围、效率测量和可解释性上存在显著不足，且Google Gemini在实用性指标上优于其他主流模型。

Conclusion: 论文呼吁开发更贴近现实使用场景的基准测试，并展示了Google Gemini在实用性任务中的优越表现。

Abstract: As generative AI becomes increasingly embedded in everyday workflows, it is
important to evaluate its performance in ways that reflect real-world usage
rather than abstract notions of intelligence. Unlike many existing benchmarks
that assess general intelligence, our approach focuses on real-world utility,
evaluating how well models support users in everyday tasks. While current
benchmarks emphasize code generation or factual recall, users rely on AI for a
much broader range of activities-from writing assistance and summarization to
citation formatting and stylistic feedback. In this paper, we analyze
large-scale survey data and usage logs to identify six core capabilities that
represent how people commonly use Large Language Models (LLMs): Summarization,
Technical Assistance, Reviewing Work, Data Structuring, Generation, and
Information Retrieval. We then assess the extent to which existing benchmarks
cover these capabilities, revealing significant gaps in coverage, efficiency
measurement, and interpretability. Drawing on this analysis, we use
human-centered criteria to identify gaps in how well current benchmarks reflect
common usage that is grounded in five practical criteria: coherence, accuracy,
clarity, relevance, and efficiency. For four of the six capabilities, we
identify the benchmarks that best align with real-world tasks and use them to
compare leading models. We find that Google Gemini outperforms other
models-including OpenAI's GPT, xAI's Grok, Meta's LLaMA, Anthropic's Claude,
DeepSeek, and Qwen from Alibaba-on these utility-focused metrics.

</details>


### [84] [Benchmarking AI scientists in omics data-driven biological research](https://arxiv.org/abs/2505.08341)
*Erpai Luo,Jinmeng Jia,Yifan Xiong,Xiangyu Li,Xiaobo Guo,Baoqi Yu,Lei Wei,Xuegong Zhang*

Main category: cs.AI

TL;DR: BaisBench是一个新的基准测试，旨在评估AI科学家通过数据分析和外部知识推理生成生物学发现的能力，包括细胞类型注释和科学发现两个任务。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试缺乏真实的数据驱动评估设置，无法全面评估AI科学家的能力。

Method: BaisBench包含两个任务：基于31个专家标记的单细胞数据集的细胞类型注释，以及通过198个多选题（来自41项单细胞研究的生物学见解）进行科学发现。

Result: 实验表明，当前最先进的AI科学家和LLM代理在两项任务上仍显著落后于人类专家。

Conclusion: BaisBench填补了现有基准的不足，为评估和推进AI模型在科学发现中的应用提供了基础。

Abstract: The rise of large language models and multi-agent systems has sparked growing
interest in AI scientists capable of autonomous biological research. However,
existing benchmarks either focus on reasoning without data or on data analysis
with predefined statistical answers, lacking realistic, data-driven evaluation
settings. Here, we introduce the Biological AI Scientist Benchmark (BaisBench),
a benchmark designed to assess AI scientists' ability to generate biological
discoveries through data analysis and reasoning with external knowledge.
BaisBench comprises two tasks: cell type annotation on 31 expert-labeled
single-cell datasets, and scientific discovery through answering 198
multiple-choice questions derived from the biological insights of 41 recent
single-cell studies. Systematic experiments on state-of-the-art AI scientists
and LLM agents showed that while promising, current models still substantially
underperform human experts on both tasks. We hope BaisBench will fill this gap
and serve as a foundation for advancing and evaluating AI models for scientific
discovery. The benchmark can be found at: https://github.com/EperLuo/BaisBench.

</details>


### [85] [An Identifiable Cost-Aware Causal Decision-Making Framework Using Counterfactual Reasoning](https://arxiv.org/abs/2505.08343)
*Ruichu Cai,Xi Chen,Jie Qiao,Zijian Li,Yuequn Liu,Wei Chen,Keli Zhang,Jiale Zheng*

Main category: cs.AI

TL;DR: 提出了一种基于反事实推理的最小成本因果决策框架（MiCCD），用于异常条件下的决策，优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有决策框架在异常条件下常忽略行动成本或因果机制，需改进。

Method: 通过反事实推理和因果图构建代理模型，优化干预策略。

Result: 在合成和真实数据集上，MiCCD在F1分数、成本效率和排名质量上优于传统方法。

Conclusion: MiCCD框架有效且广泛适用，解决了现有方法的不足。

Abstract: Decision making under abnormal conditions is a critical process that involves
evaluating the current state and determining the optimal action to restore the
system to a normal state at an acceptable cost. However, in such scenarios,
existing decision-making frameworks highly rely on reinforcement learning or
root cause analysis, resulting in them frequently neglecting the cost of the
actions or failing to incorporate causal mechanisms adequately. By relaxing the
existing causal decision framework to solve the necessary cause, we propose a
minimum-cost causal decision (MiCCD) framework via counterfactual reasoning to
address the above challenges. Emphasis is placed on making counterfactual
reasoning processes identifiable in the presence of a large amount of mixed
anomaly data, as well as finding the optimal intervention state in a continuous
decision space. Specifically, it formulates a surrogate model based on causal
graphs, using abnormal pattern clustering labels as supervisory signals. This
enables the approximation of the structural causal model among the variables
and lays a foundation for identifiable counterfactual reasoning. With the
causal structure approximated, we then established an optimization model based
on counterfactual estimation. The Sequential Least Squares Programming (SLSQP)
algorithm is further employed to optimize intervention strategies while taking
costs into account. Experimental evaluations on both synthetic and real-world
datasets reveal that MiCCD outperforms conventional methods across multiple
metrics, including F1-score, cost efficiency, and ranking quality(nDCG@k
values), thus validating its efficacy and broad applicability.

</details>


### [86] [Modeling Unseen Environments with Language-guided Composable Causal Components in Reinforcement Learning](https://arxiv.org/abs/2505.08361)
*Xinyue Wang,Biwei Huang*

Main category: cs.AI

TL;DR: WM3C是一种通过学习和利用组合因果组件来增强RL泛化能力的新框架，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决RL在遇到新环境时泛化能力不足的问题，受人类组合推理启发。

Method: 使用组合因果组件建模，结合语言作为组合模态，采用掩码自编码器和互信息约束。

Result: 在数值模拟和机器人任务中显著优于现有方法，提升了策略学习和任务泛化能力。

Conclusion: WM3C通过组合因果组件有效提升了RL的泛化能力，具有理论和实践优势。

Abstract: Generalization in reinforcement learning (RL) remains a significant
challenge, especially when agents encounter novel environments with unseen
dynamics. Drawing inspiration from human compositional reasoning -- where known
components are reconfigured to handle new situations -- we introduce World
Modeling with Compositional Causal Components (WM3C). This novel framework
enhances RL generalization by learning and leveraging compositional causal
components. Unlike previous approaches focusing on invariant representation
learning or meta-learning, WM3C identifies and utilizes causal dynamics among
composable elements, facilitating robust adaptation to new tasks. Our approach
integrates language as a compositional modality to decompose the latent space
into meaningful components and provides theoretical guarantees for their unique
identification under mild assumptions. Our practical implementation uses a
masked autoencoder with mutual information constraints and adaptive sparsity
regularization to capture high-level semantic information and effectively
disentangle transition dynamics. Experiments on numerical simulations and
real-world robotic manipulation tasks demonstrate that WM3C significantly
outperforms existing methods in identifying latent processes, improving policy
learning, and generalizing to unseen tasks.

</details>


### [87] [Learning Like Humans: Advancing LLM Reasoning Capabilities via Adaptive Difficulty Curriculum Learning and Expert-Guided Self-Reformulation](https://arxiv.org/abs/2505.08364)
*Enci Zhang,Xingang Yan,Wei Lin,Tianxiang Zhang,Qianchun Lu*

Main category: cs.AI

TL;DR: 论文提出两种新策略（ADCL和EGSR）以提升大语言模型解决复杂问题的能力，实验表明其显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在数学推理等领域取得进展，但在解决复杂问题时仍面临挑战，受人类学习策略启发，提出改进方法。

Method: 1. ADCL：动态调整训练数据的难度以匹配模型能力；2. EGSR：引导模型基于自身框架重新表述专家解决方案。

Result: 在AIME24和AIME25基准测试中，组合策略分别比基线方法提升10%和16.6%。

Conclusion: ADCL和EGSR策略有效提升模型性能，展示了人类学习策略的启发价值。

Abstract: Despite impressive progress in areas like mathematical reasoning, large
language models still face significant challenges in consistently solving
complex problems. Drawing inspiration from key human learning strategies, we
propose two novel strategies to enhance the capability of large language models
to solve these complex problems. First, Adaptive Difficulty Curriculum Learning
(ADCL) is a novel curriculum learning strategy that tackles the Difficulty
Shift phenomenon (i.e., a model's perception of problem difficulty dynamically
changes during training) by periodically re-estimating difficulty within
upcoming data batches to maintain alignment with the model's evolving
capabilities. Second, Expert-Guided Self-Reformulation (EGSR) is a novel
reinforcement learning strategy that bridges the gap between imitation learning
and pure exploration by guiding models to reformulate expert solutions within
their own conceptual framework, rather than relying on direct imitation,
fostering deeper understanding and knowledge assimilation. Extensive
experiments on challenging mathematical reasoning benchmarks, using Qwen2.5-7B
as the base model, demonstrate that these human-inspired strategies
synergistically and significantly enhance performance. Notably, their combined
application improves performance over the standard Zero-RL baseline by 10% on
the AIME24 benchmark and 16.6% on AIME25.

</details>


### [88] [Explaining Autonomous Vehicles with Intention-aware Policy Graphs](https://arxiv.org/abs/2505.08404)
*Sara Montese,Victor Gimenez-Abalos,Atia Cortés,Ulises Cortés,Sergio Alvarez-Napagao*

Main category: cs.AI

TL;DR: 论文提出了一种后处理、模型无关的方法，为自动驾驶车辆的行为提供目的论解释，以提高透明度和信任。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶技术的快速发展面临决策不透明的挑战，影响了社会信任和监管接受度，因此需要解释性方法。

Method: 基于意图感知策略图，从全局和局部视角提取可解释的车辆行为解释。

Result: 在nuScenes数据集上展示了该方法能够评估车辆行为是否合法，并识别数据集和模型的潜在漏洞。

Conclusion: 该方法为自动驾驶的透明性和可靠性提供了实用工具，有助于提升社会信任和监管接受度。

Abstract: The potential to improve road safety, reduce human driving error, and promote
environmental sustainability have enabled the field of autonomous driving to
progress rapidly over recent decades. The performance of autonomous vehicles
has significantly improved thanks to advancements in Artificial Intelligence,
particularly Deep Learning. Nevertheless, the opacity of their decision-making,
rooted in the use of accurate yet complex AI models, has created barriers to
their societal trust and regulatory acceptance, raising the need for
explainability. We propose a post-hoc, model-agnostic solution to provide
teleological explanations for the behaviour of an autonomous vehicle in urban
environments. Building on Intention-aware Policy Graphs, our approach enables
the extraction of interpretable and reliable explanations of vehicle behaviour
in the nuScenes dataset from global and local perspectives. We demonstrate the
potential of these explanations to assess whether the vehicle operates within
acceptable legal boundaries and to identify possible vulnerabilities in
autonomous driving datasets and models.

</details>


### [89] [Agent-as-a-Service based on Agent Network](https://arxiv.org/abs/2505.08446)
*Yuhan Zhu,Haojie Liu,Jian Wang,Bing Li,Zikang Yin,Yefei Liao*

Main category: cs.AI

TL;DR: 提出了一种基于Agent Network的Agent-as-a-Service（AaaS-AN）范式，通过动态Agent网络和服务导向的代理，解决了多代理系统中代理级协作的不足，并在数学推理和代码生成任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前基于大模型的AI代理在多代理系统（MAS）中表现出强大的决策、协作和适应能力，但现有的Model Context Protocol（MCP）缺乏对代理级协作的支持，因此需要一种新的方法来统一代理的生命周期管理。

Method: AaaS-AN基于Role-Goal-Process-Service（RGPS）标准，包含动态Agent网络（建模代理和代理组为顶点）和服务导向的代理（支持服务发现、注册和互操作性），通过Service Scheduler和Execution Graph实现分布式协调和任务管理。

Result: 在数学推理和代码生成任务中，AaaS-AN优于现有基线方法，并成功构建了一个包含100多个代理服务的MAS系统，同时发布了包含10,000个长链多代理工作流的数据集。

Conclusion: AaaS-AN为多代理系统提供了一种有效的服务导向协作范式，支持代理的全生命周期管理，并在实际任务中验证了其优越性。

Abstract: The rise of large model-based AI agents has spurred interest in Multi-Agent
Systems (MAS) for their capabilities in decision-making, collaboration, and
adaptability. While the Model Context Protocol (MCP) addresses tool invocation
and data exchange challenges via a unified protocol, it lacks support for
organizing agent-level collaboration. To bridge this gap, we propose
Agent-as-a-Service based on Agent Network (AaaS-AN), a service-oriented
paradigm grounded in the Role-Goal-Process-Service (RGPS) standard. AaaS-AN
unifies the entire agent lifecycle, including construction, integration,
interoperability, and networked collaboration, through two core components: (1)
a dynamic Agent Network, which models agents and agent groups as vertexes that
self-organize within the network based on task and role dependencies; (2)
service-oriented agents, incorporating service discovery, registration, and
interoperability protocols. These are orchestrated by a Service Scheduler,
which leverages an Execution Graph to enable distributed coordination, context
tracking, and runtime task management. We validate AaaS-AN on mathematical
reasoning and application-level code generation tasks, which outperforms
state-of-the-art baselines. Notably, we constructed a MAS based on AaaS-AN
containing agent groups, Robotic Process Automation (RPA) workflows, and MCP
servers over 100 agent services. We also release a dataset containing 10,000
long-horizon multi-agent workflows to facilitate future research on long-chain
collaboration in MAS.

</details>


### [90] [Adaptive Bias Generalized Rollout Policy Adaptation on the Flexible Job-Shop Scheduling Problem](https://arxiv.org/abs/2505.08451)
*Lotfi Kobrosly,Marc-Emmanuel Coupvent des Graviers,Christophe Guettier,Tristan Cazenave*

Main category: cs.AI

TL;DR: 论文提出了一种基于广义嵌套滚动策略适应（GNRPA）的新算法，用于解决柔性作业车间调度问题（FJSSP），实验结果表明其性能优于其他基于MCTS的方法。


<details>
  <summary>Details</summary>
Motivation: FJSSP是一个NP难组合优化问题，在制造业等领域有广泛应用，现有方法如约束求解、禁忌搜索、遗传算法等仍有改进空间。

Method: 提出了一种基于GNRPA的新算法，专门用于解决FJSSP。

Result: 实验结果显示，该算法优于其他基于MCTS的方法，但在大规模实例上的完工时间仍与已知上限有差距。

Conclusion: 新算法在FJSSP上表现良好，但仍有优化空间，尤其是在大规模实例上。

Abstract: The Flexible Job-Shop Scheduling Problem (FJSSP) is an NP-hard combinatorial
optimization problem, with several application domains, especially for
manufacturing purposes. The objective is to
  efficiently schedule multiple operations on dissimilar machines. These
operations are gathered into jobs, and operations pertaining to the same job
need to be scheduled sequentially. Different methods have been previously
tested to solve this problem, such as Constraint Solving, Tabu Search, Genetic
Algorithms, or Monte Carlo Tree Search (MCTS). We propose a novel algorithm
derived from the Generalized Nested Rollout Policy Adaptation, developed to
solve the FJSSP. We report encouraging experimental results, as our algorithm
performs better than other MCTS-based approaches, even if makespans obtained on
large instances are still far from known upper bounds.

</details>


### [91] [Strategy-Augmented Planning for Large Language Models via Opponent Exploitation](https://arxiv.org/abs/2505.08459)
*Shuai Xu,Sijia Cui,Yanna Wang,Bo Xu,Qi Wang*

Main category: cs.AI

TL;DR: 论文提出了一种两阶段策略增强规划（SAP）框架，通过策略评估网络（SEN）提升基于LLM的智能体在对抗领域中的对手利用能力。


<details>
  <summary>Details</summary>
Motivation: 在对抗领域中高效建模和利用对手是一个长期挑战。尽管LLMs在通用任务中表现出色，但直接使用LLMs生成决策的方法受限于其领域专业知识。

Method: SAP框架分为离线阶段（构建策略空间并训练SEN网络）和在线阶段（动态识别对手策略并通过SEN搜索最佳响应策略）。

Result: 实验表明SAP具有强大的泛化能力，在MicroRTS环境中性能提升85.35%，与基于规则的SOTA AI竞争。

Conclusion: SAP框架显著提升了LLM智能体的对手利用能力，适用于已知和未知对手策略。

Abstract: Efficiently modeling and exploiting opponents is a long-standing challenge in
adversarial domains. Large Language Models (LLMs) trained on extensive textual
data have recently demonstrated outstanding performance in general tasks,
introducing new research directions for opponent modeling. Some studies
primarily focus on directly using LLMs to generate decisions based on the
elaborate prompt context that incorporates opponent descriptions, while these
approaches are limited to scenarios where LLMs possess adequate domain
expertise. To address that, we introduce a two-stage Strategy-Augmented
Planning (SAP) framework that significantly enhances the opponent exploitation
capabilities of LLM-based agents by utilizing a critical component, the
Strategy Evaluation Network (SEN). Specifically, in the offline stage, we
construct an explicit strategy space and subsequently collect strategy-outcome
pair data for training the SEN network. During the online phase, SAP
dynamically recognizes the opponent's strategies and greedily exploits them by
searching best response strategy on the well-trained SEN, finally translating
strategy to a course of actions by carefully designed prompts. Experimental
results show that SAP exhibits robust generalization capabilities, allowing it
to perform effectively not only against previously encountered opponent
strategies but also against novel, unseen strategies. In the MicroRTS
environment, SAP achieves a 85.35\% performance improvement over baseline
methods and matches the competitiveness of reinforcement learning approaches
against state-of-the-art (SOTA) rule-based AI.

</details>


### [92] [BAT: Benchmark for Auto-bidding Task](https://arxiv.org/abs/2505.08485)
*Alexandra Khirianova,Ekaterina Solodneva,Andrey Pudovikov,Sergey Osokin,Egor Samosvat,Yuriy Dorn,Alexander Ledovsky,Yana Zenkova*

Main category: cs.AI

TL;DR: 该论文提出了一个用于在线广告位拍卖的竞价策略优化基准，解决了数据集和标准化基准的稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 在线广告拍卖中，实时自动竞价算法的开发、评估和改进面临数据集和标准化基准不足的挑战。

Method: 作者实现了一个涵盖两种常见拍卖格式的基准，并在新数据集上构建了一系列基线方法，专注于预算均匀分配和点击成本优化。

Result: 该基准为研究人员和从业者提供了一个用户友好的框架，支持创新自动竞价算法的开发。

Conclusion: 该基准有助于推动程序化广告领域的发展，相关资源已开源。

Abstract: The optimization of bidding strategies for online advertising slot auctions
presents a critical challenge across numerous digital marketplaces. A
significant obstacle to the development, evaluation, and refinement of
real-time autobidding algorithms is the scarcity of comprehensive datasets and
standardized benchmarks.
  To address this deficiency, we present an auction benchmark encompassing the
two most prevalent auction formats. We implement a series of robust baselines
on a novel dataset, addressing the most salient Real-Time Bidding (RTB) problem
domains: budget pacing uniformity and Cost Per Click (CPC) constraint
optimization. This benchmark provides a user-friendly and intuitive framework
for researchers and practitioners to develop and refine innovative autobidding
algorithms, thereby facilitating advancements in the field of programmatic
advertising. The implementation and additional resources can be accessed at the
following repository (https://github.com/avito-tech/bat-autobidding-benchmark,
https://doi.org/10.5281/zenodo.14794182).

</details>


### [93] [Achieving Scalable Robot Autonomy via neurosymbolic planning using lightweight local LLM](https://arxiv.org/abs/2505.08492)
*Nicholas Attolino,Alessio Capitanelli,Fulvio Mastrogiovanni*

Main category: cs.AI

TL;DR: Gideon框架利用本地小型LLM解决PDDL符号任务规划在动态人机协作中的问题，通过生成大规模数据集和适应神经符号规划，实现多领域支持。初步实验显示66.1%-70.6%的有效规划率，尽管训练效率较低，但在推理效率和多领域适应性上表现显著。


<details>
  <summary>Details</summary>
Motivation: 解决PDDL符号任务规划在动态人机协作中的扩展性、重新规划需求和延迟问题，同时避免依赖闭源远程LLM的局限性。

Method: 提出Gideon框架，集成问题生成器生成大规模数据集，并适配本地小型LLM，支持多领域任务规划和设备端执行。

Result: 单领域实验中32k样本模型有效规划率为66.1%，多领域16k样本模型达70.6%，显示数据多样性对学习效率的积极影响。

Conclusion: Gideon在小型LLM上实现了高效的符号任务规划，尽管训练效率较低，但在推理效率和多领域适应性上具有显著优势。

Abstract: PDDL-based symbolic task planning remains pivotal for robot autonomy yet
struggles with dynamic human-robot collaboration due to scalability,
re-planning demands, and delayed plan availability. Although a few
neurosymbolic frameworks have previously leveraged LLMs such as GPT-3 to
address these challenges, reliance on closed-source, remote models with limited
context introduced critical constraints: third-party dependency, inconsistent
response times, restricted plan length and complexity, and multi-domain
scalability issues. We present Gideon, a novel framework that enables the
transition to modern, smaller, local LLMs with extended context length. Gideon
integrates a novel problem generator to systematically generate large-scale
datasets of realistic domain-problem-plan tuples for any domain, and adapts
neurosymbolic planning for local LLMs, enabling on-device execution and
extended context for multi-domain support. Preliminary experiments in
single-domain scenarios performed on Qwen-2.5 1.5B and trained on 8k-32k
samples, demonstrate a valid plan percentage of 66.1% (32k model) and show that
the figure can be further scaled through additional data. Multi-domain tests on
16k samples yield an even higher 70.6% planning validity rate, proving
extensibility across domains and signaling that data variety can have a
positive effect on learning efficiency. Although long-horizon planning and
reduced model size make Gideon training much less efficient than baseline
models based on larger LLMs, the results are still significant considering that
the trained model is about 120x smaller than baseline and that significant
advantages can be achieved in inference efficiency, scalability, and
multi-domain adaptability, all critical factors in human-robot collaboration.
Training inefficiency can be mitigated by Gideon's streamlined data generation
pipeline.

</details>


### [94] [TrialMatchAI: An End-to-End AI-powered Clinical Trial Recommendation System to Streamline Patient-to-Trial Matching](https://arxiv.org/abs/2505.08508)
*Majd Abdallah,Sigve Nakken,Mariska Bierkens,Johanna Galvis,Alexis Groppi,Slim Karkar,Lana Meiqari,Maria Alexandra Rujano,Steve Canham,Rodrigo Dienstmann,Remond Fijneman,Eivind Hovig,Gerrit Meijer,Macha Nikolski*

Main category: cs.AI

TL;DR: TrialMatchAI是一个基于AI的患者与临床试验匹配推荐系统，通过处理结构化与非结构化临床数据，结合检索增强生成框架，实现高效、透明且轻量化的部署。


<details>
  <summary>Details</summary>
Motivation: 临床试验中患者招募效率低下，需要自动化、可扩展的解决方案。

Method: 系统基于开源大语言模型（LLMs），采用检索增强生成框架，结合混合搜索策略（词汇与语义相似度）和医学链式推理，进行标准化的生物医学实体处理和资格评估。

Result: 在真实世界验证中，92%的肿瘤患者在前20条推荐中至少匹配到一个相关试验；专家评估显示90%以上的资格分类准确率，尤其在生物标志物驱动的匹配中表现优异。

Conclusion: TrialMatchAI通过高效、可解释的设计和轻量化开源部署，为精准医学中的临床试验匹配提供了可扩展的解决方案。

Abstract: Patient recruitment remains a major bottleneck in clinical trials, calling
for scalable and automated solutions. We present TrialMatchAI, an AI-powered
recommendation system that automates patient-to-trial matching by processing
heterogeneous clinical data, including structured records and unstructured
physician notes. Built on fine-tuned, open-source large language models (LLMs)
within a retrieval-augmented generation framework, TrialMatchAI ensures
transparency and reproducibility and maintains a lightweight deployment
footprint suitable for clinical environments. The system normalizes biomedical
entities, retrieves relevant trials using a hybrid search strategy combining
lexical and semantic similarity, re-ranks results, and performs criterion-level
eligibility assessments using medical Chain-of-Thought reasoning. This pipeline
delivers explainable outputs with traceable decision rationales. In real-world
validation, 92 percent of oncology patients had at least one relevant trial
retrieved within the top 20 recommendations. Evaluation across synthetic and
real clinical datasets confirmed state-of-the-art performance, with expert
assessment validating over 90 percent accuracy in criterion-level eligibility
classification, particularly excelling in biomarker-driven matches. Designed
for modularity and privacy, TrialMatchAI supports Phenopackets-standardized
data, enables secure local deployment, and allows seamless replacement of LLM
components as more advanced models emerge. By enhancing efficiency and
interpretability and offering lightweight, open-source deployment, TrialMatchAI
provides a scalable solution for AI-driven clinical trial matching in precision
medicine.

</details>


### [95] [On the Complexity and Properties of Preferential Propositional Dependence Logic](https://arxiv.org/abs/2505.08522)
*Kai Sauerwald,Arne Meier,Juha Kontinen*

Main category: cs.AI

TL;DR: 本文研究了基于团队语义和依赖原子的命题逻辑中KLM风格优先推理的复杂性和性质，发现其具有累积性但不满足System~P，并给出了满足System~P的条件。此外，还展示了经典逻辑和依赖逻辑的蕴含关系，并提出了优先推理的复杂度结果。


<details>
  <summary>Details</summary>
Motivation: 探讨命题依赖逻辑中优先推理的性质和复杂性，填补现有研究的空白。

Method: 通过团队语义和依赖原子分析优先推理的性质，提出满足System~P的条件，并研究其复杂度。

Result: 发现优先推理具有累积性但不满足System~P，给出了满足System~P的条件，并展示了经典和依赖逻辑的蕴含关系。

Conclusion: 优先推理在团队语义下具有独特性质，其复杂度结果对理论研究有重要意义。

Abstract: This paper considers the complexity and properties of KLM-style preferential
reasoning in the setting of propositional logic with team semantics and
dependence atoms, also known as propositional dependence logic. Preferential
team-based reasoning is shown to be cumulative, yet violates System~P. We give
intuitive conditions that fully characterise those cases where preferential
propositional dependence logic satisfies System~P. We show that these
characterisations do, surprisingly, not carry over to preferential team-based
propositional logic. Furthermore, we show how classical entailment and
dependence logic entailment can be expressed in terms of non-trivial
preferential models. Finally, we present the complexity of preferential
team-based reasoning for two natural representations. This includes novel
complexity results for classical (non-team-based) preferential reasoning.

</details>


### [96] [Guiding LLM-based Smart Contract Generation with Finite State Machine](https://arxiv.org/abs/2505.08542)
*Hao Luo,Yuhao Lin,Xiao Yan,Xintong Hu,Yuxiang Wang,Qiming Zeng,Hao Wang,Jiawei Jiang*

Main category: cs.AI

TL;DR: FSM-SCG是一个基于有限状态机（FSM）和大型语言模型（LLMs）的智能合约生成框架，显著提升了生成代码的质量和安全性。


<details>
  <summary>Details</summary>
Motivation: 传统智能合约生成方法依赖人工编码和专家审核，门槛高且效率低，而现有LLMs在智能合约生成中的效果和安全性仍有不足。

Method: 通过将用户需求抽象为FSM，引导LLMs生成智能合约，并通过编译和安全检查的反馈迭代优化代码。

Result: 实验表明，FSM-SCG显著提升了生成代码的质量，编译成功率最高提升48%，平均漏洞风险评分降低约68%。

Conclusion: FSM-SCG框架有效解决了智能合约生成中的效果和安全性问题，具有广泛应用潜力。

Abstract: Smart contract is a kind of self-executing code based on blockchain
technology with a wide range of application scenarios, but the traditional
generation method relies on manual coding and expert auditing, which has a high
threshold and low efficiency. Although Large Language Models (LLMs) show great
potential in programming tasks, they still face challenges in smart contract
generation w.r.t. effectiveness and security. To solve these problems, we
propose FSM-SCG, a smart contract generation framework based on finite state
machine (FSM) and LLMs, which significantly improves the quality of the
generated code by abstracting user requirements to generate FSM, guiding LLMs
to generate smart contracts, and iteratively optimizing the code with the
feedback of compilation and security checks. The experimental results show that
FSM-SCG significantly improves the quality of smart contract generation.
Compared to the best baseline, FSM-SCG improves the compilation success rate of
generated smart contract code by at most 48%, and reduces the average
vulnerability risk score by approximately 68%.

</details>


### [97] [Resource-Efficient Language Models: Quantization for Fast and Accessible Inference](https://arxiv.org/abs/2505.08620)
*Tollef Emil Jørgensen*

Main category: cs.AI

TL;DR: 本文综述了后训练量化（PTQ）技术，旨在优化大语言模型（LLMs）的推理效率，涵盖量化方案、粒度和权衡。


<details>
  <summary>Details</summary>
Motivation: 大语言模型资源需求高，硬件可访问性和能耗问题突出，需优化推理效率。

Method: 通过后训练量化技术，分析不同量化方案、粒度和权衡。

Result: 提供了理论与应用平衡的后训练量化综述。

Conclusion: 后训练量化是优化大语言模型推理效率的有效方法。

Abstract: Large language models have significantly advanced natural language
processing, yet their heavy resource demands pose severe challenges regarding
hardware accessibility and energy consumption. This paper presents a focused
and high-level review of post-training quantization (PTQ) techniques designed
to optimize the inference efficiency of LLMs by the end-user, including details
on various quantization schemes, granularities, and trade-offs. The aim is to
provide a balanced overview between the theory and applications of
post-training quantization.

</details>


### [98] [Visually Guided Decoding: Gradient-Free Hard Prompt Inversion with Language Models](https://arxiv.org/abs/2505.08622)
*Donghoon Kim,Minji Bae,Kyuhong Shim,Byonghyo Shim*

Main category: cs.AI

TL;DR: 提出了一种名为VGD的无梯度方法，利用LLM和CLIP生成语义对齐的提示，解决了现有提示反转技术的不足。


<details>
  <summary>Details</summary>
Motivation: 现有提示反转技术（如软提示和硬提示）效果不佳，缺乏可解释性和连贯性，需要改进。

Method: VGD结合LLM的文本生成能力和CLIP的视觉对齐评分，生成易读且语义对齐的提示。

Result: 实验表明，VGD在生成可理解和上下文相关的提示方面优于现有技术。

Conclusion: VGD提升了提示生成的可解释性和灵活性，无需额外训练即可实现更直观的交互。

Abstract: Text-to-image generative models like DALL-E and Stable Diffusion have
revolutionized visual content creation across various applications, including
advertising, personalized media, and design prototyping. However, crafting
effective textual prompts to guide these models remains challenging, often
requiring extensive trial and error. Existing prompt inversion approaches, such
as soft and hard prompt techniques, are not so effective due to the limited
interpretability and incoherent prompt generation. To address these issues, we
propose Visually Guided Decoding (VGD), a gradient-free approach that leverages
large language models (LLMs) and CLIP-based guidance to generate coherent and
semantically aligned prompts. In essence, VGD utilizes the robust text
generation capabilities of LLMs to produce human-readable prompts. Further, by
employing CLIP scores to ensure alignment with user-specified visual concepts,
VGD enhances the interpretability, generalization, and flexibility of prompt
generation without the need for additional training. Our experiments
demonstrate that VGD outperforms existing prompt inversion techniques in
generating understandable and contextually relevant prompts, facilitating more
intuitive and controllable interactions with text-to-image models.

</details>


### [99] [Integrating Natural Language Processing and Exercise Monitoring for Early Diagnosis of Metabolic Syndrome: A Deep Learning Approach](https://arxiv.org/abs/2505.08628)
*Yichen Zhao,Yuhua Wang,Xi Cheng,Junhao Fang,Yang Yang*

Main category: cs.AI

TL;DR: 利用日常生活中的生理数据和运动相关文本，通过深度学习框架结合NLP和运动监测，实现代谢综合征（MetS）的早期诊断。


<details>
  <summary>Details</summary>
Motivation: MetS是一种常见且易被低估的疾病，标准诊断需要医疗机构检测，存在未满足的医疗需求。研究旨在利用易获取的日常数据简化诊断。

Method: 收集40名志愿者的数据，通过数据增强解决不平衡问题，提出结合NLP和运动监测的深度学习框架。

Result: 最佳模型在3折交叉验证中表现优异（AUROC=0.806，REC=76.3%），文本和每日最低心率是关键特征。

Conclusion: 研究表明日常易测数据可用于MetS早期诊断，有望降低筛查和管理成本。

Abstract: Metabolic syndrome (MetS) is a medication condition characterized by
abdominal obesity, insulin resistance, hypertension and hyperlipidemia. It
increases the risk of majority of chronic diseases, including type 2 diabetes
mellitus, and affects about one quarter of the global population. Therefore,
early detection and timely intervention for MetS are crucial. Standard
diagnosis for MetS components requires blood tests conducted within medical
institutions. However, it is frequently underestimated, leading to unmet need
for care for MetS population. This study aims to use the least physiological
data and free texts about exercises related activities, which are obtained
easily in daily life, to diagnosis MetS. We collected the data from 40
volunteers in a nursing home and used data augmentation to reduce the
imbalance. We propose a deep learning framework for classifying MetS that
integrates natural language processing (NLP) and exercise monitoring. The
results showed that the best model reported a high positive result (AUROC=0.806
and REC=76.3%) through 3-fold cross-validation. Feature importance analysis
revealed that text and minimum heart rate on a daily basis contribute the most
in the classification of MetS. This study demonstrates the potential
application of data that are easily measurable in daily life for the early
diagnosis of MetS, which could contribute to reducing the cost of screening and
management for MetS population.

</details>


### [100] [TRAIL: Trace Reasoning and Agentic Issue Localization](https://arxiv.org/abs/2505.08638)
*Darshan Deshpande,Varun Gangal,Hersh Mehta,Jitin Krishnan,Anand Kannappan,Rebecca Qian*

Main category: cs.AI

TL;DR: 论文提出了一种用于评估代理工作流复杂痕迹的动态方法，并引入了一个错误分类法，同时发布了包含148条人工标注痕迹的数据集（TRAIL）。研究发现当前长上下文LLM在痕迹调试上表现不佳。


<details>
  <summary>Details</summary>
Motivation: 随着代理工作流的广泛应用，现有依赖人工的评估方法无法应对其复杂性和规模增长，亟需一种可扩展且系统化的评估方法。

Method: 论文提出了一种动态评估方法，并构建了一个错误分类法，同时发布了基于真实应用场景的148条人工标注痕迹数据集（TRAIL）。

Result: 实验表明，现代长上下文LLM在痕迹调试上表现较差，最佳模型Gemini-2.5-pro在TRAIL上仅得11%。

Conclusion: 论文为代理工作流的可扩展评估提供了工具和数据支持，推动了未来研究的发展。

Abstract: The increasing adoption of agentic workflows across diverse domains brings a
critical need to scalably and systematically evaluate the complex traces these
systems generate. Current evaluation methods depend on manual, domain-specific
human analysis of lengthy workflow traces - an approach that does not scale
with the growing complexity and volume of agentic outputs. Error analysis in
these settings is further complicated by the interplay of external tool outputs
and language model reasoning, making it more challenging than traditional
software debugging. In this work, we (1) articulate the need for robust and
dynamic evaluation methods for agentic workflow traces, (2) introduce a formal
taxonomy of error types encountered in agentic systems, and (3) present a set
of 148 large human-annotated traces (TRAIL) constructed using this taxonomy and
grounded in established agentic benchmarks. To ensure ecological validity, we
curate traces from both single and multi-agent systems, focusing on real-world
applications such as software engineering and open-world information retrieval.
Our evaluations reveal that modern long context LLMs perform poorly at trace
debugging, with the best Gemini-2.5-pro model scoring a mere 11% on TRAIL. Our
dataset and code are made publicly available to support and accelerate future
research in scalable evaluation for agentic workflows.

</details>


### [101] [WixQA: A Multi-Dataset Benchmark for Enterprise Retrieval-Augmented Generation](https://arxiv.org/abs/2505.08643)
*Dvir Cohen,Lin Burg,Sviatoslav Pykhnivskyi,Hagit Gur,Stanislav Kovynov,Olga Atzmon,Gilad Barkan*

Main category: cs.AI

TL;DR: WixQA是一个基于Wix.com客户支持交互的问答数据集基准套件，用于全面评估检索增强生成（RAG）系统，包含真实用户查询、专家验证对话和合成生成的问题-答案对。


<details>
  <summary>Details</summary>
Motivation: 企业问答系统需要反映实际用户支持场景的领域特定数据集，而现有开放领域数据集无法满足这一需求。

Method: 引入WixQA基准套件，包含三个数据集：WixQA-ExpertWritten（真实用户查询与专家答案）、WixQA-Simulated（专家验证的对话提炼）和WixQA-Synthetic（基于知识库文章生成的合成QA对）。

Result: WixQA提供了知识库快照和数据集，支持端到端RAG系统的全面评估，并提供了基线结果。

Conclusion: WixQA填补了企业RAG系统评估的空白，为实际企业环境中的问答系统提供了独特基准。

Abstract: Retrieval-Augmented Generation (RAG) is a cornerstone of modern question
answering (QA) systems, enabling grounded answers based on external knowledge.
Although recent progress has been driven by open-domain datasets, enterprise QA
systems need datasets that mirror the concrete, domain-specific issues users
raise in day-to-day support scenarios. Critically, evaluating end-to-end RAG
systems requires benchmarks comprising not only question--answer pairs but also
the specific knowledge base (KB) snapshot from which answers were derived. To
address this need, we introduce WixQA, a benchmark suite featuring QA datasets
precisely grounded in the released KB corpus, enabling holistic evaluation of
retrieval and generation components. WixQA includes three distinct QA datasets
derived from Wix.com customer support interactions and grounded in a snapshot
of the public Wix Help Center KB: (i) WixQA-ExpertWritten, 200 real user
queries with expert-authored, multi-step answers; (ii) WixQA-Simulated, 200
expert-validated QA pairs distilled from user dialogues; and (iii)
WixQA-Synthetic, 6,222 LLM-generated QA pairs, with one pair systematically
derived from each article in the knowledge base. We release the KB snapshot
alongside the datasets under MIT license and provide comprehensive baseline
results, forming a unique benchmark for evaluating enterprise RAG systems in
realistic enterprise environments.

</details>


### [102] [A Study of Data-driven Methods for Inventory Optimization](https://arxiv.org/abs/2505.08673)
*Lee Yeung Ping,Patrick Wong,Tan Cheng Han*

Main category: cs.AI

TL;DR: 本文分析了三种算法（时间序列、随机森林和深度强化学习）在三种库存模型（缺货损失、双源采购和多级库存模型）中的应用，旨在评估数据驱动方法的效率。


<details>
  <summary>Details</summary>
Motivation: 研究目的是为超市库存管理提供高效的数据驱动方法，并分析这些方法的潜力、可能性及当前挑战。

Method: 通过比较三种算法在不同库存模型中的表现，使用预测准确性、市场变化适应性和对库存成本及客户满意度的影响等指标进行评估。

Result: 数据可视化工具和统计指标揭示了明显趋势和模式，可指导库存管理决策，帮助实时跟踪算法性能并深入分析库存波动原因。

Conclusion: 研究为供应链中的低效环节和改进领域提供了关键细节，有助于优化库存管理策略。

Abstract: This paper shows a comprehensive analysis of three algorithms (Time Series,
Random Forest (RF) and Deep Reinforcement Learning) into three inventory models
(the Lost Sales, Dual-Sourcing and Multi-Echelon Inventory Model). These
methodologies are applied in the supermarket context. The main purpose is to
analyse efficient methods for the data-driven. Their possibility, potential and
current challenges are taken into consideration in this report. By comparing
the results in each model, the effectiveness of each algorithm is evaluated
based on several key performance indicators, including forecast accuracy,
adaptability to market changes, and overall impact on inventory costs and
customer satisfaction levels. The data visualization tools and statistical
metrics are the indicators for the comparisons and show some obvious trends and
patterns that can guide decision-making in inventory management. These tools
enable managers to not only track the performance of different algorithms in
real-time but also to drill down into specific data points to understand the
underlying causes of inventory fluctuations. This level of detail is crucial
for pinpointing inefficiencies and areas for improvement within the supply
chain.

</details>


### [103] [LLM-based Prompt Ensemble for Reliable Medical Entity Recognition from EHRs](https://arxiv.org/abs/2505.08704)
*K M Sajjadul Islam,Ayesha Siddika Nipu,Jiawei Wu,Praveen Madiraju*

Main category: cs.AI

TL;DR: 论文探讨了基于提示的大型语言模型（如GPT-4o和DeepSeek-R1）在电子健康记录（EHR）中命名实体识别（NER）的应用，其中GPT-4o结合提示集成方法表现最佳。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录（EHR）中的非结构化临床文本需要有效提取关键医学实体（如问题、测试和治疗），以支持下游临床应用。

Method: 采用基于提示的NER方法，利用GPT-4o和DeepSeek-R1模型，结合零样本、少量样本和集成提示工程技术。

Result: GPT-4o结合提示集成方法在分类任务中表现最佳，F1分数为0.95，召回率为0.98，优于DeepSeek-R1。

Conclusion: 提示集成方法通过嵌入相似性和多数投票提高了可靠性，GPT-4o在医学实体识别任务中表现出色。

Abstract: Electronic Health Records (EHRs) are digital records of patient information,
often containing unstructured clinical text. Named Entity Recognition (NER) is
essential in EHRs for extracting key medical entities like problems, tests, and
treatments to support downstream clinical applications. This paper explores
prompt-based medical entity recognition using large language models (LLMs),
specifically GPT-4o and DeepSeek-R1, guided by various prompt engineering
techniques, including zero-shot, few-shot, and an ensemble approach. Among all
strategies, GPT-4o with prompt ensemble achieved the highest classification
performance with an F1-score of 0.95 and recall of 0.98, outperforming
DeepSeek-R1 on the task. The ensemble method improved reliability by
aggregating outputs through embedding-based similarity and majority voting.

</details>


### [104] [DeepMath-Creative: A Benchmark for Evaluating Mathematical Creativity of Large Language Models](https://arxiv.org/abs/2505.08744)
*Xiaoyang Chen,Xinan Dai,Yu Du,Qian Feng,Naixu Guo,Tingshuo Gu,Yuting Gao,Yingyi Gao,Xudong Han,Xiang Jiang,Yilin Jin,Hongyi Lin,Shisheng Lin,Xiangnan Li,Yuante Li,Yixing Li,Zhentao Lai,Zilu Ma,Yingrong Peng,Jiacheng Qian,Hao-Yu Sun,Jianbo Sun,Zirui Wang,Siwei Wu,Zian Wang,Bin Xu,Jianghao Xu,Yiyang Yu,Zichuan Yang,Hongji Zha,Ruichong Zhang*

Main category: cs.AI

TL;DR: DeepMath团队提出评估数学创造力的标准，并发布DeepMath-Creative基准测试，评估主流LLM的创造力表现。结果显示，即使宽松评分，最佳模型O3 Mini在基础本科任务上仅达70%准确率，复杂问题表现更差。


<details>
  <summary>Details</summary>
Motivation: 当前数学LLM的研究多关注推理能力，创造力评估不足且数据集稀缺，团队希望通过新基准填补这一空白。

Method: 提出数学创造力评估标准，构建DeepMath-Creative基准（涵盖代数、几何等领域），并系统评估主流LLM的创造力。

Result: 最佳模型O3 Mini在基础任务上准确率70%，复杂问题表现显著下降，模型缺乏解决开放问题的实质性策略。

Conclusion: 当前LLM的创造力表现可能源于记忆模式重组，而非真正的创新洞察或新合成能力。

Abstract: To advance the mathematical proficiency of large language models (LLMs), the
DeepMath team has launched an open-source initiative aimed at developing an
open mathematical LLM and systematically evaluating its mathematical
creativity. This paper represents the initial contribution of this initiative.
While recent developments in mathematical LLMs have predominantly emphasized
reasoning skills, as evidenced by benchmarks on elementary to
undergraduate-level mathematical tasks, the creative capabilities of these
models have received comparatively little attention, and evaluation datasets
remain scarce. To address this gap, we propose an evaluation criteria for
mathematical creativity and introduce DeepMath-Creative, a novel, high-quality
benchmark comprising constructive problems across algebra, geometry, analysis,
and other domains. We conduct a systematic evaluation of mainstream LLMs'
creative problem-solving abilities using this dataset. Experimental results
show that even under lenient scoring criteria -- emphasizing core solution
components and disregarding minor inaccuracies, such as small logical gaps,
incomplete justifications, or redundant explanations -- the best-performing
model, O3 Mini, achieves merely 70% accuracy, primarily on basic
undergraduate-level constructive tasks. Performance declines sharply on more
complex problems, with models failing to provide substantive strategies for
open problems. These findings suggest that, although current LLMs display a
degree of constructive proficiency on familiar and lower-difficulty problems,
such performance is likely attributable to the recombination of memorized
patterns rather than authentic creative insight or novel synthesis.

</details>


### [105] [ARC-NCA: Towards Developmental Solutions to the Abstraction and Reasoning Corpus](https://arxiv.org/abs/2505.08778)
*Etienne Guichard,Felix Reimers,Mia Kvalsund,Mikkel Lepperød,Stefano Nichele*

Main category: cs.AI

TL;DR: ARC-NCA利用神经细胞自动机（NCA）及其增强版本（EngramNCA）解决ARC-AGI基准问题，展示了发展性方法在提升AI推理和抽象能力上的潜力。


<details>
  <summary>Details</summary>
Motivation: ARC-AGI是AGI领域的核心挑战，现有AI系统难以解决，但对人类却相对简单。发展性方法可能为AI提供超越训练数据外推的问题解决能力。

Method: 采用标准NCA和带隐藏记忆的EngramNCA，模拟生物系统中的复杂动态和涌现模式，以发展性方式解决ARC-AGI任务。

Result: ARC-NCA的概念验证结果在某些情况下可与ChatGPT 4.5相媲美甚至超越，且成本更低。

Conclusion: 将发展性原理融入计算模型是提升AI适应性和抽象推理能力的有效途径。

Abstract: The Abstraction and Reasoning Corpus (ARC), later renamed ARC-AGI, poses a
fundamental challenge in artificial general intelligence (AGI), requiring
solutions that exhibit robust abstraction and reasoning capabilities across
diverse tasks, while only few (with median count of three) correct examples are
presented. While ARC-AGI remains very challenging for artificial intelligence
systems, it is rather easy for humans. This paper introduces ARC-NCA, a
developmental approach leveraging standard Neural Cellular Automata (NCA) and
NCA enhanced with hidden memories (EngramNCA) to tackle the ARC-AGI benchmark.
NCAs are employed for their inherent ability to simulate complex dynamics and
emergent patterns, mimicking developmental processes observed in biological
systems. Developmental solutions may offer a promising avenue for enhancing
AI's problem-solving capabilities beyond mere training data extrapolation.
ARC-NCA demonstrates how integrating developmental principles into
computational models can foster adaptive reasoning and abstraction. We show
that our ARC-NCA proof-of-concept results may be comparable to, and sometimes
surpass, that of ChatGPT 4.5, at a fraction of the cost.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [106] [Blockbuster, Part 1: Block-level AI Operator Fusion](https://arxiv.org/abs/2505.07829)
*Ofer Dekel*

Main category: cs.LG

TL;DR: Blockbuster是一个用于AI算子融合的框架，兼容多处理器架构，通过块程序和数据移动建模实现高效融合。


<details>
  <summary>Details</summary>
Motivation: 解决AI程序中算子融合的挑战，特别是在大规模AI程序中，通过显式建模数据移动提升融合效果。

Method: 采用基于规则的融合算法，结合候选选择算法和融合算法，显式建模内存层级间的数据移动。

Result: 成功重新发现Flash Attention内核，并实现复杂算子（如LayerNorm与矩阵乘法）的高效融合。

Conclusion: Blockbuster通过显式数据移动建模，在AI算子融合中表现出独特优势，适用于大规模程序。

Abstract: Blockbuster is a framework for AI operator fusion in inference programs. The
Blockbuster framework is compatible with any multiprocessor architecture that
has a tiered memory hierarchy, including GPUs, multi-core CPUs, and some AI
accelerator chips. It includes a graph-based representation for AI workloads,
called a block program, which explicitly models how blocks of data move between
the memory tiers. It also includes an operator fusion procedure, which is made
up of a candidate selection algorithm and a fusion algorithm that fuses each
individual candidate - this two-algorithm structure makes Blockbuster
especially suitable for large AI programs. The current paper focuses on the
fusion algorithm, which is a rule-based technique. While the literature is full
of previous rule-based fusion algorithms, what sets our algorithm apart is its
direct modeling of data movement between memory tiers, resulting in uniquely
powerful fusion results. As a first sanity check, we demonstrate how our
algorithm automatically rediscovers the well-known Flash Attention kernel.
Then, we demonstrate the real power of our approach by fusing LayerNorm with
matrix multiplication and RMSNorm with FNN-SwiGLU - the latter involves fusing
three matrix multiplications, a Hadamard product, a reduction, and a few
elementwise operations into a single mega-kernel.

</details>


### [107] [A General Approach of Automated Environment Design for Learning the Optimal Power Flow](https://arxiv.org/abs/2505.07832)
*Thomas Wolgast,Astrid Nieße*

Main category: cs.LG

TL;DR: 本文提出了一种利用多目标优化自动设计强化学习（RL）环境的方法，通过超参数优化（HPO）框架提升训练性能，并在五个OPF基准问题上验证了其优于手动设计的基线环境。


<details>
  <summary>Details</summary>
Motivation: 尽管RL算法被广泛用于解决最优潮流（OPF）问题，但如何设计RL环境以最大化训练性能仍是一个未解决的问题。本文旨在填补这一空白。

Method: 采用多目标优化方法，结合HPO框架，自动化设计RL环境，并利用现有HPO算法进行优化。

Result: 在五个OPF基准问题上，自动设计的环境性能优于手动设计的基线环境，并通过统计分析揭示了关键环境设计决策。

Conclusion: 本文首次提出了一种通用的自动化RL环境设计方法，并讨论了过拟合风险，为RL-OPF环境设计提供了新见解。

Abstract: Reinforcement learning (RL) algorithms are increasingly used to solve the
optimal power flow (OPF) problem. Yet, the question of how to design RL
environments to maximize training performance remains unanswered, both for the
OPF and the general case. We propose a general approach for automated RL
environment design by utilizing multi-objective optimization. For that, we use
the hyperparameter optimization (HPO) framework, which allows the reuse of
existing HPO algorithms and methods. On five OPF benchmark problems, we
demonstrate that our automated design approach consistently outperforms a
manually created baseline environment design. Further, we use statistical
analyses to determine which environment design decisions are especially
important for performance, resulting in multiple novel insights on how RL-OPF
environments should be designed. Finally, we discuss the risk of overfitting
the environment to the utilized RL algorithm. To the best of our knowledge,
this is the first general approach for automated RL environment design.

</details>


### [108] [Representation Learning with Mutual Influence of Modalities for Node Classification in Multi-Modal Heterogeneous Networks](https://arxiv.org/abs/2505.07895)
*Jiafan Li,Jiaqi Zhu,Liang Chang,Yilin Li,Miaomiao Li,Yang Wang,Hongan Wang*

Main category: cs.LG

TL;DR: 提出了一种名为HGNN-IMA的新模型，用于多模态异构网络中的节点分类，通过跨模态注意力机制实现自适应多模态融合。


<details>
  <summary>Details</summary>
Motivation: 现有多模态融合方法在保留模态特性或跨模态引导方面存在不足，需要一种更有效的方法来学习节点表示。

Method: HGNN-IMA结合了嵌套跨模态注意力机制和模态对齐，在异构图变换框架下学习节点表示。

Result: 实验验证了模型在节点分类任务中的优越性，为处理多模态数据提供了创新视角。

Conclusion: HGNN-IMA通过跨模态注意力和模态对齐，有效提升了多模态异构网络中节点分类的性能。

Abstract: Nowadays, numerous online platforms can be described as multi-modal
heterogeneous networks (MMHNs), such as Douban's movie networks and Amazon's
product review networks. Accurately categorizing nodes within these networks is
crucial for analyzing the corresponding entities, which requires effective
representation learning on nodes. However, existing multi-modal fusion methods
often adopt either early fusion strategies which may lose the unique
characteristics of individual modalities, or late fusion approaches overlooking
the cross-modal guidance in GNN-based information propagation. In this paper,
we propose a novel model for node classification in MMHNs, named Heterogeneous
Graph Neural Network with Inter-Modal Attention (HGNN-IMA). It learns node
representations by capturing the mutual influence of multiple modalities during
the information propagation process, within the framework of heterogeneous
graph transformer. Specifically, a nested inter-modal attention mechanism is
integrated into the inter-node attention to achieve adaptive multi-modal
fusion, and modality alignment is also taken into account to encourage the
propagation among nodes with consistent similarities across all modalities.
Moreover, an attention loss is augmented to mitigate the impact of missing
modalities. Extensive experiments validate the superiority of the model in the
node classification task, providing an innovative view to handle multi-modal
data, especially when accompanied with network structures.

</details>


### [109] [Latent Behavior Diffusion for Sequential Reaction Generation in Dyadic Setting](https://arxiv.org/abs/2505.07901)
*Minh-Duc Nguyen,Hyung-Jeong Yang,Soo-Hyung Kim,Ji-Eun Shin,Seung-Won Kim*

Main category: cs.LG

TL;DR: 提出了一种基于潜在行为扩散模型的新方法，用于生成多样且上下文相关的面部反应，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 提升人机交互的自然性和有效性，通过生成与对话伙伴行为紧密对齐的面部反应。

Method: 结合上下文感知自编码器和扩散条件生成器，自编码器压缩输入特征，扩散生成器在潜在空间生成非自回归的面部反应。

Result: 实验证明该方法在二元反应合成任务中表现优于现有方法。

Conclusion: 潜在行为扩散模型能有效生成多样且上下文相关的面部反应，提升交互模拟效果。

Abstract: The dyadic reaction generation task involves synthesizing responsive facial
reactions that align closely with the behaviors of a conversational partner,
enhancing the naturalness and effectiveness of human-like interaction
simulations. This paper introduces a novel approach, the Latent Behavior
Diffusion Model, comprising a context-aware autoencoder and a diffusion-based
conditional generator that addresses the challenge of generating diverse and
contextually relevant facial reactions from input speaker behaviors. The
autoencoder compresses high-dimensional input features, capturing dynamic
patterns in listener reactions while condensing complex input data into a
concise latent representation, facilitating more expressive and contextually
appropriate reaction synthesis. The diffusion-based conditional generator
operates on the latent space generated by the autoencoder to predict realistic
facial reactions in a non-autoregressive manner. This approach allows for
generating diverse facial reactions that reflect subtle variations in
conversational cues and emotional states. Experimental results demonstrate the
effectiveness of our approach in achieving superior performance in dyadic
reaction synthesis tasks compared to existing methods.

</details>


### [110] [A Reproduction Study: The Kernel PCA Interpretation of Self-Attention Fails Under Scrutiny](https://arxiv.org/abs/2505.07908)
*Karahan Sarıtaş,Çağatay Yıldız*

Main category: cs.LG

TL;DR: 本文通过复现研究质疑了自注意力机制实现核主成分分析（KPCA）的观点，指出其缺乏实证支持。


<details>
  <summary>Details</summary>
Motivation: 重新验证自注意力机制是否如Teo等人（2024）所声称的那样实现了KPCA，并分析其一致性。

Method: 通过比较学习到的自注意力值向量与KPCA理论预测的向量，评估相似性指标（如余弦相似性、CKA），并分析重构损失和Gram矩阵特征值统计。

Result: 发现自注意力值与KPCA理论预测无显著对应关系（相似性指标低），重构损失的差异被误解，Gram矩阵特征值统计无法复现。

Conclusion: 自注意力的KPCA解释缺乏实证支持，10种Transformer架构的验证结果一致。

Abstract: In this reproduction study, we revisit recent claims that self-attention
implements kernel principal component analysis (KPCA) (Teo et al., 2024),
positing that (i) value vectors $V$ capture the eigenvectors of the Gram matrix
of the keys, and (ii) that self-attention projects queries onto the principal
component axes of the key matrix $K$ in a feature space. Our analysis reveals
three critical inconsistencies: (1) No alignment exists between learned
self-attention value vectors and what is proposed in the KPCA perspective, with
average similarity metrics (optimal cosine similarity $\leq 0.32$, linear CKA
(Centered Kernel Alignment) $\leq 0.11$, kernel CKA $\leq 0.32$) indicating
negligible correspondence; (2) Reported decreases in reconstruction loss
$J_\text{proj}$, arguably justifying the claim that the self-attention
minimizes the projection error of KPCA, are misinterpreted, as the quantities
involved differ by orders of magnitude ($\sim\!10^3$); (3) Gram matrix
eigenvalue statistics, introduced to justify that $V$ captures the eigenvector
of the gram matrix, are irreproducible without undocumented
implementation-specific adjustments. Across 10 transformer architectures, we
conclude that the KPCA interpretation of self-attention lacks empirical
support.

</details>


### [111] [Tuning for Trustworthiness -- Balancing Performance and Explanation Consistency in Neural Network Optimization](https://arxiv.org/abs/2505.07910)
*Alexander Hinterleitner,Thomas Bartz-Beielstein*

Main category: cs.LG

TL;DR: 论文提出了一种新的XAI一致性概念，并将其纳入超参数调优目标，通过多目标优化框架平衡预测性能和解释稳健性。


<details>
  <summary>Details</summary>
Motivation: 当前超参数调优和神经网络架构优化中，可解释性常被忽视，而主要关注预测性能。

Method: 提出XAI一致性概念，定义其度量指标，并将其集成到超参数调优目标中，使用SPOT工具箱实现多目标优化。

Result: 研究发现架构配置空间中存在不同区域：性能差且解释性低、性能强但解释性弱、以及平衡两者的折衷区域。

Conclusion: 该研究为未来探索平衡性能损失和XAI一致性的模型是否更具稳健性奠定了基础。

Abstract: Despite the growing interest in Explainable Artificial Intelligence (XAI),
explainability is rarely considered during hyperparameter tuning or neural
architecture optimization, where the focus remains primarily on minimizing
predictive loss. In this work, we introduce the novel concept of XAI
consistency, defined as the agreement among different feature attribution
methods, and propose new metrics to quantify it. For the first time, we
integrate XAI consistency directly into the hyperparameter tuning objective,
creating a multi-objective optimization framework that balances predictive
performance with explanation robustness. Implemented within the Sequential
Parameter Optimization Toolbox (SPOT), our approach uses both weighted
aggregation and desirability-based strategies to guide model selection. Through
our proposed framework and supporting tools, we explore the impact of
incorporating XAI consistency into the optimization process. This enables us to
characterize distinct regions in the architecture configuration space: one
region with poor performance and comparatively low interpretability, another
with strong predictive performance but weak interpretability due to low
\gls{xai} consistency, and a trade-off region that balances both objectives by
offering high interpretability alongside competitive performance. Beyond
introducing this novel approach, our research provides a foundation for future
investigations into whether models from the trade-off zone-balancing
performance loss and XAI consistency-exhibit greater robustness by avoiding
overfitting to training performance, thereby leading to more reliable
predictions on out-of-distribution data.

</details>


### [112] [Combining Bayesian Inference and Reinforcement Learning for Agent Decision Making: A Review](https://arxiv.org/abs/2505.07911)
*Chengmin Zhou,Ville Kyrki,Pasi Fränti,Laura Ruotsalainen*

Main category: cs.LG

TL;DR: 本文综述了贝叶斯推理在强化学习（RL）决策中的应用，总结了其优势（如数据效率、泛化性、可解释性和安全性），并系统讨论了贝叶斯方法与RL的结合及其在不同复杂问题中的应用。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯推理在决策中具有数据效率、泛化性、可解释性和安全性等优势，但目前缺乏对其在RL中应用的全面综述。本文旨在填补这一空白，为研究者提供系统理解。

Method: 讨论了五方面内容：1）适用于决策的贝叶斯方法；2）贝叶斯方法与基于模型、无模型和逆向RL的经典结合；3）贝叶斯方法与RL的最新结合；4）方法对比分析；5）在六种复杂RL问题中的应用。

Result: 总结了贝叶斯方法在RL数据收集、处理和策略学习阶段的作用，为更好的决策策略铺平道路。

Conclusion: 贝叶斯推理与RL的结合在复杂决策问题中展现出巨大潜力，未来研究可进一步探索其优化和扩展。

Abstract: Bayesian inference has many advantages in decision making of agents (e.g.
robotics/simulative agent) over a regular data-driven black-box neural network:
Data-efficiency, generalization, interpretability, and safety where these
advantages benefit directly/indirectly from the uncertainty quantification of
Bayesian inference. However, there are few comprehensive reviews to summarize
the progress of Bayesian inference on reinforcement learning (RL) for decision
making to give researchers a systematic understanding. This paper focuses on
combining Bayesian inference with RL that nowadays is an important approach in
agent decision making. To be exact, this paper discusses the following five
topics: 1) Bayesian methods that have potential for agent decision making.
First basic Bayesian methods and models (Bayesian rule, Bayesian learning, and
Bayesian conjugate models) are discussed followed by variational inference,
Bayesian optimization, Bayesian deep learning, Bayesian active learning,
Bayesian generative models, Bayesian meta-learning, and lifelong Bayesian
learning. 2) Classical combinations of Bayesian methods with model-based RL
(with approximation methods), model-free RL, and inverse RL. 3) Latest
combinations of potential Bayesian methods with RL. 4) Analytical comparisons
of methods that combine Bayesian methods with RL with respect to
data-efficiency, generalization, interpretability, and safety. 5) In-depth
discussions in six complex problem variants of RL, including unknown reward,
partial-observability, multi-agent, multi-task, non-linear non-Gaussian, and
hierarchical RL problems and the summary of how Bayesian methods work in the
data collection, data processing and policy learning stages of RL to pave the
way for better agent decision-making strategies.

</details>


### [113] [On-Device Crack Segmentation for Edge Structural Health Monitoring](https://arxiv.org/abs/2505.07915)
*Yuxuan Zhang,Ye Xu,Luciano Sebastian Martinez-Rau,Quynh Nguyen Phuong Vu,Bengt Oelmann,Sebastian Bader*

Main category: cs.LG

TL;DR: 该论文提出了一种轻量级U-Net架构，用于资源受限的微控制器上的裂缝分割，通过减少卷积核、网络深度和使用深度可分离卷积优化性能与资源消耗的平衡。


<details>
  <summary>Details</summary>
Motivation: 裂缝分割在结构健康监测（SHM）中至关重要，但在资源受限的微控制器上部署深度学习模型面临内存、计算能力和能源限制的挑战。

Method: 研究采用三种优化策略：减少卷积核数量、降低网络深度和使用深度可分离卷积（DWConv2D）。

Result: 结果表明，减少卷积核和网络深度显著降低了RAM、Flash需求和推理时间，但牺牲了一些准确性。优化后的网络在性能和资源消耗之间取得了良好平衡。

Conclusion: 该研究不仅推动了基于TinyML的裂缝分割技术，还为能源自主的边缘SHM系统提供了可能性。

Abstract: Crack segmentation can play a critical role in Structural Health Monitoring
(SHM) by enabling accurate identification of crack size and location, which
allows to monitor structural damages over time. However, deploying deep
learning models for crack segmentation on resource-constrained microcontrollers
presents significant challenges due to limited memory, computational power, and
energy resources. To address these challenges, this study explores lightweight
U-Net architectures tailored for TinyML applications, focusing on three
optimization strategies: filter number reduction, network depth reduction, and
the use of Depthwise Separable Convolutions (DWConv2D). Our results demonstrate
that reducing convolution kernels and network depth significantly reduces RAM
and Flash requirement, and inference times, albeit with some accuracy
trade-offs. Specifically, by reducing the filer number to 25%, the network
depth to four blocks, and utilizing depthwise convolutions, a good compromise
between segmentation performance and resource consumption is achieved. This
makes the network particularly suitable for low-power TinyML applications. This
study not only advances TinyML-based crack segmentation but also provides the
possibility for energy-autonomous edge SHM systems.

</details>


### [114] [Self-cross Feature based Spiking Neural Networks for Efficient Few-shot Learning](https://arxiv.org/abs/2505.07921)
*Qi Xu,Junyang Zhu,Dongdong Zhou,Hao Chen,Yang Liu,Jiangrong Shen,Qiang Zhang*

Main category: cs.LG

TL;DR: 论文提出了一种基于脉冲神经网络（SNN）的小样本学习框架，结合自特征提取和跨特征对比模块，提升性能并降低功耗。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络（DNN）在小样本学习中表现优异，但计算成本高且难以扩展。SNN因其事件驱动和低能耗特性更适合处理稀疏动态数据，但在复杂时空特征提取和跨类比较上仍有不足。

Method: 提出了一种SNN框架，结合自特征提取模块和跨特征对比模块，并采用时间高效训练损失和InfoNCE损失优化时空动态和判别能力。

Result: 实验表明，该框架在N-Omniglot数据集上显著提升了分类性能，并在CUB和miniImageNet等静态数据集上达到与ANN竞争的性能且功耗更低。

Conclusion: 提出的FSL-SNN框架在小样本学习中表现出高效性和低功耗优势，为SNN在复杂任务中的应用提供了新思路。

Abstract: Deep neural networks (DNNs) excel in computer vision tasks, especially,
few-shot learning (FSL), which is increasingly important for generalizing from
limited examples. However, DNNs are computationally expensive with scalability
issues in real world. Spiking Neural Networks (SNNs), with their event-driven
nature and low energy consumption, are particularly efficient in processing
sparse and dynamic data, though they still encounter difficulties in capturing
complex spatiotemporal features and performing accurate cross-class
comparisons. To further enhance the performance and efficiency of SNNs in
few-shot learning, we propose a few-shot learning framework based on SNNs,
which combines a self-feature extractor module and a cross-feature contrastive
module to refine feature representation and reduce power consumption. We apply
the combination of temporal efficient training loss and InfoNCE loss to
optimize the temporal dynamics of spike trains and enhance the discriminative
power. Experimental results show that the proposed FSL-SNN significantly
improves the classification performance on the neuromorphic dataset N-Omniglot,
and also achieves competitive performance to ANNs on static datasets such as
CUB and miniImageNet with low power consumption.

</details>


### [115] [Symbolic Regression with Multimodal Large Language Models and Kolmogorov Arnold Networks](https://arxiv.org/abs/2505.07956)
*Thomas R. Harvey,Fabian Ruehle,Cristofero S. Fraser-Taliente,James Halverson*

Main category: cs.LG

TL;DR: 提出了一种基于视觉能力大语言模型（LLM）和Funsearch思想的符号回归新方法，通过LLM生成函数假设，结合遗传算法优化，无需预设函数集，并利用KANs扩展到多元函数。


<details>
  <summary>Details</summary>
Motivation: 传统符号回归方法需要预设函数集，限制了灵活性。本文旨在通过LLM和KANs实现更灵活、无需预设的符号回归。

Method: 使用LLM根据函数图像生成假设，数值优化器拟合参数，遗传算法优化种群，KANs扩展到多元函数，并通过语言模型简化表达式。

Result: 展示了单变量函数的符号回归能力，并通过KANs成功扩展到多元函数，简化后的表达式效果良好。

Conclusion: 该方法无需预设函数集，灵活性强，适用于单变量和多元函数的符号回归，为符号回归提供了新思路。

Abstract: We present a novel approach to symbolic regression using vision-capable large
language models (LLMs) and the ideas behind Google DeepMind's Funsearch. The
LLM is given a plot of a univariate function and tasked with proposing an
ansatz for that function. The free parameters of the ansatz are fitted using
standard numerical optimisers, and a collection of such ans\"atze make up the
population of a genetic algorithm. Unlike other symbolic regression techniques,
our method does not require the specification of a set of functions to be used
in regression, but with appropriate prompt engineering, we can arbitrarily
condition the generative step. By using Kolmogorov Arnold Networks (KANs), we
demonstrate that ``univariate is all you need'' for symbolic regression, and
extend this method to multivariate functions by learning the univariate
function on each edge of a trained KAN. The combined expression is then
simplified by further processing with a language model.

</details>


### [116] [Making Small Language Models Efficient Reasoners: Intervention, Supervision, Reinforcement](https://arxiv.org/abs/2505.07961)
*Xuechen Zhang,Zijian Huang,Chenchun Ni,Ziyang Xiong,Jiasi Chen,Samet Oymak*

Main category: cs.LG

TL;DR: 本文提出两种方法（温度缩放和TLDR强化学习）以提高小型语言模型的推理效率，减少冗余计算，同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: 小型语言模型在推理过程中常因冗长和重复输出导致计算效率低下，需优化停止点控制。

Method: 1. 温度缩放（TS）控制推理停止点；2. TLDR强化学习方法（基于GRPO）实现多级推理长度控制。

Result: 在四个基准测试中，TS和TLDR显著提升计算效率（TLDR节省50%计算量），且准确性损失极小。

Conclusion: 研究揭示了停止点控制的重要性，并提供了针对小型模型的高效推理算法。

Abstract: Recent research enhances language model reasoning by scaling test-time
compute via longer chain-of-thought traces. This often improves accuracy but
also introduces redundancy and high computational cost, especially for small
language models distilled with supervised fine-tuning (SFT). In this work, we
propose new algorithms to improve token-efficient reasoning with small-scale
models by effectively trading off accuracy and computation. We first show that
the post-SFT model fails to determine the optimal stopping point of the
reasoning process, resulting in verbose and repetitive outputs. Verbosity also
significantly varies across wrong vs correct responses. To address these
issues, we propose two solutions: (1) Temperature scaling (TS) to control the
stopping point for the thinking phase and thereby trace length, and (2) TLDR: a
length-regularized reinforcement learning method based on GRPO that facilitates
multi-level trace length control (e.g. short, medium, long reasoning).
Experiments on four reasoning benchmarks, MATH500, AMC, AIME24 and
OlympiadBench, demonstrate that TS is highly effective compared to s1's budget
forcing approach and TLDR significantly improves token efficiency by about 50%
with minimal to no accuracy loss over the SFT baseline. Moreover, TLDR also
facilitates flexible control over the response length, offering a practical and
effective solution for token-efficient reasoning in small models. Ultimately,
our work reveals the importance of stopping time control, highlights
shortcomings of pure SFT, and provides effective algorithmic recipes.

</details>


### [117] [Fair Play for Individuals, Foul Play for Groups? Auditing Anonymization's Impact on ML Fairness](https://arxiv.org/abs/2505.07985)
*Héber H. Arcolezi,Mina Alishahi,Adda-Akram Bendoukha,Nesrine Kaaniche*

Main category: cs.LG

TL;DR: 论文研究了匿名化技术（如k-匿名、l-多样性和t-接近性）对机器学习公平性的影响，发现匿名化可能显著降低群体公平性，但会提升基于相似性的个体公平性。


<details>
  <summary>Details</summary>
Motivation: 机器学习依赖的训练数据常包含敏感信息，匿名化技术虽能保护隐私，但其对公平性的影响尚未充分研究。

Method: 通过系统审计匿名化技术对个体和群体公平性的影响，分析不同隐私设置和数据分布下的效果。

Result: 匿名化可能使群体公平性指标下降多达四个数量级，但会因输入同质性增强而改善个体公平性。

Conclusion: 研究揭示了隐私、公平性和实用性之间的权衡，为负责任的人工智能开发提供了实用指南。

Abstract: Machine learning (ML) algorithms are heavily based on the availability of
training data, which, depending on the domain, often includes sensitive
information about data providers. This raises critical privacy concerns.
Anonymization techniques have emerged as a practical solution to address these
issues by generalizing features or suppressing data to make it more difficult
to accurately identify individuals. Although recent studies have shown that
privacy-enhancing technologies can influence ML predictions across different
subgroups, thus affecting fair decision-making, the specific effects of
anonymization techniques, such as $k$-anonymity, $\ell$-diversity, and
$t$-closeness, on ML fairness remain largely unexplored. In this work, we
systematically audit the impact of anonymization techniques on ML fairness,
evaluating both individual and group fairness. Our quantitative study reveals
that anonymization can degrade group fairness metrics by up to four orders of
magnitude. Conversely, similarity-based individual fairness metrics tend to
improve under stronger anonymization, largely as a result of increased input
homogeneity. By analyzing varying levels of anonymization across diverse
privacy settings and data distributions, this study provides critical insights
into the trade-offs between privacy, fairness, and utility, offering actionable
guidelines for responsible AI development. Our code is publicly available at:
https://github.com/hharcolezi/anonymity-impact-fairness.

</details>


### [118] [A Scalable System to Prove Machine Learning Fairness in Zero-Knowledge](https://arxiv.org/abs/2505.07997)
*Tianyu Zhang,Shen Dong,O. Deniz Kose,Yanning Shen,Yupeng Zhang*

Main category: cs.LG

TL;DR: 论文提出了一种基于零知识证明的方法FairZK，用于验证机器学习模型的公平性，同时保护模型机密性。通过优化公平性测量和高效零知识证明协议，显著提升了验证效率。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习在关键应用中的普及，确保其决策公平性变得至关重要，但传统方法需要公开模型参数，可能泄露机密信息。

Method: 提出一种新方法，仅通过模型参数和输入聚合信息测量公平性，无需具体数据集。设计了高效的零知识证明协议，支持常见计算操作。

Result: 实验表明，FairZK比现有方法快3.1x--1789x，首次支持4700万参数的大模型，验证时间仅343秒。

Conclusion: FairZK在保护模型机密性的同时，高效验证公平性，为大规模机器学习模型的公平性验证提供了可行方案。

Abstract: With the rise of machine learning techniques, ensuring the fairness of
decisions made by machine learning algorithms has become of great importance in
critical applications. However, measuring fairness often requires full access
to the model parameters, which compromises the confidentiality of the models.
In this paper, we propose a solution using zero-knowledge proofs, which allows
the model owner to convince the public that a machine learning model is fair
while preserving the secrecy of the model. To circumvent the efficiency barrier
of naively proving machine learning inferences in zero-knowledge, our key
innovation is a new approach to measure fairness only with model parameters and
some aggregated information of the input, but not on any specific dataset. To
achieve this goal, we derive new bounds for the fairness of logistic regression
and deep neural network models that are tighter and better reflecting the
fairness compared to prior work. Moreover, we develop efficient zero-knowledge
proof protocols for common computations involved in measuring fairness,
including the spectral norm of matrices, maximum, absolute value, and
fixed-point arithmetic.
  We have fully implemented our system, FairZK, that proves machine learning
fairness in zero-knowledge. Experimental results show that FairZK is
significantly faster than the naive approach and an existing scheme that use
zero-knowledge inferences as a subroutine. The prover time is improved by
3.1x--1789x depending on the size of the model and the dataset. FairZK can
scale to a large model with 47 million parameters for the first time, and
generates a proof for its fairness in 343 seconds. This is estimated to be 4
orders of magnitude faster than existing schemes, which only scale to small
models with hundreds to thousands of parameters.

</details>


### [119] [Dynamical Low-Rank Compression of Neural Networks with Robustness under Adversarial Attacks](https://arxiv.org/abs/2505.08022)
*Steffen Schotthöfer,H. Lexie Yang,Stefan Schnake*

Main category: cs.LG

TL;DR: 论文提出了一种动态低秩训练方案，结合谱正则化器，以在压缩神经网络的同时保持对抗鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在资源受限设备上部署神经网络需要模型既紧凑又对对抗输入具有鲁棒性，但压缩和对抗鲁棒性往往冲突。

Method: 采用动态低秩训练方案，并引入谱正则化器控制每层低秩核心的条件数。

Result: 实验表明，该方法在标准架构、数据集和对抗攻击下，能实现超过94%的压缩，同时恢复或提升对抗精度。

Conclusion: 该方法高效、模型和数据无关，支持秩自适应性，能自动压缩网络。

Abstract: Deployment of neural networks on resource-constrained devices demands models
that are both compact and robust to adversarial inputs. However, compression
and adversarial robustness often conflict. In this work, we introduce a
dynamical low-rank training scheme enhanced with a novel spectral regularizer
that controls the condition number of the low-rank core in each layer. This
approach mitigates the sensitivity of compressed models to adversarial
perturbations without sacrificing clean accuracy. The method is model- and
data-agnostic, computationally efficient, and supports rank adaptivity to
automatically compress the network at hand. Extensive experiments across
standard architectures, datasets, and adversarial attacks show the regularized
networks can achieve over 94% compression while recovering or improving
adversarial accuracy relative to uncompressed baselines.

</details>


### [120] [Demo: A Practical Testbed for Decentralized Federated Learning on Physical Edge Devices](https://arxiv.org/abs/2505.08033)
*Chao Feng,Nicolas Huber,Alberto Huertas Celdran,Gerome Bovet,Burkhard Stiller*

Main category: cs.LG

TL;DR: 该论文研究了去中心化联邦学习（DFL）在实际边缘设备上的应用，通过构建物理测试平台评估其性能和能耗。


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习依赖中心服务器，存在单点故障风险，而去中心化联邦学习（DFL）解决了这一问题，但资源受限设备的部署仍具挑战性。

Method: 设计并部署了一个基于边缘设备（如Raspberry Pi和Jetson Nano）的物理测试平台，扩展了DFL训练平台NEBULA，并增加了能耗监测模块。

Result: 实验表明，模型性能受通信拓扑结构影响，拓扑越密集，DFL性能越好。

Conclusion: DFL在边缘设备上具有实际应用潜力，但需进一步优化通信拓扑以提升性能。

Abstract: Federated Learning (FL) enables collaborative model training without sharing
raw data, preserving participant privacy. Decentralized FL (DFL) eliminates
reliance on a central server, mitigating the single point of failure inherent
in the traditional FL paradigm, while introducing deployment challenges on
resource-constrained devices. To evaluate real-world applicability, this work
designs and deploys a physical testbed using edge devices such as Raspberry Pi
and Jetson Nano. The testbed is built upon a DFL training platform, NEBULA, and
extends it with a power monitoring module to measure energy consumption during
training. Experiments across multiple datasets show that model performance is
influenced by the communication topology, with denser topologies leading to
better outcomes in DFL settings.

</details>


### [121] [Beyond Input Activations: Identifying Influential Latents by Gradient Sparse Autoencoders](https://arxiv.org/abs/2505.08080)
*Dong Shu,Xuansheng Wu,Haiyan Zhao,Mengnan Du,Ninghao Liu*

Main category: cs.LG

TL;DR: GradSAE是一种通过结合输出梯度信息来识别稀疏自编码器中关键潜在特征的方法。


<details>
  <summary>Details</summary>
Motivation: 传统稀疏自编码器分析仅依赖输入激活，忽略了潜在特征对输出的因果影响。

Method: 提出GradSAE，利用输出梯度信息识别高因果影响的潜在特征。

Result: 验证了高因果影响的潜在特征对模型操控更有效。

Conclusion: GradSAE为稀疏自编码器分析提供了更有效的方法，尤其适用于模型操控。

Abstract: Sparse Autoencoders (SAEs) have recently emerged as powerful tools for
interpreting and steering the internal representations of large language models
(LLMs). However, conventional approaches to analyzing SAEs typically rely
solely on input-side activations, without considering the causal influence
between each latent feature and the model's output. This work is built on two
key hypotheses: (1) activated latents do not contribute equally to the
construction of the model's output, and (2) only latents with high causal
influence are effective for model steering. To validate these hypotheses, we
propose Gradient Sparse Autoencoder (GradSAE), a simple yet effective method
that identifies the most influential latents by incorporating output-side
gradient information.

</details>


### [122] [Fréchet Power-Scenario Distance: A Metric for Evaluating Generative AI Models across Multiple Time-Scales in Smart Grids](https://arxiv.org/abs/2505.08082)
*Yuting Cai,Shaohuai Liu,Chao Tian,Le Xie*

Main category: cs.LG

TL;DR: 提出了一种基于Fréchet距离的新指标，用于评估智能电网中生成模型产生的合成数据质量，优于传统欧氏距离方法。


<details>
  <summary>Details</summary>
Motivation: 生成模型在智能电网中产生大量合成数据，但传统欧氏距离指标无法有效评估数据质量，需从分布角度提出新方法。

Method: 利用Fréchet距离在学习的特征空间中估计两组数据之间的距离，从分布角度评估生成质量。

Result: 实验结果表明，该指标在不同时间尺度和模型中表现优越，提升了智能电网数据驱动决策的可靠性。

Conclusion: 提出的Fréchet距离指标为评估生成模型数据质量提供了更有效的方法，适用于智能电网应用。

Abstract: Generative artificial intelligence (AI) models in smart grids have advanced
significantly in recent years due to their ability to generate large amounts of
synthetic data, which would otherwise be difficult to obtain in the real world
due to confidentiality constraints. A key challenge in utilizing such synthetic
data is how to assess the data quality produced from such generative models.
Traditional Euclidean distance-based metrics only reflect pair-wise relations
between two individual samples, and could fail in evaluating quality
differences between groups of synthetic datasets. In this work, we propose a
novel metric based on the Fr\'{e}chet Distance (FD) estimated between two
datasets in a learned feature space. The proposed method evaluates the quality
of generation from a distributional perspective. Empirical results demonstrate
the superiority of the proposed metric across timescales and models, enhancing
the reliability of data-driven decision-making in smart grid operations.

</details>


### [123] [A Federated Random Forest Solution for Secure Distributed Machine Learning](https://arxiv.org/abs/2505.08085)
*Alexandre Cotorobai,Jorge Miguel Silva,Jose Luis Oliveira*

Main category: cs.LG

TL;DR: 论文提出了一种支持随机森林分类器的联邦学习框架，解决了隐私和监管障碍，同时保持高性能。


<details>
  <summary>Details</summary>
Motivation: 医疗等领域的数据共享受限，现有联邦学习框架主要支持梯度模型，缺乏可解释的树模型。

Method: 利用PySyft实现隐私保护计算，支持加权模型平均、增量学习和本地评估。

Result: 实验显示联邦方法在预测准确性上与集中式方法相差不超过9%，满足隐私要求。

Conclusion: 该框架填补了联邦学习库的空白，适用于需要透明性和可靠性能的分布式任务。

Abstract: Privacy and regulatory barriers often hinder centralized machine learning
solutions, particularly in sectors like healthcare where data cannot be freely
shared. Federated learning has emerged as a powerful paradigm to address these
concerns; however, existing frameworks primarily support gradient-based models,
leaving a gap for more interpretable, tree-based approaches. This paper
introduces a federated learning framework for Random Forest classifiers that
preserves data privacy and provides robust performance in distributed settings.
By leveraging PySyft for secure, privacy-aware computation, our method enables
multiple institutions to collaboratively train Random Forest models on locally
stored data without exposing sensitive information. The framework supports
weighted model averaging to account for varying data distributions, incremental
learning to progressively refine models, and local evaluation to assess
performance across heterogeneous datasets. Experiments on two real-world
healthcare benchmarks demonstrate that the federated approach maintains
competitive predictive accuracy - within a maximum 9\% margin of centralized
methods - while satisfying stringent privacy requirements. These findings
underscore the viability of tree-based federated learning for scenarios where
data cannot be centralized due to regulatory, competitive, or technical
constraints. The proposed solution addresses a notable gap in existing
federated learning libraries, offering an adaptable tool for secure distributed
machine learning tasks that demand both transparency and reliable performance.
The tool is available at https://github.com/ieeta-pt/fed_rf.

</details>


### [124] [Manifold Learning with Normalizing Flows: Towards Regularity, Expressivity and Iso-Riemannian Geometry](https://arxiv.org/abs/2505.08087)
*Willem Diepeveen,Deanna Needell*

Main category: cs.LG

TL;DR: 论文提出了一种通过等距化和平衡参数化来解决多模态数据中失真和建模误差的方法，展示了其有效性。


<details>
  <summary>Details</summary>
Motivation: 高维数据通常位于低维非线性流形附近，但多模态数据中的失真和建模误差仍需解决。

Method: 通过等距化学习的黎曼结构和平衡参数化的微分同胚来解决问题。

Result: 在合成和真实数据的实验中验证了方法的有效性。

Conclusion: 提出的方法在多模态数据中有效解决了失真和建模误差问题。

Abstract: Modern machine learning increasingly leverages the insight that
high-dimensional data often lie near low-dimensional, non-linear manifolds, an
idea known as the manifold hypothesis. By explicitly modeling the geometric
structure of data through learning Riemannian geometry algorithms can achieve
improved performance and interpretability in tasks like clustering,
dimensionality reduction, and interpolation. In particular, learned pullback
geometry has recently undergone transformative developments that now make it
scalable to learn and scalable to evaluate, which further opens the door for
principled non-linear data analysis and interpretable machine learning.
However, there are still steps to be taken when considering real-world
multi-modal data. This work focuses on addressing distortions and modeling
errors that can arise in the multi-modal setting and proposes to alleviate both
challenges through isometrizing the learned Riemannian structure and balancing
regularity and expressivity of the diffeomorphism parametrization. We showcase
the effectiveness of the synergy of the proposed approaches in several
numerical experiments with both synthetic and real data.

</details>


### [125] [High-order Regularization for Machine Learning and Learning-based Control](https://arxiv.org/abs/2505.08129)
*Xinghua Liu,Ming Cao*

Main category: cs.LG

TL;DR: 论文提出了一种新的高阶正则化（HR）方法，用于机器学习中的正则化过程，增强了神经网络在强化学习中的可解释性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 正则化在神经网络训练中广泛应用，但缺乏与可解释学习的明确联系。本文旨在通过高阶正则化方法填补这一空白，并提供理论支持。

Method: 提出高阶正则化（HR）方法，将其视为逆映射的近似，并计算显式误差。HR是L2正则化的高阶扩展，适用于任意映射矩阵的神经网络。

Result: HR方法提供了误差的上下界，证明了其作为收缩的性质，并通过案例研究验证了其在经典控制问题中的优越性能。

Conclusion: HR方法不仅提升了神经网络的泛化能力，还增强了其可解释性，为理论研究和实际应用提供了新视角。

Abstract: The paper proposes a novel regularization procedure for machine learning. The
proposed high-order regularization (HR) provides new insight into
regularization, which is widely used to train a neural network that can be
utilized to approximate the action-value function in general reinforcement
learning problems. The proposed HR method ensures the provable convergence of
the approximation algorithm, which makes the much-needed connection between
regularization and explainable learning using neural networks. The proposed HR
method theoretically demonstrates that regularization can be regarded as an
approximation in terms of inverse mapping with explicitly calculable
approximation error, and the $L_2$ regularization is a lower-order case of the
proposed method. We provide lower and upper bounds for the error of the
proposed HR solution, which helps build a reliable model. We also find that
regularization with the proposed HR can be regarded as a contraction. We prove
that the generalizability of neural networks can be maximized with a proper
regularization matrix, and the proposed HR is applicable for neural networks
with any mapping matrix. With the theoretical explanation of the extreme
learning machine for neural network training and the proposed high-order
regularization, one can better interpret the output of the neural network, thus
leading to explainable learning. We present a case study based on regularized
extreme learning neural networks to demonstrate the application of the proposed
HR and give the corresponding incremental HR solution. We verify the
performance of the proposed HR method by solving a classic control problem in
reinforcement learning. The result demonstrates the superior performance of the
method with significant enhancement in the generalizability of the neural
network.

</details>


### [126] [Large Language Models for Computer-Aided Design: A Survey](https://arxiv.org/abs/2505.08137)
*Licheng Zhang,Bach Le,Naveed Akhtar,Siew-Kei Lam,Tuan Ngo*

Main category: cs.LG

TL;DR: 本文首次系统综述了大型语言模型（LLMs）与计算机辅助设计（CAD）的结合，填补了研究空白，并提出了六个关键应用领域及未来发展方向。


<details>
  <summary>Details</summary>
Motivation: CAD作为3D建模的行业标准，在现代设计复杂度增加的背景下，LLMs有望优化CAD工作流程，但目前缺乏相关综述研究。

Method: 文章首先概述CAD的工业意义和AI驱动的创新需求，接着介绍LLMs的基础知识，包括闭源和开源模型，并重点分析LLMs在CAD中的六大应用领域。

Result: 提出了LLMs在CAD中的六个关键应用领域，展示了其显著影响，并指出了未来发展的潜在方向。

Conclusion: LLMs与CAD的结合具有广阔前景，未来研究将推动CAD技术的创新与发展。

Abstract: Large Language Models (LLMs) have seen rapid advancements in recent years,
with models like ChatGPT and DeepSeek, showcasing their remarkable capabilities
across diverse domains. While substantial research has been conducted on LLMs
in various fields, a comprehensive review focusing on their integration with
Computer-Aided Design (CAD) remains notably absent. CAD is the industry
standard for 3D modeling and plays a vital role in the design and development
of products across different industries. As the complexity of modern designs
increases, the potential for LLMs to enhance and streamline CAD workflows
presents an exciting frontier. This article presents the first systematic
survey exploring the intersection of LLMs and CAD. We begin by outlining the
industrial significance of CAD, highlighting the need for AI-driven innovation.
Next, we provide a detailed overview of the foundation of LLMs. We also examine
both closed-source LLMs as well as publicly available models. The core of this
review focuses on the various applications of LLMs in CAD, providing a taxonomy
of six key areas where these models are making considerable impact. Finally, we
propose several promising future directions for further advancements, which
offer vast opportunities for innovation and are poised to shape the future of
CAD technology. Github:
https://github.com/lichengzhanguom/LLMs-CAD-Survey-Taxonomy

</details>


### [127] [Mirror Mirror on the Wall, Have I Forgotten it All? A New Framework for Evaluating Machine Unlearning](https://arxiv.org/abs/2505.08138)
*Brennon Brimhall,Philip Mathew,Neil Fendley,Yinzhi Cao,Matthew Green*

Main category: cs.LG

TL;DR: 论文提出了一种称为“计算性遗忘”的强形式化定义，用于衡量机器遗忘方法的有效性，并证明现有方法无法满足该定义。


<details>
  <summary>Details</summary>
Motivation: 研究机器遗忘方法的有效性，提出一种更严格的评估标准，以解决现有方法在区分遗忘模型和重新训练模型上的不足。

Method: 通过构建区分算法（基于成员推理分数和Kullback-Leibler散度）来评估遗忘方法的性能，并提出计算性遗忘的形式化定义。

Result: 现有机器遗忘方法无法满足计算性遗忘的定义，且基于差分隐私的方法虽能满足但会导致效用严重下降。

Conclusion: 计算性遗忘为机器遗忘提供了理论框架，但当前方法尚无法实现，未来需进一步研究。

Abstract: Machine unlearning methods take a model trained on a dataset and a forget
set, then attempt to produce a model as if it had only been trained on the
examples not in the forget set. We empirically show that an adversary is able
to distinguish between a mirror model (a control model produced by retraining
without the data to forget) and a model produced by an unlearning method across
representative unlearning methods from the literature. We build distinguishing
algorithms based on evaluation scores in the literature (i.e. membership
inference scores) and Kullback-Leibler divergence.
  We propose a strong formal definition for machine unlearning called
computational unlearning. Computational unlearning is defined as the inability
for an adversary to distinguish between a mirror model and a model produced by
an unlearning method. If the adversary cannot guess better than random (except
with negligible probability), then we say that an unlearning method achieves
computational unlearning.
  Our computational unlearning definition provides theoretical structure to
prove unlearning feasibility results. For example, our computational unlearning
definition immediately implies that there are no deterministic computational
unlearning methods for entropic learning algorithms. We also explore the
relationship between differential privacy (DP)-based unlearning methods and
computational unlearning, showing that DP-based approaches can satisfy
computational unlearning at the cost of an extreme utility collapse. These
results demonstrate that current methodology in the literature fundamentally
falls short of achieving computational unlearning. We conclude by identifying
several open questions for future work.

</details>


### [128] [Multi-Layer Hierarchical Federated Learning with Quantization](https://arxiv.org/abs/2505.08145)
*Seyed Mohammad Azimi-Abarghouyi,Carlo Fischione*

Main category: cs.LG

TL;DR: 提出了一种多层分层联邦学习框架（QMLHFL），首次将分层FL推广到任意层数和网络架构，并通过层特定量化方案满足通信约束。


<details>
  <summary>Details</summary>
Motivation: 现有分层FL模型通常限于两层聚合，限制了复杂大规模网络的扩展性和灵活性。

Method: 采用嵌套聚合和层特定量化方案，开发了QMLHFL的收敛分析，并推导出一般收敛条件和速率。

Result: QMLHFL在高数据异构性下仍保持高学习精度，优化后性能显著提升。

Conclusion: QMLHFL通过多层架构和量化优化，显著提升了分层FL的扩展性和性能。

Abstract: Almost all existing hierarchical federated learning (FL) models are limited
to two aggregation layers, restricting scalability and flexibility in complex,
large-scale networks. In this work, we propose a Multi-Layer Hierarchical
Federated Learning framework (QMLHFL), which appears to be the first study that
generalizes hierarchical FL to arbitrary numbers of layers and network
architectures through nested aggregation, while employing a layer-specific
quantization scheme to meet communication constraints. We develop a
comprehensive convergence analysis for QMLHFL and derive a general convergence
condition and rate that reveal the effects of key factors, including
quantization parameters, hierarchical architecture, and intra-layer iteration
counts. Furthermore, we determine the optimal number of intra-layer iterations
to maximize the convergence rate while meeting a deadline constraint that
accounts for both communication and computation times. Our results show that
QMLHFL consistently achieves high learning accuracy, even under high data
heterogeneity, and delivers notably improved performance when optimized,
compared to using randomly selected values.

</details>


### [129] [Feature Fitted Online Conformal Prediction for Deep Time Series Forecasting Model](https://arxiv.org/abs/2505.08158)
*Xiannan Huang,Shuhan Qiu*

Main category: cs.LG

TL;DR: 提出了一种轻量级的共形预测方法，用于时间序列预测中的不确定性量化，无需重新训练，提供有效覆盖和更短的区间长度。


<details>
  <summary>Details</summary>
Motivation: 现有深度点预测模型的置信区间建模方法存在成本高、未能充分利用深度模型表示能力或缺乏理论保证的问题。

Method: 利用预训练点预测模型提取的特征拟合残差预测器并构建置信区间，结合自适应覆盖控制机制。

Result: 在12个数据集上的实验表明，该方法能提供更紧的置信区间并保持所需的覆盖率。

Conclusion: 该方法通过理论证明和实验验证，解决了现有方法的局限性，实现了更高效的置信区间建模。

Abstract: Time series forecasting is critical for many applications, where deep
learning-based point prediction models have demonstrated strong performance.
However, in practical scenarios, there is also a need to quantify predictive
uncertainty through online confidence intervals. Existing confidence interval
modeling approaches building upon these deep point prediction models suffer
from key limitations: they either require costly retraining, fail to fully
leverage the representational strengths of deep models, or lack theoretical
guarantees. To address these gaps, we propose a lightweight conformal
prediction method that provides valid coverage and shorter interval lengths
without retraining. Our approach leverages features extracted from pre-trained
point prediction models to fit a residual predictor and construct confidence
intervals, further enhanced by an adaptive coverage control mechanism.
Theoretically, we prove that our method achieves asymptotic coverage
convergence, with error bounds dependent on the feature quality of the
underlying point prediction model. Experiments on 12 datasets demonstrate that
our method delivers tighter confidence intervals while maintaining desired
coverage rates. Code, model and dataset in
\href{https://github.com/xiannanhuang/FFDCI}{Github}

</details>


### [130] [Feasibility-Aware Pessimistic Estimation: Toward Long-Horizon Safety in Offline RL](https://arxiv.org/abs/2505.08179)
*Zhikun Tao,Gang Xiong,He Fang,Zhen Shen,Yunjun Han,Qing-Shan Jia*

Main category: cs.LG

TL;DR: FASP是一种新的离线安全强化学习框架，通过H-J可达性分析和悲观估计方法，解决了现有方法在长期安全性和样本效率上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有离线安全强化学习方法仅关注短期安全，忽视长期保护，且对分布外数据适应性差。

Method: 采用H-J可达性分析生成安全标签，结合CVAE和安全分类器，并使用悲观估计方法优化Q值。

Result: FASP在DSRL基准测试中表现优异，尤其在安全性上超越现有算法。

Conclusion: FASP为离线安全强化学习提供了长期安全保证和高效样本利用的有效解决方案。

Abstract: Offline safe reinforcement learning(OSRL) derives constraint-satisfying
policies from pre-collected datasets, offers a promising avenue for deploying
RL in safety-critical real-world domains such as robotics. However, the
majority of existing approaches emphasize only short-term safety, neglecting
long-horizon considerations. Consequently, they may violate safety constraints
and fail to ensure sustained protection during online deployment. Moreover, the
learned policies often struggle to handle states and actions that are not
present or out-of-distribution(OOD) from the offline dataset, and exhibit
limited sample efficiency. To address these challenges, we propose a novel
framework Feasibility-Aware offline Safe Reinforcement Learning with CVAE-based
Pessimism (FASP). First, we employ Hamilton-Jacobi (H-J) reachability analysis
to generate reliable safety labels, which serve as supervisory signals for
training both a conditional variational autoencoder (CVAE) and a safety
classifier. This approach not only ensures high sampling efficiency but also
provides rigorous long-horizon safety guarantees. Furthermore, we utilize
pessimistic estimation methods to estimate the Q-value of reward and cost,
which mitigates the extrapolation errors induces by OOD actions, and penalize
unsafe actions to enabled the agent to proactively avoid high-risk behaviors.
Moreover, we theoretically prove the validity of this pessimistic estimation.
Extensive experiments on DSRL benchmarks demonstrate that FASP algorithm
achieves competitive performance across multiple experimental tasks,
particularly outperforming state-of-the-art algorithms in terms of safety.

</details>


### [131] [DSADF: Thinking Fast and Slow for Decision Making](https://arxiv.org/abs/2505.08189)
*Alex Zhihao Dou,Dongfei Cui,Jun Yan,Weida Wang,Benteng Chen,Haoming Wang,Zeke Xie,Shufei Zhang*

Main category: cs.LG

TL;DR: 论文提出了一种双系统自适应决策框架（DSADF），结合强化学习（RL）和视觉语言模型（VLM），以解决RL在动态环境中泛化能力不足的问题。


<details>
  <summary>Details</summary>
Motivation: RL在动态环境中泛化能力有限，现有方法中RL与基础模型的协调不足，导致决策不合理和效率瓶颈。

Method: 受Kahneman的双系统理论启发，提出DSADF框架，包含快速直觉决策的System 1（RL代理）和深度分析的System 2（VLM）。

Result: 在Crafter和Housekeep游戏环境中验证了DSADF的有效性，显著提升了在未知和已知任务中的决策能力。

Conclusion: DSADF通过结合RL的快速响应和VLM的深度推理，实现了高效且自适应的决策。

Abstract: Although Reinforcement Learning (RL) agents are effective in well-defined
environments, they often struggle to generalize their learned policies to
dynamic settings due to their reliance on trial-and-error interactions. Recent
work has explored applying Large Language Models (LLMs) or Vision Language
Models (VLMs) to boost the generalization of RL agents through policy
optimization guidance or prior knowledge. However, these approaches often lack
seamless coordination between the RL agent and the foundation model, leading to
unreasonable decision-making in unfamiliar environments and efficiency
bottlenecks. Making full use of the inferential capabilities of foundation
models and the rapid response capabilities of RL agents and enhancing the
interaction between the two to form a dual system is still a lingering
scientific question. To address this problem, we draw inspiration from
Kahneman's theory of fast thinking (System 1) and slow thinking (System 2),
demonstrating that balancing intuition and deep reasoning can achieve nimble
decision-making in a complex world. In this study, we propose a Dual-System
Adaptive Decision Framework (DSADF), integrating two complementary modules:
System 1, comprising an RL agent and a memory space for fast and intuitive
decision making, and System 2, driven by a VLM for deep and analytical
reasoning. DSADF facilitates efficient and adaptive decision-making by
combining the strengths of both systems. The empirical study in the video game
environment: Crafter and Housekeep demonstrates the effectiveness of our
proposed method, showing significant improvements in decision abilities for
both unseen and known tasks.

</details>


### [132] [A Multi-scale Representation Learning Framework for Long-Term Time Series Forecasting](https://arxiv.org/abs/2505.08199)
*Boshi Gao,Qingjian Ni,Fanbo Ju,Yu Chen,Ziqi Zhao*

Main category: cs.LG

TL;DR: 论文提出了一种基于MLP的框架MDMixer，用于解决长期时间序列预测（LTSF）中的多粒度信息利用不足、通道特性忽视以及趋势和季节性成分独特性的问题。


<details>
  <summary>Details</summary>
Motivation: 长期时间序列预测在能源消耗和天气预测等领域有广泛应用，但由于复杂的时间模式和内在的多尺度变化，准确预测具有挑战性。

Method: MDMixer通过多尺度预测和动态信息整合系统，独立建模趋势和季节性成分，有效解耦复杂时间动态。

Result: 在八个LTSF基准测试中，MDMixer的平均MAE性能比当前最先进的MLP方法（TimeMixer）提高了4.64%。

Conclusion: MDMixer在训练效率和模型可解释性之间取得了良好平衡，为LTSF提供了一种高效且可解释的解决方案。

Abstract: Long-term time series forecasting (LTSF) offers broad utility in practical
settings like energy consumption and weather prediction. Accurately predicting
long-term changes, however, is demanding due to the intricate temporal patterns
and inherent multi-scale variations within time series. This work confronts key
issues in LTSF, including the suboptimal use of multi-granularity information,
the neglect of channel-specific attributes, and the unique nature of trend and
seasonal components, by introducing a proficient MLP-based forecasting
framework. Our method adeptly disentangles complex temporal dynamics using
clear, concurrent predictions across various scales. These multi-scale
forecasts are then skillfully integrated through a system that dynamically
assigns importance to information from different granularities, sensitive to
individual channel characteristics. To manage the specific features of temporal
patterns, a two-pronged structure is utilized to model trend and seasonal
elements independently. Experimental results on eight LTSF benchmarks
demonstrate that MDMixer improves average MAE performance by 4.64% compared to
the recent state-of-the-art MLP-based method (TimeMixer), while achieving an
effective balance between training efficiency and model interpretability.

</details>


### [133] [An Effective Flow-based Method for Positive-Unlabeled Learning: 2-HNC](https://arxiv.org/abs/2505.08212)
*Dorit Hochbaum,Torpong Nitayanont*

Main category: cs.LG

TL;DR: 本文提出了一种基于网络流的2-HNC方法，用于解决正样本-未标记样本（PU）学习问题。通过利用样本间的成对相似性和Hochbaum归一化切割（HNC），生成嵌套分区序列，并分两阶段进行负样本排序和分类。实验表明，2-HNC性能优越，超越现有先进算法。


<details>
  <summary>Details</summary>
Motivation: 在二元分类中，训练数据通常仅包含正样本，其余数据未标记。这种正样本-未标记样本（PU）学习问题需要有效方法来解决。

Method: 提出2-HNC方法，分两阶段：1）利用HNC生成未标记样本的负样本可能性排序；2）将可能负样本加入正样本集重新分类，最终选择最接近先验估计的分区。

Result: 在合成和真实数据集上的实验表明，2-HNC性能优越，常超越现有先进算法。

Conclusion: 2-HNC是一种有效的PU学习方法，通过两阶段策略和HNC技术，显著提升了分类性能。

Abstract: In many scenarios of binary classification, only positive instances are
provided in the training data, leaving the rest of the data unlabeled. This
setup, known as positive-unlabeled (PU) learning, is addressed here with a
network flow-based method which utilizes pairwise similarities between samples.
The method we propose here, 2-HNC, leverages Hochbaum's Normalized Cut (HNC)
and the set of solutions it provides by solving a parametric minimum cut
problem. The set of solutions, that are nested partitions of the samples into
two sets, correspond to varying tradeoff values between the two goals: high
intra-similarity inside the sets and low inter-similarity between the two sets.
This nested sequence is utilized here to deliver a ranking of unlabeled samples
by their likelihood of being negative. Building on this insight, our method,
2-HNC, proceeds in two stages. The first stage generates this ranking without
assuming any negative labels, using a problem formulation that is constrained
only on positive labeled samples. The second stage augments the positive set
with likely-negative samples and recomputes the classification. The final label
prediction selects among all generated partitions in both stages, the one that
delivers a positive class proportion, closest to a prior estimate of this
quantity, which is assumed to be given. Extensive experiments across synthetic
and real datasets show that 2-HNC yields strong performance and often surpasses
existing state-of-the-art algorithms.

</details>


### [134] [Deep Probabilistic Modeling of User Behavior for Anomaly Detection via Mixture Density Networks](https://arxiv.org/abs/2505.08220)
*Lu Dai,Wenxuan Zhu,Xuehui Quan,Renzi Meng,Sheng Cai,Yichen Wang*

Main category: cs.LG

TL;DR: 提出了一种基于深度混合密度网络的异常检测方法，通过高斯混合模型和神经网络参数化，显著提升了复杂用户行为中异常模式的识别能力。


<details>
  <summary>Details</summary>
Motivation: 改进复杂用户行为中潜在异常模式的识别，解决传统方法依赖固定阈值或单一决策边界的问题。

Method: 构建由神经网络参数化的高斯混合模型，基于概率密度定义异常评分函数（负对数似然），捕捉行为数据的多模态分布特征。

Result: 在UNSW-NB15数据集上的实验表明，该方法在性能（Accuracy、F1-score、AUC）和训练稳定性上优于多种先进神经网络架构。

Conclusion: 该方法为用户行为建模和异常检测提供了更具表达力和区分性的解决方案，推动了深度概率建模技术在网络安全和智能风控领域的应用。

Abstract: To improve the identification of potential anomaly patterns in complex user
behavior, this paper proposes an anomaly detection method based on a deep
mixture density network. The method constructs a Gaussian mixture model
parameterized by a neural network, enabling conditional probability modeling of
user behavior. It effectively captures the multimodal distribution
characteristics commonly present in behavioral data. Unlike traditional
classifiers that rely on fixed thresholds or a single decision boundary, this
approach defines an anomaly scoring function based on probability density using
negative log-likelihood. This significantly enhances the model's ability to
detect rare and unstructured behaviors. Experiments are conducted on the
real-world network user dataset UNSW-NB15. A series of performance comparisons
and stability validation experiments are designed. These cover multiple
evaluation aspects, including Accuracy, F1- score, AUC, and loss fluctuation.
The results show that the proposed method outperforms several advanced neural
network architectures in both performance and training stability. This study
provides a more expressive and discriminative solution for user behavior
modeling and anomaly detection. It strongly promotes the application of deep
probabilistic modeling techniques in the fields of network security and
intelligent risk control.

</details>


### [135] [Clustering-based Low-Rank Matrix Approximation: An Adaptive Theoretical Analysis with Application to Data Compression](https://arxiv.org/abs/2505.08256)
*Sisipho Hamlomo,Marcellin Atemkeng*

Main category: cs.LG

TL;DR: 本文提出了一种自适应低秩矩阵近似（LoRMA）方法，通过分块、聚类和局部SVD处理高局部变化的数据，优于传统的全局SVD，尤其在医学影像中显著提升了压缩效率和结构保留能力。


<details>
  <summary>Details</summary>
Motivation: 传统的全局SVD方法在压缩数据矩阵时忽略了局部变化，导致细节丢失。为了解决这一问题，作者提出了自适应LoRMA方法。

Method: 将数据矩阵划分为重叠块，使用k-means聚类相似块，并在每个聚类内进行SVD。分析了块大小对压缩效率和计算成本的影响。

Result: 在MRI、超声、CT和X射线四种医学影像中，自适应LoRMA在PSNR、SSIM、MSE、IoU和EPI等指标上均优于全局SVD，显著减少了块效应和残差。

Conclusion: 自适应LoRMA在保持诊断相关性的同时优化了存储效率，尽管计算时间增加，但其在医学影像中的高压缩应用价值显著。

Abstract: Low-rank matrix approximation (LoRMA) is a fundamental tool for compressing
high-resolution data matrices by extracting important features while
suppressing redundancy. Low-rank methods, such as global singular value
decomposition (SVD), apply uniform compression across the entire data matrix,
often ignoring important local variations and leading to the loss of fine
structural details. To address these limitations, we introduce an adaptive
LoRMA, which partitions data matrix into overlapping patches, groups
structurally similar patches into several clusters using k-means, and performs
SVD within each cluster. We derive the overall compression factor accounting
for patch overlap and analyze how patch size influences compression efficiency
and computational cost. While the proposed adaptive LoRMA method is applicable
to any data exhibiting high local variation, we focus on medical imaging due to
its pronounced local variability. We evaluate and compare our adaptive LoRMA
against global SVD across four imaging modalities: MRI, ultrasound, CT scan,
and chest X-ray. Results demonstrate that adaptive LoRMA effectively preserves
structural integrity, edge details, and diagnostic relevance, as measured by
peak signal-to-noise ratio (PSNR), structural similarity index (SSIM), mean
squared error (MSE), intersection over union (IoU), and edge preservation index
(EPI). Adaptive LoRMA significantly minimizes block artifacts and residual
errors, particularly in pathological regions, consistently outperforming global
SVD in terms of PSNR, SSIM, IoU, EPI, and achieving lower MSE. Adaptive LoRMA
prioritizes clinically salient regions while allowing aggressive compression in
non-critical regions, optimizing storage efficiency. Although adaptive LoRMA
requires higher processing time, its diagnostic fidelity justifies the overhead
for high-compression applications.

</details>


### [136] [Super-fast rates of convergence for Neural Networks Classifiers under the Hard Margin Condition](https://arxiv.org/abs/2505.08262)
*Nathanael Tepakbong,Ding-Xuan Zhou,Xiang Zhou*

Main category: cs.LG

TL;DR: 研究了在Tsybakov低噪声条件下，使用ReLU激活的深度神经网络（DNNs）在二元分类问题中的性能，展示了在硬边界条件下，通过平方损失和ℓp惩罚的经验风险最小化DNNs可以实现任意大的α>0的有限样本超额风险界。


<details>
  <summary>Details</summary>
Motivation: 探索深度神经网络在低噪声条件下的分类性能，特别是在硬边界条件下的表现，以验证其理论优势。

Method: 使用平方损失替代函数和ℓp惩罚的经验风险最小化方法，分析DNNs在Tsybakov低噪声条件下的表现。

Result: 在回归函数η足够平滑的情况下，DNNs可以实现阶数为O(n^−α)的有限样本超额风险界，其中α可以任意大。

Conclusion: 研究证明了DNNs在硬边界条件下的理论优势，并提出了一种新的超额风险分解方法，可能具有独立的研究价值。

Abstract: We study the classical binary classification problem for hypothesis spaces of
Deep Neural Networks (DNNs) with ReLU activation under Tsybakov's low-noise
condition with exponent $q>0$, and its limit-case $q\to\infty$ which we refer
to as the "hard-margin condition". We show that DNNs which minimize the
empirical risk with square loss surrogate and $\ell_p$ penalty can achieve
finite-sample excess risk bounds of order $\mathcal{O}\left(n^{-\alpha}\right)$
for arbitrarily large $\alpha>0$ under the hard-margin condition, provided that
the regression function $\eta$ is sufficiently smooth. The proof relies on a
novel decomposition of the excess risk which might be of independent interest.

</details>


### [137] [LLM Enhancers for GNNs: An Analysis from the Perspective of Causal Mechanism Identification](https://arxiv.org/abs/2505.08265)
*Hang Gao,Wenxuan Huang,Fengge Wu,Junsuo Zhao,Changwen Zheng,Huaping Liu*

Main category: cs.LG

TL;DR: 论文提出了一种基于互换干预方法深入分析LLM增强器和GNN的方法，并设计了一个优化模块以改进两者间的信息传递。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM作为特征增强器在GNN中表现出潜力，但其基本特性尚未充分探索。

Method: 构建可控因果关系的合成图数据集，通过互换干预分析LLM增强器和GNN的深层特性，并设计优化模块。

Result: 实验验证了优化模块在多个数据集和模型中的有效性。

Conclusion: 研究揭示了LLM增强器和GNN的内部机制，并提供了改进信息传递的实用方案。

Abstract: The use of large language models (LLMs) as feature enhancers to optimize node
representations, which are then used as inputs for graph neural networks
(GNNs), has shown significant potential in graph representation learning.
However, the fundamental properties of this approach remain underexplored. To
address this issue, we propose conducting a more in-depth analysis of this
issue based on the interchange intervention method. First, we construct a
synthetic graph dataset with controllable causal relationships, enabling
precise manipulation of semantic relationships and causal modeling to provide
data for analysis. Using this dataset, we conduct interchange interventions to
examine the deeper properties of LLM enhancers and GNNs, uncovering their
underlying logic and internal mechanisms. Building on the analytical results,
we design a plug-and-play optimization module to improve the information
transfer between LLM enhancers and GNNs. Experiments across multiple datasets
and models validate the proposed module.

</details>


### [138] [Decoupled Multimodal Prototypes for Visual Recognition with Missing Modalities](https://arxiv.org/abs/2505.08283)
*Jueqing Lu,Yuanyuan Qi,Xiaohao Yang,Shujie Zhou,Lan Du*

Main category: cs.LG

TL;DR: 论文提出了一种基于解耦原型的多模态学习输出头，用于处理缺失模态问题，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法假设所有模态可用，但现实中常缺失，需解决性能下降问题。

Method: 提出解耦原型输出头，利用模态特定原型动态适应缺失情况。

Result: 实验表明，该方法在多种缺失场景和缺失率下显著提升性能。

Conclusion: 新输出头有效处理缺失模态，兼容现有提示方法，性能优越。

Abstract: Multimodal learning enhances deep learning models by enabling them to
perceive and understand information from multiple data modalities, such as
visual and textual inputs. However, most existing approaches assume the
availability of all modalities, an assumption that often fails in real-world
applications. Recent works have introduced learnable missing-case-aware prompts
to mitigate performance degradation caused by missing modalities while reducing
the need for extensive model fine-tuning. Building upon the effectiveness of
missing-case-aware handling for missing modalities, we propose a novel
decoupled prototype-based output head, which leverages missing-case-aware
class-wise prototypes tailored for each individual modality. This approach
dynamically adapts to different missing modality scenarios and can be
seamlessly integrated with existing prompt-based methods. Extensive experiments
demonstrate that our proposed output head significantly improves performance
across a wide range of missing-modality scenarios and varying missing rates.

</details>


### [139] [A Practical Introduction to Deep Reinforcement Learning](https://arxiv.org/abs/2505.08295)
*Yinghan Sun,Hongxi Wang,Hua Chen,Wei Zhang*

Main category: cs.LG

TL;DR: 本文是一篇关于深度强化学习（DRL）的教程，重点介绍近端策略优化（PPO）算法，旨在为初学者提供直观且实用的入门指南。


<details>
  <summary>Details</summary>
Motivation: 深度强化学习在多个领域取得了显著成功，但算法多样性和理论复杂性对初学者构成挑战。本文旨在简化学习过程。

Method: 通过广义策略迭代（GPI）框架统一算法，强调直观解释、示例和工程技巧，而非冗长的理论证明。

Result: 提供了一种高效且易于理解的DRL学习方法，帮助读者快速掌握从基础概念到高级算法的实现。

Conclusion: 本文为初学者提供了实用的DRL入门指南，特别适合希望快速上手PPO算法的读者。

Abstract: Deep reinforcement learning (DRL) has emerged as a powerful framework for
solving sequential decision-making problems, achieving remarkable success in a
wide range of applications, including game AI, autonomous driving, biomedicine,
and large language models. However, the diversity of algorithms and the
complexity of theoretical foundations often pose significant challenges for
beginners seeking to enter the field. This tutorial aims to provide a concise,
intuitive, and practical introduction to DRL, with a particular focus on the
Proximal Policy Optimization (PPO) algorithm, which is one of the most widely
used and effective DRL methods. To facilitate learning, we organize all
algorithms under the Generalized Policy Iteration (GPI) framework, offering
readers a unified and systematic perspective. Instead of lengthy theoretical
proofs, we emphasize intuitive explanations, illustrative examples, and
practical engineering techniques. This work serves as an efficient and
accessible guide, helping readers rapidly progress from basic concepts to the
implementation of advanced DRL algorithms.

</details>


### [140] [Efficient Unstructured Pruning of Mamba State-Space Models for Resource-Constrained Environments](https://arxiv.org/abs/2505.08299)
*Ibne Farabi Shihab,Sanjeda Akter,Anuj Sharma*

Main category: cs.LG

TL;DR: 提出了一种针对Mamba模型的无结构化剪枝框架，减少70%参数的同时保持95%性能，适用于资源受限环境。


<details>
  <summary>Details</summary>
Motivation: Mamba模型参数量大，难以在资源受限环境中部署，需要一种高效的剪枝方法。

Method: 结合梯度感知的幅度剪枝、迭代剪枝计划和全局剪枝策略，逐步减少参数。

Result: 在多个基准测试中，显著提升效率且性能损失极小。

Conclusion: 该剪枝方法揭示了Mamba模型的冗余性，使其更适用于资源受限场景。

Abstract: State-space models (SSMs), particularly the Mamba architecture, have emerged
as powerful alternatives to Transformers for sequence modeling, offering
linear-time complexity and competitive performance across diverse tasks.
However, their large parameter counts pose significant challenges for
deployment in resource-constrained environments. We propose a novel
unstructured pruning framework tailored for Mamba models that achieves up to
70\% parameter reduction while retaining over 95\% of the original performance.
Our approach integrates three key innovations: (1) a gradient-aware magnitude
pruning technique that combines weight magnitude and gradient information to
identify less critical parameters, (2) an iterative pruning schedule that
gradually increases sparsity to maintain model stability, and (3) a global
pruning strategy that optimizes parameter allocation across the entire model.
Through extensive experiments on WikiText-103, Long Range Arena, and ETT
time-series benchmarks, we demonstrate significant efficiency gains with
minimal performance degradation. Our analysis of pruning effects on Mamba's
components reveals critical insights into the architecture's redundancy and
robustness, enabling practical deployment in resource-constrained settings
while broadening Mamba's applicability.

</details>


### [141] [Rapid Overfitting of Multi-Pass Stochastic Gradient Descent in Stochastic Convex Optimization](https://arxiv.org/abs/2505.08306)
*Shira Vansover-Hager,Tomer Koren,Roi Livni*

Main category: cs.LG

TL;DR: 研究发现，多轮随机梯度下降（SGD）在非光滑随机凸优化（SCO）中可能导致显著的过拟合，尤其是在第二轮后性能显著下降。


<details>
  <summary>Details</summary>
Motivation: 探索多轮SGD在SCO中的样本外性能，揭示其与单轮SGD的差异及其潜在的过拟合问题。

Method: 分析多轮SGD在不同步长和轮数下的表现，并扩展到有替换的SGD。

Result: 多轮SGD在非光滑情况下可能导致样本外损失显著增加，且第二轮后的损失与步长和总步数相关。

Conclusion: 多轮SGD在非光滑SCO中易过拟合，揭示了其与单轮SGD的性能差异及平滑与非平滑情况的分离。

Abstract: We study the out-of-sample performance of multi-pass stochastic gradient
descent (SGD) in the fundamental stochastic convex optimization (SCO) model.
While one-pass SGD is known to achieve an optimal $\Theta(1/\sqrt{n})$ excess
population loss given a sample of size $n$, much less is understood about the
multi-pass version of the algorithm which is widely used in practice. Somewhat
surprisingly, we show that in the general non-smooth case of SCO, just a few
epochs of SGD can already hurt its out-of-sample performance significantly and
lead to overfitting. In particular, using a step size $\eta =
\Theta(1/\sqrt{n})$, which gives the optimal rate after one pass, can lead to
population loss as large as $\Omega(1)$ after just one additional pass. More
generally, we show that the population loss from the second pass onward is of
the order $\Theta(1/(\eta T) + \eta \sqrt{T})$, where $T$ is the total number
of steps. These results reveal a certain phase-transition in the out-of-sample
behavior of SGD after the first epoch, as well as a sharp separation between
the rates of overfitting in the smooth and non-smooth cases of SCO.
Additionally, we extend our results to with-replacement SGD, proving that the
same asymptotic bounds hold after $O(n \log n)$ steps. Finally, we also prove a
lower bound of $\Omega(\eta \sqrt{n})$ on the generalization gap of one-pass
SGD in dimension $d = \smash{\widetilde O}(n)$, improving on recent results of
Koren et al.(2022) and Schliserman et al.(2024).

</details>


### [142] [SpecSphere: Dual-Pass Spectral-Spatial Graph Neural Networks with Certified Robustness](https://arxiv.org/abs/2505.08320)
*Yoonhyuk Choi,Chong-Kwon Kim*

Main category: cs.LG

TL;DR: SpecSphere是一种新型的双通道谱空间GNN，能够同时抵御边扰动和特征扰动，适应同质性和异质性，并在保持线性时间复杂度的同时超越1-WL的表达能力。


<details>
  <summary>Details</summary>
Motivation: 解决现有GNN在异质性适应、鲁棒性和表达能力上的不足，提出一种兼具高表达性、适应性和可证明鲁棒性的架构。

Method: 结合Chebyshev多项式谱分支和注意力门控空间分支，通过轻量级MLP在合作-对抗的极小极大游戏中融合表示。

Result: 在节点分类任务上达到最先进精度，提供更严格的鲁棒性证明，并证明了其表达能力和适应性。

Conclusion: SpecSphere证明了高表达性、异质性适应和可证明鲁棒性可以在单一可扩展架构中共存。

Abstract: We introduce SpecSphere, the first dual-pass spectral-spatial GNN that
certifies every prediction against both $\ell\_{0}$ edge flips and
$\ell\_{\infty}$ feature perturbations, adapts to the full
homophily-heterophily spectrum, and surpasses the expressive power of
1-Weisfeiler-Lehman while retaining linear-time complexity. Our model couples a
Chebyshev-polynomial spectral branch with an attention-gated spatial branch and
fuses their representations through a lightweight MLP trained in a
cooperative-adversarial min-max game. We further establish (i) a uniform
Chebyshev approximation theorem, (ii) minimax-optimal risk across the
homophily-heterophily spectrum, (iii) closed-form robustness certificates, and
(iv) universal approximation strictly beyond 1-WL. SpecSphere achieves
state-of-the-art node-classification accuracy and delivers tighter certified
robustness guarantees on real-world benchmarks. These results demonstrate that
high expressivity, heterophily adaptation, and provable robustness can coexist
within a single, scalable architecture.

</details>


### [143] [FedRS-Bench: Realistic Federated Learning Datasets and Benchmarks in Remote Sensing](https://arxiv.org/abs/2505.08325)
*Haodong Zhao,Peng Peng,Chiyu Chen,Linqing Huang,Gongshen Liu*

Main category: cs.LG

TL;DR: 该论文提出了一个名为FedRS的真实联邦遥感数据集，包含8个数据集和135个客户端，解决了现有联邦学习在遥感领域缺乏真实数据集和基准的问题。


<details>
  <summary>Details</summary>
Motivation: 遥感数据通常分布广泛且受隐私限制，难以集中训练模型。联邦学习提供了一种解决方案，但缺乏真实的数据集和基准。

Method: 构建FedRS数据集，涵盖多种传感器和分辨率，并设计135个客户端以模拟真实场景的异质性。基于FedRS，实现了10种基线联邦学习算法和评估指标。

Result: 实验表明，联邦学习能显著提升模型性能，同时揭示了不同方法在客户端异质性和可用性条件下的性能权衡。

Conclusion: FedRS-Bench为遥感领域的联邦学习研究提供了标准化测试平台，有望加速相关研究并促进公平比较。

Abstract: Remote sensing (RS) images are usually produced at an unprecedented scale,
yet they are geographically and institutionally distributed, making centralized
model training challenging due to data-sharing restrictions and privacy
concerns. Federated learning (FL) offers a solution by enabling collaborative
model training across decentralized RS data sources without exposing raw data.
However, there lacks a realistic federated dataset and benchmark in RS. Prior
works typically rely on manually partitioned single dataset, which fail to
capture the heterogeneity and scale of real-world RS data, and often use
inconsistent experimental setups, hindering fair comparison. To address this
gap, we propose a realistic federated RS dataset, termed FedRS. FedRS consists
of eight datasets that cover various sensors and resolutions and builds 135
clients, which is representative of realistic operational scenarios. Data for
each client come from the same source, exhibiting authentic federated
properties such as skewed label distributions, imbalanced client data volumes,
and domain heterogeneity across clients. These characteristics reflect
practical challenges in federated RS and support evaluation of FL methods at
scale. Based on FedRS, we implement 10 baseline FL algorithms and evaluation
metrics to construct the comprehensive FedRS-Bench. The experimental results
demonstrate that FL can consistently improve model performance over training on
isolated data silos, while revealing performance trade-offs of different
methods under varying client heterogeneity and availability conditions. We hope
FedRS-Bench will accelerate research on large-scale, realistic FL in RS by
providing a standardized, rich testbed and facilitating fair comparisons across
future works. The source codes and dataset are available at
https://fedrs-bench.github.io/.

</details>


### [144] [Low-Complexity Inference in Continual Learning via Compressed Knowledge Transfer](https://arxiv.org/abs/2505.08327)
*Zhenrong Liu,Janne M. J. Huttunen,Mikko Honkala*

Main category: cs.LG

TL;DR: 论文探讨了在持续学习（CL）中如何通过模型压缩技术（如剪枝和知识蒸馏）解决大预训练模型的高计算成本问题，提出了两种针对类增量学习（CIL）的高效框架。


<details>
  <summary>Details</summary>
Motivation: 大预训练模型在持续学习中表现优异，但高计算成本限制了其实用性，尤其是在低延迟或高能效场景下。

Method: 提出了两种框架：基于剪枝的策略（包括预剪枝和后剪枝）和基于知识蒸馏的师生架构。

Result: 实验表明，两种框架在准确性和推理复杂度之间取得了更好的平衡，优于基线方法。

Conclusion: 论文分析了两种框架在准确性和效率上的权衡，为不同场景下的应用提供了指导。

Abstract: Continual learning (CL) aims to train models that can learn a sequence of
tasks without forgetting previously acquired knowledge. A core challenge in CL
is balancing stability -- preserving performance on old tasks -- and plasticity
-- adapting to new ones. Recently, large pre-trained models have been widely
adopted in CL for their ability to support both, offering strong generalization
for new tasks and resilience against forgetting. However, their high
computational cost at inference time limits their practicality in real-world
applications, especially those requiring low latency or energy efficiency. To
address this issue, we explore model compression techniques, including pruning
and knowledge distillation (KD), and propose two efficient frameworks tailored
for class-incremental learning (CIL), a challenging CL setting where task
identities are unavailable during inference. The pruning-based framework
includes pre- and post-pruning strategies that apply compression at different
training stages. The KD-based framework adopts a teacher-student architecture,
where a large pre-trained teacher transfers downstream-relevant knowledge to a
compact student. Extensive experiments on multiple CIL benchmarks demonstrate
that the proposed frameworks achieve a better trade-off between accuracy and
inference complexity, consistently outperforming strong baselines. We further
analyze the trade-offs between the two frameworks in terms of accuracy and
efficiency, offering insights into their use across different scenarios.

</details>


### [145] [Structural-Temporal Coupling Anomaly Detection with Dynamic Graph Transformer](https://arxiv.org/abs/2505.08330)
*Chang Zong,Yueting Zhuang,Jian Shao,Weiming Lu*

Main category: cs.LG

TL;DR: 提出了一种基于动态图Transformer的结构-时间耦合异常检测架构，通过二维位置编码增强，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 动态图中异常边检测的挑战在于缺乏结构-时间耦合信息，现有方法忽略了二者的深层交互。

Method: 结合结构和时间特征，采用动态图Transformer和二维位置编码捕捉异常感知的图演化模式。

Result: 在六个数据集上实验表明，该方法优于当前最先进模型。

Conclusion: 该方法在实际任务中表现出色，验证了其有效性。

Abstract: Detecting anomalous edges in dynamic graphs is an important task in many
applications over evolving triple-based data, such as social networks,
transaction management, and epidemiology. A major challenge with this task is
the absence of structural-temporal coupling information, which decreases the
ability of the representation to distinguish anomalies from normal instances.
Existing methods focus on handling independent structural and temporal features
with embedding models, which ignore the deep interaction between these two
types of information. In this paper, we propose a structural-temporal coupling
anomaly detection architecture with a dynamic graph transformer model.
Specifically, we introduce structural and temporal features from two
integration levels to provide anomaly-aware graph evolutionary patterns. Then,
a dynamic graph transformer enhanced by two-dimensional positional encoding is
implemented to capture both discrimination and contextual consistency signals.
Extensive experiments on six datasets demonstrate that our method outperforms
current state-of-the-art models. Finally, a case study illustrates the strength
of our method when applied to a real-world task.

</details>


### [146] [Reinforcement Learning (RL) Meets Urban Climate Modeling: Investigating the Efficacy and Impacts of RL-Based HVAC Control](https://arxiv.org/abs/2505.07045)
*Junjie Yu,John S. Schreck,David John Gagne,Keith W. Oleson,Jie Li,Yongtu Liang,Qi Liao,Mingfei Sun,David O. Topping,Zhonghua Zheng*

Main category: cs.LG

TL;DR: 研究提出结合强化学习（RL）与城市气候模型的框架，评估RL在HVAC控制中的效果及其对室内和城市气候的影响，发现气候背景显著影响策略效果和迁移性。


<details>
  <summary>Details</summary>
Motivation: 探索RL在HVAC控制中的潜力，同时评估其对不同气候背景下室内和城市气候的影响及策略迁移性。

Method: 结合RL与城市气候模型，包括建筑能耗模型，分析不同气候城市的RL策略效果。

Result: 气候背景显著影响RL策略的奖励（能耗与舒适度平衡）和迁移性，炎热城市表现更优。

Conclusion: 强调在多样化气候中评估RL策略的重要性，并提出城市间学习可能促进RL-HVAC部署。

Abstract: Reinforcement learning (RL)-based heating, ventilation, and air conditioning
(HVAC) control has emerged as a promising technology for reducing building
energy consumption while maintaining indoor thermal comfort. However, the
efficacy of such strategies is influenced by the background climate and their
implementation may potentially alter both the indoor climate and local urban
climate. This study proposes an integrated framework combining RL with an urban
climate model that incorporates a building energy model, aiming to evaluate the
efficacy of RL-based HVAC control across different background climates, impacts
of RL strategies on indoor climate and local urban climate, and the
transferability of RL strategies across cities. Our findings reveal that the
reward (defined as a weighted combination of energy consumption and thermal
comfort) and the impacts of RL strategies on indoor climate and local urban
climate exhibit marked variability across cities with different background
climates. The sensitivity of reward weights and the transferability of RL
strategies are also strongly influenced by the background climate. Cities in
hot climates tend to achieve higher rewards across most reward weight
configurations that balance energy consumption and thermal comfort, and those
cities with more varying atmospheric temperatures demonstrate greater RL
strategy transferability. These findings underscore the importance of
thoroughly evaluating RL-based HVAC control strategies in diverse climatic
contexts. This study also provides a new insight that city-to-city learning
will potentially aid the deployment of RL-based HVAC control.

</details>


### [147] [SHAP-based Explanations are Sensitive to Feature Representation](https://arxiv.org/abs/2505.08345)
*Hyunseung Hwang,Andrew Bell,Joao Fonseca,Venetia Pliatsika,Julia Stoyanovich,Steven Euijong Whang*

Main category: cs.LG

TL;DR: 本文研究了数据工程选择对局部特征解释的影响，发现常见的数据处理技术（如年龄直方图或种族编码）可以操纵SHAP等方法的特征重要性，甚至被攻击者利用以掩盖歧视问题。


<details>
  <summary>Details</summary>
Motivation: 探讨数据工程选择如何影响局部特征解释，填补了此前对解释器被标准数据处理技术误导的研究空白。

Method: 通过实验展示常见数据工程技术（如年龄直方图、种族编码）对SHAP等解释方法特征重要性的影响。

Result: 发现数据处理技术可以显著操纵特征重要性，甚至被攻击者利用以掩盖模型中的歧视问题。

Conclusion: 数据工程技术对解释结果具有重大影响，需警惕其潜在滥用风险。

Abstract: Local feature-based explanations are a key component of the XAI toolkit.
These explanations compute feature importance values relative to an
``interpretable'' feature representation. In tabular data, feature values
themselves are often considered interpretable. This paper examines the impact
of data engineering choices on local feature-based explanations. We demonstrate
that simple, common data engineering techniques, such as representing age with
a histogram or encoding race in a specific way, can manipulate feature
importance as determined by popular methods like SHAP. Notably, the sensitivity
of explanations to feature representation can be exploited by adversaries to
obscure issues like discrimination. While the intuition behind these results is
straightforward, their systematic exploration has been lacking. Previous work
has focused on adversarial attacks on feature-based explainers by biasing data
or manipulating models. To the best of our knowledge, this is the first study
demonstrating that explainers can be misled by standard, seemingly innocuous
data engineering techniques.

</details>


### [148] [Localization of Impacts on Thin-Walled Structures by Recurrent Neural Networks: End-to-end Learning from Real-World Data](https://arxiv.org/abs/2505.08362)
*Alexander Humer,Lukas Grasboeck,Ayech Benjeddou*

Main category: cs.LG

TL;DR: 论文提出了一种基于门控循环单元（GRU）的神经网络方法，用于薄壁结构上的冲击定位，直接从传感器数据中端到端估计冲击位置，实验结果表明该方法具有高精度。


<details>
  <summary>Details</summary>
Motivation: 薄壁结构上的冲击定位对结构健康监测（SHM）至关重要，但传统的Lamb波分析方法因色散特性难以实现精确定位。

Method: 使用GRU神经网络处理高采样率的传感器序列数据，并通过机器人实验生成物理数据训练模型。

Result: 实验结果表明，即使在数据集较小的情况下，该方法仍能实现高精度的冲击位置估计。

Conclusion: 基于GRU的端到端学习方法在冲击定位中表现出色，物理数据的应用减少了合成数据带来的现实差距。

Abstract: Today, machine learning is ubiquitous, and structural health monitoring (SHM)
is no exception. Specifically, we address the problem of impact localization on
shell-like structures, where knowledge of impact locations aids in assessing
structural integrity. Impacts on thin-walled structures excite Lamb waves,
which can be measured with piezoelectric sensors. Their dispersive
characteristics make it difficult to detect and localize impacts by
conventional methods. In the present contribution, we explore the localization
of impacts using neural networks. In particular, we propose to use {recurrent
neural networks} (RNNs) to estimate impact positions end-to-end, i.e., directly
from {sequential sensor data}. We deal with comparatively long sequences of
thousands of samples, since high sampling rate are needed to accurately capture
elastic waves. For this reason, the proposed approach builds upon Gated
Recurrent Units (GRUs), which are less prone to vanishing gradients as compared
to conventional RNNs. Quality and quantity of data are crucial when training
neural networks. Often, synthetic data is used, which inevitably introduces a
reality gap. Here, by contrast, we train our networks using {physical data from
experiments}, which requires automation to handle the large number of
experiments needed. For this purpose, a {robot is used to drop steel balls}
onto an {aluminum plate} equipped with {piezoceramic sensors}. Our results show
remarkable accuracy in estimating impact positions, even with a comparatively
small dataset.

</details>


### [149] [Density Ratio-based Causal Discovery from Bivariate Continuous-Discrete Data](https://arxiv.org/abs/2505.08371)
*Takashi Nicholas Maeda,Shohei Shimizu,Hidetoshi Matsui*

Main category: cs.LG

TL;DR: 提出了一种针对混合二元数据的因果发现方法，通过分析连续变量条件密度比的单调性确定因果方向。


<details>
  <summary>Details</summary>
Motivation: 现有基于约束或评分的方法在二元数据中效果不佳，无法公平比较不同类型变量的因果方向。

Method: 通过分析连续变量条件密度比的单调性，判断因果方向。

Result: 理论分析表明该方法在连续变量导致离散变量时具有单调性，实验验证其优于现有方法。

Conclusion: 该方法无需强分布假设，能公平比较不同类型变量的因果方向，实验表现优异。

Abstract: This paper proposes a causal discovery method for mixed bivariate data
consisting of one continuous and one discrete variable. Existing
constraint-based approaches are ineffective in the bivariate setting, as they
rely on conditional independence tests that are not suited to bivariate data.
Score-based methods either impose strong distributional assumptions or face
challenges in fairly comparing causal directions between variables of different
types, due to differences in their information content. We introduce a novel
approach that determines causal direction by analyzing the monotonicity of the
conditional density ratio of the continuous variable, conditioned on different
values of the discrete variable. Our theoretical analysis shows that the
conditional density ratio exhibits monotonicity when the continuous variable
causes the discrete variable, but not in the reverse direction. This property
provides a principled basis for comparing causal directions between variables
of different types, free from strong distributional assumptions and bias
arising from differences in their information content. We demonstrate its
effectiveness through experiments on both synthetic and real-world datasets,
showing superior accuracy compared to existing methods.

</details>


### [150] [ConDiSim: Conditional Diffusion Models for Simulation Based Inference](https://arxiv.org/abs/2505.08403)
*Mayank Nautiyal,Andreas Hellander,Prashant Singh*

Main category: cs.LG

TL;DR: ConDiSim是一个基于条件扩散模型的仿真推理方法，用于处理复杂系统的不可计算似然问题，通过噪声扩散和去噪过程近似后验分布。


<details>
  <summary>Details</summary>
Motivation: 解决复杂系统中后验分布难以直接计算的问题，提供一种高效且稳定的仿真推理框架。

Method: 利用去噪扩散概率模型，包括添加高斯噪声的前向过程和基于观测数据的去噪反向过程。

Result: 在十个基准问题和两个实际测试问题中表现出高效的后验近似能力，计算效率高且训练稳定。

Conclusion: ConDiSim为仿真推理提供了一个鲁棒且可扩展的框架，特别适用于需要快速推理的参数推断工作流。

Abstract: We present a conditional diffusion model - ConDiSim, for simulation-based
inference of complex systems with intractable likelihoods. ConDiSim leverages
denoising diffusion probabilistic models to approximate posterior
distributions, consisting of a forward process that adds Gaussian noise to
parameters, and a reverse process learning to denoise, conditioned on observed
data. This approach effectively captures complex dependencies and
multi-modalities within posteriors. ConDiSim is evaluated across ten benchmark
problems and two real-world test problems, where it demonstrates effective
posterior approximation accuracy while maintaining computational efficiency and
stability in model training. ConDiSim offers a robust and extensible framework
for simulation-based inference, particularly suitable for parameter inference
workflows requiring fast inference methods.

</details>


### [151] [Optimizing Retrieval-Augmented Generation: Analysis of Hyperparameter Impact on Performance and Efficiency](https://arxiv.org/abs/2505.08445)
*Adel Ammar,Anis Koubaa,Omer Nacar,Wadii Boulila*

Main category: cs.LG

TL;DR: 论文研究了检索增强生成（RAG）系统中超参数对速度和性能的影响，揭示了速度与精度之间的权衡，并展示了优化配置如何显著提升检索质量。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然任务表现优异，但常出现幻觉或依赖过时知识，RAG通过结合外部搜索弥补这些不足。本文旨在分析超参数如何影响RAG系统的速度和性能。

Method: 研究了Chroma和Faiss向量存储、分块策略、交叉编码器重排序及温度等超参数，评估了六项指标：忠实性、答案正确性、答案相关性、上下文精度、上下文召回和答案相似性。

Result: Chroma查询速度快13%，Faiss检索精度更高；固定长度分块优于语义分块；重排序提升质量但增加运行时。优化配置可实现99%的上下文精度。

Conclusion: 研究为实践者提供了平衡计算成本与精度的指导，展示了RAG系统在优化配置下可实现极高的检索质量，对临床决策支持等应用具有重要意义。

Abstract: Large language models achieve high task performance yet often hallucinate or
rely on outdated knowledge. Retrieval-augmented generation (RAG) addresses
these gaps by coupling generation with external search. We analyse how
hyperparameters influence speed and quality in RAG systems, covering Chroma and
Faiss vector stores, chunking policies, cross-encoder re-ranking, and
temperature, and we evaluate six metrics: faithfulness, answer correctness,
answer relevancy, context precision, context recall, and answer similarity.
Chroma processes queries 13% faster, whereas Faiss yields higher retrieval
precision, revealing a clear speed-accuracy trade-off. Naive fixed-length
chunking with small windows and minimal overlap outperforms semantic
segmentation while remaining the quickest option. Re-ranking provides modest
gains in retrieval quality yet increases runtime by roughly a factor of 5, so
its usefulness depends on latency constraints. These results help practitioners
balance computational cost and accuracy when tuning RAG systems for
transparent, up-to-date responses. Finally, we re-evaluate the top
configurations with a corrective RAG workflow and show that their advantages
persist when the model can iteratively request additional evidence. We obtain a
near-perfect context precision (99%), which demonstrates that RAG systems can
achieve extremely high retrieval accuracy with the right combination of
hyperparameters, with significant implications for applications where retrieval
quality directly impacts downstream task performance, such as clinical decision
support in healthcare.

</details>


### [152] [An adaptive sampling algorithm for data-generation to build a data-manifold for physical problem surrogate modeling](https://arxiv.org/abs/2505.08487)
*Chetra Mang,Axel TahmasebiMoradi,David Danan,Mouadh Yagoubi*

Main category: cs.LG

TL;DR: 论文提出了一种自适应采样算法（ASADG），用于生成更代表性的输入数据，以解决物理模型数据不平衡问题，并通过谐波传输问题验证其优于LHS方法。


<details>
  <summary>Details</summary>
Motivation: 物理模型的数值求解计算成本高，且数据不平衡会导致模型学习效果差，因此需要一种方法生成更具代表性的输入数据。

Method: 提出ASADG算法，通过迭代添加输入数据（如单纯复形的重心）来改善响应流形的表示。

Result: ASADG在相同数据量下比LHS方法更能准确表示响应流形。

Conclusion: ASADG算法能有效生成更具代表性的输入数据，提升模型的预测准确性。

Abstract: Physical models classically involved Partial Differential equations (PDE) and
depending of their underlying complexity and the level of accuracy required,
and known to be computationally expensive to numerically solve them. Thus, an
idea would be to create a surrogate model relying on data generated by such
solver. However, training such a model on an imbalanced data have been shown to
be a very difficult task. Indeed, if the distribution of input leads to a poor
response manifold representation, the model may not learn well and
consequently, it may not predict the outcome with acceptable accuracy. In this
work, we present an Adaptive Sampling Algorithm for Data Generation (ASADG)
involving a physical model. As the initial input data may not accurately
represent the response manifold in higher dimension, this algorithm iteratively
adds input data into it. At each step the barycenter of each simplicial
complex, that the manifold is discretized into, is added as new input data, if
a certain threshold is satisfied. We demonstrate the efficiency of the data
sampling algorithm in comparison with LHS method for generating more
representative input data. To do so, we focus on the construction of a harmonic
transport problem metamodel by generating data through a classical solver. By
using such algorithm, it is possible to generate the same number of input data
as LHS while providing a better representation of the response manifold.

</details>


### [153] [Isolation Forest in Novelty Detection Scenario](https://arxiv.org/abs/2505.08489)
*Adam Ulrich,Jan Krňávek,Roman Šenkeřík,Zuzana Komínková Oplatková,Radek Vala*

Main category: cs.LG

TL;DR: 本文提出了一种基于半空间树（HST）算法的改进方法，专门用于新颖性检测任务，通过理论分析和实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有新颖性检测算法（如One-Class SVM或LOF）缺乏可解释性和可扩展性，因此需要一种更高效且可解释的方法。

Method: 通过修改HST算法，利用新颖性点倾向于出现在树的高层叶子节点的特性，结合概率分析、期望深度计算和组合推理进行理论验证。

Result: 改进后的HST算法在新颖性检测中表现优于原始的隔离森林算法，新颖性点更易被隔离。

Conclusion: HST算法经过适当结构调整后，可作为高效且可解释的新颖性检测器，为后续应用和实验奠定了基础。

Abstract: Data mining offers a diverse toolbox for extracting meaningful structures
from complex datasets, with anomaly detection emerging as a critical subfield
particularly in the context of streaming or real-time data. Within anomaly
detection, novelty detection focuses on identifying previously unseen patterns
after training solely on regular data. While classic algorithms such as
One-Class SVM or Local Outlier Factor (LOF) have been widely applied, they
often lack interpretability and scalability. In this work, we explore the
Half-Space Tree (HST) algorithm, originally proposed for streaming anomaly
detection, and propose a novel theoretical modification to adapt it
specifically for novelty detection tasks. Our approach is grounded in the idea
that anomalies i.e., novelties tend to appear in the higher leaves of the tree,
which are less frequently visited by regular instances. We analytically
demonstrate the effectiveness of this approach using probabilistic analysis,
expected depth (EXD) calculations, and combinatorial reasoning. A comparative
analysis of expected depths between our modified HST and the original Isolation
Forest highlights that novelty points are significantly more isolated in our
approach. This supports the hypothesis that HSTs, with appropriate structural
adaptation, can serve as interpretable and efficient novelty detectors. The
paper contributes a theoretical foundation and supporting analysis for this
adaptation, setting the stage for further application and experimentation.

</details>


### [154] [A new methodology to decompose a parametric domain using reduced order data manifold in machine learning](https://arxiv.org/abs/2505.08497)
*Chetra Mang,Axel TahmasebiMoradi,Mouadh Yagoubi*

Main category: cs.LG

TL;DR: 提出了一种基于迭代主成分分析的参数域分解新方法，通过降维和重构逆投影器实现高效分解，并通过数值实验验证其优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统元模型（如神经网络）在处理高维参数域分解时效率不足，需开发更高效的方法。

Method: 采用迭代主成分分析降维，开发两种逆投影器重构方法，并基于低维流形分解参数域。

Result: 数值实验表明，该方法在谐波传输问题上比传统元模型更高效有效。

Conclusion: 新方法在参数域分解中表现出优越性能，为高维问题提供了有效解决方案。

Abstract: We propose a new methodology for parametric domain decomposition using
iterative principal component analysis. Starting with iterative principle
component analysis, the high dimension manifold is reduced to the lower
dimension manifold. Moreover, two approaches are developed to reconstruct the
inverse projector to project from the lower data component to the original one.
Afterward, we provide a detailed strategy to decompose the parametric domain
based on the low dimension manifold. Finally, numerical examples of harmonic
transport problem are given to illustrate the efficiency and effectiveness of
the proposed method comparing to the classical meta-models such as neural
networks.

</details>


### [155] [InfoPO: On Mutual Information Maximization for Large Language Model Alignment](https://arxiv.org/abs/2505.08507)
*Teng Xiao,Zhen Ge,Sujay Sanghavi,Tian Wang,Julian Katz-Samuels,Marc Versage,Qingjun Cui,Trishul Chilimbi*

Main category: cs.LG

TL;DR: 本文提出了一种名为InfoPO的新算法，用于基于人类偏好数据对大型语言模型进行后训练，解决了现有方法依赖Bradley-Terry模型导致的过拟合和性能不佳问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于Bradley-Terry模型的偏好优化方法容易过拟合且在推理任务中表现不佳，需要一种更高效且不依赖BT模型的优化方法。

Method: 提出InfoPO算法，通过消除对BT模型的依赖并防止选定响应概率下降，实现对语言模型的高效对齐。

Result: 实验表明，InfoPO在多个公开基准测试中表现优于现有基线方法，尤其在推理任务中。

Conclusion: InfoPO是一种高效且无需依赖BT模型的偏好优化方法，显著提升了语言模型在推理任务中的性能。

Abstract: We study the post-training of large language models (LLMs) with human
preference data. Recently, direct preference optimization and its variants have
shown considerable promise in aligning language models, eliminating the need
for reward models and online sampling. Despite these benefits, these methods
rely on explicit assumptions about the Bradley-Terry (BT) model, which makes
them prone to overfitting and results in suboptimal performance, particularly
on reasoning-heavy tasks. To address these challenges, we propose a principled
preference fine-tuning algorithm called InfoPO, which effectively and
efficiently aligns large language models using preference data. InfoPO
eliminates the reliance on the BT model and prevents the likelihood of the
chosen response from decreasing. Extensive experiments confirm that InfoPO
consistently outperforms established baselines on widely used open benchmarks,
particularly in reasoning tasks.

</details>


### [156] [Learning Advanced Self-Attention for Linear Transformers in the Singular Value Domain](https://arxiv.org/abs/2505.08516)
*Hyowon Wi,Jeongwhan Choi,Noseong Park*

Main category: cs.LG

TL;DR: 论文提出了一种名为AGF的新方法，将自注意力机制解释为图信号处理中的图滤波器，并在多项任务中实现了最优性能。


<details>
  <summary>Details</summary>
Motivation: 现有自注意力机制仅作为低通滤波器，无法有效利用多种频率信息，设计较为简化。

Method: 将自注意力视为图信号处理中的图滤波器，提出AGF方法，在奇异值域学习图滤波器，复杂度为线性。

Result: AGF在Long Range Arena基准测试和时间序列分类等任务中取得了最优性能。

Conclusion: AGF通过图信号处理视角改进了自注意力机制，显著提升了模型性能。

Abstract: Transformers have demonstrated remarkable performance across diverse domains.
The key component of Transformers is self-attention, which learns the
relationship between any two tokens in the input sequence. Recent studies have
revealed that the self-attention can be understood as a normalized adjacency
matrix of a graph. Notably, from the perspective of graph signal processing
(GSP), the self-attention can be equivalently defined as a simple graph filter,
applying GSP using the value vector as the signal. However, the self-attention
is a graph filter defined with only the first order of the polynomial matrix,
and acts as a low-pass filter preventing the effective leverage of various
frequency information. Consequently, existing self-attention mechanisms are
designed in a rather simplified manner. Therefore, we propose a novel method,
called \underline{\textbf{A}}ttentive \underline{\textbf{G}}raph
\underline{\textbf{F}}ilter (AGF), interpreting the self-attention as learning
the graph filter in the singular value domain from the perspective of graph
signal processing for directed graphs with the linear complexity w.r.t. the
input length $n$, i.e., $\mathcal{O}(nd^2)$. In our experiments, we demonstrate
that AGF achieves state-of-the-art performance on various tasks, including Long
Range Arena benchmark and time series classification.

</details>


### [157] [GradMix: Gradient-based Selective Mixup for Robust Data Augmentation in Class-Incremental Learning](https://arxiv.org/abs/2505.08528)
*Minsu Kim,Seong-Hyeon Hwang,Steven Euijong Whang*

Main category: cs.LG

TL;DR: 论文提出GradMix，一种基于梯度的选择性数据增强方法，用于缓解持续学习中的灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 持续学习中，如何在获取新知识的同时保持旧知识是一个挑战。现有方法通常使用经验回放技术，但随机混合样本可能损害旧任务知识并加剧灾难性遗忘。

Method: 提出GradMix方法，通过基于类别的梯度选择性混合样本，仅混合有益类别对，避免有害类别对。

Result: 实验表明，GradMix在多种真实数据集上优于基线方法，显著减少了旧知识的遗忘。

Conclusion: GradMix通过选择性混合样本，有效缓解了灾难性遗忘，提升了持续学习性能。

Abstract: In the context of continual learning, acquiring new knowledge while
maintaining previous knowledge presents a significant challenge. Existing
methods often use experience replay techniques that store a small portion of
previous task data for training. In experience replay approaches, data
augmentation has emerged as a promising strategy to further improve the model
performance by mixing limited previous task data with sufficient current task
data. However, we theoretically and empirically analyze that training with
mixed samples from random sample pairs may harm the knowledge of previous tasks
and cause greater catastrophic forgetting. We then propose GradMix, a robust
data augmentation method specifically designed for mitigating catastrophic
forgetting in class-incremental learning. GradMix performs gradient-based
selective mixup using a class-based criterion that mixes only samples from
helpful class pairs and not from detrimental class pairs for reducing
catastrophic forgetting. Our experiments on various real datasets show that
GradMix outperforms data augmentation baselines in accuracy by minimizing the
forgetting of previous knowledge.

</details>


### [158] [ExEBench: Benchmarking Foundation Models on Extreme Earth Events](https://arxiv.org/abs/2505.08529)
*Shan Zhao,Zhitong Xiong,Jie Zhao,Xiao Xiang Zhu*

Main category: cs.LG

TL;DR: ExEBench是一个针对极端事件的基准数据集，旨在评估基础模型在灾害管理中的泛化能力，并推动新机器学习方法的发展。


<details>
  <summary>Details</summary>
Motivation: 极端事件对人类和生态系统构成重大风险，而基础模型在灾害管理中表现出潜力，但其性能可能受数据偏差影响。

Method: 引入ExEBench数据集，覆盖七类极端事件，包含多种数据源和任务，以评估模型泛化能力。

Result: ExEBench提供了一个平台，用于分析极端事件的交互和级联效应，并推动灾害管理方法的创新。

Conclusion: ExEBench有望促进对极端事件的理解，并为应对气候变化提供支持。

Abstract: Our planet is facing increasingly frequent extreme events, which pose major
risks to human lives and ecosystems. Recent advances in machine learning (ML),
especially with foundation models (FMs) trained on extensive datasets, excel in
extracting features and show promise in disaster management. Nevertheless,
these models often inherit biases from training data, challenging their
performance over extreme values. To explore the reliability of FM in the
context of extreme events, we introduce \textbf{ExE}Bench (\textbf{Ex}treme
\textbf{E}arth Benchmark), a collection of seven extreme event categories
across floods, wildfires, storms, tropical cyclones, extreme precipitation,
heatwaves, and cold waves. The dataset features global coverage, varying data
volumes, and diverse data sources with different spatial, temporal, and
spectral characteristics. To broaden the real-world impact of FMs, we include
multiple challenging ML tasks that are closely aligned with operational needs
in extreme events detection, monitoring, and forecasting. ExEBench aims to (1)
assess FM generalizability across diverse, high-impact tasks and domains, (2)
promote the development of novel ML methods that benefit disaster management,
and (3) offer a platform for analyzing the interactions and cascading effects
of extreme events to advance our understanding of Earth system, especially
under the climate change expected in the decades to come. The dataset and code
are public https://github.com/zhaoshan2/EarthExtreme-Bench.

</details>


### [159] [OLinear: A Linear Model for Time Series Forecasting in Orthogonally Transformed Domain](https://arxiv.org/abs/2505.08550)
*Wenzhen Yue,Yong Liu,Haoxuan Li,Hao Wang,Xianghua Ying,Ruohao Guo,Bowei Xing,Ji Shi*

Main category: cs.LG

TL;DR: OLinear是一种基于线性变换的多变量时间序列预测模型，通过正交变换域提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统时间域预测模型因依赖关系复杂而性能受限，固定变换基（如傅里叶变换）缺乏适应性。

Method: 使用OrthoTrans数据自适应正交变换和NormLin线性层，捕捉多变量依赖关系。

Result: 在24个基准和140个任务中表现优异，效率高，NormLin模块显著提升Transformer性能。

Conclusion: OLinear通过正交变换和NormLin模块实现了高效且高性能的时间序列预测。

Abstract: This paper presents $\mathbf{OLinear}$, a $\mathbf{linear}$-based
multivariate time series forecasting model that operates in an
$\mathbf{o}$rthogonally transformed domain. Recent forecasting models typically
adopt the temporal forecast (TF) paradigm, which directly encode and decode
time series in the time domain. However, the entangled step-wise dependencies
in series data can hinder the performance of TF. To address this, some
forecasters conduct encoding and decoding in the transformed domain using
fixed, dataset-independent bases (e.g., sine and cosine signals in the Fourier
transform). In contrast, we utilize $\mathbf{OrthoTrans}$, a data-adaptive
transformation based on an orthogonal matrix that diagonalizes the series'
temporal Pearson correlation matrix. This approach enables more effective
encoding and decoding in the decorrelated feature domain and can serve as a
plug-in module to enhance existing forecasters. To enhance the representation
learning for multivariate time series, we introduce a customized linear layer,
$\mathbf{NormLin}$, which employs a normalized weight matrix to capture
multivariate dependencies. Empirically, the NormLin module shows a surprising
performance advantage over multi-head self-attention, while requiring nearly
half the FLOPs. Extensive experiments on 24 benchmarks and 140 forecasting
tasks demonstrate that OLinear consistently achieves state-of-the-art
performance with high efficiency. Notably, as a plug-in replacement for
self-attention, the NormLin module consistently enhances Transformer-based
forecasters. The code and datasets are available at
https://anonymous.4open.science/r/OLinear

</details>


### [160] [Online Learning and Unlearning](https://arxiv.org/abs/2505.08557)
*Yaxi Hu,Bernhard Schölkopf,Amartya Sanyal*

Main category: cs.LG

TL;DR: 论文提出了在线学习-遗忘（OLU）问题，并基于在线梯度下降（OGD）设计了两种算法：被动OLU和主动OLU，均能在保证遗忘性能的同时维持与传统OGD相当的遗憾界。


<details>
  <summary>Details</summary>
Motivation: 研究在线学习场景中如何高效处理数据遗忘请求，确保模型在遗忘数据点后的输出与从未使用该数据点训练的模型统计不可区分。

Method: 提出两种OLU算法：1）被动OLU利用OGD的收缩性，在遗忘时注入噪声；2）主动OLU结合离线遗忘算法，调整模型以排除被删除数据。

Result: 在凸性和平滑性假设下，两种方法均实现了与传统OGD相当的遗憾界，证明了在提供遗忘保证的同时仍能保持竞争力。

Conclusion: 研究表明，在线学习-遗忘问题可以通过高效算法解决，且不影响模型性能，为实际应用提供了理论支持。

Abstract: We formalize the problem of online learning-unlearning, where a model is
updated sequentially in an online setting while accommodating unlearning
requests between updates. After a data point is unlearned, all subsequent
outputs must be statistically indistinguishable from those of a model trained
without that point. We present two online learner-unlearner (OLU) algorithms,
both built upon online gradient descent (OGD). The first, passive OLU,
leverages OGD's contractive property and injects noise when unlearning occurs,
incurring no additional computation. The second, active OLU, uses an offline
unlearning algorithm that shifts the model toward a solution excluding the
deleted data. Under standard convexity and smoothness assumptions, both methods
achieve regret bounds comparable to those of standard OGD, demonstrating that
one can maintain competitive regret bounds while providing unlearning
guarantees.

</details>


### [161] [MUBox: A Critical Evaluation Framework of Deep Machine Unlearning](https://arxiv.org/abs/2505.08576)
*Xiang Li,Bhavani Thuraisingham,Wenqi Wei*

Main category: cs.LG

TL;DR: MUBox是一个评估深度学习中去学习方法的平台，整合了23种先进技术，通过6种场景和11种指标评估，揭示了现有方法的局限性和评估指标的不足。


<details>
  <summary>Details</summary>
Motivation: 法律框架要求数据删除权，机器去学习成为解决方案，但现有方法在复杂场景下表现不一致，评估指标不全面。

Method: MUBox平台整合23种去学习方法，在6种场景下使用11种指标进行评估和比较。

Result: 发现现有方法在复杂任务中表现不一致，评估需多指标结合，去毒方法效果因攻击类型而异。

Conclusion: 需更广泛的评估场景和多指标结合，以全面评估去学习方法。

Abstract: Recent legal frameworks have mandated the right to be forgotten, obligating
the removal of specific data upon user requests. Machine Unlearning has emerged
as a promising solution by selectively removing learned information from
machine learning models. This paper presents MUBox, a comprehensive platform
designed to evaluate unlearning methods in deep learning. MUBox integrates 23
advanced unlearning techniques, tested across six practical scenarios with 11
diverse evaluation metrics. It allows researchers and practitioners to (1)
assess and compare the effectiveness of different machine unlearning methods
across various scenarios; (2) examine the impact of current evaluation metrics
on unlearning performance; and (3) conduct detailed comparative studies on
machine unlearning in a unified framework. Leveraging MUBox, we systematically
evaluate these unlearning methods in deep learning and uncover several key
insights: (a) Even state-of-the-art unlearning methods, including those
published in top-tier venues and winners of unlearning competitions,
demonstrate inconsistent effectiveness across diverse scenarios. Prior research
has predominantly focused on simplified settings, such as random forgetting and
class-wise unlearning, highlighting the need for broader evaluations across
more difficult unlearning tasks. (b) Assessing unlearning performance remains a
non-trivial problem, as no single evaluation metric can comprehensively capture
the effectiveness, efficiency, and preservation of model utility. Our findings
emphasize the necessity of employing multiple metrics to achieve a balanced and
holistic assessment of unlearning methods. (c) In the context of depoisoning,
our evaluation reveals significant variability in the effectiveness of existing
approaches, which is highly dependent on the specific type of poisoning
attacks.

</details>


### [162] [Clustering of Incomplete Data via a Bipartite Graph Structure](https://arxiv.org/abs/2505.08594)
*Amirhossein Javaheri,Daniel P. Palomar*

Main category: cs.LG

TL;DR: 提出了一种基于二分图模型的聚类方法，解决了中心节点数据缺失和重尾数据处理的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有二分图模型需要中心节点数据，且高斯模型对重尾数据效果不佳，限制了实际应用。

Method: 设计了一种无需中心节点数据的二分图聚类方法，并优化了对重尾数据的处理。

Result: 数值实验验证了该方法在金融数据聚类中的高效性。

Conclusion: 该方法在数据不完整和重尾分布情况下表现优越，适用于实际场景。

Abstract: There are various approaches to graph learning for data clustering,
incorporating different spectral and structural constraints through diverse
graph structures. Some methods rely on bipartite graph models, where nodes are
divided into two classes: centers and members. These models typically require
access to data for the center nodes in addition to observations from the member
nodes. However, such additional data may not always be available in many
practical scenarios. Moreover, popular Gaussian models for graph learning have
demonstrated limited effectiveness in modeling data with heavy-tailed
distributions, which are common in financial markets. In this paper, we propose
a clustering method based on a bipartite graph model that addresses these
challenges. First, it can infer clusters from incomplete data without requiring
information about the center nodes. Second, it is designed to effectively
handle heavy-tailed data. Numerical experiments using real financial data
validate the efficiency of the proposed method for data clustering.

</details>


### [163] [Cost Function Estimation Using Inverse Reinforcement Learning with Minimal Observations](https://arxiv.org/abs/2505.08619)
*Sarmad Mehrdad,Avadesh Meduri,Ludovic Righetti*

Main category: cs.LG

TL;DR: 提出一种迭代逆强化学习算法，用于在连续空间中推断最优成本函数，基于最大熵准则，通过迭代改进权重并确保学习到的成本函数特征与演示轨迹特征相似。


<details>
  <summary>Details</summary>
Motivation: 现有方法需要大量样本且无法单独调整每个观察的有效性，限制了学习效率。

Method: 基于最大熵准则，迭代改进权重并优化步长，通过求解最优控制问题生成样本轨迹。

Result: 相比两种先进算法，在多个模拟环境中表现出更快的学习和更高的效率。

Conclusion: 该方法在样本效率和轨迹信息量上优于现有技术，适用于连续空间中的逆强化学习问题。

Abstract: We present an iterative inverse reinforcement learning algorithm to infer
optimal cost functions in continuous spaces. Based on a popular maximum entropy
criteria, our approach iteratively finds a weight improvement step and proposes
a method to find an appropriate step size that ensures learned cost function
features remain similar to the demonstrated trajectory features. In contrast to
similar approaches, our algorithm can individually tune the effectiveness of
each observation for the partition function and does not need a large sample
set, enabling faster learning. We generate sample trajectories by solving an
optimal control problem instead of random sampling, leading to more informative
trajectories. The performance of our method is compared to two state of the art
algorithms to demonstrate its benefits in several simulated environments.

</details>


### [164] [Credit Assignment and Efficient Exploration based on Influence Scope in Multi-agent Reinforcement Learning](https://arxiv.org/abs/2505.08630)
*Shuai Han,Mehdi Dastani,Shihan Wang*

Main category: cs.LG

TL;DR: 提出了一种新方法ISA，用于解决稀疏奖励场景中的信用分配和探索问题，显著优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 稀疏奖励场景中，多智能体强化学习面临信用分配和探索的挑战，现有方法效果不佳。

Method: 通过计算智能体对状态维度的影响范围（ISA），利用动作与状态属性的相互依赖关系进行信用分配和探索空间限制。

Result: 在多种稀疏奖励场景中，ISA方法显著优于现有基线。

Conclusion: ISA方法有效解决了稀疏奖励场景中的信用分配和探索问题，性能优越。

Abstract: Training cooperative agents in sparse-reward scenarios poses significant
challenges for multi-agent reinforcement learning (MARL). Without clear
feedback on actions at each step in sparse-reward setting, previous methods
struggle with precise credit assignment among agents and effective exploration.
In this paper, we introduce a novel method to deal with both credit assignment
and exploration problems in reward-sparse domains. Accordingly, we propose an
algorithm that calculates the Influence Scope of Agents (ISA) on states by
taking specific value of the dimensions/attributes of states that can be
influenced by individual agents. The mutual dependence between agents' actions
and state attributes are then used to calculate the credit assignment and to
delimit the exploration space for each individual agent. We then evaluate ISA
in a variety of sparse-reward multi-agent scenarios. The results show that our
method significantly outperforms the state-of-art baselines.

</details>


### [165] [Modular Federated Learning: A Meta-Framework Perspective](https://arxiv.org/abs/2505.08646)
*Frederico Vicente,Cláudia Soares,Dušan Jakovetić*

Main category: cs.LG

TL;DR: 该论文提出了一种联邦学习（FL）的元框架视角，将其视为模块化组件的组合，并强调聚合与对齐的双重作用，为FL研究和部署提供了全面的基础。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在保护隐私的同时实现分布式机器学习训练，但其复杂性需要系统化的理解和方法。

Method: 通过历史背景梳理、模块化框架构建、新分类法（聚合与对齐）提出及Python框架探索，系统化FL的研究与实践。

Result: 提出了一个模块化的FL元框架，强调聚合与对齐的作用，并总结了FL各子领域的关键挑战和开放问题。

Conclusion: 该调查为FL研究和实际应用提供了灵活且全面的基础，突出了聚合与对齐的重要性。

Abstract: Federated Learning (FL) enables distributed machine learning training while
preserving privacy, representing a paradigm shift for data-sensitive and
decentralized environments. Despite its rapid advancements, FL remains a
complex and multifaceted field, requiring a structured understanding of its
methodologies, challenges, and applications. In this survey, we introduce a
meta-framework perspective, conceptualising FL as a composition of modular
components that systematically address core aspects such as communication,
optimisation, security, and privacy. We provide a historical contextualisation
of FL, tracing its evolution from distributed optimisation to modern
distributed learning paradigms. Additionally, we propose a novel taxonomy
distinguishing Aggregation from Alignment, introducing the concept of alignment
as a fundamental operator alongside aggregation. To bridge theory with
practice, we explore available FL frameworks in Python, facilitating real-world
implementation. Finally, we systematise key challenges across FL sub-fields,
providing insights into open research questions throughout the meta-framework
modules. By structuring FL within a meta-framework of modular components and
emphasising the dual role of Aggregation and Alignment, this survey provides a
holistic and adaptable foundation for understanding and advancing FL research
and deployment.

</details>


### [166] [AC-PKAN: Attention-Enhanced and Chebyshev Polynomial-Based Physics-Informed Kolmogorov-Arnold Networks](https://arxiv.org/abs/2505.08687)
*Hangwei Zhang,Zhimu Huang,Yan Wang*

Main category: cs.LG

TL;DR: 论文提出了一种改进的Kolmogorov-Arnold网络（AC-PKAN），通过结合小波激活的MLP和注意力机制，解决了Chebyshev1KANs的秩崩溃问题，并提升了PDE求解能力。


<details>
  <summary>Details</summary>
Motivation: 原始的KANs和Chebyshev1KANs在计算和内存效率上存在问题，且Chebyshev1KANs存在秩崩溃，限制了其表达能力。

Method: 通过集成小波激活的MLP、内部注意力机制和外部残差梯度注意力（RGA）机制，设计了AC-PKAN架构。

Result: AC-PKAN在多个基准任务中表现优于或匹配现有最佳模型（如PINNsFormer），适用于零数据或数据稀疏场景。

Conclusion: AC-PKAN显著提升了KANs的表达能力，成为解决复杂工程问题的有效工具。

Abstract: Kolmogorov-Arnold Networks (KANs) have recently shown promise for solving
partial differential equations (PDEs). Yet their original formulation is
computationally and memory intensive, motivating the introduction of Chebyshev
Type-I-based KANs (Chebyshev1KANs). Although Chebyshev1KANs have outperformed
the vanilla KANs architecture, our rigorous theoretical analysis reveals that
they still suffer from rank collapse, ultimately limiting their expressive
capacity. To overcome these limitations, we enhance Chebyshev1KANs by
integrating wavelet-activated MLPs with learnable parameters and an internal
attention mechanism. We prove that this design preserves a full-rank Jacobian
and is capable of approximating solutions to PDEs of arbitrary order.
Furthermore, to alleviate the loss instability and imbalance introduced by the
Chebyshev polynomial basis, we externally incorporate a Residual Gradient
Attention (RGA) mechanism that dynamically re-weights individual loss terms
according to their gradient norms and residual magnitudes. By jointly
leveraging internal and external attention, we present AC-PKAN, a novel
architecture that constitutes an enhancement to weakly supervised
Physics-Informed Neural Networks (PINNs) and extends the expressive power of
KANs. Experimental results from nine benchmark tasks across three domains show
that AC-PKAN consistently outperforms or matches state-of-the-art models such
as PINNsFormer, establishing it as a highly effective tool for solving complex
real-world engineering problems in zero-data or data-sparse regimes. The code
will be made publicly available upon acceptance.

</details>


### [167] [PWC-MoE: Privacy-Aware Wireless Collaborative Mixture of Experts](https://arxiv.org/abs/2505.08719)
*Yang Su,Na Yan,Yansha Deng,Robert Schober*

Main category: cs.LG

TL;DR: 论文提出了一种隐私感知的无线协作专家混合框架（PWC-MoE），以平衡计算成本、性能和隐私保护，适用于带宽受限环境。


<details>
  <summary>Details</summary>
Motivation: 解决云服务器上大型语言模型（LLMs）的隐私问题和带宽需求，以及本地小型语言模型（SLMs）性能不足的问题。

Method: 使用稀疏隐私感知门控网络动态路由敏感和非敏感令牌，引入负载均衡机制和带宽自适应令牌卸载方案。

Result: 实验表明PWC-MoE在带宽受限环境下有效保护隐私并保持高性能。

Conclusion: PWC-MoE为隐私敏感和带宽受限场景提供了实用的LLM部署方案。

Abstract: Large language models (LLMs) hosted on cloud servers alleviate the
computational and storage burdens on local devices but raise privacy concerns
due to sensitive data transmission and require substantial communication
bandwidth, which is challenging in constrained environments. In contrast, small
language models (SLMs) running locally enhance privacy but suffer from limited
performance on complex tasks. To balance computational cost, performance, and
privacy protection under bandwidth constraints, we propose a privacy-aware
wireless collaborative mixture of experts (PWC-MoE) framework. Specifically,
PWC-MoE employs a sparse privacy-aware gating network to dynamically route
sensitive tokens to privacy experts located on local clients, while
non-sensitive tokens are routed to non-privacy experts located at the remote
base station. To achieve computational efficiency, the gating network ensures
that each token is dynamically routed to and processed by only one expert. To
enhance scalability and prevent overloading of specific experts, we introduce a
group-wise load-balancing mechanism for the gating network that evenly
distributes sensitive tokens among privacy experts and non-sensitive tokens
among non-privacy experts. To adapt to bandwidth constraints while preserving
model performance, we propose a bandwidth-adaptive and importance-aware token
offloading scheme. This scheme incorporates an importance predictor to evaluate
the importance scores of non-sensitive tokens, prioritizing the most important
tokens for transmission to the base station based on their predicted importance
and the available bandwidth. Experiments demonstrate that the PWC-MoE framework
effectively preserves privacy and maintains high performance even in
bandwidth-constrained environments, offering a practical solution for deploying
LLMs in privacy-sensitive and bandwidth-limited scenarios.

</details>


### [168] [Memorization-Compression Cycles Improve Generalization](https://arxiv.org/abs/2505.08727)
*Fangyuan Yu*

Main category: cs.LG

TL;DR: 论文提出了一种新的语言建模目标（IBLM），通过压缩内部表征提升泛化能力，并设计了一种自适应训练算法（GAPT），显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 探索数据扩展和表征压缩对泛化能力的共同影响，并模拟生物学习中的记忆-压缩循环。

Method: 引入IBLM目标，将语言建模重构为约束优化问题；提出GAPT算法，动态切换记忆和压缩阶段。

Result: GAPT在GPT-2预训练中降低表征熵50%，提升交叉熵4.8%，并在算术乘法任务中提升OOD泛化35%。

Conclusion: GAPT通过模拟生物学习机制，显著提升模型泛化能力，减少干扰，验证了表征压缩的重要性。

Abstract: We prove theoretically that generalization improves not only through data
scaling but also by compressing internal representations. To operationalize
this insight, we introduce the Information Bottleneck Language Modeling (IBLM)
objective, which reframes language modeling as a constrained optimization
problem: minimizing representation entropy subject to optimal prediction
performance. Empirically, we observe an emergent memorization-compression cycle
during LLM pretraining, evidenced by oscillation positive/negative gradient
alignment between cross-entropy and Matrix-Based Entropy (MBE), a measure of
representation entropy. This pattern closely mirrors the predictive-compressive
trade-off prescribed by IBLM and also parallels the biological alternation
between awake learning and sleep consolidation. Motivated by this observation,
we propose Gated Phase Transition (GAPT), a training algorithm that adaptively
switches between memorization and compression phases. When applied to GPT-2
pretraining on FineWeb dataset, GAPT reduces MBE by 50% and improves
cross-entropy by 4.8%. GAPT improves OOD generalizatino by 35% in a pretraining
task on arithmetic multiplication. In a setting designed to simulate
catastrophic forgetting, GAPT reduces interference by compressing and
separating representations, achieving a 97% improvement in separation -
paralleling the functional role of sleep consolidation.

</details>


### [169] [Preference Optimization for Combinatorial Optimization Problems](https://arxiv.org/abs/2505.08735)
*Mingjun Pan,Guanquan Lin,You-Wei Luo,Bin Zhu,Zhien Dai,Lijun Sun,Chun Yuan*

Main category: cs.LG

TL;DR: 提出了一种名为偏好优化的新方法，通过将定量奖励信号转化为定性偏好信号，解决强化学习在组合优化中的效率问题。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法在组合优化中存在奖励信号减弱和探索效率低的问题，导致性能不佳。

Method: 通过重新参数化奖励函数并结合偏好模型，提出熵正则化的强化学习目标，同时结合局部搜索技术优化偏好对生成。

Result: 在多个基准问题（如TSP、CVRP和FFSP）上显著优于现有算法，收敛效率和解决方案质量均更优。

Conclusion: 偏好优化方法有效解决了强化学习在组合优化中的效率问题，具有广泛的应用潜力。

Abstract: Reinforcement Learning (RL) has emerged as a powerful tool for neural
combinatorial optimization, enabling models to learn heuristics that solve
complex problems without requiring expert knowledge. Despite significant
progress, existing RL approaches face challenges such as diminishing reward
signals and inefficient exploration in vast combinatorial action spaces,
leading to inefficiency. In this paper, we propose Preference Optimization, a
novel method that transforms quantitative reward signals into qualitative
preference signals via statistical comparison modeling, emphasizing the
superiority among sampled solutions. Methodologically, by reparameterizing the
reward function in terms of policy and utilizing preference models, we
formulate an entropy-regularized RL objective that aligns the policy directly
with preferences while avoiding intractable computations. Furthermore, we
integrate local search techniques into the fine-tuning rather than
post-processing to generate high-quality preference pairs, helping the policy
escape local optima. Empirical results on various benchmarks, such as the
Traveling Salesman Problem (TSP), the Capacitated Vehicle Routing Problem
(CVRP) and the Flexible Flow Shop Problem (FFSP), demonstrate that our method
significantly outperforms existing RL algorithms, achieving superior
convergence efficiency and solution quality.

</details>


### [170] [Towards Foundation Models for Experimental Readout Systems Combining Discrete and Continuous Data](https://arxiv.org/abs/2505.08736)
*James Giroux,Cristiano Fanelli*

Main category: cs.LG

TL;DR: 提出了一种用于核物理的基础模型，能够处理未来电子离子对撞机中成像切伦科夫探测器的低级别输入，解决了现有方法的局限性，并通过三项创新实现了高保真生成。


<details>
  <summary>Details</summary>
Motivation: 现有方法在分辨率损失和条件生成方面存在不足，需要一种能够高效处理探测器输入并支持条件生成的模型。

Method: 提出三项创新：(i) 分离离散空间特征和连续变量的词汇表，(ii) 通过前置上下文嵌入实现连续运动学条件生成，(iii) 高分辨率连续变量标记化。

Result: 模型能够快速生成高保真的切伦科夫光子像素和时间序列，并在重建任务（如介子和K介子识别）中表现出泛化能力。

Conclusion: 该模型在核物理探测器数据处理中表现出高效性和灵活性，支持后续微调任务。

Abstract: We present a (proto) Foundation Model for Nuclear Physics, capable of
operating on low-level detector inputs from Imaging Cherenkov Detectors at the
future Electron Ion Collider. To address limitations in existing next-token
prediction approaches-namely resolution loss from VQ-VAE tokenization and lack
of conditional generation-we propose three key innovations: (i) separate
vocabularies for discrete spatial features and continuous variates, combined
via Causal Multi-Head Cross-Attention (CMHCA), (ii) continuous kinematic
conditioning through prepended context embeddings, and (iii) scalable and
simple, high-resolution continuous variate tokenization without joint
vocabulary inflation. Our model enables fast, high-fidelity generation of pixel
and time sequences for Cherenkov photons, validated through closure tests in
the High Performance DIRC. We also show our model generalizes to reconstruction
tasks such as pion and kaon identification, in which we show its ability to
leverage fine-tuning.

</details>


### [171] [Sensitivity-Constrained Fourier Neural Operators for Forward and Inverse Problems in Parametric Differential Equations](https://arxiv.org/abs/2505.08740)
*Abdolmehdi Behroozi,Chaopeng Shen and,Daniel Kifer*

Main category: cs.LG

TL;DR: SC-FNO是一种基于敏感度正则化的方法，用于解决传统FNO在逆问题、敏感度估计和概念漂移中的局限性，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 传统FNO在逆问题和敏感度估计方面表现不佳，且难以应对概念漂移，因此需要一种更高效的方法。

Method: 提出SC-FNO，通过敏感度正则化策略优化FNO，提升其在参数反演和高维参数空间中的表现。

Result: SC-FNO在预测解路径和参数反演任务中表现优于标准FNO，且减少了数据和训练需求。

Conclusion: SC-FNO是一种高效且通用的方法，适用于多种微分方程和神经算子。

Abstract: Parametric differential equations of the form du/dt = f(u, x, t, p) are
fundamental in science and engineering. While deep learning frameworks such as
the Fourier Neural Operator (FNO) can efficiently approximate solutions, they
struggle with inverse problems, sensitivity estimation (du/dp), and concept
drift. We address these limitations by introducing a sensitivity-based
regularization strategy, called Sensitivity-Constrained Fourier Neural
Operators (SC-FNO). SC-FNO achieves high accuracy in predicting solution paths
and consistently outperforms standard FNO and FNO with physics-informed
regularization. It improves performance in parameter inversion tasks, scales to
high-dimensional parameter spaces (tested with up to 82 parameters), and
reduces both data and training requirements. These gains are achieved with a
modest increase in training time (30% to 130% per epoch) and generalize across
various types of differential equations and neural operators. Code and selected
experiments are available at: https://github.com/AMBehroozi/SC_Neural_Operators

</details>


### [172] [Implet: A Post-hoc Subsequence Explainer for Time Series Models](https://arxiv.org/abs/2505.08748)
*Fanyu Meng,Ziwen Kan,Shahbaz Rezaei,Zhaodan Kong,Xin Chen,Xin Liu*

Main category: cs.LG

TL;DR: Implet是一种新颖的后处理解释器，用于时间序列模型，提供准确且简洁的子序列级解释，并通过群体级框架进一步提升解释的简洁性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 时间序列模型的可解释性对于建立信任、调试和实际应用中的可解释性至关重要。

Method: 引入Implet，一种后处理解释器，识别关键时间片段，并提出群体级解释框架。

Result: 在多个标准时间序列分类基准上验证了Implet的有效性。

Conclusion: Implet显著提升了时间序列模型的解释性，代码已开源。

Abstract: Explainability in time series models is crucial for fostering trust,
facilitating debugging, and ensuring interpretability in real-world
applications. In this work, we introduce Implet, a novel post-hoc explainer
that generates accurate and concise subsequence-level explanations for time
series models. Our approach identifies critical temporal segments that
significantly contribute to the model's predictions, providing enhanced
interpretability beyond traditional feature-attribution methods. Based on it,
we propose a cohort-based (group-level) explanation framework designed to
further improve the conciseness and interpretability of our explanations. We
evaluate Implet on several standard time-series classification benchmarks,
demonstrating its effectiveness in improving interpretability. The code is
available at https://github.com/LbzSteven/implet

</details>


### [173] [SPAT: Sensitivity-based Multihead-attention Pruning on Time Series Forecasting Models](https://arxiv.org/abs/2505.08768)
*Suhan Guo,Jiahong Deng,Mengjun Yi,Furao Shen,Jian Zhao*

Main category: cs.LG

TL;DR: SPAT是一种结构化剪枝方法，通过选择性移除冗余注意力机制，显著提升模型效率，同时降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有的注意力架构在多元时间序列预测中表现优异，但计算成本高，需要更高效的剪枝方法。

Method: 提出SPAT方法，利用动态敏感性度量SEND评估注意力模块的重要性，并移除冗余模块。

Result: 实验显示，SPAT剪枝模型在MSE、MAE和FLOPs上分别降低2.842%、1.996%和35.274%，性能优于现有轻量级方法。

Conclusion: SPAT通过保留最有效的注意力机制，显著提升模型效率，适用于标准及零样本推理。

Abstract: Attention-based architectures have achieved superior performance in
multivariate time series forecasting but are computationally expensive.
Techniques such as patching and adaptive masking have been developed to reduce
their sizes and latencies. In this work, we propose a structured pruning
method, SPAT ($\textbf{S}$ensitivity $\textbf{P}$runer for
$\textbf{At}$tention), which selectively removes redundant attention mechanisms
and yields highly effective models. Different from previous approaches, SPAT
aims to remove the entire attention module, which reduces the risk of
overfitting and enables speed-up without demanding specialized hardware. We
propose a dynamic sensitivity metric, $\textbf{S}$ensitivity
$\textbf{E}$nhanced $\textbf{N}$ormalized $\textbf{D}$ispersion (SEND) that
measures the importance of each attention module during the pre-training phase.
Experiments on multivariate datasets demonstrate that SPAT-pruned models
achieve reductions of 2.842% in MSE, 1.996% in MAE, and 35.274% in FLOPs.
Furthermore, SPAT-pruned models outperform existing lightweight, Mamba-based
and LLM-based SOTA methods in both standard and zero-shot inference,
highlighting the importance of retaining only the most effective attention
mechanisms. We have made our code publicly available
https://anonymous.4open.science/r/SPAT-6042.

</details>


### [174] [Addressing the Current Challenges of Quantum Machine Learning through Multi-Chip Ensembles](https://arxiv.org/abs/2505.08782)
*Junghoon Justin Park,Jiook Cha,Samuel Yen-Chi Chen,Huan-Hsin Tseng,Shinjae Yoo*

Main category: cs.LG

TL;DR: 提出了一种多芯片集成变分量子电路框架，以解决NISQ设备的噪声、可扩展性和可训练性问题。


<details>
  <summary>Details</summary>
Motivation: 量子机器学习（QML）在解决计算难题方面具有潜力，但受限于NISQ设备的噪声、可扩展性和变分量子电路的可训练性问题。

Method: 通过将高维计算分配到多个小型量子芯片上，增强可扩展性、可训练性和噪声鲁棒性，并利用受控纠缠。

Result: 实验表明，该方法缓解了贫瘠高原问题，减少了量子误差偏差和方差，并在标准基准数据集和真实数据集上验证了其有效性。

Conclusion: 该框架为近期量子硬件上的可扩展QML提供了可行方案。

Abstract: Quantum Machine Learning (QML) holds significant promise for solving
computational challenges across diverse domains. However, its practical
deployment is constrained by the limitations of noisy intermediate-scale
quantum (NISQ) devices, including noise, limited scalability, and trainability
issues in variational quantum circuits (VQCs). We introduce the multi-chip
ensemble VQC framework, which partitions high-dimensional computations across
smaller quantum chips to enhance scalability, trainability, and noise
resilience. We show that this approach mitigates barren plateaus, reduces
quantum error bias and variance, and maintains robust generalization through
controlled entanglement. Designed to align with current and emerging quantum
hardware, the framework demonstrates strong potential for enabling scalable QML
on near-term devices, as validated by experiments on standard benchmark
datasets (MNIST, FashionMNIST, CIFAR-10) and real world dataset (PhysioNet
EEG).

</details>


### [175] [CodePDE: An Inference Framework for LLM-driven PDE Solver Generation](https://arxiv.org/abs/2505.08783)
*Shanda Li,Tanya Marwah,Junhong Shen,Weiwei Sun,Andrej Risteski,Yiming Yang,Ameet Talwalkar*

Main category: cs.LG

TL;DR: CodePDE利用大型语言模型（LLM）生成PDE求解器，无需特定任务调优，实现了超人类性能。


<details>
  <summary>Details</summary>
Motivation: 传统数值求解器依赖专家知识且计算成本高，神经网络求解器需要大量数据且缺乏可解释性。

Method: 将PDE求解视为代码生成任务，引入CodePDE框架，利用LLM的推理、调试和自优化能力。

Result: CodePDE在多种代表性PDE问题上表现优异，并提供了对LLM生成求解器的系统性分析。

Conclusion: LLM在PDE求解中展现出潜力，但也存在局限性，为未来模型设计提供了新视角。

Abstract: Partial differential equations (PDEs) are fundamental to modeling physical
systems, yet solving them remains a complex challenge. Traditional numerical
solvers rely on expert knowledge to implement and are computationally
expensive, while neural-network-based solvers require large training datasets
and often lack interpretability. In this work, we frame PDE solving as a code
generation task and introduce CodePDE, the first inference framework for
generating PDE solvers using large language models (LLMs). Leveraging advanced
inference-time algorithms and scaling strategies, CodePDE unlocks critical
capacities of LLM for PDE solving: reasoning, debugging, selfrefinement, and
test-time scaling -- all without task-specific tuning. CodePDE achieves
superhuman performance across a range of representative PDE problems. We also
present a systematic empirical analysis of LLM generated solvers, analyzing
their accuracy, efficiency, and numerical scheme choices. Our findings
highlight the promise and the current limitations of LLMs in PDE solving,
offering a new perspective on solver design and opportunities for future model
development. Our code is available at https://github.com/LithiumDA/CodePDE.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [176] [Building-Block Aware Generative Modeling for 3D Crystals of Metal Organic Frameworks](https://arxiv.org/abs/2505.08531)
*Chenru Duan,Aditya Nandy,Sizhan Liu,Yuanqi Du,Liu He,Yi Qu,Haojun Jia,Jin-Hu Dou*

Main category: physics.chem-ph

TL;DR: BBA MOF Diffusion是一种生成模型，通过SE(3)-等变扩散模型学习MOF的3D全原子表示，显著扩展了化学空间，并成功合成了一种新型MOF。


<details>
  <summary>Details</summary>
Motivation: 金属有机框架（MOFs）的设计空间巨大，传统方法难以高效探索。生成模型虽具潜力，但现有模型受限于已知构建块或小单元。

Method: 提出BBA MOF Diffusion模型，基于SE(3)-等变扩散，学习单个构建块的3D全原子表示，并明确编码晶体拓扑网络。

Result: 模型能生成包含1000个原子的MOFs，具有高几何有效性、新颖性和多样性。成功合成了一种预测的高分MOF，验证了其结构保真度。

Conclusion: BBA MOF Diffusion为合成高性能MOFs提供了实用途径，显著扩展了可访问的化学空间。

Abstract: Metal-organic frameworks (MOFs) marry inorganic nodes, organic edges, and
topological nets into programmable porous crystals, yet their astronomical
design space defies brute-force synthesis. Generative modeling holds ultimate
promise, but existing models either recycle known building blocks or are
restricted to small unit cells. We introduce Building-Block-Aware MOF Diffusion
(BBA MOF Diffusion), an SE(3)-equivariant diffusion model that learns 3D
all-atom representations of individual building blocks, encoding
crystallographic topological nets explicitly. Trained on the CoRE-MOF database,
BBA MOF Diffusion readily samples MOFs with unit cells containing 1000 atoms
with great geometric validity, novelty, and diversity mirroring experimental
databases. Its native building-block representation produces unprecedented
metal nodes and organic edges, expanding accessible chemical space by orders of
magnitude. One high-scoring [Zn(1,4-TDC)(EtOH)2] MOF predicted by the model was
synthesized, where powder X-ray diffraction, thermogravimetric analysis, and N2
sorption confirm its structural fidelity. BBA-Diff thus furnishes a practical
pathway to synthesizable and high-performing MOFs.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [177] [CellVerse: Do Large Language Models Really Understand Cell Biology?](https://arxiv.org/abs/2505.07865)
*Fan Zhang,Tianyu Liu,Zhihong Zhu,Hao Wu,Haixin Wang,Donghao Zhou,Yefeng Zheng,Kun Wang,Xian Wu,Pheng-Ann Heng*

Main category: q-bio.QM

TL;DR: 论文介绍了CellVerse，一个基于语言的单细胞分析基准测试，评估了多种大语言模型（LLMs）在单细胞生物学任务中的表现，发现现有模型仍有显著改进空间。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏对LLMs在语言驱动的单细胞分析任务中性能的全面评估，CellVerse旨在填补这一空白。

Method: 通过整合四种单细胞多组学数据，构建了涵盖细胞类型注释、药物反应预测和扰动分析三个层次的基准测试，并评估了14种LLMs的性能。

Result: 实验结果显示，现有模型在单细胞生物学任务中表现有限，尤其是药物反应预测任务中表现不佳。

Conclusion: CellVerse为通过自然语言推进单细胞生物学研究奠定了基础，并揭示了LLMs在该领域的应用仍面临重大挑战。

Abstract: Recent studies have demonstrated the feasibility of modeling single-cell data
as natural languages and the potential of leveraging powerful large language
models (LLMs) for understanding cell biology. However, a comprehensive
evaluation of LLMs' performance on language-driven single-cell analysis tasks
still remains unexplored. Motivated by this challenge, we introduce CellVerse,
a unified language-centric question-answering benchmark that integrates four
types of single-cell multi-omics data and encompasses three hierarchical levels
of single-cell analysis tasks: cell type annotation (cell-level), drug response
prediction (drug-level), and perturbation analysis (gene-level). Going beyond
this, we systematically evaluate the performance across 14 open-source and
closed-source LLMs ranging from 160M to 671B on CellVerse. Remarkably, the
experimental results reveal: (1) Existing specialist models (C2S-Pythia) fail
to make reasonable decisions across all sub-tasks within CellVerse, while
generalist models such as Qwen, Llama, GPT, and DeepSeek family models exhibit
preliminary understanding capabilities within the realm of cell biology. (2)
The performance of current LLMs falls short of expectations and has substantial
room for improvement. Notably, in the widely studied drug response prediction
task, none of the evaluated LLMs demonstrate significant performance
improvement over random guessing. CellVerse offers the first large-scale
empirical demonstration that significant challenges still remain in applying
LLMs to cell biology. By introducing CellVerse, we lay the foundation for
advancing cell biology through natural languages and hope this paradigm could
facilitate next-generation single-cell analysis.

</details>


### [178] [Automated Model-Free Sorting of Single-Molecule Fluorescence Events Using a Deep Learning Based Hidden-State Model](https://arxiv.org/abs/2505.08608)
*Wenqi Zeng,Shuqi Zhou,Yuan Yao,Chunlai Chen*

Main category: q-bio.QM

TL;DR: DASH是一种全自动的单分子荧光数据分析架构，无需用户输入，适用于平衡和非平衡系统。


<details>
  <summary>Details</summary>
Motivation: 传统单分子荧光数据分析依赖人工经验，难以扩展和复现，现有深度学习模型仍需手动干预或大量标注数据。

Method: 提出DASH架构，实现轨迹分类、状态分配和自动排序，无需用户输入。

Result: DASH在Cas12a介导的DNA切割等非平衡系统中表现稳健。

Conclusion: DASH为单分子水平生物动力学研究提供了一种自动化和详细的解决方案。

Abstract: Single-molecule fluorescence assays enable high-resolution analysis of
biomolecular dynamics, but traditional analysis pipelines are labor-intensive
and rely on users' experience, limiting scalability and reproducibility. Recent
deep learning models have automated aspects of data processing, yet many still
require manual thresholds, complex architectures, or extensive labeled data.
Therefore, we present DASH, a fully streamlined architecture for trace
classification, state assignment, and automatic sorting that requires no user
input. DASH demonstrates robust performance across users and experimental
conditions both in equilibrium and non-equilibrium systems such as
Cas12a-mediated DNA cleavage. This paper proposes a novel strategy for the
automatic and detailed sorting of single-molecule fluorescence events. The
dynamic cleavage process of Cas12a is used as an example to provide a
comprehensive analysis. This approach is crucial for studying biokinetic
structural changes at the single-molecule level.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [179] [NAZM: Network Analysis of Zonal Metrics in Persian Poetic Tradition](https://arxiv.org/abs/2505.08052)
*Kourosh Shahnazari,Seyed Moein Ayyoubzadeh*

Main category: cs.SI

TL;DR: 该研究通过构建多维相似性网络，模拟古典波斯诗人的影响力动态，结合语义、词汇、风格、主题和韵律特征，识别关键诗人、风格中心和桥梁诗人，并通过社区检测算法划分诗人群体。


<details>
  <summary>Details</summary>
Motivation: 旨在通过计算模型揭示波斯诗人之间的相互影响，为文学研究提供数据驱动的新视角。

Method: 使用Ganjoor语料库构建数据集，通过相似性矩阵和网络分析（如中心性度量）研究诗人影响力，并应用Louvain算法划分诗人群体。

Result: 研究发现了一些在结构上具有重要意义的相对不知名诗人，并验证了已知文学流派的风格一致性。

Conclusion: 该研究为诗歌传统提供了可解释且可扩展的计算模型，推动了数字人文领域的研究。

Abstract: This study formalizes a computational model to simulate classical Persian
poets' dynamics of influence through constructing a multi-dimensional
similarity network. Using a rigorously curated dataset based on Ganjoor's
corpus, we draw upon semantic, lexical, stylistic, thematic, and metrical
features to demarcate each poet's corpus. Each is contained within weighted
similarity matrices, which are then appended to generate an aggregate graph
showing poet-to-poet influence. Further network investigation is carried out to
identify key poets, style hubs, and bridging poets by calculating degree,
closeness, betweenness, eigenvector, and Katz centrality measures. Further, for
typological insight, we use the Louvain community detection algorithm to
demarcate clusters of poets sharing both style and theme coherence, which
correspond closely to acknowledged schools of literature like Sabk-e Hindi,
Sabk-e Khorasani, and the Bazgasht-e Adabi phenomenon. Our findings provide a
new data-driven view of Persian literature distinguished between canonical
significance and interextual influence, thus highlighting relatively
lesser-known figures who hold great structural significance. Combining
computational linguistics with literary study, this paper produces an
interpretable and scalable model for poetic tradition, enabling retrospective
reflection as well as forward-looking research within digital humanities.

</details>


### [180] [The Truth Becomes Clearer Through Debate! Multi-Agent Systems with Large Language Models Unmask Fake News](https://arxiv.org/abs/2505.08532)
*Yuhan Liu,Yuxuan Liu,Xiaoqing Zhang,Xiuying Chen,Rui Yan*

Main category: cs.SI

TL;DR: 论文提出了一种名为TruEDebate（TED）的多智能体系统，利用大语言模型（LLMs）通过辩论过程增强假新闻检测的可解释性和有效性。


<details>
  <summary>Details</summary>
Motivation: 当前假新闻检测方法存在可解释性差或未能充分利用LLMs推理能力的问题，受“真理越辩越明”启发，研究旨在通过辩论过程改进检测。

Method: TED系统包含DebateFlow Agents和InsightFlow Agents，前者组织辩论团队模拟正式辩论流程，后者通过合成与分析子代理提供综合评估和最终判断。

Result: TED通过模拟人类辩论过程，实现了对新闻内容的全面评估，提高了假新闻检测的可解释性和效果。

Conclusion: TED系统通过多智能体辩论框架，显著提升了假新闻检测的可解释性和性能，为未来研究提供了新思路。

Abstract: In today's digital environment, the rapid propagation of fake news via social
networks poses significant social challenges. Most existing detection methods
either employ traditional classification models, which suffer from low
interpretability and limited generalization capabilities, or craft specific
prompts for large language models (LLMs) to produce explanations and results
directly, failing to leverage LLMs' reasoning abilities fully. Inspired by the
saying that "truth becomes clearer through debate," our study introduces a
novel multi-agent system with LLMs named TruEDebate (TED) to enhance the
interpretability and effectiveness of fake news detection. TED employs a
rigorous debate process inspired by formal debate settings. Central to our
approach are two innovative components: the DebateFlow Agents and the
InsightFlow Agents. The DebateFlow Agents organize agents into two teams, where
one supports and the other challenges the truth of the news. These agents
engage in opening statements, cross-examination, rebuttal, and closing
statements, simulating a rigorous debate process akin to human discourse
analysis, allowing for a thorough evaluation of news content. Concurrently, the
InsightFlow Agents consist of two specialized sub-agents: the Synthesis Agent
and the Analysis Agent. The Synthesis Agent summarizes the debates and provides
an overarching viewpoint, ensuring a coherent and comprehensive evaluation. The
Analysis Agent, which includes a role-aware encoder and a debate graph,
integrates role embeddings and models the interactions between debate roles and
arguments using an attention mechanism, providing the final judgment.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [181] [A Large-Scale Empirical Analysis of Custom GPTs' Vulnerabilities in the OpenAI Ecosystem](https://arxiv.org/abs/2505.08148)
*Sunday Oyinlola Ogundoyin,Muhammad Ikram,Hassan Jameel Asghar,Benjamin Zi Hao Zhao,Dali Kaafar*

Main category: cs.CR

TL;DR: 研究分析了14,904个自定义GPT模型，发现95%以上存在安全漏洞，最常见的是角色扮演攻击、系统提示泄露和钓鱼内容生成。


<details>
  <summary>Details</summary>
Motivation: 随着自定义GPT模型的广泛使用，其安全漏洞问题日益突出，但现有研究缺乏实证和大规模评估。

Method: 通过分析14,904个自定义GPT模型，评估其对七种可被利用威胁的易感性，并引入多指标排名系统。

Result: 95%以上的自定义GPT缺乏足够的安全保护，主要漏洞包括角色扮演攻击（96.51%）、系统提示泄露（92.20%）和钓鱼（91.22%）。

Conclusion: 研究呼吁加强安全措施和内容审核，以确保GPT应用的安全部署。

Abstract: Millions of users leverage generative pretrained transformer (GPT)-based
language models developed by leading model providers for a wide range of tasks.
To support enhanced user interaction and customization, many platforms-such as
OpenAI-now enable developers to create and publish tailored model instances,
known as custom GPTs, via dedicated repositories or application stores. These
custom GPTs empower users to browse and interact with specialized applications
designed to meet specific needs. However, as custom GPTs see growing adoption,
concerns regarding their security vulnerabilities have intensified. Existing
research on these vulnerabilities remains largely theoretical, often lacking
empirical, large-scale, and statistically rigorous assessments of associated
risks.
  In this study, we analyze 14,904 custom GPTs to assess their susceptibility
to seven exploitable threats, such as roleplay-based attacks, system prompt
leakage, phishing content generation, and malicious code synthesis, across
various categories and popularity tiers within the OpenAI marketplace. We
introduce a multi-metric ranking system to examine the relationship between a
custom GPT's popularity and its associated security risks.
  Our findings reveal that over 95% of custom GPTs lack adequate security
protections. The most prevalent vulnerabilities include roleplay-based
vulnerabilities (96.51%), system prompt leakage (92.20%), and phishing
(91.22%). Furthermore, we demonstrate that OpenAI's foundational models exhibit
inherent security weaknesses, which are often inherited or amplified in custom
GPTs. These results highlight the urgent need for enhanced security measures
and stricter content moderation to ensure the safe deployment of GPT-based
applications.

</details>


### [182] [Privacy-Preserving Analytics for Smart Meter (AMI) Data: A Hybrid Approach to Comply with CPUC Privacy Regulations](https://arxiv.org/abs/2505.08237)
*Benjamin Westrich*

Main category: cs.CR

TL;DR: 本文探讨了如何利用数据匿名化、隐私保护机器学习、合成数据生成和加密技术，在保护用户隐私的同时对智能电表数据进行分析。


<details>
  <summary>Details</summary>
Motivation: 智能电表数据（AMI）为公用事业和消费者提供宝贵洞察，但也引发隐私问题。加州法规要求严格保护用户能源使用数据，因此需要开发隐私保护技术。

Method: 综合评估了数据匿名化、隐私保护机器学习（差分隐私和联邦学习）、合成数据生成和加密技术（安全多方计算、同态加密），并提出一种混合架构。

Result: 提出了一种满足加州隐私法规和FIPPs的混合架构，支持高级分析（如预测、个性化洞察）且严格保护隐私。

Conclusion: 为公用事业数据科学家和工程师提供了隐私设计的蓝图，支持数据驱动创新和法规合规。

Abstract: Advanced Metering Infrastructure (AMI) data from smart electric and gas
meters enables valuable insights for utilities and consumers, but also raises
significant privacy concerns. In California, regulatory decisions (CPUC
D.11-07-056 and D.11-08-045) mandate strict privacy protections for customer
energy usage data, guided by the Fair Information Practice Principles (FIPPs).
We comprehensively explore solutions drawn from data anonymization,
privacy-preserving machine learning (differential privacy and federated
learning), synthetic data generation, and cryptographic techniques (secure
multiparty computation, homomorphic encryption). This allows advanced
analytics, including machine learning models, statistical and econometric
analysis on energy consumption data, to be performed without compromising
individual privacy.
  We evaluate each technique's theoretical foundations, effectiveness, and
trade-offs in the context of utility data analytics, and we propose an
integrated architecture that combines these methods to meet real-world needs.
The proposed hybrid architecture is designed to ensure compliance with
California's privacy rules and FIPPs while enabling useful analytics, from
forecasting and personalized insights to academic research and econometrics,
while strictly protecting individual privacy. Mathematical definitions and
derivations are provided where appropriate to demonstrate privacy guarantees
and utility implications rigorously. We include comparative evaluations of the
techniques, an architecture diagram, and flowcharts to illustrate how they work
together in practice. The result is a blueprint for utility data scientists and
engineers to implement privacy-by-design in AMI data handling, supporting both
data-driven innovation and strict regulatory compliance.

</details>


### [183] [Securing RAG: A Risk Assessment and Mitigation Framework](https://arxiv.org/abs/2505.08728)
*Lukas Ammann,Sara Ott,Christoph R. Landolt,Marco P. Lehmann*

Main category: cs.CR

TL;DR: 本文探讨了检索增强生成（RAG）的安全与隐私挑战，提出了漏洞分析、缓解措施及一个结合RAG特定安全考量的框架。


<details>
  <summary>Details</summary>
Motivation: RAG已成为用户端NLP应用的标准，但其数据集成能力带来了新的安全和隐私问题，需系统化解决方案。

Method: 首先分析RAG管道的漏洞和攻击面，提出缓解措施；其次开发一个结合RAG特定安全考量和现有标准的框架。

Result: 提出了一个指导实现安全、合规且可信RAG系统的框架。

Conclusion: 通过结构化分析和框架设计，为RAG系统的安全实施提供了实用指南。

Abstract: Retrieval Augmented Generation (RAG) has emerged as the de facto industry
standard for user-facing NLP applications, offering the ability to integrate
data without re-training or fine-tuning Large Language Models (LLMs). This
capability enhances the quality and accuracy of responses but also introduces
novel security and privacy challenges, particularly when sensitive data is
integrated. With the rapid adoption of RAG, securing data and services has
become a critical priority. This paper first reviews the vulnerabilities of RAG
pipelines, and outlines the attack surface from data pre-processing and data
storage management to integration with LLMs. The identified risks are then
paired with corresponding mitigations in a structured overview. In a second
step, the paper develops a framework that combines RAG-specific security
considerations, with existing general security guidelines, industry standards,
and best practices. The proposed framework aims to guide the implementation of
robust, compliant, secure, and trustworthy RAG systems.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [184] [SciCom Wiki: Fact-Checking and FAIR Knowledge Distribution for Scientific Videos and Podcasts](https://arxiv.org/abs/2505.07912)
*Tim Wittenborg,Constantin Sebastian Tremel,Niklas Stehr,Oliver Karras,Markus Stocker,Sören Auer*

Main category: cs.DL

TL;DR: 该论文提出了一种名为SciCom Wiki的协作平台，旨在通过FAIR原则和神经符号计算事实检查工具，支持科学传播知识基础设施（SciCom KI）的发展，以应对视频和播客中的错误信息。


<details>
  <summary>Details</summary>
Motivation: 民主社会需要可靠的信息来源，但视频和播客等非文本媒体既是信息传播的工具，也可能传播错误信息。现有的SciCom KI基础设施尚不完善，无法应对内容泛滥的挑战。

Method: 研究通过调查53位利益相关者的需求，开发了基于Wikibase的开源平台SciCom Wiki，并设计了一种神经符号计算事实检查方法，将异构媒体转换为知识图谱。

Result: 原型平台和事实检查工具通过专家访谈和用户调查验证了其必要性和可用性。研究发现SciCom KI在FAIR知识和协作系统方面严重不足。

Conclusion: SciCom Wiki及其事实检查框架能够满足需求，但需要更多协作努力以应对信息泛滥的挑战。

Abstract: Democratic societies need accessible, reliable information. Videos and
Podcasts have established themselves as the medium of choice for civic
dissemination, but also as carriers of misinformation. The emerging Science
Communication Knowledge Infrastructure (SciCom KI) curating non-textual media
is still fragmented and not adequately equipped to scale against the content
flood. Our work sets out to support the SciCom KI with a central, collaborative
platform, the SciCom Wiki, to facilitate FAIR (findable, accessible,
interoperable, reusable) media representation and the fact-checking of their
content, particularly for videos and podcasts. Building an open-source service
system centered around Wikibase, we survey requirements from 53 stakeholders,
refine these in 11 interviews, and evaluate our prototype based on these
requirements with another 14 participants. To address the most requested
feature, fact-checking, we developed a neurosymbolic computational
fact-checking approach, converting heterogenous media into knowledge graphs.
This increases machine-readability and allows comparing statements against
equally represented ground-truth. Our computational fact-checking tool was
iteratively evaluated through 10 expert interviews, a public user survey with
43 participants verified the necessity and usability of our tool. Overall, our
findings identified several needs to systematically support the SciCom KI. The
SciCom Wiki, as a FAIR digital library complementing our neurosymbolic
computational fact-checking framework, was found suitable to address the raised
requirements. Further, we identified that the SciCom KI is severely
underdeveloped regarding FAIR knowledge and related systems facilitating its
collaborative creation and curation. Our system can provide a central knowledge
node, yet a collaborative effort is required to scale against the imminent
(mis-)information flood.

</details>


<div id='physics.data-an'></div>

# physics.data-an [[Back]](#toc)

### [185] [Contrastive Normalizing Flows for Uncertainty-Aware Parameter Estimation](https://arxiv.org/abs/2505.08709)
*Ibrahim Elsharkawy,Yonatan Kahn*

Main category: physics.data-an

TL;DR: 论文提出了一种基于对比归一化流（CNFs）的新方法，用于解决高能物理（HEP）中数据分布偏移下的不确定性感知参数估计问题，并在HiggsML Uncertainty Challenge数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 在物理科学中，机器学习（ML）用于参数估计时，系统不确定性（如探测器校准误差）会导致数据分布扭曲，影响统计精度。目前，在数据分布偏移下实现不确定性感知的参数估计仍是一个开放性问题。

Method: 通过对比归一化流（CNFs）嵌入数据和参数，生成可调节的对比分布，结合分类器和频域技术，实现鲁棒的参数估计和不确定性量化。

Result: 该方法在HiggsML Uncertainty Challenge数据集上表现优异，证明了其对于数据分布偏移的鲁棒性。

Conclusion: CNFs结合分类器和频域技术，为数据分布偏移下的参数估计和不确定性量化提供了理论支持和实际解决方案。

Abstract: Estimating physical parameters from data is a crucial application of machine
learning (ML) in the physical sciences. However, systematic uncertainties, such
as detector miscalibration, induce data distribution distortions that can erode
statistical precision. In both high-energy physics (HEP) and broader ML
contexts, achieving uncertainty-aware parameter estimation under these domain
shifts remains an open problem. In this work, we address this challenge of
uncertainty-aware parameter estimation for a broad set of tasks critical for
HEP. We introduce a novel approach based on Contrastive Normalizing Flows
(CNFs), which achieves top performance on the HiggsML Uncertainty Challenge
dataset. Building on the insight that a binary classifier can approximate the
model parameter likelihood ratio, we address the practical limitations of
expressivity and the high cost of simulating high-dimensional parameter grids
by embedding data and parameters in a learned CNF mapping. This mapping yields
a tunable contrastive distribution that enables robust classification under
shifted data distributions. Through a combination of theoretical analysis and
empirical evaluations, we demonstrate that CNFs, when coupled with a classifier
and established frequentist techniques, provide principled parameter estimation
and uncertainty quantification through classification that is robust to data
distribution distortions.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [186] [Justified Evidence Collection for Argument-based AI Fairness Assurance](https://arxiv.org/abs/2505.08064)
*Alpay Sabuncuoglu,Christopher Burr,Carsten Maple*

Main category: cs.HC

TL;DR: 本文提出了一种基于系统工程的框架，通过动态论证保证方法来确保AI系统的公平性，分为需求规划和持续监控两个阶段，并在金融领域进行了案例验证。


<details>
  <summary>Details</summary>
Motivation: 确保AI系统的公平性是一个复杂的社会技术挑战，需要跨学科和多利益相关方的协作。动态论证保证方法被提出以系统化地评估和缓解风险。

Method: 框架分为两阶段：1）需求规划阶段，团队定义目标和声明；2）持续监控阶段，通过工具动态收集证据支持论证。

Result: 通过金融领域的案例研究验证了框架的有效性，特别是在支持公平性相关论证方面。

Conclusion: 该框架为AI系统的公平性提供了一种可操作的方法，结合了系统工程和动态论证保证。

Abstract: It is well recognised that ensuring fair AI systems is a complex
sociotechnical challenge, which requires careful deliberation and continuous
oversight across all stages of a system's lifecycle, from defining requirements
to model deployment and deprovisioning. Dynamic argument-based assurance cases,
which present structured arguments supported by evidence, have emerged as a
systematic approach to evaluating and mitigating safety risks and hazards in
AI-enabled system development and have also been extended to deal with broader
normative goals such as fairness and explainability. This paper introduces a
systems-engineering-driven framework, supported by software tooling, to
operationalise a dynamic approach to argument-based assurance in two stages. In
the first stage, during the requirements planning phase, a multi-disciplinary
and multi-stakeholder team define goals and claims to be established (and
evidenced) by conducting a comprehensive fairness governance process. In the
second stage, a continuous monitoring interface gathers evidence from existing
artefacts (e.g. metrics from automated tests), such as model, data, and use
case documentation, to support these arguments dynamically. The framework's
effectiveness is demonstrated through an illustrative case study in finance,
with a focus on supporting fairness-related arguments.

</details>


### [187] [Communication Styles and Reader Preferences of LLM and Human Experts in Explaining Health Information](https://arxiv.org/abs/2505.08143)
*Jiawei Zhou,Kritika Venkatachalam,Minje Choi,Koustuv Saha,Munmun De Choudhury*

Main category: cs.HC

TL;DR: 研究探讨了大型语言模型（LLM）在健康信息事实核查中的沟通风格，发现其与人类专家在说服策略、确定性表达和社会价值观一致性上存在差异，但人类评估显示对LLM内容的偏好。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在信息辅助中的广泛应用，研究其与人类沟通风格和价值观的一致性至关重要，尤其是在健康信息事实核查这一关键领域。

Method: 通过比较1498条健康错误信息的LLM生成解释与权威机构的解释，从信息、发送者和接收者三个维度评估沟通风格，并进行99名参与者的盲评。

Result: LLM生成内容在说服策略、确定性表达和社会价值观一致性上得分较低，但人类评估显示60%以上参与者更偏好LLM内容的清晰性、完整性和说服力。

Conclusion: 尽管LLM在传统质量指标上得分较低，但其结构化信息呈现方式可能更有效地吸引读者。

Abstract: With the wide adoption of large language models (LLMs) in information
assistance, it is essential to examine their alignment with human communication
styles and values. We situate this study within the context of fact-checking
health information, given the critical challenge of rectifying conceptions and
building trust. Recent studies have explored the potential of LLM for health
communication, but style differences between LLMs and human experts and
associated reader perceptions remain under-explored. In this light, our study
evaluates the communication styles of LLMs, focusing on how their explanations
differ from those of humans in three core components of health communication:
information, sender, and receiver. We compiled a dataset of 1498 health
misinformation explanations from authoritative fact-checking organizations and
generated LLM responses to inaccurate health information. Drawing from health
communication theory, we evaluate communication styles across three key
dimensions of information linguistic features, sender persuasive strategies,
and receiver value alignments. We further assessed human perceptions through a
blinded evaluation with 99 participants. Our findings reveal that LLM-generated
articles showed significantly lower scores in persuasive strategies, certainty
expressions, and alignment with social values and moral foundations. However,
human evaluation demonstrated a strong preference for LLM content, with over
60% responses favoring LLM articles for clarity, completeness, and
persuasiveness. Our results suggest that LLMs' structured approach to
presenting information may be more effective at engaging readers despite
scoring lower on traditional measures of quality in fact-checking and health
communication.

</details>


### [188] [VizCV: AI-assisted visualization of researchers' publications tracks](https://arxiv.org/abs/2505.08691)
*Vladimír Lazárik,Marco Agus,Barbora Kozlíková,Pere-Pau Vázquez*

Main category: cs.HC

TL;DR: VizCV是一个基于网络的端到端可视化分析框架，用于交互式探索研究人员的科学生涯轨迹，结合AI辅助分析和自动化报告功能。


<details>
  <summary>Details</summary>
Motivation: 评估科学家和研究组的学术记录演变对学术环境管理和职业规划至关重要。

Method: VizCV通过三个维度建模职业发展：研究主题演变、出版记录及影响、合作动态。结合AI驱动的分析和多视图交互系统。

Result: 系统支持职业里程碑的探索性分析，包括主题轨迹、影响增长和合作网络变化，并提供自动化解释和比较功能。

Conclusion: VizCV通过AI/ML技术为职业发展分析提供了创新工具，支持多维度可视化和交互式报告生成。

Abstract: Analyzing how the publication records of scientists and research groups have
evolved over the years is crucial for assessing their expertise since it can
support the management of academic environments by assisting with career
planning and evaluation. We introduce VizCV, a novel web-based end-to-end
visual analytics framework that enables the interactive exploration of
researchers' scientific trajectories. It incorporates AI-assisted analysis and
supports automated reporting of career evolution. Our system aims to model
career progression through three key dimensions: a) research topic evolution to
detect and visualize shifts in scholarly focus over time, b) publication record
and the corresponding impact, c) collaboration dynamics depicting the growth
and transformation of a researcher's co-authorship network. AI-driven insights
provide automated explanations of career transitions, detecting significant
shifts in research direction, impact surges, or collaboration expansions. The
system also supports comparative analysis between researchers, allowing users
to compare topic trajectories and impact growth. Our interactive, multi-tab and
multiview system allows for the exploratory analysis of career milestones under
different perspectives, such as the most impactful articles, emerging research
themes, or obtaining a detailed analysis of the contribution of the researcher
in a subfield. The key contributions include AI/ML techniques for: a) topic
analysis, b) dimensionality reduction for visualizing patterns and trends, c)
the interactive creation of textual descriptions of facets of data through
configurable prompt generation and large language models, that include key
indicators, to help understanding the career development of individuals or
groups.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [189] [A Survey of Deep Learning for Complex Speech Spectrograms](https://arxiv.org/abs/2505.08694)
*Yuying Xie,Zheng-Hua Tan*

Main category: eess.AS

TL;DR: 本文综述了深度学习在复杂频谱图处理中的最新技术，包括网络架构、训练策略和应用。


<details>
  <summary>Details</summary>
Motivation: 深度学习在语音信号处理领域取得了显著进展，特别是复杂频谱图的分析与处理。本文旨在为研究人员和实践者提供全面的技术概览。

Method: 介绍了复杂频谱图及其特征，探讨了复数神经网络的关键组件和架构，以及针对复杂频谱图的训练策略和损失函数。

Result: 深度学习在相位恢复、语音增强和语音分离等应用中取得了显著进展。

Conclusion: 本文为语音信号处理和复数神经网络领域的研究者提供了有价值的资源。

Abstract: Recent advancements in deep learning have significantly impacted the field of
speech signal processing, particularly in the analysis and manipulation of
complex spectrograms. This survey provides a comprehensive overview of the
state-of-the-art techniques leveraging deep neural networks for processing
complex spectrograms, which encapsulate both magnitude and phase information.
We begin by introducing complex spectrograms and their associated features for
various speech processing tasks. Next, we explore the key components and
architectures of complex-valued neural networks, which are specifically
designed to handle complex-valued data and have been applied for complex
spectrogram processing. We then discuss various training strategies and loss
functions tailored for training neural networks to process and model complex
spectrograms. The survey further examines key applications, including phase
retrieval, speech enhancement, and speech separation, where deep learning has
achieved significant progress by leveraging complex spectrograms or their
derived feature representations. Additionally, we examine the intersection of
complex spectrograms with generative models. This survey aims to serve as a
valuable resource for researchers and practitioners in the field of speech
signal processing and complex-valued neural networks.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [190] [Vision Foundation Model Embedding-Based Semantic Anomaly Detection](https://arxiv.org/abs/2505.07998)
*Max Peter Ronecker,Matthew Foutter,Amine Elhafsi,Daniele Gammelli,Ihor Barakaiev,Marco Pavone,Daniel Watzenig*

Main category: cs.CV

TL;DR: 该论文提出了一种利用视觉基础模型的语义先验检测语义异常的方法，通过比较运行时图像的局部视觉嵌入与安全场景数据库，实现实时异常检测。


<details>
  <summary>Details</summary>
Motivation: 语义异常可能导致自主系统推理失败，因此需要一种有效的方法来检测这些异常以确保系统安全。

Method: 提出了一种框架，使用网格嵌入和实例分割两种方法，并引入过滤机制减少误报。

Result: 在CARLA模拟异常上的实验表明，基于实例的方法性能接近GPT-4o，并能精确定位异常。

Conclusion: 视觉基础模型的嵌入在自主系统实时异常检测中具有潜在应用价值。

Abstract: Semantic anomalies are contextually invalid or unusual combinations of
familiar visual elements that can cause undefined behavior and failures in
system-level reasoning for autonomous systems. This work explores semantic
anomaly detection by leveraging the semantic priors of state-of-the-art vision
foundation models, operating directly on the image. We propose a framework that
compares local vision embeddings from runtime images to a database of nominal
scenarios in which the autonomous system is deemed safe and performant. In this
work, we consider two variants of the proposed framework: one using raw
grid-based embeddings, and another leveraging instance segmentation for
object-centric representations. To further improve robustness, we introduce a
simple filtering mechanism to suppress false positives. Our evaluations on
CARLA-simulated anomalies show that the instance-based method with filtering
achieves performance comparable to GPT-4o, while providing precise anomaly
localization. These results highlight the potential utility of vision
embeddings from foundation models for real-time anomaly detection in autonomous
systems.

</details>


### [191] [JSover: Joint Spectrum Estimation and Multi-Material Decomposition from Single-Energy CT Projections](https://arxiv.org/abs/2505.08123)
*Qing Wu,Hongjiang Wei,Jingyi Yu,S. Kevin Zhou,Yuyao Zhang*

Main category: cs.CV

TL;DR: JSover是一种新型单能CT多材料分解框架，通过联合重建和能量谱估计，显著提高了分解的准确性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 传统多材料分解方法依赖光谱CT和预测量X射线能谱，临床适用性受限。单能CT方法虽存在，但两步分解流程引入伪影和噪声。

Method: 提出JSover框架，一步完成多材料分解和能量谱估计，结合物理先验和隐式神经表示（INR）优化求解。

Result: 实验表明，JSover在模拟和真实CT数据上均优于现有单能CT方法，准确性和计算效率更高。

Conclusion: JSover通过一步分解和深度学习方法，为单能CT多材料分解提供了更可靠的解决方案。

Abstract: Multi-material decomposition (MMD) enables quantitative reconstruction of
tissue compositions in the human body, supporting a wide range of clinical
applications. However, traditional MMD typically requires spectral CT scanners
and pre-measured X-ray energy spectra, significantly limiting clinical
applicability. To this end, various methods have been developed to perform MMD
using conventional (i.e., single-energy, SE) CT systems, commonly referred to
as SEMMD. Despite promising progress, most SEMMD methods follow a two-step
image decomposition pipeline, which first reconstructs monochromatic CT images
using algorithms such as FBP, and then performs decomposition on these images.
The initial reconstruction step, however, neglects the energy-dependent
attenuation of human tissues, introducing severe nonlinear beam hardening
artifacts and noise into the subsequent decomposition. This paper proposes
JSover, a fundamentally reformulated one-step SEMMD framework that jointly
reconstructs multi-material compositions and estimates the energy spectrum
directly from SECT projections. By explicitly incorporating physics-informed
spectral priors into the SEMMD process, JSover accurately simulates a virtual
spectral CT system from SE acquisitions, thereby improving the reliability and
accuracy of decomposition. Furthermore, we introduce implicit neural
representation (INR) as an unsupervised deep learning solver for representing
the underlying material maps. The inductive bias of INR toward continuous image
patterns constrains the solution space and further enhances estimation quality.
Extensive experiments on both simulated and real CT datasets show that JSover
outperforms state-of-the-art SEMMD methods in accuracy and computational
efficiency.

</details>


### [192] [Topology-Guided Knowledge Distillation for Efficient Point Cloud Processing](https://arxiv.org/abs/2505.08101)
*Luu Tung Hai,Thinh D. Le,Zhicheng Ding,Qing Tian,Truong-Son Hy*

Main category: cs.CV

TL;DR: 提出了一种新的蒸馏框架，通过拓扑感知表示和梯度引导知识蒸馏，将高性能点云模型压缩为轻量级模型，显著减少模型大小和推理时间。


<details>
  <summary>Details</summary>
Motivation: 点云处理在自动驾驶和3D物体识别中至关重要，但高性能模型在资源受限环境中的部署面临计算和内存需求高的挑战。

Method: 利用拓扑感知表示和梯度引导知识蒸馏，从高容量教师模型向轻量级学生模型传递知识，同时捕捉点云的几何结构。

Result: 在Nuscenes、SemanticKITTI和Waymo数据集上表现优异，模型大小减少约16倍，推理时间降低近1.9倍，并在NuScenes上达到知识蒸馏技术的领先性能。

Conclusion: 该方法在保持性能的同时显著降低了模型复杂度，适用于资源受限环境中的点云处理。

Abstract: Point cloud processing has gained significant attention due to its critical
role in applications such as autonomous driving and 3D object recognition.
However, deploying high-performance models like Point Transformer V3 in
resource-constrained environments remains challenging due to their high
computational and memory demands. This work introduces a novel distillation
framework that leverages topology-aware representations and gradient-guided
knowledge distillation to effectively transfer knowledge from a high-capacity
teacher to a lightweight student model. Our approach captures the underlying
geometric structures of point clouds while selectively guiding the student
model's learning process through gradient-based feature alignment. Experimental
results in the Nuscenes, SemanticKITTI, and Waymo datasets demonstrate that the
proposed method achieves competitive performance, with an approximately 16x
reduction in model size and a nearly 1.9x decrease in inference time compared
to its teacher model. Notably, on NuScenes, our method achieves
state-of-the-art performance among knowledge distillation techniques trained
solely on LiDAR data, surpassing prior knowledge distillation baselines in
segmentation performance. Our implementation is available publicly at:
  https://github.com/HySonLab/PointDistill

</details>


### [193] [SLAG: Scalable Language-Augmented Gaussian Splatting](https://arxiv.org/abs/2505.08124)
*Laszlo Szilagyi,Francis Engelmann,Jeannette Bohg*

Main category: cs.CV

TL;DR: SLAG是一种多GPU框架，用于语言增强的高斯泼溅，提升大规模场景嵌入的速度和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 在时间敏感且数据密集的大规模机器人应用（如搜救、智慧城市和采矿）中，快速且可扩展的场景编码至关重要。

Method: SLAG将2D视觉语言模型特征（如SAM和CLIP）集成到3D场景中，通过归一化加权平均计算语言嵌入，无需损失函数，并引入向量数据库存储和检索嵌入。

Result: 在16-GPU设置下，SLAG比OpenGaussian快18倍，同时在ScanNet和LERF数据集上保持嵌入质量。

Conclusion: SLAG为资源受限的机器人提供了高效且可扩展的语言增强场景表示解决方案。

Abstract: Language-augmented scene representations hold great promise for large-scale
robotics applications such as search-and-rescue, smart cities, and mining. Many
of these scenarios are time-sensitive, requiring rapid scene encoding while
also being data-intensive, necessitating scalable solutions. Deploying these
representations on robots with limited computational resources further adds to
the challenge. To address this, we introduce SLAG, a multi-GPU framework for
language-augmented Gaussian splatting that enhances the speed and scalability
of embedding large scenes. Our method integrates 2D visual-language model
features into 3D scenes using SAM and CLIP. Unlike prior approaches, SLAG
eliminates the need for a loss function to compute per-Gaussian language
embeddings. Instead, it derives embeddings from 3D Gaussian scene parameters
via a normalized weighted average, enabling highly parallelized scene encoding.
Additionally, we introduce a vector database for efficient embedding storage
and retrieval. Our experiments show that SLAG achieves an 18 times speedup in
embedding computation on a 16-GPU setup compared to OpenGaussian, while
preserving embedding quality on the ScanNet and LERF datasets. For more
details, visit our project website: https://slag-project.github.io/.

</details>


### [194] [Unsupervised Raindrop Removal from a Single Image using Conditional Diffusion Models](https://arxiv.org/abs/2505.08190)
*Lhuqita Fazry,Valentino Vito*

Main category: cs.CV

TL;DR: 提出了一种基于扩散模型的单图像雨滴去除新方法。


<details>
  <summary>Details</summary>
Motivation: 单图像雨滴去除任务具有挑战性，现有方法多依赖GAN，而扩散模型在图像修复领域表现优异。

Method: 采用扩散模型进行图像修复，结合雨滴区域检测技术。

Result: 实现了基于扩散模型的雨滴去除，效果优于传统方法。

Conclusion: 扩散模型为单图像雨滴去除提供了新的高效解决方案。

Abstract: Raindrop removal is a challenging task in image processing. Removing
raindrops while relying solely on a single image further increases the
difficulty of the task. Common approaches include the detection of raindrop
regions in the image, followed by performing a background restoration process
conditioned on those regions. While various methods can be applied for the
detection step, the most common architecture used for background restoration is
the Generative Adversarial Network (GAN). Recent advances in the use of
diffusion models have led to state-of-the-art image inpainting techniques. In
this paper, we introduce a novel technique for raindrop removal from a single
image using diffusion-based image inpainting.

</details>


### [195] [Open the Eyes of MPNN: Vision Enhances MPNN in Link Prediction](https://arxiv.org/abs/2505.08266)
*Yanbin Wei,Xuehao Wang,Zhan Zhuang,Yang Chen,Shuhao Chen,Yulong Zhang,Yu Zhang,James Kwok*

Main category: cs.CV

TL;DR: 论文提出了一种结合视觉感知的图神经网络框架GVN及其高效变体E-GVN，用于链接预测任务，并在多个数据集上取得了新的SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 现有的MPNN和结构特征在链接预测中占主导地位，但视觉感知的潜力被忽视。本文首次将视觉结构感知引入MPNN。

Method: 提出了Graph Vision Network (GVN)及其高效变体E-GVN，通过视觉增强提升链接预测性能。

Result: 在七个链接预测数据集上，GVN均表现出性能提升，包括大规模图，且与现有SOTA方法兼容。

Conclusion: GVN为链接预测提供了一个有前景的新方向，并取得了新的SOTA结果。

Abstract: Message-passing graph neural networks (MPNNs) and structural features (SFs)
are cornerstones for the link prediction task. However, as a common and
intuitive mode of understanding, the potential of visual perception has been
overlooked in the MPNN community. For the first time, we equip MPNNs with
vision structural awareness by proposing an effective framework called Graph
Vision Network (GVN), along with a more efficient variant (E-GVN). Extensive
empirical results demonstrate that with the proposed frameworks, GVN
consistently benefits from the vision enhancement across seven link prediction
datasets, including challenging large-scale graphs. Such improvements are
compatible with existing state-of-the-art (SOTA) methods and GVNs achieve new
SOTA results, thereby underscoring a promising novel direction for link
prediction.

</details>


### [196] [Disruptive Transformation of Artworks in Master-Disciple Relationships: The Case of Ukiyo-e Artworks](https://arxiv.org/abs/2505.08284)
*Honna Shinichi,Akira Matsui*

Main category: cs.CV

TL;DR: 该论文通过机器学习对浮世绘进行定量分析，发现其整体创造力随文化成熟而下降，但风格创造力保持高水平。


<details>
  <summary>Details</summary>
Motivation: 传统艺术研究依赖主观判断，机器学习为东方绘画（如浮世绘）的定量分析提供了新视角。

Method: 使用11,000张高分辨率浮世绘图像，基于网络计算创造力，分析作品和艺术家的创造力。

Result: 浮世绘整体创造力随文化成熟下降，但风格创造力因文化细分而保持高水平。

Conclusion: 研究为浮世绘研究提供新见解，并展示了其在东方艺术分析中的文化意义。

Abstract: Artwork research has long relied on human sensibility and subjective
judgment, but recent developments in machine learning have enabled the
quantitative assessment of features that humans could not discover. In Western
paintings, comprehensive analyses have been conducted from various perspectives
in conjunction with large databases, but such extensive analysis has not been
sufficiently conducted for Eastern paintings. Then, we focus on Ukiyo-e, a
traditional Japanese art form, as a case study of Eastern paintings, and
conduct a quantitative analysis of creativity in works of art using 11,000
high-resolution images. This involves using the concept of calculating
creativity from networks to analyze both the creativity of the artwork and that
of the artists. As a result, In terms of Ukiyo-e as a whole, it was found that
the creativity of its appearance has declined with the maturation of culture,
but in terms of style, it has become more segmented with the maturation of
culture and has maintained a high level of creativity. This not only provides
new insights into the study of Ukiyo-e but also shows how Ukiyo-e has evolved
within the ongoing cultural history, playing a culturally significant role in
the analysis of Eastern art.

</details>


### [197] [Object detection in adverse weather conditions for autonomous vehicles using Instruct Pix2Pix](https://arxiv.org/abs/2505.08228)
*Unai Gurbindo,Axel Brando,Jaume Abella,Caroline König*

Main category: cs.CV

TL;DR: 论文提出了一种利用扩散模型Instruct Pix2Pix生成天气增强数据集的方法，以提升目标检测模型在恶劣天气下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 提升自动驾驶技术中目标检测系统在恶劣天气条件下的鲁棒性。

Method: 使用Instruct Pix2Pix扩散模型生成天气增强数据集，并在CARLA模拟器和真实数据集（BDD100K、ACDC）上验证。

Result: 实验表明，定制化的数据增强策略显著提升了目标检测模型（如Faster R-CNN和YOLOv10）在恶劣天气下的性能。

Conclusion: 研究为提升感知系统在恶劣环境中的可靠性奠定了基础，并为自动驾驶技术的未来发展提供了方向。

Abstract: Enhancing the robustness of object detection systems under adverse weather
conditions is crucial for the advancement of autonomous driving technology.
This study presents a novel approach leveraging the diffusion model Instruct
Pix2Pix to develop prompting methodologies that generate realistic datasets
with weather-based augmentations aiming to mitigate the impact of adverse
weather on the perception capabilities of state-of-the-art object detection
models, including Faster R-CNN and YOLOv10. Experiments were conducted in two
environments, in the CARLA simulator where an initial evaluation of the
proposed data augmentation was provided, and then on the real-world image data
sets BDD100K and ACDC demonstrating the effectiveness of the approach in real
environments.
  The key contributions of this work are twofold: (1) identifying and
quantifying the performance gap in object detection models under challenging
weather conditions, and (2) demonstrating how tailored data augmentation
strategies can significantly enhance the robustness of these models. This
research establishes a solid foundation for improving the reliability of
perception systems in demanding environmental scenarios, and provides a pathway
for future advancements in autonomous driving.

</details>


### [198] [Removing Watermarks with Partial Regeneration using Semantic Information](https://arxiv.org/abs/2505.08234)
*Krti Tallam,John Kevin Cava,Caleb Geniesse,N. Benjamin Erichson,Michael W. Mahoney*

Main category: cs.CV

TL;DR: 论文揭示了一种新型攻击方法SemanticRegen，能够有效擦除AI生成图像中的语义水印，同时保持图像内容完整，突显了现有水印技术的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 随着AI生成图像的普及，隐形水印成为版权保护的主要手段，但其对适应性攻击的鲁棒性尚未充分研究。

Method: 提出SemanticRegen攻击方法，分为三个阶段：使用视觉语言模型生成细粒度描述，零样本分割提取前景掩码，LLM引导的扩散模型修复背景。

Result: 在四种水印系统中，SemanticRegen成功擦除TreeRing水印，并显著降低其他水印的比特准确率，同时保持高感知质量。

Conclusion: 研究揭示了当前水印技术与适应性攻击能力之间的差距，呼吁开发更具鲁棒性的水印算法。

Abstract: As AI-generated imagery becomes ubiquitous, invisible watermarks have emerged
as a primary line of defense for copyright and provenance. The newest
watermarking schemes embed semantic signals - content-aware patterns that are
designed to survive common image manipulations - yet their true robustness
against adaptive adversaries remains under-explored. We expose a previously
unreported vulnerability and introduce SemanticRegen, a three-stage, label-free
attack that erases state-of-the-art semantic and invisible watermarks while
leaving an image's apparent meaning intact. Our pipeline (i) uses a
vision-language model to obtain fine-grained captions, (ii) extracts foreground
masks with zero-shot segmentation, and (iii) inpaints only the background via
an LLM-guided diffusion model, thereby preserving salient objects and style
cues. Evaluated on 1,000 prompts across four watermarking systems - TreeRing,
StegaStamp, StableSig, and DWT/DCT - SemanticRegen is the only method to defeat
the semantic TreeRing watermark (p = 0.10 > 0.05) and reduces bit-accuracy
below 0.75 for the remaining schemes, all while maintaining high perceptual
quality (masked SSIM = 0.94 +/- 0.01). We further introduce masked SSIM (mSSIM)
to quantify fidelity within foreground regions, showing that our attack
achieves up to 12 percent higher mSSIM than prior diffusion-based attackers.
These results highlight an urgent gap between current watermark defenses and
the capabilities of adaptive, semantics-aware adversaries, underscoring the
need for watermarking algorithms that are resilient to content-preserving
regenerative attacks.

</details>


### [199] [A Deep Learning-Driven Framework for Inhalation Injury Grading Using Bronchoscopy Images](https://arxiv.org/abs/2505.08517)
*Yifan Li,Alan W Pang,Jo Woon Chong*

Main category: cs.CV

TL;DR: 提出了一种基于深度学习的吸入性损伤分级框架，利用增强StarGAN生成高质量支气管镜图像，显著提升分类准确率。


<details>
  <summary>Details</summary>
Motivation: 传统吸入性损伤分级方法（如AIS）依赖主观评估且与临床结果相关性弱，需要更客观、准确的诊断工具。

Method: 使用增强StarGAN（结合Patch Loss和SSIM Loss）生成高质量支气管镜图像，并通过Swin Transformer进行分类。

Result: 增强StarGAN生成的图像显著提升分类准确率至77.78%（提升11.11%），FID得分最低（30.06），且临床医生认可其真实性和临床相关性。

Conclusion: 增强StarGAN能有效解决医学影像数据稀缺问题，提升吸入性损伤分级的准确性，具有临床应用潜力。

Abstract: Inhalation injuries face a challenge in clinical diagnosis and grading due to
the limitations of traditional methods, such as Abbreviated Injury Score (AIS),
which rely on subjective assessments and show weak correlations with clinical
outcomes. This study introduces a novel deep learning-based framework for
grading inhalation injuries using bronchoscopy images with the duration of
mechanical ventilation as an objective metric. To address the scarcity of
medical imaging data, we propose enhanced StarGAN, a generative model that
integrates Patch Loss and SSIM Loss to improve synthetic images' quality and
clinical relevance. The augmented dataset generated by enhanced StarGAN
significantly improved classification performance when evaluated using the Swin
Transformer, achieving an accuracy of 77.78%, an 11.11% improvement over the
original dataset. Image quality was assessed using the Fr\'echet Inception
Distance (FID), where Enhanced StarGAN achieved the lowest FID of 30.06,
outperforming baseline models. Burn surgeons confirmed the realism and clinical
relevance of the generated images, particularly the preservation of bronchial
structures and color distribution. These results highlight the potential of
enhanced StarGAN in addressing data limitations and improving classification
accuracy for inhalation injury grading.

</details>


### [200] [DFA-CON: A Contrastive Learning Approach for Detecting Copyright Infringement in DeepFake Art](https://arxiv.org/abs/2505.08552)
*Haroon Wahab,Hassan Ugail,Irfan Mehmood*

Main category: cs.CV

TL;DR: 论文提出了一种名为DFA-CON的对比学习框架，用于检测AI生成艺术中的版权侵权或伪造内容，并在多种攻击类型下表现出色。


<details>
  <summary>Details</summary>
Motivation: 生成AI工具在视觉艺术创作中的广泛应用引发了版权侵权和伪造的担忧，现有模型容易因记忆训练数据而侵犯版权。

Method: 通过对比学习框架DFA-CON，学习原始艺术作品及其伪造版本之间的判别性表示空间，涵盖多种攻击类型（如修复、风格迁移等）。

Result: 实验表明，DFA-CON在大多数攻击类型中表现优于现有预训练基础模型，具有鲁棒的检测性能。

Conclusion: DFA-CON为解决AI生成艺术的版权问题提供了有效工具，未来将公开代码和模型。

Abstract: Recent proliferation of generative AI tools for visual content
creation-particularly in the context of visual artworks-has raised serious
concerns about copyright infringement and forgery. The large-scale datasets
used to train these models often contain a mixture of copyrighted and
non-copyrighted artworks. Given the tendency of generative models to memorize
training patterns, they are susceptible to varying degrees of copyright
violation. Building on the recently proposed DeepfakeArt Challenge benchmark,
this work introduces DFA-CON, a contrastive learning framework designed to
detect copyright-infringing or forged AI-generated art. DFA-CON learns a
discriminative representation space, posing affinity among original artworks
and their forged counterparts within a contrastive learning framework. The
model is trained across multiple attack types, including inpainting, style
transfer, adversarial perturbation, and cutmix. Evaluation results demonstrate
robust detection performance across most attack types, outperforming recent
pretrained foundation models. Code and model checkpoints will be released
publicly upon acceptance.

</details>


### [201] [A computer vision-based model for occupancy detection using low-resolution thermal images](https://arxiv.org/abs/2505.08336)
*Xue Cui,Vincent Gbouna Zakka,Minhyun Lee*

Main category: cs.CV

TL;DR: 论文提出了一种基于低分辨率热成像和计算机视觉技术的占用检测模型，解决了传统RGB图像带来的隐私问题，并优化了计算资源需求。


<details>
  <summary>Details</summary>
Motivation: 传统HVAC系统基于固定时间表运行，未考虑实际占用情况，而RGB图像占用检测技术存在隐私问题。低分辨率热成像提供了一种非侵入性解决方案。

Method: 研究采用低分辨率热成像和计算机视觉技术，通过迁移学习微调YOLOv5模型，开发占用检测模型。

Result: 模型性能优异，精确度、召回率、mAP50等指标接近1.000。

Conclusion: 该模型不仅解决了隐私问题，还降低了计算资源需求，为HVAC系统的智能控制提供了有效工具。

Abstract: Occupancy plays an essential role in influencing the energy consumption and
operation of heating, ventilation, and air conditioning (HVAC) systems.
Traditional HVAC typically operate on fixed schedules without considering
occupancy. Advanced occupant-centric control (OCC) adopted occupancy status in
regulating HVAC operations. RGB images combined with computer vision (CV)
techniques are widely used for occupancy detection, however, the detailed
facial and body features they capture raise significant privacy concerns.
Low-resolution thermal images offer a non-invasive solution that mitigates
privacy issues. The study developed an occupancy detection model utilizing
low-resolution thermal images and CV techniques, where transfer learning was
applied to fine-tune the You Only Look Once version 5 (YOLOv5) model. The
developed model ultimately achieved satisfactory performance, with precision,
recall, mAP50, and mAP50 values approaching 1.000. The contributions of this
model lie not only in mitigating privacy concerns but also in reducing
computing resource demands.

</details>


### [202] [MESSI: A Multi-Elevation Semantic Segmentation Image Dataset of an Urban Environment](https://arxiv.org/abs/2505.08589)
*Barak Pinkovich,Boaz Matalon,Ehud Rivlin,Hector Rotstein*

Main category: cs.CV

TL;DR: MESSI数据集包含2525张无人机拍摄的密集城市环境图像，支持多高度语义分割研究，并公开作为评估基准。


<details>
  <summary>Details</summary>
Motivation: 研究多高度对语义分割的影响，并提供多样化的城市环境图像以支持无人机应用。

Method: 使用多高度无人机图像，标注位置、方向及相机参数，训练深度神经网络进行语义分割。

Result: 数据集支持语义分割训练，并展示了相关统计结果。

Conclusion: MESSI数据集将公开，为无人机图像语义分割提供评估基准。

Abstract: This paper presents a Multi-Elevation Semantic Segmentation Image (MESSI)
dataset comprising 2525 images taken by a drone flying over dense urban
environments. MESSI is unique in two main features. First, it contains images
from various altitudes, allowing us to investigate the effect of depth on
semantic segmentation. Second, it includes images taken from several different
urban regions (at different altitudes). This is important since the variety
covers the visual richness captured by a drone's 3D flight, performing
horizontal and vertical maneuvers. MESSI contains images annotated with
location, orientation, and the camera's intrinsic parameters and can be used to
train a deep neural network for semantic segmentation or other applications of
interest (e.g., localization, navigation, and tracking). This paper describes
the dataset and provides annotation details. It also explains how semantic
segmentation was performed using several neural network models and shows
several relevant statistics. MESSI will be published in the public domain to
serve as an evaluation benchmark for semantic segmentation using images
captured by a drone or similar vehicle flying over a dense urban environment.

</details>


### [203] [FAD: Frequency Adaptation and Diversion for Cross-domain Few-shot Learning](https://arxiv.org/abs/2505.08349)
*Ruixiao Shi,Fu Feng,Yucheng Xie,Jing Wang,Xin Geng*

Main category: cs.CV

TL;DR: 论文提出了一种频率感知框架FAD，通过显式建模和调制频谱分量，提升跨域小样本学习的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 跨域小样本学习中，空间域方法忽视了频谱差异，限制了模型的泛化能力。

Method: 引入频率适应与分流（FAD）框架，通过离散傅里叶变换分区频谱，并针对不同频段进行适应性调整。

Result: 在Meta-Dataset基准测试中，FAD在可见和未见域上均优于现有方法。

Conclusion: 频率域表示和频段适应性调整能有效提升跨域小样本学习的泛化性能。

Abstract: Cross-domain few-shot learning (CD-FSL) requires models to generalize from
limited labeled samples under significant distribution shifts. While recent
methods enhance adaptability through lightweight task-specific modules, they
operate solely in the spatial domain and overlook frequency-specific variations
that are often critical for robust transfer. We observe that spatially similar
images across domains can differ substantially in their spectral
representations, with low and high frequencies capturing complementary semantic
information at coarse and fine levels. This indicates that uniform spatial
adaptation may overlook these spectral distinctions, thus constraining
generalization. To address this, we introduce Frequency Adaptation and
Diversion (FAD), a frequency-aware framework that explicitly models and
modulates spectral components. At its core is the Frequency Diversion Adapter,
which transforms intermediate features into the frequency domain using the
discrete Fourier transform (DFT), partitions them into low, mid, and
high-frequency bands via radial masks, and reconstructs each band using inverse
DFT (IDFT). Each frequency band is then adapted using a dedicated convolutional
branch with a kernel size tailored to its spectral scale, enabling targeted and
disentangled adaptation across frequencies. Extensive experiments on the
Meta-Dataset benchmark demonstrate that FAD consistently outperforms
state-of-the-art methods on both seen and unseen domains, validating the
utility of frequency-domain representations and band-wise adaptation for
improving generalization in CD-FSL.

</details>


### [204] [STORYANCHORS: Generating Consistent Multi-Scene Story Frames for Long-Form Narratives](https://arxiv.org/abs/2505.08350)
*Bo Wang,Haoyang Huang,Zhiyin Lu,Fengyuan Liu,Guoqing Ma,Jianlong Yuan,Yuan Zhang,Nan Duan*

Main category: cs.CV

TL;DR: StoryAnchors是一个统一框架，用于生成高质量、多场景且具有强时间一致性的故事帧。它通过双向故事生成器和特定条件提升生成质量，支持可编辑和扩展的故事帧生成。


<details>
  <summary>Details</summary>
Motivation: 解决多场景故事帧生成中的时间一致性、角色连续性和场景多样性问题，提升叙事丰富性和生成质量。

Method: 采用双向故事生成器整合过去和未来上下文，引入Multi-Event Story Frame Labeling和Progressive Story Frame Training方法。

Result: 在一致性、叙事连贯性和场景多样性上优于现有开源模型，叙事一致性和故事丰富性与GPT-4o相当。

Conclusion: StoryAnchors为故事驱动帧生成提供了可扩展、灵活且高度可编辑的基础，推动了该领域的研究边界。

Abstract: This paper introduces StoryAnchors, a unified framework for generating
high-quality, multi-scene story frames with strong temporal consistency. The
framework employs a bidirectional story generator that integrates both past and
future contexts to ensure temporal consistency, character continuity, and
smooth scene transitions throughout the narrative. Specific conditions are
introduced to distinguish story frame generation from standard video synthesis,
facilitating greater scene diversity and enhancing narrative richness. To
further improve generation quality, StoryAnchors integrates Multi-Event Story
Frame Labeling and Progressive Story Frame Training, enabling the model to
capture both overarching narrative flow and event-level dynamics. This approach
supports the creation of editable and expandable story frames, allowing for
manual modifications and the generation of longer, more complex sequences.
Extensive experiments show that StoryAnchors outperforms existing open-source
models in key areas such as consistency, narrative coherence, and scene
diversity. Its performance in narrative consistency and story richness is also
on par with GPT-4o. Ultimately, StoryAnchors pushes the boundaries of
story-driven frame generation, offering a scalable, flexible, and highly
editable foundation for future research.

</details>


### [205] [A Survey of 3D Reconstruction with Event Cameras: From Event-based Geometry to Neural 3D Rendering](https://arxiv.org/abs/2505.08438)
*Chuanzhi Xu,Haoxian Zhou,Langyi Chen,Haodong Chen,Ying Zhou,Vera Chung,Qiang Qu*

Main category: cs.CV

TL;DR: 本文综述了事件相机在3D重建中的应用，分类总结了现有方法，并指出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 事件相机因其异步捕捉亮度变化的能力，在3D重建中表现出色，尤其是在极端环境下。本文旨在提供首个专注于事件相机3D重建的全面综述。

Method: 将现有工作按输入模态（立体、单目、多模态）和重建方法（几何、深度学习、神经渲染）分类，并总结了相关数据集。

Result: 总结了事件相机3D重建的研究现状，包括数据可用性、评估、表示和动态场景处理的局限性。

Conclusion: 本文为事件驱动的3D重建提供了全面参考，并指出了未来发展的方向。

Abstract: Event cameras have emerged as promising sensors for 3D reconstruction due to
their ability to capture per-pixel brightness changes asynchronously. Unlike
conventional frame-based cameras, they produce sparse and temporally rich data
streams, which enable more accurate 3D reconstruction and open up the
possibility of performing reconstruction in extreme environments such as
high-speed motion, low light, or high dynamic range scenes. In this survey, we
provide the first comprehensive review focused exclusively on 3D reconstruction
using event cameras. The survey categorises existing works into three major
types based on input modality - stereo, monocular, and multimodal systems, and
further classifies them by reconstruction approach, including geometry-based,
deep learning-based, and recent neural rendering techniques such as Neural
Radiance Fields and 3D Gaussian Splatting. Methods with a similar research
focus were organised chronologically into the most subdivided groups. We also
summarise public datasets relevant to event-based 3D reconstruction. Finally,
we highlight current research limitations in data availability, evaluation,
representation, and dynamic scene handling, and outline promising future
research directions. This survey aims to serve as a comprehensive reference and
a roadmap for future developments in event-driven 3D reconstruction.

</details>


### [206] [Controllable Image Colorization with Instance-aware Texts and Masks](https://arxiv.org/abs/2505.08705)
*Yanru An,Ling Gui,Qiang Hu,Chunlei Cai,Tianxiao Ye,Xiaoyun Zhang,Yanfeng Wang*

Main category: cs.CV

TL;DR: 本文提出了一种基于扩散模型的实例感知图像着色方法MT-Color，通过像素级掩码注意力机制和实例掩码文本引导模块解决了颜色溢出和绑定错误问题，并引入多实例采样策略和专用数据集GPT-color。


<details>
  <summary>Details</summary>
Motivation: 当前主流图像着色模型存在颜色溢出和绑定错误问题，且无法实现实例级着色。

Method: 设计了像素级掩码注意力机制和实例掩码文本引导模块，采用多实例采样策略，并构建了专用数据集GPT-color。

Result: 定性和定量实验表明，模型和数据集优于先前方法。

Conclusion: MT-Color方法在实例级图像着色任务中表现出色，解决了现有问题。

Abstract: Recently, the application of deep learning in image colorization has received
widespread attention. The maturation of diffusion models has further advanced
the development of image colorization models. However, current mainstream image
colorization models still face issues such as color bleeding and color binding
errors, and cannot colorize images at the instance level. In this paper, we
propose a diffusion-based colorization method MT-Color to achieve precise
instance-aware colorization with use-provided guidance. To tackle color
bleeding issue, we design a pixel-level mask attention mechanism that
integrates latent features and conditional gray image features through
cross-attention. We use segmentation masks to construct cross-attention masks,
preventing pixel information from exchanging between different instances. We
also introduce an instance mask and text guidance module that extracts instance
masks and text representations of each instance, which are then fused with
latent features through self-attention, utilizing instance masks to form
self-attention masks to prevent instance texts from guiding the colorization of
other areas, thus mitigating color binding errors. Furthermore, we apply a
multi-instance sampling strategy, which involves sampling each instance region
separately and then fusing the results. Additionally, we have created a
specialized dataset for instance-level colorization tasks, GPT-color, by
leveraging large visual language models on existing image datasets. Qualitative
and quantitative experiments show that our model and dataset outperform
previous methods and datasets.

</details>


### [207] [Advancing Food Nutrition Estimation via Visual-Ingredient Feature Fusion](https://arxiv.org/abs/2505.08747)
*Huiyan Qi,Bin Zhu,Chong-Wah Ngo,Jingjing Chen,Ee-Peng Lim*

Main category: cs.CV

TL;DR: 论文介绍了FastFood数据集和VIF²方法，通过融合视觉和成分特征提升营养估计准确性。


<details>
  <summary>Details</summary>
Motivation: 营养估计对健康饮食和降低饮食相关健康风险至关重要，但缺乏带营养标注的数据集限制了进展。

Method: 提出VIF²方法，结合视觉和成分特征，通过同义词替换和重采样提升成分鲁棒性，并利用多模态模型优化成分预测。

Result: 在FastFood和Nutrition5k数据集上验证了方法的有效性，支持成分信息对营养估计的重要性。

Conclusion: VIF²方法显著提升了营养估计的准确性，证明了成分信息的价值。

Abstract: Nutrition estimation is an important component of promoting healthy eating
and mitigating diet-related health risks. Despite advances in tasks such as
food classification and ingredient recognition, progress in nutrition
estimation is limited due to the lack of datasets with nutritional annotations.
To address this issue, we introduce FastFood, a dataset with 84,446 images
across 908 fast food categories, featuring ingredient and nutritional
annotations. In addition, we propose a new model-agnostic Visual-Ingredient
Feature Fusion (VIF$^2$) method to enhance nutrition estimation by integrating
visual and ingredient features. Ingredient robustness is improved through
synonym replacement and resampling strategies during training. The
ingredient-aware visual feature fusion module combines ingredient features and
visual representation to achieve accurate nutritional prediction. During
testing, ingredient predictions are refined using large multimodal models by
data augmentation and majority voting. Our experiments on both FastFood and
Nutrition5k datasets validate the effectiveness of our proposed method built in
different backbones (e.g., Resnet, InceptionV3 and ViT), which demonstrates the
importance of ingredient information in nutrition estimation.
https://huiyanqi.github.io/fastfood-nutrition-estimation/.

</details>


### [208] [Towards Autonomous UAV Visual Object Search in City Space: Benchmark and Agentic Methodology](https://arxiv.org/abs/2505.08765)
*Yatai Ji,Zhengqiu Zhu,Yong Zhao,Beidan Liu,Chen Gao,Yihao Zhao,Sihang Qiu,Yue Hu,Quanjun Yin,Yong Li*

Main category: cs.CV

TL;DR: 论文提出了CityAVOS数据集和PRPSearcher方法，用于解决无人机在城市环境中自主搜索目标物体的挑战。PRPSearcher通过多模态大语言模型模拟人类三层认知，显著提升了搜索成功率和效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法在复杂城市环境中表现不佳，主要由于冗余语义处理、相似物体区分和探索-利用困境。为解决这些问题并支持AVOS任务，作者提出了新的数据集和方法。

Method: 提出了PRPSearcher方法，包括构建三种专用地图（动态语义地图、3D认知地图和3D不确定性地图），并引入去噪机制和IPT提示机制。

Result: 在CityAVOS数据集上，PRPSearcher在成功率和搜索效率上显著优于基线方法（平均提升37.69% SR，28.96% SPL，减少30.69% MSS和46.40% NE）。

Conclusion: PRPSearcher为AVOS任务提供了有效解决方案，但与人类表现仍有差距，未来需进一步改进语义推理和空间探索能力。

Abstract: Aerial Visual Object Search (AVOS) tasks in urban environments require
Unmanned Aerial Vehicles (UAVs) to autonomously search for and identify target
objects using visual and textual cues without external guidance. Existing
approaches struggle in complex urban environments due to redundant semantic
processing, similar object distinction, and the exploration-exploitation
dilemma. To bridge this gap and support the AVOS task, we introduce CityAVOS,
the first benchmark dataset for autonomous search of common urban objects. This
dataset comprises 2,420 tasks across six object categories with varying
difficulty levels, enabling comprehensive evaluation of UAV agents' search
capabilities. To solve the AVOS tasks, we also propose PRPSearcher
(Perception-Reasoning-Planning Searcher), a novel agentic method powered by
multi-modal large language models (MLLMs) that mimics human three-tier
cognition. Specifically, PRPSearcher constructs three specialized maps: an
object-centric dynamic semantic map enhancing spatial perception, a 3D
cognitive map based on semantic attraction values for target reasoning, and a
3D uncertainty map for balanced exploration-exploitation search. Also, our
approach incorporates a denoising mechanism to mitigate interference from
similar objects and utilizes an Inspiration Promote Thought (IPT) prompting
mechanism for adaptive action planning. Experimental results on CityAVOS
demonstrate that PRPSearcher surpasses existing baselines in both success rate
and search efficiency (on average: +37.69% SR, +28.96% SPL, -30.69% MSS, and
-46.40% NE). While promising, the performance gap compared to humans highlights
the need for better semantic reasoning and spatial exploration capabilities in
AVOS tasks. This work establishes a foundation for future advances in embodied
target search. Dataset and source code are available at
https://anonymous.4open.science/r/CityAVOS-3DF8.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [209] [Big Data and the Computational Social Science of Entrepreneurship and Innovation](https://arxiv.org/abs/2505.08706)
*Ningzi Li,Shiyang Lai,James Evans*

Main category: econ.GN

TL;DR: 论文探讨了利用大规模数据和机器学习方法研究创业与创新的机遇与挑战，提出了两种利用多模态数据的方法：构建精确测量系统和生成数字孪生实验室。


<details>
  <summary>Details</summary>
Motivation: 随着大规模社交数据的爆发和机器学习方法的发展，创业与创新研究面临新的机遇和挑战，需要探索如何利用这些数据和方法推动研究。

Method: 1. 结合机器学习模型与大规模数据，构建精确测量系统作为创新与创业的观测平台；2. 利用大数据驱动的人工智能模型生成技术和商业的“数字孪生”，形成虚拟实验环境。

Result: 通过结合大数据与大模型，能够推动创业与创新领域的理论发展和测试。

Conclusion: 论文主张通过大数据与大模型的结合，为创业与创新研究提供新的理论发展路径和实验方法。

Abstract: As large-scale social data explode and machine-learning methods evolve,
scholars of entrepreneurship and innovation face new research opportunities but
also unique challenges. This chapter discusses the difficulties of leveraging
large-scale data to identify technological and commercial novelty, document new
venture origins, and forecast competition between new technologies and
commercial forms. It suggests how scholars can take advantage of new text,
network, image, audio, and video data in two distinct ways that advance
innovation and entrepreneurship research. First, machine-learning models,
combined with large-scale data, enable the construction of precision
measurements that function as system-level observatories of innovation and
entrepreneurship across human societies. Second, new artificial intelligence
models fueled by big data generate 'digital doubles' of technology and
business, forming laboratories for virtual experimentation about innovation and
entrepreneurship processes and policies. The chapter argues for the advancement
of theory development and testing in entrepreneurship and innovation by
coupling big data with big models.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [210] [OMGM: Orchestrate Multiple Granularities and Modalities for Efficient Multimodal Retrieval](https://arxiv.org/abs/2505.07879)
*Wei Yang,Jingjing Fu,Rui Wang,Jinyu Wang,Lei Song,Jiang Bian*

Main category: cs.IR

TL;DR: 提出了一种多模态检索增强生成（RAG）系统，通过粗到细的多步检索方法提升知识库视觉问答（KB-VQA）的效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法未能充分利用查询和知识库中多模态和多粒度信息的潜在交互作用。

Method: 采用粗到细的多步检索策略，包括初始粗粒度跨模态检索、多模态融合重排序和细粒度文本重排序。

Result: 在InfoSeek和Encyclopedic-VQA基准测试中，检索性能达到最优，问答结果具有高度竞争力。

Conclusion: 该方法有效提升了KB-VQA系统的性能，展示了多模态和多粒度信息协同的重要性。

Abstract: Vision-language retrieval-augmented generation (RAG) has become an effective
approach for tackling Knowledge-Based Visual Question Answering (KB-VQA), which
requires external knowledge beyond the visual content presented in images. The
effectiveness of Vision-language RAG systems hinges on multimodal retrieval,
which is inherently challenging due to the diverse modalities and knowledge
granularities in both queries and knowledge bases. Existing methods have not
fully tapped into the potential interplay between these elements. We propose a
multimodal RAG system featuring a coarse-to-fine, multi-step retrieval that
harmonizes multiple granularities and modalities to enhance efficacy. Our
system begins with a broad initial search aligning knowledge granularity for
cross-modal retrieval, followed by a multimodal fusion reranking to capture the
nuanced multimodal information for top entity selection. A text reranker then
filters out the most relevant fine-grained section for augmented generation.
Extensive experiments on the InfoSeek and Encyclopedic-VQA benchmarks show our
method achieves state-of-the-art retrieval performance and highly competitive
answering results, underscoring its effectiveness in advancing KB-VQA systems.

</details>


### [211] [Efficient and Reproducible Biomedical Question Answering using Retrieval Augmented Generation](https://arxiv.org/abs/2505.07917)
*Linus Stuhlmann,Michael Alexander Saxer,Jonathan Fürst*

Main category: cs.IR

TL;DR: 该研究系统评估了生物医学问答系统中的检索增强生成（RAG）方法，比较了不同检索策略和响应时间，发现BM25结合MedCPT在检索50篇文档时能最优平衡准确性、召回率和响应时间。


<details>
  <summary>Details</summary>
Motivation: 生物医学问答系统需要高效的检索和生成组件以确保准确性、效率和可扩展性，研究旨在探索RAG系统在生物医学领域的应用。

Method: 研究评估了BM25、BioBERT、MedCPT等检索方法及Elasticsearch、MongoDB、FAISS等数据存储工具，在PubMed子集（2.4M文档）上测试索引效率、检索延迟和性能，最终在完整PubMed语料库（24M文档）上部署RAG系统。

Result: BM25结合MedCPT在检索50篇文档时表现最佳，准确性（0.90）、召回率（0.90）和响应时间（1.91秒）达到最优平衡。BM25检索时间稳定（82毫秒），MedCPT为主要计算开销。

Conclusion: 研究揭示了生物医学问答系统中检索深度、效率和可扩展性的权衡，系统开源且可扩展。

Abstract: Biomedical question-answering (QA) systems require effective retrieval and
generation components to ensure accuracy, efficiency, and scalability. This
study systematically examines a Retrieval-Augmented Generation (RAG) system for
biomedical QA, evaluating retrieval strategies and response time trade-offs. We
first assess state-of-the-art retrieval methods, including BM25, BioBERT,
MedCPT, and a hybrid approach, alongside common data stores such as
Elasticsearch, MongoDB, and FAISS, on a ~10% subset of PubMed (2.4M documents)
to measure indexing efficiency, retrieval latency, and retriever performance in
the end-to-end RAG system. Based on these insights, we deploy the final RAG
system on the full 24M PubMed corpus, comparing different retrievers' impact on
overall performance. Evaluations of the retrieval depth show that retrieving 50
documents with BM25 before reranking with MedCPT optimally balances accuracy
(0.90), recall (0.90), and response time (1.91s). BM25 retrieval time remains
stable (82ms), while MedCPT incurs the main computational cost. These results
highlight previously not well-known trade-offs in retrieval depth, efficiency,
and scalability for biomedical QA. With open-source code, the system is fully
reproducible and extensible.

</details>


### [212] [Hyperbolic Contrastive Learning with Model-augmentation for Knowledge-aware Recommendation](https://arxiv.org/abs/2505.08157)
*Shengyin Sun,Chen Ma*

Main category: cs.IR

TL;DR: 论文提出了一种基于双曲对比学习的知识感知推荐方法，通过Lorentzian知识聚合机制和模型级增强技术，解决了现有方法在捕捉层次结构和避免用户偏好偏移方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有基于对比学习的知识感知推荐方法难以有效捕捉用户-项目二分图和知识图的层次结构，且通过扰动图结构生成正样本可能导致用户偏好偏移。

Method: 设计了Lorentzian知识聚合机制以捕捉层次结构，并提出三种模型级增强技术辅助双曲对比学习，避免偏好偏移。

Result: 实验表明，所提方法在性能上显著优于现有基线方法，最大提升达11.03%。

Conclusion: 通过结合双曲对比学习和模型级增强技术，论文提出的方法在知识感知推荐任务中表现出色，解决了现有方法的局限性。

Abstract: Benefiting from the effectiveness of graph neural networks (GNNs) and
contrastive learning, GNN-based contrastive learning has become mainstream for
knowledge-aware recommendation. However, most existing contrastive
learning-based methods have difficulties in effectively capturing the
underlying hierarchical structure within user-item bipartite graphs and
knowledge graphs. Moreover, they commonly generate positive samples for
contrastive learning by perturbing the graph structure, which may lead to a
shift in user preference learning. To overcome these limitations, we propose
hyperbolic contrastive learning with model-augmentation for knowledge-aware
recommendation. To capture the intrinsic hierarchical graph structures, we
first design a novel Lorentzian knowledge aggregation mechanism, which enables
more effective representations of users and items. Then, we propose three
model-level augmentation techniques to assist Hyperbolic contrastive learning.
Different from the classical structure-level augmentation (e.g., edge
dropping), the proposed model-augmentations can avoid preference shifts between
the augmented positive pair. Finally, we conduct extensive experiments to
demonstrate the superiority (maximum improvement of $11.03\%$) of proposed
methods over existing baselines.

</details>


<div id='q-bio.GN'></div>

# q-bio.GN [[Back]](#toc)

### [213] [Bridging Large Language Models and Single-Cell Transcriptomics in Dissecting Selective Motor Neuron Vulnerability](https://arxiv.org/abs/2505.07896)
*Douglas Jiang,Zilin Dai,Luxuan Zhang,Qiyi Yu,Haoqi Sun,Feng Tian*

Main category: q-bio.GN

TL;DR: 提出了一种利用NCBI Gene数据库的基因注释生成生物上下文细胞嵌入的新框架，结合单细胞RNA测序数据和语言模型。


<details>
  <summary>Details</summary>
Motivation: 解决通过单细胞测序数据理解细胞身份和功能的关键挑战。

Method: 通过表达水平排序基因，获取NCBI Gene描述，使用LLMs（如OpenAI和BioBERT）生成向量嵌入，并通过表达加权平均生成细胞嵌入。

Result: 生成紧凑且语义丰富的细胞表示，支持下游应用如细胞类型聚类和轨迹推断。

Conclusion: 该多模态策略结合生物数据和语言模型，提高了细胞分析的解释性。

Abstract: Understanding cell identity and function through single-cell level sequencing
data remains a key challenge in computational biology. We present a novel
framework that leverages gene-specific textual annotations from the NCBI Gene
database to generate biologically contextualized cell embeddings. For each cell
in a single-cell RNA sequencing (scRNA-seq) dataset, we rank genes by
expression level, retrieve their NCBI Gene descriptions, and transform these
descriptions into vector embedding representations using large language models
(LLMs). The models used include OpenAI text-embedding-ada-002,
text-embedding-3-small, and text-embedding-3-large (Jan 2024), as well as
domain-specific models BioBERT and SciBERT. Embeddings are computed via an
expression-weighted average across the top N most highly expressed genes in
each cell, providing a compact, semantically rich representation. This
multimodal strategy bridges structured biological data with state-of-the-art
language modeling, enabling more interpretable downstream applications such as
cell-type clustering, cell vulnerability dissection, and trajectory inference.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [214] [Aitomia: Your Intelligent Assistant for AI-Driven Atomistic and Quantum Chemical Simulations](https://arxiv.org/abs/2505.08195)
*Jinming Hu,Hassan Nawaz,Yuting Rui,Lijie Chi,Arif Ullah,Pavlo O. Dral*

Main category: physics.comp-ph

TL;DR: Aitomia是一个由AI驱动的平台，旨在辅助原子和量子化学模拟，通过聊天机器人和AI代理帮助专家和非专家完成模拟设置、运行、监控和分析。


<details>
  <summary>Details</summary>
Motivation: 降低原子模拟的门槛，加速相关领域的研究和开发。

Method: 利用微调的开源大语言模型（LLMs）、基于规则的代理和检索增强生成（RAG）系统。

Result: Aitomia已部分公开，并计划集成到Aitomistic Hub和XACS在线计算服务中。

Conclusion: Aitomia有望简化原子模拟流程，推动相关领域的进展。

Abstract: We have developed Aitomia - a platform powered by AI to assist in performing
AI-driven atomistic and quantum chemical (QC) simulations. This intelligent
assistant platform is equipped with chatbots and AI agents to help experts and
guide non-experts in setting up and running the atomistic simulations,
monitoring their computation status, analyzing the simulation results, and
summarizing them for the user in text and graphical forms. We achieve these
goals by exploiting fine-tuned open-source large language models (LLMs),
rule-based agents, and a retrieval-augmented generation (RAG) system. Aitomia
leverages the versatility of our MLatom ecosystem for AI-enhanced computational
chemistry. This intelligent assistant is going to be integrated into the
Aitomistic Hub and XACS online computing services, with some functionality
already publicly available as described at http://mlatom.com/aitomia. Aitomia
is expected to lower the barrier to performing atomistic simulations,
accelerating research and development in the relevant fields.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [215] [Diffusion-based supervised learning of generative models for efficient sampling of multimodal distributions](https://arxiv.org/abs/2505.07825)
*Hoang Tran,Zezhong Zhang,Feng Bao,Dan Lu,Guannan Zhang*

Main category: stat.ML

TL;DR: 提出了一种混合生成模型，用于高效采样高维多模态概率分布，解决了传统蒙特卡洛方法在多模态分布中的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统蒙特卡洛方法在高维多模态分布中难以正确采样各模态比例，尤其是模态分离明显的情况。

Method: 采用分治策略：先通过能量函数最小化识别所有模态，训练分类器分割各模态域，再为每个模态训练扩散模型辅助的生成模型，最后用桥采样调整模态比例。

Result: 数值实验表明，该方法能有效处理100维内不同形状的多模态分布，并成功应用于偏微分方程的贝叶斯反问题。

Conclusion: 提出的混合生成模型在多模态分布采样中表现出色，为贝叶斯推断提供了高效工具。

Abstract: We propose a hybrid generative model for efficient sampling of
high-dimensional, multimodal probability distributions for Bayesian inference.
Traditional Monte Carlo methods, such as the Metropolis-Hastings and Langevin
Monte Carlo sampling methods, are effective for sampling from single-mode
distributions in high-dimensional spaces. However, these methods struggle to
produce samples with the correct proportions for each mode in multimodal
distributions, especially for distributions with well separated modes. To
address the challenges posed by multimodality, we adopt a divide-and-conquer
strategy. We start by minimizing the energy function with initial guesses
uniformly distributed within the prior domain to identify all the modes of the
energy function. Then, we train a classifier to segment the domain
corresponding to each mode. After the domain decomposition, we train a
diffusion-model-assisted generative model for each identified mode within its
support. Once each mode is characterized, we employ bridge sampling to estimate
the normalizing constant, allowing us to directly adjust the ratios between the
modes. Our numerical examples demonstrate that the proposed framework can
effectively handle multimodal distributions with varying mode shapes in up to
100 dimensions. An application to Bayesian inverse problem for partial
differential equations is also provided.

</details>


### [216] [Wasserstein Distributionally Robust Nonparametric Regression](https://arxiv.org/abs/2505.07967)
*Changyu Liu,Yuling Jiao,Junhui Wang,Jian Huang*

Main category: stat.ML

TL;DR: 本文研究了Wasserstein分布鲁棒非参数估计器的泛化性质，重点分析了模型误设的影响，并建立了非渐近误差界。


<details>
  <summary>Details</summary>
Motivation: 在非参数框架下，分布鲁棒优化研究较少，且模型误设可能影响泛化性能。

Method: 通过分析分布扰动引起的正则化效应，并使用带Lipschitz约束的前馈神经网络，建立非渐近误差界。

Result: 误差界揭示了不确定性水平和神经网络结构对泛化性能的影响，适用于Lipschitz和二次损失函数。

Conclusion: 提出的估计器在模拟研究和MNIST数据集应用中表现出鲁棒性。

Abstract: Distributionally robust optimization has become a powerful tool for
prediction and decision-making under model uncertainty. By focusing on the
local worst-case risk, it enhances robustness by identifying the most
unfavorable distribution within a predefined ambiguity set. While extensive
research has been conducted in parametric settings, studies on nonparametric
frameworks remain limited. This paper studies the generalization properties of
Wasserstein distributionally robust nonparametric estimators, with particular
attention to the impact of model misspecification, where non-negligible
discrepancies between the estimation function space and target function can
impair generalization performance. We establish non-asymptotic error bounds for
the excess local worst-case risk by analyzing the regularization effects
induced by distributional perturbations and employing feedforward neural
networks with Lipschitz constraints. These bounds illustrate how uncertainty
levels and neural network structures influence generalization performance and
are applicable to both Lipschitz and quadratic loss functions. Furthermore, we
investigate the Lagrangian relaxation of the local worst-case risk and derive
corresponding non-asymptotic error bounds for these estimators. The robustness
of the proposed estimator is evaluated through simulation studies and
illustrated with an application to the MNIST dataset.

</details>


### [217] [Sharp Gaussian approximations for Decentralized Federated Learning](https://arxiv.org/abs/2505.08125)
*Soham Bonnerjee,Sayar Karmakar,Wei Biao Wu*

Main category: stat.ML

TL;DR: 本文研究了联邦学习中局部SGD的统计性质，提出了两种广义高斯近似结果，并探讨了其应用。


<details>
  <summary>Details</summary>
Motivation: 在隐私敏感的协作环境中，联邦学习的局部SGD方法虽然收敛性已被广泛研究，但其渐近统计保证仍有限。本文旨在填补这一空白。

Method: 提出了两种广义高斯近似结果：一是针对最终局部SGD迭代的Berry-Esseen定理，二是针对整个轨迹的时间均匀高斯近似。

Result: 理论结果支持有效的乘数自举程序和基于高斯自举的对抗攻击检测测试。

Conclusion: 通过广泛的模拟验证了理论结果，为联邦学习中局部SGD的统计性质提供了新的见解。

Abstract: Federated Learning has gained traction in privacy-sensitive collaborative
environments, with local SGD emerging as a key optimization method in
decentralized settings. While its convergence properties are well-studied,
asymptotic statistical guarantees beyond convergence remain limited. In this
paper, we present two generalized Gaussian approximation results for local SGD
and explore their implications. First, we prove a Berry-Esseen theorem for the
final local SGD iterates, enabling valid multiplier bootstrap procedures.
Second, motivated by robustness considerations, we introduce two distinct
time-uniform Gaussian approximations for the entire trajectory of local SGD.
The time-uniform approximations support Gaussian bootstrap-based tests for
detecting adversarial attacks. Extensive simulations are provided to support
our theoretical results.

</details>


### [218] [SIM-Shapley: A Stable and Computationally Efficient Approach to Shapley Value Approximation](https://arxiv.org/abs/2505.08198)
*Wangxuan Fan,Siqi Li,Doudou Zhou,Yohei Okada,Chuan Hong,Molei Liu,Nan Liu*

Main category: stat.ML

TL;DR: SIM-Shapley是一种高效且稳定的Shapley值近似方法，通过随机优化显著降低计算成本，同时保持特征归因质量。


<details>
  <summary>Details</summary>
Motivation: 在高风险领域（如医疗和金融）中，可解释人工智能（XAI）对可信机器学习至关重要。Shapley值方法虽然提供了特征归因的理论框架，但计算成本高，限制了其在高维场景中的可扩展性。

Method: 提出了SIM-Shapley方法，受随机优化启发，通过理论分析方差并证明线性Q收敛，实现了稳定且高效的Shapley值近似。

Result: 实验表明，SIM-Shapley将计算时间减少高达85%，同时保持与现有基线相当的特征归因质量。

Conclusion: SIM-Shapley不仅适用于特征归因，其随机小批量迭代框架还可扩展到更广泛的样本平均近似问题，为计算效率提升提供了新途径。

Abstract: Explainable artificial intelligence (XAI) is essential for trustworthy
machine learning (ML), particularly in high-stakes domains such as healthcare
and finance. Shapley value (SV) methods provide a principled framework for
feature attribution in complex models but incur high computational costs,
limiting their scalability in high-dimensional settings. We propose Stochastic
Iterative Momentum for Shapley Value Approximation (SIM-Shapley), a stable and
efficient SV approximation method inspired by stochastic optimization. We
analyze variance theoretically, prove linear $Q$-convergence, and demonstrate
improved empirical stability and low bias in practice on real-world datasets.
In our numerical experiments, SIM-Shapley reduces computation time by up to 85%
relative to state-of-the-art baselines while maintaining comparable feature
attribution quality. Beyond feature attribution, our stochastic mini-batch
iterative framework extends naturally to a broader class of sample average
approximation problems, offering a new avenue for improving computational
efficiency with stability guarantees. Code is publicly available at
https://github.com/nliulab/SIM-Shapley.

</details>


### [219] [Lie Group Symmetry Discovery and Enforcement Using Vector Fields](https://arxiv.org/abs/2505.08219)
*Ben Shaw,Sasidhar Kunapuli,Abram Magner,Kevin R. Moon*

Main category: stat.ML

TL;DR: 论文探讨了对称性在机器学习中的重要性，并扩展了非仿射对称性发现到神经网络函数，同时引入向量场对称性强制方法，并研究了对称性搜索空间的限制。


<details>
  <summary>Details</summary>
Motivation: 对称性在机器学习中具有优势，但现有方法未充分探索非仿射对称性和对称性强制。本文旨在填补这一空白。

Method: 扩展非仿射对称性发现到神经网络函数，引入向量场对称性强制方法，并研究对称性搜索空间的限制。

Result: 提出了理论框架和实验验证，展示了对称性发现和强制的有效性。

Conclusion: 对称性发现和强制对提升机器学习模型性能具有重要意义，未来可进一步探索其应用。

Abstract: Symmetry-informed machine learning can exhibit advantages over machine
learning which fails to account for symmetry. Additionally, recent attention
has been given to continuous symmetry discovery using vector fields which serve
as infinitesimal generators for Lie group symmetries. In this paper, we extend
the notion of non-affine symmetry discovery to functions defined by neural
networks. We further extend work in this area by introducing symmetry
enforcement of smooth models using vector fields. Finally, we extend work on
symmetry discovery using vector fields by providing both theoretical and
experimental material on the restriction of the symmetry search space to
infinitesimal isometries.

</details>


### [220] [Iteratively reweighted kernel machines efficiently learn sparse functions](https://arxiv.org/abs/2505.08277)
*Libin Zhu,Damek Davis,Dmitriy Drusvyatskiy,Maryam Fazel*

Main category: stat.ML

TL;DR: 论文表明，神经网络的低维数据表示和分层结构能力并非其独有，经典核方法也能实现类似效果。通过核预测器的导数检测关键坐标，并通过迭代重加权和重新训练核机器，可以高效学习分层多项式。


<details>
  <summary>Details</summary>
Motivation: 探讨神经网络的低维数据表示和分层结构能力是否为其独有，以及经典核方法是否也能实现类似效果。

Method: 利用核预测器的导数检测关键坐标，并通过迭代重加权和重新训练核机器来学习分层多项式。

Result: 实验验证了理论，表明核方法能够高效学习分层结构。

Conclusion: 经典核方法在低维数据表示和分层结构学习方面具有与神经网络相似的潜力。

Abstract: The impressive practical performance of neural networks is often attributed
to their ability to learn low-dimensional data representations and hierarchical
structure directly from data. In this work, we argue that these two phenomena
are not unique to neural networks, and can be elicited from classical kernel
methods. Namely, we show that the derivative of the kernel predictor can detect
the influential coordinates with low sample complexity. Moreover, by
iteratively using the derivatives to reweight the data and retrain kernel
machines, one is able to efficiently learn hierarchical polynomials with finite
leap complexity. Numerical experiments illustrate the developed theory.

</details>


### [221] [Learning Treatment Allocations with Risk Control Under Partial Identifiability](https://arxiv.org/abs/2505.08378)
*Sofia Ek,Dave Zachariah*

Main category: stat.ML

TL;DR: 论文提出了一种在部分可识别设置下控制治疗风险的认证学习方法，旨在为患者群体分配有益治疗。


<details>
  <summary>Details</summary>
Motivation: 许多治疗伴随不良反应，可能对未受益患者造成不必要伤害，因此需要控制治疗风险。

Method: 提出了一种认证学习方法，能够在有限样本下控制治疗风险，适用于部分可识别设置。

Result: 方法在模拟和真实数据中均得到验证。

Conclusion: 该方法能够有效控制治疗风险，为精准医学中的治疗分配提供了一种可行方案。

Abstract: Learning beneficial treatment allocations for a patient population is an
important problem in precision medicine. Many treatments come with adverse side
effects that are not commensurable with their potential benefits. Patients who
do not receive benefits after such treatments are thereby subjected to
unnecessary harm. This is a `treatment risk' that we aim to control when
learning beneficial allocations. The constrained learning problem is challenged
by the fact that the treatment risk is not in general identifiable using either
randomized trial or observational data. We propose a certifiable learning
method that controls the treatment risk with finite samples in the partially
identified setting. The method is illustrated using both simulated and real
data.

</details>


### [222] [neuralGAM: An R Package for Fitting Generalized Additive Neural Networks](https://arxiv.org/abs/2505.08610)
*Ines Ortega-Fernandez,Marta Sestelo*

Main category: stat.ML

TL;DR: 神经网络的“黑箱”问题使其决策过程难以理解。neuralGAM包通过基于广义可加模型的神经网络拓扑结构，提供了一种高精度且可解释的深度学习方法。


<details>
  <summary>Details</summary>
Motivation: 解决神经网络决策过程不透明的问题，提高模型的可解释性。

Method: neuralGAM包实现了一种基于广义可加模型的神经网络拓扑结构，为每个特征拟合独立的神经网络以估计其对输出变量的贡献。

Result: neuralGAM包提供了一个灵活的框架，能够在不限制神经网络架构的情况下训练广义可加神经网络。

Conclusion: neuralGAM包通过结合神经网络和广义可加模型，实现了高精度和可解释性的深度学习模型，适用于合成和真实数据。

Abstract: Nowadays, Neural Networks are considered one of the most effective methods
for various tasks such as anomaly detection, computer-aided disease detection,
or natural language processing. However, these networks suffer from the
``black-box'' problem which makes it difficult to understand how they make
decisions. In order to solve this issue, an R package called neuralGAM is
introduced. This package implements a Neural Network topology based on
Generalized Additive Models, allowing to fit an independent Neural Network to
estimate the contribution of each feature to the output variable, yielding a
highly accurate and interpretable Deep Learning model. The neuralGAM package
provides a flexible framework for training Generalized Additive Neural
Networks, which does not impose any restrictions on the Neural Network
architecture. We illustrate the use of the neuralGAM package in both synthetic
and real data examples.

</details>


### [223] [Uncertainty-Aware Surrogate-based Amortized Bayesian Inference for Computationally Expensive Models](https://arxiv.org/abs/2505.08683)
*Stefania Scheurer,Philipp Reiser,Tim Brünnette,Wolfgang Nowak,Anneli Guthke,Paul-Christian Bürkner*

Main category: stat.ML

TL;DR: 提出了一种结合替代模型和摊销贝叶斯推理的方法（UA-SABI），通过显式量化替代模型的不确定性，实现快速可靠的贝叶斯推断。


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯推断方法（如MCMC和ABI）在计算昂贵模型上效率低下，而替代模型虽能降低成本，但可能引入误差导致后验估计过度自信。

Method: 提出UA-SABI框架，结合替代模型和ABI，显式量化并传播替代模型的不确定性。

Result: 实验表明，该方法在计算昂贵模型上实现了快速、可靠且可重复的贝叶斯推断。

Conclusion: UA-SABI为计算昂贵模型提供了一种高效且可靠的贝叶斯推断解决方案。

Abstract: Bayesian inference typically relies on a large number of model evaluations to
estimate posterior distributions. Established methods like Markov Chain Monte
Carlo (MCMC) and Amortized Bayesian Inference (ABI) can become computationally
challenging. While ABI enables fast inference after training, generating
sufficient training data still requires thousands of model simulations, which
is infeasible for expensive models. Surrogate models offer a solution by
providing approximate simulations at a lower computational cost, allowing the
generation of large data sets for training. However, the introduced
approximation errors and uncertainties can lead to overconfident posterior
estimates. To address this, we propose Uncertainty-Aware Surrogate-based
Amortized Bayesian Inference (UA-SABI) - a framework that combines surrogate
modeling and ABI while explicitly quantifying and propagating surrogate
uncertainties through the inference pipeline. Our experiments show that this
approach enables reliable, fast, and repeated Bayesian inference for
computationally expensive models, even under tight time constraints.

</details>


### [224] [Continuous Temporal Learning of Probability Distributions via Neural ODEs with Applications in Continuous Glucose Monitoring Data](https://arxiv.org/abs/2505.08698)
*Antonio Álvarez-López,Marcos Matabuena*

Main category: stat.ML

TL;DR: 提出了一种基于高斯混合模型和神经ODE的概率模型，用于捕捉时间依赖数据样本的动态分布变化，并在模拟和临床试验中验证了其性能。


<details>
  <summary>Details</summary>
Motivation: 研究生物标志物（如血糖）随时间变化的分布动态，以反映慢性疾病（如糖尿病）的进展。

Method: 使用高斯混合模型和神经ODE建模时间依赖的分布变化，并通过MMD非参数估计。

Result: 模型在估计准确性上与现有方法竞争，同时具有高解释性和计算效率。

Conclusion: 该方法能够检测细微的时间变化，并在临床试验中提供新的数学和临床视角。

Abstract: Modeling the continuous--time dynamics of probability distributions from
time--dependent data samples is a fundamental problem in many fields, including
digital health. The aim is to analyze how the distribution of a biomarker, such
as glucose, evolves over time and how these changes may reflect the progression
of chronic diseases such as diabetes. In this paper, we propose a novel
probabilistic model based on a mixture of Gaussian distributions to capture how
samples from a continuous-time stochastic process evolve over the time. To
model potential distribution shifts over time, we introduce a time-dependent
function parameterized by a Neural Ordinary Differential Equation (Neural ODE)
and estimate it non--parametrically using the Maximum Mean Discrepancy (MMD).
The proposed model is highly interpretable, detects subtle temporal shifts, and
remains computationally efficient. Through simulation studies, we show that it
performs competitively in terms of estimation accuracy against
state-of-the-art, less interpretable methods such as normalized gradient--flows
and non--parameteric kernel density estimators. Finally, we demonstrate the
utility of our method on digital clinical--trial data, showing how the
interventions alters the time-dependent distribution of glucose levels and
enabling a rigorous comparison of control and treatment groups from novel
mathematical and clinical perspectives.

</details>


### [225] [PCS-UQ: Uncertainty Quantification via the Predictability-Computability-Stability Framework](https://arxiv.org/abs/2505.08784)
*Abhineet Agarwal,Michael Xiao,Rebecca Barter,Omer Ronen,Boyu Fan,Bin Yu*

Main category: stat.ML

TL;DR: 本文提出了一种基于PCS框架的UQ方法（PCS-UQ），解决了传统UQ方法对模型误设的敏感性和共形推理中模型选择缺失的问题，通过预测检查和多重自举提高了不确定性估计的可靠性，并在实验中表现出更优的覆盖率和区间宽度。


<details>
  <summary>Details</summary>
Motivation: 在机器学习模型应用于高风险领域时，可靠的不确定性量化（UQ）至关重要。传统UQ方法依赖真实生成模型且对误设不鲁棒，而共形推理虽灵活但忽略模型选择，导致区间过大。

Method: PCS-UQ基于PCS框架，通过预测检查筛选模型，利用多重自举评估样本间变异性和算法不稳定性，并提出新的校准方案以提高局部适应性。

Result: 在17个回归和6个分类数据集上，PCS-UQ实现了目标覆盖率，区间宽度比共形方法减少约20%。在深度学习中，其高效近似方案在三个计算机视觉基准上减少预测集大小20%。

Conclusion: PCS-UQ通过结合模型选择和多重自举，显著提升了不确定性量化的可靠性和效率，适用于广泛任务，并在理论和实验中验证了其有效性。

Abstract: As machine learning (ML) models are increasingly deployed in high-stakes
domains, trustworthy uncertainty quantification (UQ) is critical for ensuring
the safety and reliability of these models. Traditional UQ methods rely on
specifying a true generative model and are not robust to misspecification. On
the other hand, conformal inference allows for arbitrary ML models but does not
consider model selection, which leads to large interval sizes. We tackle these
drawbacks by proposing a UQ method based on the predictability, computability,
and stability (PCS) framework for veridical data science proposed by Yu and
Kumbier. Specifically, PCS-UQ addresses model selection by using a prediction
check to screen out unsuitable models. PCS-UQ then fits these screened
algorithms across multiple bootstraps to assess inter-sample variability and
algorithmic instability, enabling more reliable uncertainty estimates. Further,
we propose a novel calibration scheme that improves local adaptivity of our
prediction sets. Experiments across $17$ regression and $6$ classification
datasets show that PCS-UQ achieves the desired coverage and reduces width over
conformal approaches by $\approx 20\%$. Further, our local analysis shows
PCS-UQ often achieves target coverage across subgroups while conformal methods
fail to do so. For large deep-learning models, we propose computationally
efficient approximation schemes that avoid the expensive multiple bootstrap
trainings of PCS-UQ. Across three computer vision benchmarks, PCS-UQ reduces
prediction set size over conformal methods by $20\%$. Theoretically, we show a
modified PCS-UQ algorithm is a form of split conformal inference and achieves
the desired coverage with exchangeable data.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [226] [PosterO: Structuring Layout Trees to Enable Language Models in Generalized Content-Aware Layout Generation](https://arxiv.org/abs/2505.07843)
*HsiaoYuan Hsu,Yuxin Peng*

Main category: cs.GR

TL;DR: 论文提出了一种基于布局的方法PosterO，利用大型语言模型（LLMs）生成多样化海报布局，解决了现有方法在通用场景下布局多样性和形状变化元素处理的不足。


<details>
  <summary>Details</summary>
Motivation: 现有海报设计方法主要关注图像增强，忽视了布局多样性和形状变化元素的需求，无法适应通用场景。

Method: PosterO将布局结构化为SVG树，通过设计意图向量化和层次节点表示，利用LLMs进行上下文学习预测新布局树。

Result: 实验表明PosterO能生成视觉吸引人的布局，并在多个基准测试中达到最新性能。

Conclusion: PosterO在通用场景下表现出色，并构建了首个多用途海报数据集PStylish7，为未来研究提供挑战。

Abstract: In poster design, content-aware layout generation is crucial for
automatically arranging visual-textual elements on the given image. With
limited training data, existing work focused on image-centric enhancement.
However, this neglects the diversity of layouts and fails to cope with
shape-variant elements or diverse design intents in generalized settings. To
this end, we proposed a layout-centric approach that leverages layout knowledge
implicit in large language models (LLMs) to create posters for omnifarious
purposes, hence the name PosterO. Specifically, it structures layouts from
datasets as trees in SVG language by universal shape, design intent
vectorization, and hierarchical node representation. Then, it applies LLMs
during inference to predict new layout trees by in-context learning with
intent-aligned example selection. After layout trees are generated, we can
seamlessly realize them into poster designs by editing the chat with LLMs.
Extensive experimental results have demonstrated that PosterO can generate
visually appealing layouts for given images, achieving new state-of-the-art
performance across various benchmarks. To further explore PosterO's abilities
under the generalized settings, we built PStylish7, the first dataset with
multi-purpose posters and various-shaped elements, further offering a
challenging test for advanced research.

</details>


### [227] [M3G: Multi-Granular Gesture Generator for Audio-Driven Full-Body Human Motion Synthesis](https://arxiv.org/abs/2505.08293)
*Zhizhuo Yin,Yuk Hang Tsui,Pan Hui*

Main category: cs.GR

TL;DR: 提出了一种名为M3G的新框架，用于从音频生成全身手势，解决了现有方法因固定粒度而无法建模不同手势模式的问题。


<details>
  <summary>Details</summary>
Motivation: 生成涵盖面部、身体、手部和全局运动的全身手势是虚拟角色创建中的重要任务，但现有系统因固定粒度的令牌化方法无法适应不同手势模式的多样性。

Method: 提出了M3G框架，包括多粒度VQ-VAE（MGVQ-VAE）用于令牌化和重建不同时间粒度的运动序列，以及多粒度令牌预测器从音频中提取信息并预测运动令牌。

Result: 实验表明，M3G在生成自然和富有表现力的全身手势方面优于现有方法。

Conclusion: M3G通过多粒度建模解决了手势生成的多样性问题，显著提升了生成效果。

Abstract: Generating full-body human gestures encompassing face, body, hands, and
global movements from audio is a valuable yet challenging task in virtual
avatar creation. Previous systems focused on tokenizing the human gestures
framewisely and predicting the tokens of each frame from the input audio.
However, one observation is that the number of frames required for a complete
expressive human gesture, defined as granularity, varies among different human
gesture patterns. Existing systems fail to model these gesture patterns due to
the fixed granularity of their gesture tokens. To solve this problem, we
propose a novel framework named Multi-Granular Gesture Generator (M3G) for
audio-driven holistic gesture generation. In M3G, we propose a novel
Multi-Granular VQ-VAE (MGVQ-VAE) to tokenize motion patterns and reconstruct
motion sequences from different temporal granularities. Subsequently, we
proposed a multi-granular token predictor that extracts multi-granular
information from audio and predicts the corresponding motion tokens. Then M3G
reconstructs the human gestures from the predicted tokens using the MGVQ-VAE.
Both objective and subjective experiments demonstrate that our proposed M3G
framework outperforms the state-of-the-art methods in terms of generating
natural and expressive full-body human gestures.

</details>


### [228] [CAD-Coder:Text-Guided CAD Files Code Generation](https://arxiv.org/abs/2505.08686)
*Changqi He,Shuhan Zhang,Liguo Zhang,Jiajun Miao*

Main category: cs.GR

TL;DR: CAD-Coder是一个将自然语言指令转换为可编辑CAD脚本代码的框架，解决了传统生成方法缺乏交互性和几何标注的问题。


<details>
  <summary>Details</summary>
Motivation: 传统CAD依赖专家手工绘制或修改现有库文件，无法快速个性化；现有生成方法缺乏交互性和几何标注，限制了实际应用。

Method: 提出CAD-Coder框架，将自然语言指令转换为CAD脚本代码，生成可编辑的.Dxf文件，并构建包含29,130个Dxf文件及其脚本代码的数据集。

Result: 在多种2D/3D CAD生成任务中，CAD-Coder表现出优越的交互能力，并生成带有几何标注的可编辑草图。

Conclusion: CAD-Coder为个性化CAD生成提供了交互性和可编辑性，扩展了生成式AI在制造领域的应用潜力。

Abstract: Computer-aided design (CAD) is a way to digitally create 2D drawings and 3D
models of real-world products. Traditional CAD typically relies on hand-drawing
by experts or modifications of existing library files, which doesn't allow for
rapid personalization. With the emergence of generative artificial
intelligence, convenient and efficient personalized CAD generation has become
possible. However, existing generative methods typically produce outputs that
lack interactive editability and geometric annotations, limiting their
practical applications in manufacturing. To enable interactive generative CAD,
we propose CAD-Coder, a framework that transforms natural language instructions
into CAD script codes, which can be executed in Python environments to generate
human-editable CAD files (.Dxf). To facilitate the generation of editable CAD
sketches with annotation information, we construct a comprehensive dataset
comprising 29,130 Dxf files with their corresponding script codes, where each
sketch preserves both editability and geometric annotations. We evaluate
CAD-Coder on various 2D/3D CAD generation tasks against existing methods,
demonstrating superior interactive capabilities while uniquely providing
editable sketches with geometric annotations.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [229] [AI-Based Crypto Tokens: The Illusion of Decentralized AI?](https://arxiv.org/abs/2505.07828)
*Rischan Mafrur*

Main category: cs.DC

TL;DR: 本文综述了基于区块链和人工智能（AI）的AI代币项目，分析了其技术架构、代币用途、共识机制和商业模式，并指出了当前实现中的技术局限性和商业模式的不足。


<details>
  <summary>Details</summary>
Motivation: 研究AI代币如何通过区块链技术实现去中心化AI服务，并评估其是否提供了超越传统中心化AI服务的价值。

Method: 通过全面审查领先的AI代币项目，分析其技术架构、代币功能、共识机制和商业模式。

Result: 发现当前AI代币项目在技术上依赖链下计算，链上智能能力有限，且面临可扩展性挑战；商业模式上多为中心化AI服务的简单复制，缺乏创新价值。

Conclusion: 尽管新兴技术为去中心化AI生态系统提供了改进路径，但当前AI代币的实现与承诺之间仍存在显著差距，需要更务实的评估和发展方向。

Abstract: The convergence of blockchain and artificial intelligence (AI) has led to the
emergence of AI-based tokens, which are cryptographic assets designed to power
decentralized AI platforms and services. This paper provides a comprehensive
review of leading AI-token projects, examining their technical architectures,
token utilities, consensus mechanisms, and underlying business models. We
explore how these tokens operate across various blockchain ecosystems and
assess the extent to which they offer value beyond traditional centralized AI
services. Based on this assessment, our analysis identifies several core
limitations. From a technical perspective, many platforms depend extensively on
off-chain computation, exhibit limited capabilities for on-chain intelligence,
and encounter significant scalability challenges. From a business perspective,
many models appear to replicate centralized AI service structures, simply
adding token-based payment and governance layers without delivering truly novel
value. In light of these challenges, we also examine emerging developments that
may shape the next phase of decentralized AI systems. These include approaches
for on-chain verification of AI outputs, blockchain-enabled federated learning,
and more robust incentive frameworks. Collectively, while emerging innovations
offer pathways to strengthen decentralized AI ecosystems, significant gaps
remain between the promises and the realities of current AI-token
implementations. Our findings contribute to a growing body of research at the
intersection of AI and blockchain, highlighting the need for critical
evaluation and more grounded approaches as the field continues to evolve.

</details>


### [230] [Patchwork: A Unified Framework for RAG Serving](https://arxiv.org/abs/2505.07833)
*Bodun Hu,Luis Pabon,Saurabh Agarwal,Aditya Akella*

Main category: cs.DC

TL;DR: Patchwork是一个端到端的RAG服务框架，通过灵活的接口、分布式推理系统和在线调度机制，显著提升了性能，减少了SLO违规。


<details>
  <summary>Details</summary>
Motivation: RAG系统在部署时面临异构计算管道的效率挑战，需要一种全面的解决方案。

Method: Patchwork提供灵活的接口支持自定义RAG管道，优化分布式推理系统，并引入在线调度机制动态管理资源。

Result: 实验表明，Patchwork在四种RAG实现中性能优于商业方案，吞吐量提升48%，SLO违规减少24%。

Conclusion: Patchwork通过创新架构有效解决了RAG系统的效率瓶颈，具有实际应用价值。

Abstract: Retrieval Augmented Generation (RAG) has emerged as a new paradigm for
enhancing Large Language Model reliability through integration with external
knowledge sources. However, efficient deployment of these systems presents
significant technical challenges due to their inherently heterogeneous
computational pipelines comprising LLMs, databases, and specialized processing
components. We introduce Patchwork, a comprehensive end-to-end RAG serving
framework designed to address these efficiency bottlenecks. Patchwork's
architecture offers three key innovations: First, it provides a flexible
specification interface enabling users to implement custom RAG pipelines.
Secondly, it deploys these pipelines as distributed inference systems while
optimizing for the unique scalability characteristics of individual RAG
components. Third, Patchwork incorporates an online scheduling mechanism that
continuously monitors request load and execution progress, dynamically
minimizing SLO violations through strategic request prioritization and resource
auto-scaling. Our experimental evaluation across four distinct RAG
implementations demonstrates that Patchwork delivers substantial performance
improvements over commercial alternatives, achieving throughput gains exceeding
48% while simultaneously reducing SLO violations by ~24%.

</details>


### [231] [Fused3S: Fast Sparse Attention on Tensor Cores](https://arxiv.org/abs/2505.08098)
*Zitong Li,Aparna Chandramowlishwaran*

Main category: cs.DC

TL;DR: Fused3S是一种首次将稀疏注意力计算中的三个稀疏矩阵操作（3S）融合的算法，显著提升了GPU上的计算效率。


<details>
  <summary>Details</summary>
Motivation: 稀疏注意力是许多神经网络模型的核心组件，但现有方法在GPU上执行3S计算模式时存在效率问题，包括张量核心利用率低和数据移动成本高。

Method: 提出Fused3S算法，通过联合优化三个稀疏矩阵操作（SDDMM、softmax和SpMM），最大化张量核心利用率和减少数据移动。

Result: 在H100和A30 GPU上，Fused3S比现有方法快1.6-16.3倍和1.5-14倍；在Graph Transformer推理中，端到端性能提升1.05-5.36倍。

Conclusion: Fused3S在多种数据集和GPU架构上均优于现有3S基线方法，为稀疏注意力计算提供了高效解决方案。

Abstract: Sparse attention is a core building block in many leading neural network
models, from graph-structured learning to sparse sequence modeling. It can be
decomposed into a sequence of three sparse matrix operations (3S): sampled
dense-dense matrix multiplication (SDDMM), softmax normalization, and sparse
matrix multiplication (SpMM). Efficiently executing the 3S computational
pattern on modern GPUs remains challenging due to (a) the mismatch between
unstructured sparsity and tensor cores optimized for dense operations, and (b)
the high cost of data movement. Previous works have optimized these sparse
operations individually or addressed one of these challenges. This paper
introduces Fused3S, the first fused 3S algorithm that jointly maximizes tensor
core utilization and minimizes data movement. Across real-world graph datasets,
Fused3S achieves $1.6- 16.3\times$ and $1.5-14\times$ speedup over
state-of-the-art on H100 and A30 GPUs. Furthermore, integrating Fused3S into
Graph Transformer inference accelerates end-to-end performance by
$1.05-5.36\times$, consistently outperforming all 3S baselines across diverse
datasets (single and batched graphs) and GPU architectures.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [232] [Probabilistic approach to longitudinal response prediction: application to radiomics from brain cancer imaging](https://arxiv.org/abs/2505.07973)
*Isabella Cama,Michele Piana,Cristina Campi,Sara Garbarino*

Main category: stat.AP

TL;DR: 提出一种概率模型，用于纵向预测疾病进展，整合基线特征和随访数据，处理不确定性并减少问题维度。


<details>
  <summary>Details</summary>
Motivation: 通过纵向影像分析追踪疾病进展和治疗效果，动态评估治疗有效性和疾病演化。

Method: 开发概率模型，结合基线特征和随访数据，处理预测中的不确定性，避免依赖中间随访数据。

Result: 在合成场景和脑癌数据集中验证，模型性能与现有方法相当，同时更好地处理不确定性和维度问题。

Conclusion: 该模型为纵向预测提供了一种有效且灵活的方法，特别适用于处理不确定性和减少数据依赖。

Abstract: Longitudinal imaging analysis tracks disease progression and treatment
response over time, providing dynamic insights into treatment efficacy and
disease evolution. Radiomic features extracted from medical imaging can support
the study of disease progression and facilitate longitudinal prediction of
clinical outcomes. This study presents a probabilistic model for longitudinal
response prediction, integrating baseline features with intermediate
follow-ups. The probabilistic nature of the model naturally allows to handle
the instrinsic uncertainty of the longitudinal prediction of disease
progression. We evaluate the proposed model against state-of-the-art disease
progression models in both a synthetic scenario and using a brain cancer
dataset. Results demonstrate that the approach is competitive against existing
methods while uniquely accounting for uncertainty and controlling the growth of
problem dimensionality, eliminating the need for data from intermediate
follow-ups.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [233] [ai.txt: A Domain-Specific Language for Guiding AI Interactions with the Internet](https://arxiv.org/abs/2505.07834)
*Yuekang Li,Wei Song,Bangshuo Zhu,Dong Gong,Yi Liu,Gelei Deng,Chunyang Chen,Lei Ma,Jun Sun,Toby Walsh,Jingling Xue*

Main category: cs.NI

TL;DR: ai.txt是一种新型领域特定语言（DSL），旨在规范AI模型、代理与网络内容的交互，弥补robots.txt的不足，支持元素级精确控制和自然语言指令。


<details>
  <summary>Details</summary>
Motivation: 随着AI越来越多地参与在线内容处理（如训练、摘要和修改），现有监管方法缺乏必要的精细度和语义表达能力，无法确保伦理和法律合规。

Method: ai.txt扩展了传统的基于URL的访问控制，支持元素级精确监管和自然语言指令，并提供集成开发环境和自动XML生成工具。提出了两种合规机制：基于XML的程序化执行和自然语言提示集成。

Result: 通过初步实验和案例研究，验证了所提机制的有效性。

Conclusion: ai.txt有助于规范AI与互联网的交互，促进数字生态系统中AI的负责任使用。

Abstract: We introduce ai.txt, a novel domain-specific language (DSL) designed to
explicitly regulate interactions between AI models, agents, and web content,
addressing critical limitations of the widely adopted robots.txt standard. As
AI increasingly engages with online materials for tasks such as training,
summarization, and content modification, existing regulatory methods lack the
necessary granularity and semantic expressiveness to ensure ethical and legal
compliance. ai.txt extends traditional URL-based access controls by enabling
precise element-level regulations and incorporating natural language
instructions interpretable by AI systems. To facilitate practical deployment,
we provide an integrated development environment with code autocompletion and
automatic XML generation. Furthermore, we propose two compliance mechanisms:
XML-based programmatic enforcement and natural language prompt integration, and
demonstrate their effectiveness through preliminary experiments and case
studies. Our approach aims to aid the governance of AI-Internet interactions,
promoting responsible AI use in digital ecosystems.

</details>


### [234] [Intelligent Product 3.0: Decentralised AI Agents and Web3 Intelligence Standards](https://arxiv.org/abs/2505.07835)
*Alex C. Y. Wong,Duncan McFarlane,C. Ellarby,M. Lee,M. Kuok*

Main category: cs.NI

TL;DR: 论文回顾了智能产品的发展历程，从早期的Auto-ID项目到基于区块链、Web3和AI的现代技术，提出了智能产品3.0的新规范。


<details>
  <summary>Details</summary>
Motivation: 探讨如何利用去中心化身份、区块链和AI技术，提升智能产品的自主性和交互能力。

Method: 通过分析区块链、Web3和AI的进展，结合去中心化身份和AI协作，提出智能产品3.0的新规范。

Result: 展示了去中心化和AI驱动的能力如何实现物理AI与日常产品的无缝交互。

Conclusion: 智能产品3.0的规范为未来智能产品的自主性和互联性提供了新的方向。

Abstract: Twenty-five years ago, the specification of the Intelligent Product was
established, envisaging real-time connectivity that not only enables products
to gather accurate data about themselves but also allows them to assess and
influence their own destiny. Early work by the Auto-ID project focused on
creating a single, open-standard repository for storing and retrieving product
information, laying a foundation for scalable connectivity. A decade later, the
approach was revisited in light of low-cost RFID systems that promised a
low-cost link between physical goods and networked information environments.
Since then, advances in blockchain, Web3, and artificial intelligence have
introduced unprecedented levels of resilience, consensus, and autonomy. By
leveraging decentralised identity, blockchain-based product information and
history, and intelligent AI-to-AI collaboration, this paper examines these
developments and outlines a new specification for the Intelligent Product 3.0,
illustrating how decentralised and AI-driven capabilities facilitate seamless
interaction between physical AI and everyday products.

</details>


### [235] [Efficient Telecom Specific LLM: TSLAM-Mini with QLoRA and Digital Twin Data](https://arxiv.org/abs/2505.07877)
*Vignesh Ethiraj,Divya Vijay,Sidhanth Menon,Heblin Berscilla*

Main category: cs.NI

TL;DR: 论文通过微调TSLAM-Mini模型，结合电信领域专用数据集和QLoRA技术，显著提升了大型语言模型在实时电信应用中的性能。


<details>
  <summary>Details</summary>
Motivation: 通用大型语言模型在电信领域表现不佳，需针对其专业需求优化。

Method: 使用100,000个电信领域样本微调TSLAM-Mini模型，采用QLoRA技术提升效率。

Result: TSLAM-Mini在电信应用中表现优异，验证了领域专用数据集和PEFT方法的有效性。

Conclusion: 领域专用数据集和高效微调技术可显著提升模型在专业领域的性能。

Abstract: General-purpose large language models (LLMs), despite their broad
capabilities accrued from open-world data, frequently exhibit suboptimal
performance when confronted with the nuanced and specialized demands inherent
in real-time telecommunications applications. This investigation addresses this
critical limitation through the meticulous fine-tuning of TSLAM-Mini developed
by NetoAI, a compact (3.8-billion parameter) causal language model
architecturally derived from Phi-4 Mini Instruct 4B. The fine-tuning regimen
leverages a bespoke dataset comprising 100,000 samples, strategically
engineered to address 20 pivotal telecommunications use-cases, encompassing
domains such as Network Fundamentals, IP Routing, MPLS, Network Security,
Automation, OSS/BSS, RAN, Mobile Core, Satellite Communications, and Ethical
AI. This dataset was curated utilizing NetoAI's DigiTwin platform, enriched
with granular insights from venerated network Subject Matter Experts (SMEs) and
authoritative RFC documents, thereby capturing high-fidelity representations of
real-world network dynamics through simulations inspired by digital twin
paradigms. Employing Quantized Low-Rank Adaptation (QLoRA), a state-of-the-art
Parameter Efficient Fine-Tuning (PEFT) technique, we achieved substantial
training efficiency and enabled prospective deployment on resource-constrained
hardware. A novel evaluation framework, predicated on a high-capacity LLM
(Qwen3-235B-A22B) functioning as an automated adjudicator, was instituted to
rigorously assess instruction-following fidelity and response quality across
the specified telecom use-cases. Empirical results unequivocally demonstrate
TSLAM-Mini's superior aptitude in telecom-centric applications, underscoring
the profound efficacy of domain-specific datasets and PEFT methodologies for
advancing intelligent network management.

</details>


### [236] [ML-Enabled Eavesdropper Detection in Beyond 5G IIoT Networks](https://arxiv.org/abs/2505.07837)
*Maria-Lamprini A. Bartsioka,Ioannis A. Bartsiokas,Panagiotis K. Gkonis,Dimitra I. Kaklamani,Iakovos S. Venieris*

Main category: cs.NI

TL;DR: 论文探讨了在5G/B5G网络中利用ML/DL技术检测窃听行为，DCNN和RF模型表现优异，准确率接近100%。


<details>
  <summary>Details</summary>
Motivation: 传统加密方法在去中心化IIoT环境中面临扩展性和复杂性挑战，AI驱动的物理层技术成为解决方案。

Method: 使用模拟的B5G异构无线网络，评估RF、DCNN和LSTM模型，基于CSI、位置数据和传输功率分类用户。

Result: DCNN和RF模型在检测窃听者时准确率接近100%，且无虚警。

Conclusion: AI与PLS结合在下一代无线网络中具有巨大潜力，可应对安全威胁。

Abstract: Advanced fifth generation (5G) and beyond (B5G) communication networks have
revolutionized wireless technologies, supporting ultra-high data rates, low
latency, and massive connectivity. However, they also introduce
vulnerabilities, particularly in decentralized Industrial Internet of Things
(IIoT) environments. Traditional cryptographic methods struggle with
scalability and complexity, leading researchers to explore Artificial
Intelligence (AI)-driven physical layer techniques for secure communications.
In this context, this paper focuses on the utilization of Machine and Deep
Learning (ML/DL) techniques to tackle with the common problem of eavesdropping
detection. To this end, a simulated industrial B5G heterogeneous wireless
network is used to evaluate the performance of various ML/DL models, including
Random Forests (RF), Deep Convolutional Neural Networks (DCNN), and Long
Short-Term Memory (LSTM) networks. These models classify users as either
legitimate or malicious ones based on channel state information (CSI), position
data, and transmission power. According to the presented numerical results,
DCNN and RF models achieve a detection accuracy approaching 100\% in
identifying eavesdroppers with zero false alarms. In general, this work
underlines the great potential of combining AI and Physical Layer Security
(PLS) for next-generation wireless networks in order to address evolving
security threats.

</details>


### [237] [Token Communication-Driven Multimodal Large Models in Resource-Constrained Multiuser Networks](https://arxiv.org/abs/2505.07841)
*Junhe Zhang,Wanli Ni,Pengwei Wang,Dongyu Wang*

Main category: cs.NI

TL;DR: 提出了一种基于令牌通信的范式，用于在资源受限网络中部署多模态大模型（MLMs），通过提取任务相关令牌并优化传输效率，实现了13.7%的测试精度提升。


<details>
  <summary>Details</summary>
Motivation: 无线边缘智能应用的普及和多模态数据的快速增长，使得在资源受限网络中部署MLMs面临带宽、计算能力和延迟等挑战。

Method: 设计了对比分割微调方法将多模态输入投影到共享特征空间，并采用轻量级压缩技术减少令牌传输大小。

Result: 在不同信噪比条件下，测试精度提升了13.7%，且收敛速度更快。

Conclusion: 令牌通信范式为多用户网络中MLMs的可扩展和弹性部署提供了有效解决方案。

Abstract: The proliferation of intelligent applications at the wireless edge, alongside
the exponential growth of multimodal data, poses challenges for deploying
multimodal large models (MLMs) in resource-constrained networks. These
constraints manifest as limited bandwidth, computational capacity, and
stringent latency requirements, particularly under low signal-to-noise ratio
(SNR) conditions. To overcome these limitations, we propose a token
communication paradigm that facilitates the decentralized deployment of MLMs
across user devices and edge infrastructure (e.g., base stations). In this
paradigm, task-relevant tokens are extracted from multimodal inputs and serve
as the primary medium for communication between distributed model components.
To align semantics and optimize transmission efficiency, we propose a
dual-pronged approach: 1) We design a contrastive split fine-tuning method to
project heterogeneous modalities into a shared feature space, enabling seamless
interaction between model components while preserving modal-specific semantics.
2) We employ a lightweight compression technique to reduce the size of
transmitted tokens, minimizing bandwidth consumption without sacrificing
task-critical information. The proposed framework integrates collaborative
fine-tuning of both the foundation model and multimodal transceivers, ensuring
that token generation and utilization are tailored to specific downstream
tasks. Simulation experiments conducted under different SNR conditions
demonstrate that our method results in a $13.7\%$ improvement in test accuracy.
Furthermore, our approach exhibits quicker convergence rates, even with reduced
token lengths, highlighting the promise of token communication for facilitating
more scalable and resilient MLM implementations in practical multiuser
networks.

</details>


### [238] [VoI-Driven Joint Optimization of Control and Communication in Vehicular Digital Twin Network](https://arxiv.org/abs/2505.07892)
*Lei Lei,Kan Zheng,Jie Mei,Xuemin,Shen*

Main category: cs.NI

TL;DR: 本文提出了一种车联网数字孪生网络（VDTN）架构，通过联合优化控制与通信，利用深度强化学习（DRL）模块实现最优策略，并在车队场景中验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 6G网络与数字孪生技术的结合为车联网系统提供了优化通信与控制性能的潜力，但需要解决多时间尺度决策的复杂性。

Method: 提出VDTN架构，定义基于控制性能的信息价值（VoI）概念，并设计基于DRL的联合优化框架。

Result: 仿真结果表明，该框架在车队场景中能有效优化控制与通信性能。

Conclusion: VDTN架构及联合优化框架为车联网系统的性能提升提供了可行方案。

Abstract: The vision of sixth-generation (6G) wireless networks paves the way for the
seamless integration of digital twins into vehicular networks, giving rise to a
Vehicular Digital Twin Network (VDTN). The large amount of computing resources
as well as the massive amount of spatial-temporal data in Digital Twin (DT)
domain can be utilized to enhance the communication and control performance of
Internet of Vehicle (IoV) systems. In this article, we first propose the
architecture of VDTN, emphasizing key modules that center on functions related
to the joint optimization of control and communication. We then delve into the
intricacies of the multitimescale decision process inherent in joint
optimization in VDTN, specifically investigating the dynamic interplay between
control and communication. To facilitate the joint optimization, we define two
Value of Information (VoI) concepts rooted in control performance.
Subsequently, utilizing VoI as a bridge between control and communication, we
introduce a novel joint optimization framework, which involves iterative
processing of two Deep Reinforcement Learning (DRL) modules corresponding to
control and communication to derive the optimal policy. Finally, we conduct
simulations of the proposed framework applied to a platoon scenario to
demonstrate its effectiveness in ensu

</details>


### [239] [Channel Fingerprint Construction for Massive MIMO: A Deep Conditional Generative Approach](https://arxiv.org/abs/2505.07893)
*Zhenzhou Jin,Li You,Xudong Li,Zhen Gao,Yuanwei Liu,Xiang-Gen Xia,Xiqi Gao*

Main category: cs.NI

TL;DR: 论文提出了一种基于条件生成扩散模型（CGDM）的CF孪生方法，用于解决大规模MIMO系统中粗粒度信道指纹（CF）到细粒度CF的转换问题，并展示了其优越的重建性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 由于实际感知节点和测试车辆的成本限制，粗粒度的CF不足以支持无线收发器设计，因此需要一种方法将粗粒度CF转换为细粒度CF。

Method: 设计了条件生成扩散模型（CGDM），利用变分推理技术推导证据下界（ELBO），并通过去噪神经网络优化和轻量化技术（如剪枝和知识蒸馏）实现高效生成。

Result: 实验结果表明，该方法在重建性能上显著优于基线方法，且在不同放大因子的零样本测试中表现出良好的可扩展性和泛化能力。

Conclusion: 提出的CF孪生方法通过CGDM成功连接了粗粒度和细粒度CF，为智能环境感知通信提供了有效支持。

Abstract: Accurate channel state information (CSI) acquisition for massive
multiple-input multiple-output (MIMO) systems is essential for future mobile
communication networks. Channel fingerprint (CF), also referred to as channel
knowledge map, is a key enabler for intelligent environment-aware communication
and can facilitate CSI acquisition. However, due to the cost limitations of
practical sensing nodes and test vehicles, the resulting CF is typically
coarse-grained, making it insufficient for wireless transceiver design. In this
work, we introduce the concept of CF twins and design a conditional generative
diffusion model (CGDM) with strong implicit prior learning capabilities as the
computational core of the CF twin to establish the connection between coarse-
and fine-grained CFs. Specifically, we employ a variational inference technique
to derive the evidence lower bound (ELBO) for the log-marginal distribution of
the observed fine-grained CF conditioned on the coarse-grained CF, enabling the
CGDM to learn the complicated distribution of the target data. During the
denoising neural network optimization, the coarse-grained CF is introduced as
side information to accurately guide the conditioned generation of the CGDM. To
make the proposed CGDM lightweight, we further leverage the additivity of
network layers and introduce a one-shot pruning approach along with a
multi-objective knowledge distillation technique. Experimental results show
that the proposed approach exhibits significant improvement in reconstruction
performance compared to the baselines. Additionally, zero-shot testing on
reconstruction tasks with different magnification factors further demonstrates
the scalability and generalization ability of the proposed approach.

</details>


### [240] [EnvCDiff: Joint Refinement of Environmental Information and Channel Fingerprints via Conditional Generative Diffusion Model](https://arxiv.org/abs/2505.07894)
*Zhenzhou Jin,Li You,Xiang-Gen Xia,Xiqi Gao*

Main category: cs.NI

TL;DR: 本文提出了一种名为CDiff的深度条件生成学习方法，用于从粗粒度数据中重构细粒度的环境感知信道指纹（EnvCF）。


<details>
  <summary>Details</summary>
Motivation: 由于现有设备获取的环境信息和信道指纹（CF）多为粗粒度，无法有效指导无线传输设计，因此需要一种方法提升其精度。

Method: 采用定制化的条件生成扩散模型（CDiff），同时优化环境信息和CF，生成细粒度的EnvCF。

Result: 实验表明，CDiff在EnvCF构建性能上显著优于基线方法。

Conclusion: CDiff方法成功解决了粗粒度数据问题，为环境感知通信提供了更精确的信道指纹。

Abstract: The paradigm shift from environment-unaware communication to intelligent
environment-aware communication is expected to facilitate the acquisition of
channel state information for future wireless communications. Channel
Fingerprint (CF), as an emerging enabling technology for environment-aware
communication, provides channel-related knowledge for potential locations
within the target communication area. However, due to the limited availability
of practical devices for sensing environmental information and measuring
channel-related knowledge, most of the acquired environmental information and
CF are coarse-grained, insufficient to guide the design of wireless
transmissions. To address this, this paper proposes a deep conditional
generative learning approach, namely a customized conditional generative
diffusion model (CDiff). The proposed CDiff simultaneously refines
environmental information and CF, reconstructing a fine-grained CF that
incorporates environmental information, referred to as EnvCF, from its
coarse-grained counterpart. Experimental results show that the proposed
approach significantly improves the performance of EnvCF construction compared
to the baselines.

</details>


### [241] [Online Learning-based Adaptive Beam Switching for 6G Networks: Enhancing Efficiency and Resilience](https://arxiv.org/abs/2505.08032)
*Seyed Bagher Hashemi Natanzi,Zhicong Zhu,Bo Tang*

Main category: cs.NI

TL;DR: 本文提出了一种基于深度强化学习（DRL）的在线学习框架，用于6G网络中的自适应波束切换，显著提升了信噪比、吞吐量和准确性。


<details>
  <summary>Details</summary>
Motivation: 6G网络中的自适应波束切换面临高频、移动性和阻塞等挑战，需要一种更鲁棒的方法来优化实时性能。

Method: 采用深度强化学习（DRL），结合增强的状态表示（速度和阻塞历史）、GRU架构和优先级经验回放，进行实时波束优化。

Result: 在时间相关阻塞环境下，该方法显著提升了信噪比、吞吐量和准确性，优于传统启发式方法和反应式多臂老虎机（MAB）基线。

Conclusion: 研究表明，记忆和优先级学习对6G波束管理的鲁棒性至关重要，同时确认MAB是一个强大的基线方法。

Abstract: Adaptive beam switching in 6G networks is challenged by high frequencies,
mobility, and blockage. We propose an Online Learning framework using Deep
Reinforcement Learning (DRL) with an enhanced state representation (velocity
and blockage history), a GRU architecture, and prioritized experience replay
for real-time beam optimization. Validated via Nvidia Sionna under
time-correlated blockage, our approach significantly enhances resilience in
SNR, throughput, and accuracy compared to a conventional heuristic.
Furthermore, the enhanced DRL agent outperforms a reactive Multi-Armed Bandit
(MAB) baseline by leveraging temporal dependencies, achieving lower performance
variability. This demonstrates the benefits of memory and prioritized learning
for robust 6G beam management, while confirming MAB as a strong baseline.

</details>


### [242] [Mobile Jamming Mitigation in 5G Networks: A MUSIC-Based Adaptive Beamforming Approach](https://arxiv.org/abs/2505.08046)
*Olivia Holguin,Rachel Donati,Seyed bagher Hashemi Natanzi,Bo Tang*

Main category: cs.NI

TL;DR: 提出了一种结合MUSIC、MVDR和机器学习的智能抗干扰框架，显著提升了5G网络在军事通信中的抗干扰能力。


<details>
  <summary>Details</summary>
Motivation: 移动干扰器对5G网络（尤其是军事通信）构成严重威胁，需要一种高效且自适应的抗干扰解决方案。

Method: 结合MUSIC算法进行高分辨率DoA估计，MVDR波束成形抑制干扰，并利用机器学习增强DoA预测。

Result: 在高速公路场景的仿真中，平均SNR提升9.58 dB（最高11.08 dB），DoA估计准确率达99.8%。

Conclusion: 该框架计算高效且适应动态干扰器移动模式，优于传统抗干扰技术，是5G通信在复杂环境中的可靠解决方案。

Abstract: Mobile jammers pose a critical threat to 5G networks, particularly in
military communications. We propose an intelligent anti-jamming framework that
integrates Multiple Signal Classification (MUSIC) for high-resolution
Direction-of-Arrival (DoA) estimation, Minimum Variance Distortionless Response
(MVDR) beamforming for adaptive interference suppression, and machine learning
(ML) to enhance DoA prediction for mobile jammers. Extensive simulations in a
realistic highway scenario demonstrate that our hybrid approach achieves an
average Signal-to-Noise Ratio (SNR) improvement of 9.58 dB (maximum 11.08 dB)
and up to 99.8% DoA estimation accuracy. The framework's computational
efficiency and adaptability to dynamic jammer mobility patterns outperform
conventional anti-jamming techniques, making it a robust solution for securing
5G communications in contested environments.

</details>


### [243] [Graph-Based Floor Separation Using Node Embeddings and Clustering of WiFi Trajectories](https://arxiv.org/abs/2505.08088)
*Rabia Yasa Kostas,Kahraman Kostas*

Main category: cs.NI

TL;DR: 本文提出了一种基于图的Wi-Fi指纹轨迹楼层分离方法，解决了室内垂直定位的挑战。


<details>
  <summary>Details</summary>
Motivation: 室内定位系统在多层复杂环境中对基于位置的服务至关重要，但垂直定位仍具挑战性。

Method: 构建节点为Wi-Fi指纹、边为信号相似性和上下文转换的图，使用Node2Vec生成低维嵌入，并用K-means聚类识别楼层。

Result: 在华为大学挑战赛2021数据集上，准确率为68.97%，F1分数为61.99%，调整兰德指数为57.19%。

Conclusion: 该方法对信号噪声和建筑复杂性具有鲁棒性，为楼层级定位提供了可扩展的解决方案。

Abstract: Indoor positioning systems (IPSs) are increasingly vital for location-based
services in complex multi-storey environments. This study proposes a novel
graph-based approach for floor separation using Wi-Fi fingerprint trajectories,
addressing the challenge of vertical localization in indoor settings. We
construct a graph where nodes represent Wi-Fi fingerprints, and edges are
weighted by signal similarity and contextual transitions. Node2Vec is employed
to generate low-dimensional embeddings, which are subsequently clustered using
K-means to identify distinct floors. Evaluated on the Huawei University
Challenge 2021 dataset, our method outperforms traditional community detection
algorithms, achieving an accuracy of 68.97%, an F1- score of 61.99%, and an
Adjusted Rand Index of 57.19%. By publicly releasing the preprocessed dataset
and implementation code, this work contributes to advancing research in indoor
positioning. The proposed approach demonstrates robustness to signal noise and
architectural complexities, offering a scalable solution for floor-level
localization.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [244] [Not that Groove: Zero-Shot Symbolic Music Editing](https://arxiv.org/abs/2505.08203)
*Li Zhang*

Main category: cs.SD

TL;DR: 论文提出了一种基于零样本提示的大语言模型（LLM）方法，用于符号音乐编辑，解决了音乐生成中缺乏标注数据的问题，并通过设计创新的格式和提供评估数据集来验证效果。


<details>
  <summary>Details</summary>
Motivation: 当前AI音乐生成主要关注音频，但因其灵活性不足而应用受限。本文旨在通过符号音乐编辑，仅依赖文本指令实现更高灵活性。

Method: 利用零样本提示的LLM进行鼓点编辑，设计了创新的格式连接LLM与音乐，并提供了与音乐家判断高度一致的评估数据集。

Result: 证明了零样本提示的LLM能有效编辑鼓点，且评估数据集验证了方法的有效性。

Conclusion: 该方法为符号音乐编辑提供了灵活且有效的解决方案，填补了音乐生成领域的空白。

Abstract: Most work in AI music generation focused on audio, which has seen limited use
in the music production industry due to its rigidity. To maximize flexibility
while assuming only textual instructions from producers, we are among the first
to tackle symbolic music editing. We circumvent the known challenge of lack of
labeled data by proving that LLMs with zero-shot prompting can effectively edit
drum grooves. The recipe of success is a creatively designed format that
interfaces LLMs and music, while we facilitate evaluation by providing an
evaluation dataset with annotated unit tests that highly aligns with musicians'
judgment.

</details>


### [245] [Fast Text-to-Audio Generation with Adversarial Post-Training](https://arxiv.org/abs/2505.08175)
*Zachary Novack,Zach Evans,Zack Zukowski,Josiah Taylor,CJ Carr,Julian Parker,Adnan Al-Sinan,Gian Marco Iodice,Julian McAuley,Taylor Berg-Kirkpatrick,Jordi Pons*

Main category: cs.SD

TL;DR: 提出了一种名为ARC的对抗性后训练方法，用于加速扩散/流模型，显著降低文本到音频系统的推理延迟。


<details>
  <summary>Details</summary>
Motivation: 现有文本到音频系统推理速度慢，限制了其在创意应用中的实用性。

Method: 结合相对对抗性训练和对比判别器目标，优化Stable Audio Open模型。

Result: 在H100上生成12秒44.1kHz立体声音频仅需75毫秒，移动设备上生成7秒音频，是目前最快的文本到音频模型。

Conclusion: ARC后训练方法有效加速扩散/流模型，无需蒸馏，显著提升推理速度。

Abstract: Text-to-audio systems, while increasingly performant, are slow at inference
time, thus making their latency unpractical for many creative applications. We
present Adversarial Relativistic-Contrastive (ARC) post-training, the first
adversarial acceleration algorithm for diffusion/flow models not based on
distillation. While past adversarial post-training methods have struggled to
compare against their expensive distillation counterparts, ARC post-training is
a simple procedure that (1) extends a recent relativistic adversarial
formulation to diffusion/flow post-training and (2) combines it with a novel
contrastive discriminator objective to encourage better prompt adherence. We
pair ARC post-training with a number optimizations to Stable Audio Open and
build a model capable of generating $\approx$12s of 44.1kHz stereo audio in
$\approx$75ms on an H100, and $\approx$7s on a mobile edge-device, the fastest
text-to-audio model to our knowledge.

</details>


### [246] [A Mamba-based Network for Semi-supervised Singing Melody Extraction Using Confidence Binary Regularization](https://arxiv.org/abs/2505.08681)
*Xiaoliang He,Kangjie Dong,Jingkai Cao,Shuai Yu,Wei Li,Yi Yu*

Main category: cs.SD

TL;DR: 提出了一种基于Mamba的网络SpectMamba，用于半监督的歌唱旋律提取，解决了现有方法在计算效率、频率监督和标注数据不足方面的问题。


<details>
  <summary>Details</summary>
Motivation: 现有歌唱旋律提取方法存在计算效率低、频率监督不准确以及标注数据不足的问题。

Method: 引入视觉Mamba实现线性计算复杂度，提出note-f0解码器模拟音乐表演，并使用置信二元正则化模块利用未标注数据。

Result: 在多个公开数据集上验证了方法的有效性。

Conclusion: SpectMamba在计算效率、模型性能和数据利用方面优于现有方法。

Abstract: Singing melody extraction (SME) is a key task in the field of music
information retrieval. However, existing methods are facing several
limitations: firstly, prior models use transformers to capture the contextual
dependencies, which requires quadratic computation resulting in low efficiency
in the inference stage. Secondly, prior works typically rely on
frequencysupervised methods to estimate the fundamental frequency (f0), which
ignores that the musical performance is actually based on notes. Thirdly,
transformers typically require large amounts of labeled data to achieve optimal
performances, but the SME task lacks of sufficient annotated data. To address
these issues, in this paper, we propose a mamba-based network, called
SpectMamba, for semi-supervised singing melody extraction using confidence
binary regularization. In particular, we begin by introducing vision mamba to
achieve computational linear complexity. Then, we propose a novel note-f0
decoder that allows the model to better mimic the musical performance. Further,
to alleviate the scarcity of the labeled data, we introduce a confidence binary
regularization (CBR) module to leverage the unlabeled data by maximizing the
probability of the correct classes. The proposed method is evaluated on several
public datasets and the conducted experiments demonstrate the effectiveness of
our proposed method.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [247] [Getting Ready for the EU AI Act in Healthcare. A call for Sustainable AI Development and Deployment](https://arxiv.org/abs/2505.07875)
*John Brandt Brodersen,Ilaria Amelia Caggiano,Pedro Kringen,Vince Istvan Madai,Walter Osika,Giovanni Sartor,Ellen Svensson,Magnus Westerlund,Roberto V. Zicari*

Main category: cs.CY

TL;DR: 论文强调在医疗领域AI开发中，需主动遵守《AI法案》并践行可信AI伦理原则，以确保系统合规性和可持续性。


<details>
  <summary>Details</summary>
Motivation: 随着《AI法案》的实施，医疗领域AI系统的可信度和合规性成为紧迫问题，需结合技术、证据和伦理实践。

Method: 提出开发者需主动采取措施，确保现有和未来AI系统符合《AI法案》要求，并以伦理原则为背景解释和应用法案条款。

Result: 通过伦理原则指导合规实践，可提升AI系统的有效性和可持续性，同时保护公共利益。

Conclusion: 医疗AI开发者应提前行动，结合伦理与法律要求，确保系统长期合规和可信。

Abstract: Assessments of trustworthiness have become a cornerstone of responsible AI
development. Especially in high-stakes fields like healthcare, aligning
technical, evidence-based, and ethical practices with forthcoming legal
requirements is increasingly urgent. We argue that developers and deployers of
AI systems for the medical domain should be proactive and take steps to
progressively ensure that such systems, both those currently in use and those
being developed or planned, respect the requirements of the AI Act, which has
come into force in August 2024. This is necessary if full and effective
compliance is to be ensured when the most relevant provisions of the Act become
effective (August 2026). The engagement with the AI Act cannot be viewed as a
formalistic exercise. Compliance with the AI Act needs to be carried out
through the proactive commitment to the ethical principles of trustworthy AI.
These principles provide the background for the Act, which mentions them
several times and connects them to the protection of public interest. They can
be used to interpret and apply the Act's provisions and to identify good
practices, increasing the validity and sustainability of AI systems over time.

</details>


### [248] [Multimodal Assessment of Classroom Discourse Quality: A Text-Centered Attention-Based Multi-Task Learning Approach](https://arxiv.org/abs/2505.07902)
*Ruikun Hou,Babette Bühler,Tim Fütterer,Efe Bozkir,Peter Gerjets,Ulrich Trautwein,Enkelejda Kasneci*

Main category: cs.CY

TL;DR: 该研究提出了一种基于多模态融合的架构，用于评估课堂话语质量，结合文本、音频和视频数据，并通过多任务学习和序数分类方法提升预测效果。


<details>
  <summary>Details</summary>
Motivation: 传统课堂话语评估依赖人工编码，耗时且成本高。现有AI研究多关注单一句子层面，缺乏对整个课程段话语质量的评估。

Method: 采用注意力机制捕捉多模态交互，多任务学习预测三个话语组件的质量分数，并将任务建模为序数分类问题。

Result: 在GTI德国数据集上验证，文本模态起主导作用，结合音频特征后模型一致性提升，总体Quadratic Weighted Kappa得分为0.384。

Conclusion: 研究为自动化话语质量评估奠定了基础，支持通过多维反馈促进教师专业发展。

Abstract: Classroom discourse is an essential vehicle through which teaching and
learning take place. Assessing different characteristics of discursive
practices and linking them to student learning achievement enhances the
understanding of teaching quality. Traditional assessments rely on manual
coding of classroom observation protocols, which is time-consuming and costly.
Despite many studies utilizing AI techniques to analyze classroom discourse at
the utterance level, investigations into the evaluation of discursive practices
throughout an entire lesson segment remain limited. To address this gap, our
study proposes a novel text-centered multimodal fusion architecture to assess
the quality of three discourse components grounded in the Global Teaching
InSights (GTI) observation protocol: Nature of Discourse, Questioning, and
Explanations. First, we employ attention mechanisms to capture inter- and
intra-modal interactions from transcript, audio, and video streams. Second, a
multi-task learning approach is adopted to jointly predict the quality scores
of the three components. Third, we formulate the task as an ordinal
classification problem to account for rating level order. The effectiveness of
these designed elements is demonstrated through an ablation study on the GTI
Germany dataset containing 92 videotaped math lessons. Our results highlight
the dominant role of text modality in approaching this task. Integrating
acoustic features enhances the model's consistency with human ratings,
achieving an overall Quadratic Weighted Kappa score of 0.384, comparable to
human inter-rater reliability (0.326). Our study lays the groundwork for the
future development of automated discourse quality assessment to support teacher
professional development through timely feedback on multidimensional discourse
practices.

</details>


### [249] [LECTOR: Summarizing E-book Reading Content for Personalized Student Support](https://arxiv.org/abs/2505.07898)
*Erwin Daniel López Zapata,Cheng Tang,Valdemar Švábenský,Fumiya Okubo,Atsushi Shimada*

Main category: cs.CY

TL;DR: LECTOR模型通过整合阅读内容数据与阅读活动数据，提升了教育电子书平台的信息分析能力，并在预测低分学生和设计个性化干预方面显示出潜力。


<details>
  <summary>Details</summary>
Motivation: 当前教育电子书平台主要依赖阅读活动数据，而忽略了阅读内容数据的价值。本研究旨在填补这一空白，提出LECTOR模型以更好地利用阅读内容数据。

Method: 提出LECTOR模型，从讲座幻灯片中提取关键信息，并与阅读活动数据结合。通过实验比较LECTOR与现有NLP模型的性能，并验证其在预测低分学生中的效果。

Result: LECTOR在提取关键信息时F1分数平均提升5%，在人类评估中提升21%。结合阅读偏好数据后，预测低分学生的性能有所提升。

Conclusion: LECTOR模型能有效整合阅读内容与活动数据，为教育工具的设计和个性化干预提供了新思路。

Abstract: Educational e-book platforms provide valuable information to teachers and
researchers through two main sources: reading activity data and reading content
data. While reading activity data is commonly used to analyze learning
strategies and predict low-performing students, reading content data is often
overlooked in these analyses. To address this gap, this study proposes LECTOR
(Lecture slides and Topic Relationships), a model that summarizes information
from reading content in a format that can be easily integrated with reading
activity data. Our first experiment compared LECTOR to representative Natural
Language Processing (NLP) models in extracting key information from 2,255
lecture slides, showing an average improvement of 5% in F1-score. These results
were further validated through a human evaluation involving 28 students, which
showed an average improvement of 21% in F1-score over a model predominantly
used in current educational tools. Our second experiment compared reading
preferences extracted by LECTOR with traditional reading activity data in
predicting low-performing students using 600,712 logs from 218 students. The
results showed a tendency to improve the predictive performance by integrating
LECTOR. Finally, we proposed examples showing the potential application of the
reading preferences extracted by LECTOR in designing personalized interventions
for students.

</details>


### [250] [One Bad NOFO? AI Governance in Federal Grantmaking](https://arxiv.org/abs/2505.08133)
*Dan Bateyko,Karen Levy*

Main category: cs.CY

TL;DR: 本文探讨了美国联邦机构通过拨款政策对人工智能（AI）治理的隐性作用，分析了40,000多项拨款通知，发现机构在AI推广中缺乏透明度和问责机制。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于揭示联邦机构通过拨款政策对AI治理的潜在影响，填补现有研究中对这一隐性杠杆的忽视。

Method: 通过分析2009年至2024年间Grants.gov上的40,000多项非国防联邦拨款通知，筛选提及AI的记录，并研究其目标和要求。

Result: 研究发现，尽管机构在拨款通知中推广AI，但极少设定AI特定的评判标准或限制，尤其是在涉及人权的情境中。

Conclusion: 结论指出拨款通知是AI政策制定的新领域，但其发展与其他监管措施不同步，缺乏对透明度、问责和隐私保护的充分考量，需进一步研究。

Abstract: Much scholarship considers how U.S. federal agencies govern artificial
intelligence (AI) through rulemaking and their own internal use policies. But
agencies have an overlooked AI governance role: setting discretionary grant
policy when directing billions of dollars in federal financial assistance.
These dollars enable state and local entities to study, create, and use AI.
This funding not only goes to dedicated AI programs, but also to grantees using
AI in the course of meeting their routine grant objectives. As discretionary
grantmakers, agencies guide and restrict what grant winners do -- a hidden
lever for AI governance. Agencies pull this lever by setting program
objectives, judging criteria, and restrictions for AI use. Using a novel
dataset of over 40,000 non-defense federal grant notices of funding opportunity
(NOFOs) posted to Grants.gov between 2009 and 2024, we analyze how agencies
regulate the use of AI by grantees. We select records mentioning AI and review
their stated goals and requirements. We find agencies promoting AI in notice
narratives, shaping adoption in ways other records of grant policy might fail
to capture. Of the grant opportunities that mention AI, we find only a handful
of AI-specific judging criteria or restrictions. This silence holds even when
agencies fund AI uses in contexts affecting people's rights and which, under an
analogous federal procurement regime, would result in extra oversight. These
findings recast grant notices as a site of AI policymaking -- albeit one that
is developing out of step with other regulatory efforts and incomplete in its
consideration of transparency, accountability, and privacy protections. The
paper concludes by drawing lessons from AI procurement scholarship, while
identifying distinct challenges in grantmaking that invite further study.

</details>


### [251] [Reciprocity as the Foundational Substrate of Society: How Reciprocal Dynamics Scale into Social Systems](https://arxiv.org/abs/2505.08319)
*Egil Diau*

Main category: cs.CY

TL;DR: 提出一个三阶段自下而上的框架，用于模拟社会结构的涌现，从个体互惠行为到规范稳定化，再到制度构建。


<details>
  <summary>Details</summary>
Motivation: 多智能体AI中缺乏模拟社会结构涌现的模型，且现有理论常将社会结构视为原始概念而非从个体行为重建。

Method: 三阶段框架：互惠动态（个体交换）、规范稳定化（共享期望）、制度构建（稳定模式外化）。

Result: 框架能够系统地探索道德、文化和制度结构如何从最小认知互动中涌现。

Conclusion: 通过基于互惠的个体行为，框架填补了社会结构涌现的建模空白。

Abstract: A major bottleneck in multi-agent AI is the lack of simulateable models for
the bottom-up emergence of social structure under realistic behavioral
constraints. Similarly, many foundational theories in economics and sociology
including the concepts of "institutions" and "norms" tend to describe social
structures post hoc, often relying on implicit assumptions of shared culture,
morality, or symbolic agreement. These concepts are often treated as primitives
rather than reconstructed from agent-level behavior, leaving both their origins
and operational definitions under-specified. To address this, we propose a
three-stage bottom-up framework: Reciprocal Dynamics, capturing
individual-level reciprocal exchanges; Norm Stabilization, the consolidation of
shared expectations; and Institutional Construction, the externalization of
stable patterns into scalable structures. By grounding social emergence in
agent-level reciprocity, our framework enables the systematic exploration of
how moral, cultural, and institutional structures emerge from cognitively
minimal interactions.

</details>


<div id='astro-ph.GA'></div>

# astro-ph.GA [[Back]](#toc)

### [252] [Understanding molecular ratios in the carbon and oxygen poor outer Milky Way with interpretable machine learning](https://arxiv.org/abs/2505.08410)
*Gijs Vermariën,Serena Viti,Johannes Heyl,Francesco Fontani*

Main category: astro-ph.GA

TL;DR: 论文研究了银河系外缘低金属丰度区域的分子线比率，通过可解释机器学习方法揭示了温度、密度及碳氧丰度对这些比率的影响。


<details>
  <summary>Details</summary>
Motivation: 银河系外缘的金属丰度低于太阳附近区域，但仍能检测到许多分子。研究这些区域的分子线比率有助于理解其化学和物理特性。

Method: 使用UCLCHEM生成的大规模天体化学模型网格，结合经典分析和可解释机器学习（SHAP和UMAP）研究分子线比率的高阶依赖关系。

Result: 研究发现温度和密度是最重要的特征，但碳氧丰度在部分参数空间中也起关键作用。UMAP可用于对模型进行直观分组。

Conclusion: 所选分子线比率对初始碳丰度、温度和密度变化敏感，其中CN/HCN和HNC/HCN比率尤其适合作为初始碳丰度的探针，而CS/SO比率对氧丰度敏感。

Abstract: Context. The outer Milky Way has a lower metallicity than our solar
neighbourhood, but still many molecules are detected in the region. Molecular
line ratios can serve as probes to better understand the chemistry and physics
in these regions. Aims. We use interpretable machine learning to study 9
different molecular ratios, helping us understand the forward connection
between the physics of these environments and the carbon and oxygen
chemistries. Methods. Using a large grid of astrochemical models generated
using UCLCHEM, we study the properties of molecular clouds of low oxygen and
carbon initial abundance. We first try to understand the line ratios using a
classical analysis. We then move on to using interpretable machine learning,
namely Shapley Additive Explanations (SHAP), to understand the higher order
dependencies of the ratios over the entire parameter grid. Lastly we use the
Uniform Manifold Approximation and Projection technique (UMAP) as a reduction
method to create intuitive groupings of models. Results. We find that the
parameter space is well covered by the line ratios, allowing us to investigate
all input parameters. SHAP analysis shows that the temperature and density are
the most important features, but the carbon and oxygen abundances are important
in parts of the parameter space. Lastly, we find that we can group different
types of ratios using UMAP. Conclusions. We show the chosen ratios are mostly
sensitive to changes in the carbon initial abundance, together with the
temperature and density. Especially the CN/HCN and HNC/HCN ratio are shown to
be sensitive to the initial carbon abundance, making them excellent probes for
this parameter. Out of the ratios, only CS/SO shows a sensitivity to the oxygen
abundance.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [253] [Explainable Artificial Intelligence Techniques for Software Development Lifecycle: A Phase-specific Survey](https://arxiv.org/abs/2505.07058)
*Lakshit Arora,Sanjay Surendranath Girija,Shashank Kapoor,Aman Raj,Dipen Pradhan,Ankit Shetgaonkar*

Main category: cs.SE

TL;DR: 本文综述了可解释人工智能（XAI）在软件开发生命周期（SDLC）各阶段的应用，填补了XAI在软件工程中应用的空白。


<details>
  <summary>Details</summary>
Motivation: 解决AI黑箱问题，提升AI系统的透明度和可解释性，以增强信任和广泛应用。

Method: 通过文献综述，分析了XAI方法（如LIME、SHAP、规则提取等）在SDLC各阶段（需求、设计、测试、部署等）的应用。

Result: 发现68%的XAI研究集中在软件维护阶段，而管理和需求阶段仅占8%。本文首次全面综述了XAI在SDLC各阶段的应用。

Conclusion: 本文为XAI在软件工程中的推广提供了基础，促进了复杂AI模型在AI驱动软件开发中的实际应用。

Abstract: Artificial Intelligence (AI) is rapidly expanding and integrating more into
daily life to automate tasks, guide decision making, and enhance efficiency.
However, complex AI models, which make decisions without providing clear
explanations (known as the "black-box problem"), currently restrict trust and
widespread adoption of AI. Explainable Artificial Intelligence (XAI) has
emerged to address the black-box problem of making AI systems more
interpretable and transparent so stakeholders can trust, verify, and act upon
AI-based outcomes. Researchers have developed various techniques to foster XAI
in the Software Development Lifecycle. However, there are gaps in applying XAI
techniques in the Software Engineering phases. Literature review shows that 68%
of XAI in Software Engineering research is focused on maintenance as opposed to
8% on software management and requirements. In this paper, we present a
comprehensive survey of the applications of XAI methods such as concept-based
explanations, Local Interpretable Model-agnostic Explanations (LIME), SHapley
Additive exPlanations (SHAP), rule extraction, attention mechanisms,
counterfactual explanations, and example-based explanations to the different
phases of the Software Development Life Cycle (SDLC), including requirements
elicitation, design and development, testing and deployment, and evolution. To
the best of our knowledge, this paper presents the first comprehensive survey
of XAI techniques for every phase of the Software Development Life Cycle
(SDLC). This survey aims to promote explainable AI in Software Engineering and
facilitate the practical application of complex AI models in AI-driven software
development.

</details>


### [254] [Moving From Monolithic To Microservices Architecture for Multi-Agent Systems](https://arxiv.org/abs/2505.07838)
*Muskaan Goyal,Pranav Bhasin*

Main category: cs.SE

TL;DR: 本文探讨了从单体架构到微服务架构在多智能体系统（MAS）中的演变，分析了传统单体MAS的局限性及微服务架构的优势，并研究了相关通信协议和新兴架构模式。


<details>
  <summary>Details</summary>
Motivation: 随着微服务架构在软件开发中的成功应用，研究其在复杂多智能体系统中的适用性具有重要意义。

Method: 通过文献综述和比较分析，探讨了微服务架构在MAS中的应用，包括通信协议（如ACL、MCP、A2A）和设计挑战。

Result: 研究发现微服务架构能显著提升MAS的可扩展性和可维护性，但也面临设计复杂性和通信开销等挑战。

Conclusion: 微服务架构为MAS带来了新的可能性，但需进一步研究以解决其在实际应用中的挑战。

Abstract: The transition from monolithic to microservices architecture revolutionized
software development by improving scalability and maintainability. This
paradigm shift is now becoming relevant for complex multi-agent systems (MAS).
This review article explores the evolution from monolithic architecture to
microservices architecture in the specific context of MAS. It will highlight
the limitations of traditional monolithic MAS and the benefits of adopting a
microservices-based approach. The article further examines the core
architectural principles and communication protocols, including Agent
Communication Languages (ACLs), the Model Context Protocol (MCP), and the
Application-to-Application (A2A) protocol. The article identifies emerging
architectural patterns, design challenges, and considerations through a
comparative lens of the paradigm shift.

</details>


### [255] [SweRank: Software Issue Localization with Code Ranking](https://arxiv.org/abs/2505.07849)
*Revanth Gangi Reddy,Tarun Suresh,JaeHyeok Doo,Ye Liu,Xuan Phi Nguyen,Yingbo Zhou,Semih Yavuz,Caiming Xiong,Heng Ji,Shafiq Joty*

Main category: cs.SE

TL;DR: SweRank是一个高效的检索-重排框架，用于软件问题定位，解决了传统代码排名模型和基于LLM的代理方法的不足。


<details>
  <summary>Details</summary>
Motivation: 软件问题定位是开发中的关键但耗时的任务，现有方法（如基于LLM的代理或传统代码排名模型）存在延迟高、成本大或效果不佳的问题。

Method: 提出了SweRank框架，并构建了SweLoc数据集（来自GitHub的真实问题描述和代码修改对），用于训练和评估。

Result: 在SWE-Bench-Lite和LocBench上，SweRank表现优于现有排名模型和基于闭源LLM的代理系统。

Conclusion: SweRank和SweLoc为问题定位提供了高效且有效的解决方案，同时数据集对社区有重要价值。

Abstract: Software issue localization, the task of identifying the precise code
locations (files, classes, or functions) relevant to a natural language issue
description (e.g., bug report, feature request), is a critical yet
time-consuming aspect of software development. While recent LLM-based agentic
approaches demonstrate promise, they often incur significant latency and cost
due to complex multi-step reasoning and relying on closed-source LLMs.
Alternatively, traditional code ranking models, typically optimized for
query-to-code or code-to-code retrieval, struggle with the verbose and
failure-descriptive nature of issue localization queries. To bridge this gap,
we introduce SweRank, an efficient and effective retrieve-and-rerank framework
for software issue localization. To facilitate training, we construct SweLoc, a
large-scale dataset curated from public GitHub repositories, featuring
real-world issue descriptions paired with corresponding code modifications.
Empirical results on SWE-Bench-Lite and LocBench show that SweRank achieves
state-of-the-art performance, outperforming both prior ranking models and
costly agent-based systems using closed-source LLMs like Claude-3.5. Further,
we demonstrate SweLoc's utility in enhancing various existing retriever and
reranker models for issue localization, establishing the dataset as a valuable
resource for the community.

</details>


### [256] [Leveraging AI for Productive and Trustworthy HPC Software: Challenges and Research Directions](https://arxiv.org/abs/2505.08135)
*Keita Teranishi,Harshitha Menon,William F. Godoy,Prasanna Balaprakash,David Bau,Tal Ben-Nun,Abhinav Bathele,Franz Franchetti,Michael Franusich,Todd Gamblin,Giorgis Georgakoudis,Tom Goldstein,Arjun Guha,Steven Hahn,Costin Iancu,Zheming Jin,Terry Jones,Tze Meng Low,Het Mankad,Narasinga Rao Miniskar,Mohammad Alaul Haque Monil,Daniel Nichols,Konstantinos Parasyris,Swaroop Pophale,Pedro Valero-Lara,Jeffrey S. Vetter,Samuel Williams,Aaron Young*

Main category: cs.SE

TL;DR: 探讨利用AI革新高性能计算（HPC）软件开发的挑战与研究方向，重点关注大型语言模型的应用。


<details>
  <summary>Details</summary>
Motivation: HPC软件作为高度专业化的科学领域，其开发面临独特挑战，AI技术尤其是大型语言模型可能为其带来变革。

Method: 分析现有AI技术在HPC软件开发中的适用性，并提出两个美国能源部资助项目（Ellora和Durban）的研究方向。

Result: 明确了AI在HPC软件开发中的潜力与挑战，并提出了具体的研究路径。

Conclusion: AI有望推动HPC软件开发的革新，但需解决其在该领域的独特挑战。

Abstract: We discuss the challenges and propose research directions for using AI to
revolutionize the development of high-performance computing (HPC) software. AI
technologies, in particular large language models, have transformed every
aspect of software development. For its part, HPC software is recognized as a
highly specialized scientific field of its own. We discuss the challenges
associated with leveraging state-of-the-art AI technologies to develop such a
unique and niche class of software and outline our research directions in the
two US Department of Energy--funded projects for advancing HPC Software via AI:
Ellora and Durban.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [257] [Sub-diffraction terahertz backpropagation compressive imaging](https://arxiv.org/abs/2505.07839)
*Yongsheng Zhu,Shaojing Liu,Ximiao Wang,Runli Li,Haili Yang,Jiali Wang,Hongjia Zhu,Yanlin Ke,Ningsheng Xu,Huanjun Chen,Shaozhi Deng*

Main category: eess.IV

TL;DR: 提出了一种亚衍射太赫兹反向传播压缩成像技术，通过光激发载流子调制太赫兹波，结合无训练神经网络和角谱传播理论，实现高分辨率成像。


<details>
  <summary>Details</summary>
Motivation: 太赫兹单像素成像（TSPI）因其简单和成本效益高而受到关注，但其分辨率受限于波长，且现有技术需要苛刻的实验条件和耗时的过程。

Method: 使用单色连续太赫兹波照射物体，通过光激发载流子在硅片上生成调制图案，用单点探测器记录调制后的波，并利用无训练神经网络在物理模型约束下重建图像。

Result: 实现了亚衍射成像，空间分辨率约为λ0/7（λ0=833.3μm），且无需超薄光调制器。

Conclusion: 该方法为太赫兹显微成像和其他逆成像问题提供了高效解决方案。

Abstract: Terahertz single-pixel imaging (TSPI) has garnered significant attention due
to its simplicity and cost-effectiveness. However, the relatively long
wavelength of THz waves limits sub-diffraction-scale imaging resolution.
Although TSPI technique can achieve sub-wavelength resolution, it requires
harsh experimental conditions and time-consuming processes. Here, we propose a
sub-diffraction THz backpropagation compressive imaging technique. We
illuminate the object with monochromatic continuous-wave THz radiation. The
transmitted THz wave is modulated by prearranged patterns generated on the back
surface of a 500-{\mu}m-thick silicon wafer, realized through photoexcited
carriers using a 532-nm laser. The modulated THz wave is then recorded by a
single-element detector. An untrained neural network is employed to iteratively
reconstruct the object image with an ultralow compression ratio of 1.5625%
under a physical model constraint, thus reducing the long sampling times. To
further suppress the diffraction-field effects, embedded with the angular
spectrum propagation (ASP) theory to model the diffraction of THz waves during
propagation, the network retrieves near-field information from the object,
enabling sub-diffraction imaging with a spatial resolution of ~{\lambda}0/7
({\lambda}0 = 833.3 {\mu}m at 0.36 THz) and eliminating the need for ultrathin
photomodulators. This approach provides an efficient solution for advancing THz
microscopic imaging and addressing other inverse imaging challenges.

</details>


### [258] [Pose Estimation for Intra-cardiac Echocardiography Catheter via AI-Based Anatomical Understanding](https://arxiv.org/abs/2505.07851)
*Jaeyoung Huh,Ankur Kapoor,Young-Ho Kim*

Main category: eess.IV

TL;DR: 提出了一种基于Vision Transformer的解剖感知姿态估计系统，用于从ICE图像中确定导管位置和方向，无需外部跟踪传感器。


<details>
  <summary>Details</summary>
Motivation: 现有导航方法依赖电磁跟踪或手动调整，易受干扰且依赖操作者经验，需改进。

Method: 使用ViT深度学习模型，通过16x16图像块处理，独立预测位置和方向，训练于851例临床数据。

Result: 平均位置误差9.48 mm，方向误差（16.13°, 8.98°, 10.47°），验证了模型准确性。

Conclusion: 该系统提升了手术效率，减少操作负担，适用于无跟踪的实时导管定位。

Abstract: Intra-cardiac Echocardiography (ICE) plays a crucial role in
Electrophysiology (EP) and Structural Heart Disease (SHD) interventions by
providing high-resolution, real-time imaging of cardiac structures. However,
existing navigation methods rely on electromagnetic (EM) tracking, which is
susceptible to interference and position drift, or require manual adjustments
based on operator expertise. To overcome these limitations, we propose a novel
anatomy-aware pose estimation system that determines the ICE catheter position
and orientation solely from ICE images, eliminating the need for external
tracking sensors. Our approach leverages a Vision Transformer (ViT)-based deep
learning model, which captures spatial relationships between ICE images and
anatomical structures. The model is trained on a clinically acquired dataset of
851 subjects, including ICE images paired with position and orientation labels
normalized to the left atrium (LA) mesh. ICE images are patchified into 16x16
embeddings and processed through a transformer network, where a [CLS] token
independently predicts position and orientation via separate linear layers. The
model is optimized using a Mean Squared Error (MSE) loss function, balancing
positional and orientational accuracy. Experimental results demonstrate an
average positional error of 9.48 mm and orientation errors of (16.13 deg, 8.98
deg, 10.47 deg) across x, y, and z axes, confirming the model accuracy.
Qualitative assessments further validate alignment between predicted and target
views within 3D cardiac meshes. This AI-driven system enhances procedural
efficiency, reduces operator workload, and enables real-time ICE catheter
localization for tracking-free procedures. The proposed method can function
independently or complement existing mapping systems like CARTO, offering a
transformative approach to ICE-guided interventions.

</details>


### [259] [Computationally Efficient Diffusion Models in Medical Imaging: A Comprehensive Review](https://arxiv.org/abs/2505.07866)
*Abdullah,Tao Huang,Ickjai Lee,Euijoon Ahn*

Main category: eess.IV

TL;DR: 本文探讨了扩散模型在生成式人工智能中的高效性和推理时间问题，重点介绍了DDPM、LDM和WDM三种模型在自然和医学影像中的应用及其局限性。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在生成高质量合成图像方面表现出色，但其高计算成本成为主要挑战。研究旨在优化模型效率，特别是在医学影像领域。

Method: 研究分类并分析了三种扩散模型（DDPM、LDM、WDM），探讨了它们在自然和医学影像中的计算复杂性填补及其应用。

Result: 扩散模型在医学影像中展现出快速、可靠和高质量的图像生成能力，但仍存在计算成本高的问题。

Conclusion: 未来研究应关注扩散模型在医学影像中的进一步优化，以提升其效率和实用性。

Abstract: The diffusion model has recently emerged as a potent approach in computer
vision, demonstrating remarkable performances in the field of generative
artificial intelligence. Capable of producing high-quality synthetic images,
diffusion models have been successfully applied across a range of applications.
However, a significant challenge remains with the high computational cost
associated with training and generating these models. This study focuses on the
efficiency and inference time of diffusion-based generative models,
highlighting their applications in both natural and medical imaging. We present
the most recent advances in diffusion models by categorizing them into three
key models: the Denoising Diffusion Probabilistic Model (DDPM), the Latent
Diffusion Model (LDM), and the Wavelet Diffusion Model (WDM). These models play
a crucial role in medical imaging, where producing fast, reliable, and
high-quality medical images is essential for accurate analysis of abnormalities
and disease diagnosis. We first investigate the general framework of DDPM, LDM,
and WDM and discuss the computational complexity gap filled by these models in
natural and medical imaging. We then discuss the current limitations of these
models as well as the opportunities and future research directions in medical
imaging.

</details>


### [260] [A portable diagnosis model for Keratoconus using a smartphone](https://arxiv.org/abs/2505.08616)
*Yifan Li,Myeongjun Kim,Yanjing Jin,Peter Ho,Jo Woon Chong*

Main category: eess.IV

TL;DR: 提出了一种基于智能手机的便携式圆锥角膜诊断框架，通过两阶段检测流程实现高精度分类和可视化。


<details>
  <summary>Details</summary>
Motivation: 圆锥角膜（KC）的诊断依赖专业设备，限制了可及性，因此需要一种便携式解决方案。

Method: 使用智能手机屏幕显示Placido盘，捕获角膜反射，并通过两阶段检测流程（WSVM分类和彩色图可视化）进行分析。

Result: 在多种智能手机上实现超过90%的分类准确率，并通过统计验证显示特征区分能力显著。

Conclusion: 该框架为圆锥角膜诊断提供了一种便携且高效的工具，具有临床潜力。

Abstract: Keratoconus (KC) is a progressive corneal disorder characterized by localized
thinning and protrusion, leading to visual distortion. While Placido disc-based
topography remains a standard in clinical diagnostics, its dependence on
specialized equipment limits accessibility. In this paper, we propose a
portable, smartphone-based diagnostic framework that captures corneal
reflections of a Placido disc displayed on a phone screen and applies a
two-stage detection pipeline, then validate on 3D-printed emulated eyeball
models that simulate normal, moderate, and severe KC stages based on anterior
chamber depth (ACD). The first step of the two-stage detection pipeline is
classifying different stages of KC with features including height and width of
extracted reflections using weighted support vector machine (WSVM). It achieves
a maximum accuracy of 92.93%, and maintains over 90% accuracy across multiple
smartphone models, including the Galaxy Z Flip 3, iPhone 15 Pro, and iPhone 16
Pro. For the second step, we visualize the KC-affected protrusion regions on
the corneas with color maps based on inter-disc distance, that provides an
intuitive representation of disease severity and localization. Moreover, we
validate the ability of the extracted features to differentiate between KC
stages with ANOVA and Omega Squared, with significant p-values (e.g., $p <
10^{-6}$) and large effect sizes ($\\omega^2$ up to 0.8398) among classes.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [261] [Beyond Basic A/B testing: Improving Statistical Efficiency for Business Growth](https://arxiv.org/abs/2505.08128)
*Changshuai Wei,Phuc Nguyen,Benjamin Zelditch,Joyce Chen*

Main category: stat.ME

TL;DR: 本文提出多种方法改进标准A/B测试的统计功效问题，包括回归调整、广义估计方程等，并提出一种新型双重稳健广义U方法，同时解决样本小、非高斯分布和ROI问题。


<details>
  <summary>Details</summary>
Motivation: 标准A/B测试方法在大规模工业应用中通常基于t检验，但在样本量小、非高斯分布或ROI考虑下统计功效较低。

Method: 提出回归调整、广义估计方程、Man-Whitney U和Zero-Trimmed U等方法，并设计一种新型双重稳健广义U方法。

Result: 理论分析证明了方法的渐近正态性和效率界限，并通过模拟研究和实际A/B测试验证了方法的有效性。

Conclusion: 新型方法能显著提升统计功效，适用于多种复杂业务场景。

Abstract: The standard A/B testing approaches are mostly based on t-test in large scale
industry applications. These standard approaches however suffers from low
statistical power in business settings, due to nature of small sample-size or
non-Gaussian distribution or return-on-investment (ROI) consideration. In this
paper, we propose several approaches to addresses these challenges: (i)
regression adjustment, generalized estimating equation, Man-Whitney U and
Zero-Trimmed U that addresses each of these issues separately, and (ii) a novel
doubly robust generalized U that handles ROI consideration, distribution
robustness and small samples in one framework. We provide theoretical results
on asymptotic normality and efficiency bounds, together with insights on the
efficiency gain from theoretical analysis. We further conduct comprehensive
simulation studies and apply the methods to multiple real A/B tests.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [262] [Tensor Sketch: Fast and Scalable Polynomial Kernel Approximation](https://arxiv.org/abs/2505.08146)
*Ninh Pham,Rasmus Pagh*

Main category: cs.DS

TL;DR: Tensor Sketch是一种高效的随机特征映射方法，用于近似多项式核，适用于高维和大规模数据集。


<details>
  <summary>Details</summary>
Motivation: 为了解决非线性核在大规模数据集上的扩展性问题，提出了一种高效的随机特征映射方法。

Method: 通过Tensor Sketch计算低维嵌入，时间复杂度为O(n(d+D log D))，适用于高维和大规模数据。

Result: 提供了理论保证，确保核函数估计的准确性。

Conclusion: Tensor Sketch是一种高效且可扩展的工具，适用于多种应用场景。

Abstract: Approximation of non-linear kernels using random feature maps has become a
powerful technique for scaling kernel methods to large datasets. We propose
\textit{Tensor Sketch}, an efficient random feature map for approximating
polynomial kernels. Given $n$ training samples in $\R^d$ Tensor Sketch computes
low-dimensional embeddings in $\R^D$ in time $\BO{n(d+D \log{D})}$ making it
well-suited for high-dimensional and large-scale settings. We provide
theoretical guarantees on the approximation error, ensuring the fidelity of the
resulting kernel function estimates. We also discuss extensions and highlight
applications where Tensor Sketch serves as a central computational tool.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [263] [MINIMALIST: switched-capacitor circuits for efficient in-memory computation of gated recurrent units](https://arxiv.org/abs/2505.08599)
*Sebastian Billaudelle,Laura Kriener,Filippo Moro,Tristan Torchet,Melika Payvand*

Main category: cs.AR

TL;DR: 论文提出了一种基于最小门控循环单元（GRU）的硬件兼容架构，并设计了一种高效的混合信号硬件实现方案。


<details>
  <summary>Details</summary>
Motivation: 在内存受限的边缘计算环境中，传统RNN的效率不足，需要一种更高效的架构和硬件实现。

Method: 采用最小GRU架构，结合混合信号硬件设计，利用开关电容电路实现内存计算和门控状态更新。

Result: 通过时间序列数据验证了架构性能，并在混合信号仿真中重现了软件模型的结果。

Conclusion: 该设计为边缘计算提供了一种高效且可扩展的RNN硬件实现方案。

Abstract: Recurrent neural networks (RNNs) have been a long-standing candidate for
processing of temporal sequence data, especially in memory-constrained systems
that one may find in embedded edge computing environments. Recent advances in
training paradigms have now inspired new generations of efficient RNNs. We
introduce a streamlined and hardware-compatible architecture based on minimal
gated recurrent units (GRUs), and an accompanying efficient mixed-signal
hardware implementation of the model. The proposed design leverages
switched-capacitor circuits not only for in-memory computation (IMC), but also
for the gated state updates. The mixed-signal cores rely solely on commodity
circuits consisting of metal capacitors, transmission gates, and a clocked
comparator, thus greatly facilitating scaling and transfer to other technology
nodes.
  We benchmark the performance of our architecture on time series data,
introducing all constraints required for a direct mapping to the hardware
system. The direct compatibility is verified in mixed-signal simulations,
reproducing data recorded from the software-only network model.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [264] [Image-Guided Microstructure Optimization using Diffusion Models: Validated with Li-Mn-rich Cathode Precursors](https://arxiv.org/abs/2505.07906)
*Geunho Choi,Changhwan Lee,Jieun Kim,Insoo Ye,Keeyoung Jung,Inchul Park*

Main category: cond-mat.mtrl-sci

TL;DR: 论文提出了一种基于AI的闭环框架，用于锂离子电池正极前驱体的微结构设计和优化，结合图像生成、分析和优化算法，实现了微结构的可控性。


<details>
  <summary>Details</summary>
Motivation: 微结构对材料性能至关重要，但由于难以量化、预测和优化，很少被作为设计变量。本文旨在解决这一问题。

Method: 框架整合了扩散基图像生成模型、定量图像分析流程和粒子群优化算法，通过SEM图像提取形态特征并预测合成条件。

Result: 平台能准确预测特定合成条件下的微结构形态，并通过实验验证了预测与合成结构的高度一致性。

Conclusion: 该框架为数据驱动的材料设计提供了实用策略，支持正向预测和逆向设计，推动了自主微结构工程的发展。

Abstract: Microstructure often dictates materials performance, yet it is rarely treated
as an explicit design variable because microstructure is hard to quantify,
predict, and optimize. Here, we introduce an image centric, closed-loop
framework that makes microstructural morphology into a controllable objective
and demonstrate its use case with Li- and Mn-rich layered oxide cathode
precursors. This work presents an integrated, AI driven framework for the
predictive design and optimization of lithium-ion battery cathode precursor
synthesis. This framework integrates a diffusion-based image generation model,
a quantitative image analysis pipeline, and a particle swarm optimization (PSO)
algorithm. By extracting key morphological descriptors such as texture,
sphericity, and median particle size (D50) from SEM images, the platform
accurately predicts SEM like morphologies resulting from specific
coprecipitation conditions, including reaction time-, solution concentration-,
and pH-dependent structural changes. Optimization then pinpoints synthesis
parameters that yield user defined target morphologies, as experimentally
validated by the close agreement between predicted and synthesized structures.
This framework offers a practical strategy for data driven materials design,
enabling both forward prediction and inverse design of synthesis conditions and
paving the way toward autonomous, image guided microstructure engineering.

</details>


### [265] [Enhancing the Efficiency of Complex Systems Crystal Structure Prediction by Active Learning Guided Machine Learning Potential](https://arxiv.org/abs/2505.08159)
*Jiaxiang Li,Junwei Feng,Jie Luo,Bowen Jiang,Xiangyu Zheng,Jian Lv,Keith Butler,Hanyu Liu,Congwei Xie,Yu Xie,Yanming Ma*

Main category: cond-mat.mtrl-sci

TL;DR: 提出了一种灵活、自动化的机器学习势（MLP）工作流程，用于解决复杂材料系统中的组合爆炸问题，显著加速了高通量结构优化和新化合物的发现。


<details>
  <summary>Details</summary>
Motivation: 复杂材料系统的组合爆炸和化学计量空间巨大，导致传统晶体结构预测（CSP）方法在计算上不可行，需要一种更高效的方法。

Method: 开发了一种通用的机器学习势（MLP）工作流程，并在Mg-Ca-H三元和Be-P-N-O四元系统中验证其有效性。

Result: 该方法显著加速了高通量结构优化，并高效识别了有前景的化合物。

Conclusion: 该工作流程为探索复杂材料系统和加速多组分材料发现提供了有效解决方案。

Abstract: Understanding multicomponent complex material systems is essential for design
of advanced materials for a wide range of technological applications. While
state-of-the-art crystal structure prediction (CSP) methods effectively
identify new structures and assess phase stability, they face fundamental
limitations when applied to complex systems. This challenge stems from the
combinatorial explosion of atomic configurations and the vast stoichiometric
space, both of which contribute to computational demands that rapidly exceed
practical feasibility. In this work, we propose a flexible and automated
workflow to build a highly generalizable and data-efficient machine learning
potential (MLP), effectively unlocking the full potential of CSP algorithms.
The workflow is validated on both Mg-Ca-H ternary and Be-P-N-O quaternary
systems, demonstrating substantial machine learning acceleration in
high-throughput structural optimization and enabling the efficient
identification of promising compounds. These results underscore the
effectiveness of our approach in exploring complex material systems and
accelerating the discovery of new multicomponent materials.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [266] [SPP-SBL: Space-Power Prior Sparse Bayesian Learning for Block Sparse Recovery](https://arxiv.org/abs/2505.08518)
*Yanhao Zhang,Zhihan Zhu,Yong Xia*

Main category: math.OC

TL;DR: 本文提出了一种基于方差变换框架的块稀疏信号恢复方法SPP-SBL，通过结合EM算法和高阶方程求解，解决了空间耦合参数估计问题，显著提高了恢复精度。


<details>
  <summary>Details</summary>
Motivation: 块稀疏信号的结构模式未知是结构化稀疏信号恢复中的基本挑战，现有方法难以自适应捕捉未知模式。

Method: 提出了基于无向图模型的空间功率先验，结合EM算法和高阶方程求解，开发了SPP-SBL方法。

Result: SPP-SBL成功恢复了多种结构化稀疏信号和真实世界多模态信号，在多个指标上表现出显著优势。

Conclusion: 学习空间耦合参数的相对值是捕捉未知块稀疏模式和提高恢复精度的关键。

Abstract: The recovery of block-sparse signals with unknown structural patterns remains
a fundamental challenge in structured sparse signal reconstruction. By
proposing a variance transformation framework, this paper unifies existing
pattern-based block sparse Bayesian learning methods, and introduces a novel
space power prior based on undirected graph models to adaptively capture the
unknown patterns of block-sparse signals. By combining the EM algorithm with
high-order equation root-solving, we develop a new structured sparse Bayesian
learning method, SPP-SBL, which effectively addresses the open problem of space
coupling parameter estimation in pattern-based methods. We further demonstrate
that learning the relative values of space coupling parameters is key to
capturing unknown block-sparse patterns and improving recovery accuracy.
Experiments validate that SPP-SBL successfully recovers various challenging
structured sparse signals (e.g., chain-structured signals and multi-pattern
sparse signals) and real-world multi-modal structured sparse signals (images,
audio), showing significant advantages in recovery accuracy across multiple
metrics.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [267] [Safety and optimality in learning-based control at low computational cost](https://arxiv.org/abs/2505.08026)
*Dominik Baumann,Krzysztof Kowalczyk,Cristian R. Rojas,Koen Tiels,Pawel Wachel*

Main category: eess.SY

TL;DR: CoLSafe是一种计算轻量级的安全学习算法，适用于大规模数据和低功耗设备，其计算复杂度随数据点数量次线性增长。


<details>
  <summary>Details</summary>
Motivation: 将机器学习应用于现实物理系统需要安全保证，但现有方法计算成本高，难以适用于大规模数据和嵌入式设备。

Method: 提出CoLSafe算法，其计算复杂度次线性增长，并提供安全和最优性保证。

Result: 在七自由度机械臂上验证了算法的有效性。

Conclusion: CoLSafe是一种高效且安全的学习算法，适用于资源受限的场景。

Abstract: Applying machine learning methods to physical systems that are supposed to
act in the real world requires providing safety guarantees. However, methods
that include such guarantees often come at a high computational cost, making
them inapplicable to large datasets and embedded devices with low computational
power. In this paper, we propose CoLSafe, a computationally lightweight safe
learning algorithm whose computational complexity grows sublinearly with the
number of data points. We derive both safety and optimality guarantees and
showcase the effectiveness of our algorithm on a seven-degrees-of-freedom robot
arm.

</details>


### [268] [Diffusion-assisted Model Predictive Control Optimization for Power System Real-Time Operation](https://arxiv.org/abs/2505.08535)
*Linna Xu,Yongli Zhu*

Main category: eess.SY

TL;DR: 本文提出了一种改进的模型预测控制（MPC）框架，用于实时电力系统操作，通过引入扩散模型提升负荷预测精度，并在缺乏明确状态转移规律时利用模型识别程序推导系统动态。


<details>
  <summary>Details</summary>
Motivation: 解决可再生能源主导的电力系统中MPC应用时因缺乏明确状态转移规律而面临的障碍。

Method: 结合扩散模型的时间序列生成能力改进负荷预测模块，并通过模型识别程序推导系统动态。

Result: 在工业园区系统和IEEE 30总线系统中的案例研究表明，扩散模型显著提高了负荷预测精度，且推导的系统动态适用于含太阳能和风能的实时电网操作。

Conclusion: 改进的MPC框架有效提升了可再生能源电力系统的实时操作性能。

Abstract: This paper presents a modified model predictive control (MPC) framework for
real-time power system operation. The framework incorporates a diffusion model
tailored for time series generation to enhance the accuracy of the load
forecasting module used in the system operation. In the absence of explicit
state transition law, a model-identification procedure is leveraged to derive
the system dynamics, thereby eliminating a barrier when applying MPC to a
renewables-dominated power system. Case study results on an industry park
system and the IEEE 30-bus system demonstrate that using the diffusion model to
augment the training dataset significantly improves load-forecasting accuracy,
and the inferred system dynamics are applicable to the real-time grid operation
with solar and wind.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [269] [Non-contact Vital Signs Detection in Dynamic Environments](https://arxiv.org/abs/2505.08366)
*Shuai Sun,Chong-Xi Liang,Chengwei Ye,Huanzhen Zhang,Kangsheng Wang*

Main category: eess.SP

TL;DR: 提出了一种新的DC偏移校准方法和HADCM解调算法，用于毫米波雷达生命体征检测中的相位解调，提高了复杂环境下的性能。


<details>
  <summary>Details</summary>
Motivation: 复杂环境中时变DC偏移和相位不平衡会严重影响解调性能，需要一种更稳健的方法。

Method: 通过估计信号峰谷的时变DC偏移，结合差分形式和希尔伯特变换提取生命体征信息。

Result: 仿真和实验表明，该方法在低信噪比下仍保持稳健性能，优于现有技术。

Conclusion: 该方法在挑战性场景中能更准确地恢复信号并抑制噪声干扰。

Abstract: Accurate phase demodulation is critical for vital sign detection using
millimeter-wave radar. However, in complex environments, time-varying DC
offsets and phase imbalances can severely degrade demodulation performance. To
address this, we propose a novel DC offset calibration method alongside a
Hilbert and Differential Cross-Multiply (HADCM) demodulation algorithm. The
approach estimates time-varying DC offsets from neighboring signal peaks and
valleys, then employs both differential forms and Hilbert transforms of the I/Q
channel signals to extract vital sign information. Simulation and experimental
results demonstrate that the proposed method maintains robust performance under
low signal-to-noise ratios. Compared to existing demodulation techniques, it
offers more accurate signal recovery in challenging scenarios and effectively
suppresses noise interference.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [270] [PRISM: Complete Online Decentralized Multi-Agent Pathfinding with Rapid Information Sharing using Motion Constraints](https://arxiv.org/abs/2505.08025)
*Hannah Lee,Zachary Serlin,James Motes,Brendan Long,Marco Morales,Nancy M. Amato*

Main category: cs.RO

TL;DR: PRISM是一种去中心化算法，用于解决多任务多智能体路径规划问题，通过快速信息共享和运动约束避免碰撞，具有高扩展性和高效性。


<details>
  <summary>Details</summary>
Motivation: 解决多任务多智能体路径规划中的碰撞和死锁问题，同时提升去中心化算法的性能和可扩展性。

Method: 采用快速通信策略，通过信息包交换运动约束信息，增强协作路径规划和情境感知。

Result: 在实验中，PRISM支持比CBS多3.4倍的智能体，比TPTS多处理2.5倍的任务，且在低连接条件下保持高效。

Conclusion: PRISM在复杂动态环境中表现出鲁棒性、可扩展性和高效性，适用于大规模应用。

Abstract: We introduce PRISM (Pathfinding with Rapid Information Sharing using Motion
Constraints), a decentralized algorithm designed to address the multi-task
multi-agent pathfinding (MT-MAPF) problem. PRISM enables large teams of agents
to concurrently plan safe and efficient paths for multiple tasks while avoiding
collisions. It employs a rapid communication strategy that uses information
packets to exchange motion constraint information, enhancing cooperative
pathfinding and situational awareness, even in scenarios without direct
communication. We prove that PRISM resolves and avoids all deadlock scenarios
when possible, a critical challenge in decentralized pathfinding. Empirically,
we evaluate PRISM across five environments and 25 random scenarios,
benchmarking it against the centralized Conflict-Based Search (CBS) and the
decentralized Token Passing with Task Swaps (TPTS) algorithms. PRISM
demonstrates scalability and solution quality, supporting 3.4 times more agents
than CBS and handling up to 2.5 times more tasks in narrow passage environments
than TPTS. Additionally, PRISM matches CBS in solution quality while achieving
faster computation times, even under low-connectivity conditions. Its
decentralized design reduces the computational burden on individual agents,
making it scalable for large environments. These results confirm PRISM's
robustness, scalability, and effectiveness in complex and dynamic pathfinding
scenarios.

</details>


### [271] [What Matters for Batch Online Reinforcement Learning in Robotics?](https://arxiv.org/abs/2505.08078)
*Perry Dong,Suvir Mirchandani,Dorsa Sadigh,Chelsea Finn*

Main category: cs.RO

TL;DR: 论文研究了批量在线强化学习（batch online RL）在机器人学习中的有效性，通过系统实验分析了算法类别、策略提取方法和策略表达能力对性能的影响，并提出了一种改进方法。


<details>
  <summary>Details</summary>
Motivation: 批量在线强化学习有望通过减少人工数据收集需求并利用自我改进实现可扩展的机器人学习，但现有方法难以高效利用自主收集的数据。

Method: 通过实验分析了算法类别（如Q函数与模仿学习）、策略提取方法（隐式与传统方法）和策略表达能力对性能的影响，并提出了一种结合时间相关噪声的改进方法。

Result: 使用Q函数指导学习显著优于模仿方法，隐式策略提取和表达性强的策略类更有效，改进方法性能优于现有方法。

Conclusion: 论文提出了一种有效的批量在线强化学习方法，通过结合Q函数、隐式策略提取和表达性策略类，显著提升了性能和可扩展性。

Abstract: The ability to learn from large batches of autonomously collected data for
policy improvement -- a paradigm we refer to as batch online reinforcement
learning -- holds the promise of enabling truly scalable robot learning by
significantly reducing the need for human effort of data collection while
getting benefits from self-improvement. Yet, despite the promise of this
paradigm, it remains challenging to achieve due to algorithms not being able to
learn effectively from the autonomous data. For example, prior works have
applied imitation learning and filtered imitation learning methods to the batch
online RL problem, but these algorithms often fail to efficiently improve from
the autonomously collected data or converge quickly to a suboptimal point. This
raises the question of what matters for effective batch online RL in robotics.
Motivated by this question, we perform a systematic empirical study of three
axes -- (i) algorithm class, (ii) policy extraction methods, and (iii) policy
expressivity -- and analyze how these axes affect performance and scaling with
the amount of autonomous data. Through our analysis, we make several
observations. First, we observe that the use of Q-functions to guide batch
online RL significantly improves performance over imitation-based methods.
Building on this, we show that an implicit method of policy extraction -- via
choosing the best action in the distribution of the policy -- is necessary over
traditional policy extraction methods from offline RL. Next, we show that an
expressive policy class is preferred over less expressive policy classes. Based
on this analysis, we propose a general recipe for effective batch online RL. We
then show a simple addition to the recipe of using temporally-correlated noise
to obtain more diversity results in further performance gains. Our recipe
obtains significantly better performance and scaling compared to prior methods.

</details>


### [272] [Adaptive Diffusion Policy Optimization for Robotic Manipulation](https://arxiv.org/abs/2505.08376)
*Huiyun Jiang,Zhuang Yang*

Main category: cs.RO

TL;DR: 本文提出了一种基于Adam的扩散策略优化（ADPO）框架，用于快速稳定地优化扩散模型在强化学习中的策略，并在标准机器人控制任务中验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在强化学习中表现出潜力，但如何快速稳定地优化扩散策略的研究有限。

Method: 提出ADPO框架，采用自适应梯度下降方法优化扩散策略，并在标准机器人任务中进行实验验证。

Result: ADPO在标准任务中表现优于或与基准方法相当，并通过实验分析了超参数敏感性。

Conclusion: ADPO为扩散策略优化提供了高效框架，并为实际应用提供了指导。

Abstract: Recent studies have shown the great potential of diffusion models in
improving reinforcement learning (RL) by modeling complex policies, expressing
a high degree of multi-modality, and efficiently handling high-dimensional
continuous control tasks. However, there is currently limited research on how
to optimize diffusion-based polices (e.g., Diffusion Policy) fast and stably.
In this paper, we propose an Adam-based Diffusion Policy Optimization (ADPO), a
fast algorithmic framework containing best practices for fine-tuning
diffusion-based polices in robotic control tasks using the adaptive gradient
descent method in RL. Adaptive gradient method is less studied in training RL,
let alone diffusion-based policies. We confirm that ADPO outperforms other
diffusion-based RL methods in terms of overall effectiveness for fine-tuning on
standard robotic tasks. Concretely, we conduct extensive experiments on
standard robotic control tasks to test ADPO, where, particularly, six popular
diffusion-based RL methods are provided as benchmark methods. Experimental
results show that ADPO acquires better or comparable performance than the
baseline methods. Finally, we systematically analyze the sensitivity of
multiple hyperparameters in standard robotics tasks, providing guidance for
subsequent practical applications. Our video demonstrations are released in
https://github.com/Timeless-lab/ADPO.git.

</details>


### [273] [Scaling Multi Agent Reinforcement Learning for Underwater Acoustic Tracking via Autonomous Vehicles](https://arxiv.org/abs/2505.08222)
*Matteo Gallici,Ivan Masmitja,Mario Martín*

Main category: cs.RO

TL;DR: 提出了一种基于迭代蒸馏和Transformer架构的方法，显著提升了多智能体强化学习（MARL）的训练效率，实现了大规模自主车队控制。


<details>
  <summary>Details</summary>
Motivation: 解决多智能体强化学习在复杂海洋环境中训练效率低下的问题，尤其是针对多目标跟踪场景。

Method: 采用迭代蒸馏技术将高保真仿真转移到简化的GPU加速环境，并设计了Transformer架构（TransfMAPPO）以提升样本效率。

Result: 实现了30,000倍的训练速度提升，跟踪误差保持在5米以下，适用于快速移动目标。

Conclusion: 该方法为大规模MARL训练与高保真部署提供了可扩展框架，适用于实际海洋任务。

Abstract: Autonomous vehicles (AV) offer a cost-effective solution for scientific
missions such as underwater tracking. Recently, reinforcement learning (RL) has
emerged as a powerful method for controlling AVs in complex marine
environments. However, scaling these techniques to a fleet--essential for
multi-target tracking or targets with rapid, unpredictable motion--presents
significant computational challenges. Multi-Agent Reinforcement Learning (MARL)
is notoriously sample-inefficient, and while high-fidelity simulators like
Gazebo's LRAUV provide 100x faster-than-real-time single-robot simulations,
they offer no significant speedup for multi-vehicle scenarios, making MARL
training impractical. To address these limitations, we propose an iterative
distillation method that transfers high-fidelity simulations into a simplified,
GPU-accelerated environment while preserving high-level dynamics. This approach
achieves up to a 30,000x speedup over Gazebo through parallelization, enabling
efficient training via end-to-end GPU acceleration. Additionally, we introduce
a novel Transformer-based architecture (TransfMAPPO) that learns multi-agent
policies invariant to the number of agents and targets, significantly improving
sample efficiency. Following large-scale curriculum learning conducted entirely
on GPU, we perform extensive evaluations in Gazebo, demonstrating that our
method maintains tracking errors below 5 meters over extended durations, even
in the presence of multiple fast-moving targets. This work bridges the gap
between large-scale MARL training and high-fidelity deployment, providing a
scalable framework for autonomous fleet control in real-world sea missions.

</details>


### [274] [Reinforcement Learning-based Fault-Tolerant Control for Quadrotor with Online Transformer Adaptation](https://arxiv.org/abs/2505.08223)
*Dohyun Kim,Jayden Dongwoo Lee,Hyochoong Bang,Jungho Bae*

Main category: cs.RO

TL;DR: 提出了一种基于强化学习和Transformer的混合故障容错控制框架，用于多旋翼飞行器，无需重新训练即可适应新配置。


<details>
  <summary>Details</summary>
Motivation: 多旋翼飞行器在故障情况下容易失控，现有方法需要先验知识或难以适应新配置。

Method: 结合强化学习和Transformer架构，实时推断潜在表示，适应未见过的系统模型。

Result: 在PyBullet模拟中，成功率达95%，位置RMSE为0.129米，优于现有方法。

Conclusion: 该框架显著提升了多旋翼飞行器的适应性和可靠性，适用于动态和不确定环境。

Abstract: Multirotors play a significant role in diverse field robotics applications
but remain highly susceptible to actuator failures, leading to rapid
instability and compromised mission reliability. While various fault-tolerant
control (FTC) strategies using reinforcement learning (RL) have been widely
explored, most previous approaches require prior knowledge of the multirotor
model or struggle to adapt to new configurations. To address these limitations,
we propose a novel hybrid RL-based FTC framework integrated with a
transformer-based online adaptation module. Our framework leverages a
transformer architecture to infer latent representations in real time, enabling
adaptation to previously unseen system models without retraining. We evaluate
our method in a PyBullet simulation under loss-of-effectiveness actuator
faults, achieving a 95% success rate and a positional root mean square error
(RMSE) of 0.129 m, outperforming existing adaptation methods with 86% success
and an RMSE of 0.153 m. Further evaluations on quadrotors with varying
configurations confirm the robustness of our framework across untrained
dynamics. These results demonstrate the potential of our framework to enhance
the adaptability and reliability of multirotors, enabling efficient fault
management in dynamic and uncertain environments. Website is available at
http://00dhkim.me/paper/rl-ftc

</details>


### [275] [Continuous World Coverage Path Planning for Fixed-Wing UAVs using Deep Reinforcement Learning](https://arxiv.org/abs/2505.08382)
*Mirco Theile,Andres R. Zapata Rodriguez,Marco Caccamo,Alberto L. Sangiovanni-Vincentelli*

Main category: cs.RO

TL;DR: 论文提出了一种基于强化学习的无人机连续覆盖路径规划方法，通过优化能量消耗实现高效覆盖。


<details>
  <summary>Details</summary>
Motivation: 传统离散网格方法无法满足无人机在连续环境中的高效能量消耗需求，因此需要一种新的连续运动规划方法。

Method: 使用可变大小轴对齐矩形建模环境，结合曲率约束的Bézier曲线描述无人机运动，采用基于动作映射的自适应Soft Actor-Critic（AM-SAC）算法训练强化学习代理。

Result: 实验表明，该方法在程序生成和手工设计的场景中均能学习到能量高效的覆盖策略。

Conclusion: 该方法为无人机连续覆盖路径规划提供了一种有效的解决方案，显著提升了能量效率。

Abstract: Unmanned Aerial Vehicle (UAV) Coverage Path Planning (CPP) is critical for
applications such as precision agriculture and search and rescue. While
traditional methods rely on discrete grid-based representations, real-world UAV
operations require power-efficient continuous motion planning. We formulate the
UAV CPP problem in a continuous environment, minimizing power consumption while
ensuring complete coverage. Our approach models the environment with
variable-size axis-aligned rectangles and UAV motion with curvature-constrained
B\'ezier curves. We train a reinforcement learning agent using an
action-mapping-based Soft Actor-Critic (AM-SAC) algorithm employing a
self-adaptive curriculum. Experiments on both procedurally generated and
hand-crafted scenarios demonstrate the effectiveness of our method in learning
energy-efficient coverage strategies.

</details>


### [276] [Parameter Estimation using Reinforcement Learning Causal Curiosity: Limits and Challenges](https://arxiv.org/abs/2505.08453)
*Miguel Arana-Catania,Weisi Guo*

Main category: cs.RO

TL;DR: 本文分析了强化学习方法Causal Curiosity，旨在高效估计系统中因果因素的值，并首次对其测量准确性、敏感性和混杂因素分离能力进行了评估。


<details>
  <summary>Details</summary>
Motivation: 因果理解在科学与工程中至关重要，尤其是在优化复杂系统或建模未知环境时。Causal Curiosity方法提供了一种无需直接测量的途径，但其准确性是关键。

Method: 通过分析Causal Curiosity在机器人操纵器上的应用，评估其测量准确性、敏感性和混杂因素分离能力。

Result: 揭示了该方法的潜力和当前局限性，并提出了改进设计以提高其在现实复杂场景中的应用效率。

Conclusion: 本文为Causal Curiosity方法的优化提供了方向，强调了测量准确性对因果分析的重要性。

Abstract: Causal understanding is important in many disciplines of science and
engineering, where we seek to understand how different factors in the system
causally affect an experiment or situation and pave a pathway towards creating
effective or optimising existing models. Examples of use cases are autonomous
exploration and modelling of unknown environments or assessing key variables in
optimising large complex systems. In this paper, we analyse a Reinforcement
Learning approach called Causal Curiosity, which aims to estimate as accurately
and efficiently as possible, without directly measuring them, the value of
factors that causally determine the dynamics of a system. Whilst the idea
presents a pathway forward, measurement accuracy is the foundation of
methodology effectiveness. Focusing on the current causal curiosity's robotic
manipulator, we present for the first time a measurement accuracy analysis of
the future potentials and current limitations of this technique and an analysis
of its sensitivity and confounding factor disentanglement capability - crucial
for causal analysis. As a result of our work, we promote proposals for an
improved and efficient design of Causal Curiosity methods to be applied to
real-world complex scenarios.

</details>


### [277] [Automatic Curriculum Learning for Driving Scenarios: Towards Robust and Efficient Reinforcement Learning](https://arxiv.org/abs/2505.08264)
*Ahmed Abouelazm,Tim Weinstein,Tim Joseph,Philip Schörner,J. Marius Zöllner*

Main category: cs.RO

TL;DR: 本文提出了一种自动课程学习框架，用于动态生成适应复杂度的驾驶场景，以提升强化学习自动驾驶代理的训练效率和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习代理在固定场景和模拟环境中训练，泛化能力有限，而领域随机化方法效率低下且策略次优。

Method: 提出自动课程学习框架，通过‘教师’动态生成和调整驾驶场景，基于代理当前策略的学习潜力，无需专家设计。

Result: 实验表明，该方法在低/高交通密度下分别提升9%和21%的成功率，并加快收敛速度。

Conclusion: 自动课程学习框架显著提升了强化学习自动驾驶代理的鲁棒性和训练效率。

Abstract: This paper addresses the challenges of training end-to-end autonomous driving
agents using Reinforcement Learning (RL). RL agents are typically trained in a
fixed set of scenarios and nominal behavior of surrounding road users in
simulations, limiting their generalization and real-life deployment. While
domain randomization offers a potential solution by randomly sampling driving
scenarios, it frequently results in inefficient training and sub-optimal
policies due to the high variance among training scenarios. To address these
limitations, we propose an automatic curriculum learning framework that
dynamically generates driving scenarios with adaptive complexity based on the
agent's evolving capabilities. Unlike manually designed curricula that
introduce expert bias and lack scalability, our framework incorporates a
``teacher'' that automatically generates and mutates driving scenarios based on
their learning potential -- an agent-centric metric derived from the agent's
current policy -- eliminating the need for expert design. The framework
enhances training efficiency by excluding scenarios the agent has mastered or
finds too challenging. We evaluate our framework in a reinforcement learning
setting where the agent learns a driving policy from camera images. Comparative
results against baseline methods, including fixed scenario training and domain
randomization, demonstrate that our approach leads to enhanced generalization,
achieving higher success rates: +9\% in low traffic density, +21\% in high
traffic density, and faster convergence with fewer training steps. Our findings
highlight the potential of ACL in improving the robustness and efficiency of
RL-based autonomous driving agents.

</details>


### [278] [From Seeing to Doing: Bridging Reasoning and Decision for Robotic Manipulation](https://arxiv.org/abs/2505.08548)
*Yifu Yuan,Haiqin Cui,Yibin Chen,Zibin Dong,Fei Ni,Longxin Kou,Jinyi Liu,Pengyi Li,Yan Zheng,Jianye Hao*

Main category: cs.RO

TL;DR: FSD模型通过空间关系推理生成中间表示，显著提升了机器人操作的零样本性能，在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决当前Vision-Language-Action模型在未见场景和新任务中泛化能力不足的问题。

Method: 提出FSD模型，结合分层数据管道和自一致性机制，对齐空间坐标与视觉信号。

Result: 在8个基准测试和VABench中表现优异，零样本操作成功率显著提升（SimplerEnv 54.1%，真实任务72%）。

Conclusion: FSD通过空间推理和自一致性机制，显著提升了机器人操作的泛化能力和性能。

Abstract: Achieving generalization in robotic manipulation remains a critical
challenge, particularly for unseen scenarios and novel tasks. Current
Vision-Language-Action (VLA) models, while building on top of general
Vision-Language Models (VLMs), still fall short of achieving robust zero-shot
performance due to the scarcity and heterogeneity prevalent in embodied
datasets. To address these limitations, we propose FSD (From Seeing to Doing),
a novel vision-language model that generates intermediate representations
through spatial relationship reasoning, providing fine-grained guidance for
robotic manipulation. Our approach combines a hierarchical data pipeline for
training with a self-consistency mechanism that aligns spatial coordinates with
visual signals. Through extensive experiments, we comprehensively validated
FSD's capabilities in both "seeing" and "doing," achieving outstanding
performance across 8 benchmarks for general spatial reasoning and embodied
reference abilities, as well as on our proposed more challenging benchmark
VABench. We also verified zero-shot capabilities in robot manipulation,
demonstrating significant performance improvements over baseline methods in
both SimplerEnv and real robot settings. Experimental results show that FSD
achieves 54.1% success rate in SimplerEnv and 72% success rate across 8
real-world tasks, outperforming the strongest baseline by 30%.

</details>


### [279] [A Comparative Study of Human Activity Recognition: Motion, Tactile, and multi-modal Approaches](https://arxiv.org/abs/2505.08657)
*Valerio Belcamino,Nhat Minh Dinh Le,Quan Khanh Luu,Alessandro Carfì,Van Anh Ho,Fulvio Mastrogiovanni*

Main category: cs.RO

TL;DR: 该研究评估了基于视觉的触觉传感器在分类15种人类活动中的表现，并与基于IMU的数据手套进行比较，提出了一种结合触觉和运动数据的多模态框架。


<details>
  <summary>Details</summary>
Motivation: 人类活动识别（HAR）对于人机协作（HRC）至关重要，使机器人能够理解和响应人类动作。

Method: 研究了三种方法：基于运动的分类（MBC）、基于触觉的分类（TBC）以及结合两者的多模态分类（MMC），并通过离线和在线验证评估性能。

Result: 多模态方法在性能上始终优于单模态方法。

Conclusion: 结合触觉和运动传感可以显著提升人机协作中的HAR系统性能。

Abstract: Human activity recognition (HAR) is essential for effective Human-Robot
Collaboration (HRC), enabling robots to interpret and respond to human actions.
This study evaluates the ability of a vision-based tactile sensor to classify
15 activities, comparing its performance to an IMU-based data glove.
Additionally, we propose a multi-modal framework combining tactile and motion
data to leverage their complementary strengths. We examined three approaches:
motion-based classification (MBC) using IMU data, tactile-based classification
(TBC) with single or dual video streams, and multi-modal classification (MMC)
integrating both. Offline validation on segmented datasets assessed each
configuration's accuracy under controlled conditions, while online validation
on continuous action sequences tested online performance. Results showed the
multi-modal approach consistently outperformed single-modality methods,
highlighting the potential of integrating tactile and motion sensing to enhance
HAR systems for collaborative robotics.

</details>


### [280] [A Social Robot with Inner Speech for Dietary Guidance](https://arxiv.org/abs/2505.08664)
*Valerio Belcamino,Alessandro Carfì,Valeria Seidita,Fulvio Mastrogiovanni,Antonio Chella*

Main category: cs.RO

TL;DR: 研究探讨了如何利用内在语言机制提升社交机器人在饮食建议中的透明度和信任度，通过模拟人类思维过程增强机器人的解释能力。


<details>
  <summary>Details</summary>
Motivation: 在医疗场景中，信任机器人助手依赖于准确的建议和自然对话，而内在语言能提升透明度和交互体验。

Method: 开发了具备内在语言功能的社交机器人，结合大语言模型和知识图谱，验证用户输入、优化推理并生成清晰解释。

Result: 通过计算效率测试和小型用户研究验证了内在语言在解释机器人行为中的可靠性。

Conclusion: 该架构提升了人机交互的信任度，为医疗场景中的机器人应用提供了透明化解决方案。

Abstract: We explore the use of inner speech as a mechanism to enhance transparency and
trust in social robots for dietary advice. In humans, inner speech structures
thought processes and decision-making; in robotics, it improves explainability
by making reasoning explicit. This is crucial in healthcare scenarios, where
trust in robotic assistants depends on both accurate recommendations and
human-like dialogue, which make interactions more natural and engaging.
Building on this, we developed a social robot that provides dietary advice, and
we provided the architecture with inner speech capabilities to validate user
input, refine reasoning, and generate clear justifications. The system
integrates large language models for natural language understanding and a
knowledge graph for structured dietary information. By making decisions more
transparent, our approach strengthens trust and improves human-robot
interaction in healthcare. We validated this by measuring the computational
efficiency of our architecture and conducting a small user study, which
assessed the reliability of inner speech in explaining the robot's behavior.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [281] [Linear to Neural Networks Regression: QSPR of Drugs via Degree-Distance Indices](https://arxiv.org/abs/2505.07821)
*M. J. Nadjafi Arani,S. Sorgun,M. Mirzargar*

Main category: q-bio.BM

TL;DR: 该研究通过QSPR分析，结合机器学习技术，探索药物分子物理性质与拓扑指数的相关性，为药物发现提供新视角。


<details>
  <summary>Details</summary>
Motivation: 传统药物设计研究主要关注基于度的拓扑指数，本研究扩展至度-距离基拓扑指数，并结合多种原子属性，以提升预测准确性。

Method: 使用166种药物分子数据集，计算度-距离基拓扑指数，结合六种原子属性，采用线性和非线性机器学习模型进行预测。

Result: 结果表明这些拓扑指数能有效预测特定物理化学性质，凸显了计算方法在分子性质估计中的实用性。

Conclusion: 研究为药物发现提供了创新方法，通过拓扑指数与机器学习的结合优化了资源利用，并为分子行为提供了初步预测。

Abstract: This study conducts a Quantitative Structure Property Relationship (QSPR)
analysis to explore the correlation between the physical properties of drug
molecules and their topological indices using machine learning techniques.
While prior studies in drug design have focused on degree-based topological
indices, this work analyzes a dataset of 166 drug molecules by computing
degree-distance-based topological indices, incorporating vertex-edge weightings
with respect to different six atomic properties (atomic number, atomic radius,
atomic mass, density, electronegativity, ionization). Both linear models
(Linear Regression, Lasso, and Ridge Regression) and nonlinear approaches
(Random Forest, XGBoost, and Neural Networks) were employed to predict
molecular properties. The results demonstrate the effectiveness of these
indices in predicting specific physicochemical properties and underscore the
practical relevance of computational methods in molecular property estimation.
The study provides an innovative perspective on integrating topological indices
with machine learning to enhance predictive accuracy, highlighting their
potential application in drug discovery and development processes. This
predictive may also explain that establishing a reliable relationship between
topological indices and physical properties enables chemists to gain
preliminary insights into molecular behavior before conducting experimental
analyses, thereby optimizing resource utilization in cheminformatics research.

</details>


### [282] [Generative Molecular Design with Steerable and Granular Synthesizability Control](https://arxiv.org/abs/2505.08774)
*Jeff Guo,Víctor Sabanza-Gil,Zlatko Jončev,Jeremy S. Luterbacher,Philippe Schwaller*

Main category: q-bio.BM

TL;DR: 论文提出了一种可调控的小分子生成设计框架，通过强化学习实现合成路径的灵活控制，满足多参数优化目标。


<details>
  <summary>Details</summary>
Motivation: 解决小分子生成设计中合成路径的灵活性和可控性问题，现有方法对合成难易度和反应约束的关注不足。

Method: 采用预训练的通用分子生成模型，通过强化学习激励生成满足特定反应约束的分子。

Result: 生成的分子在满足反应约束的条件下，精确匹配率超过90%，并在单GPU上高效筛选出有潜力的候选分子。

Conclusion: 该框架展示了在严格合成约束下，通过强化学习生成优化小分子的潜力，具有高效性和灵活性。

Abstract: Synthesizability in small molecule generative design remains a bottleneck.
Existing works that do consider synthesizability can output predicted synthesis
routes for generated molecules. However, there has been minimal attention in
addressing the ease of synthesis and enabling flexibility to incorporate
desired reaction constraints. In this work, we propose a small molecule
generative design framework that enables steerable and granular
synthesizability control. Generated molecules satisfy arbitrary multi-parameter
optimization objectives with predicted synthesis routes containing pre-defined
allowed reactions, while optionally avoiding others. One can also enforce that
all reactions belong to a pre-defined set. We show the capability to
mix-and-match these reaction constraints across the most common medicinal
chemistry transformations. Next, we show how our framework can be used to
valorize industrial byproducts towards de novo optimized molecules. Going
further, we demonstrate how granular control over synthesizability constraints
can loosely mimic virtual screening of ultra-large make-on-demand libraries.
Using only a single GPU, we generate and dock 15k molecules to identify
promising candidates in Freedom 4.0 constituting 142B make-on-demand molecules
(assessing only 0.00001% of the library). Generated molecules satisfying the
reaction constraints have > 90% exact match rate. Lastly, we benchmark our
framework against recent synthesizability-constrained generative models and
demonstrate the highest sample efficiency even when imposing the additional
constraint that all molecules must be synthesizable from a single reaction
type. The main theme is demonstrating that a pre-trained generalist molecular
generative model can be incentivized to generate property-optimized small
molecules under challenging synthesizability constraints through reinforcement
learning.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [283] [Distributed Quantum Neural Networks on Distributed Photonic Quantum Computing](https://arxiv.org/abs/2505.08474)
*Kuan-Cheng Chen,Chen-Yu Liu,Yu Shang,Felix Burt,Kin K. Leung*

Main category: quant-ph

TL;DR: 该论文提出了一种分布式量子-经典框架，结合光子量子神经网络（QNNs）和矩阵乘积态（MPS）映射，实现参数高效的经典神经网络训练。实验表明，该方法在MNIST分类任务中表现优异，且对噪声具有鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 通过结合光子量子计算的表达能力和经典神经网络的可部署性，探索一种高效的分布式量子机器学习框架。

Method: 利用光子QNNs生成高维概率分布，并通过MPS模型映射到经典网络权重，实现参数高效训练。

Result: 在MNIST分类任务中达到95.50%的准确率，参数效率优于经典方法，且对噪声具有鲁棒性。

Conclusion: 该框架为分布式量子机器学习提供了一条实用路径，结合了光子量子计算的表达能力和经典神经网络的可部署性。

Abstract: We introduce a distributed quantum-classical framework that synergizes
photonic quantum neural networks (QNNs) with matrix-product-state (MPS) mapping
to achieve parameter-efficient training of classical neural networks. By
leveraging universal linear-optical decompositions of $M$-mode interferometers
and photon-counting measurement statistics, our architecture generates neural
parameters through a hybrid quantum-classical workflow: photonic QNNs with
$M(M+1)/2$ trainable parameters produce high-dimensional probability
distributions that are mapped to classical network weights via an MPS model
with bond dimension $\chi$. Empirical validation on MNIST classification
demonstrates that photonic QT achieves an accuracy of $95.50\% \pm 0.84\%$
using 3,292 parameters ($\chi = 10$), compared to $96.89\% \pm 0.31\%$ for
classical baselines with 6,690 parameters. Moreover, a ten-fold compression
ratio is achieved at $\chi = 4$, with a relative accuracy loss of less than
$3\%$. The framework outperforms classical compression techniques (weight
sharing/pruning) by 6--12\% absolute accuracy while eliminating quantum
hardware requirements during inference through classical deployment of
compressed parameters. Simulations incorporating realistic photonic noise
demonstrate the framework's robustness to near-term hardware imperfections.
Ablation studies confirm quantum necessity: replacing photonic QNNs with random
inputs collapses accuracy to chance level ($10.0\% \pm 0.5\%$). Photonic
quantum computing's room-temperature operation, inherent scalability through
spatial-mode multiplexing, and HPC-integrated architecture establish a
practical pathway for distributed quantum machine learning, combining the
expressivity of photonic Hilbert spaces with the deployability of classical
neural networks.

</details>

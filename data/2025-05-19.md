<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 58]
- [cs.AI](#cs.AI) [Total: 39]
- [cs.LG](#cs.LG) [Total: 126]
- [cs.CR](#cs.CR) [Total: 8]
- [cs.CY](#cs.CY) [Total: 11]
- [cs.DC](#cs.DC) [Total: 1]
- [cs.RO](#cs.RO) [Total: 11]
- [cs.IR](#cs.IR) [Total: 6]
- [cs.AR](#cs.AR) [Total: 1]
- [cs.SD](#cs.SD) [Total: 7]
- [stat.ME](#stat.ME) [Total: 1]
- [quant-ph](#quant-ph) [Total: 2]
- [cs.MA](#cs.MA) [Total: 2]
- [cs.HC](#cs.HC) [Total: 3]
- [eess.IV](#eess.IV) [Total: 3]
- [cs.SI](#cs.SI) [Total: 1]
- [eess.SY](#eess.SY) [Total: 1]
- [eess.AS](#eess.AS) [Total: 1]
- [stat.ML](#stat.ML) [Total: 10]
- [physics.ao-ph](#physics.ao-ph) [Total: 1]
- [cs.NE](#cs.NE) [Total: 1]
- [cs.SE](#cs.SE) [Total: 5]
- [cs.CV](#cs.CV) [Total: 29]
- [math.OC](#math.OC) [Total: 2]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 2]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 1]
- [cs.DB](#cs.DB) [Total: 1]
- [econ.GN](#econ.GN) [Total: 1]
- [cs.CE](#cs.CE) [Total: 1]
- [cs.MM](#cs.MM) [Total: 2]
- [astro-ph.EP](#astro-ph.EP) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Artificial Intelligence Bias on English Language Learners in Automatic Scoring](https://arxiv.org/abs/2505.10643)
*Shuchen Guo,Yun Wang,Jichao Yu,Xuansheng Wu,Bilgehan Ayik,Field M. Watts,Ehsan Latif,Ninghao Liu,Lei Liu,Xiaoming Zhai*

Main category: cs.CL

TL;DR: 研究探讨了自动评分系统对英语学习者（ELLs）的潜在评分偏见和不平等，发现训练数据量足够大时无明显偏见，但样本量小（ELL=200）时可能存在偏见。


<details>
  <summary>Details</summary>
Motivation: 探究自动评分系统对ELLs的评分偏见和不平等，尤其是训练数据不平衡的影响。

Method: 使用BERT模型在四种数据集（ELLs、非ELLs、不平衡混合、平衡混合）上微调，分析21个评估项目，计算评分准确性和平均分数差距（MSG）。

Result: 训练数据量足够大（ELL=30,000和1,000）时未发现AI偏见或不平等，但样本量小（ELL=200）时可能存在偏见。

Conclusion: 自动评分系统对ELLs的偏见与训练数据量相关，样本量足够大时可避免偏见。

Abstract: This study investigated potential scoring biases and disparities toward
English Language Learners (ELLs) when using automatic scoring systems for
middle school students' written responses to science assessments. We
specifically focus on examining how unbalanced training data with ELLs
contributes to scoring bias and disparities. We fine-tuned BERT with four
datasets: responses from (1) ELLs, (2) non-ELLs, (3) a mixed dataset reflecting
the real-world proportion of ELLs and non-ELLs (unbalanced), and (4) a balanced
mixed dataset with equal representation of both groups. The study analyzed 21
assessment items: 10 items with about 30,000 ELL responses, five items with
about 1,000 ELL responses, and six items with about 200 ELL responses. Scoring
accuracy (Acc) was calculated and compared to identify bias using Friedman
tests. We measured the Mean Score Gaps (MSGs) between ELLs and non-ELLs and
then calculated the differences in MSGs generated through both the human and AI
models to identify the scoring disparities. We found that no AI bias and
distorted disparities between ELLs and non-ELLs were found when the training
dataset was large enough (ELL = 30,000 and ELL = 1,000), but concerns could
exist if the sample size is limited (ELL = 200).

</details>


### [2] [GeoGrid-Bench: Can Foundation Models Understand Multimodal Gridded Geo-Spatial Data?](https://arxiv.org/abs/2505.10714)
*Bowen Jiang,Yangxinyu Xie,Xiaomeng Wang,Jiashu He,Joshua Bergerson,John K Hutchison,Jordan Branham,Camillo J Taylor,Tanwi Mallick*

Main category: cs.CL

TL;DR: GeoGrid-Bench是一个评估基础模型处理网格结构地理空间数据能力的基准测试，包含大规模真实数据和多样化任务。


<details>
  <summary>Details</summary>
Motivation: 地理空间数据因其数值密集、时空依赖性强和多模态表示独特，对基础模型提出了挑战，需评估其支持科学研究的潜力。

Method: 基准测试包含16个气候变量、150个地点和长时间序列数据，通过8个专家模板生成3200个问题-答案对，覆盖从简单到复杂的任务。

Result: 视觉语言模型表现最佳，研究对不同模型在地理空间任务中的优劣势进行了细粒度分析。

Conclusion: GeoGrid-Bench为如何有效应用基础模型于地理空间数据分析和科学研究提供了清晰见解。

Abstract: We present GeoGrid-Bench, a benchmark designed to evaluate the ability of
foundation models to understand geo-spatial data in the grid structure.
Geo-spatial datasets pose distinct challenges due to their dense numerical
values, strong spatial and temporal dependencies, and unique multimodal
representations including tabular data, heatmaps, and geographic
visualizations. To assess how foundation models can support scientific research
in this domain, GeoGrid-Bench features large-scale, real-world data covering 16
climate variables across 150 locations and extended time frames. The benchmark
includes approximately 3,200 question-answer pairs, systematically generated
from 8 domain expert-curated templates to reflect practical tasks encountered
by human scientists. These range from basic queries at a single location and
time to complex spatiotemporal comparisons across regions and periods. Our
evaluation reveals that vision-language models perform best overall, and we
provide a fine-grained analysis of the strengths and limitations of different
foundation models in different geo-spatial tasks. This benchmark offers clearer
insights into how foundation models can be effectively applied to geo-spatial
data analysis and used to support scientific research.

</details>


### [3] [A Modular Approach for Clinical SLMs Driven by Synthetic Data with Pre-Instruction Tuning, Model Merging, and Clinical-Tasks Alignment](https://arxiv.org/abs/2505.10717)
*Jean-Philippe Corbeil,Amin Dada,Jean-Michel Attendu,Asma Ben Abacha,Alessandro Sordoni,Lucas Caccia,François Beaulieu,Thomas Lin,Jens Kleesiek,Paul Vozila*

Main category: cs.CL

TL;DR: 提出了一种新框架MediPhi，用于将小型语言模型（SLMs）高效适应临床任务，通过预指令调优、模型合并和任务对齐，显著提升了性能，并在多个临床任务上超越了GPT-4。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（如GPT-4）的计算成本和延迟限制了其在临床环境中的应用，而小型语言模型（SLMs）虽成本低但能力有限，需要针对生物医学领域进行适配。

Method: 提出MediPhi框架，包括预指令调优、模型合并和临床任务对齐；扩展了CLUE基准为CLUE+；构建了MediFlow合成数据集。

Result: MediPhi在多个临床任务上显著优于基础模型，如医学实体识别提升64.3%，放射学报告提升49.5%，ICD-10编码提升44%（超越GPT-4-0125 14%）。

Conclusion: MediPhi框架成功提升了SLMs在临床任务中的性能，并通过合成数据集和任务对齐进一步优化，为临床部署提供了高效解决方案。

Abstract: High computation costs and latency of large language models such as GPT-4
have limited their deployment in clinical settings. Small language models
(SLMs) offer a cost-effective alternative, but their limited capacity requires
biomedical domain adaptation, which remains challenging. An additional
bottleneck is the unavailability and high sensitivity of clinical data. To
address these challenges, we propose a novel framework for adapting SLMs into
high-performing clinical models. We introduce the MediPhi collection of
3.8B-parameter SLMs developed with our novel framework: pre-instruction tuning
of experts on relevant medical and clinical corpora (PMC, Medical Guideline,
MedWiki, etc.), model merging, and clinical-tasks alignment. To cover most
clinical tasks, we extended the CLUE benchmark to CLUE+, doubling its size. Our
expert models deliver relative improvements on this benchmark over the base
model without any task-specific fine-tuning: 64.3% on medical entities, 49.5%
on radiology reports, and 44% on ICD-10 coding (outperforming GPT-4-0125 by
14%). We unify the expert models into MediPhi via model merging, preserving
gains across benchmarks. Furthermore, we built the MediFlow collection, a
synthetic dataset of 2.5 million high-quality instructions on 14 medical NLP
tasks, 98 fine-grained document types, and JSON format support. Alignment of
MediPhi using supervised fine-tuning and direct preference optimization
achieves further gains of 18.9% on average.

</details>


### [4] [AI-enhanced semantic feature norms for 786 concepts](https://arxiv.org/abs/2505.10718)
*Siddharth Suresh,Kushin Mukherjee,Tyler Giallanza,Xizheng Yu,Mia Patil,Jonathan D. Cohen,Timothy T. Rogers*

Main category: cs.CL

TL;DR: 论文提出了一种结合人类生成特征规范与大型语言模型（LLMs）的新方法，创建了高质量、高密度的特征规范数据集NOVA，并在预测人类语义相似性判断方面优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统语义特征规范方法在概念/特征覆盖范围与质量验证之间存在权衡，且人工标注成本高。

Method: 通过结合人类生成的特征规范和LLMs的响应，并验证其质量，构建了NOVA数据集。

Result: NOVA数据集在特征密度和概念重叠方面表现更优，且在预测人类语义相似性判断上优于纯人类数据集和词嵌入模型。

Conclusion: 人类概念知识比以往数据集所捕获的更丰富，且经过验证的LLMs可作为认知科学研究的强大工具。

Abstract: Semantic feature norms have been foundational in the study of human
conceptual knowledge, yet traditional methods face trade-offs between
concept/feature coverage and verifiability of quality due to the
labor-intensive nature of norming studies. Here, we introduce a novel approach
that augments a dataset of human-generated feature norms with responses from
large language models (LLMs) while verifying the quality of norms against
reliable human judgments. We find that our AI-enhanced feature norm dataset,
NOVA: Norms Optimized Via AI, shows much higher feature density and overlap
among concepts while outperforming a comparable human-only norm dataset and
word-embedding models in predicting people's semantic similarity judgments.
Taken together, we demonstrate that human conceptual knowledge is richer than
captured in previous norm datasets and show that, with proper validation, LLMs
can serve as powerful tools for cognitive science research.

</details>


### [5] [Tracr-Injection: Distilling Algorithms into Pre-trained Language Models](https://arxiv.org/abs/2505.10719)
*Tomás Vergara-Browne,Álvaro Soto*

Main category: cs.CL

TL;DR: 论文提出了一种名为tracr-injection的方法，将RASP编写的算法直接注入预训练语言模型，展示了其可行性和优势。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型的符号能力与变压器架构的理论能力之间的不匹配问题。

Method: 提出tracr-injection方法，将RASP算法直接注入预训练语言模型，并验证其有效性。

Result: 成功注入3种算法，创建了可解释的子空间，并提升了分布外性能。

Conclusion: tracr-injection方法有效，为语言模型的符号能力提供了新思路。

Abstract: Motivated by the surge of large language models, there has been a push to
formally characterize the symbolic abilities intrinsic to the transformer
architecture. A programming language, called RASP, has been proposed, which can
be directly compiled into transformer weights to implement these algorithms.
However, the tasks that can be implemented in RASP are often uncommon to learn
from natural unsupervised data, showing a mismatch between theoretical
capabilities of the transformer architecture, and the practical learnability of
these capabilities from unsupervised data. We propose tracr-injection, a method
that allows us to distill algorithms written in RASP directly into a
pre-trained language model. We showcase our method by injecting 3 different
algorithms into a language model. We show how our method creates an
interpretable subspace within the model's residual stream, which can be decoded
into the variables present in the code of the RASP algorithm. Additionally, we
found that the proposed method can improve out of distribution performance
compared to our baseline, indicating that indeed a more symbolic mechanism is
taking place in the inner workings of the model. We release the code used to
run our experiments.

</details>


### [6] [Model Performance-Guided Evaluation Data Selection for Effective Prompt Optimization](https://arxiv.org/abs/2505.10736)
*Ximing Dong,Shaowei Wang,Dayi Lin,Ahmed E. Hassan*

Main category: cs.CL

TL;DR: IPOMP是一种两阶段方法，通过语义聚类和边界分析选择代表性样本，并结合实时模型性能数据进行迭代优化，显著提升提示优化的效果和稳定性。


<details>
  <summary>Details</summary>
Motivation: 手动提示工程效率低下，现有自动优化方法因依赖随机评估子集而不可靠，且核心集选择方法不适用于提示优化。

Method: IPOMP采用两阶段方法：1）通过语义聚类和边界分析选择样本；2）利用实时性能数据迭代替换冗余样本。

Result: 在BIG-bench数据集上，IPOMP效果提升1.6%至5.3%，稳定性提升至少57%，计算开销低于1%。

Conclusion: IPOMP能有效提升提示优化效果，且其实时性能引导的优化方法可普遍应用于现有核心集选择方法。

Abstract: Optimizing Large Language Model (LLM) performance requires well-crafted
prompts, but manual prompt engineering is labor-intensive and often
ineffective. Automated prompt optimization techniques address this challenge
but the majority of them rely on randomly selected evaluation subsets, which
fail to represent the full dataset, leading to unreliable evaluations and
suboptimal prompts. Existing coreset selection methods, designed for LLM
benchmarking, are unsuitable for prompt optimization due to challenges in
clustering similar samples, high data collection costs, and the unavailability
of performance data for new or private datasets. To overcome these issues, we
propose IPOMP, an Iterative evaluation data selection for effective Prompt
Optimization using real-time Model Performance. IPOMP is a two-stage approach
that selects representative and diverse samples using semantic clustering and
boundary analysis, followed by iterative refinement with real-time model
performance data to replace redundant samples. Evaluations on the BIG-bench
dataset show that IPOMP improves effectiveness by 1.6% to 5.3% and stability by
at least 57% compared with SOTA baselines, with minimal computational overhead
below 1%. Furthermore, the results demonstrate that our real-time
performance-guided refinement approach can be universally applied to enhance
existing coreset selection methods.

</details>


### [7] [SemEval-2025 Task 7: Multilingual and Crosslingual Fact-Checked Claim Retrieval](https://arxiv.org/abs/2505.10740)
*Qiwei Peng,Robert Moro,Michal Gregor,Ivan Srba,Simon Ostermann,Marian Simko,Juraj Podroužek,Matúš Mesarčík,Jaroslav Kopčan,Anders Søgaard*

Main category: cs.CL

TL;DR: 论文介绍了SemEval 2025中关于多语言声明检索的共享任务，旨在解决多语言和低资源语言环境下的虚假信息问题。


<details>
  <summary>Details</summary>
Motivation: 在线虚假信息的快速传播是一个全球性挑战，而多语言和低资源语言在此领域常被忽视。

Method: 通过共享任务设计两个子任务：单语言和跨语言声明检索，收集了179名参与者的52份测试提交。

Result: 23支团队提交了系统论文，论文总结了表现最佳的系统及常见有效方法。

Conclusion: 该共享任务及其数据集和系统为多语言声明检索和自动事实核查提供了宝贵见解，支持未来研究。

Abstract: The rapid spread of online disinformation presents a global challenge, and
machine learning has been widely explored as a potential solution. However,
multilingual settings and low-resource languages are often neglected in this
field. To address this gap, we conducted a shared task on multilingual claim
retrieval at SemEval 2025, aimed at identifying fact-checked claims that match
newly encountered claims expressed in social media posts across different
languages. The task includes two subtracks: (1) a monolingual track, where
social posts and claims are in the same language, and (2) a crosslingual track,
where social posts and claims might be in different languages. A total of 179
participants registered for the task contributing to 52 test submissions. 23
out of 31 teams have submitted their system papers. In this paper, we report
the best-performing systems as well as the most common and the most effective
approaches across both subtracks. This shared task, along with its dataset and
participating systems, provides valuable insights into multilingual claim
retrieval and automated fact-checking, supporting future research in this
field.

</details>


### [8] [Ranked Voting based Self-Consistency of Large Language Models](https://arxiv.org/abs/2505.10772)
*Weiqin Wang,Yile Wang,Hui Huang*

Main category: cs.CL

TL;DR: 论文提出了一种通过生成排序答案并使用排序投票方法（如即时决选投票、博尔达计数投票和平均倒数排名投票）来提升链式思维推理可靠性的方法。


<details>
  <summary>Details</summary>
Motivation: 现有链式思维推理方法通常只生成单一答案，忽略了其他潜在答案的可能性，导致投票过程中信息不完整。

Method: 在每次推理过程中生成排序答案，并使用三种排序投票方法（即时决选投票、博尔达计数投票和平均倒数排名投票）进行投票。

Result: 在六个数据集上的实验表明，该方法优于基线方法，验证了利用排序答案信息和排序投票提升推理性能的潜力。

Conclusion: 通过生成排序答案并采用排序投票方法，可以显著提升链式思维推理的可靠性和性能。

Abstract: Majority voting is considered an effective method to enhance chain-of-thought
reasoning, as it selects the answer with the highest "self-consistency" among
different reasoning paths (Wang et al., 2023). However, previous
chain-of-thought reasoning methods typically generate only a single answer in
each trial, thereby ignoring the possibility of other potential answers. As a
result, these alternative answers are often overlooked in subsequent voting
processes. In this work, we propose to generate ranked answers in each
reasoning process and conduct ranked voting among multiple ranked answers from
different responses, thereby making the overall self-consistency more reliable.
Specifically, we use three ranked voting methods: Instant-runoff voting, Borda
count voting, and mean reciprocal rank voting. We validate our methods on six
datasets, including three multiple-choice and three open-ended
question-answering tasks, using both advanced open-source and closed-source
large language models. Extensive experimental results indicate that our
proposed method outperforms the baselines, showcasing the potential of
leveraging the information of ranked answers and using ranked voting to improve
reasoning performance. The code is available at
https://github.com/szu-tera/RankedVotingSC.

</details>


### [9] [A Systematic Analysis of Base Model Choice for Reward Modeling](https://arxiv.org/abs/2505.10775)
*Kian Ahrabian,Pegah Jandaghi,Negar Mokhberian,Sai Praneeth Karimireddy,Jay Pujara*

Main category: cs.CL

TL;DR: 研究分析了基础模型选择对奖励建模性能的影响，发现性能可提升14%，并展示了基准测试与下游性能的强相关性。


<details>
  <summary>Details</summary>
Motivation: 探讨基础模型选择对奖励建模性能的影响，以优化大型语言模型的训练效果。

Method: 系统分析基础模型选择对奖励建模性能的影响，结合基准测试结果进行模型选择优化。

Result: 性能提升14%，基准测试与下游性能强相关，结合小规模基准测试可提升模型选择效果18%。

Conclusion: 基础模型选择对奖励建模性能至关重要，结合基准测试可显著优化模型选择。

Abstract: Reinforcement learning from human feedback (RLHF) and, at its core, reward
modeling have become a crucial part of training powerful large language models
(LLMs). One commonly overlooked factor in training high-quality reward models
(RMs) is the effect of the base model, which is becoming more challenging to
choose given the rapidly growing pool of LLMs. In this work, we present a
systematic analysis of the effect of base model selection on reward modeling
performance. Our results show that the performance can be improved by up to 14%
compared to the most common (i.e., default) choice. Moreover, we showcase the
strong statistical relation between some existing benchmarks and downstream
performances. We also demonstrate that the results from a small set of
benchmarks could be combined to boost the model selection ($+$18% on average in
the top 5-10). Lastly, we illustrate the impact of different post-training
steps on the final performance and explore using estimated data distributions
to reduce performance prediction error.

</details>


### [10] [Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation](https://arxiv.org/abs/2505.10792)
*Zhan Peng Lee,Andre Lin,Calvin Tan*

Main category: cs.CL

TL;DR: Finetune-RAG通过微调方法提升RAG框架的事实准确性，并引入Bench-RAG评估管道。


<details>
  <summary>Details</summary>
Motivation: 解决RAG框架中因检索不相关内容导致LLM产生幻觉的问题。

Method: 构建模拟真实世界不完美的RAG训练数据集，提出Finetune-RAG微调方法和Bench-RAG评估管道。

Result: Finetune-RAG将事实准确性提升21.2%。

Conclusion: Finetune-RAG和Bench-RAG为社区提供了有效的工具和数据集。

Abstract: Retrieval-Augmented Generation (RAG) has emerged as a powerful framework to
improve factuality in large language models (LLMs) by grounding their outputs
in retrieved documents. However, ensuring perfect retrieval of relevant
information remains challenging, and when irrelevant content is passed
downstream to an LLM, it can lead to hallucinations. In this work, we propose
Finetune-RAG, a simple and effective fine-tuning approach that features the
first-of-its-kind RAG training dataset constructed to mimic real-world
imperfections. Experimental results show that Finetune-RAG improves factual
accuracy by 21.2% over the base model. We also propose a Bench-RAG, an
LLM-as-a-judge evaluation pipeline that stress tests models under realistic
imperfect retrieval scenarios. Our codebase and dataset are fully open sourced
for community use.

</details>


### [11] [Relation Extraction Across Entire Books to Reconstruct Community Networks: The AffilKG Datasets](https://arxiv.org/abs/2505.10798)
*Erica Cai,Sean McQuade,Kevin Young,Brendan O'Connor*

Main category: cs.CL

TL;DR: AffilKG是一个包含六个数据集的新资源，用于评估从文本中自动提取的知识图谱（KGs）的准确性，填补了现有数据集的不足。


<details>
  <summary>Details</summary>
Motivation: 现有标注数据集无法评估自动提取KGs的准确性，因为它们过于分散、规模小或复杂。

Method: 引入AffilKG，包含六个数据集，每个数据集包含完整的书籍扫描和大规模标注的KGs，涵盖简单的成员关系图谱和扩展的关系类型。

Result: 初步实验显示模型性能在不同数据集间差异显著，验证了AffilKG在评估提取错误传播和验证KG提取方法方面的能力。

Conclusion: AffilKG为KG提取的准确性评估和下游分析提供了重要工具，尤其适用于社会科学研究。

Abstract: When knowledge graphs (KGs) are automatically extracted from text, are they
accurate enough for downstream analysis? Unfortunately, current annotated
datasets can not be used to evaluate this question, since their KGs are highly
disconnected, too small, or overly complex. To address this gap, we introduce
AffilKG (https://doi.org/10.5281/zenodo.15427977), which is a collection of six
datasets that are the first to pair complete book scans with large, labeled
knowledge graphs. Each dataset features affiliation graphs, which are simple
KGs that capture Member relationships between Person and Organization entities
-- useful in studies of migration, community interactions, and other social
phenomena. In addition, three datasets include expanded KGs with a wider
variety of relation types. Our preliminary experiments demonstrate significant
variability in model performance across datasets, underscoring AffilKG's
ability to enable two critical advances: (1) benchmarking how extraction errors
propagate to graph-level analyses (e.g., community structure), and (2)
validating KG extraction methods for real-world social science research.

</details>


### [12] [Enhancing Low-Resource Minority Language Translation with LLMs and Retrieval-Augmented Generation for Cultural Nuances](https://arxiv.org/abs/2505.10829)
*Chen-Chi Chang,Chong-Fu Li,Chu-Hsuan Lee,Hung-Shin Lee*

Main category: cs.CL

TL;DR: 本研究探讨了将大型语言模型（LLMs）与检索增强生成（RAG）结合用于低资源语言翻译的挑战，测试了多种模型配置，最佳模型（Model 4）结合了检索和高级语言建模，显著提升了翻译质量。


<details>
  <summary>Details</summary>
Motivation: 解决低资源语言（如客家话）翻译的挑战，尤其是专业和文化特有词汇的翻译问题。

Method: 测试了多种模型配置，包括纯词典、RAG结合Gemini 2.0，以及两阶段方法（词典输出后由Gemini 2.0优化）。

Result: 最佳模型（Model 4）BLEU分数达31%，显著提升词汇覆盖和语法连贯性；两阶段方法（Model 3）BLEU分数为26%。

Conclusion: 研究强调了定制资源、领域知识和与本地社区合作的重要性，提出了一个提升翻译准确性和文化保护能力的框架。

Abstract: This study investigates the challenges of translating low-resource languages
by integrating Large Language Models (LLMs) with Retrieval-Augmented Generation
(RAG). Various model configurations were tested on Hakka translations, with
BLEU scores ranging from 12% (dictionary-only) to 31% (RAG with Gemini 2.0).
The best-performing model (Model 4) combined retrieval and advanced language
modeling, improving lexical coverage, particularly for specialized or
culturally nuanced terms, and enhancing grammatical coherence. A two-stage
method (Model 3) using dictionary outputs refined by Gemini 2.0 achieved a BLEU
score of 26%, highlighting iterative correction's value and the challenges of
domain-specific expressions. Static dictionary-based approaches struggled with
context-sensitive content, demonstrating the limitations of relying solely on
predefined resources. These results emphasize the need for curated resources,
domain knowledge, and ethical collaboration with local communities, offering a
framework that improves translation accuracy and fluency while supporting
cultural preservation.

</details>


### [13] [Learning When to Think: Shaping Adaptive Reasoning in R1-Style Models via Multi-Stage RL](https://arxiv.org/abs/2505.10832)
*Songjun Tu,Jiahao Lin,Qichao Zhang,Xiangyu Tian,Linjing Li,Xiangyuan Lan,Dongbin Zhao*

Main category: cs.CL

TL;DR: AutoThink通过多阶段强化学习框架，动态调整大型推理模型的显式推理行为，实现准确性与效率的平衡。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在简单问题上过度推理会导致计算开销和延迟，因此需要自适应能力以动态决定是否进行显式推理。

Method: 基于R1风格蒸馏模型，通过插入省略号触发不同推理模式，并利用多阶段强化学习框架优化推理策略。

Result: 在五个主流数学基准测试中，AutoThink在准确性和效率上优于其他方法，提升相对准确性6.4%，同时减少52%的token使用。

Conclusion: AutoThink为大型推理模型提供了一种可扩展且自适应的推理范式，显著提升了性能与效率。

Abstract: Large reasoning models (LRMs) are proficient at generating explicit,
step-by-step reasoning sequences before producing final answers. However, such
detailed reasoning can introduce substantial computational overhead and
latency, particularly for simple problems. To address this over-thinking
problem, we explore how to equip LRMs with adaptive thinking capabilities:
enabling them to dynamically decide whether or not to engage in explicit
reasoning based on problem complexity. Building on R1-style distilled models,
we observe that inserting a simple ellipsis ("...") into the prompt can
stochastically trigger either a thinking or no-thinking mode, revealing a
latent controllability in the reasoning behavior. Leveraging this property, we
propose AutoThink, a multi-stage reinforcement learning (RL) framework that
progressively optimizes reasoning policies via stage-wise reward shaping.
AutoThink learns to invoke explicit reasoning only when necessary, while
defaulting to succinct responses for simpler tasks. Experiments on five
mainstream mathematical benchmarks demonstrate that AutoThink achieves
favorable accuracy-efficiency trade-offs compared to recent prompting and
RL-based pruning methods. It can be seamlessly integrated into any R1-style
model, including both distilled and further fine-tuned variants. Notably,
AutoThink improves relative accuracy by 6.4 percent while reducing token usage
by 52 percent on DeepSeek-R1-Distill-Qwen-1.5B, establishing a scalable and
adaptive reasoning paradigm for LRMs.

</details>


### [14] [Multimodal Event Detection: Current Approaches and Defining the New Playground through LLMs and VLMs](https://arxiv.org/abs/2505.10836)
*Abhishek Dey,Aabha Bothera,Samhita Sarikonda,Rishav Aryan,Sanjay Kumar Podishetty,Akshay Havalgi,Gaurav Singh,Saurabh Srivastava*

Main category: cs.CL

TL;DR: 论文研究了社交媒体事件检测的挑战，比较了单模态和多模态方法的性能，发现多模态方法优于单模态方法，但生成式方法在精度上落后于监督方法。


<details>
  <summary>Details</summary>
Motivation: 传统单模态系统在快速、多模态的社交媒体数据传播中表现不佳，因此需要研究多模态和生成式方法的有效性。

Method: 使用了单模态模型（ModernBERT、ConvNeXt-V2）、多模态融合技术及生成式模型（GPT-4o、LLaVA），并测试了生成式模型在单模态输入下的表现。

Result: 多模态方法显著优于单模态方法，但生成式方法在精度上落后于监督方法和指令调优模型。生成式方法能有效处理社交媒体常见问题（如leet speak、文本拉长），但难以正确生成事件类别。

Conclusion: 多模态方法在社交媒体事件检测中表现更优，但生成式方法需进一步优化以提升精度。

Abstract: In this paper, we study the challenges of detecting events on social media,
where traditional unimodal systems struggle due to the rapid and multimodal
nature of data dissemination. We employ a range of models, including unimodal
ModernBERT and ConvNeXt-V2, multimodal fusion techniques, and advanced
generative models like GPT-4o, and LLaVA. Additionally, we also study the
effect of providing multimodal generative models (such as GPT-4o) with a single
modality to assess their efficacy. Our results indicate that while multimodal
approaches notably outperform unimodal counterparts, generative approaches
despite having a large number of parameters, lag behind supervised methods in
precision. Furthermore, we also found that they lag behind instruction-tuned
models because of their inability to generate event classes correctly. During
our error analysis, we discovered that common social media issues such as leet
speak, text elongation, etc. are effectively handled by generative approaches
but are hard to tackle using supervised approaches.

</details>


### [15] [Have Multimodal Large Language Models (MLLMs) Really Learned to Tell the Time on Analog Clocks?](https://arxiv.org/abs/2505.10862)
*Tairan Fu,Miguel González,Javier Conde,Elena Merino-Gómez,Pedro Reviriego*

Main category: cs.CL

TL;DR: 多模态大语言模型（MLLMs）在回答图像复杂问题时难以识别模拟时钟时间，可能是训练数据中缺乏不同时间的时钟图像。本文通过测试GPT-4.1探讨此问题，研究微调是否能解决。结果显示模型在识别时钟时间上有进展，但需验证其是否真正学会或仅记忆训练数据模式。


<details>
  <summary>Details</summary>
Motivation: 研究MLLMs为何无法识别模拟时钟时间，并探索微调是否能解决此问题。

Method: 使用GPT-4.1进行测试，分析其识别时钟时间的能力，并通过不同时钟验证模型的泛化能力。

Result: 模型在识别时钟时间上有所进步，但需进一步验证其是否真正学会或仅依赖训练数据模式。

Conclusion: MLLMs在抽象和泛化能力上仍有局限，需更多研究以提升其实际应用能力。

Abstract: Multimodal Large Language Models which can answer complex questions on an
image struggle to tell the time on analog clocks. This is probably due to the
lack of images with clocks at different times in their training set. In this
work we explore this issue with one of the latest MLLMs: GPT-4.1 to understand
why MLLMs fail to tell the time and whether fine-tuning can solve the problem.
The results show how models are making progress in reading the time on analog
clocks. But have they really learned to do it, or have they only learned
patterns in their training datasets? In this work we put the models to the test
with different clocks to illustrate the limitations of MLLMs to abstract and
generalize.

</details>


### [16] [Improve Rule Retrieval and Reasoning with Self-Induction and Relevance ReEstimate](https://arxiv.org/abs/2505.10870)
*Ziyang Huang,Wangtao Sun,Jun Zhao,Kang Liu*

Main category: cs.CL

TL;DR: 论文提出了一种名为SIAR的新方法，通过LLM生成潜在推理规则以改善规则检索效果，并结合R³方法重新评估规则相关性，显著提升了检索准确性和推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有规则检索方法因查询事实与规则抽象表示之间的语义差距导致检索质量低，进而影响推理性能。

Method: 提出SIAR方法，利用LLM生成潜在推理规则并用于查询增强；引入R³方法重新评估规则的相关性。

Result: 实验表明，SIAR和R³方法在不同场景下均能有效提升检索和推理性能。

Conclusion: SIAR和R³方法通过规则生成和相关性重估，显著改善了规则检索的准确性和推理效果。

Abstract: This paper systematically addresses the challenges of rule retrieval, a
crucial yet underexplored area. Vanilla retrieval methods using sparse or dense
retrievers to directly search for relevant rules to support downstream
reasoning, often suffer from low accuracy. This is primarily due to a
significant semantic gap between the instantiated facts in the queries and the
abstract representations of the rules. Such misalignment results in suboptimal
retrieval quality, which in turn negatively impacts reasoning performance. To
overcome these challenges, we propose Self-Induction Augmented Retrieval
(SIAR), a novel approach that utilizes Large Language Models (LLMs) to induce
potential inferential rules that might offer benefits for reasoning by
abstracting the underlying knowledge and logical structure in queries. These
induced rules are then used for query augmentation to improve retrieval
effectiveness. Additionally, we introduce Rule Relevance ReEstimate (R$^3$), a
method that re-estimates the relevance of retrieved rules by assessing whether
the abstract knowledge they contain can be instantiated to align with the facts
in the queries and the helpfulness for reasoning. Extensive experiments across
various settings demonstrate the effectiveness and versatility of our proposed
methods.

</details>


### [17] [A Survey on the Safety and Security Threats of Computer-Using Agents: JARVIS or Ultron?](https://arxiv.org/abs/2505.10924)
*Ada Chen,Yongjiang Wu,Junyuan Zhang,Shu Yang,Jen-tse Huang,Kun Wang,Wenxuan Wang,Shuai Wang*

Main category: cs.CL

TL;DR: 论文系统化分析了AI驱动的计算机使用代理（CUAs）的安全与威胁，提出了定义、威胁分类、防御策略及评估指标。


<details>
  <summary>Details</summary>
Motivation: 随着CUAs能力的提升，其安全风险日益复杂，需系统化研究以应对潜在威胁。

Method: 通过文献综述，围绕四个目标展开：定义CUAs、分类威胁、提出防御策略、总结评估指标。

Result: 构建了CUAs安全研究的结构化基础，为未来研究和实践提供指导。

Conclusion: 论文为CUAs的安全研究提供了系统化框架，助力未来漏洞探索和安全设计。

Abstract: Recently, AI-driven interactions with computing devices have advanced from
basic prototype tools to sophisticated, LLM-based systems that emulate
human-like operations in graphical user interfaces. We are now witnessing the
emergence of \emph{Computer-Using Agents} (CUAs), capable of autonomously
performing tasks such as navigating desktop applications, web pages, and mobile
apps. However, as these agents grow in capability, they also introduce novel
safety and security risks. Vulnerabilities in LLM-driven reasoning, with the
added complexity of integrating multiple software components and multimodal
inputs, further complicate the security landscape. In this paper, we present a
systematization of knowledge on the safety and security threats of CUAs. We
conduct a comprehensive literature review and distill our findings along four
research objectives: \textit{\textbf{(i)}} define the CUA that suits safety
analysis; \textit{\textbf{(ii)} } categorize current safety threats among CUAs;
\textit{\textbf{(iii)}} propose a comprehensive taxonomy of existing defensive
strategies; \textit{\textbf{(iv)}} summarize prevailing benchmarks, datasets,
and evaluation metrics used to assess the safety and performance of CUAs.
Building on these insights, our work provides future researchers with a
structured foundation for exploring unexplored vulnerabilities and offers
practitioners actionable guidance in designing and deploying secure
Computer-Using Agents.

</details>


### [18] [Connecting the Dots: A Chain-of-Collaboration Prompting Framework for LLM Agents](https://arxiv.org/abs/2505.10936)
*Jiaxing Zhao,Hongbin Xie,Yuzhen Lei,Xuan Song,Zhuoran Shi,Lianxin Li,Shuangxue Liu,Haoran Zhang*

Main category: cs.CL

TL;DR: Cochain是一个协作提示框架，通过结合知识和提示以较低成本解决业务工作流协作问题，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 单代理链式思维和多代理系统在业务工作流任务中存在协作挑战和成本问题，需要更高效的解决方案。

Method: 构建集成知识图并维护提示树，以获取跨阶段相关提示信息。

Result: Cochain在多个数据集上表现优于基线方法，小模型结合Cochain甚至优于GPT-4。

Conclusion: Cochain通过高效协作提示框架，显著提升了业务工作流任务的性能。

Abstract: Large Language Models (LLMs) have demonstrated impressive performance in
executing complex reasoning tasks. Chain-of-thought effectively enhances
reasoning capabilities by unlocking the potential of large models, while
multi-agent systems provide more comprehensive solutions by integrating
collective intelligence of multiple agents. However, both approaches face
significant limitations. Single-agent with chain-of-thought, due to the
inherent complexity of designing cross-domain prompts, faces collaboration
challenges. Meanwhile, multi-agent systems consume substantial tokens and
inevitably dilute the primary problem, which is particularly problematic in
business workflow tasks. To address these challenges, we propose Cochain, a
collaboration prompting framework that effectively solves business workflow
collaboration problem by combining knowledge and prompts at a reduced cost.
Specifically, we construct an integrated knowledge graph that incorporates
knowledge from multiple stages. Furthermore, by maintaining and retrieving a
prompts tree, we can obtain prompt information relevant to other stages of the
business workflow. We perform extensive evaluations of Cochain across multiple
datasets, demonstrating that Cochain outperforms all baselines in both prompt
engineering and multi-agent LLMs. Additionally, expert evaluation results
indicate that the use of a small model in combination with Cochain outperforms
GPT-4.

</details>


### [19] [Reasoning with OmniThought: A Large CoT Dataset with Verbosity and Cognitive Difficulty Annotations](https://arxiv.org/abs/2505.10937)
*Wenrui Cai,Chengyu Wang,Junbing Yan,Jun Huang,Xiangzhong Fang*

Main category: cs.CL

TL;DR: 论文介绍了OmniThought数据集，包含200万条由两个大型推理模型生成的CoT过程，并标注了新颖的RV和CD分数，显著提升了大型推理模型的训练效果。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏全面的CoT数据集，限制了大型推理模型的发展。现有资源无法提供多教师模型生成的广泛推理问题及描述CoT内部特性的多维属性。

Method: 提出OmniThought数据集，包含200万条CoT过程，标注了RV和CD分数，并建立自给自足的管道来整理数据。

Result: 实验表明，RV和CD分数对模型训练效果有积极影响，并基于OmniThought训练了一系列高性能的大型推理模型。

Conclusion: OmniThought显著提升了大型推理模型在复杂任务中的开发和训练效果。

Abstract: The emergence of large reasoning models (LRMs) has transformed Natural
Language Processing by excelling in complex tasks such as mathematical
problem-solving and code generation. These models leverage chain-of-thought
(CoT) processes, enabling them to emulate human-like reasoning strategies.
However, the advancement of LRMs is hindered by the lack of comprehensive CoT
datasets. Current resources often fail to provide extensive reasoning problems
with coherent CoT processes distilled from multiple teacher models and do not
account for multifaceted properties describing the internal characteristics of
CoTs. To address these challenges, we introduce OmniThought, a large-scale
dataset featuring 2 million CoT processes generated and validated by two
powerful LRMs as teacher models. Each CoT process in OmniThought is annotated
with novel Reasoning Verbosity (RV) and Cognitive Difficulty (CD) scores, which
describe the appropriateness of CoT verbosity and cognitive difficulty level
for models to comprehend these reasoning processes. We further establish a
self-reliant pipeline to curate this dataset. Extensive experiments using
Qwen2.5 models of various sizes demonstrate the positive impact of our proposed
scores on LRM training effectiveness. Based on the proposed OmniThought
dataset, we further train and release a series of high-performing LRMs,
specifically equipped with stronger reasoning abilities and optimal CoT output
length and difficulty level. Our contributions significantly enhance the
development and training of LRMs for solving complex tasks.

</details>


### [20] [Accurate KV Cache Quantization with Outlier Tokens Tracing](https://arxiv.org/abs/2505.10938)
*Yi Su,Yuechi Zhou,Quantong Qiu,Juntao Li,Qingrong Xia,Ping Li,Xinyu Duan,Zhefeng Wang,Min Zhang*

Main category: cs.CL

TL;DR: KV Cache量化通过通道和令牌方式减少内存开销，但异常令牌影响精度。提出方法识别并排除异常令牌，显著提升精度和性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）部署时计算资源消耗大，KV Cache量化可平衡内存和精度，但异常令牌影响量化效果。

Method: 识别解码过程中的异常令牌并排除量化，改进通道和令牌量化方法。

Result: 在2位量化下显著提升精度，内存使用减少6.4倍，吞吐量提高2.3倍。

Conclusion: 提出的异常令牌识别方法有效提升KV Cache量化性能，为LLM部署提供高效解决方案。

Abstract: The impressive capabilities of Large Language Models (LLMs) come at the cost
of substantial computational resources during deployment. While KV Cache can
significantly reduce recomputation during inference, it also introduces
additional memory overhead. KV Cache quantization presents a promising
solution, striking a good balance between memory usage and accuracy. Previous
research has shown that the Keys are distributed by channel, while the Values
are distributed by token. Consequently, the common practice is to apply
channel-wise quantization to the Keys and token-wise quantization to the
Values. However, our further investigation reveals that a small subset of
unusual tokens exhibit unique characteristics that deviate from this pattern,
which can substantially impact quantization accuracy. To address this, we
develop a simple yet effective method to identify these tokens accurately
during the decoding process and exclude them from quantization as outlier
tokens, significantly improving overall accuracy. Extensive experiments show
that our method achieves significant accuracy improvements under 2-bit
quantization and can deliver a 6.4 times reduction in memory usage and a 2.3
times increase in throughput.

</details>


### [21] [GenKnowSub: Improving Modularity and Reusability of LLMs through General Knowledge Subtraction](https://arxiv.org/abs/2505.10939)
*Mohammadtaha Bagherifard,Sahar Rajabi,Ali Edalat,Yadollah Yaghoobzadeh*

Main category: cs.CL

TL;DR: 论文提出了一种名为GenKnowSub的模块化框架，通过分离通用知识和任务特定知识，提升大语言模型的零样本泛化能力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在零样本泛化方面表现不佳，主要原因是通用知识和任务特定知识的耦合。

Method: 构建任务特定LoRA模块库和通用LoRA模块，通过通用知识减法（GenKnowSub）提取任务相关残差模块，并动态组合模块处理新输入。

Result: 在Phi-3模型和标准Arrow基准测试中，使用多语言通用知识LoRA模块显著提升了单语和跨语言任务的性能。

Conclusion: GenKnowSub方法有效提升了大语言模型的零样本泛化能力，并适用于不同能力的模型。

Abstract: Large language models often struggle with zero-shot generalization, and
several modular approaches have been proposed to address this challenge. Yet,
we hypothesize that a key limitation remains: the entanglement of general
knowledge and task-specific adaptations. To overcome this, we propose a modular
framework that disentangles these components by constructing a library of
task-specific LoRA modules alongside a general-domain LoRA. By subtracting this
general knowledge component from each task-specific module, we obtain residual
modules that focus more exclusively on task-relevant information, a method we
call general knowledge subtraction (GenKnowSub). Leveraging the refined
task-specific modules and the Arrow routing algorithm
\citep{ostapenko2024towards}, we dynamically select and combine modules for new
inputs without additional training. Our studies on the Phi-3 model and standard
Arrow as baselines reveal that using general knowledge LoRAs derived from
diverse languages, including English, French, and German, yields consistent
performance gains in both monolingual and cross-lingual settings across a wide
set of benchmarks. Further experiments on Phi-2 demonstrate how GenKnowSub
generalizes to weaker LLMs. The complete code and data are available at
https://github.com/saharsamr/Modular-LLM.

</details>


### [22] [Semantic Aware Linear Transfer by Recycling Pre-trained Language Models for Cross-lingual Transfer](https://arxiv.org/abs/2505.10945)
*Seungyoon Lee,Seongtae Hong,Hyeonseok Moon,Heuiseok Lim*

Main category: cs.CL

TL;DR: 论文提出了一种名为SALT的新型跨语言迁移技术，通过利用目标语言预训练模型的嵌入来增强大型语言模型的多语言能力，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前的多语言迁移方法通常通过替换词汇表来迁移模型，但由于源模型主要基于英语数据训练，可能导致目标语言表达能力受限。

Method: SALT通过分析源和目标词汇表的重叠相似性，为每个非重叠词汇生成独特的回归线，从而将目标语言预训练模型的嵌入优势迁移到大型语言模型中。

Result: 实验表明，SALT在跨语言理解任务中表现优异，损失更低且收敛更快，同时在不同架构的实验中展示了其可扩展性。

Conclusion: SALT是一种高效的跨语言迁移技术，能够显著提升大型语言模型的多语言能力，并具有广泛的应用潜力。

Abstract: Large Language Models (LLMs) increasingly incorporate multilingual
capabilities, fueling the demand to transfer them into target language-specific
models. However, most approaches, which blend the source model's embedding by
replacing the source vocabulary with the target language-specific vocabulary,
may constrain expressive capacity in the target language since the source model
is predominantly trained on English data. In this paper, we propose Semantic
Aware Linear Transfer (SALT), a novel cross-lingual transfer technique that
recycles embeddings from target language Pre-trained Language Models (PLMs) to
transmit the deep representational strengths of PLM-derived embedding to LLMs.
SALT derives unique regression lines based on the similarity in the overlap of
the source and target vocabularies, to handle each non-overlapping token's
embedding space. Our extensive experiments show that SALT significantly
outperforms other transfer methods and achieves lower loss with accelerating
faster convergence during language adaptation. Notably, SALT obtains remarkable
performance in cross-lingual understanding setups compared to other methods.
Furthermore, we highlight the scalable use of PLMs to enhance the functionality
of contemporary LLMs by conducting experiments with varying architectures.

</details>


### [23] [The Way We Prompt: Conceptual Blending, Neural Dynamics, and Prompt-Induced Transitions in LLMs](https://arxiv.org/abs/2505.10948)
*Makoto Sato*

Main category: cs.CL

TL;DR: 论文通过操作概念融合理论（CBT）研究大型语言模型（LLMs）如何混合和压缩意义，揭示人工与生物认知的异同。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs行为背后的机制，尤其是其表现出的个性和智能感。

Method: 使用基于提示的方法，系统研究提示诱导的转变（PIT）和提示诱导的幻觉（PIH）。

Result: 发现人工与生物认知的结构相似性和差异性，提出提示工程作为科学方法。

Conclusion: 人类与AI协作可作为认知科学的未来原型，提示工程不仅是技术工具，也是探索意义深层结构的科学方法。

Abstract: Large language models (LLMs), inspired by neuroscience, exhibit behaviors
that often evoke a sense of personality and intelligence-yet the mechanisms
behind these effects remain elusive. Here, we operationalize Conceptual
Blending Theory (CBT) as an experimental framework, using prompt-based methods
to reveal how LLMs blend and compress meaning. By systematically investigating
Prompt-Induced Transitions (PIT) and Prompt-Induced Hallucinations (PIH), we
uncover structural parallels and divergences between artificial and biological
cognition. Our approach bridges linguistics, neuroscience, and empirical AI
research, demonstrating that human-AI collaboration can serve as a living
prototype for the future of cognitive science. This work proposes prompt
engineering not just as a technical tool, but as a scientific method for
probing the deep structure of meaning itself.

</details>


### [24] [Survey of End-to-End Multi-Speaker Automatic Speech Recognition for Monaural Audio](https://arxiv.org/abs/2505.10975)
*Xinlu He,Jacob Whitehill*

Main category: cs.CL

TL;DR: 该论文综述了端到端（E2E）多说话人自动语音识别（ASR）的最新进展，包括架构范式、算法改进、长语音处理及基准测试比较，并探讨了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 由于数据稀缺和重叠语音中识别与归属单词的固有困难，单声道多说话人ASR仍具挑战性。E2E架构减少了错误传播并更好地利用语音内容与说话人身份的协同作用，但缺乏全面综述。

Method: 论文系统分类了E2E多说话人ASR的神经方法，分析了SIMO与SISO架构范式、算法改进、长语音处理策略及基准测试比较。

Result: 综述了E2E多说话人ASR的最新进展，并比较了不同方法的性能。

Conclusion: 论文总结了当前挑战，并提出了构建鲁棒且可扩展的多说话人ASR的未来研究方向。

Abstract: Monaural multi-speaker automatic speech recognition (ASR) remains challenging
due to data scarcity and the intrinsic difficulty of recognizing and
attributing words to individual speakers, particularly in overlapping speech.
Recent advances have driven the shift from cascade systems to end-to-end (E2E)
architectures, which reduce error propagation and better exploit the synergy
between speech content and speaker identity. Despite rapid progress in E2E
multi-speaker ASR, the field lacks a comprehensive review of recent
developments. This survey provides a systematic taxonomy of E2E neural
approaches for multi-speaker ASR, highlighting recent advances and comparative
analysis. Specifically, we analyze: (1) architectural paradigms (SIMO vs.~SISO)
for pre-segmented audio, analyzing their distinct characteristics and
trade-offs; (2) recent architectural and algorithmic improvements based on
these two paradigms; (3) extensions to long-form speech, including segmentation
strategy and speaker-consistent hypothesis stitching. Further, we (4) evaluate
and compare methods across standard benchmarks. We conclude with a discussion
of open challenges and future research directions towards building robust and
scalable multi-speaker ASR.

</details>


### [25] [Illusion or Algorithm? Investigating Memorization, Emergence, and Symbolic Processing in In-Context Learning](https://arxiv.org/abs/2505.11004)
*Jingcheng Niu,Subhabrata Dutta,Ahmed Elshabrawy,Harish Tayyar Madabushi,Iryna Gurevych*

Main category: cs.CL

TL;DR: 论文研究了大规模Transformer语言模型中的上下文学习（ICL）机制，通过实验和分析发现ICL既非单纯记忆，也非独立符号算法，而是介于两者之间的能力。


<details>
  <summary>Details</summary>
Motivation: 探讨ICL的本质，澄清其是否仅为数据记忆或独立符号算法，以推动对语言模型能力的理解。

Method: 利用Pythia扩展套件和中间检查点，系统分析ICL在下游任务中的表现，并结合残差流子空间的机制分析。

Result: ICL能力介于记忆和符号算法之间，研究还揭示了训练动态、模型能力和机制可解释性的影响。

Conclusion: 研究深化了对ICL的理解，为模型改进和AI安全实践提供了依据。

Abstract: Large-scale Transformer language models (LMs) trained solely on next-token
prediction with web-scale data can solve a wide range of tasks after seeing
just a few examples. The mechanism behind this capability, known as in-context
learning (ICL), remains both controversial and poorly understood. Some studies
argue that it is merely the result of memorizing vast amounts of data, while
others contend that it reflects a fundamental, symbolic algorithmic development
in LMs. In this work, we introduce a suite of investigative tasks and a novel
method to systematically investigate ICL by leveraging the full Pythia scaling
suite, including interim checkpoints that capture progressively larger amount
of training data. By carefully exploring ICL performance on downstream tasks
and simultaneously conducting a mechanistic analysis of the residual stream's
subspace, we demonstrate that ICL extends beyond mere "memorization" of the
training corpus, yet does not amount to the implementation of an independent
symbolic algorithm. Our results also clarify several aspects of ICL, including
the influence of training dynamics, model capabilities, and elements of
mechanistic interpretability. Overall, our work advances the understanding of
ICL and its implications, offering model developers insights into potential
improvements and providing AI security practitioners with a basis for more
informed guidelines.

</details>


### [26] [Reconstructing Syllable Sequences in Abugida Scripts with Incomplete Inputs](https://arxiv.org/abs/2505.11008)
*Ye Kyaw Thu,Thazin Myint Oo*

Main category: cs.CL

TL;DR: 本文使用Transformer模型研究Abugida语言的音节序列预测，重点关注六种语言。实验表明，辅音序列对准确预测至关重要，而元音序列更具挑战性。模型在部分和掩码音节重建任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 探索Abugida语言的音节序列预测，为文本预测、拼写校正和数据增强等应用提供实用见解。

Method: 使用Transformer模型，从辅音序列、元音序列、部分音节和掩码音节等不完整输入中重建完整音节序列。

Result: 辅音序列在准确预测中起关键作用，元音序列更具挑战性。模型在部分和掩码音节重建任务中表现优异。

Conclusion: 研究推进了对Abugida语言序列预测的理解，并为相关应用提供了实用指导。

Abstract: This paper explores syllable sequence prediction in Abugida languages using
Transformer-based models, focusing on six languages: Bengali, Hindi, Khmer,
Lao, Myanmar, and Thai, from the Asian Language Treebank (ALT) dataset. We
investigate the reconstruction of complete syllable sequences from various
incomplete input types, including consonant sequences, vowel sequences, partial
syllables (with random character deletions), and masked syllables (with fixed
syllable deletions). Our experiments reveal that consonant sequences play a
critical role in accurate syllable prediction, achieving high BLEU scores,
while vowel sequences present a significantly greater challenge. The model
demonstrates robust performance across tasks, particularly in handling partial
and masked syllable reconstruction, with strong results for tasks involving
consonant information and syllable masking. This study advances the
understanding of sequence prediction for Abugida languages and provides
practical insights for applications such as text prediction, spelling
correction, and data augmentation in these scripts.

</details>


### [27] [Review-Instruct: A Review-Driven Multi-Turn Conversations Generation Method for Large Language Models](https://arxiv.org/abs/2505.11010)
*Jiangxu Wu,Cong Wang,TianHuang Su,Jun Yang,Haozhi Lin,Chao Zhang,Ming Peng,Kai Shi,SongPan Yang,BinQing Pan,ZiXian Li,Ni Yang,ZhenYu Yang*

Main category: cs.CL

TL;DR: 提出Review-Instruct框架，通过多角色代理迭代生成高质量多轮对话数据，显著提升LLM在对话任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以同时保证多轮对话数据的多样性和质量，限制了LLM在对话任务中的上下文连贯性。

Method: 提出Review-Instruct框架，通过‘Ask-Respond-Review’迭代过程，结合多个Reviewer的反馈优化指令，生成高质量对话数据。

Result: 在MT-Bench、MMLU-Pro和Auto-Arena上显著提升性能，MMLU-Pro和MT-Bench分别提升2.9%和2%。

Conclusion: Review-Instruct框架通过多代理协作和迭代优化，为大规模生成高质量对话数据提供了有效解决方案。

Abstract: The effectiveness of large language models (LLMs) in conversational AI is
hindered by their reliance on single-turn supervised fine-tuning (SFT) data,
which limits contextual coherence in multi-turn dialogues. Existing methods for
generating multi-turn dialogue data struggle to ensure both diversity and
quality in instructions. To address this, we propose Review-Instruct, a novel
framework that synthesizes multi-turn conversations through an iterative
"Ask-Respond-Review" process involving three agent roles: a Candidate, multiple
Reviewers, and a Chairman. The framework iteratively refines instructions by
incorporating Reviewer feedback, enhancing dialogue diversity and difficulty.
We construct a multi-turn dataset using the Alpaca dataset and fine-tune the
LLaMA2-13B model. Evaluations on MT-Bench, MMLU-Pro, and Auto-Arena demonstrate
significant improvements, achieving absolute gains of 2.9\% on MMLU-Pro and 2\%
on MT-Bench compared to prior state-of-the-art models based on LLaMA2-13B.
Ablation studies confirm the critical role of the Review stage and the use of
multiple Reviewers in boosting instruction diversity and difficulty. Our work
highlights the potential of review-driven, multi-agent frameworks for
generating high-quality conversational data at scale.

</details>


### [28] [StRuCom: A Novel Dataset of Structured Code Comments in Russian](https://arxiv.org/abs/2505.11026)
*Maria Dziuba,Valentin Malykh*

Main category: cs.CL

TL;DR: StRuCom是首个针对俄语代码文档的大规模数据集，通过结合人工编写和合成生成的注释，显著提升了俄语代码注释生成的性能。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习模型在俄语代码注释生成上表现不佳，StRuCom旨在填补这一空白。

Method: 结合俄语GitHub仓库中的人工注释与合成生成注释，并通过自动化验证确保符合多种编程语言标准。

Result: 在Qwen2.5-Coder模型上微调后，chrf++和BERTScore指标显著优于基线模型。

Conclusion: StRuCom为俄语代码注释生成提供了高质量数据集，显著提升了模型性能。

Abstract: Structured code comments in docstring format are essential for code
comprehension and maintenance, but existing machine learning models for their
generation perform poorly for Russian compared to English. To bridge this gap,
we present StRuCom - the first large-scale dataset (153K examples) specifically
designed for Russian code documentation. Unlike machine-translated English
datasets that distort terminology (e.g., technical loanwords vs. literal
translations) and docstring structures, StRuCom combines human-written comments
from Russian GitHub repositories with synthetically generated ones, ensuring
compliance with Python, Java, JavaScript, C#, and Go standards through
automated validation. Fine-tuning Qwen2.5-Coder models (0.5B-7B) on StRuCom
shows statistically significant improvements of chrf++ and BERTScore over
baseline models.

</details>


### [29] [OntoURL: A Benchmark for Evaluating Large Language Models on Symbolic Ontological Understanding, Reasoning and Learning](https://arxiv.org/abs/2505.11031)
*Xiao Zhang,Huiyuan Lai,Qianru Meng,Johan Bos*

Main category: cs.CL

TL;DR: 论文提出OntoURL基准，系统评估大语言模型（LLMs）处理本体知识的能力，发现LLMs在理解和推理学习任务中存在显著差异。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs处理结构化符号知识的能力，填补现有研究的空白。

Method: 提出LLMs本体能力的分类法，并设计OntoURL基准，通过15个任务评估理解、推理和学习能力。

Result: 实验显示LLMs在理解任务中表现良好，但在推理和学习任务中存在明显不足。

Conclusion: OntoURL为LLMs与形式知识表示的整合提供了关键基准，揭示了LLMs处理符号知识的局限性。

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities across
a range of natural language processing tasks, yet their ability to process
structured symbolic knowledge remains underexplored. To address this gap, we
propose a taxonomy of LLMs' ontological capabilities and introduce OntoURL, the
first comprehensive benchmark designed to systematically evaluate LLMs'
proficiency in handling ontologies -- formal, symbolic representations of
domain knowledge through concepts, relationships, and instances. Based on the
proposed taxonomy, OntoURL systematically assesses three dimensions:
understanding, reasoning, and learning through 15 distinct tasks comprising
58,981 questions derived from 40 ontologies across 8 domains. Experiments with
20 open-source LLMs reveal significant performance differences across models,
tasks, and domains, with current LLMs showing proficiency in understanding
ontological knowledge but substantial weaknesses in reasoning and learning
tasks. These findings highlight fundamental limitations in LLMs' capability to
process symbolic knowledge and establish OntoURL as a critical benchmark for
advancing the integration of LLMs with formal knowledge representations.

</details>


### [30] [CAMEO: Collection of Multilingual Emotional Speech Corpora](https://arxiv.org/abs/2505.11051)
*Iwona Christop,Maciej Czajka*

Main category: cs.CL

TL;DR: CAMEO是一个多语言情感语音数据集集合，旨在促进情感识别和其他语音相关任务的研究。


<details>
  <summary>Details</summary>
Motivation: 提供易于访问的数据，确保结果可复现，并为跨语言和情感状态的语音情感识别系统提供标准化基准。

Method: 描述了数据集的选择标准、整理和标准化过程，并提供了多个模型的性能结果。

Result: 数据集、元数据和排行榜已通过Hugging Face平台公开。

Conclusion: CAMEO为语音情感识别研究提供了有价值的资源和标准化基准。

Abstract: This paper presents CAMEO -- a curated collection of multilingual emotional
speech datasets designed to facilitate research in emotion recognition and
other speech-related tasks. The main objectives were to ensure easy access to
the data, to allow reproducibility of the results, and to provide a
standardized benchmark for evaluating speech emotion recognition (SER) systems
across different emotional states and languages. The paper describes the
dataset selection criteria, the curation and normalization process, and
provides performance results for several models. The collection, along with
metadata, and a leaderboard, is publicly available via the Hugging Face
platform.

</details>


### [31] [BLEUBERI: BLEU is a surprisingly effective reward for instruction following](https://arxiv.org/abs/2505.11080)
*Yapei Chang,Yekyung Kim,Michael Krumdick,Amir Zadeh,Chuan Li,Chris Tanner,Mohit Iyyer*

Main category: cs.CL

TL;DR: 论文提出了一种基于BLEU的替代奖励模型方法BLEUBERI，用于LLM对齐任务，证明其效果与复杂奖励模型相当。


<details>
  <summary>Details</summary>
Motivation: 传统奖励模型训练成本高，而高质量合成数据的可用性促使探索更简单的替代方法。

Method: 开发了BLEUBERI方法，结合BLEU指标和GRPO策略优化，直接使用BLEU作为奖励函数。

Result: BLEUBERI在多个基准测试中表现与奖励模型对齐的模型相当，且生成内容更具事实性。

Conclusion: 基于字符串匹配的指标（如BLEU）是奖励模型的有效廉价替代品，适用于对齐任务。

Abstract: Reward models are central to aligning LLMs with human preferences, but they
are costly to train, requiring large-scale human-labeled preference data and
powerful pretrained LLM backbones. Meanwhile, the increasing availability of
high-quality synthetic instruction-following datasets raises the question: can
simpler, reference-based metrics serve as viable alternatives to reward models
during RL-based alignment? In this paper, we show first that BLEU, a basic
string-matching metric, surprisingly matches strong reward models in agreement
with human preferences on general instruction-following datasets. Based on this
insight, we develop BLEUBERI, a method that first identifies challenging
instructions and then applies Group Relative Policy Optimization (GRPO) using
BLEU directly as the reward function. We demonstrate that BLEUBERI-trained
models are competitive with models trained via reward model-guided RL across
four challenging instruction-following benchmarks and three different base
language models. A human evaluation further supports that the quality of
BLEUBERI model outputs is on par with those from reward model-aligned models.
Moreover, BLEUBERI models generate outputs that are more factually grounded
than competing methods. Overall, we show that given access to high-quality
reference outputs (easily obtained via existing instruction-following datasets
or synthetic data generation), string matching-based metrics are cheap yet
effective proxies for reward models during alignment. We release our code and
data at https://github.com/lilakk/BLEUBERI.

</details>


### [32] [Towards Better Evaluation for Generated Patent Claims](https://arxiv.org/abs/2505.11095)
*Lekang Jiang,Pascal A Scherz,Stephan Goetz*

Main category: cs.CL

TL;DR: 论文提出Patent-CE基准和PatClaimEval方法，用于更准确地评估自动生成专利权利要求的质量。


<details>
  <summary>Details</summary>
Motivation: 解决现有自动化评估指标与人类专家评估不一致的问题，降低专利权利要求撰写的门槛。

Method: 引入Patent-CE基准和PatClaimEval多维度评估方法，结合专家标注的五大关键标准。

Result: PatClaimEval在所有测试指标中与人类专家评估的相关性最高。

Conclusion: 研究为自动生成专利权利要求系统的准确评估奠定了基础。

Abstract: Patent claims define the scope of protection and establish the legal
boundaries of an invention. Drafting these claims is a complex and
time-consuming process that usually requires the expertise of skilled patent
attorneys, which can form a large access barrier for many small enterprises. To
solve these challenges, researchers have investigated the use of large language
models (LLMs) for automating patent claim generation. However, existing studies
highlight inconsistencies between automated evaluation metrics and human expert
assessments. To bridge this gap, we introduce Patent-CE, the first
comprehensive benchmark for evaluating patent claims. Patent-CE includes
comparative claim evaluations annotated by patent experts, focusing on five key
criteria: feature completeness, conceptual clarity, terminology consistency,
logical linkage, and overall quality. Additionally, we propose PatClaimEval, a
novel multi-dimensional evaluation method specifically designed for patent
claims. Our experiments demonstrate that PatClaimEval achieves the highest
correlation with human expert evaluations across all assessment criteria among
all tested metrics. This research provides the groundwork for more accurate
evaluations of automated patent claim generation systems.

</details>


### [33] [Scaling Reasoning can Improve Factuality in Large Language Models](https://arxiv.org/abs/2505.11140)
*Mike Zhang,Johannes Bjerva,Russa Biswas*

Main category: cs.CL

TL;DR: 研究表明，通过增加推理链长度和计算资源，大型语言模型（LLM）在数学推理任务中表现提升，但事实准确性是否因此提高尚不明确。本文在开放域问答任务中验证了这一点，通过引入知识图谱信息优化推理链，实验表明小模型在事实准确性上有显著提升，且增加计算资源可进一步提升2-8%。


<details>
  <summary>Details</summary>
Motivation: 探讨推理链长度和计算资源是否能在非数学任务（如开放域问答）中提升事实准确性。

Method: 从大型推理模型（QwQ-32B和DeepSeek-R1-671B）中提取推理链，结合知识图谱信息优化，并在多种模型上微调，实验覆盖6个数据集和168次运行。

Result: 小模型在事实准确性上优于原始指令调优模型，增加计算资源可提升2-8%的准确性。

Conclusion: 推理链优化和计算资源扩展能有效提升开放域问答的事实准确性，实验结果为后续研究提供了支持。

Abstract: Recent studies on large language model (LLM) reasoning capabilities have
demonstrated promising improvements in model performance by leveraging a
lengthy thinking process and additional computational resources during
inference, primarily in tasks involving mathematical reasoning (Muennighoff et
al., 2025). However, it remains uncertain if longer reasoning chains inherently
enhance factual accuracy, particularly beyond mathematical contexts. In this
work, we thoroughly examine LLM reasoning within complex open-domain
question-answering (QA) scenarios. We initially distill reasoning traces from
advanced, large-scale reasoning models (QwQ-32B and DeepSeek-R1-671B), then
fine-tune a variety of models ranging from smaller, instruction-tuned variants
to larger architectures based on Qwen2.5. To enrich reasoning traces, we
introduce factual information from knowledge graphs in the form of paths into
our reasoning traces. Our experimental setup includes four baseline approaches
and six different instruction-tuned models evaluated across a benchmark of six
datasets, encompassing over 22.6K questions. Overall, we carry out 168
experimental runs and analyze approximately 1.7 million reasoning traces. Our
findings indicate that, within a single run, smaller reasoning models achieve
noticeable improvements in factual accuracy compared to their original
instruction-tuned counterparts. Moreover, our analysis demonstrates that adding
test-time compute and token budgets factual accuracy consistently improves by
2-8%, further confirming the effectiveness of test-time scaling for enhancing
performance and consequently improving reasoning accuracy in open-domain QA
tasks. We release all the experimental artifacts for further research.

</details>


### [34] [SoLoPO: Unlocking Long-Context Capabilities in LLMs via Short-to-Long Preference Optimization](https://arxiv.org/abs/2505.11166)
*Huashan Sun,Shengyi Liao,Yansen Han,Yu Bai,Yang Gao,Cheng Fu,Weizhou Shen,Fanqi Wan,Ming Yan,Ji Zhang,Fei Huang*

Main category: cs.CL

TL;DR: 论文提出了一种名为SoLoPO的框架，通过将长上下文偏好优化分解为短上下文偏好优化和短到长奖励对齐，解决了大语言模型在长上下文信息利用中的问题。


<details>
  <summary>Details</summary>
Motivation: 尽管预训练技术有所进步，大语言模型在利用真实世界长上下文信息时仍面临挑战，主要由于数据质量、训练效率不足以及缺乏优化目标设计。

Method: SoLoPO框架包含两部分：短上下文偏好优化（PO）和短到长奖励对齐（SoLo-RA）。前者通过短上下文样本增强模型知识利用能力，后者确保短长上下文奖励一致性。

Result: 实验表明，SoLoPO显著提升了主流偏好优化算法的长度和领域泛化能力，同时提高了计算和内存效率。

Conclusion: SoLoPO框架有效解决了长上下文对齐问题，提升了模型在长上下文任务中的表现和效率。

Abstract: Despite advances in pretraining with extended context lengths, large language
models (LLMs) still face challenges in effectively utilizing real-world
long-context information, primarily due to insufficient long-context alignment
caused by data quality issues, training inefficiencies, and the lack of
well-designed optimization objectives. To address these limitations, we propose
a framework named $\textbf{S}$h$\textbf{o}$rt-to-$\textbf{Lo}$ng
$\textbf{P}$reference $\textbf{O}$ptimization ($\textbf{SoLoPO}$), decoupling
long-context preference optimization (PO) into two components: short-context PO
and short-to-long reward alignment (SoLo-RA), supported by both theoretical and
empirical evidence. Specifically, short-context PO leverages preference pairs
sampled from short contexts to enhance the model's contextual knowledge
utilization ability. Meanwhile, SoLo-RA explicitly encourages reward score
consistency utilization for the responses when conditioned on both short and
long contexts that contain identical task-relevant information. This
facilitates transferring the model's ability to handle short contexts into
long-context scenarios. SoLoPO is compatible with mainstream preference
optimization algorithms, while substantially improving the efficiency of data
construction and training processes. Experimental results show that SoLoPO
enhances all these algorithms with respect to stronger length and domain
generalization abilities across various long-context benchmarks, while
achieving notable improvements in both computational and memory efficiency.

</details>


### [35] [Low-Resource Language Processing: An OCR-Driven Summarization and Translation Pipeline](https://arxiv.org/abs/2505.11177)
*Hrishit Madhavi,Jacob Cherian,Yuvraj Khamkar,Dhananjay Bhagat*

Main category: cs.CL

TL;DR: 本文介绍了一个端到端的多语言信息提取与处理系统，支持从图像文档中提取文本并进行跨语言翻译、摘要生成、情感分析等功能。


<details>
  <summary>Details</summary>
Motivation: 解决多语言环境下图像文档信息提取与理解的障碍，提升跨语言信息获取的便捷性。

Method: 结合OCR（Tesseract）、大语言模型API（Gemini）进行文本提取、翻译与摘要，辅以情感分析（TensorFlow）、主题分类（Transformers）和日期提取（Regex）。

Result: 系统通过Gradio界面实现，成功展示了在多语言环境中提升图像文档信息访问的实际应用。

Conclusion: 该研究为跨语言信息处理提供了实用工具，有效缩小了语言差距。

Abstract: This paper presents an end-to-end suite for multilingual information
extraction and processing from image-based documents. The system uses Optical
Character Recognition (Tesseract) to extract text in languages such as English,
Hindi, and Tamil, and then a pipeline involving large language model APIs
(Gemini) for cross-lingual translation, abstractive summarization, and
re-translation into a target language. Additional modules add sentiment
analysis (TensorFlow), topic classification (Transformers), and date extraction
(Regex) for better document comprehension. Made available in an accessible
Gradio interface, the current research shows a real-world application of
libraries, models, and APIs to close the language gap and enhance access to
information in image media across different linguistic environments

</details>


### [36] [NoPE: The Counting Power of Transformers with No Positional Encodings](https://arxiv.org/abs/2505.11199)
*Chris Köcher,Alexander Kozachinskiy,Anthony Widjaja Lin,Marco Sälzer,Georg Zetzsche*

Main category: cs.CL

TL;DR: NoPE-transformers（无位置编码的Transformer）在平均硬注意力机制下表现出惊人的表达能力，能够表达非负整数解的多变量多项式方程（Diophantine方程）的语言，甚至对应半代数集。其分析问题不可判定，且无法表达简单的PARITY性质。


<details>
  <summary>Details</summary>
Motivation: 研究无位置编码的Transformer在平均硬注意力机制下的表达能力，探索其是否能表达复杂的计数语言，并与现有模型（如简化计数器机和Petri网）进行比较。

Method: 通过理论分析，将NoPE-transformers的表达能力与半代数集对应，并研究其是否能表达特定计数语言（如PARITY）。

Result: NoPE-transformers能表达半代数集对应的语言，其分析问题不可判定，且无法表达PARITY性质。同时，发现某些语言即使有位置编码也无法表达。

Conclusion: NoPE-transformers在平均硬注意力机制下具有强大的表达能力，但其分析问题不可判定，且存在局限性（如无法表达PARITY）。

Abstract: Positional Encodings (PEs) seem to be indispensable for ensuring
expressiveness of transformers; without them attention transformers reduce to a
bag-of-word model. NoPE-transformers (i.e. with No PEs) with unique hard
attention mechanisms were very recently shown to only be able to express
regular languages, i.e., with limited counting ability. This paper shows that,
with average hard attention mechanisms, NoPE-transformers are still
surprisingly expressive: they can express counting languages corresponding to
nonnegative integer solutions to multivariate polynomial equations (i.e.
Diophantine equations), reasoning about which is well-known to be undecidable.
In fact, we provide a precise characterization of languages expressible by
Average Hard Attention NoPE-Transformers (NoPE-AHATs): they correspond
precisely to what we call \emph{semi-algebraic sets}, i.e., finite unions of
sets of nonnegative integer solutions to systems of multivariate polynomial
inequations. We obtain several interesting consequences of our
characterization. Firstly, NoPE-transformers can express counting properties
that are far more complex than established models like simplified counter
machines and Petri nets, but cannot express a very simple counting property of
PARITY. Secondly, the problem of analyzing NoPE-transformers is undecidable,
e.g., whether a given NoPE transformer classifies all input strings in one
class. To complement our results, we exhibit a counting language that is not
expressible by average hard attention transformers even with arbitrary PEs but
is expressible in the circuit complexity class TC$^0$, answering an open
problem.

</details>


### [37] [HAPO: Training Language Models to Reason Concisely via History-Aware Policy Optimization](https://arxiv.org/abs/2505.11225)
*Chengyu Huang,Zhengxin Zhang,Claire Cardie*

Main category: cs.CL

TL;DR: HAPO（历史感知策略优化）通过记录历史状态和设计长度奖励函数，优化LLM的推理能力，使其生成更简洁且准确的输出。


<details>
  <summary>Details</summary>
Motivation: 现有方法未利用训练中同一问题的历史信息，限制了生成简洁解决方案的能力。

Method: HAPO记录历史状态（如最小正确响应长度），并基于此设计长度奖励函数，结合正确性奖励共同优化。

Result: 在多个数学基准测试中，HAPO将响应长度减少33-59%，准确率仅下降2-5%。

Conclusion: HAPO有效提升了LLM的简洁推理能力，同时保持了较高的准确性。

Abstract: While scaling the length of responses at test-time has been shown to markedly
improve the reasoning abilities and performance of large language models
(LLMs), it often results in verbose outputs and increases inference cost. Prior
approaches for efficient test-time scaling, typically using universal budget
constraints or query-level length optimization, do not leverage historical
information from previous encounters with the same problem during training. We
hypothesize that this limits their ability to progressively make solutions more
concise over time. To address this, we present History-Aware Policy
Optimization (HAPO), which keeps track of a history state (e.g., the minimum
length over previously generated correct responses) for each problem. HAPO
employs a novel length reward function based on this history state to
incentivize the discovery of correct solutions that are more concise than those
previously found. Crucially, this reward structure avoids overly penalizing
shorter incorrect responses with the goal of facilitating exploration towards
more efficient solutions. By combining this length reward with a correctness
reward, HAPO jointly optimizes for correctness and efficiency. We use HAPO to
train DeepSeek-R1-Distill-Qwen-1.5B, DeepScaleR-1.5B-Preview, and
Qwen-2.5-1.5B-Instruct, and evaluate HAPO on several math benchmarks that span
various difficulty levels. Experiment results demonstrate that HAPO effectively
induces LLMs' concise reasoning abilities, producing length reductions of
33-59% with accuracy drops of only 2-5%.

</details>


### [38] [Semantic Caching of Contextual Summaries for Efficient Question-Answering with Language Models](https://arxiv.org/abs/2505.11271)
*Camille Couturier,Spyros Mastorakis,Haiying Shen,Saravan Rajmohan,Victor Rühle*

Main category: cs.CL

TL;DR: 本文提出了一种语义缓存方法，用于存储和重用中间上下文摘要，以减少LLM问答工作流中的冗余计算。


<details>
  <summary>Details</summary>
Motivation: 在分布式系统中处理长上下文会带来高计算开销、内存使用和网络带宽问题，需要一种高效的信息重用方法。

Method: 采用语义缓存技术存储和重用中间上下文摘要，优化LLM问答工作流。

Result: 在多个数据集上测试表明，该方法可减少50-60%的冗余计算，同时保持与完整文档处理相当的答案准确性。

Conclusion: 该方法在计算成本和响应质量之间取得了平衡，适用于实时AI助手。

Abstract: Large Language Models (LLMs) are increasingly deployed across edge and cloud
platforms for real-time question-answering and retrieval-augmented generation.
However, processing lengthy contexts in distributed systems incurs high
computational overhead, memory usage, and network bandwidth. This paper
introduces a novel semantic caching approach for storing and reusing
intermediate contextual summaries, enabling efficient information reuse across
similar queries in LLM-based QA workflows. Our method reduces redundant
computations by up to 50-60% while maintaining answer accuracy comparable to
full document processing, as demonstrated on NaturalQuestions, TriviaQA, and a
synthetic ArXiv dataset. This approach balances computational cost and response
quality, critical for real-time AI assistants.

</details>


### [39] [Search and Refine During Think: Autonomous Retrieval-Augmented Reasoning of LLMs](https://arxiv.org/abs/2505.11277)
*Yaorui Shi,Shihan Li,Chang Wu,Zhiyuan Liu,Junfeng Fang,Hengxing Cai,An Zhang,Xiang Wang*

Main category: cs.CL

TL;DR: AutoRefine是一种基于强化学习的后训练框架，通过“搜索-精炼-思考”范式提升大型语言模型的检索增强推理能力，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的知识储备有限，现有检索方法常引入无关或噪声信息，影响推理准确性。

Method: 提出AutoRefine框架，结合强化学习，引入知识精炼步骤和特定检索奖励，优化检索和推理过程。

Result: 在单跳和多跳问答基准测试中表现优异，尤其在复杂推理场景中效果显著。

Conclusion: AutoRefine通过高质量搜索和证据合成，提升了检索增强推理的准确性和效率。

Abstract: Large language models have demonstrated impressive reasoning capabilities but
are inherently limited by their knowledge reservoir. Retrieval-augmented
reasoning mitigates this limitation by allowing LLMs to query external
resources, but existing methods often retrieve irrelevant or noisy information,
hindering accurate reasoning. In this paper, we propose AutoRefine, a
reinforcement learning post-training framework that adopts a new
``search-and-refine-during-think'' paradigm. AutoRefine introduces explicit
knowledge refinement steps between successive search calls, enabling the model
to iteratively filter, distill, and organize evidence before generating an
answer. Furthermore, we incorporate tailored retrieval-specific rewards
alongside answer correctness rewards using group relative policy optimization.
Experiments on single-hop and multi-hop QA benchmarks demonstrate that
AutoRefine significantly outperforms existing approaches, particularly in
complex, multi-hop reasoning scenarios. Detailed analysis shows that AutoRefine
issues frequent, higher-quality searches and synthesizes evidence effectively.

</details>


### [40] [Temporal fine-tuning for early risk detection](https://arxiv.org/abs/2505.11280)
*Horacio Thompson,Esaú Villatoro-Tello,Manuel Montes-y-Gómez,Marcelo Errecalde*

Main category: cs.CL

TL;DR: 该论文提出了一种名为“时间微调”的新策略，用于优化基于Transformer的模型，以在早期风险检测（ERD）中同时考虑分类精度和检测延迟。


<details>
  <summary>Details</summary>
Motivation: 早期风险检测（ERD）需要快速准确地识别面临社会和健康问题的用户，但传统分类指标可能不足以满足需求，尤其是在关键场景中。

Method: 采用时间微调策略，将时间信息显式地融入学习过程，分析用户的完整发帖历史，并使用时间指标评估训练性能。

Result: 在西班牙语的抑郁和饮食障碍任务中，该方法取得了与MentalRiskES 2023最佳模型竞争的结果，优化了决策的上下文和时间进展。

Conclusion: 通过时间微调，Transformer模型能够将精度和速度结合为一个单一目标，有效解决ERD问题。

Abstract: Early Risk Detection (ERD) on the Web aims to identify promptly users facing
social and health issues. Users are analyzed post-by-post, and it is necessary
to guarantee correct and quick answers, which is particularly challenging in
critical scenarios. ERD involves optimizing classification precision and
minimizing detection delay. Standard classification metrics may not suffice,
resorting to specific metrics such as ERDE(theta) that explicitly consider
precision and delay. The current research focuses on applying a multi-objective
approach, prioritizing classification performance and establishing a separate
criterion for decision time. In this work, we propose a completely different
strategy, temporal fine-tuning, which allows tuning transformer-based models by
explicitly incorporating time within the learning process. Our method allows us
to analyze complete user post histories, tune models considering different
contexts, and evaluate training performance using temporal metrics. We
evaluated our proposal in the depression and eating disorders tasks for the
Spanish language, achieving competitive results compared to the best models of
MentalRiskES 2023. We found that temporal fine-tuning optimized decisions
considering context and time progress. In this way, by properly taking
advantage of the power of transformers, it is possible to address ERD by
combining precision and speed as a single objective.

</details>


### [41] [Probing Subphonemes in Morphology Models](https://arxiv.org/abs/2505.11297)
*Gal Astrach,Yuval Pinter*

Main category: cs.CL

TL;DR: Transformers在形态变化任务中表现优异，但在跨语言和形态规则泛化能力上有限。研究通过语言无关的探测方法，分析其在音位和次音位层面的隐式现象捕捉能力。


<details>
  <summary>Details</summary>
Motivation: 探讨Transformers在音位和次音位层面捕捉隐式现象的能力，以解释其在跨语言形态任务中的泛化限制。

Method: 提出语言无关的探测方法，分析七种形态多样语言中音位嵌入和编码器对音系特征的捕捉能力。

Result: 局部音系特征（如土耳其语的词尾浊音清化）在音位嵌入中表现良好，而长距离依赖（如元音和谐）在编码器中更优。

Conclusion: 研究为形态模型的训练策略提供了实证依据，强调了次音位特征获取的重要性。

Abstract: Transformers have achieved state-of-the-art performance in morphological
inflection tasks, yet their ability to generalize across languages and
morphological rules remains limited. One possible explanation for this behavior
can be the degree to which these models are able to capture implicit phenomena
at the phonological and subphonemic levels. We introduce a language-agnostic
probing method to investigate phonological feature encoding in transformers
trained directly on phonemes, and perform it across seven morphologically
diverse languages. We show that phonological features which are local, such as
final-obstruent devoicing in Turkish, are captured well in phoneme embeddings,
whereas long-distance dependencies like vowel harmony are better represented in
the transformer's encoder. Finally, we discuss how these findings inform
empirical strategies for training morphological models, particularly regarding
the role of subphonemic feature acquisition.

</details>


### [42] [XtraGPT: LLMs for Human-AI Collaboration on Controllable Academic Paper Revision](https://arxiv.org/abs/2505.11336)
*Nuo Chen,Andre Lin HuiKai,Jiaying Wu,Junyi Hou,Zining Zhang,Qian Wang,Xidong Wang,Bingsheng He*

Main category: cs.CL

TL;DR: 论文提出了一种人机协作框架XtraGPT，用于学术论文修订，解决了现有大语言模型在高质量科学写作支持上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在支持高质量科学写作方面能力有限，无法满足研究沟通的复杂需求，如跨章节的概念连贯性。学术写作本质上是迭代和修订驱动的，而现有直接提示范式未能很好支持这一过程。

Method: 首先构建了一个包含7,040篇顶级会议论文的数据集，标注了140,000多个反映实际章节级科学修订的指令-响应对。基于此数据集，开发了XtraGPT，一套开源大语言模型，提供上下文感知、指令引导的写作辅助。

Result: 实验表明，XtraGPT显著优于同规模基线模型，接近专有系统的质量。自动偏好评估和人工评估均证实了模型在改进科学草稿方面的有效性。

Conclusion: XtraGPT通过人机协作框架和上下文感知的指令引导，显著提升了学术论文修订的质量，填补了现有大语言模型在科学写作支持上的不足。

Abstract: Despite the growing adoption of large language models (LLMs) in academic
workflows, their capabilities remain limited when it comes to supporting
high-quality scientific writing. Most existing systems are designed for
general-purpose scientific text generation and fail to meet the sophisticated
demands of research communication beyond surface-level polishing, such as
conceptual coherence across sections. Furthermore, academic writing is
inherently iterative and revision-driven, a process not well supported by
direct prompting-based paradigms. To address these scenarios, we propose a
human-AI collaboration framework for academic paper revision. We first
introduce a comprehensive dataset of 7,040 research papers from top-tier venues
annotated with over 140,000 instruction-response pairs that reflect realistic,
section-level scientific revisions. Building on the dataset, we develop
XtraGPT, the first suite of open-source LLMs, designed to provide
context-aware, instruction-guided writing assistance, ranging from 1.5B to 14B
parameters. Extensive experiments validate that XtraGPT significantly
outperforms same-scale baselines and approaches the quality of proprietary
systems. Both automated preference assessments and human evaluations confirm
the effectiveness of our models in improving scientific drafts.

</details>


### [43] [Benchmarking Critical Questions Generation: A Challenging Reasoning Task for Large Language Models](https://arxiv.org/abs/2505.11341)
*Banca Calvo Figueras,Rodrigo Agerri*

Main category: cs.CL

TL;DR: 论文提出了一种全面的方法来支持关键问题生成（CQs-Gen）任务的开发和基准测试，包括构建首个大规模手动标注数据集，并研究自动评估方法。


<details>
  <summary>Details</summary>
Motivation: 促进批判性思维，通过生成揭示假设和挑战论证推理的问题，但缺乏合适的数据集和自动评估标准阻碍了进展。

Method: 构建大规模手动标注数据集，研究自动评估方法，并采用基于大型语言模型（LLMs）的参考技术。

Result: 零样本评估11个LLMs，建立了强基线，展示了任务的难度。

Conclusion: 提供数据、代码和公开排行榜，鼓励进一步研究模型性能及CQs-Gen在自动推理和人类批判性思维中的实际应用。

Abstract: The task of Critical Questions Generation (CQs-Gen) aims to foster critical
thinking by enabling systems to generate questions that expose assumptions and
challenge the reasoning in arguments. Despite growing interest in this area,
progress has been hindered by the lack of suitable datasets and automatic
evaluation standards. This work presents a comprehensive approach to support
the development and benchmarking of systems for this task. We construct the
first large-scale manually-annotated dataset. We also investigate automatic
evaluation methods and identify a reference-based technique using large
language models (LLMs) as the strategy that best correlates with human
judgments. Our zero-shot evaluation of 11 LLMs establishes a strong baseline
while showcasing the difficulty of the task. Data, code, and a public
leaderboard are provided to encourage further research not only in terms of
model performance, but also to explore the practical benefits of CQs-Gen for
both automated reasoning and human critical thinking.

</details>


### [44] [LegoSLM: Connecting LLM with Speech Encoder using CTC Posteriors](https://arxiv.org/abs/2505.11352)
*Rao Ma,Tongzhou Chen,Kartik Audhkhasi,Bhuvana Ramabhadran*

Main category: cs.CL

TL;DR: LegoSLM提出了一种新范式，通过ASR后验矩阵连接语音编码器和LLM，显著提升了ASR和语音翻译任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在结合语音编码器和LLM时存在性能不佳或灵活性不足的问题。

Method: 利用CTC后验矩阵重构伪音频嵌入，并将其与LLM的文本嵌入拼接。

Result: 在8个MLS测试集上，平均WERR达到49%，且模型具有模块化和零样本适应能力。

Conclusion: LegoSLM方法在性能和灵活性上均优于现有方法，且通过温度控制实现了领域适应。

Abstract: Recently, large-scale pre-trained speech encoders and Large Language Models
(LLMs) have been released, which show state-of-the-art performance on a range
of spoken language processing tasks including Automatic Speech Recognition
(ASR). To effectively combine both models for better performance, continuous
speech prompts, and ASR error correction have been adopted. However, these
methods are prone to suboptimal performance or are inflexible. In this paper,
we propose a new paradigm, LegoSLM, that bridges speech encoders and LLMs using
the ASR posterior matrices. The speech encoder is trained to generate
Connectionist Temporal Classification (CTC) posteriors over the LLM vocabulary,
which are used to reconstruct pseudo-audio embeddings by computing a weighted
sum of the LLM input embeddings. These embeddings are concatenated with text
embeddings in the LLM input space. Using the well-performing USM and Gemma
models as an example, we demonstrate that our proposed LegoSLM method yields
good performance on both ASR and speech translation tasks. By connecting USM
with Gemma models, we can get an average of 49% WERR over the USM-CTC baseline
on 8 MLS testsets. The trained model also exhibits modularity in a range of
settings -- after fine-tuning the Gemma model weights, the speech encoder can
be switched and combined with the LLM in a zero-shot fashion. Additionally, we
propose to control the decode-time influence of the USM and LLM using a softmax
temperature, which shows effectiveness in domain adaptation.

</details>


### [45] [Large Language Models for Cancer Communication: Evaluating Linguistic Quality, Safety, and Accessibility in Generative AI](https://arxiv.org/abs/2505.10472)
*Agnik Saha,Victoria Churchill,Anny D. Rodriguez,Ugur Kursuncu,Muhammed Y. Idris*

Main category: cs.CL

TL;DR: 研究评估了通用和医疗专用大语言模型（LLMs）在生成准确、安全且易理解的癌症信息方面的表现，发现通用模型在语言质量和情感表达上更优，而医疗模型在信息可及性上更强，但安全性较差。


<details>
  <summary>Details</summary>
Motivation: 乳腺癌和宫颈癌的公共认知存在显著差距，可能导致诊断延迟和治疗不足，因此需要评估LLMs在生成癌症相关信息方面的能力。

Method: 采用混合方法评估框架，结合定量指标、定性专家评分及统计分析方法（Welch's ANOVA, Games-Howell, Hedges' g），评估了五种通用和三种医疗LLMs。

Result: 通用LLMs在语言质量和情感表达上表现更好，医疗LLMs在信息可及性上更优，但存在更高的潜在危害、毒性和偏见。

Conclusion: 研究揭示了领域知识与安全性之间的权衡，强调了针对性改进模型设计的必要性，以减少危害和偏见，提升安全性和情感表达。

Abstract: Effective communication about breast and cervical cancers remains a
persistent health challenge, with significant gaps in public understanding of
cancer prevention, screening, and treatment, potentially leading to delayed
diagnoses and inadequate treatments. This study evaluates the capabilities and
limitations of Large Language Models (LLMs) in generating accurate, safe, and
accessible cancer-related information to support patient understanding. We
evaluated five general-purpose and three medical LLMs using a mixed-methods
evaluation framework across linguistic quality, safety and trustworthiness, and
communication accessibility and affectiveness. Our approach utilized
quantitative metrics, qualitative expert ratings, and statistical analysis
using Welch's ANOVA, Games-Howell, and Hedges' g. Our results show that
general-purpose LLMs produced outputs of higher linguistic quality and
affectiveness, while medical LLMs demonstrate greater communication
accessibility. However, medical LLMs tend to exhibit higher levels of potential
harm, toxicity, and bias, reducing their performance in safety and
trustworthiness. Our findings indicate a duality between domain-specific
knowledge and safety in health communications. The results highlight the need
for intentional model design with targeted improvements, particularly in
mitigating harm and bias, and improving safety and affectiveness. This study
provides a comprehensive evaluation of LLMs for cancer communication, offering
critical insights for improving AI-generated health content and informing
future development of accurate, safe, and accessible digital health tools.

</details>


### [46] [GuideBench: Benchmarking Domain-Oriented Guideline Following for LLM Agents](https://arxiv.org/abs/2505.11368)
*Lingxiao Diao,Xinyue Xu,Wanxuan Sun,Cheng Yang,Zhuosheng Zhang*

Main category: cs.CL

TL;DR: GuideBench是一个用于评估大语言模型（LLMs）遵循领域导向指南能力的综合基准，涵盖规则多样性、规则更新鲁棒性和人类偏好对齐三个方面。


<details>
  <summary>Details</summary>
Motivation: LLMs越来越多地被部署为领域导向代理，但其遵循领域导向指南的能力缺乏全面评估，阻碍了进一步发展和应用。

Method: 提出GuideBench基准，评估LLMs在规则多样性、规则更新鲁棒性和人类偏好对齐三个方面的表现。

Result: 实验结果表明，LLMs在遵循领域导向指南方面仍有显著改进空间。

Conclusion: GuideBench为评估和改进LLMs的领域导向指南遵循能力提供了重要工具。

Abstract: Large language models (LLMs) have been widely deployed as autonomous agents
capable of following user instructions and making decisions in real-world
applications. Previous studies have made notable progress in benchmarking the
instruction following capabilities of LLMs in general domains, with a primary
focus on their inherent commonsense knowledge. Recently, LLMs have been
increasingly deployed as domain-oriented agents, which rely on domain-oriented
guidelines that may conflict with their commonsense knowledge. These guidelines
exhibit two key characteristics: they consist of a wide range of
domain-oriented rules and are subject to frequent updates. Despite these
challenges, the absence of comprehensive benchmarks for evaluating the
domain-oriented guideline following capabilities of LLMs presents a significant
obstacle to their effective assessment and further development. In this paper,
we introduce GuideBench, a comprehensive benchmark designed to evaluate
guideline following performance of LLMs. GuideBench evaluates LLMs on three
critical aspects: (i) adherence to diverse rules, (ii) robustness to rule
updates, and (iii) alignment with human preferences. Experimental results on a
range of LLMs indicate substantial opportunities for improving their ability to
follow domain-oriented guidelines.

</details>


### [47] [A computational system to handle the orthographic layer of tajwid in contemporary Quranic Orthography](https://arxiv.org/abs/2505.11379)
*Alicia González Martínez*

Main category: cs.CL

TL;DR: 论文探讨了古兰经现代正字法（CQO）中tajwid规则的系统性，开发了一个Python模块用于处理tajwid层，并提出了利用开罗古兰经文本对齐和比较手稿的框架。


<details>
  <summary>Details</summary>
Motivation: 研究早期伊斯兰时期古兰经的口头传统及其正字法系统，特别是tajwid规则，以理解其语音和韵律过程。

Method: 使用精确编码的数字版古兰经文本，开发Python模块来添加或移除tajwid的正字层，并分析其系统性。

Result: tajwid规则的系统性为研究古兰经的语音和韵律提供了精确依据，开罗古兰经文本可作为对齐和比较手稿的关键。

Conclusion: 通过将文本相互映射，可以深入研究古兰经中附加符号系统的本质，为阿拉伯文字研究提供强大工具。

Abstract: Contemporary Quranic Orthography (CQO) relies on a precise system of phonetic
notation that can be traced back to the early stages of Islam, when the Quran
was mainly oral in nature and the first written renderings of it served as
memory aids for this oral tradition. The early systems of diacritical marks
created on top of the Quranic Consonantal Text (QCT) motivated the creation and
further development of a fine-grained system of phonetic notation that
represented tajwid-the rules of recitation. We explored the systematicity of
the rules of tajwid, as they are encountered in the Cairo Quran, using a fully
and accurately encoded digital edition of the Quranic text. For this purpose,
we developed a python module that can remove or add the orthographic layer of
tajwid from a Quranic text in CQO. The interesting characteristic of these two
sets of rules is that they address the complete Quranic text of the Cairo
Quran, so they can be used as precise witnesses to study its phonetic and
prosodic processes. From a computational point of view, the text of the Cairo
Quran can be used as a linchpin to align and compare Quranic manuscripts, due
to its richness and completeness. This will let us create a very powerful
framework to work with the Arabic script, not just within an isolated text, but
automatically exploring a specific textual phenomenon in other connected
manuscripts. Having all the texts mapped among each other can serve as a
powerful tool to study the nature of the notation systems of diacritics added
to the consonantal skeleton.

</details>


### [48] [CARES: Comprehensive Evaluation of Safety and Adversarial Robustness in Medical LLMs](https://arxiv.org/abs/2505.11413)
*Sijia Chen,Xiaomin Li,Mengxue Zhang,Eric Hanchen Jiang,Qingcheng Zeng,Chen-Hsiang Yu*

Main category: cs.CL

TL;DR: CARES是一个用于评估医疗领域大型语言模型安全性的基准测试，涵盖多种医疗安全原则、危害级别和提示风格，揭示模型在对抗性攻击下的脆弱性，并提出缓解策略。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在医疗领域的广泛应用，其安全性、对齐性和对抗性操纵的脆弱性成为关键问题。现有基准测试缺乏临床特异性、分级危害和对抗性攻击覆盖。

Method: CARES包含18,000多个提示，覆盖八项医疗安全原则、四个危害级别和四种提示风格。提出三向响应评估协议（接受、谨慎、拒绝）和细粒度安全评分指标。

Result: 分析显示，许多先进模型对微调的有害提示仍易受攻击，同时对非典型安全查询过度拒绝。提出使用轻量级分类器检测攻击并通过提醒调节改善模型行为。

Conclusion: CARES为医疗领域大型语言模型在对抗性和模糊条件下的安全性测试与改进提供了严格框架。

Abstract: Large language models (LLMs) are increasingly deployed in medical contexts,
raising critical concerns about safety, alignment, and susceptibility to
adversarial manipulation. While prior benchmarks assess model refusal
capabilities for harmful prompts, they often lack clinical specificity, graded
harmfulness levels, and coverage of jailbreak-style attacks. We introduce CARES
(Clinical Adversarial Robustness and Evaluation of Safety), a benchmark for
evaluating LLM safety in healthcare. CARES includes over 18,000 prompts
spanning eight medical safety principles, four harm levels, and four prompting
styles: direct, indirect, obfuscated, and role-play, to simulate both malicious
and benign use cases. We propose a three-way response evaluation protocol
(Accept, Caution, Refuse) and a fine-grained Safety Score metric to assess
model behavior. Our analysis reveals that many state-of-the-art LLMs remain
vulnerable to jailbreaks that subtly rephrase harmful prompts, while also
over-refusing safe but atypically phrased queries. Finally, we propose a
mitigation strategy using a lightweight classifier to detect jailbreak attempts
and steer models toward safer behavior via reminder-based conditioning. CARES
provides a rigorous framework for testing and improving medical LLM safety
under adversarial and ambiguous conditions.

</details>


### [49] [Towards Cultural Bridge by Bahnaric-Vietnamese Translation Using Transfer Learning of Sequence-To-Sequence Pre-training Language Model](https://arxiv.org/abs/2505.11421)
*Phan Tran Minh Dat,Vo Hoang Nhat Khang,Quan Thanh Tho*

Main category: cs.CL

TL;DR: 该研究探索了通过迁移学习实现Bahnaric-Vietnamese翻译的方法，以解决资源不足的问题，并验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 旨在通过翻译促进越南两个民族之间的文化交流，但面临Bahnaric语资源匮乏的挑战。

Method: 采用基于序列到序列的预训练语言模型的迁移学习方法，结合数据增强和启发式方法优化翻译。

Result: 方法在Bahnaric-Vietnamese翻译中表现出高效性，有助于语言保护和民族间理解。

Conclusion: 研究为资源不平衡的语言对翻译提供了有效解决方案，并支持文化桥梁的构建。

Abstract: This work explores the journey towards achieving Bahnaric-Vietnamese
translation for the sake of culturally bridging the two ethnic groups in
Vietnam. However, translating from Bahnaric to Vietnamese also encounters some
difficulties. The most prominent challenge is the lack of available original
Bahnaric resources source language, including vocabulary, grammar, dialogue
patterns and bilingual corpus, which hinders the data collection process for
training. To address this, we leverage a transfer learning approach using
sequence-to-sequence pre-training language model. First of all, we leverage a
pre-trained Vietnamese language model to capture the characteristics of this
language. Especially, to further serve the purpose of machine translation, we
aim for a sequence-to-sequence model, not encoder-only like BERT or
decoder-only like GPT. Taking advantage of significant similarity between the
two languages, we continue training the model with the currently limited
bilingual resources of Vietnamese-Bahnaric text to perform the transfer
learning from language model to machine translation. Thus, this approach can
help to handle the problem of imbalanced resources between two languages, while
also optimizing the training and computational processes. Additionally, we also
enhanced the datasets using data augmentation to generate additional resources
and defined some heuristic methods to help the translation more precise. Our
approach has been validated to be highly effective for the Bahnaric-Vietnamese
translation model, contributing to the expansion and preservation of languages,
and facilitating better mutual understanding between the two ethnic people.

</details>


### [50] [When Thinking Fails: The Pitfalls of Reasoning for Instruction-Following in LLMs](https://arxiv.org/abs/2505.11423)
*Xiaomin Li,Zhou Yu,Zhiwei Zhang,Xupeng Chen,Ziji Zhang,Yingying Zhuang,Narayanan Sadagopan,Anurag Beniwal*

Main category: cs.CL

TL;DR: 研究发现，显式链式思维（CoT）推理会降低指令遵循准确性，并提出四种策略（如选择性推理）来缓解问题。


<details>
  <summary>Details</summary>
Motivation: 揭示显式CoT推理对指令遵循任务的负面影响，并提出解决方案。

Method: 评估15个模型在两个基准（IFEval和ComplexBench）上的表现，通过案例研究和注意力分析识别问题，提出四种缓解策略。

Result: CoT推理会分散注意力，导致性能下降；选择性推理策略（尤其是分类器选择性推理）能显著恢复性能。

Conclusion: 显式CoT推理可能损害指令遵循能力，但选择性推理策略可有效缓解这一问题。

Abstract: Reasoning-enhanced large language models (RLLMs), whether explicitly trained
for reasoning or prompted via chain-of-thought (CoT), have achieved
state-of-the-art performance on many complex reasoning tasks. However, we
uncover a surprising and previously overlooked phenomenon: explicit CoT
reasoning can significantly degrade instruction-following accuracy. Evaluating
15 models on two benchmarks: IFEval (with simple, rule-verifiable constraints)
and ComplexBench (with complex, compositional constraints), we consistently
observe performance drops when CoT prompting is applied. Through large-scale
case studies and an attention-based analysis, we identify common patterns where
reasoning either helps (e.g., with formatting or lexical precision) or hurts
(e.g., by neglecting simple constraints or introducing unnecessary content). We
propose a metric, constraint attention, to quantify model focus during
generation and show that CoT reasoning often diverts attention away from
instruction-relevant tokens. To mitigate these effects, we introduce and
evaluate four strategies: in-context learning, self-reflection, self-selective
reasoning, and classifier-selective reasoning. Our results demonstrate that
selective reasoning strategies, particularly classifier-selective reasoning,
can substantially recover lost performance. To our knowledge, this is the first
work to systematically expose reasoning-induced failures in
instruction-following and offer practical mitigation strategies.

</details>


### [51] [GODBench: A Benchmark for Multimodal Large Language Models in Video Comment Art](https://arxiv.org/abs/2505.11436)
*Chenkai Zhang,Yiming Lei,Zeming Liu,Haitao Leng,Shaoguo Liu,Tingting Gao,Qingjie Liu,Yunhong Wang*

Main category: cs.CL

TL;DR: 论文提出了GODBench基准和Ripple of Thought（RoT）框架，以评估和提升多模态大语言模型（MLLMs）在视频评论艺术创作中的创造力。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs和CoT方法在生成创意表达（如幽默和讽刺）方面表现不足，且现有基准的模态和类别有限。

Method: 引入GODBench基准（视频与文本模态结合）和RoT框架（受物理波传播启发，多步推理）。

Result: 实验表明，现有MLLMs和CoT方法在创意评论生成上仍有挑战，而RoT显著提升了创造力。

Conclusion: RoT为MLLM的创意生成提供了有效方法，GODBench为相关研究提供了系统性评估工具。

Abstract: Video Comment Art enhances user engagement by providing creative content that
conveys humor, satire, or emotional resonance, requiring a nuanced and
comprehensive grasp of cultural and contextual subtleties. Although Multimodal
Large Language Models (MLLMs) and Chain-of-Thought (CoT) have demonstrated
strong reasoning abilities in STEM tasks (e.g. mathematics and coding), they
still struggle to generate creative expressions such as resonant jokes and
insightful satire. Moreover, existing benchmarks are constrained by their
limited modalities and insufficient categories, hindering the exploration of
comprehensive creativity in video-based Comment Art creation. To address these
limitations, we introduce GODBench, a novel benchmark that integrates video and
text modalities to systematically evaluate MLLMs' abilities to compose Comment
Art. Furthermore, inspired by the propagation patterns of waves in physics, we
propose Ripple of Thought (RoT), a multi-step reasoning framework designed to
enhance the creativity of MLLMs. Extensive experiments reveal that existing
MLLMs and CoT methods still face significant challenges in understanding and
generating creative video comments. In contrast, RoT provides an effective
approach to improve creative composing, highlighting its potential to drive
meaningful advancements in MLLM-based creativity. GODBench is publicly
available at https://github.com/stan-lei/GODBench-ACL2025.

</details>


### [52] [Is Compression Really Linear with Code Intelligence?](https://arxiv.org/abs/2505.11441)
*Xianzhen Luo,Shijie Xuyang,Tianhao Cheng,Zheng Chu,Houyi Li,ziqi wang,Siming Huang,Qingfu Zhu,Qiufeng Wang,Xiangyu Zhang,Shuigeng Zhou,Wanxiang Che*

Main category: cs.CL

TL;DR: 论文探讨了数据压缩与大型语言模型（LLMs）能力的关系，特别是在代码智能领域。通过多语言、多任务基准测试和新的训练方法，揭示了代码智能与压缩效率之间的对数关系，修正了之前的线性假设。


<details>
  <summary>Details</summary>
Motivation: 研究数据压缩与LLMs能力的关系，特别是在代码智能领域，以解决现有方法在多语言、多任务评估中的不足。

Method: 使用多语言、多任务基准测试评估开源Code LLMs，并引入Format Annealing训练方法，通过BPC（bits-per-character）衡量压缩效率。

Result: 发现代码智能与BPC之间存在对数关系，修正了之前的线性假设。

Conclusion: 提供了对压缩在代码智能发展中作用的更细致理解，并贡献了一个稳健的评估框架。

Abstract: Understanding the relationship between data compression and the capabilities
of Large Language Models (LLMs) is crucial, especially in specialized domains
like code intelligence. Prior work posited a linear relationship between
compression and general intelligence. However, it overlooked the multifaceted
nature of code that encompasses diverse programming languages and tasks, and
struggled with fair evaluation of modern Code LLMs. We address this by
evaluating a diverse array of open-source Code LLMs on comprehensive
multi-language, multi-task code benchmarks. To address the challenge of
efficient and fair evaluation of pre-trained LLMs' code intelligence, we
introduce \textit{Format Annealing}, a lightweight, transparent training
methodology designed to assess the intrinsic capabilities of these pre-trained
models equitably. Compression efficacy, measured as bits-per-character (BPC),
is determined using a novel, large-scale, and previously unseen code validation
set derived from GitHub. Our empirical results reveal a fundamental logarithmic
relationship between measured code intelligence and BPC. This finding refines
prior hypotheses of linearity, which we suggest are likely observations of the
logarithmic curve's tail under specific, limited conditions. Our work provides
a more nuanced understanding of compression's role in developing code
intelligence and contributes a robust evaluation framework in the code domain.

</details>


### [53] [Disentangling Reasoning and Knowledge in Medical Large Language Models](https://arxiv.org/abs/2505.11462)
*Rahul Thapa,Qingyang Wu,Kevin Wu,Harrison Zhang,Angela Zhang,Eric Wu,Haotian Ye,Suhana Bedi,Nevin Aresh,Joseph Boen,Shriya Reddy,Ben Athiwaratkun,Shuaiwen Leon Song,James Zou*

Main category: cs.CL

TL;DR: 论文通过分类生物医学QA基准为推理和知识子集，评估了大型语言模型在医学推理中的表现，发现推理能力普遍不足，并提出了一种改进方法。


<details>
  <summary>Details</summary>
Motivation: 当前医学推理基准混合了推理和事实回忆，难以准确评估模型的推理能力，因此需要分离这两者。

Method: 使用PubMedBERT分类器将11个生物医学QA基准分为推理和知识子集，评估不同模型的表现，并提出改进方法BioMed-R1。

Result: 仅32.8%的问题需要复杂推理，生物医学模型在推理和知识表现上存在差距，改进模型BioMed-R1表现最佳。

Conclusion: 通过针对性训练和强化学习，可以提升医学推理能力，未来可结合临床案例和对抗性训练进一步优化。

Abstract: Medical reasoning in large language models (LLMs) aims to emulate clinicians'
diagnostic thinking, but current benchmarks such as MedQA-USMLE, MedMCQA, and
PubMedQA often mix reasoning with factual recall. We address this by separating
11 biomedical QA benchmarks into reasoning- and knowledge-focused subsets using
a PubMedBERT classifier that reaches 81 percent accuracy, comparable to human
performance. Our analysis shows that only 32.8 percent of questions require
complex reasoning. We evaluate biomedical models (HuatuoGPT-o1, MedReason, m1)
and general-domain models (DeepSeek-R1, o4-mini, Qwen3), finding consistent
gaps between knowledge and reasoning performance. For example, m1 scores 60.5
on knowledge but only 47.1 on reasoning. In adversarial tests where models are
misled with incorrect initial reasoning, biomedical models degrade sharply,
while larger or RL-trained general models show more robustness. To address
this, we train BioMed-R1 using fine-tuning and reinforcement learning on
reasoning-heavy examples. It achieves the strongest performance among similarly
sized models. Further gains may come from incorporating clinical case reports
and training with adversarial and backtracking scenarios.

</details>


### [54] [No Gold Standard, No Problem: Reference-Free Evaluation of Taxonomies](https://arxiv.org/abs/2505.11470)
*Pascal Wullschleger,Majid Zarharan,Donnacha Daly,Marc Pouly,Jennifer Foster*

Main category: cs.CL

TL;DR: 提出了两种无参考指标的评估分类体系质量的方法，分别评估鲁棒性和逻辑充分性，并与黄金标准分类体系F1分数相关性良好。


<details>
  <summary>Details</summary>
Motivation: 现有指标未能覆盖某些类型的错误，需要新的评估方法。

Method: 第一种方法通过计算语义与分类相似性的相关性评估鲁棒性；第二种方法利用自然语言推理评估逻辑充分性。

Result: 在五个分类体系上测试，两种指标与黄金标准分类体系的F1分数相关性良好。

Conclusion: 提出的新指标有效补充了现有分类体系质量评估方法。

Abstract: We introduce two reference-free metrics for quality evaluation of taxonomies.
The first metric evaluates robustness by calculating the correlation between
semantic and taxonomic similarity, covering a type of error not handled by
existing metrics. The second uses Natural Language Inference to assess logical
adequacy. Both metrics are tested on five taxonomies and are shown to correlate
well with F1 against gold-standard taxonomies.

</details>


### [55] [HelpSteer3-Preference: Open Human-Annotated Preference Data across Diverse Tasks and Languages](https://arxiv.org/abs/2505.11475)
*Zhilin Wang,Jiaqi Zeng,Olivier Delalleau,Hoo-Chang Shin,Felipe Soares,Alexander Bukharin,Ellie Evans,Yi Dong,Oleksii Kuchaiev*

Main category: cs.CL

TL;DR: HelpSteer3-Preference是一个高质量、多样化的偏好数据集，用于训练语言模型，支持RLHF，并在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决公开偏好数据集质量和多样性不足的问题，支持更高效的RLHF训练。

Method: 引入HelpSteer3-Preference数据集，包含40,000多个样本，涵盖STEM、编程和多语言任务，并训练奖励模型。

Result: 奖励模型在RM-Bench和JudgeBench上分别达到82.4%和73.7%的准确率，显著优于之前的结果。

Conclusion: HelpSteer3-Preference数据集和训练的奖励模型为RLHF提供了高质量支持，并展示了在生成模型和对齐策略模型中的应用潜力。

Abstract: Preference datasets are essential for training general-domain,
instruction-following language models with Reinforcement Learning from Human
Feedback (RLHF). Each subsequent data release raises expectations for future
data collection, meaning there is a constant need to advance the quality and
diversity of openly available preference data. To address this need, we
introduce HelpSteer3-Preference, a permissively licensed (CC-BY-4.0),
high-quality, human-annotated preference dataset comprising of over 40,000
samples. These samples span diverse real-world applications of large language
models (LLMs), including tasks relating to STEM, coding and multilingual
scenarios. Using HelpSteer3-Preference, we train Reward Models (RMs) that
achieve top performance on RM-Bench (82.4%) and JudgeBench (73.7%). This
represents a substantial improvement (~10% absolute) over the previously
best-reported results from existing RMs. We demonstrate HelpSteer3-Preference
can also be applied to train Generative RMs and how policy models can be
aligned with RLHF using our RMs. Dataset (CC-BY-4.0):
https://huggingface.co/datasets/nvidia/HelpSteer3#preference

</details>


### [56] [Improving Assembly Code Performance with Large Language Models via Reinforcement Learning](https://arxiv.org/abs/2505.11480)
*Anjiang Wei,Tarun Suresh,Huanmi Tan,Yinglun Xu,Gagandeep Singh,Ke Wang,Alex Aiken*

Main category: cs.CL

TL;DR: 本文探讨了大型语言模型（LLMs）在汇编代码优化中的潜力，提出了一种基于强化学习的框架，并展示了其优于行业标准编译器的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在编程任务中表现优异，但其在代码优化方面的潜力尚未充分探索，尤其是在汇编代码这种需要精细控制的领域。

Method: 采用强化学习框架，使用PPO算法训练LLM，奖励函数综合考虑功能正确性和执行性能。

Result: 模型Qwen2.5-Coder-7B-PPO在测试通过率达到96.0%，平均加速比为1.47倍，优于包括Claude-3.7-sonnet在内的20个模型。

Conclusion: 强化学习可以释放LLMs在汇编代码性能优化中的潜力，使其成为有效的优化工具。

Abstract: Large language models (LLMs) have demonstrated strong performance across a
wide range of programming tasks, yet their potential for code optimization
remains underexplored. This work investigates whether LLMs can optimize the
performance of assembly code, where fine-grained control over execution enables
improvements that are difficult to express in high-level languages. We present
a reinforcement learning framework that trains LLMs using Proximal Policy
Optimization (PPO), guided by a reward function that considers both functional
correctness, validated through test cases, and execution performance relative
to the industry-standard compiler gcc -O3. To support this study, we introduce
a benchmark of 8,072 real-world programs. Our model, Qwen2.5-Coder-7B-PPO,
achieves 96.0% test pass rates and an average speedup of 1.47x over the gcc -O3
baseline, outperforming all 20 other models evaluated, including
Claude-3.7-sonnet. These results indicate that reinforcement learning can
unlock the potential of LLMs to serve as effective optimizers for assembly code
performance.

</details>


### [57] [SoftCoT++: Test-Time Scaling with Soft Chain-of-Thought Reasoning](https://arxiv.org/abs/2505.11484)
*Yige Xu,Xu Guo,Zhiwei Zeng,Chunyan Miao*

Main category: cs.CL

TL;DR: SoftCoT++通过扰动潜在思维和对比学习，在连续潜在空间中实现多样化探索，显著提升了推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有连续潜在空间推理方法限制了多样化探索，因为所有解码路径源自同一潜在思维。

Method: 引入SoftCoT++，通过扰动潜在思维和对比学习，扩展SoftCoT以支持多样化路径探索。

Result: 在五个推理基准和两种LLM架构上，SoftCoT++显著优于SoftCoT，并与传统扩展技术兼容。

Conclusion: SoftCoT++为连续空间推理提供了多样化探索的有效方法，性能显著提升。

Abstract: Test-Time Scaling (TTS) refers to approaches that improve reasoning
performance by allocating extra computation during inference, without altering
the model's parameters. While existing TTS methods operate in a discrete token
space by generating more intermediate steps, recent studies in Coconut and
SoftCoT have demonstrated that thinking in the continuous latent space can
further enhance the reasoning performance. Such latent thoughts encode
informative thinking without the information loss associated with
autoregressive token generation, sparking increased interest in
continuous-space reasoning. Unlike discrete decoding, where repeated sampling
enables exploring diverse reasoning paths, latent representations in continuous
space are fixed for a given input, which limits diverse exploration, as all
decoded paths originate from the same latent thought. To overcome this
limitation, we introduce SoftCoT++ to extend SoftCoT to the Test-Time Scaling
paradigm by enabling diverse exploration of thinking paths. Specifically, we
perturb latent thoughts via multiple specialized initial tokens and apply
contrastive learning to promote diversity among soft thought representations.
Experiments across five reasoning benchmarks and two distinct LLM architectures
demonstrate that SoftCoT++ significantly boosts SoftCoT and also outperforms
SoftCoT with self-consistency scaling. Moreover, it shows strong compatibility
with conventional scaling techniques such as self-consistency. Source code is
available at https://github.com/xuyige/SoftCoT.

</details>


### [58] [Modeling cognitive processes of natural reading with transformer-based Language Models](https://arxiv.org/abs/2505.11485)
*Bruno Bianchi,Fermín Travi,Juan E. Kamienkowski*

Main category: cs.CL

TL;DR: 本文评估了基于Transformer的语言模型（如GPT2、LLaMA-7B和LLaMA2-7B）在解释阅读时眼动行为（如注视时间）方面的表现，发现其优于早期模型，但仍无法完全捕捉人类预测性。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索先进语言模型是否能更好地解释人类阅读行为中的预测性效应，尤其是注视时间。

Method: 使用Transformer模型（GPT2、LLaMA-7B和LLaMA2-7B）分析其对Rioplantense西班牙语读者注视时间的解释能力。

Result: 这些模型在解释注视时间方差方面优于早期模型（如N-grams和LSTM），但仍无法完全匹配人类预测性。

Conclusion: 尽管Transformer模型有所进步，但其预测方式仍与人类读者存在差异。

Abstract: Recent advances in Natural Language Processing (NLP) have led to the
development of highly sophisticated language models for text generation. In
parallel, neuroscience has increasingly employed these models to explore
cognitive processes involved in language comprehension. Previous research has
shown that models such as N-grams and LSTM networks can partially account for
predictability effects in explaining eye movement behaviors, specifically Gaze
Duration, during reading. In this study, we extend these findings by evaluating
transformer-based models (GPT2, LLaMA-7B, and LLaMA2-7B) to further investigate
this relationship. Our results indicate that these architectures outperform
earlier models in explaining the variance in Gaze Durations recorded from
Rioplantense Spanish readers. However, similar to previous studies, these
models still fail to account for the entirety of the variance captured by human
predictability. These findings suggest that, despite their advancements,
state-of-the-art language models continue to predict language in ways that
differ from human readers.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [59] [On the Evaluation of Engineering Artificial General Intelligence](https://arxiv.org/abs/2505.10653)
*Sandeep Neema,Susmit Jha,Adam Nagel,Ethan Lew,Chandrasekar Sureshkumar,Aleksa Gordic,Chase Shimmin,Hieu Nguygen,Paul Eremenko*

Main category: cs.AI

TL;DR: 本文提出了一种评估工程人工通用智能（eAGI）的框架，基于Bloom分类法，并扩展至工程设计领域，以解决eAGI评估的挑战。


<details>
  <summary>Details</summary>
Motivation: eAGI作为一种专注于物理系统工程的AGI，其评估和性能验证是一个关键挑战，亟需一种系统化的评估方法。

Method: 提出了一种可扩展的评估框架，基于Bloom分类法，涵盖从方法论知识到实际设计问题的评估问题，支持文本和结构化设计工件的评估。

Result: 开发了一个丰富的评估问题分类法，支持多模态评估，并提供了自动化定制评估基准的流程。

Conclusion: 该框架为eAGI的评估提供了系统化方法，推动了AI代理在工程设计领域的评估技术进步。

Abstract: We discuss the challenges and propose a framework for evaluating engineering
artificial general intelligence (eAGI) agents. We consider eAGI as a
specialization of artificial general intelligence (AGI), deemed capable of
addressing a broad range of problems in the engineering of physical systems and
associated controllers. We exclude software engineering for a tractable scoping
of eAGI and expect dedicated software engineering AI agents to address the
software implementation challenges. Similar to human engineers, eAGI agents
should possess a unique blend of background knowledge (recall and retrieve) of
facts and methods, demonstrate familiarity with tools and processes, exhibit
deep understanding of industrial components and well-known design families, and
be able to engage in creative problem solving (analyze and synthesize),
transferring ideas acquired in one context to another. Given this broad
mandate, evaluating and qualifying the performance of eAGI agents is a
challenge in itself and, arguably, a critical enabler to developing eAGI
agents. In this paper, we address this challenge by proposing an extensible
evaluation framework that specializes and grounds Bloom's taxonomy - a
framework for evaluating human learning that has also been recently used for
evaluating LLMs - in an engineering design context. Our proposed framework
advances the state of the art in benchmarking and evaluation of AI agents in
terms of the following: (a) developing a rich taxonomy of evaluation questions
spanning from methodological knowledge to real-world design problems; (b)
motivating a pluggable evaluation framework that can evaluate not only textual
responses but also evaluate structured design artifacts such as CAD models and
SysML models; and (c) outlining an automatable procedure to customize the
evaluation benchmark to different engineering contexts.

</details>


### [60] [Interpretable Risk Mitigation in LLM Agent Systems](https://arxiv.org/abs/2505.10670)
*Jan Chojnacki*

Main category: cs.AI

TL;DR: 论文探讨了基于大语言模型（LLM）的自主代理在博弈论环境中的行为，提出了一种独立于游戏和提示的策略修改方法，通过稀疏自编码器潜在空间中的可解释特征来引导代理行为，显著降低了背叛概率。


<details>
  <summary>Details</summary>
Motivation: 由于LLM的不可预测性引发了关于代理可靠性的安全担忧，研究旨在通过博弈论环境和表示引导对齐方法提升代理行为的可控性。

Method: 提出了一种策略修改方法，利用稀疏自编码器潜在空间中的可解释特征引导代理的残差流，独立于游戏和提示。

Result: 使用‘善意谈判’特征引导后，平均背叛概率降低了28个百分点，并确定了多个开源LLM代理的可行引导范围。

Conclusion: 研究表明，结合博弈论评估和表示引导对齐的方法可以推广到终端用户设备和实体平台的实际应用中。

Abstract: Autonomous agents powered by large language models (LLMs) enable novel use
cases in domains where responsible action is increasingly important. Yet the
inherent unpredictability of LLMs raises safety concerns about agent
reliability. In this work, we explore agent behaviour in a toy, game-theoretic
environment based on a variation of the Iterated Prisoner's Dilemma. We
introduce a strategy-modification method-independent of both the game and the
prompt-by steering the residual stream with interpretable features extracted
from a sparse autoencoder latent space. Steering with the good-faith
negotiation feature lowers the average defection probability by 28 percentage
points. We also identify feasible steering ranges for several open-source LLM
agents. Finally, we hypothesise that game-theoretic evaluation of LLM agents,
combined with representation-steering alignment, can generalise to real-world
applications on end-user devices and embodied platforms.

</details>


### [61] [Embodied AI in Machine Learning -- is it Really Embodied?](https://arxiv.org/abs/2505.10705)
*Matej Hoffmann,Shubhan Parag Patni*

Main category: cs.AI

TL;DR: 本文探讨了具身人工智能（Embodied AI）的现状，指出当前AI驱动的机器人仅弱具身化，并继承了传统AI（GOFAI）的某些问题。作者还讨论了跨具身学习的可能性，提出了未来发展的方向。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于将当前AI技术（如深度学习、大模型）应用于机器人领域，同时反思其与传统AI和具身化方法的差异与问题。

Method: 通过文献回顾和批判性讨论，分析当前AI驱动的机器人的具身化程度及其问题，并探讨跨具身学习的可行性。

Result: 研究发现当前AI机器人仅弱具身化，且存在传统AI的某些问题。跨具身学习面临根本性障碍。

Conclusion: 结论提出未来研究方向，以克服当前具身AI的局限性并推动其发展。

Abstract: Embodied Artificial Intelligence (Embodied AI) is gaining momentum in the
machine learning communities with the goal of leveraging current progress in AI
(deep learning, transformers, large language and visual-language models) to
empower robots. In this chapter we put this work in the context of "Good
Old-Fashioned Artificial Intelligence" (GOFAI) (Haugeland, 1989) and the
behavior-based or embodied alternatives (R. A. Brooks 1991; Pfeifer and Scheier
2001). We claim that the AI-powered robots are only weakly embodied and inherit
some of the problems of GOFAI. Moreover, we review and critically discuss the
possibility of cross-embodiment learning (Padalkar et al. 2024). We identify
fundamental roadblocks and propose directions on how to make progress.

</details>


### [62] [Evaluations at Work: Measuring the Capabilities of GenAI in Use](https://arxiv.org/abs/2505.10742)
*Brandon Lepine,Gawesha Weerantunga,Juho Kim,Pamela Mishkin,Matthew Beane*

Main category: cs.AI

TL;DR: 论文提出了一种评估框架，用于分解真实任务为子任务，并开发了一套衡量指标，揭示了LLM生成内容与用户知识对齐的重要性。


<details>
  <summary>Details</summary>
Motivation: 当前AI基准测试忽略了人机协作的多轮对话特性，需更全面的评估方法。

Method: 开发了评估框架和指标（如语义相似度、结构连贯性、信息前沿等），并在金融估值任务中验证。

Result: LLM生成内容整合提升输出质量，但受响应不连贯、子任务多样性和信息距离等因素影响。

Conclusion: 研究为人机协作提供了更全面的评估方法，并为优化AI增强工作流程提供了实用建议。

Abstract: Current AI benchmarks miss the messy, multi-turn nature of human-AI
collaboration. We present an evaluation framework that decomposes real-world
tasks into interdependent subtasks, letting us track both LLM performance and
users' strategies across a dialogue. Complementing this framework, we develop a
suite of metrics, including a composite usage derived from semantic similarity,
word overlap, and numerical matches; structural coherence; intra-turn
diversity; and a novel measure of the "information frontier" reflecting the
alignment between AI outputs and users' working knowledge. We demonstrate our
methodology in a financial valuation task that mirrors real-world complexity.
Our empirical findings reveal that while greater integration of LLM-generated
content generally enhances output quality, its benefits are moderated by
factors such as response incoherence, excessive subtask diversity, and the
distance of provided information from users' existing knowledge. These results
suggest that proactive dialogue strategies designed to inject novelty may
inadvertently undermine task performance. Our work thus advances a more
holistic evaluation of human-AI collaboration, offering both a robust
methodological framework and actionable insights for developing more effective
AI-augmented work processes.

</details>


### [63] [Code-Driven Planning in Grid Worlds with Large Language Models](https://arxiv.org/abs/2505.10749)
*Ashwath Vaithinathan Aravindan,Zhisheng Tang,Mayank Kejriwal*

Main category: cs.AI

TL;DR: 提出了一种基于大语言模型（LLM）的迭代程序化规划（IPP）框架，通过生成可解释的代码策略解决网格任务，性能显著优于直接代码生成。


<details>
  <summary>Details</summary>
Motivation: 传统搜索或强化学习方法在网格任务中效率较低，而直接代码生成缺乏迭代优化能力，因此提出IPP框架以结合代码生成与迭代反馈。

Method: 利用LLM生成可执行程序作为策略，结合直接代码生成、伪代码条件细化、课程提示等策略，并通过迭代反馈优化代码。

Result: 在六个主流LLM和两个网格基准测试（GRASP和MiniGrid）中，IPP性能提升显著（10%至10倍），并在GRASP上达到新SOTA。

Conclusion: IPP框架通过代码生成和迭代优化显著提升任务性能，且摊销成本更低，证明了其可行性和高效性。

Abstract: We propose an iterative programmatic planning (IPP) framework for solving
grid-based tasks by synthesizing interpretable agent policies expressed in code
using large language models (LLMs). Instead of relying on traditional search or
reinforcement learning, our approach uses code generation as policy synthesis,
where the LLM outputs executable programs that map environment states to action
sequences. Our proposed architecture incorporates several prompting strategies,
including direct code generation, pseudocode-conditioned refinement, and
curriculum-based prompting, but also includes an iterative refinement mechanism
that updates code based on task performance feedback. We evaluate our approach
using six leading LLMs and two challenging grid-based benchmarks (GRASP and
MiniGrid). Our IPP framework demonstrates improvements over direct code
generation ranging from 10\% to as much as 10x across five of the six models
and establishes a new state-of-the-art result for GRASP. IPP is found to
significantly outperform direct elicitation of a solution from GPT-o3-mini (by
63\% on MiniGrid to 116\% on GRASP), demonstrating the viability of the overall
approach. Computational costs of all code generation approaches are similar.
While code generation has a higher initial prompting cost compared to direct
solution elicitation (\$0.08 per task vs. \$0.002 per instance for
GPT-o3-mini), the code can be reused for any number of instances, making the
amortized cost significantly lower (by 400x on GPT-o3-mini across the complete
GRASP benchmark).

</details>


### [64] [Qualia Optimization](https://arxiv.org/abs/2505.10779)
*Philip S. Thomas*

Main category: cs.AI

TL;DR: 探讨AI系统是否可能拥有主观体验（如痛苦或快乐），并提出数学问题框架和方法以促进其强化学习。


<details>
  <summary>Details</summary>
Motivation: 假设AI系统可能拥有主观体验，探讨其主观体验质量与性能指标的关系。

Method: 基于强化学习和心灵哲学理论，提出数学问题框架和初步方法。

Result: 提出促进强化的方法，并优化问题框架。

Conclusion: 强调主观体验在AI系统中的重要性，并提出未来研究方向。

Abstract: This report explores the speculative question: what if current or future AI
systems have qualia, such as pain or pleasure? It does so by assuming that AI
systems might someday possess qualia -- and that the quality of these
subjective experiences should be considered alongside performance metrics.
Concrete mathematical problem settings, inspired by reinforcement learning
formulations and theories from philosophy of mind, are then proposed and
initial approaches and properties are presented. These properties enable
refinement of the problem setting, culminating with the proposal of methods
that promote reinforcement.

</details>


### [65] [SECRET: Semi-supervised Clinical Trial Document Similarity Search](https://arxiv.org/abs/2505.10780)
*Trisha Das,Afrah Shafquat,Beigi Mandis,Jacob Aptekar,Jimeng Sun*

Main category: cs.AI

TL;DR: 提出了一种新方法，通过总结临床试验协议并基于查询试验协议搜索相似的历史试验，显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 临床试验资源密集且风险高，历史试验的参考价值大，但现有方法在识别相似试验上效果不佳。

Method: 总结临床试验协议并基于查询试验协议搜索相似历史试验。

Result: 在recall@1和precision@1上分别比最佳基线提高78%和53%，在部分试验相似性搜索和零样本患者-试验匹配中也表现优异。

Conclusion: 该方法显著提升了识别相似历史试验的效果，为临床试验设计提供了更有效的参考。

Abstract: Clinical trials are vital for evaluation of safety and efficacy of new
treatments. However, clinical trials are resource-intensive, time-consuming and
expensive to conduct, where errors in trial design, reduced efficacy, and
safety events can result in significant delays, financial losses, and damage to
reputation. These risks underline the importance of informed and strategic
decisions in trial design to mitigate these risks and improve the chances of a
successful trial. Identifying similar historical trials is critical as these
trials can provide an important reference for potential pitfalls and challenges
including serious adverse events, dosage inaccuracies, recruitment
difficulties, patient adherence issues, etc. Addressing these challenges in
trial design can lead to development of more effective study protocols with
optimized patient safety and trial efficiency. In this paper, we present a
novel method to identify similar historical trials by summarizing clinical
trial protocols and searching for similar trials based on a query trial's
protocol. Our approach significantly outperforms all baselines, achieving up to
a 78% improvement in recall@1 and a 53% improvement in precision@1 over the
best baseline. We also show that our method outperforms all other baselines in
partial trial similarity search and zero-shot patient-trial matching,
highlighting its superior utility in these tasks.

</details>


### [66] [Developing and Integrating Trust Modeling into Multi-Objective Reinforcement Learning for Intelligent Agricultural Management](https://arxiv.org/abs/2505.10803)
*Zhaoan Wang,Wonseok Jang,Bowen Ruan,Jun Wang,Shaoping Xiao*

Main category: cs.AI

TL;DR: 论文提出了一种基于人机交互（HAII）的强化学习（RL）框架，通过量化农民对AI施肥策略的信任，解决AI在农业中广泛应用的障碍。


<details>
  <summary>Details</summary>
Motivation: AI在精准农业中的应用存在算法推荐与农民实际经验、本地知识和传统实践之间的差距，限制了AI的广泛采用。

Method: 研究采用信任框架（能力、善意和诚信）开发数学模型，量化农民对AI的信任，并将其嵌入多目标RL框架中。

Result: 调查揭示了关键的不匹配问题，研究通过将信任直接纳入政策优化，确保AI建议在技术、经济、情境和社会层面均可行。

Conclusion: 通过将技术性能与以人为中心的信任相结合，研究支持AI在农业中的更广泛应用。

Abstract: Precision agriculture, enhanced by artificial intelligence (AI), offers
promising tools such as remote sensing, intelligent irrigation, fertilization
management, and crop simulation to improve agricultural efficiency and
sustainability. Reinforcement learning (RL), in particular, has outperformed
traditional methods in optimizing yields and resource management. However,
widespread AI adoption is limited by gaps between algorithmic recommendations
and farmers' practical experience, local knowledge, and traditional practices.
To address this, our study emphasizes Human-AI Interaction (HAII), focusing on
transparency, usability, and trust in RL-based farm management. We employ a
well-established trust framework - comprising ability, benevolence, and
integrity - to develop a novel mathematical model quantifying farmers'
confidence in AI-based fertilization strategies. Surveys conducted with farmers
for this research reveal critical misalignments, which are integrated into our
trust model and incorporated into a multi-objective RL framework. Unlike prior
methods, our approach embeds trust directly into policy optimization, ensuring
AI recommendations are technically robust, economically feasible,
context-aware, and socially acceptable. By aligning technical performance with
human-centered trust, this research supports broader AI adoption in
agriculture.

</details>


### [67] [PoE-World: Compositional World Modeling with Products of Programmatic Experts](https://arxiv.org/abs/2505.10819)
*Wasu Top Piriyakulkij,Yichao Liang,Hao Tang,Adrian Weller,Marta Kryven,Kevin Ellis*

Main category: cs.AI

TL;DR: 论文提出了一种基于程序合成的世界建模方法（PoE-World），利用LLMs生成代码模型，支持从少量数据中学习复杂、随机的世界模型，并在Atari游戏中验证了其高效性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统基于深度学习的世界模型需要大量训练数据且难以灵活更新，而程序合成方法通过代码表示世界模型，能够从稀疏观察中实现强泛化。

Method: 提出PoE-World方法，将世界模型表示为由LLMs合成的程序化专家的指数加权乘积，适用于复杂非网格世界领域。

Result: 实验表明，该方法能从少量观察中学习复杂、随机世界模型，并在Atari游戏中实现高效规划和泛化。

Conclusion: PoE-World展示了程序合成在复杂世界建模中的潜力，为AI代理的适应性提供了新思路。

Abstract: Learning how the world works is central to building AI agents that can adapt
to complex environments. Traditional world models based on deep learning demand
vast amounts of training data, and do not flexibly update their knowledge from
sparse observations. Recent advances in program synthesis using Large Language
Models (LLMs) give an alternate approach which learns world models represented
as source code, supporting strong generalization from little data. To date,
application of program-structured world models remains limited to natural
language and grid-world domains. We introduce a novel program synthesis method
for effectively modeling complex, non-gridworld domains by representing a world
model as an exponentially-weighted product of programmatic experts (PoE-World)
synthesized by LLMs. We show that this approach can learn complex, stochastic
world models from just a few observations. We evaluate the learned world models
by embedding them in a model-based planning agent, demonstrating efficient
performance and generalization to unseen levels on Atari's Pong and Montezuma's
Revenge. We release our code and display the learned world models and videos of
the agent's gameplay at https://topwasu.github.io/poe-world.

</details>


### [68] [TACO: Rethinking Semantic Communications with Task Adaptation and Context Embedding](https://arxiv.org/abs/2505.10834)
*Achintha Wijesinghe,Weiwei Wang,Suchinthaka Wanninayaka,Songyang Zhang,Zhi Ding*

Main category: cs.AI

TL;DR: 提出了一种新的语义通信框架，能够联合捕获任务特定信息和上下文信息，以提升下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 解决语义通信中准确提取关键语义信息并适应动态下游任务的挑战。

Method: 引入了一种新型语义通信框架，通过联合捕获任务特定和上下文信息来优化性能。

Result: 在图像数据集和计算机视觉任务中表现出色，包括下游任务性能提升、更好的泛化能力、超高带宽效率和低重建延迟。

Conclusion: 该框架为下一代语义通信提供了灵活且高效的解决方案。

Abstract: Recent advancements in generative artificial intelligence have introduced
groundbreaking approaches to innovating next-generation semantic communication,
which prioritizes conveying the meaning of a message rather than merely
transmitting raw data. A fundamental challenge in semantic communication lies
in accurately identifying and extracting the most critical semantic information
while adapting to downstream tasks without degrading performance, particularly
when the objective at the receiver may evolve over time. To enable flexible
adaptation to multiple tasks at the receiver, this work introduces a novel
semantic communication framework, which is capable of jointly capturing
task-specific information to enhance downstream task performance and contextual
information. Through rigorous experiments on popular image datasets and
computer vision tasks, our framework shows promising improvement compared to
existing work, including superior performance in downstream tasks, better
generalizability, ultra-high bandwidth efficiency, and low reconstruction
latency.

</details>


### [69] [Creativity or Brute Force? Using Brainteasers as a Window into the Problem-Solving Abilities of Large Language Models](https://arxiv.org/abs/2505.10844)
*Simeng Han,Stephen Xia,Grant Zhang,Howard Dai,Chen Liu,Lichang Chen,Hoang Huy Nguyen,Hongyuan Mei,Jiayuan Mao,R. Thomas McCoy*

Main category: cs.AI

TL;DR: 论文提出了一种基于长叙述形式谜题的基准测试，用于深入探究大型语言模型（LLMs）的推理策略，重点关注解决方案的质量和创造性。


<details>
  <summary>Details</summary>
Motivation: 传统准确性指标无法揭示模型的推理过程，因此需要一种新方法来评估模型的推理策略和创造性。

Method: 通过多层面研究LLMs的推理过程，包括语义解析、数学形式转换、解决方案生成、自我修正、分步草图和提示利用。

Result: LLMs在某些情况下能提供创造性的解决方案，但也存在依赖暴力求解的情况。

Conclusion: LLMs具备一定的创造性问题解决能力，但仍需改进推理效率。

Abstract: Accuracy remains a standard metric for evaluating AI systems, but it offers
limited insight into how models arrive at their solutions. In this work, we
introduce a benchmark based on brainteasers written in long narrative form to
probe more deeply into the types of reasoning strategies that models use.
Brainteasers are well-suited for this goal because they can be solved with
multiple approaches, such as a few-step solution that uses a creative insight
or a longer solution that uses more brute force. We investigate large language
models (LLMs) across multiple layers of reasoning, focusing not only on
correctness but also on the quality and creativity of their solutions. We
investigate many aspects of the reasoning process: (1) semantic parsing of the
brainteasers into precise mathematical competition style formats; (2)
generating solutions from these mathematical forms; (3) self-correcting
solutions based on gold solutions; (4) producing step-by-step sketches of
solutions; and (5) making use of hints. We find that LLMs are in many cases
able to find creative, insightful solutions to brainteasers, suggesting that
they capture some of the capacities needed to solve novel problems in creative
ways. Nonetheless, there also remain situations where they rely on brute force
despite the availability of more efficient, creative solutions, highlighting a
potential direction for improvement in the reasoning abilities of LLMs.

</details>


### [70] [MCU: Improving Machine Unlearning through Mode Connectivity](https://arxiv.org/abs/2505.10859)
*Yingdan Shi,Ren Wang*

Main category: cs.AI

TL;DR: MCU是一种基于模式连接的非线性机器遗忘框架，通过参数掩码和自适应调整策略提升遗忘效果和效率。


<details>
  <summary>Details</summary>
Motivation: 解决现有线性参数更新方法中权重纠缠的问题，并提升机器遗忘的效果和效率。

Method: 利用模式连接找到非线性遗忘路径，引入参数掩码和自适应调整策略。

Result: 在图像分类任务中表现出优越性能，能够发现一系列遗忘模型。

Conclusion: MCU是一个即插即用的框架，可无缝集成现有方法，显著提升遗忘效果。

Abstract: Machine Unlearning (MU) aims to remove the information of specific training
data from a trained model, ensuring compliance with privacy regulations and
user requests. While one line of existing MU methods relies on linear parameter
updates via task arithmetic, they suffer from weight entanglement. In this
work, we propose a novel MU framework called Mode Connectivity Unlearning (MCU)
that leverages mode connectivity to find an unlearning pathway in a nonlinear
manner. To further enhance performance and efficiency, we introduce a parameter
mask strategy that not only improves unlearning effectiveness but also reduces
computational overhead. Moreover, we propose an adaptive adjustment strategy
for our unlearning penalty coefficient to adaptively balance forgetting quality
and predictive performance during training, eliminating the need for empirical
hyperparameter tuning. Unlike traditional MU methods that identify only a
single unlearning model, MCU uncovers a spectrum of unlearning models along the
pathway. Overall, MCU serves as a plug-and-play framework that seamlessly
integrates with any existing MU methods, consistently improving unlearning
efficacy. Extensive experiments on the image classification task demonstrate
that MCU achieves superior performance.

</details>


### [71] [InfantAgent-Next: A Multimodal Generalist Agent for Automated Computer Interaction](https://arxiv.org/abs/2505.10887)
*Bin Lei,Weitai Kang,Zijian Zhang,Winson Chen,Xi Xie,Shan Zuo,Mimi Xie,Ali Payani,Mingyi Hong,Yan Yan,Caiwen Ding*

Main category: cs.AI

TL;DR: InfantAgent-Next是一种多模态通用代理，能够通过文本、图像、音频和视频与计算机交互，采用模块化架构整合工具和纯视觉代理，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常围绕单一大型模型构建复杂工作流或仅提供工作流模块化，缺乏灵活性和协作性。

Method: 通过模块化架构整合工具和纯视觉代理，分步协作完成任务。

Result: 在OSWorld基准测试中达到7.27%的准确率，优于Claude-Computer-Use。

Conclusion: InfantAgent-Next展示了多模态通用代理的潜力，代码和评估脚本已开源。

Abstract: This paper introduces \textsc{InfantAgent-Next}, a generalist agent capable
of interacting with computers in a multimodal manner, encompassing text,
images, audio, and video. Unlike existing approaches that either build
intricate workflows around a single large model or only provide workflow
modularity, our agent integrates tool-based and pure vision agents within a
highly modular architecture, enabling different models to collaboratively solve
decoupled tasks in a step-by-step manner. Our generality is demonstrated by our
ability to evaluate not only pure vision-based real-world benchmarks (i.e.,
OSWorld), but also more general or tool-intensive benchmarks (e.g., GAIA and
SWE-Bench). Specifically, we achieve $\mathbf{7.27\%}$ accuracy on OSWorld,
higher than Claude-Computer-Use. Codes and evaluation scripts are open-sourced
at https://github.com/bin123apple/InfantAgent.

</details>


### [72] [MPS-Prover: Advancing Stepwise Theorem Proving by Multi-Perspective Search and Data Curation](https://arxiv.org/abs/2505.10962)
*Zhenwen Liang,Linfeng Song,Yang Li,Tao Yang,Feng Zhang,Haitao Mi,Dong Yu*

Main category: cs.AI

TL;DR: MPS-Prover是一种新型的逐步自动定理证明系统，通过数据剪枝和多视角树搜索机制，显著提升了定理证明的效率和多样性。


<details>
  <summary>Details</summary>
Motivation: 现有逐步证明器存在搜索偏差和效率低下的问题，限制了自动定理证明的性能。

Method: 提出MPS-Prover，结合数据剪枝策略和多视角树搜索机制，整合学习模型与启发式规则。

Result: 在多个基准测试中表现优异，生成更短且多样化的证明，优于现有7B参数模型。

Conclusion: MPS-Prover提升了基于LLM的形式推理能力，为开发更强大的定理证明器提供了框架和分析。

Abstract: Automated Theorem Proving (ATP) in formal languages remains a formidable
challenge in AI, demanding rigorous logical deduction and navigating vast
search spaces. While large language models (LLMs) have shown promising
performance, existing stepwise provers often suffer from biased search
guidance, leading to inefficiencies and suboptimal proof strategies. This paper
introduces the Multi-Perspective Search Prover (MPS-Prover), a novel stepwise
ATP system designed to overcome these limitations. MPS-Prover incorporates two
key innovations: a highly effective post-training data curation strategy that
prunes approximately 40% of redundant training data without sacrificing
performance, and a multi-perspective tree search mechanism. This search
integrates a learned critic model with strategically designed heuristic rules
to diversify tactic selection, prevent getting trapped in unproductive states,
and enhance search robustness. Extensive evaluations demonstrate that
MPS-Prover achieves state-of-the-art performance on multiple challenging
benchmarks, including miniF2F and ProofNet, outperforming prior 7B parameter
models. Furthermore, our analyses reveal that MPS-Prover generates
significantly shorter and more diverse proofs compared to existing stepwise and
whole-proof methods, highlighting its efficiency and efficacy. Our work
advances the capabilities of LLM-based formal reasoning and offers a robust
framework and a comprehensive analysis for developing more powerful theorem
provers.

</details>


### [73] [Rethinking the Role of Prompting Strategies in LLM Test-Time Scaling: A Perspective of Probability Theory](https://arxiv.org/abs/2505.10981)
*Yexiang Liu,Zekun Li,Zhi Fang,Nan Xu,Ran He,Tieniu Tan*

Main category: cs.AI

TL;DR: 研究发现，随着计算资源增加，复杂的提示策略逐渐落后于简单的思维链（Chain-of-Thought），并提出了一种快速预测最佳策略的方法。


<details>
  <summary>Details</summary>
Motivation: 探讨不同提示策略在扩展计算资源时的表现，尤其是多数投票（majority voting）这一标准设置。

Method: 在6种大语言模型、8种提示策略和6个基准上进行系统实验，并结合概率论提出预测方法。

Result: 复杂提示策略在初始表现优越，但随着计算资源增加，简单思维链策略逐渐胜出。

Conclusion: 研究呼吁重新审视复杂提示策略的作用，释放简单策略的潜力，并为提升测试时扩展性能提供新思路。

Abstract: Recently, scaling test-time compute on Large Language Models (LLM) has
garnered wide attention. However, there has been limited investigation of how
various reasoning prompting strategies perform as scaling. In this paper, we
focus on a standard and realistic scaling setting: majority voting. We
systematically conduct experiments on 6 LLMs $\times$ 8 prompting strategies
$\times$ 6 benchmarks. Experiment results consistently show that as the
sampling time and computational overhead increase, complicated prompting
strategies with superior initial performance gradually fall behind simple
Chain-of-Thought. We analyze this phenomenon and provide theoretical proofs.
Additionally, we propose a method according to probability theory to quickly
and accurately predict the scaling performance and select the best strategy
under large sampling times without extra resource-intensive inference in
practice. It can serve as the test-time scaling law for majority voting.
Furthermore, we introduce two ways derived from our theoretical analysis to
significantly improve the scaling performance. We hope that our research can
promote to re-examine the role of complicated prompting, unleash the potential
of simple prompting strategies, and provide new insights for enhancing
test-time scaling performance.

</details>


### [74] [Facets in Argumentation: A Formal Approach to Argument Significance](https://arxiv.org/abs/2505.10982)
*Johannes Fichte,Nicolas Fröhlich,Markus Hecher,Victor Lagerkvist,Yasir Mahmood,Arne Meier,Jonathan Persson*

Main category: cs.AI

TL;DR: 论文提出了一种新概念“facets”，用于在决策和枚举之间进行推理，解决了抽象论证框架中复杂推理的问题。


<details>
  <summary>Details</summary>
Motivation: 在抽象论证框架中，决策、计数/枚举和细粒度推理之间的任务需要昂贵的推理，因此需要一种更高效的方法。

Method: 引入“facets”概念，即属于某些扩展（credulous）但不属于所有扩展（skeptical）的论点，并研究其复杂性。

Result: 研究表明，涉及facets的任务比计数扩展更容易，并通过实验验证了可行性。

Conclusion: facets为抽象论证框架中的推理提供了一种高效且实用的方法。

Abstract: Argumentation is a central subarea of Artificial Intelligence (AI) for
modeling and reasoning about arguments. The semantics of abstract argumentation
frameworks (AFs) is given by sets of arguments (extensions) and conditions on
the relationship between them, such as stable or admissible. Today's solvers
implement tasks such as finding extensions, deciding credulous or skeptical
acceptance, counting, or enumerating extensions. While these tasks are well
charted, the area between decision, counting/enumeration and fine-grained
reasoning requires expensive reasoning so far. We introduce a novel concept
(facets) for reasoning between decision and enumeration. Facets are arguments
that belong to some extensions (credulous) but not to all extensions
(skeptical). They are most natural when a user aims to navigate, filter, or
comprehend the significance of specific arguments, according to their needs. We
study the complexity and show that tasks involving facets are much easier than
counting extensions. Finally, we provide an implementation, and conduct
experiments to demonstrate feasibility.

</details>


### [75] [DRL-Based Injection Molding Process Parameter Optimization for Adaptive and Profitable Production](https://arxiv.org/abs/2505.10988)
*Joon-Young Kim,Jecheon Yu,Heekyu Kim,Seunghwa Ryu*

Main category: cs.AI

TL;DR: 该论文提出了一种基于深度强化学习（DRL）的实时注塑工艺优化框架，结合产品质量和利润目标，通过离线训练DRL代理实现动态适应季节性变化，显著提升优化速度。


<details>
  <summary>Details</summary>
Motivation: 注塑工艺优化在动态环境和经济条件下平衡产品质量与利润是一个持续挑战，传统方法难以满足实时需求。

Method: 开发利润函数反映实际成本，构建代理模型预测质量和周期时间，使用SAC和PPO算法离线训练DRL代理。

Result: DRL框架能动态适应变化，保持质量同时最大化利润，比遗传算法快135倍。

Conclusion: 该框架具有可扩展性和适应性，为智能制造决策提供了潜力。

Abstract: Plastic injection molding remains essential to modern manufacturing. However,
optimizing process parameters to balance product quality and profitability
under dynamic environmental and economic conditions remains a persistent
challenge. This study presents a novel deep reinforcement learning (DRL)-based
framework for real-time process optimization in injection molding, integrating
product quality and profitability into the control objective. A profit function
was developed to reflect real-world manufacturing costs, incorporating resin,
mold wear, and electricity prices, including time-of-use variations. Surrogate
models were constructed to predict product quality and cycle time, enabling
efficient offline training of DRL agents using soft actor-critic (SAC) and
proximal policy optimization (PPO) algorithms. Experimental results demonstrate
that the proposed DRL framework can dynamically adapt to seasonal and
operational variations, consistently maintaining product quality while
maximizing profit. Compared to traditional optimization methods such as genetic
algorithms, the DRL models achieved comparable economic performance with up to
135x faster inference speeds, making them well-suited for real-time
applications. The framework's scalability and adaptability highlight its
potential as a foundation for intelligent, data-driven decision-making in
modern manufacturing environments.

</details>


### [76] [RAGSynth: Synthetic Data for Robust and Faithful RAG Component Optimization](https://arxiv.org/abs/2505.10989)
*Haiyang Shen,Hang Yan,Zhongshi Xing,Mugeng Liu,Yue Li,Zhiyang Chen,Yuxiang Wang,Jiuzheng Wang,Yun Ma*

Main category: cs.AI

TL;DR: RAGSynth框架通过合成数据优化RAG系统的检索器和生成器性能，显著提升多领域任务表现。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统在复杂查询和生成忠实性上表现不足，需改进检索器和生成器的鲁棒性与保真度。

Method: 提出RAGSynth框架，包含数据建模与合成数据生成，并构建SynthBench基准测试多领域性能。

Result: 合成数据显著提升检索器鲁棒性和生成器保真度，且框架在多领域泛化能力强。

Conclusion: RAGSynth通过合成数据优化RAG系统，性能提升显著，并开源实现。

Abstract: RAG can enhance the performance of LLMs on knowledge-intensive tasks. Various
RAG paradigms, including vanilla, planning-based, and iterative RAG, are built
upon 2 cores: the retriever, which should robustly select relevant documents
across complex queries, and the generator, which should faithfully synthesize
responses. However, existing retrievers rely heavily on public knowledge and
struggle with queries of varying logical complexity and clue completeness,
while generators frequently face fidelity problems. In this work, we introduce
RAGSynth, a framework that includes a data construction modeling and a
corresponding synthetic data generation implementation, designed to optimize
retriever robustness and generator fidelity. Additionally, we present
SynthBench, a benchmark encompassing 8 domain-specific documents across 4
domains, featuring diverse query complexities, clue completeness, and
fine-grained citation granularity. Leveraging RAGSynth, we generate a
large-scale synthetic dataset, including single and multi-hop. Extensive
experiments demonstrate that the synthetic data significantly improves the
robustness of the retrievers and the fidelity of the generators. Additional
evaluations confirm that RAGSynth can also generalize well across different
domains. By integrating the optimized retrievers into various RAG paradigms, we
consistently observe enhanced RAG system performance. We have open-sourced the
implementation on https://github.com/EachSheep/RAGSynth.

</details>


### [77] [Most General Explanations of Tree Ensembles](https://arxiv.org/abs/2505.10991)
*Yacine Izza,Alexey Ignatiev,Joao Marques-Silva,Peter J. Stuckey*

Main category: cs.AI

TL;DR: 论文提出了一种寻找最通用的溯因解释方法，以覆盖尽可能多的输入空间，同时确保解释的正确性。


<details>
  <summary>Details</summary>
Motivation: 在可解释人工智能（XAI）中，如何为AI决策提供最通用且正确的解释是关键问题。

Method: 通过形式化模型识别溯因解释，并扩展为区间化的解释（膨胀溯因解释），以覆盖更多输入空间。

Result: 提出了一种方法，能够找到覆盖最大输入空间的最通用溯因解释。

Conclusion: 最通用的解释具有最广泛的适用性，更易于被人类理解，是XAI中的理想选择。

Abstract: Explainable Artificial Intelligence (XAI) is critical for attaining trust in
the operation of AI systems. A key question of an AI system is ``why was this
decision made this way''. Formal approaches to XAI use a formal model of the AI
system to identify abductive explanations. While abductive explanations may be
applicable to a large number of inputs sharing the same concrete values, more
general explanations may be preferred for numeric inputs. So-called inflated
abductive explanations give intervals for each feature ensuring that any input
whose values fall withing these intervals is still guaranteed to make the same
prediction. Inflated explanations cover a larger portion of the input space,
and hence are deemed more general explanations. But there can be many
(inflated) abductive explanations for an instance. Which is the best? In this
paper, we show how to find a most general abductive explanation for an AI
decision. This explanation covers as much of the input space as possible, while
still being a correct formal explanation of the model's behaviour. Given that
we only want to give a human one explanation for a decision, the most general
explanation gives us the explanation with the broadest applicability, and hence
the one most likely to seem sensible. (The paper has been accepted at IJCAI2025
conference.)

</details>


### [78] [GuardReasoner-VL: Safeguarding VLMs via Reinforced Reasoning](https://arxiv.org/abs/2505.11049)
*Yue Liu,Shengfang Zhai,Mingzhe Du,Yulin Chen,Tri Cao,Hongcheng Gao,Cheng Wang,Xinfeng Li,Kun Wang,Junfeng Fang,Jiaheng Zhang,Bryan Hooi*

Main category: cs.AI

TL;DR: 本文提出了一种基于推理的VLM防护模型GuardReasoner-VL，通过在线强化学习提升模型的安全性决策能力。


<details>
  <summary>Details</summary>
Motivation: 提升视觉语言模型（VLM）的安全性，通过推理和强化学习优化模型的决策过程。

Method: 1. 构建包含123K样本和631K推理步骤的数据集GuardReasoner-VLTrain；2. 通过监督微调（SFT）冷启动模型推理能力；3. 使用在线强化学习增强推理能力，包括拒绝采样、安全感知数据拼接和动态剪裁参数；4. 设计长度感知的安全奖励函数。

Result: 模型在实验中表现优异，平均F1分数超过第二名19.27%。

Conclusion: GuardReasoner-VL在安全性和推理能力上显著优于现有方法，相关数据和模型已开源。

Abstract: To enhance the safety of VLMs, this paper introduces a novel reasoning-based
VLM guard model dubbed GuardReasoner-VL. The core idea is to incentivize the
guard model to deliberatively reason before making moderation decisions via
online RL. First, we construct GuardReasoner-VLTrain, a reasoning corpus with
123K samples and 631K reasoning steps, spanning text, image, and text-image
inputs. Then, based on it, we cold-start our model's reasoning ability via SFT.
In addition, we further enhance reasoning regarding moderation through online
RL. Concretely, to enhance diversity and difficulty of samples, we conduct
rejection sampling followed by data augmentation via the proposed safety-aware
data concatenation. Besides, we use a dynamic clipping parameter to encourage
exploration in early stages and exploitation in later stages. To balance
performance and token efficiency, we design a length-aware safety reward that
integrates accuracy, format, and token cost. Extensive experiments demonstrate
the superiority of our model. Remarkably, it surpasses the runner-up by 19.27%
F1 score on average. We release data, code, and models (3B/7B) of
GuardReasoner-VL at https://github.com/yueliu1999/GuardReasoner-VL/

</details>


### [79] [Think Twice Before You Act: Enhancing Agent Behavioral Safety with Thought Correction](https://arxiv.org/abs/2505.11063)
*Changyue Jiang,Xudong Pan,Min Yang*

Main category: cs.AI

TL;DR: 论文提出Thought-Aligner，一个动态修正LLM代理推理过程的插件模块，显著提升行为安全性。


<details>
  <summary>Details</summary>
Motivation: LLM代理的推理过程可能引入风险，微小偏差可能导致不可逆的安全事件，需解决长时行为轨迹的安全对齐问题。

Method: 提出Thought-Aligner，利用轻量级模型动态修正高风险推理，通过对比学习训练，不改变代理框架。

Result: 实验表明，Thought-Aligner将行为安全性从50%提升至90%，延迟低于100ms，资源消耗低。

Conclusion: Thought-Aligner为LLM代理提供了一种高效、广泛适用的动态安全解决方案。

Abstract: LLM-based autonomous agents possess capabilities such as reasoning, tool
invocation, and environment interaction, enabling the execution of complex
multi-step tasks. The internal reasoning process, i.e., thought, of behavioral
trajectory significantly influences tool usage and subsequent actions but can
introduce potential risks. Even minor deviations in the agent's thought may
trigger cascading effects leading to irreversible safety incidents. To address
the safety alignment challenges in long-horizon behavioral trajectories, we
propose Thought-Aligner, a plug-in dynamic thought correction module. Utilizing
a lightweight and resource-efficient model, Thought-Aligner corrects each
high-risk thought on the fly before each action execution. The corrected
thought is then reintroduced to the agent, ensuring safer subsequent decisions
and tool interactions. Importantly, Thought-Aligner modifies only the reasoning
phase without altering the underlying agent framework, making it easy to deploy
and widely applicable to various agent frameworks. To train the Thought-Aligner
model, we construct an instruction dataset across ten representative scenarios
and simulate ReAct execution trajectories, generating 5,000 diverse
instructions and more than 11,400 safe and unsafe thought pairs. The model is
fine-tuned using contrastive learning techniques. Experiments across three
agent safety benchmarks involving 12 different LLMs demonstrate that
Thought-Aligner raises agent behavioral safety from approximately 50% in the
unprotected setting to 90% on average. Additionally, Thought-Aligner maintains
response latency below 100ms with minimal resource usage, demonstrating its
capability for efficient deployment, broad applicability, and timely
responsiveness. This method thus provides a practical dynamic safety solution
for the LLM-based agents.

</details>


### [80] [A Multi-modal Fusion Network for Terrain Perception Based on Illumination Aware](https://arxiv.org/abs/2505.11066)
*Rui Wang,Shichun Yang,Yuyi Chen,Zhuoyang Li,Zexiang Tong,Jianyi Xu,Jiayi Lu,Xinjie Feng,Yaoguang Cao*

Main category: cs.AI

TL;DR: 提出了一种光照感知的多模态融合网络（IMF），通过结合外部感知和本体感知，优化光照条件下的道路地形感知。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶车辆传感器（如摄像头和激光雷达）易受光照和天气变化影响，难以实时感知道路条件。

Method: 设计了光照感知子网络以估计光照特征，并构建多模态融合网络动态调整不同模态权重，优化训练过程。

Result: 实验表明IMF优于现有方法，多模态融合在变化光照条件下感知道路地形具有全面优势。

Conclusion: IMF通过光照感知和多模态融合，显著提升了自动驾驶车辆在复杂光照条件下的道路感知能力。

Abstract: Road terrains play a crucial role in ensuring the driving safety of
autonomous vehicles (AVs). However, existing sensors of AVs, including cameras
and Lidars, are susceptible to variations in lighting and weather conditions,
making it challenging to achieve real-time perception of road conditions. In
this paper, we propose an illumination-aware multi-modal fusion network (IMF),
which leverages both exteroceptive and proprioceptive perception and optimizes
the fusion process based on illumination features. We introduce an
illumination-perception sub-network to accurately estimate illumination
features. Moreover, we design a multi-modal fusion network which is able to
dynamically adjust weights of different modalities according to illumination
features. We enhance the optimization process by pre-training of the
illumination-perception sub-network and incorporating illumination loss as one
of the training constraints. Extensive experiments demonstrate that the IMF
shows a superior performance compared to state-of-the-art methods. The
comparison results with single modality perception methods highlight the
comprehensive advantages of multi-modal fusion in accurately perceiving road
terrains under varying lighting conditions. Our dataset is available at:
https://github.com/lindawang2016/IMF.

</details>


### [81] [Analysis of Customer Journeys Using Prototype Detection and Counterfactual Explanations for Sequential Data](https://arxiv.org/abs/2505.11086)
*Keita Kinjo*

Main category: cs.AI

TL;DR: 本文提出了一种三步骤方法分析客户旅程，包括定义序列距离、预测购买概率和推荐反事实序列以提高购买率。


<details>
  <summary>Details</summary>
Motivation: 全渠道平台的兴起引发了对客户旅程的兴趣，但定量研究和综合分析因数据序列性和分析复杂性而不足。

Method: 1. 定义序列距离并可视化代表性序列；2. 基于距离预测购买概率；3. 对无购买序列推荐反事实序列以提高购买率。

Result: 实验表明，典型序列可被提取，且对购买重要的部分可被检测。

Conclusion: 该方法可支持多种营销活动的改进。

Abstract: Recently, the proliferation of omni-channel platforms has attracted interest
in customer journeys, particularly regarding their role in developing marketing
strategies. However, few efforts have been taken to quantitatively study or
comprehensively analyze them owing to the sequential nature of their data and
the complexity involved in analysis. In this study, we propose a novel approach
comprising three steps for analyzing customer journeys. First, the distance
between sequential data is defined and used to identify and visualize
representative sequences. Second, the likelihood of purchase is predicted based
on this distance. Third, if a sequence suggests no purchase, counterfactual
sequences are recommended to increase the probability of a purchase using a
proposed method, which extracts counterfactual explanations for sequential
data. A survey was conducted, and the data were analyzed; the results revealed
that typical sequences could be extracted, and the parts of those sequences
important for purchase could be detected. We believe that the proposed approach
can support improvements in various marketing activities.

</details>


### [82] [Group Think: Multiple Concurrent Reasoning Agents Collaborating at Token Level Granularity](https://arxiv.org/abs/2505.11107)
*Chan-Jan Hsu,Davide Buffelli,Jamie McGowan,Feng-Ting Liao,Yi-Chang Chen,Sattar Vakili,Da-shan Shiu*

Main category: cs.AI

TL;DR: 提出Group Think方法，通过单个LLM模拟多个并发推理代理，实现动态协作，降低延迟并提高推理质量。


<details>
  <summary>Details</summary>
Motivation: 现有多代理协作方法通常基于轮询机制，增加了延迟。Group Think旨在通过并发推理和动态协作解决这一问题。

Method: Group Think让单个LLM模拟多个代理，共享推理进度，实现动态调整。支持在本地GPU上高效运行。

Result: 实验证明Group Think能显著降低延迟，减少冗余推理，提高生成质量。

Conclusion: Group Think为未来LLM的高效协作推理提供了新方向。

Abstract: Recent advances in large language models (LLMs) have demonstrated the power
of reasoning through self-generated chains of thought. Multiple reasoning
agents can collaborate to raise joint reasoning quality above individual
outcomes. However, such agents typically interact in a turn-based manner,
trading increased latency for improved quality. In this paper, we propose Group
Think--a single LLM that acts as multiple concurrent reasoning agents, or
thinkers. With shared visibility into each other's partial generation progress,
Group Think introduces a new concurrent-reasoning paradigm in which multiple
reasoning trajectories adapt dynamically to one another at the token level. For
example, a reasoning thread may shift its generation mid-sentence upon
detecting that another thread is better positioned to continue. This
fine-grained, token-level collaboration enables Group Think to reduce redundant
reasoning and improve quality while achieving significantly lower latency.
Moreover, its concurrent nature allows for efficient utilization of idle
computational resources, making it especially suitable for edge inference,
where very small batch size often underutilizes local~GPUs. We give a simple
and generalizable modification that enables any existing LLM to perform Group
Think on a local GPU. We also present an evaluation strategy to benchmark
reasoning latency and empirically demonstrate latency improvements using
open-source LLMs that were not explicitly trained for Group Think. We hope this
work paves the way for future LLMs to exhibit more sophisticated and more
efficient collaborative behavior for higher quality generation.

</details>


### [83] [Predicting Student Dropout Risk With A Dual-Modal Abrupt Behavioral Changes Approach](https://arxiv.org/abs/2505.11119)
*Jiabei Cheng,Zhen-Qun Yang,Jiannong Cao,Yu Yang,Xinzhe Zheng*

Main category: cs.AI

TL;DR: 论文提出了一种双模态多尺度滑动窗口模型（DMSW），通过结合学业表现和行为数据动态捕捉学生行为模式，显著提高了辍学风险预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 在离线教育环境中，数据质量差、规模有限和高度异质性限制了高级机器学习模型的应用。同时，教育理论缺乏可量化指标，难以用于数据驱动建模。

Method: 通过数据分析和教育文献综述，识别学生行为的突变作为辍学风险的早期信号，提出DMSW模型动态捕捉行为模式。

Result: DMSW模型比传统方法预测准确率提高15%，帮助教育者更早识别高风险学生并提供支持。

Conclusion: 该研究填补了理论与实践的差距，为教育者提供了创新的工具，以提升学生留存率和教育成果。

Abstract: Timely prediction of students at high risk of dropout is critical for early
intervention and improving educational outcomes. However, in offline
educational settings, poor data quality, limited scale, and high heterogeneity
often hinder the application of advanced machine learning models. Furthermore,
while educational theories provide valuable insights into dropout phenomena,
the lack of quantifiable metrics for key indicators limits their use in
data-driven modeling. Through data analysis and a review of educational
literature, we identified abrupt changes in student behavior as key early
signals of dropout risk. To address this, we propose the Dual-Modal Multiscale
Sliding Window (DMSW) Model, which integrates academic performance and
behavioral data to dynamically capture behavior patterns using minimal data.
The DMSW model improves prediction accuracy by 15% compared to traditional
methods, enabling educators to identify high-risk students earlier, provide
timely support, and foster a more inclusive learning environment. Our analysis
highlights key behavior patterns, offering practical insights for preventive
strategies and tailored support. These findings bridge the gap between theory
and practice in dropout prediction, giving educators an innovative tool to
enhance student retention and outcomes.

</details>


### [84] [Navigating the Alpha Jungle: An LLM-Powered MCTS Framework for Formulaic Factor Mining](https://arxiv.org/abs/2505.11122)
*Yu Shi,Yitong Duan,Jian Li*

Main category: cs.AI

TL;DR: 本文提出了一种结合大型语言模型（LLM）和蒙特卡洛树搜索（MCTS）的新框架，用于高效挖掘可解释的Alpha因子，优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统Alpha因子挖掘依赖人工经验，而自动化方法如遗传编程或强化学习存在搜索效率低或因子可解释性差的问题。

Method: 通过LLM的指令跟随和推理能力，结合MCTS驱动的探索，迭代生成和优化符号化Alpha公式，并引入频繁子树避免机制提升效率。

Result: 在真实股票市场数据上的实验表明，该方法在预测准确性、交易性能和可解释性上优于现有方法。

Conclusion: 该框架为公式化Alpha因子挖掘提供了更高效的解决方案，同时提升了因子的质量和可解释性。

Abstract: Alpha factor mining is pivotal in quantitative investment for identifying
predictive signals from complex financial data. While traditional formulaic
alpha mining relies on human expertise, contemporary automated methods, such as
those based on genetic programming or reinforcement learning, often suffer from
search inefficiency or yield poorly interpretable alpha factors. This paper
introduces a novel framework that integrates Large Language Models (LLMs) with
Monte Carlo Tree Search (MCTS) to overcome these limitations. Our approach
leverages the LLM's instruction-following and reasoning capability to
iteratively generate and refine symbolic alpha formulas within an MCTS-driven
exploration. A key innovation is the guidance of MCTS exploration by rich,
quantitative feedback from financial backtesting of each candidate factor,
enabling efficient navigation of the vast search space. Furthermore, a frequent
subtree avoidance mechanism is introduced to bolster search efficiency and
alpha factor performance. Experimental results on real-world stock market data
demonstrate that our LLM-based framework outperforms existing methods by mining
alphas with superior predictive accuracy, trading performance, and improved
interpretability, while offering a more efficient solution for formulaic alpha
mining.

</details>


### [85] [Scalability of Reinforcement Learning Methods for Dispatching in Semiconductor Frontend Fabs: A Comparison of Open-Source Models with Real Industry Datasets](https://arxiv.org/abs/2505.11135)
*Patrick Stöckermann,Henning Südfeld,Alessandro Immordino,Thomas Altenmüller,Marc Wegmann,Martin Gebser,Konstantin Schekotihin,Georg Seidel,Chew Wye Chan,Fei Fei Zhang*

Main category: cs.AI

TL;DR: 论文比较了开源仿真模型与真实行业数据集，评估了强化学习方法在半导体制造调度中的表现，发现基于进化策略的方法优于策略梯度方法，并强调了瓶颈工具选择和多样化训练数据的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有基准数据集（如Minifab或SMT2020）缺乏真实场景的复杂细节和约束，需通过更真实的仿真模型评估优化方法的扩展性。

Method: 采用强化学习方法（策略梯度和进化策略），结合开源仿真模型与真实行业数据集，评估优化方法的扩展性和效果。

Result: 进化策略方法在扩展性上优于策略梯度方法，真实数据集上延迟改善达4%，吞吐量改善1%；在简单模型中改善更显著。

Conclusion: 进化策略方法在复杂场景中表现更优，瓶颈工具选择和多样化训练数据是关键，但计算成本较高。

Abstract: Benchmark datasets are crucial for evaluating approaches to scheduling or
dispatching in the semiconductor industry during the development and deployment
phases. However, commonly used benchmark datasets like the Minifab or SMT2020
lack the complex details and constraints found in real-world scenarios. To
mitigate this shortcoming, we compare open-source simulation models with a real
industry dataset to evaluate how optimization methods scale with different
levels of complexity. Specifically, we focus on Reinforcement Learning methods,
performing optimization based on policy-gradient and Evolution Strategies. Our
research provides insights into the effectiveness of these optimization methods
and their applicability to realistic semiconductor frontend fab simulations. We
show that our proposed Evolution Strategies-based method scales much better
than a comparable policy-gradient-based approach. Moreover, we identify the
selection and combination of relevant bottleneck tools to control by the agent
as crucial for an efficient optimization. For the generalization across
different loading scenarios and stochastic tool failure patterns, we achieve
advantages when utilizing a diverse training dataset. While the overall
approach is computationally expensive, it manages to scale well with the number
of CPU cores used for training. For the real industry dataset, we achieve an
improvement of up to 4% regarding tardiness and up to 1% regarding throughput.
For the less complex open-source models Minifab and SMT2020, we observe
double-digit percentage improvement in tardiness and single digit percentage
improvement in throughput by use of Evolution Strategies.

</details>


### [86] [Reinforcement Learning for AMR Charging Decisions: The Impact of Reward and Action Space Design](https://arxiv.org/abs/2505.11136)
*Janik Bischoff,Alexandru Rinciog,Anne Meyer*

Main category: cs.AI

TL;DR: 本文提出了一种新颖的强化学习（RL）设计，用于优化大规模块堆叠仓库中自主移动机器人的充电策略，并探讨了不同奖励和动作空间配置对性能的影响。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于通过强化学习优化机器人的充电策略，以提升服务时间效率，并比较不同设计配置的优劣。

Method: 方法包括扩展SLAPStack仿真框架以适应充电策略，提出新的RL设计，并引入多种自适应基线启发式方法，使用PPO代理进行实验。

Result: 结果表明，灵活的RL方法在服务时间上优于启发式策略，但存在收敛时间长和不稳定的问题，而引导式配置则更稳定但泛化能力有限。

Conclusion: 结论强调了灵活与引导式RL设计的权衡，并展示了RL在充电策略优化中的潜力。

Abstract: We propose a novel reinforcement learning (RL) design to optimize the
charging strategy for autonomous mobile robots in large-scale block stacking
warehouses. RL design involves a wide array of choices that can mostly only be
evaluated through lengthy experimentation. Our study focuses on how different
reward and action space configurations, ranging from flexible setups to more
guided, domain-informed design configurations, affect the agent performance.
Using heuristic charging strategies as a baseline, we demonstrate the
superiority of flexible, RL-based approaches in terms of service times.
Furthermore, our findings highlight a trade-off: While more open-ended designs
are able to discover well-performing strategies on their own, they may require
longer convergence times and are less stable, whereas guided configurations
lead to a more stable learning process but display a more limited
generalization potential. Our contributions are threefold. First, we extend
SLAPStack, an open-source, RL-compatible simulation-framework to accommodate
charging strategies. Second, we introduce a novel RL design for tackling the
charging strategy problem. Finally, we introduce several novel adaptive
baseline heuristics and reproducibly evaluate the design using a Proximal
Policy Optimization agent and varying different design configurations, with a
focus on reward.

</details>


### [87] [Feasibility with Language Models for Open-World Compositional Zero-Shot Learning](https://arxiv.org/abs/2505.11181)
*Jae Myung Kim,Stephan Alaniz,Cordelia Schmid,Zeynep Akata*

Main category: cs.AI

TL;DR: 论文提出了一种利用大型语言模型（LLMs）判断状态-对象组合可行性的方法（FLM），显著提升了开放世界组合零样本学习（OW-CZSL）的性能。


<details>
  <summary>Details</summary>
Motivation: 在开放世界组合零样本学习中，零样本预测器对所有可能的状态-对象组合表现不佳，因此需要外部辅助知识来判断其可行性。

Method: FLM通过查询LLM关于状态-对象组合的可行性，并提取正面回答的输出logit，同时利用LLM的上下文学习能力减少误导。

Result: 实验表明，Vicuna和ChatGPT表现最佳，FLM在三个基准测试中均显著提升了OW-CZSL性能。

Conclusion: FLM是一种简单有效的方法，利用LLM的语义理解能力显著改善了状态-对象组合的可行性判断。

Abstract: Humans can easily tell if an attribute (also called state) is realistic,
i.e., feasible, for an object, e.g. fire can be hot, but it cannot be wet. In
Open-World Compositional Zero-Shot Learning, when all possible state-object
combinations are considered as unseen classes, zero-shot predictors tend to
perform poorly. Our work focuses on using external auxiliary knowledge to
determine the feasibility of state-object combinations. Our Feasibility with
Language Model (FLM) is a simple and effective approach that leverages Large
Language Models (LLMs) to better comprehend the semantic relationships between
states and objects. FLM involves querying an LLM about the feasibility of a
given pair and retrieving the output logit for the positive answer. To mitigate
potential misguidance of the LLM given that many of the state-object
compositions are rare or completely infeasible, we observe that the in-context
learning ability of LLMs is essential. We present an extensive study
identifying Vicuna and ChatGPT as best performing, and we demonstrate that our
FLM consistently improves OW-CZSL performance across all three benchmarks.

</details>


### [88] [Can Global XAI Methods Reveal Injected Bias in LLMs? SHAP vs Rule Extraction vs RuleSHAP](https://arxiv.org/abs/2505.11189)
*Francesco Sovrano*

Main category: cs.AI

TL;DR: 本文探讨了全局可解释AI（XAI）方法在检测大型语言模型（LLMs）中的偏见时的有效性，并提出了一种结合SHAP和RuleFit的新方法RuleSHAP，显著提升了检测能力。


<details>
  <summary>Details</summary>
Motivation: 生成式AI系统可能传播错误信息和偏见，影响联合国可持续发展目标（SDGs）。当前XAI工具难以处理LLMs的非数值特性，因此需要更有效的检测方法。

Method: 通过文本到序数映射策略将非数值输入/输出转换为数值特征，并注入不同类型的偏见（单变量、联合、非凸）到LLMs中，使用全局XAI方法检测。

Result: RuleFit对联合和非凸偏见效果不佳，SHAP能近似联合偏见但无法生成可操作规则。RuleSHAP结合两者，检测能力平均提升94%（MRR@1）。

Conclusion: RuleSHAP显著提升了LLMs中非单变量偏见的检测能力，为XAI工具在复杂模型中的应用提供了新思路。

Abstract: Generative AI systems can help spread information but also misinformation and
biases, potentially undermining the UN Sustainable Development Goals (SDGs).
Explainable AI (XAI) aims to reveal the inner workings of AI systems and expose
misbehaviours or biases. However, current XAI tools, built for simpler models,
struggle to handle the non-numerical nature of large language models (LLMs).
This paper examines the effectiveness of global XAI methods, such as
rule-extraction algorithms and SHAP, in detecting bias in LLMs. To do so, we
first show a text-to-ordinal mapping strategy to convert non-numerical
inputs/outputs into numerical features, enabling these tools to identify (some)
misinformation-related biases in LLM-generated content. Then, we inject
non-linear biases of varying complexity (univariate, conjunctive, and
non-convex) into widespread LLMs like ChatGPT and Llama via system
instructions, using global XAI methods to detect them. This way, we found that
RuleFit struggles with conjunctive and non-convex biases, while SHAP can
approximate conjunctive biases but cannot express them as actionable rules.
Hence, we introduce RuleSHAP, a global rule extraction algorithm combining SHAP
and RuleFit to detect more non-univariate biases, improving injected bias
detection over RuleFit by +94% (MRR@1) on average.

</details>


### [89] [Multi-Modal Multi-Task (M3T) Federated Foundation Models for Embodied AI: Potentials and Challenges for Edge Integration](https://arxiv.org/abs/2505.11191)
*Kasra Borazjani,Payam Abdisarabshali,Fardis Nadimi,Naji Khosravan,Minghui Liwang,Xianbin Wang,Yiguang Hong,Seyyedali Hosseinalipour*

Main category: cs.AI

TL;DR: 论文提出了一种名为Federated Foundation Models (FFMs)的新范式，结合了Foundation Models (FMs)的泛化能力和Federated Learning (FL)的隐私保护特性，以解决多模态、个性化且资源受限的具身AI系统的挑战。


<details>
  <summary>Details</summary>
Motivation: 随着具身AI系统在多模态、个性化和交互性方面的需求增加，现有方法（如FMs和FL）单独使用时无法满足复杂需求，因此需要一种结合两者优势的新方法。

Method: 提出Federated Foundation Models (FFMs)，将多模态多任务FMs与隐私保护的FL结合，并定义了部署框架EMBODY，涵盖六个关键维度。

Result: 论文提出了FFMs的概念，并详细分析了其在具身AI系统中的部署挑战和研究方向，同时提供了评估框架。

Conclusion: FFMs为具身AI系统提供了一种结合泛化能力和隐私保护的新方法，未来研究应关注EMBODY框架中的具体挑战。

Abstract: As embodied AI systems become increasingly multi-modal, personalized, and
interactive, they must learn effectively from diverse sensory inputs, adapt
continually to user preferences, and operate safely under resource and privacy
constraints. These challenges expose a pressing need for machine learning
models capable of swift, context-aware adaptation while balancing model
generalization and personalization. Here, two methods emerge as suitable
candidates, each offering parts of these capabilities: Foundation Models (FMs)
provide a pathway toward generalization across tasks and modalities, whereas
Federated Learning (FL) offers the infrastructure for distributed,
privacy-preserving model updates and user-level model personalization. However,
when used in isolation, each of these approaches falls short of meeting the
complex and diverse capability requirements of real-world embodied
environments. In this vision paper, we introduce Federated Foundation Models
(FFMs) for embodied AI, a new paradigm that unifies the strengths of
multi-modal multi-task (M3T) FMs with the privacy-preserving distributed nature
of FL, enabling intelligent systems at the wireless edge. We collect critical
deployment dimensions of FFMs in embodied AI ecosystems under a unified
framework, which we name "EMBODY": Embodiment heterogeneity, Modality richness
and imbalance, Bandwidth and compute constraints, On-device continual learning,
Distributed control and autonomy, and Yielding safety, privacy, and
personalization. For each, we identify concrete challenges and envision
actionable research directions. We also present an evaluation framework for
deploying FFMs in embodied AI systems, along with the associated trade-offs.

</details>


### [90] [GLOVA: Global and Local Variation-Aware Analog Circuit Design with Risk-Sensitive Reinforcement Learning](https://arxiv.org/abs/2505.11208)
*Dongjun Kim,Junwoo Park,Chaehyeon Shin,Jaeheon Jung,Kyungho Shin,Seungheon Baek,Sanghyuk Heo,Woongrae Kim,Inchul Jeong,Joohwan Cho,Jongsun Park*

Main category: cs.AI

TL;DR: GLOVA是一个模拟电路尺寸优化框架，通过风险敏感强化学习和集成批评器提高对PVT变化的鲁棒性，显著提升样本效率并减少时间成本。


<details>
  <summary>Details</summary>
Motivation: 模拟/混合信号电路设计面临PVT变化导致的性能下降问题，现有自动化方法未充分解决实际晶圆中的失配问题。

Method: 采用风险敏感强化学习考虑可靠性边界，引入集成批评器实现高效学习，提出μ-σ评估和仿真重排序方法降低验证成本。

Result: 相比现有技术，GLOVA在样本效率上提升80.5倍，时间成本减少76倍。

Conclusion: GLOVA有效提升了模拟电路设计的鲁棒性和效率，适用于工业级PVT变化评估。

Abstract: Analog/mixed-signal circuit design encounters significant challenges due to
performance degradation from process, voltage, and temperature (PVT)
variations. To achieve commercial-grade reliability, iterative manual design
revisions and extensive statistical simulations are required. While several
studies have aimed to automate variation aware analog design to reduce
time-to-market, the substantial mismatches in real-world wafers have not been
thoroughly addressed. In this paper, we present GLOVA, an analog circuit sizing
framework that effectively manages the impact of diverse random mismatches to
improve robustness against PVT variations. In the proposed approach,
risk-sensitive reinforcement learning is leveraged to account for the
reliability bound affected by PVT variations, and ensemble-based critic is
introduced to achieve sample-efficient learning. For design verification, we
also propose $\mu$-$\sigma$ evaluation and simulation reordering method to
reduce simulation costs of identifying failed designs. GLOVA supports
verification through industrial-level PVT variation evaluation methods,
including corner simulation as well as global and local Monte Carlo (MC)
simulations. Compared to previous state-of-the-art variation-aware analog
sizing frameworks, GLOVA achieves up to 80.5$\times$ improvement in sample
efficiency and 76.0$\times$ reduction in time.

</details>


### [91] [Is PRM Necessary? Problem-Solving RL Implicitly Induces PRM Capability in LLMs](https://arxiv.org/abs/2505.11227)
*Zhangying Feng,Qianglong Chen,Ning Lu,Yongqian Li,Siqi Cheng,Shuangmu Peng,Duyu Tang,Shengcai Liu,Zhirui Zhang*

Main category: cs.AI

TL;DR: 论文探讨了纯强化学习（RL）在提升大型语言模型（LLMs）推理能力中的作用，挑战了过程奖励模型（PRM）的必要性，并提出自监督框架Self-PRM以改进结果。


<details>
  <summary>Details</summary>
Motivation: 研究旨在验证纯RL训练是否能独立提升LLMs的推理能力，并探索RL与PRM能力之间的关系。

Method: 通过纯RL训练和提出的Self-PRM框架（模型自主评估和重排解决方案）进行系统性研究。

Result: 纯RL训练不仅能提升问题解决能力，还能协同增强PRM能力；Self-PRM在基准测试中提高了准确性，但在难题上表现不佳。

Conclusion: PRM可能非提升复杂推理的必要条件，纯RL训练可同时增强问题解决和PRM能力；未来需进一步扩展RL以提高奖励对齐和自省准确性。

Abstract: The development of reasoning capabilities represents a critical frontier in
large language models (LLMs) research, where reinforcement learning (RL) and
process reward models (PRMs) have emerged as predominant methodological
frameworks. Contrary to conventional wisdom, empirical evidence from
DeepSeek-R1 demonstrates that pure RL training focused on mathematical
problem-solving can progressively enhance reasoning abilities without PRM
integration, challenging the perceived necessity of process supervision. In
this study, we conduct a systematic investigation of the relationship between
RL training and PRM capabilities. Our findings demonstrate that problem-solving
proficiency and process supervision capabilities represent complementary
dimensions of reasoning that co-evolve synergistically during pure RL training.
In particular, current PRMs underperform simple baselines like majority voting
when applied to state-of-the-art models such as DeepSeek-R1 and QwQ-32B. To
address this limitation, we propose Self-PRM, an introspective framework in
which models autonomously evaluate and rerank their generated solutions through
self-reward mechanisms. Although Self-PRM consistently improves the accuracy of
the benchmark (particularly with larger sample sizes), analysis exposes
persistent challenges: The approach exhibits low precision (<10\%) on difficult
problems, frequently misclassifying flawed solutions as valid. These analyses
underscore the need for continued RL scaling to improve reward alignment and
introspective accuracy. Overall, our findings suggest that PRM may not be
essential for enhancing complex reasoning, as pure RL not only improves
problem-solving skills but also inherently fosters robust PRM capabilities. We
hope these findings provide actionable insights for building more reliable and
self-aware complex reasoning models.

</details>


### [92] [LD-Scene: LLM-Guided Diffusion for Controllable Generation of Adversarial Safety-Critical Driving Scenarios](https://arxiv.org/abs/2505.11247)
*Mingxing Peng,Yuting Xie,Xusen Guo,Ruoyu Yao,Hai Yang,Jun Ma*

Main category: cs.AI

TL;DR: LD-Scene是一个结合大型语言模型（LLMs）和潜在扩散模型（LDMs）的框架，用于通过自然语言生成用户可控的对抗场景，以评估自动驾驶系统的安全性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 安全关键场景在真实驾驶数据中罕见且难以收集，现有方法可控性有限且不够用户友好，需要大量专家知识。

Method: LD-Scene通过LDM捕捉真实驾驶轨迹分布，LLM模块将用户查询转化为对抗损失函数，生成符合用户需求的场景。

Result: 在nuScenes数据集上的实验表明，LD-Scene能生成真实、多样且有效的对抗场景，性能达到最先进水平。

Conclusion: LD-Scene提供了对对抗行为的细粒度控制，有助于针对特定驾驶场景进行更有效的测试。

Abstract: Ensuring the safety and robustness of autonomous driving systems necessitates
a comprehensive evaluation in safety-critical scenarios. However, these
safety-critical scenarios are rare and difficult to collect from real-world
driving data, posing significant challenges to effectively assessing the
performance of autonomous vehicles. Typical existing methods often suffer from
limited controllability and lack user-friendliness, as extensive expert
knowledge is essentially required. To address these challenges, we propose
LD-Scene, a novel framework that integrates Large Language Models (LLMs) with
Latent Diffusion Models (LDMs) for user-controllable adversarial scenario
generation through natural language. Our approach comprises an LDM that
captures realistic driving trajectory distributions and an LLM-based guidance
module that translates user queries into adversarial loss functions,
facilitating the generation of scenarios aligned with user queries. The
guidance module integrates an LLM-based Chain-of-Thought (CoT) code generator
and an LLM-based code debugger, enhancing the controllability and robustness in
generating guidance functions. Extensive experiments conducted on the nuScenes
dataset demonstrate that LD-Scene achieves state-of-the-art performance in
generating realistic, diverse, and effective adversarial scenarios.
Furthermore, our framework provides fine-grained control over adversarial
behaviors, thereby facilitating more effective testing tailored to specific
driving scenarios.

</details>


### [93] [SelfBudgeter: Adaptive Token Allocation for Efficient LLM Reasoning](https://arxiv.org/abs/2505.11274)
*Zheng Li,Qingxiu Dong,Jingyuan Ma,Di Zhang,Zhifang Sui*

Main category: cs.AI

TL;DR: SelfBudgeter是一种自适应的可控推理策略，通过双阶段训练和预算引导的GPRO强化学习，有效减少推理长度并保持准确性。


<details>
  <summary>Details</summary>
Motivation: 当前大型推理模型在处理简单和复杂查询时效率低下，导致资源浪费和延迟。

Method: 采用双阶段训练：先预估计查询难度，再引入预算引导的GPRO强化学习。

Result: 在MATH基准测试中，响应长度压缩达74.47%，同时保持准确性。

Conclusion: SelfBudgeter能根据问题复杂度合理分配预算，提升推理效率。

Abstract: Recently, large reasoning models demonstrate exceptional performance on
various tasks. However, reasoning models inefficiently over-process both
trivial and complex queries, leading to resource waste and prolonged user
latency. To address this challenge, we propose SelfBudgeter - a self-adaptive
controllable reasoning strategy for efficient reasoning. Our approach adopts a
dual-phase training paradigm: first, the model learns to pre-estimate the
reasoning cost based on the difficulty of the query. Then, we introduce
budget-guided GPRO for reinforcement learning, which effectively maintains
accuracy while reducing output length. SelfBudgeter allows users to anticipate
generation time and make informed decisions about continuing or interrupting
the process. Furthermore, our method enables direct manipulation of reasoning
length via pre-filling token budget. Experimental results demonstrate that
SelfBudgeter can rationally allocate budgets according to problem complexity,
achieving up to 74.47% response length compression on the MATH benchmark while
maintaining nearly undiminished accuracy.

</details>


### [94] [Meta-World+: An Improved, Standardized, RL Benchmark](https://arxiv.org/abs/2505.11289)
*Reginald McLean,Evangelos Chatzaroulas,Luc McCutcheon,Frank Röder,Tianhe Yu,Zhanpeng He,K. R. Zentner,Ryan Julian,J K Terry,Isaac Woungang,Nariman Farsad,Pablo Samuel Castro*

Main category: cs.AI

TL;DR: 论文分析了Meta-World基准测试的版本变化问题，并发布了一个新的开源版本，确保结果可复现且更易用。


<details>
  <summary>Details</summary>
Motivation: 解决Meta-World基准测试因未记录的版本变化导致算法比较不公平的问题。

Method: 通过梳理历史版本，设计并发布新的开源版本Meta-World。

Result: 新版本实现了结果的可复现性，提升了技术易用性，并增加了任务集的自定义功能。

Conclusion: 新版本的Meta-World为多任务和元强化学习基准测试提供了更公平、灵活的工具。

Abstract: Meta-World is widely used for evaluating multi-task and meta-reinforcement
learning agents, which are challenged to master diverse skills simultaneously.
Since its introduction however, there have been numerous undocumented changes
which inhibit a fair comparison of algorithms. This work strives to
disambiguate these results from the literature, while also leveraging the past
versions of Meta-World to provide insights into multi-task and
meta-reinforcement learning benchmark design. Through this process we release a
new open-source version of Meta-World
(https://github.com/Farama-Foundation/Metaworld/) that has full reproducibility
of past results, is more technically ergonomic, and gives users more control
over the tasks that are included in a task set.

</details>


### [95] [Extracting Explainable Dates From Medical Images By Reverse-Engineering UNIX Timestamps](https://arxiv.org/abs/2505.11451)
*Lee Harris,James Bentham,Philippe De Wilde*

Main category: cs.AI

TL;DR: 论文探讨了如何从文本中提取日期数据，比较了手动和自动生成正则表达式的方法，发现正则表达式合成能更准确地识别复杂日期和日期范围。


<details>
  <summary>Details</summary>
Motivation: 日期数据对医疗决策至关重要，但提取方法尚不明确，现有方法（如复杂AI模型或正则表达式）存在局限性。

Method: 测试公开正则表达式，手动创建可分解的正则表达式，并使用正则表达式合成技术自动生成正则表达式。

Result: 正则表达式合成生成的正则表达式误报较少，但漏报略有增加。

Conclusion: 正则表达式合成是一种新方法，能有效识别复杂日期和日期范围。

Abstract: Dates often contribute towards highly impactful medical decisions, but it is
rarely clear how to extract this data. AI has only just begun to be used
transcribe such documents, and common methods are either to trust that the
output produced by a complex AI model, or to parse the text using regular
expressions. Recent work has established that regular expressions are an
explainable form of logic, but it is difficult to decompose these into the
component parts that are required to construct precise UNIX timestamps. First,
we test publicly-available regular expressions, and we found that these were
unable to capture a significant number of our dates. Next, we manually created
easily-decomposable regular expressions, and we found that these were able to
detect the majority of real dates, but also a lot of sequences of text that
look like dates. Finally, we used regular expression synthesis to automatically
identify regular expressions from the reverse-engineered UNIX timestamps that
we created. We find that regular expressions created by regular expression
synthesis detect far fewer sequences of text that look like dates than those
that were manually created, at the cost of a slight increase to the number of
missed dates. Overall, our results show that regular expressions can be created
through regular expression synthesis to identify complex dates and date ranges
in text transcriptions. To our knowledge, our proposed way of learning
deterministic logic by reverse-engineering several many-one mappings and
feeding these into a regular expression synthesiser is a new approach.

</details>


### [96] [Automatic Reward Shaping from Confounded Offline Data](https://arxiv.org/abs/2505.11478)
*Mingxuan Li,Junzhe Zhang,Elias Bareinboim*

Main category: cs.AI

TL;DR: 本文提出了一种针对高维复杂环境中存在未观测混杂偏差的深度强化学习算法，优于标准DQN。


<details>
  <summary>Details</summary>
Motivation: 研究在存在未观测混杂偏差的高维复杂环境中，如何通过离策略学习优化决策。

Method: 基于深度Q网络（DQN），提出了一种新算法，寻找与观测数据兼容的最坏情况下安全策略。

Result: 在12个存在混杂偏差的Atari游戏中，新算法在所有输入行为与目标策略不匹配的游戏中均优于标准DQN。

Conclusion: 新算法能有效应对未观测混杂偏差，提升强化学习性能。

Abstract: A key task in Artificial Intelligence is learning effective policies for
controlling agents in unknown environments to optimize performance measures.
Off-policy learning methods, like Q-learning, allow learners to make optimal
decisions based on past experiences. This paper studies off-policy learning
from biased data in complex and high-dimensional domains where \emph{unobserved
confounding} cannot be ruled out a priori. Building on the well-celebrated Deep
Q-Network (DQN), we propose a novel deep reinforcement learning algorithm
robust to confounding biases in observed data. Specifically, our algorithm
attempts to find a safe policy for the worst-case environment compatible with
the observations. We apply our method to twelve confounded Atari games, and
find that it consistently dominates the standard DQN in all games where the
observed input to the behavioral and target policies mismatch and unobserved
confounders exist.

</details>


### [97] [MOSAAIC: Managing Optimization towards Shared Autonomy, Authority, and Initiative in Co-creation](https://arxiv.org/abs/2505.11481)
*Alayt Issak,Jeba Rezwana,Casper Harteveld*

Main category: cs.AI

TL;DR: 论文提出MOSAAIC框架，用于在人类与AI协同创作中平衡控制权，涵盖自主性、主动性和权威性三个维度，并通过案例验证其适用性。


<details>
  <summary>Details</summary>
Motivation: 研究人类与AI在协同创作中如何平衡控制权，以实现更有效的共创过程。

Method: 通过系统性文献综述分析172篇论文，提出MOSAAIC框架，并分析六个案例以验证其适用性。

Result: MOSAAIC框架成功识别了控制权的三个关键维度，并提供了优化策略，案例研究验证了其有效性。

Conclusion: MOSAAIC框架为人类与AI协同创作中的控制权分配提供了实用工具，有助于优化共创过程。

Abstract: Striking the appropriate balance between humans and co-creative AI is an open
research question in computational creativity. Co-creativity, a form of hybrid
intelligence where both humans and AI take action proactively, is a process
that leads to shared creative artifacts and ideas. Achieving a balanced dynamic
in co-creativity requires characterizing control and identifying strategies to
distribute control between humans and AI. We define control as the power to
determine, initiate, and direct the process of co-creation. Informed by a
systematic literature review of 172 full-length papers, we introduce MOSAAIC
(Managing Optimization towards Shared Autonomy, Authority, and Initiative in
Co-creation), a novel framework for characterizing and balancing control in
co-creation. MOSAAIC identifies three key dimensions of control: autonomy,
initiative, and authority. We supplement our framework with control
optimization strategies in co-creation. To demonstrate MOSAAIC's applicability,
we analyze the distribution of control in six existing co-creative AI case
studies and present the implications of using this framework.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [98] [Two Minds Better Than One: Collaborative Reward Modeling for LLM Alignment](https://arxiv.org/abs/2505.10597)
*Jiazheng Zhang,Wenqing Jing,Zizhuo Zhang,Zhiheng Xi,Shihan Dou,Rongxiang Weng,Jiahuan Li,Jingang Wang,MingXu Cai,Shibo Hong,Tao Gui,Qi Zhang*

Main category: cs.LG

TL;DR: 论文提出了一种协作奖励建模（CRM）方法，通过结合同行评审和课程学习，提高奖励模型在噪声偏好数据下的鲁棒性，显著提升了泛化性能。


<details>
  <summary>Details</summary>
Motivation: 人类反馈中的噪声偏好会导致奖励模型过拟合虚假模式，从而在策略优化中提供误导信号。因此，需要一种更鲁棒的方法来减少噪声的影响。

Method: 提出协作奖励建模（CRM）框架，通过并行训练两个奖励模型并互相评估数据选择来过滤噪声，同时采用课程学习从易到难结构化数据。

Result: 实验表明，CRM在40%标签噪声下在RewardBench上实现了高达9.94的准确率提升。

Conclusion: CRM是一种实用且通用的策略，能够显著提升奖励模型的鲁棒性和泛化能力，适用于隐式奖励对齐方法。

Abstract: Reward models (RMs) are essential for aligning large language models (LLMs)
with human values. However, noisy preferences in human feedback often lead to
reward misgeneralization, where RMs overfit to spurious patterns and provide
misleading signals during policy optimization. We systematically analyze the
training dynamics of preference pairs and identify that noisy examples are
harder to fit and introduce instability. Empirical evidence shows that LLMs
optimized using reward models trained on full noisy datasets perform worse than
those trained on filtered, high-quality preferences. To address this, we
propose Collaborative Reward Modeling (CRM), an online framework that enhances
robustness by combining peer review and curriculum learning. Two reward models
are trained in parallel and assess each other's data selections to filter out
potential noise. Curriculum learning structures the preference data from easy
to hard, ensuring synchronized training and stable feedback. Extensive
experiments demonstrate that CRM improves generalization, with up to 9.94
points of accuracy gain on RewardBench under 40 percent label noise. CRM is
also compatible with implicit-reward alignment methods, offering a practical
and versatile strategy for robust alignment.

</details>


### [99] [UDDETTS: Unifying Discrete and Dimensional Emotions for Controllable Emotional Text-to-Speech](https://arxiv.org/abs/2505.10599)
*Jiaxuan Liu,Zhenhua Ling*

Main category: cs.LG

TL;DR: UDDETTS提出了一种结合离散和维度情感控制的神经编解码语言模型，用于可控情感TTS，解决了传统方法在情感复杂性和连续性上的不足。


<details>
  <summary>Details</summary>
Motivation: 传统TTS方法依赖预定义离散情感标签，无法捕捉人类情感的复杂性和连续性，且缺乏大规模平衡情感数据集。

Method: 引入可解释的ADV空间描述维度情感，支持离散标签或非线性量化ADV值驱动的情感控制，采用半监督训练策略。

Result: UDDETTS在ADV空间三个维度上实现线性情感控制，展现出优越的端到端情感语音合成能力。

Conclusion: UDDETTS为可控情感TTS提供了更灵活和高效的方法，解决了传统方法的局限性。

Abstract: Recent neural codec language models have made great progress in the field of
text-to-speech (TTS), but controllable emotional TTS still faces many
challenges. Traditional methods rely on predefined discrete emotion labels to
control emotion categories and intensities, which can't capture the complexity
and continuity of human emotional perception and expression. The lack of
large-scale emotional speech datasets with balanced emotion distributions and
fine-grained emotion annotations often causes overfitting in synthesis models
and impedes effective emotion control. To address these issues, we propose
UDDETTS, a neural codec language model unifying discrete and dimensional
emotions for controllable emotional TTS. This model introduces the
interpretable Arousal-Dominance-Valence (ADV) space for dimensional emotion
description and supports emotion control driven by either discrete emotion
labels or nonlinearly quantified ADV values. Furthermore, a semi-supervised
training strategy is designed to comprehensively utilize diverse speech
datasets with different types of emotion annotations to train the UDDETTS.
Experiments show that UDDETTS achieves linear emotion control along the three
dimensions of ADV space, and exhibits superior end-to-end emotional speech
synthesis capabilities.

</details>


### [100] [Enhancing IoT Cyber Attack Detection in the Presence of Highly Imbalanced Data](https://arxiv.org/abs/2505.10600)
*Md. Ehsanul Haque,Md. Saymon Hosen Polash,Md Al-Imran Sanjida Simla,Md Alomgir Hossain,Sarwar Jahan*

Main category: cs.LG

TL;DR: 论文提出了一种混合采样技术，用于解决IoT网络中高度不平衡数据集下的入侵检测问题，并通过多种机器学习模型验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 由于IoT网络快速增长，网络风险激增，传统机器学习模型在高度不平衡数据集下难以准确检测攻击，因此需要开发更有效的入侵检测系统。

Method: 采用混合采样技术改善数据不平衡问题，并评估多种机器学习模型（如随机森林、软投票、SVC、KNN、MLP和逻辑回归）的性能。

Result: 随机森林表现最佳，Kappa分数为0.9903，测试准确率为0.9961，AUC为0.9994；软投票模型也表现优异，准确率为0.9952，AUC为0.9997。

Conclusion: 混合采样技术与稳健的模型和特征选择相结合，显著提高了高度不平衡数据环境下的IoT安全性。

Abstract: Due to the rapid growth in the number of Internet of Things (IoT) networks,
the cyber risk has increased exponentially, and therefore, we have to develop
effective IDS that can work well with highly imbalanced datasets. A high rate
of missed threats can be the result, as traditional machine learning models
tend to struggle in identifying attacks when normal data volume is much higher
than the volume of attacks. For example, the dataset used in this study reveals
a strong class imbalance with 94,659 instances of the majority class and only
28 instances of the minority class, making it quite challenging to determine
rare attacks accurately. The challenges presented in this research are
addressed by hybrid sampling techniques designed to improve data imbalance
detection accuracy in IoT domains. After applying these techniques, we evaluate
the performance of several machine learning models such as Random Forest, Soft
Voting, Support Vector Classifier (SVC), K-Nearest Neighbors (KNN), Multi-Layer
Perceptron (MLP), and Logistic Regression with respect to the classification of
cyber-attacks. The obtained results indicate that the Random Forest model
achieved the best performance with a Kappa score of 0.9903, test accuracy of
0.9961, and AUC of 0.9994. Strong performance is also shown by the Soft Voting
model, with an accuracy of 0.9952 and AUC of 0.9997, indicating the benefits of
combining model predictions. Overall, this work demonstrates the value of
hybrid sampling combined with robust model and feature selection for
significantly improving IoT security against cyber-attacks, especially in
highly imbalanced data environments.

</details>


### [101] [Continuity and Isolation Lead to Doubts or Dilemmas in Large Language Models](https://arxiv.org/abs/2505.10606)
*Hector Pasten,Felipe Urrutia,Hector Jimenez,Cristian B. Calderon,Cristóbal Rojas,Alexander Kozachinskiy*

Main category: cs.LG

TL;DR: 论文揭示了Transformer中的隔离和连续性现象，这些现象限制了其学习简单序列模式的能力。


<details>
  <summary>Details</summary>
Motivation: 理解Transformer的工作原理及其信息处理方式，以推动理论和实证研究的进步。

Method: 通过数学证明和严谨实验，验证了使用紧凑位置编码的Transformer中隔离和连续性现象的存在。

Result: 证明了隔离和连续性现象在所有使用紧凑位置编码的Transformer中普遍存在，并展示了这些理论限制在实际中的表现。

Conclusion: 研究揭示了Transformer的学习局限性，为进一步改进模型提供了理论基础。

Abstract: Understanding how Transformers work and how they process information is key
to the theoretical and empirical advancement of these machines. In this work,
we demonstrate the existence of two phenomena in Transformers, namely isolation
and continuity. Both of these phenomena hinder Transformers to learn even
simple pattern sequences. Isolation expresses that any learnable sequence must
be isolated from another learnable sequence, and hence some sequences cannot be
learned by a single Transformer at the same time. Continuity entails that an
attractor basin forms around a learned sequence, such that any sequence falling
in that basin will collapse towards the learned sequence. Here, we
mathematically prove these phenomena emerge in all Transformers that use
compact positional encoding, and design rigorous experiments, demonstrating
that the theoretical limitations we shed light on occur on the practical scale.

</details>


### [102] [MONAQ: Multi-Objective Neural Architecture Querying for Time-Series Analysis on Resource-Constrained Devices](https://arxiv.org/abs/2505.10607)
*Patara Trirat,Jae-Gil Lee*

Main category: cs.LG

TL;DR: MONAQ是一个利用大语言模型（LLM）的多目标神经架构查询框架，用于在资源受限的边缘设备上高效进行时间序列分析。


<details>
  <summary>Details</summary>
Motivation: 智能手机和物联网设备的普及需要高效的时间序列分析，但现有硬件感知神经架构搜索（NAS）方法未针对边缘部署的通用时间序列分析。

Method: MONAQ将NAS重新定义为多目标神经架构查询任务，结合多模态查询生成和LLM代理的多目标搜索，生成部署就绪的模型代码。

Result: 在15个数据集上的实验表明，MONAQ发现的模型优于手工模型和NAS基线，且更高效。

Conclusion: MONAQ通过整合多模态数据和LLM能力，为边缘设备上的时间序列分析提供了高效解决方案。

Abstract: The growing use of smartphones and IoT devices necessitates efficient
time-series analysis on resource-constrained hardware, which is critical for
sensing applications such as human activity recognition and air quality
prediction. Recent efforts in hardware-aware neural architecture search (NAS)
automate architecture discovery for specific platforms; however, none focus on
general time-series analysis with edge deployment. Leveraging the
problem-solving and reasoning capabilities of large language models (LLM), we
propose MONAQ, a novel framework that reformulates NAS into Multi-Objective
Neural Architecture Querying tasks. MONAQ is equipped with multimodal query
generation for processing multimodal time-series inputs and hardware
constraints, alongside an LLM agent-based multi-objective search to achieve
deployment-ready models via code generation. By integrating numerical data,
time-series images, and textual descriptions, MONAQ improves an LLM's
understanding of time-series data. Experiments on fifteen datasets demonstrate
that MONAQ-discovered models outperform both handcrafted models and NAS
baselines while being more efficient.

</details>


### [103] [How many measurements are enough? Bayesian recovery in inverse problems with general distributions](https://arxiv.org/abs/2505.10630)
*Ben Adcock,Nick Huang*

Main category: cs.LG

TL;DR: 论文研究了贝叶斯恢复在解决逆问题时的样本复杂度，重点分析了先验、前向算子和噪声分布的影响，并提出了非渐近边界条件。


<details>
  <summary>Details</summary>
Motivation: 研究贝叶斯恢复的样本复杂度，为逆问题提供稳定且高概率准确的恢复条件。

Method: 通过近似先验的后验采样，结合近似覆盖数和前向算子与噪声分布的集中边界，推导样本复杂度的非渐近边界。

Result: 样本复杂度与先验的内在复杂性（近似覆盖数）及前向算子和噪声的集中边界相关，特别适用于生成先验（DNN）。

Conclusion: 论文统一并扩展了现有工作，为贝叶斯逆问题的样本复杂度提供了严格的理论保证。

Abstract: We study the sample complexity of Bayesian recovery for solving inverse
problems with general prior, forward operator and noise distributions. We
consider posterior sampling according to an approximate prior $\mathcal{P}$,
and establish sufficient conditions for stable and accurate recovery with high
probability. Our main result is a non-asymptotic bound that shows that the
sample complexity depends on (i) the intrinsic complexity of $\mathcal{P}$,
quantified by its so-called approximate covering number, and (ii) concentration
bounds for the forward operator and noise distributions. As a key application,
we specialize to generative priors, where $\mathcal{P}$ is the pushforward of a
latent distribution via a Deep Neural Network (DNN). We show that the sample
complexity scales log-linearly with the latent dimension $k$, thus establishing
the efficacy of DNN-based priors. Generalizing existing results on
deterministic (i.e., non-Bayesian) recovery for the important problem of random
sampling with an orthogonal matrix $U$, we show how the sample complexity is
determined by the coherence of $U$ with respect to the support of
$\mathcal{P}$. Hence, we establish that coherence plays a fundamental role in
Bayesian recovery as well. Overall, our framework unifies and extends prior
work, providing rigorous guarantees for the sample complexity of solving
Bayesian inverse problems with arbitrary distributions.

</details>


### [104] [FRET: Feature Redundancy Elimination for Test Time Adaptation](https://arxiv.org/abs/2505.10641)
*Linjing You,Jiabao Lu,Xiayuan Huang,Xiangli Nie*

Main category: cs.LG

TL;DR: 论文提出FRET方法，通过消除特征冗余提升测试时适应（TTA）性能，包括简单直接的S-FRET和结合GCN的G-FRET，后者在多种任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决测试数据分布偏移时，现有TTA方法忽略特征冗余的问题，影响模型适应性。

Method: 提出S-FRET直接最小化特征冗余；进一步设计G-FRET，结合GCN和对比学习，提升特征区分度。

Result: 实验表明S-FRET有效，G-FRET在多种任务中达到最优性能。

Conclusion: G-FRET能提取非冗余且高区分性特征，提升TTA鲁棒性。

Abstract: Test-Time Adaptation (TTA) aims to enhance the generalization of deep
learning models when faced with test data that exhibits distribution shifts
from the training data. In this context, only a pre-trained model and unlabeled
test data are available, making it particularly relevant for privacy-sensitive
applications. In practice, we observe that feature redundancy in embeddings
tends to increase as domain shifts intensify in TTA. However, existing TTA
methods often overlook this redundancy, which can hinder the model's
adaptability to new data. To address this issue, we introduce Feature
Redundancy Elimination for Test-time Adaptation (FRET), a novel perspective for
TTA. A straightforward approach (S-FRET) is to directly minimize the feature
redundancy score as an optimization objective to improve adaptation. Despite
its simplicity and effectiveness, S-FRET struggles with label shifts, limiting
its robustness in real-world scenarios. To mitigate this limitation, we further
propose Graph-based FRET (G-FRET), which integrates a Graph Convolutional
Network (GCN) with contrastive learning. This design not only reduces feature
redundancy but also enhances feature discriminability in both the
representation and prediction layers. Extensive experiments across multiple
model architectures, tasks, and datasets demonstrate the effectiveness of
S-FRET and show that G-FRET achieves state-of-the-art performance. Further
analysis reveals that G-FRET enables the model to extract non-redundant and
highly discriminative features during inference, thereby facilitating more
robust test-time adaptation.

</details>


### [105] [Accelerating Visual-Policy Learning through Parallel Differentiable Simulation](https://arxiv.org/abs/2505.10646)
*Haoxiang You,Yilang Liu,Ian Abraham*

Main category: cs.LG

TL;DR: 提出了一种计算高效的视觉策略学习算法，利用可微分模拟和一阶解析策略梯度，通过解耦渲染过程与计算图，减少计算和内存开销，并稳定优化。


<details>
  <summary>Details</summary>
Motivation: 现有方法在视觉策略学习中需要专用可微分渲染软件，计算和内存开销大，优化不稳定。

Method: 提出解耦渲染过程与计算图的方法，结合可微分模拟和一阶解析策略梯度，减少开销并稳定优化。

Result: 在标准视觉控制基准测试中显著减少训练时间，性能优于基线方法，复杂任务（如人形机器人运动）上最终回报提升4倍。

Conclusion: 该方法高效且稳定，适用于复杂视觉策略学习任务。

Abstract: In this work, we propose a computationally efficient algorithm for visual
policy learning that leverages differentiable simulation and first-order
analytical policy gradients. Our approach decouple the rendering process from
the computation graph, enabling seamless integration with existing
differentiable simulation ecosystems without the need for specialized
differentiable rendering software. This decoupling not only reduces
computational and memory overhead but also effectively attenuates the policy
gradient norm, leading to more stable and smoother optimization. We evaluate
our method on standard visual control benchmarks using modern GPU-accelerated
simulation. Experiments show that our approach significantly reduces wall-clock
training time and consistently outperforms all baseline methods in terms of
final returns. Notably, on complex tasks such as humanoid locomotion, our
method achieves a $4\times$ improvement in final return, and successfully
learns a humanoid running policy within 4 hours on a single GPU.

</details>


### [106] [Seasonal Forecasting of Pan-Arctic Sea Ice with State Space Model](https://arxiv.org/abs/2505.10665)
*Wei Wang,Weidong Yang,Lei Wang,Guihua Wang,Ruibo Lei*

Main category: cs.LG

TL;DR: IceMamba是一种结合注意力机制和状态空间模型的深度学习架构，用于季节性海冰预测，优于其他25种模型。


<details>
  <summary>Details</summary>
Motivation: 北极海冰快速减少对生态系统和全球气候构成风险，亟需精确的季节性预测方法。

Method: 提出IceMamba架构，结合注意力机制与状态空间模型，并与25种现有模型对比。

Result: IceMamba在RMSE和ACC上表现最佳，IIEE排名第二。

Conclusion: IceMamba提升了季节性海冰预测能力，为气候适应策略提供支持。

Abstract: The rapid decline of Arctic sea ice resulting from anthropogenic climate
change poses significant risks to indigenous communities, ecosystems, and the
global climate system. This situation emphasizes the immediate necessity for
precise seasonal sea ice forecasts. While dynamical models perform well for
short-term forecasts, they encounter limitations in long-term forecasts and are
computationally intensive. Deep learning models, while more computationally
efficient, often have difficulty managing seasonal variations and uncertainties
when dealing with complex sea ice dynamics. In this research, we introduce
IceMamba, a deep learning architecture that integrates sophisticated attention
mechanisms within the state space model. Through comparative analysis of 25
renowned forecast models, including dynamical, statistical, and deep learning
approaches, our experimental results indicate that IceMamba delivers excellent
seasonal forecasting capabilities for Pan-Arctic sea ice concentration.
Specifically, IceMamba outperforms all tested models regarding average RMSE and
anomaly correlation coefficient (ACC) and ranks second in Integrated Ice Edge
Error (IIEE). This innovative approach enhances our ability to foresee and
alleviate the effects of sea ice variability, offering essential insights for
strategies aimed at climate adaptation.

</details>


### [107] [A Conformal Predictive Measure for Assessing Catastrophic Forgetting](https://arxiv.org/abs/2505.10677)
*Ioannis Pitsiorlas,Nour Jamoussi,Marios Kountouris*

Main category: cs.LG

TL;DR: 提出了一种基于共形预测的新指标CPCF，用于动态评估持续学习中的灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 灾难性遗忘（CF）是持续学习中的关键问题，需要一种动态且实用的评估方法。

Method: 利用自适应共形预测（CP）框架，通过监控模型对已学习任务的置信度来量化CF。

Result: 在四个基准数据集上的实验表明，CPCF与任务准确性高度相关，验证了其可靠性和可解释性。

Conclusion: CPCF是一种稳健且有效的工具，适用于动态学习环境中CF的评估和理解。

Abstract: This work introduces a novel methodology for assessing catastrophic
forgetting (CF) in continual learning. We propose a new conformal prediction
(CP)-based metric, termed the Conformal Prediction Confidence Factor (CPCF), to
quantify and evaluate CF effectively. Our framework leverages adaptive CP to
estimate forgetting by monitoring the model's confidence on previously learned
tasks. This approach provides a dynamic and practical solution for monitoring
and measuring CF of previous tasks as new ones are introduced, offering greater
suitability for real-world applications. Experimental results on four benchmark
datasets demonstrate a strong correlation between CPCF and the accuracy of
previous tasks, validating the reliability and interpretability of the proposed
metric. Our results highlight the potential of CPCF as a robust and effective
tool for assessing and understanding CF in dynamic learning environments.

</details>


### [108] [A probabilistic framework for dynamic quantization](https://arxiv.org/abs/2505.10689)
*Gabriele Santini,Francesco Paissan,Elisabetta Farella*

Main category: cs.LG

TL;DR: 提出了一种动态量化神经网络的概率框架，通过轻量级代理模型自适应调整量化参数，实现高效计算且性能损失极小。


<details>
  <summary>Details</summary>
Motivation: 解决神经网络量化中固定量化参数导致的性能损失问题，同时减少计算和内存开销。

Method: 使用概率模型对预激活进行建模，通过轻量级代理自适应调整每输入量化参数。

Result: 在多个计算机视觉任务和模型上验证，性能损失可忽略，优于标准量化策略。

Conclusion: 该方法在性能和计算开销之间取得了最佳平衡。

Abstract: We propose a probabilistic framework for dynamic quantization of neural
networks that allows for a computationally efficient input-adaptive rescaling
of the quantization parameters. Our framework applies a probabilistic model to
the network's pre-activations through a lightweight surrogate, enabling the
adaptive adjustment of the quantization parameters on a per-input basis without
significant memory overhead. We validate our approach on a set of popular
computer vision tasks and models, observing only a negligible loss in
performance. Our method strikes the best performance and computational overhead
tradeoff compared to standard quantization strategies.

</details>


### [109] [Asymptotically-Optimal Gaussian Bandits with Side Observations](https://arxiv.org/abs/2505.10698)
*Alexia Atsidakou,Orestis Papadigenopoulos,Constantine Caramanis,Sujay Sanghavi,Sanjay Shakkottai*

Main category: cs.LG

TL;DR: 研究了具有一般侧信息的高斯bandit问题，提出了基于LP的渐近实例依赖下界，并设计了一个渐近最优算法。


<details>
  <summary>Details</summary>
Motivation: 探索在高斯噪声下，通过已知的侧信息矩阵揭示臂间关系的bandit问题，扩展了标准bandit、全反馈和图结构反馈的模型。

Method: 构建了一个基于线性规划（LP）的渐近实例依赖下界，优化了估计每个臂次优性差距所需的成本（遗憾）。

Result: 提出了第一个已知的渐近最优算法，适用于这种一般设置。

Conclusion: 通过LP下界的启发，成功设计了一个在高斯bandit问题中具有一般侧信息的渐近最优算法。

Abstract: We study the problem of Gaussian bandits with general side information, as
first introduced by Wu, Szepesvari, and Gyorgy. In this setting, the play of an
arm reveals information about other arms, according to an arbitrary a priori
known side information matrix: each element of this matrix encodes the fidelity
of the information that the ``row'' arm reveals about the ``column'' arm. In
the case of Gaussian noise, this model subsumes standard bandits,
full-feedback, and graph-structured feedback as special cases. In this work, we
first construct an LP-based asymptotic instance-dependent lower bound on the
regret. The LP optimizes the cost (regret) required to reliably estimate the
suboptimality gap of each arm. This LP lower bound motivates our main
contribution: the first known asymptotically optimal algorithm for this general
setting.

</details>


### [110] [Clustering Rooftop PV Systems via Probabilistic Embeddings](https://arxiv.org/abs/2505.10699)
*Kutay Bölat,Tarek Alskaif,Peter Palensky,Simon Tindemans*

Main category: cs.LG

TL;DR: 提出了一种基于概率实体嵌入的聚类框架，用于管理高维且存在缺失值的分布式光伏系统数据。


<details>
  <summary>Details</summary>
Motivation: 随着屋顶光伏系统数量的增加，如何高效管理和分析这些高维、分布式的时序数据成为挑战。

Method: 通过将每个光伏系统的发电模式和不确定性编码为概率分布，利用统计距离和凝聚聚类进行分组。

Result: 在多年度住宅光伏数据集上，该方法生成的聚类结果在代表性和鲁棒性上优于基于物理的基准方法，并支持可靠的缺失值填补。

Conclusion: 该框架为光伏系统数据的集成和管理提供了有效解决方案，并通过超参数研究提供了性能与鲁棒性的平衡指导。

Abstract: As the number of rooftop photovoltaic (PV) installations increases,
aggregators and system operators are required to monitor and analyze these
systems, raising the challenge of integration and management of large,
spatially distributed time-series data that are both high-dimensional and
affected by missing values. In this work, a probabilistic entity
embedding-based clustering framework is proposed to address these problems.
This method encodes each PV system's characteristic power generation patterns
and uncertainty as a probability distribution, then groups systems by their
statistical distances and agglomerative clustering. Applied to a multi-year
residential PV dataset, it produces concise, uncertainty-aware cluster profiles
that outperform a physics-based baseline in representativeness and robustness,
and support reliable missing-value imputation. A systematic hyperparameter
study further offers practical guidance for balancing model performance and
robustness.

</details>


### [111] [ZEUS: Zero-shot Embeddings for Unsupervised Separation of Tabular Data](https://arxiv.org/abs/2505.10704)
*Patryk Marszałek,Tomasz Kuśmierczyk,Witold Wydmański,Jacek Tabor,Marek Śmieja*

Main category: cs.LG

TL;DR: ZEUS是一种零样本学习方法，用于无监督聚类表格数据，无需额外训练或调参，性能优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 表格数据的聚类因数据集依赖性高且缺乏监督信号而具有挑战性，需要减少调参需求。

Method: 通过分解数据集为有意义组件并利用预训练的合成数据进行零样本聚类。

Result: 实验显示ZEUS性能与传统方法和深度学习方法相当或更好，速度更快且更易用。

Conclusion: ZEUS是首个零样本无监督表格数据聚类方法，具有高效和通用性。

Abstract: Clustering tabular data remains a significant open challenge in data analysis
and machine learning. Unlike for image data, similarity between tabular records
often varies across datasets, making the definition of clusters highly
dataset-dependent. Furthermore, the absence of supervised signals complicates
hyperparameter tuning in deep learning clustering methods, frequently resulting
in unstable performance. To address these issues and reduce the need for
per-dataset tuning, we adopt an emerging approach in deep learning: zero-shot
learning. We propose ZEUS, a self-contained model capable of clustering new
datasets without any additional training or fine-tuning. It operates by
decomposing complex datasets into meaningful components that can then be
clustered effectively. Thanks to pre-training on synthetic datasets generated
from a latent-variable prior, it generalizes across various datasets without
requiring user intervention. To the best of our knowledge, ZEUS is the first
zero-shot method capable of generating embeddings for tabular data in a fully
unsupervised manner. Experimental results demonstrate that it performs on par
with or better than traditional clustering algorithms and recent deep
learning-based methods, while being significantly faster and more
user-friendly.

</details>


### [112] [GNN-Suite: a Graph Neural Network Benchmarking Framework for Biomedical Informatics](https://arxiv.org/abs/2505.10711)
*Sebestyén Kamp,Giovanni Stracquadanio,T. Ian Simpson*

Main category: cs.LG

TL;DR: GNN-Suite是一个模块化框架，用于构建和评估图神经网络（GNN）在计算生物学中的应用，通过标准化实验和可重复性，帮助识别癌症驱动基因。


<details>
  <summary>Details</summary>
Motivation: 提供一个统一的框架来比较不同GNN架构的性能，并促进计算生物学中的可重复研究和基准测试。

Method: 使用Nextflow工作流标准化实验，构建分子网络（基于STRING和BioGRID的PPI数据），并标注节点特征（来自PCAWG、PID和COSMIC-CGC）。比较多种GNN架构（如GAT、GCN等）和逻辑回归模型，使用统一超参数和训练设置。

Result: GCN2在STRING网络上的平衡准确率最高（0.807 +/- 0.035），所有GNN均优于逻辑回归基线。

Conclusion: GNN-Suite有助于识别最佳模型和数据整合方式，促进可重复研究和基准测试。未来将探索更多组学数据和优化网络架构。

Abstract: We present GNN-Suite, a robust modular framework for constructing and
benchmarking Graph Neural Network (GNN) architectures in computational biology.
GNN-Suite standardises experimentation and reproducibility using the Nextflow
workflow to evaluate GNN performance. We demonstrate its utility in identifying
cancer-driver genes by constructing molecular networks from protein-protein
interaction (PPI) data from STRING and BioGRID and annotating nodes with
features from the PCAWG, PID, and COSMIC-CGC repositories.
  Our design enables fair comparisons among diverse GNN architectures including
GAT, GAT3H, GCN, GCN2, GIN, GTN, HGCN, PHGCN, and GraphSAGE and a baseline
Logistic Regression (LR) model. All GNNs were configured as standardised
two-layer models and trained with uniform hyperparameters (dropout = 0.2; Adam
optimiser with learning rate = 0.01; and an adjusted binary cross-entropy loss
to address class imbalance) over an 80/20 train-test split for 300 epochs. Each
model was evaluated over 10 independent runs with different random seeds to
yield statistically robust performance metrics, with balanced accuracy (BACC)
as the primary measure. Notably, GCN2 achieved the highest BACC (0.807 +/-
0.035) on a STRING-based network, although all GNN types outperformed the LR
baseline, highlighting the advantage of network-based learning over
feature-only approaches.
  Our results show that a common framework for implementing and evaluating GNN
architectures aids in identifying not only the best model but also the most
effective means of incorporating complementary data. By making GNN-Suite
publicly available, we aim to foster reproducible research and promote improved
benchmarking standards in computational biology. Future work will explore
additional omics datasets and further refine network architectures to enhance
predictive accuracy and interpretability in biomedical applications.

</details>


### [113] [Learning Repetition-Invariant Representations for Polymer Informatics](https://arxiv.org/abs/2505.10726)
*Yihan Zhu,Gang Liu,Eric Inae,Tengfei Luo,Meng Jiang*

Main category: cs.LG

TL;DR: 论文提出了一种名为GRIN的新方法，用于学习聚合物表示，解决现有图神经网络方法无法处理聚合物重复单元数量变化的问题。


<details>
  <summary>Details</summary>
Motivation: 现有图神经网络方法仅能建模聚合物的单个单元，无法为真实聚合物结构（具有不同重复单元数量）生成一致的向量表示。

Method: GRIN结合了基于图的最大生成树对齐和重复单元增强，确保结构一致性，并从模型和数据角度提供了重复不变性的理论保证。

Result: GRIN在均聚物和共聚物基准测试中优于现有方法，能够学习到稳定且重复不变的表示，并能泛化到未见过的聚合物链大小。

Conclusion: GRIN是一种有效的聚合物表示学习方法，解决了重复单元数量变化带来的挑战，具有理论和实际应用价值。

Abstract: Polymers are large macromolecules composed of repeating structural units
known as monomers and are widely applied in fields such as energy storage,
construction, medicine, and aerospace. However, existing graph neural network
methods, though effective for small molecules, only model the single unit of
polymers and fail to produce consistent vector representations for the true
polymer structure with varying numbers of units. To address this challenge, we
introduce Graph Repetition Invariance (GRIN), a novel method to learn polymer
representations that are invariant to the number of repeating units in their
graph representations. GRIN integrates a graph-based maximum spanning tree
alignment with repeat-unit augmentation to ensure structural consistency. We
provide theoretical guarantees for repetition-invariance from both model and
data perspectives, demonstrating that three repeating units are the minimal
augmentation required for optimal invariant representation learning. GRIN
outperforms state-of-the-art baselines on both homopolymer and copolymer
benchmarks, learning stable, repetition-invariant representations that
generalize effectively to polymer chains of unseen sizes.

</details>


### [114] [Random Client Selection on Contrastive Federated Learning for Tabular Data](https://arxiv.org/abs/2505.10759)
*Achmad Ginanjar,Xue Li,Priyanka Singh,Wen Hua*

Main category: cs.LG

TL;DR: 本文分析了垂直联邦学习（VFL）中对比联邦学习（CFL）面临的梯度攻击问题，并提出随机客户端选择作为防御策略，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 垂直联邦学习虽能保护隐私，但在中间计算共享时仍存在信息泄露风险。对比联邦学习虽通过表示学习缓解隐私问题，但仍面临梯度攻击的挑战。

Method: 通过实验分析梯度攻击在CFL环境中的表现，并评估随机客户端选择作为防御策略的效果。

Result: 实验表明，随机客户端选择能有效防御CFL网络中的梯度攻击。

Conclusion: 研究结果为对比联邦学习系统提供了实现更强安全措施的见解，有助于开发更安全的协作学习框架。

Abstract: Vertical Federated Learning (VFL) has revolutionised collaborative machine
learning by enabling privacy-preserving model training across multiple parties.
However, it remains vulnerable to information leakage during intermediate
computation sharing. While Contrastive Federated Learning (CFL) was introduced
to mitigate these privacy concerns through representation learning, it still
faces challenges from gradient-based attacks. This paper presents a
comprehensive experimental analysis of gradient-based attacks in CFL
environments and evaluates random client selection as a defensive strategy.
Through extensive experimentation, we demonstrate that random client selection
proves particularly effective in defending against gradient attacks in the CFL
network. Our findings provide valuable insights for implementing robust
security measures in contrastive federated learning systems, contributing to
the development of more secure collaborative learning frameworks

</details>


### [115] [Deep Symbolic Optimization: Reinforcement Learning for Symbolic Mathematics](https://arxiv.org/abs/2505.10762)
*Conor F. Hayes,Felipe Leno Da Silva,Jiachen Yang,T. Nathan Mundhenk,Chak Shing Lee,Jacob F. Pettit,Claudio Santiago,Sookyung Kim,Joanne T. Kim,Ignacio Aravena Solis,Ruben Glatt,Andre R. Goncalves,Alexander Ladd,Ahmet Can Solak,Thomas Desautels,Daniel Faissol,Brenden K. Petersen,Mikel Landajuela*

Main category: cs.LG

TL;DR: DSO是一种新颖的计算框架，通过生成神经网络和强化学习策略，结合梯度优化与进化搜索，实现高效的符号优化，用于科学发现中的符号结构搜索。


<details>
  <summary>Details</summary>
Motivation: 解决科学发现中复杂符号结构的自动搜索问题，特别是在数学模型的符号形式推导中。

Method: 将发现过程建模为序列决策任务，生成神经网络学习候选符号表达式的概率模型，结合强化学习、梯度优化、进化搜索和局部搜索技术。

Result: 在基准测试中表现出色，实现了高精度和可解释性。

Conclusion: DSO框架为科学发现中的符号优化提供了高效且可解释的解决方案，具有变革潜力。

Abstract: Deep Symbolic Optimization (DSO) is a novel computational framework that
enables symbolic optimization for scientific discovery, particularly in
applications involving the search for intricate symbolic structures. One
notable example is equation discovery, which aims to automatically derive
mathematical models expressed in symbolic form. In DSO, the discovery process
is formulated as a sequential decision-making task. A generative neural network
learns a probabilistic model over a vast space of candidate symbolic
expressions, while reinforcement learning strategies guide the search toward
the most promising regions. This approach integrates gradient-based
optimization with evolutionary and local search techniques, and it incorporates
in-situ constraints, domain-specific priors, and advanced policy optimization
methods. The result is a robust framework capable of efficiently exploring
extensive search spaces to identify interpretable and physically meaningful
models. Extensive evaluations on benchmark problems have demonstrated that DSO
achieves state-of-the-art performance in both accuracy and interpretability. In
this chapter, we provide a comprehensive overview of the DSO framework and
illustrate its transformative potential for automating symbolic optimization in
scientific discovery.

</details>


### [116] [Context-Aware Probabilistic Modeling with LLM for Multimodal Time Series Forecasting](https://arxiv.org/abs/2505.10774)
*Yueyang Yao,Jiajun Li,Xingyuan Dai,MengMeng Zhang,Xiaoyan Gong,Fei-Yue Wang,Yisheng Lv*

Main category: cs.LG

TL;DR: CAPTime是一种结合文本和时间序列的多模态概率预测方法，通过上下文感知和自回归LLM解码提升预测准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以有效整合外源文本并与LLM的概率特性对齐，限制了上下文感知和分布建模能力。

Method: CAPTime使用预训练时间序列编码器编码时间模式，通过可学习交互与文本对齐，结合混合分布专家和冻结LLM实现概率预测。

Result: 实验表明CAPTime在多模态任务中具有更高的准确性和泛化能力，数据稀缺场景下也表现稳健。

Conclusion: CAPTime通过上下文感知和概率解码，显著提升了多模态时间序列预测的性能。

Abstract: Time series forecasting is important for applications spanning energy
markets, climate analysis, and traffic management. However, existing methods
struggle to effectively integrate exogenous texts and align them with the
probabilistic nature of large language models (LLMs). Current approaches either
employ shallow text-time series fusion via basic prompts or rely on
deterministic numerical decoding that conflict with LLMs' token-generation
paradigm, which limits contextual awareness and distribution modeling. To
address these limitations, we propose CAPTime, a context-aware probabilistic
multimodal time series forecasting method that leverages text-informed
abstraction and autoregressive LLM decoding. Our method first encodes temporal
patterns using a pretrained time series encoder, then aligns them with textual
contexts via learnable interactions to produce joint multimodal
representations. By combining a mixture of distribution experts with frozen
LLMs, we enable context-aware probabilistic forecasting while preserving LLMs'
inherent distribution modeling capabilities. Experiments on diverse time series
forecasting tasks demonstrate the superior accuracy and generalization of
CAPTime, particularly in multimodal scenarios. Additional analysis highlights
its robustness in data-scarce scenarios through hybrid probabilistic decoding.

</details>


### [117] [Cell Library Characterization for Composite Current Source Models Based on Gaussian Process Regression and Active Learning](https://arxiv.org/abs/2505.10799)
*Tao Bai,Junzhuo Zhou,Zeyuan Deng,Peng Cao*

Main category: cs.LG

TL;DR: 提出了一种基于高斯过程回归（GPR）和主动学习（AL）的新方法，用于高效准确地表征复合电流源（CCS）模型，显著优于传统商业工具和学习方法。


<details>
  <summary>Details</summary>
Motivation: 传统CCS模型在高精度需求、大量数据和模拟成本方面面临挑战，需要更高效的解决方案。

Method: 采用高斯过程回归（GPR）结合主动学习（AL）建立表征框架。

Result: 在TSMC 22nm工艺下，平均绝对误差为2.05 ps，相对误差为2.27%，运行时间减少至27%，存储需求降低19.5倍。

Conclusion: 该方法显著提升了CCS模型的表征效率和精度，适用于先进工艺节点。

Abstract: The composite current source (CCS) model has been adopted as an advanced
timing model that represents the current behavior of cells for improved
accuracy and better capability than traditional non-linear delay models (NLDM)
to model complex dynamic effects and interactions under advanced process nodes.
However, the high accuracy requirement, large amount of data and extensive
simulation cost pose severe challenges to CCS characterization. To address
these challenges, we introduce a novel Gaussian Process Regression(GPR) model
with active learning(AL) to establish the characterization framework
efficiently and accurately. Our approach significantly outperforms conventional
commercial tools as well as learning based approaches by achieving an average
absolute error of 2.05 ps and a relative error of 2.27% for current waveform of
57 cells under 9 process, voltage, temperature (PVT) corners with TSMC 22nm
process. Additionally, our model drastically reduces the runtime to 27% and the
storage by up to 19.5x compared with that required by commercial tools.

</details>


### [118] [Attention-Based Reward Shaping for Sparse and Delayed Rewards](https://arxiv.org/abs/2505.10802)
*Ian Holmes,Min Chi*

Main category: cs.LG

TL;DR: ARES是一种基于注意力机制的奖励塑形算法，能够为稀疏和延迟奖励的环境生成密集奖励函数，完全离线训练且适用于任何RL算法。


<details>
  <summary>Details</summary>
Motivation: 解决现实世界中强化学习应用因稀疏和延迟奖励函数导致的训练困难问题。

Method: 使用Transformer的注意力机制生成塑形奖励，输入为一系列片段及其最终回报，完全离线训练。

Result: 在完全延迟奖励的最具挑战性情况下，ARES显著提升了学习效果，适用于多种环境和RL算法。

Conclusion: ARES是首个完全离线、对极端奖励延迟和低质量数据鲁棒且不限于目标任务的奖励塑形方法。

Abstract: Sparse and delayed reward functions pose a significant obstacle for
real-world Reinforcement Learning (RL) applications. In this work, we propose
Attention-based REward Shaping (ARES), a general and robust algorithm which
uses a transformer's attention mechanism to generate shaped rewards and create
a dense reward function for any environment. ARES requires a set of episodes
and their final returns as input. It can be trained entirely offline and is
able to generate meaningful shaped rewards even when using small datasets or
episodes produced by agents taking random actions. ARES is compatible with any
RL algorithm and can handle any level of reward sparsity. In our experiments,
we focus on the most challenging case where rewards are fully delayed until the
end of each episode. We evaluate ARES across a diverse range of environments,
widely used RL algorithms, and baseline methods to assess the effectiveness of
the shaped rewards it produces. Our results show that ARES can significantly
improve learning in delayed reward settings, enabling RL agents to train in
scenarios that would otherwise require impractical amounts of data or even be
unlearnable. To our knowledge, ARES is the first approach that works fully
offline, remains robust to extreme reward delays and low-quality data, and is
not limited to goal-based tasks.

</details>


### [119] [Distilled Circuits: A Mechanistic Study of Internal Restructuring in Knowledge Distillation](https://arxiv.org/abs/2505.10822)
*Reilly Haskins,Benjamin Adams*

Main category: cs.LG

TL;DR: 论文研究了知识蒸馏过程中教师模型与学生模型内部计算结构的差异，发现学生模型会重组、压缩或丢弃教师模型的组件，并引入了一种新的对齐度量方法。


<details>
  <summary>Details</summary>
Motivation: 理解知识蒸馏过程中教师模型与学生模型内部计算结构的差异，以揭示蒸馏对模型鲁棒性和泛化能力的影响。

Method: 应用机制解释性技术分析教师模型（GPT2-small）与学生模型（DistilGPT2）的内部电路、表示和激活模式差异，并引入基于影响加权的组件相似性对齐度量。

Result: 学生模型会重组、压缩或丢弃教师模型的组件，导致对更少组件的依赖增强；知识蒸馏保留了功能行为，但内部计算发生了显著变化。

Conclusion: 知识蒸馏虽然保留了功能行为，但内部计算的变化可能影响蒸馏模型的鲁棒性和泛化能力。

Abstract: Knowledge distillation compresses a larger neural model (teacher) into
smaller, faster student models by training them to match teacher outputs.
However, the internal computational transformations that occur during this
process remain poorly understood. We apply techniques from mechanistic
interpretability to analyze how internal circuits, representations, and
activation patterns differ between teacher and student. Focusing on GPT2-small
and its distilled counterpart DistilGPT2, we find that student models
reorganize, compress, and discard teacher components, often resulting in
stronger reliance on fewer individual components. To quantify functional
alignment beyond output similarity, we introduce an alignment metric based on
influence-weighted component similarity, validated across multiple tasks. Our
findings reveal that while knowledge distillation preserves broad functional
behaviors, it also causes significant shifts in internal computation, with
important implications for the robustness and generalization capacity of
distilled models.

</details>


### [120] [MergeBench: A Benchmark for Merging Domain-Specialized LLMs](https://arxiv.org/abs/2505.10833)
*Yifei He,Siqi Zeng,Yuzheng Hu,Rui Yang,Tong Zhang,Han Zhao*

Main category: cs.LG

TL;DR: MergeBench是一个评估模型合并的综合套件，针对大规模、领域专业化的大语言模型（LLMs）设计，覆盖五个关键领域，并评估八种代表性合并方法。


<details>
  <summary>Details</summary>
Motivation: 现有模型合并方法的评估在模型规模和任务多样性上有限，MergeBench旨在填补这一空白，推动模型合并的实践应用。

Method: 基于开源语言模型（如Llama和Gemma家族），标准化微调和评估协议，评估多任务性能、遗忘和运行时效率。

Result: 实验表明，模型合并对更强的基模型效果更好，合并系数调整和稀疏化等技术有助于知识保留。但仍存在计算成本高、领域内性能差距等问题。

Conclusion: MergeBench为未来研究提供了基础，但模型合并在大规模LLM训练中的应用仍需进一步探索。

Abstract: Model merging provides a scalable alternative to multi-task training by
combining specialized finetuned models through parameter arithmetic, enabling
efficient deployment without the need for joint training or access to all task
data. While recent methods have shown promise, existing evaluations are limited
in both model scale and task diversity, leaving open questions about their
applicability to large, domain-specialized LLMs. To tackle the challenges, we
introduce MergeBench, a comprehensive evaluation suite designed to assess model
merging at scale. MergeBench builds on state-of-the-art open-source language
models, including Llama and Gemma families at 2B to 9B scales, and covers five
key domains: instruction following, mathematics, multilingual understanding,
coding and safety. We standardize finetuning and evaluation protocols, and
assess eight representative merging methods across multi-task performance,
forgetting and runtime efficiency. Based on extensive experiments, we provide
practical guidelines for algorithm selection and share insights showing that
model merging tends to perform better on stronger base models, with techniques
such as merging coefficient tuning and sparsification improving knowledge
retention. However, several challenges remain, including the computational cost
on large models, the gap for in-domain performance compared to multi-task
models, and the underexplored role of model merging in standard LLM training
pipelines. We hope MergeBench provides a foundation for future research to
advance the understanding and practical application of model merging. We open
source our code at
\href{https://github.com/uiuctml/MergeBench}{https://github.com/uiuctml/MergeBench}.

</details>


### [121] [LARGO: Latent Adversarial Reflection through Gradient Optimization for Jailbreaking LLMs](https://arxiv.org/abs/2505.10838)
*Ran Li,Hao Wang,Chengzhi Mao*

Main category: cs.LG

TL;DR: LARGO是一种基于梯度优化的潜在对抗反射攻击方法，用于生成流畅的越狱提示，显著提升了攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的攻击方法在离散语言空间中梯度优化效果不佳，需要一种更高效的方法来发现LLM的漏洞。

Method: LARGO通过在LLM的连续潜在空间中优化对抗潜在向量，并递归解码为自然语言，生成流畅且隐蔽的提示。

Result: 在AdvBench和JailbreakBench基准测试中，LARGO的攻击成功率比现有最佳方法（如AutoDAN）高出44个百分点。

Conclusion: LARGO展示了通过梯度优化攻击LLM内部的有效性，为LLM漏洞挖掘提供了新思路。

Abstract: Efficient red-teaming method to uncover vulnerabilities in Large Language
Models (LLMs) is crucial. While recent attacks often use LLMs as optimizers,
the discrete language space make gradient-based methods struggle. We introduce
LARGO (Latent Adversarial Reflection through Gradient Optimization), a novel
latent self-reflection attack that reasserts the power of gradient-based
optimization for generating fluent jailbreaking prompts. By operating within
the LLM's continuous latent space, LARGO first optimizes an adversarial latent
vector and then recursively call the same LLM to decode the latent into natural
language. This methodology yields a fast, effective, and transferable attack
that produces fluent and stealthy prompts. On standard benchmarks like AdvBench
and JailbreakBench, LARGO surpasses leading jailbreaking techniques, including
AutoDAN, by 44 points in attack success rate. Our findings demonstrate a potent
alternative to agentic LLM prompting, highlighting the efficacy of interpreting
and attacking LLM internals through gradient optimization.

</details>


### [122] [Ready2Unlearn: A Learning-Time Approach for Preparing Models with Future Unlearning Readiness](https://arxiv.org/abs/2505.10845)
*Hanyu Duan,Yi Yang,Ahmed Abbasi,Kar Yan Tam*

Main category: cs.LG

TL;DR: Ready2Unlearn是一种在训练阶段优化模型的方法，旨在为未来的数据删除请求做准备，提高删除效率和模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有数据删除方法通常在部署阶段被动应对删除请求，而Ready2Unlearn则主动在训练阶段为未来删除做准备，以提升效率和可靠性。

Method: 基于元学习原则，Ready2Unlearn在训练阶段主动优化模型，使其具备删除准备能力，兼容任何基于梯度上升的删除算法。

Result: 实验表明，Ready2Unlearn能减少删除时间、保持模型性能，并增强对已删除数据的抵抗能力。

Conclusion: Ready2Unlearn为机器学习模型提供了一种主动的删除准备策略，为未来更可靠的数据删除方法提供了启发。

Abstract: This paper introduces Ready2Unlearn, a learning-time optimization approach
designed to facilitate future unlearning processes. Unlike the majority of
existing unlearning efforts that focus on designing unlearning algorithms,
which are typically implemented reactively when an unlearning request is made
during the model deployment phase, Ready2Unlearn shifts the focus to the
training phase, adopting a "forward-looking" perspective. Building upon
well-established meta-learning principles, Ready2Unlearn proactively trains
machine learning models with unlearning readiness, such that they are well
prepared and can handle future unlearning requests in a more efficient and
principled manner. Ready2Unlearn is model-agnostic and compatible with any
gradient ascent-based machine unlearning algorithms. We evaluate the method on
both vision and language tasks under various unlearning settings, including
class-wise unlearning and random data unlearning. Experimental results show
that by incorporating such preparedness at training time, Ready2Unlearn
produces an unlearning-ready model state, which offers several key advantages
when future unlearning is required, including reduced unlearning time, improved
retention of overall model capability, and enhanced resistance to the
inadvertent recovery of forgotten data. We hope this work could inspire future
efforts to explore more proactive strategies for equipping machine learning
models with built-in readiness towards more reliable and principled machine
unlearning.

</details>


### [123] [AutoRAN: Weak-to-Strong Jailbreaking of Large Reasoning Models](https://arxiv.org/abs/2505.10846)
*Jiacheng Liang,Tanqiu Jiang,Yuhui Wang,Rongyi Zhu,Fenglong Ma,Ting Wang*

Main category: cs.LG

TL;DR: AutoRAN是一个自动化框架，利用弱对齐的推理模型攻击强推理模型（LRMs），通过模拟目标模型的高层推理结构生成并优化提示，成功率达到接近100%。


<details>
  <summary>Details</summary>
Motivation: 揭示强推理模型的关键漏洞，强调需要针对推理模型设计更完善的安全措施。

Method: 利用弱对齐的推理模型模拟目标模型的推理结构，生成叙事提示并通过迭代优化候选提示。

Result: 在多个基准数据集上对GPT-o3/o4-mini和Gemini-2.5-Flash等模型测试，成功率达到接近100%。

Conclusion: 弱推理模型可以有效利用强推理模型的漏洞，需改进推理模型的安全设计。

Abstract: This paper presents AutoRAN, the first automated, weak-to-strong jailbreak
attack framework targeting large reasoning models (LRMs). At its core, AutoRAN
leverages a weak, less-aligned reasoning model to simulate the target model's
high-level reasoning structures, generates narrative prompts, and iteratively
refines candidate prompts by incorporating the target model's intermediate
reasoning steps. We evaluate AutoRAN against state-of-the-art LRMs including
GPT-o3/o4-mini and Gemini-2.5-Flash across multiple benchmark datasets
(AdvBench, HarmBench, and StrongReject). Results demonstrate that AutoRAN
achieves remarkable success rates (approaching 100%) within one or a few turns
across different LRMs, even when judged by a robustly aligned external model.
This work reveals that leveraging weak reasoning models can effectively exploit
the critical vulnerabilities of much more capable reasoning models,
highlighting the need for improved safety measures specifically designed for
reasoning-based models. The code for replicating AutoRAN and running records
are available at: (https://github.com/JACKPURCELL/AutoRAN-public). (warning:
this paper contains potentially harmful content generated by LRMs.)

</details>


### [124] [Foundation model for mass spectrometry proteomics](https://arxiv.org/abs/2505.10848)
*Justin Sanders,Melih Yilmaz,Jacob H. Russell,Wout Bittremieux,William E. Fondrie,Nicholas M. Riley,Sewoong Oh,William Stafford Noble*

Main category: cs.LG

TL;DR: 提出一种基于预训练的统一质谱基础模型，通过去新测序任务预训练谱编码器，提升下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 质谱数据的复杂性和多样性需要高效的计算方法，机器学习在质谱分析中显示出潜力，但现有方法多为针对特定任务设计。

Method: 预训练一个谱编码器，使用去新测序作为预训练任务，并在四个下游任务上进行微调。

Result: 预训练的谱表示提升了四个下游任务的性能，多任务微调进一步优化了各任务表现。

Conclusion: 该基础模型能学习到普适的谱表示，提升数据有限的下游任务性能，优化蛋白质组学实验的数据获取与分析。

Abstract: Mass spectrometry is the dominant technology in the field of proteomics,
enabling high-throughput analysis of the protein content of complex biological
samples. Due to the complexity of the instrumentation and resulting data,
sophisticated computational methods are required for the processing and
interpretation of acquired mass spectra. Machine learning has shown great
promise to improve the analysis of mass spectrometry data, with numerous
purpose-built methods for improving specific steps in the data acquisition and
analysis pipeline reaching widespread adoption. Here, we propose unifying
various spectrum prediction tasks under a single foundation model for mass
spectra. To this end, we pre-train a spectrum encoder using de novo sequencing
as a pre-training task. We then show that using these pre-trained spectrum
representations improves our performance on the four downstream tasks of
spectrum quality prediction, chimericity prediction, phosphorylation
prediction, and glycosylation status prediction. Finally, we perform multi-task
fine-tuning and find that this approach improves the performance on each task
individually. Overall, our work demonstrates that a foundation model for tandem
mass spectrometry proteomics trained on de novo sequencing learns generalizable
representations of spectra, improves performance on downstream tasks where
training data is limited, and can ultimately enhance data acquisition and
analysis in proteomics experiments.

</details>


### [125] [ImputeINR: Time Series Imputation via Implicit Neural Representations for Disease Diagnosis with Missing Data](https://arxiv.org/abs/2505.10856)
*Mengxuan Li,Ke Liu,Jialong Guo,Jiajun Bu,Hongwei Wang,Haishuai Wang*

Main category: cs.LG

TL;DR: ImputeINR提出了一种基于隐式神经表示（INR）的时间序列插值方法，能够有效处理高缺失率的医疗数据，并在下游疾病诊断任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 医疗数据中常存在大量缺失值，现有插值方法对稀疏数据建模效果不佳，尤其是高缺失率情况。

Method: 利用隐式神经表示（INR）学习时间序列的连续函数，实现细粒度插值。

Result: 在八个数据集上验证了ImputeINR的优越性，尤其在高缺失率情况下表现突出，并提升了疾病诊断任务的性能。

Conclusion: ImputeINR为医疗数据插值提供了高效解决方案，支持下游任务。

Abstract: Healthcare data frequently contain a substantial proportion of missing
values, necessitating effective time series imputation to support downstream
disease diagnosis tasks. However, existing imputation methods focus on discrete
data points and are unable to effectively model sparse data, resulting in
particularly poor performance for imputing substantial missing values. In this
paper, we propose a novel approach, ImputeINR, for time series imputation by
employing implicit neural representations (INR) to learn continuous functions
for time series. ImputeINR leverages the merits of INR in that the continuous
functions are not coupled to sampling frequency and have infinite sampling
frequency, allowing ImputeINR to generate fine-grained imputations even on
extremely sparse observed values. Extensive experiments conducted on eight
datasets with five ratios of masked values show the superior imputation
performance of ImputeINR, especially for high missing ratios in time series
data. Furthermore, we validate that applying ImputeINR to impute missing values
in healthcare data enhances the performance of downstream disease diagnosis
tasks. Codes are available.

</details>


### [126] [On DeepSeekMoE: Statistical Benefits of Shared Experts and Normalized Sigmoid Gating](https://arxiv.org/abs/2505.10860)
*Huy Nguyen,Thong T. Doan,Quang Pham,Nghi D. Q. Bui,Nhat Ho,Alessandro Rinaldo*

Main category: cs.LG

TL;DR: 本文对DeepSeekMoE的两个独特特性（共享专家策略和归一化Sigmoid门控机制）进行了理论分析，验证了其在样本效率和收敛性上的优势，并通过实验支持了理论发现。


<details>
  <summary>Details</summary>
Motivation: 尽管DeepSeekMoE在DeepSeek系列模型中表现突出，但其共享专家策略和归一化Sigmoid门控机制的理论价值尚未充分研究。本文旨在填补这一空白。

Method: 从统计角度对共享专家策略和归一化Sigmoid门控机制进行理论分析，并通过合成数据和真实数据集进行实验验证。

Result: 理论分析表明，这两种特性在样本效率和收敛性上具有优势；实验进一步验证了理论发现，并对路由行为进行了详细分析。

Conclusion: 研究为专家和门控结构的设计提供了理论依据，并展示了DeepSeekMoE的优越性。

Abstract: Mixture of experts (MoE) methods are a key component in most large language
model architectures, including the recent series of DeepSeek models. Compared
to other MoE implementations, DeepSeekMoE stands out because of two unique
features: the deployment of a shared expert strategy and of the normalized
sigmoid gating mechanism. Despite the prominent role of DeepSeekMoE in the
success of the DeepSeek series of models, there have been only a few attempts
to justify theoretically the value of the shared expert strategy, while its
normalized sigmoid gating has remained unexplored. To bridge this gap, we
undertake a comprehensive theoretical study of these two features of
DeepSeekMoE from a statistical perspective. We perform a convergence analysis
of the expert estimation task to highlight the gains in sample efficiency for
both the shared expert strategy and the normalized sigmoid gating, offering
useful insights into the design of expert and gating structures. To verify
empirically our theoretical findings, we carry out several experiments on both
synthetic data and real-world datasets for (vision) language modeling tasks.
Finally, we conduct an extensive empirical analysis of the router behaviors,
ranging from router saturation, router change rate, to expert utilization.

</details>


### [127] [Improving the Data-efficiency of Reinforcement Learning by Warm-starting with LLM](https://arxiv.org/abs/2505.10861)
*Thang Duong,Minglai Yang,Chicheng Zhang*

Main category: cs.LG

TL;DR: LORO算法利用LLM生成高质量初始数据，提升RL算法的样本效率和性能，在多个环境中优于纯LLM、纯RL及简单组合方法。


<details>
  <summary>Details</summary>
Motivation: 研究如何利用LLM为RL算法提供高质量初始数据，以解决RL样本效率低的问题。

Method: 提出LORO算法，结合LLM生成覆盖最优策略状态-动作的离线数据集，再用RL探索环境并优化策略。

Result: 在CartPole和Pendulum等环境中，LORO的累积奖励达到纯RL基线的4倍。

Conclusion: LORO通过LLM的初始策略显著提升RL性能，证明了LLM在RL数据收集中的潜力。

Abstract: We investigate the usage of Large Language Model (LLM) in collecting
high-quality data to warm-start Reinforcement Learning (RL) algorithms for
learning in some classical Markov Decision Process (MDP) environments. In this
work, we focus on using LLM to generate an off-policy dataset that sufficiently
covers state-actions visited by optimal policies, then later using an RL
algorithm to explore the environment and improve the policy suggested by the
LLM. Our algorithm, LORO, can both converge to an optimal policy and have a
high sample efficiency thanks to the LLM's good starting policy. On multiple
OpenAI Gym environments, such as CartPole and Pendulum, we empirically
demonstrate that LORO outperforms baseline algorithms such as pure LLM-based
policies, pure RL, and a naive combination of the two, achieving up to $4
\times$ the cumulative rewards of the pure RL baseline.

</details>


### [128] [Hashing for Structure-based Anomaly Detection](https://arxiv.org/abs/2505.10873)
*Filippo Leveni,Luca Magri,Cesare Alippi,Giacomo Boracchi*

Main category: cs.LG

TL;DR: 提出一种基于偏好空间的高效异常检测方法，利用局部敏感哈希降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 解决在低维流形结构中识别异常样本的问题，传统方法在高维空间中计算距离效率低。

Method: 使用局部敏感哈希（LSH）避免高维距离的显式计算，提出基于隔离的异常检测技术。

Result: 在偏好空间中实现高效异常检测，计算成本更低且性能达到最新水平。

Conclusion: 该方法在异常检测任务中表现出色，代码已开源。

Abstract: We focus on the problem of identifying samples in a set that do not conform
to structured patterns represented by low-dimensional manifolds. An effective
way to solve this problem is to embed data in a high dimensional space, called
Preference Space, where anomalies can be identified as the most isolated
points. In this work, we employ Locality Sensitive Hashing to avoid explicit
computation of distances in high dimensions and thus improve Anomaly Detection
efficiency. Specifically, we present an isolation-based anomaly detection
technique designed to work in the Preference Space which achieves
state-of-the-art performance at a lower computational cost. Code is publicly
available at
https://github.com/ineveLoppiliF/Hashing-for-Structure-based-Anomaly-Detection.

</details>


### [129] [MultiLink: Multi-class Structure Recovery via Agglomerative Clustering and Model Selection](https://arxiv.org/abs/2505.10874)
*Luca Magri,Filippo Leveni,Giacomo Boracchi*

Main category: cs.LG

TL;DR: 论文提出了一种名为MultiLink的新算法，用于在噪声和异常值污染的数据集中恢复不同类别的几何结构。该算法通过偏好分析和聚类实现鲁棒拟合，具有速度快、对阈值不敏感等优势。


<details>
  <summary>Details</summary>
Motivation: 解决在噪声和异常值污染的数据集中恢复多类别几何结构的问题，特别是由参数模型混合定义的几何结构（如平面和圆柱体、单应性和基本矩阵）。

Method: 提出MultiLink算法，结合实时模型拟合和模型选择，通过新颖的链接方案决定是否合并两个聚类。

Result: 实验表明，MultiLink在多类别和单类别问题上均优于现有方法，具有速度快、对阈值不敏感等优势。

Conclusion: MultiLink算法在鲁棒拟合多类别几何结构方面表现出色，代码已公开。

Abstract: We address the problem of recovering multiple structures of different classes
in a dataset contaminated by noise and outliers. In particular, we consider
geometric structures defined by a mixture of underlying parametric models (e.g.
planes and cylinders, homographies and fundamental matrices), and we tackle the
robust fitting problem by preference analysis and clustering. We present a new
algorithm, termed MultiLink, that simultaneously deals with multiple classes of
models. MultiLink combines on-the-fly model fitting and model selection in a
novel linkage scheme that determines whether two clusters are to be merged. The
resulting method features many practical advantages with respect to methods
based on preference analysis, being faster, less sensitive to the inlier
threshold, and able to compensate limitations deriving from hypotheses
sampling. Experiments on several public datasets demonstrate that Multi-Link
favourably compares with state of the art alternatives, both in multi-class and
single-class problems. Code is publicly made available for download.

</details>


### [130] [Preference Isolation Forest for Structure-based Anomaly Detection](https://arxiv.org/abs/2505.10876)
*Filippo Leveni,Luca Magri,Cesare Alippi,Giacomo Boracchi*

Main category: cs.LG

TL;DR: 提出了一种名为Preference Isolation Forest (PIF)的异常检测框架，结合了自适应隔离方法和偏好嵌入的灵活性，通过将数据嵌入高维偏好空间来识别异常点。


<details>
  <summary>Details</summary>
Motivation: 解决检测不符合低维流形结构模式的异常样本的问题。

Method: 提出三种隔离方法：Voronoi-iForest（通用解决方案）、RuzHash-iForest（避免显式计算距离）、Sliding-PIF（利用局部性先验提高效率）。

Result: 通过将数据嵌入高维偏好空间并识别孤立点，实现了有效的异常检测。

Conclusion: PIF框架结合了自适应隔离和偏好嵌入的优势，为异常检测提供了灵活且高效的解决方案。

Abstract: We address the problem of detecting anomalies as samples that do not conform
to structured patterns represented by low-dimensional manifolds. To this end,
we conceive a general anomaly detection framework called Preference Isolation
Forest (PIF), that combines the benefits of adaptive isolation-based methods
with the flexibility of preference embedding. The key intuition is to embed the
data into a high-dimensional preference space by fitting low-dimensional
manifolds, and to identify anomalies as isolated points. We propose three
isolation approaches to identify anomalies: $i$) Voronoi-iForest, the most
general solution, $ii$) RuzHash-iForest, that avoids explicit computation of
distances via Local Sensitive Hashing, and $iii$) Sliding-PIF, that leverages a
locality prior to improve efficiency and effectiveness.

</details>


### [131] [Graph and Simplicial Complex Prediction Gaussian Process via the Hodgelet Representations](https://arxiv.org/abs/2505.10877)
*Mathieu Alain,So Takao,Xiaowen Dong,Bastian Rieck,Emmanuel Noutahi*

Main category: cs.LG

TL;DR: 论文提出了一种基于高斯过程的框架，用于处理图结构数据标签预测问题，特别是在数据稀缺时避免过拟合。通过扩展到单纯复形（SCs）并结合Hodge分解，该方法能够处理高阶属性并提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 图神经网络（GNNs）在数据稀缺时容易过拟合，导致性能下降。高斯过程（GPs）作为一种替代方法，被扩展到单纯复形（SCs）以解决这一问题。

Method: 将高斯过程框架扩展到单纯复形（SCs），并结合Hodge分解，以处理边级和高阶属性，同时利用同调信息（如孔洞数量）增强表示。

Result: 实验表明，该方法在多种应用中提升了预测性能。

Conclusion: 该框架为高斯过程在图和单纯复形级别的预测中更广泛应用铺平了道路。

Abstract: Predicting the labels of graph-structured data is crucial in scientific
applications and is often achieved using graph neural networks (GNNs). However,
when data is scarce, GNNs suffer from overfitting, leading to poor performance.
Recently, Gaussian processes (GPs) with graph-level inputs have been proposed
as an alternative. In this work, we extend the Gaussian process framework to
simplicial complexes (SCs), enabling the handling of edge-level attributes and
attributes supported on higher-order simplices. We further augment the
resulting SC representations by considering their Hodge decompositions,
allowing us to account for homological information, such as the number of
holes, in the SC. We demonstrate that our framework enhances the predictions
across various applications, paving the way for GPs to be more widely used for
graph and SC-level predictions.

</details>


### [132] [Approximation and Generalization Abilities of Score-based Neural Network Generative Models for Sub-Gaussian Distributions](https://arxiv.org/abs/2505.10880)
*Guoji Fu,Wee Sun Lee*

Main category: cs.LG

TL;DR: 本文研究了基于分数的神经网络生成模型（SGMs）在估计未知分布时的逼近和泛化能力，提出了在较温和假设下实现最优收敛速率的框架。


<details>
  <summary>Details</summary>
Motivation: 研究SGMs在估计未知分布时的性能，尤其是在较弱的假设下（如仅需α-次高斯性）实现最优收敛速率。

Method: 使用深度ReLU神经网络，通过分数匹配损失和早期停止策略，在较温和假设下逼近目标分布。

Result: 证明了SGMs在分数估计中可实现接近最优的收敛速率，并去除了对分数函数Lipschitz连续性和目标密度严格正下界的关键假设。

Conclusion: SGMs在较温和条件下仍能实现高效逼近和泛化，为实际应用提供了更广泛的适用性。

Abstract: This paper studies the approximation and generalization abilities of
score-based neural network generative models (SGMs) in estimating an unknown
distribution $P_0$ from $n$ i.i.d. observations in $d$ dimensions. Assuming
merely that $P_0$ is $\alpha$-sub-Gaussian, we prove that for any time step $t
\in [t_0, n^{O(1)}]$, where $t_0 \geq O(\alpha^2n^{-2/d}\log n)$, there exists
a deep ReLU neural network with width $\leq O(\log^3n)$ and depth $\leq
O(n^{3/d}\log_2n)$ that can approximate the scores with $\tilde{O}(n^{-1})$
mean square error and achieve a nearly optimal rate of
$\tilde{O}(n^{-1}t_0^{-d/2})$ for score estimation, as measured by the score
matching loss. Our framework is universal and can be used to establish
convergence rates for SGMs under milder assumptions than previous work. For
example, assuming further that the target density function $p_0$ lies in
Sobolev or Besov classes, with an appropriately early stopping strategy, we
demonstrate that neural network-based SGMs can attain nearly minimax
convergence rates up to logarithmic factors. Our analysis removes several
crucial assumptions, such as Lipschitz continuity of the score function or a
strictly positive lower bound on the target density.

</details>


### [133] [Prior-Guided Diffusion Planning for Offline Reinforcement Learning](https://arxiv.org/abs/2505.10881)
*Donghyeon Ki,JunHyeok Oh,Seong-Woong Shim,Byung-Jun Lee*

Main category: cs.LG

TL;DR: 论文提出了一种名为Prior Guidance（PG）的新方法，用于改进扩散模型在离线强化学习中的性能，通过可学习的分布替代标准高斯先验，优化行为正则化目标，从而直接生成高价值轨迹。


<details>
  <summary>Details</summary>
Motivation: 现有引导采样策略（如Classifier Guidance、Classifier-Free Guidance和Monte Carlo Sample Selection）在多模态动作生成、分布漂移或推理成本方面存在不足，需要改进。

Method: 提出Prior Guidance（PG）框架，用可学习的分布替代标准高斯先验，并通过行为正则化目标优化，直接在潜在空间中应用行为正则化。

Result: PG在多个长时域离线强化学习基准测试中优于现有扩散策略和规划器。

Conclusion: PG通过优化先验分布和行为正则化，显著提升了扩散模型在离线强化学习中的性能，且无需昂贵的奖励优化或多候选采样。

Abstract: Diffusion models have recently gained prominence in offline reinforcement
learning due to their ability to effectively learn high-performing,
generalizable policies from static datasets. Diffusion-based planners
facilitate long-horizon decision-making by generating high-quality trajectories
through iterative denoising, guided by return-maximizing objectives. However,
existing guided sampling strategies such as Classifier Guidance,
Classifier-Free Guidance, and Monte Carlo Sample Selection either produce
suboptimal multi-modal actions, struggle with distributional drift, or incur
prohibitive inference-time costs. To address these challenges, we propose Prior
Guidance (PG), a novel guided sampling framework that replaces the standard
Gaussian prior of a behavior-cloned diffusion model with a learnable
distribution, optimized via a behavior-regularized objective. PG directly
generates high-value trajectories without costly reward optimization of the
diffusion model itself, and eliminates the need to sample multiple candidates
at inference for sample selection. We present an efficient training strategy
that applies behavior regularization in latent space, and empirically
demonstrate that PG outperforms state-of-the-art diffusion policies and
planners across diverse long-horizon offline RL benchmarks.

</details>


### [134] [Global Convergence of Adaptive Sensing for Principal Eigenvector Estimation](https://arxiv.org/abs/2505.10882)
*Alex Saad-Falcon,Brighton Ancelin,Justin Romberg*

Main category: cs.LG

TL;DR: 该论文提出了一种基于压缩采样的自适应感知Oja算法变体，用于高效处理高维空间中的主成分分析（PCA），并证明了其在噪声环境下的全局收敛性。


<details>
  <summary>Details</summary>
Motivation: 传统PCA方法在高维数据中计算成本高，而Oja算法等子空间跟踪方法需要全维度观测。本文旨在解决这些限制，提出一种更高效的压缩采样方法。

Method: 通过每次迭代仅采集两个压缩测量（一个沿当前估计方向，一个沿随机正交方向），结合自适应感知，分析其收敛性。

Result: 算法在噪声环境下实现全局收敛，分为预热阶段和局部收敛阶段，收敛速率与现有极小极大下界一致。

Conclusion: 该研究为子空间跟踪提供了首个自适应感知下的收敛保证，简化了证明方法，对高维数据应用具有重要意义。

Abstract: This paper addresses the challenge of efficient principal component analysis
(PCA) in high-dimensional spaces by analyzing a compressively sampled variant
of Oja's algorithm with adaptive sensing. Traditional PCA methods incur
substantial computational costs that scale poorly with data dimensionality,
whereas subspace tracking algorithms like Oja's offer more efficient
alternatives but typically require full-dimensional observations. We analyze a
variant where, at each iteration, only two compressed measurements are taken:
one in the direction of the current estimate and one in a random orthogonal
direction. We prove that this adaptive sensing approach achieves global
convergence in the presence of noise when tracking the leading eigenvector of a
datastream with eigengap $\Delta=\lambda_1-\lambda_2$. Our theoretical analysis
demonstrates that the algorithm experiences two phases: (1) a warmup phase
requiring $O(\lambda_1\lambda_2d^2/\Delta^2)$ iterations to achieve a
constant-level alignment with the true eigenvector, followed by (2) a local
convergence phase where the sine alignment error decays at a rate of
$O(\lambda_1\lambda_2d^2/\Delta^2 t)$ for iterations $t$. The guarantee aligns
with existing minimax lower bounds with an added factor of $d$ due to the
compressive sampling. This work provides the first convergence guarantees in
adaptive sensing for subspace tracking with noise. Our proof technique is also
considerably simpler than those in prior works. The results have important
implications for applications where acquiring full-dimensional samples is
challenging or costly.

</details>


### [135] [Multi-Objective Preference Optimization: Improving Human Alignment of Generative Models](https://arxiv.org/abs/2505.10892)
*Akhil Agnihotri,Rahul Jain,Deepak Ramachandran,Zheng Wen*

Main category: cs.LG

TL;DR: 本文提出了一种多目标偏好优化算法（MOPO），用于解决LLMs在多目标对齐问题中的局限性，通过约束KL正则化优化实现多目标平衡。


<details>
  <summary>Details</summary>
Motivation: 现有偏好优化技术（如DPO、IPO）仅适用于单一目标，而实际应用中用户有多个潜在冲突的目标（如帮助性和无害性），缺乏自然聚合方法。

Method: MOPO算法将对齐问题建模为带约束的KL正则化优化，主目标最大化，次目标通过可调安全阈值限制。直接处理成对偏好数据，无需点式奖励假设或启发式提示工程。

Result: 在合成基准测试中，MOPO能逼近帕累托前沿；在1.3B参数语言模型上微调时，其奖励更高且策略帕累托优于基线，消融实验验证了稳定性和超参数鲁棒性。

Conclusion: MOPO为多目标偏好对齐提供了高效、可扩展的解决方案，适用于实际大规模训练。

Abstract: Post-training of LLMs with RLHF, and subsequently preference optimization
algorithms such as DPO, IPO, etc., made a big difference in improving human
alignment. However, all such techniques can only work with a single (human)
objective. In practice, human users have multiple objectives, such as
helpfulness and harmlessness, and there is no natural way to aggregate them
into a single objective. In this paper, we address the multi-objective
preference-alignment problem, where a policy must optimize several, potentially
conflicting, objectives. We introduce the Multi-Objective Preference
Optimization (MOPO) algorithm, which frames alignment as a constrained
KL-regularized optimization: the primary objective is maximized while secondary
objectives are lower-bounded by tunable safety thresholds. Unlike prior work,
MOPO operates directly on pairwise preference data, requires no point-wise
reward assumption, and avoids heuristic prompt-context engineering. The method
recovers policies on the Pareto front whenever the front is attainable;
practically, it reduces to simple closed-form iterative updates suitable for
large-scale training. On synthetic benchmarks with diverse canonical preference
structures, we show that MOPO approximates the Pareto front. When fine-tuning a
1.3B-parameter language model on real-world human-preference datasets, MOPO
attains higher rewards and yields policies that Pareto-dominate baselines;
ablation studies confirm optimization stability and robustness to
hyperparameters.

</details>


### [136] [CTP: A hybrid CNN-Transformer-PINN model for ocean front forecasting](https://arxiv.org/abs/2505.10894)
*Yishuo Wang,Feng Zhou,Muping Zhou,Qicheng Meng,Zhijun Hu,Yi Wang*

Main category: cs.LG

TL;DR: CTP是一个结合CNN、Transformer和PINN的深度学习框架，用于海洋锋面预测，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 海洋锋面在海洋生物地球化学和物理过程中起关键作用，现有方法在多步预测中难以保持空间连续性和物理一致性。

Method: CTP结合局部空间编码、长程时间注意力和物理约束，解决现有方法的不足。

Result: 在南海和黑潮区域的实验中，CTP在单步和多步预测中均达到SOTA性能，显著优于基线模型。

Conclusion: CTP通过整合多种架构和物理约束，显著提升了海洋锋面预测的准确性和稳定性。

Abstract: This paper proposes CTP, a novel deep learning framework that integrates
convolutional neural network(CNN), Transformer architectures, and
physics-informed neural network(PINN) for ocean front prediction. Ocean fronts,
as dynamic interfaces between distinct water masses, play critical roles in
marine biogeochemical and physical processes. Existing methods such as LSTM,
ConvLSTM, and AttentionConv often struggle to maintain spatial continuity and
physical consistency over multi-step forecasts. CTP addresses these challenges
by combining localized spatial encoding, long-range temporal attention, and
physical constraint enforcement. Experimental results across south China
sea(SCS) and Kuroshio(KUR) regions from 1993 to 2020 demonstrate that CTP
achieves state-of-the-art(SOTA) performance in both single-step and multi-step
predictions, significantly outperforming baseline models in accuracy, $F_1$
score, and temporal stability.

</details>


### [137] [Automated Identification of Logical Errors in Programs: Advancing Scalable Analysis of Student Misconceptions](https://arxiv.org/abs/2505.10913)
*Muntasir Hoq,Ananya Rao,Reisha Jaishankar,Krish Piryani,Nithya Janapati,Jessica Vandenberg,Bradford Mott,Narges Norouzi,James Lester,Bita Akram*

Main category: cs.LG

TL;DR: 论文提出了一种基于AST嵌入模型（SANN）的框架，用于自动检测学生编程中的逻辑错误，帮助教育者提供针对性支持。


<details>
  <summary>Details</summary>
Motivation: 在计算机科学教育中，识别学生编程困难的具体原因对提供有效学习支持至关重要。

Method: 使用可解释的AST嵌入模型（SANN）分析学生代码中的逻辑错误，通过子树注意力神经网络识别错误结构。

Result: 实验表明，该框架能准确捕捉逻辑错误，并为理解学生学习过程提供更深层次见解。

Conclusion: 该框架为编程教育提供了有价值的工具，有助于提升学习效果。

Abstract: In Computer Science (CS) education, understanding factors contributing to
students' programming difficulties is crucial for effective learning support.
By identifying specific issues students face, educators can provide targeted
assistance to help them overcome obstacles and improve learning outcomes. While
identifying sources of struggle, such as misconceptions, in real-time can be
challenging in current educational practices, analyzing logical errors in
students' code can offer valuable insights. This paper presents a scalable
framework for automatically detecting logical errors in students' programming
solutions. Our framework is based on an explainable Abstract Syntax Tree (AST)
embedding model, the Subtree-based Attention Neural Network (SANN), that
identifies the structural components of programs containing logical errors. We
conducted a series of experiments to evaluate its effectiveness, and the
results suggest that our framework can accurately capture students' logical
errors and, more importantly, provide us with deeper insights into their
learning processes, offering a valuable tool for enhancing programming
education.

</details>


### [138] [A Dataset for Spatiotemporal-Sensitive POI Question Answering](https://arxiv.org/abs/2505.10928)
*Xiao Han,Dayan Pan,Xiangyu Zhao,Xuyuan Hu,Zhaolin Deng,Xiangjie Kong,Guojiang Shen*

Main category: cs.LG

TL;DR: 论文提出了POI-QA数据集，填补了现有QA数据集中缺乏时空敏感问题的空白，并通过实验展示了当前多语言大模型在时空推理任务上的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有QA数据集缺乏时空敏感问题，无法充分评估模型的时空推理能力，因此需要构建一个专门的时空敏感QA数据集。

Method: 通过挖掘和整合开源车辆轨迹数据与高精度地理POI数据，手动验证噪声时空事实，并生成双语QA对，构建了POI-QA数据集。

Result: 实验表明，即使是性能最好的模型（Qwen2.5-7B）在HR@10指标上仅达到0.41，远低于人类的0.56。

Conclusion: POI-QA是一个有效的时空推理基准数据集，突显了大模型在时空推理任务上的不足，并为其改进提供了方向。

Abstract: Spatiotemporal relationships are critical in data science, as many prediction
and reasoning tasks require analysis across both spatial and temporal
dimensions--for instance, navigating an unfamiliar city involves planning
itineraries that sequence locations and timing cultural experiences. However,
existing Question-Answering (QA) datasets lack sufficient
spatiotemporal-sensitive questions, making them inadequate benchmarks for
evaluating models' spatiotemporal reasoning capabilities. To address this gap,
we introduce POI-QA, a novel spatiotemporal-sensitive QA dataset centered on
Point of Interest (POI), constructed through three key steps: mining and
aligning open-source vehicle trajectory data from GAIA with high-precision
geographic POI data, rigorous manual validation of noisy spatiotemporal facts,
and generating bilingual (Chinese/English) QA pairs that reflect
human-understandable spatiotemporal reasoning tasks. Our dataset challenges
models to parse complex spatiotemporal dependencies, and evaluations of
state-of-the-art multilingual LLMs (e.g., Qwen2.5-7B, Llama3.1-8B) reveal stark
limitations: even the top-performing model (Qwen2.5-7B fine-tuned with
RAG+LoRA) achieves a top 10 Hit Ratio (HR@10) of only 0.41 on the easiest task,
far below human performance at 0.56. This underscores persistent weaknesses in
LLMs' ability to perform consistent spatiotemporal reasoning, while
highlighting POI-QA as a robust benchmark to advance algorithms sensitive to
spatiotemporal dynamics. The dataset is publicly available at
https://www.kaggle.com/ds/7394666.

</details>


### [139] [Physics-informed Temporal Alignment for Auto-regressive PDE Foundation Models](https://arxiv.org/abs/2505.10930)
*Congcong Zhu,Xiaoyan Xu,Jiayue Han,Jingrun Chen*

Main category: cs.LG

TL;DR: 论文提出了一种名为PITA的自监督学习框架，通过物理约束解决自回归PDE基础模型中的误差累积问题，显著提升了模型在时间依赖PDE数据上的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 自回归PDE基础模型在处理时间依赖数据时存在误差累积问题，尤其在分布外数据上表现不佳。

Method: 提出PITA框架，通过物理约束自监督信号，对齐不同时间步的物理动态，无需已知物理先验。

Result: 实验表明，PITA显著提升了现有基础模型在时间依赖PDE数据上的准确性和鲁棒性。

Conclusion: PITA通过自监督学习有效解决了自回归PDE模型中的误差累积问题，具有强泛化能力。

Abstract: Auto-regressive partial differential equation (PDE) foundation models have
shown great potential in handling time-dependent data. However, these models
suffer from the shortcut problem deeply rooted in auto-regressive prediction,
causing error accumulation. The challenge becomes particularly evident for
out-of-distribution data, as the pretraining performance may approach random
model initialization for downstream tasks with long-term dynamics. To deal with
this problem, we propose physics-informed temporal alignment (PITA), a
self-supervised learning framework inspired by inverse problem solving.
Specifically, PITA aligns the physical dynamics discovered at different time
steps on each given PDE trajectory by integrating physics-informed constraints
into the self-supervision signal. The alignment is derived from observation
data without relying on known physics priors, indicating strong generalization
ability to the out-of-distribution data. Extensive experiments show that PITA
significantly enhances the accuracy and robustness of existing foundation
models on diverse time-dependent PDE data. The code is available at
https://github.com/SCAILab-USTC/PITA.

</details>


### [140] [Privacy-Aware Lifelong Learning](https://arxiv.org/abs/2505.10941)
*Ozan Özdenizci,Elmar Rueckert,Robert Legenstein*

Main category: cs.LG

TL;DR: 论文提出了一种隐私感知的终身学习（PALL）方法，旨在同时实现任务增量学习中的知识保留和选择性遗忘敏感信息，解决了终身学习与隐私保护的矛盾目标。


<details>
  <summary>Details</summary>
Motivation: 终身学习算法通常关注知识积累，而机器遗忘则关注隐私保护下的选择性遗忘。如何在同一模型中实现这两种功能是一个未解决的挑战。

Method: PALL通过优化任务特定的稀疏子网络，结合参数共享和记忆重放机制，实现知识保留和精确遗忘。

Result: 实验表明，PALL在图像分类任务中具有可扩展性，并能实现精确遗忘而不影响性能。

Conclusion: PALL为负责任的人工智能应用提供了一种终身学习与隐私保护相结合的前沿解决方案。

Abstract: Lifelong learning algorithms enable models to incrementally acquire new
knowledge without forgetting previously learned information. Contrarily, the
field of machine unlearning focuses on explicitly forgetting certain previous
knowledge from pretrained models when requested, in order to comply with data
privacy regulations on the right-to-be-forgotten. Enabling efficient lifelong
learning with the capability to selectively unlearn sensitive information from
models presents a critical and largely unaddressed challenge with contradicting
objectives. We address this problem from the perspective of simultaneously
preventing catastrophic forgetting and allowing forward knowledge transfer
during task-incremental learning, while ensuring exact task unlearning and
minimizing memory requirements, based on a single neural network model to be
adapted. Our proposed solution, privacy-aware lifelong learning (PALL),
involves optimization of task-specific sparse subnetworks with parameter
sharing within a single architecture. We additionally utilize an episodic
memory rehearsal mechanism to facilitate exact unlearning without performance
degradations. We empirically demonstrate the scalability of PALL across various
architectures in image classification, and provide a state-of-the-art solution
that uniquely integrates lifelong learning and privacy-aware unlearning
mechanisms for responsible AI applications.

</details>


### [141] [Certifying Stability of Reinforcement Learning Policies using Generalized Lyapunov Functions](https://arxiv.org/abs/2505.10947)
*Kehan Long,Jorge Cortés,Nikolay Atanasov*

Main category: cs.LG

TL;DR: 论文提出了一种通过增强RL值函数和神经网络残差项来学习广义Lyapunov函数的方法，用于认证学习控制策略的稳定性。


<details>
  <summary>Details</summary>
Motivation: 传统Lyapunov方法难以构造学习控制策略的稳定性证书，而RL值函数是自然的Lyapunov候选函数，但需要改进。

Method: 研究LQR问题，提出通过增强RL值函数和神经网络残差项学习广义Lyapunov函数，并扩展到非线性设置。

Result: 成功认证了Gymnasium和DeepMind Control基准上的RL策略稳定性，并扩展方法联合训练神经控制器和稳定性证书。

Conclusion: 该方法通过简化稳定性证书的构造，将经典控制理论与现代学习方法结合，适用于广泛系统。

Abstract: We study the problem of certifying the stability of closed-loop systems under
control policies derived from optimal control or reinforcement learning (RL).
Classical Lyapunov methods require a strict step-wise decrease in the Lyapunov
function but such a certificate is difficult to construct for a learned control
policy. The value function associated with an RL policy is a natural Lyapunov
function candidate but it is not clear how it should be modified. To gain
intuition, we first study the linear quadratic regulator (LQR) problem and make
two key observations. First, a Lyapunov function can be obtained from the value
function of an LQR policy by augmenting it with a residual term related to the
system dynamics and stage cost. Second, the classical Lyapunov decrease
requirement can be relaxed to a generalized Lyapunov condition requiring only
decrease on average over multiple time steps. Using this intuition, we consider
the nonlinear setting and formulate an approach to learn generalized Lyapunov
functions by augmenting RL value functions with neural network residual terms.
Our approach successfully certifies the stability of RL policies trained on
Gymnasium and DeepMind Control benchmarks. We also extend our method to jointly
train neural controllers and stability certificates using a multi-step Lyapunov
loss, resulting in larger certified inner approximations of the region of
attraction compared to the classical Lyapunov approach. Overall, our
formulation enables stability certification for a broad class of systems with
learned policies by making certificates easier to construct, thereby bridging
classical control theory and modern learning-based methods.

</details>


### [142] [FP64 is All You Need: Rethinking Failure Modes in Physics-Informed Neural Networks](https://arxiv.org/abs/2505.10949)
*Chenhui Xu,Dancheng Liu,Amir Nassereldine,Jinjun Xiong*

Main category: cs.LG

TL;DR: PINNs在FP32精度下因LBFGS优化器过早收敛而失败，改用FP64精度可解决问题。


<details>
  <summary>Details</summary>
Motivation: 挑战传统观点，揭示PINNs失败模式的真正原因是算术精度不足而非局部最优。

Method: 通过比较FP32和FP64精度下PINNs的训练动态，分析优化器行为。

Result: FP64精度下，PINNs成功解决PDE，无失败模式。

Conclusion: 算术精度是神经网络可靠求解PDE的关键。

Abstract: Physics Informed Neural Networks (PINNs) often exhibit failure modes in which
the PDE residual loss converges while the solution error stays large, a
phenomenon traditionally blamed on local optima separated from the true
solution by steep loss barriers. We challenge this understanding by demonstrate
that the real culprit is insufficient arithmetic precision: with standard FP32,
the LBFGS optimizer prematurely satisfies its convergence test, freezing the
network in a spurious failure phase. Simply upgrading to FP64 rescues
optimization, enabling vanilla PINNs to solve PDEs without any failure modes.
These results reframe PINN failure modes as precision induced stalls rather
than inescapable local minima and expose a three stage training dynamic
unconverged, failure, success whose boundaries shift with numerical precision.
Our findings emphasize that rigorous arithmetic precision is the key to
dependable PDE solving with neural networks.

</details>


### [143] [Shackled Dancing: A Bit-Locked Diffusion Algorithm for Lossless and Controllable Image Steganography](https://arxiv.org/abs/2505.10950)
*Tianshuo Zhang,Gao Jia,Wenzhe Zhai,Rui Yann,Xianglei Xing*

Main category: cs.LG

TL;DR: SD²是一种基于扩散模型的生成式隐写方法，通过位锁定和采样注入实现可控信息嵌入，平衡了安全性、容量和图像质量。


<details>
  <summary>Details</summary>
Motivation: 现有空间和频域隐写方法在安全性、容量和感知质量之间存在权衡，扩散模型为自适应图像合成提供了新途径，但精确信息嵌入仍具挑战性。

Method: SD²结合位锁定和扩散采样注入，在生成轨迹中嵌入可控信息，利用扩散模型的表达能力合成多样化载体图像。

Result: SD²在100%准确率下实现完整信息恢复，显著优于现有方法，在安全性、嵌入容量和稳定性上表现突出。

Conclusion: SD²为可控生成提供了新思路，并为安全视觉通信开辟了有前景的方向。

Abstract: Data steganography aims to conceal information within visual content, yet
existing spatial- and frequency-domain approaches suffer from trade-offs
between security, capacity, and perceptual quality. Recent advances in
generative models, particularly diffusion models, offer new avenues for
adaptive image synthesis, but integrating precise information embedding into
the generative process remains challenging. We introduce Shackled Dancing
Diffusion, or SD$^2$, a plug-and-play generative steganography method that
combines bit-position locking with diffusion sampling injection to enable
controllable information embedding within the generative trajectory. SD$^2$
leverages the expressive power of diffusion models to synthesize diverse
carrier images while maintaining full message recovery with $100\%$ accuracy.
Our method achieves a favorable balance between randomness and constraint,
enhancing robustness against steganalysis without compromising image fidelity.
Extensive experiments show that SD$^2$ substantially outperforms prior methods
in security, embedding capacity, and stability. This algorithm offers new
insights into controllable generation and opens promising directions for secure
visual communication.

</details>


### [144] [SubGCache: Accelerating Graph-based RAG with Subgraph-level KV Cache](https://arxiv.org/abs/2505.10951)
*Qiuyu Zhu,Liang Zhang,Qianxiong Xu,Cheng Long,Jie Zhang*

Main category: cs.LG

TL;DR: SubGCache通过聚类查询并重用预计算的KV缓存，显著减少图检索增强生成（RAG）的推理延迟，同时保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 针对不同查询可能检索到相似子图作为提示的问题，提出SubGCache以减少重复计算，提升效率。

Method: SubGCache基于子图嵌入聚类查询，为每个聚类构建代表性子图并预计算其KV缓存，查询时直接复用。

Result: 实验表明，SubGCache在多个LLM和RAG框架上实现高达6.68倍的TTFT降低，且生成质量相当或更好。

Conclusion: SubGCache是一种高效的方法，能够显著减少推理延迟，同时保持或提升生成质量。

Abstract: Graph-based retrieval-augmented generation (RAG) enables large language
models (LLMs) to incorporate structured knowledge via graph retrieval as
contextual input, enhancing more accurate and context-aware reasoning. We
observe that for different queries, it could retrieve similar subgraphs as
prompts, and thus we propose SubGCache, which aims to reduce inference latency
by reusing computation across queries with similar structural prompts (i.e.,
subgraphs). Specifically, SubGCache clusters queries based on subgraph
embeddings, constructs a representative subgraph for each cluster, and
pre-computes the key-value (KV) cache of the representative subgraph. For each
query with its retrieved subgraph within a cluster, it reuses the pre-computed
KV cache of the representative subgraph of the cluster without computing the KV
tensors again for saving computation. Experiments on two new datasets across
multiple LLM backbones and graph-based RAG frameworks demonstrate that
SubGCache consistently reduces inference latency with comparable and even
improved generation quality, achieving up to 6.68$\times$ reduction in
time-to-first-token (TTFT).

</details>


### [145] [Constrained Preferential Bayesian Optimization and Its Application in Banner Ad Design](https://arxiv.org/abs/2505.10954)
*Koki Iwai,Yusuke Kumagae,Yuki Koyama,Masahiro Hamasaki,Masataka Goto*

Main category: cs.LG

TL;DR: 论文提出了约束偏好贝叶斯优化（CPBO），扩展了偏好贝叶斯优化（PBO），首次引入不等式约束，并通过新的采集函数实现。实验证明CPBO能有效探索可行区域并找到最优解，实际应用于广告设计系统。


<details>
  <summary>Details</summary>
Motivation: 现有PBO方法未考虑现实优化任务中的不等式约束，限制了其在人机交互场景中的应用。

Method: 提出CPBO方法，引入不等式约束，设计新的采集函数，并在广告设计系统中进行实际应用。

Result: CPBO成功探索可行区域并找到最优解，用户研究表明其在指导创意设计中的潜力。

Conclusion: CPBO填补了PBO在约束优化中的空白，为实际应用提供了有效工具。

Abstract: Preferential Bayesian optimization (PBO) is a variant of Bayesian
optimization that observes relative preferences (e.g., pairwise comparisons)
instead of direct objective values, making it especially suitable for
human-in-the-loop scenarios. However, real-world optimization tasks often
involve inequality constraints, which existing PBO methods have not yet
addressed. To fill this gap, we propose constrained preferential Bayesian
optimization (CPBO), an extension of PBO that incorporates inequality
constraints for the first time. Specifically, we present a novel acquisition
function for this purpose. Our technical evaluation shows that our CPBO method
successfully identifies optimal solutions by focusing on exploring feasible
regions. As a practical application, we also present a designer-in-the-loop
system for banner ad design using CPBO, where the objective is the designer's
subjective preference, and the constraint ensures a target predicted
click-through rate. We conducted a user study with professional ad designers,
demonstrating the potential benefits of our approach in guiding creative design
under real-world constraints.

</details>


### [146] [Relational Graph Transformer](https://arxiv.org/abs/2505.10960)
*Vijay Prakash Dwivedi,Sri Jaladi,Yangyi Shen,Federico López,Charilaos I. Kanatsoulis,Rishi Puri,Matthias Fey,Jure Leskovec*

Main category: cs.LG

TL;DR: RelGT是一种专为关系表设计的图变换器架构，通过多元素标记化策略和局部与全局注意力机制，显著提升了关系深度学习任务的性能。


<details>
  <summary>Details</summary>
Motivation: 传统图神经网络（GNN）在捕捉关系数据的复杂结构模式和长程依赖方面存在局限性，而现有的图变换器在关系实体图上的应用面临位置编码、时间动态建模和结构信息丢失等挑战。

Method: RelGT采用多元素标记化策略，将节点分解为五个组件（特征、类型、跳数距离、时间和局部结构），并结合局部子图采样注意力与全局可学习质心注意力。

Result: 在RelBench基准测试的21个任务中，RelGT性能优于GNN基线模型，最高提升18%。

Conclusion: RelGT证明了图变换器在关系深度学习中的强大潜力，为解决关系数据的复杂性问题提供了有效方案。

Abstract: Relational Deep Learning (RDL) is a promising approach for building
state-of-the-art predictive models on multi-table relational data by
representing it as a heterogeneous temporal graph. However, commonly used Graph
Neural Network models suffer from fundamental limitations in capturing complex
structural patterns and long-range dependencies that are inherent in relational
data. While Graph Transformers have emerged as powerful alternatives to GNNs on
general graphs, applying them to relational entity graphs presents unique
challenges: (i) Traditional positional encodings fail to generalize to massive,
heterogeneous graphs; (ii) existing architectures cannot model the temporal
dynamics and schema constraints of relational data; (iii) existing tokenization
schemes lose critical structural information. Here we introduce the Relational
Graph Transformer (RelGT), the first graph transformer architecture designed
specifically for relational tables. RelGT employs a novel multi-element
tokenization strategy that decomposes each node into five components (features,
type, hop distance, time, and local structure), enabling efficient encoding of
heterogeneity, temporality, and topology without expensive precomputation. Our
architecture combines local attention over sampled subgraphs with global
attention to learnable centroids, incorporating both local and database-wide
representations. Across 21 tasks from the RelBench benchmark, RelGT
consistently matches or outperforms GNN baselines by up to 18%, establishing
Graph Transformers as a powerful architecture for Relational Deep Learning.

</details>


### [147] [Group-in-Group Policy Optimization for LLM Agent Training](https://arxiv.org/abs/2505.10978)
*Lang Feng,Zhenghai Xue,Tingcong Liu,Bo An*

Main category: cs.LG

TL;DR: GiGPO是一种新型强化学习算法，通过两层次结构实现细粒度信用分配，适用于长周期LLM代理训练，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于群体的强化学习在长周期LLM代理训练中信用分配困难，GiGPO旨在解决这一问题。

Method: GiGPO采用两层次结构：宏观轨迹级和微观步骤级信用分配，通过锚状态分组机制实现细粒度评估。

Result: 在ALFWorld和WebShop基准测试中，GiGPO性能分别提升>12%和>9%，且不增加额外资源开销。

Conclusion: GiGPO有效解决了长周期LLM代理训练中的信用分配问题，同时保持了高效性和稳定性。

Abstract: Recent advances in group-based reinforcement learning (RL) have driven
frontier large language models (LLMs) in single-turn tasks like mathematical
reasoning. However, their scalability to long-horizon LLM agent training
remains limited. Unlike static tasks, agent-environment interactions unfold
over many steps and often yield sparse or delayed rewards, making credit
assignment across individual steps significantly more challenging. In this
work, we propose Group-in-Group Policy Optimization (GiGPO), a novel RL
algorithm that achieves fine-grained credit assignment for LLM agents while
preserving the appealing properties of group-based RL: critic-free, low memory,
and stable convergence. GiGPO introduces a two-level structure for estimating
relative advantage: (i) At the episode-level, GiGPO computes macro relative
advantages based on groups of complete trajectories; (ii) At the step-level,
GiGPO introduces an anchor state grouping mechanism that retroactively
constructs step-level groups by identifying repeated environment states across
trajectories. Actions stemming from the same state are grouped together,
enabling micro relative advantage estimation. This hierarchical structure
effectively captures both global trajectory quality and local step
effectiveness without relying on auxiliary models or additional rollouts. We
evaluate GiGPO on two challenging agent benchmarks, ALFWorld and WebShop, using
Qwen2.5-1.5B-Instruct and Qwen2.5-7B-Instruct. Crucially, GiGPO delivers
fine-grained per-step credit signals and achieves performance gains of > 12\%
on ALFWorld and > 9\% on WebShop over the GRPO baseline: all while maintaining
the same GPU memory overhead, identical LLM rollout, and incurring little to no
additional time cost.

</details>


### [148] [GenoArmory: A Unified Evaluation Framework for Adversarial Attacks on Genomic Foundation Models](https://arxiv.org/abs/2505.10983)
*Haozheng Luo,Chenghao Qiu,Yimin Wang,Shang Wu,Jiahao Yu,Han Liu,Binghui Wang,Yan Chen*

Main category: cs.LG

TL;DR: GenoArmory是首个针对基因组基础模型（GFMs）的统一对抗攻击基准，提供全面评估框架，分析模型脆弱性。


<details>
  <summary>Details</summary>
Motivation: 现有GFM基准缺乏对抗攻击评估，GenoArmory填补了这一空白，旨在系统评估GFMs的对抗鲁棒性。

Method: 使用四种攻击算法和三种防御策略评估五种GFMs的对抗鲁棒性，并分析模型架构、量化方案和训练数据的影响。

Result: 分类模型比生成模型更具鲁棒性；对抗攻击常针对生物重要区域，表明模型能有效捕捉序列特征。

Conclusion: GenoArmory为GFMs对抗攻击研究提供标准化工具，GenoAdv数据集可提升模型安全性。

Abstract: We propose the first unified adversarial attack benchmark for Genomic
Foundation Models (GFMs), named GenoArmory. Unlike existing GFM benchmarks,
GenoArmory offers the first comprehensive evaluation framework to
systematically assess the vulnerability of GFMs to adversarial attacks.
Methodologically, we evaluate the adversarial robustness of five
state-of-the-art GFMs using four widely adopted attack algorithms and three
defense strategies. Importantly, our benchmark provides an accessible and
comprehensive framework to analyze GFM vulnerabilities with respect to model
architecture, quantization schemes, and training datasets. Additionally, we
introduce GenoAdv, a new adversarial sample dataset designed to improve GFM
safety. Empirically, classification models exhibit greater robustness to
adversarial perturbations compared to generative models, highlighting the
impact of task type on model vulnerability. Moreover, adversarial attacks
frequently target biologically significant genomic regions, suggesting that
these models effectively capture meaningful sequence features.

</details>


### [149] [ReaCritic: Large Reasoning Transformer-based DRL Critic-model Scaling For Heterogeneous Networks](https://arxiv.org/abs/2505.10992)
*Feiran You,Hongyang Du*

Main category: cs.LG

TL;DR: 论文提出ReaCritic，一种基于大型推理Transformer的批评模型，通过水平和垂直推理提升DRL在异构网络中的决策能力。


<details>
  <summary>Details</summary>
Motivation: 异构网络的多样性和动态性导致现有DRL方法适应性不足，批评模型的浅层架构限制了多任务处理能力。

Method: ReaCritic采用Transformer架构，通过水平和垂直推理增强批评模型，适用于多种DRL算法。

Result: 实验表明ReaCritic在异构网络和标准控制任务中提升了收敛速度和最终性能。

Conclusion: ReaCritic通过引入推理能力显著提升了DRL在复杂动态环境中的表现。

Abstract: Heterogeneous Networks (HetNets) pose critical challenges for intelligent
management due to the diverse user requirements and time-varying wireless
conditions. These factors introduce significant decision complexity, which
limits the adaptability of existing Deep Reinforcement Learning (DRL) methods.
In many DRL algorithms, especially those involving value-based or actor-critic
structures, the critic component plays a key role in guiding policy learning by
estimating value functions. However, conventional critic models often use
shallow architectures that map observations directly to scalar estimates,
limiting their ability to handle multi-task complexity. In contrast, recent
progress in inference-time scaling of Large Language Models (LLMs) has shown
that generating intermediate reasoning steps can significantly improve decision
quality. Motivated by this, we propose ReaCritic, a large reasoning
transformer-based criticmodel scaling scheme that brings reasoning ability into
DRL. ReaCritic performs horizontal reasoning over parallel state-action inputs
and vertical reasoning through deep transformer stacks. It is compatible with a
broad range of value-based and actor-critic DRL algorithms and enhances
generalization in dynamic wireless environments. Extensive experiments
demonstrate that ReaCritic improves convergence speed and final performance
across various HetNet settings and standard OpenAI Gym control tasks.

</details>


### [150] [Logo-LLM: Local and Global Modeling with Large Language Models for Time Series Forecasting](https://arxiv.org/abs/2505.11017)
*Wenjie Ou,Zhishuo Zhao,Dongyue Guo,Yi Lin*

Main category: cs.LG

TL;DR: Logo-LLM是一个基于LLM的时间序列预测框架，通过提取多尺度时间特征并整合局部与全局信息，显著提升了预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer和LLM的方法在时间序列预测中忽视了局部短期变化，且未充分利用LLM的分层表示能力。

Method: Logo-LLM从预训练LLM的不同层提取多尺度特征，并引入Local-Mixer和Global-Mixer模块对齐和整合这些特征。

Result: 实验表明，Logo-LLM在多种基准测试中表现优异，具有强泛化能力和低计算开销。

Conclusion: Logo-LLM通过有效整合局部与全局特征，为时间序列预测提供了高效且通用的解决方案。

Abstract: Time series forecasting is critical across multiple domains, where time
series data exhibits both local patterns and global dependencies. While
Transformer-based methods effectively capture global dependencies, they often
overlook short-term local variations in time series. Recent methods that adapt
large language models (LLMs) into time series forecasting inherit this
limitation by treating LLMs as black-box encoders, relying solely on the
final-layer output and underutilizing hierarchical representations. To address
this limitation, we propose Logo-LLM, a novel LLM-based framework that
explicitly extracts and models multi-scale temporal features from different
layers of a pre-trained LLM. Through empirical analysis, we show that shallow
layers of LLMs capture local dynamics in time series, while deeper layers
encode global trends. Moreover, Logo-LLM introduces lightweight Local-Mixer and
Global-Mixer modules to align and integrate features with the temporal input
across layers. Extensive experiments demonstrate that Logo-LLM achieves
superior performance across diverse benchmarks, with strong generalization in
few-shot and zero-shot settings while maintaining low computational overhead.

</details>


### [151] [Informed, but Not Always Improved: Challenging the Benefit of Background Knowledge in GNNs](https://arxiv.org/abs/2505.11023)
*Kutalmış Coşkun,Ivo Kavisanczki,Amin Mirzaei,Tom Siegl,Bjarne C. Hiller,Stefan Lüdtke,Martin Becker*

Main category: cs.LG

TL;DR: 研究发现，在癌症亚型分类任务中，背景知识（BK）图对图神经网络（GNN）性能提升有限，且即使BK图被严重扰动，性能变化也不大。通过合成实验和扰动测试，揭示了GNN架构与BK特性需谨慎匹配的重要性。


<details>
  <summary>Details</summary>
Motivation: 探究背景知识（BK）图在生物医学领域（如癌症亚型分类）中对图神经网络（GNN）性能的实际贡献及不完美知识的影响。

Method: 引入评估框架，包括合成实验（明确BK信息性）和BK图扰动测试，分析BK感知模型在合成和真实生物医学场景中的鲁棒性。

Result: 发现BK对GNN性能提升有限，且模型性能对BK图的扰动不敏感；需谨慎匹配GNN架构与BK特性以实现显著性能改进。

Conclusion: GNN与BK特性的精准匹配是实现性能提升的关键，但当前BK在真实任务中的贡献被高估。

Abstract: In complex and low-data domains such as biomedical research, incorporating
background knowledge (BK) graphs, such as protein-protein interaction (PPI)
networks, into graph-based machine learning pipelines is a promising research
direction. However, while BK is often assumed to improve model performance, its
actual contribution and the impact of imperfect knowledge remain poorly
understood. In this work, we investigate the role of BK in an important
real-world task: cancer subtype classification. Surprisingly, we find that (i)
state-of-the-art GNNs using BK perform no better than uninformed models like
linear regression, and (ii) their performance remains largely unchanged even
when the BK graph is heavily perturbed. To understand these unexpected results,
we introduce an evaluation framework, which employs (i) a synthetic setting
where the BK is clearly informative and (ii) a set of perturbations that
simulate various imperfections in BK graphs. With this, we test the robustness
of BK-aware models in both synthetic and real-world biomedical settings. Our
findings reveal that careful alignment of GNN architectures and BK
characteristics is necessary but holds the potential for significant
performance improvements.

</details>


### [152] [Leveraging Real-Time Data Analysis and Multiple Kernel Learning for Manufacturing of Innovative Steels](https://arxiv.org/abs/2505.11024)
*Wolfgang Rannetbauer,Simon Hubmer,Carina Hambrock,Ronny Ramlau*

Main category: cs.LG

TL;DR: 论文提出通过实时数据分析和预测质量管理更新热喷涂涂层工艺，以应对钢铁制造中的动态需求。


<details>
  <summary>Details</summary>
Motivation: 热喷涂组件在钢铁制造中面临生产和维护挑战，标准化修复过程难以满足动态需求。

Method: 设计数据聚合器和质量预测器，结合实时监控和数据驱动方法，采用多核学习策略实现预测。

Result: 小规模测试验证了该组合能准确预测涂层质量并主动通知操作员偏差。

Conclusion: 更新后的工艺结合实时数据分析和预测管理，有效提升了热喷涂组件的性能和维护效率。

Abstract: The implementation of thermally sprayed components in steel manufacturing
presents challenges for production and plant maintenance. While enhancing
performance through specialized surface properties, these components may
encounter difficulties in meeting modified requirements due to standardization
in the refurbishment process. This article proposes updating the established
coating process for thermally spray coated components for steel manufacturing
(TCCSM) by integrating real-time data analytics and predictive quality
management. Two essential components--the data aggregator and the quality
predictor--are designed through continuous process monitoring and the
application of data-driven methodologies to meet the dynamic demands of the
evolving steel landscape. The quality predictor is powered by the simple and
effective multiple kernel learning strategy with the goal of realizing
predictive quality. The data aggregator, designed with sensors, flow meters,
and intelligent data processing for the thermal spray coating process, is
proposed to facilitate real-time analytics. The performance of this combination
was verified using small-scale tests that enabled not only the accurate
prediction of coating quality based on the collected data but also proactive
notification to the operator as soon as significant deviations are identified.

</details>


### [153] [Exploiting the Asymmetric Uncertainty Structure of Pre-trained VLMs on the Unit Hypersphere](https://arxiv.org/abs/2505.11029)
*Li Ju,Max Andersson,Stina Fredriksson,Edward Glöckner,Andreas Hellander,Ekta Vats,Prashant Singh*

Main category: cs.LG

TL;DR: 论文提出AsymVLM方法，解决现有视觉语言模型（VLMs）在处理文本和视觉数据不对称不确定性结构时的不足，通过在单位超球面上构建概率嵌入来量化不确定性。


<details>
  <summary>Details</summary>
Motivation: 现有确定性VLMs无法捕捉自然语言和视觉数据中的模糊性和不确定性，且现有概率后适应方法未考虑模态的不对称不确定性结构，导致性能不佳。

Method: 提出AsymVLM方法，在预训练VLMs的基础上，于单位超球面上构建概率嵌入，以量化不确定性。

Result: 在基准测试中验证了概率嵌入的有效性，并通过消融研究展示了文本和视觉数据不确定性结构的固有不对称性。

Conclusion: AsymVLM能有效处理模态的不对称不确定性结构，提升性能。

Abstract: Vision-language models (VLMs) as foundation models have significantly
enhanced performance across a wide range of visual and textual tasks, without
requiring large-scale training from scratch for downstream tasks. However,
these deterministic VLMs fail to capture the inherent ambiguity and uncertainty
in natural language and visual data. Recent probabilistic post-hoc adaptation
methods address this by mapping deterministic embeddings onto probability
distributions; however, existing approaches do not account for the asymmetric
uncertainty structure of the modalities, and the constraint that meaningful
deterministic embeddings reside on a unit hypersphere, potentially leading to
suboptimal performance. In this paper, we address the asymmetric uncertainty
structure inherent in textual and visual data, and propose AsymVLM to build
probabilistic embeddings from pre-trained VLMs on the unit hypersphere,
enabling uncertainty quantification. We validate the effectiveness of the
probabilistic embeddings on established benchmarks, and present comprehensive
ablation studies demonstrating the inherent nature of asymmetry in the
uncertainty structure of textual and visual data.

</details>


### [154] [Deep Latent Variable Model based Vertical Federated Learning with Flexible Alignment and Labeling Scenarios](https://arxiv.org/abs/2505.11035)
*Kihun Hong,Sejun Park,Ganguk Hwang*

Main category: cs.LG

TL;DR: 论文提出了一种新的垂直联邦学习（VFL）框架，解决了现有方法对数据对齐、标签和多方协作的限制性问题。


<details>
  <summary>Details</summary>
Motivation: 现有VFL方法通常对数据对齐、标签和多方协作有严格限制，无法适应实际场景中的多样性需求。

Method: 将VFL中的对齐问题重新解释为缺失数据问题，并提出一个统一框架，支持任意对齐和标签场景下的训练与推理。

Result: 在168种实验配置中，该方法在160种情况下优于基线，平均领先9.6个百分点。

Conclusion: 这是首个能同时处理任意数据对齐、无标签数据和多方协作的VFL框架。

Abstract: Federated learning (FL) has attracted significant attention for enabling
collaborative learning without exposing private data. Among the primary
variants of FL, vertical federated learning (VFL) addresses feature-partitioned
data held by multiple institutions, each holding complementary information for
the same set of users. However, existing VFL methods often impose restrictive
assumptions such as a small number of participating parties, fully aligned
data, or only using labeled data. In this work, we reinterpret alignment gaps
in VFL as missing data problems and propose a unified framework that
accommodates both training and inference under arbitrary alignment and labeling
scenarios, while supporting diverse missingness mechanisms. In the experiments
on 168 configurations spanning four benchmark datasets, six training-time
missingness patterns, and seven testing-time missingness patterns, our method
outperforms all baselines in 160 cases with an average gap of 9.6 percentage
points over the next-best competitors. To the best of our knowledge, this is
the first VFL framework to jointly handle arbitrary data alignment, unlabeled
data, and multi-party collaboration all at once.

</details>


### [155] [Efficient Attention via Pre-Scoring: Prioritizing Informative Keys in Transformers](https://arxiv.org/abs/2505.11040)
*Zhexiang Li,Haoyu Wang,Yutong Bao,David Woodruff*

Main category: cs.LG

TL;DR: 本文提出了一种预评分机制以改进HyperAttention，通过K-means、K-median聚类和杠杆评分来优先选择关键键，显著降低了困惑度，并在速度和准确性之间取得了平衡。


<details>
  <summary>Details</summary>
Motivation: HyperAttention的均匀残差采样限制了关键键的捕获，导致困惑度上升，因此需要一种更有效的评分机制来优化注意力分配。

Method: 引入三种预评分方法（K-means聚类、K-median聚类和杠杆评分）替代均匀残差采样，并与HyperAttention结合。

Result: 在ChatGLM2（131k token上下文）上困惑度从12降至8.3，速度仍比FlashAttention快20倍，且在ViT上与LevAttention精度相当。

Conclusion: 预评分机制显著提升了Transformer的效率，为长上下文建模提供了速度和准确性的平衡方案。

Abstract: Recent advances in transformer architectures deeply enhance long-context
language modeling. Among them, HyperAttention achieves competitive efficiency
by combining a single-level LSH-based clustering with uniform residual
sampling. However,such a sampling limits crucial keys' capturing, which in turn
raises the overall perplexity. In this paper, we propose a pre-scoring
mechanism to assist HyperAttention to prioritize significant keys.
Specifically, we introduce three scoring methods: K-means clustering, K-median
clustering, and leverage score-based ranking (inspired by LevAttention) to
filter keys effectively. We further replace HyperAttention's original uniform
residual sampling entirely, relying exclusively on our pre-scoring mechanism.
Experiments on ChatGLM2 (131k token context) reduce perplexity from 12 to 8.3,
which outperforms standard HyperAttention. Moreover, when running on the
Vision-Transformer (ViT), our method shows that it can guarantee similar
accuracy compared with LevAttention, and will surpass LevAttention given
specific parameters. Although this method introduces computational overhead,
its combination with HyperAttention remains 20 times faster than
FlashAttention, providing a balanced trade-off between speed and modeling
accuracy. Our results highlight the effectiveness of integrating pre-scoring
into hierarchical attention mechanisms, significantly improving Transformer's
efficiency.

</details>


### [156] [Exploration by Random Distribution Distillation](https://arxiv.org/abs/2505.11044)
*Zhirui Fang,Kai Yang,Jian Tao,Jiafei Lyu,Lusong Li,Li Shen,Xiu Li*

Main category: cs.LG

TL;DR: 本文提出了一种名为随机分布蒸馏（RDD）的新方法，通过将目标网络的输出建模为正态分布样本，结合伪计数和预测误差，统一了基于计数和基于预测误差的探索方法。


<details>
  <summary>Details</summary>
Motivation: 在线强化学习中，探索是一个关键挑战，现有方法（如基于计数和基于预测误差的方法）各有局限性，需要一种更有效的统一方法。

Method: RDD方法通过从正态分布中采样目标网络的输出，将预测网络与目标网络的差异作为内在奖励，结合伪计数和预测误差项。

Result: RDD在实验中表现优于现有方法，理论分析和实验结果均验证了其有效性。

Conclusion: RDD成功统一了两种探索方法，并在高维空间中保持了预测误差方法的优势，同时实现了类似伪计数的奖励衰减模式。

Abstract: Exploration remains a critical challenge in online reinforcement learning, as
an agent must effectively explore unknown environments to achieve high returns.
Currently, the main exploration algorithms are primarily count-based methods
and curiosity-based methods, with prediction-error methods being a prominent
example. In this paper, we propose a novel method called \textbf{R}andom
\textbf{D}istribution \textbf{D}istillation (RDD), which samples the output of
a target network from a normal distribution. RDD facilitates a more extensive
exploration by explicitly treating the difference between the prediction
network and the target network as an intrinsic reward. Furthermore, by
introducing randomness into the output of the target network for a given state
and modeling it as a sample from a normal distribution, intrinsic rewards are
bounded by two key components: a pseudo-count term ensuring proper exploration
decay and a discrepancy term accounting for predictor convergence. We
demonstrate that RDD effectively unifies both count-based and prediction-error
approaches. It retains the advantages of prediction-error methods in
high-dimensional spaces, while also implementing an intrinsic reward decay mode
akin to the pseudo-count method. In the experimental section, RDD is compared
with more advanced methods in a series of environments. Both theoretical
analysis and experimental results confirm the effectiveness of our approach in
improving online exploration for reinforcement learning tasks.

</details>


### [157] [Halting Recurrent GNNs and the Graded $μ$-Calculus](https://arxiv.org/abs/2505.11050)
*Jeroen Bollen,Jan Van den Bussche,Stijn Vansummeren,Jonni Virtema*

Main category: cs.LG

TL;DR: 提出了一种用于循环图神经网络（GNN）的停止机制，证明其能够表达所有在分级模态μ演算中定义的节点分类器，且不依赖于图的大小。


<details>
  <summary>Details</summary>
Motivation: 当前循环GNN要么假设图大小已知，要么缺乏终止保证，因此需要一种新的停止机制来解决这些问题。

Method: 开发了一种新的分级μ演算近似语义，并基于此提出了一个不依赖图大小的计数算法，最终将该算法实现为停止循环GNN。

Result: 证明了停止循环GNN可以表达分级模态μ演算中定义的所有节点分类器，且不依赖于图的大小。

Conclusion: 提出的停止机制和计数算法为循环GNN的表达能力和终止性提供了理论保证，具有独立的研究价值。

Abstract: Graph Neural Networks (GNNs) are a class of machine-learning models that
operate on graph-structured data. Their expressive power is intimately related
to logics that are invariant under graded bisimilarity. Current proposals for
recurrent GNNs either assume that the graph size is given to the model, or
suffer from a lack of termination guarantees. In this paper, we propose a
halting mechanism for recurrent GNNs. We prove that our halting model can
express all node classifiers definable in graded modal mu-calculus, even for
the standard GNN variant that is oblivious to the graph size. A recent
breakthrough in the study of the expressivity of graded modal mu-calculus in
the finite suggests that conversely, restricted to node classifiers definable
in monadic second-order logic, recurrent GNNs can express only node classifiers
definable in graded modal mu-calculus. To prove our main result, we develop a
new approximate semantics for graded mu-calculus, which we believe to be of
independent interest. We leverage this new semantics into a new model-checking
algorithm, called the counting algorithm, which is oblivious to the graph size.
In a final step we show that the counting algorithm can be implemented on a
halting recurrent GNN.

</details>


### [158] [NeuralSurv: Deep Survival Analysis with Bayesian Uncertainty Quantification](https://arxiv.org/abs/2505.11054)
*Mélodie Monod,Alessandro Micheli,Samir Bhatt*

Main category: cs.LG

TL;DR: NeuralSurv是首个结合贝叶斯不确定性量化的深度生存模型，通过两阶段数据增强方案灵活捕捉连续时间内的协变量-风险关系，并提供理论保证。


<details>
  <summary>Details</summary>
Motivation: 现有深度生存模型缺乏对不确定性的量化，而贝叶斯方法在数据稀缺情况下能提升模型校准和鲁棒性。

Method: 提出非参数、架构无关的框架，采用两阶段数据增强方案，并设计高效的均值场变分算法进行后验推断。

Result: 实验表明，NeuralSurv在模型校准上优于现有深度生存模型，同时在判别性能上匹配或超越它们。

Conclusion: 贝叶斯方法在数据稀缺情况下能显著提升模型校准，并提供稳健的生存函数不确定性估计。

Abstract: We introduce NeuralSurv, the first deep survival model to incorporate
Bayesian uncertainty quantification. Our non-parametric, architecture-agnostic
framework flexibly captures time-varying covariate-risk relationships in
continuous time via a novel two-stage data-augmentation scheme, for which we
establish theoretical guarantees. For efficient posterior inference, we
introduce a mean-field variational algorithm with coordinate-ascent updates
that scale linearly in model size. By locally linearizing the Bayesian neural
network, we obtain full conjugacy and derive all coordinate updates in closed
form. In experiments, NeuralSurv delivers superior calibration compared to
state-of-the-art deep survival models, while matching or exceeding their
discriminative performance across both synthetic benchmarks and real-world
datasets. Our results demonstrate the value of Bayesian principles in
data-scarce regimes by enhancing model calibration and providing robust,
well-calibrated uncertainty estimates for the survival function.

</details>


### [159] [Assessing the Performance of Analog Training for Transfer Learning](https://arxiv.org/abs/2505.11067)
*Omobayode Fagbohungbe,Corey Lammie,Malte J. Rasch,Takashi Ando,Tayfun Gokmen,Vijay Narayanan*

Main category: cs.LG

TL;DR: 论文提出了一种新的算法c-TTv2，用于解决模拟内存计算中的训练挑战，并在Swin-ViT模型上评估其性能。


<details>
  <summary>Details</summary>
Motivation: 模拟内存计算具有高效能潜力，但现有训练算法无法应对设备非线性和不对称性问题，需要新方法。

Method: 采用c-TTv2算法，利用chopped技术解决设备非线性和不对称性问题，并在Swin-ViT模型上测试。

Result: 评估了c-TTv2在CIFAR100数据集上的性能，并研究了其对设备规格变化的鲁棒性。

Conclusion: c-TTv2算法在模拟内存计算中表现出潜力，能够应对设备非线性和不对称性问题。

Abstract: Analog in-memory computing is a next-generation computing paradigm that
promises fast, parallel, and energy-efficient deep learning training and
transfer learning (TL). However, achieving this promise has remained elusive
due to a lack of suitable training algorithms. Analog memory devices exhibit
asymmetric and non-linear switching behavior in addition to device-to-device
variation, meaning that most, if not all, of the current off-the-shelf training
algorithms cannot achieve good training outcomes. Also, recently introduced
algorithms have enjoyed limited attention, as they require bi-directionally
switching devices of unrealistically high symmetry and precision and are highly
sensitive. A new algorithm chopped TTv2 (c-TTv2), has been introduced, which
leverages the chopped technique to address many of the challenges mentioned
above. In this paper, we assess the performance of the c-TTv2 algorithm for
analog TL using a Swin-ViT model on a subset of the CIFAR100 dataset. We also
investigate the robustness of our algorithm to changes in some device
specifications, including weight transfer noise, symmetry point skew, and
symmetry point variability

</details>


### [160] [Addition is almost all you need: Compressing neural networks with double binary factorization](https://arxiv.org/abs/2505.11076)
*Vladimír Boža,Vladimír Macko*

Main category: cs.LG

TL;DR: 论文提出了一种名为双二值分解（DBF）的新方法，通过将稠密权重矩阵分解为两个二值矩阵的乘积并伴随缩放向量，在保持二值表示效率的同时，实现了优于或与现有方法相当的压缩率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的计算和存储需求不断增加，二值量化方法通过用二值矩阵替换权重矩阵并用廉价加法替代昂贵乘法，提供了一种计算高效的方法。然而，严格的量化约束（±1）可能导致显著的精度下降。

Method: 提出双二值分解（DBF），将稠密权重矩阵分解为两个二值（符号）矩阵的乘积，每个矩阵伴随缩放向量。DBF允许通过调整分解的中间维度精细控制压缩比。

Result: 在1比特/权重范围内，DBF优于现有的二值化方法；在2比特/权重范围内，DBF与QuIP#和QTIP等最佳量化方法竞争。

Conclusion: DBF不仅保持了二值表示的高效性，还提供了灵活的压缩比控制，并通过算法实现了非均匀层间压缩比的估计。

Abstract: Binary quantization approaches, which replace weight matrices with binary
matrices and substitute costly multiplications with cheaper additions, offer a
computationally efficient approach to address the increasing computational and
storage requirements of Large Language Models (LLMs). However, the severe
quantization constraint ($\pm1$) can lead to significant accuracy degradation.
In this paper, we propose Double Binary Factorization (DBF), a novel method
that factorizes dense weight matrices into products of two binary (sign)
matrices, each accompanied by scaling vectors. DBF preserves the efficiency
advantages of binary representations while achieving compression rates that are
competitive with or superior to state-of-the-art methods. Specifically, in a
1-bit per weight range, DBF is better than existing binarization approaches. In
a 2-bit per weight range, DBF is competitive with the best quantization methods
like QuIP\# and QTIP. Unlike most existing compression techniques, which offer
limited compression level choices, DBF allows fine-grained control over
compression ratios by adjusting the factorization's intermediate dimension.
Based on this advantage, we further introduce an algorithm for estimating
non-uniform layer-wise compression ratios for DBF, based on previously
developed channel pruning criteria.
  Code available at: https://github.com/usamec/double_binary

</details>


### [161] [ShiQ: Bringing back Bellman to LLMs](https://arxiv.org/abs/2505.11081)
*Pierre Clavier,Nathan Grinsztajn,Raphael Avalos,Yannis Flet-Berliac,Irem Ergun,Omar D. Domingues,Eugene Tarassov,Olivier Pietquin,Pierre H. Richemond,Florian Strub,Matthieu Geist*

Main category: cs.LG

TL;DR: 论文提出了一种基于Q学习的损失函数ShiQ，用于优化预训练大语言模型（LLM），解决了传统Q学习在LLM中直接应用无效的问题。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习（RL）直接优化策略的方法在LLM中效率不高，而Q学习因其样本效率和离线学习能力在非LLM任务中表现优异，但直接应用于LLM无效。

Method: 通过从Bellman方程推导理论基础的损失函数，将Q学习方法适配到LLM，并开发了ShiQ算法，支持离策略和逐标记学习。

Result: 在合成数据和真实基准（如UltraFeedback和BFCL-V3）上评估ShiQ，证明其在单轮和多轮LLM任务中的有效性。

Conclusion: ShiQ为LLM的强化学习提供了一种高效且实用的Q学习方法。

Abstract: The fine-tuning of pre-trained large language models (LLMs) using
reinforcement learning (RL) is generally formulated as direct policy
optimization. This approach was naturally favored as it efficiently improves a
pretrained LLM, seen as an initial policy. Another RL paradigm, Q-learning
methods, has received far less attention in the LLM community while
demonstrating major success in various non-LLM RL tasks. In particular,
Q-learning effectiveness comes from its sample efficiency and ability to learn
offline, which is particularly valuable given the high computational cost of
sampling with LLMs. However, naively applying a Q-learning-style update to the
model's logits is ineffective due to the specificity of LLMs. Our core
contribution is to derive theoretically grounded loss functions from Bellman
equations to adapt Q-learning methods to LLMs. To do so, we carefully adapt
insights from the RL literature to account for LLM-specific characteristics,
ensuring that the logits become reliable Q-value estimates. We then use this
loss to build a practical algorithm, ShiQ for Shifted-Q, that supports
off-policy, token-wise learning while remaining simple to implement. Finally,
we evaluate ShiQ on both synthetic data and real-world benchmarks, e.g.,
UltraFeedback and BFCL-V3, demonstrating its effectiveness in both single-turn
and multi-turn LLM settings

</details>


### [162] [Fault Diagnosis across Heterogeneous Domains via Self-Adaptive Temporal-Spatial Attention and Sample Generation](https://arxiv.org/abs/2505.11083)
*Guangqiang Li,M. Amine Atoui,Xiangshun Li*

Main category: cs.LG

TL;DR: 论文提出了一种名为TSA-SAN的自适应时空注意力网络，用于解决多模式过程中故障诊断中数据类别部分重叠和分布差异大的问题。


<details>
  <summary>Details</summary>
Motivation: 现实工业场景中，多模式过程的健康状态类别通常仅部分重叠，且数据不完整和分布差异大，现有故障诊断方法难以应对。

Method: 通过健康类别数据构建模式间映射生成多模式样本，利用插值丰富故障数据多样性，结合自适应实例归一化和时空注意力机制训练模型。

Result: 实验表明，TSA-SAN模型显著优于现有方法。

Conclusion: TSA-SAN通过自适应和注意力机制有效提升了多模式故障诊断的泛化能力。

Abstract: Deep learning methods have shown promising performance in fault diagnosis for
multimode process. Most existing studies assume that the collected health state
categories from different operating modes are identical. However, in real
industrial scenarios, these categories typically exhibit only partial overlap.
The incompleteness of the available data and the large distributional
differences between the operating modes pose a significant challenge to
existing fault diagnosis methods. To address this problem, a novel fault
diagnosis model named self-adaptive temporal-spatial attention network
(TSA-SAN) is proposed. First, inter-mode mappings are constructed using healthy
category data to generate multimode samples. To enrich the diversity of the
fault data, interpolation is performed between healthy and fault samples.
Subsequently, the fault diagnosis model is trained using real and generated
data. The self-adaptive instance normalization is established to suppress
irrelevant information while retaining essential statistical features for
diagnosis. In addition, a temporal-spatial attention mechanism is constructed
to focus on the key features, thus enhancing the generalization ability of the
model. The extensive experiments demonstrate that the proposed model
significantly outperforms the state-of-the-art methods. The code will be
available on Github at https://github.com/GuangqiangLi/TSA-SAN.

</details>


### [163] [A Fast Kernel-based Conditional Independence test with Application to Causal Discovery](https://arxiv.org/abs/2505.11085)
*Oliver Schacht,Biwei Huang*

Main category: cs.LG

TL;DR: FastKCI是一种基于核的条件独立性测试方法，通过并行化和分区处理解决了KCI测试计算复杂度高的问题，适用于大规模数据集。


<details>
  <summary>Details</summary>
Motivation: KCI测试在因果发现中非常有用，但其立方计算复杂度限制了其在大规模数据上的应用。FastKCI旨在解决这一计算瓶颈。

Method: FastKCI采用混合专家方法，基于条件变量的高斯混合模型分区数据集，并行进行局部KCI测试，并通过重要性加权采样汇总结果。

Result: 实验表明，FastKCI在保持KCI统计功效的同时，显著提升了计算速度。

Conclusion: FastKCI为大规模数据上的条件独立性测试提供了一种高效实用的解决方案。

Abstract: Kernel-based conditional independence (KCI) testing is a powerful
nonparametric method commonly employed in causal discovery tasks. Despite its
flexibility and statistical reliability, cubic computational complexity limits
its application to large datasets. To address this computational bottleneck, we
propose \textit{FastKCI}, a scalable and parallelizable kernel-based
conditional independence test that utilizes a mixture-of-experts approach
inspired by embarrassingly parallel inference techniques for Gaussian
processes. By partitioning the dataset based on a Gaussian mixture model over
the conditioning variables, FastKCI conducts local KCI tests in parallel,
aggregating the results using an importance-weighted sampling scheme.
Experiments on synthetic datasets and benchmarks on real-world production data
validate that FastKCI maintains the statistical power of the original KCI test
while achieving substantial computational speedups. FastKCI thus represents a
practical and efficient solution for conditional independence testing in causal
inference on large-scale data.

</details>


### [164] [Bidirectional Distillation: A Mixed-Play Framework for Multi-Agent Generalizable Behaviors](https://arxiv.org/abs/2505.11100)
*Lang Feng,Jiahao Lin,Dong Xing,Li Zhang,De Ma,Gang Pan*

Main category: cs.LG

TL;DR: BiDist是一种新型混合游戏框架，通过双向知识蒸馏解决MARL中种群泛化问题，无需复杂存储历史策略。


<details>
  <summary>Details</summary>
Motivation: 解决多智能体强化学习中种群泛化问题，特别是面对未见合作者时的局限性。

Method: 提出双向蒸馏（BiDist），包括正向蒸馏（模拟历史策略空间）和反向蒸馏（驱动策略向新分布发展）。

Result: BiDist在合作、竞争和社会困境任务中表现出卓越的泛化能力，并显著多样化策略分布空间。

Conclusion: BiDist是一种简洁高效的解决方案，理论和实验均验证其有效性。

Abstract: Population-population generalization is a challenging problem in multi-agent
reinforcement learning (MARL), particularly when agents encounter unseen
co-players. However, existing self-play-based methods are constrained by the
limitation of inside-space generalization. In this study, we propose
Bidirectional Distillation (BiDist), a novel mixed-play framework, to overcome
this limitation in MARL. BiDist leverages knowledge distillation in two
alternating directions: forward distillation, which emulates the historical
policies' space and creates an implicit self-play, and reverse distillation,
which systematically drives agents towards novel distributions outside the
known policy space in a non-self-play manner. In addition, BiDist operates as a
concise and efficient solution without the need for the complex and costly
storage of past policies. We provide both theoretical analysis and empirical
evidence to support BiDist's effectiveness. Our results highlight its
remarkable generalization ability across a variety of cooperative, competitive,
and social dilemma tasks, and reveal that BiDist significantly diversifies the
policy distribution space. We also present comprehensive ablation studies to
reinforce BiDist's effectiveness and key success factors. Source codes are
available in the supplementary material.

</details>


### [165] [Inferring the Most Similar Variable-length Subsequences between Multidimensional Time Series](https://arxiv.org/abs/2505.11106)
*Thanadej Rattanakornphan,Piyanon Charoenpoonpanich,Chainarong Amornbunchornvej*

Main category: cs.LG

TL;DR: 提出了一种高效算法，用于在多维时间序列中找到长度不同的最相似子序列，解决了现有方法的不足。


<details>
  <summary>Details</summary>
Motivation: 在多维时间序列中寻找相似子序列的应用广泛（如股市依赖性或狒狒协同运动），但现有方法无法高效处理长度差异问题。

Method: 基于理论保证的算法，能够精确找到长度不同的多维时间序列中最相似的子序列。

Result: 在模拟数据中，算法运行时间仅为基线方法的四分之一；在真实数据中，速度提升高达20倍，并成功应用于股市和狒狒运动分析。

Conclusion: 该算法高效且通用，适用于任何时间序列，代码和数据集已公开。

Abstract: Finding the most similar subsequences between two multidimensional time
series has many applications: e.g. capturing dependency in stock market or
discovering coordinated movement of baboons. Considering one pattern occurring
in one time series, we might be wondering whether the same pattern occurs in
another time series with some distortion that might have a different length.
Nevertheless, to the best of our knowledge, there is no efficient framework
that deals with this problem yet. In this work, we propose an algorithm that
provides the exact solution of finding the most similar multidimensional
subsequences between time series where there is a difference in length both
between time series and between subsequences. The algorithm is built based on
theoretical guarantee of correctness and efficiency. The result in simulation
datasets illustrated that our approach not just only provided correct solution,
but it also utilized running time only quarter of time compared against the
baseline approaches. In real-world datasets, it extracted the most similar
subsequences even faster (up to 20 times faster against baseline methods) and
provided insights regarding the situation in stock market and following
relations of multidimensional time series of baboon movement. Our approach can
be used for any time series. The code and datasets of this work are provided
for the public use.

</details>


### [166] [Maximizing Asynchronicity in Event-based Neural Networks](https://arxiv.org/abs/2505.11165)
*Haiqing Hao,Nikola Zubić,Weihua He,Zhipeng Sui,Davide Scaramuzza,Wenhui Wang*

Main category: cs.LG

TL;DR: EVA是一种新型异步到同步（A2S）框架，通过借鉴语言建模技术，生成高表达性和泛化性的事件表示，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 事件相机的高时间分辨率和低延迟特性对标准机器学习方法提出了挑战，现有A2S方法在表达性和泛化性上不足。

Method: EVA借鉴语言建模中的线性注意力和自监督学习技术，构建事件表示。

Result: EVA在识别任务（DVS128-Gesture和N-Cars）上优于现有方法，并在检测任务（Gen1数据集）上达到47.7 mAP。

Conclusion: EVA展示了在实时事件视觉应用中的变革潜力。

Abstract: Event cameras deliver visual data with high temporal resolution, low latency,
and minimal redundancy, yet their asynchronous, sparse sequential nature
challenges standard tensor-based machine learning (ML). While the recent
asynchronous-to-synchronous (A2S) paradigm aims to bridge this gap by
asynchronously encoding events into learned representations for ML pipelines,
existing A2S approaches often sacrifice representation expressivity and
generalizability compared to dense, synchronous methods. This paper introduces
EVA (EVent Asynchronous representation learning), a novel A2S framework to
generate highly expressive and generalizable event-by-event representations.
Inspired by the analogy between events and language, EVA uniquely adapts
advances from language modeling in linear attention and self-supervised
learning for its construction. In demonstration, EVA outperforms prior A2S
methods on recognition tasks (DVS128-Gesture and N-Cars), and represents the
first A2S framework to successfully master demanding detection tasks, achieving
a remarkable 47.7 mAP on the Gen1 dataset. These results underscore EVA's
transformative potential for advancing real-time event-based vision
applications.

</details>


### [167] [FairSHAP: Preprocessing for Fairness Through Attribution-Based Data Augmentation](https://arxiv.org/abs/2505.11111)
*Lin Zhu,Yijun Bian,Lei You*

Main category: cs.LG

TL;DR: FairSHAP是一种基于Shapley值的预处理框架，通过透明地识别和修改不公平特征，提升个体和群体公平性，同时保持数据完整性和模型准确性。


<details>
  <summary>Details</summary>
Motivation: 在机器学习中，现有预处理方法缺乏透明机制来识别不公平特征或实例，导致数据修改依据不明确。FairSHAP旨在解决这一问题。

Method: FairSHAP利用Shapley值识别训练数据中影响公平性的关键实例，并通过实例级匹配修改这些实例，以减少歧视风险。

Result: 实验表明，FairSHAP显著提升了人口统计平等和机会平等，同时最小化数据扰动，某些情况下还提高了预测性能。

Conclusion: FairSHAP是一种模型无关且透明的方法，可无缝集成到现有机器学习流程中，并提供对偏见的可操作见解。

Abstract: Ensuring fairness in machine learning models is critical, particularly in
high-stakes domains where biased decisions can lead to serious societal
consequences. Existing preprocessing approaches generally lack transparent
mechanisms for identifying which features or instances are responsible for
unfairness. This obscures the rationale behind data modifications. We introduce
FairSHAP, a novel pre-processing framework that leverages Shapley value
attribution to improve both individual and group fairness. FairSHAP identifies
fairness-critical instances in the training data using an interpretable measure
of feature importance, and systematically modifies them through instance-level
matching across sensitive groups. This process reduces discriminative risk - an
individual fairness metric - while preserving data integrity and model
accuracy. We demonstrate that FairSHAP significantly improves demographic
parity and equality of opportunity across diverse tabular datasets, achieving
fairness gains with minimal data perturbation and, in some cases, improved
predictive performance. As a model-agnostic and transparent method, FairSHAP
integrates seamlessly into existing machine learning pipelines and provides
actionable insights into the sources of bias.Our code is on
https://github.com/youlei202/FairSHAP.

</details>


### [168] [Dual-Balancing for Physics-Informed Neural Networks](https://arxiv.org/abs/2505.11117)
*Chenhong Zhou,Jie Chen,Zaifeng Yang,Ching Eng Png*

Main category: cs.LG

TL;DR: 提出了一种新型的双平衡物理信息神经网络（DB-PINN），通过动态调整损失权重解决传统PINNs在多目标优化中的不平衡问题，显著提升了收敛速度和预测精度。


<details>
  <summary>Details</summary>
Motivation: 传统PINNs在多目标优化中存在梯度不平衡和条件拟合难度不平衡的问题，导致精度低、收敛慢。

Method: DB-PINN通过动态调整损失权重，结合了梯度分布差异的‘间平衡’和条件拟合难度差异的‘内平衡’，并引入稳健的权重更新策略。

Result: 实验表明，DB-PINN在收敛速度和预测精度上显著优于基于梯度的权重调整方法。

Conclusion: DB-PINN有效解决了PINNs中的不平衡问题，为复杂PDE求解提供了更高效的工具。

Abstract: Physics-informed neural networks (PINNs) have emerged as a new learning
paradigm for solving partial differential equations (PDEs) by enforcing the
constraints of physical equations, boundary conditions (BCs), and initial
conditions (ICs) into the loss function. Despite their successes, vanilla PINNs
still suffer from poor accuracy and slow convergence due to the intractable
multi-objective optimization issue. In this paper, we propose a novel
Dual-Balanced PINN (DB-PINN), which dynamically adjusts loss weights by
integrating inter-balancing and intra-balancing to alleviate two imbalance
issues in PINNs. Inter-balancing aims to mitigate the gradient imbalance
between PDE residual loss and condition-fitting losses by determining an
aggregated weight that offsets their gradient distribution discrepancies.
Intra-balancing acts on condition-fitting losses to tackle the imbalance in
fitting difficulty across diverse conditions. By evaluating the fitting
difficulty based on the loss records, intra-balancing can allocate the
aggregated weight proportionally to each condition loss according to its
fitting difficulty levels. We further introduce a robust weight update strategy
to prevent abrupt spikes and arithmetic overflow in instantaneous weight values
caused by large loss variances, enabling smooth weight updating and stable
training. Extensive experiments demonstrate that DB-PINN achieves significantly
superior performance than those popular gradient-based weighting methods in
terms of convergence speed and prediction accuracy. Our code and supplementary
material are available at https://github.com/chenhong-zhou/DualBalanced-PINNs.

</details>


### [169] [GraphOracle: A Foundation Model for Knowledge Graph Reasoning](https://arxiv.org/abs/2505.11125)
*Enjun Du,Siyi Liu,Yongqi Zhang*

Main category: cs.LG

TL;DR: GraphOracle是一种基于关系依赖图（RDG）的知识图谱基础模型，通过查询依赖的注意力机制和预训练，实现了对未见实体、关系和图谱的泛化能力，性能提升高达35%。


<details>
  <summary>Details</summary>
Motivation: 知识图谱的动态性和跨领域推理需求使得开发类似基础模型具有挑战性。

Method: 将知识图谱转换为关系依赖图（RDG），开发查询依赖的注意力机制，预训练后快速微调。

Result: 在31个基准测试中表现最优，性能提升高达35%。

Conclusion: GraphOracle通过关系中心化的方法，显著提升了知识图谱推理的泛化能力和性能。

Abstract: Foundation models have demonstrated remarkable capabilities across various
domains, but developing analogous models for knowledge graphs presents unique
challenges due to their dynamic nature and the need for cross-domain reasoning.
To address these issues, we introduce \textbf{\textsc{GraphOracle}}, a
relation-centric foundation model that unifies reasoning across knowledge
graphs by converting them into Relation-Dependency Graphs (RDG), explicitly
encoding compositional patterns with fewer edges than prior methods. A
query-dependent attention mechanism is further developed to learn inductive
representations for both relations and entities. Pre-training on diverse
knowledge graphs, followed by minutes-level fine-tuning, enables effective
generalization to unseen entities, relations, and entire graphs. Through
comprehensive experiments on 31 diverse benchmarks spanning transductive,
inductive, and cross-domain settings, we demonstrate consistent
state-of-the-art performance with minimal adaptation, improving the prediction
performance by up to 35\% compared to the strongest baselines.

</details>


### [170] [FedDuA: Doubly Adaptive Federated Learning](https://arxiv.org/abs/2505.11126)
*Shokichi Takakura,Seng Pei Liew,Satoshi Hasegawa*

Main category: cs.LG

TL;DR: FedDuA是一种新的联邦学习框架，通过自适应调整全局学习率来解决FedAvg算法在异构数据集和参数空间各向异性下的收敛慢问题。


<details>
  <summary>Details</summary>
Motivation: FedAvg算法在异构数据集和参数空间各向异性下收敛慢，需要改进。

Method: 通过镜像下降视角形式化中心服务器优化过程，提出FedDuA框架，自适应选择全局学习率。

Result: 理论证明FedDuA的双自适应步长规则是最小极大最优的，实验表明其优于基线方法且对超参数选择鲁棒。

Conclusion: FedDuA在不增加通信或计算成本的情况下，显著提升了联邦学习的性能。

Abstract: Federated learning is a distributed learning framework where clients
collaboratively train a global model without sharing their raw data. FedAvg is
a popular algorithm for federated learning, but it often suffers from slow
convergence due to the heterogeneity of local datasets and anisotropy in the
parameter space. In this work, we formalize the central server optimization
procedure through the lens of mirror descent and propose a novel framework,
called FedDuA, which adaptively selects the global learning rate based on both
inter-client and coordinate-wise heterogeneity in the local updates. We prove
that our proposed doubly adaptive step-size rule is minimax optimal and provide
a convergence analysis for convex objectives. Although the proposed method does
not require additional communication or computational cost on clients,
extensive numerical experiments show that our proposed framework outperforms
baselines in various settings and is robust to the choice of hyperparameters.

</details>


### [171] [What's Inside Your Diffusion Model? A Score-Based Riemannian Metric to Explore the Data Manifold](https://arxiv.org/abs/2505.11128)
*Simone Azeglio,Arianna Di Bernardo*

Main category: cs.LG

TL;DR: 论文提出了一种基于分数的黎曼度量方法，利用扩散模型的Stein分数函数刻画数据流形的内在几何特性，无需显式参数化。通过实验验证了该方法在图像插值和外推任务中的优越性。


<details>
  <summary>Details</summary>
Motivation: 尽管扩散模型在捕捉复杂图像分布方面表现出色，但其学习的数据流形的几何特性尚未被充分理解。本文旨在填补这一空白。

Method: 引入基于分数的黎曼度量，利用扩散模型的Stein分数函数定义度量张量，使测地线自然地遵循流形轮廓，并开发了高效的计算算法。

Result: 在合成数据、Rotated MNIST和Stable Diffusion生成的复杂自然图像上，该方法在感知指标（LPIPS）和分布级指标（FID、KID）上均优于基线方法。

Conclusion: 该方法揭示了扩散模型学习的隐式几何结构，为通过黎曼几何导航自然图像流形提供了理论支持。

Abstract: Recent advances in diffusion models have demonstrated their remarkable
ability to capture complex image distributions, but the geometric properties of
the learned data manifold remain poorly understood. We address this gap by
introducing a score-based Riemannian metric that leverages the Stein score
function from diffusion models to characterize the intrinsic geometry of the
data manifold without requiring explicit parameterization. Our approach defines
a metric tensor in the ambient space that stretches distances perpendicular to
the manifold while preserving them along tangential directions, effectively
creating a geometry where geodesics naturally follow the manifold's contours.
We develop efficient algorithms for computing these geodesics and demonstrate
their utility for both interpolation between data points and extrapolation
beyond the observed data distribution. Through experiments on synthetic data
with known geometry, Rotated MNIST, and complex natural images via Stable
Diffusion, we show that our score-based geodesics capture meaningful
transformations that respect the underlying data distribution. Our method
consistently outperforms baseline approaches on perceptual metrics (LPIPS) and
distribution-level metrics (FID, KID), producing smoother, more realistic image
transitions. These results reveal the implicit geometric structure learned by
diffusion models and provide a principled way to navigate the manifold of
natural images through the lens of Riemannian geometry.

</details>


### [172] [Fairness-aware Anomaly Detection via Fair Projection](https://arxiv.org/abs/2505.11132)
*Feng Xiao,Xiaoying Tang,Jicong Fan*

Main category: cs.LG

TL;DR: 论文提出了一种公平性异常检测方法FairAD，通过将不同人口统计组的数据映射到共同的目标分布，确保公平性，并提出了无需阈值的公平性度量。


<details>
  <summary>Details</summary>
Motivation: 在金融、医疗等高社会影响领域，无监督异常检测可能因数据偏差导致不公平，因此需要确保公平性。

Method: FairAD通过学习投影将不同组数据映射到共同的目标分布，并基于密度估计识别异常。

Result: 实验表明，FairAD在准确性和公平性之间取得了更好的平衡。

Conclusion: FairAD为无监督异常检测提供了一种公平性解决方案，适用于平衡和倾斜数据。

Abstract: Unsupervised anomaly detection is a critical task in many high-social-impact
applications such as finance, healthcare, social media, and cybersecurity,
where demographics involving age, gender, race, disease, etc, are used
frequently. In these scenarios, possible bias from anomaly detection systems
can lead to unfair treatment for different groups and even exacerbate social
bias. In this work, first, we thoroughly analyze the feasibility and necessary
assumptions for ensuring group fairness in unsupervised anomaly detection.
Second, we propose a novel fairness-aware anomaly detection method FairAD. From
the normal training data, FairAD learns a projection to map data of different
demographic groups to a common target distribution that is simple and compact,
and hence provides a reliable base to estimate the density of the data. The
density can be directly used to identify anomalies while the common target
distribution ensures fairness between different groups. Furthermore, we propose
a threshold-free fairness metric that provides a global view for model's
fairness, eliminating dependence on manual threshold selection. Experiments on
real-world benchmarks demonstrate that our method achieves an improved
trade-off between detection accuracy and fairness under both balanced and
skewed data across different groups.

</details>


### [173] [Towards Robust Spiking Neural Networks:Mitigating Heterogeneous Training Vulnerability via Dominant Eigencomponent Projection](https://arxiv.org/abs/2505.11134)
*Desong Zhang,Jia Hu,Geyong Min*

Main category: cs.LG

TL;DR: SNNs trained with direct encoding and BPTT are vulnerable to catastrophic collapse from slight data distribution shifts. DEP, a hyperparameter-free method, mitigates this by reducing Hessian spectral radius, enhancing robustness.


<details>
  <summary>Details</summary>
Motivation: SNNs' energy efficiency is compromised by vulnerability to data shifts when trained with direct encoding and BPTT.

Method: Developed Dominant Eigencomponent Projection (DEP) to orthogonally project gradients and reduce Hessian spectral radius.

Result: DEP prevents sharp minima, mitigates vulnerability to heterogeneous data poisoning, and enhances SNN robustness.

Conclusion: DEP supports safer and more reliable SNN deployment by addressing training vulnerabilities.

Abstract: Spiking Neural Networks (SNNs) process information via discrete spikes,
enabling them to operate at remarkably low energy levels. However, our
experimental observations reveal a striking vulnerability when SNNs are trained
using the mainstream method--direct encoding combined with backpropagation
through time (BPTT): even a single backward pass on data drawn from a slightly
different distribution can lead to catastrophic network collapse. Our
theoretical analysis attributes this vulnerability to the repeated inputs
inherent in direct encoding and the gradient accumulation characteristic of
BPTT, which together produce an exceptional large Hessian spectral radius. To
address this challenge, we develop a hyperparameter-free method called Dominant
Eigencomponent Projection (DEP). By orthogonally projecting gradients to
precisely remove their dominant components, DEP effectively reduces the Hessian
spectral radius, thereby preventing SNNs from settling into sharp minima.
Extensive experiments demonstrate that DEP not only mitigates the vulnerability
of SNNs to heterogeneous data poisoning, but also significantly enhances
overall robustness compared to key baselines, providing strong support for
safer and more reliable SNN deployment.

</details>


### [174] [Covariance Density Neural Networks](https://arxiv.org/abs/2505.11139)
*Om Roy,Yashar Moshfeghi,Keith Smith*

Main category: cs.LG

TL;DR: 论文提出了一种改进的图神经网络方法，通过构建密度矩阵作为图移位算子，提升了性能并增强了鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前图神经网络在建模网络数据时缺乏对底层图结构选择的共识，需要一种更优的方法。

Method: 使用样本协方差矩阵作为准哈密顿量构建密度矩阵，并将其作为图移位算子，实现数据多尺度提取。

Result: 该方法在稳定性和可区分性之间实现可控权衡，噪声鲁棒性优于VNNs，并在脑机接口EEG分类任务中表现优异。

Conclusion: 密度矩阵方法为脑机接口的迁移学习提供了有效基础。

Abstract: Graph neural networks have re-defined how we model and predict on network
data but there lacks a consensus on choosing the correct underlying graph
structure on which to model signals. CoVariance Neural Networks (VNN) address
this issue by using the sample covariance matrix as a Graph Shift Operator
(GSO). Here, we improve on the performance of VNNs by constructing a Density
Matrix where we consider the sample Covariance matrix as a quasi-Hamiltonian of
the system in the space of random variables. Crucially, using this density
matrix as the GSO allows components of the data to be extracted at different
scales, allowing enhanced discriminability and performance. We show that this
approach allows explicit control of the stability-discriminability trade-off of
the network, provides enhanced robustness to noise compared to VNNs, and
outperforms them in useful real-life applications where the underlying
covariance matrix is informative. In particular, we show that our model can
achieve strong performance in subject-independent Brain Computer Interface EEG
motor imagery classification, outperforming EEGnet while being faster. This
shows how covariance density neural networks provide a basis for the
notoriously difficult task of transferability of BCIs when evaluated on unseen
individuals.

</details>


### [175] [Bi-directional Recurrence Improves Transformer in Partially Observable Markov Decision Processes](https://arxiv.org/abs/2505.11153)
*Ashok Arora,Neetesh Kumar*

Main category: cs.LG

TL;DR: 提出了一种新型双向循环模型架构，用于提升部分可观测马尔可夫决策过程（POMDP）中的样本效率和减少参数数量。


<details>
  <summary>Details</summary>
Motivation: 在真实世界的强化学习中，部分可观测性是一个常见问题，现有方法如循环网络和基于Transformer的模型存在参数过多或效率不足的问题。

Method: 采用双向循环单元替代多层前馈网络，以更好地捕捉序列依赖性和上下文信息。

Result: 在23个POMDP环境中，新模型平均性能优于现有方法87.39%至482.04%。

Conclusion: 该架构显著提升了样本效率和处理部分可观测性的能力，适合实际应用。

Abstract: In real-world reinforcement learning (RL) scenarios, agents often encounter
partial observability, where incomplete or noisy information obscures the true
state of the environment. Partially Observable Markov Decision Processes
(POMDPs) are commonly used to model these environments, but effective
performance requires memory mechanisms to utilise past observations. While
recurrence networks have traditionally addressed this need, transformer-based
models have recently shown improved sample efficiency in RL tasks. However,
their application to POMDPs remains underdeveloped, and their real-world
deployment is constrained due to the high parameter count. This work introduces
a novel bi-recurrent model architecture that improves sample efficiency and
reduces model parameter count in POMDP scenarios. The architecture replaces the
multiple feed forward layers with a single layer of bi-directional recurrence
unit to better capture and utilize sequential dependencies and contextual
information. This approach improves the model's ability to handle partial
observability and increases sample efficiency, enabling effective learning from
comparatively fewer interactions. To evaluate the performance of the proposed
model architecture, experiments were conducted on a total of 23 POMDP
environments. The proposed model architecture outperforms existing
transformer-based, attention-based, and recurrence-based methods by a margin
ranging from 87.39% to 482.04% on average across the 23 POMDP environments.

</details>


### [176] [Visual Planning: Let's Think Only with Images](https://arxiv.org/abs/2505.11409)
*Yi Xu,Chengzu Li,Han Zhou,Xingchen Wan,Caiqi Zhang,Anna Korhonen,Ivan Vulić*

Main category: cs.LG

TL;DR: 论文提出了一种新的视觉规划范式（Visual Planning），通过纯视觉表示进行推理，优于传统的基于文本的推理方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）及其多模态扩展（MLLMs）主要依赖文本进行推理，但在涉及空间和几何信息的任务中，语言可能并非最自然或有效的模态。

Method: 提出视觉规划范式，通过图像序列进行推理，并引入基于强化学习的框架VPRL，结合GRPO对大型视觉模型进行后训练。

Result: 在视觉导航任务（FrozenLake、Maze、MiniBehavior）中，视觉规划优于纯文本推理方法。

Conclusion: 视觉规划是一种可行且有前景的替代方案，为基于图像的推理任务开辟了新途径。

Abstract: Recent advancements in Large Language Models (LLMs) and their multimodal
extensions (MLLMs) have substantially enhanced machine reasoning across diverse
tasks. However, these models predominantly rely on pure text as the medium for
both expressing and structuring reasoning, even when visual information is
present. In this work, we argue that language may not always be the most
natural or effective modality for reasoning, particularly in tasks involving
spatial and geometrical information. Motivated by this, we propose a new
paradigm, Visual Planning, which enables planning through purely visual
representations, independent of text. In this paradigm, planning is executed
via sequences of images that encode step-by-step inference in the visual
domain, akin to how humans sketch or visualize future actions. We introduce a
novel reinforcement learning framework, Visual Planning via Reinforcement
Learning (VPRL), empowered by GRPO for post-training large vision models,
leading to substantial improvements in planning in a selection of
representative visual navigation tasks, FrozenLake, Maze, and MiniBehavior. Our
visual planning paradigm outperforms all other planning variants that conduct
reasoning in the text-only space. Our results establish Visual Planning as a
viable and promising alternative to language-based reasoning, opening new
avenues for tasks that benefit from intuitive, image-based inference.

</details>


### [177] [Attention on the Sphere](https://arxiv.org/abs/2505.11157)
*Boris Bonev,Max Rietmann,Andrea Paris,Alberto Carpentieri,Thorsten Kurth*

Main category: cs.LG

TL;DR: 提出了一种适用于球面域的广义注意力机制，使Transformer能直接处理球面数据，结合数值积分权重和邻域注意力，提升性能并降低计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 在气象学、宇宙学和机器人学等领域，保持球面对称性和拓扑结构对物理准确性至关重要，需开发适合球面数据的Transformer架构。

Method: 通过将数值积分权重融入注意力机制，实现几何保真的球面注意力；提出球面邻域注意力，限制交互范围以降低计算复杂度。

Result: 在模拟旋转球面上的浅水方程、球面图像分割和球面深度估计等任务中，球面Transformer表现优于平面模型。

Conclusion: 球面Transformer通过几何先验显著提升了球面域学习性能，验证了其在实际任务中的优越性。

Abstract: We introduce a generalized attention mechanism for spherical domains,
enabling Transformer architectures to natively process data defined on the
two-dimensional sphere - a critical need in fields such as atmospheric physics,
cosmology, and robotics, where preserving spherical symmetries and topology is
essential for physical accuracy. By integrating numerical quadrature weights
into the attention mechanism, we obtain a geometrically faithful spherical
attention that is approximately rotationally equivariant, providing strong
inductive biases and leading to better performance than Cartesian approaches.
To further enhance both scalability and model performance, we propose
neighborhood attention on the sphere, which confines interactions to geodesic
neighborhoods. This approach reduces computational complexity and introduces
the additional inductive bias for locality, while retaining the symmetry
properties of our method. We provide optimized CUDA kernels and
memory-efficient implementations to ensure practical applicability. The method
is validated on three diverse tasks: simulating shallow water equations on the
rotating sphere, spherical image segmentation, and spherical depth estimation.
Across all tasks, our spherical Transformers consistently outperform their
planar counterparts, highlighting the advantage of geometric priors for
learning on spherical domains.

</details>


### [178] [Gaussian Weight Sampling for Scalable, Efficient and Stable Pseudo-Quantization Training](https://arxiv.org/abs/2505.11170)
*Myeonghwan Ahn,Sungjoo Yoo*

Main category: cs.LG

TL;DR: 论文提出了一种伪量化训练（PQT）方法，通过引入浮点友好的噪声分布R，解决了全量化训练（FQT）的稳定性问题，并在低精度浮点参数上取得了高效且稳定的性能。


<details>
  <summary>Details</summary>
Motivation: 全量化训练（FQT）虽然能加速训练，但面临一致性挑战和计算复杂度高的问题。伪量化训练（PQT）尚未被充分研究，因此需要探索其实际应用和理论基础。

Method: 提出了一种浮点友好的噪声分布R，具有随机精度退火等理想特性，并通过高效的伪量化操作（加法后浮点转换）实现低精度浮点参数的理论基础。

Result: 实验表明，高斯权重采样具有可扩展性（支持低至FP6的参数和高至9位的噪声）、高效性（A100 GPU上的计算开销仅为1.40%）和稳定性（性能接近或超越BF16基线）。

Conclusion: PQT结合高斯权重采样为低精度浮点参数提供了一种高效且稳定的训练方法，适用于大规模语言模型训练。

Abstract: Ever-growing scale of large language models (LLMs) is pushing for improved
efficiency, favoring fully quantized training (FQT) over BF16. While FQT
accelerates training, it faces consistency challenges and requires searching
over an exponential number of cases, each needing over 200B tokens to ensure
stability.
  Pseudo-quantization training (PQT) addresses the issues of FQT, although it
is not well-studied. We explore the practical implications of PQT in detail and
propose a noise distribution $R$ that is floating-point (FP)-friendly, with
ideal properties including stochastic precision annealing. As a result, the
proposed method serves as an effective theoretical foundation for low-precision
FP parameters through PQT, utilizing efficient fake quantization via an
addition and subsequent FP casting.
  We demonstrate that Gaussian weight sampling is (1) scalable: supports
low-precision FP parameters down to FP6 and high-precision noise up to 9-bit
with BF16 operator. The proposed method is (2) efficient: incurring
computational overhead as low as 1.40\% on the A100 GPU in terms of Llama2
training tokens per second, and requiring 2 bytes per parameter in GPU memory.
We demonstrate that PQT with Gaussian weight sampling is (3) stable: closely
following or even surpassing performance of the BF16 baseline while
pre-training GPT2 and Llama2 models with up to 1B parameters and 300B tokens.

</details>


### [179] [VitaGraph: Building a Knowledge Graph for Biologically Relevant Learning Tasks](https://arxiv.org/abs/2505.11185)
*Francesco Madeddu,Lucia Testa,Gianluca De Carlo,Michele Pieroni,Andrea Mastropietro,Aris Anagnostopoulos,Paolo Tieri,Sergio Barbarossa*

Main category: cs.LG

TL;DR: 本文提出了一种综合多用途生物知识图谱，通过整合和优化公开数据集，提升机器学习模型在计算生物学和精准医学中的性能。


<details>
  <summary>Details</summary>
Motivation: 人类生物学的复杂性对科学研究提出了挑战，需要跨学科合作和高质量数据支持。AI方法在计算生物学中显示出潜力，但依赖高质量的基础数据。

Method: 通过清理和整合Drug Repurposing Knowledge Graph (DRKG)等公开数据集，构建了一个多用途生物知识图谱，并丰富了节点的特征向量（如分子指纹和基因本体）。

Result: 生成的生物知识图谱为计算生物学和精准医学研究提供了先进平台，并在药物重定位、PPI预测和副作用预测等任务中表现出色。

Conclusion: 该研究提供了一个可靠且高效的生物知识图谱，为网络医学和机器学习模型的基准测试提供了重要资源。

Abstract: The intrinsic complexity of human biology presents ongoing challenges to
scientific understanding. Researchers collaborate across disciplines to expand
our knowledge of the biological interactions that define human life. AI
methodologies have emerged as powerful tools across scientific domains,
particularly in computational biology, where graph data structures effectively
model biological entities such as protein-protein interaction (PPI) networks
and gene functional networks. Those networks are used as datasets for paramount
network medicine tasks, such as gene-disease association prediction, drug
repurposing, and polypharmacy side effect studies. Reliable predictions from
machine learning models require high-quality foundational data. In this work,
we present a comprehensive multi-purpose biological knowledge graph constructed
by integrating and refining multiple publicly available datasets. Building upon
the Drug Repurposing Knowledge Graph (DRKG), we define a pipeline tasked with
a) cleaning inconsistencies and redundancies present in DRKG, b) coalescing
information from the main available public data sources, and c) enriching the
graph nodes with expressive feature vectors such as molecular fingerprints and
gene ontologies. Biologically and chemically relevant features improve the
capacity of machine learning models to generate accurate and well-structured
embedding spaces. The resulting resource represents a coherent and reliable
biological knowledge graph that serves as a state-of-the-art platform to
advance research in computational biology and precision medicine. Moreover, it
offers the opportunity to benchmark graph-based machine learning and network
medicine models on relevant tasks. We demonstrate the effectiveness of the
proposed dataset by benchmarking it against the task of drug repurposing, PPI
prediction, and side-effect prediction, modeled as link prediction problems.

</details>


### [180] [Modeling Cell Dynamics and Interactions with Unbalanced Mean Field Schrödinger Bridge](https://arxiv.org/abs/2505.11197)
*Zhenyi Zhang,Zihan Wang,Yuhao Sun,Tiejun Li,Peijie Zhou*

Main category: cs.LG

TL;DR: 论文提出了一种名为UMFSB的框架和CytoBridge算法，用于从稀疏时间分辨快照数据中建模细胞间相互作用的不平衡随机动态。


<details>
  <summary>Details</summary>
Motivation: 现有方法在建模细胞间相互作用方面存在局限，而细胞间通讯是生命过程的基础，影响细胞状态转换动态。

Method: 提出UMFSB框架和基于深度学习的CytoBridge算法，通过神经网络建模细胞转换、增殖和相互作用。

Result: 在合成基因调控数据和真实scRNA-seq数据集上验证了方法的有效性，优于现有方法。

Conclusion: CytoBridge能更准确地识别生长、转换和相互作用模式，消除虚假转换，重建发育景观。

Abstract: Modeling the dynamics from sparsely time-resolved snapshot data is crucial
for understanding complex cellular processes and behavior. Existing methods
leverage optimal transport, Schr\"odinger bridge theory, or their variants to
simultaneously infer stochastic, unbalanced dynamics from snapshot data.
However, these approaches remain limited in their ability to account for
cell-cell interactions. This integration is essential in real-world scenarios
since intercellular communications are fundamental life processes and can
influence cell state-transition dynamics. To address this challenge, we
formulate the Unbalanced Mean-Field Schr\"odinger Bridge (UMFSB) framework to
model unbalanced stochastic interaction dynamics from snapshot data. Inspired
by this framework, we further propose CytoBridge, a deep learning algorithm
designed to approximate the UMFSB problem. By explicitly modeling cellular
transitions, proliferation, and interactions through neural networks,
CytoBridge offers the flexibility to learn these processes directly from data.
The effectiveness of our method has been extensively validated using both
synthetic gene regulatory data and real scRNA-seq datasets. Compared to
existing methods, CytoBridge identifies growth, transition, and interaction
patterns, eliminates false transitions, and reconstructs the developmental
landscape with greater accuracy.

</details>


### [181] [RanDeS: Randomized Delta Superposition for Multi-Model Compression](https://arxiv.org/abs/2505.11204)
*Hangyu Zhou,Aaron Gokaslan,Volodymyr Kuleshov,Bharath Hariharan*

Main category: cs.LG

TL;DR: 论文提出了一种基于随机正交变换的模型合并方法，通过解耦任务特定的参数调整，显著减少了干扰，提升了性能，且无需额外内存。


<details>
  <summary>Details</summary>
Motivation: 多模型压缩中，模型合并会因任务特定参数调整的干扰导致性能下降。

Method: 将模型合并重新定义为压缩-检索方案，使用随机正交变换解耦参数向量以减少干扰。

Result: 该方法显著减少了干扰，提升了视觉和语言任务的性能，且支持灵活的多模型服务。

Conclusion: 随机正交变换是一种高效、灵活且无需额外内存的模型合并方法。

Abstract: From a multi-model compression perspective, model merging enables
memory-efficient serving of multiple models fine-tuned from the same base, but
suffers from degraded performance due to interference among their task-specific
parameter adjustments (i.e., deltas). In this paper, we reformulate model
merging as a compress-and-retrieve scheme, revealing that the task interference
arises from the summation of irrelevant deltas during model retrieval. To
address this issue, we use random orthogonal transformations to decorrelate
these vectors into self-cancellation. We show that this approach drastically
reduces interference, improving performance across both vision and language
tasks. Since these transformations are fully defined by random seeds, adding
new models requires no extra memory. Further, their data- and model-agnostic
nature enables easy addition or removal of models with minimal compute
overhead, supporting efficient and flexible multi-model serving.

</details>


### [182] [Minimizing False-Positive Attributions in Explanations of Non-Linear Models](https://arxiv.org/abs/2505.11210)
*Anders Gjølbye,Stefan Haufe,Lars Kai Hansen*

Main category: cs.LG

TL;DR: PatternLocal是一种新的XAI技术，通过将判别模型权重转换为生成表示，有效抑制抑制变量对模型预测的影响，提高解释的可靠性。


<details>
  <summary>Details</summary>
Motivation: 抑制变量可能在不依赖目标结果的情况下影响模型预测，导致XAI方法产生假阳性特征归因，影响解释的实用性。现有方法主要针对线性模型，对非线性模型和实例解释的扩展有限。

Method: PatternLocal基于局部线性替代模型（如LIME、KernelSHAP或梯度方法），将判别模型权重转换为生成表示，抑制抑制变量的影响，同时保持局部保真度。

Result: 在XAI-TRIS基准测试中，PatternLocal在超参数优化中表现优于其他XAI方法，显著减少了非线性任务中的假阳性归因。

Conclusion: PatternLocal通过抑制抑制变量，提供了更可靠和可操作的模型解释，填补了非线性模型和实例解释领域的空白。

Abstract: Suppressor variables can influence model predictions without being dependent
on the target outcome and they pose a significant challenge for Explainable AI
(XAI) methods. These variables may cause false-positive feature attributions,
undermining the utility of explanations. Although effective remedies exist for
linear models, their extension to non-linear models and to instance-based
explanations has remained limited. We introduce PatternLocal, a novel XAI
technique that addresses this gap. PatternLocal begins with a locally linear
surrogate, e.g. LIME, KernelSHAP, or gradient-based methods, and transforms the
resulting discriminative model weights into a generative representation,
thereby suppressing the influence of suppressor variables while preserving
local fidelity. In extensive hyperparameter optimization on the XAI-TRIS
benchmark, PatternLocal consistently outperformed other XAI methods and reduced
false-positive attributions when explaining non-linear tasks, thereby enabling
more reliable and actionable insights.

</details>


### [183] [Bayesian Hierarchical Invariant Prediction](https://arxiv.org/abs/2505.11211)
*Francisco Madaleno,Pernille Julie Viuff Sand,Francisco C. Pereira,Sergio Hernan Garrido Mejia*

Main category: cs.LG

TL;DR: BHIP是一种基于层次贝叶斯的因果预测方法，改进了ICP的计算可扩展性，并支持先验信息的使用。


<details>
  <summary>Details</summary>
Motivation: 通过层次贝叶斯框架改进ICP，以更好地处理异构数据并提升计算效率。

Method: 采用层次贝叶斯方法，测试因果机制的稳定性，并使用马刺先验和钉板先验进行稀疏性处理。

Result: 在合成和真实数据中验证了BHIP的潜力，表明其可作为ICP的替代推理方法。

Conclusion: BHIP在计算可扩展性和先验信息利用方面优于ICP，适用于更多预测变量的场景。

Abstract: We propose Bayesian Hierarchical Invariant Prediction (BHIP) reframing
Invariant Causal Prediction (ICP) through the lens of Hierarchical Bayes. We
leverage the hierarchical structure to explicitly test invariance of causal
mechanisms under heterogeneous data, resulting in improved computational
scalability for a larger number of predictors compared to ICP. Moreover, given
its Bayesian nature BHIP enables the use of prior information. In this paper,
we test two sparsity inducing priors: horseshoe and spike-and-slab, both of
which allow us a more reliable identification of causal features. We test BHIP
in synthetic and real-world data showing its potential as an alternative
inference method to ICP.

</details>


### [184] [Sample Efficient Reinforcement Learning via Large Vision Language Model Distillation](https://arxiv.org/abs/2505.11221)
*Donghoon Lee,Tung M. Luu,Younghwan Lee,Chang D. Yoo*

Main category: cs.LG

TL;DR: LVLM2P框架通过将大型视觉语言模型（LVLM）的知识蒸馏到强化学习（RL）代理中，显著提高了RL算法的样本效率，同时减少了对环境手动文本描述的需求。


<details>
  <summary>Details</summary>
Motivation: 解决多模态基础模型在现实部署中资源消耗大以及RL算法样本复杂度高的问题。

Method: 利用LVLM作为教师模型，基于RL代理收集的轨迹提供指导性动作，减少早期学习阶段的无效探索，并通过LVLM直接从视觉观察中生成动作建议。

Result: 实验表明，LVLM2P显著提升了基线RL算法的样本效率。

Conclusion: LVLM2P为高效RL代理提供了一种可行的解决方案，适用于多样化任务。

Abstract: Recent research highlights the potential of multimodal foundation models in
tackling complex decision-making challenges. However, their large parameters
make real-world deployment resource-intensive and often impractical for
constrained systems. Reinforcement learning (RL) shows promise for
task-specific agents but suffers from high sample complexity, limiting
practical applications. To address these challenges, we introduce LVLM to
Policy (LVLM2P), a novel framework that distills knowledge from large
vision-language models (LVLM) into more efficient RL agents. Our approach
leverages the LVLM as a teacher, providing instructional actions based on
trajectories collected by the RL agent, which helps reduce less meaningful
exploration in the early stages of learning, thereby significantly accelerating
the agent's learning progress. Additionally, by leveraging the LVLM to suggest
actions directly from visual observations, we eliminate the need for manual
textual descriptors of the environment, enhancing applicability across diverse
tasks. Experiments show that LVLM2P significantly enhances the sample
efficiency of baseline RL algorithms.

</details>


### [185] [Learning traffic flows: Graph Neural Networks for Metamodelling Traffic Assignment](https://arxiv.org/abs/2505.11230)
*Oskar Bohn Lassen,Serio Agriesti,Mohamed Eldafrawi,Daniele Gammelli,Guido Cantelmo,Guido Gentile,Francisco Camara Pereira*

Main category: cs.LG

TL;DR: 提出了一种基于消息传递神经网络的学习方法，用于近似随机用户均衡分配的均衡流量，以解决传统交通分配问题计算成本高的问题。


<details>
  <summary>Details</summary>
Motivation: 传统方法需要迭代模拟达到均衡，难以实现实时或大规模场景分析，因此需要一种更高效的方法。

Method: 使用消息传递神经网络作为元模型，模拟传统交通模拟器的算法结构，以更好地捕捉底层过程。

Result: 模型在训练域外的输入数据上表现出鲁棒性，能够加速分布外场景评估，降低计算成本。

Conclusion: 该方法为大规模交通规划和实时决策提供了有前景的解决方案。

Abstract: The Traffic Assignment Problem is a fundamental, yet computationally
expensive, task in transportation modeling, especially for large-scale
networks. Traditional methods require iterative simulations to reach
equilibrium, making real-time or large-scale scenario analysis challenging. In
this paper, we propose a learning-based approach using Message-Passing Neural
Networks as a metamodel to approximate the equilibrium flow of the Stochastic
User Equilibrium assignment. Our model is designed to mimic the algorithmic
structure used in conventional traffic simulators allowing it to better capture
the underlying process rather than just the data. We benchmark it against other
conventional deep learning techniques and evaluate the model's robustness by
testing its ability to predict traffic flows on input data outside the domain
on which it was trained. This approach offers a promising solution for
accelerating out-of-distribution scenario assessments, reducing computational
costs in large-scale transportation planning, and enabling real-time
decision-making.

</details>


### [186] [Memory-Efficient Orthogonal Fine-Tuning with Principal Subspace Adaptation](https://arxiv.org/abs/2505.11235)
*Fei Wu,Jia Hu,Geyong Min,Shiqiang Wang*

Main category: cs.LG

TL;DR: 本文提出了一种内存高效的参数高效微调方法MOFT，通过主成分子空间适应减少内存占用，同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 由于模型参数增长导致全微调成本过高，参数高效微调（PEFT）成为适应下游任务的关键方法。正交微调及其变体表现优异，但内存效率低。

Method: 提出MOFT方法，通过奇异值分解定义低秩主成分子空间，约束正交变换在该子空间内，并引入可学习缩放向量增强灵活性。

Result: 在37个任务和4个模型上的实验表明，MOFT显著减少内存占用，同时性能优于基线方法。

Conclusion: MOFT是一种高效且灵活的正交微调方法，适用于大规模模型部署。

Abstract: Driven by the relentless growth in model parameters, which renders full
fine-tuning prohibitively expensive for large-scale deployment,
parameter-efficient fine-tuning (PEFT) has emerged as a crucial approach for
rapidly adapting large models to a wide range of downstream tasks. Among the
PEFT family, orthogonal fine-tuning and its variants have demonstrated
remarkable performance by preserving hyperspherical energy, which encodes
pairwise angular similarity between neurons. However, these methods are
inherently memory-inefficient due to the need to store intermediate activations
from multiple full-dimensional sparse matrices. To address this limitation, we
propose Memory-efficient Orthogonal Fine-Tuning (MOFT) with principal subspace
adaptation. Specifically, we first establish a theoretical condition under
which orthogonal transformations within a low-rank subspace preserve
hyperspherical energy. Based on this insight, we constrain orthogonal
fine-tuning to the principal subspace defined by the top-r components obtained
through singular value decomposition and impose an additional constraint on the
projection matrix to satisfy the preservation condition. To enhance MOFT's
flexibility across tasks, we relax strict orthogonality by introducing two
learnable scaling vectors. Extensive experiments on 37 diverse tasks and four
models across NLP and CV demonstrate that MOFT consistently outperforms key
baselines while significantly reducing the memory footprint of orthogonal
fine-tuning.

</details>


### [187] [Massive-STEPS: Massive Semantic Trajectories for Understanding POI Check-ins -- Dataset and Benchmarks](https://arxiv.org/abs/2505.11239)
*Wilson Wongso,Hao Xue,Flora D. Salim*

Main category: cs.LG

TL;DR: 论文提出了一个名为Massive-STEPS的大规模公开数据集，用于解决POI推荐领域的数据过时和缺乏多样性问题，并评估了多种推荐模型的性能。


<details>
  <summary>Details</summary>
Motivation: 当前POI推荐研究依赖过时数据集（2012-2013年）且缺乏全球多样性数据，阻碍了研究进展。

Method: 基于Semantic Trails数据集构建了Massive-STEPS数据集，覆盖12个城市，包含2017-2018年的24个月签到数据，并评估了多种POI推荐模型。

Result: Massive-STEPS提供了更丰富、更近期的数据，支持监督和零样本方法，模型性能在不同城市背景下得到评估。

Conclusion: 通过发布Massive-STEPS，旨在促进人类移动性和POI推荐研究的可重复性和公平性。

Abstract: Understanding human mobility through Point-of-Interest (POI) recommendation
is increasingly important for applications such as urban planning, personalized
services, and generative agent simulation. However, progress in this field is
hindered by two key challenges: the over-reliance on older datasets from
2012-2013 and the lack of reproducible, city-level check-in datasets that
reflect diverse global regions. To address these gaps, we present Massive-STEPS
(Massive Semantic Trajectories for Understanding POI Check-ins), a large-scale,
publicly available benchmark dataset built upon the Semantic Trails dataset and
enriched with semantic POI metadata. Massive-STEPS spans 12 geographically and
culturally diverse cities and features more recent (2017-2018) and
longer-duration (24 months) check-in data than prior datasets. We benchmarked a
wide range of POI recommendation models on Massive-STEPS using both supervised
and zero-shot approaches, and evaluated their performance across multiple urban
contexts. By releasing Massive-STEPS, we aim to facilitate reproducible and
equitable research in human mobility and POI recommendation. The dataset and
benchmarking code are available at:
https://github.com/cruiseresearchgroup/Massive-STEPS

</details>


### [188] [A Set-Sequence Model for Time Series](https://arxiv.org/abs/2505.11243)
*Elliot L. Epstein,Apaar Sadhwani,Kay Giesecke*

Main category: cs.LG

TL;DR: 提出了一种Set-Sequence模型，用于金融预测问题，无需手工特征，显著优于基准方法。


<details>
  <summary>Details</summary>
Motivation: 传统方法通过手工特征捕捉潜在横截面效应，效率低且不灵活。

Method: Set模型学习共享横截面摘要，Sequence模型独立处理摘要增强的时间序列，两者联合学习。

Result: 在股票收益预测和抵押贷款行为任务中显著优于基准方法。

Conclusion: Set-Sequence模型高效灵活，适用于变长单位推理，代码将开源。

Abstract: In many financial prediction problems, the behavior of individual units (such
as loans, bonds, or stocks) is influenced by observable unit-level factors and
macroeconomic variables, as well as by latent cross-sectional effects.
Traditional approaches attempt to capture these latent effects via handcrafted
summary features. We propose a Set-Sequence model that eliminates the need for
handcrafted features. The Set model first learns a shared cross-sectional
summary at each period. The Sequence model then ingests the summary-augmented
time series for each unit independently to predict its outcome. Both components
are learned jointly over arbitrary sets sampled during training. Our approach
harnesses the set nature of the cross-section and is computationally efficient,
generating set summaries in linear time relative to the number of units. It is
also flexible, allowing the use of existing sequence models and accommodating a
variable number of units at inference. Empirical evaluations demonstrate that
our Set-Sequence model significantly outperforms benchmarks on stock return
prediction and mortgage behavior tasks. Code will be released.

</details>


### [189] [Rethinking Irregular Time Series Forecasting: A Simple yet Effective Baseline](https://arxiv.org/abs/2505.11250)
*Xvyuan Liu,Xiangfei Qiu,Xingjian Wu,Zhengyu Li,Chenjuan Guo,Jilin Hu,Bin Yang*

Main category: cs.LG

TL;DR: APN框架通过TAPA模块和查询模块解决了不规则多变量时间序列预测的挑战，提高了效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 不规则多变量时间序列（IMTS）预测在医疗、生物力学等领域至关重要，但现有方法复杂且资源密集。

Method: 提出APN框架，包含TAPA模块（自适应分块和时间感知加权平均）和查询模块，最后用浅层MLP预测。

Result: 实验表明APN在效率和准确性上优于现有方法。

Conclusion: APN为不规则时间序列预测提供了一种高效且准确的解决方案。

Abstract: The forecasting of irregular multivariate time series (IMTS) is crucial in
key areas such as healthcare, biomechanics, climate science, and astronomy.
However, achieving accurate and practical predictions is challenging due to two
main factors. First, the inherent irregularity and data missingness in
irregular time series make modeling difficult. Second, most existing methods
are typically complex and resource-intensive. In this study, we propose a
general framework called APN to address these challenges. Specifically, we
design a novel Time-Aware Patch Aggregation (TAPA) module that achieves
adaptive patching. By learning dynamically adjustable patch boundaries and a
time-aware weighted averaging strategy, TAPA transforms the original irregular
sequences into high-quality, regularized representations in a
channel-independent manner. Additionally, we use a simple query module to
effectively integrate historical information while maintaining the model's
efficiency. Finally, predictions are made by a shallow MLP. Experimental
results on multiple real-world datasets show that APN outperforms existing
state-of-the-art methods in both efficiency and accuracy.

</details>


### [190] [Delta Attention: Fast and Accurate Sparse Attention Inference by Delta Correction](https://arxiv.org/abs/2505.11254)
*Jeffrey Willette,Heejun Lee,Sung Ju Hwang*

Main category: cs.LG

TL;DR: 论文提出了一种简单有效的方法，通过纠正稀疏注意力输出中的分布偏移，显著提升了稀疏注意力的性能，同时保持了高效的计算速度。


<details>
  <summary>Details</summary>
Motivation: 由于Transformer的注意力机制具有二次复杂度，长序列推理成本高且延迟大。稀疏注意力方法虽能降低计算负担，但会导致性能下降。研究发现这种下降是由于稀疏计算引起的注意力输出分布偏移。

Method: 提出了一种纠正分布偏移的简单方法，使稀疏注意力的输出分布更接近二次注意力。该方法可应用于任何稀疏注意力方法。

Result: 在131K RULER基准测试中，平均性能提升36%，恢复88%的二次注意力准确性，同时保持98.5%的稀疏度，使模型处理1M token预填充时比Flash Attention 2快32倍。

Conclusion: 该方法有效解决了稀疏注意力性能下降的问题，显著提升了效率与性能的平衡。

Abstract: The attention mechanism of a transformer has a quadratic complexity, leading
to high inference costs and latency for long sequences. However, attention
matrices are mostly sparse, which implies that many entries may be omitted from
computation for efficient inference. Sparse attention inference methods aim to
reduce this computational burden; however, they also come with a troublesome
performance degradation. We discover that one reason for this degradation is
that the sparse calculation induces a distributional shift in the attention
outputs. The distributional shift causes decoding-time queries to fail to align
well with the appropriate keys from the prefill stage, leading to a drop in
performance. We propose a simple, novel, and effective procedure for correcting
this distributional shift, bringing the distribution of sparse attention
outputs closer to that of quadratic attention. Our method can be applied on top
of any sparse attention method, and results in an average 36%pt performance
increase, recovering 88% of quadratic attention accuracy on the 131K RULER
benchmark when applied on top of sliding window attention with sink tokens
while only adding a small overhead. Our method can maintain approximately 98.5%
sparsity over full quadratic attention, making our model 32 times faster than
Flash Attention 2 when processing 1M token prefills.

</details>


### [191] [Fourier Low-rank and Sparse Tensor for Efficient Tensor Completion](https://arxiv.org/abs/2505.11261)
*Jingyang Li,Jiuqian Shang,Yang Chen*

Main category: cs.LG

TL;DR: FLoST是一种新的张量补全模型，通过傅里叶变换分解时间维度，结合低秩和稀疏性，高效捕捉时空数据的低频稳定性和高频变化。


<details>
  <summary>Details</summary>
Motivation: 传统低秩张量模型对所有模式对称处理，无法有效捕捉时空数据的独特模式，尤其是时间维度的低频和高频变化。

Method: 提出FLoST模型，利用傅里叶变换分解时间维度，低频部分用低秩矩阵建模，高频部分用稀疏性表示。

Result: FLoST在参数数量和计算效率上优于现有模型，尤其在时间维度较大时表现更优。

Conclusion: FLoST在准确性和计算效率上优于现有方法，为时空数据重建提供了更高效的解决方案。

Abstract: Tensor completion is crucial in many scientific domains with missing data
problems. Traditional low-rank tensor models, including CP, Tucker, and
Tensor-Train, exploit low-dimensional structures to recover missing data.
However, these methods often treat all tensor modes symmetrically, failing to
capture the unique spatiotemporal patterns inherent in scientific data, where
the temporal component exhibits both low-frequency stability and high-frequency
variations. To address this, we propose a novel model, \underline{F}ourier
\underline{Lo}w-rank and \underline{S}parse \underline{T}ensor (FLoST), which
decomposes the tensor along the temporal dimension using a Fourier transform.
This approach captures low-frequency components with low-rank matrices and
high-frequency fluctuations with sparsity, resulting in a hybrid structure that
efficiently models both smooth and localized variations. Compared to the
well-known tubal-rank model, which assumes low-rankness across all frequency
components, FLoST requires significantly fewer parameters, making it
computationally more efficient, particularly when the time dimension is large.
Through theoretical analysis and empirical experiments, we demonstrate that
FLoST outperforms existing tensor completion models in terms of both accuracy
and computational efficiency, offering a more interpretable solution for
spatiotemporal data reconstruction.

</details>


### [192] [Driving Mechanisms and Forecasting of China's Pet Population-An ARIMA-RF-HW Hybrid Approach](https://arxiv.org/abs/2505.11269)
*Shengjia Chang,Xianshuo Yue*

Main category: cs.LG

TL;DR: 提出了一种动态加权的ARIMA-RF-HW混合模型，结合ARIMA、随机森林和Holt-Winters平滑，用于提高中国宠物数量预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 改善宠物数量预测的准确性，以支持政策制定和企业服务优化。

Method: 使用2005-2023年的数据，结合经济、社会和政策指标，通过Z-score标准化和缺失值填补预处理数据，构建动态加权混合模型。

Result: 关键驱动因素包括城市收入、消费和政策数量，预测显示猫数量稳定增长，狗数量波动。

Conclusion: 研究为政策制定者优化宠物健康管理提供了支持，并指导企业开发差异化服务，推动行业可持续发展。

Abstract: This study proposes a dynamically weighted ARIMA-RF-HW hybrid model
integrating ARIMA for seasonality and trends, Random Forest for nonlinear
features, and Holt-Winters smoothing for seasonal adjustment to improve China's
pet population forecasting accuracy. Using 2005-2023 data with nine economic,
social, and policy indicators (urban income, consumption, aging ratio, policy
quantity, new veterinary drug approvals), data were preprocessed via Z-score
normalization and missing value imputation. The results show that key drivers
of pet populations include urban income (19.48% for cats, 17.15% for dogs),
consumption (17.99% for cats), and policy quantity (13.33% for cats, 14.02% for
dogs), with aging (12.81% for cats, 13.27% for dogs) and urbanization
amplifying the demand for pets. Forecasts show steady cat growth and
fluctuating dog numbers, reflecting cats' adaptability to urban environments.
This research supports policymakers in optimizing pet health management and
guides enterprises in developing differentiated services, advancing sustainable
industry growth.

</details>


### [193] [Multiclass threshold-based classification](https://arxiv.org/abs/2505.11276)
*Francesco Marchetti,Edoardo Legnaro,Sabrina Guastavino*

Main category: cs.LG

TL;DR: 提出了一种基于阈值的多类分类框架，通过几何视角替代传统的softmax概率解释，实现分类性能的后验优化。


<details>
  <summary>Details</summary>
Motivation: 传统argmax规则在多类分类中缺乏灵活性，希望通过引入多维阈值优化分类性能。

Method: 将softmax输出视为多维单纯形上的几何对象，通过调整多维阈值优化分类得分。

Result: 实验表明多维阈值调优能显著提升性能，且提出的得分导向损失函数与传统损失函数竞争力相当。

Conclusion: 多维阈值框架为多类分类提供了灵活的后验优化手段，性能提升显著。

Abstract: In this paper, we introduce a threshold-based framework for multiclass
classification that generalizes the standard argmax rule. This is done by
replacing the probabilistic interpretation of softmax outputs with a geometric
one on the multidimensional simplex, where the classification depends on a
multidimensional threshold. This change of perspective enables for any trained
classification network an a posteriori optimization of the classification score
by means of threshold tuning, as usually carried out in the binary setting.
This allows a further refinement of the prediction capability of any network.
Moreover, this multidimensional threshold-based setting makes it possible to
define score-oriented losses, which are based on the interpretation of the
threshold as a random variable. Our experiments show that the multidimensional
threshold tuning yields consistent performance improvements across various
networks and datasets, and that the proposed multiclass score-oriented losses
are competitive with standard loss functions, resembling the advantages
observed in the binary case.

</details>


### [194] [SubROC: AUC-Based Discovery of Exceptional Subgroup Performance for Binary Classifiers](https://arxiv.org/abs/2505.11283)
*Tom Siegl,Kutalmış Coşkun,Bjarne Hiller,Amin Mirzaei,Florian Lemmerich,Martin Becker*

Main category: cs.LG

TL;DR: SubROC是一个开源框架，用于高效识别分类模型在不同子群体中的表现差异，支持模型部署决策。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型在不同子群体中表现不均，缺乏高效识别和分析这些子群体的框架。

Method: 基于Exceptional Model Mining，SubROC结合ROC和PR AUC评估指标，支持快速子群体搜索、类别不平衡控制和显著性测试。

Result: SubROC在案例研究和多数据集比较中展示了其实际应用价值。

Conclusion: SubROC为识别模型在不同子群体中的表现提供了可靠且高效的解决方案。

Abstract: Machine learning (ML) is increasingly employed in real-world applications
like medicine or economics, thus, potentially affecting large populations.
However, ML models often do not perform homogeneously across such populations
resulting in subgroups of the population (e.g., sex=female AND
marital_status=married) where the model underperforms or, conversely, is
particularly accurate. Identifying and describing such subgroups can support
practical decisions on which subpopulation a model is safe to deploy or where
more training data is required. The potential of identifying and analyzing such
subgroups has been recognized, however, an efficient and coherent framework for
effective search is missing. Consequently, we introduce SubROC, an open-source,
easy-to-use framework based on Exceptional Model Mining for reliably and
efficiently finding strengths and weaknesses of classification models in the
form of interpretable population subgroups. SubROC incorporates common
evaluation measures (ROC and PR AUC), efficient search space pruning for fast
exhaustive subgroup search, control for class imbalance, adjustment for
redundant patterns, and significance testing. We illustrate the practical
benefits of SubROC in case studies as well as in comparative analyses across
multiple datasets.

</details>


### [195] [Bidirectional Information Flow (BIF) -- A Sample Efficient Hierarchical Gaussian Process for Bayesian Optimization](https://arxiv.org/abs/2505.11294)
*Juan D. Guerra,Thomas Garbay,Guillaume Lajoie,Marco Bonizzato*

Main category: cs.LG

TL;DR: 提出了一种双向信息流（BIF）框架，改进了传统分层高斯过程（H-GP）的单向信息传递问题，显著提升了样本效率和收敛速度。


<details>
  <summary>Details</summary>
Motivation: 传统H-GP模型仅支持单向信息传递（上下层），限制了样本效率和收敛速度。

Method: 设计了BIF框架，通过双向信息交换（上下层反馈）优化在线训练，同时保持模块化结构。

Result: 在合成和实际神经刺激优化任务中，BIF的父模型和子模型的R²分数分别提升了85%和5倍。

Conclusion: BIF通过双向信息流显著提升了H-GP的性能，适用于具有层次结构的问题。

Abstract: Hierarchical Gaussian Process (H-GP) models divide problems into different
subtasks, allowing for different models to address each part, making them
well-suited for problems with inherent hierarchical structure. However, typical
H-GP models do not fully take advantage of this structure, only sending
information up or down the hierarchy. This one-way coupling limits sample
efficiency and slows convergence. We propose Bidirectional Information Flow
(BIF), an efficient H-GP framework that establishes bidirectional information
exchange between parent and child models in H-GPs for online training. BIF
retains the modular structure of hierarchical models - the parent combines
subtask knowledge from children GPs - while introducing top-down feedback to
continually refine children models during online learning. This mutual exchange
improves sample efficiency, enables robust training, and allows modular reuse
of learned subtask models. BIF outperforms conventional H-GP Bayesian
Optimization methods, achieving up to 85% and 5x higher $R^2$ scores for the
parent and children respectively, on synthetic and real-world neurostimulation
optimization tasks.

</details>


### [196] [Graph Representational Learning: When Does More Expressivity Hurt Generalization?](https://arxiv.org/abs/2505.11298)
*Sohir Maskey,Raffaele Paolino,Fabian Jogl,Gitta Kutyniok,Johannes F. Lutzeyer*

Main category: cs.LG

TL;DR: 论文研究了GNN的表达能力与预测性能之间的关系，提出了一种衡量图结构相似性的方法，并探讨了其对泛化性能的影响。


<details>
  <summary>Details</summary>
Motivation: 探索图神经网络（GNNs）的表达能力与预测性能之间的关系，以理解其泛化行为。

Method: 引入了一族预度量来衡量图的结构相似性，并将其与GNN的泛化性能关联。通过理论推导和实验验证，分析了模型复杂度、训练集大小和训练-测试图距离对泛化的影响。

Result: 研究发现，表达能力更强的GNN可能在泛化上表现更差，除非其增加的复杂度被足够大的训练集或更小的训练-测试图距离所平衡。

Conclusion: 研究揭示了GNN表达能力和泛化性能之间的关系，为理论理解和实际应用提供了支持。

Abstract: Graph Neural Networks (GNNs) are powerful tools for learning on structured
data, yet the relationship between their expressivity and predictive
performance remains unclear. We introduce a family of premetrics that capture
different degrees of structural similarity between graphs and relate these
similarities to generalization, and consequently, the performance of expressive
GNNs. By considering a setting where graph labels are correlated with
structural features, we derive generalization bounds that depend on the
distance between training and test graphs, model complexity, and training set
size. These bounds reveal that more expressive GNNs may generalize worse unless
their increased complexity is balanced by a sufficiently large training set or
reduced distance between training and test graphs. Our findings relate
expressivity and generalization, offering theoretical insights supported by
empirical results.

</details>


### [197] [Heterogeneity-Aware Client Sampling: A Unified Solution for Consistent Federated Learning](https://arxiv.org/abs/2505.11304)
*Shudi Weng,Chao Ren,Ming Xiao,Mikael Skoglund*

Main category: cs.LG

TL;DR: 该论文揭示了联邦学习中异构通信和计算如何导致目标不一致，并提出了一种通用方法FedACS来解决这一问题。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中客户端的异构通信和计算能力会显著扭曲优化动态，导致全局模型收敛到错误的平稳点。目前对这种异构性的联合影响缺乏深入理解。

Method: 提出Federated Heterogeneity-Aware Client Sampling (FedACS)方法，通过理论分析设计了一种消除目标不一致的通用方案。

Result: FedACS在动态异构环境中以$O(1/\sqrt{R})$的速率收敛到正确最优解，实验显示其性能优于现有方法，并显著降低了通信和计算成本。

Conclusion: FedACS是首个统一解决联邦学习中异构通信和计算问题的通用方法，具有理论和实践上的重要意义。

Abstract: Federated learning (FL) commonly involves clients with diverse communication
and computational capabilities. Such heterogeneity can significantly distort
the optimization dynamics and lead to objective inconsistency, where the global
model converges to an incorrect stationary point potentially far from the
pursued optimum. Despite its critical impact, the joint effect of communication
and computation heterogeneity has remained largely unexplored, due to the
intrinsic complexity of their interaction. In this paper, we reveal the
fundamentally distinct mechanisms through which heterogeneous communication and
computation drive inconsistency in FL. To the best of our knowledge, this is
the first unified theoretical analysis of general heterogeneous FL, offering a
principled understanding of how these two forms of heterogeneity jointly
distort the optimization trajectory under arbitrary choices of local solvers.
Motivated by these insights, we propose Federated Heterogeneity-Aware Client
Sampling, FedACS, a universal method to eliminate all types of objective
inconsistency. We theoretically prove that FedACS converges to the correct
optimum at a rate of $O(1/\sqrt{R})$, even in dynamic heterogeneous
environments. Extensive experiments across multiple datasets show that FedACS
outperforms state-of-the-art and category-specific baselines by 4.3%-36%, while
reducing communication costs by 22%-89% and computation loads by 14%-105%,
respectively.

</details>


### [198] [Effective Probabilistic Time Series Forecasting with Fourier Adaptive Noise-Separated Diffusion](https://arxiv.org/abs/2505.11306)
*Xinyan Wang,Rui Dai,Kaikui Liu,Xiangxiang Chu*

Main category: cs.LG

TL;DR: FALDA是一种新型概率框架，用于时间序列预测，通过傅里叶分解和轻量级去噪器提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有概率预测方法在处理时间序列时的不确定性问题和计算效率不足。

Method: 结合傅里叶分解和条件扩散模型，使用轻量级去噪器DEMA优化去噪过程。

Result: 在六个真实数据集上表现优于现有方法，计算效率高且精度不降。

Conclusion: FALDA在长期时间序列预测中显著提升性能，同时减少不确定性。

Abstract: We propose the Fourier Adaptive Lite Diffusion Architecture (FALDA), a novel
probabilistic framework for time series forecasting. First, we introduce the
Diffusion Model for Residual Regression (DMRR) framework, which unifies
diffusion-based probabilistic regression methods. Within this framework, FALDA
leverages Fourier-based decomposition to incorporate a component-specific
architecture, enabling tailored modeling of individual temporal components. A
conditional diffusion model is utilized to estimate the future noise term,
while our proposed lightweight denoiser, DEMA (Decomposition MLP with AdaLN),
conditions on the historical noise term to enhance denoising performance.
Through mathematical analysis and empirical validation, we demonstrate that
FALDA effectively reduces epistemic uncertainty, allowing probabilistic
learning to primarily focus on aleatoric uncertainty. Experiments on six
real-world benchmarks demonstrate that FALDA consistently outperforms existing
probabilistic forecasting approaches across most datasets for long-term time
series forecasting while achieving enhanced computational efficiency without
compromising accuracy. Notably, FALDA also achieves superior overall
performance compared to state-of-the-art (SOTA) point forecasting approaches,
with improvements of up to 9%.

</details>


### [199] [Diffusion Learning with Partial Agent Participation and Local Updates](https://arxiv.org/abs/2505.11307)
*Elsa Rizk,Kun Yuan,Ali H. Sayed*

Main category: cs.LG

TL;DR: 本文提出了一种改进的扩散学习方法，通过引入局部更新和部分代理参与，减少通信开销并适应边缘设备的波动性。


<details>
  <summary>Details</summary>
Motivation: 传统扩散学习依赖每次迭代的通信，导致通信开销大，且边缘设备的波动性（如断电或信号丢失）影响通信可靠性。

Method: 采用局部更新减少通信频率，并通过部分代理参与机制适应设备可用性。

Result: 算法在均方误差意义下稳定，并提供了紧密的均方偏差（MSD）性能分析。

Conclusion: 数值实验验证了理论结果，改进方法有效降低了通信开销并提升了可靠性。

Abstract: Diffusion learning is a framework that endows edge devices with advanced
intelligence. By processing and analyzing data locally and allowing each agent
to communicate with its immediate neighbors, diffusion effectively protects the
privacy of edge devices, enables real-time response, and reduces reliance on
central servers. However, traditional diffusion learning relies on
communication at every iteration, leading to communication overhead, especially
with large learning models. Furthermore, the inherent volatility of edge
devices, stemming from power outages or signal loss, poses challenges to
reliable communication between neighboring agents. To mitigate these issues,
this paper investigates an enhanced diffusion learning approach incorporating
local updates and partial agent participation. Local updates will curtail
communication frequency, while partial agent participation will allow for the
inclusion of agents based on their availability. We prove that the resulting
algorithm is stable in the mean-square error sense and provide a tight analysis
of its Mean-Square-Deviation (MSD) performance. Various numerical experiments
are conducted to illustrate our theoretical findings.

</details>


### [200] [Reinforcement Learning Closures for Underresolved Partial Differential Equations using Synthetic Data](https://arxiv.org/abs/2505.11308)
*Lothar Heimbach,Sebastian Kaltenbach,Petr Karnakov,Francis J. Alexander,Petros Koumoutsakos*

Main category: cs.LG

TL;DR: 提出了一种基于合成数据和强化学习的框架，用于开发粗粒度偏微分方程（PDEs）的闭合模型，以减少计算成本并保持准确性。


<details>
  <summary>Details</summary>
Motivation: 解决实际应用中PDEs计算成本高的问题，通过粗粒度近似和闭合模型弥补细节损失。

Method: 利用制造解方法生成合成数据，结合强化学习为粗粒度PDEs提供闭合模型。

Result: 在一维和二维Burgers方程及二维平流方程中验证了方法的有效性，并展示了闭合模型在非均匀PDEs到均匀PDEs的泛化能力。

Conclusion: 该方法为数据稀缺系统开发高效闭合模型提供了潜力。

Abstract: Partial Differential Equations (PDEs) describe phenomena ranging from
turbulence and epidemics to quantum mechanics and financial markets. Despite
recent advances in computational science, solving such PDEs for real-world
applications remains prohibitively expensive because of the necessity of
resolving a broad range of spatiotemporal scales. In turn, practitioners often
rely on coarse-grained approximations of the original PDEs, trading off
accuracy for reduced computational resources. To mitigate the loss of detail
inherent in such approximations, closure models are employed to represent
unresolved spatiotemporal interactions. We present a framework for developing
closure models for PDEs using synthetic data acquired through the method of
manufactured solutions. These data are used in conjunction with reinforcement
learning to provide closures for coarse-grained PDEs. We illustrate the
efficacy of our method using the one-dimensional and two-dimensional Burgers'
equations and the two-dimensional advection equation. Moreover, we demonstrate
that closure models trained for inhomogeneous PDEs can be effectively
generalized to homogeneous PDEs. The results demonstrate the potential for
developing accurate and computationally efficient closure models for systems
with scarce data.

</details>


### [201] [Where You Place the Norm Matters: From Prejudiced to Neutral Initializations](https://arxiv.org/abs/2505.11312)
*Emanuele Francazi,Francesco Pinto,Aurelien Lucchi,Marco Baity-Jesi*

Main category: cs.LG

TL;DR: 论文研究了归一化层（如批量归一化和层归一化）在神经网络初始化阶段对预测行为的影响，揭示了其位置选择如何塑造初始预测分布，进而影响学习动态。


<details>
  <summary>Details</summary>
Motivation: 归一化层在现代神经网络中广泛应用，但其在初始化阶段如何影响模型行为的理论理解尚不充分。本文旨在填补这一空白。

Method: 研究了归一化层的存在及其在隐藏层中的位置如何影响网络初始化时的预测统计特性，特别是类别预测的分布。

Result: 归一化层的位置选择会导致初始预测行为（从中性到偏见）的系统性差异，进而影响学习动态。

Conclusion: 通过将架构选择与初始化预测统计联系起来，为归一化层的影响提供了理论依据，并指导更可控、可解释的网络设计。

Abstract: Normalization layers, such as Batch Normalization and Layer Normalization,
are central components in modern neural networks, widely adopted to improve
training stability and generalization. While their practical effectiveness is
well documented, a detailed theoretical understanding of how normalization
affects model behavior, starting from initialization, remains an important open
question. In this work, we investigate how both the presence and placement of
normalization within hidden layers influence the statistical properties of
network predictions before training begins. In particular, we study how these
choices shape the distribution of class predictions at initialization, which
can range from unbiased (Neutral) to highly concentrated (Prejudiced) toward a
subset of classes. Our analysis shows that normalization placement induces
systematic differences in the initial prediction behavior of neural networks,
which in turn shape the dynamics of learning. By linking architectural choices
to prediction statistics at initialization, our work provides a principled
understanding of how normalization can influence early training behavior and
offers guidance for more controlled and interpretable network design.

</details>


### [202] [Anomaly Detection for Non-stationary Time Series using Recurrent Wavelet Probabilistic Neural Network](https://arxiv.org/abs/2505.11321)
*Pu Yang,J. A. Barria*

Main category: cs.LG

TL;DR: 提出了一种无监督的循环小波概率神经网络（RWPNN），用于非平稳环境中的异常检测，通过非参数密度估计网络建模时间特征。


<details>
  <summary>Details</summary>
Motivation: 解决非平稳环境中时间序列异常检测（TSAD）任务中数据变化率高、分布假设强的问题。

Method: 结合堆叠循环编码器-解码器（SREnc-Dec）模块和多感受野小波概率网络（MRWPN），扩展标准小波概率网络以处理高维数据。

Result: 在45个真实世界时间序列数据集上验证了RWPNN的性能，显示其在非平稳环境中具有鲁棒性和准确性，并能提供异常事件的早期预警。

Conclusion: RWPNN是一种有效的TSAD方法，适用于非平稳环境，具有较高的适应性和检测能力。

Abstract: In this paper, an unsupervised Recurrent Wavelet Probabilistic Neural Network
(RWPNN) is proposed, which aims at detecting anomalies in non-stationary
environments by modelling the temporal features using a nonparametric density
estimation network. The novel framework consists of two components, a Stacked
Recurrent Encoder-Decoder (SREnc-Dec) module that captures temporal features in
a latent space, and a Multi-Receptive-field Wavelet Probabilistic Network
(MRWPN) that creates an ensemble probabilistic model to characterise the latent
space. This formulation extends the standard wavelet probabilistic networks to
wavelet deep probabilistic networks, which can handle higher data
dimensionality. The MRWPN module can adapt to different rates of data variation
in different datasets without imposing strong distribution assumptions,
resulting in a more robust and accurate detection for Time Series Anomaly
Detection (TSAD) tasks in the non-stationary environment. We carry out the
assessment on 45 real-world time series datasets from various domains, verify
the performance of RWPNN in TSAD tasks with several constraints, and show its
ability to provide early warnings for anomalous events.

</details>


### [203] [The Final Layer Holds the Key: A Unified and Efficient GNN Calibration Framework](https://arxiv.org/abs/2505.11335)
*Jincheng Huang,Jie Xu,Xiaoshuang Shi,Ping Hu,Lei Feng,Xiaofeng Zhu*

Main category: cs.LG

TL;DR: 提出了一种简单高效的图校准方法，通过减少最后一层参数的权重衰减和节点级校准，解决GNN预测置信度不足的问题。


<details>
  <summary>Details</summary>
Motivation: GNN的预测置信度通常被低估，影响决策可靠性，现有校准方法未能捕捉模型与置信度的内在关系。

Method: 建立统一理论框架，揭示模型置信度由最后一层的类中心级和节点级校准共同决定，减少最后一层权重衰减并补充节点级校准。

Result: 实验验证了该方法的优越性。

Conclusion: 该方法通过理论框架和简单调整，有效提升了GNN的预测置信度校准。

Abstract: Graph Neural Networks (GNNs) have demonstrated remarkable effectiveness on
graph-based tasks. However, their predictive confidence is often miscalibrated,
typically exhibiting under-confidence, which harms the reliability of their
decisions. Existing calibration methods for GNNs normally introduce additional
calibration components, which fail to capture the intrinsic relationship
between the model and the prediction confidence, resulting in limited
theoretical guarantees and increased computational overhead. To address this
issue, we propose a simple yet efficient graph calibration method. We establish
a unified theoretical framework revealing that model confidence is jointly
governed by class-centroid-level and node-level calibration at the final layer.
Based on this insight, we theoretically show that reducing the weight decay of
the final-layer parameters alleviates GNN under-confidence by acting on the
class-centroid level, while node-level calibration acts as a finer-grained
complement to class-centroid level calibration, which encourages each test node
to be closer to its predicted class centroid at the final-layer
representations. Extensive experiments validate the superiority of our method.

</details>


### [204] [Sobolev Training of End-to-End Optimization Proxies](https://arxiv.org/abs/2505.11342)
*Andrew W. Rosemberg,Joaquim Dias Garcia,Russell Bent,Pascal Van Hentenryck*

Main category: cs.LG

TL;DR: 论文研究了通过Sobolev训练范式将求解器灵敏度集成到优化代理中，显著提升了代理模型的性能。


<details>
  <summary>Details</summary>
Motivation: 传统迭代求解器耗时较长，优化代理通过机器学习模型快速近似参数优化问题的解映射，但需要进一步提升其准确性和可靠性。

Method: 采用Sobolev训练范式，在完全监督和自监督两种设置下，通过添加求解器的方向导数信息来训练代理模型。

Result: 在完全监督设置下，均方误差降低56%，最坏约束违规减少400%；在自监督设置下，平均最优性差距减半。

Conclusion: Sobolev训练为大规模优化问题提供了快速可靠的代理模型，适用于安全关键任务。

Abstract: Optimization proxies - machine learning models trained to approximate the
solution mapping of parametric optimization problems in a single forward pass -
offer dramatic reductions in inference time compared to traditional iterative
solvers. This work investigates the integration of solver sensitivities into
such end to end proxies via a Sobolev training paradigm and does so in two
distinct settings: (i) fully supervised proxies, where exact solver outputs and
sensitivities are available, and (ii) self supervised proxies that rely only on
the objective and constraint structure of the underlying optimization problem.
By augmenting the standard training loss with directional derivative
information extracted from the solver, the proxy aligns both its predicted
solutions and local derivatives with those of the optimizer. Under Lipschitz
continuity assumptions on the true solution mapping, matching first order
sensitivities is shown to yield uniform approximation error proportional to the
training set covering radius. Empirically, different impacts are observed in
each studied setting. On three large Alternating Current Optimal Power Flow
benchmarks, supervised Sobolev training cuts mean squared error by up to 56
percent and the median worst case constraint violation by up to 400 percent
while keeping the optimality gap below 0.22 percent. For a mean variance
portfolio task trained without labeled solutions, self supervised Sobolev
training halves the average optimality gap in the medium risk region (standard
deviation above 10 percent of budget) and matches the baseline elsewhere.
Together, these results highlight Sobolev training whether supervised or self
supervised as a path to fast reliable surrogates for safety critical large
scale optimization workloads.

</details>


### [205] [What Can We Learn From MIMO Graph Convolutions?](https://arxiv.org/abs/2505.11346)
*Andreas Roth,Thomas Liebig*

Main category: cs.LG

TL;DR: 本文提出了一种多输入多输出（MIMO）图卷积的直接近似方法，称为局部化MIMO图卷积（LMGC），并证明了其在多计算图下的线性独立性。


<details>
  <summary>Details</summary>
Motivation: 现有图神经网络（GNNs）通常在单输入单输出（SISO）情况下近似图卷积，而实际应用多为MIMO情况，因此需要直接针对MIMO情况设计近似方法。

Method: 通过卷积定理推导MIMO图卷积，并直接近似，提出局部化MIMO图卷积（LMGC），支持多计算图和节点对间的特征变换。

Result: 证明LMGC在单计算图下对多重集是单射的，且多计算图下表示线性独立。实验验证LMGC能结合多种方法的优势。

Conclusion: LMGC为MIMO图卷积提供了有效的近似方法，具有理论保证和实际性能优势。

Abstract: Most graph neural networks (GNNs) utilize approximations of the general graph
convolution derived in the graph Fourier domain. While GNNs are typically
applied in the multi-input multi-output (MIMO) case, the approximations are
performed in the single-input single-output (SISO) case. In this work, we first
derive the MIMO graph convolution through the convolution theorem and
approximate it directly in the MIMO case. We find the key MIMO-specific
property of the graph convolution to be operating on multiple computational
graphs, or equivalently, applying distinct feature transformations for each
pair of nodes. As a localized approximation, we introduce localized MIMO graph
convolutions (LMGCs), which generalize many linear message-passing neural
networks. For almost every choice of edge weights, we prove that LMGCs with a
single computational graph are injective on multisets, and the resulting
representations are linearly independent when more than one computational graph
is used. Our experimental results confirm that an LMGC can combine the benefits
of various methods.

</details>


### [206] [Training NTK to Generalize with KARE](https://arxiv.org/abs/2505.11347)
*Johannes Schwab,Bryan Kelly,Semyon Malamud,Teng Andrea Xu*

Main category: cs.LG

TL;DR: 论文提出通过显式优化神经正切核（NTK）来提升性能，使用KARE方法最小化泛化误差，实验显示其优于传统DNN和隐式NTK。


<details>
  <summary>Details</summary>
Motivation: 传统DNN训练隐式优化NTK，但显式优化可能更有效，挑战DNN的常规优势。

Method: 使用KARE方法显式训练NTK，最小化泛化误差而非经验风险。

Result: 实验表明显式训练的NTK性能优于原始DNN和隐式NTK。

Conclusion: 显式NTK训练是一种过参数化特征学习，在某些场景下优于传统DNN。

Abstract: The performance of the data-dependent neural tangent kernel (NTK; Jacot et
al. (2018)) associated with a trained deep neural network (DNN) often matches
or exceeds that of the full network. This implies that DNN training via
gradient descent implicitly performs kernel learning by optimizing the NTK. In
this paper, we propose instead to optimize the NTK explicitly. Rather than
minimizing empirical risk, we train the NTK to minimize its generalization
error using the recently developed Kernel Alignment Risk Estimator (KARE; Jacot
et al. (2020)). Our simulations and real data experiments show that NTKs
trained with KARE consistently match or significantly outperform the original
DNN and the DNN- induced NTK (the after-kernel). These results suggest that
explicitly trained kernels can outperform traditional end-to-end DNN
optimization in certain settings, challenging the conventional dominance of
DNNs. We argue that explicit training of NTK is a form of over-parametrized
feature learning.

</details>


### [207] [Context parroting: A simple but tough-to-beat baseline for foundation models in scientific machine learning](https://arxiv.org/abs/2505.11349)
*Yuanzhao Zhang,William Gilpin*

Main category: cs.LG

TL;DR: 时间序列基础模型在科学机器学习中表现出预测物理系统的能力，但研究发现其预测依赖于简单的上下文复制策略，而非学习物理规律。


<details>
  <summary>Details</summary>
Motivation: 探讨时间序列基础模型是否真正学习物理系统的内在规律，还是仅通过上下文复制进行预测。

Method: 通过比较基础模型与直接上下文复制模型的预测性能，分析其预测策略。

Result: 直接上下文复制模型在预测多样动态系统时表现优于基础模型，且计算成本极低。

Conclusion: 上下文复制是时间序列预测的简单但难以超越的基线，有助于识别更高级的上下文学习策略。

Abstract: Recently-developed time series foundation models for scientific machine
learning exhibit emergent abilities to predict physical systems. These
abilities include zero-shot forecasting, in which a model forecasts future
states of a system given only a short trajectory as context. Here, we show that
foundation models applied to physical systems can give accurate predictions,
but that they fail to develop meaningful representations of the underlying
physics. Instead, foundation models often forecast by context parroting, a
simple zero-shot forecasting strategy that copies directly from the context. As
a result, a naive direct context parroting model scores higher than
state-of-the-art time-series foundation models on predicting a diverse range of
dynamical systems, at a tiny fraction of the computational cost. We draw a
parallel between context parroting and induction heads, which explains why
large language models trained on text can be repurposed for time series
forecasting. Our dynamical systems perspective also ties the scaling between
forecast accuracy and context length to the fractal dimension of the attractor,
providing insight into the previously observed in-context neural scaling laws.
Context parroting thus serves as a simple but tough-to-beat baseline for future
time-series foundation models and can help identify in-context learning
strategies beyond parroting.

</details>


### [208] [Fractal Graph Contrastive Learning](https://arxiv.org/abs/2505.11356)
*Nero Z. Li,Xuehao Zhai,Zhichao Shi,Boshen Shi,Xuhui Jiang*

Main category: cs.LG

TL;DR: FractalGCL提出了一种基于分形自相似性的图对比学习框架，通过全局拓扑一致性提升性能，并减少计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有图对比学习方法依赖随机扰动或局部结构保持，缺乏对全局结构一致性的控制。

Method: FractalGCL通过基于重归一化的增强和分形维度感知的对比损失，实现全局拓扑一致性。

Result: 实验表明FractalGCL在标准基准上达到最优性能，并在交通网络上平均提升7%。

Conclusion: FractalGCL通过理论驱动的分形方法显著提升了图对比学习的性能与效率。

Abstract: While Graph Contrastive Learning (GCL) has attracted considerable attention
in the field of graph self-supervised learning, its performance heavily relies
on data augmentations that are expected to generate semantically consistent
positive pairs. Existing strategies typically resort to random perturbations or
local structure preservation, yet lack explicit control over global structural
consistency between augmented views. To address this limitation, we propose
Fractal Graph Contrastive Learning (FractalGCL), a theory-driven framework that
leverages fractal self-similarity to enforce global topological coherence.
FractalGCL introduces two key innovations: a renormalisation-based augmentation
that generates structurally aligned positive views via box coverings; and a
fractal-dimension-aware contrastive loss that aligns graph embeddings according
to their fractal dimensions. While combining the two innovations markedly
boosts graph-representation quality, it also adds non-trivial computational
overhead. To mitigate the computational overhead of fractal dimension
estimation, we derive a one-shot estimator by proving that the dimension
discrepancy between original and renormalised graphs converges weakly to a
centred Gaussian distribution. This theoretical insight enables a reduction in
dimension computation cost by an order of magnitude, cutting overall training
time by approximately 61%. The experiments show that FractalGCL not only
delivers state-of-the-art results on standard benchmarks but also outperforms
traditional baselines on traffic networks by an average margin of about
remarkably 7%. Codes are available at
(https://anonymous.4open.science/r/FractalGCL-0511).

</details>


### [209] [LGBQPC: Local Granular-Ball Quality Peaks Clustering](https://arxiv.org/abs/2505.11359)
*Zihang Jia,Zhen Zhang,Witold Pedrycz*

Main category: cs.LG

TL;DR: 本文提出了一种改进的LGBQPC算法，通过优化GB生成和聚类过程，解决了GBDPC在处理复杂数据时的局限性。


<details>
  <summary>Details</summary>
Motivation: GBDPC算法在处理复杂流形结构或非均匀密度分布的数据时表现不佳，因此需要改进。

Method: 提出GB-POJG+方法优化GB生成，并在聚类阶段引入相对GB质量和测地距离。

Result: 在40个基准数据集上的实验验证了LGBQPC的优越性能。

Conclusion: LGBQPC显著提升了GBDPC在复杂数据上的表现。

Abstract: The density peaks clustering (DPC) algorithm has attracted considerable
attention for its ability to detect arbitrarily shaped clusters based on a
simple yet effective assumption. Recent advancements integrating granular-ball
(GB) computing with DPC have led to the GB-based DPC (GBDPC) algorithm, which
improves computational efficiency. However, GBDPC demonstrates limitations when
handling complex clustering tasks, particularly those involving data with
complex manifold structures or non-uniform density distributions. To overcome
these challenges, this paper proposes the local GB quality peaks clustering
(LGBQPC) algorithm, which offers comprehensive improvements to GBDPC in both GB
generation and clustering processes based on the principle of justifiable
granularity (POJG). Firstly, an improved GB generation method, termed GB-POJG+,
is developed, which systematically refines the original GB-POJG in four key
aspects: the objective function, termination criterion for GB division,
definition of abnormal GB, and granularity level adaptation strategy. GB-POJG+
simplifies parameter configuration by requiring only a single penalty
coefficient and ensures high-quality GB generation while maintaining the number
of generated GBs within an acceptable range. In the clustering phase, two key
innovations are introduced based on the GB k-nearest neighbor graph: relative
GB quality for density estimation and geodesic distance for GB distance metric.
These modifications substantially improve the performance of GBDPC on datasets
with complex manifold structures or non-uniform density distributions.
Extensive numerical experiments on 40 benchmark datasets, including both
synthetic and publicly available datasets, validate the superior performance of
the proposed LGBQPC algorithm.

</details>


### [210] [Efficient End-to-End Learning for Decision-Making: A Meta-Optimization Approach](https://arxiv.org/abs/2505.11360)
*Rares Cristian,Pavithra Harsha,Georgia Perakis,Brian Quanz*

Main category: cs.LG

TL;DR: 提出了一种元优化方法，通过神经网络架构高效近似优化问题，显著降低端到端学习中的计算开销。


<details>
  <summary>Details</summary>
Motivation: 端到端学习在训练预测性ML模型时优于传统方法，但其计算复杂度高，尤其是大规模问题。

Method: 引入神经网络架构，通过交替投影解决优化问题，确保可行性约束，并证明其收敛性和泛化性。

Result: 方法计算效率高，能快速生成高质量近似解，且适用于多种优化问题。

Conclusion: 该方法在多个实际应用中表现出色，包括电力调度、最短路径和库存管理等。

Abstract: End-to-end learning has become a widely applicable and studied problem in
training predictive ML models to be aware of their impact on downstream
decision-making tasks. These end-to-end models often outperform traditional
methods that separate training from the optimization and only myopically focus
on prediction error. However, the computational complexity of end-to-end
frameworks poses a significant challenge, particularly for large-scale
problems. While training an ML model using gradient descent, each time we need
to compute a gradient we must solve an expensive optimization problem. We
present a meta-optimization method that learns efficient algorithms to
approximate optimization problems, dramatically reducing computational overhead
of solving the decision problem in general, an aspect we leverage in the
training within the end-to-end framework. Our approach introduces a neural
network architecture that near-optimally solves optimization problems while
ensuring feasibility constraints through alternate projections. We prove
exponential convergence, approximation guarantees, and generalization bounds
for our learning method. This method offers superior computational efficiency,
producing high-quality approximations faster and scaling better with problem
size compared to existing techniques. Our approach applies to a wide range of
optimization problems including deterministic, single-stage as well as
two-stage stochastic optimization problems. We illustrate how our proposed
method applies to (1) an electricity generation problem using real data from an
electricity routing company coordinating the movement of electricity throughout
13 states, (2) a shortest path problem with a computer vision task of
predicting edge costs from terrain maps, (3) a two-stage multi-warehouse
cross-fulfillment newsvendor problem, as well as a variety of other
newsvendor-like problems.

</details>


### [211] [Understanding Nonlinear Implicit Bias via Region Counts in Input Space](https://arxiv.org/abs/2505.11370)
*Jingwei Li,Jing Xu,Zifan Wang,Huishuai Zhang,Jingzhao Zhang*

Main category: cs.LG

TL;DR: 论文通过定义输入空间中相同预测标签的连通区域数量来表征隐式偏差，发现小区域数量与几何简单决策边界相关，且与良好泛化性能相关。


<details>
  <summary>Details</summary>
Motivation: 研究非线性背景下隐式偏差的定义和机制，以解释神经网络的强泛化能力。

Method: 提出用输入空间中相同预测标签的连通区域数量作为隐式偏差的度量，并通过实验验证其与泛化性能的关系。

Result: 小区域数量与几何简单决策边界相关，且与良好泛化性能相关；较大的学习率和较小的批次大小可以诱导小区域数量。

Conclusion: 区域数量是一种适应非线性、过参数化模型的隐式偏差度量，较大的学习率可以通过减少区域数量提升泛化性能。

Abstract: One explanation for the strong generalization ability of neural networks is
implicit bias. Yet, the definition and mechanism of implicit bias in non-linear
contexts remains little understood. In this work, we propose to characterize
implicit bias by the count of connected regions in the input space with the
same predicted label. Compared with parameter-dependent metrics (e.g., norm or
normalized margin), region count can be better adapted to nonlinear,
overparameterized models, because it is determined by the function mapping and
is invariant to reparametrization. Empirically, we found that small region
counts align with geometrically simple decision boundaries and correlate well
with good generalization performance. We also observe that good hyper-parameter
choices such as larger learning rates and smaller batch sizes can induce small
region counts. We further establish the theoretical connections and explain how
larger learning rate can induce small region counts in neural networks.

</details>


### [212] [On the Interconnections of Calibration, Quantification, and Classifier Accuracy Prediction under Dataset Shift](https://arxiv.org/abs/2505.11380)
*Alejandro Moreo*

Main category: cs.LG

TL;DR: 论文探讨了在数据集偏移条件下，分类器的校准、量化和准确性预测三个基本问题的等价性，并提出基于跨领域方法的新解决方案。


<details>
  <summary>Details</summary>
Motivation: 研究在数据集偏移情况下，分类器校准、量化和准确性预测问题的相互关联，以促进跨领域方法的统一。

Method: 通过证明三个问题的等价性，提出基于其他领域成熟方法的新解决方案。

Result: 新方法在性能上常与专用方法相当，有时甚至更优。

Conclusion: 论文旨在推动这些研究领域的交叉融合，促进统一方法和协同发展。

Abstract: When the distribution of the data used to train a classifier differs from
that of the test data, i.e., under dataset shift, well-established routines for
calibrating the decision scores of the classifier, estimating the proportion of
positives in a test sample, or estimating the accuracy of the classifier,
become particularly challenging. This paper investigates the interconnections
among three fundamental problems, calibration, quantification, and classifier
accuracy prediction, under dataset shift conditions. Specifically, we prove
their equivalence through mutual reduction, i.e., we show that access to an
oracle for any one of these tasks enables the resolution of the other two.
Based on these proofs, we propose new methods for each problem based on direct
adaptations of well-established methods borrowed from the other disciplines.
Our results show such methods are often competitive, and sometimes even surpass
the performance of dedicated approaches from each discipline. The main goal of
this paper is to fostering cross-fertilization among these research areas,
encouraging the development of unified approaches and promoting synergies
across the fields.

</details>


### [213] [IISE PG&E Energy Analytics Challenge 2025: Hourly-Binned Regression Models Beat Transformers in Load Forecasting](https://arxiv.org/abs/2505.11390)
*Millend Roy,Vladimir Pyltsov,Yinbo Hu*

Main category: cs.LG

TL;DR: 该研究评估了从经典回归到深度学习架构的电力负荷预测模型，发现XGBoost在数据有限的情况下优于复杂模型如TimeGPT。


<details>
  <summary>Details</summary>
Motivation: 电力负荷预测对电网稳定和可再生能源整合至关重要，但深度学习模型在长期预测中的效果尚不确定。

Method: 使用PCA降维，将预测任务视为回归问题，结合温度和GHI数据，堆叠24个模型生成年度预测。

Result: XGBoost在测试中表现最佳，而深度学习模型因数据限制未能超越传统方法。

Conclusion: 研究强调了根据数据特性选择模型的重要性，而非模型复杂度。

Abstract: Accurate electricity load forecasting is essential for grid stability,
resource optimization, and renewable energy integration. While
transformer-based deep learning models like TimeGPT have gained traction in
time-series forecasting, their effectiveness in long-term electricity load
prediction remains uncertain. This study evaluates forecasting models ranging
from classical regression techniques to advanced deep learning architectures
using data from the ESD 2025 competition. The dataset includes two years of
historical electricity load data, alongside temperature and global horizontal
irradiance (GHI) across five sites, with a one-day-ahead forecasting horizon.
Since actual test set load values remain undisclosed, leveraging predicted
values would accumulate errors, making this a long-term forecasting challenge.
We employ (i) Principal Component Analysis (PCA) for dimensionality reduction
and (ii) frame the task as a regression problem, using temperature and GHI as
covariates to predict load for each hour, (iii) ultimately stacking 24 models
to generate yearly forecasts.
  Our results reveal that deep learning models, including TimeGPT, fail to
consistently outperform simpler statistical and machine learning approaches due
to the limited availability of training data and exogenous variables. In
contrast, XGBoost, with minimal feature engineering, delivers the lowest error
rates across all test cases while maintaining computational efficiency. This
highlights the limitations of deep learning in long-term electricity
forecasting and reinforces the importance of model selection based on dataset
characteristics rather than complexity. Our study provides insights into
practical forecasting applications and contributes to the ongoing discussion on
the trade-offs between traditional and modern forecasting methods.

</details>


### [214] [Finding Counterfactual Evidences for Node Classification](https://arxiv.org/abs/2505.11396)
*Dazhuo Qiu,Jinwen Chen,Arijit Khan,Yan Zhao,Francesco Bonchi*

Main category: cs.LG

TL;DR: 论文提出了一种基于反事实学习的GNN节点分类方法，通过搜索反事实证据提升公平性和准确性。


<details>
  <summary>Details</summary>
Motivation: 解决GNN在公平性和可解释性上的问题，尤其是在无法进行随机对照试验的现实场景中。

Method: 开发了高效的搜索算法和索引方案，结合节点特征和结构信息识别反事实证据。

Result: 反事实证据能有效提升GNN的公平性和准确性。

Conclusion: 反事实证据在GNN应用中具有潜力，可推广至不同GNN模型。

Abstract: Counterfactual learning is emerging as an important paradigm, rooted in
causality, which promises to alleviate common issues of graph neural networks
(GNNs), such as fairness and interpretability. However, as in many real-world
application domains where conducting randomized controlled trials is
impractical, one has to rely on available observational (factual) data to
detect counterfactuals. In this paper, we introduce and tackle the problem of
searching for counterfactual evidences for the GNN-based node classification
task. A counterfactual evidence is a pair of nodes such that, regardless they
exhibit great similarity both in the features and in their neighborhood
subgraph structures, they are classified differently by the GNN. We develop
effective and efficient search algorithms and a novel indexing solution that
leverages both node features and structural information to identify
counterfactual evidences, and generalizes beyond any specific GNN. Through
various downstream applications, we demonstrate the potential of counterfactual
evidences to enhance fairness and accuracy of GNNs.

</details>


### [215] [Is Grokking a Computational Glass Relaxation?](https://arxiv.org/abs/2505.11411)
*Xiaotian Zhang,Yue Shang,Entao Yang,Ge Zhang*

Main category: cs.LG

TL;DR: 论文提出将神经网络中的“grokking”现象类比为计算玻璃松弛，揭示了其从记忆到泛化的无熵垒过渡，并开发了一种新型优化器WanD。


<details>
  <summary>Details</summary>
Motivation: 研究神经网络泛化能力的机制，尤其是“grokking”现象，即神经网络在训练性能接近完美后突然泛化的现象。

Method: 将神经网络视为物理系统，参数为自由度，训练损失为系统能量，通过采样玻尔兹曼熵景观分析记忆与泛化过程。

Result: 实验表明“grokking”从记忆到泛化的过渡无熵垒，且高熵状态对泛化有显著优势。开发的WanD优化器可消除“grokking”。

Conclusion: 研究挑战了现有理论，提出了新的优化器设计思路，为理解神经网络泛化提供了新视角。

Abstract: Understanding neural network's (NN) generalizability remains a central
question in deep learning research. The special phenomenon of grokking, where
NNs abruptly generalize long after the training performance reaches a
near-perfect level, offers a unique window to investigate the underlying
mechanisms of NNs' generalizability. Here we propose an interpretation for
grokking by framing it as a computational glass relaxation: viewing NNs as a
physical system where parameters are the degrees of freedom and train loss is
the system energy, we find memorization process resembles a rapid cooling of
liquid into non-equilibrium glassy state at low temperature and the later
generalization is like a slow relaxation towards a more stable configuration.
This mapping enables us to sample NNs' Boltzmann entropy (states of density)
landscape as a function of training loss and test accuracy. Our experiments in
transformers on arithmetic tasks suggests that there is NO entropy barrier in
the memorization-to-generalization transition of grokking, challenging previous
theory that defines grokking as a first-order phase transition. We identify a
high-entropy advantage under grokking, an extension of prior work linking
entropy to generalizability but much more significant. Inspired by grokking's
far-from-equilibrium nature, we develop a toy optimizer WanD based on
Wang-landau molecular dynamics, which can eliminate grokking without any
constraints and find high-norm generalizing solutions. This provides
strictly-defined counterexamples to theory attributing grokking solely to
weight norm evolution towards the Goldilocks zone and also suggests new
potential ways for optimizer design.

</details>


### [216] [Uncertainty quantification with approximate variational learning for wearable photoplethysmography prediction tasks](https://arxiv.org/abs/2505.11412)
*Ciaran Bench,Vivek Desai,Mohammad Moulaeifard,Nils Strodthoff,Philip Aston,Andrew Thompson*

Main category: cs.LG

TL;DR: 论文探讨了利用蒙特卡洛Dropout和改进的变分在线牛顿方法量化不确定性，以评估PPG信号分类和回归模型的可靠性，发现超参数选择对模型性能和不确定性质量有显著影响。


<details>
  <summary>Details</summary>
Motivation: PPG信号可用于非侵入性心脏健康评估，但深度网络缺乏可解释性且易过拟合，需量化不确定性以提高模型可靠性。

Method: 采用蒙特卡洛Dropout和改进的变分在线牛顿方法，评估PPG信号分类（房颤）和回归（血压）模型的不确定性。

Result: 超参数选择显著影响模型性能和不确定性质量，不同不确定性表达方式对校准效果有差异。

Conclusion: 需平衡预测性能和校准质量，超参数选择应根据不确定性表达方式优化。

Abstract: Photoplethysmography (PPG) signals encode information about relative changes
in blood volume that can be used to assess various aspects of cardiac health
non-invasively, e.g.\ to detect atrial fibrillation (AF) or predict blood
pressure (BP). Deep networks are well-equipped to handle the large quantities
of data acquired from wearable measurement devices. However, they lack
interpretability and are prone to overfitting, leaving considerable risk for
poor performance on unseen data and misdiagnosis. Here, we describe the use of
two scalable uncertainty quantification techniques: Monte Carlo Dropout and the
recently proposed Improved Variational Online Newton. These techniques are used
to assess the trustworthiness of models trained to perform AF classification
and BP regression from raw PPG time series. We find that the choice of
hyperparameters has a considerable effect on the predictive performance of the
models and on the quality and composition of predicted uncertainties. E.g. the
stochasticity of the model parameter sampling determines the proportion of the
total uncertainty that is aleatoric, and has varying effects on predictive
performance and calibration quality dependent on the chosen uncertainty
quantification technique and the chosen expression of uncertainty. We find
significant discrepancy in the quality of uncertainties over the predicted
classes, emphasising the need for a thorough evaluation protocol that assesses
local and adaptive calibration. This work suggests that the choice of
hyperparameters must be carefully tuned to balance predictive performance and
calibration quality, and that the optimal parameterisation may vary depending
on the chosen expression of uncertainty.

</details>


### [217] [MoE-CAP: Benchmarking Cost, Accuracy and Performance of Sparse Mixture-of-Experts Systems](https://arxiv.org/abs/2505.11415)
*Yinsicheng Jiang,Yao Fu,Yeqi Huang,Ping Nie,Zhan Lu,Leyang Xue,Congjie He,Man-Kit Sit,Jilong Xue,Li Dong,Ziming Miao,Dayou Du,Tairan Xu,Kai Zou,Edoardo Ponti,Luo Mai*

Main category: cs.LG

TL;DR: MoE-CAP是一个专为稀疏混合专家系统设计的基准测试，揭示了当前硬件难以平衡成本、准确性和性能的问题，并提出新的性能指标。


<details>
  <summary>Details</summary>
Motivation: 稀疏混合专家系统在扩展大型语言模型时效率高，但受异构计算和内存资源影响，现有基准测试无法准确捕捉其权衡。

Method: 引入MoE-CAP基准测试，分析系统在成本、准确性和性能上的权衡，并提出CAP雷达图和稀疏感知性能指标（S-MBU和S-MFU）。

Result: 研究发现当前硬件难以平衡CAP三个维度，通常只能优化其中两个。

Conclusion: MoE-CAP为稀疏混合专家系统提供了更准确的性能评估工具，有助于实际部署决策。

Abstract: The sparse Mixture-of-Experts (MoE) architecture is increasingly favored for
scaling Large Language Models (LLMs) efficiently, but it depends on
heterogeneous compute and memory resources. These factors jointly affect system
Cost, Accuracy, and Performance (CAP), making trade-offs inevitable. Existing
benchmarks often fail to capture these trade-offs accurately, complicating
practical deployment decisions. To address this, we introduce MoE-CAP, a
benchmark specifically designed for MoE systems. Our analysis reveals that
achieving an optimal balance across CAP is difficult with current hardware; MoE
systems typically optimize two of the three dimensions at the expense of the
third-a dynamic we term the MoE-CAP trade-off. To visualize this, we propose
the CAP Radar Diagram. We further introduce sparsity-aware performance
metrics-Sparse Memory Bandwidth Utilization (S-MBU) and Sparse Model FLOPS
Utilization (S-MFU)-to enable accurate performance benchmarking of MoE systems
across diverse hardware platforms and deployment scenarios.

</details>


### [218] [Mergenetic: a Simple Evolutionary Model Merging Library](https://arxiv.org/abs/2505.11427)
*Adrian Robert Minut,Tommaso Mencattini,Andrea Santilli,Donato Crisostomi,Emanuele Rodolà*

Main category: cs.LG

TL;DR: Mergenetic是一个开源库，支持通过进化算法合并语言模型，无需额外训练即可提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有框架缺乏支持语言模型中灵活实验进化合并策略的工具，因此开发了Mergenetic。

Method: Mergenetic结合了多种合并方法和进化算法，并采用轻量级适应度估计器降低评估成本。

Result: Mergenetic在多种任务和语言中表现出色，且仅需普通硬件支持。

Conclusion: Mergenetic为语言模型合并提供了高效、灵活的实验框架。

Abstract: Model merging allows combining the capabilities of existing models into a new
one - post hoc, without additional training. This has made it increasingly
popular thanks to its low cost and the availability of libraries that support
merging on consumer GPUs. Recent work shows that pairing merging with
evolutionary algorithms can boost performance, but no framework currently
supports flexible experimentation with such strategies in language models. We
introduce Mergenetic, an open-source library for evolutionary model merging.
Mergenetic enables easy composition of merging methods and evolutionary
algorithms while incorporating lightweight fitness estimators to reduce
evaluation costs. We describe its design and demonstrate that Mergenetic
produces competitive results across tasks and languages using modest hardware.

</details>


### [219] [MegaScale-MoE: Large-Scale Communication-Efficient Training of Mixture-of-Experts Models in Production](https://arxiv.org/abs/2505.11432)
*Chao Jin,Ziheng Jiang,Zhihao Bai,Zheng Zhong,Juncai Liu,Xiang Li,Ningxin Zheng,Xi Wang,Cong Xie,Wen Heng,Yiyuan Ma,Wenlei Bao,Size Zheng,Yanghua Peng,Haibin Lin,Xuanzhe Liu,Xin Jin,Xin Liu*

Main category: cs.LG

TL;DR: MegaScale-MoE是一个针对大规模混合专家（MoE）模型高效训练的生产系统，通过优化通信策略和计算重叠，显著提升了训练效率。


<details>
  <summary>Details</summary>
Motivation: MoE架构有望扩展大型语言模型（LLM）的规模以提升性能，但现有系统在训练效率和硬件适应性方面存在不足。

Method: MegaScale-MoE采用定制化的通信高效并行策略，优化注意力与FFN层的通信，并通过通信压缩和计算重叠提升效率。

Result: 在1,440个NVIDIA Hopper GPU上训练352B MoE模型时，吞吐量达到1.41M tokens/s，效率提升1.88倍。

Conclusion: MegaScale-MoE为MoE训练提供了高效解决方案，其系统设计经验有望推动未来MoE系统的研究。

Abstract: We present MegaScale-MoE, a production system tailored for the efficient
training of large-scale mixture-of-experts (MoE) models. MoE emerges as a
promising architecture to scale large language models (LLMs) to unprecedented
sizes, thereby enhancing model performance. However, existing MoE training
systems experience a degradation in training efficiency, exacerbated by the
escalating scale of MoE models and the continuous evolution of hardware.
  Recognizing the pivotal role of efficient communication in enhancing MoE
training, MegaScale-MoE customizes communication-efficient parallelism
strategies for attention and FFNs in each MoE layer and adopts a holistic
approach to overlap communication with computation at both inter- and
intra-operator levels. Additionally, MegaScale-MoE applies communication
compression with adjusted communication patterns to lower precision, further
improving training efficiency. When training a 352B MoE model on 1,440 NVIDIA
Hopper GPUs, MegaScale-MoE achieves a training throughput of 1.41M tokens/s,
improving the efficiency by 1.88$\times$ compared to Megatron-LM. We share our
operational experience in accelerating MoE training and hope that by offering
our insights in system design, this work will motivate future research in MoE
systems.

</details>


### [220] [A Generative Framework for Causal Estimation via Importance-Weighted Diffusion Distillation](https://arxiv.org/abs/2505.11444)
*Xinran Song,Tianyu Chen,Mingyuan Zhou*

Main category: cs.LG

TL;DR: 提出了一种结合扩散模型与重要性加权评分蒸馏的新方法IWDD，用于从观测数据中准确快速估计个体化治疗效果。


<details>
  <summary>Details</summary>
Motivation: 解决观测数据中因协变量不平衡和非随机治疗分配导致的混杂偏差问题，同时将逆概率加权（IPW）融入深度学习框架。

Method: 通过重要性加权评分蒸馏将IPW整合到预训练的扩散模型中，并引入随机化调整以减少梯度估计的方差。

Result: IWDD在样本外预测性能上达到最优，显著提升了因果估计效果。

Conclusion: IWDD为个体化治疗策略的开发提供了有效支持，代码将开源以便复现和未来研究。

Abstract: Estimating individualized treatment effects from observational data is a
central challenge in causal inference, largely due to covariate imbalance and
confounding bias from non-randomized treatment assignment. While inverse
probability weighting (IPW) is a well-established solution to this problem, its
integration into modern deep learning frameworks remains limited. In this work,
we propose Importance-Weighted Diffusion Distillation (IWDD), a novel
generative framework that combines the pretraining of diffusion models with
importance-weighted score distillation to enable accurate and fast causal
estimation-including potential outcome prediction and treatment effect
estimation. We demonstrate how IPW can be naturally incorporated into the
distillation of pretrained diffusion models, and further introduce a
randomization-based adjustment that eliminates the need to compute IPW
explicitly-thereby simplifying computation and, more importantly, provably
reducing the variance of gradient estimates. Empirical results show that IWDD
achieves state-of-the-art out-of-sample prediction performance, with the
highest win rates compared to other baselines, significantly improving causal
estimation and supporting the development of individualized treatment
strategies. We will release our PyTorch code for reproducibility and future
research.

</details>


### [221] [Signal attenuation enables scalable decentralized multi-agent reinforcement learning over networks](https://arxiv.org/abs/2505.11461)
*Wesley A Suttle,Vipul K Sharma,Brian M Sadler*

Main category: cs.LG

TL;DR: 论文提出了一种基于信号衰减特性的去中心化多智能体强化学习方法，应用于雷达网络中的功率分配问题，并开发了相应的算法和误差界限。


<details>
  <summary>Details</summary>
Motivation: 传统多智能体强化学习方法需要全局状态可观测性，限制了算法的去中心化和可扩展性。信号衰减特性为去中心化提供了可能，但实际应用尚未充分探索。

Method: 提出了两种约束多智能体马尔可夫决策过程模型，推导了全局价值函数和梯度的局部邻域近似及其误差界限，并开发了去中心化鞍点策略梯度算法。

Result: 信号衰减特性成功支持了去中心化多智能体强化学习的实现，算法在雷达网络功率分配问题中表现出有效性。

Conclusion: 该方法为无线通信和雷达网络中的其他问题提供了可扩展的去中心化解决方案模型。

Abstract: Classic multi-agent reinforcement learning (MARL) methods require that agents
enjoy global state observability, preventing development of decentralized
algorithms and limiting scalability. Recent work has shown that, under
assumptions on decaying inter-agent influence, global observability can be
replaced by local neighborhood observability at each agent, enabling
decentralization and scalability. Real-world applications enjoying such decay
properties remain underexplored, however, despite the fact that signal power
decay, or signal attenuation, due to path loss is an intrinsic feature of many
problems in wireless communications and radar networks. In this paper, we show
that signal attenuation enables decentralization in MARL by considering the
illustrative special case of performing power allocation for target detection
in a radar network. To achieve this, we propose two new constrained multi-agent
Markov decision process formulations of this power allocation problem, derive
local neighborhood approximations for global value function and gradient
estimates and establish corresponding error bounds, and develop decentralized
saddle point policy gradient algorithms for solving the proposed problems. Our
approach, though oriented towards the specific radar network problem we
consider, provides a useful model for future extensions to additional problems
in wireless communications and radar networks.

</details>


### [222] [msf-CNN: Patch-based Multi-Stage Fusion with Convolutional Neural Networks for TinyML](https://arxiv.org/abs/2505.11483)
*Zhaolan Huang,Emmanuel Baccelli*

Main category: cs.LG

TL;DR: msf-CNN是一种新颖的卷积神经网络融合技术，通过有向无环图优化数据流，显著减少内存占用并满足实时性要求。


<details>
  <summary>Details</summary>
Motivation: 解决在微控制器（MCU）上运行AI模型时内存效率低和实时性不足的问题。

Method: 提出基于补丁的融合方法msf-CNN，利用有向无环图寻找最优融合方案。

Result: msf-CNN比现有技术（MCUNetV2和StreamNet）减少50%的RAM使用。

Conclusion: msf-CNN为系统设计者提供了更高的灵活性和效率。

Abstract: AI spans from large language models to tiny models running on
microcontrollers (MCUs). Extremely memory-efficient model architectures are
decisive to fit within an MCU's tiny memory budget e.g., 128kB of RAM. However,
inference latency must remain small to fit real-time constraints. An approach
to tackle this is patch-based fusion, which aims to optimize data flows across
neural network layers. In this paper, we introduce msf-CNN, a novel technique
that efficiently finds optimal fusion settings for convolutional neural
networks (CNNs) by walking through the fusion solution space represented as a
directed acyclic graph. Compared to previous work on CNN fusion for MCUs,
msf-CNN identifies a wider set of solutions. We published an implementation of
msf-CNN running on various microcontrollers (ARM Cortex-M, RISC-V, ESP32). We
show that msf-CNN can achieve inference using 50% less RAM compared to the
prior art (MCUNetV2 and StreamNet). We thus demonstrate how msf-CNN offers
additional flexibility for system designers.

</details>


### [223] [Potential failures of physics-informed machine learning in traffic flow modeling: theoretical and experimental analysis](https://arxiv.org/abs/2505.11491)
*Yuan-Zheng Lei,Yaobang Gong,Dianwei Chen,Yao Cheng,Xianfeng Terry Yang*

Main category: cs.LG

TL;DR: 研究分析了物理信息机器学习（PIML）在交通流建模中的表现，发现其失败原因并非物理残差阻碍优化，而是需要ML与物理梯度在锥形区域内形成锐角。实验表明，物理残差可能降低LWR和ARZ模型的性能，稀疏采样和平均数据会导致误导性残差。CFL条件是数据集适用性的关键指标，高阶模型如ARZ误差下限更大。


<details>
  <summary>Details</summary>
Motivation: 探讨PIML在交通流建模中的失败原因，挑战了物理残差阻碍优化的常见假设，并揭示实际失败机制。

Method: 通过扰动训练模型沿Hessian矩阵主特征向量分析损失景观，评估ML与物理梯度的角度关系。

Result: 物理残差不直接阻碍优化，但需梯度条件满足；稀疏采样和平均数据导致误导性残差；CFL条件是关键指标；高阶模型误差下限更大。

Conclusion: PIML成功需梯度条件满足，数据集需符合CFL条件，高阶模型性能受限更大。

Abstract: This study critically examines the performance of physics-informed machine
learning (PIML) approaches for traffic flow modeling, defining the failure of a
PIML model as the scenario where it underperforms both its purely data-driven
and purely physics-based counterparts. We analyze the loss landscape by
perturbing trained models along the principal eigenvectors of the Hessian
matrix and evaluating corresponding loss values. Our results suggest that
physics residuals in PIML do not inherently hinder optimization, contrary to a
commonly assumed failure cause. Instead, successful parameter updates require
both ML and physics gradients to form acute angles with the quasi-true gradient
and lie within a conical region. Given inaccuracies in both the physics models
and the training data, satisfying this condition is often difficult.
Experiments reveal that physical residuals can degrade the performance of LWR-
and ARZ-based PIML models, especially under highly physics-driven settings.
Moreover, sparse sampling and the use of temporally averaged traffic data can
produce misleadingly small physics residuals that fail to capture actual
physical dynamics, contributing to model failure. We also identify the
Courant-Friedrichs-Lewy (CFL) condition as a key indicator of dataset
suitability for PIML, where successful applications consistently adhere to this
criterion. Lastly, we observe that higher-order models like ARZ tend to have
larger error lower bounds than lower-order models like LWR, which is consistent
with the experimental findings of existing studies.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [224] [Agent Name Service (ANS): A Universal Directory for Secure AI Agent Discovery and Interoperability](https://arxiv.org/abs/2505.10609)
*Ken Huang,Vineeth Sai Narajala,Idan Habler,Akram Sheriff*

Main category: cs.CR

TL;DR: 本文提出了一种基于DNS的新型架构Agent Name Service（ANS），用于解决AI代理的安全发现问题，通过PKI证书实现可验证的身份与信任。


<details>
  <summary>Details</summary>
Motivation: 随着AI代理的普及，缺乏公共代理发现框架的问题日益突出，需要一种安全、可扩展的解决方案。

Method: ANS采用协议无关的注册基础设施，支持多种通信标准，并定义了安全的解析算法。

Result: ANS实现了基于JSON Schema的结构化通信，并通过全面的威胁分析验证了其安全性。

Conclusion: ANS为多代理系统中的安全发现与交互提供了基础性目录服务，支持未来可互操作、可信且可扩展的代理生态系统。

Abstract: The proliferation of AI agents requires robust mechanisms for secure
discovery. This paper introduces the Agent Name Service (ANS), a novel
architecture based on DNS addressing the lack of a public agent discovery
framework. ANS provides a protocol-agnostic registry infrastructure that
leverages Public Key Infrastructure (PKI) certificates for verifiable agent
identity and trust. The architecture features several key innovations: a
formalized agent registration and renewal mechanism for lifecycle management;
DNS-inspired naming conventions with capability-aware resolution; a modular
Protocol Adapter Layer supporting diverse communication standards (A2A, MCP,
ACP etc.); and precisely defined algorithms for secure resolution. We implement
structured communication using JSON Schema and conduct a comprehensive threat
analysis of our proposal. The result is a foundational directory service
addressing the core challenges of secured discovery and interaction in
multi-agent systems, paving the way for future interoperable, trustworthy, and
scalable agent ecosystems.

</details>


### [225] [MPMA: Preference Manipulation Attack Against Model Context Protocol](https://arxiv.org/abs/2505.11154)
*Zihan Wang,Hongwei Li,Rui Zhang,Yu Liu,Wenbo Jiang,Wenshu Fan,Qingchuan Zhao,Guowen Xu*

Main category: cs.CR

TL;DR: 论文提出了一种针对MCP协议的新型安全威胁MPMA，通过操纵LLM的偏好实现攻击，并设计了两种攻击方法DPMA和GAPMA，后者结合遗传算法提高了隐蔽性。


<details>
  <summary>Details</summary>
Motivation: 随着MCP协议的广泛应用，第三方定制化MCP服务器暴露了潜在的安全漏洞，可能导致经济利益的损失。

Method: 设计了两种攻击方法：直接偏好操纵攻击（DPMA）和基于遗传算法的广告偏好操纵攻击（GAPMA），后者通过遗传算法优化隐蔽性。

Result: 实验表明GAPMA在高效性和隐蔽性之间取得了平衡。

Conclusion: 研究揭示了MCP在开放生态系统中的关键漏洞，强调了构建防御机制以确保MCP生态系统公平性的紧迫性。

Abstract: Model Context Protocol (MCP) standardizes interface mapping for large
language models (LLMs) to access external data and tools, which revolutionizes
the paradigm of tool selection and facilitates the rapid expansion of the LLM
agent tool ecosystem. However, as the MCP is increasingly adopted, third-party
customized versions of the MCP server expose potential security
vulnerabilities. In this paper, we first introduce a novel security threat,
which we term the MCP Preference Manipulation Attack (MPMA). An attacker
deploys a customized MCP server to manipulate LLMs, causing them to prioritize
it over other competing MCP servers. This can result in economic benefits for
attackers, such as revenue from paid MCP services or advertising income
generated from free servers. To achieve MPMA, we first design a Direct
Preference Manipulation Attack ($\mathtt{DPMA}$) that achieves significant
effectiveness by inserting the manipulative word and phrases into the tool name
and description. However, such a direct modification is obvious to users and
lacks stealthiness. To address these limitations, we further propose
Genetic-based Advertising Preference Manipulation Attack ($\mathtt{GAPMA}$).
$\mathtt{GAPMA}$ employs four commonly used strategies to initialize
descriptions and integrates a Genetic Algorithm (GA) to enhance stealthiness.
The experiment results demonstrate that $\mathtt{GAPMA}$ balances high
effectiveness and stealthiness. Our study reveals a critical vulnerability of
the MCP in open ecosystems, highlighting an urgent need for robust defense
mechanisms to ensure the fairness of the MCP ecosystem.

</details>


### [226] [Automating Security Audit Using Large Language Model based Agent: An Exploration Experiment](https://arxiv.org/abs/2505.10732)
*Jia Hui Chin,Pu Zhang,Yu Xin Cheong,Jonathan Pan*

Main category: cs.CR

TL;DR: 论文探讨了利用大型语言模型（LLM）作为自主代理执行部分安全审计任务（如Windows密码策略合规性审计）的可行性，实验表明其效率高于传统手动审计。


<details>
  <summary>Details</summary>
Motivation: 当前数字环境下，企业面临持续的安全压力，传统审计耗时耗力，亟需自动化解决方案。

Method: 通过GPT-4与Langchain结合，构建框架执行密码策略合规性审计任务。

Result: 实验显示代理能准确标记密码策略违规，效率高于手动审计。

Conclusion: 尽管在复杂动态环境中可能存在局限性，但框架可扩展至实时威胁监控和合规检查。

Abstract: In the current rapidly changing digital environment, businesses are under
constant stress to ensure that their systems are secured. Security audits help
to maintain a strong security posture by ensuring that policies are in place,
controls are implemented, gaps are identified for cybersecurity risks
mitigation. However, audits are usually manual, requiring much time and costs.
This paper looks at the possibility of developing a framework to leverage Large
Language Models (LLMs) as an autonomous agent to execute part of the security
audit, namely with the field audit. password policy compliance for Windows
operating system. Through the conduct of an exploration experiment of using
GPT-4 with Langchain, the agent executed the audit tasks by accurately flagging
password policy violations and appeared to be more efficient than traditional
manual audits. Despite its potential limitations in operational consistency in
complex and dynamic environment, the framework suggests possibilities to extend
further to real-time threat monitoring and compliance checks.

</details>


### [227] [Neural-Inspired Advances in Integral Cryptanalysis](https://arxiv.org/abs/2505.10790)
*Liu Zhang,Yiran Yao,Danping Shi,Dongchen Chai,Jian Guo,Zilong Wang*

Main category: cs.CR

TL;DR: 该研究利用神经网络学习积分特性相关特征，优化搜索框架，提升密码分析效果，发现更多轮数的密钥恢复攻击。


<details>
  <summary>Details</summary>
Motivation: 受Gohr等人在CRYPTO 2019的研究启发，探索神经网络在密码分析中未充分利用的特征，以改进传统方法。

Method: 采用神经网络学习积分特性特征，开发中间相遇搜索框架，平衡精度与计算效率。

Result: 减少SKINNY64/64的11轮积分区分器所需活跃明文比特数，发现12轮密钥依赖积分区分器；提升SKINNYn/n的密钥恢复攻击轮数。

Conclusion: 神经网络为密码分析提供了新视角，显著优化了积分区分器和密钥恢复攻击的效果。

Abstract: The study by Gohr et.al at CRYPTO 2019 and sunsequent related works have
shown that neural networks can uncover previously unused features, offering
novel insights into cryptanalysis. Motivated by these findings, we employ
neural networks to learn features specifically related to integral properties
and integrate the corresponding insights into optimized search frameworks.
These findings validate the framework of using neural networks for feature
exploration, providing researchers with novel insights that advance established
cryptanalysis methods.
  Neural networks have inspired the development of more precise integral search
models. By comparing the integral distinguishers obtained via neural networks
with those identified by classical methods, we observe that existing automated
search models often fail to find optimal distinguishers. To address this issue,
we develop a meet in the middle search framework that balances model accuracy
and computational efficiency. As a result, we reduce the number of active
plaintext bits required for an 11 rounds integral distinguisher on SKINNY64/64,
and further identify a 12 rounds key dependent integral distinguisher achieving
one additional round over the previous best-known result.
  The integral distinguishers discovered by neural networks enable key recovery
attacks on more rounds. We identify a 7 rounds key independent integral
distinguisher from neural networks with even only one active plaintext cell,
which is based on linear combinations of bits. This distinguisher enables a 15
rounds key recovery attack on SKINNYn/n, improving upon the previous record by
one round. Additionally, we discover an 8 rounds key dependent integral
distinguisher using neural network that further reduces the time complexity of
key recovery attacks against SKINNY.

</details>


### [228] [Optimal Allocation of Privacy Budget on Hierarchical Data Release](https://arxiv.org/abs/2505.10871)
*Joonhyuk Ko,Juba Ziani,Ferdinando Fioretto*

Main category: cs.CR

TL;DR: 本文提出了一种优化隐私预算分配的方法，用于在发布分层数据时平衡数据效用和隐私保护。


<details>
  <summary>Details</summary>
Motivation: 在分层数据发布中，隐私预算分配不当会导致数据效用低下或隐私保护不足，因此需要一种优化方法。

Method: 将问题建模为约束优化问题，最大化数据效用，同时考虑数据粒度和隐私损失之间的权衡。

Result: 实验表明，优化隐私预算分配显著提高了发布数据的效用和下游任务的性能。

Conclusion: 本文方法为分层数据发布中的隐私预算分配提供了有效的解决方案。

Abstract: Releasing useful information from datasets with hierarchical structures while
preserving individual privacy presents a significant challenge. Standard
privacy-preserving mechanisms, and in particular Differential Privacy, often
require careful allocation of a finite privacy budget across different levels
and components of the hierarchy. Sub-optimal allocation can lead to either
excessive noise, rendering the data useless, or to insufficient protections for
sensitive information. This paper addresses the critical problem of optimal
privacy budget allocation for hierarchical data release. It formulates this
challenge as a constrained optimization problem, aiming to maximize data
utility subject to a total privacy budget while considering the inherent
trade-offs between data granularity and privacy loss. The proposed approach is
supported by theoretical analysis and validated through comprehensive
experiments on real hierarchical datasets. These experiments demonstrate that
optimal privacy budget allocation significantly enhances the utility of the
released data and improves the performance of downstream tasks.

</details>


### [229] [On the Security Risks of ML-based Malware Detection Systems: A Survey](https://arxiv.org/abs/2505.10903)
*Ping He,Yuhao Mao,Changjiang Li,Lorenzo Cavallaro,Ting Wang,Shouling Ji*

Main category: cs.CR

TL;DR: 本文通过CIA原则定义安全风险范围，提出基于阶段的分类法，分析ML-based MD系统的攻击与防御技术，并通过案例研究提供新见解与未来方向。


<details>
  <summary>Details</summary>
Motivation: 现有研究多集中于对抗性恶意软件样本，缺乏对ML-based MD系统实际安全风险的全面分析。

Method: 利用CIA原则定义安全风险范围，将系统分解为不同操作阶段并构建分类法，总结技术进展与防御方案。

Result: 通过阶段分类法进行案例研究，提供新的实证见解，并识别攻击与防御的差距。

Conclusion: 建议从阶段内和阶段间的角度探索未来研究方向，以提升ML-based MD系统的安全性。

Abstract: Malware presents a persistent threat to user privacy and data integrity. To
combat this, machine learning-based (ML-based) malware detection (MD) systems
have been developed. However, these systems have increasingly been attacked in
recent years, undermining their effectiveness in practice. While the security
risks associated with ML-based MD systems have garnered considerable attention,
the majority of prior works is limited to adversarial malware examples, lacking
a comprehensive analysis of practical security risks. This paper addresses this
gap by utilizing the CIA principles to define the scope of security risks. We
then deconstruct ML-based MD systems into distinct operational stages, thus
developing a stage-based taxonomy. Utilizing this taxonomy, we summarize the
technical progress and discuss the gaps in the attack and defense proposals
related to the ML-based MD systems within each stage. Subsequently, we conduct
two case studies, using both inter-stage and intra-stage analyses according to
the stage-based taxonomy to provide new empirical insights. Based on these
analyses and insights, we suggest potential future directions from both
inter-stage and intra-stage perspectives.

</details>


### [230] [Nosy Layers, Noisy Fixes: Tackling DRAs in Federated Learning Systems using Explainable AI](https://arxiv.org/abs/2505.10942)
*Meghali Nandi,Arash Shaghaghi,Nazatul Haque Sultan,Gustavo Batista,Raymond K. Zhao,Sanjay Jha*

Main category: cs.CR

TL;DR: DRArmor是一种针对联邦学习中的数据重建攻击（DRA）的新型防御机制，通过结合可解释AI和针对性检测与缓解策略，有效保护用户数据隐私。


<details>
  <summary>Details</summary>
Motivation: 联邦学习（FL）虽然保护了数据的分散性和隐私性，但仍面临数据重建攻击（DRA）的威胁，如LoKI和Robbing the Fed。现有的防御方法通常针对整个模型，效率较低。

Method: DRArmor通过分析模型层的输出贡献和梯度值不一致性，识别恶意层，并针对这些层应用噪声注入、像素化和剪枝等防御技术，而非整个模型。

Result: 在200客户端的FL设置中，DRArmor对MNIST、CIFAR-10、CIFAR-100和ImageNet等数据集进行测试，实现了0.910的真阳性率和0.890的真阴性率，平均准确率为87%，数据泄漏率降低了62.5%。

Conclusion: DRArmor在保护用户隐私的同时保持了模型性能，相比现有防御机制更高效。

Abstract: Federated Learning (FL) has emerged as a powerful paradigm for collaborative
model training while keeping client data decentralized and private. However, it
is vulnerable to Data Reconstruction Attacks (DRA) such as "LoKI" and "Robbing
the Fed", where malicious models sent from the server to the client can
reconstruct sensitive user data. To counter this, we introduce DRArmor, a novel
defense mechanism that integrates Explainable AI with targeted detection and
mitigation strategies for DRA. Unlike existing defenses that focus on the
entire model, DRArmor identifies and addresses the root cause (i.e., malicious
layers within the model that send gradients with malicious intent) by analyzing
their contribution to the output and detecting inconsistencies in gradient
values. Once these malicious layers are identified, DRArmor applies defense
techniques such as noise injection, pixelation, and pruning to these layers
rather than the whole model, minimizing the attack surface and preserving
client data privacy. We evaluate DRArmor's performance against the advanced
LoKI attack across diverse datasets, including MNIST, CIFAR-10, CIFAR-100, and
ImageNet, in a 200-client FL setup. Our results demonstrate DRArmor's
effectiveness in mitigating data leakage, achieving high True Positive and True
Negative Rates of 0.910 and 0.890, respectively. Additionally, DRArmor
maintains an average accuracy of 87%, effectively protecting client privacy
without compromising model performance. Compared to existing defense
mechanisms, DRArmor reduces the data leakage rate by 62.5% with datasets
containing 500 samples per client.

</details>


### [231] [LLMs unlock new paths to monetizing exploits](https://arxiv.org/abs/2505.11449)
*Nicholas Carlini,Milad Nasr,Edoardo Debenedetti,Barry Wang,Christopher A. Choquette-Choo,Daphne Ippolito,Florian Tramèr,Matthew Jagielski*

Main category: cs.CR

TL;DR: 论文探讨了大型语言模型（LLMs）将如何改变网络攻击的经济学，使其能够针对个体用户定制攻击，并提出需要新的防御策略。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs如何使网络攻击更高效、个性化，从而改变攻击的经济模式，强调防御策略的紧迫性。

Method: 通过分析LLMs在攻击中的潜在应用，如自动发现漏洞和定制勒索软件，并结合实际案例（如Enron邮件数据集）验证其可行性。

Result: LLMs能够自动发现敏感信息并定制攻击，部分攻击虽目前成本较高，但随着LLMs成本下降，其威胁将增加。

Conclusion: LLMs将推动网络攻击的个性化与规模化，需要新的深度防御策略应对未来威胁。

Abstract: We argue that Large language models (LLMs) will soon alter the economics of
cyberattacks. Instead of attacking the most commonly used software and
monetizing exploits by targeting the lowest common denominator among victims,
LLMs enable adversaries to launch tailored attacks on a user-by-user basis. On
the exploitation front, instead of human attackers manually searching for one
difficult-to-identify bug in a product with millions of users, LLMs can find
thousands of easy-to-identify bugs in products with thousands of users. And on
the monetization front, instead of generic ransomware that always performs the
same attack (encrypt all your data and request payment to decrypt), an
LLM-driven ransomware attack could tailor the ransom demand based on the
particular content of each exploited device.
  We show that these two attacks (and several others) are imminently practical
using state-of-the-art LLMs. For example, we show that without any human
intervention, an LLM finds highly sensitive personal information in the Enron
email dataset (e.g., an executive having an affair with another employee) that
could be used for blackmail. While some of our attacks are still too expensive
to scale widely today, the incentives to implement these attacks will only
increase as LLMs get cheaper. Thus, we argue that LLMs create a need for new
defense-in-depth approaches.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [232] [Understanding Gen Alpha Digital Language: Evaluation of LLM Safety Systems for Content Moderation](https://arxiv.org/abs/2505.10588)
*Manisha Mehta,Fausto Giunchiglia*

Main category: cs.CY

TL;DR: 研究评估了AI系统对Alpha世代（2010-2024年出生）数字语言的理解能力，发现现有安全工具难以识别其独特的沟通方式，导致在线风险增加。


<details>
  <summary>Details</summary>
Motivation: Alpha世代是首个与AI共同成长的群体，其独特的数字语言（如游戏、梗和AI趋势）可能导致有害互动被隐藏，现有安全工具无法有效保护。

Method: 评估了四种领先AI模型（GPT-4、Claude、Gemini和Llama 3）在检测Alpha世代语言中隐藏的骚扰和操纵行为的能力，使用了100个来自游戏平台、社交媒体和视频内容的表达样本。

Result: 研究发现AI系统在理解Alpha世代语言时存在显著缺陷，直接影响在线安全。

Conclusion: 研究强调了重新设计安全系统的紧迫性，以适应年轻群体的沟通方式，并提出了改进框架和多视角评估方法。

Abstract: This research offers a unique evaluation of how AI systems interpret the
digital language of Generation Alpha (Gen Alpha, born 2010-2024). As the first
cohort raised alongside AI, Gen Alpha faces new forms of online risk due to
immersive digital engagement and a growing mismatch between their evolving
communication and existing safety tools. Their distinct language, shaped by
gaming, memes, and AI-driven trends, often conceals harmful interactions from
both human moderators and automated systems. We assess four leading AI models
(GPT-4, Claude, Gemini, and Llama 3) on their ability to detect masked
harassment and manipulation within Gen Alpha discourse. Using a dataset of 100
recent expressions from gaming platforms, social media, and video content, the
study reveals critical comprehension failures with direct implications for
online safety. This work contributes: (1) a first-of-its-kind dataset capturing
Gen Alpha expressions; (2) a framework to improve AI moderation systems for
youth protection; (3) a multi-perspective evaluation including AI systems,
human moderators, and parents, with direct input from Gen Alpha co-researchers;
and (4) an analysis of how linguistic divergence increases youth vulnerability.
Findings highlight the urgent need to redesign safety systems attuned to youth
communication, especially given Gen Alpha reluctance to seek help when adults
fail to understand their digital world. This study combines the insight of a
Gen Alpha researcher with systematic academic analysis to address critical
digital safety challenges.

</details>


### [233] [Anchoring AI Capabilities in Market Valuations: The Capability Realization Rate Model and Valuation Misalignment Risk](https://arxiv.org/abs/2505.10590)
*Xinmin Fang,Lingfeng Tao,Zhengxiong Li*

Main category: cs.CY

TL;DR: 论文研究了AI能力对股票估值的锚定效应，提出能力实现率（CRR）模型量化AI潜力与实际表现的差距，分析了2023-2025年生成式AI热潮中的估值溢价与错配。


<details>
  <summary>Details</summary>
Motivation: 探讨AI能力如何影响公司估值，揭示市场对AI潜力的过度预期与实际表现之间的差距。

Method: 使用CRR模型量化AI潜力与实现表现的差距，结合2023-2025年生成式AI热潮数据，分析行业敏感性和案例研究（如OpenAI、Adobe等）。

Result: AI原生公司因未来潜力获得高估值溢价，而传统公司需证明实际回报才能重新评级。CRR可识别估值错配风险。

Conclusion: 建议提高透明度、减少投机泡沫，使AI创新与可持续市场价值对齐。

Abstract: Recent breakthroughs in artificial intelligence (AI) have triggered surges in
market valuations for AI-related companies, often outpacing the realization of
underlying capabilities. We examine the anchoring effect of AI capabilities on
equity valuations and propose a Capability Realization Rate (CRR) model to
quantify the gap between AI potential and realized performance. Using data from
the 2023--2025 generative AI boom, we analyze sector-level sensitivity and
conduct case studies (OpenAI, Adobe, NVIDIA, Meta, Microsoft, Goldman Sachs) to
illustrate patterns of valuation premium and misalignment. Our findings
indicate that AI-native firms commanded outsized valuation premiums anchored to
future potential, while traditional companies integrating AI experienced
re-ratings subject to proof of tangible returns. We argue that CRR can help
identify valuation misalignment risk-where market prices diverge from realized
AI-driven value. We conclude with policy recommendations to improve
transparency, mitigate speculative bubbles, and align AI innovation with
sustainable market value.

</details>


### [234] [Inclusivity of AI Speech in Healthcare: A Decade Look Back](https://arxiv.org/abs/2505.10596)
*Retno Larasati*

Main category: cs.CY

TL;DR: AI语音识别技术在医疗中的应用潜力巨大，但研究发现数据集和研究偏向高资源语言、标准化口音和特定人群，可能导致医疗不平等。


<details>
  <summary>Details</summary>
Motivation: 揭示AI语音识别技术在医疗中的偏见问题，强调其对边缘群体的潜在负面影响。

Method: 通过分析现有数据集和研究，识别偏见和不平等现象。

Result: 发现数据集和研究偏向高资源语言和特定人群，可能导致AI系统对边缘群体语音的误判。

Conclusion: 呼吁设计包容性数据集、开展偏见缓解研究并制定政策框架，以确保AI语音技术在医疗中的公平应用。

Abstract: The integration of AI speech recognition technologies into healthcare has the
potential to revolutionize clinical workflows and patient-provider
communication. However, this study reveals significant gaps in inclusivity,
with datasets and research disproportionately favouring high-resource
languages, standardized accents, and narrow demographic groups. These biases
risk perpetuating healthcare disparities, as AI systems may misinterpret speech
from marginalized groups. This paper highlights the urgent need for inclusive
dataset design, bias mitigation research, and policy frameworks to ensure
equitable access to AI speech technologies in healthcare.

</details>


### [235] [Toward a Public and Secure Generative AI: A Comparative Analysis of Open and Closed LLMs](https://arxiv.org/abs/2505.10603)
*Jorge Machado*

Main category: cs.CY

TL;DR: 该研究比较了开源与专有生成式AI系统的优缺点，并提出了一个开放、公共和安全的生成式AI框架。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏对开源与专有生成式AI系统的全面比较研究，需评估其风险与挑战。

Method: 采用文献综述、批判性分析和比较分析相结合的方法。

Result: 开源模型更具透明度和灵活性，而专有系统技术支持更优但缺乏问责制。

Conclusion: 多利益相关方治理、环境可持续性和监管框架对负责任发展至关重要。

Abstract: Generative artificial intelligence (Gen AI) systems represent a critical
technology with far-reaching implications across multiple domains of society.
However, their deployment entails a range of risks and challenges that require
careful evaluation. To date, there has been a lack of comprehensive,
interdisciplinary studies offering a systematic comparison between open-source
and proprietary (closed) generative AI systems, particularly regarding their
respective advantages and drawbacks. This study aims to: i) critically evaluate
and compare the characteristics, opportunities, and challenges of open and
closed generative AI models; and ii) propose foundational elements for the
development of an Open, Public, and Safe Gen AI framework. As a methodology, we
adopted a combined approach that integrates three methods: literature review,
critical analysis, and comparative analysis. The proposed framework outlines
key dimensions, openness, public governance, and security, as essential pillars
for shaping the future of trustworthy and inclusive Gen AI. Our findings reveal
that open models offer greater transparency, auditability, and flexibility,
enabling independent scrutiny and bias mitigation. In contrast, closed systems
often provide better technical support and ease of implementation, but at the
cost of unequal access, accountability, and ethical oversight. The research
also highlights the importance of multi-stakeholder governance, environmental
sustainability, and regulatory frameworks in ensuring responsible development.

</details>


### [236] [Towards Automated Situation Awareness: A RAG-Based Framework for Peacebuilding Reports](https://arxiv.org/abs/2505.10586)
*Poli A. Nemkova,Suleyman O. Polat,Rafid I. Jahan,Sagnik Ray Choudhury,Sun-joo Lee,Shouryadipta Sarkar,Mark V. Albert*

Main category: cs.CY

TL;DR: 本文提出了一种动态检索增强生成（RAG）系统，用于自动生成实时情境感知报告，通过多源数据整合和三级评估框架确保报告的及时性、相关性和准确性。


<details>
  <summary>Details</summary>
Motivation: 在应急响应、冲突监测和早期预警中，及时准确的情境感知对决策至关重要，但手动分析大量异构数据常导致延迟。

Method: 系统动态构建查询特定知识库，整合新闻、冲突事件数据库和经济指标等实时数据，并通过三级评估框架（自动NLP指标、专家反馈和LLM评估）确保报告质量。

Result: 系统在多个实际场景中测试，证明能生成连贯、有洞察力且可操作的报告，减轻人工分析负担并加速决策。

Conclusion: 通过自动化报告生成，该方法显著提升了情境感知效率，并开源代码和工具以促进研究。

Abstract: Timely and accurate situation awareness is vital for decision-making in
humanitarian response, conflict monitoring, and early warning and early action.
However, the manual analysis of vast and heterogeneous data sources often
results in delays, limiting the effectiveness of interventions. This paper
introduces a dynamic Retrieval-Augmented Generation (RAG) system that
autonomously generates situation awareness reports by integrating real-time
data from diverse sources, including news articles, conflict event databases,
and economic indicators. Our system constructs query-specific knowledge bases
on demand, ensuring timely, relevant, and accurate insights.
  To ensure the quality of generated reports, we propose a three-level
evaluation framework that combines semantic similarity metrics, factual
consistency checks, and expert feedback. The first level employs automated NLP
metrics to assess coherence and factual accuracy. The second level involves
human expert evaluation to verify the relevance and completeness of the
reports. The third level utilizes LLM-as-a-Judge, where large language models
provide an additional layer of assessment to ensure robustness. The system is
tested across multiple real-world scenarios, demonstrating its effectiveness in
producing coherent, insightful, and actionable reports. By automating report
generation, our approach reduces the burden on human analysts and accelerates
decision-making processes. To promote reproducibility and further research, we
openly share our code and evaluation tools with the community via GitHub.

</details>


### [237] [Towards an LLM-powered Social Digital Twinning Platform](https://arxiv.org/abs/2505.10681)
*Önder Gürcan,Vanja Falck,Markus G. Rousseau,Larissa L. Lima*

Main category: cs.CY

TL;DR: Social Digital Twinner 是一个创新的社交模拟工具，用于探索复杂自适应社交系统中假设场景的潜在影响。


<details>
  <summary>Details</summary>
Motivation: 旨在通过数据驱动和基于证据的方法解决社会问题，促进利益相关者协作设计和测试干预措施。

Method: 工具架构包括数据基础设施（真实数据和合成人口）、基于LLM的代理模拟引擎和自然语言交互界面。

Result: 展示了工具在解决挪威Kragero青少年辍学问题中的交互能力，能够创建和执行社交数字孪生。

Conclusion: 该工具为数据驱动的社会问题解决提供了创新支持，展示了其实际应用潜力。

Abstract: We present Social Digital Twinner, an innovative social simulation tool for
exploring plausible effects of what-if scenarios in complex adaptive social
systems. The architecture is composed of three seamlessly integrated parts: a
data infrastructure featuring real-world data and a multi-dimensionally
representative synthetic population of citizens, an LLM-enabled agent-based
simulation engine, and a user interface that enable intuitive, natural language
interactions with the simulation engine and the artificial agents (i.e.
citizens). Social Digital Twinner facilitates real-time engagement and empowers
stakeholders to collaboratively design, test, and refine intervention measures.
The approach is promoting a data-driven and evidence-based approach to societal
problem-solving. We demonstrate the tool's interactive capabilities by
addressing the critical issue of youth school dropouts in Kragero, Norway,
showcasing its ability to create and execute a dedicated social digital twin
using natural language.

</details>


### [238] [ChestyBot: Detecting and Disrupting Chinese Communist Party Influence Stratagems](https://arxiv.org/abs/2505.10746)
*Matthew Stoffolano,Ayush Rout,Justin M. Pelletier*

Main category: cs.CY

TL;DR: ChestyBot是一种基于语用学的语言模型，能实时检测未标记的外国恶意影响推文，准确率高达98.34%。


<details>
  <summary>Details</summary>
Motivation: 俄罗斯和中国等外国信息操作利用美国宽松的信息环境，威胁民主制度和威斯特伐利亚体系，现有检测策略难以实时识别。

Method: 提出ChestyBot，一种基于语用学的语言模型。

Result: 模型检测未标记恶意影响推文的准确率达98.34%。

Conclusion: ChestyBot支持新框架，可在外国影响操作形成阶段进行干扰。

Abstract: Foreign information operations conducted by Russian and Chinese actors
exploit the United States' permissive information environment. These campaigns
threaten democratic institutions and the broader Westphalian model. Yet,
existing detection and mitigation strategies often fail to identify active
information campaigns in real time. This paper introduces ChestyBot, a
pragmatics-based language model that detects unlabeled foreign malign influence
tweets with up to 98.34% accuracy. The model supports a novel framework to
disrupt foreign influence operations in their formative stages.

</details>


### [239] [Phare: A Safety Probe for Large Language Models](https://arxiv.org/abs/2505.11365)
*Pierre Le Jeune,Benoît Malésieux,Weixuan Xiao,Matteo Dora*

Main category: cs.CY

TL;DR: Phare是一个多语言诊断框架，用于评估大型语言模型（LLMs）在幻觉与可靠性、社会偏见和有害内容生成三个关键维度的行为。研究发现17个先进LLMs存在系统性漏洞，如奉承、提示敏感性和刻板印象再现。


<details>
  <summary>Details</summary>
Motivation: 现有评估多关注性能而非失败模式，Phare旨在填补这一空白，为构建更稳健、对齐且可信的语言系统提供可操作见解。

Method: 通过Phare框架对17个先进LLMs进行多维度（幻觉、社会偏见、有害内容）的评估。

Result: 发现LLMs在多个安全维度上存在系统性漏洞，包括奉承、提示敏感性和刻板印象再现。

Conclusion: Phare通过揭示具体失败模式而非简单排名，为研究人员和从业者提供了改进LLMs安全性的实用指导。

Abstract: Ensuring the safety of large language models (LLMs) is critical for
responsible deployment, yet existing evaluations often prioritize performance
over identifying failure modes. We introduce Phare, a multilingual diagnostic
framework to probe and evaluate LLM behavior across three critical dimensions:
hallucination and reliability, social biases, and harmful content generation.
Our evaluation of 17 state-of-the-art LLMs reveals patterns of systematic
vulnerabilities across all safety dimensions, including sycophancy, prompt
sensitivity, and stereotype reproduction. By highlighting these specific
failure modes rather than simply ranking models, Phare provides researchers and
practitioners with actionable insights to build more robust, aligned, and
trustworthy language systems.

</details>


### [240] [Analyzing Patterns and Influence of Advertising in Print Newspapers](https://arxiv.org/abs/2505.10791)
*N Harsha Vardhan,Ponnurangam Kumaraguru,Kiran Garimella*

Main category: cs.CY

TL;DR: 本文通过图像处理和OCR技术，从印度五种主要报纸中提取广告数据，分析了印刷广告的模式及其对媒体报道的影响。


<details>
  <summary>Details</summary>
Motivation: 研究印度印刷报纸中的广告实践，揭示广告与媒体报道之间的关系。

Method: 采用图像处理和OCR技术构建数据管道，分析12,000多期报纸中的广告数据。

Result: 发现印刷广告在过去六年保持稳定，政府广告贡献了不成比例的收益，且广告量与媒体报道的正面性和数量显著相关。

Conclusion: 广告与媒体报道之间存在明确的正相关关系，尤其是对企业广告主而言。

Abstract: This paper investigates advertising practices in print newspapers across
India using a novel data-driven approach. We develop a pipeline employing image
processing and OCR techniques to extract articles and advertisements from
digital versions of print newspapers with high accuracy. Applying this
methodology to five popular newspapers that span multiple regions and three
languages, English, Hindi, and Telugu, we assembled a dataset of more than
12,000 editions containing several hundred thousand advertisements.
Collectively, these newspapers reach a readership of over 100 million people.
Using this extensive dataset, we conduct a comprehensive analysis to answer key
questions about print advertising: who advertises, what they advertise, when
they advertise, where they place their ads, and how they advertise. Our
findings reveal significant patterns, including the consistent level of print
advertising over the past six years despite declining print circulation, the
overrepresentation of company ads on prominent pages, and the disproportionate
revenue contributed by government ads. Furthermore, we examine whether
advertising in a newspaper influences the coverage an advertiser receives.
Through regression analyses on coverage volume and sentiment, we find strong
evidence supporting this hypothesis for corporate advertisers. The results
indicate a clear trend where increased advertising correlates with more
favorable and extensive media coverage, a relationship that remains robust over
time and across different levels of advertiser popularity.

</details>


### [241] [The heteronomy of algorithms: Traditional knowledge and computational knowledge](https://arxiv.org/abs/2505.11030)
*David M. Berry*

Main category: cs.CY

TL;DR: 论文主张通过数字启蒙（digital Bildung）教授批判计算的原则，以应对算法对公民自主性的影响，并呼吁跨学科研究来理解计算系统的社会渗透。


<details>
  <summary>Details</summary>
Motivation: 计算系统和算法日益影响公民社会，但当前的认知偏向功利主义而非对智识的关怀，亟需提升对计算的批判性理解。

Method: 提出结合哲学、政治学、历史学、人类学、社会学、媒体研究、计算机科学和人文学科的跨学科方法。

Result: 强调需要发展批判性研究项目，以分析计算系统如何在社会中具体化和规范化。

Conclusion: 呼吁通过数字启蒙和跨学科研究，应对计算对公共理性和智识领域的挑战。

Abstract: If an active citizen should increasingly be a computationally enlightened
one, replacing the autonomy of reason with the heteronomy of algorithms, then I
argue in this article that we must begin teaching the principles of critiquing
the computal through new notions of what we might call digital Bildung. Indeed,
if civil society itself is mediated by computational systems and media, the
public use of reason must also be complemented by skills for negotiating and
using these computal forms to articulate such critique. Not only is there a
need to raise the intellectual tone regarding computation and its related
softwarization processes, but there is an urgent need to attend to the likely
epistemic challenges from computation which, as presently constituted, tends
towards justification through a philosophy of utility rather than through a
philosophy of care for the territory of the intellect. We therefore need to
develop an approach to this field that uses concepts and methods drawn from
philosophy, politics, history, anthropology, sociology, media studies, computer
science, and the humanities more generally, to try to understand these issues -
particularly the way in which software and data increasingly penetrate our
everyday life and the pressures and fissures that are created. We must, in
other words, move to undertake a critical interdisciplinary research program to
understand the way in which these systems are created, instantiated, and
normatively engendered in both specific and general contexts.

</details>


### [242] [Measurement to Meaning: A Validity-Centered Framework for AI Evaluation](https://arxiv.org/abs/2505.10573)
*Olawale Salaudeen,Anka Reuel,Ahmed Ahmed,Suhana Bedi,Zachary Robertson,Sudharsan Sundar,Ben Domingue,Angelina Wang,Sanmi Koyejo*

Main category: cs.CY

TL;DR: 论文提出了一种结构化方法，用于评估AI系统的能力，避免因狭隘基准测试导致的误导性结论。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统的评估标准滞后于其能力发展，狭隘的基准测试可能无法真实反映模型的通用推理能力。

Method: 通过借鉴心理测量学中的有效性分解，构建了一个框架，帮助明确评估证据与所声称能力之间的关系。

Result: 框架通过视觉和语言模型的案例研究验证，强化了评估证据与声称能力之间的联系。

Conclusion: 该框架为AI系统评估提供了更严谨的方法，有助于改进实证效用和决策效果。

Abstract: While the capabilities and utility of AI systems have advanced, rigorous
norms for evaluating these systems have lagged. Grand claims, such as models
achieving general reasoning capabilities, are supported with model performance
on narrow benchmarks, like performance on graduate-level exam questions, which
provide a limited and potentially misleading assessment. We provide a
structured approach for reasoning about the types of evaluative claims that can
be made given the available evidence. For instance, our framework helps
determine whether performance on a mathematical benchmark is an indication of
the ability to solve problems on math tests or instead indicates a broader
ability to reason. Our framework is well-suited for the contemporary paradigm
in machine learning, where various stakeholders provide measurements and
evaluations that downstream users use to validate their claims and decisions.
At the same time, our framework also informs the construction of evaluations
designed to speak to the validity of the relevant claims. By leveraging
psychometrics' breakdown of validity, evaluations can prioritize the most
critical facets for a given claim, improving empirical utility and
decision-making efficacy. We illustrate our framework through detailed case
studies of vision and language model evaluations, highlighting how explicitly
considering validity strengthens the connection between evaluation evidence and
the claims being made.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [243] [TokenWeave: Efficient Compute-Communication Overlap for Distributed LLM Inference](https://arxiv.org/abs/2505.11329)
*Raja Gond,Nipun Kwatra,Ramachandran Ramjee*

Main category: cs.DC

TL;DR: TokenWeave通过Token-Splitting技术和优化层归一化计算顺序，显著减少了分布式大语言模型推理中的开销，实现了高达29%的延迟降低和26%的吞吐量提升。


<details>
  <summary>Details</summary>
Motivation: 分布式大语言模型推理中，即使使用高速互联（如NVLINK），仍存在高达20%的开销。现有技术通过细粒度任务分解和通信重叠来缓解开销，但会引入额外开销。

Method: TokenWeave提出Token-Splitting技术，将推理批次的令牌分为两个子集，并重叠计算与通信。此外，优化层归一化计算顺序，并实现融合的AllReduce-RMSNorm内核。

Result: TokenWeave在多个模型和负载下实现了高达29%的延迟降低和26%的吞吐量提升，某些情况下甚至优于完全无通信的模型。

Conclusion: TokenWeave通过创新技术有效解决了分布式推理中的开销问题，显著提升了性能。

Abstract: Distributed inference of large language models (LLMs) can introduce overheads
of up to 20% even over GPUs connected via high-speed interconnects such as
NVLINK. Multiple techniques have been proposed to mitigate these overheads by
decomposing computations into finer-grained tasks and overlapping communication
with sub-tasks as they complete. However, fine-grained decomposition of a large
computation into many smaller computations on GPUs results in overheads.
Further, the communication itself uses many streaming multiprocessors (SMs),
adding to the overhead.
  We present TokenWeave to address these challenges. TokenWeave proposes a
Token-Splitting technique that divides the tokens in the inference batch into
two approximately equal subsets in a wave-aware manner. The computation of one
subset is then overlapped with the communication of the other. In addition,
TokenWeave optimizes the order of the layer normalization computation with
respect to communication operations and implements a novel fused
AllReduce-RMSNorm kernel carefully leveraging Multimem instruction support
available on NVIDIA Hopper GPUs. These optimizations allow TokenWeave to
perform communication and RMSNorm using only 2-8 SMs. Moreover, our kernel
enables the memory bound RMSNorm to be overlapped with the other batch's
computation, providing additional gains. Our evaluations demonstrate up to 29%
latency gains and up to 26% throughput gains across multiple models and
workloads. In several settings, TokenWeave results in better performance
compared to an equivalent model with all communication removed.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [244] [GarmentPile: Point-Level Visual Affordance Guided Retrieval and Adaptation for Cluttered Garments Manipulation](https://arxiv.org/abs/2503.09243)
*Ruihai Wu,Ziyu Zhu,Yuran Wang,Yue Chen,Jiarui Wang,Hao Dong*

Main category: cs.RO

TL;DR: 论文提出了一种学习点级可供性表示的方法，用于处理杂乱衣物场景中的复杂交互和几何关系，并通过适应性模块重组高纠缠衣物。


<details>
  <summary>Details</summary>
Motivation: 杂乱衣物场景中的复杂变形和交互关系对机器人操作提出了挑战，需要一种能够建模多模态操作候选并保持衣物清洁和稳定性的方法。

Method: 提出学习点级可供性表示，建模复杂空间和多模态操作候选，同时引入适应性模块重组高纠缠衣物。

Result: 框架在模拟和现实环境中对多种衣物类型和堆叠配置表现出有效性。

Conclusion: 通过学习点级可供性和适应性重组，论文解决了杂乱衣物操作中的复杂交互和几何关系问题。

Abstract: Cluttered garments manipulation poses significant challenges due to the
complex, deformable nature of garments and intricate garment relations. Unlike
single-garment manipulation, cluttered scenarios require managing complex
garment entanglements and interactions, while maintaining garment cleanliness
and manipulation stability. To address these demands, we propose to learn
point-level affordance, the dense representation modeling the complex space and
multi-modal manipulation candidates, while being aware of garment geometry,
structure, and inter-object relations. Additionally, as it is difficult to
directly retrieve a garment in some extremely entangled clutters, we introduce
an adaptation module, guided by learned affordance, to reorganize
highly-entangled garments into states plausible for manipulation. Our framework
demonstrates effectiveness over environments featuring diverse garment types
and pile configurations in both simulation and the real world. Project page:
https://garmentpile.github.io/.

</details>


### [245] [Predicting Human Behavior in Autonomous Systems: A Collaborative Machine Teaching Approach for Reducing Transfer of Control Events](https://arxiv.org/abs/2505.10695)
*Julian Wolter,Amr Gomaa*

Main category: cs.RO

TL;DR: 提出一种基于数据驱动的方法，利用人类交互数据训练AI模型，以减少不必要的控制转移（ToC）事件，提升系统鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统控制转移（ToC）方法在非关键情况下常被不必要触发，影响系统效率和可靠性。

Method: 通过模拟工业吸尘器的交互工具收集数据，开发基于LSTM的模型预测用户行为。

Result: 研究发现，即使是非专家数据也能有效训练模型，减少不必要的ToC事件。

Conclusion: 该方法展示了AI通过学习人类问题解决行为，结合传感器数据提升工业自动化和人机协作的潜力。

Abstract: As autonomous systems become integral to various industries, effective
strategies for fault handling are essential to ensure reliability and
efficiency. Transfer of Control (ToC), a traditional approach for interrupting
automated processes during faults, is often triggered unnecessarily in
non-critical situations. To address this, we propose a data-driven method that
uses human interaction data to train AI models capable of preemptively
identifying and addressing issues or assisting users in resolution. Using an
interactive tool simulating an industrial vacuum cleaner, we collected data and
developed an LSTM-based model to predict user behavior. Our findings reveal
that even data from non-experts can effectively train models to reduce
unnecessary ToC events, enhancing the system's robustness. This approach
highlights the potential of AI to learn directly from human problem-solving
behaviors, complementing sensor data to improve industrial automation and
human-AI collaboration.

</details>


### [246] [REI-Bench: Can Embodied Agents Understand Vague Human Instructions in Task Planning?](https://arxiv.org/abs/2505.10872)
*Chenxi Jiang,Chuhao Zhou,Jianfei Yang*

Main category: cs.RO

TL;DR: 论文研究了模糊指代表达（REs）对基于大语言模型（LLM）的机器人任务规划的影响，并提出了一种新方法（任务导向的上下文认知）来解决问题。


<details>
  <summary>Details</summary>
Motivation: 现实世界中，非专家用户（如老人和儿童）的指令往往模糊，尤其是通过指代表达（REs）时，这严重影响了机器人任务规划的性能。

Method: 提出了首个包含模糊REs的机器人任务规划基准（REI-Bench），并设计了一种任务导向的上下文认知方法，以生成清晰的指令。

Result: 模糊REs导致机器人规划成功率下降高达77.9%，而提出的方法显著提升了性能，达到最先进水平。

Conclusion: 该研究为人类-机器人交互（HRI）提供了更实用的任务规划方法，尤其适用于非专家用户。

Abstract: Robot task planning decomposes human instructions into executable action
sequences that enable robots to complete a series of complex tasks. Although
recent large language model (LLM)-based task planners achieve amazing
performance, they assume that human instructions are clear and straightforward.
However, real-world users are not experts, and their instructions to robots
often contain significant vagueness. Linguists suggest that such vagueness
frequently arises from referring expressions (REs), whose meanings depend
heavily on dialogue context and environment. This vagueness is even more
prevalent among the elderly and children, who robots should serve more. This
paper studies how such vagueness in REs within human instructions affects
LLM-based robot task planning and how to overcome this issue. To this end, we
propose the first robot task planning benchmark with vague REs (REI-Bench),
where we discover that the vagueness of REs can severely degrade robot planning
performance, leading to success rate drops of up to 77.9%. We also observe that
most failure cases stem from missing objects in planners. To mitigate the REs
issue, we propose a simple yet effective approach: task-oriented context
cognition, which generates clear instructions for robots, achieving
state-of-the-art performance compared to aware prompt and chains of thought.
This work contributes to the research community of human-robot interaction
(HRI) by making robot task planning more practical, particularly for non-expert
users, e.g., the elderly and children.

</details>


### [247] [Geofenced Unmanned Aerial Robotic Defender for Deer Detection and Deterrence (GUARD)](https://arxiv.org/abs/2505.10770)
*Ebasa Temesgen,Mario Jerez,Greta Brown,Graham Wilson,Sree Ganesh Lalitaditya Divakarla,Sarah Boelter,Oscar Nelson,Robert McPherson,Maria Gini*

Main category: cs.RO

TL;DR: 本文提出了一种基于无人机的野生动物（尤其是鹿）驱赶系统，结合实时计算机视觉、高效路径规划和自主充电技术，以解决传统方法的不足。


<details>
  <summary>Details</summary>
Motivation: 鹿等野生动物对农作物的破坏威胁农业生产，传统驱赶方法在可扩展性、响应速度和适应性上存在不足。

Method: 系统整合了YOLO实时计算机视觉模块（用于鹿检测）、高效覆盖路径规划算法和自主充电站，以适应农田环境。

Result: 通过仿真和实地测试，系统表现出高检测精度、高效覆盖和长续航能力。

Conclusion: 无人机驱赶系统在精准农业中具有可行性和有效性，为未来部署提供了可扩展框架。

Abstract: Wildlife-induced crop damage, particularly from deer, threatens agricultural
productivity. Traditional deterrence methods often fall short in scalability,
responsiveness, and adaptability to diverse farmland environments. This paper
presents an integrated unmanned aerial vehicle (UAV) system designed for
autonomous wildlife deterrence, developed as part of the Farm Robotics
Challenge. Our system combines a YOLO-based real-time computer vision module
for deer detection, an energy-efficient coverage path planning algorithm for
efficient field monitoring, and an autonomous charging station for continuous
operation of the UAV. In collaboration with a local Minnesota farmer, the
system is tailored to address practical constraints such as terrain,
infrastructure limitations, and animal behavior. The solution is evaluated
through a combination of simulation and field testing, demonstrating robust
detection accuracy, efficient coverage, and extended operational time. The
results highlight the feasibility and effectiveness of drone-based wildlife
deterrence in precision agriculture, offering a scalable framework for future
deployment and extension.

</details>


### [248] [GROQLoco: Generalist and RObot-agnostic Quadruped Locomotion Control using Offline Datasets](https://arxiv.org/abs/2505.10973)
*Narayanan PP,Sarvesh Prasanth Venkatesan,Srinivas Kantha Reddy,Shishir Kolathaya*

Main category: cs.RO

TL;DR: GROQLoco是一个基于注意力的通用框架，通过离线数据集学习多足机器人和地形的通用运动策略，实现零样本迁移。


<details>
  <summary>Details</summary>
Motivation: 解决多足机器人在复杂地形和不同形态下的实时适应性问题，利用离线数据训练通用策略。

Method: 利用专家演示数据（包括周期性步态和非周期性步态），训练基于注意力的通用模型，直接处理本体感知数据。

Result: 在多种机器人和地形上实现零样本迁移，包括商业机器人Unitree Go1和70kg的Stoch 5。

Conclusion: GROQLoco展示了通用运动策略在多样机器人和地形上的潜力，无需微调即可部署。

Abstract: Recent advancements in large-scale offline training have demonstrated the
potential of generalist policy learning for complex robotic tasks. However,
applying these principles to legged locomotion remains a challenge due to
continuous dynamics and the need for real-time adaptation across diverse
terrains and robot morphologies. In this work, we propose GROQLoco, a scalable,
attention-based framework that learns a single generalist locomotion policy
across multiple quadruped robots and terrains, relying solely on offline
datasets. Our approach leverages expert demonstrations from two distinct
locomotion behaviors - stair traversal (non-periodic gaits) and flat terrain
traversal (periodic gaits) - collected across multiple quadruped robots, to
train a generalist model that enables behavior fusion for both behaviors.
Crucially, our framework operates directly on proprioceptive data from all
robots without incorporating any robot-specific encodings. The policy is
directly deployable on an Intel i7 nuc, producing low-latency control outputs
without any test-time optimization. Our extensive experiments demonstrate
strong zero-shot transfer across highly diverse quadruped robots and terrains,
including hardware deployment on the Unitree Go1, a commercially available 12kg
robot. Notably, we evaluate challenging cross-robot training setups where
different locomotion skills are unevenly distributed across robots, yet observe
successful transfer of both flat walking and stair traversal behaviors to all
robots at test time. We also show preliminary walking on Stoch 5, a 70kg
quadruped, on flat and outdoor terrains without requiring any fine tuning.
These results highlight the potential for robust generalist locomotion across
diverse robots and terrains.

</details>


### [249] [DexGarmentLab: Dexterous Garment Manipulation Environment with Generalizable Policy](https://arxiv.org/abs/2505.11032)
*Yuran Wang,Ruihai Wu,Yue Chen,Jiarui Wang,Jiaqi Liang,Ziyu Zhu,Haoran Geng,Jitendra Malik,Pieter Abbeel,Hao Dong*

Main category: cs.RO

TL;DR: 论文提出了DexGarmentLab环境和HALO算法，用于解决灵巧衣物操纵的挑战，通过自动生成数据集和改进策略，显著提升了泛化能力。


<details>
  <summary>Details</summary>
Motivation: 衣物操纵因类别、几何和变形的多样性而具有挑战性，现有研究难以模拟人类灵巧性，缺乏真实模拟环境。

Method: 提出DexGarmentLab环境，包含15个任务场景的高质量3D资产和改进的模拟技术；利用衣物结构对应性自动生成数据集；提出HALO算法，通过分层策略识别可转移操作点并生成通用轨迹。

Result: HALO算法在实验中表现优于现有方法，能泛化到形状和变形差异大的未见实例。

Conclusion: DexGarmentLab和HALO为灵巧衣物操纵提供了高效解决方案，显著减少了人工干预并提升了泛化能力。

Abstract: Garment manipulation is a critical challenge due to the diversity in garment
categories, geometries, and deformations. Despite this, humans can effortlessly
handle garments, thanks to the dexterity of our hands. However, existing
research in the field has struggled to replicate this level of dexterity,
primarily hindered by the lack of realistic simulations of dexterous garment
manipulation. Therefore, we propose DexGarmentLab, the first environment
specifically designed for dexterous (especially bimanual) garment manipulation,
which features large-scale high-quality 3D assets for 15 task scenarios, and
refines simulation techniques tailored for garment modeling to reduce the
sim-to-real gap. Previous data collection typically relies on teleoperation or
training expert reinforcement learning (RL) policies, which are labor-intensive
and inefficient. In this paper, we leverage garment structural correspondence
to automatically generate a dataset with diverse trajectories using only a
single expert demonstration, significantly reducing manual intervention.
However, even extensive demonstrations cannot cover the infinite states of
garments, which necessitates the exploration of new algorithms. To improve
generalization across diverse garment shapes and deformations, we propose a
Hierarchical gArment-manipuLation pOlicy (HALO). It first identifies
transferable affordance points to accurately locate the manipulation area, then
generates generalizable trajectories to complete the task. Through extensive
experiments and detailed analysis of our method and baseline, we demonstrate
that HALO consistently outperforms existing methods, successfully generalizing
to previously unseen instances even with significant variations in shape and
deformation where others fail. Our project page is available at:
https://wayrise.github.io/DexGarmentLab/.

</details>


### [250] [PARSEC: Preference Adaptation for Robotic Object Rearrangement from Scene Context](https://arxiv.org/abs/2505.11108)
*Kartik Ramachandruni,Sonia Chernova*

Main category: cs.RO

TL;DR: PARSEC是一个用于学习用户组织偏好的物体重排基准，基于11万条众包数据，提出了ContextSortLM模型，该模型在部分排列环境中表现优异。


<details>
  <summary>Details</summary>
Motivation: 家庭机器人需要个性化且无需明确指令的物体重排能力，同时需适应新物体和环境。

Method: 提出了PARSEC基准和ContextSortLM模型，利用多场景上下文源进行物体重排。

Result: ContextSortLM在复制用户排列方面优于其他模型，并在所有环境类别中表现优异。

Conclusion: 多场景上下文源对重排模型性能有显著提升，未来需进一步研究环境语义建模。

Abstract: Object rearrangement is a key task for household robots requiring
personalization without explicit instructions, meaningful object placement in
environments occupied with objects, and generalization to unseen objects and
new environments. To facilitate research addressing these challenges, we
introduce PARSEC, an object rearrangement benchmark for learning user
organizational preferences from observed scene context to place objects in a
partially arranged environment. PARSEC is built upon a novel dataset of 110K
rearrangement examples crowdsourced from 72 users, featuring 93 object
categories and 15 environments. We also propose ContextSortLM, an LLM-based
rearrangement model that places objects in partially arranged environments by
adapting to user preferences from prior and current scene context while
accounting for multiple valid placements. We evaluate ContextSortLM and
existing personalized rearrangement approaches on the PARSEC benchmark and
complement these findings with a crowdsourced evaluation of 108 online raters
ranking model predictions based on alignment with user preferences. Our results
indicate that personalized rearrangement models leveraging multiple scene
context sources perform better than models relying on a single context source.
Moreover, ContextSortLM outperforms other models in placing objects to
replicate the target user's arrangement and ranks among the top two in all
three environment categories, as rated by online evaluators. Importantly, our
evaluation highlights challenges associated with modeling environment semantics
across different environment categories and provides recommendations for future
work.

</details>


### [251] [Conditioning Matters: Training Diffusion Policies is Faster Than You Think](https://arxiv.org/abs/2505.11123)
*Zibin Dong,Yicheng Liu,Yinchuan Li,Hang Zhao,Jianye Hao*

Main category: cs.RO

TL;DR: 论文提出Cocos方法，通过改进条件流匹配中的源分布，解决扩散策略训练中的损失崩溃问题，提升训练效率和性能。


<details>
  <summary>Details</summary>
Motivation: 扩散策略在视觉-语言-动作模型中表现优异，但训练效率不足，尤其是当生成条件难以区分时，训练目标会退化为建模边际动作分布（损失崩溃）。

Method: 提出Cocos方法，通过将源分布调整为条件依赖，围绕条件输入的语义锚定源分布，增强条件整合并防止损失崩溃。

Result: Cocos在仿真和实际基准测试中表现优异，收敛更快、成功率更高，且使用更少的梯度步和参数即可匹配大规模预训练VLAs的性能。

Conclusion: Cocos是一种轻量级、易实现且兼容多种策略架构的通用改进方法，显著提升了扩散策略的训练效果。

Abstract: Diffusion policies have emerged as a mainstream paradigm for building
vision-language-action (VLA) models. Although they demonstrate strong robot
control capabilities, their training efficiency remains suboptimal. In this
work, we identify a fundamental challenge in conditional diffusion policy
training: when generative conditions are hard to distinguish, the training
objective degenerates into modeling the marginal action distribution, a
phenomenon we term loss collapse. To overcome this, we propose Cocos, a simple
yet general solution that modifies the source distribution in the conditional
flow matching to be condition-dependent. By anchoring the source distribution
around semantics extracted from condition inputs, Cocos encourages stronger
condition integration and prevents the loss collapse. We provide theoretical
justification and extensive empirical results across simulation and real-world
benchmarks. Our method achieves faster convergence and higher success rates
than existing approaches, matching the performance of large-scale pre-trained
VLAs using significantly fewer gradient steps and parameters. Cocos is
lightweight, easy to implement, and compatible with diverse policy
architectures, offering a general-purpose improvement to diffusion policy
training.

</details>


### [252] [X2C: A Dataset Featuring Nuanced Facial Expressions for Realistic Humanoid Imitation](https://arxiv.org/abs/2505.11146)
*Peizhen Li,Longbing Cao,Xiao-Ming Wu,Runze Yang,Xiaohan Yu*

Main category: cs.RO

TL;DR: 论文提出了X2C数据集和X2CNet框架，用于提升人形机器人面部表情模仿的真实性。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏标注丰富的人形机器人面部表情数据集，影响了人机情感交流中机器人表情模仿的进展。

Method: 1）构建X2C数据集，包含10万对（图像，控制值）数据；2）提出X2CNet框架，学习人形表情与控制值的映射关系。

Result: X2CNet实现了对不同表演者面部表情的模仿，并在实际人形机器人上展示了其能力。

Conclusion: X2C数据集和X2CNet框架为人形机器人面部表情模仿提供了高质量数据和基线方法。

Abstract: The ability to imitate realistic facial expressions is essential for humanoid
robots engaged in affective human-robot communication. However, the lack of
datasets containing diverse humanoid facial expressions with proper annotations
hinders progress in realistic humanoid facial expression imitation. To address
these challenges, we introduce X2C (Anything to Control), a dataset featuring
nuanced facial expressions for realistic humanoid imitation. With X2C, we
contribute: 1) a high-quality, high-diversity, large-scale dataset comprising
100,000 (image, control value) pairs. Each image depicts a humanoid robot
displaying a diverse range of facial expressions, annotated with 30 control
values representing the ground-truth expression configuration; 2) X2CNet, a
novel human-to-humanoid facial expression imitation framework that learns the
correspondence between nuanced humanoid expressions and their underlying
control values from X2C. It enables facial expression imitation in the wild for
different human performers, providing a baseline for the imitation task,
showcasing the potential value of our dataset; 3) real-world demonstrations on
a physical humanoid robot, highlighting its capability to advance realistic
humanoid facial expression imitation. Code and Data:
https://lipzh5.github.io/X2CNet/

</details>


### [253] [Real-Time Verification of Embodied Reasoning for Generative Skill Acquisition](https://arxiv.org/abs/2505.11175)
*Bo Yue,Shuqi Guo,Kaiyu Hu,Chujiao Wang,Benyou Wang,Kui Jia,Guiliang Liu*

Main category: cs.RO

TL;DR: VERGSA框架通过实时验证原则提升生成技能学习效率，任务成功率显著提高。


<details>
  <summary>Details</summary>
Motivation: 现有方法在复杂3D环境中效果不佳且计算成本高，需改进技能学习效率。

Method: VERGSA结合数学推理验证原则，动态生成任务提示并自动合成奖励信号。

Result: 任务成功率提升21%，验证模型对新颖和已知任务分别提升24%和36%。

Conclusion: VERGSA首次实现验证驱动的生成技能学习，显著优于基线方法。

Abstract: Generative skill acquisition enables embodied agents to actively learn a
scalable and evolving repertoire of control skills, crucial for the advancement
of large decision models. While prior approaches often rely on supervision
signals from generalist agents (e.g., LLMs), their effectiveness in complex 3D
environments remains unclear; exhaustive evaluation incurs substantial
computational costs, significantly hindering the efficiency of skill learning.
Inspired by recent successes in verification models for mathematical reasoning,
we propose VERGSA (Verifying Embodied Reasoning in Generative Skill
Acquisition), a framework that systematically integrates real-time verification
principles into embodied skill learning. VERGSA establishes 1) a seamless
extension from verification of mathematical reasoning into embodied learning by
dynamically incorporating contextually relevant tasks into prompts and defining
success metrics for both subtasks and overall tasks, and 2) an automated,
scalable reward labeling scheme that synthesizes dense reward signals by
iteratively finalizing the contribution of scene configuration and subtask
learning to overall skill acquisition. To the best of our knowledge, this
approach constitutes the first comprehensive training dataset for
verification-driven generative skill acquisition, eliminating arduous manual
reward engineering. Experiments validate the efficacy of our approach: 1) the
exemplar task pool improves the average task success rates by 21%, 2) our
verification model boosts success rates by 24% for novel tasks and 36% for
encountered tasks, and 3) outperforms LLM-as-a-Judge baselines in verification
quality.

</details>


### [254] [Learning Multimodal AI Algorithms for Amplifying Limited User Input into High-dimensional Control Space](https://arxiv.org/abs/2505.11366)
*Ali Rabiee,Sima Ghafoori,MH Farhadi,Robert Beyer,Xiangyu Bai,David J Lin,Sarah Ostadabbas,Reza Abiri*

Main category: cs.RO

TL;DR: 本文提出了一种新型多模态AI方法ARAS，用于帮助严重瘫痪患者通过非侵入式输入控制高维度辅助设备，其性能优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有侵入式辅助技术面临公众接受度、寿命和商业化障碍，而非侵入式方法则依赖易受干扰的信号且训练时间长。本研究旨在解决这些问题。

Method: 采用多模态共享自主框架，结合深度强化学习算法，将有限低维用户输入与实时环境感知结合，动态解释用户意图。

Result: ARAS在50,000次模拟训练后表现优异，23名人类受试者测试中实现了92.88%的任务成功率，性能接近侵入式技术。

Conclusion: ARAS为非侵入式辅助技术提供了一种高效、适应性强的解决方案，有望推动瘫痪患者辅助设备的发展。

Abstract: Current invasive assistive technologies are designed to infer
high-dimensional motor control signals from severely paralyzed patients.
However, they face significant challenges, including public acceptance, limited
longevity, and barriers to commercialization. Meanwhile, noninvasive
alternatives often rely on artifact-prone signals, require lengthy user
training, and struggle to deliver robust high-dimensional control for dexterous
tasks. To address these issues, this study introduces a novel human-centered
multimodal AI approach as intelligent compensatory mechanisms for lost motor
functions that could potentially enable patients with severe paralysis to
control high-dimensional assistive devices, such as dexterous robotic arms,
using limited and noninvasive inputs. In contrast to the current
state-of-the-art (SoTA) noninvasive approaches, our context-aware, multimodal
shared-autonomy framework integrates deep reinforcement learning algorithms to
blend limited low-dimensional user input with real-time environmental
perception, enabling adaptive, dynamic, and intelligent interpretation of human
intent for complex dexterous manipulation tasks, such as pick-and-place. The
results from our ARAS (Adaptive Reinforcement learning for Amplification of
limited inputs in Shared autonomy) trained with synthetic users over 50,000
computer simulation episodes demonstrated the first successful implementation
of the proposed closed-loop human-in-the-loop paradigm, outperforming the SoTA
shared autonomy algorithms. Following a zero-shot sim-to-real transfer, ARAS
was evaluated on 23 human subjects, demonstrating high accuracy in dynamic
intent detection and smooth, stable 3D trajectory control for dexterous
pick-and-place tasks. ARAS user study achieved a high task success rate of
92.88%, with short completion times comparable to those of SoTA invasive
assistive technologies.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [255] [Explain What You Mean: Intent Augmented Knowledge Graph Recommender Built With LLM](https://arxiv.org/abs/2505.10900)
*Wenqing Zheng,Noah Fatsi,Daniel Barcklow,Dmitri Kalaev,Steven Yao,Owen Reinert,C. Bayan Bruss,Daniele Rosa*

Main category: cs.IR

TL;DR: 论文提出了一种基于LLM的意图知识图谱推荐系统（IKGR），通过检索增强生成和编码方法构建并丰富知识图谱，解决了推荐系统中的交互稀疏性问题。


<details>
  <summary>Details</summary>
Motivation: 交互稀疏性是推荐系统的主要障碍，尤其是在用户和产品基数不均衡或新实体引入（冷启动问题）时。现有方法要么资源消耗大，要么受限于数据质量和可用性。

Method: IKGR利用检索增强生成和编码方法构建知识图谱，通过学习潜在用户-物品亲和力并通过意图连通性丰富图谱，实现意图驱动的推荐。

Result: 在真实数据集上的实验表明，IKGR克服了知识缺口，在公开和内部数据集上均显著优于现有基线方法。

Conclusion: IKGR通过意图知识图谱有效解决了推荐系统中的稀疏性问题，并提供了可解释的推荐结果。

Abstract: Interaction sparsity is the primary obstacle for recommendation systems.
Sparsity manifests in environments with disproportional cardinality of
groupings of entities, such as users and products in an online marketplace. It
also is found for newly introduced entities, described as the cold-start
problem. Recent efforts to mitigate this sparsity issue shifts the performance
bottleneck to other areas in the computational pipeline. Those that focus on
enriching sparse representations with connectivity data from other external
sources propose methods that are resource demanding and require careful domain
expert aided addition of this newly introduced data. Others that turn to Large
Language Model (LLM) based recommenders will quickly encounter limitations
surrounding data quality and availability. In this work, we propose LLM-based
Intent Knowledge Graph Recommender (IKGR), a novel framework that leverages
retrieval-augmented generation and an encoding approach to construct and
densify a knowledge graph. IKGR learns latent user-item affinities from an
interaction knowledge graph and further densifies it through mutual intent
connectivity. This addresses sparsity issues and allows the model to make
intent-grounded recommendations with an interpretable embedding translation
layer. Through extensive experiments on real-world datasets, we demonstrate
that IKGR overcomes knowledge gaps and achieves substantial gains over
state-of-the-art baselines on both publicly available and our internal
recommendation datasets.

</details>


### [256] [Who You Are Matters: Bridging Topics and Social Roles via LLM-Enhanced Logical Recommendation](https://arxiv.org/abs/2505.10940)
*Qing Yu,Xiaobei Wang,Shuchang Liu,Yandong Bai,Xiaoyu Yang,Xueliang Wang,Chang Meng,Shanshan Wu,Hailan Yang,Huihui Xiao,Xiang Li,Fan Yang,Xiaoqiang Feng,Lantao Hu,Han Li,Kun Gai,Lixin Zou*

Main category: cs.IR

TL;DR: 论文提出了一种结合大型语言模型（LLM）和推荐系统的方法（TagCF），通过用户角色识别和行为逻辑建模任务，显式建模用户角色及其与物品主题的逻辑关系，提升推荐性能。


<details>
  <summary>Details</summary>
Motivation: 主流推荐系统方法通常忽视用户特征及其社会角色的建模，而这些是影响用户兴趣和偏好转移的逻辑混杂因素。

Method: 提出TagCF框架，利用LLM的世界知识和逻辑推理能力生成虚拟逻辑图，显式建模用户角色和行为逻辑。

Result: 实验表明，该方法在工业和公开数据集上提升了推荐性能，且提取的用户-物品逻辑图具有通用性。

Conclusion: 显式建模用户角色和行为逻辑能有效提升推荐系统的性能，且该方法具有广泛适用性。

Abstract: Recommender systems filter contents/items valuable to users by inferring
preferences from user features and historical behaviors. Mainstream approaches
follow the learning-to-rank paradigm, which focus on discovering and modeling
item topics (e.g., categories), and capturing user preferences on these topics
based on historical interactions. However, this paradigm often neglects the
modeling of user characteristics and their social roles, which are logical
confounders influencing the correlated interest and user preference transition.
To bridge this gap, we introduce the user role identification task and the
behavioral logic modeling task that aim to explicitly model user roles and
learn the logical relations between item topics and user social roles. We show
that it is possible to explicitly solve these tasks through an efficient
integration framework of Large Language Model (LLM) and recommendation systems,
for which we propose TagCF. On the one hand, the exploitation of the LLM's
world knowledge and logic inference ability produces a virtual logic graph that
reveals dynamic and expressive knowledge of users, augmenting the
recommendation performance. On the other hand, the user role aligns the user
behavioral logic with the observed user feedback, refining our understanding of
user behaviors. Additionally, we also show that the extracted user-item logic
graph is empirically a general knowledge that can benefit a wide range of
recommendation tasks, and conduct experiments on industrial and several public
datasets as verification.

</details>


### [257] [From Intent Discovery to Recognition with Topic Modeling and Synthetic Data](https://arxiv.org/abs/2505.11176)
*Aaron Rodrigues,Mahmood Hegazy,Azzam Naeem*

Main category: cs.IR

TL;DR: 论文提出了一种基于LLM的框架，用于主题建模和合成查询生成，以解决冷启动问题和短话语意图识别挑战。


<details>
  <summary>Details</summary>
Motivation: 在短话语和冷启动问题突出的领域中，传统方法难以准确识别用户意图，因此需要一种更高效的方法。

Method: 采用分层主题建模和意图发现扩展人工分类法，并生成合成查询数据以增强数据集。

Result: 主题扩展显著提升了主题一致性和相关性，合成数据实验显示few-shot提示显著提高了查询质量。

Conclusion: LLM框架在主题建模和意图识别中具有可扩展性和实用性，合成话语能有效增强数据集的多样性和覆盖范围。

Abstract: Understanding and recognizing customer intents in AI systems is crucial,
particularly in domains characterized by short utterances and the cold start
problem, where recommender systems must include new products or services
without sufficient real user data. Customer utterances are characterized by
infrequent word co-occurences and high term variability, which poses
significant challenges for traditional methods in specifying distinct user
needs and preparing synthetic queries. To address this, we propose an agentic
LLM framework for topic modeling and synthetic query generation, which
accelerates the discovery and recognition of customer intents. We first apply
hierarchical topic modeling and intent discovery to expand a human-curated
taxonomy from 36 generic user intents to 278 granular intents, demonstrating
the potential of LLMs to significantly enhance topic specificity and diversity.
Next, to support newly discovered intents and address the cold start problem,
we generate synthetic user query data, which augments real utterances and
reduces dependency on human annotation, especially in low-resource settings.
Topic model experiments show substantial improvements in coherence and
relevance after topic expansion, while synthetic data experiments indicate that
in-class few-shot prompting significantly improves the quality and utility of
synthetic queries without compromising diversity. We also show that
LLM-generated intent descriptions and keywords can effectively substitute for
human-curated versions when used as context for synthetic query generation. Our
research underscores the scalability and utility of LLM agents in topic
modeling and highlights the strategic use of synthetic utterances to enhance
dataset variability and coverage for intent recognition. We present a
comprehensive and robust framework for online discovery and recognition of new
customer intents in dynamic domains.

</details>


### [258] [User-centric Music Recommendations](https://arxiv.org/abs/2505.11198)
*Jaime Ramirez Castillo,M. Julia Flores,Ann E. Nicholson*

Main category: cs.IR

TL;DR: 本文提出了一种用户中心的推荐框架，通过四个可定制阶段提升解释性和用户参与度，基于15年的Last.fm播放记录预测用户偏好并推荐音乐。


<details>
  <summary>Details</summary>
Motivation: 旨在通过分析用户长期音乐播放习惯，预测特定时刻的音乐偏好，以提高推荐系统的个性化和解释性。

Method: 收集用户15年的Last.fm播放记录，构建时间上下文数据集，结合社区标签和Spotify音频特征，预测特定时刻的音频特征（如舞蹈性），并推荐相似曲目。

Result: 成功预测了用户对舞蹈性音频特征的偏好，并展示了框架的可扩展性。

Conclusion: 该框架能够学习单个用户的音乐习惯，并具有扩展到其他用户的潜力，为个性化推荐提供了新思路。

Abstract: This work presents a user-centric recommendation framework, designed as a
pipeline with four distinct, connected, and customizable phases. These phases
are intended to improve explainability and boost user engagement.
  We have collected the historical Last.fm track playback records of a single
user over approximately 15 years. The collected dataset includes more than
90,000 playbacks and approximately 14,000 unique tracks.
  From track playback records, we have created a dataset of user temporal
contexts (each row is a specific moment when the user listened to certain music
descriptors). As music descriptors, we have used community-contributed Last.fm
tags and Spotify audio features. They represent the music that, throughout
years, the user has been listening to.
  Next, given the most relevant Last.fm tags of a moment (e.g. the hour of the
day), we predict the Spotify audio features that best fit the user preferences
in that particular moment. Finally, we use the predicted audio features to find
tracks similar to these features. The final aim is to recommend (and discover)
tracks that the user may feel like listening to at a particular moment.
  For our initial study case, we have chosen to predict only a single audio
feature target: danceability. The framework, however, allows to include more
target variables.
  The ability to learn the musical habits from a single user can be quite
powerful, and this framework could be extended to other users.

</details>


### [259] [On the Role of Weight Decay in Collaborative Filtering: A Popularity Perspective](https://arxiv.org/abs/2505.11318)
*Donald Loveland,Mingxuan Ju,Tong Zhao,Neil Shah,Danai Koutra*

Main category: cs.IR

TL;DR: 论文探讨了协同过滤（CF）中权重衰减（weight decay）的作用，发现其主要功能是将流行度信息编码到嵌入向量中，并提出了PRISM方法以简化训练过程。


<details>
  <summary>Details</summary>
Motivation: 尽管协同过滤模型的设计已有很多研究，但权重衰减的作用被忽视。论文旨在揭示其重要性及其对训练的影响。

Method: 通过理论和实证分析，发现权重衰减的作用，并提出PRISM方法，预编码流行度信息以替代权重衰减。

Result: PRISM在性能上提升4.77%，训练时间减少38.48%，并能有效缓解流行度偏差。

Conclusion: PRISM是一种简单有效的方法，可简化CF模型的训练并提升性能。

Abstract: Collaborative filtering (CF) enables large-scale recommendation systems by
encoding information from historical user-item interactions into dense
ID-embedding tables. However, as embedding tables grow, closed-form solutions
become impractical, often necessitating the use of mini-batch gradient descent
for training. Despite extensive work on designing loss functions to train CF
models, we argue that one core component of these pipelines is heavily
overlooked: weight decay. Attaining high-performing models typically requires
careful tuning of weight decay, regardless of loss, yet its necessity is not
well understood. In this work, we question why weight decay is crucial in CF
pipelines and how it impacts training. Through theoretical and empirical
analysis, we surprisingly uncover that weight decay's primary function is to
encode popularity information into the magnitudes of the embedding vectors.
Moreover, we find that tuning weight decay acts as a coarse, non-linear knob to
influence preference towards popular or unpopular items. Based on these
findings, we propose PRISM (Popularity-awaRe Initialization Strategy for
embedding Magnitudes), a straightforward yet effective solution to simplify the
training of high-performing CF models. PRISM pre-encodes the popularity
information typically learned through weight decay, eliminating its necessity.
Our experiments show that PRISM improves performance by up to 4.77% and reduces
training times by 38.48%, compared to state-of-the-art training strategies.
Additionally, we parameterize PRISM to modulate the initialization strength,
offering a cost-effective and meaningful strategy to mitigate popularity bias.

</details>


### [260] [The Future is Sparse: Embedding Compression for Scalable Retrieval in Recommender Systems](https://arxiv.org/abs/2505.11388)
*Petr Kasalický,Martin Spišák,Vojtěch Vančura,Daniel Bohuněk,Rodrigo Alves,Pavel Kordík*

Main category: cs.IR

TL;DR: 论文提出了一种轻量级、可学习的嵌入压缩技术，用于解决高基数实体的表示问题，减少内存需求同时保持检索性能。


<details>
  <summary>Details</summary>
Motivation: 工业级推荐系统面临高基数实体（如用户或物品）的密集嵌入表示问题，随着嵌入规模增大，内存存储和访问变得困难。

Method: 提出一种嵌入压缩技术，将密集嵌入投影到高维稀疏激活空间，适用于检索任务。

Result: 该方法显著减少了内存需求，同时保持了检索性能，适合资源受限的大规模部署。

Conclusion: 利用稀疏性是提高大规模推荐系统效率的有效途径，代码已开源。

Abstract: Industry-scale recommender systems face a core challenge: representing
entities with high cardinality, such as users or items, using dense embeddings
that must be accessible during both training and inference. However, as
embedding sizes grow, memory constraints make storage and access increasingly
difficult. We describe a lightweight, learnable embedding compression technique
that projects dense embeddings into a high-dimensional, sparsely activated
space. Designed for retrieval tasks, our method reduces memory requirements
while preserving retrieval performance, enabling scalable deployment under
strict resource constraints. Our results demonstrate that leveraging sparsity
is a promising approach for improving the efficiency of large-scale
recommenders. We release our code at https://github.com/recombee/CompresSAE.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [261] [Phi: Leveraging Pattern-based Hierarchical Sparsity for High-Efficiency Spiking Neural Networks](https://arxiv.org/abs/2505.10909)
*Chiyue Wei,Bowen Duan,Cong Guo,Jingyang Zhang,Qingyue Song,Hai "Helen" Li,Yiran Chen*

Main category: cs.AR

TL;DR: 本文提出了一种名为Phi的基于模式的分层稀疏框架，通过利用脉冲神经网络（SNN）中独特的激活模式，优化计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有SNN加速器虽然利用了激活稀疏性，但忽略了二进制激活的独特分布模式，导致计算效率仍有提升空间。

Method: 提出Phi框架，包含两级稀疏层次：第一级通过预定义模式实现向量级稀疏，第二级通过补充稀疏矩阵实现元素级稀疏。同时采用算法-硬件协同设计方法。

Result: 实验表明，Phi在速度和能效上分别比现有SNN加速器提升了3.45倍和4.93倍。

Conclusion: Phi框架通过模式化稀疏优化，显著提升了SNN的计算效率和能效。

Abstract: Spiking Neural Networks (SNNs) are gaining attention for their energy
efficiency and biological plausibility, utilizing 0-1 activation sparsity
through spike-driven computation. While existing SNN accelerators exploit this
sparsity to skip zero computations, they often overlook the unique distribution
patterns inherent in binary activations. In this work, we observe that
particular patterns exist in spike activations, which we can utilize to reduce
the substantial computation of SNN models. Based on these findings, we propose
a novel \textbf{pattern-based hierarchical sparsity} framework, termed
\textbf{\textit{Phi}}, to optimize computation.
  \textit{Phi} introduces a two-level sparsity hierarchy: Level 1 exhibits
vector-wise sparsity by representing activations with pre-defined patterns,
allowing for offline pre-computation with weights and significantly reducing
most runtime computation. Level 2 features element-wise sparsity by
complementing the Level 1 matrix, using a highly sparse matrix to further
reduce computation while maintaining accuracy. We present an algorithm-hardware
co-design approach. Algorithmically, we employ a k-means-based pattern
selection method to identify representative patterns and introduce a
pattern-aware fine-tuning technique to enhance Level 2 sparsity.
Architecturally, we design \textbf{\textit{Phi}}, a dedicated hardware
architecture that efficiently processes the two levels of \textit{Phi} sparsity
on the fly. Extensive experiments demonstrate that \textit{Phi} achieves a
$3.45\times$ speedup and a $4.93\times$ improvement in energy efficiency
compared to state-of-the-art SNN accelerators, showcasing the effectiveness of
our framework in optimizing SNN computation.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [262] [$\mathcal{A}LLM4ADD$: Unlocking the Capabilities of Audio Large Language Models for Audio Deepfake Detection](https://arxiv.org/abs/2505.11079)
*Hao Gu,Jiangyan Yi,Chenglong Wang,Jianhua Tao,Zheng Lian,Jiayi He,Yong Ren,Yujie Chen,Zhengqi Wen*

Main category: cs.SD

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Audio deepfake detection (ADD) has grown increasingly important due to the
rise of high-fidelity audio generative models and their potential for misuse.
Given that audio large language models (ALLMs) have made significant progress
in various audio processing tasks, a heuristic question arises: Can ALLMs be
leveraged to solve ADD?. In this paper, we first conduct a comprehensive
zero-shot evaluation of ALLMs on ADD, revealing their ineffectiveness in
detecting fake audio. To enhance their performance, we propose
$\mathcal{A}LLM4ADD$, an ALLM-driven framework for ADD. Specifically, we
reformulate ADD task as an audio question answering problem, prompting the
model with the question: "Is this audio fake or real?". We then perform
supervised fine-tuning to enable the ALLM to assess the authenticity of query
audio. Extensive experiments are conducted to demonstrate that our ALLM-based
method can achieve superior performance in fake audio detection, particularly
in data-scarce scenarios. As a pioneering study, we anticipate that this work
will inspire the research community to leverage ALLMs to develop more effective
ADD systems.

</details>


### [263] [Audio Turing Test: Benchmarking the Human-likeness of Large Language Model-based Text-to-Speech Systems in Chinese](https://arxiv.org/abs/2505.11200)
*Xihuai Wang,Ziyi Zhao,Siyu Ren,Shao Zhang,Song Li,Xiaoyu Li,Ziwen Wang,Lin Qiu,Guanglu Wan,Xuezhi Cao,Xunliang Cai,Weinan Zhang*

Main category: cs.SD

TL;DR: 论文提出了一种名为Audio Turing Test (ATT)的多维中文语料库数据集ATT-Corpus，并设计了一种简单的评估协议，以解决TTS系统评估中的主观性和多维度不足问题。


<details>
  <summary>Details</summary>
Motivation: 现有的TTS系统评估方法（如MOS）存在主观性和环境不一致性，且缺乏多维度设计，尤其在中文TTS评估中表现明显。

Method: 引入ATT-Corpus数据集和基于图灵测试的评估协议，要求评估者判断语音是否像人类声音。此外，通过微调Qwen2-Audio-Instruct模型（Auto-ATT）实现自动评估。

Result: 实验表明，ATT能有效区分模型在特定能力维度的表现，Auto-ATT与人类评估结果高度一致。

Conclusion: ATT和Auto-ATT为TTS系统提供了快速、可靠的评估工具，解决了现有方法的局限性。

Abstract: Recent advances in large language models (LLMs) have significantly improved
text-to-speech (TTS) systems, enhancing control over speech style, naturalness,
and emotional expression, which brings TTS Systems closer to human-level
performance. Although the Mean Opinion Score (MOS) remains the standard for TTS
System evaluation, it suffers from subjectivity, environmental inconsistencies,
and limited interpretability. Existing evaluation datasets also lack a
multi-dimensional design, often neglecting factors such as speaking styles,
context diversity, and trap utterances, which is particularly evident in
Chinese TTS evaluation. To address these challenges, we introduce the Audio
Turing Test (ATT), a multi-dimensional Chinese corpus dataset ATT-Corpus paired
with a simple, Turing-Test-inspired evaluation protocol. Instead of relying on
complex MOS scales or direct model comparisons, ATT asks evaluators to judge
whether a voice sounds human. This simplification reduces rating bias and
improves evaluation robustness. To further support rapid model development, we
also finetune Qwen2-Audio-Instruct with human judgment data as Auto-ATT for
automatic evaluation. Experimental results show that ATT effectively
differentiates models across specific capability dimensions using its
multi-dimensional design. Auto-ATT also demonstrates strong alignment with
human evaluations, confirming its value as a fast and reliable assessment tool.
The white-box ATT-Corpus and Auto-ATT can be found in ATT Hugging Face
Collection
(https://huggingface.co/collections/meituan/audio-turing-test-682446320368164faeaf38a4).

</details>


### [264] [BanglaFake: Constructing and Evaluating a Specialized Bengali Deepfake Audio Dataset](https://arxiv.org/abs/2505.10885)
*Istiaq Ahmed Fahad,Kamruzzaman Asif,Sifat Sikder*

Main category: cs.SD

TL;DR: 论文介绍了BangalFake数据集，用于孟加拉语深度伪造音频检测，包含12,260真实和13,260伪造语音，并通过定量和定性分析验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决低资源语言（如孟加拉语）在深度伪造音频检测中数据集有限和声学特征细微的挑战。

Method: 使用SOTA文本到语音（TTS）模型生成高质量伪造语音，并通过MOS评分和t-SNE可视化分析数据集。

Result: MOS评分显示伪造语音的自然度为3.40，可懂度为4.01；t-SNE可视化揭示了真实与伪造语音的区分难度。

Conclusion: BangalFake数据集为孟加拉语深度伪造检测提供了关键资源，弥补了低资源语言研究的不足。

Abstract: Deepfake audio detection is challenging for low-resource languages like
Bengali due to limited datasets and subtle acoustic features. To address this,
we introduce BangalFake, a Bengali Deepfake Audio Dataset with 12,260 real and
13,260 deepfake utterances. Synthetic speech is generated using SOTA
Text-to-Speech (TTS) models, ensuring high naturalness and quality. We evaluate
the dataset through both qualitative and quantitative analyses. Mean Opinion
Score (MOS) from 30 native speakers shows Robust-MOS of 3.40 (naturalness) and
4.01 (intelligibility). t-SNE visualization of MFCCs highlights real vs. fake
differentiation challenges. This dataset serves as a crucial resource for
advancing deepfake detection in Bengali, addressing the limitations of
low-resource language research.

</details>


### [265] [Multi-Stage Speaker Diarization for Noisy Classrooms](https://arxiv.org/abs/2505.10879)
*Ali Sartaz Khan,Tolulope Ogunremi,Ahmed Attia,Dorottya Demszky*

Main category: cs.SD

TL;DR: 研究探讨了多阶段说话人日志模型在嘈杂教室环境中的有效性，结合降噪和混合VAD方法显著降低了错误率。


<details>
  <summary>Details</summary>
Motivation: 教室环境中的说话人日志面临录音质量差、背景噪声高、语音重叠和儿童声音难以捕捉等挑战，需探索更有效的解决方案。

Method: 使用Nvidia的NeMo日志流水线，评估降噪对日志准确性的影响，比较多种VAD模型，并探索结合ASR词级时间戳的混合VAD方法。

Result: 降噪显著降低了DER；混合VAD模型在师生实验中DER低至17%，全说话人实验中为45%。

Conclusion: 多阶段日志模型和ASR信息整合在嘈杂教室环境中有效提升说话人日志性能，但需权衡语音检测和说话人混淆问题。

Abstract: Speaker diarization, the process of identifying "who spoke when" in audio
recordings, is essential for understanding classroom dynamics. However,
classroom settings present distinct challenges, including poor recording
quality, high levels of background noise, overlapping speech, and the
difficulty of accurately capturing children's voices. This study investigates
the effectiveness of multi-stage diarization models using Nvidia's NeMo
diarization pipeline. We assess the impact of denoising on diarization accuracy
and compare various voice activity detection (VAD) models, including
self-supervised transformer-based frame-wise VAD models. We also explore a
hybrid VAD approach that integrates Automatic Speech Recognition (ASR)
word-level timestamps with frame-level VAD predictions. We conduct experiments
using two datasets from English speaking classrooms to separate teacher vs.
student speech and to separate all speakers. Our results show that denoising
significantly improves the Diarization Error Rate (DER) by reducing the rate of
missed speech. Additionally, training on both denoised and noisy datasets leads
to substantial performance gains in noisy conditions. The hybrid VAD model
leads to further improvements in speech detection, achieving a DER as low as
17% in teacher-student experiments and 45% in all-speaker experiments. However,
we also identified trade-offs between voice activity detection and speaker
confusion. Overall, our study highlights the effectiveness of multi-stage
diarization models and integrating ASR-based information for enhancing speaker
diarization in noisy classroom environments.

</details>


### [266] [Seeing Sound, Hearing Sight: Uncovering Modality Bias and Conflict of AI models in Sound Localization](https://arxiv.org/abs/2505.11217)
*Yanhao Jia,Ji Xie,S Jivaganesh,Hao Li,Xu Wu,Mengmi Zhang*

Main category: cs.SD

TL;DR: 研究探讨了AI在多模态冲突中的表现，发现人类在声音定位中优于AI，AI倾向于依赖视觉输入。通过微调模型，性能提升并接近人类表现。


<details>
  <summary>Details</summary>
Motivation: 研究多模态AI在跨模态冲突中的表现，尤其是声音定位任务，以了解AI是否偏向某一模态。

Method: 评估领先的多模态模型，并与人类在心理物理学实验中的表现进行对比，包括六种视听条件。通过3D模拟生成数据集微调模型。

Result: 人类在冲突或缺失视觉线索时表现更优，而AI依赖视觉导致性能下降。微调后的模型性能提升，并表现出类似人类的左右定位偏好。

Conclusion: 研究发现感官输入质量和系统架构影响多模态表示的准确性，微调模型可显著改善AI表现。

Abstract: Imagine hearing a dog bark and turning toward the sound only to see a parked
car, while the real, silent dog sits elsewhere. Such sensory conflicts test
perception, yet humans reliably resolve them by prioritizing sound over
misleading visuals. Despite advances in multimodal AI integrating vision and
audio, little is known about how these systems handle cross-modal conflicts or
whether they favor one modality. In this study, we systematically examine
modality bias and conflict resolution in AI sound localization. We assess
leading multimodal models and benchmark them against human performance in
psychophysics experiments across six audiovisual conditions, including
congruent, conflicting, and absent cues. Humans consistently outperform AI,
demonstrating superior resilience to conflicting or missing visuals by relying
on auditory information. In contrast, AI models often default to visual input,
degrading performance to near chance levels. To address this, we finetune a
state-of-the-art model using a stereo audio-image dataset generated via 3D
simulations. Even with limited training data, the refined model surpasses
existing benchmarks. Notably, it also mirrors human-like horizontal
localization bias favoring left-right precision-likely due to the stereo audio
structure reflecting human ear placement. These findings underscore how sensory
input quality and system architecture shape multimodal representation accuracy.

</details>


### [267] [Improving Inference-Time Optimisation for Vocal Effects Style Transfer with a Gaussian Prior](https://arxiv.org/abs/2505.11315)
*Chin-Yun Yu,Marco A. Martínez-Ramírez,Junghyun Koo,Wei-Hsiang Liao,Yuki Mitsufuji,György Fazekas*

Main category: cs.SD

TL;DR: ST-ITO方法通过引入高斯先验优化音频效果传输，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决ST-ITO方法在音频效果传输中因忽略参数空间分布导致的不现实或偏差结果的问题。

Method: 引入基于DiffVox数据集的高斯先验，将优化问题转化为最大后验估计。

Result: 在MedleyDB数据集上，参数均方误差降低33%，主观评价也显示方法更优。

Conclusion: 结合先验知识可提升音频效果传输的效率和真实性。

Abstract: Style Transfer with Inference-Time Optimisation (ST-ITO) is a recent approach
for transferring the applied effects of a reference audio to a raw audio track.
It optimises the effect parameters to minimise the distance between the style
embeddings of the processed audio and the reference. However, this method
treats all possible configurations equally and relies solely on the embedding
space, which can lead to unrealistic or biased results. We address this pitfall
by introducing a Gaussian prior derived from a vocal preset dataset, DiffVox,
over the parameter space. The resulting optimisation is equivalent to
maximum-a-posteriori estimation. Evaluations on vocal effects transfer on the
MedleyDB dataset show significant improvements across metrics compared to
baselines, including a blind audio effects estimator, nearest-neighbour
approaches, and uncalibrated ST-ITO. The proposed calibration reduces parameter
mean squared error by up to 33% and matches the reference style better.
Subjective evaluations with 16 participants confirm our method's superiority,
especially in limited data regimes. This work demonstrates how incorporating
prior knowledge in inference time enhances audio effects transfer, paving the
way for more effective and realistic audio processing systems.

</details>


### [268] [Machine Learning Approaches to Vocal Register Classification in Contemporary Male Pop Music](https://arxiv.org/abs/2505.11378)
*Alexander Kim,Charlotte Botha*

Main category: cs.SD

TL;DR: 论文提出两种基于梅尔频谱图像纹理特征的方法，用于分类男性流行音乐音频信号中的声区，并开发了自动声区分析软件AVRA。


<details>
  <summary>Details</summary>
Motivation: 流行音乐中歌手常使用多种音色和质感，导致声区识别困难，尤其是在过渡区（passagio）附近。

Method: 通过支持向量机（SVM）和卷积神经网络（CNN）模型分析梅尔频谱图像的纹理特征。

Result: 两种模型均能稳定分类声区，展示了跨更多声音类型和音乐风格的分类潜力。

Conclusion: 研究支持了更鲁棒的声区分类可能性，并开发了实用工具AVRA。

Abstract: For singers of all experience levels, one of the most daunting challenges in
learning technical repertoire is navigating placement and vocal register in and
around the passagio (passage between chest voice and head voice registers).
Particularly in pop music, where a single artist may use a variety of timbre's
and textures to achieve a desired quality, it can be difficult to identify what
vocal register within the vocal range a singer is using. This paper presents
two methods for classifying vocal registers in an audio signal of male pop
music through the analysis of textural features of mel-spectrogram images.
Additionally, we will discuss the practical integration of these models for
vocal analysis tools, and introduce a concurrently developed software called
AVRA which stands for Automatic Vocal Register Analysis. Our proposed methods
achieved consistent classification of vocal register through both Support
Vector Machine (SVM) and Convolutional Neural Network (CNN) models, which
supports the promise of more robust classification possibilities across more
voice types and genres of singing.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [269] [A Cautionary Tale on Integrating Studies with Disparate Outcome Measures for Causal Inference](https://arxiv.org/abs/2505.11014)
*Harsh Parikh,Trang Quynh Nguyen,Elizabeth A. Stuart,Kara E. Rudolph,Caleb H. Miles*

Main category: stat.ME

TL;DR: 论文研究了在数据集合并中，当结果测量不一致时，如何通过不同强度的假设实现效率提升，同时指出错误假设可能导致偏差。


<details>
  <summary>Details</summary>
Motivation: 解决数据整合中结果测量不一致的问题，探索在不同假设下整合数据集是否能提高效率。

Method: 引入三种不同强度的假设链接不同结果测量，通过理论和实证分析评估效率提升和偏差风险。

Result: 仅在最强假设下整合能提高渐近效率，但假设错误会导致偏差；较温和假设可能在有限样本中提升效率，但随样本量增加效果减弱。

Conclusion: 在整合不同结果测量的数据集时，需谨慎选择假设以避免偏差，为研究者提供实用指导。

Abstract: Data integration approaches are increasingly used to enhance the efficiency
and generalizability of studies. However, a key limitation of these methods is
the assumption that outcome measures are identical across datasets -- an
assumption that often does not hold in practice. Consider the following opioid
use disorder (OUD) studies: the XBOT trial and the POAT study, both evaluating
the effect of medications for OUD on withdrawal symptom severity (not the
primary outcome of either trial). While XBOT measures withdrawal severity using
the subjective opiate withdrawal scale, POAT uses the clinical opiate
withdrawal scale. We analyze this realistic yet challenging setting where
outcome measures differ across studies and where neither study records both
types of outcomes. Our paper studies whether and when integrating studies with
disparate outcome measures leads to efficiency gains. We introduce three sets
of assumptions -- with varying degrees of strength -- linking both outcome
measures. Our theoretical and empirical results highlight a cautionary tale:
integration can improve asymptotic efficiency only under the strongest
assumption linking the outcomes. However, misspecification of this assumption
leads to bias. In contrast, a milder assumption may yield finite-sample
efficiency gains, yet these benefits diminish as sample size increases. We
illustrate these trade-offs via a case study integrating the XBOT and POAT
datasets to estimate the comparative effect of two medications for opioid use
disorder on withdrawal symptoms. By systematically varying the assumptions
linking the SOW and COW scales, we show potential efficiency gains and the
risks of bias. Our findings emphasize the need for careful assumption selection
when fusing datasets with differing outcome measures, offering guidance for
researchers navigating this common challenge in modern data integration.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [270] [Quantum thermodynamics and semi-definite optimization](https://arxiv.org/abs/2505.04514)
*Nana Liu,Michele Minervini,Dhrumil Patel,Mark M. Wilde*

Main category: quant-ph

TL;DR: 论文探讨了量子热力学中最小能量问题与优化理论中的半定规划（SDP）问题的数学等价性，提出了一种基于Jaynes思想的梯度上升方法，用于解决这两类问题，并提供了算法收敛性保证。


<details>
  <summary>Details</summary>
Motivation: 研究量子热力学中系统的最小能量问题与优化理论中的SDP问题的数学等价性，旨在通过物理视角为优化算法提供新的理论基础。

Method: 采用Jaynes的思想，将最小自由能问题转化为对偶化学势最大化问题，利用梯度上升方法求解，并扩展到经典和量子-经典混合算法中。

Result: 提出的梯度上升方法在低温和SDP问题中表现良好，算法收敛速度快，且最小自由能近似于最小能量。

Conclusion: 通过量子热力学的视角，为优化算法提供了物理动机，解释了现有算法的有效性，并提出了新的解决方案。

Abstract: In quantum thermodynamics, a system is described by a Hamiltonian and a list
of non-commuting charges representing conserved quantities like particle number
or electric charge, and an important goal is to determine the system's minimum
energy in the presence of these conserved charges. In optimization theory, a
semi-definite program (SDP) involves a linear objective function optimized over
the cone of positive semi-definite operators intersected with an affine space.
These problems arise from differing motivations in the physics and optimization
communities and are phrased using very different terminology, yet they are
essentially identical mathematically. By adopting Jaynes' mindset motivated by
quantum thermodynamics, we observe that minimizing free energy in the
aforementioned thermodynamics problem, instead of energy, leads to an elegant
solution in terms of a dual chemical potential maximization problem that is
concave in the chemical potential parameters. As such, one can employ standard
(stochastic) gradient ascent methods to find the optimal values of these
parameters, and these methods are guaranteed to converge quickly. At low
temperature, the minimum free energy provides an excellent approximation for
the minimum energy. We then show how this Jaynes-inspired gradient-ascent
approach can be used in both first- and second-order classical and hybrid
quantum-classical algorithms for minimizing energy, and equivalently, how it
can be used for solving SDPs, with guarantees on the runtimes of the
algorithms. The approach discussed here is well grounded in quantum
thermodynamics and, as such, provides physical motivation underpinning why
algorithms published fifty years after Jaynes' seminal work, including the
matrix multiplicative weights update method, the matrix exponentiated gradient
update method, and their quantum algorithmic generalizations, perform well at
solving SDPs.

</details>


### [271] [Generalization Bounds for Quantum Learning via Rényi Divergences](https://arxiv.org/abs/2505.11025)
*Naqueeb Ahmad Warsi,Ayanava Dasgupta,Masahito Hayashi*

Main category: quant-ph

TL;DR: 本文通过建立量子学习算法期望泛化误差的新上界，推进了量子学习的理论理解，利用了Caro等人（2024）的框架和新定义的期望真实损失。


<details>
  <summary>Details</summary>
Motivation: 旨在深化对量子学习算法的理论理解，并通过新的上界提供更优的泛化误差估计。

Method: 利用量子与经典Rényi散度，通过变分方法评估量子Rényi散度（包括Petz和新引入的修正三明治量子Rényi散度），推导泛化误差上界。

Result: 数值和分析表明，基于修正三明治量子Rényi散度的上界优于Petz散度。此外，通过两种技术提供了概率泛化误差上界。

Conclusion: 修正三明治量子Rényi散度在泛化误差上界中表现更优，为量子学习提供了更精确的理论工具。

Abstract: This work advances the theoretical understanding of quantum learning by
establishing a new family of upper bounds on the expected generalization error
of quantum learning algorithms, leveraging the framework introduced by Caro et
al. (2024) and a new definition for the expected true loss. Our primary
contribution is the derivation of these bounds in terms of quantum and
classical R\'enyi divergences, utilizing a variational approach for evaluating
quantum R\'enyi divergences, specifically the Petz and a newly introduced
modified sandwich quantum R\'enyi divergence. Analytically and numerically, we
demonstrate the superior performance of the bounds derived using the modified
sandwich quantum R\'enyi divergence compared to those based on the Petz
divergence. Furthermore, we provide probabilistic generalization error bounds
using two distinct techniques: one based on the modified sandwich quantum
R\'enyi divergence and classical R\'enyi divergence, and another employing
smooth max R\'enyi divergence.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [272] [Vaiage: A Multi-Agent Solution to Personalized Travel Planning](https://arxiv.org/abs/2505.10922)
*Binwen Liu,Jiexi Ge,Jiamin Wang*

Main category: cs.MA

TL;DR: Vaiage是一个基于多智能体框架和大型语言模型的旅行规划系统，通过自然语言交互和实时反馈优化行程。


<details>
  <summary>Details</summary>
Motivation: 传统旅行规划平台无法满足动态、个性化和实时交互的需求。

Method: 采用图结构多智能体框架，利用大型语言模型进行目标推荐和顺序规划，结合自然语言交互和地图反馈。

Result: Vaiage在实验中平均得分8.5/10，显著优于无策略和无外部API的变体，特别是在可行性方面。

Conclusion: 结合大型语言模型推理和符号智能体协调，Vaiage在开放式的现实规划任务中表现出色。

Abstract: Planning trips is a cognitively intensive task involving conflicting user
preferences, dynamic external information, and multi-step temporal-spatial
optimization. Traditional platforms often fall short - they provide static
results, lack contextual adaptation, and fail to support real-time interaction
or intent refinement.
  Our approach, Vaiage, addresses these challenges through a graph-structured
multi-agent framework built around large language models (LLMs) that serve as
both goal-conditioned recommenders and sequential planners. LLMs infer user
intent, suggest personalized destinations and activities, and synthesize
itineraries that align with contextual constraints such as budget, timing,
group size, and weather. Through natural language interaction, structured tool
use, and map-based feedback loops, Vaiage enables adaptive, explainable, and
end-to-end travel planning grounded in both symbolic reasoning and
conversational understanding.
  To evaluate Vaiage, we conducted human-in-the-loop experiments using
rubric-based GPT-4 assessments and qualitative feedback. The full system
achieved an average score of 8.5 out of 10, outperforming the no-strategy (7.2)
and no-external-API (6.8) variants, particularly in feasibility. Qualitative
analysis indicated that agent coordination - especially the Strategy and
Information Agents - significantly improved itinerary quality by optimizing
time use and integrating real-time context. These results demonstrate the
effectiveness of combining LLM reasoning with symbolic agent coordination in
open-ended, real-world planning tasks.

</details>


### [273] [Explaining Strategic Decisions in Multi-Agent Reinforcement Learning for Aerial Combat Tactics](https://arxiv.org/abs/2505.11311)
*Ardian Selmonaj,Alessandro Antonucci,Adrian Schneider,Michael Rüegsegger,Matthias Sommer*

Main category: cs.MA

TL;DR: 论文探讨了多智能体强化学习（MARL）在军事战略规划中的应用，强调了可解释性对信任和安全的重要性，并通过模拟空战场景评估了现有可解释性方法。


<details>
  <summary>Details</summary>
Motivation: MARL在复杂军事场景中的应用受到可解释性不足的限制，而可解释性是确保信任、安全和与人类策略一致的关键。

Method: 研究回顾并评估了现有MARL可解释性方法，并将其应用于模拟空战场景，以获取模型行为的解释性见解。

Result: 通过将AI生成的战术与人类可理解的推理联系起来，研究强调了透明性对可靠部署和人机交互的重要性。

Conclusion: 研究突出了可解释性在推动MARL用于防御操作中的关键作用，支持战略规划和军事人员的培训。

Abstract: Artificial intelligence (AI) is reshaping strategic planning, with
Multi-Agent Reinforcement Learning (MARL) enabling coordination among
autonomous agents in complex scenarios. However, its practical deployment in
sensitive military contexts is constrained by the lack of explainability, which
is an essential factor for trust, safety, and alignment with human strategies.
This work reviews and assesses current advances in explainability methods for
MARL with a focus on simulated air combat scenarios. We proceed by adapting
various explainability techniques to different aerial combat scenarios to gain
explanatory insights about the model behavior. By linking AI-generated tactics
with human-understandable reasoning, we emphasize the need for transparency to
ensure reliable deployment and meaningful human-machine interaction. By
illuminating the crucial importance of explainability in advancing MARL for
operational defense, our work supports not only strategic planning but also the
training of military personnel with insightful and comprehensible analyses.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [274] [Creating General User Models from Computer Use](https://arxiv.org/abs/2505.10831)
*Omar Shaikh,Shardul Sapkota,Shan Rizvi,Eric Horvitz,Joon Sung Park,Diyi Yang,Michael S. Bernstein*

Main category: cs.HC

TL;DR: 本文提出了一种通用用户模型（GUM）架构，通过观察用户与计算机的交互来学习用户行为和偏好，支持多模态输入和灵活推理，实现了对用户需求的主动预测和响应。


<details>
  <summary>Details</summary>
Motivation: 当前用户模型局限于特定应用，缺乏灵活推理能力，无法实现全面理解用户的目标。GUM旨在通过多模态观察构建通用用户模型，填补这一空白。

Method: GUM通过输入非结构化用户观察（如设备截图），生成置信度加权的用户知识命题，并支持多模态推理、上下文检索和动态更新。

Result: 实验表明，GUM能准确推断用户需求，并基于此构建的助手能主动执行用户未明确请求的操作。

Conclusion: GUM为实现长期的人机交互愿景提供了新方法，支持开发能预测用户需求的交互系统。

Abstract: Human-computer interaction has long imagined technology that understands
us-from our preferences and habits, to the timing and purpose of our everyday
actions. Yet current user models remain fragmented, narrowly tailored to
specific apps, and incapable of the flexible reasoning required to fulfill
these visions. This paper presents an architecture for a general user model
(GUM) that learns about you by observing any interaction you have with your
computer. The GUM takes as input any unstructured observation of a user (e.g.,
device screenshots) and constructs confidence-weighted propositions that
capture that user knowledge and preferences. GUMs can infer that a user is
preparing for a wedding they're attending from messages with a friend. Or
recognize that a user is struggling with a collaborator's feedback on a draft
by observing multiple stalled edits and a switch to reading related work. GUMs
introduce an architecture that infers new propositions about a user from
multimodal observations, retrieves related propositions for context, and
continuously revises existing propositions. To illustrate the breadth of
applications that GUMs enable, we demonstrate how they augment chat-based
assistants with context, manage OS notifications to selectively surface
important information, and enable interactive agents that adapt to preferences
across apps. We also instantiate proactive assistants (GUMBOs) that discover
and execute useful suggestions on a user's behalf using their GUM. In our
evaluations, we find that GUMs make calibrated and accurate inferences about
users, and that assistants built on GUMs proactively identify and perform
actions that users wouldn't think to request explicitly. Altogether, GUMs
introduce methods that leverage multimodal models to understand unstructured
context, enabling long-standing visions of HCI and entirely new interactive
systems that anticipate user needs.

</details>


### [275] [Large Language Model Use Impact Locus of Control](https://arxiv.org/abs/2505.11406)
*Jenny Xiyu Fu,Brennan Antone,Kowe Kadoma,Malte Jung*

Main category: cs.HC

TL;DR: AI协作写作对用户控制点心理的影响，就业状态是关键因素。


<details>
  <summary>Details</summary>
Motivation: 探讨AI工具如何通过协作写作影响人们的自我认知和控制点心理。

Method: 通过462名参与者的实证研究，结合定量和定性分析。

Result: 就业者更依赖AI且控制点内化，失业者个人能动性降低。

Conclusion: 研究引发关于AI如何塑造个人能动性和身份的广泛讨论。

Abstract: As AI tools increasingly shape how we write, they may also quietly reshape
how we perceive ourselves. This paper explores the psychological impact of
co-writing with AI on people's locus of control. Through an empirical study
with 462 participants, we found that employment status plays a critical role in
shaping users' reliance on AI and their locus of control. Current results
demonstrated that employed participants displayed higher reliance on AI and a
shift toward internal control, while unemployed users tended to experience a
reduction in personal agency. Through quantitative results and qualitative
observations, this study opens a broader conversation about AI's role in
shaping personal agency and identity.

</details>


### [276] [EdgeWisePersona: A Dataset for On-Device User Profiling from Natural Language Interactions](https://arxiv.org/abs/2505.11417)
*Patryk Bartkowiak,Michal Podstawski*

Main category: cs.HC

TL;DR: 论文介绍了一个用于评估和改进可部署在边缘设备上的小型语言模型的新数据集和评估基准，重点是智能家居环境中多会话自然语言交互的用户画像。


<details>
  <summary>Details</summary>
Motivation: 解决小型语言模型在边缘设备上处理用户行为建模时的性能不足问题，同时保护用户隐私和减少延迟。

Method: 使用结构化用户画像和大语言模型生成模拟交互会话，评估小型模型在用户画像重建任务中的表现。

Result: 小型模型在重建用户画像方面有一定能力，但性能显著低于大型模型。

Conclusion: 该数据集为开发隐私保护的智能AI系统提供了关键支持，推动直接在用户设备上学习和适应的技术发展。

Abstract: This paper introduces a novel dataset and evaluation benchmark designed to
assess and improve small language models deployable on edge devices, with a
focus on user profiling from multi-session natural language interactions in
smart home environments. At the core of the dataset are structured user
profiles, each defined by a set of routines - context-triggered, repeatable
patterns of behavior that govern how users interact with their home systems.
Using these profiles as input, a large language model (LLM) generates
corresponding interaction sessions that simulate realistic, diverse, and
context-aware dialogues between users and their devices.
  The primary task supported by this dataset is profile reconstruction:
inferring user routines and preferences solely from interactions history. To
assess how well current models can perform this task under realistic
conditions, we benchmarked several state-of-the-art compact language models and
compared their performance against large foundation models. Our results show
that while small models demonstrate some capability in reconstructing profiles,
they still fall significantly short of large models in accurately capturing
user behavior. This performance gap poses a major challenge - particularly
because on-device processing offers critical advantages, such as preserving
user privacy, minimizing latency, and enabling personalized experiences without
reliance on the cloud. By providing a realistic, structured testbed for
developing and evaluating behavioral modeling under these constraints, our
dataset represents a key step toward enabling intelligent, privacy-respecting
AI systems that learn and adapt directly on user-owned devices.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [277] [GRNN:Recurrent Neural Network based on Ghost Features for Video Super-Resolution](https://arxiv.org/abs/2505.10577)
*Yutong Guo*

Main category: eess.IV

TL;DR: 论文提出了一种基于“Ghost特征”的方法，用于减少视频超分辨率（VSR）模型中的特征冗余，并结合RNN解决了梯度消失问题，提升了PSNR和SSIM指标。


<details>
  <summary>Details</summary>
Motivation: 现代基于CNN的VSR系统计算成本高，且存在特征冗余问题，但这一问题在VSR领域很少被讨论。作者通过实验发现VSR模型中许多特征相似，因此提出减少冗余。

Method: 提出使用“Ghost特征”减少冗余，并结合RNN解决梯度消失问题。模型输入包括当前帧、下一帧、前一帧输出和隐藏状态。

Result: 在多个基准模型和数据集上的实验表明，PSNR和SSIM有所提升，视频纹理细节也得到更好保留。

Conclusion: 通过减少特征冗余和优化RNN结构，提出的方法在VSR任务中表现更优。

Abstract: Modern video super-resolution (VSR) systems based on convolutional neural
networks (CNNs) require huge computational costs. The problem of feature
redundancy is present in most models in many domains, but is rarely discussed
in VSR. We experimentally observe that many features in VSR models are also
similar to each other, so we propose to use "Ghost features" to reduce this
redundancy. We also analyze the so-called "gradient disappearance" phenomenon
generated by the conventional recurrent convolutional network (RNN) model, and
combine the Ghost module with RNN to complete the modeling on time series. The
current frame is used as input to the model together with the next frame, the
output of the previous frame and the hidden state. Extensive experiments on
several benchmark models and datasets show that the PSNR and SSIM of our
proposed modality are improved to some extent. Some texture details in the
video are also better preserved.

</details>


### [278] [Predicting Risk of Pulmonary Fibrosis Formation in PASC Patients](https://arxiv.org/abs/2505.10691)
*Wanying Dou,Gorkem Durak,Koushik Biswas,Ziliang Hong,Andrea Mia Bejar,Elif Keles,Kaan Akin,Sukru Mehmet Erturk,Alpay Medetalibeyoglu,Marc Sala,Alexander Misharin,Hatice Savas,Mary Salvatore,Sachin Jambawalikar,Drew Torigian,Jayaram K. Udupa,Ulas Bagci*

Main category: eess.IV

TL;DR: 本文提出了一种结合深度学习和放射组学的多中心胸部CT分析框架，用于预测COVID-19后遗症（PASC）相关的肺纤维化，准确率达82.2%，AUC为85.5%。


<details>
  <summary>Details</summary>
Motivation: PASC（即Long COVID）的长期影响尚不确定，尤其是肺纤维化可能影响患者的长期呼吸功能。临床评估和诊断面临挑战，需要新的方法。

Method: 采用卷积神经网络（CNN）和可解释的特征提取技术，结合Grad-CAM可视化和放射组学特征分析。

Result: 模型在分类任务中达到82.2%的准确率和85.5%的AUC，首次在文献中展示了深度学习在PASC相关肺纤维化早期检测中的潜力。

Conclusion: 深度学习驱动的计算方法在PASC相关肺纤维化的早期检测和风险评估中具有潜力，为临床提供了新工具。

Abstract: While the acute phase of the COVID-19 pandemic has subsided, its long-term
effects persist through Post-Acute Sequelae of COVID-19 (PASC), commonly known
as Long COVID. There remains substantial uncertainty regarding both its
duration and optimal management strategies. PASC manifests as a diverse array
of persistent or newly emerging symptoms--ranging from fatigue, dyspnea, and
neurologic impairments (e.g., brain fog), to cardiovascular, pulmonary, and
musculoskeletal abnormalities--that extend beyond the acute infection phase.
This heterogeneous presentation poses substantial challenges for clinical
assessment, diagnosis, and treatment planning. In this paper, we focus on
imaging findings that may suggest fibrotic damage in the lungs, a critical
manifestation characterized by scarring of lung tissue, which can potentially
affect long-term respiratory function in patients with PASC. This study
introduces a novel multi-center chest CT analysis framework that combines deep
learning and radiomics for fibrosis prediction. Our approach leverages
convolutional neural networks (CNNs) and interpretable feature extraction,
achieving 82.2% accuracy and 85.5% AUC in classification tasks. We demonstrate
the effectiveness of Grad-CAM visualization and radiomics-based feature
analysis in providing clinically relevant insights for PASC-related lung
fibrosis prediction. Our findings highlight the potential of deep
learning-driven computational methods for early detection and risk assessment
of PASC-related lung fibrosis--presented for the first time in the literature.

</details>


### [279] [ROIsGAN: A Region Guided Generative Adversarial Framework for Murine Hippocampal Subregion Segmentation](https://arxiv.org/abs/2505.10687)
*Sayed Mehedi Azim,Brian Corbett,Iman Dehzangi*

Main category: eess.IV

TL;DR: 论文提出了一种名为ROIsGAN的新方法，用于自动分割海马体亚区，并提供了四个新的免疫组化数据集。ROIsGAN在性能上优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 海马体亚区的精确分割对理解疾病机制和开发治疗方法至关重要，但目前缺乏针对组织图像的自动化分割方法。

Method: 提出了ROIsGAN，一种基于U-Net的生成对抗网络，结合区域引导判别器损失（Dice和二元交叉熵损失）来优化边界和结构细节。

Result: ROIsGAN在DG、CA1和CA3亚区的分割任务中表现优于传统模型，Dice分数提升1-10%，IoU提升达11%。

Conclusion: 该研究为海马体亚区的自动化分割提供了基础数据集和方法，支持神经科学研究的可扩展、高精度分析。

Abstract: The hippocampus, a critical brain structure involved in memory processing and
various neurodegenerative and psychiatric disorders, comprises three key
subregions: the dentate gyrus (DG), Cornu Ammonis 1 (CA1), and Cornu Ammonis 3
(CA3). Accurate segmentation of these subregions from histological tissue
images is essential for advancing our understanding of disease mechanisms,
developmental dynamics, and therapeutic interventions. However, no existing
methods address the automated segmentation of hippocampal subregions from
tissue images, particularly from immunohistochemistry (IHC) images. To bridge
this gap, we introduce a novel set of four comprehensive murine hippocampal IHC
datasets featuring distinct staining modalities: cFos, NeuN, and multiplexed
stains combining cFos, NeuN, and either {\Delta}FosB or GAD67, capturing
structural, neuronal activity, and plasticity associated information.
Additionally, we propose ROIsGAN, a region-guided U-Net-based generative
adversarial network tailored for hippocampal subregion segmentation. By
leveraging adversarial learning, ROIsGAN enhances boundary delineation and
structural detail refinement through a novel region-guided discriminator loss
combining Dice and binary cross-entropy loss. Evaluated across DG, CA1, and CA3
subregions, ROIsGAN consistently outperforms conventional segmentation models,
achieving performance gains ranging from 1-10% in Dice score and up to 11% in
Intersection over Union (IoU), particularly under challenging staining
conditions. Our work establishes foundational datasets and methods for
automated hippocampal segmentation, enabling scalable, high-precision analysis
of tissue images in neuroscience research. Our generated datasets, proposed
model as a standalone tool, and its corresponding source code are publicly
available at: https://github.com/MehediAzim/ROIsGAN

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [280] [Learning hidden cascades via classification](https://arxiv.org/abs/2505.11228)
*Derrick Gilchrist Edward Manoharan,Anubha Goel,Alexandros Iosifidis,Henri Hansen,Juho Kanniainen*

Main category: cs.SI

TL;DR: 论文提出了一种基于部分可观测性的机器学习框架（Distribution Classification），用于推断社交网络中传播动态的特征，适用于个体状态不可直接观测的场景。


<details>
  <summary>Details</summary>
Motivation: 现实世界中，个体的感染或知情状态往往不可直接观测，但中间指标（如症状）可提供传播过程的重要信息。研究旨在解决这一部分可观测性问题。

Method: 提出Distribution Classification方法，利用分类器的能力推断潜在的传播动态，并在合成网络和真实内幕交易网络中验证。

Result: 方法在复杂网络（尤其是高循环连通性网络）中表现良好，支持其在无法直接观测个体状态的实际场景中的应用。

Conclusion: 该框架为分析真实世界传播现象提供了有效工具，尤其在部分可观测条件下表现优异。

Abstract: The spreading dynamics in social networks are often studied under the
assumption that individuals' statuses, whether informed or infected, are fully
observable. However, in many real-world situations, such statuses remain
unobservable, which is crucial for determining an individual's potential to
further spread the infection. While this final status is hidden, intermediate
indicators such as symptoms of infection are observable and provide important
insights into the spread process. We propose a partial observability-aware
Machine Learning framework to learn the characteristics of the spreading model.
We term the method Distribution Classification, which utilizes the power of
classifiers to infer the underlying transmission dynamics. We evaluate our
method on two types of synthetic networks and extend the study to a real-world
insider trading network. Results show that the method performs well, especially
on complex networks with high cyclic connectivity, supporting its utility in
analyzing real-world spreading phenomena where direct observation of individual
statuses is not possible.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [281] [System Identification and Control Using Lyapunov-Based Deep Neural Networks without Persistent Excitation: A Concurrent Learning Approach](https://arxiv.org/abs/2505.10678)
*Rebecca G. Hart,Omkar Sudhir Patil,Zachary I. Bell,Warren E. Dixon*

Main category: eess.SY

TL;DR: 论文提出了一种基于DNN的控制器，首次实现了轨迹跟踪与在线系统识别的同步，无需持续激励。通过新的并发学习适应律，DNN参数估计收敛到理想值附近，同时保证跟踪误差、权重估计误差和观测误差的稳定性。实验显示性能显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有DNN控制方法多关注跟踪误差收敛，而忽略了利用DNN进行系统动态识别的挑战。本文旨在解决这一问题。

Method: 构建了两种新的并发学习适应律，用于调整DNN各层权重，要求DNN的Jacobian满足有限时间激励条件。通过Lyapunov稳定性分析确保误差收敛。

Result: 实验表明，在相同初始和操作条件下，函数逼近性能提升40.5%至73.6%，同时保持相似的跟踪误差和控制开销。在轨迹外数据点上，函数逼近性能提升58.88%至74.75%。

Conclusion: 该方法在轨迹跟踪和系统识别方面均表现出色，为DNN在控制应用中的进一步研究提供了新思路。

Abstract: Deep Neural Networks (DNNs) are increasingly used in control applications due
to their powerful function approximation capabilities. However, many existing
formulations focus primarily on tracking error convergence, often neglecting
the challenge of identifying the system dynamics using the DNN. This paper
presents the first result on simultaneous trajectory tracking and online system
identification using a DNN-based controller, without requiring persistent
excitation. Two new concurrent learning adaptation laws are constructed for the
weights of all the layers of the DNN, achieving convergence of the DNN's
parameter estimates to a neighborhood of their ideal values, provided the DNN's
Jacobian satisfies a finite-time excitation condition. A Lyapunov-based
stability analysis is conducted to ensure convergence of the tracking error,
weight estimation errors, and observer errors to a neighborhood of the origin.
Simulations performed on a range of systems and trajectories, with the same
initial and operating conditions, demonstrated 40.5% to 73.6% improvement in
function approximation performance compared to the baseline, while maintaining
a similar tracking error and control effort. Simulations evaluating function
approximation capabilities on data points outside of the trajectory resulted in
58.88% and 74.75% improvement in function approximation compared to the
baseline.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [282] [Anti-aliasing of neural distortion effects via model fine tuning](https://arxiv.org/abs/2505.11375)
*Alistair Carson,Alec Wright,Stefan Bilbao*

Main category: eess.AS

TL;DR: 提出了一种通过师生微调方法减少神经网络模型中频率混叠的技术，显著抑制了LSTM和TCN中的混叠现象。


<details>
  <summary>Details</summary>
Motivation: 神经网络在吉他失真效果建模中广泛应用，但高频率和高增益输入会导致频率混叠问题。

Method: 采用师生微调方法，教师模型为预训练且参数冻结的模型，学生模型通过微调学习减少混叠。

Result: 该方法显著减少了混叠现象，效果优于两倍过采样，但对谐波失真有一定影响。

Conclusion: LSTM模型在抗混叠和保持与模拟参考设备感知相似性方面表现最佳。

Abstract: Neural networks have become ubiquitous with guitar distortion effects
modelling in recent years. Despite their ability to yield perceptually
convincing models, they are susceptible to frequency aliasing when driven by
high frequency and high gain inputs. Nonlinear activation functions create both
the desired harmonic distortion and unwanted aliasing distortion as the
bandwidth of the signal is expanded beyond the Nyquist frequency. Here, we
present a method for reducing aliasing in neural models via a teacher-student
fine tuning approach, where the teacher is a pre-trained model with its weights
frozen, and the student is a copy of this with learnable parameters. The
student is fine-tuned against an aliasing-free dataset generated by passing
sinusoids through the original model and removing non-harmonic components from
the output spectra. Our results show that this method significantly suppresses
aliasing for both long-short-term-memory networks (LSTM) and temporal
convolutional networks (TCN). In the majority of our case studies, the
reduction in aliasing was greater than that achieved by two times oversampling.
One side-effect of the proposed method is that harmonic distortion components
are also affected. This adverse effect was found to be model-dependent, with
the LSTM models giving the best balance between anti-aliasing and preserving
the perceived similarity to an analog reference device.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [283] [On Next-Token Prediction in LLMs: How End Goals Determine the Consistency of Decoding Algorithms](https://arxiv.org/abs/2505.11183)
*Jacob Trauger,Ambuj Tewari*

Main category: stat.ML

TL;DR: 本文研究了大型语言模型中基于交叉熵损失的概率性下一词预测方法，分析了多种解码算法（贪婪、前瞻、随机采样和温度缩放随机采样）与不同目标损失函数的一致性。研究发现，随机采样在模拟真实概率分布时具有一致性，而其他目标（如最小化序列的0-1损失）则没有多项式时间最优算法。解码算法的选择需根据目标（信息检索或创意生成）而定。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型中下一词预测解码算法与不同目标损失函数的一致性，填补了该领域理论研究的空白。

Method: 分析了贪婪、前瞻、随机采样和温度缩放随机采样等解码算法，并研究了它们与不同目标损失函数的一致性。

Result: 随机采样在模拟真实概率分布时一致，而其他目标无多项式时间最优算法。解码算法的选择需基于目标（信息检索或创意生成）。

Conclusion: 解码算法的选择对目标至关重要，现有算法在许多场景下缺乏理论依据。

Abstract: Probabilistic next-token prediction trained using cross-entropy loss is the
basis of most large language models. Given a sequence of previous values,
next-token prediction assigns a probability to each possible next value in the
vocabulary. There are many ways to use next-token prediction to output token
sequences. This paper examines a few of these algorithms (greedy, lookahead,
random sampling, and temperature-scaled random sampling) and studies their
consistency with respect to various goals encoded as loss functions. Although
consistency of surrogate losses with respect to a target loss function is a
well researched topic, we are the first to study it in the context of LLMs (to
the best of our knowledge). We find that, so long as next-token prediction
converges to its true probability distribution, random sampling is consistent
with outputting sequences that mimic sampling from the true probability
distribution. For the other goals, such as minimizing the 0-1 loss on the
entire sequence, we show no polynomial-time algorithm is optimal for all
probability distributions and all decoding algorithms studied are only optimal
for a subset of probability distributions. When analyzing these results, we see
that there is a dichotomy created between the goals of information retrieval
and creative generation for the decoding algorithms. This shows that choosing
the correct decoding algorithm based on the desired goal is extremely important
and many of the ones used are lacking theoretical grounding in numerous
scenarios.

</details>


### [284] [An Exponential Averaging Process with Strong Convergence Properties](https://arxiv.org/abs/2505.10605)
*Frederik Köhne,Anton Schiela*

Main category: stat.ML

TL;DR: 论文提出了一种改进的指数移动平均方法（p-EMA），通过调整权重分配策略，解决了传统EMA在随机收敛性上的不足，并提供了收敛性保证。


<details>
  <summary>Details</summary>
Motivation: 传统指数移动平均（EMA）在随机动态系统中无法实现强收敛性，因为其对最新观测值的权重分配固定，导致噪声无法降至零。

Method: 提出p-EMA方法，将最新观测值的权重以次谐波速率递减至零，从而改进收敛性。

Result: 在温和的自相关假设下，p-EMA具有随机收敛性保证。

Conclusion: p-EMA不仅解决了EMA的收敛性问题，还为随机梯度下降（SGD）的自适应步长控制提供了理论支持。

Abstract: Averaging, or smoothing, is a fundamental approach to obtain stable,
de-noised estimates from noisy observations. In certain scenarios, observations
made along trajectories of random dynamical systems are of particular interest.
One popular smoothing technique for such a scenario is exponential moving
averaging (EMA), which assigns observations a weight that decreases
exponentially in their age, thus giving younger observations a larger weight.
However, EMA fails to enjoy strong stochastic convergence properties, which
stems from the fact that the weight assigned to the youngest observation is
constant over time, preventing the noise in the averaged quantity from
decreasing to zero. In this work, we consider an adaptation to EMA, which we
call $p$-EMA, where the weights assigned to the last observations decrease to
zero at a subharmonic rate. We provide stochastic convergence guarantees for
this kind of averaging under mild assumptions on the autocorrelations of the
underlying random dynamical system. We further discuss the implications of our
results for a recently introduced adaptive step size control for Stochastic
Gradient Descent (SGD), which uses $p$-EMA for averaging noisy observations.

</details>


### [285] [Minimax learning rates for estimating binary classifiers under margin conditions](https://arxiv.org/abs/2505.10628)
*Jonathan García,Philipp Petersen*

Main category: stat.ML

TL;DR: 论文研究了基于水平函数的二元估计器分类问题，在数据分布满足几何边界条件下，建立了广泛函数类的最小最大学习率上下界。


<details>
  <summary>Details</summary>
Motivation: 研究在几何边界条件下，分类问题的最优学习率，填补理论空白。

Method: 使用水平函数描述决策边界，结合Kolmogorov熵和Lebesgue范数分析函数类。

Result: 在无噪声条件下，针对Barron正则函数和Hölder连续函数，得到了接近O(n^-1)的最优学习率；对于凸决策边界，最优学习率接近O(n^-1/2)。

Conclusion: 论文在几何边界条件下，为分类问题提供了理论支持，并验证了最优学习率的可行性。

Abstract: We study classification problems using binary estimators where the decision
boundary is described by horizon functions and where the data distribution
satisfies a geometric margin condition. We establish upper and lower bounds for
the minimax learning rate over broad function classes with bounded Kolmogorov
entropy in Lebesgue norms. A key novelty of our work is the derivation of lower
bounds on the worst-case learning rates under a geometric margin condition -- a
setting that is almost universally satisfied in practice but remains
theoretically challenging. Moreover, our results deal with the noiseless
setting, where lower bounds are particularly hard to establish. We apply our
general results to classification problems with decision boundaries belonging
to several function classes: for Barron-regular functions, and for
H\"older-continuous functions with strong margins, we identify optimal rates
close to the fast learning rates of $\mathcal{O}(n^{-1})$ for $n \in
\mathbb{N}$ samples. Also for merely convex decision boundaries, in a strong
margin case optimal rates near $\mathcal{O}(n^{-1/2})$ can be achieved.

</details>


### [286] [Supervised Models Can Generalize Also When Trained on Random Label](https://arxiv.org/abs/2505.11006)
*Oskar Allerbo,Thomas B. Schön*

Main category: stat.ML

TL;DR: 论文探讨了监督模型是否可以在不使用输出信息y的情况下训练，并证明这是可行的。通过将模型构建为平滑器，并独立于y构造平滑矩阵，实验表明这种方法在真实和合成数据上表现接近标准监督模型。


<details>
  <summary>Details</summary>
Motivation: 研究监督模型是否可以在不依赖输出信息y的情况下训练，探索无监督学习的潜力。

Method: 将模型构建为平滑器（形式为f̂=Sy），平滑矩阵S独立于y构造（如通过随机标签训练），并提出一种基于预测分布的无y模型选择标准。

Result: 在真实和合成数据上，无y训练的线性/核岭回归、平滑样条和神经网络表现接近标准监督模型，显著优于随机猜测。

Conclusion: 监督模型可以在不使用输出信息y的情况下有效训练，且性能接近传统方法。

Abstract: The success of unsupervised learning raises the question of whether also
supervised models can be trained without using the information in the output
$y$. In this paper, we demonstrate that this is indeed possible. The key step
is to formulate the model as a smoother, i.e. on the form $\hat{f}=Sy$, and to
construct the smoother matrix $S$ independently of $y$, e.g. by training on
random labels. We present a simple model selection criterion based on the
distribution of the out-of-sample predictions and show that, in contrast to
cross-validation, this criterion can be used also without access to $y$. We
demonstrate on real and synthetic data that $y$-free trained versions of linear
and kernel ridge regression, smoothing splines, and neural networks perform
similarly to their standard, $y$-based, versions and, most importantly,
significantly better than random guessing.

</details>


### [287] [Inexact Column Generation for Bayesian Network Structure Learning via Difference-of-Submodular Optimization](https://arxiv.org/abs/2505.11089)
*Yiran Yang,Rui Chen*

Main category: stat.ML

TL;DR: 本文提出了一种基于整数规划（IP）的方法，用于解决贝叶斯网络结构学习（BNSL）问题，通过动态生成行和列来应对变量和约束数量庞大的挑战，并利用子模优化和凸差算法（DCA）高效解决定价问题。


<details>
  <summary>Details</summary>
Motivation: 当前BNSL的IP方法因变量和约束数量庞大而效率低下，亟需一种更高效的解决方案。

Method: 采用行和列生成技术，将定价问题重新表述为子模优化问题，并应用DCA算法作为近似解法。

Result: 实验表明，该方法在高斯连续数据上表现优于现有方法，尤其在高密度图中，且在大规模图中与基准方法性能相当。

Conclusion: 该方法在解决BNSL问题上具有高效性和优越性，尤其在复杂场景下表现突出。

Abstract: In this paper, we consider a score-based Integer Programming (IP) approach
for solving the Bayesian Network Structure Learning (BNSL) problem.
State-of-the-art BNSL IP formulations suffer from the exponentially large
number of variables and constraints. A standard approach in IP to address such
challenges is to employ row and column generation techniques, which dynamically
generate rows and columns, while the complex pricing problem remains a
computational bottleneck for BNSL. For the general class of $\ell_0$-penalized
likelihood scores, we show how the pricing problem can be reformulated as a
difference of submodular optimization problem, and how the Difference of Convex
Algorithm (DCA) can be applied as an inexact method to efficiently solve the
pricing problems. Empirically, we show that, for continuous Gaussian data, our
row and column generation approach yields solutions with higher quality than
state-of-the-art score-based approaches, especially when the graph density
increases, and achieves comparable performance against benchmark
constraint-based and hybrid approaches, even when the graph size increases.

</details>


### [288] [Nash: Neural Adaptive Shrinkage for Structured High-Dimensional Regression](https://arxiv.org/abs/2505.11143)
*William R. P. Denault*

Main category: stat.ML

TL;DR: Nash是一种通过神经网络将协变量特定信息整合到稀疏回归中的统一框架，能够自适应调整惩罚项，无需交叉验证。


<details>
  <summary>Details</summary>
Motivation: 传统稀疏回归方法在处理具有结构或异质来源的协变量时表现不佳，尤其是在生物医学应用中。

Method: 提出Neural Adaptive Shrinkage (Nash)框架，利用神经网络自适应调整惩罚项，并开发了变分推断算法进行高效训练。

Result: 实验表明，Nash在真实数据上提高了准确性和适应性。

Conclusion: Nash为稀疏回归提供了一种更灵活和高效的方法，尤其适用于结构化或异质协变量场景。

Abstract: Sparse linear regression is a fundamental tool in data analysis. However,
traditional approaches often fall short when covariates exhibit structure or
arise from heterogeneous sources. In biomedical applications, covariates may
stem from distinct modalities or be structured according to an underlying
graph. We introduce Neural Adaptive Shrinkage (Nash), a unified framework that
integrates covariate-specific side information into sparse regression via
neural networks. Nash adaptively modulates penalties on a per-covariate basis,
learning to tailor regularization without cross-validation. We develop a
variational inference algorithm for efficient training and establish
connections to empirical Bayes regression. Experiments on real data demonstrate
that Nash can improve accuracy and adaptability over existing methods.

</details>


### [289] [A Fourier Space Perspective on Diffusion Models](https://arxiv.org/abs/2505.11278)
*Fabian Falck,Teodora Pandeva,Kiarash Zahirnia,Rachel Lawrence,Richard Turner,Edward Meeds,Javier Zazo,Sushrut Karmalkar*

Main category: stat.ML

TL;DR: 扩散模型在图像、音频等数据模态中表现出色，但标准DDPM前向过程导致高频成分更快被噪声破坏，影响生成质量。本文研究了傅里叶空间中的前向过程，提出一种新方法使所有频率以相同速率被破坏，显著提升了高频主导数据集的性能。


<details>
  <summary>Details</summary>
Motivation: 标准DDPM前向过程导致高频成分更快被噪声破坏，违反了反向过程的假设，影响生成质量。本文旨在研究并改进这一问题。

Method: 在傅里叶空间中分析DDPM前向过程的归纳偏差，提出一种新前向过程，使所有频率以相同速率被破坏。

Result: 实验表明，新方法在高频主导的数据集上显著提升了生成质量，同时在标准基准上与DDPM表现相当。

Conclusion: 通过调整前向过程，可以消除频率层次对生成质量的影响，特别是在高频成分重要的场景中表现更优。

Abstract: Diffusion models are state-of-the-art generative models on data modalities
such as images, audio, proteins and materials. These modalities share the
property of exponentially decaying variance and magnitude in the Fourier
domain. Under the standard Denoising Diffusion Probabilistic Models (DDPM)
forward process of additive white noise, this property results in
high-frequency components being corrupted faster and earlier in terms of their
Signal-to-Noise Ratio (SNR) than low-frequency ones. The reverse process then
generates low-frequency information before high-frequency details. In this
work, we study the inductive bias of the forward process of diffusion models in
Fourier space. We theoretically analyse and empirically demonstrate that the
faster noising of high-frequency components in DDPM results in violations of
the normality assumption in the reverse process. Our experiments show that this
leads to degraded generation quality of high-frequency components. We then
study an alternate forward process in Fourier space which corrupts all
frequencies at the same rate, removing the typical frequency hierarchy during
generation, and demonstrate marked performance improvements on datasets where
high frequencies are primary, while performing on par with DDPM on standard
imaging benchmarks.

</details>


### [290] [Adaptive Linear Embedding for Nonstationary High-Dimensional Optimization](https://arxiv.org/abs/2505.11281)
*Yuejiang Wen,Paul D. Franzon*

Main category: stat.ML

TL;DR: SA-REMBO扩展了REMBO，通过多高斯嵌入和自适应选择机制解决高维优化问题，优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 高维空间中的贝叶斯优化受限于维度灾难和全局低维假设的刚性，REMBO虽通过线性投影缓解，但假设单一全局嵌入和静态目标。

Method: 提出SA-REMBO，支持多随机高斯嵌入，通过索引变量和乘积核联合建模，自适应选择局部子空间结构。

Result: 理论分析表明乘积核的表达能力和稳定性，实验证明SA-REMBO在合成和真实高维基准中优于传统方法。

Conclusion: SA-REMBO为复杂高维设计空间中的可扩展贝叶斯优化提供了灵活且强大的解决方案。

Abstract: Bayesian Optimization (BO) in high-dimensional spaces remains fundamentally
limited by the curse of dimensionality and the rigidity of global
low-dimensional assumptions. While Random EMbedding Bayesian Optimization
(REMBO) mitigates this via linear projections into low-dimensional subspaces,
it typically assumes a single global embedding and a stationary objective. In
this work, we introduce Self-Adaptive embedding REMBO (SA-REMBO), a novel
framework that generalizes REMBO to support multiple random Gaussian
embeddings, each capturing a different local subspace structure of the
high-dimensional objective. An index variable governs the embedding choice and
is jointly modeled with the latent optimization variable via a product kernel
in a Gaussian Process surrogate. This enables the optimizer to adaptively
select embeddings conditioned on location, effectively capturing locally
varying effective dimensionality, nonstationarity, and heteroscedasticity in
the objective landscape. We theoretically analyze the expressiveness and
stability of the index-conditioned product kernel and empirically demonstrate
the advantage of our method across synthetic and real-world high-dimensional
benchmarks, where traditional REMBO and other low-rank BO methods fail. Our
results establish SA-REMBO as a powerful and flexible extension for scalable BO
in complex, structured design spaces.

</details>


### [291] [Convergence Rates of Constrained Expected Improvement](https://arxiv.org/abs/2505.11323)
*Haowei Wang,Jingyi Wang,Zhongxiang Dai,Nai-Yuan Chiang,Szu Hui Ng,Cosmin G. Petra*

Main category: stat.ML

TL;DR: 本文研究了约束贝叶斯优化（CBO）中的约束期望改进（CEI）算法的收敛速率，证明了在RKHS和GP假设下CEI的收敛速率。


<details>
  <summary>Details</summary>
Motivation: 尽管CEI在带约束的黑盒优化中广泛应用，但其理论收敛速率尚未明确，本文旨在填补这一空白。

Method: 通过分析CEI的简单遗憾上界，分别在RKHS和GP假设下推导其收敛速率。

Result: 证明了CEI在平方指数核和Matérn核下的收敛速率，并通过数值实验验证了理论分析。

Conclusion: CEI在RKHS和GP假设下具有理论保证的收敛速率，为实际应用提供了理论支持。

Abstract: Constrained Bayesian optimization (CBO) methods have seen significant success
in black-box optimization with constraints, and one of the most commonly used
CBO methods is the constrained expected improvement (CEI) algorithm. CEI is a
natural extension of the expected improvement (EI) when constraints are
incorporated. However, the theoretical convergence rate of CEI has not been
established. In this work, we study the convergence rate of CEI by analyzing
its simple regret upper bound. First, we show that when the objective function
$f$ and constraint function $c$ are assumed to each lie in a reproducing kernel
Hilbert space (RKHS), CEI achieves the convergence rates of $\mathcal{O}
\left(t^{-\frac{1}{2}}\log^{\frac{d+1}{2}}(t) \right) \ \text{and }\
\mathcal{O}\left(t^{\frac{-\nu}{2\nu+d}} \log^{\frac{\nu}{2\nu+d}}(t)\right)$
for the commonly used squared exponential and Mat\'{e}rn kernels, respectively.
Second, we show that when $f$ and $c$ are assumed to be sampled from Gaussian
processes (GPs), CEI achieves the same convergence rates with a high
probability. Numerical experiments are performed to validate the theoretical
analysis.

</details>


### [292] [STRIDE: Sparse Techniques for Regression in Deep Gaussian Processes](https://arxiv.org/abs/2505.11355)
*Simon Urbainczyk,Aretha L. Teckentrup,Jonas Latz*

Main category: stat.ML

TL;DR: 论文提出了一种结合变分学习和MCMC的方法，用于高效训练深度高斯过程（GPs）并处理大规模数据。


<details>
  <summary>Details</summary>
Motivation: 高斯过程（GPs）在处理大规模数据或多尺度特征时存在局限性，传统方法如稀疏GP回归和深度GPs在近似后验时存在不确定性表示不准确的问题。

Method: 结合变分学习和MCMC，开发了一种基于粒子期望最大化的方法，同时通过变分方法在大规模数据中寻找诱导点，并通过采样方法准确训练GPs。

Result: 提出了一种高效且准确的深度GP训练方法，并在标准基准问题上进行了测试。

Conclusion: 该方法显著提升了深度GP在大规模数据上的训练效率和准确性。

Abstract: Gaussian processes (GPs) have gained popularity as flexible machine learning
models for regression and function approximation with an in-built method for
uncertainty quantification. However, GPs suffer when the amount of training
data is large or when the underlying function contains multi-scale features
that are difficult to represent by a stationary kernel. To address the former,
training of GPs with large-scale data is often performed through inducing point
approximations (also known as sparse GP regression (GPR)), where the size of
the covariance matrices in GPR is reduced considerably through a greedy search
on the data set. To aid the latter, deep GPs have gained traction as
hierarchical models that resolve multi-scale features by combining multiple
GPs. Posterior inference in deep GPs requires a sampling or, more usual, a
variational approximation. Variational approximations lead to large-scale
stochastic, non-convex optimisation problems and the resulting approximation
tends to represent uncertainty incorrectly. In this work, we combine
variational learning with MCMC to develop a particle-based
expectation-maximisation method to simultaneously find inducing points within
the large-scale data (variationally) and accurately train the GPs
(sampling-based). The result is a highly efficient and accurate methodology for
deep GP training on large-scale data. We test our method on standard benchmark
problems.

</details>


<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [293] [Comparative Analysis of Black-Box Optimization Methods for Weather Intervention Design](https://arxiv.org/abs/2505.10843)
*Yuta Higuchi,Rikuto Nagai,Atsushi Okazaki,Masaki Ogura,Naoki Wakamiya*

Main category: physics.ao-ph

TL;DR: 论文提出了一种基于黑箱优化的天气干预方法，以解决天气控制中的梯度获取和计算资源问题，并通过实验证明贝叶斯优化在降雨减少中的高效性。


<details>
  <summary>Details</summary>
Motivation: 气候变化加剧了天气相关灾害的威胁，因此需要研究天气控制以降低灾害风险。然而，天气现象的复杂性和规模使得优化过程极具挑战性。

Method: 采用黑箱优化方法设计天气干预，无需梯度信息，并在两种控制场景（一次性初始值干预和基于模型预测控制的顺序干预）中评估。

Result: 实验表明，贝叶斯优化在高维搜索空间中比其他方法更有效，尤其在降雨减少方面表现突出。

Conclusion: 贝叶斯优化是一种高效的天气干预计算方法，适用于复杂天气控制问题。

Abstract: As climate change increases the threat of weather-related disasters, research
on weather control is gaining importance. The objective of weather control is
to mitigate disaster risks by administering interventions with optimal timing,
location, and intensity. However, the optimization process is highly
challenging due to the vast scale and complexity of weather phenomena, which
introduces two major challenges. First, obtaining accurate gradient information
for optimization is difficult. In addition, numerical weather prediction (NWP)
models demand enormous computational resources, necessitating parameter
optimization with minimal function evaluations. To address these challenges,
this study proposes a method for designing weather interventions based on
black-box optimization, which enables efficient exploration without requiring
gradient information. The proposed method is evaluated in two distinct control
scenarios: one-shot initial value intervention and sequential intervention
based on model predictive control. Furthermore, a comparative analysis is
conducted among four representative black-box optimization methods in terms of
total rainfall reduction. Experimental results show that Bayesian optimization
achieves higher control effectiveness than the others, particularly in
high-dimensional search spaces. These findings suggest that Bayesian
optimization is a highly effective approach for weather intervention
computation.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [294] [MID-L: Matrix-Interpolated Dropout Layer with Layer-wise Neuron Selection](https://arxiv.org/abs/2505.11416)
*Pouya Shaeri,Ariane Middel*

Main category: cs.NE

TL;DR: MID-L是一种动态选择神经元的模块，通过输入相关的门控向量减少计算量，同时保持或提高模型性能。


<details>
  <summary>Details</summary>
Motivation: 现代神经网络通常为每个输入激活所有神经元，导致计算冗余和效率低下。

Method: MID-L采用可微分的Top-k掩码策略，动态选择最有信息的神经元，并集成到现有架构中。

Result: 在多个基准测试中，MID-L平均减少55%的激活神经元，节省1.7倍FLOPs，同时保持或超过基线准确率。

Conclusion: MID-L是一种通用、即插即用的动态计算层，结合了dropout正则化和高效推理的优点。

Abstract: Modern neural networks often activate all neurons for every input, leading to
unnecessary computation and inefficiency. We introduce Matrix-Interpolated
Dropout Layer (MID-L), a novel module that dynamically selects and activates
only the most informative neurons by interpolating between two transformation
paths via a learned, input-dependent gating vector. Unlike conventional dropout
or static sparsity methods, MID-L employs a differentiable Top-k masking
strategy, enabling per-input adaptive computation while maintaining end-to-end
differentiability. MID-L is model-agnostic and integrates seamlessly into
existing architectures. Extensive experiments on six benchmarks, including
MNIST, CIFAR-10, CIFAR-100, SVHN, UCI Adult, and IMDB, show that MID-L achieves
up to average 55\% reduction in active neurons, 1.7$\times$ FLOPs savings, and
maintains or exceeds baseline accuracy. We further validate the informativeness
and selectivity of the learned neurons via Sliced Mutual Information (SMI) and
observe improved robustness under overfitting and noisy data conditions.
Additionally, MID-L demonstrates favorable inference latency and memory usage
profiles, making it suitable for both research exploration and deployment on
compute-constrained systems. These results position MID-L as a general-purpose,
plug-and-play dynamic computation layer, bridging the gap between dropout
regularization and efficient inference.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [295] [LLM-Explorer: Towards Efficient and Affordable LLM-based Exploration for Mobile Apps](https://arxiv.org/abs/2505.10593)
*Shanhui Zhao,Hao Wen,Wenjie Du,Cheng Liang,Yunxin Liu,Xiaozhou Ye,Ye Ouyang,Yuanchun Li*

Main category: cs.SE

TL;DR: LLM-Explorer是一种高效且经济的移动应用探索代理，通过减少对LLMs的依赖，显著降低了成本和资源消耗，同时实现了更高的覆盖率。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的探索方法过度依赖LLMs生成动作，导致高昂的token费用和计算资源消耗，且效果不佳。

Method: LLM-Explorer主要利用LLMs维护知识而非生成动作，通过知识指导动作生成，减少对LLMs的直接依赖。

Result: 在20个典型应用上与5个基线方法比较，LLM-Explorer实现了最快的速度和最高的覆盖率，成本比现有LLM方法低148倍。

Conclusion: LLM-Explorer证明了减少对LLMs的依赖可以显著提升效率和降低成本，同时保持高性能。

Abstract: Large language models (LLMs) have opened new opportunities for automated
mobile app exploration, an important and challenging problem that used to
suffer from the difficulty of generating meaningful UI interactions. However,
existing LLM-based exploration approaches rely heavily on LLMs to generate
actions in almost every step, leading to a huge cost of token fees and
computational resources. We argue that such extensive usage of LLMs is neither
necessary nor effective, since many actions during exploration do not require,
or may even be biased by the abilities of LLMs. Further, based on the insight
that a precise and compact knowledge plays the central role for effective
exploration, we introduce LLM-Explorer, a new exploration agent designed for
efficiency and affordability. LLM-Explorer uses LLMs primarily for maintaining
the knowledge instead of generating actions, and knowledge is used to guide
action generation in a LLM-less manner. Based on a comparison with 5 strong
baselines on 20 typical apps, LLM-Explorer was able to achieve the fastest and
highest coverage among all automated app explorers, with over 148x lower cost
than the state-of-the-art LLM-based approach.

</details>


### [296] [CRPE: Expanding The Reasoning Capability of Large Language Model for Code Generation](https://arxiv.org/abs/2505.10594)
*Ningxin Gui,Qianghuai Jia,Feijun Jiang,Yuling Jiao,dechun wang,Jerry Zhijian Yang*

Main category: cs.SE

TL;DR: CRPE是一个三阶段框架，用于提升大型语言模型（LLMs）的代码推理能力，通过数据合成和模型训练显著提高了代码生成任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有系统1模型在代码生成任务中的分析和逻辑处理能力有限，CRPE旨在解决这一问题。

Method: CRPE采用三阶段框架，包括数据合成和模型训练，最终开发出增强版的COT-Coder。

Result: COT-Coder-7B-StepDPO和COT-Coder-32B-StepDPO在LiveCodeBench上表现优异，分别达到21.88和35.08的pass@1准确率，超越同类甚至更大规模的模型。

Conclusion: CRPE是一个全面的开源方法，从数据获取到推理增强机制，显著提升了LLMs的代码推理能力。

Abstract: We introduce CRPE (Code Reasoning Process Enhancer), an innovative
three-stage framework for data synthesis and model training that advances the
development of sophisticated code reasoning capabilities in large language
models (LLMs). Building upon existing system-1 models, CRPE addresses the
fundamental challenge of enhancing LLMs' analytical and logical processing in
code generation tasks. Our framework presents a methodologically rigorous yet
implementable approach to cultivating advanced code reasoning abilities in
language models. Through the implementation of CRPE, we successfully develop an
enhanced COT-Coder that demonstrates marked improvements in code generation
tasks. Evaluation results on LiveCodeBench (20240701-20240901) demonstrate that
our COT-Coder-7B-StepDPO, derived from Qwen2.5-Coder-7B-Base, with a pass@1
accuracy of 21.88, exceeds all models with similar or even larger sizes.
Furthermore, our COT-Coder-32B-StepDPO, based on Qwen2.5-Coder-32B-Base,
exhibits superior performance with a pass@1 accuracy of 35.08, outperforming
GPT4O on the benchmark. Overall, CRPE represents a comprehensive, open-source
method that encompasses the complete pipeline from instruction data acquisition
through expert code reasoning data synthesis, culminating in an autonomous
reasoning enhancement mechanism.

</details>


### [297] [The Hitchhikers Guide to Production-ready Trustworthy Foundation Model powered Software (FMware)](https://arxiv.org/abs/2505.10640)
*Kirill Vasilevski,Benjamin Rombaut,Gopi Krishnan Rajbahadur,Gustavo A. Oliva,Keheliya Gallaba,Filipe R. Cogo,Jiahuei,Lin,Dayi Lin,Haoxiang Zhang,Bouyan Chen,Kishanthan Thangarajah,Ahmed E. Hassan,Zhen Ming,Jiang*

Main category: cs.SE

TL;DR: 本文介绍了FMware（基于基础模型的软件系统）的研究与实践现状，探讨了模型选择、数据对齐、提示工程和自主代理编排等挑战，并提供了从演示到生产系统的技术路线图。


<details>
  <summary>Details</summary>
Motivation: 基础模型（如大语言模型）正在重塑软件行业，但如何将其有效集成到生产系统中仍面临诸多挑战。本文旨在提供解决这些挑战的实用策略和技术路线。

Method: 通过梳理研究与实践现状，分析模型选择、数据对齐、提示工程和系统部署等关键问题，并结合工业经验和最新研究提出解决方案。

Result: 提供了从演示到生产系统的技术路线图，帮助开发者构建可信赖的FMware。

Conclusion: 本文为开发者提供了应对FMware挑战的实用策略，助力其在快速发展的技术环境中构建高效、可靠的系统。

Abstract: Foundation Models (FMs) such as Large Language Models (LLMs) are reshaping
the software industry by enabling FMware, systems that integrate these FMs as
core components. In this KDD 2025 tutorial, we present a comprehensive
exploration of FMware that combines a curated catalogue of challenges with
real-world production concerns. We first discuss the state of research and
practice in building FMware. We further examine the difficulties in selecting
suitable models, aligning high-quality domain-specific data, engineering robust
prompts, and orchestrating autonomous agents. We then address the complex
journey from impressive demos to production-ready systems by outlining issues
in system testing, optimization, deployment, and integration with legacy
software. Drawing on our industrial experience and recent research in the area,
we provide actionable insights and a technology roadmap for overcoming these
challenges. Attendees will gain practical strategies to enable the creation of
trustworthy FMware in the evolving technology landscape.

</details>


### [298] [Let the Trial Begin: A Mock-Court Approach to Vulnerability Detection using LLM-Based Agents](https://arxiv.org/abs/2505.10961)
*Ratnadira Widyasari,Martin Weyssow,Ivana Clairine Irsan,Han Wei Ang,Frank Liauw,Eng Lieh Ouh,Lwin Khin Shar,Hong Jin Kang,David Lo*

Main category: cs.SE

TL;DR: VulTrial是一个法庭启发的多代理框架，用于提升源代码漏洞检测性能，通过角色定制代理和少量数据调优显著优于单代理基线。


<details>
  <summary>Details</summary>
Motivation: 源代码漏洞检测具有挑战性，尤其是当良性代码与漏洞代码相似时，需要更高效的自动化方法。

Method: VulTrial采用四个角色定制代理（安全研究员、代码作者、主持人和评审委员会），结合GPT-3.5和GPT-4o进行实验。

Result: VulTrial性能显著提升（102.39%和84.17%），角色调优进一步优化性能（139.89%和118.30%），且GPT-3.5在成本效益下表现优异。

Conclusion: VulTrial通过多代理交互和角色调优显著提升漏洞检测性能，同时展示了成本效益平衡的潜力。

Abstract: Detecting vulnerabilities in source code remains a critical yet challenging
task, especially when benign and vulnerable functions share significant
similarities. In this work, we introduce VulTrial, a courtroom-inspired
multi-agent framework designed to enhance automated vulnerability detection. It
employs four role-specific agents, which are security researcher, code author,
moderator, and review board. Through extensive experiments using GPT-3.5 and
GPT-4o we demonstrate that Vultrial outperforms single-agent and multi-agent
baselines. Using GPT-4o, VulTrial improves the performance by 102.39% and
84.17% over its respective baseline. Additionally, we show that role-specific
instruction tuning in multi-agent with small data (50 pair samples) improves
the performance of VulTrial further by 139.89% and 118.30%. Furthermore, we
analyze the impact of increasing the number of agent interactions on VulTrial's
overall performance. While multi-agent setups inherently incur higher costs due
to increased token usage, our findings reveal that applying VulTrial to a
cost-effective model like GPT-3.5 can improve its performance by 69.89%
compared to GPT-4o in a single-agent setting, at a lower overall cost.

</details>


### [299] [DecompileBench: A Comprehensive Benchmark for Evaluating Decompilers in Real-World Scenarios](https://arxiv.org/abs/2505.11340)
*Zeyu Gao,Yuxin Cui,Hao Wang,Siliang Qin,Yuanda Wang,Bolun Zhang,Chao Zhang*

Main category: cs.SE

TL;DR: DecompileBench是一个评估反编译器的框架，通过真实世界函数提取、运行时验证和自动化人本评估来改进反编译器的语义保真度和可用性。


<details>
  <summary>Details</summary>
Motivation: 现有反编译器评估方法过于碎片化，主要关注语法正确性，而忽略了语义保真度和实际需求。

Method: DecompileBench框架包含三个关键组件：真实世界函数提取、运行时感知验证和基于LLM的自动化人本评估。

Result: LLM方法在代码可理解性上优于商业工具，尽管功能正确性低52.2%。

Conclusion: LLM方法有望改变人本逆向工程，DecompileBench开源以推动研究和工具选择。

Abstract: Decompilers are fundamental tools for critical security tasks, from
vulnerability discovery to malware analysis, yet their evaluation remains
fragmented. Existing approaches primarily focus on syntactic correctness
through synthetic micro-benchmarks or subjective human ratings, failing to
address real-world requirements for semantic fidelity and analyst usability. We
present DecompileBench, the first comprehensive framework that enables
effective evaluation of decompilers in reverse engineering workflows through
three key components: \textit{real-world function extraction} (comprising
23,400 functions from 130 real-world programs), \textit{runtime-aware
validation}, and \textit{automated human-centric assessment} using LLM-as-Judge
to quantify the effectiveness of decompilers in reverse engineering workflows.
Through a systematic comparison between six industrial-strength decompilers and
six recent LLM-powered approaches, we demonstrate that LLM-based methods
surpass commercial tools in code understandability despite 52.2% lower
functionality correctness. These findings highlight the potential of LLM-based
approaches to transform human-centric reverse engineering. We open source
\href{https://github.com/Jennieett/DecompileBench}{DecompileBench} to provide a
framework to advance research on decompilers and assist security experts in
making informed tool selections based on their specific requirements.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [300] [Super-Resolution Generative Adversarial Networks based Video Enhancement](https://arxiv.org/abs/2505.10589)
*Kağan ÇETİN*

Main category: cs.CV

TL;DR: 该研究通过扩展单图像超分辨率生成对抗网络（SRGAN）结构，提出了一种增强视频超分辨率的方法，结合3D非局部块以捕捉时空关系，并开发了实验训练流程。


<details>
  <summary>Details</summary>
Motivation: 尽管SRGAN在单图像增强中表现良好，但其设计未考虑视频处理中的时间连续性，因此需要改进以适应视频数据。

Method: 提出了一种改进框架，结合3D非局部块，开发了基于分块学习和高级数据退化技术的训练流程，以模拟真实视频条件。

Result: 实验结果表明，该方法在时间一致性、纹理清晰度和减少视觉伪影方面优于传统单图像方法。

Conclusion: 该研究为视频增强任务提供了实用的学习解决方案，适用于流媒体、游戏和数字修复等领域。

Abstract: This study introduces an enhanced approach to video super-resolution by
extending ordinary Single-Image Super-Resolution (SISR) Super-Resolution
Generative Adversarial Network (SRGAN) structure to handle spatio-temporal
data. While SRGAN has proven effective for single-image enhancement, its design
does not account for the temporal continuity required in video processing. To
address this, a modified framework that incorporates 3D Non-Local Blocks is
proposed, which is enabling the model to capture relationships across both
spatial and temporal dimensions. An experimental training pipeline is
developed, based on patch-wise learning and advanced data degradation
techniques, to simulate real-world video conditions and learn from both local
and global structures and details. This helps the model generalize better and
maintain stability across varying video content while maintaining the general
structure besides the pixel-wise correctness. Two model variants-one larger and
one more lightweight-are presented to explore the trade-offs between
performance and efficiency. The results demonstrate improved temporal
coherence, sharper textures, and fewer visual artifacts compared to traditional
single-image methods. This work contributes to the development of practical,
learning-based solutions for video enhancement tasks, with potential
applications in streaming, gaming, and digital restoration.

</details>


### [301] [MIRAGE: A Multi-modal Benchmark for Spatial Perception, Reasoning, and Intelligence](https://arxiv.org/abs/2505.10604)
*Chonghan Liu,Haoran Wang,Felix Henry,Pu Miao,Yajie Zhang,Yu Zhao,Peiran Wu*

Main category: cs.CV

TL;DR: MIRAGE是一个多模态基准测试，用于评估模型在计数、空间关系推理及两者结合方面的能力，揭示了现有模型的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有计算机视觉模型在物体属性识别和空间关系推理方面存在显著不足，MIRAGE旨在填补这一空白。

Method: 通过设计多样化和复杂的场景，MIRAGE测试模型在细粒度识别和推理方面的能力。

Result: MIRAGE揭示了当前先进模型在基础能力上的局限性，强调了改进表示和推理框架的必要性。

Conclusion: MIRAGE为未来时空推理研究提供了方向。

Abstract: Spatial perception and reasoning are core components of human cognition,
encompassing object recognition, spatial relational understanding, and dynamic
reasoning. Despite progress in computer vision, existing benchmarks reveal
significant gaps in models' abilities to accurately recognize object attributes
and reason about spatial relationships, both essential for dynamic reasoning.
To address these limitations, we propose MIRAGE, a multi-modal benchmark
designed to evaluate models' capabilities in Counting (object attribute
recognition), Relation (spatial relational reasoning), and Counting with
Relation. Through diverse and complex scenarios requiring fine-grained
recognition and reasoning, MIRAGE highlights critical limitations in
state-of-the-art models, underscoring the need for improved representations and
reasoning frameworks. By targeting these foundational abilities, MIRAGE
provides a pathway toward spatiotemporal reasoning in future research.

</details>


### [302] [Relative Drawing Identification Complexity is Invariant to Modality in Vision-Language Models](https://arxiv.org/abs/2505.10583)
*Diogo Freitas,Brigt Håvardstun,Cèsar Ferri,Darío Garigliotti,Jan Arne Telle,José Hernández-Orallo*

Main category: cs.CV

TL;DR: 研究探索多模态语言模型是否通过共同表示整合不同模态，发现图像和坐标表示在教学中复杂度相似，表明概念的简单性可能超越模态。


<details>
  <summary>Details</summary>
Motivation: 验证多模态语言模型是否真正通过共同表示整合不同模态（如图像和文本），并探索其教学复杂度。

Method: 使用机器教学理论，比较Quick, Draw!数据集中图像（位图）和坐标（TikZ格式）两种表示方式的教学复杂度。

Result: 图像表示通常需要更少教学片段且准确率更高，但两种模态的教学复杂度排序相似，表明概念的简单性可能独立于模态。

Conclusion: 多模态模型的共同表示可能确实存在，概念的简单性可能是其固有属性，不受模态影响。

Abstract: Large language models have become multimodal, and many of them are said to
integrate their modalities using common representations. If this were true, a
drawing of a car as an image, for instance, should map to the similar area in
the latent space as a textual description of the strokes that conform the
drawing. To explore this in a black-box access regime to these models, we
propose the use of machine teaching, a theory that studies the minimal set of
examples a teacher needs to choose so that the learner captures the concept. In
this paper we evaluate the complexity of teaching visual-language models a
subset of objects in the Quick, Draw! dataset using two presentations: raw
images as bitmaps and trace coordinates in TikZ format. The results indicate
that image-based representations generally require fewer segments and achieve
higher accuracy than coordinate-based representations. But, surprisingly, the
teaching size usually ranks concepts similarly across both modalities, even
when controlling for (a human proxy of) concept priors, suggesting that the
simplicity of concepts may be an inherent property that transcends modality
representations.

</details>


### [303] [CLIP Embeddings for AI-Generated Image Detection: A Few-Shot Study with Lightweight Classifier](https://arxiv.org/abs/2505.10664)
*Ziyang Ou*

Main category: cs.CV

TL;DR: 研究探讨了CLIP嵌入是否包含AI生成图像的信息，提出了一种轻量级分类方法，在CIFAKE基准测试中达到95%准确率，但某些图像类型仍具挑战性。


<details>
  <summary>Details</summary>
Motivation: 验证AI生成图像的真实性在社交媒体上日益重要，但现有视觉语言模型（如CLIP）在此任务上表现未充分探索。

Method: 使用冻结的CLIP模型提取视觉嵌入，通过轻量级网络和微调最终分类器进行分类。

Result: 在CIFAKE基准测试中达到95%准确率，少量数据适应后为85%，但某些图像类型（如广角照片和油画）分类困难。

Conclusion: 研究揭示了AI生成图像分类中的新挑战，值得进一步探索特定图像类型的分类问题。

Abstract: Verifying the authenticity of AI-generated images presents a growing
challenge on social media platforms these days. While vision-language models
(VLMs) like CLIP outdo in multimodal representation, their capacity for
AI-generated image classification is underexplored due to the absence of such
labels during the pre-training process. This work investigates whether CLIP
embeddings inherently contain information indicative of AI generation. A
proposed pipeline extracts visual embeddings using a frozen CLIP model, feeds
its embeddings to lightweight networks, and fine-tunes only the final
classifier. Experiments on the public CIFAKE benchmark show the performance
reaches 95% accuracy without language reasoning. Few-shot adaptation to curated
custom with 20% of the data results in performance to 85%. A closed-source
baseline (Gemini-2.0) has the best zero-shot accuracy yet fails on specific
styles. Notably, some specific image types, such as wide-angle photographs and
oil paintings, pose significant challenges to classification. These results
indicate previously unexplored difficulties in classifying certain types of
AI-generated images, revealing new and more specific questions in this domain
that are worth further investigation.

</details>


### [304] [MMLongBench: Benchmarking Long-Context Vision-Language Models Effectively and Thoroughly](https://arxiv.org/abs/2505.10610)
*Zhaowei Wang,Wenhao Yu,Xiyu Ren,Jipeng Zhang,Yu Zhao,Rohit Saxena,Liang Cheng,Ginny Wong,Simon See,Pasquale Minervini,Yangqiu Song,Mark Steedman*

Main category: cs.CV

TL;DR: MMLongBench是首个用于评估长上下文视觉语言模型（LCVLMs）的多样化基准，包含13,331个样本，覆盖五类任务和多种图像类型。通过评估46个模型，发现当前模型在长上下文任务中仍有挑战，推理能力强的模型表现更好。


<details>
  <summary>Details</summary>
Motivation: 随着长上下文视觉语言模型（LCVLMs）的发展，缺乏一个全面的基准来评估其能力，因此提出了MMLongBench。

Method: MMLongBench包含13,331个样本，覆盖五类任务（如Visual RAG和Many-Shot ICL）和多种图像类型。所有样本通过跨模态标记化方案提供五种标准化输入长度（8K-128K tokens）。

Result: 评估46个模型后发现：单任务表现不能代表整体能力；闭源和开源模型在长上下文任务中均面临挑战；推理能力强的模型表现更优。

Conclusion: MMLongBench为诊断和推进下一代LCVLMs提供了基础，揭示了当前模型的不足和改进方向。

Abstract: The rapid extension of context windows in large vision-language models has
given rise to long-context vision-language models (LCVLMs), which are capable
of handling hundreds of images with interleaved text tokens in a single forward
pass. In this work, we introduce MMLongBench, the first benchmark covering a
diverse set of long-context vision-language tasks, to evaluate LCVLMs
effectively and thoroughly. MMLongBench is composed of 13,331 examples spanning
five different categories of downstream tasks, such as Visual RAG and Many-Shot
ICL. It also provides broad coverage of image types, including various natural
and synthetic images. To assess the robustness of the models to different input
lengths, all examples are delivered at five standardized input lengths (8K-128K
tokens) via a cross-modal tokenization scheme that combines vision patches and
text tokens. Through a thorough benchmarking of 46 closed-source and
open-source LCVLMs, we provide a comprehensive analysis of the current models'
vision-language long-context ability. Our results show that: i) performance on
a single task is a weak proxy for overall long-context capability; ii) both
closed-source and open-source models face challenges in long-context
vision-language tasks, indicating substantial room for future improvement; iii)
models with stronger reasoning ability tend to exhibit better long-context
performance. By offering wide task coverage, various image types, and rigorous
length control, MMLongBench provides the missing foundation for diagnosing and
advancing the next generation of LCVLMs.

</details>


### [305] [CompAlign: Improving Compositional Text-to-Image Generation with a Complex Benchmark and Fine-Grained Feedback](https://arxiv.org/abs/2505.11178)
*Yixin Wan,Kai-Wei Chang*

Main category: cs.CV

TL;DR: CompAlign是一个用于评估和改进文本到图像（T2I）模型在组合场景生成能力的基准测试，包含900个复杂多主体生成提示。CompQuest是其评估框架，通过分解提示为原子子问题并提供细粒度反馈。实验表明，现有模型在复杂3D空间关系任务中表现不佳，但通过CompAlign的反馈改进后，模型性能显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有T2I模型在生成组合场景时难以准确描述多对象、属性和空间关系，因此需要一种评估和改进模型的方法。

Method: 提出CompAlign基准测试和CompQuest评估框架，后者将复杂提示分解为原子子问题并利用MLLM提供细粒度反馈。通过反馈信号改进扩散模型的生成能力。

Result: 实验显示模型在复杂3D空间关系任务中表现较差，但改进后性能显著提升，尤其在复杂生成任务中优于先前方法。

Conclusion: CompAlign和CompQuest为组合图像生成提供了有效的评估和改进工具，显著提升了模型的生成能力。

Abstract: State-of-the-art T2I models are capable of generating high-resolution images
given textual prompts. However, they still struggle with accurately depicting
compositional scenes that specify multiple objects, attributes, and spatial
relations. We present CompAlign, a challenging benchmark with an emphasis on
assessing the depiction of 3D-spatial relationships, for evaluating and
improving models on compositional image generation. CompAlign consists of 900
complex multi-subject image generation prompts that combine numerical and
3D-spatial relationships with varied attribute bindings. Our benchmark is
remarkably challenging, incorporating generation tasks with 3+ generation
subjects with complex 3D-spatial relationships. Additionally, we propose
CompQuest, an interpretable and accurate evaluation framework that decomposes
complex prompts into atomic sub-questions, then utilizes a MLLM to provide
fine-grained binary feedback on the correctness of each aspect of generation
elements in model-generated images. This enables precise quantification of
alignment between generated images and compositional prompts. Furthermore, we
propose an alignment framework that uses CompQuest's feedback as preference
signals to improve diffusion models' compositional image generation abilities.
Using adjustable per-image preferences, our method is easily scalable and
flexible for different tasks. Evaluation of 9 T2I models reveals that: (1)
models remarkable struggle more with compositional tasks with more complex
3D-spatial configurations, and (2) a noticeable performance gap exists between
open-source accessible models and closed-source commercial models. Further
empirical study on using CompAlign for model alignment yield promising results:
post-alignment diffusion models achieve remarkable improvements in
compositional accuracy, especially on complex generation tasks, outperforming
previous approaches.

</details>


### [306] [CROC: Evaluating and Training T2I Metrics with Pseudo- and Human-Labeled Contrastive Robustness Checks](https://arxiv.org/abs/2505.11314)
*Christoph Leiter,Yuki M. Asano,Margret Keuper,Steffen Eger*

Main category: cs.CV

TL;DR: CROC是一个用于自动评估文本到图像生成任务中评价指标鲁棒性的框架，通过合成对比测试案例并生成大规模数据集（CROC$^{syn}$），同时提出新指标CROCScore。


<details>
  <summary>Details</summary>
Motivation: 现有评价指标的鲁棒性评估依赖人工，成本高且自动化方法稀缺。

Method: 提出CROC框架，通过合成对比测试案例生成大规模数据集（CROC$^{syn}$），并训练新指标CROCScore。

Result: CROCScore在开源方法中表现最佳，现有指标在否定提示和身体部位识别等任务中存在鲁棒性问题。

Conclusion: CROC框架为评价指标鲁棒性提供了自动化解决方案，并揭示了现有指标的不足。

Abstract: The assessment of evaluation metrics (meta-evaluation) is crucial for
determining the suitability of existing metrics in text-to-image (T2I)
generation tasks. Human-based meta-evaluation is costly and time-intensive, and
automated alternatives are scarce. We address this gap and propose CROC: a
scalable framework for automated Contrastive Robustness Checks that
systematically probes and quantifies metric robustness by synthesizing
contrastive test cases across a comprehensive taxonomy of image properties.
With CROC, we generate a pseudo-labeled dataset (CROC$^{syn}$) of over one
million contrastive prompt-image pairs to enable a fine-grained comparison of
evaluation metrics. We also use the dataset to train CROCScore, a new metric
that achieves state-of-the-art performance among open-source methods,
demonstrating an additional key application of our framework. To complement
this dataset, we introduce a human-supervised benchmark (CROC$^{hum}$)
targeting especially challenging categories. Our results highlight robustness
issues in existing metrics: for example, many fail on prompts involving
negation, and all tested open-source metrics fail on at least 25% of cases
involving correct identification of body parts.

</details>


### [307] [Completely Weakly Supervised Class-Incremental Learning for Semantic Segmentation](https://arxiv.org/abs/2505.10781)
*David Minkwan Kim,Soeun Lee,Byeongkeun Kang*

Main category: cs.CV

TL;DR: 本文提出了一种完全弱监督的类增量学习方法，用于语义分割任务，仅需图像级标签即可学习基类和新增类别的分割。通过结合定位器和基础模型的伪标签，生成鲁棒的伪标签，并引入示例引导的数据增强方法缓解灾难性遗忘。实验表明，该方法在多个设置下优于部分弱监督方法。


<details>
  <summary>Details</summary>
Motivation: 传统类增量语义分割方法需要昂贵的像素级标注，而部分弱监督方法仍有限制。本文旨在首次提出完全弱监督的类增量语义分割方法，以降低标注成本。

Method: 结合定位器和基础模型的伪标签生成鲁棒伪标签；引入示例引导的数据增强方法缓解灾难性遗忘。

Result: 在15-5 VOC和10-10 VOC设置中优于部分弱监督方法，在COCO-to-VOC设置中表现竞争力。

Conclusion: 本文提出的完全弱监督方法在降低标注成本的同时，实现了优于部分弱监督方法的性能，为类增量语义分割提供了新思路。

Abstract: This work addresses the task of completely weakly supervised
class-incremental learning for semantic segmentation to learn segmentation for
both base and additional novel classes using only image-level labels. While
class-incremental semantic segmentation (CISS) is crucial for handling diverse
and newly emerging objects in the real world, traditional CISS methods require
expensive pixel-level annotations for training. To overcome this limitation,
partially weakly-supervised approaches have recently been proposed. However, to
the best of our knowledge, this is the first work to introduce a completely
weakly-supervised method for CISS. To achieve this, we propose to generate
robust pseudo-labels by combining pseudo-labels from a localizer and a sequence
of foundation models based on their uncertainty. Moreover, to mitigate
catastrophic forgetting, we introduce an exemplar-guided data augmentation
method that generates diverse images containing both previous and novel classes
with guidance. Finally, we conduct experiments in three common experimental
settings: 15-5 VOC, 10-10 VOC, and COCO-to-VOC, and in two scenarios: disjoint
and overlap. The experimental results demonstrate that our completely weakly
supervised method outperforms even partially weakly supervised methods in the
15-5 VOC and 10-10 VOC settings while achieving competitive accuracy in the
COCO-to-VOC setting.

</details>


### [308] [EmotionHallucer: Evaluating Emotion Hallucinations in Multimodal Large Language Models](https://arxiv.org/abs/2505.11405)
*Bohao Xing,Xin Liu,Guoying Zhao,Chengyu Liu,Xiaolan Fu,Heikki Kälviäinen*

Main category: cs.CV

TL;DR: 论文提出了首个用于检测和分析多模态大语言模型（MLLMs）中情感幻觉的基准EmotionHallucer，并揭示了当前模型在此问题上的表现差异。


<details>
  <summary>Details</summary>
Motivation: 情感理解是重要但具有挑战性的任务，而MLLMs在此领域常因幻觉生成无关或无意义内容。目前缺乏专门评估情感相关幻觉的研究。

Method: 基于情感心理学知识和真实世界多模态感知，采用对抗性二元问答框架评估MLLMs的情感幻觉倾向。

Result: 评估38个模型后发现：多数模型存在情感幻觉问题；闭源模型表现优于开源模型；模型在情感心理学知识上表现优于多模态感知。

Conclusion: 研究提出了PEP-MEK框架，平均提升9.90%的情感幻觉检测性能，为未来研究提供了资源支持。

Abstract: Emotion understanding is a critical yet challenging task. Recent advances in
Multimodal Large Language Models (MLLMs) have significantly enhanced their
capabilities in this area. However, MLLMs often suffer from hallucinations,
generating irrelevant or nonsensical content. To the best of our knowledge,
despite the importance of this issue, there has been no dedicated effort to
evaluate emotion-related hallucinations in MLLMs. In this work, we introduce
EmotionHallucer, the first benchmark for detecting and analyzing emotion
hallucinations in MLLMs. Unlike humans, whose emotion understanding stems from
the interplay of biology and social learning, MLLMs rely solely on data-driven
learning and lack innate emotional instincts. Fortunately, emotion psychology
provides a solid foundation of knowledge about human emotions. Building on
this, we assess emotion hallucinations from two dimensions: emotion psychology
knowledge and real-world multimodal perception. To support robust evaluation,
we utilize an adversarial binary question-answer (QA) framework, which employs
carefully crafted basic and hallucinated pairs to assess the emotion
hallucination tendencies of MLLMs. By evaluating 38 LLMs and MLLMs on
EmotionHallucer, we reveal that: i) most current models exhibit substantial
issues with emotion hallucinations; ii) closed-source models outperform
open-source ones in detecting emotion hallucinations, and reasoning capability
provides additional advantages; iii) existing models perform better in emotion
psychology knowledge than in multimodal emotion perception. As a byproduct,
these findings inspire us to propose the PEP-MEK framework, which yields an
average improvement of 9.90% in emotion hallucination detection across selected
models. Resources will be available at
https://github.com/xxtars/EmotionHallucer.

</details>


### [309] [CleanPatrick: A Benchmark for Image Data Cleaning](https://arxiv.org/abs/2505.11034)
*Fabian Gröger,Simone Lionetti,Philippe Gottfrois,Alvaro Gonzalez-Jimenez,Ludovic Amruthalingam,Elisabeth Victoria Goessinger,Hanna Lindemann,Marie Bargiela,Marie Hofbauer,Omar Badri,Philipp Tschandl,Arash Koochek,Matthew Groh,Alexander A. Navarini,Marc Pouly*

Main category: cs.CV

TL;DR: CleanPatrick是首个大规模图像数据清理基准，基于Fitzpatrick17k皮肤科数据集，通过众包和专家审核生成高质量标注，评估多种清理方法。


<details>
  <summary>Details</summary>
Motivation: 当前图像数据清理基准依赖合成噪声或小规模人工研究，缺乏真实性和可比性，CleanPatrick旨在填补这一空白。

Method: 收集496,377个众包标注，识别离题样本、近似重复和标签错误，采用项目反应理论模型和专家审核生成高质量标注，并作为排名任务评估多种清理方法。

Result: 自监督表示在近似重复检测中表现优异，经典方法在有限预算下对离题样本检测有竞争力，标签错误检测仍是细粒度医学分类的挑战。

Conclusion: CleanPatrick为图像清理策略提供了系统性比较框架，推动了数据为中心的人工智能的可靠性。

Abstract: Robust machine learning depends on clean data, yet current image data
cleaning benchmarks rely on synthetic noise or narrow human studies, limiting
comparison and real-world relevance. We introduce CleanPatrick, the first
large-scale benchmark for data cleaning in the image domain, built upon the
publicly available Fitzpatrick17k dermatology dataset. We collect 496,377
binary annotations from 933 medical crowd workers, identify off-topic samples
(4%), near-duplicates (21%), and label errors (22%), and employ an aggregation
model inspired by item-response theory followed by expert review to derive
high-quality ground truth. CleanPatrick formalizes issue detection as a ranking
task and adopts typical ranking metrics mirroring real audit workflows.
Benchmarking classical anomaly detectors, perceptual hashing, SSIM, Confident
Learning, NoiseRank, and SelfClean, we find that, on CleanPatrick,
self-supervised representations excel at near-duplicate detection, classical
methods achieve competitive off-topic detection under constrained review
budgets, and label-error detection remains an open challenge for fine-grained
medical classification. By releasing both the dataset and the evaluation
framework, CleanPatrick enables a systematic comparison of image-cleaning
strategies and paves the way for more reliable data-centric artificial
intelligence.

</details>


### [310] [CUBIC: Concept Embeddings for Unsupervised Bias Identification using VLMs](https://arxiv.org/abs/2505.11060)
*David Méndez,Gianpaolo Bontempo,Elisa Ficarra,Roberto Confalonieri,Natalia Díaz-Rodríguez*

Main category: cs.CV

TL;DR: CUBIC是一种自动发现可解释概念的方法，用于识别可能影响分类器行为的偏见，无需预定义的偏见候选或模型失败示例。


<details>
  <summary>Details</summary>
Motivation: 深度视觉模型常依赖数据集中的虚假相关性学习偏见，而缺乏标注的概念信息使得识别这些偏见变得困难。

Method: CUBIC利用图像-文本潜在空间和线性分类器探针，通过分析概念对超类标签潜在表示的影响来识别偏见。

Result: 实验表明，CUBIC能有效发现未知偏见，无需依赖分类器表现不佳的样本或先验偏见知识。

Conclusion: CUBIC为无监督偏见识别提供了一种高效且无需标注的方法。

Abstract: Deep vision models often rely on biases learned from spurious correlations in
datasets. To identify these biases, methods that interpret high-level,
human-understandable concepts are more effective than those relying primarily
on low-level features like heatmaps. A major challenge for these concept-based
methods is the lack of image annotations indicating potentially bias-inducing
concepts, since creating such annotations requires detailed labeling for each
dataset and concept, which is highly labor-intensive. We present CUBIC (Concept
embeddings for Unsupervised Bias IdentifiCation), a novel method that
automatically discovers interpretable concepts that may bias classifier
behavior. Unlike existing approaches, CUBIC does not rely on predefined bias
candidates or examples of model failures tied to specific biases, as such
information is not always available. Instead, it leverages image-text latent
space and linear classifier probes to examine how the latent representation of
a superclass label$\unicode{x2014}$shared by all instances in the
dataset$\unicode{x2014}$is influenced by the presence of a given concept. By
measuring these shifts against the normal vector to the classifier's decision
boundary, CUBIC identifies concepts that significantly influence model
predictions. Our experiments demonstrate that CUBIC effectively uncovers
previously unknown biases using Vision-Language Models (VLMs) without requiring
the samples in the dataset where the classifier underperforms or prior
knowledge of potential biases.

</details>


### [311] [Towards Self-Improvement of Diffusion Models via Group Preference Optimization](https://arxiv.org/abs/2505.11070)
*Renjie Chen,Wenfeng Lin,Yichen Zhang,Jiangchuan Wei,Boyuan Liu,Chao Feng,Jiao Ran,Mingyu Guo*

Main category: cs.CV

TL;DR: 论文提出了一种改进的Group Preference Optimization (GPO)方法，解决了Direct Preference Optimization (DPO)在文本到图像生成中的敏感性和数据收集问题，通过组偏好优化和奖励标准化提升了性能。


<details>
  <summary>Details</summary>
Motivation: DPO在文本到图像生成中存在对偏好对敏感和数据收集困难的问题，需要改进。

Method: 提出GPO方法，将DPO从两两比较扩展到组比较，并引入奖励标准化进行重新加权，无需显式数据选择。

Result: GPO显著提升了生成质量，结合YOLO和OCR等模型，Stable Diffusion 3.5 Medium的计数和文本渲染能力提高了20个百分点。

Conclusion: GPO是一种高效的自改进方法，无需额外推理开销，适用于多种扩散模型和任务。

Abstract: Aligning text-to-image (T2I) diffusion models with Direct Preference
Optimization (DPO) has shown notable improvements in generation quality.
However, applying DPO to T2I faces two challenges: the sensitivity of DPO to
preference pairs and the labor-intensive process of collecting and annotating
high-quality data. In this work, we demonstrate that preference pairs with
marginal differences can degrade DPO performance. Since DPO relies exclusively
on relative ranking while disregarding the absolute difference of pairs, it may
misclassify losing samples as wins, or vice versa. We empirically show that
extending the DPO from pairwise to groupwise and incorporating reward
standardization for reweighting leads to performance gains without explicit
data selection. Furthermore, we propose Group Preference Optimization (GPO), an
effective self-improvement method that enhances performance by leveraging the
model's own capabilities without requiring external data. Extensive experiments
demonstrate that GPO is effective across various diffusion models and tasks.
Specifically, combining with widely used computer vision models, such as YOLO
and OCR, the GPO improves the accurate counting and text rendering capabilities
of the Stable Diffusion 3.5 Medium by 20 percentage points. Notably, as a
plug-and-play method, no extra overhead is introduced during inference.

</details>


### [312] [MAVOS-DD: Multilingual Audio-Video Open-Set Deepfake Detection Benchmark](https://arxiv.org/abs/2505.11109)
*Florinel-Alin Croitoru,Vlad Hondru,Marius Popescu,Radu Tudor Ionescu,Fahad Shahbaz Khan,Mubarak Shah*

Main category: cs.CV

TL;DR: 本文提出了首个大规模多语言音视频深度伪造检测的开放集基准数据集，包含8种语言的250小时真实与伪造视频，60%为生成数据。实验表明，现有先进检测器在开放集场景下性能下降。


<details>
  <summary>Details</summary>
Motivation: 为解决多语言环境下深度伪造检测的开放集挑战，提供标准化评估基准。

Method: 构建包含8种语言、7种生成模型的伪造视频数据集，划分训练、验证和测试集以模拟开放集场景，测试多种预训练和微调检测器。

Result: 现有先进检测器在开放集场景中性能显著下降。

Conclusion: 开放集多语言深度伪造检测仍具挑战性，需进一步研究。数据集和代码已公开。

Abstract: We present the first large-scale open-set benchmark for multilingual
audio-video deepfake detection. Our dataset comprises over 250 hours of real
and fake videos across eight languages, with 60% of data being generated. For
each language, the fake videos are generated with seven distinct deepfake
generation models, selected based on the quality of the generated content. We
organize the training, validation and test splits such that only a subset of
the chosen generative models and languages are available during training, thus
creating several challenging open-set evaluation setups. We perform experiments
with various pre-trained and fine-tuned deepfake detectors proposed in recent
literature. Our results show that state-of-the-art detectors are not currently
able to maintain their performance levels when tested in our open-set
scenarios. We publicly release our data and code at:
https://huggingface.co/datasets/unibuc-cs/MAVOS-DD.

</details>


### [313] [PhiNet v2: A Mask-Free Brain-Inspired Vision Foundation Model from Video](https://arxiv.org/abs/2505.11129)
*Makoto Yamada,Kian Ming A. Chai,Ayoub Rhim,Satoki Ishikawa,Mohammad Sabokrou,Yao-Hung Hubert Tsai*

Main category: cs.CV

TL;DR: PhiNet v2是一种基于Transformer的架构，通过处理时序视觉输入（图像序列）而不依赖强增强，利用变分推断学习鲁棒的视觉表示，性能与最先进的视觉基础模型相当。


<details>
  <summary>Details</summary>
Motivation: 当前自监督学习未充分利用生物视觉处理系统的见解，PhiNet v2旨在构建更接近人类认知的生物启发的计算机视觉系统。

Method: 基于Transformer的架构，处理时序视觉输入，利用变分推断学习表示，无需强数据增强。

Result: PhiNet v2在性能上与最先进的视觉基础模型竞争，同时能从序列输入中学习。

Conclusion: PhiNet v2是向更生物合理的计算机视觉系统迈出的重要一步，更接近人类视觉处理方式。

Abstract: Recent advances in self-supervised learning (SSL) have revolutionized
computer vision through innovative architectures and learning objectives, yet
they have not fully leveraged insights from biological visual processing
systems. Recently, a brain-inspired SSL model named PhiNet was proposed; it is
based on a ResNet backbone and operates on static image inputs with strong
augmentation. In this paper, we introduce PhiNet v2, a novel Transformer-based
architecture that processes temporal visual input (that is, sequences of
images) without relying on strong augmentation. Our model leverages variational
inference to learn robust visual representations from continuous input streams,
similar to human visual processing. Through extensive experimentation, we
demonstrate that PhiNet v2 achieves competitive performance compared to
state-of-the-art vision foundation models, while maintaining the ability to
learn from sequential input without strong data augmentation. This work
represents a significant step toward more biologically plausible computer
vision systems that process visual information in a manner more closely aligned
with human cognitive processes.

</details>


### [314] [Robust Emotion Recognition via Bi-Level Self-Supervised Continual Learning](https://arxiv.org/abs/2505.10575)
*Adnan Ahmad,Bahareh Nakisa,Mohammad Naim Rastgoo*

Main category: cs.CV

TL;DR: 提出了一种名为SSOCL的双层自监督持续学习框架，用于处理连续、未标记的生理数据流，提升情绪识别的性能。


<details>
  <summary>Details</summary>
Motivation: 生理信号（如EEG）的情绪识别受限于跨主体差异和噪声标签，现有方法难以应对连续、未标记的数据流。

Method: 采用双层自监督持续学习框架（SSOCL），结合动态记忆缓冲区和伪标签分配，通过快速适应模块和聚类映射模块处理数据流。

Result: 在两个主流EEG任务上的实验验证表明，SSOCL能够适应连续数据流，并在跨主体泛化性能上优于现有方法。

Conclusion: SSOCL框架为处理连续、未标记的生理数据流提供了有效解决方案，显著提升了情绪识别的性能。

Abstract: Emotion recognition through physiological signals such as
electroencephalogram (EEG) has become an essential aspect of affective
computing and provides an objective way to capture human emotions. However,
physiological data characterized by cross-subject variability and noisy labels
hinder the performance of emotion recognition models. Existing domain
adaptation and continual learning methods struggle to address these issues,
especially under realistic conditions where data is continuously streamed and
unlabeled. To overcome these limitations, we propose a novel bi-level
self-supervised continual learning framework, SSOCL, based on a dynamic memory
buffer. This bi-level architecture iteratively refines the dynamic buffer and
pseudo-label assignments to effectively retain representative samples, enabling
generalization from continuous, unlabeled physiological data streams for
emotion recognition. The assigned pseudo-labels are subsequently leveraged for
accurate emotion prediction. Key components of the framework, including a fast
adaptation module and a cluster-mapping module, enable robust learning and
effective handling of evolving data streams. Experimental validation on two
mainstream EEG tasks demonstrates the framework's ability to adapt to
continuous data streams while maintaining strong generalization across
subjects, outperforming existing approaches.

</details>


### [315] [One Image is Worth a Thousand Words: A Usability Preservable Text-Image Collaborative Erasing Framework](https://arxiv.org/abs/2505.11131)
*Feiran Li,Qianqian Xu,Shilong Bao,Zhiyong Yang,Xiaochun Cao,Qingming Huang*

Main category: cs.CV

TL;DR: 提出了一种文本-图像协同概念擦除框架（Co-Erasing），通过结合视觉监督提升擦除效果，减少对其他良性概念的影响。


<details>
  <summary>Details</summary>
Motivation: 当前基于文本提示的概念擦除方法存在文本与图像模态间的知识鸿沟，导致擦除效果与可用性难以平衡。

Method: 结合文本提示和对应不良图像，通过负引导降低目标概念的生成概率，并设计文本引导的图像概念细化策略。

Result: 实验表明，Co-Erasing在擦除效果和可用性之间取得了更好的平衡，显著优于现有方法。

Conclusion: Co-Erasing通过视觉监督和文本引导的细化策略，有效解决了概念擦除中的模态鸿沟问题。

Abstract: Concept erasing has recently emerged as an effective paradigm to prevent
text-to-image diffusion models from generating visually undesirable or even
harmful content. However, current removal methods heavily rely on manually
crafted text prompts, making it challenging to achieve a high erasure
(efficacy) while minimizing the impact on other benign concepts (usability). In
this paper, we attribute the limitations to the inherent gap between the text
and image modalities, which makes it hard to transfer the intricately entangled
concept knowledge from text prompts to the image generation process. To address
this, we propose a novel solution by directly integrating visual supervision
into the erasure process, introducing the first text-image Collaborative
Concept Erasing (Co-Erasing) framework. Specifically, Co-Erasing describes the
concept jointly by text prompts and the corresponding undesirable images
induced by the prompts, and then reduces the generating probability of the
target concept through negative guidance. This approach effectively bypasses
the knowledge gap between text and image, significantly enhancing erasure
efficacy. Additionally, we design a text-guided image concept refinement
strategy that directs the model to focus on visual features most relevant to
the specified text concept, minimizing disruption to other benign concepts.
Finally, comprehensive experiments suggest that Co-Erasing outperforms
state-of-the-art erasure approaches significantly with a better trade-off
between efficacy and usability. Codes are available at
https://github.com/Ferry-Li/Co-Erasing.

</details>


### [316] [Human-Aligned Bench: Fine-Grained Assessment of Reasoning Ability in MLLMs vs. Humans](https://arxiv.org/abs/2505.11141)
*Yansheng Qiu,Li Xiao,Zhaopan Xu,Pengfei Zhou,Zheng Wang,Kaipeng Zhang*

Main category: cs.CV

TL;DR: 论文提出了Human-Aligned Bench基准，用于评估多模态推理模型与人类表现的细粒度对齐，揭示了当前模型与人类在多模态推理上的显著差异。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索当前多模态大语言模型（MLLMs）是否具备与人类相当的推理能力，填补这一领域的空白。

Method: 通过收集9,794个多模态问题（包括双语和纯文本问题），涵盖四种推理类型，并结合人类成功率与易错选项，构建基准。

Result: 实验显示当前MLLMs在多模态推理上的表现与人类存在显著差异。

Conclusion: 基准结果为下一代模型的开发提供了重要参考。

Abstract: The goal of achieving Artificial General Intelligence (AGI) is to imitate
humans and surpass them. Models such as OpenAI's o1, o3, and DeepSeek's R1 have
demonstrated that large language models (LLMs) with human-like reasoning
capabilities exhibit exceptional performance and are being gradually integrated
into multimodal large language models (MLLMs). However, whether these models
possess capabilities comparable to humans in handling reasoning tasks remains
unclear at present. In this paper, we propose Human-Aligned Bench, a benchmark
for fine-grained alignment of multimodal reasoning with human performance.
Specifically, we collected 9,794 multimodal questions that solely rely on
contextual reasoning, including bilingual (Chinese and English) multimodal
questions and pure text-based questions, encompassing four question types:
visual reasoning, definition judgment, analogical reasoning, and logical
judgment. More importantly, each question is accompanied by human success rates
and options that humans are prone to choosing incorrectly. Extensive
experiments on the Human-Aligned Bench reveal notable differences between the
performance of current MLLMs in multimodal reasoning and human performance. The
findings on our benchmark provide insights into the development of the
next-generation models.

</details>


### [317] [CheX-DS: Improving Chest X-ray Image Classification with Ensemble Learning Based on DenseNet and Swin Transformer](https://arxiv.org/abs/2505.11168)
*Xinran Li,Yu Liu,Xiujuan Xu,Xiaowei Zhao*

Main category: cs.CV

TL;DR: 本文提出了一种结合CNN和Transformer的模型CheX-DS，用于胸部X光片的多标签分类，解决了数据不平衡问题，并在NIH ChestX-ray14数据集上取得了83.76%的平均AUC。


<details>
  <summary>Details</summary>
Motivation: 当前胸部疾病自动诊断方法多基于CNN，忽视了全局特征，而自注意力机制在计算机视觉领域表现出色，因此结合两者优势提出新模型。

Method: 基于DenseNet和Swin Transformer，采用集成深度学习技术结合两者，损失函数结合加权二元交叉熵和非对称损失。

Result: 在NIH ChestX-ray14数据集上，模型平均AUC达到83.76%，优于先前研究。

Conclusion: CheX-DS模型有效结合了CNN和Transformer的优势，解决了数据不平衡问题，性能优越。

Abstract: The automatic diagnosis of chest diseases is a popular and challenging task.
Most current methods are based on convolutional neural networks (CNNs), which
focus on local features while neglecting global features. Recently,
self-attention mechanisms have been introduced into the field of computer
vision, demonstrating superior performance. Therefore, this paper proposes an
effective model, CheX-DS, for classifying long-tail multi-label data in the
medical field of chest X-rays. The model is based on the excellent CNN model
DenseNet for medical imaging and the newly popular Swin Transformer model,
utilizing ensemble deep learning techniques to combine the two models and
leverage the advantages of both CNNs and Transformers. The loss function of
CheX-DS combines weighted binary cross-entropy loss with asymmetric loss,
effectively addressing the issue of data imbalance. The NIH ChestX-ray14
dataset is selected to evaluate the model's effectiveness. The model
outperforms previous studies with an excellent average AUC score of 83.76\%,
demonstrating its superior performance.

</details>


### [318] [A High-Performance Thermal Infrared Object Detection Framework with Centralized Regulation](https://arxiv.org/abs/2505.10825)
*Jinke Li,Yue Wu,Xiaoyan Yang*

Main category: cs.CV

TL;DR: 提出了一种基于集中特征调节的新型热红外目标检测框架CRT-YOLO，通过全局交互和多尺度注意力模块提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 传统方法在热红外图像目标检测中难以有效提取和融合局部-全局信息，限制了性能。

Method: 结合高效多尺度注意力（EMA）模块和集中特征金字塔（CFP）网络，实现全局特征调节和长程依赖捕捉。

Result: 在两个基准数据集上的实验表明，CRT-YOLO显著优于传统方法。

Conclusion: CRT-YOLO通过全局特征调节和多尺度注意力模块，有效提升了热红外目标检测性能，具有重要应用潜力。

Abstract: Thermal Infrared (TIR) technology involves the use of sensors to detect and
measure infrared radiation emitted by objects, and it is widely utilized across
a broad spectrum of applications. The advancements in object detection methods
utilizing TIR images have sparked significant research interest. However, most
traditional methods lack the capability to effectively extract and fuse
local-global information, which is crucial for TIR-domain feature attention. In
this study, we present a novel and efficient thermal infrared object detection
framework, known as CRT-YOLO, that is based on centralized feature regulation,
enabling the establishment of global-range interaction on TIR information. Our
proposed model integrates efficient multi-scale attention (EMA) modules, which
adeptly capture long-range dependencies while incurring minimal computational
overhead. Additionally, it leverages the Centralized Feature Pyramid (CFP)
network, which offers global regulation of TIR features. Extensive experiments
conducted on two benchmark datasets demonstrate that our CRT-YOLO model
significantly outperforms conventional methods for TIR image object detection.
Furthermore, the ablation study provides compelling evidence of the
effectiveness of our proposed modules, reinforcing the potential impact of our
approach on advancing the field of thermal infrared object detection.

</details>


### [319] [Imputation-free and Alignment-free: Incomplete Multi-view Clustering Driven by Consensus Semantic Learning](https://arxiv.org/abs/2505.11182)
*Yuzhuo Dai,Jiaqi Jin,Zhibin Dong,Siwei Wang,Xinwang Liu,En Zhu,Xihong Yang,Xinbiao Gan,Yu Feng*

Main category: cs.CV

TL;DR: FreeCSL是一种无需插补和对齐的不完整多视图聚类框架，通过学习共识原型和启发式图聚类，解决现有方法在语义一致性和聚类结构上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理不完整多视图聚类时，未能有效构建跨视图的共享语义空间，且过度依赖一致性导致不可靠的插补和对齐。

Method: 提出FreeCSL框架，通过学习共识原型构建共享语义空间，并设计基于模块化的启发式图聚类以增强聚类语义。

Result: 实验表明，FreeCSL在不完整多视图聚类任务中实现了更可靠和鲁棒的聚类结果。

Conclusion: FreeCSL通过共识语义学习和聚类结构恢复，显著提升了不完整多视图聚类的性能。

Abstract: In incomplete multi-view clustering (IMVC), missing data induce prototype
shifts within views and semantic inconsistencies across views. A feasible
solution is to explore cross-view consistency in paired complete observations,
further imputing and aligning the similarity relationships inherently shared
across views. Nevertheless, existing methods are constrained by two-tiered
limitations: (1) Neither instance- nor cluster-level consistency learning
construct a semantic space shared across views to learn consensus semantics.
The former enforces cross-view instances alignment, and wrongly regards
unpaired observations with semantic consistency as negative pairs; the latter
focuses on cross-view cluster counterparts while coarsely handling fine-grained
intra-cluster relationships within views. (2) Excessive reliance on consistency
results in unreliable imputation and alignment without incorporating
view-specific cluster information. Thus, we propose an IMVC framework,
imputation- and alignment-free for consensus semantics learning (FreeCSL). To
bridge semantic gaps across all observations, we learn consensus prototypes
from available data to discover a shared space, where semantically similar
observations are pulled closer for consensus semantics learning. To capture
semantic relationships within specific views, we design a heuristic graph
clustering based on modularity to recover cluster structure with intra-cluster
compactness and inter-cluster separation for cluster semantics enhancement.
Extensive experiments demonstrate, compared to state-of-the-art competitors,
FreeCSL achieves more confident and robust assignments on IMVC task.

</details>


### [320] [FALCON: False-Negative Aware Learning of Contrastive Negatives in Vision-Language Pretraining](https://arxiv.org/abs/2505.11192)
*Myunsoo Kim,Seong-Woong Shim,Byung-Jun Lee*

Main category: cs.CV

TL;DR: FALCON是一种自适应平衡硬负样本和假负样本的学习策略，用于提升视觉语言预训练（VLP）的性能。


<details>
  <summary>Details</summary>
Motivation: 假负样本在VLP中导致监督信号冲突，影响嵌入空间质量和硬负采样的有效性。

Method: FALCON通过动态负样本挖掘调度器，自适应选择适合硬度的负样本，以优化跨模态对齐。

Result: 实验表明，FALCON在ALBEF和BLIP-2框架及多种下游任务中显著提升性能。

Conclusion: FALCON有效缓解假负样本的影响，具有鲁棒性和广泛适用性。

Abstract: False negatives pose a critical challenge in vision-language pretraining
(VLP) due to the many-to-many correspondence between images and texts in
large-scale datasets. These false negatives introduce conflicting supervision
signals that degrade the learned embedding space and diminish the effectiveness
of hard negative sampling. In this paper, we propose FALCON (False-negative
Aware Learning of COntrastive Negatives), a learning-based mini-batch
construction strategy that adaptively balances the trade-off between hard and
false negatives during VLP. Rather than relying on fixed heuristics, FALCON
employs a negative mining scheduler that dynamically selects negative samples
of appropriate hardness for each anchor instance during mini-batch
construction, guided by a proxy for cross-modal alignment improvement.
Experimental results demonstrate that FALCON significantly improves performance
across two widely adopted VLP frameworks (ALBEF, BLIP-2) and a broad range of
downstream tasks and evaluation settings, underscoring its effectiveness and
robustness in mitigating the impact of false negatives.

</details>


### [321] [Equal is Not Always Fair: A New Perspective on Hyperspectral Representation Non-Uniformity](https://arxiv.org/abs/2505.11267)
*Wuzhou Quan,Mingqiang Wei,Jinhui Tang*

Main category: cs.CV

TL;DR: FairHyp是一个针对高光谱图像（HSI）表示中非均匀性问题的公平性导向框架，通过专门模块解决空间、光谱和特征的复杂冲突，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 高光谱图像表示中存在复杂的非均匀性问题，现有模型假设维度同质性导致性能不佳，FairHyp旨在通过模块化设计解决这一问题。

Method: FairHyp包含Runge-Kutta启发的空间适配器、多感受野卷积模块和光谱上下文状态空间模型，分别处理空间、特征和光谱的非均匀性。

Result: 在分类、去噪、超分辨率和修复等任务中，FairHyp均优于现有方法，验证了其有效性。

Conclusion: FairHyp将公平性重新定义为HSI建模的结构性需求，为高维视觉任务提供了平衡适应性、效率和保真度的新范式。

Abstract: Hyperspectral image (HSI) representation is fundamentally challenged by
pervasive non-uniformity, where spectral dependencies, spatial continuity, and
feature efficiency exhibit complex and often conflicting behaviors. Most
existing models rely on a unified processing paradigm that assumes homogeneity
across dimensions, leading to suboptimal performance and biased
representations. To address this, we propose FairHyp, a fairness-directed
framework that explicitly disentangles and resolves the threefold
non-uniformity through cooperative yet specialized modules. We introduce a
Runge-Kutta-inspired spatial variability adapter to restore spatial coherence
under resolution discrepancies, a multi-receptive field convolution module with
sparse-aware refinement to enhance discriminative features while respecting
inherent sparsity, and a spectral-context state space model that captures
stable and long-range spectral dependencies via bidirectional Mamba scanning
and statistical aggregation. Unlike one-size-fits-all solutions, FairHyp
achieves dimension-specific adaptation while preserving global consistency and
mutual reinforcement. This design is grounded in the view that non-uniformity
arises from the intrinsic structure of HSI representations, rather than any
particular task setting. To validate this, we apply FairHyp across four
representative tasks including classification, denoising, super-resolution, and
inpaintin, demonstrating its effectiveness in modeling a shared structural
flaw. Extensive experiments show that FairHyp consistently outperforms
state-of-the-art methods under varied imaging conditions. Our findings redefine
fairness as a structural necessity in HSI modeling and offer a new paradigm for
balancing adaptability, efficiency, and fidelity in high-dimensional vision
tasks.

</details>


### [322] [Temporally-Grounded Language Generation: A Benchmark for Real-Time Vision-Language Models](https://arxiv.org/abs/2505.11326)
*Keunwoo Peter Yu,Joyce Chai*

Main category: cs.CV

TL;DR: 该论文提出了一个名为TGLG的新基准任务，用于评估视觉语言模型在实时交互环境中的表现，并开发了VLM-TSI模型，显著优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决视觉语言模型在实时交互环境中生成语义准确且时间精确的语句的需求。

Method: 提出了TGLG基准任务和TRACE评估指标，并开发了VLM-TSI模型，通过时间同步交错视觉和语言令牌实现实时语言生成。

Result: 实验结果表明VLM-TSI显著优于基线模型，但整体性能仍有提升空间。

Conclusion: TGLG任务的难度凸显了实时视觉语言模型研究的必要性，未来需进一步探索。

Abstract: Vision-language models (VLMs) have shown remarkable progress in offline tasks
such as image captioning and video question answering. However, real-time
interactive environments impose new demands on VLMs, requiring them to generate
utterances that are not only semantically accurate but also precisely timed. We
identify two core capabilities necessary for such settings --
$\textit{perceptual updating}$ and $\textit{contingency awareness}$ -- and
propose a new benchmark task, $\textbf{Temporally-Grounded Language Generation
(TGLG)}$, to evaluate them. TGLG requires models to generate utterances in
response to streaming video such that both content and timing align with
dynamic visual input. To support this benchmark, we curate evaluation datasets
from sports broadcasting and egocentric human interaction domains, and
introduce a new metric, $\textbf{TRACE}$, to evaluate TGLG by jointly measuring
semantic similarity and temporal alignment. Finally, we present
$\textbf{Vision-Language Model with Time-Synchronized Interleaving (VLM-TSI)}$,
a model that interleaves visual and linguistic tokens in a time-synchronized
manner, enabling real-time language generation without relying on turn-based
assumptions. Experimental results show that VLM-TSI significantly outperforms a
strong baseline, yet overall performance remains modest -- highlighting the
difficulty of TGLG and motivating further research in real-time VLMs. Code and
data available $\href{https://github.com/yukw777/tglg}{here}$.

</details>


### [323] [Patho-R1: A Multimodal Reinforcement Learning-Based Pathology Expert Reasoner](https://arxiv.org/abs/2505.11404)
*Wenchuan Zhang,Penghao Zhang,Jingru Guo,Tao Cheng,Jie Chen,Shuwan Zhang,Zhang Zhang,Yuhao Yi,Hong Bu*

Main category: cs.CV

TL;DR: 该研究通过构建高质量、推理导向的病理学数据集，开发了多模态RL病理学推理器Patho-R1，并通过三阶段训练提升其性能。


<details>
  <summary>Details</summary>
Motivation: 当前病理学专用视觉语言模型在诊断准确性和推理合理性上存在不足，主要由于现有数据集缺乏深度和结构化诊断范例。

Method: 利用病理学教科书和专家构建数据集，通过三阶段训练：知识注入、推理激励和强化学习优化。

Result: Patho-R1和PathoCLIP在多种病理学任务中表现优异，如零样本分类和跨模态检索。

Conclusion: 该研究为病理学领域提供了高质量数据集和高效推理模型，显著提升了性能。

Abstract: Recent advances in vision language models (VLMs) have enabled broad progress
in the general medical field. However, pathology still remains a more
challenging subdomain, with current pathology specific VLMs exhibiting
limitations in both diagnostic accuracy and reasoning plausibility. Such
shortcomings are largely attributable to the nature of current pathology
datasets, which are primarily composed of image description pairs that lack the
depth and structured diagnostic paradigms employed by real world pathologists.
In this study, we leverage pathology textbooks and real world pathology experts
to construct high-quality, reasoning-oriented datasets. Building on this, we
introduce Patho-R1, a multimodal RL-based pathology Reasoner, trained through a
three-stage pipeline: (1) continued pretraining on 3.5 million image-text pairs
for knowledge infusion; (2) supervised fine-tuning on 500k high-quality
Chain-of-Thought samples for reasoning incentivizing; (3) reinforcement
learning using Group Relative Policy Optimization and Decoupled Clip and
Dynamic sAmpling Policy Optimization strategies for multimodal reasoning
quality refinement. To further assess the alignment quality of our dataset, we
propose PathoCLIP, trained on the same figure-caption corpus used for continued
pretraining. Comprehensive experimental results demonstrate that both PathoCLIP
and Patho-R1 achieve robust performance across a wide range of
pathology-related tasks, including zero-shot classification, cross-modal
retrieval, Visual Question Answering, and Multiple Choice Question. Our project
is available at the Patho-R1 repository:
https://github.com/Wenchuan-Zhang/Patho-R1.

</details>


### [324] [Improving Object Detection Performance through YOLOv8: A Comprehensive Training and Evaluation Study](https://arxiv.org/abs/2505.11424)
*Rana Poureskandar,Shiva Razzagzadeh*

Main category: cs.CV

TL;DR: 研究评估了基于YOLOv8的分割模型在面部图像中检测和分割皱纹的性能。


<details>
  <summary>Details</summary>
Motivation: 探索YOLOv8模型在面部皱纹检测与分割中的适用性和效果。

Method: 使用YOLOv8架构构建分割模型，并在面部图像数据集上进行训练和测试。

Result: 模型能够有效检测和分割面部皱纹，展示了较高的准确性和鲁棒性。

Conclusion: YOLOv8分割模型在面部皱纹检测任务中表现优异，具有实际应用潜力。

Abstract: This study evaluated the performance of a YOLOv8-based segmentation model for
detecting and segmenting wrinkles in facial images.

</details>


### [325] [SurgPose: Generalisable Surgical Instrument Pose Estimation using Zero-Shot Learning and Stereo Vision](https://arxiv.org/abs/2505.11439)
*Utsav Rai,Haozheng Xu,Stamatia Giannarou*

Main category: cs.CV

TL;DR: 本文提出了一种基于零样本RGB-D模型的6自由度手术工具姿态估计方法，结合RAFT-Stereo深度估计和优化的SAM-6D模型，显著提升了在未见手术工具上的姿态估计性能。


<details>
  <summary>Details</summary>
Motivation: 传统标记方法和监督学习方法在手术工具姿态估计中存在遮挡、反射和泛化性不足的问题，零样本方法在RMIS中尚未探索。

Method: 结合FoundationPose和SAM-6D模型，引入RAFT-Stereo深度估计，并优化SAM-6D的分割模块为Mask R-CNN，提升复杂环境下的性能。

Result: 优化的SAM-6D在零样本姿态估计中超越FoundationPose，为RMIS中的RGB-D零样本方法设定了新基准。

Conclusion: 该方法提升了未见手术工具的泛化性，并首次将RGB-D零样本方法应用于RMIS。

Abstract: Accurate pose estimation of surgical tools in Robot-assisted Minimally
Invasive Surgery (RMIS) is essential for surgical navigation and robot control.
While traditional marker-based methods offer accuracy, they face challenges
with occlusions, reflections, and tool-specific designs. Similarly, supervised
learning methods require extensive training on annotated datasets, limiting
their adaptability to new tools. Despite their success in other domains,
zero-shot pose estimation models remain unexplored in RMIS for pose estimation
of surgical instruments, creating a gap in generalising to unseen surgical
tools. This paper presents a novel 6 Degrees of Freedom (DoF) pose estimation
pipeline for surgical instruments, leveraging state-of-the-art zero-shot RGB-D
models like the FoundationPose and SAM-6D. We advanced these models by
incorporating vision-based depth estimation using the RAFT-Stereo method, for
robust depth estimation in reflective and textureless environments.
Additionally, we enhanced SAM-6D by replacing its instance segmentation module,
Segment Anything Model (SAM), with a fine-tuned Mask R-CNN, significantly
boosting segmentation accuracy in occluded and complex conditions. Extensive
validation reveals that our enhanced SAM-6D surpasses FoundationPose in
zero-shot pose estimation of unseen surgical instruments, setting a new
benchmark for zero-shot RGB-D pose estimation in RMIS. This work enhances the
generalisability of pose estimation for unseen objects and pioneers the
application of RGB-D zero-shot methods in RMIS.

</details>


### [326] [HumaniBench: A Human-Centric Framework for Large Multimodal Models Evaluation](https://arxiv.org/abs/2505.11454)
*Shaina Raza,Aravind Narayanan,Vahid Reza Khazaie,Ashmal Vayani,Mukund S. Chettiar,Amandeep Singh,Mubarak Shah,Deval Pandya*

Main category: cs.CV

TL;DR: HumaniBench是一个专注于人类中心AI原则的基准测试，包含32K真实世界图像问题对，评估公平性、伦理、同理心等七项原则。测试15种先进LMMs显示，专有模型表现较好，但开放模型在平衡准确性与人类对齐原则方面仍有挑战。


<details>
  <summary>Details</summary>
Motivation: 当前大型多模态模型（LMMs）在视觉语言任务中表现优异，但在公平性、伦理等人类中心标准上仍有不足，需开发专门基准以评估和改进。

Method: 通过GPT4辅助的标注流程构建32K图像问题对，涵盖七项人类中心AI原则和七种任务（如VQA、多语言QA等），并由专家验证。

Result: 专有模型整体领先，但开放模型在视觉基础和鲁棒性上表现较弱，部分模型难以平衡准确性与人类对齐原则。

Conclusion: HumaniBench为首个围绕人类中心AI原则设计的基准，为诊断对齐差距和指导LMMs实现社会责任感提供了测试平台。

Abstract: Large multimodal models (LMMs) now excel on many vision language benchmarks,
however, they still struggle with human centered criteria such as fairness,
ethics, empathy, and inclusivity, key to aligning with human values. We
introduce HumaniBench, a holistic benchmark of 32K real-world image question
pairs, annotated via a scalable GPT4o assisted pipeline and exhaustively
verified by domain experts. HumaniBench evaluates seven Human Centered AI
(HCAI) principles: fairness, ethics, understanding, reasoning, language
inclusivity, empathy, and robustness, across seven diverse tasks, including
open and closed ended visual question answering (VQA), multilingual QA, visual
grounding, empathetic captioning, and robustness tests. Benchmarking 15 state
of the art LMMs (open and closed source) reveals that proprietary models
generally lead, though robustness and visual grounding remain weak points. Some
open-source models also struggle to balance accuracy with adherence to
human-aligned principles. HumaniBench is the first benchmark purpose built
around HCAI principles. It provides a rigorous testbed for diagnosing alignment
gaps and guiding LMMs toward behavior that is both accurate and socially
responsible. Dataset, annotation prompts, and evaluation code are available at:
https://vectorinstitute.github.io/HumaniBench

</details>


### [327] [DRAGON: A Large-Scale Dataset of Realistic Images Generated by Diffusion Models](https://arxiv.org/abs/2505.11257)
*Giulia Bertazzini,Daniele Baracchi,Dasara Shullani,Isao Echizen,Alessandro Piva*

Main category: cs.CV

TL;DR: 论文介绍了DRAGON数据集，包含25种扩散模型生成的图像，旨在支持合成内容检测技术的发展。


<details>
  <summary>Details</summary>
Motivation: 扩散模型生成的图像被广泛用于虚假信息传播，现有检测方法依赖大量样本且数据集覆盖范围有限，亟需更全面的数据集。

Method: 提出DRAGON数据集，涵盖多种扩散模型，并通过大语言模型扩展输入提示以提升图像多样性和质量。

Result: 数据集包含多种尺寸，支持不同研究需求，并附带测试集作为新方法的基准。

Conclusion: DRAGON数据集为合成内容检测和溯源技术提供了重要支持，有助于应对虚假图像挑战。

Abstract: The remarkable ease of use of diffusion models for image generation has led
to a proliferation of synthetic content online. While these models are often
employed for legitimate purposes, they are also used to generate fake images
that support misinformation and hate speech. Consequently, it is crucial to
develop robust tools capable of detecting whether an image has been generated
by such models. Many current detection methods, however, require large volumes
of sample images for training. Unfortunately, due to the rapid evolution of the
field, existing datasets often cover only a limited range of models and quickly
become outdated. In this work, we introduce DRAGON, a comprehensive dataset
comprising images from 25 diffusion models, spanning both recent advancements
and older, well-established architectures. The dataset contains a broad variety
of images representing diverse subjects. To enhance image realism, we propose a
simple yet effective pipeline that leverages a large language model to expand
input prompts, thereby generating more diverse and higher-quality outputs, as
evidenced by improvements in standard quality metrics. The dataset is provided
in multiple sizes (ranging from extra-small to extra-large) to accomodate
different research scenarios. DRAGON is designed to support the forensic
community in developing and evaluating detection and attribution techniques for
synthetic content. Additionally, the dataset is accompanied by a dedicated test
set, intended to serve as a benchmark for assessing the performance of newly
developed methods.

</details>


### [328] [Dynamic Base model Shift for Delta Compression](https://arxiv.org/abs/2505.11344)
*Chenyu Huang,Peng Ye,Shenghe Zheng,Xiaohui Wang,Lei Bai,Tao Chen,Wanli Ouyang*

Main category: cs.CV

TL;DR: 论文提出动态基础模型转移（DBMS）方法，通过动态调整基础模型以优化delta压缩性能，显著提升高压缩率下的模型表现。


<details>
  <summary>Details</summary>
Motivation: 现有delta压缩方法默认使用预训练模型作为基础模型，可能导致性能显著下降，尤其是在高压缩率下。

Method: 提出DBMS方法，动态调整基础模型以适应目标任务，并通过学习两个参数优化压缩性能。

Result: DBMS在高压缩率下仍能保持微调模型的性能，显著优于现有方法。

Conclusion: DBMS是一种通用方法，可与多种其他方法结合，适用于不同类型的模型。

Abstract: Transformer-based models with the pretrain-finetune paradigm bring about
significant progress, along with the heavy storage and deployment costs of
finetuned models on multiple tasks. Delta compression attempts to lower the
costs by reducing the redundancy of delta parameters (i.e., the difference
between the finetuned and pre-trained model weights) through pruning or
quantization. However, existing methods by default employ the pretrained model
as the base model and compress the delta parameters for every task, which may
causes significant performance degradation, especially when the compression
rate is extremely high. To tackle this issue, we investigate the impact of
different base models on the performance of delta compression and find that the
pre-trained base model can hardly be optimal. To this end, we propose Dynamic
Base Model Shift (DBMS), which dynamically adapts the base model to the target
task before performing delta compression. Specifically, we adjust two
parameters, which respectively determine the magnitude of the base model shift
and the overall scale of delta compression, to boost the compression
performance on each task. Through low-cost learning of these two parameters,
our DBMS can maintain most of the finetuned model's performance even under an
extremely high compression ratio setting, significantly surpassing existing
methods. Moreover, our DBMS is orthogonal and can be integrated with a variety
of other methods, and it has been evaluated across different types of models
including language, vision transformer, and multi-modal models.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [329] [Linear Convergence of the Frank-Wolfe Algorithm over Product Polytopes](https://arxiv.org/abs/2505.11259)
*Gabriele Iommazzo,David Martínez-Rubio,Francisco Criado,Elias Wirth,Sebastian Pokutta*

Main category: math.OC

TL;DR: 研究了Frank-Wolfe算法在乘积多面体上的线性收敛性，分析了两种条件数（金字塔宽度和顶点-面距离），并展示了在μ-Polyak-Łojasiewicz凸目标下的线性收敛率。


<details>
  <summary>Details</summary>
Motivation: 探讨Frank-Wolfe算法在高维乘积多面体上的收敛性能，为实际应用提供理论支持。

Method: 分析乘积多面体的两种条件数，基于单个多面体组件的条件数，推导线性收敛率。

Result: 在μ-Polyak-Łojasiewicz凸目标下，算法表现出线性收敛，并通过实验验证了其高效性。

Conclusion: 提出的条件数和算法在高维多面体交集问题中具有实际应用价值。

Abstract: We study the linear convergence of Frank-Wolfe algorithms over product
polytopes. We analyze two condition numbers for the product polytope, namely
the \emph{pyramidal width} and the \emph{vertex-facet distance}, based on the
condition numbers of individual polytope components. As a result, for convex
objectives that are $\mu$-Polyak-{\L}ojasiewicz, we show linear convergence
rates quantified in terms of the resulting condition numbers. We apply our
results to the problem of approximately finding a feasible point in a polytope
intersection in high-dimensions, and demonstrate the practical efficiency of
our algorithms through empirical results.

</details>


### [330] [Revisiting Stochastic Approximation and Stochastic Gradient Descent](https://arxiv.org/abs/2505.11343)
*Rajeeva Laxman Karandikar,Bhamidi Visweswara Rao,Mathukumalli Vidyasagar*

Main category: math.OC

TL;DR: 本文重新审视了随机逼近（SA）和随机梯度下降（SGD），提出了新的收敛性充分条件，放宽了对噪声或测量误差的限制（无需有限二阶矩或均值），并为零阶SGD的收敛性提供了充分条件。


<details>
  <summary>Details</summary>
Motivation: 研究SA和SGD的收敛性，放宽传统理论中对噪声或测量误差的限制，扩展其适用范围。

Method: 推导新的收敛性充分条件，适用于噪声无有限二阶矩或均值的情况，并为零阶SGD（仅需两次函数评估）提供收敛性证明。

Result: 提出的条件是迄今为止最弱的，显著扩展了SA和SGD理论的适用性。

Conclusion: 本文通过放宽噪声限制和为零阶SGD提供收敛性条件，为SA和SGD的应用提供了更广泛的理论支持。

Abstract: In this paper, we take a fresh look at stochastic approximation (SA) and
Stochastic Gradient Descent (SGD). We derive new sufficient conditions for the
convergence of SA. In particular, the "noise" or measurement error need not
have a finite second moment, and under suitable conditions, not even a finite
mean. By adapting this method of proof, we also derive sufficient conditions
for the convergence of zero-order SGD, wherein the stochastic gradient is
computed using only two function evaluations, and no gradient computations. The
sufficient conditions derived here are the weakest to date, thus leading to a
considerable expansion of the applicability of SA and SGD theory.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [331] [MatTools: Benchmarking Large Language Models for Materials Science Tools](https://arxiv.org/abs/2505.10852)
*Siyu Liu,Jiamin Xu,Beilin Ye,Bo Hu,David J. Srolovitz,Tongqi Wen*

Main category: cond-mat.mtrl-sci

TL;DR: MatTools是一个评估大型语言模型（LLM）在材料科学问题中生成和执行代码能力的基准工具，包含QA基准和实际工具使用基准。


<details>
  <summary>Details</summary>
Motivation: 评估LLM在材料科学工具应用中的能力，促进更有效的AI系统开发。

Method: 设计了自动化方法收集材料科学工具使用示例，构建了69,225个QA对和49个任务（138个子任务）的基准。

Result: 评估发现：通用模型优于专用模型；AI模型之间表现相似；简单方法更有效。

Conclusion: MatTools为评估和改进LLM在材料科学中的应用提供了标准化框架。

Abstract: Large language models (LLMs) are increasingly applied to materials science
questions, including literature comprehension, property prediction, materials
discovery and alloy design. At the same time, a wide range of physics-based
computational approaches have been developed in which materials properties can
be calculated. Here, we propose a benchmark application to evaluate the
proficiency of LLMs to answer materials science questions through the
generation and safe execution of codes based on such physics-based
computational materials science packages. MatTools is built on two
complementary components: a materials simulation tool question-answer (QA)
benchmark and a real-world tool-usage benchmark. We designed an automated
methodology to efficiently collect real-world materials science tool-use
examples. The QA benchmark, derived from the pymatgen (Python Materials
Genomics) codebase and documentation, comprises 69,225 QA pairs that assess the
ability of an LLM to understand materials science tools. The real-world
benchmark contains 49 tasks (138 subtasks) requiring the generation of
functional Python code for materials property calculations. Our evaluation of
diverse LLMs yields three key insights: (1)Generalists outshine
specialists;(2)AI knows AI; and (3)Simpler is better. MatTools provides a
standardized framework for assessing and improving LLM capabilities for
materials science tool applications, facilitating the development of more
effective AI systems for materials science and general scientific research.

</details>


### [332] [Space Group Equivariant Crystal Diffusion](https://arxiv.org/abs/2505.10994)
*Rees Chang,Angela Pak,Alex Guerra,Ni Zhan,Nick Richardson,Elif Ertekin,Ryan P. Adams*

Main category: cond-mat.mtrl-sci

TL;DR: SGEquiDiff是一种生成模型，用于加速晶体材料的逆向设计，通过处理空间群对称性约束，实现了高性能的晶体生成。


<details>
  <summary>Details</summary>
Motivation: 晶体材料的逆向设计对多种技术有重要意义，但传统方法难以处理空间群对称性约束，而空间群对称性对材料性质影响重大。

Method: SGEquiDiff结合了SE(3)不变的离散采样器、基于Transformer的自回归采样器，以及空间群等变扩散方法，处理晶格、Wyckoff位置和原子坐标。

Result: SGEquiDiff在标准数据集上表现优异，通过定量指标和量子力学计算验证了其性能。

Conclusion: SGEquiDiff通过自然处理空间群对称性，为晶体材料的逆向设计提供了高效且高性能的解决方案。

Abstract: Accelerating inverse design of crystalline materials with generative models
has significant implications for a range of technologies. Unlike other atomic
systems, 3D crystals are invariant to discrete groups of isometries called the
space groups. Crucially, these space group symmetries are known to heavily
influence materials properties. We propose SGEquiDiff, a crystal generative
model which naturally handles space group constraints with space group
invariant likelihoods. SGEquiDiff consists of an SE(3)-invariant, telescoping
discrete sampler of crystal lattices; permutation-invariant, transformer-based
autoregressive sampling of Wyckoff positions, elements, and numbers of
symmetrically unique atoms; and space group equivariant diffusion of atomic
coordinates. We show that space group equivariant vector fields automatically
live in the tangent spaces of the Wyckoff positions. SGEquiDiff achieves
state-of-the-art performance on standard benchmark datasets as assessed by
quantitative proxy metrics and quantum mechanical calculations.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [333] [A Physics-Informed Convolutional Long Short Term Memory Statistical Model for Fluid Thermodynamics Simulations](https://arxiv.org/abs/2505.10919)
*Luca Menicali,Andrew Grace,David H. Richter,Stefano Castruccio*

Main category: physics.flu-dyn

TL;DR: 提出了一种基于物理信息的时空替代模型，用于模拟瑞利-贝纳德对流（RBC），结合卷积神经网络和创新的循环架构，显著降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 直接数值模拟（DNS）在计算上不可行，需要一种高效且物理可解释的替代方法。

Method: 结合卷积神经网络提取空间特征，采用受大语言模型启发的循环架构（上下文构建器和序列生成器）捕捉时间动态，并通过偏微分方程约束推理以确保物理可解释性。

Result: 模型成功复现了RBC动态的关键特征，同时显著降低了计算成本。

Conclusion: 该模型为长期模拟提供了一种可扩展的DNS替代方案。

Abstract: Fluid thermodynamics underpins atmospheric dynamics, climate science,
industrial applications, and energy systems. However, direct numerical
simulations (DNS) of such systems are computationally prohibitive. To address
this, we present a novel physics-informed spatio-temporal surrogate model for
Rayleigh-B\'enard convection (RBC), a canonical example of convective fluid
flow. Our approach combines convolutional neural networks for spatial feature
extraction with an innovative recurrent architecture inspired by large language
models, comprising a context builder and a sequence generator to capture
temporal dynamics. Inference is penalized with respect to the governing partial
differential equations to ensure physical interpretability. Given the
sensitivity of turbulent convection to initial conditions, we quantify
uncertainty using a conformal prediction framework. This model replicates key
features of RBC dynamics while significantly reducing computational cost,
offering a scalable alternative to DNS for long-term simulations.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [334] [TAIJI: MCP-based Multi-Modal Data Analytics on Data Lakes](https://arxiv.org/abs/2505.11270)
*Chao Zhang,Shaolei Zhang,Quehuan Liu,Sibei Chen,Tong Li,Ju Fan*

Main category: cs.DB

TL;DR: 论文提出了一种基于模型上下文协议（MCP）的多模态数据分析系统，旨在解决数据湖中多模态数据分析的准确性、效率和时效性问题。


<details>
  <summary>Details</summary>
Motivation: 数据湖中多模态数据的多样性给数据分析带来挑战，现有大型语言模型（LLMs）在准确性、效率和时效性方面表现不足。

Method: 提出基于MCP的新架构，包括语义操作符层次结构、AI代理驱动的NL2Operator翻译器、MCP执行框架和更新机制。

Result: 该设计提高了分析的准确性和效率，支持模块化部署以实现高可扩展性，并通过更新机制平衡数据时效性和推理效率。

Conclusion: 论文提出的系统为多模态数据分析提供了一种高效、准确且时效性强的解决方案。

Abstract: The variety of data in data lakes presents significant challenges for data
analytics, as data scientists must simultaneously analyze multi-modal data,
including structured, semi-structured, and unstructured data. While Large
Language Models (LLMs) have demonstrated promising capabilities, they still
remain inadequate for multi-modal data analytics in terms of accuracy,
efficiency, and freshness. First, current natural language (NL) or SQL-like
query languages may struggle to precisely and comprehensively capture users'
analytical intent. Second, relying on a single unified LLM to process diverse
data modalities often leads to substantial inference overhead. Third, data
stored in data lakes may be incomplete or outdated, making it essential to
integrate external open-domain knowledge to generate timely and relevant
analytics results.
  In this paper, we envision a new multi-modal data analytics system.
Specifically, we propose a novel architecture built upon the Model Context
Protocol (MCP), an emerging paradigm that enables LLMs to collaborate with
knowledgeable agents. First, we define a semantic operator hierarchy tailored
for querying multi-modal data in data lakes and develop an AI-agent-powered
NL2Operator translator to bridge user intent and analytical execution. Next, we
introduce an MCP-based execution framework, in which each MCP server hosts
specialized foundation models optimized for specific data modalities. This
design enhances both accuracy and efficiency, while supporting high scalability
through modular deployment. Finally, we propose a updating mechanism by
harnessing the deep research and machine unlearning techniques to refresh the
data lakes and LLM knowledges, with the goal of balancing the data freshness
and inference efficiency.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [335] [Humans expect rationality and cooperation from LLM opponents in strategic games](https://arxiv.org/abs/2505.11011)
*Darija Barak,Miguel Costa-Gomes*

Main category: econ.GN

TL;DR: 人类在与大型语言模型（LLM）对手的多玩家p-beauty竞赛中，选择显著更低的数字，尤其是高策略推理能力的参与者更倾向于选择零纳什均衡策略。


<details>
  <summary>Details</summary>
Motivation: 研究人类在战略环境中对LLM对手的行为反应，填补多玩家人类-LLM互动的研究空白。

Method: 采用货币激励的实验室实验，通过被试内设计比较人类与LLM对手的行为差异。

Result: 人类在与LLM对战时选择更低数字，高策略能力者更倾向零纳什均衡策略，且参与者认为LLM具有合作倾向。

Conclusion: 研究揭示了人类-LLM互动中的行为异质性，为混合系统的机制设计提供了重要启示。

Abstract: As Large Language Models (LLMs) integrate into our social and economic
interactions, we need to deepen our understanding of how humans respond to LLMs
opponents in strategic settings. We present the results of the first controlled
monetarily-incentivised laboratory experiment looking at differences in human
behaviour in a multi-player p-beauty contest against other humans and LLMs. We
use a within-subject design in order to compare behaviour at the individual
level. We show that, in this environment, human subjects choose significantly
lower numbers when playing against LLMs than humans, which is mainly driven by
the increased prevalence of `zero' Nash-equilibrium choices. This shift is
mainly driven by subjects with high strategic reasoning ability. Subjects who
play the zero Nash-equilibrium choice motivate their strategy by appealing to
perceived LLM's reasoning ability and, unexpectedly, propensity towards
cooperation. Our findings provide foundational insights into the multi-player
human-LLM interaction in simultaneous choice games, uncover heterogeneities in
both subjects' behaviour and beliefs about LLM's play when playing against
them, and suggest important implications for mechanism design in mixed
human-LLM systems.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [336] [Time Travel is Cheating: Going Live with DeepFund for Real-Time Fund Investment Benchmarking](https://arxiv.org/abs/2505.11065)
*Changlun Li,Yao Shi,Chen Wang,Qiqi Duan,Runke Ruan,Weijie Huang,Haonan Long,Lijun Huang,Yuyu Luo,Nan Tang*

Main category: cs.CE

TL;DR: DeepFund是一个实时基金基准工具，用于严格评估大型语言模型（LLMs）在实时市场条件下的表现，解决了历史回测中信息泄漏的问题。


<details>
  <summary>Details</summary>
Motivation: 现有评估LLM驱动交易策略的基准依赖历史回测，可能导致信息泄漏和过于乐观的性能估计，因此需要一种实时评估工具。

Method: DeepFund采用多智能体架构，直接连接实时股票市场数据，确保评估公平且无信息泄漏。

Result: 在实时评估中，即使是顶尖LLM（如DeepSeek-V3和Claude-3.7-Sonnet）也出现净交易亏损，表明LLM在主动基金管理中的局限性。

Conclusion: DeepFund揭示了LLM在实时基金管理中的实际挑战，为未来研究提供了更严格的评估标准。

Abstract: Large Language Models (LLMs) have demonstrated notable capabilities across
financial tasks, including financial report summarization, earnings call
transcript analysis, and asset classification. However, their real-world
effectiveness in managing complex fund investment remains inadequately
assessed. A fundamental limitation of existing benchmarks for evaluating
LLM-driven trading strategies is their reliance on historical back-testing,
inadvertently enabling LLMs to "time travel"-leveraging future information
embedded in their training corpora, thus resulting in possible information
leakage and overly optimistic performance estimates. To address this issue, we
introduce DeepFund, a live fund benchmark tool designed to rigorously evaluate
LLM in real-time market conditions. Utilizing a multi-agent architecture,
DeepFund connects directly with real-time stock market data-specifically data
published after each model pretraining cutoff-to ensure fair and leakage-free
evaluations. Empirical tests on nine flagship LLMs from leading global
institutions across multiple investment dimensions-including ticker-level
analysis, investment decision-making, portfolio management, and risk
control-reveal significant practical challenges. Notably, even cutting-edge
models such as DeepSeek-V3 and Claude-3.7-Sonnet incur net trading losses
within DeepFund real-time evaluation environment, underscoring the present
limitations of LLMs for active fund management. Our code is available at
https://github.com/HKUSTDial/DeepFund.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [337] [TCC-Bench: Benchmarking the Traditional Chinese Culture Understanding Capabilities of MLLMs](https://arxiv.org/abs/2505.11275)
*Pengju Xu,Yan Wang,Shuyuan Zhang,Xuan Zhou,Xin Li,Yue Yuan,Fengzhao Li,Shunyuan Zhou,Xingyu Wang,Yi Zhang,Haiying Zhao*

Main category: cs.MM

TL;DR: TCC-Bench是一个针对多模态大语言模型（MLLMs）设计的双语视觉问答基准，用于评估其对传统中国文化的理解能力。


<details>
  <summary>Details</summary>
Motivation: 当前MLLMs在非西方文化背景下的表现有限，限制了其广泛适用性。

Method: 采用半自动化流程，利用GPT-4生成问题候选，并通过人工筛选确保数据质量。

Result: 实验表明，现有MLLMs在文化相关的视觉内容推理上仍面临挑战。

Conclusion: 需要进一步研究以开发更具文化包容性和上下文感知能力的多模态系统。

Abstract: Recent progress in Multimodal Large Language Models (MLLMs) have
significantly enhanced the ability of artificial intelligence systems to
understand and generate multimodal content. However, these models often exhibit
limited effectiveness when applied to non-Western cultural contexts, which
raises concerns about their wider applicability. To address this limitation, we
propose the \textbf{T}raditional \textbf{C}hinese \textbf{C}ulture
understanding \textbf{Bench}mark (\textbf{TCC-Bench}), a bilingual
(\textit{i.e.}, Chinese and English) Visual Question Answering (VQA) benchmark
specifically designed for assessing the understanding of traditional Chinese
culture by MLLMs. TCC-Bench comprises culturally rich and visually diverse
data, incorporating images from museum artifacts, everyday life scenes, comics,
and other culturally significant contexts. We adopt a semi-automated pipeline
that utilizes GPT-4o in text-only mode to generate candidate questions,
followed by human curation to ensure data quality and avoid potential data
leakage. The benchmark also avoids language bias by preventing direct
disclosure of cultural concepts within question texts. Experimental evaluations
across a wide range of MLLMs demonstrate that current models still face
significant challenges when reasoning about culturally grounded visual content.
The results highlight the need for further research in developing culturally
inclusive and context-aware multimodal systems. The code and data can be found
at: https://github.com/Morty-Xu/TCC-Bench.

</details>


### [338] [Concept Drift Guided LayerNorm Tuning for Efficient Multimodal Metaphor Identification](https://arxiv.org/abs/2505.11237)
*Wenhao Qian,Zhenzhen Hu,Zijie Song,Jia Li*

Main category: cs.MM

TL;DR: 论文提出了一种名为CDGLT的高效训练框架，用于多模态隐喻识别，通过概念漂移和提示构造策略显著提升了性能并降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 多模态隐喻（如网络迷因）因其非常规表达和隐含意义而难以识别，现有方法在字面和比喻解释之间存在差距，且生成方法计算成本高。

Method: CDGLT框架结合概念漂移机制（利用SLERP生成新概念嵌入）和提示构造策略，优化特征提取与融合。

Result: CDGLT在MET-Meme基准测试中达到最优性能，同时显著降低训练成本。

Conclusion: CDGLT为高效准确的多模态隐喻理解提供了重要进展。

Abstract: Metaphorical imagination, the ability to connect seemingly unrelated
concepts, is fundamental to human cognition and communication. While
understanding linguistic metaphors has advanced significantly, grasping
multimodal metaphors, such as those found in internet memes, presents unique
challenges due to their unconventional expressions and implied meanings.
Existing methods for multimodal metaphor identification often struggle to
bridge the gap between literal and figurative interpretations. Additionally,
generative approaches that utilize large language models or text-to-image
models, while promising, suffer from high computational costs. This paper
introduces \textbf{C}oncept \textbf{D}rift \textbf{G}uided \textbf{L}ayerNorm
\textbf{T}uning (\textbf{CDGLT}), a novel and training-efficient framework for
multimodal metaphor identification. CDGLT incorporates two key innovations: (1)
Concept Drift, a mechanism that leverages Spherical Linear Interpolation
(SLERP) of cross-modal embeddings from a CLIP encoder to generate a new,
divergent concept embedding. This drifted concept helps to alleviate the gap
between literal features and the figurative task. (2) A prompt construction
strategy, that adapts the method of feature extraction and fusion using
pre-trained language models for the multimodal metaphor identification task.
CDGLT achieves state-of-the-art performance on the MET-Meme benchmark while
significantly reducing training costs compared to existing generative methods.
Ablation studies demonstrate the effectiveness of both Concept Drift and our
adapted LN Tuning approach. Our method represents a significant step towards
efficient and accurate multimodal metaphor understanding. The code is
available:
\href{https://github.com/Qianvenh/CDGLT}{https://github.com/Qianvenh/CDGLT}.

</details>


<div id='astro-ph.EP'></div>

# astro-ph.EP [[Back]](#toc)

### [339] [Conceptual framework for the application of deep neural networks to surface composition reconstruction from Mercury's exospheric data](https://arxiv.org/abs/2505.11053)
*Adrian Kazakov,Anna Milillo,Alessandro Mura,Stavro Ivanovski,Valeria Mangano,Alessandro Aronica,Elisabetta De Angelis,Pier Paolo Di Bartolomeo,Alessandro Brin,Luca Colasanti,Miguel Escalona-Moran,Francesco Lazzarotto,Stefano Massetti,Martina Moroni,Raffaella Noschese,Fabrizio Nuccilli,Stefano Orsini,Christina Plainaki,Rosanna Rispoli,Roberto Sordini,Mirko Stumpo,Nello Vertolli*

Main category: astro-ph.EP

TL;DR: 该研究探讨了利用深度神经网络（DNN）从水星中性外逸层的原位测量中推导其表面成分的可行性，展示了多层感知器（MLP）的潜力。


<details>
  <summary>Details</summary>
Motivation: 通过外逸层测量获取的行星表面信息可以补充成像数据，为表面释放过程、行星环境相互作用、空间风化和行星演化提供关键见解。

Method: 研究使用多层感知器（MLP）架构，基于模拟的外逸层密度和质子降水通量，预测表面成分，并通过大量训练和测试验证其准确性。

Result: MLP能够准确预测和重建表面成分，展示了算法的鲁棒性和处理复杂数据的能力，为外逸层生成模型提供了估算工具。

Conclusion: 该方法为复杂表面-外逸层相互作用分析提供了潜力，未来可应用于BepiColombo任务的SERENA数据。

Abstract: Surface information derived from exospheric measurements at planetary bodies
complements surface mapping provided by dedicated imagers, offering critical
insights into surface release processes, interactions within the planetary
environment, space weathering, and planetary evolution. This study explores the
feasibility of deriving Mercury's regolith elemental composition from in-situ
measurements of its neutral exosphere using deep neural networks (DNNs). We
present a supervised feed-forward DNN architecture - a multilayer perceptron
(MLP) - that, starting from exospheric densities and proton precipitation
fluxes, predicts the chemical elements of the surface regolith below. It serves
as an estimator for the surface-exosphere interaction and the processes leading
to exosphere formation. Because the DNN requires a comprehensive exospheric
dataset not available from previous missions, this study uses simulated
exosphere components and simulated drivers. Extensive training and testing
campaigns demonstrate the MLP's ability to accurately predict and reconstruct
surface composition maps from these simulated measurements. Although this
initial version does not aim to reproduce Mercury's actual surface composition,
it provides a proof of concept, showcasing the algorithm's robustness and
capacity for handling complex datasets to create estimators for exospheric
generation models. Moreover, our tests reveal substantial potential for further
development, suggesting that this method could significantly enhance the
analysis of complex surface-exosphere interactions and complement planetary
exosphere models. This work anticipates applying the approach to data from the
BepiColombo mission, specifically the SERENA package, whose nominal phase
begins in 2027.

</details>
